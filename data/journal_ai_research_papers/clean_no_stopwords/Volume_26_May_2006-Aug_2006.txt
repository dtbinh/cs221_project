Journal Artificial Intelligence Research 26 (2006) 153-190Submitted 10/05; published 06/06Convexity Arguments Efficient MinimizationBethe Kikuchi Free EnergiesTom Heskest.heskes@science.ru.nlIRIS, Faculty Science, Radboud University NijmegenToernooiveld 1, 6525 ED, Nijmegen, NetherlandsAbstractLoopy generalized belief propagation popular algorithms approximate inference Markov random fields Bayesian networks. Fixed points algorithmsshown correspond extrema Bethe Kikuchi free energy,approximations exact Helmholtz free energy. However, belief propagation always converge, motivates approaches explicitly minimizeKikuchi/Bethe free energy, CCCP UPS.describe class algorithms solves typically non-convex constrainedminimization problem sequence convex constrained minimizations upperbounds Kikuchi free energy. Intuitively one would expect tighter bounds leadfaster algorithms, indeed convincingly demonstrated simulations. Severalideas applied obtain tight convex bounds yield dramatic speed-ups CCCP.1. IntroductionPearls belief propagation (Pearl, 1988) popular algorithm inference Bayesiannetworks. known exact special cases, e.g., tree-structured (singly connected)networks Gaussian discrete nodes. also networks containing cycles,so-called loopy belief propagation empirically often leads good performance (approximatemarginals close exact marginals) (Murphy, Weiss, & Jordan, 1999; McEliece, MacKay,& Cheng, 1998). notion fixed points loopy belief propagation correspondextrema so-called Bethe free energy (Yedidia, Freeman, & Weiss, 2001) importantstep theoretical understanding success.Kikuchi free energy (Kikuchi, 1951) generalization Bethe free energylead better approximations exact Helmholtz free energy. like fixed pointsloopy belief propagation correspond extrema Bethe free energy, fixed pointsalgorithm called generalized belief propagation (Yedidia et al., 2001) correspondextrema Kikuchi free energy.problem loopy generalized belief propagation alwaysconverge stable fixed point. New algorithms (Yuille, 2002; Teh & Welling, 2002)derived therefore explicitly minimize Bethe Kikuchi free energy.describe Section 2, minimization Kikuchi free energy corresponds usually nonconvex constrained minimization problem. Non-convex constrained minimization problemsknown rather difficult solve, Section 3 first derive sufficientconditions Kikuchi free energy convex (over set constraints). Section 4derive class converging double-loop algorithms, inner loopcorresponds constrained minimization convex bound Kikuchi free energy,c2006AI Access Foundation. rights reserved.fiHeskesouter-loop step recalculation bound. Based intuitiontightest bound yields fastest algorithm, come several ideas constructtight bounds. see Yuilles (2002) CCCP algorithm corresponds specialcase rather loose bound discuss relationship UPS algorithm TehWelling (2002) Section 4.5. simulations Section 5 illustrate use tightconvex bounds several inference problems. Implications issues discussedSection 6. Technical details treated appendices.2. Kikuchi ApproximationExact inference graphical models often intractable. section introduceKikuchi approximation particular example variational approach towards approximate inference.2.1 Graphical Modelsundirected graph G = (V, E) consists set nodes vertices V = {1, . . . , N }joined set edges E. place node variable xi takes valuesfinite discrete alphabet. vector containing variables denoted x (x1 , . . . , xn ).Let subset V ; call region. clique fully connected subset V ; Cset cliques. potential, also referred compatibility kernel function, (x )strictly positive function depends variables part clique. define probability distribution probability mass functionpexact (x)1(x ) ,Z(1)CZ normalizing constant, often called partition function. HammersleyClifford theorem (Besag, 1974) guarantees us underlying probability processMarkov respect graph and, vice versa, distribution Markov random field G strictly positive expressed form. processmoralization, directed graphical model (Bayesian network) transformedcorresponding undirected model. Consequently, probability distribution correspondingBayesian network also written form (1) (Lauritzen, 1996).Computing partition function Z, well computing marginals subsets variables, principle requires summation exponential number states. circumventexponential summation two kinds approaches: sampling techniquesvariational methods. sampling, one draws samples exact probability distribution. variational methods try find approximation exact probabilitydistribution.2.2 Variational MethodsVariational methods often derived approximation so-called free energyXXXp(x ) log (x ) +p(x) log p(x) E(p) S(p) .(2)F (p) =C xx154fiEfficient minimization Kikuchi free energyfirst term, E(p), referred energy, second term S(p) entropy.Functional minimization F (p) respect functions p(x) constraintp(x) properly normalized yields pexact (x). Furthermore, partition function Zfollowslog Z = F (pexact ) .stick exact free energy (2), really gain anything: entropypart S(p) still consists sum exponentially many terms. Variational methodsbased tractable approximation free energy. roughly divided twoclasses, mean-field Kikuchi approximations. mean-field approach oneconfines minimization free energy restricted class (tractable) probabilitydistributions instead considering class P probability distributions:log Z = F (pexact ) = min F (p) min F (p) .pPpTcrux choose class entropy S(p) becomes tractable p .Note however restriction typically also affects energy term E(p) (Jordan,Ghahramani, Jaakkola, & Saul, 1998; Jaakkola & Jordan, 1999).Kikuchi approximation free energy (2) leaves energy termapproximates entropy S(p) combination marginal entropies:XXS(p) =p(x) log p(x)c (p)Rx=XcRXp(x ) log p(x ) .(3)xR denotes collection so-called regions; parameters c called Moebiusovercounting numbers.2.2.1 Partially Ordered SetsFollowing Pakzad Anantharam (2002, 2005), use language partially orderedsets posets. Specifically, collection R regions viewed posetordering defined respect inclusion operator . region includesregion , written , variables also part . use denotestrict inclusion, i.e., 6 . say covers R, written ,exists R . visualize posetso-called Hasse diagram region graph (see examples below). Given particular posetR, Hasse diagram GR directed acyclic graph, whose vertices elements R,whose edges corresponds cover relationships. is, edgeiff .2.3 Cluster Variation MethodKikuchis (1951) original cluster variation method (CVM), collections regionsovercounting numbers constructed follows. start defining collectionouter regions. minimal choice original set cliques C, also choose155fiHeskescombine cliques construct larger ones, similar process triangulation (Lauritzen,1996). convenience, redefine potentials correspondingly, i.e.,precisely one potential (x ) per outer region (see example below).Given outer regions, construct new regions taking intersectionsouter regions, intersections intersections, on, intersectionsmade. refer regions constructed way inner regions, combinedcollection I. collection regions R (3) union outerinner regions: R = I.overcounting Moebius numbers original CVM follow MoebiusformulaX(4)c = 1c .definition c = 1 outer regions O.Bethe free energy considered special case Kikuchi free energy.Bethe free energy intersectionsPof intersections, i.e., one levelinner regions c = 1 n n O; 1 equals number outer regionscovering inner region .2.3.1 AlternativesSeveral alternatives original CVM, weaker constraints and/or constraintschoice regions overcounting numbers, proposed recently. Yedidia,Freeman, Weiss (2005) present overview. particular choice inner regionssubsets overcounting numbers junction graphs (Aji & McEliece, 2001) joingraphs (Dechter, Kask, & Mateescu, 2002) leads entropy approximationovercounting numbers inner regions negative. resulting algorithmssimilar junction tree algorithm, applied graph loops.entropy approximation follows original cluster variation method takesaccount entropy contributions level outer regions consistent mannerand, theoretical grounds, seems reason deviate (Pakzad& Anantharam, 2005). paper, therefore focus original cluster variationmethod, analysis holds much generally poset region graph.2.4 Constrained MinimizationKikuchi approximation free energy depends marginals p(x )R. replace minimization exact free energy completedistribution p(x) minimization Kikuchi free energyXXX XFKikuchi (q) =q (x ) log (x ) +cq (x ) log q (x )(5)xR156xfiEfficient minimization Kikuchi free energypseudo-marginals q {q ; R} consistency normalization constraintsq (x ) 0 R xXq (x ) = 1 R(positive)(6a)(normalized)(6b)(consistent)(6c)xXq (x ) = q (x ) , R;x \Referring class pseudo-marginals satisfying constraints Q,approximationlog Z min FKikuchi (q) .qQFurthermore, hope pseudo-marginals q (x ) corresponding minimum accurate approximations exact marginals pexact (x ). Kikuchi freeenergy corresponding marginals exact Hasse diagram turns singlyconnected (Pakzad & Anantharam, 2005).2.5 Illustrationillustration main concepts, consider probability model 4 variables(nodes) pairwise interactions nodes visualized Figure 1(a).obvious shorthand notation, exact distribution form11pexact (x) =ij (xi , xj ) = 12 13 14 23 24 34 .ZZ{i,j}Note potentials originally defined single nodes always incorporateddefinition two-node potentials. region graph corresponding minimalchoice outer regions, i.e., equivalent potential subsets, given Figure 1(b).outer regions pairs nodes, inner regions subsets single nodes.fact, case region graph equivalent so-called factor graph (Kschischang,Frey, & Loeliger, 2001) Kikuchi approximation free energy boilsBethe approximation:XXqij (xi , xj ) log ij (xi , xj )FKikuchi (q) ={i,j} xi ,xj+XXqij (xi , xj ) log qij (xi , xj ) +{i,j} xi ,xjXX(1 ni )qi (xi ) log qi (xi ) ,xini = 3 number outer regions containing inner region i.cluster variation method allows us choose larger outer regions, example,consisting triples {i, j, k}. redefine factorization potentialspexact (x) =1ijk (xi , xj , xk ) = 123 124 134 234 ,Z{i,j,k}157fiHeskes1x1x3EEEE yyyEyEyyy EEEEyy111111, 2 1, 3 1, 4 2, 32, 43, 466 IIIIIIuuuuIIIIu66 IIIuuuu66 III IIII IuIuIuIuuuu66II uu II uuIuIuIu6 IIIIuuuIIuu1234x2x4-2(a) Markov random field.-2-2-2(b) Hasse diagram Bethe approximation.11111, 2, 3I 1, 2, 4I 1, 3, 4I 2, 3, 4II 66IuIuIIuuIIuuIIuuu III uuu IIIII 666II 6uuuuII 66uuIIIIuuuII 6uuuuuuu1, 2 1, 3 1, 4 2, 32, 43, 466 IIIIIIuuuuuuIIII uu -1 uu -1II-1 666 -1-1-1u66 IIII IIII uuIuIuIIuuIIII uuuII uu66uIIIuuuIIuu12341111(c) Region graph Kikuchi approximation.Figure 1: Region graphs Bethe Kikuchi approximations. Lines nodesMarkov random field (a) indicate edges. region graphs (b) (c),outer regions drawn highest level. Lines indicate coveringrelationship, lower regions covered higher regions. obliquenumbers overcounting numbers follow Moebius formula.Bethe approximation (b) corresponds minimal approximationouter regions equivalent cliques graph; pairs nodes.particular Kikuchi approximation (c) follows taking outer regionsnode triples.158fiEfficient minimization Kikuchi free energyexample (distribute symmetrically)1123 [12 13 23 ] 21124 [12 14 24 ] 21134 [13 14 34 ] 21234 [23 24 34 ] 2 ,(assign first outer region)123 12 13 23124 14 24134 34234 1 .corresponding region graph given Figure 1(c). first-level inner regionspairs nodes second-level inner regions single nodes, overcounting numbers -1 1, respectively. Kikuchi approximation entropy boilsXXXSKikuchi (q) =SijkSij +Si .{i,j,k}{i,j}intuitive reasoning behind approximation follows. sum threenode entropies overcounts two-node interactions (each combination {i, j} appears twicerather once), therefore discounted once. single-nodeinteractions much discounted (overcounting number -1 times 3 appearances, compared 3 appearances overcounting number 1 three-node entropies),yielding overcounting number 1 3 (1) 3 (1) = 1.2.6 Generalized Loopy Belief Propagationsummarize, finding Kikuchi approximation partition function Z boilsminimization Kikuchi free energy respect set pseudo-marginalslinear constraints them. Introducing Lagrange multipliers constraints,shown fixed points popular algorithm called loopy belief propagation correspond extrema Bethe free energy and, generally, fixed points generalizedbelief propagation extrema Kikuchi free energy (Yedidia et al., 2001). However,algorithms guaranteed converge minimum practice get stuckexample limit cycles. explains search convergent alternatives directlyminimize Kikuchi free energy, topic rest paper.3. Convexity Kikuchi Free Energysection derive sufficient conditions Kikuchi free energy convexset consistency constraints (6). relevant Kikuchi freeenergy indeed convex constraint set, must unique minimumminimization problem relatively straightforward. Furthermore, argument159fiHeskesuse deriving conditions play important role construction efficientminimization algorithms later on.3.1 Sufficient Conditionsconsider Kikuchi free energy (5) function pseudo-marginals q.reasoning convexity, disregard energy term linear q.entropy terms give either convex concave contribution, depending whethercorresponding overcounting numbers positive negative, respectively. Ignoringconstraints (6), free energy (5) convex concave contributions vanish,i.e., c = 0 R .However, really care subspace induced constraints (6). Therefore introduce notion convexity set constraints. call free energyconvex set constraints (6)F (q1 + (1 )q2 ) F (q1 ) + (1 )F (q2 ) 0<<1 q1 ,q2 Q .Note that, since constraints linear, q1 q2 satisfy constraints (6),q1 + (1 )q2 . following, talk convexity Kikuchi freeenergy, conditioning constraint set implicitly assumed.One way proceed make use (consistency) constraints express Kikuchifree energy terms outer region pseudo-marginals study convexity.approach along lines. particular, replace inner region pseudomarginals correspond concave contributions outer region pseudo-marginals.pseudo-marginals corresponding convex contributions concern. fact, mayable use convex contributions well compensate concavecontributions.make reasoning precise, define positive regions (or perhaps better,nonnegative) R+ , R+ { R; c 0} I+ negative regions R ,R { R; c < 0} . idea, formulated following theorem,Kikuchi free energy convex compensate concave contributionsnegative regions R convex contributions positive regions R+ .Theorem 3.1. Kikuchi free energy convex set constraints (6)exists allocation matrix positive regions R+ negative regionsR satisfying6= 0( used compensate )(7a)0Xc(positivity)(7b)(sufficient amount resources)(7c)(sufficient compensation)(7d)R+X|c |R160fiEfficient minimization Kikuchi free energyProof First all, note worry energy termslinear q. words, prove theorem restrict showingminus entropyXXS(q) =c (q )|c |S (q )R+Rconvex set constraints.intermediate step, let us consider combination convex entropy contributionpositive region R+ concave entropy contribution negative inner regionR , subset :XX(q) [S (q) (q)] =q (x ) log q (x )q (x ) log q (x )x=Xq (x ) log q (x )=xq (x ) log q (x )xxXxXq (x )Xx\q (x\ |x ) log q (x\ |x ) ,used standard definitionsXq (x )q (x ) q (x\ |x )q (x ).q (x )x\first step, applied constraint q (x ) = q (x ) extended summationx second term summation x . second step basically turneddifference two entropies (a weighted sum of) conditional entropies.difference , depends q , is, Lemma A.1 Appendix A, convexq . words, concave contribution fully compensated convexcontribution , yielding overall convex term relevant set constraints.resulting operation matter resource allocation. concave contribution |c |S find convex contributions compensate it. Let denoteamount resources take positive region R+ compensatenegative region R . Obviously, positive region compensate negative regionscontains, = 0 subset , explains condition (7a).Now, shorthand notation little bit rewritingXX|c |ScS(q) =R+=XR+=XR+ccRX+XXX XR+161XRX+[S ]XXR|c |X|c | .fiHeskesPConvexity first term guaranteedP c 0 (7c), second term0 (7b), third term |c | 0 (7d).3.2 Checking ConditionsChecking conditions Theorem 3.1 cast form linear programmingproblem, example follows. define auxiliary variable replacing condition (7c)X(8)= |c | R (variable compensation)solve linear programming problem attempts maximize single variable constraints implied four conditions. interpretationtry use available resources compensate much concave contributionscan. find solution 1 conditions satisfied: Kikuchi free energyconvex set constraints unique minimum. optimal turnssmaller 1, matrix satisfying constraints convexityKikuchi free energy guaranteed Theorem 3.1.Instead solving linear program, often get away simpler checks.example, guess particular check whether conditions (7) hold. obviouschoiceXc= n1,nR ,satisfies condition (7c) substituted (7d) yields conditionXc +R+ ,c0nR .(9)Similarly, choice=X|c |+n1n+R ,+satisfies condition (7d) yields conditionXR ,c+ c 0 R+n+(10)substituted (7c). (9) (10) holds, Theorem 3.1 guarantees convexityKikuchi free energy.two conditions sufficient, necessary Theorem 3.1 apply.necessary conditionXXc 0(11)c +RR+easily derived summing condition (7d) R substituting condition (7c). condition (11) fails, cannot use Theorem 3.1 prove convexityKikuchi free energy.162fiEfficient minimization Kikuchi free energywould like conjecture conditions Theorem 3.1 sufficient,also necessary convexity Kikuchi free energy. pursuehere, irrelevant current purposes. Furthermore, mayrelevant practice either, since convexity sufficient necessary condition unique minimum. Tatikonda Jordan (2002), Heskes (2004), Ihler,Fisher, Willsky (2005) give conditions convergence loopy belief propagationuniqueness minimum corresponding Bethe free energy. conditionsdepend graphical structure, also (strength the) kernels (x ).3.3 Related WorkChiang Forney (2001) present similar ideas, convex entropy terms compensatingconcave terms set constraints, derive conditions convexity Bethefree energy pairwise potentials. resulting conditions formulated termssingle-node marginals, may difficult validate practice generalizeKikuchi case.Closely related Theorem 3.1 following theorem Pakzad Anantharam(2002, 2005).Theorem 3.2. (Pakzad & Anantharam, 2002, 2005) Kikuchi free energy (5) convexset consistency constraints imposed collection regions R (and henceconstrained minimization problem unique solution) overcounting numbers cc satisfy:XXR,c +c 0 .(12)R\S:S,words, subset R, sum overcounting numbers elementsancestors R must nonnegative.fact, using Halls (1935) matching theorem, shown conditions (7)Theorem 3.1 equivalent conditions (12) Theorem 3.2. latterdirect require solution linear program.Theorem 3.1 Theorem 3.2 used show Bethe free energygraphs single loop convex set constraints (Heskes, 2004; McEliece &Yildirim, 2003; Pakzad & Anantharam, 2002, 2005).3.4 Minimization Convex Kikuchi Free EnergyKikuchi free energy convex, guaranteed unique minimum,minimum also relatively easy find message-passing algorithm similarstandard (loopy) belief propagation.basic idea follows. focus case overcounting numberspositive. case negative overcounting numbers involved workedAppendix B. Furthermore, rest paper ignore positivityconstraints (6a). easy check satisfied solutions obtain.introduce Lagrange multipliers (x ) consistency constraints well163fiHeskesnormalization constraints construct LagrangianXXXL(q, ) = FKikuchi (q) +q (x )(x ) q (x ),+Xx \x1Xxq (x ) .(13)Minimization Kikuchi free energy appropriate consistency normalization constraints is, terms Lagrangian, equivalentmin FKikuchi (q) = min max L(q, ) ,qqQminimization q unconstrained. Standard results constrainedoptimization (e.g., Luenberger, 1984) tell usmin max L(q, ) max min L(q, ) ,qqequality convex problems linear equality constraints. is, convexproblems allowed interchange maximum minimum q.Furthermore, optimal q () corresponding minimum Lagrangian (13)function unique, since L(q, ) convex q . Substitution solutionyields so-called dualL () min L(q, ) = L(q (), ) .(14)qdual concave unique maximum.Many algorithms used find maximum dual (14). particularone, derived Appendix B, given Algorithm 1. slightly differs presentedYedidia et al. (2005) Yuille (2002) sending messages (messages directly relatedLagrange multipliers) inner regions outer regions, i.e., neverinner regions subsets inner regions. price one pay updateline 7 depends overcounting number c . Bethe free energy, c = 1 n ,obtain standard (loopy) belief propagation update rules. particular orderingAlgorithm 1, running inner regions updating messages innerregion neighboring outer regions, guarantees dual (14) increasesiteration1 . local partition functions Z Z lines 10 7 chosennormalize pseudo-marginals q (x ) q (x ). normalization strictlynecessary, helps prevent numerical instability. Algorithm 1 initializedsetting messages (x ) = 1 skipping lines 3 6 first iteration.1. positive overcounting numbers c . argumentation negative overcounting numberscomplicated may require damping updates achieve convergence. See Appendix B details.164fiEfficient minimization Kikuchi free energyAlgorithm 1 Message-passing algorithm constrained minimization Kikuchi freeenergy.1:converged2:3:4:O,Xq (x ) =q (x )x\5:(x ) =q (x )(x )6:end7:q (x ) =8:O,q (x )(x ) =(x )1q (x ) =(x )(x )ZI,1n +c1(x )Z O,9:10:11:end12:end13:end4. Double-Loop Algorithms Guaranteed ConvergenceEven Kikuchi free energy convex, still run Algorithm 1hope converges fixed point. fixed point must correspondextremum Kikuchi free energy appropriate constraints (Yedidia et al.,2001). Even better, empirically general Kikuchi free energy provablyBethe free energy (Heskes, 2003), extremum fact minimum. However, practicesingle-loop2 algorithm always converge resort double-loopalgorithms guarantee convergence minimum Kikuchi free energy.4.1 General Procedureintroduce class double-loop algorithms based following theorem.2. Note single loop refers message-passing algorithm nothingnotion single loop graphical model.165fiHeskesTheorem 4.1. Given function Fconvex (q; q ) propertiesFconvex (q; q ) FKikuchi (q)q,q QFconvex (q; q) = FKikuchi (q)fiFconvex (q; q ) fifiFKikuchi (q)fi =qqq =qFconvex (q; q ) convex q Q(bound)(15a)qQ(touching)(15b)q Q(convex)(15c)algorithmqn+1 = argmin Fconvex (q; qn ) ,(16)qQqn pseudo-marginals iteration n, guaranteed converge local minimumKikuchi free energy FKikuchi (q) appropriate constraints.Proof immediate Kikuchi free energy decreases iteration:FKikuchi (qn+1 ) Fconvex (qn+1 ; qn ) Fconvex (qn ; qn ) = FKikuchi (qn ) ,first inequality follows condition (15a) (upper bound) seconddefinition algorithm. gradient property (15b) ensures algorithmstationary points gradient FKikuchi zero. construction qn Qn.See Figure 2 illustration algorithm proof. fact, convexityFconvex used establish proof. But, argued Section 3.4,algorithmic point view constrained minimization convex functional much simplerconstrained minimization non-convex functional. general idea, replacingminimization complex functional consecutive minimization easierhandle upper bound functional, forms basis popular algorithmsEM algorithm (Dempster, Laird, & Rubin, 1977; Neal & Hinton, 1998) iterativescaling/iterative proportional fitting (Darroch & Ratcliff, 1972; Jirousek & Preucil, 1995).Intuitively, tighter bound, faster algorithm.4.2 Bounding Concave Termsfirst step, lay main ideas, build convex bound removing concaveentropy contributions . so, make use linear boundXXq (x ) log q (x )(17)q (x ) log q (x ) ,xxdirectly follows0KL(q , q )=Xx"q (x )q (x ) logq (x )166#fiEfficient minimization Kikuchi free energy(1)(2)(3)Figure 2: Illustration proposed algorithm corresponding convergence proof.iteration n, Fconvex (q; qn ) (dashed line) convex bound non-convexFKikuchi (q) (solid line). touch qn , point (1), Fconvex (qn ; qn ) =FKikuchi (qn ).minimum, point (2), Fconvex (qn+1 ; qn )Fconvex (qn ; qn ). corresponding Kikuchi free energy, point (3), obeysFKikuchi (qn+1 ) Fconvex (qn+1 ; qn ) bounding property.KL Kullback-Leibler divergence. choice Fconvex readsXXXXq (x )(1)Fconvex (q; q ) =q (x ) logq (x ) log q (x )c+(x )xxI+XXXX|c | 1q (x ) . (18)|c |q (x ) log q (x ) +xxeasy check functional properties (15a) (15c). last termadded fulfill property (15b). Next make crucial observation that, using(1)constraints (6) fixed q , rewrite Fconvex normal form (5):X XXXq (x )(1)Fconvex(q; q ) =q (x ) log+cq (x ) log q (x ) + C(q) , (19)(x)xxC(q) evaluates zero q Q , implicitly depends q ,c definedX |c |0log q (x ) c.(20)log (x ) log (x ) +c I+n,is, always incorporate terms linear q energy termredefinition potentials. chosen distribute terms equallyn neighboring outer regions, choices possible well.167fiHeskesterm C(q) (19) evaluates zero q Q thus irrelevantoptimization inner loop. consists terms last one (18)(1)serve make bound Fconvex satisfy (15b). construction bounds below,ignore terms: affect algorithm way3 .(1)Fconvex convex normal form, use Algorithm 1solve constrained problem (16). resulting double-loop algorithm describedtwo lines.Outer loop: recompute (20) q = qn .Inner loop: run Algorithm 1 c c, yielding qn+1 .inner loop, initialize messages converged values previousinner loop.4.3 Bounding Convex Termssection show many cases make algorithm bettersimpler. idea bound concave, also convex entropycontributions inner regions. is, enforce c 0 setXXq (x )(2)Fconvex (q; q ) =q (x ) log,(21)(x )xlog (x ) log (x )X clog q (x ) .n(22)(2)Let us first explain algorithm based Fconvex simpler one based(21), reference inner regions disappeared. fact, constraintscare outer regions pseudo-marginals agreeintersections. Consequently, inner loop (Algorithm 1), runinner regions direct intersections outer regions, is,exist outer regions x = x x . Similar argumentsused algorithm based (19) well, neglecting negative inner regionscorrespond direct intersections outer regions. practice, however,negative inner regions direct intersections outer regions, whereas many positiveinner regions arise next level, intersections intersections. See instanceexample Figure 1, six negative inner regions direct intersections outerregions, contrast four positive inner regions.(2)(17), applied positive inner regions, clear Fconvex (q; q )(1)(2)(1)Fconvex (q; q ): bound, Fconvex tighter bound Fconvex expect(2)algorithm based Fconvex perform better. remains shown(2)conditions FKikuchi (q) Fconvex (q; q ). following theorem comes in.(1)Fconvex .3. Alternatively, could relax condition (15b) statement gradients Fconvex FKikuchiequal subspace orthogonal constraints. milder condition, C(q)well last term (18) longer needed.168fiEfficient minimization Kikuchi free energyTheorem 4.2. functional Fconvex (21) convex bound Kikuchi free energy (5) exists allocation matrix negative inner regionspositive inner regions I+ satisfying6= 0( used compensate )(23a)0X|c |(positivity)(23b)(sufficient amount resources)(23c)(sufficient compensation)(23d)XcI+Proof surprisingly, proof follows line reasoning proof Theorem 3.1. First consider combination concave entropy contribution(17) convex entropy contribution I+ , :XXq (x ) log q (x ) +q (x ) log q (x )xxXq (x ) log q (x ) +xXq (x ) log q (x ) ,(24)xfollowsq (x\ |x )0q (x ) q (x\ |x )q (x\ |x )xXq (x )q (x ) q (x )=q (x )log,(x )q(x)q(x)qxXrecognize term braces Kullback-Leibler divergence twoprobability distributions.(2)show difference Fconvex FKikuchi nonnegative,able compensate concave contributions c I+ convex contributions , without exceeding available amount resources|c |. shorthand notation,#"Xq (x )K,q (x ) logq (x )xdecompositionXXX(2)FconvexFKikuchi =c K =c K|c |K=X|c |XK +X XI+(K K ) +XI+169Xc K 0 ,fiHeskes123456123451237124571346723567456761212331245134623564561271372371472574573674675671 1 1 1 1 1 1 1 1 1 1 1 1 14561213231425453646561727374757671111111111111117(a) Outer regions.71234561111111(b) Region graph.Figure 3: Smallest example showing conditions Theorem 4.2 need alwayshold region graph overcounting numbers constructed clustervariation method. (a) Visualization outer regions: black meansvariable (1 7) part outer region (1 6).(b) Region graph overcounting numbers boldface. positive overcounting numbers third level outweigh negative overcounting numbers second level.inequality follows since terms guaranteed nonnegativeconditions (23) satisfied.above, conditions Theorem 4.2 checked linear program.generated many different sets overcounting numbers resulting Moebius formula (4), started wondering whether conditions (23) perhaps automatically satisfied. However, exhaustively checking possible outer region combinations given fixednumber variables, come counterexample. smallest counterexampleviolates conditions Theorem 4.2, illustrated Figure 3.Even if, counterexample, positive inner regions compensatednegative inner regions, pay get rid many possible. Finding optimalassignment may complex problem, heuristics easy find (see Appendix C).4.4 Pulling Tree(1)previous section tightened convex bound Fconvex Kikuchi free energyFKikuchi bounding convex contributions positive regions well. Another wayget tighter bound bound part concave contributions negative170fiEfficient minimization Kikuchi free energyinner regions. first illustrate considering Bethe free energy, i.e.,non-overlapping negative inner regions (nodes) c = 1 n .Bethe free energy convex singly-connected structures. Inspired TehWelling (2002), choose set nodes Ibound remaining nodes Ifreebecome singly-connected takeXXXXq (x )(3)Fconvex (q; q ) =q (x ) log+(1 n )q (x ) log q (x )(x )xxIfreeXX+(1 n )(25)q (x ) log q (x ) .Iboundxis, bound entropy terms corresponding bounded nodes Iboundsimply keep entropy terms correspond free nodes Ifree . constructionFconvex satisfies conditions (15). Furthermore, rewritten normal form (5)definitionsX 1 n0Iboundlog (x ) log (x ).log q (x ) c1nnIfree,boundNote resulting inner-loop algorithm completely equivalent running standard belief propagation tree free nodes: send messagesbounded nodes Ibound well enforce constraints q (x ) = q (x ), .Rather pulling single tree, also pull convex combinationtrees. is, suppose several bounds, result pullingparticular tree corresponding set overcounting numbers ci .convex combinationXXc =wi ci wi 0wi = 1also corresponds convex bound. generally, combine ideasprevious section choosing c resulting bound convex.procedure given Appendix C. Basically, first try shield muchconcave entropy contributions convex entropy contributions can. Next,tighten bound incorporating convex contributions linear boundsconcave contributions manage shield first step. stepscast form easy solve linear programming problem.4.5 Related Work(1)double-loop algorithm described Section 4.2 based Fconvex closely relatedYuilles (2002) CCCP (concave-convex procedure) algorithm. Although originally formulated completely different way, CCCP applied minimization Kikuchi freeenergy also understood particular case general procedure outlinedTheorem 4.1. specifically, based bounding concave contributionsXXX|c |q (x ) log q (x )q (x ) log q (x ) (|c | 1)q (x ) log q (x ) , (26)xxx171fiHeskescompared (17). is, bounding concave entropy contributions, part concave terms taken convex side. reasonCCCP algorithm requires functional convex, independentconstraints involved4 . procedure, hand, makes use factfunctional convex set constraints. allows us usetighter bounds, yielding efficient sometimes simpler algorithms. less important note, inner-loop algorithm particular message-passing scheme appliedYuille (2002) somewhat different.(3)double-loop algorithm based Fconvex (25) inspired Teh Wellings(2002) UPS (unified propagation scaling) algorithm. differencebound entropy contributions nodes tree, UPS nodes (and thusentropy contributions) clamped values resulting previous inner loop.is, inner loop UPS algorithm corresponds minimizingXXXXq (x )UPSFconvex (q; q ) =q (x ) log+(1 n )q (x ) log q (x )(x )xxIfreeXX(1 n )q (x ) log q (x ) .Iclampedxconstraintsq (x ) = q (x ) Ifree , , yet q (x ) = q (x ) Iclamped , .boils iterative scaling algorithm, also relatively easy solve.outer-loop iteration, different choice made Ifree Iclamped . UPSalgorithm understood coordinate descent guaranteed convergelocal minimum Bethe free energy (under appropriate conditions choices made(3)Ifree Iclamped ). inner loop results Fconvex also allows changesmarginals q (x ) Ibound , i.e., flexible make larger steps. Loosely(3)UPS . Furthermore, approachspeaking, Fconvex tighter bound Fconvexchoose different subdivisions bounded free nodeswithin inner loop.Wainwright, Jaakkola, Willsky (2002b, 2002a) present similar ideas, exploitingconvexity Bethe free energy tree structures. Wainwright et al. (2002b) usetree structure obtain efficient implementation loopy belief propagation, withouthowever guaranteeing convergence. Wainwright et al. (2002a) show particular convexcombinations convex Bethe free energies lead convex bounds exact Helmholtzfree energy (2). bounds, overcounting numbers inner regions still followMoebius relation (4), overcounting numbers outer regions smallerequal 1. Constrained minimization bound similar constrained(3)minimization Fconvex algorithm used Wainwright, Jaakkola, Willsky (2003)indeed closely related Algorithm 1.4. procedure described Yuille (2002) often even moves part convex terms concave side.makes (implicit) bound even worse corresponding algorithm slower. followingstick favorable interpretation CCCP algorithm based implicitbound (26).172fiEfficient minimization Kikuchi free energy5. SimulationsIntuitively, would expect algorithms based tightest bound convergefastest terms outer-loop iterations. However, larger steps outer loop,might need inner-loop iterations achieve convergence inner loop.following simulations designed check this.5.1 General Set-upsimulations compare four different algorithms, based differentbound.convex tightest bound Kikuchi free energy convex. Basedideas described Section 4.4 Appendix C.negative zero bound obtained setting negative overcounting numberszero, explained Section 4.2.zero bound described Section 4.3 follows setting overcountingnumbers, negative positive, zero. models considered below,overcounting numbers satisfy conditions Theorem 4.2, i.e., setting zeroindeed yields bound Kikuchi free energy. Note zeroequivalent negative zero Bethe free energy.cccp (rather favorable interpretation the) bound implicit Yuilles (2002) CCCPalgorithm, explained Section 4.5.Algorithm 1 applied inner loop algorithms: differencesetting overcounting numbers c implied bound.inner loop runs preset convergence criterion met. Specifically, end inner loopinner region marginals change less 104 . criterion algorithmshappened converge, probably would also case looser criteria.example, Yuille (2002) reports two inner-loop iterations sufficient obtainconvergence.simulations report Kullback-Leibler (KL) divergence exactapproximate marginals, either summed nodes subset nodes. Plotsdifferent error functions look much same. Kikuchi/Bethe free energysomewhat less illustrative: close minimum, marginalsthus KL divergence still change considerably. visualize KL divergencefunction outer-loop iterations function floating point operations,count necessary operations involved inner-loop outer-loop updates (i.e.,involved convergence checks, computing KL divergence, on).comparing number inner-loop iterations used different algorithms meetconvergence criterion, scale outer-loop iterations relative outer-loop iterationsconvex algorithm. is, number outer-loop iterations usedalgorithm reach particular level accuracy, consider corresponding numberouter-loop iterations used convex algorithm reach level.173fiHeskes(a)(b)just_convexnegative_to_zerocccpkldivergencekldivergencejust_convexnegative_to_zerocccp010201021010020406080outerloop iterations100012flops346x 10Figure 4: Bethe approximation 9 9 Boltzmann grid. Kullback-Leibler divergenceexact approximate single-node marginals function outerloop iterations (a) floating point operations (b) three different algorithms.done simulations quite number different problems problem instances, involving Markov random fields Bayesian networks. results shownexemplary meant illustrate general findings summarizebelow.5.2 Bethe Free Energy Boltzmann Gridfirst set simulations concerns minimization Bethe free energy Boltzmann grid 9 9 nodes pairwise interactions formtjti(27)ij (xi , xj ) = exp wij (2xi 1)(2xj 1) + (2xi 1) + (2xj 1)ninjni number neighbors node i, i.e., 2 corner node, 3 nodesboundary, 4 nodes middle. Weights wij biases ti drawnrandom normal distribution mean zero standard deviation 0.5.Bethe approximation outer regions pairs neighboring nodes.Figure 4 shows summed KL divergence exact approximate single-nodemarginals function number outer loop iterations (a) functionnumber floating point operations (b) convex, negative zero,cccp algorithms. seen that, expected, convex algorithms convergesfaster negative zero algorithm, converges faster cccp algorithm. speed-up terms outer-loop iterations translates almost equivalentspeed-up terms flops. Indeed, seen Figure 5(a), number inner-loopiterations required convex algorithm slightly highertwo algorithms.curves Figure 4(a) mapped onto rough linear scalingnumber outer-loop iterations. also suggested straight lines174fiEfficient minimization Kikuchi free energy2outerloop iterations10(b)number innerloop iterations(a)just_convexnegative_to_zerocccp110010010864201just_convexnegative_to_zerocccp10outerloop iterations10203040outerloop iterations (scaled)Figure 5: Bethe approximation 9 9 Boltzmann grid. (a) Outer loop iterationsconvex algorithm versus corresponding outer-loop iterationstwo algorithms. (b) Number inner loop iterations needed meetconvergence criterion function outer-loop iterations, scaled according(a).Figure 5(a). slope lines relate 0.34, 1 (by definition), 1.35convex, negative zero cccp, respectively (see also convergence ratesTable 1). following argumentation shows striking correspondencenumbers respective bounds.negative overcounting numbersPBethe free energy FKikuchiP add c = 207. respective convexbounds Fconvex , sums c = 144, 0, 81. translatefraction negative overcounting mass bounded, i.e.,PPccP,cobtain, respectively 0.30, 1 (by definition), 1.39. is, appearsalmost linear relationship tightness bound (here expressed fractionconcave entropy contributions bounded linearly) speed convergence.noticed almost linear relationship simulations involvingBethe free energy (no positive overcounting numbers).5.3 Kikuchi Free Energy Boltzmann Gridsecond set simulations also 99 Boltzmann grid, outer regionschosen squares four neighboring nodes. Potentials form (27)weights biases drawn normal distribution standard deviation 4 0.5,respectively. Note size weights much larger previous setsimulations, make problem still bit challenge Kikuchi approximation.weights, Bethe approximation badly (summed Kullback-Leibler175fiHeskes(a)(b)2210just_convexnegative_to_zeroall_to_zerocccp110kldivergencekldivergence10010just_convexnegative_to_zeroall_to_zerocccp1100100200400600outerloop iterations8000510flops157x 10Figure 6: Kikuchi approximation 9 9 Boltzmann grid. Kullback-Leibler divergenceexact approximate single-node marginals function outerloop iterations (a) floating point operations (b) four different algorithms.divergence larger 10). Bethe Kikuchi algorithm, singleloop algorithm convergence problems: Bethe approximation typically getsstuck limit cycle Kikuchi approximation tends diverge. total8 8 = 64 outer regions (8 7) 2 = 122 negative inner regions (all node pairscorrespond intersections outer regions) 7 7 = 49 positive inner regions(all single nodes correspond intersections node pairs).Figure 6 shows KL divergence approximate exact single-node marginalsfour different algorithms terms outer-loop iterations (a) floating pointoperations (b). seen ordering (a) expected: tighterbound, faster algorithm. terms floating point operations, convexzero algorithm get much closer together.Part explanation given Figure 7: convex algorithm requires considerably inner-loop iterations meet convergence criterion.effect zero algorithm inner loop runs 112 negativeinner regions instead 161 positive negative inner regions. makesinner-loop iteration zero requires factor 1.8 less floating point operationsinner-loop iteration three algorithms.difficult find quantitative relationship tightnessbounds (asymptotic) convergence rates. One complicationsnegative, also positive overcounting numbers play role. case,algorithms still seem converge linearly, faster convergence rates tighter bounds.convergence rates, expressed time scale corresponding exponential decay(KL(t) KL() exp[t/ ], outer-loop iterations), summarizedTable 1.176fiEfficient minimization Kikuchi free energy3outerloop iterations10(b)number innerloop iterations(a)just_convexnegative_to_zeroall_to_zerocccp2101100100102520151050110outerloop iterationsjust_convexnegative_to_zeroall_to_zerocccp510152025outerloop iterations (scaled)Figure 7: Kikuchi approximation 9 9 Boltzmann grid. (a) Outer loop iterationsconvex algorithm versus corresponding outer-loop iterationsthree algorithms. (b) Number inner loop iterations needed meetconvergence criterion function outer-loop iterations, scaled according(a).Figure 8: Graphical structure QMR-like network.5.4 QMR Networkthird set simulations concerns QMR-like (Quick Medical Reference) Bayesiannetwork (Heckerman, 1989; Jaakkola & Jordan, 1999): bipartite graph layerdisease nodes layer findings. particular network used simulationsgenerated Bayes Net Toolbox (Murphy, 2001). contains 20 finding nodes,18 observed (positive), 10 hidden disease nodes; see Figure 8. diseasesBernoulli probability distributions prior drawn random 0 0.01.findings noisy-or conditional probability distributions without leakage. Diseasesfindings linked randomly probability 0.5. absence leakage, large amountfindings, strong connectivity make relatively difficult inference problem.outer regions take subsets implied conditional probability distribution, i.e.,outer region consists disease findings linked it. Figure 9 givescorresponding region graph.177fiHeskes111111111111111111111111111111111 0 0 1 0 0 0 1 1 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 100110111102122011111011111011Figure 9: Region graph resulting QMR-like network.(a)just_convexnegative_to_zeroall_to_zerocccp10102100200400600outerloop iterationsjust_convexnegative_to_zeroall_to_zerocccp0kldivergence0kldivergence(b)8002100246flops8108x 10Figure 10: Kikuchi approximation QMR-like network. Kullback-Leibler divergenceexact approximate single-node marginals function outerloop iterations (a) floating point operations (b) four different algorithms.178fiEfficient minimization Kikuchi free energy(b)number innerloop iterationsouterloop iterations(a)just_convexnegative_to_zeroall_to_zerocccp210110010010352520151050110outerloop iterationsjust_convexnegative_to_zeroall_to_zerocccp3051015outerloop iterations (scaled)20Figure 11: Kikuchi approximation QMR-like network. (a) Outer loop iterationsconvex algorithm versus corresponding outer-loop iterationsthree algorithms. (b) Number inner loop iterations needed meetconvergence criterion function outer-loop iterations, scaled according(a).originalconvexnegative zerozerocccp-207-1440081Bethe+003.80 11.30 11.30 15.3Kikuchi+-112 49-641110 49410029112 49 153-54-340052QMR+35156356701735 166Table 1: Summary asymptotic convergence ( time constant, time outerloop iterations, exponential decay) sums negative positive overcounting numbers original Kikuchi/Bethe free energy convex boundsused different algorithms.results found Figure 10 11. comparableKikuchi approximation Boltzmann grid. Also single-loop algorithm failsconverge. convex algorithm converges much faster three algorithms, requires inner-loop iterations less efficient zero algorithm, makes latter preferable terms floating point operations. However,relatively straightforward speed-up convex algorithm. First, probablyneed many inner-loop iterations outer loop converge properly.secondly, bound part entropy contribution, efficient choicewould many zero overcounting numbers possible.179fiHeskes5.5 General Findingssummarize points illustratedencountered many simulations well.tighter (convex) bound used inner loop, faster convergenceterms outer-loop iterations.number outer-loop iterations needed meet prespecified convergence criterion tends decrease looser bound, never nearly enough compensateslower convergence outer loop.fact, observed strong dependency number inner-loopiterations tightness bound bound convex problemhard sense single-loop algorithm would fail converge.terms floating point operations, looser bound sets overcounting numbersinner loop zero, beat tighter bound negative overcounting numbers:slower convergence terms outer-loop iterations compensatedefficient inner loop.Pelizzola (2005) tests several convergent algorithms Kikuchi approximations problems statistical physics reports similar findings. Also study, convex algorithm, described first time Heskes, Albers, Kappen (2003), clearly outperforms competitors.6. Discussionarticle based perspective interested minima Kikuchifree energy appropriate constraints. Finding minimum becomes possibly non-convex constrained minimization problem. Here, well studies,approach solve non-convex problem sequential constrained minimization convex bounds Kikuchi free energy. presumption tighterbounds yield faster algorithms, worked several ideas construct tight convexbounds. simulation results article well obtained Pelizzola (2005)clearly validate presumption show speed-ups significant.Heskes, Zoeter, Wiegerinck (2004) apply bounds (approximate) parameterlearning directed graphical models.double-loop algorithms considered article based convex boundsKikuchi free energy. principle, necessary: concerninner-loop algorithm converges might well case tighter bounds. Onepractical solution simply choose (tight) bound Kikuchi free, check whetherinner-loop algorithm converge, restart looser bound not. Alternatively,construct tighter bounds making use conditions guaranteed convergencebelief propagation derived Tatikonda Jordan (2002), Heskes (2004),Ihler et al. (2005) Bethe approximation.suggested non-convergence single-loop generalized/loopy belief propagation indication Kikuchi/Bethe approximation inaccurate.180fiEfficient minimization Kikuchi free energyresults Section 5.3 5.4 show need always case. Apparently,exist middle range problems Kikuchi free energy easy minimize, yield decent approximations. problems algorithmsdescribed article useful.Acknowledgmentsauthor would like thank Wim Wiegerinck, Onno Zoeter, Kees Albers, Bert Kappen fruitful discussions anonymous reviewers constructive comments.work supported part Dutch Technology Foundation STW.Appendix A: Convexity Difference Two Entropiesappendix treats two lemmas convexity difference two entropies.first one used proof Theorem 3.1. similar lemma used McElieceYildirim (2003).Lemma A.1. difference two entropiesXXq (x ) log q (x )q (x ) log q (x )(q )xx=Xxconvex q .q (x )Xx\q (x\ |x ) log q (x\ |x )Proof take step backwards write(q ) =Xxq (x ).q (x ) logXq (x )x\taking derivatives, best interpret table q , specifying value q (x )possible realization x , vector x playing role index. Taking secondderivatives, obtainHx ,x (q )2 (q )11.=Ix ,xq (x )q (x )q (x ) q (x ) x ,xIx,x 1 elements x x equal zero otherwise.181fiHeskesNext would like show matrix positive semi-definite, i.e.,tables q, interpreted vectors indices x ,0Xq(x )Hx ,x (q )q(x ) =x ,xxX q 2 (x , x )X\=q (x\ , x )x x\X q(x )q(x )q (x )q (x ) x ,xx ,xq(x\ , x )q(x\ , x )X q 2 (x )Xx\ ,x\q (x )i2hPX X q 2 (x\ , x )x\ q(x\ , x )=P.q (x\ , x )x\ q (x\ , x )x x\Cauchys inequality,Xa2kkXb2kk"Xkak bk#2,follows term braces indeed semi-positive qrealization x .see this, make substitutions x\ k, q(x\ , x )/ q (x\ , x ) ak ,qq (x\ , x ) bk find{. . .}Xka2kP2k ak bk ]P0.2k bk[following related lemma used Appendix B.Lemma A.2. difference two entropiesXX(q , q )q (x ) log q (x )q (x ) log q (x )xxconvex {q , q }.Proof Hessian matrix componentsHx ,x2 (q )1=Ix ,xq (x )q (x )q (x )Hx ,x2 (q )1=q (x )q (x )q (x ) x ,xHx ,xq (x )2 (q ).= 2q (x )q (x )q (x ) x ,x182fiEfficient minimization Kikuchi free energyConvexity requires q = (q (x ), q (x )),q (x ) q (x )0=X q2 (x )2Hx ,xHx ,xHx ,xHx ,xX q (x )q (x )q (x )Xq (x ) q (x ) 2.q (x )=q (x ) q (x )xxq (x )x+!q (x )q (x )X q (x )q2 (x )q2 (x )xAppendix B: Minimizing Convex Kikuchi Free Energyappendix, derive Algorithm 1 minimizing convex Kikuchi free energyappropriate linear constraints. simplify notation, use convention runsouter regions, inner regions.First, note principle necessary explicitly take accountconstraints (6), since constraints implied others. Obviously, constrainttwo inner region marginals,q (x ) = q (x ) ,implied corresponding constraints inner region marginals outerregion subsuming inner regions,q (x ) = q (x ) q (x ) = q (x ) .is, take account constraints inner regionsinner regions. Similarly, normalization constraints outer region pseudo-marginals follownormalization constraints inner region pseudo-marginals. So, sufficient setconstraintsXq (x )q (x ) = q (x ) q (x ) =x\Xq (x ) = 1.xIntroducing Lagrange multipliers (x ) corresponding constraints,obtain LagrangianX XXXq (x )q (x ) log q (x )+cL(q, ) =q (x ) log(x )xxXXXXXX+(x ) q (x )q (x ) +1q (x ) . (B-1)xx\183xfiHeskesConvex Independent ConstraintsLet us first consider case overcounting numbers c strictly positive (c >0). Then, Lagrangian convex set constraints, convexq independent constraints. Minimization Lagrangian respectpseudo-marginals follows setting derivatives zero, yieldinge (x )(B-2)q (x ) = (x )e1q (x )/c 1= ee (x )/c ,(B-3)following noted q q functionsLagrange multipliers . Substituting solution back Lagrangian, obtaindualXXXX XL () L(q (), ) =q (x )cq (x ) .(B-4)xxNow, consider optimizing L () respect subset components correspondinginner region , collected ( , (x ) ,x ), keeping6= fixed. concavity dual L (), find maximumdirection setting corresponding derivatives zero. yieldsfiL () fifi= qnew (x ) qnew (x ) = 0 x ;(x ) fi=newfiXL () fifi=1qnew (x ) = 0 ,(B-5)fi=newxq new refers solution (B-2) (B-3) (x ) replaced new(x )new.Since(B-2)newqnew (x )e (x )= (x ) q (x ) ,esolution new(x ) must obeynewnew(x ) = log q (x ) + (x ) + log q (x ) ,still solve qnew (x ). Summing expression , substituting (B-3), solving qnew (x ) getlog qnew (x ) =X11[log q (x ) (x )] +(new c ) .n + cn + cNow, obtain exactly updates Algorithm 1 define(x ) = e (x ) (x ) = q (x )e (x ) ,184fiEfficient minimization Kikuchi free energyproperly normalize q (x ), line 7. normalization q (x ) line 10fact unnecessary, since construction updates ensure q (x ) = q (x )Z = 1.bottom line particular ordering Algorithm 1 joint updatemessages particular subset interpreted coordinate-wise gradientascent dual L (), updating Lagrange multipliers (x ) particulartime. Therefore Algorithm 1 guaranteed convergeunique maximum case positive overcounting numbers c .Convex Set ConstraintsNext, let us consider general case (some of) overcounting numbersnegative, Kikuchi free energy still convex set constraints.consider case inner region overcounting numbers negative5 .show that, sufficient damping updates, Algorithm 1 still guaranteedconverge unique minimum Kikuchi free energy set constraints.Note direct application argumentation fails, solution (B-3)q (x ) negative c corresponds maximum rather minimum. Consequently, dual L () (B-4) need concave. updates Algorithm 1follow setting derivatives zero interpreted fixed-point iterations, coordinate ascent L (). Still, practice seem work fine indeed withoutalways increasing L (). following explain why: argue updates Algorithm 1 correspond coordinate ascent, rather something likecoordinate descent-ascent convex-concave saddle function. sufficient damping,algorithm converge unique saddle point, correspondsminimum Kikuchi free energy set constraints.Convexity setP according Theorem 3.1, existsP constraints implies,matrix = |c | 1. Using q (x ) = q (x ), replaceLagrangian (B-1)XXXXXq (x )L(q, ) =q (x ) logq (x ) log q (x )(x )xxXXXXXXX1+(x )q (x )q (x ) +1q (x ) . (B-6)nxxx\since, Lemma A.2 Appendix A,XXq (x ) log q (x )q (x ) log q (x )xxconvex {q (x ), q (x )}, Lagrangian (B-6) indeed convex q independentconstraints. Thus could apply argumentation above: find minimum5. argumentation hold negative inner region entropy contributionscompensated positive inner region subset entropy contributions prove convexity Kikuchi freeenergy. case, might need slightly different algorithm guarantee convergence.185fiHeskesconvex Lagrangian respect q, substitute corresponding solution q () backLagrangian obtain concave dual L (), maximize dual respect. problem closed-form expression optimal q ()thus also closed-form expression dual L (), makes procedurerather awkward.Instead, distinguish outer region marginals, collected qO ,inner region marginals, collected qI . rewritten consistency constraint termsouter region marginals alone, replace constrained minimization respectqO unconstrained maximization respect corresponding Lagrange multipliers, leaving minimization respect qI normalization constraintis. gives us saddle-point problem type minqI maxO . Even without explicitlywriting equations, tell maximization respect particularcorresponds findingqnew (x ) = qnew(x ), .Then, minimization respect q given fixed qnew (x ) immediately yieldsXqnew (x ) ,qnew (x )properly normalized sum 1. exactly updates particular innerregion Algorithm 1 amount to: yield unique maximum respectminimum respect q , keeping q 6= fixed.coordinate descent-ascent procedure works fine saddle function convex minimizing parameter concave maximizing parameter (e.g., Seung,Richardson, Lagarias, & Hopfield, 1998). concavity immediate, convexityqI follows convexity Lagrangian (B-6) q = (qO , qI ): minimizingoverall convex function parameters, qO , yields convex functionremaining parameters, qI . Technically, convergence unique solutionsaddle-point problem proven construction Lyapunov functiondecreases infinitesimal updates parameters descent ascent directionzero unique saddle point (Seung et al., 1998). Convergence guaranteedsufficiently damped updates, full ones Algorithm 1. Empirically full updates, correspond full maximization minimization one inner regionmoving next one, work fine cases, occasionally indeed require littledamping. Wainwright et al. (2003) successfully apply damping similar algorithmattempt minimize convexified Bethe free energy.Appendix C: Constructing Tight Convex Boundappendix, describe procedure constructing tight convex bound FconvexKikuchi free energy FKikuchi . combines ideas Section 4.3 4.4. is,first convexify Kikuchi free energy, bounding little concave contributionsnegative inner regions possible. Next, terms bound anyways,try incorporate many convex contributions can. leads followingprocedure.186fiEfficient minimization Kikuchi free energyConsider minus entropy=X+Xc +Xc,I+choose c c first termXXXXcc +(c c )S ,=+I+(just) convex.corresponding allocation matrix Theorem 3.1, define used resourcesX|c | c ,crewrite=X+c +XXXcI+(c c )S +XI+construction, first term still convex.(c c )S.guarantee convexity, bound entropy contributions secondterm . make bound tighter, include many convexcontributions can, still satisfying conditions Theorem 4.2. Callcorresponding overcounting numbers c c c c put remainingc c back first term:XXXc=c ++I+XX(c c )S .(c c )S +I+Choose Fconvex first term plus linear bound second term.find c first step similarly c third, use linear programsimilar one described Section 3.2 checking conditions Theorem 3.1.introduce slack variables replace condition (7d)X= (variable compensation) ,187fiHeskessimilar spirit (8). Furthermore, add inequality constraints |cP|(no need compensate |c |) search maximum(compensate much possible). terms corresponding solution , set c =c .ReferencesAji, S., & McEliece, R. (2001). generalized distributive law free energy minimization. Proceedings Allerton Conference Communication, Control,Computing.Besag, J. (1974). Spatial interaction statistical analysis lattice systems. JournalRoyal Statistical Society Series B, 36, 192236.Chiang, M., & Forney, G. (2001). Statistical physics, convex optimization sumproduct algorithm. Tech. rep., Stanford University.Darroch, J., & Ratcliff, D. (1972). Generalized iterative scaling. Annals MathematicalStatistics, 43, 14701480.Dechter, R., Kask, K., & Mateescu, R. (2002). Iterative join-graph propagation. Darwiche, A., & Friedman, N. (Eds.), Proceedings UAI-2002, pp. 128136.Dempster, A., Laird, N., & Rubin, D. (1977). Maximum likelihood incomplete datavia EM algorithm. Journal Royal Statistical Society B, 39, 138.Hall, P. (1935). representatives subsets. Journal London Mathematical Society,10, 2630.Heckerman, D. (1989). tractable inference algorithm diagnosing multiple diseases.Kanal, L., Henrion, M., Shachter, R., & Lemmer, J. (Eds.), Proceedings FifthWorkshop Uncertainty Artificial Intelligence, pp. 163171, Amsterdam. Elsevier.Heskes, T. (2003). Stable fixed points loopy belief propagation minima Bethefree energy. Becker, S., Thrun, S., & Obermayer, K. (Eds.), Advances NeuralInformation Processing Systems 15, pp. 359366, Cambridge. MIT Press.Heskes, T. (2004). uniqueness loopy belief propagation fixed points. NeuralComputation, 16, 23792413.Heskes, T., Albers, K., & Kappen, B. (2003). Approximate inference constrained optimization. Uncertainty Artificial Intelligence: Proceedings NineteenthConference (UAI-2003), pp. 313320, San Francisco, CA. Morgan Kaufmann Publishers.Heskes, T., Zoeter, O., & Wiegerinck, W. (2004). Approximate Expectation Maximization. Thrun, S., Saul, L., & Scholkopf, B. (Eds.), Advances Neural InformationProcessing Systems 16, pp. 353360, Cambridge. MIT Press.Ihler, A., Fisher, J., & Willsky, A. (2005). Loopy belief propagation: Convergenceeffects message errors. Journal Machine Learning Research, 6, 905936.Jaakkola, T., & Jordan, M. (1999). Variational probabilistic inference QMR-DTnetwork. Journal Artificial Intelligence Research, 10, 291299.188fiEfficient minimization Kikuchi free energyJirousek, R., & Preucil, S. (1995). effective implementation iterative proportional fitting procedure. Computational Statistics Data Analysis, 19, 177189.Jordan, M., Ghahramani, Z., Jaakkola, T., & Saul, L. (1998). introduction variationalmethods graphical models. Jordan, M. (Ed.), Learning Graphical Models,pp. 183233. Kluwer Academic Publishers, Dordrecht.Kikuchi, R. (1951). theory cooperative phenomena. Physical Review, 81, 9881003.Kschischang, F., Frey, B., & Loeliger, H. (2001). Factor graphs sum-product algorithm. IEEE Transactions Information Theory, 47 (2), 498519.Lauritzen, S. (1996). Graphical models. Oxford University Press, Oxford.Luenberger, D. (1984). Linear Nonlinear Programming. Addison-Wesley, Reading,Massachusetts.McEliece, R., MacKay, D., & Cheng, J. (1998). Turbo decoding instance Pearlsbelief propagation algorithm. IEEE Journal Selected Areas Communication,16 (2), 140152.McEliece, R., & Yildirim, M. (2003). Belief propagation partially ordered sets.Gilliam, D., & Rosenthal, J. (Eds.), Mathematical Systems Theory Biology, Communications, Computation, Finance, pp. 275300. Springer, New York.Murphy, K. (2001). Bayes Net toolbox Matlab. Computing Science Statistics,33, 331350.Murphy, K., Weiss, Y., & Jordan, M. (1999). Loopy belief propagation approximateinference: empirical study. Laskey, K., & Prade, H. (Eds.), ProceedingsFifteenth Conference Uncertainty Articial Intelligence, pp. 467475, SanFrancisco, CA. Morgan Kaufmann Publishers.Neal, R., & Hinton, G. (1998). view EM algorithm justifies incremental,sparse, variants. Jordan, M. (Ed.), Learning Graphical Models, pp.355368. Kluwer Academic Publishers, Dordrecht.Pakzad, P., & Anantharam, V. (2002). Belief propagation statistical physics. 2002Conference Information Sciences Systems, Princeton University.Pakzad, P., & Anantharam, V. (2005). Estimation marginalization using Kikuchi approximation methods. Neural Computation, 17, 18361873.Pearl, J. (1988). Probabilistic Reasoning Intelligent systems: Networks Plausible Inference. Morgan Kaufmann, San Francisco, CA.Pelizzola, A. (2005). Cluster variation method statistical physics graphical models.Journal Physics A, 38, R309R339.Seung, S., Richardson, T., Lagarias, J., & Hopfield, J. (1998). Minimax Hamiltoniandynamics excitatory-inhibitory networks. Jordan, M., Kearns, M., & Solla, S.(Eds.), Advances Neural Information Processing Systems 10, pp. 329335. MITPress.Tatikonda, S., & Jordan, M. (2002). Loopy belief propagation Gibbs measures. Darwiche, A., & Friedman, N. (Eds.), Uncertainty Artificial Intelligence: Proceedings189fiHeskesEighteenth Conference (UAI-2002), pp. 493500, San Francisco, CA. MorganKaufmann Publishers.Teh, Y., & Welling, M. (2002). unified propagation scaling algorithm. Dietterich,T., Becker, S., & Ghahramani, Z. (Eds.), Advances Neural Information ProcessingSystems 14, pp. 953960, Cambridge. MIT Press.Wainwright, M., Jaakkola, T., & Willsky, A. (2002a). new class upper bounds logpartition function. Darwiche, A., & Friedman, N. (Eds.), Uncertainty ArtificialIntelligence: Proceedings Eighteenth Conference (UAI-2002), pp. 536543, SanFrancisco, CA. Morgan Kaufmann Publishers.Wainwright, M., Jaakkola, T., & Willsky, A. (2002b). Tree-based reparameterizationapproximate estimation loopy graphs. Dietterich, T., Becker, S., & Ghahramani,Z. (Eds.), Advances Neural Information Processing Systems 14, pp. 10011008,Cambridge. MIT Press.Wainwright, M., Jaakkola, T., & Willsky, A. (2003). Tree-reweighted belief propagationalgorithms approximate ML estimation via pseudo-moment matching. Bishop,C., & Frey, B. (Eds.), Proceedings Ninth International Workshop ArtificialIntelligence Statistics. Society Artificial Intelligence Statistics.Yedidia, J., Freeman, W., & Weiss, Y. (2001). Generalized belief propagation. Leen,T., Dietterich, T., & Tresp, V. (Eds.), Advances Neural Information ProcessingSystems 13, pp. 689695, Cambridge. MIT Press.Yedidia, J., Freeman, W., & Weiss, Y. (2005). Constructing free energy approximationsgeneralized belief propagation algorithms. IEEE Transactions InformationTheory, 51, 22822312.Yuille, A. (2002). CCCP algorithms minimize Bethe Kikuchi free energies:Convergent alternatives belief propagation. Neural Computation, 14, 16911722.190fiJournal Artificial Intelligence Research 26 (2006) 417-451Submitted 11/05; published 08/06Multiple-Goal Heuristic SearchDmitry DavidovShaul Markovitchdmitry@cs.technion.ac.ilshaulm@cs.technion.ac.ilComputer Science DepartmentTechnion, Haifa 32000, IsraelAbstractpaper presents new framework anytime heuristic search taskachieve many goals possible within allocated resources. show inadequacytraditional distance-estimation heuristics tasks type present alternativeheuristics appropriate multiple-goal search. particular, introducemarginal-utility heuristic, estimates cost benet exploring subtreesearch node. developed two methods online learning marginal-utilityheuristic. One based local similarity partial marginal utility sibling nodes,generalizes marginal-utility state feature space. apply adaptivenon-adaptive multiple-goal search algorithms several problems, including focusedcrawling, show superiority existing methods.1. IntroductionInternet search engines build indices using brute-force crawlers attempt scanlarge portions Web. Due size Web, crawlers require several weekscomplete one scan, even using high computational power bandwidth (Brin& Page, 1998; Douglis, Feldmann, Krishnamurthy, & Mogul, 1997), still leavelarge part Web uncovered (Lawrence & Giles, 1998; Najork & Wiener, 1998). Manytimes, however, necessary retrieve small portion Web pages dealingspecic topic satisfying various user criteria. Using brute-force crawlers taskwould require enormous resources, would wasted irrelevant pages.possible design focused crawler would scan relevant parts Webretrieve desired pages using far fewer resources exhaustive crawlers?Since Web viewed large graph (Cooper & Frieze, 2002; Kumar, Raghavan,Rajagopalan, Sivakumar, Tomkins, & Upfal, 2000; Pandurangan, Raghavan, & Upfal, 2002),pages nodes links arcs, may look solution problemeld heuristic graph-search algorithms. quick analysis, however, revealsproblem denition assumed designers heuristic search algorithms inappropriatefocused crawling, uses entirely dierent setup. crucial dierenceheuristic search focused crawling success criterion. setups setgoal states. heuristic search setup, however, search completed soonsingle goal state found, focused crawling setup, search continues reachmany goal states possible within given resources.Changing success criterion existing search algorithms enough. informed search algorithms based heuristic function estimates distancenode nearest goal node. heuristics usually appropriate multiplec2006AI Access Foundation. rights reserved.fiDavidov & Markovitchgoal search. Consider search graph described Figure 1. grey area expandedStartBFigure 1: Using distance-estimation heuristic multiple-goal search problemgraph. Assume evaluate nodes B using distance-based heuristic. Nodebetter heuristic value therefore selected. indeed right decisiontraditional search task nd one goal. multiple-goal search, however,B looks like much promising direction since leads area high densitygoal nodes.many problems wide variety domains interested ndingset goals rather single goal. genetic engineering, example, wantnd multiple possible alignments several DNA sequences (Yoshizumi, Miura, & Ishida,2000; Korf & Zhang, 2000). chemistry may want nd multiple substructurescomplex molecule. robotics, may want plan paths multiple robots accessmultiple objects. cases, possible solution would invoke single-goalsearch multiple times. approach, however, likely wasteful, resourcebounded computation, may wish exploit multiple-goal1 nature problemmake search ecient.specic multiple-goal task focused crawling received much attention (Chakrabarti,van den Berg, & Dom, 1999; Cho & Garcia-Molina, 2000; Cho, Garca-Molina, & Page, 1998;Diligenti, Coetzee, Lawrence, Giles, & Gori, 2000; Rennie & McCallum, 1999)popularity Web domain. works, however, focused Web-specictechniques tailored particular problem crawling.goal research described paper establish new domain-independentframework multiple-goal search problems develop anytime heuristic algorithmssolving eciently. framework focuses mainly problem domainslooking goal states paths goals either irrelevant, costconcern (except eect search cost).1. Note term multiple-goal also used planning domain. There, however, tasksatisfy set dependent goals possible order constraints.418fiMultiple-Goal Heuristic Searchstart formal denition multiple-goal search problem. describeversions existing heuristic search algorithms, modied multiple-goal framework.main dierences single-goal multiple-goal search heuristic functionsused. describe new set heuristics better suited multiple-goal search.particular, introduce marginal-utility heuristic, considers expected costswell expected benets associated search direction. dicultspecify heuristics explicitly. therefore present adaptive methods allow onlinelearning them. Finally describe extensive empirical study algorithmsvarious domains, including focused crawling problem.contributions paper fourfold:1. identify dene framework multiple-goal heuristic search. novelframework heuristic search.2. dene set search algorithms heuristics appropriate multiplegoal search problems.3. dene utility-based heuristic present method automatic acquisitionvia online learning.4. provide extensive empirical study presented methods various domainsshow superiority existing general algorithms.2. Multiple-Goal Search ProblemLet S, E potentially innite state graph nite degree, set statesE set edges. single-goal search problem dened follows:1. Input:set initial states Sisuccessor function Succ : 2S Succ(s) = {s | s, E}.(Sometimes Succ given implicitly nite set operators O.)goal predicate G : {0, 1}. denote Sg = {s | G(s)}. Sometimes Sggiven explicitly.2. Search objective: Find goal state g Sg directed path S, Estate Si g. Sometimes also interested path itself.3. Performance evaluation: Although performance evaluation criterion commonly considered integral part problem denition, considersuch. determines class algorithms considered.common criteria evaluating search solution quality, usuallymeasured solution path cost, search eciency, mostly evaluatedresources consumed search.Although multiple-goal search problem bears similarity single-goal searchproblem, diers several ways:419fiDavidov & Markovitch1. Input: Includes additional resource limit R. simplicity presentationassume R given us number generated nodes2 . Later discussassumption.2. Search objective: Find set goal states SgR Sg satises:SgR , directed path S, E si Si s.search resources consumed exceed R.Sometimes also interested set corresponding paths.3. Performance evaluation: SgR . Obviously, higher values considered better.looks mix reasoning meta reasoning inserting resourcelimit part problem input, many problems much naturally denedresource limitation. Consider, example, minimax algorithm, maximaldepth search (which determines resources consumed) given part algorithmsinput. formulation, resource limit given input, fallsscope problems solved anytime algorithms (Boddy & Dean, 1994; Hovitz, 1990;Zilberstein, 1996) specically contract algorithms (Russell & Zilberstein, 1991;Zilberstein, Charpillet, & Chassaing, 1999).Sometimes resource limit known advance. setup, searchalgorithm interrupted time required return current set collected goals. type problem solved interruptible anytime algorithms (Hansen& Zilberstein, 1996; Russell & Zilberstein, 1991). alternative formalization requirealgorithm nd specied number goals evaluate performanceresources consumed search.3. Multiple-Goal Heuristic Search AlgorithmsAssume hmg : heuristic function estimates merit statesrespect objective performance evaluation criterion multiple-goal searchproblem dened previous section.objective develop search algorithms exploit heuristics similarmanner heuristic search algorithms developed single-goal search problems.start describing multiple-goal version greedy best-rst search3 .two main dierences existing single-goal best-rst search algorithm newly dened multiple-goal version:1. single-goal best-rst search stops soon encounters goal state,multi-goal version collects goal continues allocated resourcesexhausted.2. framework therefore applicable resource proportional number generatednodes. CPU time, internet bandwidth, energy consumption robots, etc. best-firstsearch algorithms, number also corresponds memory consumption.3. follow Russell Norvig (2003, page 95) use term describe search algorithmalways expands node estimated closest goal.420fiMultiple-Goal Heuristic Search2. single-goal best-rst search typically uses heuristic function triesapproximate distance nearest goal, multiple-goal version usedierent type heuristic appropriate multiple-goal task.heuristic search algorithms also modied nd multiple goals.algorithm Pearl Kim (1982) converted handle multiple goal searchcollecting goal nds focal list. goals optimal paths.multiple-goal heuristic used select node focal list expansion.algorithm stops, before, allocated resources exhausted. addition,stop algorithm nodes focal satisfy f (n) > (1 + )gmin , gminminimal g value among collected goals.Multiple-goal hill-climbing uses multiple-goal heuristic choose best direction.modify algorithm allow search continue goal found. One possiblemethod continuing search perform random walk found goal.Multiple-goal backtracking works similarly single goal version. However,goal encountered, algorithm simulates failure therefore continues. multiple goalheuristic used ordering operators node. constraint satisfaction,means ordering values associated variable.4. Heuristics Multiple-Goal Problemsintroduction, illustrated problem using traditional distance-estimationheuristic multiple-goal search. One main problems using distanceestimation heuristic take account goal density distancenearest goal. lead search relatively futile branch,left branch Figure 1, rather much fruitful right branch. sectionconsider several alternative heuristic functions appropriate multiple-goalsearch.4.1 Perfect Heuristicdescribe analyze heuristic functions multiple-goal search, would likeconsider function trying approximate. Assume, example,perform multiple-goal greedy best-rst search perfect knowledgesearch graph. node would like heuristic select next? Assume givenresource limit allows us expand additional nodes, look search forests4size rooted current list open nodes. perfect heuristic select nodebelonging forest largest number goals.Definition 1 Let Sopen set currently open states. Let R resource limitRc resources consumed far. Let Sgf Sg set goals found far. LetF set possible forests size R Rc starting roots Sopen . forest f Foptimalf F, (Sg \ Sgf ) f |(Sg \ Sgf ) f | .4. search space graph, search algorithm expands forest currently open nodes.421fiDavidov & Markovitchstate Sopen optimal, denoted OP (s), exists optimalforest f f .Definition 2 heuristic function h perfect respect multiple-goal search problemevery possible search stage dened Sopen ,s1 , s2 Sopen [OPT(s1 ) OPT(s2 ) = h(s1 ) < h(s2 )] .Thus perfect heuristic never selects expansion state optimalforest. Using heuristic multiple-goal best-rst search make search optimal.Note optimality respect search resources respectcost paths leading goal states.Obviously, denition lead practical multiple-goal heuristic. Evensimple problems, even perfect knowledge graph, calculatingheuristic hard. number possible forests exponentialresource limit number nodes open list.4.2 Sum HeuristicsMany search algorithms look single goal state use heuristic function estimates cost cheapest path goal state. Optimizing algorithmsrequire admissible heuristics (that underestimate real distance goal) satiscing search algorithms, greedy best-rst, use non-admissible heuristics well.Distance-estimation heuristics therefore developed many domains. addition,several researchers developed automatic methods inferring admissible heuristicsrelaxation (Held & Karp, 1970; Mostow & Prieditis, 1989; Prieditis, 1993) patterndatabases (Culberson & Schaeer, 1998; Gasser, 1995; Korf & Felner, 2002).Figure 1 illustrates straightforward use distance heuristics multiple-goalsearch appropriate. dene method utilizing (admissible nonadmissible) distance heuristics multiple-goal search. Assume set goal states,Sg , given explicitly, given common distance-estimation heuristichdist (s1 , s2 ) estimates graph distance two given states5 . sum-ofdistances heuristic, denoted hsum , estimates sum distances goal set:hsum (s) =hdist (s, g).(1)gSgMinimizing heuristic bias search towards larger groups goals, thus selectingnode B example Figure 1. indeed better decision, provided enoughresources left reaching goals subgraph B.4.3 Progress HeuristicsOne problem sum heuristic tendency try progress towardsgoals simultaneously. Hence, groups goals scattered around search front,states around front similar heuristic values. step reduces5. Assume Euclidean distance figure reflects heuristic distance.422fiMultiple-Goal Heuristic Searchdistances increases others, leading more-or-less constant sum. constantsum regions, algorithm relies sum heuristic informationnode choose expansion. Even distinct groupsFigure 2: behavior sum heuristic vs. progress heuristic. solidline ellipse indicates area covered search using sum heuristic.dotted-line ellipse marks area searched using progress heuristic.goals dierent directions, sum heuristic may lead simultaneous progress towardsgroups. two groups goal states shown Figure 2 illustrate problem.sum heuristic strategy work enough enough resources reachgroups. If, however, resources sucient, sum heuristic may wasteavailable resources trying progress towards groups, reaching none.avoid problems, dene progress heuristic, takes accountnumber goals towards progress made average distance them. Thus,instead trying pursue multiple groups goals, heuristic pursue one grouptime, preferring less distant groups.Let Sopen set currently opened states. before, assumeexplicit goal list, Sg = g1 , . . . , gk , distance-estimation heuristic, hdist . Letmi = minsSopen hdist (s, gi ) minimal estimated distance search frontiergoal gi . Sopen dene Gp (s) = {gi Sg | hdist (s, gi ) = mi } setgoals estimated closest among states thesearch frontier.gG (s)hdist (s,g)paverage estimated distance states Gp Dp (s) =|Gp (s)|average distance s. interested states many membersGp small average distance. Hence, dene progress heuristichprogress (s) =Dp (s).|Gp (s)|(2)Minimizing heuristic direct search larger closer groups goals towardsprogress made. example, simple space shown Figure 2, progressheuristic advance correctly, towards group left side indicated dashedellipse. Note although right group larger, progress heuristic nonethelessprefers left one smaller distance. Since progress heuristic considersgoal exactly once, misled multiple paths goal.423fiDavidov & Markovitch4.4 Marginal-Utility Heuristicscomparing two search directions, far considered concentrationgoal states, preferring directions lead larger groups goals. Thus, formerheuristics would considered node node B Figure 3 equivalent.reasoning, however, take account resources invested reach setgoals. example Figure 3, clear visiting set goals node Brequires less search resources node A. Therefore node B preferable.StartBFigure 3: Searching node result number goals searchingnode B. consume, however, far greater resources.account cases suggesting another approach multiple-goal heuristics,one considers expected benet search also expected cost.Obviously, prefer subgraphs cost low benet high. words,would like high return resource investment. call heuristic triesestimate return marginal-utility heuristic.Assume (for now) search space S, E tree. Let (s) set statesreachable s. Let Tg (s) = (s) Sg set goal states reachables. Let Sv set states visited completed search process. denemarginal utility state Sv respect SvMU(s) =|Tg (s) Sv |.|T (s) Sv |(3)Thus, MU measures density goal states subtree expanded search process.One possible good search strategy select states eventually yield highvalues U respect Sv . search process consumed Rc R resources424fiMultiple-Goal Heuristic Searchfar, largest tree visited size r = R Rc . Section 4.1 deneperfect heuristic considering possible ways distributing r among opennodes. approach obviously impractical, take greedy approachinstead. look heuristic function hM U (s, r) tries estimate best marginalutility s, assuming remaining resources, r, consumed exploring(s). Let (s, r) set trees size r root s. hM U (s, r) tries estimateresource-bounded marginal utility, dened|Tg (s) |.r(s,r)MU(s, r) = max(4)MU(s, r) measures best ratio number goals achieved searchresources used it. Naturally, dicult build heuristics estimate marginalutility accurately. following sections show heuristics learned.4.5 Additional ConsiderationsOne possible side-eect stopping discovering goals continuous inuencealready discovered goals search process. found goals continue attractsearch front, would preferable search progress towardsundiscovered goals. explicit set goal states given - sum progressheuristics - disable inuence visited goals simply removingset. set features states given instead, reduce eect visited goalspreferring nodes farther feature space. Specically, let d(n)minimal distance n members set visited goals, let h(n)multiple-goal heuristic value n. modied heuristic h (n) = h(n)(1+c1 ec2 d(n) )c1 c2 parameters determine magnitude eect d(n).second term penalty add heuristic value. penalty decays exponentiallydistance visited goals.Note tension tendency search dense groups goalstendency push search away visited goals. groups goalsdense, method detrimental nding goalsgroup reduces tendency pursue goals group. eectcontrolled ci parameters. domains high goal density, ci setlower values. Hence, values set dynamically search, accordingmeasurements goal density explored graph.One problem using marginal utility heuristic non-tree graphs possibleoverlap marginal utility. means search algorithm might pursue setgoals dierent directions. One way overcome problem try diversifysearch progress measuring feature-based average distance bestnodes set recently expanded nodes, prefer maximal diversitynodes explored. gives maximal diversity exploration directionsminimize expected overlap visited subtrees.425fiDavidov & Markovitch5. Learning Marginal Utility Heuristicsquite possible marginal-utility heuristics supplied user,many domains heuristics dicult design. use learning approachacquire marginal-utility heuristics online search. present twoalternative methods inferring marginal utility. One approach estimates marginalutility node based partial marginal utility siblings. approachpredicts marginal utility using feature-based induction.5.1 Inferring Marginal Utility Based Marginal Utility Siblingsrst approach predicting marginal utility based assumption siblingnodes similar marginal utility. dene partial marginal utility statestep executing multiple-goal search algorithm number goals found farsubtree divided number states visited far subtree. Thus,Sv (t) set states visited step t, partial marginal utility denedU (s, t) =|Tg (s) Sv (t)|.|T (s) Sv (t)|(5)method estimates marginal utility siblings based partial marginalutility, marginal utility node based average estimated marginal utilitysiblings.discussed previous subsection, expected marginal utility node stronglydepends resources invested exploring it. Thus, learn heuristic hM U (s, r),one would need estimate partial marginal utility values dierent values r. Oneway reducing complexity two-dimensional estimation divide twostages: estimating depth tree searchable within r resources,U depth (s, d)estimating marginal utility predicted depth. Thus,estimated marginal utility searching node depth d, compute estimatedr)),U resources(s, r) =U depth (s, d(s,marginal utility node using r resourcesd(s, r) estimated depth searching node using r resources.following subsections show values estimated.5.1.1 Updating Partial Marginal-Utility Valuesmaintain node two vectors counters, parameter limitsmaximal lookahead partial marginal utility. One vector, N (n), stores currentnumber visited nodes node n depth 1, . . . , D, Ni (n) contains currentnumber visited nodes n depth less equal i. vector, G(n)holds similarly number visited goals.Whenever new node n generated, appropriate entries ancestors N vectorsincremented. n goal, G vectors updated well. p ancestorn connected path length l D, Nl (p), . . . ND (p) incrementedone. one path exists n p, consider shortest one.memory requirements procedure linear number stored nodes.),number operations required one marginal-utility update bounded O(BdegreeBdegree upper bound maximum indegree graph. Therefore,426fiMultiple-Goal Heuristic Searchbackward degree bounded, number calculations per node growsearch progresses. depth limit determines complexity update givensearch graph; hence, desirable reduce value. value low, however,make possible infer local marginal-utility values.5.1.2 Inferring Marginal Utilityinference algorithm estimates marginal utility node basis averagepartial marginal utility siblings. nodes sucient statistics partialmarginal utility used predict marginal utility new nodes. call nodessupported nodes. node supported siblings, base estimate averageestimated marginal utility parents (computed recursively using procedure).Figure 4 illustrates method. left tree, marginal utility grey nodeMU=0.4MU=0.2MU=(0.4+0.2)/2=0.3MU=0.4MU=0.2MU=(0.4+0.2)/2=0.3??MU=0.3Figure 4: Inferring marginal utility partial marginal utility supported siblings (left)supported uncles (right). Nodes question mark unsupported.computed average marginal utility siblings. right tree, marginalutility grey node computed average marginal utility uncles. inputmarginal utility estimation heuristic remaining unconsumed resources, r.algorithm rst nds largest depth, d, number predicted nodes smallerr. prediction based supported siblings parent nodedescribed above.found depth, d, used determine counters used estimatemarginal utilities supported uncles. complete algorithm listed Figure5.5.1.3 Sibling Clusteringalgorithm described Figure 5, marginal utility node induced averagingpartial marginal utility siblings. dene meaningful similarity metricnodes, try making prediction less noisy using nodes similarsiblings. One way use similarity metric cluster set siblingsgenerate virtual node cluster. virtual node linked clustermembers parent parent original sibling set. requiredchange. existing algorithm described Figure 5 rest. predictingmarginal utility node, algorithm rst looks partial marginal utility427fiDavidov & Markovitchprocedure MU(s,d)Gd (s)Supported(s,d) return N(s)else P Parents(s)|P | = 0 return 0SupportedSiblings {c Children(p) | p P, Supported(c)}|SupportedSiblings| > 0Gd (c)cSupportedSiblingsreturn AvgNd (c)else return Avg({M U (p, in(d + 1, D)) | p P })procedure TreeSize(s,d)P Parents(s)Supported(s,d) |P | = 0 return Nd (s)elseSupportedSiblings {c Children(p) | p P, Supported(c)}|SupportedSiblings | > 0return Avg({{N}) fiff (c) | c SupportedSiblingsTreeSize(p,Min(d+1,D)) p Pelse return Avg|Children(p)|procedure Get-marginal-utility(s,ResourceLimit )Depth = max(d D|TreeSize(s, d) < ResourceLimit )return MU(s, Depth)Figure 5: algorithm marginal-utility estimationsiblings. case members cluster. siblingsunsupported algorithm use information clusters propagatedcommon parent.mechanism illustrated Figure 6. Without clustering, predicted marginalutility unsupported nodes would average three supported siblings,0.5. Note average large variance associated it. Clustering nodesB one virtual node, C, D, E another, yields (we hope)accurate prediction, since based uniform sets. similarity metricusually Euclidean distance vectors features states correspondingsibling nodes. domains, try reduce number generated nodesdeciding step node-operator pair proceed with. cases needsimilarity measurement operators implement approach.5.2 Feature-Based Induction Marginal UtilityUnfortunately, partial marginal-utility information allows us predict marginal utilitynodes proximity one another graph structure. addition,428fiMultiple-Goal Heuristic SearchSibling clustering0.5B? 0.1C0.70.70.5E?0.1?B0.1C0.70.7E0.7?Figure 6: eect sibling clustering marginal utility estimationdomains local uniformity sibling nodes respect marginal utilitycannot assumed. overcome problems view marginal-utility inferenceproblem function learning use common induction algorithms. depthd, induce marginal utility function using set supported nodes (with respectd) examples. state features domain independent (such in-degreeout-degree node) domain specic, supplied user. inductionproblem, quality induced function highly dependent qualitysupplied features.Since learning scheme performed on-line, high cost learning,using classier directly, reduce utility learning process. One way essentially eliminate learning costs use lazy learner, KNN (Cover & Hart,1967). approach also advantage incremental: new example contributes immediately learned model. problem approach highcosts associated using classier.alternative approach would learn ecient classier regressiontree (Breiman, Friedman, Olshen, & Stone, 1984). learned incrementally usingalgorithms ID5 (Utgo, 1988). Batch learning algorithms C4.5 usuallyyield better classiers incremental algorithms. However, due higher costapplying batch learning, one decide often call it. Applying nodegeneration would increase induction cost, yield better classiers earlier - mayimprove performance search process. Applying large intervals would reduceinduction costs lead poorer search performance.on-line learning process gives rise another problem: initial search periodyet sucient examples make learning helpful. One way reduceeect lack knowledge using classiers induced beforehand, on-lineo-line, goals similar goals current search.6. Empirical Evaluationtest eectiveness methods described previous sections showversatility, experimented intensively several domains. challenging domain,however, focused crawling apply algorithms task collecting targetweb pages sub-web millions pages. rst compare anytime behaviordistance-based methods uninformed search best rst search.429fiDavidov & Markovitchtest performance marginal utility methods. also test eectvarious suggested enhancements algorithms performance. also show realtimeperformance algorithm allowing search real web.6.1 Experimental Methodologycompare performance algorithms two competitors: breadth-rst searchbest-rst search using distance estimation heuristics. algorithms adoptedmultiple-goal framework allowing continue search nding rstgoal.basic experiment compares two multiple-goal search algorithms conductedfollowing way:1. set initial states goal predicate given.2. algorithms perform multiple-goal search.3. resources consumed number goals found executionmonitored.4. last two steps repeated several times accumulate sucient statistics (allalgorithms contain least one random component).5. performance two algorithms, measured number goals foundallocated resources, compared.problem estimating performance algorithm resource allocationgiven extensively discussed context anytime algorithms. Measuringperformance anytime algorithms problematic (Hansen & Zilberstein, 1996).probability distribution resource allocation given, computeexpected performance anytime algorithm basis performance prole.many cases, however, probability distribution available. therefore measureperformance tested algorithm means obtained quality dierentresource allocation values. multiple-goal search, quality measured numbergoals found allocated resources. know total number goals,report instead percentage goals found.Alternatively, anytime algorithms evaluated measuring amount resources required achieve given quality. multiple-goal search, obviousmeasurement time. Time, however, overly aected irrelevant factors hardware, software, programming quality. Moreover, Web domain, spentaccessing Web pages. long takes depends many factors, networkserver loads, irrelevant research topic.thus decided measure resource consumption number generated nodes.Nevertheless, cannot ignore time completely: must make sure overheadmethods described paper outweigh benets. therefore reporttime results experiment uses real Web.Many parameters aect performance algorithms described paper.Ideally, would like perform factorial analysis (Montgomery, 2001) combination values tested. experimentation, however, infeasible large430fiMultiple-Goal Heuristic Searchnumber variables involved. therefore take one-factor-at-a-time approach,use default value parameters except one tested. addition, whereverappropriate, perform several experiments testing two factors together.6.2 Tasks Domainsexperiments conducted context several Web domains. showgenerality approach, applied methods several additional domains, includingn-queens, open knight tours, multiple robot path planning multiple sequence alignment.algorithms applied following tasks:1. Focused crawling: One main motivations research problemfocused crawling Web (Chakrabarti et al., 1999; Cho & Garcia-Molina, 2000;Kleinberg, 1999; Menczer, Pant, Srinivasan, & Ruiz, 2001). task ndmany goal pages possible using limited resources, basic resource unitusually actual retrieval page link. looks taskretrieval information internet could achieved using general-purposesearch engines, several circumstances focused crawling still needed:(a) search criterion complicated expressible querylanguage search engines.(b) one needs updated set goals search engines updated everyweeks due huge space brute-force crawlers cover.(c) coverage general engines sucient.Previous work focused crawling concentrated Web-specic techniques directing search. experiments test whether generalization single-goalheuristic search multiple-goal search contribute task focused crawling.Performing rigorous empirical research Web problematic. First, Webdynamic therefore likely modied dierent runs algorithms (Douglis et al., 1997). Second, enormous time required crawlingWeb disallows parametric experimentation. solve problems downloaded signicant section Web local storage performed experimentsusing local copy (Cho et al., 1998; Hirai, Raghavan, Garcia-Molina, & Paepcke,2000). Specically, downloaded large part .edu domain, containing, cleanup, approximately 8,000,000 valid accessible HTML pages.resulting graph average branching factor 10.6 (hyperlinks).tested performance algorithms entire downloaded domain.parametric experiments, however, time-consuming even localcopy used. Therefore, used small sub-domains experiments. subdomain generated randomly selecting root page predesignated setroots extracting sub-graph size 35,000 it.ensure overhead algorithms signicantly aect performance, also conducted several online experiments real Web.use three types goal predicates:431fiDavidov & Markovitch(a) Predicates test pages specic topics: robotics, mathematics, football, food, sport. predicates automatically induced applyingdecision tree learning set manually supplied examples.(b) Predicates test certain types pages: pages containing publication lists,laboratory pages, student home pages, project pages news pages.predicates also learned examples.(c) Predicates test home pages people members speciclist. list people generated Web page listed namespeople together personal information (such name, aliation areainterest). predicates employ commonly used heuristics determiningwhether HTML document home page specic person. use threepredicates corresponding three dierent lists found Web.limit list contain 100 names.2. Finding paths multiple goals: Path-nding algorithms usually search singlepath one goals. applications, however, get set initial statesset goal states task nd set paths initial stategoal state. paths may used another algorithm evaluatesselects one execute according various criteria. simulated physical environment 500 500 grid random walls inserted obstacles (anaverage 210000 nodes average 3.9 branching factor). parameterscontrolling maximal length walls desired density grid. Wallsinserted randomly making certain resulting graph remains connected.set goal states randomly generated using one following two methods:(a) set states independently uniformly drawn set states.(b) 10% goal states generated above. randomly uniformlyselect K states used cluster centers. rest goal statesrandomly generated distances centers normallydistributed.Figure 7 shows example multiple path-nding problem solutionincludes 9 paths.3. Planning movement multiple robots: Assume given set N robotslocated various states, task collect set K > N objects scatteredaround. case need plan N paths pass many objectspossible. situation illustrated Figure 8. Although problem appearsresemble one Figure 7, solution two paths (while solutionprevious problem 9). many similar planning problems: example,planning product delivery several starting points multiple customersusing xed number delivery trucks. experiments usedtype grids previous problem. robots placed randomly.assumed collisions harmful.4. Open knight tours: famous problem task nd pathknight chess board squares visited none visited432fiMultiple-Goal Heuristic SearchGGGGGGGGGGGGGGGGGGFigure 7: Searching set paths multiple goals gridGGGGGGGGGGGGGGGGGR1GR2R1R2Figure 8: Multiple-robot path planning gridtwice. tried multiple-goal version task nd manypaths possible within allocated resources. experiments, used boards6 6, 7 7 8 8 squares.5. N-Queens: constraint satisfaction problem goal place N queenschessboard two queens row, column diagonal.multiple goal version problem, want nd many satisfying congurationspossible within allocated resources.6. Multiple sequence alignment: known bioinformatics problem goalalign several biological sequences optimally respect given cost function.multiple-goal version interested obtaining many almost optimalsolutions possible within allocated resources.6.3 Performance Distance-Based Heuristicsexperimented rst two multiple-goal heuristic functions based graphdistance estimation: sum heuristic progress heuristic. compare performance multiple-goal best-rst search uses heuristics with:433fiDavidov & MarkovitchDomainTaskBFSMultiple pathndingMultiple robotmovementscatteredclusteredscatteredclustered8.5(0.1)10.2(0.1)7.1(0.8)10.1(0.9)Focusedcrawling100-person searchGroup 1Group 2Group 30.1(0.0)3.2(1.4)0.3(0.1)Min. dist.SumWithoutWithoutdisab.disab.disabdisab% goals found 20% resources21.3(1.2) 29.0(0.5) 21.5(0.9) 28.9(0.3)34.0(0.5) 45.1(0.4) 32.6(1.1) 59.2(0.3)20.3(0.8) 26.5(0.4) 22.3(0.6) 25.8(0.2)31.2(1.4) 47.4(1.2) 42.0(0.6) 64.1(0.7)% goals found 2% resources13.5(0.5) 17.3(1.3) 24.1(0.7) 28.0(1.1)18.4(2.1) 26.2(1.9) 19.7(1.0) 23.5(1.1)5.8(0.9)10.7(1.4)6.4(0.9)11.7(0.9)Progress76.8(2.1)94(1.2)89.9(1.8)98.6(0.9)51.3(0.8)78.1(3.1)60.9(0.8)Table 1: performance multiple-goal search various heuristics. numbers parentheses standard deviations.1. Multiple-goal best-rst search uses distance estimation heuristic function.2. Breadth-rst search (BFS) (shown Najork & Wiener, 2001, perform wellWeb crawling).sum heuristic requires distance estimates individual goals. dene distances three domains. multiple path nding multiple robot planninguse Manhattan distance. focused crawling task experiment personalhome page domain. estimate distance given page home pagelist member computing cosine vector distance bag words givenpage bag words description text person.distance heuristic sum heuristic tested without disablinginuence visited goals. use disabling progress heuristic sincesubsumes behavior. two grid-based tasks, goals given, usedcomplete disabling removing visited goals goal list described Section4.5.personal home page search task, set goals explicitly given,used feature-based method described Section 4.5. features used determiningdistance candidate pages visited goals words highestTFIDF value (Joachims, 1997; Salton & Buckley, 1988).Table 1 summarizes results experiment. number represents average50 experiments. measure performance consuming 20% maximalamount resources, i.e., expanding 20% total number nodes search graph.100-person home page search proved relatively easy domain methodsable nd goals consuming little resources. Therefore,domain, measure performance 2% nodes. Figure 9 shows anytimebehavior various methods two domains. graphs domainsshow similar patterns.434fiMultiple-Goal Heuristic Search40100Breadth First SearchDistance-estimation heuristicsSum heuristic3580% Goals found% Goals found3025201560401020Breadth First SearchDistance-estimation heuristicsSum heuristicProgress heuristic50001234560% Nodes generated20406080100% Nodes generated(a)(b)Figure 9: Anytime performance various heuristics: (a)Focused crawling (b)Multiplepath ndingbasis table corresponding graphs, make following observations:1. progress heuristic superior methods tested far.advantageous case clustered goals comes surprise leadsone cluster pursued time. superior distance estimationcase scattered goals far less obvious: would expect heuristicspursue one goal another therefore yield similar results. weightedprogress heuristic, however, prefers pursuing goals closer goals, thusyielding better results.2. results clearly demonstrate goal inuence phenomenon indeed signicant, method ecient reducing eect.3. almost every case, heuristic methods signicantly better blind search.exception using sum heuristic without inuence disabling graphsscattered goals. cases, behavior marginally better blindsearch.6.4 Performance Marginal-Utility HeuristicsNone methods previous subsection take account expected searchresources involved pursuing alternative directions. addition, assumeeither knowledge specic set goals heuristic distances each.subsection test performance two methods based marginal utilitydescribed Section 5.1.2. experiments described subsection performedfocused crawling task 10 goal predicates described Section 6.2.topic-based goals, cannot compare performance marginal-utilityalgorithms sum heuristic access list goalslist heuristic values specic goals previously tested domains. Therefore435fiDavidov & Markovitchuse comparison blind BFS common best-rst search using distanceestimation heuristic. heuristic based list words selected inductionalgorithm generating goal predicates.6.4.1 Inferring Marginal Utility Partial Marginal Utilityimplemented tested marginal-utility estimation algorithm described Section5.1. Figure 10 shows performance proles tested algorithms robotics100100Distance-estimation heuristicsBreadth First SearchMarginal-utility inferenceDistance-estimation heuristicsBreadth First SearchMarginal-utility inference9080807070% Goals found% Goals found906050406050403030202010100005101520253035400% Nodes generated510152025303540% Nodes generated(a)(b)Figure 10: performance multiple-goal best-rst search using marginal-utilityinference method applied focused crawling (with D=4). results shown(a)Robotics pages (b) Mathematics pagesmathematics goal predicates. graph represents average 5 runs testedalgorithm using 5 starting pages randomly selected xed 200 root pages.cases see signicant advantage marginal-utility methodtwo methods. advantage becomes evident initial training period,sucient statistics accumulated. full data 10 domains resourceallocation 10% 20% available Appendix. Figure 11 shows averageimprovement factor (compared distance estimation) 10 domains functionsearch resources. graph shows nicely initial exploratory stagestatistics accumulated 8% resources consumed,improvement factor becomes larger 1. improvement factor reaches peak2.8 17%, starts decline towards value 1 100% search resources,algorithm necessarily nds goals.Performance exploratory stage improved combining two methods.tested hybrid method, uses linear combination marginal-utility prediction heuristic estimation. determine linear coecients part,conducted 100 experiments small Web subgraph (below 35,000 pages) using dierentgoal predicates (unrelated tested main experiments). Figure 12 showsresults obtained combined method compared individual methods.see indeed combined method better algorithms alone.436fiImprovement factor M.U. vs dist. estim. heuristicsMultiple-Goal Heuristic Search32.521.510.50010203040506070% Nodes generated8090100Figure 11: improvement factor best-rst using marginal-utility inference comparedbest-rst using distance-estimation heuristic100100Distance-estimation heuristicsMarginal-utility inferenceCombined approachDistance-estimation heuristicsMarginal-utility inferenceCombined approach9080807070% Goals found% Goals found906050406050403030202010100005101520253035400% Nodes generated510152025303540% Nodes generated(a)(b)Figure 12: Combining marginal-utility inference heuristic search (a)Robotics pages(b)Mathematics pages.interesting phenomenon that, points, result combined method bettermaximal results two. One possible explanation earlysearch process distance-estimation heuristic leads sucient number goalsjump-start learning process much earlier. results 10 domains availableAppendix.6.4.2 Learning Marginal-Utility FeaturesSection 5 describe method feature-based generalization visited search nodesorder induce marginal-utility values. conducted set experiments test437fiDavidov & Markovitcheciency learning mechanism problem focused crawling 10goal types previous experiments.crawling, accumulate supported visited pages tagmarginal utility measured time learning took place (see Section 5). converttagged pages feature vectors hand CART algorithm (Breiman et al.,1984) regression-tree induction6 . used induced tree estimate marginalutility newly generated nodes.features, use bag-of-words approach, dominant eld textcategorization classication. value word-feature appearance frequency.Words appearing HTML title tags given weight. apply feature selectionchoose words highest TFIDF (Joachims, 1997; Salton & Buckley, 1988).100100Feature-based marginal-utility inductionDistance-estimation heuristicsMarginal-utility inferenceFeature-based marginal-utility inductionDistance-estimation heuristicsMarginal-utility inference9080807070% Goals found% Goals found906050406050403030202010100005101520253035400% Nodes generated510152025303540% Nodes generated(a)(b)Figure 13: performance best-rst search marginal utility induced usingregression trees classier. experiments performed problemfocused crawling with: (a)Robotics pages (b) Mathematics pagesFigure 13 shows results obtained robotics mathematics goal predicates.full report 10 domains available Appendix. cases,initial period sibling-based inference method outperforms sophisticated feature-based induction. period, however, induction-based methodsignicantly outperforms sibling-based method. One possible reason initial inferior performance feature-based induction requires examples simplisticsibling-based method computes averages therefore needs fewer examples.inspected produced trees found reect reasonable concepts.several dozens nodes contain features related searchedgoal features correspond hubs (such repository collection).tested whether choice classier aects performance inductionbased method performing set experiments using KNN classier.results obtained essentially identical.6. induction algorithms, SVM Naive Bayes, could used well.438fiMultiple-Goal Heuristic Search6.5 Testing Full System Architecture100100909080807070% Goals found% Goals founddescribed various enhancements marginal-utility methods, including siblingclustering, overlap minimization, combining marginal-utility heuristic distanceestimation heuristic, disabling visited goals. Figure 14 shows performancetwo marginal-utility methods full set enhancements. full results 10domains available Appendix.6050403060504030Enhanced inductionInferenceInductionEnhanced inferenceDistance-estimation heuristics2010Enhanced inductionInferenceInductionEnhanced inferenceDistance-estimation heuristics20100005101520253035400% Nodes generated510152025303540% Nodes generated(a)(b)Figure 14: performance best-rst search (using dierent marginal-utility heuristics)enhancements enabled: (a)Robotics pages (b) Mathematics pages.gure contains 5 plots: one baseline performance (distance estimation), two unenhanced methods (inference induction) twoenhanced methods (enhanced inference enhanced induction).sibling- feature-based methods indeed improve enhancementsenabled. Furthermore, feature-based method maintains advantage, albeitslightly decreased magnitude. Although enabling enhancements improve systemperformance, recalled true enabling enhancement separately. question thus arises whether improvements least partially cumulative.words, would performance using enhancements better performanceusing enhancements separately? Figure 15 compares graphssibling-based method. see fully enhanced method indeed superiorrest.6.6 Realtime Performanceprevious experiments took number generated nodes basic resourceunit. must careful, however, since measurement take accountoverhead method. ensure overhead outweigh benets,conducted realtime evaluation system architecture performing focused crawlingonline Web measured resources consumed time elapsed.7 . Figures7. experiment performed using Pentium-4 2.53 GHz computer 1GB main memorycable modem.439fi100100909080807070% Goals found% Goals foundDavidov & Markovitch6050403060504030enchancementsCombined approachOverlap minimizationSiblings clusteringenchancements2010enchancementsCombined approachOverlap minimizationSiblings clusteringenchancements20100005101520253035400510% Nodes generated152025303540% Nodes generated(a)(b)100100909080807070% Goals found% Goals foundFigure 15: performance best-rst search (using marginal-utility inference)enhancements enabled compared performance algorithmsingle option enabled: (a)Robotics pages (b) Mathematics pages. gurecontains 5 plots: One marginal utility inference method enhancements, one method enhanced combined approach, oneenhancement overlap estimation, one enhancement sibling clusteringand, nally, one enhancements together.60504030605040302020Distance-estimation heuristicsMarginal-utility inferenceFeature-based marginal-utility induction10Distance-estimation heuristicsMarginal-utility inferenceFeature-based marginal-utility induction1000010203040506070809001020304050Time (hours)Time (hours)(a)(b)60708090Figure 16: performance marginal-utility based methods function realtime: (a)Robotics pages (b)Mathematics pages16(a),(b) show performance methods enhancements enabledfunction real time. comparison graphs graphs shown Figure 14reveals overhead methods noticeably aect performance.see overhead increases size visited graph, plotted Figure 17real time function number generated nodes. graphs showmajority time required focused crawling tasks indeed loading time itself,440fiMultiple-Goal Heuristic Search90Pure loading timeDistance-estimation heuristicsMarginal-utility inferenceFeature-based induction8070Time (hours)60504030201000510152025% Nodes generated303540Figure 17: average real time used marginal-utility methods functionnumber generated nodes focused crawling domaineven calculations related discussed options enabled. fact, usingdistance-estimation increases computation time factor 1.07; usingsimilarity-based inference marginal utility, 1.1; using feature-basedinduction, 1.17. Thus, improvement greater factors,algorithms benet focused crawling systems.6.7 Contract Algorithms Versus Interruptible Anytime Algorithmsexperiments described far test performance methods interruptibleanytime algorithms. point graph also considered test resultcontract algorithm using specic resource allocation. However, multiple-goalsearch algorithm called contract mode, utilize additional input (of resourceallocation) improve performance algorithm, described Section 5.1.2.test eect exploiting resource allocation, repeated experimentdescribed Section 6.4.1 using algorithm Section 5.1.2. foundusing algorithm contract mode, obtained average improvement factor 7.6(factor 2.8 dropping two extremes) 5% resource allocation 1.4 10% resourceallocation. full results available Appendix.algorithms also allow contract quality mode input requiredquality instead allocated resources. case quality specied percentage goals found. contract algorithm achieved average improvement factor1.9 5% goals 1.4 20% goals. full results availableAppendix.6.8 Performance Multiple-Goal Search Algorithmsprevious subsections test heuristic methods best-rst search. showgenerality approach, test whether marginal-utility heuristics used ecientlydynamic ordering variables backtracking, choosing node focal441fiDavidov & Markovitchgroup multiple-goal algorithm. cases used sibling clusteringmethod.backtracking used multiple-goal version two known problems: open knighttour n-queens. applied marginal-utility inference algorithm basedsimilarity siblings sort variable values multiple-goal version backtrackingsearch. open knight tour problem used 6 6 board (this board contains524, 486 goal congurations, maximal size collect goalseciently). n-queens problem used 16 16 board containing 14, 772, 512 goals.neither case use domain-specic technique increase search eciency. Figure100100908080% Goals found% Goals found70604060504030202010Simple backtrackingBacktracking marginal utility0Simple backtrackingBacktracking marginal utility001020304050607080901000% Nodes generated102030405060708090100% Nodes generated(a)(b)Figure 18: Applying marginal-utility inference backtracking search (a)Open knight tour(b) N-queens task18(a),(b) compares performance multiple-goal backtracking search withoutmarginal utility. see applying marginal-utility inference signicantly increasesperformance problems.also tested multiple-goal algorithm, described Section 3, siblingbased marginal-utility heuristic selects node focal group. experimentperformed known multiple-sequence alignment problem. Since graph degreelarge, applied modied version described Yoshizumi, MiuraIshida (2000). used heuristic, methodology data set describedpaper, requiring algorithm collect optimal 1.1-suboptimal solutions.Figure 19 shows results obtained two data sets. see marginal utilityimproves performance search algorithm.7. Related Workmultiple-goal search framework dened paper novel. previous worktreated heuristic search multiple goals general search framework previouswork provided general algorithms multiple-goal search. planning communitydealt multiple goals entirely dierent setup, goals conjunctive442fiMultiple-Goal Heuristic Search100100A* using regular heuristicA* marginal-utility inference80% Goals found% Goals found80A* using regular heuristicA* marginal-utility inference6040206040200001020304050607080901000% Nodes generated102030405060708090100% Nodes generated(a)(b)Figure 19: Applying marginal-utility inference search (a)Data set 1 (b) Data set 2possibly conicting. particular, setup easily applicable generic graphsearch.Even domain-specic algorithms multiple-goal heuristic search common.domains mentioned Section 6.2, one given attention multiplegoal problem domain Web crawling. sequence alignment domain used severalworks heuristic search (for example, Korf & Felner, 2002; Zhou & Hansen, 2002, 2003;Schroedl, 2005), single-goal search problem.popularity Web led many researchers explore problem multiplegoal search Web graph. problem better known focused crawling. Chakrabartiet al. (1999) dened focused crawler Web agent selectively seeks pagesrelevant pre-dened set topics retrieving links live Web.agents used building domain-specic Web indices (see example McCallum,Nigam, Rennie, & Seymore, 1999). Focused Web-crawling algorithms use various methodsbreadth-rst search (Najork & Wiener, 2001), best-rst search (Cho & GarciaMolina, 2000; Cho et al., 1998), reinforcement learning (Boyan, Freitag, & Joachims,1996; Rennie & McCallum, 1999). heuristic methods focused crawlingbased Web-specic features. example, page-rank model (Page, Brin, Motwani,& Winograd, 1998) used Brin Page (1998), Haveliwala (1999).hubs-and-authorities model (Kleinberg, 1999) used Borodin, Roberts, Rosenthal,Tsaparas (2001). addition, several theoretical works provide analysis boundsproblem Web crawling (for example Cooper & Frieze, 2002; Kumar et al., 2000).approach similar taken Rennie McCallum (Rennie &McCallum, 1999), apply reinforcement learning Web crawling. Like marginalutility induction method, method also estimates reward value generalizesunvisited nodes. are, however, several important dierences two methods, particulary denition reward. approach based maximalnumber goals achieved given resources, method focused immediacygoal achievement. sooner goal achieved optimal algorithm startinggraph node, contributes reward value node regardlesswhether algorithm enough resources collect goal. Thus, setup443fiDavidov & Markovitchallow direct incorporation supplied resource limit input. Furthermore, approachrelies relatively heavy o-line processing training set. propose online updatemethod estimate update marginal-utility based system. approacheliminates need fetching xed training set, also gives exibilityalgorithm.8. Discussionwork described paper presents new framework heuristic search.framework task collect many goals possible within allocated resources.show traditional distance-estimation heuristic suciently eectivemultiple-goal search. introduce sum progress heuristics, take advantage explicitly given goal set estimate direction larger closer groupsgoals.One problem heuristics ignore expected resources requiredcollect goals alternative directions. introduce marginal-utility heuristic,attempts estimate cost per goal search direction. Thus, usinglead productive search.Designing eective marginal-utility heuristic rather dicult task. thereforedeveloped two methods online learning marginal-utility heuristics. One basedlocal similarity partial marginal-utility sibling nodes, generalizesmarginal-utility state feature space. methods infer marginal utilitypartial marginal-utility values based number visited goal non-goalnodes partially explored subgraphs.sibling-based inference method requires basic input search problem:set starting nodes, successor function, goal predicate. method alsotake advantage input resource allocation, demonstrated Section 6.7.distance-estimation heuristic given, sibling-based method utilize initialstages search data base inference sucient.marginal-utility generalization method requires set meaningful features setstates. common requirement learning systems.applied methodology several tasks, including focused Web crawling,showed merit various conditions. also applied tasks nding pathsmultiple goals, planning movement multiple robots, knight-tour, n-queens, ndingset multiple sequence alignments. experiments show even without priorknowledge goal type, given goal predicate, algorithm,initiation period, signicantly outperforms blind best-rst search using distanceestimation heuristic. enhance method regular distance-estimationheuristic, method shows threefold improvement distanceestimation heuristic alone.framework proposed algorithms applicable wide variety problemsinterested nding many goal states rather one. show,example, multiple sequence alignment problem, methods allow reachmany nearly-optimal congurations. methods also applied con-444fiMultiple-Goal Heuristic Searchstraint satisfaction problems may useful nd many solutions applyanother algorithm selecting solution found set.apply framework new problems, following requirements must fullled:1. problem domain formulated state space.2. exists predicate identies goal states.3. using sum progress heuristics:(a) traditional function estimates distance two statesgiven.(b) set goals states given explicitly.4. sibling-based method marginal utility inference assume marginalutility values sibling nodes relatively similar.5. induction-based marginal utility inference assume availability setstate features informative respect marginal utility.main message research induction-based method marginalutility inference used possible. Unlike sum progress heuristics,takes account resources needed collecting goals. Unlike siblingbased inference method, makes assumptions similarity marginal utility valuessiblings. require informative state features; however, many domains,reach set state features sucient inducing marginal utility estimation functionavailable.marginal-utility based heuristic techniques greedy sense alwayschoose node leading subgraph would expect nd maximal numbergoals, use remaining resources subgraph. sophisticated approach would try wisely distribute remaining resources promisingdirections, leading, hope, better performance greedy approach.Although marginal utility approach consider possible overlap subgraphs, propose, Section 4.5, technique reduce it. interesting directioncould predict actual overlap search progresses, using methodspartial marginal-utility calculation.framework described paper opens new research direction. eld wideopen development new algorithms application multiple-goal searchalgorithms tasks.Acknowledgementswould like thank Adam Darlo, Irena Koifman Yaron Goren, helped usprogramming. research supported fund promotion researchTechnion Israeli Ministry Science.445fiDavidov & MarkovitchAppendix A. Detailed Resultsappendix provide breakdown results 10 Web topics. Tables 23 refer results described Section 6.4.1. Table 4 refers results describedSection 6.4.2. Table 5 refers results described Section 6.5. Tables 6 7 refersresults described Section 6.7.Goal typeRoboticsStudentsMathematicsFootballSportsLaboratoriesFoodPublicationsProjectsNews% goals found 10% resourcesBFSDistanceMarginalestimationutility2.6(0.9)7.2(0.2)18.1(0.6)10.5(1.0)11.8(0.5)17.0(1.3)7.2(0.1)17.6(1.6)15.4(0.8)0.0(0.0)31.4(0.5)25.3(0.9)3.6(0.4)37.0(1.1)45.1(2.7)0.3(0.1)12.5(0.7)33.4(0.8)7.2(1.2)25.1(0.9)30.3(1.7)0.3(0.1)12.6(1.0)35.2(2.6)0.1(0.0)30.1(1.4)41.2(1.6)8.5(0.9)23.1(0.8)22.6(0.8)% goals found 20% resourcesBFSDistanceMarginalestimationutility9.3(1.6)30.5(1.1)58.3(2.0)12.5(1.5)23.7(0.6)54.6(1.3)13.1(1.2)28.3(0.8)60.3(2.9)0.8(0.0)42.1(1.7)71.5(0.7)16.2(0.7)41.6(0.7)68.5(1.4)5.3(0.5)18.1(0.8)80.4(3.1)26.5(3.9)48.5(1.7)92.7(2.2)3.5(0.1)18.5(1.1)64.2(1.9)0.8(0.2)32.0(1.4)80.5(2.0)15.3(1.0)40.4(1.2)88.9(0.7)Table 2: Marginal-utility distance-estimation heuristics focused crawling (with D=4).numbers parentheses standard deviations.Goal typeRoboticsStudentsMathematicsFootballSportsLaboratoriesFoodPublicationsProjectsNews% goals found 10% resourcesDistanceMarginal Combinedestimationutilitymethod7.2(0.2)18.1(0.6)21.1(0.5)11.8(0.5)17.0(1.8)23.3(0.6)17.6(1.6)15.4(0.8)26.5(1.9)31.4(0.5)25.3(0.9)33.9(1.0)37.0(1.1)45.1(2.7)48.3(2.2)12.5(0.7)33.4(0.7)40.2(0.9)25.1(0.9)30.3(1.7)30.1(1.2)12.6(1.0)35.2(2.7)42.0(2.4)30.1(1.4)41.2(1.6)41.3(1.0)23.1(0.8)22.6(0.8)30.5(1.9)% goals found 20% resourcesDistanceMarginal Combinedestimationutilitymethod30.5(1.1)58.3(2.0)65.1(2.1)23.7(0.6)54.6(1.3)59.6(1.2)28.3(0.8)60.3(2.9)68.7(2.5)42.1(1.7)71.5(0.7)70.9(1.1)41.6(0.7)68.5(1.4)75.0(1.3)18.1(0.8)80.4(3.1)79.8(1.0)48.5(1.7)92.7(2.2)93.3(1.6)18.5(1.1)64.2(1.9)77.8(1.7)32.0(1.4)80.5(2.0)84.7(1.8)40.4(1.2)88.9(0.7)90.5(0.9)Table 3: Combining marginal-utility distance-estimation heuristics focused crawling.numbers parentheses standard deviations.446fiMultiple-Goal Heuristic SearchGoal typeRoboticsStudentsMathematicsFootballSportsLaboratoriesFoodPublicationsProjectsNews% goals found 10% resourcesDistanceInferenceInferenceestim.siblings induction7.2(0.2)18.1(0.7)8.4(1.2)11.8(0.6)17.0(1.9)12.4(1.1)17.6(1.6)15.4(0.9)10.8(1.5)31.4(0.5)25.3(0.4)21.1(0.7)37.0(1.2)45.1(2.7)36.2(1.6)12.5(0.7)33.4(0.8)19.4(1.3)25.1(0.9)30.3(1.7)27.1(0.6)12.6(1.0)35.2(2.7)21.8(2.4)30.1(1.4)41.2(1.8)35.5(1.2)23.1(0.8)22.6(0.8)23.2(0.9)% goals found 20% resourcesDistanceInferenceInferenceestim.siblings induction30.5(1.1)58.3(2.0)75.5(2.5)23.7(0.6)54.6(1.3)68.2(1.6)28.3(0.8)60.3(2.9)69.7(2.5)42.1(1.7)71.5(0.7)71.6(1.1)41.6(0.7)68.5(1.4)79.4(1.5)18.1(0.8)80.4(3.1)86.0(1.3)48.5(1.7)92.7(2.2)89.1(1.8)18.5(1.1)64.2(1.9)78.9(1.2)32.0(1.4)80.5(2.0)89.8(2.5)40.4(1.2)88.9(0.7)93.9(1.1)Table 4: performance two methods marginal-utility estimation focused crawlingtask. numbers parentheses standard deviations.Goal typeRoboticsStudentsMathematicsFootballSportsLaboratoriesFoodPublicationsProjectsNewsRoboticsStudentsMathematicsFootballSportsLaboratoriesFoodPublicationsProjectsNews% goals found 10% resourcesDistance-estimation Inference siblings Inference inductionWithoutWithoutenhanc.enhanc.enhanc.enhanc.7.2(0.2)18.1(0.7)33.4(2.1)8.4(1.2)22.1(2.2)11.8(0.5)17.0(1.8)26.5(1.0)12.4(1.1)25.2(1.6)17.6(1.6)15.4(0.8)35.2(1.4)10.8(1.4)36.8(1.5)31.4(0.6)25.3(0.9)46.9(1.4)21.1(0.7)39.5(1.4)37.0(1.2)45.1(2.7)54.9(1.6)36.2(1.6)46.8(1.5)12.5(0.7)33.4(0.8)46.4(1.6)19.4(1.3)39.5(1.4)25.1(0.9)30.3(1.7)35.1(1.4)27.1(0.6)30.4(1.4)12.6(1.1)35.2(2.6)46.8(1.5)21.8(2.4)30.0(1.4)30.1(1.4)41.2(1.6)49.2(1.2)35.5(1.2)44.6(1.3)23.1(0.8)22.6(0.8)41.1(1.6)23.2(0.9)42.3(1.1)% goals found 20% resources30.5(1.2)58.3(2.0)83.2(1.8)75.5(2.6)89.1(1.5)23.7(0.6)54.6(1.3)65.4(1.7)68.2(1.6)78.3(1.6)28.3(0.8)60.3(2.9)88.6(1.0)69.7(2.5)92.0(1.1)42.1(1.7)71.5(0.8)80.6(1.4)71.6(1.1)91.3(1.4)41.6(0.7)68.5(1.4)78.5(1.6)79.4(1.6)86.4(1.5)18.1(0.8)80.4(3.1)86.4(2.0)86.0(1.3)92.1(1.4)48.5(1.7)92.7(2.2)95.1(1.6)89.1(1.8)95.0(1.7)18.5(1.1)64.2(1.9)87.2(2.1)78.9(1.2)94.7(1.4)32.0(1.4)80.5(2.1)85.4(1.7)89.8(2.5)95.2(1.7)40.4(1.3)88.9(0.8)93.8(1.2)93.9(1.1)96.5(1.5)Table 5: performance two marginal-utility based methods enhancementsenabled. numbers parentheses standard deviations.447fiDavidov & MarkovitchGoal typeRoboticsStudentsMathematicsFootballSportsLaboratoriesFoodPublicationsProjectsNews% goals foundContract:10% ResourcesAnytime Contract18.1(0.7) 24.6(1.1)17.0(1.9) 29.3(1.5)15.4(0.8) 24.1(0.9)25.3(0.9) 29.9(1.0)45.1(2.7) 60.4(1.1)33.4(0.7) 40.7(0.9)30.3(1.7) 32.1(1.6)35.2(2.6) 55.0(2.3)41.2(1.6) 48.6(0.9)22.6(0.8) 41.5(1.0)Contract:5% ResourcesAnytime Contract0.1(0.0)2.9(0.2)0.7(0.0)1.9(0.1)1.8(0.3)8.4(0.8)4.2(0.2)12.1(0.5)12.7(0.6) 20.4(1.1)10.5(1.0) 11.3(0.6)9.4(0.9)22.1(1.2)2.1(0.3)12.9(1.7)0.9(0.0)21.5(1.6)8.8(1.0)13.2(0.9)Contract:20% ResourcesAnytime Contract58.3(2.0) 69.5(2.2)54.6(1.3) 62.8(1.3)55.3(0.9) 64.0(1.1)71.5(0.8) 75.6(0.8)68.5(1.4) 77.7(1.3)80.4(3.1) 81.6(0.1)92.7(2.2) 92.8(2.2)64.2(1.9) 76.2(1.5)80.5(2.1) 86.5(1.6)88.9(0.7) 91.1(1.0)Table 6: Contract vs. anytime performance supplying resource limit. numbersparentheses standard deviations.Goal typeRoboticsStudentsMathematicsFootballSportsLaboratoriesFoodPublicationsProjectsNewsContract: 5% GoalsAnytime Contract8.4(0.6)6.2(1.1)9.0(0.5)7.1(0.4)5.4(0.4)3.9(0.2)5.2(0.2)2.1(0.2)3.9(0.2)1.9(0.1)4.2(0.1)1.8(0.1)5.9(0.3)2.2(0.3)6.1(0.5)3.3(0.4)4.0(0.3)1.7(0.1)7.6(0.7)5.9(0.4)% resources consumedContract: 20% GoalsAnytime Contract12.1(0.7)9.8(0.5)11.3(0.5)8.4(0.3)10.3(0.5)6.8(0.4)8.4(0.3)5.2(0.2)7.1(0.6)6.0(0.4)7.4(0.4)5.3(0.3)8.8(0.5)5.1(0.2)7.7(1.1)5.9(0.5)8.3(0.5)6.1(0.3)9.8(0.2)6.5(0.3)Contract: 50% GoalsAnytime Contract17.2(1.2)10.2(1.0)18.7(0.6)9.9(0.5)14.3(0.6)9.9(0.4)14.6(0.5)9.9(0.7)12.2(0.6)6.5(0.6)14.1(0.6)6.6(0.2)14.3(0.5)7.0(0.6)18.8(0.7)10.3(0.6)16.5(0.6)9.0(0.5)16.0(0.4)10.3(0.6)Table 7: Contract vs. anytime performance supplying goal requirements. numbersparentheses standard deviations.448fiMultiple-Goal Heuristic SearchReferencesBoddy, M., & Dean, T. L. (1994). Deliberation scheduling problem solving timeconstrained environments. Articial Intelligence, 67 (2), 245285.Borodin, A., Roberts, G. O., Rosenthal, J. S., & Tsaparas, P. (2001). Finding authoritieshubs link structures www. 10th International WWW conference,pp. 415429. ACM Press.Boyan, J., Freitag, D., & Joachims, T. (1996). machine learning architecture optimizing web search engines. Proceedings AAAI Workshop Internet-BasedInformation Systems, pp. 324335.Breiman, L., Friedman, J. H., Olshen, R. A., & Stone, P. J. (1984). ClassicationRegression Trees. Wadsworth International, Monterey, CA.Brin, S., & Page, L. (1998). anatomy large-scale hypertextual Web search engine.Computer Networks ISDN Systems, 30 (17), 107117.Chakrabarti, S., van den Berg, M., & Dom, B. (1999). Focused crawling: new approachtopic-specic Web resource discovery. Computer Networks, 31 (1116), 16231640.Cho, J., & Garcia-Molina, H. (2000). evolution Web implicationsincremental crawler. El Abbadi, A., Brodie, M. L., Chakravarthy, S., Dayal, U.,Kamel, N., Schlageter, G., & Whang, K.-Y. (Eds.), VLDB 2000, pp. 200209, LosAltos, CA 94022, USA. Morgan Kaufmann Publishers.Cho, J., Garca-Molina, H., & Page, L. (1998). Ecient crawling URL ordering.Computer Networks ISDN Systems, 30 (17), 161172.Cooper, C., & Frieze, A. (2002). Crawling web graphs. Proceedings STOC, pp.419427.Cover, T. M., & Hart, P. E. (1967). Nearest neighbor pattern classication. IEEE Transactions Information Theory, 13, 2127.Culberson, J. C., & Schaeer, J. (1998). Pattern databases. Computational Intelligence,14 (4), 318334.Diligenti, M., Coetzee, F., Lawrence, S., Giles, C. L., & Gori, M. (2000). Focused crawlingusing context graphs. 26th International Conference Large Databases,VLDB 2000, pp. 527534, Cairo, Egypt.Douglis, F., Feldmann, A., Krishnamurthy, B., & Mogul, J. C. (1997). Rate changemetrics: live study www. USENIX Symposium InternetTechnologies Systems, pp. 147158.Gasser, R. U. (1995). Harnessing Computational Resources Ecient Exhaustive Search.Ph.D. thesis, ETH, Swiss Federal Institute Technology, Zurich, Switzerland.Hansen, E. A., & Zilberstein, S. (1996). Monitoring progress anytime problem-solving.Proceedings Thirteenth National Conference Articial Intelligence (AAAI96), pp. 12291234, Portland, Oregon, USA. AAAI Press / MIT Press.Haveliwala, T. (1999). Ecient computation PageRank. Tech. rep. 1999-31.449fiDavidov & MarkovitchHeld, M., & Karp, R. M. (1970). traveling salesman problem minimum spanningtrees. Operations Research, 18, 11381162.Hirai, J., Raghavan, S., Garcia-Molina, H., & Paepcke, A. (2000). Webbase: repositoryweb pages. Proceedings 9th International WWW Conference, pp. 277293.Hovitz, E. (1990). Computation Action Bounded Resources. Ph.D. thesis, Stanford University.Joachims, T. (1997). probabilistic analysis Rocchio algorithm TFIDFtext categorization. Proc. 14th International Conference Machine Learning, pp.143151. Morgan Kaufmann.Kleinberg, J. M. (1999). Authoritative sources hyperlinked environment. JournalACM, 46 (5), 604632.Korf, R. E., & Zhang, W. (2000). Divide-and-conquer frontier search applied optimalsequence allignment. National Conference Articial Intelligence (AAAI), pp.910916.Korf, R. E., & Felner, A. (2002). Disjoint pattern database heuristics. Articial Intelligence,134 (12), 922.Kumar, R., Raghavan, P., Rajagopalan, S., Sivakumar, D., Tomkins, A., & Upfal, E. (2000).Web graph. Proc. 19th Symp. Principles Database Systems, PODS,pp. 110. ACM Press.Lawrence, S., & Giles, C. L. (1998). Searching WWW. Science, 280 (5360), 98100.McCallum, A., Nigam, K., Rennie, J., & Seymore, K. (1999). Building domain-specicsearch engines machine learning techniques. Proc. AAAI-99 Spring SymposiumIntelligent Agents Cyberspace, pp. 2839.Menczer, F., Pant, G., Srinivasan, P., & Ruiz, M. (2001). Evaluating topic-driven webcrawlers. Proceedings SIGIR-01), pp. 241249, New York. ACM Press.Montgomery, D. C. (2001). Design Analysis Experiments (5 edition). John WileySons.Mostow, J., & Prieditis, A. E. (1989). Discovering admissible heuristics abstractingoptimizing: transformational approach. Proceedings IJCAI-89, Vol. 1, pp.701707.Najork, M., & Wiener, J. L. (1998). technique measuring relative size overlappublic web search engines. Proceedings Seventh International WWWConference[WWW7], pp. 379388.Najork, M., & Wiener, J. L. (2001). Breadth-rst crawling yields high-quality pages.Proceedings 10th International WWW Conference, pp. 114118.Page, L., Brin, S., Motwani, R., & Winograd, T. (1998). pagerank citation ranking:Bringing order web. Tech. rep., Stanford Digital Library Technologies Project.Pandurangan, G., Raghavan, P., & Upfal, E. (2002). Using PageRank Characterize WebStructure. 8th Annual International Computing Combinatorics Conference(COCOON), pp. 330339.450fiMultiple-Goal Heuristic SearchPearl, J., & Kim, J. H. (1982). Studies semi-admissible heuristics. IEEE TransactionsPattern Analysis Machine Intelligence, 4 (4), 392399.Prieditis, A. E. (1993). Machine discovery eective admissible heuristics.. 12 (13),117141.Rennie, J., & McCallum, A. K. (1999). Using reinforcement learning spider Webeciently. Bratko, I., & Dzeroski, S. (Eds.), Proceedings ICML-99, 16th International Conference Machine Learning, pp. 335343, Bled, SL. Morgan KaufmannPublishers, San Francisco, US.Russell, S. J., & Zilberstein, S. (1991). Composing real-time systems. ProceedingsIJCAI-91, pp. 213217, Sydney. Morgan Kaufmann.Russell, S., & Norvig, P. (2003). Articial Intelligence: Modern Approach (2nd editionedition). Prentice-Hall, Englewood Clis, NJ.Salton, G., & Buckley, C. (1988). Term weighting approaches automatic text retrieval.Information Processing Management, 24 (5), 513523.Schroedl, S. (2005). improved search algorithm optimal multiple-sequence alignment.Journal Articial Intelligence Research, 23, 587623.Utgo, P. (1988). incremental ID3. Fifth International Conference MachineLearning, pp. 107120. Morgan Kaufmann.Yoshizumi, T., Miura, T., & Ishida, T. (2000). * partial expansion large branchingfactor problems. AAAI/IAAI, pp. 923929.Zhou, R., & Hansen, E. A. (2002). Multiple sequence alignment using anytime a*.Proceedings Eighteenth National Conference Articial Intelligence, pp. 975976, Edmonton, Alberta, Canada.Zhou, R., & Hansen, E. A. (2003). Sweep a*: Space-ecient heuristic search partiallyordered graphs. Proceedings 15th IEEE International Conference ToolsArticial Intelligence, pp. 427434, Sacramento, CA.Zilberstein, S. (1996). Using anytime algorithms intelligent systems. AI Magazine, 17 (3),7383.Zilberstein, S., Charpillet, F., & Chassaing, P. (1999). Real-time problem-solvingcontract algorithms. IJCAI, pp. 10081015.451fiJournal Artificial Intelligence Research 26 (2006) 371-416Submitted 11/05; published 8/06Clause/Term Resolution Learning EvaluationQuantified Boolean FormulasEnrico GiunchigliaMassimo NarizzanoArmando Tacchellagiunchiglia@unige.itmox@dist.unige.ittac@dist.unige.itDIST - Universita di GenovaViale Causa 13, 16145 Genova, ItalyAbstractResolution rule inference basis procedures automated reasoning. procedures, input formula first translated equisatisfiableformula conjunctive normal form (CNF) represented set clauses. Deduction starts inferring new clauses resolution, goes empty clausegenerated satisfiability set clauses proven, e.g., new clausesgenerated.paper, restrict attention problem evaluating Quantified BooleanFormulas (QBFs). setting, outlined deduction process knownsound complete given formula CNF form resolution, called Qresolution, used. introduce Q-resolution terms, used formulas disjunctive normal form. show computation performed availableprocedures QBFs based Davis-Logemann-Loveland procedure (DLL) propositional satisfiability corresponds tree Q-resolution terms clausesalternate. poses theoretical bases introduction learning, correspondingrecording Q-resolution formulas associated nodes tree. discussproblems related introduction learning DLL based procedures, presentsolutions extending state-of-the-art proposals coming literature propositionalsatisfiability. Finally, show DLL based solver extended learning, performssignificantly better benchmarks used 2003 QBF solvers comparative evaluation.1. IntroductionResolution (Robinson, 1965) rule inference basis proceduresautomated reasoning (see, e.g., Fermuller, Leitsch, Hustadt, & Tammet, 2001; Bachmair &Ganzinger, 2001). procedures, input formula first translated equisatisfiable formula conjunctive normal form (CNF) represented set clauses.Deduction starts inferring new clauses resolution, goes empty clausegenerated satisfiability set clauses proven, e.g., new clausesgenerated. restrict attention problem evaluating Quantified BooleanFormulas (QBFs). setting, outlined deduction process known soundcomplete given formula CNF form resolution, called Q-resolution,used (Kleine-Buning, Karpinski, & Flogel, 1995). However, available decisionprocedures QBFs based extend Davis-Logemann-Loveland procedure(DLL) (Davis, Logemann, & Loveland, 1962) propositional satisfiability (SAT).c2006AI Access Foundation. rights reserved.fiGiunchiglia, Narizzano & Tacchellapropositional case, well known computation performed DLL correspondsspecific form resolution called regular tree resolution (see, e.g., Urquhart, 1995).paper introduce Q-resolution terms, used formulas disjunctivenormal form. show computation performed DLL based decision proceduresQBFs corresponds tree Q-resolution terms clauses alternate.correspondence poses theoretical bases introduction learning, correspondingrecording Q-resolution formulas associated nodes tree. particular,recording Q-resolutions clauses generalizes popular nogood learning constraint satisfaction SAT literatures (see, e.g., Dechter, 1990; Bayardo, Jr. & Schrag,1997): nogood corresponds set assignments falsifying input formula,useful pruning assignments existential variables. Recording Q-resolutionsterms corresponds good learning: good corresponds set assignments satisfying input formula, useful pruning assignments universal variables.discuss problems related introduction learning DLL based proceduresQBFs, present solutions extending state-of-the-art proposals coming literatureSAT. show effectiveness learning QBFs evaluation problem,implemented QuBE, state-of-the-art QBF solver. Using QuBE, doneexperimental tests several real-world QBFs, corresponding planning (Rintanen, 1999;Castellini, Giunchiglia, & Tacchella, 2003) circuit verification (Scholl & Becker, 2001;Abdelwaheb & Basin, 2000) problems, two primary application domainsinterest. results witness effectiveness learning.paper structured follows. first review basics Quantified BooleanLogic, time introducing terminology notation usedthroughout paper. Section 3, introduce clause term resolution,relation DLL based decision procedures QBFs. Then, Section 4, introducenogood good learning, show effectively integrated DLLbased decision procedures QBFs. implementation experimental resultspresented Section 5. paper ends conclusions related work.paper builds extends many ways AAAI paper (Giunchiglia, Narizzano, & Tacchella, 2002). respect paper, (i) introduce clauseterm resolution; (ii) show correspondence clause/term Q-resolutioncomputation tree searched DLL based decision procedures; (iii) basiscorrespondence, extend basic backtracking search procedure, first backjumpinglearning, prove soundness completeness; (iv) discussimplementation QuBE providing many details, (v) present resultsmuch broader detailed experimental analysis.on, simply write resolution Q-resolution.2. Quantified Boolean LogicConsider set P symbols. variable element P. literal variablenegation variable. following, literal l,|l| variable occurring l;l negation l l variable, |l| otherwise.372fiClause/Term Resolution Learning Quantified Boolean Formulassake simplicity, consider formulas negation normal form (NNF).Thus, us, propositional formula combination literals using k-ary (k 0)connectives (for conjunctions) (for disjunctions). following, use TrueFalse abbreviations empty conjunction empty disjunction respectively.QBF expression form= Q1 z1 Q2 z2 . . . Qn zn(n 0)(1)every Qi (1 n) quantifier, either existential universal ,z1 , . . . , zn distinct variables,propositional formula z1 , . . . , zn .example,x1 yx2 ((x1 x2 ) (y x2 ) (x2 ((x1 y) (y x2 ))))(2)QBF.(1), Q1 z1 . . . Qn zn prefix matrix. also say literal lexistential |l| belongs prefix, universal otherwise. Finally, (1),definelevel variable zi , 1 + number expressions Qj zj Qj+1 zj+1prefix j Qj 6= Qj+1 ;level literal l, level |l|.example, (2) x2 existential level 1, universal level 2, x1existential level 3.value semantics QBF defined recursively follows:1. prefix empty, evaluated according truth tables propositionallogic.2. x, true x true x true.3. y, true true.(1) l literal |l| = zi , l QBFwhose matrix obtained substitutingzi True z False l = zi ,zi False z True l = z .whose prefix Q1 z1 Q2 z2 . . . Qi1 zi1 Qi+1 zi+1 . . . Qn zn .easy see QBF without universal quantifiers, problem determiningvalue reduces SAT problem.Two QBFs equivalent either true false.373fiGiunchiglia, Narizzano & Tacchella3. Resolution DLL Based Decision Procedures QBFssection first introduce clause/term resolution DLL based decision proceduresQBFs, show correspondence two.3.1 Clause Term ResolutionAccording definition QBF, matrix combination conjunctionsdisjunctions literals. However, using common clause form transformations basedrenaming first used Tseitin (1970), possible perform linear time conversionarbitrary QBF equivalent one matrix conjunctive normal form(CNF). conversions based fact QBF (1) equivalentQ1 z1 Q2 z2 . . . Qn zn x((x ) [x/])(n 0)propositional formula literal;x variable distinct z1 , z2 , . . . , zn ;[x/] propositional formula obtained substituting one occurrences x.Thus,((x1 y) (y x2 ))follows (2) equivalentx1 yx2 x3 ((x1 x2 ) (y x2 ) (x2 x3 ) (x1 x3 ) (y x2 x3 ))(3)Thanks conversions, restrict attention QBFs matrixCNF, represent matrix formula set clauses interpretedconjunctively, clause finite set literals interpreted disjunctively. Further,assume clause non-tautological minimal. clause tautologicalcontains variable negation. clause C minimal literals Cminimum level existential. minimal form clause C clause obtainedC deleting universal literals cause C non-minimal. instance,(4), clauses non-tautological minimal. assumption clausesnon-tautological minimal restriction, following theorem states.Theorem 1 Let QBF matrix CNF. Let 0 QBF obtained1. eliminating tautological clauses;2. replacing non-tautological non-minimal clause minimal form.0 equivalent.374fiClause/Term Resolution Learning Quantified Boolean FormulasProof. Clearly, tautological clauses eliminated result equivalentQBF. Let C = {l1 , . . . , ln , ln+1 , . . . , lm } non-tautological non-minimal clauseln+1 , . . . , lm universal literals C \ min(C) (0 n < m). Further, withoutloss generality, assume level li less equal level li+1 ,1 < m. Then, form (p m). . . Q1 |l1 | . . . |ln | . . . |ln+1 | . . . |lm |Qm+1 zm+1 . . . Qp zp {{l1 , . . . , ln , ln+1 , . . . , lm }, . . .},standing. . . Q1 |l1 | . . . |ln | . . . |ln+1 | . . . |lm |Qm+1 zm+1 . . . Qp zp ((l1 . . . ln ln+1 . . . lm ) ).Then, applying standard rules quantifiers, rewritten. . . Q1 |l1 | . . . |ln | . . . |ln+1 | . . . |lm |((l1 . . . ln ln+1 . . . lm ) Qm+1 zm+1 . . . Qp zp ),equivalent. . . Q1 |l1 | . . . |ln | . . . |ln+1 | . . . (|lm |(l1 . . .ln ln+1 . . .lm )|lm |Qm+1 zm+1 . . . Qp zp ),equivalent. . . Q1 |l1 | . . . |ln | . . . |ln+1 | . . . ((l1 . . . ln ln+1 . . . lm1 ) |lm |Qm+1 zm+1 . . . Qp zp ),equivalent. . . Q1 |l1 | . . . |ln | . . . |ln+1 | . . . |lm |Qm+1 zm+1 . . . Qp zp ((l1 . . . ln ln+1 . . . lm1 ) ),i.e., QBF obtained deleting lm clause C. iteratingreasoning process, literals C \ min(C) eliminated C, hencethesis.on, QBF CNF matrix conjunction clauses,clause minimal non-tautological. represent matrix QBFset clauses,empty clause {} stands False;empty set clauses {} stands True;formula {{}} equivalent False;QBF (3) writtenx1 yx2 x3 {{x1 , y, x2 }, {y, x2 }, {x2 , x3 }, {x1 , y, x3 }, {y, x2 , x3 }}.(4)Clause resolution (Kleine-Buning et al., 1995) similar ordinary resolutionexistential literals matched. precisely, clause resolution (on literal l)ruleC1C2(5)min(C)375fiGiunchiglia, Narizzano & Tacchella(c1)(c2)(c3)(c4){x1 , y, x2 }{y, x2 }{x2 , x3 }{x1 , y, x3 }InputInputInputInputformulaformulaformulaformula(c5)(c6)(c7)(c8){x1 }{x3 , y}{x1 }{}(c1),(c2),(c4),(c5),(c2)(c3)(c6)(c7)Table 1: clause resolution deduction showing (4) false. prefix x1 yx2 x3 .l existential literal;C1 , C2 two clauses {l, l} (C1 C2 ), literal l0 6= l, {l0 , l0 }(C1 C2 );C (C1 C2 ) \ {l, l}.C1 C2 antecedents, min(C) resolvent rule.Theorem 2 ((Kleine-Buning et al., 1995)) Clause resolution sound completeproof system deciding QBFs CNF: QBF CNF true emptyclause derivable clause resolution.instance, fact (4) false follows deduction Table 1.Alternatively CNF conversion, could converted (2) QBFmatrix disjunctive normal form (DNF), linear time, basis QBF(1), equivalentQ1 z1 Q2 z2 . . . Qn zn y((y ) [y/])(n 0),assuming propositional formula literal, variable distinctz1 , z2 , . . . , zn .simple recursive application equivalence (2) leads followingequivalent QBF:x1 yx2 y1 y2 y3 y4 y5 y6 ((y1 y2 y3 )(y 1 x1 ) (y 1 y) (y 1 x2 )(y 2 y) (y 2 x2 )(y 3 x2 ) (y 3 y4 )(y 4 y5 y6 )(y 5 x1 ) (y 5 y)(y 6 y) (y 6 x2 )).(6)Given QBF matrix DNF, represent matrix set termsinterpreted disjunctively, term finite set literals interpretedconjunctively. Further, assume term non-contradictory minimal.term contradictory contains variable negation. term minimalliterals minimum level universal. minimal form termterm obtained deleting existential literals cause non-minimal.terms (6) non-contradictory minimal. Analogously saidQBFs CNF, QBF DNF assume termsnon-contradictory minimal without loss generality.376fiClause/Term Resolution Learning Quantified Boolean FormulasTheorem 3 Let QBF matrix DNF. Let 0 QBF obtained1. eliminating contradictory terms;2. replacing non-contradictory non-minimal term minimal form.0 equivalent.Proof. Analogous proof Theorem 1.before, on, QBF DNF matrix disjunctionterms, term minimal non-contradictory.introduce term resolution (on literal l) consists ruleT1T2min(T )l universal literal;T1 , T2 two terms {l, l} (T1 T2 ), literal l0 6= l, {l0 , l0 }(T1 T2 );(T1 T2 ) \ {l, l}.T1 T2 antecedents, min(T ) resolvent rule.Theorem 4 Term resolution sound complete proof system deciding QBFsDNF: QBF DNF true empty term derivable term resolution.Proof. fact term resolution sound complete proof system followssoundness completeness clause resolution.Let set sets literals, = Q1 z1 Q2 z2 . . . Qn zn QBFinterpreted set clauses. Without loss generality assume clausenon-tautological minimal. following chain equivalences holds:exists deduction empty clause using clause resolutionfalseQBF = Q1 z1 Q2 z2 . . . Qn zn interpreted set terms truededuction empty term using term resolution.chain equivalences, Q Q = , Q = .example term resolution deduction empty term, consider QBF :x1 yx2 x3 ((x1 x2 ) (y x2 ) (x2 x3 ) (x1 x3 ) (y x2 x3 ))377fiGiunchiglia, Narizzano & Tacchellai.e., QBF obtained (3) simultaneously replacing , , ,. Then, deduction Table 1 also deduction empty termusing term resolution.QBF DNF CNF term resolution cannot applied,thus term resolution sufficient proving truth falsity . However,also following model generation rulemin(T )matrix ;non-contradictory term clause C , C 6= ,get sound complete proof system QBFs CNF. Intuitively, model generation rule allows us start minimal form terms propositionally entailmatrix input formula.Theorem 5 Term resolution model generation sound complete proof systemdeciding QBFs CNF: QBF CNF true empty term derivableterm resolution model generation.Proof. Given QBF CNF matrix , model generation rule deriveset terms form min(T )term non-contradictory clause C , C 6= ;disjunction terms propositionally logically equivalent .Let 0 QBF DNF obtained substituting . 0value. Hence thesis thanks Theorem 4.3.2 DLL Based Decision Procedures QBFsGiven said far, arbitrary QBF converted (in linear time)equivalent QBF CNF. this, end paper, restrictattention QBFs format. assumption, (1) l literal|l| = zi , redefine l QBFwhose matrix obtained removing clauses C l C,removing l clauses;whose prefix Q1 z1 Q2 z2 . . . Qi1 zi1 Qi+1 zi+1 . . . Qn zn .378fiClause/Term Resolution Learning Quantified Boolean FormulasFurther, extend notation sequence literals: = l1 ; l2 ; . . . ; lm (m 0),defined (. . . ((l1 )l2 ) . . .)lm .Consider QBF .simple procedure determining value starts empty assignmentrecursively extends current assignment z and/or z, z heuristicallychosen variable highest level , either empty clause empty setclauses produced . basis values ;z ;z , valuedetermined according semantics QBFs. value value .Cadoli, Giovanardi, Giovanardi Schaerf (2002) introduced various improvementsbasic procedure.first improvement directly conclude valuematrix contains contradictory clause (Lemma 2.1 Cadoli et al., 2002). clauseC contradictory contains existential literal. example contradictory clauseempty clause.second improvement allows us directly extend l l unit monotone(Lemmas 2.4, 2.5, 2.6 Cadoli et al., 2002). (1), literal l is:Unit l existential 0,clause {l, l1 , . . . , lm } belongs ;literal li (1 m) universal level lower level l.Monotone pureeither l existential, l belong clause , l occurs ;l universal, l belong clause , l occurs .example, QBF form. . . x1 yx2 . . . {{x1 , y}, {x2 }, . . .},x1 x2 unit. QBFy1 x1 y2 x2 {{y 1 , y2 , x2 }, {x1 , 2 , x2 }},monotone literals y1 x1 .improvements, resulting procedure, called Q-DLL, essentially onepresented work Cadoli, Giovanardi, Schaerf (1998), extends DLLorder deal QBFs. Figure 1 simple, recursive presentation it. figure,given QBF ,1. False returned contradictory clause matrix (line 1); otherwise2. True returned matrix empty (line 2); otherwise3. line 3, recursively extended ; l l unit (and say lassigned unit); otherwise379fiGiunchiglia, Narizzano & Tacchella0 function Q-DLL(, )1(ha contradictory clause matrix i) return False;2(hthe matrix emptyi) return True;3(hl unit i) return Q-DLL(, ; l);4(hl monotone i) return Q-DLL(, ; l);5l := ha literal highest level i;6(hl existentiali) return Q-DLL(, ; l) Q-DLL(, ; l);7else return Q-DLL(, ; l) Q-DLL(, ; l).Figure 1: algorithm Q-DLL.4. line 4, recursively extended ; l l monotone (and say lassigned monotone); otherwise5. literal l highest level chosenl existential (line 6), extended ; l first (and say lassigned left split). result False, ; l tried returned (andcase say l assigned right split).Otherwise (line 7), l universal, extended ; l first (and say lassigned left split). result True, ; l tried returned(and case say l assigned right split).Theorem 6 Q-DLL(, ) returns True true, False otherwise.Proof. Trivial consequence Lemmas 2.1, 2.4, 2.5, 2.6 work Cadoli, Giovanardi,Giovanardi, Schaerf (2002) semantics QBFs.Given said far, clear Q-DLL evaluates generatingsemantic tree (Robinson, 1968) node corresponds invocation Q-DLLthus assignment . us,assignment (for QBF ) possibly empty sequence = l1 ; l2 ; . . . ; lm (m 0)literals li , li unit, monotone, highest levell1 ;l2 ;...;li1 ;(semantic) tree representing run Q-DLL treenode call Q-DLL(, );edge connecting two nodes ; l, l literal.tree representing run Q-DLL least node .example run Q-DLL, consider QBF (4). simplicity, assumeliteral returned line 5 Figure 1 negation first variable prefixoccurs matrix QBF consideration. Then, tree searchedQ-DLL (4) represented Figure 2. figure:380fiClause/Term Resolution Learning Quantified Boolean Formulas{}{{x1 , y, x2 }, {x1 , y, x3 }, {y, x2 }, {y, x2 , x3 }, {x2 , x3 }}{}{x1 } hx1 , li {x1 }{{y, x3 }, {y, x2 }, {y, x2 , x3 }, {x2 , x3 }}{x1 } hx1 , ri {x1 }{{y, x2 }, {y, x2 }, {y, x2 , x3 }, {x2 , x3 }}hy, ri {x1 }hy, li {y}hy, li {y}hy, ri {x1 }hx2 , pi{y} {x1 , y, x2 }hx2 , ui{x1 }hx2 , pi{y} {x1 , y, x3 }hx3 , ui{x1 }{{}} {y, x2 }{y} {} {y}{y, x2 }hx2 , ui{y, x3 } {y} {} {y}{{}} {x2 , x3 }Figure 2: tree generated Q-DLL (4). matrix (4) shown rootnode, prefix x1 yx2 x3 . u, p, l, r stand unit, pure, leftsplit, right split respectively, obvious meaning.node labeled literal assigned Q-DLL order extend assignment built far. Thus, assignment corresponding node sequencelabels path root node. instance, assignment corresponding node label x3 x1 ; y; x3 .literals assigned unit monotone, corresponding nodes alignedone other. assigned literal l, also show whether lassigned unit, monotone, left right split marking u, p, l, rrespectively.l assigned left right split, also show matrix ;l ,sequence literals assigned l.node leaf, matrix either empty (in casewrite {} node), contains contradictory clause (in casewrite {{}} node).Considering Figure 2, easy see Q-DLL would correctly return False, meaning (4) (and thus also (2)) false.3.3 Resolution DLL Based Decision Procedures QBFswell known correspondence SAT semantic trees resolution (see, e.g.,Urquhart, 1995) gives us starting point analysis, aimed establish correspondence Q-DLL clause/term resolution.Consider QBF . Let tree explored Q-DLL evaluating .time being, assume dealing SAT problem, i.e.,contain universal quantifiers. Then, Q-DLL reduces DLL, falseuse generate clause resolution deduction empty clause . basic ideaassociate node clause C -falsified, i.e.,381fiGiunchiglia, Narizzano & Tacchellaliteral l C, l . (We say literal l assigned l1 ; . . . ; lml {l1 , . . . , lm }). precisely:every leaf , associate arbitrarily selected clause matrix-falsified. least one clause exists contains emptyclause.C clause associated node ; l,1. l 6 C C also clause associated . Notice l monotonel 6 C.2. l C l unit clause associated resolventC arbitrarily selected clause causes l unit .3. l C l unit consider clause C 0 associatednode ; l. l 6 C 0 C 0 clause associated (asfirst case). l C 0 , clause associated resolvent C C 0 .Lemma 1 Let QBF without universal quantifiers. Let tree searchedQ-DLL(, ). Let assignment . false, clause associatednode-falsified;contain existential literals whose negation assigned monotone.Proof. Let set assignments extend . Clearly, assignment0 S, 0 false ( contain universal quantifiers). S, define partialorder relation according two assignments 0 00 0 000 extends 00 . Clearly well founded minimal elementsassignments extending corresponding leaves .0 extends leaf , 0 contains contradictory clause C. Sincecontain universal quantifiers, C 0 -falsified associated node0 . Clearly, C contain existential literals whose negation assignedmonotone.induction hypothesis, assignment 0 = 00 ; l 00 0 -falsifiedclause containing existential literals whose negation assigned monotone.show thesis 00 . three cases:1. l assigned unit. Let C1 clause associated 00 ; l. inductionhypothesis, thesis holds C1 . C1 contain l, thesis trivially follows.Otherwise, clause associated 00 resolvent C C1 clause C2causes l unit 00 . C2 00 ; l-falsified contain existentialliterals whose negation assigned monotone. C = C1 C2 \ {l, l} thusthesis trivially holds.2. l assigned monotone. case clause C associated 00clause associated 00 ; l. induction hypothesis C contain lthus C 00 -falsified.382fiClause/Term Resolution Learning Quantified Boolean Formulas3. l split. case clause C1 associated 00 ; l clause C2associated 00 ; l. thesis holds C1 C2 induction hypothesis.C1 contain l, clause associated 00 C1 thesistrivially holds. Otherwise, C2 contain l, clause associated00 C2 thesis trivially holds. Otherwise, clause associated 00C1 C2 \ {l, l} thesis trivially holds.Theorem 7 Let false QBF without universal quantifiers. tree searched QDLL(, ) corresponds clause resolution deduction empty clause.Proof. Let sequence clauses obtained listing clauses matrixaccording arbitrary order, followed clauses associated internal nodestree searched Q-DLL(, ), assuming visited post order. Clearly,deduction. deduction empty clause node associated-falsified clause (Lemma 1), i.e., empty clause.theorem points close correspondence computation Q-DLLclause resolution, assuming input formula false containuniversal quantifiers. input formula contain universal quantifiers true,still tree explored Q-DLL generating path ending empty matrixcorresponds sequence clause resolutions, one maximal subtree whose leavescontains empty clause.longer assume input formula contain universal quantifiers,consider case arbitrary QBF, situation gets complicated,also possibility assigning unit literals highest level.So, assume literal l assigned unit node , l highestlevel .Then, input formula false, use tree searched Q-DLLgenerate clause resolution deduction empty clause. construction analogousone described before. difference restrict attentionminimal false subtree , i.e., tree obtained deleting subtreesstarting left split universal literal: subtrees originated wrongchoices deciding branch explore first. minimal false subtree 0 ,leaves terminate empty clause, associate node 0clause exactly way described SAT case. instance, (4),Q-DLL assigns unit literals highest level. Figure 3 showsminimal false subtree Q-DLLs computation, associated clause resolutiondeduction empty clause. figure,clause associated node written red right nodeitself;node corresponds assignment unit literal l, clausecauses l unit node (used corresponding clause resolution) writtenred left node.383fiGiunchiglia, Narizzano & Tacchella{}{{x1 , y, x2 }, {x1 , y, x3 }, {y, x2 }, {y, x2 , x3 }, {x2 , x3 }}{}{x1 } hx1 , li {x1 }{{y, x3 }, {y, x2 }, {y, x2 , x3 }, {x2 , x3 }}{x1 } hx1 , ri {x1 }{{y, x2 }, {y, x2 }, {y, x2 , x3 }, {x2 , x3 }}hy, ri {x1 }{x1 , y, x3 }hx3 , ui{x1 }{y, x2 }hx2 , ui{y, x3 }{{}} {x2 , x3 }hy, ri {x1 }{x1 , y, x2 }hx2 , ui{x1 }{{}} {y, x2 }Figure 3: clause resolution corresponding tree generated Q-DLL (4).prefix x1 yx2 x3 .Lemma 2 Let false QBF. Let minimal false subtree tree searchedQ-DLL(, ) assume node ; l , l unit l alsohighest level . Let assignment . false, clause associatednode-falsified;contain existential literals whose negation assigned monotone.Proof. Trivial extension proof Lemma 1. assumption node ; l, l unit l highest level , ensures clause associatednode -falsified.Theorem 8 Let false QBF. Let minimal false subtree tree searchedQ-DLL(, ) assume node ; l , l unit l alsohighest level . corresponds clause resolution deduction emptyclause.Proof. Given Lemma 2, proof analogous one Theorem 7.Regardless whether input formula true false, tree explored Q-DLLmay contain (exponentially many) subtrees whose nodes false.procedure described above, allows us associate clause resolution deductionsubtrees.input formula true, situation simpler far unituniversal literals, use tree searched Q-DLL generate deductionempty term . Intuitively, process analogous one describedfalse, except leaves term resolution deduction terms correspondingassignments computed Q-DLL entailing matrix . details:384fiClause/Term Resolution Learning Quantified Boolean FormulasFirst, restrict attention minimal true subtree , i.e., treeobtained deleting subtrees starting left split existentialliteral: Analogously case false, leaf minimal truesubtree terminates empty matrix.Second, associate node term, represented set, follows:term associated leaf minimal term min(T ) 11. contain universal literals assigned monotone,2. propositionally entail matrix, i.e., clause C matrix, C 6= ,3. subset literals , i.e., {l : l }.term associated node ; l,1. l 6 term associated node . Notice leither existential universal monotone , l 6 .2. l consider also term 0 associated node; l. l 6 0 0 term associated (as first case).l 0 , term associated resolvent 0 .easy see term associated node -entailed: literalalso .Lemma 3 Let true QBF. Let minimal true subtree tree searchedQ-DLL(, ). Let assignment . true, term associatednode-entailed;contain universal literals assigned monotone.Proof. Analogous proof Lemma 2.Theorem 9 Let true QBF. Let minimal true subtree tree searchedQ-DLL(, ). corresponds model generation term resolution deductionempty term.Proof. Let sequence terms obtained listing terms associatednodes visited post order. Clearly, model generation term resolution deduction. deduction empty term node associated -entailedterm (Lemma 3), i.e., empty term.before, regardless whether input formula true false, tree exploredQ-DLL may contain (exponentially many) subtrees whose nodes associated1. sake efficiency, also important term satisfies properties. However,necessary time being, discussed next section.385fiGiunchiglia, Narizzano & Tacchella{}{{x1 , y, x2 }, {x1 , y, x3 }, {y, x2 }, {y, x2 , x3 }, {x2 , x3 }}{}{x1 } hx1 , ri {x1 }{x1 } hx1 , li {x1 }{{y, x3 }, {y, x2 }, {y, x2 , x3 }, {x2 , x3 }} {{y, x2 }, {y, x2 }, {y, x2 , x3 }, {x2 , x3 }}hy, li {y}hx2 , pi{y}{y, x2 } {} {y}hy, li {y}hx2 , pi{y}{y, x2 } {} {y}Figure 4: term resolutions corresponding tree generated Q-DLL (4).prefix x1 yx2 x3 .{}{{x1 , y, x2 }, {x1 , y, x3 }, {y, x2 }, {y, x2 , x3 }, {x2 , x3 }}{}{x1 } hx1 , li {x1 }{{y, x3 }, {y, x2 }, {y, x2 , x3 }, {x2 , x3 }}{x1 } hx1 , ri {x1 }{{y, x2 }, {y, x2 }, {y, x2 , x3 }, {x2 , x3 }}hy, li {y}hy, li {y}hy, ri {x1 }hy, ri {x1 }hx2 , pi{y} {x1 , y, x3 }hx3 , ui{x1 }hx2 , pi{y} {x1 , y, x2 }hx2 , ui{x1 }{y, x2 } {} {y}{{}} {y, x2 }{y, x2 }hx2 , ui{y, x3 } {y, x2 } {} {y}{{}} {x2 , x3 }Figure 5: resolution corresponding tree generated Q-DLL (4). prefixx1 yx2 x3 .assignments true. described procedure allows us associateterm resolution deduction subtrees. instance, (4)two maximal subtrees, roots x1 ; x1 ; y. associated deductionsrepresented Figure 4. figure,represent also nodes along path root subtrees,term associated node written green right nodeitself,leaf, non-contradictory term entailing matrix whose minimalform min(T ) associated , written green left .Merging trees Figures 3 4 obtain whole tree deductions corresponding search tree explored Q-DLL (represented Figure 5) clauseterm resolutions intermixed.386fiClause/Term Resolution Learning Quantified Boolean Formulasconsider case input QBF false longer assumeliterals assigned unit highest level. restrictattention minimal false subtree tree searched Q-DLL(, ). Then,procedure described associating clause node may longer work.one thing, given leaf , may -falsified clauses matrix inputformula. However, guaranteed existence -contradicted clausematrix input formula. clause C -contradicted if2literal l C, l ;existential literal l C, l .long associate node -contradicted clause (either belongingmatrix obtained clause resolution) corresponds clause resolutiondeduction empty clause: Indeed clause associated rootempty (remember resolvent clause resolution minimal form). Thus,obvious solution try associate1. leaf -contradicted clause input formula,2. internal node -contradicted clause obtained resolving input clausesand/or previously deduced clauses along lines outlined before.cases process runs smoothly. Consider instance, QBF form:x1 x2 yx3 {{x1 , x3 }, {x2 , x3 }, {x2 , y, x3 }, . . .}.(7)Then, assume split x1 occurs first, following path explored (weusing conventions Figure 3):hx1 , lihx3 , uihx2 , ui{{}}(8)clause associated node are:hx1 , li{x1 , x3 } hx3 , ui{x2 , x3 } hx2 , ui{{}}{x1 }{x1 }{y, x3 }{x2 , y, x3 }see that:1. clause associated leaf = x1 ; x3 ; x2 -falsified -contradicted;2. respect definition contradictory clause given Section 3.2, clear clause Ccontradictory -contradicted. Further, QBF assignment , exists-contradicted clause matrix , contains -contradicted clause,contains contradictory clause.387fiGiunchiglia, Narizzano & Tacchella0 function Rec-C-Resolve(, C1 , C2 , l, )1:= {l : l C1 , l C2 };2(S = ) return C-Resolve(C1 , C2 );3l1 := han existential literal C1 level level literals C1 }i;4C := ha clause causes l1 unit 0 , 0 ; l1 prefix i;5C3 := C-Resolve(C1 , C);6return Rec-C-Resolve(, C3 , C2 , l, ).Figure 6: algorithm Rec-C-Resolve.2. able associate node -contradicted clause.Unfortunately, cases things run smoothly, i.e., may possibleassociate clause internal node simple single resolution input and/orpreviously deduced clauses. Indeed, clause resolutions may blockeduniversal variables occurring clauses used resolution.Consider instance QBF form (obtained (7) replacing clause {x2 , x3 }{x2 , y, x3 }):x1 x2 yx3 {{x1 , x3 }, {x2 , y, x3 }, {x2 , y, x3 }, . . .}.(9)Then, (8) would still valid path, corresponding clause resolutions would be:hx1 , li{x1 , x3 } hx3 , ui{x2 , y, x3 } hx2 , ui . . .{{}} {x2 , y, x3 }(10)possible perform clause resolution associated nodelabel hx2 , ui. example, clause resolution (5) may blockedblocking universal literal ll l ,l C1 l C2 .Since C1 C2 minimal form, possible C1 C2 containexistential literal l0level less equal level literals clause;assigned unit.Then, obvious solution get rid, e.g., blocking literals l C1 resolvingaway C1 existential literals level lower level l.idea behind procedure Rec-C-Resolve Figure 6. figure,assume1. input QBF;388fiClause/Term Resolution Learning Quantified Boolean Formulas2. ; l assignment;3. l existential literal either unit highest level ;4. C1 clause containing l, minimal form ; l-contradicted;5. C2 clause containing l, minimal form ; l-contradicted. Further, l unit, C2 clause causes l unit ;6. C-Resolve(C1 , C2 ) returns resolvent clause resolution twoclauses C1 C2 .on, h, C1 , C2 , l, satisfies first 5 conditions, saypair hC1 , C2 ; l-Rec-C-Resolved (in ). Given two clauses hC1 , C2; l-Rec-C-Resolved:1. set universal literals blocking clause resolution C1 C2computed (line 1).2. empty, simply return resolvent C1 C2 (line 2);otherwise3. pick existential literal l1 C1 minimum level C1 (line 3): l1assigned unit earlier search, consider clause C causedl1 assigned unit (line 4). C3 resolvent C1 C (line 5),Rec-C-Resolve(, C3 , C2 , l, ) returned (line 6).hC1 , C2 ; l-Rec-C-Resolved , Rec-C-Resolve(, C1 , C2 , l, ) returns minimal clause -contradicted without existential literals whose negationassigned monotone . formally stated following lemma.Lemma 4 Let C1 C2 two clauses hC1 , C2 ; l-Rec-C-ResolvedQBF . Rec-C-Resolve(, C1 , C2 , l, ) terminates returns clauseminimal form -contradicted;contain existential literals whose negation assigned monotone .proof lemma quite long reported appendix.Assuming input QBF false, construction deduction emptyclause (associated minimal false subtree tree searched Q-DLL)following:every leaf , associate clause C input formula contradicted.C clause associated node ; l,1. l 6 C l universal C clause associated parent; l, i.e., node . Notice l existential monotonel 6 C.389fiGiunchiglia, Narizzano & Tacchella2. l C l unit clause associated noderesult Rec-C-Resolve(, C, C 0 , l, ), C 0 clause causes lunit .3. l C, l existential unit , consider alsoclause C 0 associated node ; l. l 6 C 0 C 0 clause associated(as first case). l C 0 , clause associated noderesult Rec-C-Resolve(, C, C 0 , l, ).example, (9) reference deduction (10), blocked resolution one associated node x1 ; x3 ; x2 . Rec-C-Resolve(, {x2 , y, x3 }, {x2 , y, x3 }, x2 , x1 ; x3 )1. line 5, resolves {x2 , y, x3 } {x1 ; x3 }, resolvent C3 min({x1 , x2 , y}) ={x1 , x2 };2. following recursive call Rec-C-Resolve(, {x1 , x2 }, {x2 , y, x3 }, x2 , x1 ; x3 ) line 6returns {x1 , y, x3 }.Thus, clause associated node are:hx1 , li{x1 , x3 } hx3 , ui{x2 , y, x3 } hx2 , ui{{}}{x1 }{x1 }{x1 , y, x3 }{x2 , y, x3 }Notice that, reference Figure 6, choice eliminating blocking literalsC1 maintaining C2 invariant, arbitrary. Indeed, could eliminate blockingliterals C2 maintain C1 invariant. case deduction (10), amountseliminate universal literal {x2 , y, x3 }: resolving clause {x1 , x3 }x3 , get resolvent {x1 , x2 }, leads following legal deduction:hx1 , li{x1 , x3 } hx3 , ui(From {x2 , y, x3 }, {x1 , x3 }) {x1 , x2 } hx2 , ui{{}}{x1 }{x1 }{x1 , y, x3 }{x2 , y, x3 }Lemma 5 Let false QBF. Let minimal false subtree tree searchedQ-DLL(, ). Let assignment . false, clause associatednodeminimal form -contradicted;contain existential literals whose negation assigned monotone.Proof. construction, clause associated leaf -contradicted.show also clause C associated internal node -contradicted,assuming clause C 0 associated child ; l ; l-contradicted. alsochild ; l, also assume clause C 00 associated child ; l ; l-contradicted.390fiClause/Term Resolution Learning Quantified Boolean Formulas1. l 6 C 0 l universal C = C 0 . Hence, C minimal form. Since l 6 C 0l universal, C 0 ; l-contradicted C 0 -contradicted. thesisfollows C = C 0 .2. l C 0 l unit C =Rec-C-Resolve(, C 0 , C 00 , l, ), C 00clause causes l unit . thesis follows Lemma 4.3. l C, l existential unit , consider also clauseC 0 associated node ; l. Assuming l C 0 (otherwise would firstcase), clause associated result Rec-C-Resolve(, C, C 0 , l, ).previous case, thesis follows Lemma 4.Theorem 10 Let false QBF. Let minimal false subtree tree searchedQ-DLL(, ). corresponds clause resolution deduction empty clause.Proof. Given Lemma 5, proof analogous one Theorem 7.4. Backjumping Learning DLL Based Procedures QBFssection first show computing resolvent associated node allowsbackjump branches backtracking (Subsection 4.1). Then, showlearning resolvents allows prune search tree branches different onesresolvents computed learned (Subsection 4.2).4.1 Conflict Solution Directed Backjumpingprocedure described Section 3.2 uses standard backtracking schema wheneverempty clause (resp. matrix) generated: Q-DLL backtrack first existential(resp. universal) literal assigned left split. instance, given QBFy1 x1 y2 x2 x3 {{y1 , y2 , x2 }, {y1 , 2 , x2 , x3 }, {y1 , x2 , x3 },{y 1 , x1 , x3 }, {y 1 , y2 , x2 }, {y 1 , y2 , x2 }, {y 1 , x1 , 2 , x3 }},(11)tree searched Q-DLL represented Figure 7, use conventionsSection 3.2001 work Giunchiglia, Narizzano, Tacchella (2001), shownexploration branches necessary. particular, input QBFassignment, show possible compute reason (un)satisfiabilitybacktracking. Intuitively speaking, reason result subsetliterals assignment 0assigns true false literals assigned (i.e., {|l| :l 0 } = {|l| : l });extends (i.e., {l : l 0 }),391fiGiunchiglia, Narizzano & Tacchella{}{{y1 , y2 , x2 }, {y1 , 2 , x2 , x3 }, {y1 , x2 , x3 },{y 1 , x1 , x3 }, {y 1 , y2 , x2 }, {y 1 , y2 , x2 }, {y 1 , x1 , 2 , x3 }}{y 1 }hy 1 , li{y 1 }{{y2 , x2 }, {y 2 , x2 , x3 }, {x2 , x3 }}hy 2 , li{y 1 }{y 1 , 2 , x2 }hx2 , ui{y 1 }{y 1 , x2 , x3 }hx3 , ui{y 1 }{y 1 , x2 , x3 } {} {y 1 }hy1 , ri{}{{x1 , x3 }, {y2 , x2 }, {y2 , x2 }, {x1 , 2 , x3 }}hy2 , ri{{x2 , x3 }, {x2 , x3 }}hx2 , lihx3 , ui{}hx1 , li{}{y 1 , x1 , x3 }hx3 , ui{}hy 2 , pi{}{y 1 , y2 , x2 }hx2 , ui{}{{}} {y 1 , y2 , x2 }hx1 , rihx3 , pihy 2 , pihx2 , ui{{}}Figure 7: resolution corresponding tree generated Q-DLL (11). prefixy1 x1 y2 x2 x3 .0 equivalent . Then, computing reasons, avoid right splitliteral l l reason: assigning l false would change result.resulting procedure generalization QBF popular Conflict-directed Backjumping(CBJ) (Prosser, 1993b), also introduces concept Solution-directed Backjumping(SBJ), avoiding useless splits universal variables.later paper, Giunchiglia, Narizzano, Tacchella (2003) show possibleoptimize computation reasons. particular, paper, shownassuming unsatisfiable, consider reasons subset existentialliterals ,assuming satisfiable, consider reasons subset universalliterals .Apart optimizations, tree searched procedures described formerlatter papers same, and, case (11), exploration branchesstarting hy2 , ri, hx1 , ri skipped (see Figure 7).show computation resolutions corresponding Q-DLL allowsavoid exploration branches pretty much CBJ SBJ do: caseQBF (11), branches skipped skipped CBJ SBJ.key point think Q-DLL procedure producing clause (resp. term)deduction empty clause (resp. term), proving unsatisfiable (resp. satisfiable).Then, according rules use associating deduction tree searched QDLL, that:C clause associated node ; l l 6 C, clause associatednode C, even l existential assigned left split.392fiClause/Term Resolution Learning Quantified Boolean Formulas0 function Q-DLL-BJ(, )1(ha clause C -contradictedi)2return C;3(hthe matrix emptyi) return ModelGenerate();4(hl unit i)5C := ha clause matrix causes l unit i;6W R := Q-DLL-BJ(, ; l);7(hW R termi l 6 W R) return W R;8return Rec-C-Resolve(, W R, C, l, );9(hl monotone i) return Q-DLL-BJ(, ; l);10l := ha literal highest level i;11W R := Q-DLL-BJ(, ; l);12(hl existentiali (hW R termi l 6 W R)) return W R;13(hl universali (hW R clausei l 6 W R)) return W R;14W R0 := Q-DLL-BJ(, ; l);15(hl existentiali (hW R0 termi l 6 W R0 )) return W R0 ;16(hl universali (hW R0 clausei l 6 W R0 )) return W R0 ;17(hl existentiali) return Rec-C-Resolve(, W R0 , W R, l, );18return T-Resolve(W R0 , W R, l, ).Figure 8: algorithm Q-DLL-BJ.Analogously, term associated node ; l l 6 , termassociated node , even l universal assigned leftsplit.rules take account clause/term associated node ; l,thus need explore branch starting ; l.Consider example Figure 7, use standard conventions and, e.g., writeclause (resp. term) associated node red (resp. green) rightnode. reference figure, clear considering term {y 1 } associatednode 1 ; 2 , need explore branch starting hy2 , ri orderassociate 1 -entailed term node 1 . Similarly, considering empty clause {}associated node y1 ; x1 , need explore branch startinghx1 , ri order associate y1 -contradicted clause node y1 .procedure Q-DLL-BJ(, ) Figure 8 incorporates ideas. figure,ModelGenerate() returns minimal form non-contradictory -entailedtermclause C , C 6= ;universal literal l assigned monotone, l 6 .Rec-C-Resolve(, C1 , C2 , l, ) Figure 6.393fiGiunchiglia, Narizzano & TacchellaT-Resolve(T1 , T2 ) returns resolvent term resolution two termsT1 T2 .behavior Q-DLL-BJ illustrated words saying Q-DLL-BJ(, )computes returns clause/term would associated node treeexplored Q-DLL. particular, assumingW R clause (resp. term) returned Q-DLL-BJ(, ; l);l existential (resp. universal);l assigned left split,Q-DLL-BJ(, ) explore branch starting ; l l 6 W R (resp. l 6 W R),see line 12 (resp. line 13) Q-DLL-BJ.far, reference Figure 7, interpret clause (resp. term) red(resp. green) right node value returned Q-DLL-BJ(, ). Then,considering term {y 1 } associated node 1 ; 2 , Q-DLL-BJ explorebranch starting hy2 , ri. Similarly, considering empty clause {} associatednode y1 ; x1 , Q-DLL-BJ explore branch starting hx1 , ri.Theorem 11 Q-DLL-BJ(, ) returns empty clause false, empty termtrue.Proof.(Sketch) enough notice that:node associated clause C, C -contradicted, C resultsequence clause resolutions.node associated term , -entailed, resultsequence model generations term resolutions.Then, previous section:empty clause associated initial node , false.empty term associated initial node , true.4.2 LearningLearning well known technique SAT avoiding useless traversal branches.SAT, learning amounts storing (clause) resolvents associated nodes treeexplored DLL: resolvents called nogoods simply added setinput clauses.case QBFs, situation different complicated. Indeed,two types resolutions (term clause), resolvents clause resolutionsadded conjunctively matrix, resolvents term resolutions (thatcall goods) considered disjunction matrix.practice, handle three sets formulas:394fiClause/Term Resolution Learning Quantified Boolean Formulasset terms corresponding goods learned search;set clauses corresponding matrix input QBF;set clauses corresponding nogoods learned search.Formally, QBF form (1), QBF Extended Learning (EQBF)expression formQ1 z1 . . . Qn zn h, ,(n 0)(12)set terms, also called goods, interpreted disjunctively. goodobtained model generation and/or term resolution ;set clauses, also called nogoods, interpreted conjunctively. nogoodobtained clause resolution .Clearly,Q1 z1 . . . Qn zn ( )Q1 z1 . . . Qn zn ( )equivalent (1).Initially empty set, input set clauses. searchproceeds,Nogoods determined backtracking contradiction (i.e., assignment unsatisfiable) possibly added ;Goods determined backtracking solution (i.e., assignmentsatisfiable) possibly added .following, use term constraints want refer goodsnogoods indifferently.Consider EQBF (12). constraints and/or , searchpruned considerably. Indeed, descending search tree, literal assignedlong guaranteed reconstruct valid clause/term deductionbacktracking empty clause/term. availability already derived clauses/termsallows prune search constraints : Given assignment ,exists -contradicted clause C (resp. -satisfied term ) stopsearch return C (resp. ). term -satisfiedliteral l , l ;universal literal l , l .Clearly, -entailed term also -satisfied. Further, extend notion unittake account constraints and/or . literal lunit EQBF (12)395fiGiunchiglia, Narizzano & Tacchella0 function Rec-Resolve(, W1 , W2 , l, )1:= {l : l W1 , l W2 };2(S = ) return Resolve(W1 , W2 );3l0 := ha literal W1 level level literals W1 }i;4W := ha constraint causes l0 unit 0 , 0 ; l0 prefix i;5W3 := Resolve(W1 , W );6return Rec-Resolve(, W3 , W2 , l, ).Figure 9: algorithm Rec-Resolve.either l existential 0,clause {l, l1 , . . . , lm } belongs ,expression |li | (1 m) occurs right |l| prefix(12).l universal 0,term {l, l1 , . . . , lm } belongs ,expression |li | (1 m) occurs right |l| prefix(12).definition monotone literals, crucial property ensureddealing EQBFs, existential (resp. universal) literal l assigned monotone; l never enter nogood (resp. good) associated node extending ; l.guaranteed defining literal l monotone pure if3either l existential l belong constraint ;l universal l belong constraint .possibility assigning also universal literals unit, may caseterm resolutions may blocked existential literals l l,occurring one terms used antecedents term resolution. However,procedure Rec-C-Resolve presented Subsection 3.3 easily generalizedwork also case constraints resolved terms. resultprocedure Rec-Resolve(, W1 , W2 , l, ) Figure 9, assumed1. EQBF;3. various ways guarantee existential literal l assigned monotone ; lenter nogood associated node extending ; l. Another onekeep definition existential monotone literal unchanged: existential literal assignedmonotone (12) l belong clause ;update (or proceed search updated to) \ {C : C , l C}.Analogously universal monotone literals. See work Giunchiglia, Narizzano Tacchella (2004a) details possibilities, including discussion interactionmonotone rule learning.396fiClause/Term Resolution Learning Quantified Boolean Formulas2. ; l assignment;3. l existential (resp. universal) literal either unit highest level;4. W1 clause (resp. term) containing l (resp. l), minimal form ; l-contradicted(resp. ; l-satisfied);5. W2 clause (resp. term) containing l (resp. l), minimal form ; l-contradicted(resp. ; l-satisfied). Further, l unit , W2 clause (resp. term)causes l unit ;6. existential (resp. universal) literal l0 assigned unit 0 ; l0 , 0 ; l0prefix ; l, clause (resp. term) causes l0 unit0 .7. Resolve(W1 , W2 ) returns C-Resolve(W1 , W2 ) (resp. T-Resolve(W1 , W2 )).h, W1 , W2 , l, satisfy first 6 7 conditions, say pair hW1 , W2; l-Rec-Resolved (in ).above, (12), l defined EQBF obtainedremoving (resp. ) clauses C (resp. terms ) l C (resp.l ), removing l (resp. l) clauses (resp. terms);removing Q|l| prefix.= l1 ; l2 ; . . . ; lm (m 0), defined (. . . ((l1 )l2 ) . . .)lm .hW1 , W2 ; l-Rec-Resolved , Rec-Resolve(, W1 , W2 , l, ) returnsconstraint minimal form -contradicted -satisfied, stated followinglemma.Lemma 6 Let W1 W2 two clauses (resp. terms) hW1 , W2 ; lRec-Resolved EQBF . Rec-Resolve(, W1 , W2 , l, ) terminates returns minimalclause (resp. term)-contradicted (resp. -satisfied);contain existential literals whose negation (resp. universal literalsbeen) assigned monotone .Proof.(Sketch) proof equal (resp. analogous to) proof Lemma 4 lexistential (resp. universal).procedure Q-DLL-LN (, ) incorporates new definitions ideas,represented Figure 10. Considering figure,definition ModelGenerate() relaxed respect definition provided Subsection 4.1 order return minimal form non-contradictory-satisfied term397fiGiunchiglia, Narizzano & Tacchella0 := {};1 := {};2 function Q-DLL-LN (, )3Q := hthe prefix i;4:= hthe matrix i;5(ha -contradicted clause C i)6return C;7(ha -satisfied term i)8return ;9(hthe matrix emptyi) return ModelGenerate();10(hl unit (Qh, , i) i)11W := ha constraint causes l unit (Qh, , i) i;12W R := Q-DLL-LN (, ; l);13(hl existentiali (hW R termi l 6 W R)) return W R;14(hl universali (hW R clausei l 6 W R)) return W R;15W R := Rec-Resolve(Qh, , i, W R, W, l, );16Learn(, W R);17return W R;18(hl monotone (Qh, , i) i) return Q-DLL-LN (, ; l);19l := ha literal highest level i;20W R := Q-DLL-LN (, ; l);21(hl existentiali (hW R termi l 6 W R)) return W R;22(hl universali (hW R clausei l 6 W R)) return W R;23W R0 := Q-DLL-LN (, ; l);24(hl existentiali (hW R0 termi l 6 W R0 )) return W R0 ;25(hl universali (hW R0 clausei l 6 W R0 )) return W R0 ;26W R := Rec-Resolve(Qh, , i, W R0 , W R, l, );27Learn(, W R);28return W R.Figure 10: algorithm Q-DLL-LN.clause C , C 6= ;universal literal l assigned monotone, l 6 .Learn(, W R) updates set goods nogoods according given policy.simply assume Learn(, W R) updates 0 0 respectively,0 0 satisfy following conditions:0 subset {W R} W R term, otherwise;0 subset {W R} W R clause, otherwise;existential (resp. universal) literal l assigned unit initial prefix0 ; l , 0 (resp. 0 ) still contains clause (resp. term) causes lassigned unit (Qh0 , , 0 i)0 .398fiClause/Term Resolution Learning Quantified Boolean Formulasreference Figure 9, last condition necessary order guaranteeexistence constraint W satisfying condition line 4.conditions Learn(, W R) general ensure soundnesscompleteness Q-DLL-LN.Theorem 12 Q-DLL-LN(, ) returns empty clause false, empty termtrue.Proof. Analogous proof Theorem 11.understand benefits learning, assume input QBF (4). correspondingEQBFx1 yx2 x3 h{}, {{x1 , y, x2 }, {y, x2 }, {x2 , x3 }, {x1 , y, x3 }, {y, x2 , x3 }}, {}i,search proceeds Figure 2, first path leading empty matrix,starts term resolution process. Assuming term min({y, x2 }) = {y} addedset goods checking value x1 ;y , soon x1 assigned true,detected unit correspondingly assigned;path corresponding assignment x1 ; explored.example shows, (good) learning avoid useless exploration brancheswould explored backtracking backjumping schema. Indeed,assuming deduced term learned backtracking. policy accordingLearn(, W R) simply adds W RW R clause;otherwise,easily implemented. However, simple policy may easily lead store exponential number goods and/or nogoods (notice call Learn(, W R)literal assigned unit right split). Thus, practical implementations incorporatepolicies guaranteed space bounded, i.e., ones store polynomial number goodsnogoods most. SAT, three popular space bounded learning schemes are:Size learning order n (Dechter, 1990): nogood addedcardinality less equal n. added, never deleted.Relevance learning order n (Ginsberg, 1993): given current assignment ,nogood C always added , deleted soon numberliterals l C l 6 bigger n.Unique Implication Point (UIP) based learning (Marques-Silva & Sakallah, 1996):nogood C stored C contains one literal maximum decisionlevel. Given assignment , decision level literal l numbersplits done l . UIP based learning, set added clausesperiodically inspected clauses deleted according various criteria.399fiGiunchiglia, Narizzano & TacchellaThus, size learning, nogood stored, never deleted. relevance UIPbased learning, nogoods dynamically added deleted depending current assignment. See work Bayardo (1996) details related size relevancelearning (including complexity analysis), work Zhang, Madigan, MoskewiczMalik (2001) discussion various UIP based learning mechanisms SAT. Size,relevance, UIP based learning various possibilities limitingnumber stored clauses, one generalized various ways consideringQBFs instead SAT formulas. next section, present particular learningschema implemented QuBE.5. Implementation Experimental Analysissection first describe details implementation nogood goodlearning QuBE, report experimental analysis conducted orderevaluate (separate) benefits nogood good learning, also relative efficiencysolver compared state-of-the-art QBF solvers.5.1 Implementation QuBEevaluate benefits deriving learning, implemented good nogood learning QuBE. QuBE QBF solver based search which, non-randominstances, compares well respect state-of-the-art solvers based search, likesemprop (Letz, 2002), yquaffle (Zhang & Malik, 2002a), i.e., best solvers basedsearch non-random instances according (Le Berre, Simon, & Tacchella, 2003),see (Giunchiglia, Narizzano, & Tacchella, 2004c) details.Besides learning, version QuBE used featuresefficient detection unit monotone literals using lazy data structures (Gent,Giunchiglia, Narizzano, Rowley, & Tacchella, 2004);branching strategy exploits information gleaned input formula initially,leverages information extracted learning phase.See (Giunchiglia, Narizzano, & Tacchella, 2004b) description characteristics.learning, computation nogoods goods corresponding internalnodes search tree carried clause term resolutionworking reason initialized backtracking starts, reasons storeddescending search treeunit literal, stored reason constraint literal unit;literal assigned right split, stored reason constraint computedbacktracking left branch;monotone literals, way working reasons initialized ensures existential(resp. universal) monotone literals never belong working reason computedbacktracking contradiction (resp. solution).400fiClause/Term Resolution Learning Quantified Boolean FormulasAssume = l1 ; l2 ; . . . ; lm assignment corresponding leaf consideration. Considering problem initializing working reason, wayQuBEreturn -contradicted clause matrix input QBF set learnednogoods, contradiction;compute minimal form -satisfied prime implicant matrix contains universal literals possible, solution.second case, computation prime implicant important order shortreasons, possible universal literals important order backjumpnodes. requirements met recursively removing irrelevant literalsset literals , starting universals ones. Given set literals, sayliteral irrelevant clause C matrix l C exists anotherliteral l0 l0 VC. prime() set literals result recursiveprocedure, term prime()satisfied ;prime implicant matrix input QBF;exist another term satisfying first two propertiessmaller (under set inclusion) set universal literals.order reduce number universal literals initial goods, takeadvantage fact assignment may partial: literal l maycase neither l l . Then, use existential literals ,level lower level universal literals assigned left split,order reduce number universals prime(). fact, sequence 0literals extending existential literals, set universals prime(0 ) subsetprime(). instance, considering QBF (11) = 1 ; y2 ; x2 ; x3 ,prime() {y 1 , y2 , x2 , x3 };extend 0 = ; x1 prime(0 ) {x1 , y2 , x2 , x3 }.Finally, evaluating universal literals irrelevant, follow reverseorder assigned, order try backjump high possiblesearch tree.said previous section, besides problem setting initial workingreason, another problem learning unconstrained storage clauses (resp. terms)obtained reasons conflicts (resp. solutions) may lead exponential memoryblow up. practice, necessary introduce criteria1. limiting constraints learned; and/or2. unlearning them.401fiGiunchiglia, Narizzano & Tacchellaimplementation learning QuBE works follows. Assume backtrackingliteral l assigned decision level n. constraint corresponding reasoncurrent conflict (resp. solution) learned following conditions satisfied:1. l existential (resp. universal);2. assigned literals reason except l, decision level strictly lowern;3. open universal (resp. existential) literals reason lprefix.Notice three conditions ensure l unit constraint correspondingreason. QuBE learned constraint, backjumps nodemaximum decision level among literals reason, excluding l. say lUnique Implication Point (UIP) therefore lookback QuBE UIP based.Notice definition UIP generalizes QBF concepts first described SilvaSakallah (1996) used SAT solver grasp. SAT instance, QuBE lookbackscheme behaves similarly 1-UIP-learning scheme used zCHAFF (and describedZhang et al., 2001). Even QuBE guaranteed learn one clause (resp. term)per conflict (resp. solution), still number learned constraints may blow up,number backtracks exponential. stop course, QuBE scans periodicallyset learned constraints search became irrelevant, i.e., clauses (resp.terms) number open literals exceeds parameter n, correspondingrelevance order. Thus, implementation uses UIP based learning decide storeconstraint, relevance based criteria decide forget constraint.experimental analysis presented next subsection, parameter n set 20set learned constraints scanned every 5000 nodes.Besides learning mechanism, current version QuBE features lazy datastructures unit literal detection propagation (as described Gent et al., 2004),monotone literal fixing (as described Giunchiglia et al., 2004a), Variable State Independent Decaying Sum heuristic (VSIDS) (as introduced SAT Moskewicz, Madigan,Zhao, Zhang, & Malik, 2001). SAT, basic ideas heuristic (i) initiallyrank literals basis occurrences matrix, (ii) increment weightliterals learned constraints, (iii) periodically divide constant weightliteral.5.2 Experimental Resultsevaluate effectiveness implementation, considered 450 formal verification planning benchmarks constituted part 2003 QBF solvers comparativeevaluation4 : 25% instances comes verification problems (described Scholl& Becker, 2001; Abdelwaheb & Basin, 2000), remaining planning domains (described Rintanen, 1999; Castellini, Giunchiglia, & Tacchella, 2001). startanalysis considering QuBE without learning enabled. versions QuBE4. respect non-random instances used 2003 QBF comparative evaluation, test setinclude QBF encodings modal K formulas submitted Pan Vardi (2003).402fiClause/Term Resolution Learning Quantified Boolean FormulasFigure 11: Effectiveness learning: QuBE versus QuBE(cbj,sbj). CPU time (left)number backtracks instances solved solvers (right).compute goods nogoods order backjump irrelevant existential universalbranching nodes. differ treatment computed goods nogoods:learning enabled, QuBE records goods nogoods;learning disabled, QuBE records neither nogoods goods.call two versions QuBE(cln,sln) QuBE(cbj,sbj) respectively, orderspecify type look-back used two systems. Notice considerQuBE backtracking (i.e., version computes neither nogoods goodsperforms simple chronological backtracking) competitivesolvers.experiments run farm identical PCs, one equippedPentium 4, 3.2GHz processor, 1GB RAM, running Linux Debian (sarge). Finally,system timeout value 900s per instance.Figure 11 left shows performances QuBE(cln,sln) versus QuBE(cbj,sbj).plot, x-axis CPU-time QuBE(cln,sln) y-axis CPU-timeQuBE(cbj,sbj). plotted point hx, yi represents benchmark QuBE(cln,sln)QuBE(cbj,sbj) take x seconds respectively.5 convenience, also plotpoints hx, xi, representing benchmarks solved QuBE(cln,sln) x seconds.first observation learning pays off:5. principle, one point hx, yi could correspond many benchmarks solved QuBE(cln,sln)QuBE(cbj,sbj) x seconds respectively. However, scatter diagramspresent, point (except point h900, 900i, representing instances solverstime-out) corresponds single instance cases.403fiGiunchiglia, Narizzano & TacchellaFigure 12: Effectiveness learning random heuristic: QuBE(rnd,cln,sln)[3] versus QuBE(rnd,cbj,sbj)[3]. CPU time (left) number backtracksinstances solved solvers (right).QuBE(cln,sln) (resp. QuBE(cbj,sbj)) able solve 16 (resp. 1) instancessolved QuBE(cbj,sbj) (resp. QuBE(cln,sln));among instances solved solvers, QuBE(cln,sln) (resp. QuBE(cbj,sbj))least one order magnitude faster QuBE(cbj,sbj) (resp. QuBE(cln,sln))39 (resp. 0) instances.order implementation-quality independent measure pruning introducedlearning, right plot figure shows number backtracks (i.e., numbersolutions conflicts found) QuBE(cbj,sbj) versus QuBE(cln,sln) 358problems solved systems. plotted point hx, yi represents benchmarksolved QuBE(cln,sln) QuBE(cbj,sbj) performing x backtracksrespectively. seen, learning substantially prunes search space:point diagonal, meaning never case QuBE(cbj,sbj) performsless backtracks QuBE(cln,sln).6 Still, learning overhead, thuspruning caused learning always pays terms speed, provedpoints diagonal left plot.experimental data entirely satisfactory two reasons.First, learning heuristic tightly coupled QuBE: Whenever QuBE learnsconstraint, also increments score literals it. QuBE(cbj,sbj) constraint6. imply tree searched QuBE(cln,sln) subtree tree searchedQuBE(cbj,sbj): Indeed, literal selected branching node two systems guaranteedsame.404fiClause/Term Resolution Learning Quantified Boolean FormulasFigure 13: Effectiveness conflict learning:QuBE(rnd,cln,sln)[3] versusQuBE(rnd,cbj,sln)[3]. CPU time (left) number conflict backtracksinstances solved solvers (right).ever learned. consequence, QuBE(cbj,sbj), (i) literals initially sortedbasis occurrences input QBF, (ii) score literal periodicallyhalved becomes 0. literals score 0, literals prefixlevel chosen according lexicographic order.Second, independently heuristic used, plot showing performancesQuBE without learning, say two learning schemes (conflict,solution) effective (Gent & Rowley, 2004).address first problem, consider QuBE random heuristic, i.e., heuristicrandomly selects literal among maximum level yet assigned.call resulting systems QuBE(rnd,cln,sln) QuBE(rnd,cbj,sbj) respectively:names suggest, first learning enabled, second learningdisabled. randomness, run solver 5 times instance. Then,define QuBE(rnd,cln,sln)[i] system whose performances are,instance, i-th best among 5 results obtained running QuBE(rnd,cln,sln)instance. QuBE(rnd,cbj,sbj)[i] defined analogously.Figure 12 shows CPU time (left) number backtracks solved instances(right) QuBE(rnd,cln,sln)[3] QuBE(rnd,cbj,sbj)[3]. plots, easysee QuBE(rnd,cln,sln)[3] faster QuBE(rnd,cbj,sbj)[3] cases.witness factQuBE(rnd,cln,sln) (resp. QuBE(rnd,cbj,sbj)) able solve 21 (resp. 2) instances solved QuBE(cbj,sbj) (resp. QuBE);405fiGiunchiglia, Narizzano & TacchellaFigure 14: Effectiveness solution learning:QuBE(rnd,cln,sln)[3] versusQuBE(rnd,cln,sbj)[3]. CPU time (left) number solution backtracks instances solved solvers (right).among instances solved solvers, QuBE (resp. QuBE(cbj,sbj)) leastone order magnitude faster QuBE(cbj,sbj) (resp. QuBE) 68 (resp. 2)instances.Still, longer case enabling learning always causes reduction numberbacktracks. different literals selected branching node,also pruning node may prevent long backjump (Prosser, 1993a)would cause vast reduction search space. Interestingly, comparing resultsFigure 11, seems random heuristic learning becomes important.fact witnesses also setting well known tension look-ahead look-backtechniques: smart look-ahead makes look-back less important, viceversa.address second problem, considered systems QuBE(rnd,cbj,sln)QuBE(rnd,cln,sbj), i.e., systems obtained QuBE(rnd,cln,sln) disablingconflict learning solution learning respectively. usual, system run 5 timesinstance, QuBE(rnd,cbj,sln)[i] QuBE(rnd,cln,sbj)[i] (1 5)defined before. left plots Figures 13 14 show performancesQuBE(rnd,cln,sln)[3] versus QuBE(rnd,cbj,sln)[3] QuBE(rnd,cln,sbj)[3] respectively. also measured number backtracks. However, order betterhighlight pruning due conflict (resp. solution) learning, right plot Figure 13(resp. 14) shows number conflict (resp. solution) backtracks QuBE(rnd,cbj,sln)[3](resp. QuBE(rnd,cln,sbj)[3]). plots, see conflict solutionlearning prune search space pay off: plot, pointswell diagonal. Comparing two left plots, also see that, test set406fiClause/Term Resolution Learning Quantified Boolean FormulasQuBE(rnd,cln,sln)[3]QuBE(rnd,cln,sln)[1]QuBE(rnd,cln,sln)[2]QuBE(rnd,cln,sln)[4]QuBE(rnd,cln,sln)[5]QuBE(rnd,cbj,sbj)[1]QuBE(rnd,cbj,sbj)[2]QuBE(rnd,cbj,sbj)[3]QuBE(rnd,cbj,sbj)[4]QuBE(rnd,cbj,sbj)[5]QuBE(rnd,cbj,sln)[1]QuBE(rnd,cbj,sln)[2]QuBE(rnd,cbj,sln)[3]QuBE(rnd,cbj,sln)[4]QuBE(rnd,cbj,sln)[5]QuBE(rnd,cln,sbj)[1]QuBE(rnd,cln,sbj)[2]QuBE(rnd,cln,sbj)[3]QuBE(rnd,cln,sbj)[4]QuBE(rnd,cln,sbj)[5]=1361691561091311371231108413013312911586135151169141103<002032441451641922052229613416920924578110134183218>22519200724325171012882482061429039112002813172129457121517246101926383100722225531144100./8688898982878787878484868888858588898910<002761274368839920274054877152951690.1>43190020721126145103615500868891979510410811613291961011051129195107115127Table 2: Comparison among various versions QuBE. row compares system written first column respect QuBE(rnd,cln,sln)[3] taken reference.QuBE(rnd,cln,sln)[3] B solver first column,columns report number problems that: =, B solvetime; <, B solve takes less time B; >, B solvetakes time B; , solves B not; , solveB does; ./, B solve; 10<, B solveleast one order magnitude faster; 0.1<, B solveleast one order magnitude slower; TO, B solve.number timeouts QuBE(rnd,cln,sln)[3] 89.considered, solution learning helps solving problems conflict learning:QuBE(rnd,cbj,sln)[3] times 101 QuBE(rnd,cln,sbj)[3] times 107.hand, two right plots suggest conflict learning prunes solution learning, conclusion correct. Indeed, plot shows either numberconflicts number solutions: Pruning node (no matter whether existentialuniversal) may avoid finding (exponentially many) solutions and/or conflicts. particular,given instances CNF thus form. . . yx1 x2 . . . xn(n 1) pruning variable {x1 , x2 , . . . , xn } potential prune 2n conflicts.407fiGiunchiglia, Narizzano & Tacchelladetailed quantitative information CPU times reported Table 2. last column table see that, indicate TO(S)number timeouts system S, then, {1, 2, 3, 4, 5},TO(QuBE(rnd,cln,sln)[i]) <TO(QuBE(rnd,cbj,sln)[i])< TO(QuBE(rnd,cbj,sbj)[i]).TO(QuBE(rnd,cln,sbj)[i])gives indication capacity solvers, i.e., ability solveproblems. order get indication productivity, i.e., considering problemssolve, ability solve quickly, consider number FS(S)difference 0.1 > 10 < columns: lower FS(S) is, betteris.FS(QuBE(rnd,cln,sln)[i]) <FS(QuBE(rnd,cbj,sln)[i])< FS(QuBE(rnd,cbj,sbj)[i])FS(QuBE(rnd,cln,sbj)[i]){1, 2, 3, 4, 5}. above, clear conflict solution learningallow improve capacity productivity. experimental results thus seemcontradict negative results reported Gents Rowleys work (2004) solutionbased look-back mechanisms. However, results comparable ours, givendifferent mechanisms implemented respective solvers (e.g., computinginitial solution monotone literal fixing), different experimental setting (e.g.,testset).6. Conclusions Related Workpaper based extends (Giunchiglia et al., 2002) introduces nogoodgood learning QBFs satisfiability. show correspondence computation trees searched DLL based QBF solvers clause/term resolution deductions.Nogoods goods clauses terms respectively resolution deductions.perspective, learning simply amounts storing nogoods goods. showincorporate nogoods goods learning DLL based QBF solvers consideringEQBFs (QBFs extended learning), illustrate means examplescomputation nogoods goods:allows solution conflict directed backjumping spirit (Giunchiglia et al.,2001, 2003);stored, allows pruning branches parts search tree.present high level description algorithms incorporating ideas, formallyprove soundness completeness. also discuss problems related effectiveimplementations DLL based QBF solvers, present (in details) implementation QuBE, state-of-the-art QBF solver. experimental analysis shows QuBEenhanced nogood good learning effective, considering selectionnonrandom problems consisting planning formal verification benchmarks. alsoshow QuBE competitive respect state art.408fiClause/Term Resolution Learning Quantified Boolean Formulasalready said, work builds (Giunchiglia et al., 2002). papers dealinglearning QBFs satisfiability (Letz, 2002), (Zhang & Malik, 2002a) (Gent &Rowley, 2004). particular, (Letz, 2002) conflict solution learning called lemmamodel caching. paper also proposes technique based model caching dealing QBFs variable-independent subformulas. Zhang Malik (2002a) proposeconflict learning (which extended solution learning Zhang & Malik, 2002b).second paper, terms called cubes. Gent Rowley (2004) introduce new formsolution learning: new technique revisits less solutions standard techniques,experimental results reported paper positive. works shareintuitions thus propose similar techniques. Though difficult establishprecise relation among works due differences terminology and/ordifferent level detail presentations,7 believe main differencesimplementation level, i.e., way solution conflict learning implemented.therefore quite difficult impossible compare different alternatives, withoutre-implementing recasting different learning mechanisms even different solverscommon framework. Indeed, specific learning mechanism implemented withinsolver may motivated characteristics solver, e.g., data structures used heuristic. instance, watched data structures (used, e.g.,QuBE, yquaffle semprop) allow efficient detection propagation unit pure literals (Gent et al., 2004). consequence, solvers watcheddata structures may profitably maintain huge databases goods nogoods. solversstandard data structures, costs involved managing huge databases mayoverwhelm advantages. Considering solvers whole, experimentalanalysis conducted (Giunchiglia et al., 2004c) shows solver QuBE compareswell respect semprop yquaffle 450 formal verification planningbenchmarks considered also paper.Acknowledgmentswould like thank Ian Gent Andrew Rowley discussions related subjectpaper, anonymous reviewers suggestions corrections. workpartially supported MIUR.Appendix A. Proof Lemma 4proof well founded induction. Thus, steps follow are:1. definition well founded order tuples hC1 , C2 , l, i;2. proof thesis holds minimal elements partial order;3. assuming thesis holds tuples hC3 , C2 , l, hC3 , C2 , l,hC1 , C2 , l, i, proof thesis holds also hC1 , C2 , l, i.7. instance, (Letz, 2002; Zhang & Malik, 2002b) also initial work (Giunchiglia et al.,2002), method used computing initial working reason corresponding solution (procedureModelGenerate Figure 10) detailed.409fiGiunchiglia, Narizzano & Tacchelladeliberately omitted properties elements tupleshC1 , C2 , l, partial order satisfy. Indeed, standard assumption wouldC1 C2 two clauses hC1 , C2 ; l-Rec-C-Resolved. However,sufficient. Indeed, may happen starting two clauses hC1 , C2; l-Rec-C-Resolved (line numbers refer Figure 6)1. set {l : l C1 , l C2 , l universal} empty (see line 1);2. clause C3 computed line 5 Figure 6 ; l-contradicted; thus3. tuple hC3 , C2 , l, element partial order.better understand problem, consider following simple example:x1 y1 x2 y2 x3 x4 {{x4 }, {x2 , y2 , x4 }, {y 2 , x3 }, {x1 , 1 , x3 }, {x1 , y1 , x2 , x3 }}.(13)QBF :1. x4 ; x2 ; y2 ; x3 ; x1 assignment producing contradictory clause;2. h{x1 , y1 , x2 , x3 },{x1 , 1 , x3 }i x4 ; x2 ; y2 ; x3 ; x1 -Rec-C-Resolved;3. Rec-C-Resolve(, {x1 , y1 , x2 , x3 }, {x1 , 1 , x3 }, x1 , x4 ; x2 ; y2 ; x3 ; x1 ),(a) causes call Rec-C-Resolve(, {x1 , y1 , y2 , x4 }, {x1 , 1 , x3 }, x1 , x4 ; x2 ; y2 ; x3 ; x1 ),clause C3 = {x1 , y1 , y2 , x4 } x4 ; x2 ; y2 ; x3 ; x1 -contradicted;(b) returns clause {y 1 , x3 } x4 ; x2 ; y2 ; x3 ; x1 -contradicted, expected.fact universal literal y2 causes C3 x4 ; x2 ; y2 ; x3 ; x1 -contradictedappear clause returned Rec-C-Resolve due following two facts:1. y2 lower level blocking literal y1 ;2. negation existential literals C3 level lower y2 assignedy2 x4 ; x2 ; y2 ; x3 ; x1 .formally define notions, need additional notation. First, considergiven clause C2 . ResC2 (C1 ) set literals C1 level lower literalblocking resolution C1 C2 . Formally:ResC2 (C1 ) = {l : l C1 , l0 BlockingC2 (C1 )level(l) < level(l0 )},literal l, level(l) prefix level l;BlockingC2 () function definedBlockingC2 (C1 ) = {l : l C1 , l C2 , l universal}Let assignment. say clause C1 -contradictable (with respectC2 )410fiClause/Term Resolution Learning Quantified Boolean Formulas1. existential literal l C1 , l ;2. universal literal l C1 , l(a) l ResC2 (C1 );(b) existential literal l0 C1 , level(l0 ) < level(l) l0 left l.Clearly, clause -contradicted also -contradictable. Considering QBF(13), clause {x1 , y1 , y2 , x4 } x4 ; x2 ; y2 ; x3 ; x1 -contradicted; x4 ; x2 ; y2 ; x3 ; x1 contradictable (with respect {x1 , 1 , x3 }).well founded order induction set tuples hC1 , C2 , l,C1 ; l-contradictable. preliminary step, first define well founded orderliterals according l0 l00 either l0 = l00 l0 l00l0 assigned l00 (i.e., l0 left l00 ).extend partial order relation literals clauses (i) minimal form, (ii)containing l, (iii) ; l-contradictable, saying two clauses C1 C3 ,C3 C1either C3 = C1 ;EEEE00000l0 (ResEC2 (C1 )\ResC2 (C3 ))l (ResC2 (C3 )\ResC2 (C1 ))l l , ResC2 (C1 )subset existential literals ResC2 (C1 ), similarly ResEC2 (C3 ).order well founded, minimal elements ResC2 (C) (or,equivalently, BlockingC2 (C)) empty.Finally, consider set W tuples hC1 , C2 , l,1. ; l assignment;2. l existential literal either unit highest level ;3. C1 clause containing l, minimal form ; l-contradictable respectC2 ;4. C2 contains l, minimal form ; l-contradicted. Further, l unit ,C2 clause causes l unit .set, define well founded order according hC3 , C2 , l, hC1 , C2 , l,C3 C1 .consider procedure Rec-C-Resolve Figure 6. prove well foundedinduction that, tuple hC1 , C2 , l, W , Rec-C-Resolve(, C1 , C2 , l, ) terminatesreturns clause C minimal form -contradicted. end, also showassume C1 ; l-contradicted (and simply ; l-contradictable),C contain existential literals whose negation assigned monotone.base case, C1 ResC2 (C1 ) empty. Hence, universal literall C1 , l thus C1 ; l-contradicted. Since ResC2 (C1 ) empty, setcomputed line 1 empty thus Rec-C-Resolve(, C1 , C2 , l, ) terminates returning411fiGiunchiglia, Narizzano & Tacchellaresolvent C C1 C2 . Clearly C minimal form, easy show C-contradicted.step case, induction hypothesis, thesis holds Rec-CResolve(, C3 , C2 , l, ) show holds also Rec-C-Resolve(, C1 , C2 , l, ),assuming hC3 , C2 , l, hC1 , C2 , l, i. set ResC2 (C1 ) empty, see base case.Assume ResC2 (C1 ) empty, thus also BlockingC2 (C1 ) empty.on, let l0 literal BlockingC2 (C1 ) highest level. l0l0 6 ResC2 (C1 ) C1 ; l-contradictable. l0 l0 C2 C2; l-contradicted. Further, level(l0 ) < level(l). see why, consider two possiblecases:1. l unit : Since l0 C2 , C2 clause causes l unit ,must level(l0 ) = level(l0 ) < level(l).2. l highest level : Since l0 l0 l highestlevel , level(l0 ) level(l). hand, level(l0 ) 6= level(l) l0universal l existential.Since C1 minimal form, exists existential literal l00 l00 C1 , l00, level(l00 ) < level(l0 ) < level(l). on, let l1 existential literalC1 (not necessarily distinct l00 ) level less equal levelliterals C1 (see line 3). Sincelevel(l1 ) < level(l0 ) < level(l)(14)l1 (because C1 ; l-contradictable), follows l1 assigned unit,thus exists clause C causes l1 unit 0 , 0 ; l1initial prefix (see line 4).Consider setBlockingC (C1 ) = {l : l C1 , l C, l universal}.BlockingC (C1 ) empty. fact, universal literal l00 Clevel(l00 ) < level(l1 ) l00 6 C1 since C1 minimal form;level(l00 ) > level(l1 ) l00 l1 . Assume l00 C1 . Since C1 ; l-contradictable,l1 l00 . However, l00 l1 l1 l00 possible l1 6= l00 (l1 existentiall00 universal).Since BlockingC (C1 ) empty, resolve C C1 l1 , obtainingC3 = min((C1 C) \ {l1 , l1 })resolvent. C3 minimal form contains l.show C3 C1 remains showed C3 ; l-contradictable. Indeed,existential literal l C3 , l , universal literals C3 , considertwo cases:412fiClause/Term Resolution Learning Quantified Boolean Formulas1. BlockingC2 (C3 ) empty. case, l0 BlockingC2 (C3 ). easyconsequence following facts:(a) literal l00 BlockingC2 (C), level(l00 ) < level(l0 ): l00 C2 definitionBlockingC2 (C), hence l00 C2 ; l-contradicted, thereforelevel(l00 ) < level(l1 ), thus thesis (see (14));(b) BlockingC2 (C3 ) = (BlockingC2 (C1 )BlockingC2 (C))C3 thus (BlockingC2 (C1 )BlockingC2 (C)) \ BlockingC2 (C3 ) = (C C1 ) \ ({l1 , l1 } C3 ), i.e., literalsBlockingC2 (C1 ) BlockingC2 (C) BlockingC2 (C3 )omitted minimal form C3 ;(c) BlockingC2 (C3 ) empty.Since l0 BlockingC2 (C3 ), literals ResC2 (C1 ) also C3 , also belongResC2 (C3 ), i.e.,ResC2 (C3 ) ResC2 (C1 ) C3 .(15)consider universal literal l00 C1 C3 . l00(a) l00 ResC2 (C1 ) C1 ; l-contradictable, hence l00 ResC2 (C3 )(see (15));(b) existential literal l000 C1 , level(l000 ) < level(l00 ) l000 l00C1 ; l-contradictable;(c) existential literal l000 6= l1 C, l000 l00 . fact, level(l1 ) < level(l00 ),l1 l00 C1 ; l-contradictable, existential literal l000 6= l1C, l000 l1 .Finally, consider universal literal l00 C C3 . l00 level(l00 ) < level(l1 )hence(a) l00 ResC2 (C3 ) level(l1 ) < level(l0 ) (see (14));(b) existential literal l000 C3 , level(l000 ) < level(l00 ) l000 C hencel000 l00 .2. BlockingC2 (C3 ) empty. Let lowest among level literals C3 .level(l0 ) < since l0 6 C3 . Then, universal literal l00 C3 , l00 , i.e.,C3 ; l-contradicted. fact, assume exists universal literal l00 C3. Then, level(l00 ) > either l00 C1 l00 C. Consider first case l00 C1 .Then, l00 ResC2 (C1 ) C1 ; l-contradictable, level(l00 ) < level(l0 ).possible level(l00 ) > level(l0 ) < m. Consider casel00 C. Then, level(l00 ) < level(l1 ) hence level(l00 ) < level(l0 ) (see (14))possible.Since C3 C1 , hC3 , C2 , l, hC1 , C2 , l, i, conclude induction hypothesisRec-C-Resolve(, C3 , C2 , l, ) returns clause minimal form -contradicted.413fiGiunchiglia, Narizzano & Tacchellamake assumption input clause C1 ; l-contradicted.Then, C1 contain existential literals whose negation assigned monotone, holds C2 clause C used line 5. Hence, Rec-CResolve(, C3 , C2 , l, ) returns clause without existential literals whose negationassigned monotone .ReferencesAbdelwaheb, A., & Basin, D. (2000). Bounded model construction monadic second-orderlogics. 12th International Conference Computer-Aided Verification (CAV00),No. 1855 Lecture Notes Computer Science, pp. 99113, Chicago, USA. SpringerVerlag.Bachmair, L., & Ganzinger, H. (2001). Resolution theorem proving. Robinson, A., &Voronkov, A. (Eds.), Handbook Automated Reasoning, Vol. I, chap. 2, pp. 1999.Elsevier Science.Bayardo, Jr., R. J., & Schrag, R. C. (1997). Using CSP look-back techniques solvereal-world SAT instances. Proceedings 14th National Conference Artificial Intelligence 9th Innovative Applications Artificial Intelligence Conference(AAAI-97/IAAI-97), pp. 203208, Menlo Park. AAAI Press.Bayardo, Jr., Roberto J., & Miranker, D. P. (1996). complexity analysis space-boundedlearning algorithms constraint satisfaction problem. ProceedingsThirteenth National Conference Artificial Intelligence Eighth InnovativeApplications Artificial Intelligence Conference, pp. 298304, Menlo Park. AAAIPress / MIT Press.Cadoli, M., Schaerf, M., Giovanardi, A., & Giovanardi, M. (2002). algorithm evaluatequantified Boolean formulae experimental evaluation. Journal AutomatedReasoning, 28, 101142.Cadoli, M., Giovanardi, A., & Schaerf, M. (1998). algorithm evaluate QuantifiedBoolean Formulae. Proceedings 15th National Conference Artificial Intelligence (AAAI-98) 10th Conference Innovative Applications ArtificialIntelligence (IAAI-98), pp. 262267, Menlo Park. AAAI Press.Castellini, C., Giunchiglia, E., & Tacchella, A. (2001). Improvements SAT-based conformant planning. Proc. ECP.Castellini, C., Giunchiglia, E., & Tacchella, A. (2003). SAT-based planning complexdomains: Concurrency, constraints nondeterminism. Artificial Intelligence, 147 (12), 85117.Davis, M., Logemann, G., & Loveland, D. W. (1962). machine program theoremproving. Communication ACM, 5 (7), 394397.de la Tour, T. B. (1990). Minimizing Number Clauses Renaming. Proc.10th Conference Automated Deduction, pp. 558572. Springer-Verlag.Dechter, R. (1990). Enhancement schemes constraint processing: Backjumping, learning,cutset decomposition. Artificial Intelligence, 41 (3), 273312.414fiClause/Term Resolution Learning Quantified Boolean FormulasFermuller, C. G., Leitsch, A., Hustadt, U., & Tammet, T. (2001). Resolution decision procedures. Robinson, A., & Voronkov, A. (Eds.), Handbook Automated Reasoning,Vol. II, chap. 25, pp. 17911849. Elsevier Science B.V.Gent, I., Giunchiglia, E., Narizzano, M., Rowley, A., & Tacchella, A. (2004). Watched datastructures QBF solvers. Giunchiglia, E., & Tacchella, A. (Eds.), TheoryApplications Satisfiability Testing, 6th International Conference, SAT 2003. SantaMargherita Ligure, Italy, May 5-8, 2003 Selected Revised Papers, Vol. 2919 LectureNotes Computer Science, pp. 2536. Springer.Gent, I. P., & Rowley, A. G. (2004). Solution learning solution directed backjumping revisited. Tech. rep. APES-80-2004, APES Research Group. Availablehttp://www.dcs.st-and.ac.uk/apes/apesreports.html.Ginsberg, M. L. (1993). Dynamic backtracking. Journal Artificial Intelligence Research,1, 2546.Giunchiglia, E., Narizzano, M., & Tacchella, A. (2001). Backjumping quantified Booleanlogic satisfiability. Proc. International Joint Conference Artificial Intelligence (IJCAI2001).Giunchiglia, E., Narizzano, M., & Tacchella, A. (2002). Learning Quantified BooleanLogic Satisfiability. Proceedings Eighteenth National Conference ArtificialIntelligence Fourteenth Conference Innovative Applications Artificial Intelligence, July 28 - August 1, 2002, Edmonton, Alberta, Canada. AAAI Press, 2002,pp. 649654.Giunchiglia, E., Narizzano, M., & Tacchella, A. (2003). Backjumping Quantified BooleanLogic Satisfiability. Artificial Intelligence, 145, 99120.Giunchiglia, E., Narizzano, M., & Tacchella, A. (2004a). Monotone literals learningQBF reasoning. Tenth International Conference Principles PracticeConstraint Programming, CP 2004, pp. 260273.Giunchiglia, E., Narizzano, M., & Tacchella, A. (2004b). Qbf reasoning real-world instances. Theory Applications Satisfiability Testing, 7th International Conference, SAT 2004, Vancouver, BC, Canada, May 10-13, 2004, Revised Selected Papers,pp. 105121.Giunchiglia, E., Narizzano, M., & Tacchella, A. (2004c). Qube++: efficient qbf solver.5th International Conference Formal Methods Computer-Aided Design, FMCAD2004, pp. 201213.Kleine-Buning, H., Karpinski, M., & Flogel, A. (1995). Resolution quantified Booleanformulas. Information Computation, 117 (1), 1218.Le Berre, D., Simon, L., & Tacchella, A. (2003). Challenges QBF arena: SAT03evaluation QBF solvers. Sixth International Conference Theory Applications Satisfiability Testing (SAT 2003), Vol. 2919 LNCS. Springer Verlag.Letz, R. (2002). Lemma model caching decision procedures quantified Booleanformulas. Proceedings Tableaux 2002, LNAI 2381, pp. 160175. Springer.415fiGiunchiglia, Narizzano & TacchellaMarques-Silva, J. P., & Sakallah, K. A. (1996). GRASP - New Search AlgorithmSatisfiability. Proceedings IEEE/ACM International Conference ComputerAided Design, pp. 220227.Moskewicz, M. W., Madigan, C. F., Zhao, Y., Zhang, L., & Malik, S. (2001). Chaff: Engineering Efficient SAT Solver. Proceedings 38th Design AutomationConference (DAC01), pp. 530535.Pan, G., & Vardi, M. Y. (2003). Optimizing BDD-based modal solver. AutomatedDeduction - CADE-19, 19th International Conference Automated Deduction MiamiBeach, FL, USA, July 28 - August 2, 2003, Proceedings, pp. 7589.Plaisted, D., & Greenbaum, S. (1986). Structure-preserving Clause Form Translation.Journal Symbolic Computation, 2, 293304.Prosser, P. (1993a). Domain filtering degrade intelligent backjumping search. Proceedings 13th International Joint Conference Artificial Intelligence (IJCAI99-Vol2), pp. 262267.Prosser, P. (1993b). Hybrid algorithms constraint satisfaction problem. Computational Intelligence, 9 (3), 268299.Rintanen, J. (1999). Constructing conditional plans theorem prover. Journal Artificial Intelligence Research, 10, 323352.Robinson, A. (1965). machine-oriented logic based resolution principle. JournalACM, 12 (1), 2341.Robinson, A. (1968). generalized resolution principle. Machine Intelligence, Vol. 3,pp. 7793. Oliver Boyd, Edinburgh.Scholl, C., & Becker, B. (2001). Checking equivalence partial implementations.Proceedings 38th Design Automation Conference (DAC01), pp. 238243.Tseitin, G. (1970). complexity proofs propositional logics. Seminars Mathematics, 8.Urquhart, A. (1995). complexity propositional proofs. Bulletin SymbolicLogic, 1 (4), 425467.Zhang, L., Madigan, C. F., Moskewicz, M. W., & Malik, S. (2001). Efficient conflict drivenlearning Boolean satisfiability solver. International Conference ComputerAided Design (ICCAD01), pp. 279285.Zhang, L., & Malik, S. (2002a). Conflict driven learning quantified Boolean satisfiability solver. Proceedings International Conference Computer Aided Design(ICCAD02).Zhang, L., & Malik, S. (2002b). Towards symmetric treatment satisfaction conflictsquantified Boolean formula evaluation. Proceedings Eighth InternationalConference Principles Practice Constraint Programming, pp. 200215.416fiJournal Artificial Intelligence Research 26 (2006) 247287Submitted 01/06; published 07/06Landscape Random Job Shop SchedulingInstances Depends Ratio Jobs MachinesMatthew J. StreeterStephen F. Smithmatts@cs.cmu.edusfs@cs.cmu.eduCarnegie Mellon University5000 Forbes Avenue, Pittsburgh, PA, 15213 USAAbstractcharacterize search landscape random instances job shop schedulingproblem (JSP). Specifically, investigate expected values (1) backbone size,(2) distance near-optimal schedules, (3) makespan random schedules varyNNNfunction job machine ratio (). limiting cases0Nprovide analytical results, intermediate values perform experiments.NNprove0, backbone size approaches 100%,backboneNNvanishes. process show 0 (resp. ), simple priority rulesalmost surely generate optimal schedule, providing theoretical evidence easyhard-easy pattern typical-case instance difficulty job shop scheduling. also drawconnections theoretical results big valley picture JSP landscapes.1. Introduction1.1 Motivationsgoal work provide picture typical landscape random instancejob shop scheduling problem (JSP), determine picture changesNfunction job machine ratio (). picture potentially useful (1)Nunderstanding typical-case instance difficulty varies function(2) designingselecting search heuristics take advantage regularities typical instancesJSP.1.1.1 Understanding instance difficulty functionNjob shop scheduling literature contains much empirical evidence square JSPs (thoseN= 1) difficult solve rectangular instances (Fisher & Thompson,1963). work makes theoretical empirical contributions toward understandingphenomenon. Empirically, show random schedules random localN1. Analytically, prove twooptima furthest optimalityNNlimiting cases ( 0 ) exist simple priority rules almost surelyproduce optimal schedule, providing theoretical evidence easy-hard-easy patterninstance difficulty JSP.c2006AI Access Foundation. rights reserved.fiStreeter & Smith1.1.2 Informing design search heuristicsHeuristics based local search, example tabu search (Glover & Laguna, 1997; Nowicki& Smutnicki, 1996) iterated local search (Lourenco, Martin, Stutzle, 2003),shown excellent performance benchmark instances job shop scheduling problem(Jain & Meeran, 1998; Jones & Rabelo, 1998). order design effective heuristic, onemust (explicitly implicitly) make assumptions search landscape instancesheuristic applied. example, Nowicki Smutnicki motivateuse path relinking state-of-the-art i-TSAB algorithm citing evidenceJSP big valley distribution local optima (Nowicki & Smutnicki, 2005). Oneconclusions work typical landscape random instancesNNthought big valley valuesclose 1; larger values(including valuescommon benchmark instances), landscape breaks many big valleys, suggestingmodifications i-TSAB may allow better handle case (we discuss i-TSAB9.3).1.2 Contributionscontributions paper twofold. First, design novel set experimentsrun experiments random instances JSP. Second, derive analytical resultsconfirm provide insight trends suggested experiments.main contributions empirical work follows.N, show low-makespan schedules clustered smalllow valuesregion search space many attributes (i.e., directed disjunctive graph edges)Ncommon low-makespan schedules.increases, low-makespan schedulesbecome dispersed throughout search space attributes commonlow-makespan schedules.introduce statistic (neighborhood exactness) used quantitativelymeasure smoothness search landscape, estimate expected valuestatistic random instances JSP. results, combinationresults clustering, suggest landscape typical instances JSPNNdescribed big valley low values; high valuesmany separate big valleys.limiting casesproveN0N, derive analytical results. Specifically,N0, expected size backbone (i.e., set problem variablesNcommon value global optima) approaches 100%,,expected backbone size approaches 0%;NN0 (resp.), randomly generated schedule almost surely (a)located close search space optimal schedule (b) near-optimalmakespan.248fiThe Landscape Random Job Shop Scheduling Instances2. Related Workleast three threads research conducted search space analysesrelated ones conduct here. include literature big valley distributioncommon number combinatorial optimization problems, studies backbone sizeBoolean satisfiability, statistical mechanical analysis TSP. briefly reviewthree areas below, well relevant work phase transitions easy-hardeasy pattern instance difficulty.2.1 Big Valleyterm big valley originated paper Boese et al. (1994) examineddistribution local optima Traveling Salesman Problem (TSP). Based samplelocal optima obtained next-descent starting random TSP tours, Boese calculatedtwo correlations:1. correlation cost locally optimal tour average distancelocally optimal tours,2. correlation cost locally optimal tour distancetour best tour sample.distance two TSP tours defined total number edges minusnumber edges common two tours. Based factcorrelations surprisingly high, Boese conjectured local optima TSParranged big valley. Adapted work Boese et al. (1994), Figure 1gives intuitive picture big valley, set local minima appears convexone central global minimum (Boese et al., 1994). offer formal definitionbig valley landscape 6.Boeses analysis applied combinatorial problems (Kim & Moon, 2004),including permutation flow shop scheduling problem (Watson, Barbulescu, Whitley, &Howe, 2002; Reeves & Yamada, 1998) JSP (Nowicki & Smutnicki, 2001). Correlations observed JSP generally weaker observed TSP.related study, Mattfeld (1996) examined cost-distance correlations famousJSP instance ft10 (Beasley, 1990) found evidence Massif Central. . . manynear optimal solutions reside laying closer together local optima. 4 containsrelated results backbone size ft10.2.2 Backbone Sizebackbone problem instance set variables assigned common valueglobally optimal solutions instance. example, Boolean satisfiabilityproblem (SAT), backbone set variables assigned fixed truth valuesatisfying assignments. JSP, backbone defined numberdisjunctive edges (3.2) common orientation globally optimal schedules(a formal definition given 4).large literature backbones combinatorial optimization problems, including many empirical analytical results (Slaney & Walsh, 2001; Monasson, Zecchina,249fiStreeter & SmithFigure 1: intuitive picture big valley landscape.Kirkpatrick, Selman, & Troyansky, 1999). analysis problem difficulty JSP,Watson et al. (2001) present histograms backbone size random 6x6 (6 job, 6 machine)6x4 (6 job, 4 machine) JSP instances. Summarizing experiments reportedpaper, Watson et al. note [job:machine ratios] > 1.5, bias toward small backbones becomes pronounced, ratios < 1, bias toward larger backbonesmagnified. 4 generalizes observations proves two theorems giveinsight phenomenon occurs.2.3 Statistical Mechanical Analyseslarge growing literature applies techniques statistical mechanics analysiscombinatorial optimization problems (Martin, Monasson, & Zecchina, 2001). leastone result obtained literature concerns clustering low-cost solutions. studyTSP, Mezard Parisi (1986) obtain expression expected overlap (numbercommon edges) random TSP tours drawn Boltzmann distribution.show temperature parameter Boltzmann distribution lowered (placingprobability mass low-cost TSP tours), expected overlap approaches 100%. Thoughuse Boltzmann weighting, 5 paper examines expected overlaprandom JSP schedules changes probability mass placed low-makespanschedules.2.4 Phase Transitions Easy-hard-easy PatternLoosely speaking, phase transition occurs system expected valuestatistic varies discontinuously (asymptotically) function parameter.example, > 0 holds random instances 2-SAT problem satisfiableprobability asymptotically approaching 1 clause variable ratio (n ) 1,satisfiable probability approaching 0 clause variable ratio 1 + .similar statement conjectured hold 3-SAT; critical value kn (if exists)must satisfy 3.42 k 4.51 (Achlioptas & Peres, 2004).problems exhibit phase transitions (notably 3-SAT), average-case instancedifficulty (for typical solvers) appears first increase decrease one increasesrelevant parameter, hardest instances appearing close threshold value250fiThe Landscape Random Job Shop Scheduling Instances(A) JSP instanceJ1 :J11J 12J 2 : J12J 13J 22(B) JSP scheduleJ 14J 32J 42J11J12J 22time(C) DisjunctivegraphJ11J 12J 13J 32J 14J 42J 14o*J12J 12 J 13J 22J 32J 42Figure 2: (A) JSP instance,(B) afeasibleschedule instance, (C) disjunctive graph representation schedule. Boxes represent operations; operationdurations proportional width box; machineoperation performed represented texture. (C), solid arrows representconjunctive arcs dashed arrows represent disjunctive arcs (arc weightsproportional duration operation arc points of).(Cheeseman, Kanefsky, & Taylor, 1991; Yokoo, 1997). phenomenon referredeasy-hard-easy pattern instance difficulty (Mammen & Hogg, 1997). 7.4discuss evidence easy-hard-easy pattern instance difficulty JSP, though(to knowledge) associated phase transition.results 4-5 empirical results 6 previously presented conference paper (Streeter & Smith, 2005a).3. Job Shop Scheduling Problemadopt notation [n] {1, 2, . . . , n}.3.1 Problem DefinitionDefinition (JSP instance). N JSP instance = {J 1 ,J 2 , . . . , J N } set Nk ) sequence operations. operationjobs , job J k = (J1k , J2k , . . . , JMk= Ji associated duration (o) (0, max ] machine m(o) [M ]. requirejob uses machine exactly (i.e., J k [M ],exactly one [M ] m(Jik ) = m). define1. ops(I) {Jik : k [N ], [M ]},2. (J k )PMki=1 (Ji ),251fiStreeter & Smith3. job-predecessor J (Jik ) operation JikJ (Jik )kJi1> 1otherwisefictitious operation (o ) = 0 m(o ) undefined.Definition (JSP schedule). JSP schedule instance function : ops(I)<+ associates operation ops(I) start time S(o) (operation performedmachine m(o) time S(o) time S(o) + (o); preemption allowed). makefollowing definitions.1. completion time operation + (o) S(o) + (o).2. machine-predecessor M(o) operation ops(I)M(o)arg maxoOprev (o) S(o)Oprev (o) 6=otherwise.Oprev (o) = {o ops(I) : m(o) = m(o), S(o) < S(o)} set operationsscheduled run os machine.3. feasible schedule S(o) max(S + (J (o)), + (M(o))) ops(I).4. quantity`(S) max + (o)oops(I)called makespan S.consider makespan-minimization version JSP, goal findschedule minimizes makespan.remainder paper, whenever refer JSP schedule shall adoptconvention S(o ) = 0 shall assumeS(o) = max(S + (J (o)), + (M(o))) ops(I)(3.1)(i.e., so-called semi-active schedule, French, 1982). words, ignore schedulessuperfluous idle time start schedule end one operationstart another.Figure 2 (A) (B) depict, respectively, JSP instance feasible scheduleinstance.3.2 Disjunctive Graphsschedule satisfying (3.1) uniquely represented weighted, directed graph calleddisjunctive graph. disjunctive graph representation schedule JSPinstance I, operation ops(I) vertex directed edge (o1 , o2 ) indicatesoperation o1 completes o2 starts.252fiThe Landscape Random Job Shop Scheduling InstancesDefinition (disjunctive graph). disjunctive graph G = G(I, S) schedule~ w) defined follows.JSP instance weighted, directed graph G = (V, E,V = ops(I) {o , }, (like ) fictitious operation (o ) = 0m(o ) undefined.~ =C~ D,~E~ = {(J (o), o) : ops(I)} (J k , ) : k [N ] called set conjunctiveCarcs (which specify cannot start J (o) completes),~ = {(o1 , o2 ) : {o1 , o2 } ops(I), m(o1 ) = m(o2 ), S(o1 ) < S(o2 )} called setdisjunctive arcs (which specify, pair operations performedmachine, two operations performed first).w((o1 , o2 )) = (o1 ).Figure 2 (C) depicts disjunctive graph schedule depicted Figure 2 (B).connection schedule disjunctive graph established followingproposition (Roy & Sussmann, 1964).Proposition 1. Let feasible schedule satisfying (3.1), let G = G(I, S)corresponding disjunctive graph. `(S) equal length longest weightedpath G.Proof. operation o, let L(o) denote length longest weighted pathG. suffices show ops(I), S(o) = L(o). followsinduction number edges path, base case S(o ) = L(o ) = 0.undirected version disjunctive arc called disjunctive edge.Definition (disjunctive edge). Let JSP instance. disjunctive edge set{o1 , o2 } ops(I) m(o1 ) = m(o2 ). define following notation.E(I) set disjunctive edges I.Let schedule let e = {o1 , o2 } disjunctive edge. denote~e(S) unique arc {(o1 , o2 ), (o2 , o1 )} appears disjunctive graph G(I, S)(this arc called orientation e S).measure distance two schedules S1 S2 JSP instancecounting number disjunctive edges oriented opposite directions G(I, S1 )G(I, S2 ).Definition (disjunctive graph distance). disjunctive graph distance kS1 S2 ktwo schedules S1 S2 JSP instance definedkS1 S2 k |{e E(I) : ~e(S1 ) 6= ~e(S2 )}| .253fiStreeter & Smith3.3 Random Schedules Instancesdefine uniform distribution JSP instances follows. distribution identicalone used Taillard (1993).Definition (random JSP instance). random N JSP instance generatedfollows.1. Let 1 , 2 , . . . , N random permutations [M ].2. Let G probability distribution (0, max ] mean variance 2 > 0.3. Define = {J 1 , J 2 , . . . , J N }, m(Jik ) = k (i) (Jik ) drawn (independently random) G.Note definition (and likewise, theoretical results) assumes maximumoperation duration max , makes assumptions form distributionoperation durations. empirical results reported paper, choose operationdurations uniform distribution {1, 2, . . . , 100}.proofs frequently make use priority rules. priority rule greedy schedulebuilding algorithm assigns priority operation and, step greedyalgorithm, assigns earliest possible start time operation minimum priority.Definition (priority rule). priority rule function that, given instanceoperation ops(I), returns priority (I, o) <. schedule = S(, I) associateddefined following procedure.1. U nscheduled ops(I), S(o ) 0.2. |U nscheduled| > 0 do:(a) Ready {o U nscheduled : J (o)/ U nscheduled}.(b) element Ready least priority.(c) S(o) max(S + (J (o)), + (M(o))).(d) Remove U nscheduled.priority rule called instance-independent if, N JSP instanceintegers k [N ], [M ], value (I, Jik ) depends k, i, N , .obtain random schedule assigning random priorities operation.resulting distribution equivalent one used Mattfeld (1996).Definition (random schedule). random schedule N JSP instancegenerated performing following steps.1. Create list L containing occurrences integer k k [N ] (we thinkoccurrences k representing operations job J k ).2. Shuffle L (obtaining permutation equal probability).3. Return schedule S(rand , I) rand (I, Jik ) = index ith occurrencek L.254fiThe Landscape Random Job Shop Scheduling Instances4. Number Common Attributes Function Makespanbackbone JSP instance set disjunctive edges common orientation schedules whose makespan globally optimal. 1, definebackbone set disjunctive edges common orientation schedules whose makespan within factor optimal (a related definition appears Slaney& Walsh, 2001).Definition ( backbone). Let JSP instance optimal makespan `min (I).1, let opt(I) {S : `(S) `min (I)} set schedules whose makespanwithin factor optimal.backbone(I) {e E(I) : ~e(S1 ) = ~e(S2 ) {S1 , S2 } opt(I)} .section compute expected value | backbone| functionrandom N JSP instances, examine shape curve changesNfunction.4.1 Computing backbonecompute backbone use following proposition.Proposition 2. Let JSP instance optimal makespan `min (I). Let e = {o1 , o2 }disjunctive edge orientations a1 = (o1 , o2 ) a2 = (o2 , o1 ). disjunctivearc a, let `min (I|a) denote optimum makespan among schedules whose disjunctive graphcontains arc a.e backbone(I) max {`min (I|a1 ), `min (I|a2 )} > lmin (I) .Proof. e backbone, e must common orientation (say a1 ) schedules`(S) `min (I), implies `min (I|a2 ) > `min (I). e/ backbone,must {S1 , S2 } opt(I) ~e(S1 ) = a1 ~e(S2 ) = a2 , impliesmax{`min (I|a1 ), `min (I|a2 )} `min (I).Thus compute backbone(I) need compute `min (I|a) 2M N2possible choices a. Given disjunctive arc a, compute `min (I|a) using branchbound. branch bound algorithms JSP, nodes search tree representchoices orientations subset disjunctive edges. constructing root searchtree node fixed arc, determine `min (I|a). use branch boundalgorithm due Brucker et al. (1994) efficient codefreely available via ORSEP (Brucker, Jurisch,& Sievers, 1992).NComputing `min (I|a) 2M 2 possible choices requires 1 + N2 runsbranch bound. first run used find globally optimal schedule,givesNNvalue `min (I|a) 2 possible choices (namely, 2 disjunctive arcspresent globally optimal schedule). separate run usedN2 remaining choices a.Figure 3 graphs fraction disjunctive edges belong backbonefunction instance ft10 (a 10 job, 10 machine instance) library (Beasley,255fiStreeter & SmithInstance ft10Normalized |-backbone|10.80.60.40.201.001.021.041.061.091.111.13Figure 3: Normalized | backbone| function library instance ft10.1990). Note definition curve non-increasing respect ,curve exact . noteworthy among schedules whose makespan withinfactor 1.005 optimal, 80% disjunctive edges fixed orientation. seeNbehavior typical JSP instances= 1.4.2 Resultsplotted | backbone| function instances library 10fewer jobs 10 fewer machines. results available online (Streeter & Smith,2005b). Inspection graphs revealed shape curve largely functionjob:machine ratio. investigate further, repeat experiments largenumber randomly generated JSP instances.use randomly generated instances 7 different combinations N studyNNinstancesequal 1, 2, 3.= 1 use 6x6, 7x7, 8x8 instances;NN=2use8x410x5instances;= 3 use 9x3 12x4 instances.generate 1000 random instances combination N .Figure 4 parts (A), (B), (C) graph expected fraction edges belongingN-backbone function combination N , grouped according.NFigure 4 (D) compares curves different values , plots 0.25 0.75quantiles. purposes study two important observations Figure4 follows.curves depend size instance (i.e., N ) shape (i.e.,NN). two factors, far stronger influence shapecurves.values , expected fraction edges belonging backbone decreasesNincreases.256fiThe Landscape Random Job Shop Scheduling Instances(A) Job:machine ratio 1:1(B) Job:machine ratio 2:110.8E[frac. edges -backbone]E[frac. edges -backbone]16x6 instances7x7 instances0.68x8 instances0.40.20.88x4 instances10x5 instances0.60.40.20011.11.21.31.411.51.11.31.41.5(C) Job:machine ratio 3:1(D) Comparison110.8Frac. edges -backboneE[frac. edges -backbone]1.29x3 instances0.612x4 instances0.40.200.88x8 instances10x5 instances0.612x4 instances0.40.2011.11.21.31.41.511.11.21.31.41.5Figure 4: Expected fraction edges -backbone function random JSPNinstances. Graphs (A), (B), (C) depict curves random instances= 1, 2, 3, respectively. Graph (D) compares curves depicted (A), (B),(C) (only curves largest instance sizes shown (D)). (D),top bottom error bars represent 0.75 0.25 quantiles, respectively.257fiStreeter & Smith4.3 Analysisgive insight Figure 4 analyzing two limiting cases. proveN0, expected fraction disjunctive edges belong backbone approachesN1,expected fraction approaches 0.NIntuitively, happens follows.0 (i.e., N held constant )jobs becomes long. Individual disjunctive edges represent precedencerelations among operations performed far apart time. example,10,000 machines (and job consists 10,000 operations), disjunctiveedge might specify whether operation 1,200 job performed operation8,500 job B. Clearly, waiting job B complete 8,500 operations allowingjob complete 12% operations likely produce inefficient schedule. Thus,orienting single disjunctive edge wrong direction likely prevent scheduleoptimal, particular edge likely common orientationglobally optimal schedules.Ncontrast,, workloads machines become long.order jobs processed particular machine matter muchlong machine longest workload kept busy, fact particularedge oriented particular way unlikely prevent schedule optimal.formalized below.make use following well-known definition.Definition (whp). sequence events n occurs high probability (whp) limnP[n ] = 1.Lemma 1 Theorem 1 show constant N , randomly chosen edge randomN JSP instance backbone whp (as ). Lemma 2 Theorem 2show constant , randomly chosen edge random N JSP instancebackbone whp (as N ).Lemma 1. Let random N JSP instance, let = S(, I) scheduleobtained using instance-independent priority rule . arbitrary job J I,define SJ + (JM ) (J). E[SJ ] O(N ).Proof. assume N = 2 > 1. generalization larger N straightforward,cases N = 1 = 1 trivial. Let = {J 1 , J 2 } let J = J 1 .Let = (o1 , o2 , . . . , ) sequence operations selected Ready (in line2(b) definition priority rule 3.3) constructing S. say operationJi1 overlaps operation Jj21. Jj2 appears Ji1 ,1 ), + (J 1 ) + (J 1 )] 6= .2. [S(Jj2 ), + (Jj2 )] [S + (Ji1i1additionally m(Ji1 ) = m(Jj2 ), say Ji1 contends Jj2 . Intuitively, Ji1overlaps o0 Jj2 start time might delayed os machineused o0 . contends o0 , start time actually delayed.258fiThe Landscape Random Job Shop Scheduling InstancesLet i,j (resp. i,j ) indicator event Ji1 overlaps (resp. contends)221LetCi {Jj : i,j = 1} set operations J Ji overlaps with.|Ci i0 >i Ci0 | 1. ThusJj2 .X|Ci | =X|Ci \[Ci0 | +i0 >iX|Ci[Ci0 | 2M .(4.1)i0 >iLet = IN,M 1 random N 1 JSP instance, define i,j , i,j , Cianalogously above. i, j 1,P i,j = 1|m(Ji1 ) = m(Jj2 ) = P i,j = 1 .true P[i,j = 1] function joint distribution operationsset {Ji10 : i0 < i} {Jj20 : j 0 < j}; and, far joint distribution concerned,conditioning event m(Ji1 ) = m(Jj2 ) like deleting operations usemachine m(Ji1 ).h111P i,j = 1|m(Ji1 ) = m(Jj2 ) =P i,j = 1 =E i,j .Thus E [i,j ] = P [i,j = 1] =Therefore,PM 1 PM 1PM PMj=1 E[i,j ]i=1j=1 E[i,j ] 2 +i=11 PM 1 PM 1= 2 + i=1j=1 E[i,j ]1 PM 1= 2 + i=1 E[|Ci |]4last step used (4.1). follows E[SJ ] 4max (maxmaximum operation duration defined 3). consider arbitrary N , get E[SJ ]4max (N 1).corollary Lemma 1, show simple priority rule (0 ) almost surelyN0.generates optimal schedule caseDefinition (priority rule 0 ). Given N JSP instance I, let k = arg maxk[N ](J k ) index longest job. priority rule 0 first schedules operationsJ k , schedules remaining operations fixed order.k = kk0 (I, Ji ) =k + otherwise.Corollary 1. Let random N JSP instance. fixed N , holdswhp (as ) schedule = S(0 , i) optimal makespan `(S) =maxk[N ] (J k ).Proof. Define priority rule k k (I, Jik ) = k = k; k + otherwise. kinstance-independent, 0 equivalent k . Thus JE[J0 ]XE[Jk ] = O(N 2 )k259fiStreeter & SmithS(,I)define J J, second step uses Lemma 1. Markovs inequal10ity, J < 4 J whp. Central Limit Theorem,(J) asymptoticallynormally distributed mean standard deviation . follows whp,1(J k ) (J k ) > 4 k 6= k . implies `(S) = (J k ). (J k ) lowerbound makespan schedule, corollary follows.Theorem 1. Let random N JSP instance, let e randomly selectedelement E(I). fixed N , holds whp (as ) e 1 backbone(I).Proof. Let e = {Ji , Jj0 } j let = (Jj0 , Ji ). Proposition 1 Corollary 1,suffices show whp, disjunctive graphs containing contain pathweighted length > maxk[N ] (J k ).3Assume j 4 (this holds whp j selected uniformlyrandom [M ]), consider pathP = (o , J10 , J20 , . . . , Jj0 , Ji , Ji+1 , . . . , JM , )3passes |P | 3+M +M 4 vertices weighted length w(P ). wantshow w(P ) > maxJI (J) whp. Central Limit Theorem, (1) fixedj, w(Pp) asymptotically normally distributed mean (|P | 2) standarddeviation (|P | 2) (2)J, (J) asymptotically normally distributedmean standard deviation . w(P ) > maxJI (J) whp followsChebyshevs inequality.N, simple priority rule ( ) almost surely generatesLemma 2 showsschedule machine idle operations performed machinecompleted (a schedule property clearly optimal).Definition (priority rule ). Given N JSP instance I, priority rulefirst schedules first operation job (taking jobs order ascending indices),second operation job, forth. defined (I, Jik ) = + k.Lemma 2. Let random N JSP instance. fixed , holds whp (asN ) schedule = S( , I) propertyS(o) = + (M(o)) ops(I) .Proof. Suppose executing replace line S(o) max(S + (J (o)),+ (M(o))) (line 2(c) definition priority rule given 3.3) S(o) + (M(o)).resulting feasible replacement must effect. Thus sufficesshow resulting feasible whp. Equivalently, want show whp,S(o) + (J (o)) ops(I) constructed using modified version line 2(c).Let ops2+ (I) = {Jik ops(I) : > 1} set operations firstjob. suffices show S(o) S(J (o)) max ops2+ (I). end, considerarbitrary operation = Jik ops2+ (I). , number operations lower260fiThe Landscape Random Job Shop Scheduling Instancespriority (i 1)N + (k 1). number operations lower priority1Jik run machine m(o) is, expectation, equal[(i 1)(N 1) + (k 1)](where switch N N 1 due fact operation job J kuses machine m(o)). followsE[S(o)] =[(i 1)(N 1) + (k 1)]k)] =E[S(o) S(J (o))] = E[S(Jik ) S(Ji1N 1.Appendix use martingale tail inequality establish following claim.Claim 2.1. high probability, ops2+ (I)1S(o) S(J (o)) E[S(o) S(J (o))] .2Lemma follows fact 21 E[S(o) S(J (o))] > max N sufficientlylarge.Based results computational experiments, Taillard (1994) conjecturedoptimal makespan almost surely equal maximum machine workload.following corollary Lemma 2 confirms conjecture.NCorollary 2. Let random N JSP instance optimal makespan `min (I).Let (m) ({o ops(I) : m(o) = m}) denote workload machine m. fixed, holds whp (as N ) `min (I) = maxm[M ] (m).Theorem 2. Let random N JSP instance, let e randomly selectedelement E(I). fixed , holds whp (as N ) e/ 1 backbone(I).Proof. Let e = {Ji , Jj0 }. Remove J J 0 create N 2 instancecomes distribution random N 2 JSP instance. LemmaI,2 shows whp exists optimal schedule property describedstatement lemma.: m(o) = m}) denote workload machineLet (m) ({o ops(I)instance I. Central Limit Theorem,(m) asymptotically normally distributedmean (N 2) standard deviation N 2. follows whp, | (m) (m0 )| >1N 4 6= m0 .Thus whp one machine still processing operations interval1[`(S) N 4 , `(S)]. max( (J), (J 0 )) max = O(1), use intervalconstruct optimal schedules containing disjunctive arc (Ji , Jj0 ) well optimalschedules containing disjunctive arc (Jj0 , Ji ).261fiStreeter & Smith5. Clustering Function Makespansection estimate expected distance random schedules whose makespanwithin factor optimal, function various combinations N .Nexamine shape curve changes function. formally,random N JSP instance optimal makespan `min (I),opt(I) {S : `(S) `min (I)},S1 S2 drawn independently random opt(I),wish compute E[kS1 S2 k].Note experiments 4 provide upper bound quantity:NE [| backbone|]E [kS1 S2 k]2provide lower bound (a low backbone size evidence mean distanceglobal optima large). experiments section viewed testdegree upper bound provided 4 tight.5.1 Methodologygenerate random samples opt(I) running simulated annealing algorithmvan Laarhoven et al. (1992) finds schedule. precisely, proceduresampling distances follows.1. Generate random N JSP instance I.2. Using branch bound algorithm Brucker et al. (1994), determine optimalmakespan I.3. Perform k runs, R1 , R2 , . . . , Rk , van Laarhoven et al. (1992) simulated annealingalgorithm. Restart run many times necessary find schedule whosemakespan optimal.4. {1, 1.01, 1.02, . . . , 1.5}, find first schedule, call Si (), runRi whose makespan within factor optimal. k2 pairsruns (Ri , Rj ), add distance Si () Sj () sample distancesassociated .ran procedure random JSP instances 7 combinations Nused 4.2. smallest instance sizes ratio (i.e., 6x6, 8x49x3 instances) generate 100 random JSP instances run procedure k = 100.Setting k = 100 allows us measure variation instance-specific expected values.4 combinations N , performing 10,000 simulated annealing runscomputationally expensive, instead generate 1000 random JSP instances runprocedure k = 2.262fiThe Landscape Random Job Shop Scheduling InstancesFigure 5 (A), (B), (C) plot expected distance random -optimal schedNules function three values. Figure 5 (D) shows 0.750.25 quantiles 100 instance-specific sample means three smallestinstance sizes. Examining Figure 5 (D), see variation among random instancesN small relative differences curves differentNvalues.5.2 Discussionexamining Figure 5 see , expected distance random Noptimal schedules increasesincreases. Indeed, global optima dispersed widelyNNthroughout search space= 3, true lesser extent= 2.immediate implication Figure 5 whether exhibit twocorrelations operational definition big valley, typical landscapes JSPN= 3 cannot expected big valleys sense centralinstancescluster optimal near-optimal solutions. anything, one might posit existencemultiple big valleys, leading separate global optimum. next section expandsupon observations.6. Big Valleysection define formal properties big valley landscape, conduct experiments determine extent random JSP instances exhibit propertiesNNNvary, present analytical results limiting cases0.Considering intuitive picture given Figure 1, take followingnecessary (though perhaps sufficient) conditions function f (x) big valley.1. Small improving moves. x global minimum f , must exist nearbyx0 f (x0 ) < f (x).2. Clustering global optima. maximum distance two global minimaf small.Note direct relationship two properties cost-distancecorrelations considered Boese et al. (1994).6.1 Formalizationfollowing four definitions allow us formalize notion big valley landscape.Definition (Neighborhood Nr ). Let arbitrary JSP instance, let U setschedules I. Let r positive integer. neighborhood Nr : U 2U definedNr (S) {S 0 U : kS 0 k r} .Definition (local optimum L(S, N )). Let U above; let N : U 2Uarbitrary neighborhood function; let schedule I. L(S, N ) schedulereturned following procedure (which finds local optimum performing next-descentstarting using neighborhood N ).263fiStreeter & Smith(A) Job:machine ratio 1:1(B) Job:machine ratio 2:10.5E[dist. schedules]E[dist. schedules]0.50.40.30.26x6 instances7x7 instances0.18x8 instances00.40.30.28x4 instances0.110x5 instances011.11.21.31.41.511.11.2(C) Job:machine ratio 3:11.41.5(D) Comparison0.50.5E[dist. schedules]E[dist. schedules]1.30.40.30.29x3 instances0.112x4 instances0.40.30.26x6 instances8x4 instances0.19x3 instances0011.11.21.31.41.511.11.21.31.41.5Figure 5: Expected distance random schedules within factor optimal,function . Graphs (A), (B), (C) depict curves random instancesN= 1, 2, 3, respectively. Graph (D) compares curves depicted (A),(B), (C) (only curves smallest instance sizes shown (D)).(D), top bottom error bars represent 0.75 0.25 quantiles (respectively)instance-specific sample means.264fiThe Landscape Random Job Shop Scheduling Instances(A) (r,)-valley(B) Three (r,)-valleysrrrrFigure 6: Two landscapes comprised (r, )-valleys. (A) single (r, ) valley (forvalues r shown figure), (B) either viewed threedistinct (r, ) valleys single (r, 0 )-valley. (The values r shownfigure slightly larger necessary.)1. Let N (S) = {S1 , S2 , . . . , S|N (S)| } (where elements N (S) indexed fixedarbitrary manner).2. Find least `(Si ) < `(S). exists, return S; otherwise setSi go 1.Definition ((r, )-valley). Let U above, let r non-negativeintegers. set V U (r, )-valley V following two properties.1. V , schedule L(S, Nr ) V globally optimal.2. two globally optimal schedules S1 S2 V , kS1 S2 k .Figure 6 illustrates definition (r, )-valley. would say landscapedepicted Figure 6 (A) big valley, depicted 6 (B) comprised threebig valleys.Definition ((r, , p) landscape). Let U above, let random scheduleI. (r, , p) landscape exists V U1. V (r, )-valley,2. P[S V ] p.JSP instance trivially (M N2 , N2 , 1) landscape (because r = N2Nr includes possible schedules). JSP instance (r, N2 , 1) landscape,globally optimal schedule always found starting random scheduleapplying next-descent using neighborhood Nr .say JSP instance big valley landscape (r, , p) landscapesmall r combination p near 1. contrast, small r combinationp near 1 require large , say landscape consists multiple big valleys.265fiStreeter & Smith6.2 Neighborhood Exactnesssection seek determine extent random JSP instancessmall improving moves property. require following definition.Definition (neighborhood exactness). Let I, U , N above, letrandom schedule I. exactness neighborhood N instanceprobability L(S, N ) global optimum.exactness Nr p, (r, N2 , p) landscape (let V consistschedules L(S, N ) global optimum). estimate expected exactnessNr function r various combinations N . examining resultingcurves, able draw conclusions extent landscapesrandom N JSP instance typically small improving moves property.Ndetermine presence absence property depends.fixed N , compute expected exactness Nr 1 r N2repeatedly executing following procedure.1. Generate random N JSP instance I.2. Using algorithm Brucker et al. (1994), compute optimal makespan I.3. Repeat k times:(a) random feasible schedule, r 1, opt f alse.(b) opt = f alse do:L(S, Nr ).global optimum, opt true.Record pair (r, opt).r r + 1.(c) r0 r r0N2record pair (r0 , true).pairs recorded procedure (in step 3(c) third bullet point 3 (b))used obvious way estimate expected exactness. Specifically, restimated expected exactness Nr fraction pairs (r, x) x = true.implementation first bullet point step 3 (b) deserves discussion.determine L(S, Nr ), step next-descent must able determine best schedule{S 0 : kS 0 k r}. large r impractical brute force. Insteaddeveloped radius-limited branch bound algorithm that, given arbitrary centerschedule Sc radius r, finds schedule arg min{S 0 :kSc 0 kr} `(S 0 ). radius-limitedbranch bound algorithm uses branching rule Balas (1969) combinedlower bounds branch ordering heuristic Brucker et al. (1994).266fiThe Landscape Random Job Shop Scheduling Instances6.3 ResultsNuse three combinations N= 15 (3x15, 4x20, 5x25 instances), threeNNcombinations = 1 (6x6, 7x7, 8x8 instances) two combinations=5(15x3 20x4 instances). smallest instance sizes ratio (i.e., 3x15, 6x6,15x3 instances) generate 100 random JSP instances run procedurek = 100. Otherwise, generate 1000 random JSP instances run procedurek = 1.Figure 7 (A), (B), (C) plot expected exactness function neighborhood radiusN(normalized number disjunctive edges) three values. Figure7 (D) shows 0.75 0.25 quantiles 100 instance-specific sample meansthree smallest instance sizes.6.4 DiscussionExamining Figure 7, see normalized neighborhood radius, neighborhoodNexactness lowest instances= 1 higher two extreme ratios1NN( = 5 = 5). view neighborhood exactness measuring smoothnesslandscape, data suggest typical JSP landscapes least smoothNNNintermediate value, become smooth0.suggests easy-hard-easy pattern typical-case instance difficulty JSP,phenomenon explored fully next section.Using methodology 4-5, found expected proportions backboneedges 3x15, 4x20, 5x25 instances 0.94, 0.93, 0.92, respectively,expected distance global optima 0.02 three cases. contrast,expected proportions backbone edges 15x3 20x4 instances near-zero,expected distances global optima 0.33 0.28, respectively. concludelandcapes random N JSP instances typically clustering globalNNoptima property= 51= 5. However, Figure 7 suggests smallNNimproving moves property present= 51= 5. Accordingly, wouldN1Nsay typical landscapes = 5 big valleys,= 5 landscapecomprised many big valleys rather one.Ndata 4-5 show= 1, typical landscapes clusteringglobal optima property. Examining Figure 7 (B), see able descendrandom schedule globally optimal schedule probability 12 (normalized)neighborhood radius 6%. reason, think landscapes randomNJSP instances= 1 small improving moves property extent.This, combination curve Figure 5 (A) (which shows expected distancerandom -optimal schedules function ) leads us say typical landscapesNrandom JSP instances= 1 still roughly described big valleys. However,valley much rougher (meaning larger steps required move randomschedule global optimum via sequence improving moves) extremeNvalues.Table 1 summarizes empirical findings discussed.267fiStreeter & Smith(A) Job:machine ratio 1:5(B) Job:machine ratio 1:1110.83x15 instances0.6E[exactness]E[exactness]0.84x20 instances0.45x25 instances6x6 instances0.67x7 instances8x8 instances0.40.20.20000.10.200.30.20.3Normalized radiusNormalized radius(C) Job:machine ratio 5:1(D) Comparison110.8E[exactness]0.8E[exactness]0.115x3 instances0.620x4 instances0.43x15 instances0.66x6 instances0.415x3 instances0.20.20000.10.20.300.10.20.3Normalized radiusNormalized radiusFigure 7: Expected exactness Nr function (normalized) neighborhood radiusNr. Graphs (A), (B), (C) depict curves random instances= 15 ,1, 5, respectively. Graph (D) compares curves depicted (A), (B),(C) (only curves largest instances shown (D)). (D), topbottom error bars represent 0.75 0.25 quantiles (respectively) instancespecific exactness.268fiThe Landscape Random Job Shop Scheduling InstancesN15NTable 1. Landscape attributes three values.ClusteringSmallimprovingDescriptionglobal optima? moves?YesYesBig valley1YesSomewhat(Rough) big valley5YesMultiple big valleys6.5 Analysisfirst establish behavior curves depicted Figure 7 limiting casesNN0 . use results characterize landscapes randomJSP instances using (r, , p) notation introduced 6.1.NNfollowing two lemmas show0 (resp.), random schedulealmost surely close optimal schedule. proofs given Appendix A.Lemma 3. Let random N JSP instance, let random scheduleI. Let optimal schedule kS Sk minimal. Let f (M )unbounded, increasing function . fixed N , holds whp (as )kS Sk < f (M ).Lemma 4. Let random N JSP instance, let random schedule I,let optimal schedule kS Sk minimal. fixed> 0, holds whp (as N ) kS Sk < N 1+ .following immediate corollaries Lemmas 3 4.Corollary 3. fixed N , expected exactness Nf (M ) approaches 1 ,f (M ) unbounded, increasing function .Corollary 4. fixed > 0, expected exactness NN 1+ approaches 1N .total number disjunctive edges N2 , two corollaries implyNN0 (resp.), curve depicted Figure 7 approaches horizontal lineheight 1.Using Lemmas 3 4, Theorems 3 4 characterize landscape random JSPinstances using (r, , p) notation 6.1. presenting theorems, slightdisclaimer order. Lemmas 3 4 (the proofs fairly involved) indicateNNextreme cases0jump random scheduleglobally optimal schedule via single small move. strongly believe casesalso possible go random schedule global optimum sequence many(smaller) improving moves, although proving seems difficult. Nevertheless,understood theoretical results strictly imply existence landscapeslike depicted Figure 6 (where starting points sequence twosmall improving moves leading global optimum).NTheorem 3 shows0, random JSP instance almost certainly(r, , p) landscape r grows arbitrarily slowly function , o(M N2 ),269fiStreeter & SmithNp arbitrarily close 1. words,0 landscape smallimproving move(s) property clustering global optima property. contrast,NTheorem 4 shows, random JSP instance almost surely(r, , p) landscape unless (N 2 ). Instead, landscape contains (N !) (r, 1)-valleys,Nr o(M N2 ). Thus,, landscape small improving move(s)property clustering global optima property. analytical results confirmtrend suggested Figure 7 discussed 6.4.Theorem 3. Let random N JSP instance. Let f (M ) unbounded,increasing function . fixed N > 0, holds whp (as )(r, , p) landscape r = f (M ), = N2 p = 1 .Proof. Let V set schedules L(S, Nr ) global optimum. followsCorollary 3 whp, exactness r least p, means Vprobability least p. remains show V (r, )-valley whp. Part 1definition (r, )-valley satisfied definition V . Part 2 follows Theorem1.Theorem 4. Let random N JSP instance, let random scheduleI. exists set V (I) = ni=1 Vi schedules fixed > 0, Vfollowing properties whp:1. V ;2. Vi (r, )-valley r = N 1+ = 1 [n];3. n > N !(1 );4. max{S1 ,S2 }V kS1 S2 k > (N 2 ).Proof. Let {S1 , S2 , . . . , Sn } set globally optimal schedules I, define Vi{S : L(S, N 1+ ) = Si }. Property 1 holds whp Lemma 4. Property 2 holds definitionVi .fact property 3 holds whp consequence Lemma 2. Recall LemmaN2 showed, priority rule generates optimal schedule whp,k(I, Ji ) = + k. indices assigned jobs arbitrary, Lemma 2 alsoapplies priority rule (I, Jik ) = + (k), permutation [N ].N ! possible choices . Let f number choices fail yield globallyoptimal schedule. Property 3 fail hold f N !. Lemma 1, E[f ]o(1)N !; hence f < N ! whp Markovs inequality.establish property 4, choose permutations 1 2 list elements [N ]reverse order (i.e., 1 (i) = 2 (N i) [N ]). Lemma 2, schedules S1 =S( 1 , I) S2 = S( 2 , I) globally optimal whp. disjunctive edgee = {J1 , J10 } must ~e(S1 ) 6= ~e(S2 ), hence kS1 S2 k |{{J, J 0 } : m(J1 ) =11m(J10 )}| N M2= (N 2 ), obtain expression N M2using pigeonholeprinciple.270fiThe Landscape Random Job Shop Scheduling Instances7. Quality Random Schedules7.1 Methodologysection examine quality randomly generated schedules changesfunction job:machine ratio. Specifically, various combinations N ,estimate expected value following four quantities:(A) makespan random schedule,(B) makespan locally optimal schedule obtained starting random scheduleapplying next-descent using N1 move operator,(C) makespan optimal schedule,(D) lower bound makespan optimal schedule given maximummaximum job duration maximum machine workload:Xmax max (J), max(o) .JIm[M ]oops(I):m(o)=mNconsidered experiments set R = { 17 , 16 , 51 , 14 ,values1 1 233 , 2 ,S3 , 1, 2 , 2, 3, 4, 5, 6, 7 }. consider combinations N setNrR Sr , Sr {(N, ) := r, min(N, ) 2, max(N, ) 6, N < 1000}.(N, ) S, estimate expected value (A) (resp. (B)) generating 100random N JSP instances and, instance, generating 100 random schedules(resp. local optima). estimate (D) generating 1000 random JSP instances(N, ) S. combinations (N, ) Ssmall S, also practical computeNquantity (C). Let nr = |Ssmall Sr | number combinations (N, )= r3computed (C). chose Ssmall nr 4 r 6= 2 n 3 = 3.2(N, ) Ssmall , estimate (C) using 1000 random JSP instances.7.2 ResultsFigure 8 plots mean values (A), (B), (C), respectively, mean value(D), various combinations N . data points combination NNassigned symbol based value. Top bottom error bars represent 0.750.25 quantiles (respectively) instance-specific sample means. Note widtherror bars small relative differences curves different valuesN.NExamining Figure 8, see set data points valueapproximately (though exactly) collinear. Furthermore, three graphs slope lineNformed data points= r maximized r = 1, decreases r getsaway 1 (see also Figure 9 (A)).investigate trend, performed least squares linear regression setNdata points value. slopes resulting lines shown functionNFigure 9 (A).examination Figure 9 (A), apparent271fiStreeter & Smith(A) Random schedulesMean makespan7000600050004000Ratio 1:53000Ratio 1:3Ratio 1:12000Ratio 3:11000Ratio 5:1001000 2000 3000 4000 5000 6000 7000Mean lower bound(B) Random local optimaMean makespan7000600050004000Ratio 1:53000Ratio 1:3Ratio 1:12000Ratio 3:11000Ratio 5:1001000 2000 3000 4000 5000 6000 7000Mean lower bound(C) Optimal schedulesMean makespan7000600050004000Ratio 1:53000Ratio 1:3Ratio 1:12000Ratio 3:11000Ratio 5:1001000 2000 3000 4000 5000 6000 7000Mean lower boundFigure 8: Expected makespan (A) random schedules, (B) random local optima, (C)optimal schedules vs. expected lower bound, various combinations NN(grouped symbol according). Top bottom error bars represent0.75 0.25 quantiles (respectively) instance-specific sample means.272fiThe Landscape Random Job Shop Scheduling Instances(A) Results least squares regressionSlope E[makespan]vs. E[lower bound]4Random schedulesRandom local optimaOptimal schedules3210.1110Job:machine ratio(B) Branch bound search cost2:1Num. tree nodes100003:21:12:310001:21001:33:14:15:1101:41:51:66:11:77:11050010001500log(search space size)1:71:61:51:41:31:22:31:13:22:13:14:15:16:17:12000Figure 9: (A) graphs slope least squares fits data Figure 8 (A), (B),NN(C) function(includes valuesdepicted Figure 8). (B)thgraphs number search tree nodes (90 percentile) used branchbound algorithm Brucker et al. (1994) find optimal schedule.273fiStreeter & SmithNvaluebecomes extreme (i.e., approaches either 0 ), expected makespan random schedules (resp. random local optima) comes closerexpected value lower bound makespan;difference expected makespan random schedules (resp. randomlocal optima) expected value lower bound makespan maximizedNvalue1.Nfirst two observations suggestsapproaches either 0 ,random schedule almost certainly near-optimal. 7.3 contains two theorems confirmthis.second two observations suggests expected differencemakespan random schedule makespan optimal schedule maximizedNvaluesomewhere neighborhood 1. observation particularly interestingNlight empirical fact square instances JSP (i.e.,= 1)harder solve rectangular ones (Fisher & Thompson, 1963).Figure 9 (B) graphs number search tree nodes (90th percentile) requiredbranch bound algorithm Brucker et al. (1994) optimally solve random Ninstances, function log (base 10) search space size. take sizesearch space N JSP instance number possible disjunctive graphs,namely 2N ( 2 ) . Note disjunctive graphs contain cycles thereforecorrespond feasible schedules, expression overestimates size searchspace. Data points given combination N could affordrun branch bound (i.e., combination N computed quantityN(C)). data points grouped curves according.Examining Figure 9 (B), see curves steepest ratios 23 , 1, 32 , 2,N3, curves substantially less steep extreme values177. Thus, least point view particular branch bound algorithm,random JSP instances exhibit easy-hard-easy pattern instance difficulty. discusspattern 7.4.7.3 Analysisfollowing two theorems show that,almost surely near-optimal.Napproaches either 0 , random scheduleTheorem 5. Let random N JSP instance optimal makespan `min (I)let random schedule I. fixed N > 0, holds whp (as )`(S) (1 + )`min (I).Proof. priority rule rand associates priority operation ops(I). Letsequence contain elements ops(I), sorted ascending order priority.schedule = S(rand , I) depends , N ! possible choices . Thusrand seen choosing random set N ! instance-independent priorityrules. instance-independent priority rules subject Lemma 1, randLemma 1 thus J, E[SJ ] O(N ). Thus E[`(S) `min (I)]Palso subject2J E[J ] = O(N ), `(S) `min (I) exceed `min (I) = (M ) whp Markovsinequality.274fiThe Landscape Random Job Shop Scheduling InstancesTheorem 6. Let random N JSP instance optimal makespan `min (I)let random schedule I. fixed > 0, holds whp (as N )`(S) (1 + )`min (I).Proof. See Appendix A.idea behind proof Theorem 6 following. shown Lemma 2,priority rule almost surely generates optimal schedule. relevant propertythat, operations sorted order ascending priority, numberoperations J (o) (N ). key proof Theorem 6expectation, rand shares property operations ops(I).7.4 Easy-hard-easy Pattern Instance DifficultyNNproofs Corollary 1 (resp. Lemma 2) show0 (resp.) existsimple priority rules almost surely produce optimal schedule. Moreover, Theorems5 6 show two limiting cases, even random schedule almost surelyNNmakespan close optimal. Thus,0, almostJSP instances easy.Ncontrast,1, Figure 9 (A) suggests random schedules (as well randomlocal optima) far optimal. literature JSP (as well results depictedNFigure 9 (B)) attests fact random JSP instances1 hard.Thus conjecture that, 3-SAT, typical instance difficulty JSP follows easyhard-easy pattern function certain parameter. contrast 3-SAT, easyhard-easy pattern JSP (to knowledge) associated phase transitionN(i.e., identified quantity undergoes sharp threshold1).Furthermore, although empirical results Figures 9 (A) (B) support ideatypical-case instance difficulty JSP follows easy-hard-easy pattern,Nclaim isolated particular valuepoint maximumdifficulty. shown Figure 9 (B), random JSP N JSP instances difficultNbranch bound algorithm Brucker et. al (1994)2, maytrue branch bound algorithms JSP heuristics based local search.leave task characterizing easy-hard-easy pattern precisely futurework.related work, Beck (1997) studied constraint-satisfaction (as opposed makespanminimization) version JSP, gave empirical evidence probabilityrandom JSP instance satisfiable undergoes sharp threshold function quantitycalled constrainedness instance.8. Limitations Extensionsprimary limitation work reported paper theoreticalempirical results apply random instances job shop scheduling problem.guarantee observations generalize instances drawn distributionsinteresting structure (Watson et al., 2002). difficulty extendinganalysis distributions analytical results similar ones presented275fiStreeter & Smithpaper may become much difficult derive. However, least threedistributions studied scheduling literature believedifficult adapt proofs (the conclusions may change partadaptation process).Random workflow JSP instances. workflow JSP instance, set machinespartitioned sets (say M1 , M2 , . . . , Mk ). < j, job must usemachines Mi using machines Mj . Mattfeld et al. (1999) definerandom distribution workflow JSPs generalizes natural waydistribution defined 3.3 (the difference permutations 1 , 2 , . . . , Nchosen uniformly random set permutations satisfy workflowconstraints).Random instances (permutation) flow shop scheduling problem. instanceflow shop scheduling problem (FSP) JSP instance jobs usemachines order (equivalently, FSP instance workflow JSP instancek = ). permutation flow shop problem (PFSP) special case FSPwhich, additionally, machine must process jobs order.large literature (P)FSP; Framinan et al. (2004) Hejazi Saghafian(2005) provide relevant surveys.Job-correlated machine-correlated JSP instances. job-correlated JSP instance,distribution operation durations drawn depends joboperation belongs. Similarly, machine-correlated JSP instance distributiondepends machine operation performed. Watson et al. (2002)studied job-correlated machine-correlated instances PFSP.Regarding difficulty instances drawn three distributions, computationalexperience shows (i) random workflow JSPs harder random JSPs; (ii) random PFSPs easier random JSPs; (iii) job-correlated machine-correlatedPFSPs easier random PFSPs. Extending theoretical analysisdistributions may give insight relevant differences them.8.1 Big Valley vs. Cost-Distance Correlations6, defined big valley landscape one exhibits two properties: small improving moves clustering global optima. analytical experimental resultsbased definition. Although believe definition captures properties JSPlandscapes important designers heuristics understand, properties(e.g., cost-distance correlations) likely important well. particular, maypossible algorithms exploit cost-distance correlations landscapes neithersmall improving moves clustering global optima properties.existing literature, term big valley used amorphously mean either(1) landscape like depicted Figure 1 (2) landscape exhibits high costdistance correlations. making sharper distinction two distinct concepts,improve understanding JSP landscapes well landscapescombinatorial problems.276fiThe Landscape Random Job Shop Scheduling Instances9. Conclusions9.1 Summary Experimental ResultsNEmpirically, demonstrated low values job machine ratio (), lowmakespan schedules clustered small region search space backboneNsize high.increases, low-makespan schedules become dispersed throughoutNsearch space backbone vanishes. function, smoothnesslandscape (as measured statistic called neighborhood exactness) starts smallNNNlow values(e.g.,= 15 ), relatively high1, becomes smallNNNhigh values (e.g., = 5). extremely low extremely high values,expected makespan random schedules comes close optimal schedules.quality random schedules (resp. random local optima) appears worstNvalue1.6.4 discussed implications results big valley picture JSP searchN1, concluded typical landscape described biglandscapes.NNvalley, larger values(e.g.,3) many big valleys. 7.4 discusseddata support idea JSP instance difficulty exhibits easy-hard-easyN.pattern function9.2 Summary Theoretical ResultsTable 2 shows asymptotic expected values various attributes random NNNJSP instance limiting cases0.Table 2. Attributes random JSP instances.Fixed N , Fixed , NOptimum makespanMax. job length(Corollary 1)Max. machine workload(Corollary 2)Normalized backbone size1 (Theorem 1)0 (Theorem 2)Normalized maximum distance global optimaNormalized distance randomschedule nearest global optimumRatio makespan random scheduleoptimum makespan0 (Theorem 1)(1) (Theorem 4)0 (Lemma 3)0 (Lemma 4)1 (Theorem 5)1 (Theorem 6)9.3 Rules Thumb Designing JSP HeuristicsThough claim deep insights solve random instancesJSP, results suggest two general rules thumb:NNlow (say,1 lower), algorithm attempt locatecluster global optima exploit it;277fiStreeter & SmithNNhigh (say,3) algorithm attempt isolate oneclusters global optima deal separately them.briefly discuss ideas relation two recent algorithms: backbone-guided localsearch (Zhang, 2004) i-TSAB (Nowicki & Smutnicki, 2005).9.3.1 Backbone-guided local searchSeveral recent algorithms attempt use backbone information bias move operator employed local search. example, Zhang (2004) describes approach calledbackbone-guided local search frequency attribute (e.g., assignment particular value particular variable Boolean formula) appearsrandom local optima used proxy frequency attribute appears global optima. approach improved performance WalkSAT algorithm(Selman, Kautz, & Cohen, 1994) large instances SATLIB (Hoos & Stutzle, 2000).similar algorithm successfully applied TSP (Zhang & Looks, 2005)improve performance iterated Lin-Kernighan algorithm (Martin, Otto, & Felten,1991). Zhang writes:method built upon following working hypothesis: problemwhose optimal near optimal solutions form cluster, local search algorithm reach close vicinities solutions, algorithm effectivefinding information solution structures, backbone particular.(Zhang, 2004, p. 3)Based results 4-5, working hypothesis satisfied random JSPs1 lower. seems plausible backbone-guided local search could used boostperformance early local search heuristics JSP van Laarhovenet al. (1992) Taillard (1994) (whether results would competitiverecent algorithms i-TSAB separate question).Nhypothesis typically violated random JSP instances larger values.cases makes sense attempt exploit local clustering optimalnear-optimal schedules.N9.3.2 i-TSABNowicki Smutnicki (2005) present JSP heuristic called i-TSAB employs multipleruns tabu search algorithm TSAB (Nowicki & Smutnicki, 1996). i-TSAB employs pathrelinking localize center BV [big valley], probably close global minimum(Nowicki & Smutnicki, 2005). words, i-TSAB designed based intuitivepicture depicted Figure 6 (A), inaccurate typical random JSP instancesNN3. Note although random JSP instances become easy , instancesN3 means easy, evidenced Figure 9 (B).concreteness, briefly describe i-TSAB works. Initially, i-TSAB performsnumber independent runs TSAB adds best-of-run schedule pool elitesolutions. performs additional runs TSAB uses best-of-run schedulesadditional runs replace schedules pool elite solutions. Starting points278fiThe Landscape Random Job Shop Scheduling Instancesadditional TSAB runs either (i) random elite solutions (ii) schedules obtainedperforming path relinking random pair elite solutions. Given two schedules S1S2 , path relinking uses move operator generate new schedule midway(in terms disjunctive graph distance) S1 S2 . pool elite solutionsthought cloud particles hovers search space (hopefully)converges region space containing global optimum.Nrandom JSP instances1, results consistent ideacloud elite solutions converges center big valley. random JSPNinstances3, however, cloud must either converge one many big valleysconverge all. alternate approach one imagine using multiple clouds,intention cloud specializes particular big valley. seems plausibleideas could improve performance i-TSAB random JSP instancesNlarger values.Appendix A: Additional ProofsPproofs section, define (O) oO (o), set operations.make use following inequality (Spencer, 2005).Azumas Perimetric Inequality (A.P.I.). Let X = (X1 , X2 , . . . , Xn ) vector n independent random variables. Let function f (x) take input vector x = (x1 , x2 , . . . , xn ),xi realization Xi [n], produce output real number. Suppose> 0 holds two vectors x x0 differ onecomponent,|f (x) f (x0 )| .> 0,2P X > E[X] + n exp 2 .2inequality holds P [X E[X] n].Lemma 2. Let random N JSP instance. fixed , holds whp (asN ) schedule = S( , I) propertyS(o) = + (M(o)) ops(I) .Proof. remains prove Claim 2.1 proof 4, says whp,ops2+ (I)1S(o) S(J (o)) E[S(o) S(J (o))] .2Pick arbitrary operation ops2+ (I), suppose random choices usedconstruct made following order:1. Randomly choose m1 = m(o) m2 = J (o).2. k 1 N :279fiStreeter & Smith(a) Randomly choose order job J k uses machines (if J kpart choice already made step 1).(b) Randomly choose (Jik ) [M ].Let random variable Xk denote sequence random bits used steps (a)(b) k th iteration loop. Define S(o) S(J (o)). Then, fixed choicesm1 m2 , function N independent events X1 , X2 , . . . , XN , easycheck altering particular Xi changes value 2max . Thush1)P < 12 E[o ] = P < E[o ] (N2MhNP < E[o ] 2M2exp 2(4M N2max )first step used fact (from proof 4) E[o ] = (NM1)last step used A.P.I. Taking union bound N (M 1) operationsops2+ (I) proves claim.Lemma 3. Let random N JSP instance, let random scheduleI. Let optimal schedule kS Sk minimal. Let f (M )unbounded, increasing function . fixed N , holds whp (as )kS Sk < f (M ).Proof. Let = S(0 , I). proof Corollary 1 showed J, E[SJ ] O(N 2 ).Thus holds whp SJ < log(f (M )) J. proof Theorem 5, procedureused produce mixture instance-independent priority rules, subjectLemma 1. Thus J, E[SJ ] O(N ), whp SJ < log(f (M )) J.PPLet Onear (Ji ) = {Jj0 : J 0 6= J, | i0 <i (Ji0 ) j 0 <j (Jj0 0 )| < log(f (M ))}. (Onear (Ji )set operations would scheduled near time Ji ignored factmachine may perform one operation time.) Let Enear = {e = {Ji , Jj0 }E(I) : Jj0 Onear (Ji )}. assumptions previous paragraph (eachhold whp), kS Sk |Enear |. Ji , E[|Onear (Ji )|] O(N log f (M )). ThusE kS Sk E [|Enear |] =Xoops(I)1E [|Onear (o)|] = N 2 log(f (M ))kS Sk < f (M ) whp Markovs inequality.purpose remaining proofs, convenient introduce additionalnotation. Let = (T1 , T2 , . . . , T|T | ) sequence operations. defineT(i1 ,i2 ] {Ti : i1 < i2 },T(im1 ,i2 ] {Ti T(i1 ,i2 ] : m(Ti ) = m} .280fiThe Landscape Random Job Shop Scheduling InstancesLemma 4. Let random N JSP instance, let random schedule I,let optimal schedule kS Sk minimal. fixed> 0, holds whp (as N ) kS Sk < N 1+ .Proof. Let sequence operations ops(I), sorted ascending order priorityrand (I, o) (where rand random priority rule used create S). Noteops(I) J (o) 6= , J (o) must appear . Let Ti denote ith operation.Consider schedule defined following procedure:1. S(o) ops(I).2. Q (). Let Qj denote j th operation Q.3. Let function ready(o) return true + (M(o)) + (J (o)), false otherwise.4. 1 N do:(a) ready(Ti ), set S(o) + (M(Ti )). Otherwise append Ti onto Q.(b) j 1 |Q| do:i. ready(Qj ), set S(Qj ) + (M(Qj )) remove Qj Q.5. Schedule remaining operations Q manner specified (in lastparagraph proof).construction like construction S, except manipulationsinvolving Q. purpose Q delay scheduling operation that,scheduled immediately, might produce schedule S(o) > + (M(o)). firstshow kS Sk < N 1+ whp; show optimal whp.Let PQi denote Q exists iterations step 4 performed. LetNMiterations Q. claimq(o) =i=1P |o Q | numberN|. Letting E 6= = {e E(I) : ~e(S) 6= ~e(S)},kS Sk oops(I) q(o) + (N 1)|QkS Sk = |{e E 6= : e QN = }| + |{e E 6= : e QN 6= }||{e E 6= : e QN = }| + (N 1)|QN |Psuffices show |{e E 6= : e QN = }|oops(I) q(o). see this, let=6Ne = {o1 , o2 } E e Q= . must q(o1 ) + q(o2 ) > 0. charge eoperation {o1 , o2 } inserted Q first. easy see operationcharged one edge perPiteration spends Q, establishing claim.Thus suffices show kS Sk oops(I) q(o) + (N 1)|QN | N 1+ whp.1010divide construction n = N 2 epochs, consisting N 2 +iterations step 4, to-be-specified 0 > 0. Let zj denote number iterationsstep 4 occur end j th epoch, zj = 0 j 0 convention. LetCjm T(0,z\ Qzj set operations scheduled runj]end j th epoch;281fiStreeter & SmithOnear j[n] {o T(zj1 ,zj ] : J (o) T(zj(M +2) ,zj ] } set operations whosejob-predecessor belongs nearby epoch.10[N ], P[Ti Onear ] (M + 2)N 2 + . Thus j [n], E[|Onear0T(zj1 ,zj ] |] (M + 2)N 2 . Using A.P.I. straightforward show whp,|Onear T(zj1 ,zj ] | N1+02j [n] .(9.1)claim whp, following statements hold j [n]:[Qi Onear ,(9.2)izjJ Qzj1 6= |J Qzj1 Qzj | < |J Qzj1 |zjzjMQ Q|Qzj | NJ ,(9.3)= ,1+02(9.4).(9.5)prove induction, step induction fails exponentiallysmall probability. j = 0, (9.3) (9.4) hold trivially. (9.2) trueoperations T(0,z1 ] \ Onear first operations jobs, hence cannot addedQ. (9.5) follows (9.2) (9.1).Consider case j > 0. show (9.2), let arbitrary operation T(zj1 ,zj ] \Onear .m(J (o))induction hypothesis (specifically, equation (9.4)), J (o) Cj2m(J (o))m(o)0 Cj2> Cj1 . induction hypothesis,. Thus q(o) >1+0m(o)m(J (o))m(o)m(J (o)).Cj1 Cj2T(0,zj1 ] N 2 T(0,zj2 ]Letting denote right hand side inequality, E[] =1+021+012MN, A.P.I. used show K > 0 independent N , P[ <MN000] exp( K1 N ). Thus (9.2) holds probability least 1 exp( K1 N ).show (9.3), let J J Qzj1 6= , let Ji Qzj1chosensom(J (Ji ))m(J (Ji ))m(J )minimal. J (Ji ) Cj1. Thus Ji Qzj Cj1> Cj . (9.1),1+0(9.2), induction hypothesis (equation (9.5)), |Qzj | (M + 1)N 2 . Using0technique above, show (9.3) holds probability least 1 exp( K1 N )K > 0 independent N .(9.3) implies (9.4). (9.2) (9.4) together (9.1) imply (9.5). Thus whp, (9.2)(9.5) hold j [n].(9.2) (9.4),X100Eq(o) E[|Onear |]M N 2 + 2 (M + 2)N 1+2oops(I)also282fiThe Landscape Random Job Shop Scheduling Instances0E[|QN |] E[|T(znM ,zn ] Onear |] (M + 2)N 2Psetting 0 = 3 gives kS Sk oops(I) q(o) + (N 1)|QN | N 1+ whp.remains show optimal whp. first prove following claim.Claim 4.1. non-negative integers b, probability T(a,b] containstwo operations job(ba)2N .Proof Claim 4.1. Let X denote number pairs operations T(a,b] belong1(ba)2job. P[X > 0] E[X] ba2 NN .see optimal whp, note operations scheduled prior step 5cause idle time machine, operations QN causesub-optimal. Let (m) ({o ops(I) : m(o) = m}) denote workload machinem. Let = arg maxm[M ] (m). following hold whp.set Zlast. (It holds1consists operations belonging jobs use(N 2M N 4 ,N ]whp Z Z,Z1(N N 3 ,N ]. Z containsoperation job use last, Z must contain two operationsjob. Claim 4.1, probability happens1(N 3 )2 N1 = o(1).)1N 4 (Z ) (Z ) (m) (m) 6= m. (This follows applyingCentral Limit Theorem (Z ), (m), (m)).Thus whp holds prior execution step 5, contains period length1least (Z ) N 4 operations processed Z ,0{o ops(I) : J (o) Z } = . Assuming |QN | < N 3 (holds whp), alwaysschedule operations QN guarantee `(S) = (m), implies optimal.Theorem 6. Let random N JSP instance optimal makespan `min (I)let random schedule I. fixed > 0, holds whp (as N )`(S) (1 + )`min (I).Proof. proof Lemma 4, let sequence operations ops(I), sortedascending order priority rand (I, o) (where rand random priority rule usedcreate S). Note ops(I) J (o) 6= , J (o) must appear .Let Ti denote ith operation .Rather analyze directly, analyze schedule defined following procedure:1. 0.2. 1 N do:283fiStreeter & Smith(a) Set S(Ti ) = max(t, + (J (Ti )), + (M(Ti ))) .(b) + (J (Ti )) > + (M(Ti )), set = maxi0 + (Ti0 ).procedure identical one used construct S, except that, wheneveroperation Ti assigned start time S(Ti ) > + (M(Ti )), procedure inserts artificialdelays schedule order re-synchronize machines. , clear`(S) `(S). Thus, suffices show `(S) (1 + )`min (I) whp.divide construction n epochs, update (in step 2(b)) defines beginning new epoch. Let zi number operations scheduledend ith epoch, z0 = 0 convention. Let ti = maxi0 zi + (oi0 ) (updated)P+0value end ofPthe ith epoch. Definem=1Ptni maxi0 <i,m(Ti0 )=m (Ti ).n`(S) `min (I) i=1 , suffices show i=1 `min (I) whp.P2Let P= [n], let L = {i : zi zi1 N 7 }. first consider iL ;consider iI\L .2Let i1 i2 arbitrary integers 0 i1 , i2 N i2 i1 N 7 . Leti1. , function outcome= (T(im1 ,i2 ] ). E[ ] = i2Mi2 i1 events (namely, definition jobs {J : J T(i1 ,i2 ] 6= }),alters value max . follows A.P.I.!0N 20P[| E[ ]| > Ni2 i1 ] 2 exp 22max0 > 0. Thus, holds whp | E[ ]| N i2 i1 possible choices0i1 i2 . particular, whp2M N zi zi1 L, impliespP5 P620072M N N 7 = 2M N 7 + .iL NiLPconsideriI\L . shown proof Lemma 4 (Claim 4.1),non-negative integers b probability T(a,b] contains two operations227job (ba)N . Thus probability arbitrary subsequence size N437contains two operations job N , E[|I \ L|] N 7 . ClearlyP26max N 7 \ L, E[ iI\L ] O(N 7 ).PP6600Thus E[ iI ] O(N 7 + ) 0 > 0, iI N 7 +2 whp, easysee `min (I) N2 whp.ReferencesAchlioptas, D., & Peres, Y. (2004). threshold random k-SAT 2k log 2 O(k).Journal AMS, 17, 947973.Balas, E. (1969). Machine sequencing via disjunctive graphs: implicit enumerationalgorithm. Operations Research, 17, 110.Beasley, J. E. (1990). OR-library: Distributing test problems electronic mail. JournalOperational Research Society, 41(11), 10691072.284fiThe Landscape Random Job Shop Scheduling InstancesBeck, J. C., & Jackson, W. K. (1997). Constrainedness phase transition jobshop scheduling. Tech. rep. CMPT97-21, School Computing Science, Simon FraserUniversity.Boese, K. D., Kahng, A. B., & Muddu, S. (1994). new adaptive multi-start techniquecombinatorial global optimizations. Operations Research Letters, 16, 101113.Brucker, P., Jurisch, B., & Sievers, B. (1992). Job-shop (C-codes). European Journal Operational Research, 57, 132133. Code available http://optimierung.mathematik.uni-kl.de/ORSEP/contents.html.Brucker, P., Jurisch, B., & Sievers, B. (1994). branch bound algorithmjob-shop scheduling problem. Discrete Applied Mathematics, 49(1-3), 107127.Cheeseman, P., Kanefsky, B., & Taylor, W. M. (1991). really hard problems are.Proceedings Twelfth International Joint Conference Artificial Intelligence,IJCAI-91, Sidney, Australia, pp. 331337.Fisher, H., & Thompson, G. L. (1963). Probabilistic learning combinations local job-shopscheduling rules. Muth, J. F., & Thompson, G. L. (Eds.), Industrial Scheduling,pp. 225251. Prentice-Hall, Englewood Cliffs, NJ.Framinan, J. M., Gupta, J. N. D., & Leisten, R. (2004). review classificationheuristics permutation flow-shop scheduling makespan objective. JournalOperational Research Society, 55(12), 12431255.French, S. (1982). Sequencing Scheduling: Introduction MathematicsJob-Shop. Wiley, New York.Glover, F., & Laguna, M. (1997). Tabu Search. Kluwer Academic Publishers, Boston, MA.Hejazi, S. R., & Saghafian, S. (2005). Flowshop-scheduling problems makespan criterion: review. International Journal Production Research, 43(14), 28952929.Hoos, H. H., & Stutzle, T. (2000). SATLIB: online resource research SAT.Gent, I. P., v. Maaren, H., & Walsh, T. (Eds.), Proceedings SAT 2000, pp. 283292.SATLIB available online www.satlib.org.Jain, A., & Meeran, S. (1998). state-of-the-art review job-shop scheduling techniques.Tech. rep., Department Applied Physics, Electronic Mechanical Engineering,University Dundee, Dundee, Scotland.Jones, A., & Rabelo, L. C. (1998). Survey job shop scheduling techniques. Tech. rep.,National Institute Standards Technology, Gaithersburg, MD.Kim, Y.-H., & Moon, B.-R. (2004). Investigation fitness landscapes graph bipartitioning: empirical study. Journal Heuristics, 10, 111133.Lourenco, H., Martin, O., & Stutzle, T. (2003). Iterated local search. Glover, F., &Kochenberger, G. (Eds.), Handbook Metaheuristics. Kluwer Academic Publishers,Boston, MA.Mammen, D. L., & Hogg, T. (1997). new look easy-hard-easy pattern combinatorial search difficulty. Journal Artificial Intelligence Research, 7, 4766.285fiStreeter & SmithMartin, O. C., Otto, S. W., & Felten, E. W. (1991). Large-step Markov chainstraveling salesman problem. Complex Systems, 5, 299326.Martin, O. C., Monasson, R., & Zecchina, R. (2001). Statistical mechanics methodsphase transitions combinatorial problems. Theoretical Computer Science, 265(1-2),367.Mattfeld, D. C. (1996). Evolutionary Search Job Shop: Investigations GeneticAlgorithms Production Scheduling. Physica-Verlag, Heidelberg.Mattfeld, D. C., Bierwirth, C., & Kopfer, H. (1999). search space analysis job shopscheduling problem. Annals Operations Research, 86, 441453.Mezard, M., & Parisi, G. (1986). replica analysis traveling salesman problem.Journal de Physique, 47, 12851296.Monasson, R., Zecchina, R., Kirkpatrick, S., Selman, B., & Troyansky, L. (1999). Determining computational complexity characteristic phase transitions. Nature, 400,133137.Nowicki, E., & Smutnicki, C. (1996). fast taboo search algorithm job-shop problem.Management Science, 42(6), 797813.Nowicki, E., & Smutnicki, C. (2001). new ideas TS job shop scheduling. Tech.rep. 50/2001, University Wroclaw.Nowicki, E., & Smutnicki, C. (2005). advanced tabu search algorithm job shopproblem. Journal Scheduling, 8, 145159.Reeves, C. R., & Yamada, T. (1998). Genetic algorithms, path relinking, flowshopsequencing problem. Evolutionary Computation, 6, 4560.Roy, B., & Sussmann, B. (1964). Les problemes dordonnancement avec contraintes disjonctives. Note D.S. no. 9 bis, SEMA, Paris, France, Decembre.Selman, B., Kautz, H., & Cohen, B. (1994). Noise strategies local search. ProceedingsAAAI-94, pp. 337343.Slaney, J., & Walsh, T. (2001). Backbones optimization approximation. Proceedings 17th International Joint Conference Artificial Intelligence (IJCAI2001), pp. 254259.Spencer, J. (2005). Modern probabilistic methods combinatorics. http://www.cs.nyu.edu/cs/faculty/spencer/papers/stirlingtalk.pdf.Streeter, M. J., & Smith, S. F. (2005a). Characterizing distribution low-makespanschedules job shop scheduling problem. Biundo, S., Myers, K., & Rajan, K.(Eds.), Proceedings ICAPS 2005, pp. 6170.Streeter, M. J., & Smith, S. F. (2005b). Supplemental material ICAPS 2005 paperCharacterizing distribution low-makespan schedules job shop schedulingproblem. http://www.cs.cmu.edu/~matts/icaps_2005.Taillard, E. (1993). Benchmarks basic scheduling problems. European Journal Operational Research, 64, 278285.286fiThe Landscape Random Job Shop Scheduling InstancesTaillard, E. (1994). Parallel taboo search techniques job shop scheduling problem.ORSA Journal Computing, 6, 108117.van Laarhoven, P., Aarts, E., & Lenstra, J. (1992). Job shop scheduling simulatedannealing. Operations Research, 40(1), 113125.Watson, J.-P., Barbulescu, L., Whitley, L. D., & Howe, A. (2002). Contrasting structuredrandom permutation flow-shop scheduling problems: search-space topologyalgorithm performance. INFORMS Journal Computing, 14(2), 98123.Watson, J.-P., Beck, J. C., Howe, A. E., & Whitley, L. D. (2001). Toward understandinglocal search cost job-shop scheduling. Cesta, A. (Ed.), Proceedings SixthEuropean Conference Planning.Yokoo, M. (1997). adding constraints makes problem easier hill-climbingalgorithms: Analyzing landscapes CSPs. Principles Practice ConstraintProgramming, pp. 356370.Zhang, W. (2004). Configuartion landscape analysis backbone guided local search: PartI: Satisfiability maximum satisfiability. Artificial Intelligence, 158(1), 126.Zhang, W., & Looks, M. (2005). novel local search algorithm traveling salesman problem exploits backbones. Proceedings 19th International JointConference Artificial Intelligence, pp. 343350.287fiJournal Artificial Intelligence Research 26 (2006) 134Submitted 11/05; published 05/06Logic Reasoning EvidenceJoseph Y. Halpernhalpern@cs.cornell.eduCornell University, Ithaca, NY 14853 USARiccardo Pucellariccardo@ccs.neu.eduNortheastern University, Boston, 02115 USAAbstractintroduce logic reasoning evidence essentially views evidencefunction prior beliefs (before making observation) posterior beliefs (aftermaking observation). provide sound complete axiomatization logic,consider complexity decision problem. Although reasoning logicmainly propositional, allow variables representing numbers quantificationthem. expressive power seems necessary capture important properties evidence.1. IntroductionConsider following situation, essentially taken Halpern Tuttle (1993)Fagin Halpern (1994). coin tossed, either fair double-headed. coinlands heads. likely coin double-headed? coin tossed20 times lands heads time? Intuitively, much likely coindouble-headed latter case former. likelihoodmeasured? cannot simply compute probability coin double-headed;assigning probability event requires prior probability coindouble-headed. example, coin chosen random barrelone billion fair coins one double-headed coin, still overwhelmingly likelycoin fair, sequence 20 heads unlucky. However, problemstatement, prior probability given. show given prior probabilitycoin double-headed increases significantly result seeing 20 heads. But,intuitively, seems able say seeing 20 heads row providesgreat deal evidence favor coin double-headed without invoking prior.great deal work trying make intuition precise,review.main feature coin example involves combination probabilistic outcomes (e.g., coin tosses) nonprobabilistic outcomes (e.g., choicecoin). great deal work reasoning systems combineprobabilistic nondeterministic choices; see, example, Vardi (1985), Fischer Zuck(1988), Halpern, Moses, Tuttle (1988), Halpern Tuttle (1993), de Alfaro (1998),He, Seidel, McIver (1997). However, observations suggest attemptformally analyze situation one frameworks, essentially permitmodeling probabilities, able directly capture intuitionincreasing likelihood. see plays out, consider formal analysis situationHalpern-Tuttle (1993) framework. Suppose Alice nonprobabilistically choosesc2006AI Access Foundation. rights reserved.fiHalpern & Pucellaone two coins: fair coin probability 1/2 landing heads, double-headed coinprobability 1 landing heads. Alice tosses coin repeatedly. Let k formulastating: kth coin toss lands heads. probability k according Bob,know coin Alice chose, even probability Alices choice?According Halpern-Tuttle framework, modeled consideringset runs describing states system point time, partitioningset two subsets, one coin used. set runs fair coinused, probability k 1/2; set runs double-headed coinused, probability k 1. setting, conclusion drawn(PrB (k ) = 1/2) (PrB (k ) = 1). (This course probability Bobs pointview; Alice presumably knows coin using.) Intuitively, seems reasonable:fair coin chosen, probability kth coin toss lands heads, accordingBob, 1/2; double-headed coin chosen, probability 1. Since Bobknow coins used, said.suppose that, 101st coin toss, Bob learns result first 100tosses. Suppose, moreover, landed heads. probability101st coin toss lands heads? analysis, still either 1/2 1, dependingcoin used.hardly useful. make matters worse, matter many coin tosses Bobwitnesses, probability next toss lands heads remains unchanged.answer misses important information. fact first 100 cointosses heads strong evidence coin fact double-headed. Indeed,straightforward computation using Bayes Rule shows prior probabilitycoin double-headed , observing 100 tosses land heads,probability coin double-headed becomes+ 2100 (1 )=2100.2100 + (1 )However, note possible determine posterior probability coindouble-headed (or 101st coin toss heads) without prior probability .all, Alice chooses double-headed coin probability 10100 , stilloverwhelmingly likely coin used fact fair, Bob unluckysee unrepresentative sequence coin tosses.None frameworks described reasoning nondeterminism probability takes issue evidence account. hand, evidencediscussed extensively philosophical literature. Much discussion occursphilosophy science, specifically confirmation theory, concern historically assess support evidence obtained experimentation lends variousscientific theories (Carnap, 1962; Popper, 1959; Good, 1950; Milne, 1996). (Kyburg (1983)provides good overview literature.)paper, introduce logic reasoning evidence. logic extendslogic defined Fagin, Halpern Megiddo (1990) (FHM on) reasoninglikelihood expressed either probability belief. logic first-order quantificationreals (so includes theory real closed fields), FHM logic,reasons shortly become clear. add observations states, provide2fiA Logic Reasoning Evidenceadditional operator talk evidence provided particular observations. alsorefine language talk prior probability hypotheses posteriorprobability hypotheses, taking account observation states. lets uswrite formulas talk relationship prior probabilities, posteriorprobabilities, evidence provided observations.provide sound complete axiomatization logic. obtainaxiomatization, seem need first-order quantification fundamental way. Roughlyspeaking, ensuring evidence operator appropriate propertiesrequires us assert existence suitable probability measures. seem possible without existential quantification. Finally, consider complexitysatisfiability problem. complexity problem full language requires exponentialspace, since incorporates theory real closed fields, exponential-spacelower bound known (Ben-Or, Kozen, & Reif, 1986). However, show satisfiability problem propositional fragment language, still strong enoughallow us express many properties interest, decidable polynomial space.reasonable ask point bother logic evidence.claim many decisions practical applications made basis evidence.take example security, consider enforcement mechanism used detectreact intrusions computer system. enforcement mechanism analyzesbehavior users attempts recognize intruders. Clearly mechanism wantsmake sensible decisions based observations user behaviors. this?One way think enforcement mechanism accumulating evidencehypothesis user intruder. accumulated evidence usedbasis decision quarantine user. context, clearreasonable way assign prior probability whether user intruder. wantspecify behavior systems prove meet specifications,helpful logic allows us this. believe logic proposefirst so.rest paper organized follows. next section, formalize notionevidence captures intuitions outlined above. Section 3, introduce logicreasoning evidence. Section 4, present axiomatization logicshow sound complete respect intended models. Section 5,discuss complexity decision problem logic. Section 6, examinealternatives definition weight evidence use. ease exposition,paper, consider system two time points:observation. Section 7, extend work dynamic systems,multiple pieces evidence, obtained different points time. proofs technicalresults found appendix.2. Measures Confirmation Evidenceorder develop logic reasoning evidence, need first formalizeappropriate notion evidence. section, review various formalizationsliterature, discuss formalization use. Evidence studied depthphilosophical literature, name confirmation theory. Confirmation theory aims3fiHalpern & Pucelladetermining measuring support piece evidence provides hypothesis.mentioned introduction, many different measures confirmation proposedliterature. Typically, proposal judged degree satisfiesvarious properties considered appropriate confirmation. example, mayrequired piece evidence e confirms hypothesis h e makes hprobable. desire enter debate class measures confirmationappropriate. purposes, confirmation functions inappropriate:assume given prior set hypotheses observations.marginalization, also prior hypotheses, exactly informationwant assume. One exception measures evidence uselog-likelihood ratio. case, rather prior hypotheses observations,suffices probability h observations hypothesis h: intuitively,h (ob) probability observing ob h holds. Given observation ob, degreeconfirmation provides hypothesis hh (ob),l(ob, h) = logh (ob)h represents hypothesis h (recall approach appliestwo hypotheses). Thus, degree confirmation ratiotwo probabilities. use logarithm critical here. Using ensureslikelihood positive observation confirms hypothesis. approachadvocated Good (1950, 1960), among others.1One problem log-likelihood ratio measure l definedused reason evidence discriminating two competing hypotheses,namely hypothesis h holding hypothesis h holding. would likemeasure confirmation along lines log-likelihood ratio measure,handle multiple competing hypotheses. number generalizations,example, Pearl (1988) Chan Darwiche (2005). focus generalization given Shafer (1982) context Dempster-Shafer theory evidencebased belief functions (Shafer, 1976); studied Walley (1987).description taken mostly Halpern Fagin (1992). measureconfirmation number nice properties take advantage, much workpresented paper adapted different measures confirmation.start finite set H mutually exclusive exhaustive hypotheses; thus,exactly one hypothesis holds given time. Let set possible observations(or pieces evidence). simplicity, assume finite. case loglikelihood, also assume that, hypotheses h H, probability measureh h (ob) probability ob hypothesis h holds. Furthermore,assume observations relevant hypotheses: every observationob O, must hypothesis h h (ob) > 0. (The measures h oftencalled likelihood functions literature.) define evidence space (over H O)1. Another related approach, Bayes factor approach, based taking ratio odds ratherlikelihoods (Good, 1950; Jeffrey, 1992). remark literature, confirmation usually takenrespect background knowledge. ease exposition, ignore background knowledgehere, although easily incorporated framework present.4fiA Logic Reasoning Evidencetuple E = (H, O, ), function assigns every hypothesis h Hlikelihood function (h) = h . (For simplicity, usually write h (h),function clear context.)Given evidence space E, define weight observation ob lends hypothesis h, written (ob, h),h (ob).h0 H h0 (ob)(ob, h) = P(1)measure always lies 0 1; intuitively, (ob, h) = 1, ob fullyconfirms h (i.e., h certainly true ob observed), (ob, h) = 0, obdisconfirms h (i.e.,P h certainly falseP ob observed). Moreover, fixed observationob(ob)>0,hhHhH (ob, h) = 1, thus weight evidencelooks like probability measure ob. useful technicalconsequences, one interpret probability measure. Roughly speaking,weight (ob, h) likelihood h right hypothesis light observationob.2 advantages known measures confirmation (a)applicable given prior probability distribution hypotheses, (b)applicable two competing hypotheses, (c) fairlyintuitive probabilistic interpretation.important problem statistical inference (Casella & Berger, 2001) choosingbest parameter (i.e., hypothesis) explains observed data. priorparameters, best parameter typically taken one maximizeslikelihood data given parameter. Since normalized likelihoodfunction, parameter maximizes likelihood also maximize . Thus,interested maximizing likelihood, need normalize evidencedo. return issue normalization Section 6.3Note H = {h1 , h2 }, sense generalizes log-likelihood ratiomeasure. precisely, fixed observation ob, (ob, ) induces relative orderhypotheses l(ob, ), fixed hypothesis h, (, h) induces relativeorder observations l(, h).Proposition 2.1: ob, (ob, hi ) (ob, h3i ) l(ob, hi )l(ob, h3i ), = 1, 2, h, ob, ob 0 , (ob, h) (ob 0 , h)l(ob, h) l(ob 0 , h).2. could taken log ratio make parallel log-likelihood ratio l defined earlier,technical advantages weight evidence number 0 1.3. Another representation evidence similar characteristics Shafers original representation evidence via belief functions (Shafer, 1976), definedwES (ob, h) =h (ob).maxhH h (ob)measure known statistical hypothesis testing generalized likelihood-ratio statistic.another generalization log-likelihood ratio measure l. main difference wESbehave one considers combination evidence, discuss later section.Walley (1987) Halpern Fagin (1992) point out, gives intuitive results case.remark parameter (hypothesis) maximized likelihood also maximizes wES , wESalso used statistical inference.5fiHalpern & PucellaAlthough (ob, ) behaves like probability measure hypotheses every observation ob, one think probability; weight evidence combinedhypothesis, instance, generally sum weights individual hypotheses (Halpern & Pucella, 2005a). Rather, (ob, ) encoding evidence.evidence? Halpern Fagin (1992) suggested evidence thoughtfunction mapping prior probability hypotheses posterior probability, basedobservation made. precise sense viewed functionmaps prior probability 0 hypotheses H posterior probability ob basedobserving ob, applying Dempsters Rule Combination (Shafer, 1976). is,ob = 0 (ob, ),(2)combines two probability distributions H get new probability distributionH defined follows:P1 (h)2 (h)(1 2 )(H) = PhH.hH 1 (h)2 (h)(Dempsters Rule Combination used combine belief functions. definitioncomplicated considering arbitrary belief functions, special casebelief functions fact probability measures, takes form give here.)Bayes Rule standard way updating prior probability based observation,applicable joint probability distribution hypothesesobservations (or, equivalently, prior hypotheses together likelihoodfunctions h h H), something want assume given.particular, willing assume given likelihood functions,willing assume given prior hypotheses. Dempsters RuleCombination essentially simulates effects Bayes Rule. relationshipDempsters Rule Bayes Rule made precise following well-known theorem.Proposition 2.2: (Halpern & Fagin, 1992) Let E = (H, O, ) evidence space. SupposeP probability H P (H {ob} | {h} O) = h (ob) h Hob O. Let 0 probability H induced marginalizing P ; is, 0 (h) =P ({h} O). ob O, let ob = 0 (ob, ). ob (h) = P ({h} | H {ob}).words, joint probability hypotheses observations, Dempsters Rule Combination gives us result straightforwardapplication Bayes Rule.Example 2.3: get feel measure evidence used, considervariation two-coins example introduction. Assume coin chosenAlice either double-headed fair, consider sequences hundred tosses coin.Let = {m : 0 100} (the number heads observed), let H = {F, D}, Fcoin fair, coin double-headed. probability spaces associatedhypotheses generated following probabilities simple observations m:1 1001 = 100F (m) = 100(m) =0 otherwise.26fiA Logic Reasoning Evidence(We extend additivity whole set O.) Take E = (H, O, ), (F ) = F(D) = . observation 6= 100, weight favor F given1 100(m, F ) =021001 100+ 2100= 1,means support unconditionally provided F ; indeed,sequence tosses cannot appear double-headed coin. Thus, 6= 100, get0= 0.(m, D) =1 1000 + 2100happens hundred coin tosses heads? straightforward check1112100100w(100,D)=;(100, F ) = 2 1 ==E11 + 21001 + 21001 + 21001 + 2100time overwhelmingly evidence favor F .Note assumed prior probability. Thus, cannot talkprobability coin fair double-headed. quantitative assessmentevidence favor one hypotheses. However, assume prior probabilitycoin fair heads observed 100 tosses, probabilitycoin fair 1 6= 100; = 100 then, applying rule combination,posterior probability coin fair /( + (1 )2100 ).ucharacterize weight functions using small number properties? precisely,given sets H O, function f H [0, 1], properties fensure likelihood functions f = E = (H, O, )?saw earlier, fixed observation ob, f essentially acts like probability measureH. However, sufficient guarantee f weight function. Considerfollowing example, = {ob 1 , ob 2 } H = {h1 , h2 , h3 }:f (ob 1 , h1 ) = 1/4f (ob 1 , h2 ) = 1/4f (ob 1 , h3 ) = 1/2f (ob 2 , h1 ) = 1/4f (ob 2 , h2 ) = 1/2f (ob 2 , h3 ) = 1/4.straightforward check f (ob 1 , ) f (ob 2 , ) probability measures H,evidence space E = (H, O, ) f = . Indeed, assumeh1 , h2 , h3 . definition weight evidence, fact fweight evidence, get following system equations:h1 (ob 1 )h1 (ob 1 )+h2 (ob 1 )+h3 (ob 1 )h2 (ob 1 )h1 (ob 1 )+h2 (ob 1 )+h3 (ob 1 )h3 (ob 1 )h1 (ob 1 )+h2 (ob 1 )+h3 (ob 1 )h1 (ob 2 )h1 (ob 2 )+h2 (ob 2 )+h3 (ob 2 )h2 (ob 2 )h1 (ob 2 )+h2 (ob 2 )+h3 (ob 2 )h3 (ob 2 )h1 (ob 2 )+h2 (ob 2 )+h3 (ob 2 )= 1/4= 1/4= 1/2= 1/4= 1/2= 1/4.immediate exist 1 2 hi (ob j ) = j f (ob j , hi ),= 1, 2, 3. Indeed, j = h1 (ob j ) + h2 (ob j ) + h3 (ob j ), j = 1, 2. Moreover, since hiprobability measure, musthi (ob 1 ) + hi (ob 2 ) = 1 f (ob 1 , hi ) + 2 f (ob 2 , hi ) = 1,7fiHalpern & Pucella= 1, 2, 3. Thus,1 /4 + 2 /4 = 1 /4 + 2 /2 = 1 /2 + 4 /4 = 1.constraints easily seen unsatisfiable.argument generalizes arbitrary functions f ; thus, necessary condition fweight function exists observation ob h (ob ) =f (ob , h) hypothesis h probability measure, is, 1 f (ob 1 , h) + +k f (ob k , h) = 1. fact, combined constraint f (ob, ) probabilitymeasure fixed ob, condition turns sufficient, following theoremestablishes.Theorem 2.4: Let H = {h1 , . . . , hm } = {ob 1 , . . . , ob n }, let f real-valuedfunction domain H f (ob, h) [0, 1]. exists evidence spaceE = (H, O, ) f = f satisfies following properties:WF1. every ob O, f (ob, ) probability measure H.PWF2. exists x1 , . . . , xn > 0 that, h H, ni=1 f (ob , h)xi = 1.characterization fundamental completeness axiomatizationlogic introduce next section. characterization complicated factweight evidence essentially normalized likelihood: likelihoodobservation given particular hypothesis normalized using sum likelihoodsobservation, possible hypotheses. One consequence this, alreadymentioned above, weight evidence always 0 1, superficiallybehaves like probability measure. Section 6, examine issue normalizationcarefully, describe changes framework would occurtake unnormalized likelihoods weight evidence.Let E = (H, O, ) evidence space. Let set sequences observationshob 1 , . . . , ob k O.4 Assume observations independent, is, basichypothesis h, take h (hob 1 , . . . , ob k i), probability observing particular sequenceobservations given h, h (ob 1 ) h (ob k ), product probability makingobservation sequence. Let E = (H, , ). assumption, wellknown Dempsters Rule Combination used combine evidence setting;is,(hob 1 , . . . , ob k i, ) = (ob 1 , ) (ob k , )(Halpern & Fagin, 1992, Theorem 4.3). easy exercise check weightprovided sequence observations hob 1 , . . . , ob k expressed termsweight individual observations:(ob 1 , h) (ob k , h).1 0k 0h0 H (ob , h ) (ob , h )(hob 1 , . . . , ob k i, h) = P(3)4. use superscript rather subscripts index observations sequence observationsconfused basic observations ob 1 , . . . , ob n O.8fiA Logic Reasoning Evidencelet 0 prior probability hypotheses, hob 1 ,...,ob k probabilityhypotheses observing ob 1 , . . . , ob k , verifyhob1 ,...,ob k = 0 (hob 1 , . . . , ob k i, ).Example 2.5: Consider variant Example 2.3, take coin tosses individual observations, rather number heads turn one hundred cointosses. before, assume coin chosen Alice either double-headed fair. Let= {H, }, result individual coin toss, H coin landed headscoin landed tails. Let H = {F, D}, F coin fair,coin double-headed. Let E = (H, , ). probability measure h associatedhypothesis h generated following probabilities simple observations:F (H) =12(H) = 1.Thus, example, F (hH, H, T, Hi) = 1/16, (hH, H, Hi) = 1, H (hH, H, T, Hi) =0.easily verify results similar obtained Example 2.3.instance, weight observing favor F given(T, F ) =120+12= 1,indicates observing provides unconditional support F ; doubleheaded coin cannot land tails.sequences observations? weight provided sequence hob 1 , . . . , ob khypothesis h given Equation (3). Thus, H = hH, . . . , Hi, sequence hundredcoin tosses, check(H, F ) =1121001+ 2100=11 + 2100(H, D) =Unsurprisingly, result Example 2.3.21001.=11 + 21001 + 2100u3. Reasoning Evidenceintroduce logic Lfo -ev reasoning evidence, inspired logic introducedFHM reasoning probability. logic lets us reason weight evidenceobservations hypotheses; moreover, able talk relationshipprior probabilities, evidence, posterior probabilities, provide operators reasonprior posterior probabilities hypotheses. remarksomewhat agnostic whether priors exist given (orknown) whether prior exist all. beyond scope paperenter debate whether always appropriate assume existence prior.Although definition evidence makes sense even priors exist, logicimplicitly assumes priors (although may known), since provide9fiHalpern & Pucellaoperators reasoning prior. make use operatorsexamples below. However, fragment logic use operatorsappropriate prior-free reasoning.logic propositional features first-order features. take probability propositions weight evidence observations hypotheses, viewprobability evidence propositions, allow first-order quantification numerical quantities, probabilities evidence. logic essentially considers twotime periods, thought time observation madetime observation made. section, assume exactly one observationmade. (We consider sequences observations Section 7.) Thus, talkprobability formula observation made, denoted Pr0 (), probabilityobservation, denoted Pr(), evidence provided observation obhypothesis h, denoted w(ob, h). course, want able use logicrelate quantities.Formally, start two finite sets primitive propositions, h = {h1 , . . . , hnh }representing hypotheses, = {ob 1 , . . . , ob } representing observations. LetLh (h ) propositional sublanguage hypothesis formulas obtained taking primitive propositions h closing negation conjunction; use rangeformulas sublanguage.basic term form Pr0 (), Pr(), w(ob, h), hypothesis formula,ob observation, h hypothesis. said, interpret Pr0 ()prior probability , Pr() posterior probability , w(ob, h) weightevidence observation ob hypothesis h. may seem strange allow languagetalk prior probability hypotheses, although saidwant assume prior known. could, course, simplify syntaxinclude formulas form Pr0 () Pr(). advantagethat, even prior known, given view evidence function priorsposteriors, make statements prior probability h 2/3, obobserved, weight evidence ob h 3/4, posterior probability h6/7;Pr0 (h) = 1/2 ob w(ob, h) = 3/4 Pr(h) = 6/7.polynomial term form t1 + + tn , term ti product integers,basic terms, variables (which range reals). polynomial inequality formulaform p c, p polynomial term c integer. Let Lfo -ev (h , )language obtained starting primitive propositions hpolynomial inequality formulas, closing conjunction, negation, firstorder quantification. Let true abbreviation arbitrary propositional tautologyinvolving hypotheses, h1 h1 ; let false abbreviation true.definition, true false considered part sublanguage Lh (h ).clear allow integer coefficients appear polynomialterms, fact express polynomial terms rational coefficients crossmultiplying.instance, 31 Pr() + 12 Pr(0 ) 1 represented polynomial inequality formula2Pr() + 3Pr(0 ) 6. difficulty giving semantics polynomial termsuse arbitrary real coefficients, need restriction integers order make use10fiA Logic Reasoning Evidenceresults theory real closed fields axiomatization Section 4complexity results Section 5.use obvious abbreviations needed, ( ),, x x(), Pr() Pr() c Pr() + (1)Pr() c, Pr() Pr()Pr() Pr() 0, Pr() c Pr() c, Pr() < c (Pr() c), Pr() = c(Pr() c) (Pr() c) (and analogous abbreviations inequalities involving Pr0w).Example 3.1: Consider situation given Example 2.3. Let , observations,consist primitive propositions form heads[m], integer 0100, indicating heads 100 tosses appeared. Let h consist twoprimitive propositions fair doubleheaded. computations Example 2.3written follows:w(heads[100], fair) = 1/(1 + 2100 ) w(heads[100], doubleheaded) = 2100 /(1 + 2100 ).also capture fact weight evidence observation maps priorprobability posterior probability Dempsters Rule Combination. example,following formula captures update prior probability hypothesis fairupon observation hundred coin tosses landing heads:Pr0 (fair) = w(heads[100], fair) = 1/(1 + 2100 ) Pr(fair) = /( + (1 )2100 ).develop deductive system derive conclusions next section.uconsider semantics. formula interpreted world specifieshypothesis true observation made, well evidence space interpretweight evidence observations probability distribution hypothesesinterpret prior probabilities talk updating based evidence. (We needinclude posterior probability distribution, since computed priorweights evidence using Equation (2).) evidential world tuple w = (h, ob, , E),h hypothesis, ob observation, probability distribution h , Eevidence space h .interpret propositional formulas Lh (h ), associate hypothesis formulaset [[]] hypotheses, induction structure :[[h]] = {h}[[]] = h [[]][[1 2 ]] = [[1 ]] [[2 ]].interpret first-order formulas may contain variables, need valuation vassigns real number every variable. Given evidential world w = (h, ob, , E)valuation v, assign polynomial term p real number [p]w,v straightforward way:[x]w,v = v(x)[a]w,v =[Pr0 ()]w,v = ([[]])11fiHalpern & Pucella[Pr()]w,v = ( (ob, ))([[]])[w(ob 0 , h0 )]w,v = (ob 0 , h0 )[t1 t2 ]w,v = [t1 ]w,v [t2 ]w,v[p1 + p2 ]w,v = [p1 ]w,v + [p2 ]w,v .Note that, interpret Pr(), posterior probability observed ob (theobservation world w), use Equation (2), says posterior obtainedcombining prior probability (ob, ).define means formula true (or satisfied) evidential worldw valuation v, written (w, v) |= , follows:(w, v) |= h w = (h, ob, , E) ob, , E(w, v) |= ob w = (h, ob, , E) h, , E(w, v) |= (w, v) 6|=(w, v) |= (w, v) |= (w, v) |=(w, v) |= p c [p]w,v c(w, v) |= x (w, v 0 ) |= v 0 agree v variables x.(w, v) |= true v, write simply w |= . easy checkclosed formula (that is, one free variables), (w, v) |=(w, v 0 ) |= , v, v 0 . Therefore, given closed formula , (M, w, v) |= , factw |= . typically concerned closed formulas. Finally, w |=evidential worlds w, write |= say valid. next section,characterize axiomatically valid formulas logic.Example 3.2: following formula valid, is, true evidential worlds:|= (w(ob, h1 ) = 2/3 w(ob, h2 ) = 1/3) (Pr0 (h1 ) 1/100 ob) Pr(h1 ) 2/101.words, evidential worlds weight evidence observation obhypothesis h1 2/3 weight evidence observation ob hypothesis h2 1/3,must case prior probability h1 least 1/100 ob actuallyobserved, posterior probability h1 least 2/101. shows extentreason evidence independently prior probabilities.ulogic imposes restriction prior probabilities used models.implies, instance, formulafair Pr0 (fair) = 0satisfiable: exists evidential world w formula true w.words, consistent hypothesis true, despite prior probabilitytrue 0. simple matter impose restriction modelsh true world, (h) > 0 prior world.12fiA Logic Reasoning Evidenceconclude section remarks concerning semantic model. semantic model implicitly assumes prior probability known likelihoodfunctions (i.e., measures h ) known. course, many situationsuncertainty both. Indeed, motivation focusing evidence precisely dealsituations prior known. Handling uncertainty prior easyframework, since notion evidence independent prior hypotheses.straightforward extend model allowing set possible worlds, differentprior each, using evidence space them. extendlogic knowledge operator, statement known true trueworlds. allows us make statements like know prior hypothesish . Since observation ob provides evidence 3/4 h, knowposterior h given ob (3)/(2 + 1) (3)/(2 + 1).Dealing uncertainty likelihood functions somewhat subtle.understand issue, suppose one two coins chosen tossed. biascoin 1 (i.e., probability coin 1 lands heads) 2/3 3/4; bias coin2 1/4 1/3. uncertainty probability coin 1picked (this uncertainty prior) uncertainty biascoin (this uncertainty likelihood functions). problem that,deal this, must consider possible worlds possibly different evidencespace world. obvious define weight evidence. exploreissue detail companion paper (Halpern & Pucella, 2005a).4. Axiomatizing Evidencesection present sound complete axiomatization AX(h , ) logic.axiomatization divided four parts. first part, consistingfollowing axiom inference rule, accounts first-order reasoning:Taut. substitution instances valid formulas first-order logic equality.MP. infer .Instances Taut include, example, formulas form ,arbitrary formula logic. also includes formulas (x) x free. particular, (x(h)) h hypotheses h , similarly observations. Note Taut includes substitution instances valid formulas first-order logicequality; words, valid formula first-order logic equalityfree variables replaced arbitrary terms language (including Pr0 (), Pr(),w(ob, h)) instance Taut. Axiom Taut replaced sound completeaxiomatization first-order logic equality, given, instance, Shoenfield (1967)Enderton (1972).second set axioms accounts reasoning polynomial inequalities, relyingtheory real closed fields:RCF. instances formulas valid real closed fields (and, thus, true reals),nonlogical symbols +, , <, 0, 1, 1, 2, 2, 3, 3, . . . .13fiHalpern & PucellaFormulas valid real closed fields include, example, fact additionreals associative, xyz((x+y)+z = x+(y +z)), 1 identity multiplication,x(x1 = x), formulas relating constant symbols, k = 1+ +1 (k times)1 + 1 = 0. Taut, could replace RCF sound complete axiomatizationreal closed fields (cf. Fagin et al., 1990; Shoenfield, 1967; Tarski, 1951).third set axioms essentially captures fact single hypothesissingle observation holds per state.H1. h1 hnh .H2. hi hj 6= j.O1. ob 1 ob .O2. ob ob j 6= j.axioms illustrate subtlety logic. Like propositional logics,parameterized primitive propositions, case, h . However, axiomatizations propositional logics typically depend exact set primitivepropositions, does. Clearly, axiom H1 sound hypothesis primitivesexactly h1 , . . . , hnh . Similarly, axiom O1 sound observation primitivesexactly ob 1 , . . . , ob . therefore important us identify primitive propositionstalking axiomatization AX(h , ).last set axioms concerns reasoning probabilities evidence proper.axioms probability taken FHM.Pr1. Pr0 (true) = 1.Pr2. Pr0 () 0.Pr3. Pr0 (1 2 ) + Pr0 (1 2 ) = Pr0 (1 ).Pr4. Pr0 (1 ) = Pr0 (2 ) 1 2 propositional tautology.Axiom Pr1 simply says event true probability 1. Axiom Pr2 says probability nonnegative. Axiom Pr3 captures finite additivity. possible expresscountable additivity logic. hand, FHM, needaxiom countable additivity. Roughly speaking, establish next section,formula satisfiable all, satisfiable finite structure. Similar axioms captureposterior probability formulas:Po1. Pr(true) = 1.Po2. Pr() 0.Po3. Pr(1 2 ) + Pr(1 2 ) = Pr(1 ).Po4. Pr(1 ) = Pr(2 ) 1 2 propositional tautology.14fiA Logic Reasoning EvidenceFinally, need axioms account behavior evidence operator w.properties? one thing, weight function acts essentially like probabilityhypotheses, fixed observation, except restricted taking weightevidence basic hypotheses only. gives following axioms:E1. w(ob, h) 0.E2. w(ob, h1 ) + + w(ob, hnh ) = 1.Second, evidence connects prior posterior beliefs via Dempsters Rule Combination, (2). captured following axiom. (Note that, sincedivision language, crossmultiply clear denominator.)E3. ob (Pr0 (h)w(ob, h) = Pr(h)Pr0 (h1 )w(ob, h1 ) + + Pr(h)Pr0 (hnh )w(ob, hnh )).quite enough. saw Section 2, property WF2 Theorem 2.4required function evidence function. following axiom captures WF2logic:E4. x1 . . . xno (x1 > 0 xno > 0 w(ob 1 , h1 )x1 + + w(ob , h1 )xno = 1w(ob 1 , hnh )x1 + + w(ob , hnh )xno = 1).Note axiom E4 axiom requires quantification. Moreover, axioms E3E4 depend h .example, show h h0 distinct hypotheses h , formula(w(ob, h) = 2/3 w(ob, h0 ) = 2/3)provable. First, RCF, following valid formula theory real closed fieldsprovable:xy(x = 2/3 = 2/3 x + > 1).Moreover, (x, y) first-order logic formula two free variables x y,(xy((x, y))) (w(ob, h), w(ob, h0 ))substitution instance valid formula first-order logic equality, henceinstance Taut. Thus, MP, provew(ob, h) = 2/3 w(ob, h0 ) = 2/3 w(ob, h) + w(ob, h0 ) > 1,provably equivalent (by Taut MP) contrapositivew(ob, h) + w(ob, h0 ) 1 (w(ob, h) = 2/3 w(ob, h0 ) = 2/3).argument similar above, using RCF, Taut, MP, E1, E2, derivew(ob, h) + w(ob, h0 ) 1,MP, obtain desired conclusion: (w(ob, h) = 2/3 w(ob, h0 ) = 2/3).15fiHalpern & PucellaTheorem 4.1: AX(h , ) sound complete axiomatization Lfo -ev (h , )respect evidential worlds.usual, soundness straightforward, prove completeness, suffices showformula consistent AX(h , ), satisfiable evidential structure. However, usual approach proving completeness modal logic, involvesconsidering maximal consistent sets canonical structures work. problemmaximal consistent sets formulas satisfiable. example,maximal consistent set formulas includes Pr() > 0 Pr() 1/nn = 1, 2, . . . . clearly unsatisfiable. proof follows techniques developedFHM.express axiom E4, needed quantification logic.fact representation evidence normalized nontrivial effect logic: E4corresponds property WF2, essentially says function weight evidencefunction one find normalization factor. interesting question whetherpossible find sound complete axiomatization propositional fragmentlogic (without quantification variables). this, need give quantifier-freeaxioms replace axiom E4. amounts asking whether simpler propertyWF2 Theorem 2.4 characterizes weight evidence functions. remainsopen question.5. Decision Proceduressection, consider decision problem logic, is, problemdeciding whether given formula satisfiable. order state problem precisely,however, need deal carefully fact logic parameterized setsh primitive propositions representing hypotheses observations.logics, choice underlying primitive propositions essentially irrelevant. example,propositional formula contains primitive propositions settrue respect truth assignments , remains true respecttruth assignments set 0 . monotonicity property hold here.example, already observed, axiom H1 clearly depends set hypothesesobservations; longer valid set changed. true O1, E3,E4.means careful, stating decision problems, roleh algorithm. straightforward way deal assumesatisfiability algorithm gets input h , , formula Lfo -ev (h , ).Lfo -ev (h , ) contains full theory real closed fields, unsurprisingly difficultdecide. decision procedure, use exponential-space algorithm Ben-Or,Kozen, Reif (1986) decide satisfiability real closed field formulas. definelength || number symbols required write , countlength coefficient 1. Similarly, define kk length longestcoefficient appearing f , written binary.Theorem 5.1: procedure runs space exponential || kk deciding,given h , whether formula Lfo -ev (h , ) satisfiable evidential world.16fiA Logic Reasoning Evidenceessentially best do, since Ben-Or, Kozen, Reif (1986) provedecision problem real closed fields complete exponential space, logiccontains full language real closed fields.assumed algorithm takes input set primitive propositionsh , really affect complexity algorithm. precisely,given formula Lfo -ev set hypotheses observations,still decide whether satisfiable, is, whether sets h primitivepropositions containing primitive propositions evidential world wsatisfies .Theorem 5.2: procedure runs space exponential || kk decidingwhether exists sets primitive propositions h Lfo -ev (h , )satisfiable evidential world.main culprit exponential-space complexity theory real closed fields,add logic able even write axiom E4 axiomatization AX(h , ).5 However, interested axiomatizations, simplyverifying properties probabilities weights evidence, consider followingpropositional (quantifier-free) fragment logic. before, start sets hhypothesis observation primitives, form sublanguage Lh hypothesisformulas. Basic terms form Pr0 (), Pr(), w(ob, h), hypothesisformula, ob observation, h hypothesis. quantifier-free polynomial termform a1 t1 + + tn , ai integer ti productbasic terms. quantifier-free polynomial inequality formula form p c,p quantifier-free polynomial term, c integer. instance, quantifier-freepolynomial inequality formula takes form Pr0 () + 3w(ob, h) + 5Pr0 ()Pr(0 ) 7.Let Lev (h , ) language obtained starting primitive propositionsh quantifier-free polynomial inequality formulas, closing conjunction negation. Since quantifier-free polynomial inequality formulas polynomialinequality formulas, Lev (h , ) sublanguage Lfo -ev (h , ). logic Lev (h , )sufficiently expressive express many properties interest; instance, certainlyexpress general connection priors, posteriors, evidence captured axiomE3, well specific relationships prior probability posterior probabilityweight evidence particular observation, Example 3.1. Reasoningpropositional fragment logic Lev (h , ) easier full language.65. Recall axiom E4 requires existential quantification. Thus, restrict sublanguageconsisting formulas single block existential quantifiers prefix position. satisfiabilityproblem sublanguage shown decidable time exponential size formula(Renegar, 1992).6. preliminary version paper (Halpern & Pucella, 2003), examined quantifier-free fragmentLfo -ev (h , ) uses linear inequality formulas, form a1 t1 + + tn c,ti basic term. claimed problem deciding, given h , whether formulafragment satisfiable evidential world NP-complete. claimed resultfollowed small-model theorem: satisfiable, satisfiable evidential worldsmall number hypotheses observations. small-model theorem true, argumentsatisfiability problem NP also implicitly assumed numbers associatedprobability measure evidence space evidential world small. true17fiHalpern & PucellaTheorem 5.3: procedure runs space polynomial || kk deciding,given h , whether formula Lev (h , ) satisfiable evidential world.Theorem 5.3 relies Cannys (1988) procedure deciding validity quantifierfree formulas theory real closed fields. general case, complexityunaffected whether decision problem takes input sets hprimitive propositions.Theorem 5.4: procedure runs space polynomial || kk decidingwhether exists sets primitive propositions h Lev (h , )satisfiable evidential world.6. Normalized Versus Unnormalized Likelihoodsweight evidence used throughout paper generalization log-likelihoodratio advocated Good (1950, 1960). pointed earlier, measure confirmation essentially normalized likelihood: likelihood observation given particularhypothesis normalized sum likelihoods observation, possible hypotheses. would change take (unnormalized) likelihoods hweight evidence? things would simplify. example, WF2consequence normalization, corresponding axiom E4, axiomrequires quantification.main argument normalizing likelihood normalizing probability measures. like probability, using normalized likelihood, weightevidence always 0 1, provides absolute scale judgereports evidence. impact psychologicalit permits one userules thumb situations, since numbers obtained independent context use. Thus, instance, weight evidence 0.95 one situation correspondsamount evidence weight evidence 0.95 different situation;acceptable decision based weight evidence first situation oughtacceptable situation well. importance uniform scaledepends, course, intended applications.sake completeness, describe changes framework requireduse unnormalized likelihoods weight evidence. Define wEu (ob, h) = h (ob).general. Even though formula involves linear inequality formulas, every evidential worldsatisfies axiom E3. constraint enables us write formulas exist modelsprobabilities weights evidence rational. example, consider formulaPr0 (h1 ) = w(ob 1 , h1 ) Pr0 (h2 ) = 1 Pr0 (h1 ) Pr(h1 ) = 1/2 w(ob 1 , h2 ) = 1/4evidential world satisfying formula must satisfyPr0 (h1 ) = w(ob 1 , h1 ) = 1/8(117)irrational. exact complexity fragment remains open. use techniquesshow PSPACE, matching lower bound. (In particular, may indeedNP.) re-examine fragment logic Section 6, different interpretation weightsevidence.18fiA Logic Reasoning EvidenceFirst, note update prior probability 0 via set likelihood functions husing form Dempsters Rule Combination. precisely, define 0 wEu (ob, )probability measure defined0 (h)h (ob).0h0 H 0 (h )h0 (ob)(0 wEu (ob, ))(h) = Plogic introduced Section 3 applies well new interpretationweights evidence. syntax remains unchanged, models remain evidential worlds,semantics formulas simply take new interpretation weight evidenceaccount. particular, assignment [p]w,v uses definition wEu ,becomes[Pr()]w,v = ( wEu (ob, ))([[]])[w(ob 0 , h0 )]w,v = wEu (ob 0 , h0 ).axiomatization new logic slightly different somewhat simplerone Section 3. particular, E1 E2, say w(ob, h) acts probabilitymeasure fixed ob, replaced axioms say w(ob, h) acts probabilitymeasure fixed h:E10 . w(ob, h) 0.E20 . w(ob 1 , h) + + w(ob , h) = 1.Axiom E3 unchanged, since wEu updated essentially way . Axiom E4becomes unnecessary.complexity decision procedure? Section 5, complexitydecision problem full logic Lfo -ev (h , ) remains dominated complexity reasoning real closed fields. course, now, express full axiomatizationunnormalized likelihood interpretation weight evidence Lev (h , ) fragment, decided polynomial space. advantage unnormalizedlikelihood interpretation weight evidence, however, leads useful fragmentLev (h , ) perhaps easier decide.Suppose interested reasoning exclusively weights evidence,prior posterior probability. kind reasoning actually underliesmany computer science applications involving randomized algorithms (Halpern & Pucella,2005b). before, start sets h hypothesis observation primitives,form sublanguage Lh hypothesis formulas. quantifier-free linear termform a1 w(ob 1 , h1 ) + + w(ob n , hn ), ai integer, obobservation, hi hypothesis. quantifier-free linear inequality formulaform p c, p quantifier-free linear term c integer. example,w(ob 0 , h) + 3w(ob, h) 7 quantifier-free linear inequality formula.Let Lw (h , ) language obtained starting primitive propositionsh quantifier-free linear inequality formulas, closing conjunctionnegation. Since quantifier-free linear inequality formulas polynomial inequalityformulas, Lw (h , ) sublanguage Lfo -ev (h , ). Reasoning Lw (h , )easier full language, possibly easier Lev (h , ) fragment.19fiHalpern & PucellaTheorem 6.1: problem deciding, given h , whether formula Lw (h , )satisfiable evidential world NP-complete.general case, complexity unaffected whether decisionproblem takes input sets h primitive propositions.Theorem 6.2: problem deciding, formula , whether exists setsprimitive propositions h Lw (h , ) satisfiableevidential world NP-complete.7. Evidence Dynamic Systemsevidential worlds considered essentially static, modelsituation single observation made. Considering static worlds letsus focus relationship prior posterior probabilities hypothesesweight evidence single observation. related paper (Halpern & Pucella,2005b), consider evidence context randomized algorithms; use evidencecharacterize information provided by, example, randomized algorithm primalitysays number prime. framework work dynamic; sequencesobservations made time. section, extend logic reasonevidence sequences observations, using approach combining evidence describedSection 2.subtleties involved trying find appropriate logic reasoningsituations like Example 2.5. important one relationshipobservations time. way illustration, consider following example. Bobexpecting email Alice stating rendezvous take place. Calmpressure, Bob reading waits. assume Bob concernedtime. purposes example, one three things occur given pointtime:(1) Bob check received email;(2) Bob checks received email, notices received emailAlice;(3) Bob checks received email, notices received email Alice.view world affected events? (1), clear that,things equal, Bobs view world change: observation made.Contrast (2) (3). (2), Bob make observation, namelyyet received Alices email. fact checks indicates wants observeresult. (3), also makes observation, namely received email Alice.cases, check yields observation, use update viewworld. case (2), essentially observed nothing happened, emphasizeobservation, distinguished case Bobeven check whether email arrived, explicit set evidencespace.20fiA Logic Reasoning Evidencediscussion motivates models use section. characterizeagents state observations made, including possibly nothinghappened observation. Although explicitly model time, easy incorporatetime framework, since agent observe times clock ticks. modelssection admittedly simple, already highlight issues involved reasoningevidence dynamic systems. long agents forget observations,loss generality associating agents state sequence observations. do,however, make simplifying assumption evidence space usedobservations sequence. words, assume evidence space fixedevolution system. many situations interest, external world changes.possible observations may depend state world, may likelihood functions.intrinsic difficulties extending model handle state changes,additional details would obscure presentation.ways, considering dynamic setting simplifies things. Rather talkingprior posterior probability using different operators, need singleprobability operator represents probability hypothesis current time.express analogue axiom E3 logic, need able talkprobability next time step. done adding next-time operatorlogic, holds current time holds next time step.7extend logic talk weight evidence sequence observations.-evdefine logic Lfodyn follows. Section 3, start set primitivepropositions h , respectively representing hypotheses observations.Again, let Lh (h ) propositional sublanguage hypotheses formulas obtainedtaking primitive propositions h closing negation conjunction; userange formulas sublanguage.basic term form Pr() w(ob, h), hypothesis formula,ob = hob 1 , . . . , ob k nonempty sequence observations, h hypothesis.ob = hob 1 i, write w(ob 1 , h) rather w(hob 1 i, h). before, polynomial termform t1 + + tn , term ti product integers, basic terms, variables(which intuitively range reals). polynomial inequality formula form-evp c, p polynomial term c integer. Let Lfodyn (h , ) languageobtained starting primitive propositions h polynomialinequality formulas, closing conjunction, negation, first-order quantification,application operator. use abbreviations Section 3.semantics logic involves models dynamic behavior. Ratherconsidering individual worlds, consider sequences worlds,call runs, representing evolution system time. model infiniterun, run describes possible dynamic evolution system. before, runrecords observations made hypothesis true run, wellprobability distribution describing prior probability hypothesis initialstate run, evidence space E h interpret w. defineevidential run r map natural numbers (representing time) histories7. Following discussion above, time steps associated new observations. Thus, meanstrue next time step, is, next observation. simplifies presentationlogic.21fiHalpern & Pucellasystem time. history time records relevant informationrunthe hypothesis true, prior probability hypotheses, evidencespace E observations made time m. Hence, historyform h(h, , E ), ob 1 , . . . , ob k i. assume r(0) = h(h, , E )i h, ,E , r(m) = h(h, , E ), ob 1 , . . . , ob > 0. define point runpair (r, m) consisting run r time m.associate propositional formula Lh (h ) set [[]] hypotheses,Section 3.order ascribe semantics first-order formulas may contain variables,need valuation v assigns real number every variable. Given valuation v,evidential run r, point (r, m), r(m) = h(h, , E ), ob 1 , . . . , ob i, assignpolynomial term p real number [p]r,m,v using essentially approachSection 3:[x]r,m,v = v(x)[a]r,m,v =[Pr()]r,m,v = ( (hob 1 , . . . , ob i, )))([[]])r(m) = h(h, , E ), ob 1 , . . . , ob[w(ob, h0 )]r,m,v = (ob, h0 )r(m) = h(h, , E ), ob 1 , . . . , ob[t1 t2 ]r,m,v = [t1 ]r,m,v [t2 ]r,m,v[p1 + p2 ]r,m,,v = [p1 ]r,m,v + [p2 ]r,m,v .define means formula true (or satisfied) point (r, m)evidential run r valuation v, written (r, m, v) |= , using essentiallyapproach Section 3:(r, m, v) |= h r(m) = h(h, , E ), . . .i(r, m, v) |= ob r(m) = h(h, , E ), . . . , obi(r, m, v) |= (r, m, v) 6|=(r, m, v) |= (r, m, v) |= (r, m, v) |=(r, m, v) |= p c [p]r,m,v c(r, m, v) |= (r, + 1, v) |=(r, m, v) |= x (r, m, v 0 ) |= valuations v 0 agree v variablesx.(r, m, v) |= true v, simply write (r, m) |= . (r, m) |= points(r, m) r, write r |= say valid r. Finally, r |=evidential runs r, write |= say valid.straightforward axiomatize new logic. axiomatization showscapture combination evidence directly logic, pleasant property.22fiA Logic Reasoning Evidenceaxioms Section 3 carry immediately. Let axiomatization AXdyn (h , )consists following axioms inference rules: first-order reasoning (Taut, MP), reasoning polynomial inequalities (RCF), reasoning hypotheses observations(H1,H2,O1,O2), reasoning probabilities (Po14 only, since Pr0language), reasoning weights evidence (E1, E2, E4), well new axiomspresent.Basically, axiom needs replacing E3, links prior posteriorprobabilities, since needs expressed using operator. Moreover,need axiom relate weight evidence sequence observation weightevidence individual observations, given Equation (3).E5. ob x( (Pr(h) = x)Pr(h)w(ob, h) = xPr(h1 )w(ob, h1 ) + + xPr(hnh )w(ob, hnh )).E6. w(ob 1 , h) w(ob k , h) = w(hob 1 , . . . , ob k i, h)w(ob 1 , h1 ) w(ob k , h1 ) + +w(hob 1 , . . . , ob k i, h)w(ob 1 , hnh ) w(ob k , hnh ).get complete axiomatization, also need axioms inference rules captureproperties temporal operator .T1. ( ) .T2. .T3. infer .Finally, need axioms say truth hypotheses well value polynomialterms containing occurrences Pr time-independent:T4. .T5. (p c) p c p contain occurrence Pr.T6. (x) x( ).-evTheorem 7.1: AXdyn (h , ) sound complete axiomatization Lfodyn (h , )respect evidential runs.8. Conclusionliterature, reasoning effect observations typically done contextprior probability set hypotheses conditionobservations made obtain new probability hypotheses reflects effectobservations. paper, presented logic evidence lets us reasonweight evidence observations, independently prior probabilityhypotheses. logic expressive enough capture logical form relationshipprior probability hypotheses, weight evidence observations,result posterior probability hypotheses. also capture reasoninginvolve prior probabilities.23fiHalpern & Pucellalogic essentially propositional, obtaining sound complete axiomatization seems require quantification reals. adds complexitylogicthe decision problem full logic exponential space. However, interesting potentially useful fragment, propositional fragment, decidable polynomialspace.Acknowledgments. preliminary version paper appeared ProceedingsNineteenth Conference Uncertainty Artificial Intelligence, pp. 297304, 2003.work mainly done second author Cornell University. thank DexterKozen Nimrod Megiddo useful discussions. Special thanks Manfred Jaegercareful reading paper subsequent comments. Manfred found bugproof satisfiability problem quantifier-free fragment Lfo -ev (h , )uses linear inequality formulas NP-complete. comments also led us discussissue normalization. also thank reviewers, whose comments greatly improvedpaper. work supported part NSF grants CTC-0208535, ITR-0325453,IIS-0534064, ONR grant N00014-01-10-511, DoD MultidisciplinaryUniversity Research Initiative (MURI) program administered ONR grantsN00014-01-1-0795 N00014-04-1-0725, AFOSR grant F49620-02-1-0101.Appendix A. ProofsProposition 2.1: ob, (ob, hi ) (ob, h3i ) l(ob, hi )l(ob, h3i ), = 1, 2, h, ob, ob 0 , (ob, h) (ob 0 , h)l(ob, h) l(ob 0 , h).Proof. Let ob arbitrary observation. result follows following argument:(ob, hi ) (ob, h3i )iff hi (ob)/(hi (ob) + h3i (ob)) h3i (ob)/(hi (ob) + h3i (ob))iff hi (ob)hi (ob) h3i (ob)h3i (ob)iff hi (ob)/h3i (ob) h3i (ob)/hi (ob)iff l(ob, hi ) l(ob, h3i ).similar argument establishes result hypotheses.uTheorem 2.4: Let H = {h1 , . . . , hm } = {ob 1 , . . . , ob n }, let f real-valuedfunction domain H f (ob, h) [0, 1]. exists evidence spaceE = (H, O, h1 , . . . , hm ) f = f satisfies following properties:WF1. every ob O, f (ob, ) probability measure H.PWF2. exists x1 , . . . , xn > 0 that, h H, ni=1 f (ob , h)xi = 1.Proof. () Assume f = evidence space E = (H, O, h1 , . . . , hm ).routine verify WF1, fixed ob O, wEP(ob, ) probability measure H.verify WF2, note simply take xi = h0 H h0 (ob ).() Let f function H [0, 1] satisfies WF1 WF2. Letx1 , . . . , xnh positive reals guaranteed WF2. straightforward verify24fiA Logic Reasoning Evidencetaking h (ob ) = f (ob , h)/xi h H yields evidence space E f =.ufollowing lemmas useful prove completeness axiomatizationspaper. results depend soundness axiomatization AX(h , ).Lemma A.1: AX(h , ) sound axiomatization logic Lfo -ev (h , ) respect evidential worlds.Proof. easy see axiom valid evidential worlds.uLemma A.2: hypothesis formulas , h1 hk provable AX(h , ),[[]] = {h1 , . . . , hk }.Proof. Using Taut, show provably equivalent formula 0 disjunctivenormal form. Moreover, axiom H2, assume without loss generalitydisjuncts 0 consists single hypothesis. Thus, h1 hk . easyinduction structure shows hypothesis formula evidential world w,w |= iff w |= h h [[]]. Moreover, follows immediatelysoundness axiomatization (Lemma A.1) h1 . . . hk provable iffevidential worlds w, w |= iff w |= hi {1, . . . , k}. Thus, h1 . . . hkprovable iff [[]] = {h1 , . . . , hk }.ueasy consequence Lemma A.2 1 provably equivalent 2[[1 ]] = [[2 ]].Lemma A.3: Let hypothesis formula. formulasPPr(h)Pr() =h[[]]Pr0 () =PPr0 (h)h[[]]provable AX(h , ).Proof. Let h = {h1 , . . . , hnh } = {ob 1 , . . . , ob }. prove result Pr.proceed induction size [[]]. base case, assume |[[]]| = 0.Lemma A.2, implies provably equivalent false. Po4, Pr() =Pr(false), easy check Pr(false) = 0 provable using Po1, Po3, Po4,thus Pr() = 0, required. |[[]]| = n + 1 > 0, [[]] = {hi1 , . . . , hin+1 },Lemma A.2, provably equivalent hi1 hin+1 . Po4, Pr() = Pr( hin+1 ) +Pr( hin+1 ). easy check hin+1 provably equivalent hin+1 (usingH2), similarly hin+1 provably equivalent hi1 hin . Thus, Pr() =hin ]]| = n, inductionprovable. Since |[[hi1PPr(hin+1 ) + Pr(hi1 hin ) Phypothesis, Pr(hi1 hin ) = h{hi ,...,hin } Pr(h) = h[[]]{hi } Pr(h). Thus, Pr() =1n+1PPPr(hin+1 ) + h[[]]{hi } Pr(h), is, Pr() = h[[]] Pr(h), required.n+1argument applies mutatis mutandis Pr0 , using axioms Pr14 insteadPo14.u25fiHalpern & PucellaTheorem 4.1: AX(h , ) sound complete axiomatization logic respect evidential worlds.Proof. Soundness established Lemma A.1. prove completeness, recall following definitions. formula consistent axiom system AX(h , )provable AX(h , ). prove completeness, sufficient showconsistent, satisfiable, is, exists evidential world w valuation v(w, v) |= .body paper, let h = {h1 , . . . , hnh } = {ob 1 , . . . , ob }. Letconsistent formula. way contradiction, assume unsatisfiable. reduceformula equivalent formula language real closed fields. Let u1 , . . . , unh ,v1 , . . . , vno , x1 , . . . , xnh , y1 , . . . , yno , z11 , . . . , zn1 h , . . . , z1no , . . . , znnho new variables, where,intuitively,ui gets value 1 hypothesis hi holds, 0 otherwise;vi gets value 1 observation ob holds, 0 otherwise;xi represents Pr0 (hi );yi represents Pr(hi );zi,j represents w(ob , hj ).Let v represent list new variables. Consider following formulas. Let hformula saying exactly one hypothesis holds:(u1 = 0 u1 = 1) (unh = 0 unh = 1) u1 + + unh = 1.Similarly, let formula saying exactly one observation holds:(v1 = 0 v1 = 1) (vno = 0 vnh = 1) v1 + + vnh = 1.Let pr formula expresses Pr0 probability measure:pr = x1 0 xnh 0 x1 + + xnh = 1.Similarly, let po formula expresses Pr probability measure:po = y1 0 ynh 0 y1 + + ynh = 1.Finally, need formulas saying w weight evidence function. formulaw ,p simply says w satisfies WF1, is, acts probability measure fixedobservation:z1,1 0 z1,nh 0 zno ,1 0 zno ,nh 0z1,1 + + z1,nh = 1 zno ,1 + + zno ,nh = 1.formula w ,f says w satisfies WF2:w1 , . . . , wno (w1 > 0 wno > 0 z1,1 w1 + + zno ,1 wno = 1z1,nh w1 + + zno ,nh wno = 1)26fiA Logic Reasoning Evidencew1 , . . . , wno new variables.Finally, formula w ,up captures fact weights evidence viewed updating prior probability posterior probability, via Dempsters Rule Combination:(v1 = 1 (x1 z1,1 = y1 x1 z1,1 + + y1 xnh z1,nhxnh z1,nh = ynh x1 z1,1 + + ynh xnh z1,nh ))(vno = 1 (x1 zno ,1 = y1 x1 zno ,1 + + y1 xnh zno ,nhxnh zno ,nh = ynh x1 zno ,1 + . . . ynh xnh zno ,nh )).Let formula language real closed fields obtained replacingoccurrence primitive propositionhi ui = 1, occurrencePof ob vi =P1, occurrence Pr0 () hi [[]] xi , occurrence Pr() hi [[]] yi ,occurrence w(ob , hj ) zi,j , occurrence integer coefficient k 1 + + 1(k times). Finally, let 0 formula v(h pr po w ,p w ,f w ,up ).easy see unsatisfiable evidential worlds, 0 falseinterpreted real numbers. Therefore, 0 must formula valid real closedfields, hence instance RCF. Thus, 0 provable. straightforward show,using Lemma A.3, provable, contradicting fact consistent.Thus, must satisfiable, establishing completeness.umentioned beginning Section 5, Lfo -ev monotone respectvalidity: axiom H1 depends set hypotheses observations, generallonger valid set changed. true O1, E3, E4. do,however, form monotonicity respect satisfiability, following lemmashows.Lemma A.4: Given h , let formula Lfo -ev (h , ), let H hhypotheses observations occur . satisfiableevidential world h , satisfiable evidential world 0h0o , |0h | = |H| + 1 |0o | = |O| + 1.Proof. two steps, clarify presentation. First, showadd single hypothesis observation h preserve satisfiability .means second step assume h 6= H 6= O. Assumesatisfied evidential world w = (h, ob, , E) h , existsv (w, v) |= . Let 0h = h {h }, h new hypothesis h ,let 0o = {ob }, ob new observation . Define evidentialworld w0 = (h, ob, 0 , E 0 ) 0h 0o , E 0 0 defined follows. Defineprobability measure 0 taking:((h) h h0(h) =0h = h .27fiHalpern & PucellaSimilarly, define evidence space E 0 = (0h , 0o , 0 ) derived E = (h , , ) taking:h (ob) h h ob0h h ob = ob0h (ob) =0h = h ob1h = h ob ob .Thus, 0h extends existing h assigning probability 0 new observation ob ;contrast, new probability 0h assigns probability 1 new observation ob .check (w0 , v) |= .second step collapse hypotheses observations appearone hypotheses appear H O, previous stepguaranteed exist. previous step, assume h 6= H 6= O.Assume satisfiable evidential world w = (h, ob, , E) h , is,exists v (w, v) |= . Pick hypothesis observation hfollows, depending hypothesis h observation ob w. Let h h h 6 H,otherwise, let h arbitrary element h H; let 0h = H {h }. Similarly, let obob ob 6 O, otherwise, let ob arbitrary element O; let 0o = {ob }.Let w0 = (h, ob, 0 , E 0 ) evidential world 0h 0o obtained w follows.Define probability measure 0 taking:((h)h H0 (h) = P0h0 h H (h ) h = h .Define E 0 = (0h , 0o , 0 ) derived E = (h , , ) taking:h H obh (ob)P 00h (ob )h H ob = ob0h (ob) = Pobh = h obh0 h H h0 (ob)PP0h0 h Hob 0 h0 (ob ) h = h ob = ob .check induction (w0 , v) |= .uTheorem 5.1: procedure runs space exponential || kk deciding,given h , whether formula Lfo -ev (h , ) satisfiable evidential world.Proof. Let formula Lfo -ev (h , ). Lemma A.4, satisfiableconstruct probability measure 0h = H {h } (where H set hypothesesappearing , h 6 H) probability measures h1 , . . . , hm 0o = {ob }(where set observations appearing ob 6 O) E = (0h , 0o , ),w = (h, ob, , E) (w, v) |= h, ob, v.aim derive formula 0 language real closed fields assertsexistence probability measures. precisely, adapt constructionformula 0 proof Theorem 4.1. one change need makeensure 0 polynomial size , construction proof28fiA Logic Reasoning EvidenceTheorem 4.1 guarantee. culprit fact encode integer constants k1+ +1. straightforward modify construction use efficientrepresentation integer constants, namely, binary representation. example,write 42 2(1 + 22 (1 + 22 )), expressed language real closed fields(1 + 1)(1 + (1 + 1)(1 + 1)(1 + (1 + 1)(1 + 1))). check k coefficient lengthk (when written binary), written term length O(k) languagereal closed fields. Thus, modify construction 0 proof Theorem 4.1integer constants k represented using binary encoding. easy see|0 | polynomial || kk (since |0h | |0o | polynomial ||).use exponential-space algorithm Ben-Or, Kozen, Reif (1986) 0 : 0satisfiable, construct required probability measures, satisfiable;otherwise, probability measures exist, unsatisfiable.uTheorem 5.2: procedure runs space exponential || kk decidingwhether exist sets primitive propositions h Lfo -ev (h , )satisfiable evidential world.Proof. Let h1 , . . . , hm hypotheses appearing , ob 1 , . . . , ob n hypothesesappearing . Let h = {h1 , . . . , hm , h } = {ob 1 , . . . , ob n , ob }, h obhypothesis observation appearing . Clearly, |h | |o | polynomial||. Lemma A.4, satisfiable evidential world, satisfiable evidentialworld h . Theorem 5.1, algorithm determine satisfiedevidential world h runs space exponential || kk.uTheorem 5.3: procedure runs space polynomial || kk deciding,given h , whether formula Lev (h , ) satisfiable evidential world.Proof. proof result similar Theorem 5.1. Let formulaLev (h , ). Lemma A.4, satisfiable exists probability measure0h = H {h } (where H set hypotheses appearing , h 6 H), probabilitymeasures h1 , . . . , hm 0o = {ob } (where set observations appearingob 6 O), hypothesis h, observation o, valuation v (w, v) |= ,w = (h, ob, , E) E = (0h , 0o , ).derive formula 0 language real closed fields asserts existenceprobability measures adapting construction formula 0proof Theorem 4.1. proof Theorem 5.1, need make sure 0polynomial size , construction proof Theorem 4.1guarantee. modify construction use efficient representationinteger constants, namely, binary representation. example, write 422(1 + 22 (1 + 22 )), expressed language real closed fields (1 + 1)(1 +(1 + 1)(1 + 1)(1 + (1 + 1)(1 + 1))). check k coefficient length k(when written binary), written term length O(k) languagereal closed fields. modify construction 0 proof Theorem 4.1integer constants k represented using binary encoding. easy see |0 |polynomial || kk (since |0h | |0o | polynomial ||). keynotice resulting formula 0 written x1 . . . xn (00 ) quantifierfree formula 00 . form, apply polynomial space algorithm Canny (1988)29fiHalpern & Pucella00 : 00 satisfiable, construct required probability measures,satisfiable; otherwise, probability measures exist, unsatisfiable.uTheorem 5.4: procedure runs space polynomial || kk decidingwhether exists sets primitive propositions h Lev (h , )satisfiable evidential world.Proof. Let h1 , . . . , hm hypotheses appearing , ob 1 , . . . , ob n hypothesesappearing . Let h = {h1 , . . . , hm , h } = {ob 1 , . . . , ob n , ob }, h obhypothesis observation appearing . Clearly, |h | |o | polynomial||. Lemma A.4, satisfiable evidential world, satisfiable evidentialworld h . Theorem 5.3, algorithm determine satisfiedevidential world h runs space polynomial || kk.uproofs Theorem 6.1 6.2 rely following small model result, variationLemma A.4.Lemma A.5: Given h , let formula Lfo -ev (h , ), let H hhypotheses observations occur . satisfiableevidential world h , satisfiable evidential world 0h0o |0h | = |H| + 1 |0o | = |O| + 1, where, h 0h ob 0o ,likelihood h (ob) rational number size O(|| kk + || log(||)).Proof. Let formula satisfiable evidential world h . Lemma A.4,satisfiable evidential world 0h 0o , |0h | = |H|+1 |0o | = |O|+1.force likelihoods small, adapt Theorem 2.6 FHM, saysformula f FHM logic satisfiable, satisfiable structureprobability assigned state structure rational number size O(|f | kf k+|f | log(|f |)). formulas Lw (0h , 0o ) formulas FHM logic. resultadapts immediately, yields required bounds size likelihoods.uTheorem 6.1: problem deciding, given h , whether formula Lw (h , )satisfiable evidential world NP-complete.Proof. establish lower bound, observe reduce propositional satisfiabilitysatisfiability Lw (h , ). precisely, let f propositional formula,p1 , . . . , pn primitive propositions appearing f . Let = {ob 1 , . . . , ob n , ob }set observations, observation ob corresponds primitive proposition pi ,ob another (distinct) observation; let h arbitrary set hypotheses, leth arbitrary hypothesis h . Consider formula f obtained replacing everyoccurrence pi f w(ob , h) > 0. straightforward verify f satisfiablef satisfiable Lw (h , ). (We need extra observation ob takecare case f satisfiable model p1 , . . . , pn false. case,w(ob 1 , h) = w(ob n , h) = 0, take w(ob , h) = 1.) establishes lowerbound,upper bound straightforward. Lemma A.5, evidential world hguessed time polynomial |h | + |o | + || kk, since prior probabilityworld requires assigning value |h | hypotheses, evidence space requires30fiA Logic Reasoning Evidence|h | likelihood functions, assigning value |o | observations, size polynomial|| kk. verify world satisfies time polynomial || kk + |h | + |h |.establishes problem NP.uTheorem 6.2: problem deciding, formula , whether exists setsprimitive propositions h Lw (h , ) satisfiableevidential world NP-complete.Proof. lower bound, reduce decision problem Lw (h , ) fixedh . Let h = {h1 , . . . , hm } = {ob 1 , . . . , ob n }, let formulaLw (h , ). check satisfiable evidential world h(h1 hm ) (ob 1 ob n ) satisfiable evidential world arbitrary0h 0o . Thus, Theorem 6.1, get lower bound.upper bound, Lemma A.5, satisfiable, satisfiable evidentialworld h , h = H {h }, H consists hypotheses appearing ,= {ob }, consists observations appearing , h ob newhypotheses observations. Thus, |h | || + 1, |o | || + 1. proofTheorem 6.1, world guessed time polynomial || kk + |h | + |o |,therefore time polynomial || kk. verify world satisfies timepolynomial || kk, establishing problem NP.u-evTheorem 7.1: AXdyn (h , ) sound complete axiomatization Lfodyn (h , )respect evidential runs.Proof. easy see axiom valid evidential runs. prove completeness,follow procedure proof Theorem 4.1, showing consistent,satisfiable, is, exists evidential run r valuation v(r, m, v) |= point (r, m) r.body paper, let h = {h1 , . . . , hnh } = {ob 1 , . . . , ob }. Letconsistent formula. first step process reduce formula canonicalform respect operator. Intuitively, push every occurrencepolynomial inequality formulas present formula. easy see axiomsinference rules T1T6 used establish provably equivalent formula0 every occurrence form subformulas n (ob) n (p c),p polynomial term contains least one occurrence Pr operator. usenotation n . . . , n-fold application . write 0 . Let Nmaximum coefficient 0 .way contradiction, assume 0 (and hence ) unsatisfiable. proofTheorem 4.1, reduce formula 0 equivalent formula language realclosed fields. Let u1 , . . . , unh , v10 , . . . , vn0 , . . . , v1N , . . . , vnNo , y10 , . . . , yn0 , . . . , y1N , . . . , ynNo ,zhi1 ,...,ik i,1 , . . . , zhi1 ,...,ik i,nh (for every sequence hi1 , . . . , ik i) new variables, where, intuitively,ui gets value 1 hypothesis hi holds, 0 otherwise;vin gets value 1 observation ob holds time n, 0 otherwise;yin represents Pr(hi ) time n;31fiHalpern & Pucellazhi1 ,...,ik i,j represents w(hob i1 , . . . , ob ik i, hj ).main difference construction proof Theorem 4.1 variables vin representing observations every time step n, rather variables representing observations time step, variables yin representing hypothesis probabilityevery time step, rather variables representing prior posterior probabilities,variables zhi1 ,...,ik i,j representing weight evidence sequences observations, rathervariables representing weight evidence single observations. Let v representlist new variables. consider formulas proof Theorem 4.1,modified account new variables, fact reasoning multipletime steps. specifically, formula h unchanged. Instead , consider formulas 1o , . . . , Nsaying exactly one observation holds time time step,given by:(v1n = 0 v1n = 1) (vnno = 0 vnnh = 1) v1n + + vnnh = 1.Let 0o = 1o N.Similarly, instead pr po , consider formulas 1p , . . . , Np expressing Prprobability measure time step, np given by:y1n 0 ynnh 0 y1n + + ynnh = 1.Let p = 1p Np .Similarly, consider w ,p w ,f , except replace variables zi,j zhii,j ,reflect fact consider sequences observations. formula w ,up , capturingupdate prior probability posterior probability given E5, replacedformulas 1w ,up , . . . , Nw ,up representing update probability time step,nw ,up given obvious generalization w ,up :z1,nh(v1n = 1 (y1n1 z1,1 = y1n y1n1 z1,1 + + y1n ynn1hn1n1nz1,nh ))ynh z1,nh = ynh y1 z1,1 + + ynnh ynn1hzno ,nh(vnno = 1 (y1n1 zno ,1 = y1n y1n1 zno ,1 + + y1n ynn1hn n1 zn n1 zynn1z+...=,nh,1,nh )).nh 1nh nhhLet 0w ,up = 1w ,up Nw ,up .Finally, need new formula w ,c capturing relationship weightevidence sequence observations, weight evidence individualobservations, capture axiom E6:^zhi1 i,h1 zhik i,h1 = zhi1 ,...,ik i,h1 zhi1 i,h1 zhik i,h11kN+ + zhi1 ,...,ik i,h1 zhi1 i,hnh zhik i,hnh1i1 ,...,ik^zhi1 i,hnh zhik i,hnh = zhi1 ,...,ik i,hnh zhi1 i,h1 zhik i,h11kN+ + zhi1 ,...,ik i,hnh zhi1 i,hnh zhik i,hnh .1i1 ,...,ik32fiA Logic Reasoning EvidenceLet formula language real closed fields obtained replacingoccurrence primitive proposition hi ui = 1, occurrence n obvin = 1, withinpolynomial inequality formula n (p c), replacing occurrencePPr() hi [[]] yin , occurrence w(hob i1 , . . . , ob ik i, hj ) zhi1 ,...,ik i,j ,occurrence integer coefficient k 1 + + 1 (k times). Finally, let 0 formulav(h 0o p w ,p w ,f 0w ,up w ,c ).easy see unsatisfiable evidential systems, 0 falsereal numbers. Therefore, 0 must formula valid real closed fields, henceinstance RCF. Thus, 0 provable. straightforward show, using obviousvariant Lemma A.3 provable, contradicting fact consistent.Thus, must satisfiable, establishing completeness.uReferencesBen-Or, M., Kozen, D., & Reif, J. H. (1986). complexity elementary algebrageometry. Journal Computer System Sciences, 32 (1), 251264.Canny, J. F. (1988). algebraic geometric computations PSPACE. Proc. 20thAnnual ACM Symposium Theory Computing (STOC88), pp. 460467.Carnap, R. (1962). Logical Foundations Probability (Second edition). UniversityChicago Press.Casella, G., & Berger, R. L. (2001). Statistical Inference (Second edition). Duxbury.Chan, H., & Darwiche, A. (2005). revision probabilistic beliefs using uncertainevidence. Artificial Intelligence, 163, 6790.de Alfaro, L. (1998). Formal Verification Probabilistic Systems. Ph.D. thesis, StanfordUniversity. Available Technical Report STAN-CS-TR-98-1601.Enderton, H. B. (1972). Mathematical Introduction Logic. Academic Press.Fagin, R., & Halpern, J. Y. (1994). Reasoning knowledge probability. JournalACM, 41 (2), 340367.Fagin, R., Halpern, J. Y., & Megiddo, N. (1990). logic reasoning probabilities.Information Computation, 87 (1/2), 78128.Fischer, M. J., & Zuck, L. D. (1988). Reasoning uncertainty fault-tolerant distributed systems. Technical report YALEU/DCS/TR643, Yale University.Good, I. J. (1950). Probability Weighing Evidence. Charles Griffin & Co. Ltd.Good, I. J. (1960). Weights evidence, corroboration, explanatory power, informationutility experiments. Journal Royal Statistical Society, Series B, 22,319331.Halpern, J. Y., & Fagin, R. (1992). Two views belief: belief generalized probabilitybelief evidence. Artificial Intelligence, 54, 275317.Halpern, J. Y., Moses, Y., & Tuttle, M. R. (1988). knowledge-based analysis zeroknowledge. Proc. 20th Annual ACM Symposium Theory Computing(STOC88), pp. 132147.33fiHalpern & PucellaHalpern, J. Y., & Pucella, R. (2003). logic reasoning evidence. Proc. 19thConference Uncertainty Artificial Intelligence (UAI03), pp. 297304.Halpern, J. Y., & Pucella, R. (2005a). Evidence uncertain likelihoods. Proc. 21thConference Uncertainty Artificial Intelligence (UAI05), pp. 243250.Halpern, J. Y., & Pucella, R. (2005b). Probabilistic algorithmic knowledge. Logical MethodsComputer Science, 1 (3:1).Halpern, J. Y., & Tuttle, M. R. (1993). Knowledge, probability, adversaries. JournalACM, 40 (4), 917962.He, J., Seidel, K., & McIver, A. (1997). Probabilistic models guarded commandlanguage. Science Computer Programming, 28 (23), 171192.Jeffrey, R. C. (1992). Probability Art Judgement. Cambridge University Press.Kyburg, Jr., H. E. (1983). Recent work inductive logic. Machan, T., & Lucey, K.(Eds.), Recent Work Philosophy, pp. 87150. Rowman & Allanheld.Milne, P. (1996). log[p(h|eb)/p(h|b)] one true measure confirmation. PhilosophyScience, 63, 2126.Pearl, J. (1988). Probabilistic Reasoning Intelligent Systems: Networks Plausible Inference. Morgan Kaufmann.Popper, K. R. (1959). Logic Scientific Discovery. Hutchinson.Renegar, J. (1992). computational complexity geometry first order theoryreals. Journal Symbolic Computation, 13 (3), 255352.Shafer, G. (1976). Mathematical Theory Evidence. Princeton University Press.Shafer, G. (1982). Belief functions parametric models (with commentary). JournalRoyal Statistical Society, Series B, 44, 322352.Shoenfield, J. R. (1967). Mathematical Logic. Addison-Wesley, Reading, Mass.Tarski, A. (1951). Decision Method Elementary Algebra Geometry (2nd edition).Univ. California Press.Vardi, M. Y. (1985). Automatic verification probabilistic concurrent finite-state programs.Proc. 26th IEEE Symposium Foundations Computer Science (FOCS85),pp. 327338.Walley, P. (1987). Belief function representations statistical evidence. Annals Statistics,18 (4), 14391465.34fiJournal Artificial Intelligence Research 26 (2006) 323-369Submitted 10/05; published 08/06Temporal Planning using Subgoal PartitioningResolution SGPlanYixin Chenchen@cse.wustl.eduDepartment Computer Science EngineeringWashington University St LouisSt Louis, MO 63130 USABenjamin W. WahChih-Wei Hsuwah@manip.crhc.uiuc.educhsu@manip.crhc.uiuc.eduDepartment Electrical Computer EngineeringCoordinated Science LaboratoryUniversity Illinois Urbana-ChampaignUrbana, IL 61801 USAAbstractpaper, present partitioning mutual-exclusion (mutex) constraintstemporal planning problems implementation SGPlan4 planner. Basedstrong locality mutex constraints observed many benchmarks Fourth International Planning Competition (IPC4), propose partition constraintsplanning problem groups based subgoals. Constraint partitioning leadssignificantly easier subproblems similar original problemefficiently solved planner modifications objective function. present partition-and-resolve strategy looks locally optimal subplansconstraint-partitioned temporal planning subproblems resolves inconsistentglobal constraints across subproblems. also discuss implementation detailsSGPlan4 , include resolution violated global constraints, techniques handling producible resources, landmark analysis, path finding optimization, search-spacereduction, modifications Metric-FF used basic planner SGPlan4 . Last,show results sensitivity techniques quality-time trade-offsexperimentally demonstrate SGPlan4 effective solving IPC3 IPC4benchmarks.1. Introductionpaper, present innovative partition-and-resolve strategy implementation SGPlan4 solving temporal planning problems PDDL2.2. strategy partitions mutual-exclusion (mutex) constraints temporal planning problemsubgoals subproblems, solves subproblems individually using modified Metric-FFplanner, resolves violated global constraints iteratively across subproblems.evaluate various heuristics resolving global constraints demonstrate performance SGPlan4 solving benchmarks Third (IPC3) Fourth (IPC4)International Planning Competitions.general popular methods solving large planning problems, systematic search, heuristic search, transformation methods, viewed recursivec2006AI Access Foundation. rights reserved.fiChen, Wah, & HsuP:VariablepartitioningorderedheuristicfunctionsBCSP = SA SB SCVariables:a) Search-space partitioninga11111111111111111111111111100000000000000000000000000000000000000000000000000001111111111111111111111111100000000000000000000000000111111111111111111111111110000000000000000000000000011111111111111111111111111000000000000000000000000001111111111111111111111111100000000000000000000000000111111111111111111111111110000000000000000000000000011111111111111111111111111000000000000000000000001111111111111111111111100000000000000000000000111111111111111111111110000000000000000000000011111111111111111111111000000000000000000000001111111111111111111111100000000000000000000000111111111111111111111110000000000000000000000011111111111111111111111a2a3b) Complete heuristic searchesFigure 1: Search-space partitioning branches variable assignments order decompose P disjunction () subproblems disjoint search spaces.complexity subproblem similar P .partitioning search space independent subproblems iterative evaluationsubproblems feasible solution found. level applicationapproach, problem subproblem decomposed partitioning variable spacedisjunction () subspaces (Figure 1a). reduce search complexity, approachoften combined intelligent backtracking employs variable/value ordering ordersubproblems generated, pre-filters partial inconsistent assignments eliminateinfeasible subproblems, prunes subproblems using bounds computed relaxationapproximation.Search-space partitioning directly applied planning problem transformed version problem. Direct methods include complete heuristic searches.illustrated Figure 1b, methods partition search space recursively branchingassigned variables (selection actions). difference complete searchheuristic search former enumerates subspaces systematically, whereaslatter prioritizes subspaces heuristic function evaluates selectively. Examples complete planners include UCPOP (Penberethy & Weld, 1992), Graphplan (Blum& Furst, 1997), STAN (Long & Fox, 1998), PropPLAN (Fourman, 2000), System R (Lin,2001), SIPE-2 (Wilkins, 1990), O-Plan2 (Tate, Drabble, & Kirby, 1994), ZENO (Penberethy& Weld, 1994), TALplanner (Doherty & Kvarnstrm, 1999), SHOP2 (Nau, Muoz-Avila,Cao, Lotem, & Mitchell, 2001); examples heuristic planners include HSP (Bonet &Geffner, 2001), FF (Hoffmann & Nebel, 2001), AltAlt (Nigenda, Nguyen, & Kambhampati, 2000), GRT (Refanidis & Vlahavas, 2001), MO-GRT (Refanidis & Vlahavas, 2002),ASPEN (Chien, Rabideau, Knight, Sherwood, Engelhardt, Mutz, Estlin, Smith, Fisher,Barrett, Stebbins, & Tran, 2000), Metric-FF (Hoffmann & Nebel, 2001), GRT-R (Refanidis& Vlahavas, 2001), LPG (Gerevini & Serina, 2002), MIPS (Edelkamp, 2002), Sapa (Subbarao & Kambhampati, 2002), Europa (Jonsson, Morris, Muscettola, & Rajan, 2000).contrast, transformation approach, problem first transformed satisfiabilityoptimization problem, transformed problem solved SAT integerprogramming solver employs search-space partitioning. Notable planners using324fiTemporal Planning using Subgoal Partitioning Resolution10000P:Run time1000GBC1001010.1AIRPORT-NT-20PIPESWORLD-NT-NT-500.01SP = SA SB SC SG1a) Constraint partitioning2345Number Subgoals Subproblemb) Exponential behavior solution timeFigure 2: Constraint partitioning decomposes P conjunction () subproblemsdisjoint constraints possibly overlapping search spaces, setglobal constraints (G) resolved. Since complexity subproblemsubstantially smaller P , leads exponential decrease solutiontime Metric-FF two IPC4 benchmarks (AIRPORT-NONTEMP-20PIPESWORLD-NOTANKAGE-NONTEMP-50) number subgoalssubproblem decreased 5 1.approach include SATPLAN (Kautz & Selman, 1996), Blackbox (Kautz & Selman, 1999),ILP-PLAN (Kautz & Walser, 2000), LPSAT (Wolfman & Weld, 2000).One limitations search-space partitioning complexity problemdramatically reduced partitioning. Although pruning ordering strategies make search efficient requiring search every subspace,aggregate complexity finding solution one subproblemsoriginal problem.paper, study constraint-partitioning approach decomposes constraints planning problem conjunction () subproblems disjoint constraints possibly overlapping search spaces (Figure 2a). concept constraintsplanning problems studied paper precisely defined Section 2.1. Informally,(mutex) constraint refers condition two actions overlapexecution. Since constraints must satisfied, subproblemsmust solved order solve original problem.decomposing constraints problem subproblems solvingindependently, subproblem require significantly less time solve muchrelaxed original problem. illustration, Figure 2b shows exponentialdecrease solution time number subgoals subproblem reduced linearly.Here, subgoal collection conjuncts conjunctive top-level goal problem.IPC4 instances evaluated, run time 1500 secondsfive subgoals subproblem less one second one. Hence,aggregate complexity solving decomposed subproblems exponentially smalleroriginal problem.325fiChen, Wah, & HsuS0P1110000111100001111000011110000111110000001110001110001111100S100 0001111100011100011100011100011100011100011100011111000011P21110000001111100001111000011111000000111000111000111000111000111000111000111S2110000011111 1110000000011100011111000011P311000011S31100110011000011110000111100001111001100Figure 3: Partitioning constraints planning problem along temporal horizonthree stages requires finding suitable intermediate states S1 S2 orderconnect subplans three stages together. S0 S3 are, respectively,initial final states.Constraint partitioning, however, leads global constraints across subproblems (SGFigure 2a) need resolved. global constraints include spanacross common variables multiple subproblems, relate two actionsdifferent subproblems. Since constraints may satisfied solvingsubproblems independently, subproblems may need solved multiple times orderresolve violated global constraints.general, violated global constraints across subproblems cannot efficiently resolvedbrute-force enumeration search space global constraints definedCartesian product search spaces across subproblems exponentiallylarge. Dynamic programming cannot applied global constraints may span acrossmultiple subproblems. means partial feasible plan dominates anotherpartial feasible plan one subproblem fail execute dominating plan violatesglobal constraint another subproblem.address resolution violated global constraints, summarize Section 3theory extended saddle points developed previous work (Wah & Chen, 2006).choosing suitable neighborhood, theory allows mixed-integer nonlinear programming problem (MINLP) partitioned subproblems related necessarycondition global constraints. Further, necessary condition subproblemsignificantly prunes Cartesian product search spaces across subproblemsinconsistent global constraints resolved.addition efficient resolution violated global constraints, successapproach depends strong locality constraints respect actionsrelate. observed informally previous work strong locality constraints. Based strong locality, studied two alternatives partitioningconstraints: partitioning time (Wah & Chen, 2006; Chen & Wah, 2003)partitioning subgoals (Wah & Chen, 2004, 2003).idea partitioning planning problem time partition constraintstemporal bindings stages. find overall feasible plan, planner needfind subplan initial final states stage satisfy localwell global constraints, final state one stage initial state326fiTemporal Planning using Subgoal Partitioning Resolutionnext stage. example, partitioning horizon three stages (Figure 3),planner assigns values intermediate states S1 S2 , solves subproblemindividually, perturbs S1 S2 look another solution feasible subplans cannotfound stages.major drawback partitioning planning problem temporal horizonconstraint resolutions may sequentially propagate multiple stages.found partitioning constraints PDDL2.1 benchmarks along temporalhorizon often leads many global constraints relate states adjacent stages.result, violated subgoal caused incorrect assignment statesearly stage horizon, resolution incorrect assignment propagatesequentially stages. Oftentimes, propagation information may causesearch get stuck infeasible point extended period time (Wah & Chen,2004). end, expensive enumeration final state stage (S1 S2Figure 3) may needed order resolve inconsistencies.second approach studied previous work partition constraintsplanning problem subgoals (Wah & Chen, 2004, 2003). evaluatingsubproblems, inconsistent global constraints among first identified,subproblems re-evaluated global constraints satisfied. Partitioningsubgoals eliminates need selecting final state subprobleminitial final states subgoal known. Using approach, previouswork shown improvements time quality MIPS planner solvingIPC3 benchmarks.respect second approach, made four main contributionspaper.First, quantitatively evaluate Section 2.2 locality constraints IPC4benchmarks well benchmarks Blocksworld domain Depots domain.results show constraint partitioning subgoals consistently leads lowerfraction initial active global constraints constraint partitioning time. resultsalso explain constraint partitioning work well domains,Blocksworld Depots.Second, incorporate Metric-FF (Hoffmann, 2003) basic planner SGPlan4SGPlan4.1 , instead MIPS previous work (Wah & Chen, 2004). changenon-trivial requires significant extensions Metric-FF order handlenew features PDDL2.2 beyond PDDL2.1. extensions include supporttemporal planning, handling derived predicates timed initial literals,handling wrappers timed initial literals (Section 5.3).Third, describe new techniques improving search efficiency global-local-level architectures partition-and-resolve approach (Section 4.1). includehandling producible resources (Section 4.3), subgoal-level decomposition using landmark analysis, path finding path optimization (Section 5.1), subgoal-level planningusing search-space reduction (Section 5.2). explain integration plannersanalyze effectiveness.Last, study Section 4.2 trade-offs solution time quality heuristics updating penalties violated global constraints. trade-offs allow usgenerate plans either better quality time (SGPlan4.1 ), lower quality327fiChen, Wah, & Hsuless time (SGPlan4 ). optimization quality requires estimation makespanmultiple subplans enhanced PERT algorithm (Section 5.3). previous workconstraint partitioning subgoals (Wah & Chen, 2004), focused minimizing planning time. Without optimizing quality, violated global constraints ofteneasier resolve planner always delay one actions order avoidconstraints. Finally, compare Section 7 performance plannersrespect planners.2. Locality Mutex Constraints Temporal Planningsection, define mutex constraints planning problems. Based structureconstraints IPC4 benchmarks, show constraint partitioning subgoalsleads constraints localized better constraint partitioning time.2.1 Representation Mutex Constraintsfollowing standard notations definitions literature (Hoffmann & Nebel, 2001;Garrido, Fox, & Long, 2002), summarize section basic definitions mutexconstraints used paper.Definition 1. planning problem = (O, F, I, G) quadruple, setpossible actions , F set facts, set initial facts, G setgoal facts.Definition 2. state = f1 , , fnS subset facts F true.Definition 3. STRIPS action associated following attributes:a) pre(a), set facts define preconditions action a;b) add(a), set facts define add effects a;c) del(a), set facts define delete effects a.resulting state applying action state defined as:((S add(a))\del(a) pre(a)Result(S, a) =pre(a) 6 S.(1)resulting state applying sequence actions a1 , , recursively definedas:Result(S, (a1 , , )) = Result(Result(S, (a1 , , an1 )), ).(2)Next, extend action model temporal planning. durative actions supportedPDDL2.2, precondition fact effective beginning, end,entire duration action; whereas add effect delete effect effectivebeginning end action.Definition 4. temporal action associated following attributes:a) s(a) e(a) define, respectively, start time end time a.328fiTemporal Planning using Subgoal Partitioning Resolutionactionactive mutexespre(a1 )add(a4 )a1pre(a5 )a4a5a2del(a2 )add(a2 )a7a6a3del(a3 )pre(a6 )del(a7 )timeFigure 4: example temporal plan, active mutexes actions showndashed lines, inactive mutexes dotted lines.b) preconditions divided three types: prestart (a), set initialpreconditions held s(a); preend (a), set final preconditions held e(a);preoverall (a), set invariant preconditions open interval (s(a), e(a)).c) two types add effects: addstart (a), set initial add effectsasserted s(a); addend (a), set final add effects asserted e(a).d) two type delete effects: delstart (a), set initial delete effectsasserted s(a); delend (a), set final delete effects asserted e(a).Definition 5. temporal plan P = {a1 , a2 , , } list temporal actions,ai assigned start time s(ai ) end time e(ai ).Figure 4 illustrates temporal plan seven actions. action, indicate,appropriate, preconditions, add effects, delete effects.Concurrent actions plan must arranged way observes mutualexclusions (mutexes). notion mutex first proposed GraphPlan (Blum & Furst,1997). defined planning graph, level-by-level constraint graphalternates fact level action level. Mutex relationships planning graphclassified transient (level-dependent) persistent (level-independent) (Blum& Furst, 1997). mutex transient exists certain levels graphvanishes levels graph built. contrast, mutex persistent holdsevery level fix-point level (the last level graph) achieved. paper,consider level-independent, persistent mutex relationships, transient mutexesexclusively used searches GraphPlan.Actions b marked persistently mutual exclusive one followingoccurs.329fiChen, Wah, & Hsua) Actions b persistent competing needs,1 competing needsrepresented persistent mutex preconditions b;b) persistent inconsistent effects, one action deletes add effectother.c) persistent interference, one action deletes precondition other.Two facts p q persistently mutual exclusive possible ways making p truepersistently exclusive possible ways making q true; is, actionp add effect (p add(a)) persistently mutual exclusive actionb q add effect (q add(b)). simplicity, rest paper, mutexactions facts refer corresponding persistent mutex actions facts.Given temporal plan, mutex relationship active inactive. example,actions a1 a2 Figure 4 active mutex two actions overlapexecution persistent interference. However, a2 a3 inactive mutexoverlap execution.Based discussion, conditions active mutex occur twoactions b summarized four cases (Garrido et al., 2002):a) Actions b start together, nonempty intersectioninitial preconditions (resp. add effects) initial delete effects (resp. delete effects).b) Actions b end together, nonempty intersectionfinal preconditions (resp. add effects) final delete effects (resp. delete effects).c) Action ends b starts, nonempty intersection finaldelete effects (resp. delete effects, add effects, preconditions) initial addeffects (resp. preconditions, delete effects, delete effects) b.d) Action starts (resp. ends) execution b, nonempty intersection initial (resp. final) delete effects invariant preconditionsb.conditions introduced prevent two mutually exclusive actionsexecuting simultaneously, may actions block propagation facts(no-op action) cause unsupported actions later. condition detectedlooking actions delete existing facts current plan. respectconditions mutex due competing needs, need represent explicitlymutexes due competing needs must accompany two types mutex:two preconditions mutually exclusive due competing needs, two actionsequences making true also mutually exclusive. example, active mutexa5 a6 Figure 4 due competing needs caused active mutexa3 a4 .mutex constraints studied paper closed form. Instead,defined discrete procedural function checks pair actions meet one fourconditions above. inputs function start time end timeaction, continuous temporal problems discrete propositional problems.1. terms competing needs, inconsistent effects, interference originally proposedGraphPlan (Blum & Furst, 1997).330fiTemporal Planning using Subgoal Partitioning Resolution110000111100001111001100110000111100001100110011111111111110000000000000110000000000011111111111001100000000000111111111110000000000011111111111000111000000000001111111111100011111000011Subproblem 21100001111000011Subproblem 10011001111111111111000000000000011000000000001111111111100110000000000011111111111000000000001111111111100110000000000011111111111001111000011110000111100001111001100110011001100001111000011110011001100110011000011110011001100001111000011110000111100001111000011Subproblem 31100001111000011110011001100110011000011110000111100001111000011a) 63 mutex constraints among actions b) Partitioning mutex constraints subgoalsFigure 5: Mutex constraints IPC4 AIRPORT-TEMP-4 instance. rectangularbox represents action, line joining two actions represents mutex constraint (that may inactive). constraints (52 63 83%) localconstraints partitioning subgoals. Global mutex constraintsshown dashed lines (b).2.2 Locality Mutex Constraintssection, evaluate partitioning mutex constraints planning benchmarks. analysis shows strong locality constraints partitionedsubgoals compared case partitioned time. studycriteria partitioning may lead subproblems whose initial final states specified. subproblems hard solve existing plannersmay require systematic enumeration initial final statesfinding feasible plans.Figure 5a shows 63 mutex constraints solution plan fourth instanceIPC4 AIRPORT-TEMP domain. instance involves moving three planes airportdesignated gates. rectangular box figure represents action, whereas linejoining two actions represents mutex constraint (that may inactive). Figure 5b showspartitioning constraints three subproblems, involving movementone plane. show local constraints (those relevant actions one subproblem) solid lines global constraints relating actions different subproblemsdashed lines. clear majority (83%) constraints local partitioningsubgoals.demonstrate localization mutex constraints partitioned subgoals,analyze IPC4 instances. first modify original Metric-FF planner (Hoffmann,2003) order support new features PDDL2.2, temporal actions derivedpredicates. instance, use modified planner find initial subplansubproblems. find mutexes among actions, including activeinactive ones. Finally, compute number global constraints related actions331firga,G0.60.40.201.00.60.40.20rga,G0.60.40.201.0rga,G0.60.40.200 5 10 15 20 25 30 35 40 45 50Instance ID0rg,Trg,G0.8rga,G0.60.40.200 5 10 15 20 25 30 35 40 45 50Instance IDf) UMTS-TEMP0.40.205241.0rg,Trg,G0.8rga,G0.60.40.20051015Instance IDg) DEPOTS-TIME2012140.60.40.2010 15 20 25 30 35 40Instance ID1.068 10Instance IDrg,Trg,Grga,G0.80 2 4 6 8 10 12 14 16 18 20Instance IDd) SATELLITE-TIMEGlobal-constraint fractionGlobal-constraint fractiond) PSR-SMALL1.00.6c) PROMELA-OPTICALTELEGRAPHrg,Trg,G0.8rg,Trg,Grga,G0.80Global-constraint fractionb) PIPESWORLD-NOTANKAGENONTEMPrg,Trg,G0.8rga,G1.00 5 10 15 20 25 30 35 40 45 50Instance IDGlobal-constraint fractionGlobal-constraint fraction1.0rg,Trg,G0.80 5 10 15 20 25 30 35 40 45 50Instance IDa) AIRPORT-TEMPGlobal-constraint fractionrg,Trg,G0.8e) SETTLERGlobal-constraint fraction1.0Global-constraint fractionGlobal-constraint fractionChen, Wah, & Hsu251.0rg,Trg,G0.8rga,G0.60.40.20051015Instance ID2025h) BLOCKSWORLDFigure 6: Variations rg,T , rg,G , rga,G across instances seven IPC4 domainvariants well instances DEPOTS-TIME domain variantIPC3 Blocksworld domain IPC2. (The latter two domainsdeemed difficult constraint partitioning.)different subplans, well number initial active global constraints basedsubplan evaluated subproblem. comparison, also evaluate partitioningconstraints temporal horizon.Figure 6 illustrates results seven IPC4 domain variants, well Blocksworlddomain IPC2 DEPOTS-TIME variant IPC3. Table 1 summarizesaverage statistics across instances IPC4 domain variantBlocksworld domain Depots domain variants. instance partitioningtime, use modified Metric-FF planner find initial plan, set numbertemporal stages number subgoals, partition horizonsolution plan evenly multiple stages. count number local constraintsstage number global constraints relating actions different stages.instance, let Nc total number mutex constraints, NgT number globalconstraints constraint partitioning time, NgG number global constraints332fiTemporal Planning using Subgoal Partitioning ResolutionTable 1: Average rg,T , rg,G , rga,G across instances IPC4 domains well Depotsdomain IPC3 Blocksworld domain IPC2. (The latter twodeemed difficult constraint partitioning.) Boxed numbers less 0.1.Domain Variantr g,Tr g,GAIRPORT-NONTEMPAIRPORT-TEMPAIRPORT-TEMP-TIMEWINDOWSAIRPORT-TEMP-TIMEWINDOWS-COPIPESWORLD-NOTANKAGE-NONTEMPPIPESWORLD-NOTANKAGE-TEMPPIPESWORLD-NOTANKAGE-TEMP-DEADLINEPIPESWORLD-TANKAGE-NONTEMPPIPESWORLD-TANKAGE-TEMPPIPESWORLD-NOTANKAGE-TEMP-DEADLINE-COPROMELA-OPTICAL-TELEGRAPHPROMELA-OPTICAL-TELEGRAPH-DPPROMELA-OPTICAL-TELEGRAPH-FLPROMELA-PHILOSOPHERPROMELA-PHILOSOPHER-DPPROMELA-PHILOSOPHER-FLPSR-SMALLPSR-MIDDLEPSR-MIDDLE-COPSR-LARGESATELLITE-STRIPSSATELLITE-TIMESATELLITE-TIME-TIMEWINDOWSSATELLITE-TIME-TIMEWINDOWS-COSATELLITE-NUMERICSATELLITE-COMPLEXSATELLITE-COMPLEX-TIMEWINDOWSSATELLITE-COMPLEX-TIMEWINDOWS-COSETTLERSUMTS-TEMPUMTS-TEMP-TIMEWINDOWSUMTS-TEMP-TIMEWINDOWS-COUMTS-FLAW-TEMPUMTS-FLAW-TEMP-TIMEWINDOWSUMTS-FLAW-TEMP-TIMEWINDOWS-CODEPOTS-STRIPSDEPOTS-SIMPLETIMEDEPOTS-NUMERICDEPOTS-TIMEBLOCKSWORLD0.5570.5680.4940.4950.6950.6820.6740.6870.6830.6820.5750.7590.7990.5540.8550.8220.8970.8960.8820.9020.6890.6860.6480.6330.2880.6420.6330.6980.5490.4630.4370.4070.4590.4280.4140.5370.5720.4910.4480.5490.2190.2080.1840.1880.3130.3010.2970.6770.4590.2960.3990.2650.4260.3700.5760.5070.4890.5040.4780.6650.2880.2890.1140.3070.3050.2820.1240.1530.4510.1570.1260.0980.1360.1100.0860.4180.3040.3540.2370.314333rga,G0.0170.0140.0130.0140.0440.0420.0330.0700.1260.0390.0520.0200.0370.0660.0190.0870.1140.0920.0490.0960.0960.0930.0270.0750.0780.0690.0410.0420.1000.0060.0080.0080.0060.0080.0070.2310.1670.1880.1970.254fiChen, Wah, & HsuG number initial active globalconstraint partitioning subgoals, Ngaconstraints constraint partitioning subgoals. compute following ratios:NgT: fraction global constraints constraint partitioning time;NcNgGrg,G =: fraction global constraints constraint partitioning subgoals;NcGNgarga,G =: fraction initial active global constraints subgoal partitioning.Ncrg,T =respect instances IPC4 domains, results show constraint partitioning subgoals leads lower rg,G rg,T , fractions vary significantly,rga,G small instances. Except PSR-SMALL SETTLERS, rga,Gconsistently less 0.1. behavior important active constraintsneed resolved planning, number constraints decreaseplanning progresses. describe Section 4.2 two strategies reducing numberactive global constraints planning.behavior worse instances Blocksworld domain Depots domain variants. two domains, rga,G consistently high (over 20%) constraintspartitioned subgoals. reason actions different subgoalsinstance highly related, making difficult cluster constraints leadinglarger fraction global constraints. evaluate performance approachtwo domains Section 7.3. Constraint Partitioning using Penalty FormulationsGiven constrained formulation planning problem, summarize sectiontheory extended saddle points mixed space (Wah & Chen, 2006) designplanners based upon.3.1 Extended Saddle-Point ConditionConsider following MINLP variable z = (x, y), x Rv Dw :(Pm ) :minzsubjectf (z),(3)h(z) = 0 g(z) 0,f continuous differentiable respect x, g = (g1 , . . . , gr )T h =(h1 , . . . , hm )T general functions necessarily continuous differentiable.assumptions important constraints planners proceduralfunctions closed form. assume f lower bounded, g hunbounded.goal solving Pm find constrained local minimum z = (x , )respect Nm (z ), mixed neighborhood z . results publishedearlier (Wah & Chen, 2006), summarize high-level concepts withoutprecise formalism.334fiTemporal Planning using Subgoal Partitioning Resolutionmixed neighborhood Nm (z), z = (x, y), mixed space Rv Dw is:fifififi(4)Nm (z) = (x , y) x Nc (x) (x, ) Nd (y) ,Definition 6.Nc (x) = {x : kx xk 0} continuous neighborhood x,discrete neighborhood Nd (y) finite user-defined set points {y Dw }.Definition 7. Point z CLMm , constrained local minimum Pm respectpoints Nm (z ), z feasible f (z ) f (z) feasible z Nm (z ).Definition 8.penalty function Pm penalty vectors Rm Rr is:Lm (z, , ) = f (z) + |h(z)| + max(0, g(z)).(5)Next, define informally constraint-qualification condition needed main theorem (Wah & Chen, 2006). Consider feasible point z = (x , ) neighboring pointz = (x + p~, ) infinitely small perturbation along direction p~ X x subspace. constraint-qualification condition satisfied z , means~p rates change equality active inequality constraintsz z zero. see necessary, assume f (z) z decreases along p~equality active inequality constraints z zero rates change zz . case, possible find finite penalty values constraintsz way leads local minimum penalty function z respectz . Hence, scenario true p~ z , possiblelocal minimum penalty function z . short, constraint qualificationz requires least one equality active inequality constraint non-zero ratechange along direction ~p z x subspace.Theorem 1. Necessary sufficient ESPC CLMm Pm (Wah & Chen, 2006).Assuming z Rv Dw Pm satisfies constraint-qualification condition, zCLMm Pm iff exist finite 0 0 satisfies following extendedsaddle-point condition (ESPC):Lm (z , , ) Lm (z , , ) Lm (z, , )(6)> > z Nm (z ), Rm , Rr .Note (6) satisfied rather loose conditions requireslarger critical . theorem importantestablishes one-to-one correspondence CLMm z Pm ESP(extended saddle point) corresponding unconstrained penalty function (5)penalties sufficiently large. theorem also leads easy way finding CLMm .Since ESP local minimum (5) (but converse), z found graduallyincreasing penalties violated constraints (5) repeatedly findinglocal minima (5) feasible solution Pm obtained. possibleexist many algorithms locating local minima unconstrained functions.335fiChen, Wah, & Hsu3.2 Partitioned Extended Saddle-Point Conditionimportant feature ESPC Theorem 1 condition partitionedway subproblem implementing partitioned condition solvedlooking larger .Consider Pt , version Pm whose constraints partitioned N subproblems:(Pt ) :minzsubjectJ(z)h(t) (z(t)) = 0,H(z) = 0,g(t) (z(t)) 0G(z) 0(local constraints)(7)(global constraints).Subproblem t, = 1, . . . , N , Pt local state vector z(t) = (z1 (t), . . . , zut (t))T utmixed variables, Nt=1 z(t) = z. Here, z(t) includes variables appear(t)(t)mt local equality constraint functions h(t) = (h1 , . . . , hmt )T rt local inequal(t)(t)ity constraint functions g(t) = (g1 , . . . , grt )T . Since partitioning constraints,z(1), . . . , z(N ) may overlap other. H = (H1 , . . . , Hp )T G = (G1 , . . . , Gq )Tglobal-constraint functions z. assume J continuous differentiablerespect continuous variables, f lower bounded, g, h, G, Hgeneral functions discontinuous, non-differentiable, unbounded.first define Np (z), mixed neighborhood z Pt , decompose ESPC(6) set necessary conditions collectively sufficient. partitionedcondition satisfied finding local ESP subproblem, violated globalconstraints resolved using appropriate penalties.Np (z), mixed neighborhood z partitioned problem, is:NN fi[[fi(t)fiNp (z) =Np (z) =/ z(t) ,z fi z (t) Nm (z(t)) zi = zi ziDefinition 9.t=1(8)t=1Nm (z(t)) mixed neighborhood z(t).Intuitively, Np (z) separated N neighborhoods, tth neighborhood perturbs variables z(t) leaving variables z\z(t) unchanged.Without showing details, consider Pt MINLP apply Theorem 1derive ESPC Pt . decompose ESPC N necessary conditions, onesubproblem, overall necessary condition global constraints acrosssubproblems. first define penalty function Subproblem t.Definition 10. Let (z, , ) = |H(z)|+ max(0, G(z)) sum transformedpglobal constraint functions weighted penalties, = (1 , . . . , p )T Rq= (1 , . . . , q )T R penalty vectors global constraints. penaltyfunction Pt (7) corresponding penalty function Subproblem definedfollows:NX(t)(t)Lm (z, , , , ) = J(z) +(t) |h (z(t))| + (t) max(0, g (z(t)) +(z, , ), (9)t=1(z, (t), (t), , ) = J(z) + (t)T |h(t) (z(t))| + (t)T max(0, g(t) (z(t))) + (z, , ), (10)336fiTemporal Planning using Subgoal Partitioning Resolution(t) = (1 (t), . . . , mt (t))T R (t) = (1 (t), . . . , rt (t))T Rpenalty vectors local constraints Subproblem t.rtTheorem 2. Partitioned necessary sufficient ESPC CLMm Pt (Wah & Chen,2006). Given Np (z), ESPC (6) rewritten N + 1 necessary conditions that,collectively, sufficient:(z , (t), (t), , ) (z , (t) , (t) , , ) (z, (t) , (t) , , ), (11)Lm (z , , , , ) Lm (z , , , , ),(12)(t) > (t) 0, (t) > (t) 0, 0, 0,rpq(t)z Np (z ), (t) R , (t) R , R , R , = 1, . . . , N .Theorem 2 shows original ESPC Theorem 1 partitioned N necessary conditions (11) overall necessary condition (12) global constraintsacross subproblems. partitioned condition Subproblem satisfiedfinding ESPs subproblem. finding ESP equivalent solvingMINLP, reformulate search Subproblem solution followingoptimization problem:(t)PtJ(z) + |H(z)| + max(0, G(z))(13):minz(t)subjecth(t) (z(t)) = 0g(t) (z(t)) 0.(t)weighted sum global constraint functions objective Pt importantleads points minimize violations global constraints.(t)large enough, solving Pt lead points, exist, satisfy global constraints.short, finding solutions Pt satisfy (6) reduced solving multiplesubproblems, (13) solved existing solver modificationsobjective function optimized, reweighting violated global constraintsdefined (12).3.3 Formulation Partitioned Planning Subproblems PDDL2.2PDDL2.2 planning problem solved paper, solution plan specifiedstart time end time action O. Hence, variable vector z ={s(a), e(a) O}; objective function J(z) optimized depends makespan(or number actions propositional domains) plan z; constraintsmutex constraints defined Section 2.1:h(ai , aj ) = mutex s(ai ), e(ai ), s(aj ), e(aj ) = 0,ai , aj O.(14)Here, mutex binary procedure checking whether ai aj satisfy mutex conditions defined Section 2.1. returns one conditions satisfied zero otherwise.constraints partitioned subgoals N subproblems G1 , , GN ,variable z partitioned N subsets z(1), , z(N ), z(t) includes start time337fiChen, Wah, & Hsu(1)PtxLm(z, , , , ), findminz(1) J(z) + |H(z)| + max(0, G(z)):subject h(1)(z(1)) = 0 g (1)(z(1)) 0(N)Ptminz(N) J(z) + |H(z)| + max(0, G(z)):subject h(N) (z(N )) = 0 g (N) (z(N )) 0a) Partitioned search look points satisfy (11) (12)1. procedure partition resolve(Pt )2.0; 0;3.repeat// increase penalties violated global constraints maximum bounds //4.= 1 p (Hi (z) 6= 0 < ) increase end end for;5.j = 1 q (Gj (z) 0 j < j ) increase j end end for;// inner loop solving N subproblems //6.= 1 N apply existing solver solve (13) end for;7.((i > Hi (z) 6= 0 j > j Gj (z) 0) (a CLMm Pt found))8. end procedureb) Implementation finding CLMm Pt satisfies (11) (12)Figure 7: partition-and-resolve procedure look CLMm Pt .end time actions Gt . local constraints mutex constraintsrelate actions within subproblem, global constraints relateactions across subproblems.(t)Pt defined Gt , objective find feasible plan z(t) satisfiesconstraints Gt , minimizing objective function biased violated globalconstraints:(t)Pt:minz(t)subjectJ(z) +NXk=1k6=tt,k mt,kh(t) (ai , aj ) = 0(15)ai , aj z(t),J(z) defined later Section 5.3. Here, mt,k number global constraintsactions z(t) z(k):mt,k =Xh(at , ak ).(16)z(t)ak z(k)k6=tlimit number penalties characterizing priorities among subproblems,assigned single penalty t,k pair subproblems Gt Gk , insteadpenalty global constraint Gt Gk .338fiSubgoal-Level PlanningGlobal-Level PlanningTemporal Planning using Subgoal Partitioning ResolutionPlanEvaluationTechniquesStudiedPenalty-ValueUpdate StrategyGlobalConstraintResolutionGlobal Constraints SubgoalsG1P1,1G2ProducibleResourcesGNPN,1P1,c1PN,cNConstraintPartitioningSubgoalsLandmarkAnalysisTemporalEngineModified MetricFFDerivedpredicatesengineTemporalengineDerivedPredicatesEngineSearchspacereductionSearchSpaceReductionFigure 8: SGPlan4 : planner implementing partition-and-resolve procedure Figure 7.3.4 Partition-and-Resolve ProcedureFigure 7 presents partition-and-resolve procedure finding points satisfyconditions Theorem 2. Using fixed specified outer loop, inner loopSubproblem Figure 7b solves (13) existing solver, results ESPsatisfies (11). possible (13) well-defined MINLP. solvingN subproblems, penalties violated global constraints increased outerloop. process repeated CLMm Pt found exceedmaximum bounds.procedure Figure 7 may generate fixed points (9) satisfy (11)(12). happens ESP local minimum (9) (but converse). Oneway escape fixed points allow periodic decreases . goaldecreases lower barrier penalty function order local descentsinner loop escape infeasible region. Note decreasedgradually order help search escape infeasible regions.reach minimum thresholds, scaled up, search repeated.339fiChen, Wah, & Hsu4. System Architecture SGPlan4Figure 8 shows design SGPlan4 implements partition-and-resolve procedure.procedure alternates global-level planning subgoal-level planning.section, describe techniques implemented global level, leavingdiscussion techniques subgoal level next section.4.1 Partition-and-Resolve Process SGPlan4global level, SGPlan4 partitions planning problem N subproblems, G1 , , GN ,Gt corresponds tth subgoal. orders subproblems, evaluatesusing techniques subgoal-level planning, identifies violated global constraints,updates penalties order bias search next iteration towards resolvingthem. SGPlan4 , adopted implementation LPG1.2 (Gerevini & Serina,2002) detecting persistent mutexes.partition-and-resolve process understood calculating subplans separatelymerging consistent plan. goals optimize multiple subplansensure consistency merging. Prior work plan merging focusesmerging redundant actions finding optimal composed plan. particular, Foulser,Li, Yang (1992) developed algorithms merging feasible classic plansefficient ones. complete evaluation plan-merging algorithms classical domainsconducted Yang (1997). Tsamardinos, Pollack, Horty (2000) extendedconcept domains temporal constraints. plan merging meansmaking infeasible plan feasible, different approach aims resolveinconsistencies terms mutexes among subplans.alternative view resolution approach reuse modificationsubplans consistent plan. Plan-reuse systems adapt existing plans new initial states goals. approach demonstrated SPA (Hanks & Weld, 1995)PRIAR (Kambhampati & Hendler, 1992) show improvements efficiency manydomains. major difference current plan-reuse approaches partitionand-resolve process generate candidate subproblems based partitioningmutex constraints, whereas traditional methods reuse plans generatedmeans. Since assumption conservative plan modification existing methodsalways achievable, may necessary replan feasible plan candidate cannotfound. cases, may expensive planning scratch.reason complexity analysis empirical study cannot prove plan-reuse approachesconsistent improvements planing scratch (Nebel & Koehler, 1995).contrast, approach augments search subproblem explicitly penalizingglobal inconsistencies forcing solution towards resolving global constraints.partition-and-resolve approach different incremental planning (Koehler &Hoffmann, 2000) uses goal agenda. incremental planning, planner maintainsset target facts, adds goal states incrementally target set, extendssolution using new target set. goal state must always satisfiedachieved, ordering goal states important order avoid un-doingpreviously achieved goal state planning current goal state. invalidationsoccur, planning task point complex planning one340fiTemporal Planning using Subgoal Partitioning Resolutiongoal state. contrast, SGPlan4 tries achieve one subgoal time allowssubgoals invalidated process. Moreover, subgoal, needstart ending state previous subgoal incremental learning,need pre-order subgoals order avoid invalidations. show Section 6performance SGPlan4 sensitive order evaluating subgoals.4.2 Resolving Violated Global Constraintssection, present two penalty-update strategies resolving violated global constraints. constraints identified finding subplan subproblem independently.SGPlan4 first initializes penalties global constraints starts. firstiteration, SGPlan4 solves subproblem individually, without considering globalconstraints. combines subplans integrated plan order determine initial active global constraints across subproblems. subsequent iterations,SGPlan4 finds local feasible plan subproblem, minimizing global objective weighted sum violated global constraints. end iteration,SGPlan4 increases penalty violated global constraint proportion violation.process ends constraints satisfied.designed two strategies updating penalty global constraints.SGPlan4 participated IPC4 sets large initial penalty values updatesrate , whereas SGPlan4.1 studied paper sets initial penalty values zero:(0 (for SGPlan4 )(0)()(1)t,k =t,k = t,k + mt,k ,= 1, 2, . . .(17)0(for SGPlan4.1 ),()Here, t,k penalty global constraints Gt Gk th iteration,mt,k defined (16), 0 large initial value, parameter controllingrate penalty updates. experiments, set 0 = 100 = 0.1.Figure 9 illustrates planning process SGPlan4 AIRPORT-TEMP-14 instance. Given three subproblems instance, SGPlan4 first evaluates subproblem first iteration order determine initial active global constraints.figure shows, respectively, subplans active global constraints evaluatingthree subproblems second iteration. strategy effective reducingnumber active global constraints quickly 14 beginning zero oneiteration.penalty-update strategy SGPlan4 may lead longer makespans useslarge initial penalty values order reduce number violated global constraintsquickly. Hence, subplans found may poor temporal concurrency. addressissue, implemented new strategy SGPlan4.1 (17) sets initial penaltyvalues zero.Figure 10 illustrates time-quality trade-offs SGPlan4 SGPlan4.1 usedsolve nine representative instances IPC4, Blocksworld, Depots domains.number active global constraints changes evaluating subproblem,plot progress remaining number active global constraints respecttotal number subproblems evaluated. results show planners341fiChen, Wah, & Hsu1001 001 110101001100110101001110100101000110 1101001101010G110101 0011 010G210011001100110011001001101011010010110100011100110010101100110100 01110011001G3100110101001a) start Iteration 21001 001 110101101001 00101 1001101010101001000 11101100 11001101010101001101001011010001110101011 000110011010101010001101101001011001101010011010100110011001101010100101100 011011001 001 10101011001 001101 1001110100101000110 1101001101010100110100110100110101 0011 0101001001101011010010110001101100110100101100110100 011G20101100 011011001G310011001101010101010G1b) solving Subproblem G10111 001 00101010110011001100001001111 1001000 11101100 11001G1G20101100 011011010101010011001G31010c) solving Subproblem G201011010001110101011 000110011010101010001101101001011001100110101001011010011010G1G20101100 011011001G31010d) solving Subproblem G3Figure 9: planning process IPC4 version SGPlan4 second iterationsolving AIRPORT-TEMP-14 instance. box corresponds actionsubplan, whereas arrow corresponds active global constraint.placing emphasis violated global constraints, number violatedconstraints quickly reduced expense longer makespan.resolve remaining number active global constraints almost linear fashion,SGPlan4 generally faster resolving active global constraints generatesplans worse quality. detailed experimental results Section 7, showSGPlan4.1 generally leads plans better quality.planners, however, difficulty solving PIPESWORLD-NOTANKAGETEMP-DEADLINE-10 instance (Figure 10c). domain, SGPlan4 cannot solveinstances, whereas SGPlan4.1 solve eight instances (1, 2, 5, 6, 8, 14, 22, 30). Although fraction initial active global constraints constraints 3.3%average (Table 1), planners may get stuck infeasible solutions cannot make progress afterward. reason basic planner SGPlan4SGPlan4.1 enough backtracking generate new candidate subplanssubproblem. Hence, basic planner keeps generating subplan point,regardless violated constraints penalized.4.3 Handling Producible Resourcesplanning problems, may facts made true numericalresources produced anytime needed. example, Settlers domain,342fi15105060SGPlang (Q=61.73)SGPlang2 (Q=52.00)504030201008 10 12 14 16 18 20 22 24 26 288total # subproblems evaluated1005003040506070100908070605040302010080108642015202530total # subproblems evaluatedg) UMTS-TEMP-502022510152025303540total # subproblems evaluated35454035302520151050SGPlang (Q=544.00)SGPlang2 (Q=541.00)15 20 25 30 35 40 45 50 55 60total # subproblems evaluatedtotal # subproblems evaluatede) SATELLITE-TIME-20# active global constraints# active global constraintsSGPlang (Q=2230.40)SGPlang2 (Q=818.00)101840 60 80 100 120 140 160 180 200d) PROMELA-OPTICALTELEGRAPH-101216SGPlang (Q=704.26)SGPlang2 (Q=645.01)total # subproblems evaluated1414# active global constraints# active global constraints# active global constraints1502012SGPlang (Q=N/A)SGPlang2 (Q=N/A)total # subproblems evaluatedSGPlang (Q=198.98)SGPlang2 (Q=197.34)2001050454035302520151050b) PIPESWORLD-NOTANKAGE- c) PIPESWORLD-NOTANKAGENONTEMP-30TEMP-DEADLINE-10a) AIRPORT-TEMP-30250# active global constraintsSGPlang (Q=708.10)SGPlang2 (Q=705.03)2070# active global constraints25# active global constraints# active global constraintsTemporal Planning using Subgoal Partitioning ResolutionSGPlang (Q=56.00)SGPlang2 (Q=42.00)60504030201001520253035total # subproblems evaluatedh) BLOCKSWORLD-17-040f) SETTLERS-20200180160140120100806040200SGPlang (Q=107.00)SGPlang2 (Q=103.00)10152025303540total # subproblems evaluatedi) DEPOTS-TIME-20Figure 10: Resolution active global constraints nine benchmark instances original penalty-update strategy SGPlan4 new penalty-update strategySGPlan4.1 . x axis includes number subproblems evaluated,corresponding subgoal, first iteration order determine initialactive global constraints.coal always produced mine. define producible logical numericalresources follows.a) fact producible add effect either action without preconditionsaction whose preconditions always producible.b) numerical resource producible increased either action withoutpreconditions action whose preconditions always producible.planning tasks significantly easier producible facts resourcesdetected preprocessing phase made available planning. firstidentifying facts resources, SGPlan4 derives relaxed initial state settingproducible facts true producible numerical resources large enough.Every time producible fact turned false, made true again. finding feasibleplan relaxed initial state, SGPlan4 removes unused numerical resources343fiChen, Wah, & Hsuinitial state plans again. process repeated redundant initialresources. point, SGPlan4 inserts necessary actions beginning plangenerate minimum initial producible resources needed.example, suppose timber detected producible resource one alwaysfell trees get timber. SGPlan4 initially set large number, say 1000 units,timber available. solving problem, suppose 900 units left unused,reduces initial timber 100 units plans again. process repeatedeither unused timber final state problem becomes unsolvablereducing initial resource.Note approach may incur redundant actions producing unused resources, optimal amount resources needed cannot predicted ahead time.5. Subgoal-Level Planningsubgoal level, SGPlan4 applies landmark analysis partition subproblem,performs path finding optimization, carries subspace-reduction analysis pruneirrelevant facts actions subproblem, calls modified Metric-FF plannersolve subproblem.5.1 Subgoal-Level Decomposition Techniquesa) Landmark analysis. First proposed Porteous, Sebastia, Hoffmann (Porteous,Sebastia, & Hoffmann, 2001), landmark analysis allows large planning problemdecomposed series simpler subproblems. Given initial state, aims findintermediate facts must true feasible plan reaching goal state.example, assume object delivered D, pathB C D. (O, B) (O, C) landmark facts,since feasible plan must make true reaching goal state (O, D).planning problem first partitioned subgoals subproblems,apply landmark analysis subproblem order find intermediate factsreaching corresponding subgoal. Landmark analysis important SGPlan4allows subproblem decomposed simpler subproblemssolved easily.subproblem, find landmarks relaxed planning approach. Given planning subproblem = (O, F, I, G), first construct relaxed planning graphinitial state ignoring delete effects actions. force f F levelgraph false (even made true actions). result, actionspreconditioned f pruned. exists goal fact G cannot reachedf false, f landmark fact must reached plan relaxedproblem. finding partial order landmarks, SGPlan4 builds sequentiallist subproblems joined landmarks found applies basic planner solvesubproblem order. Note landmark analysis expensive, SGPlan4detects landmarks beginning every iteration.landmarks found relaxed planning graph necessary solutionplan original problem also solution plan relaxed problem. Hence,feasible plan original problem must reach landmark found relaxed ap344fiTemporal Planning using Subgoal Partitioning ResolutionInitial StateON(B10 A2)ON(B0 A1)LAST(B10 S12)ON(B4 A2)ON(B6 A2)ON(B12 A3)LAST(B4 S12)FIRST(B0 S12)LAST(B12 S13)ON(B9 A2)ON(B8 A3)LAST(B9 S12)LAST(B8 S13)LAST(B6 S12)ON(B10 A1)ON(B5 A2)LAST(B5 S12)ON(B12 A1)ON(B9 A1)ON(B8 A1)FIRST(B9 S13)FIRST(B10 S13)ON(B10 A3)FIRST(B12 S12)ON(B0 A2)ON(B4 A1)ON(B6 A1)ON(B12 A2)FIRST(B8 S12)ON(B9 A3)ON(B8 A2)ON(B5 A1)Goal StateFigure 11: Landmarks partial orders PIPESWORLD-NOTANKAGENONTEMP-10 instance.proach least once. However, landmarks found sufficient test goalreachability relaxed approach, may exist undetected landmarks evenevery fact tested.Figure 11 shows landmarks found IPC4 PIPESWORLD-NOTANKAGENONTEMP-10 instance. considering first goal fact (B10, A3), LAST (B10, S12)landmark also landmarks (B10, A1) F IRST (B10, S13).means LAST (B10, S12) must ordered (B10, A1) F IRST (B10, S13).way, decompose subproblem (B10, A3) 4 smaller tasksmust carried sequence, namely, LAST (B10, S12), (B10, A1), F IRST (B10, S13),(B10, A3).b) Landmarks identified path finding. Landmark analysis may sometimes producelandmark facts decomposing subproblem. example, gatesalong path Airport instance identified landmark facts (that is,must-visit points) usually multiple paths given source destination. Consider airport topology Figure 12a goal move A1SG1 SG8. two alternative paths none facts(A1, SG2), (A1, SG3), , (A1, SG7) true reaching SG8, cannot detect landmark facts.identify landmark facts decomposing subproblem, developedSGPlan4 new path-finding technique. technique based concept factgroups used existing planners, MIPS (Edelkamp, 2002)Downward (Helmert & Richter, 2004). fact group includes group mutually exclusivefacts one true time, typically involves multiple possiblestates object. example Airport instance discussed above, fact group includesdifferent locations A1 at:Fg =(A1, SG1), (A1, SG2), , (A1, SG7), (A1, SG8) .(18)345fiChen, Wah, & Hsu(A1, SG4)(A1, SG2)(A1, SG3)(A1, SG3)(A1, SG8)(A1, SG1)(A1, SG4)(A1, SG2)(A1, SG8)(A1, SG1)(A1, SG5) (A1, SG6)(A1, SG7)(A1, SG5) (A1, SG6)a) Transition graph Fg(A1, SG7)b) Path findingFigure 12: Illustration transition graph Fact Group Fg path finding algorithm. Shaded nodes (b) new landmark facts detected path finding.SGPlan4 , adopted approach MIPS based analysis static mutexgroups finding fact groups subgoal facts.apply path finding Subproblem Gt none landmarksdetected landmark analysis. Assuming subgoal reached gt , first findfact group belongs to. previous example, subgoal gt = (A1, G8),fact group Fg (18).fact group two facts, determine transition relationsconstructing directed graph. Given two facts f1 f2 fact group, add edgef1 f2 exists action f1 precondition f2add effect (which implies f1 delete effect since f1 f2 mutuallyexclusive). Figure 12a illustrates transition graph airport example discussedabove.Last, find path, look facts immediate predecessors gtgraph. arbitrarily select one must-visit landmark disable others.perform landmark analysis initial fact gt . analysis returnlandmark facts.example airport instance, (A1, SG4) (A1, SG7) two immediatepredecessor facts Subgoal gt = (A1, SG8). disable (A1, SG7) landmarkanalysis, one path (A1, SG1) gt , (A1, SG2),(A1, SG3), (A1, SG4) detected landmark facts. Figure 12b illustratesprocess.c) Path optimization used find better landmark facts problems timedinitial literals numerical effects. invoked deadlinedynamically changing numerical resource appears preconditions actions.conditions satisfied IPC4 Satellite instances technique founduseful.technique works choosing path optimizes time duration usagenumerical resource multiple paths different quality, settingnodes along optimal path landmark facts. Given subproblem trying reachSubgoal gt , construct transition graph fact group gt apply Dijkstrasalgorithm find shortest path initial fact gt . weight edgeeither time duration problems time windows, usage numerical resource346fiTemporal Planning using Subgoal Partitioning Resolution0G1a3a1a2G2Timea5a4a6S6G3S4S2S0S5S3S1Figure 13: Generating multiple starting states Subproblem G3 , given initial stateS0 Si , = 1, . . . , 6, state action ai finished. SGPlan4 callsbasic planner generate local subplan starting state picksfirst one improves objective (15).problems numerical preconditions. set facts along optimal pathlandmark facts force planner choose path others. landmarks alongoptimal path allows us decompose problem subproblems.two limitations current implementation path optimization. First,since needs path initial fact goal fact transition graph,cannot apply technique initial goals facts disconnected. Second,studied case one dynamically changing numerical resource appearspreconditions actions studied optimization multiple numericalresources.5.2 Subgoal-Level Planning Techniquesa) Evaluating multiple subplans subproblem. finding local feasible subplansubproblem improves objective (15), SGPlan4 generates number subplansmultiple starting states. Since active global constraints exist two identicalsubplans, generate multiple starting states given subproblem applying possible prefix actions subproblems. example, given six actionsplanned G1 G2 Figure 13, six possible starting states developingsubplan G3 . starting state, SGPlan4 calls basic planner generate localfeasible subplan accepts subplan improves objective (15). bettersubplans found possible starting states, SGPlan4 leaves local subplanunchanged moves next subproblem.b) Search-space reduction. solving partitioned subproblem, often eliminate search space many irrelevant actions related facts subgoalssubproblems. reductions useful planning problemspartitioned actions generally relevant.347fiChen, Wah, & Hsuexample, consider transportation domain whose goal move packages,drivers, trucks various locations initial configuration. Suppose problem instance, goal set {AT (D1, S1), (T 1, S1), (P 1, S0), (P 2, S0)} twopackages P 1 P 2, one driver D1, one truck 1, two locations S1 S2. Without partitioning, actions relevant resolving subgoals. contrast,partitioning, actions moving P 2 around irrelevant subproblem resolving (P 1, S0) eliminated. Similarly, actions moving P 1 P 2irrelevant subproblem resolving (D1, S1).designed backward relevance analysis eliminate irrelevant actionssubproblem solving basic planner. analysis, maintainopen list unsupported facts, close list relevant facts, relevance list relevantactions. beginning, open list contains subgoal facts subproblem,relevance list empty. iteration, fact open list, findactions support fact already relevance list. addactions relevance list add action preconditions close listopen list. move fact open list close list processed.analysis ends open list empty. point, relevance list containpossible relevant actions. analysis takes polynomial time.Note relevance analysis complete stops, since relevance listmay still contain irrelevant actions. example, reduce relevancelist forward analysis finding applicable actions initial statesbackward analysis. However, analysis may cost effective reducingoverhead planning.reduction method belongs family heuristics proposed Nebel, DimopoulosKoehler (1997). Since select possible supporting actions processing fact,approach indeed one selects union elements possibilityset according classification. conservatively reduce irrelevant information, number tighter reductions approximately minimize useinitial facts (Nebel et al., 1997). However, aggressive heuristics may solutionpreserving solution-length preserving.5.3 Modified Metric-FF Basic Plannerdecomposing subproblem associated subgoal smaller subproblems boundedlandmark facts, SGPlan4 solves subproblem identified (or original subproblemcase landmark facts identified) modified Metric-FF planner. modifications consist two components: adaptation original Metric-FF (Hoffmann,2003) order entertain new features PDDL2.2, support planningmutex constraints partitioned. fact, lot efforts embedding Metric-FFSGPlan4 spent first component.original Metric-FF solve problems PDDL2.1 propositional actionssupport temporal features. extended parser Metric-FFsupport full PDDL2.2 syntax definition actions atomic logical durational temporal. planning process also extended sequential propositionalplanning parallel temporal planning. Specifically, extended sequential actions348fiTemporal Planning using Subgoal Partitioning ResolutionFixedSubproblemsG1.....Components Objective Function(15)Estimated makespan TeGt1Gt+1PN.....k=1k6=tGNt,ke t,kWeighted sum global mutexconstraint violationsGtCurrent PlanHeuristic value (z(t)) original Metric-FFRelaxed PlanFigure 14: Temporal planning partitioned search context incorporates objectivefunction (15) makespan Te estimated enhanced PERT algorithmheuristic value Metric-FF planner.atomic length original Metric-FF actions predefined durationsscheduled parallel.extended Metric-FF support new feature called derived predicates introduced PDDL2.2. Derived predicates define axioms whose facts derived setprecondition facts. example, domain boxes, B BC, derived predicate C generated. Derived predicatesappear preconditions goals effects. modified Metric-FF,implemented technique proposed MIPS 2.2 (Edelkamp, 2003) handling derivedpredicates. encode derived predicate special action a, preconditionfacts preconditions facts d, add effects derived facts d,delete effect empty. planning, derived-predicate actionsincluded relaxed plan. However, heuristic function computed Metric-FFcounts number real actions relaxed plan number derivedpredicate actions, real actions considered candidates forward expansionstate. state, expand set true facts applying applicable derivedpredicates iteratively reach fixed-point state true factsadded.second component modifications Metric-FF involves supportpartitioned search context solving subproblem, say Gt . case, Metric-FF needsincorporate objective aggregate state schedulable actions G1 , , GNplanning actions Gt . Referring Figure 14, aggregate state representedestimated makespan Te actions evaluated enhanced PERTalgorithm.PERT originally developed generate parallel plan scheduling actionearly possible blocked dependency mutex relation. PreviousPERT algorithms detect propositional conflict two actions checking oneaction adds/deletes anothers precondition, detect numerical conflict two actionsmodify numerical variable. latter case, two actions would allowed349fiChen, Wah, & Hsuoverlap execution consume resource, even totalamount required exceed amount available. Obviously, resulting schedulesuboptimal.developed enhanced PERT algorithm considers resource constraintsschedule. algorithm assigns action early possible longpropositional conflicts violations numerical/resource constraints. Besidesmaintaining operator dependency original PERT, also keep track changesnumerical variables. algorithm greedy schedules applicable actionsearly possible without backtracking.general, PERT schedule valid sequential plan parallel plan without mutexconflicts. However, enhanced PERT may generate parallel plan mutex conflicts.reason subproblem solved initial state sequentiallystate previous subproblem. Hence, actions multiple subplanscombined, one action may delete precondition another causes mutex conflict.example, consider sequential plans two subproblems G1 G2 scheduledinitial state Blocksworld domain: a) MOVE (A, B) MOVE (B, C);b) MOVE (D, E) MOVE (E, C), MOVE (x, y) places x top y,precondition CLEAR (y) (y clear nothing it). example, PERT cannotgenerate parallel plan mutex conflict MOVE (E, C) MOVE (B, C),regardless two actions scheduled. conflict occurs actiondeletes CLEAR (C) precondition other.modified Metric-FF planner carries search heuristically looks plansminimize (15) rewritten follows:PNNe t,kminz(t) (z(t)) + k=1 t,kXk6=tmin J(z) +t,je t,j =PNz(t)ej=1e t,kminz(t) (z(t)) + + k=1 t,kj6=tk6=t(for SGPlan4 )(19)(for SGPlan4.1 ),(z(t)) heuristic value original Metric-FF solving Gt ;e t,kestimated number active mutexes plan Gk relaxed plan Gtobtained ignoring delete effects unscheduled actions; Te makespan estimatedenhanced PERT algorithm composing relaxed plan Gt planssubproblems; t,k penalty value dynamically updated global-level planning;constant fixed 0.0001. Although search guarantee optimality,always resolve global mutual-exclusion constraints between, say z(t) z(k),move one subplan backward order avoid overlapping another conflictingsubplan penalty t,k large enough.implementation (19) modified Metric-FF planner, setSGPlan4.1 small penalty term due makespan dominateterms. fact, since Te much smaller one test problems, mainpurpose break ties among states close heuristic values.hand, implementation (19) SGPlan4 IPC4 include Te objectivefunction. result, focuses eliminating mutual-exclusion conflicts tendsgenerate plans longer makespan.350fiTemporal Planning using Subgoal Partitioning Resolution1. procedure SGPlan(problem file)2.parse problem file instantiate facts actions;3.detect encode timed initial literals (TIL);4.detect encode derived predicates;5.detect TIL wrappers translate regular TILs;6.detect producible resources;7.(there producible resources) set maximum possible end if;8.repeat9.subgoal fact goal list10.call search-space reduction eliminate irrelevant actions;11.call basic planner (modified Metric-FF) reach subgoal;12.(the basic planner times out)13.perform landmark analysis generate list subproblems;14.subproblem list15.call basic planner solve subproblem;16.(solution found time limit)17.(problem TIL numerical fluents) perform path optimization18.else perform path finding decompose subproblem end if;19.call basic planner solve decomposed subproblem;20.end21.end22.end23.end24.evaluate plan z update penalty values violated global constraints;25.feasible solution plan found time limit exceeded;26.((new solution found) && (there unused producible resources))27.reduce initial producible resources goto step 8;28.end29. end procedureFigure 15: high-level pseudo code common SGPlan4 SGPlan4.1 .general, embedding basic planner partition-and-resolve framework requiresmodifications objective function basic planner order implement(15). Hence, cannot done without source code basic planner.5.4 Putting Pieces TogetherFigure 15 shows high-level code common SGPlan4 SGPlan4.1 .preprocessing phase parses problem file instantiates facts actions (Line2), detects encodes timed initial literals (TIL) derived predicates, (Lines 34), translates problem regular TIL problem problem compiled TILproblem (Line 5), detects producible resources sets always available (Lines6 7).major loop Lines 8 28. subgoal, SGPlan4 uses search-spacereduction eliminate irrelevant actions (Line 10) solves using basic planner(Line 11). basic planner fails find feasible plan within time limit (3000 node351fiChen, Wah, & HsuTable 2: Summary useful techniques domain variant. check mark indicatestechnique found useful domain variant class domainvariants.Domain VariantAIRPORT-*AIRPORT-TEMP-TIMEWINDOWS-COPIPESWORLD-*PROMELA-*PROMELA-*-DPPSR-SMALLPSR-MIDDLEPSR-MIDDLE-COPSR-LARGESATELLITE-STRIPSSATELLITE-TIMESATELLITE-NUMERICSATELLITE-COMPLEXSATELLITE-TIME-TIMEWINDOWSSATELLITE-TIME-TIMEWINDOWS-COSETTLERSUMTS-TEMPUMTS-TEMP-TIMEWINDOWSUMTS-TEMP-TIMEWINDOWS-COUMTS-FLAW-TEMPUMTS-FLAW-TEMP-TIMEWINDOWSUMTS-FLAW-TEMP-TIMEWINDOWS-COSGLMPFPOTILTIL-wDPPRSRKeys SG: subgoal partitioningLM: landmark analysisPF:path findingPO: path optimizationTIL: timed initial literals handling TIL-w: TIL wrapper detectionDP: derived predicates handling PR: producible resourcesSR:search-space reductionexpansions Metric-FF), SGPlan4 aborts run Metric-FF tries decomposeproblem further. first applies landmark analysis decompose solve subproblem(Lines 13-15). unsuccessful solving subproblem, tries path optimizationnumerical TIL problems (Line 17) path finding (Line 18) partitionsubproblem. subgoals evaluated, composes solution, evaluatesglobal constraints, updates penalty values (Line 24). Finally, new solutionfound unused producible resources, reduces initial producibleresources (Lines 26-28) repeats major loop again.6. Sensitivity Analysis Techniques SGPlan4section describe ablation study various techniques SGPlan4 ordertest effectiveness. Table 2 lists techniques useful IPC4domain variant. defer discussion performance improvement SGPlan4.1SGPlan4 Section 7.352fiTemporal Planning using Subgoal Partitioning ResolutionAirport variants, useful techniques include subgoal partitioning, landmark analysis, path finding. addition, TIL wrapper detection neededTIMEWINDOWS-CO variant. ablation study, applied SGPlan4 subgoalpartitioning alone. case, SGPlan4 solve 107 200 (53.5%) instancescannot solve numbered higher 28 (namely, P29, P30, etc.). reasonsubproblems without landmark analysis path finding largeMetric-FF difficulty solving them. contrast, SGPlan4 landmark analysispath finding solve 159 (79.5%) instances.Pipesworld variants, useful techniques include subgoal partitioning, landmark analysis, path finding, search-space reduction. Although search-space reductionslightly reduce run time 5.3% average, landmark analysis path findingsignificant effects performance. SGPlan4 without landmark analysis pathfinding solve 102 200 (51%) instances, whereas SGPlan4 landmarkanalysis path finding solve 186 instances (93%). Landmark analysis path finding also leads 8% average improvement run time instances versionssolve.Promela domain, subgoal partitioning found useful, besides applyingderived-predicate handling corresponding variants.PSR variants except PSR-SMALL, search-space reduction particularlyuseful addition subgoal partitioning. three variants, SGPlan4 searchspace reduction solve, respectively, 50, 14, 11 instances; whereas SGPlan4 withoutsearch-space reduction solve, respectively, 47, 8, 6 instances. addition, average run-time improvements due search-space reduction are, respectively, 34.1%, 46.9%,62.5%. PSR-SMALL variant, search-space reduction significant effectsrun time solution quality. Last, derived-predicate handling important PSRMIDDLE, encoded using derived predicates.Satellite domain, subgoal partitioning found useful solvingTIME, STRIPS, COMPLEX variants. NUMERIC, TIME-TIMEWINDOWS,TIME-TIMEWINDOWS-CO variants, landmark analysis path optimizationalso useful. three variants, SGPlan4 solve, respectively, 25, 25, and, 21instances, whereas SGPlan4 without landmark analysis path optimization solve,respectively, 16, 16, 13 instances.Settlers domain, subgoal partitioning well techniques handling producible resources important solving one instances. (The eighth instanceinfeasible.) Without detecting producible resources, SGPlan4 solve nine20 instances.UMTS domain, subgoal partitioning found useful, besides applyingTIL handling TIL wrapper detection corresponding variants. Landmark analysishelp domain detect none landmark facts300 instances. Also, search-space reduction prune facts little effectsperformance.also studied effects subgoal ordering SGPlan4.1 eighteen representative variants IPC4 domains well Depots domain (Figure 16).instance, test SGPlan4.1 using five random subgoal orders normalize run time(resp. quality) respect corresponding measure SGPlan4.1 run using353fiChen, Wah, & HsuAIRPORT: NONTEMP, TEMPPIPESWORLD: NOTANKAGE-NONTEMP, NOTANKAGE-TEMPPROMELA: OPT-TELEGRAPH, OPT-TELGRAPH-DP, PHIL, PHIL-DPPSR: SMALL, MIDDLESATELLITE: STRIPS, TIMESETTLERS: SETTLERSUMTS: TEMP, FLAW-TEMPDEPOTS: STRIPS, SIMPLETIME, TIMENormalized Quality1010.10.010.1110100Normalized Run TimeFigure 16: Run time-quality distribution SGPlan4.1 run using different random subgoalorders selected IPC4 Depots domain variants. results normalized respect run time quality SGPlan4.1 run using defaultsubgoal order. (Performance values larger one better SGPlan4.1 .)original order problem definition. use makespan quality measuretemporal domains number actions propositional domains (evenobjective specified problem definition).results show performance SGPlan4.1 quite insensitive subgoal ordering Airport, Promela, Settlers, UMTS domains. However, significantvariations run time quality Pipesworld PSR domains, althoughdefinitive trend random subgoal order better. Depots domain,exist smaller variations run time quality. common feature amongPipesworld, PSR, Depots domains intensive subgoal interactions,make sensitive order subgoals evaluated. example,354fiTemporal Planning using Subgoal Partitioning ResolutionPSR-MIDDLE variant, number subgoals large, different subgoalshighly related derived predicates. Last, note using original subgoal orderleads better run time quality Satellite domain. reason originalorder avoid unnecessary subgoal invalidations finding local feasible subplans, sincestarting states generated applying prefix subplans subgoals.clear advantage using random subgoal orders originalsubgoal order, SGPlan4 SGPlan4.1 use original subgoal order implementations.7. Experimental Resultssection, experimentally compare performance SGPlan4 , SGPlan4.1 (theirdifferences (17) (19)) planners solving IPC3 IPC4 benchmarksuites well Blocksworld domain IPC2. suite contains multiple domains,several variants each. variants IPC4 address different featuresPDDL2.2, include versions STRIPS, STRIPS DP (derived predicates), temporal, temporal TIL (deadlines), numeric, complex (temporal numeric).complete description variant problem files found Web sitecompetitions2runs carried AMD Athlon MP2800 PC Redhat Linux AS32-Gbyte main memory unless otherwise noted. Following rules IPC4, randomplanners set fixed random seed, all, throughout experiments. Moreover,planners must fully automated, run parameter settinginstances attempted, execute CPU time limit 30 minutes main memorylimit 1 Gbytes.Table 3 summarizes performance SGPlan4 , SGPlan4.1 , Downward (Helmert &Richter, 2004), LPG-TD-SPEED-1.0 seed 2004, YAHSP-1.1.3 use makespanquality metric temporal domains number actions propositional domains. Since code Downward unavailable, report IPC4 results adjustingrun times factor governed difference speeds computer usedIPC4 competition computer used SGPlan4.1 . Likewise, unableevaluate Downward IPC2 IPC3 benchmarks.Table 3 include results domain variants target planner cannothandle. example, LPG-TD-SPEED cannot solve compiled domainssupport grammatical features PSR-LARGE two FLUENTS variantsPROMELA domain; YAHSP cannot handle derived predicates. contrast,SGPlan4 SGPlan4.1 designed solve variants except ROVERS-TIMEvariant dynamic durations. Note since Satellite Settlers domainsexist IPC3 IPC4 benchmarks, table include results2. URL competitions http://ls5-www.cs.uni-dortmund.de/~edelkamp/ipc-4/IPC4, http://planning.cis.strath.ac.uk/competition/ IPC3, http://www.cs.toronto.edu/aips2000/ IPC2.3. object code LPG-TD downloaded http://zeus.ing.unibs.it/lpg/register-lpg-td.html, object code YAHSP-1.1 downloaded http://www.cril.univ-artois.fr/~vidal/Yahsp/yahsp.linux.x86.gz. object code Downward unavailable testingtime paper revised.355fiChen, Wah, & HsuIPC3 Settlers domain variants IPC3 Satellite domainreported IPC4.Table 3:Performance comparison SGPlan4.1 planners.table comparing SGPlan4.1 SGPlan4 , four missing variants (PIPESWORLDNOTANKAGE-TEMP-DEADLINES-CO, PROMELA-OPTICAL-TELEGRAPH-FLUENTS-DP,PROMELA-PHILOSOPHERS-FLUENTS-DP, ROVERS-TIME) cannot solvedplanners. table comparing SGPlan4.1 LPG-TD-SPEED, missing variants except ROVERS-TIME cannot solved LPG-TD-SPEED. ROVERS-TIME variant,LPG-TD-SPEED solve instances planners cannot. tablescomparing SGPlan4.1 , Downward, YAHSP, missing variants cannot solvedtarget planners compared.Domain VariantInstances Solvable (Fb )FiFqFtFw Fwt FwqFnInstancesFgFuFbComparison SGPlan4.1 SGPlan4AIRPORT-NONTEMP0.78 0.00AIRPORT-TEMP0.60 0.28AIRPORT-TEMP-TIMEWINDOWS0.48 0.14AIRPORT-TEMP-TIMEWINDOWS-CO0.28 0.00PIPESWORLD-NOTANKAGE-NONTEMP0.16 0.10PIPESWORLD-NOTANKAGE-TEMP0.72 0.28PIPESWORLD-TANKAGE-NONTEMP0.12 0.00PIPESWORLD-TANKAGE-TEMP0.52 0.14PIPESWORLD-NOTANKAGE-TEMP-DEAD0.00 0.00PROMELA-OPTICAL-TELEGRAPH0.19 0.00PROMELA-OPTICAL-TELEGRAPH-DP0.40 0.00PROMELA-OPTICAL-TELEGRAPH-FLUENTS 0.06 0.00PROMELA-PHILOSOPHERS0.58 0.00PROMELA-PHILOSOPHERS-DP0.94 0.00PROMELA-PHILOSOPHERS-FLUENTS0.02 0.00PSR-SMALL0.24 0.00PSR-MIDDLE0.98 0.02PSR-MIDDLE-CO0.26 0.00PSR-LARGE0.16 0.00SATELLITE-STRIPS0.53 0.06SATELLITE-TIME0.39 0.44SATELLITE-TIME-TIMEWINDOWS0.58 0.00SATELLITE-TIME-TIMEWINDOWS-CO0.53 0.03SATELLITE-NUMERIC0.44 0.00SATELLITE-COMPLEX0.36 0.22SATELLITE-COMPLEX-TIMEWINDOWS0.50 0.14SATELLITE-COMPLEX-TIMEWINDOWS-CO0.56 0.03SETTLERS0.10 0.00UMTS-TEMP0.96 0.04UMTS-TEMP-TIMEWINDOWS0.88 0.12UMTS-TEMP-TIMEWINDOWS-CO0.76 0.00UMTS-FLAW-TEMP0.02 0.88UMTS-FLAW-TEMP-TIMEWINDOWS0.00 0.44UMTS-FLAW-TEMP-TIMEWINDOWS-CO0.54 0.00DEPOTS-STRIPS0.27 0.27Continued . . .3560.000.000.160.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.050.000.000.060.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.080.000.000.000.000.000.000.000.000.000.000.100.000.000.160.740.000.540.000.000.100.000.130.020.060.190.700.000.020.060.250.000.080.110.110.080.030.080.850.000.000.240.100.100.000.410.000.000.020.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.030.000.000.000.000.000.000.000.000.000.000.000.000.000.020.000.000.000.000.270.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.460.460.000.000.000.020.040.000.000.000.000.000.000.000.000.000.000.790.000.000.000.000.000.000.000.000.030.060.000.000.000.000.000.000.000.000.000.000.120.120.120.500.000.000.340.340.730.710.600.810.400.000.000.060.000.720.780.170.170.330.330.420.170.330.330.050.000.000.000.000.000.000.000.880.880.860.441.001.000.660.660.000.290.400.190.601.000.210.941.000.280.220.830.830.670.670.550.770.670.670.951.001.001.001.000.540.541.00fiTemporal Planning using Subgoal Partitioning ResolutionTable 3: (continued)Domain VariantDEPOTS-SIMPLETIMEDEPOTS-TIMEDEPOTS-NUMERICDRIVERLOG-STRIPSDRIVERLOG-SIMPLETIMEDRIVERLOG-TIMEDRIVERLOG-NUMERICDRIVERLOG-HARDNUMERICFREECELL-STRIPSROVERS-STRIPSROVERS-SIMPLETIMEROVERS-NUMERICSATELLITE-SIMPLETIMESATELLITE-HARDNUMERICZENOTRAVEL-STRIPSZENOTRAVEL-SIMPLETIMEZENOTRAVEL-TIMEZENOTRAVEL-NUMERICBLOCKSWORLDInstances Solvable (Fb )FiFqFtFw Fwt Fwq0.230.270.180.700.600.450.600.550.050.700.550.450.750.500.850.800.450.650.570.680.590.270.100.200.350.150.200.100.000.450.050.000.000.000.200.550.000.290.050.050.050.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.060.000.050.000.000.000.000.000.000.050.000.000.000.000.000.000.000.000.000.030.000.000.410.000.000.000.050.050.700.300.000.100.250.200.150.000.000.350.06FnInstancesFgFuFb0.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.050.050.090.100.100.050.050.050.100.000.000.250.000.000.000.000.000.000.000.000.000.000.100.100.150.150.150.000.000.000.150.000.300.000.000.000.000.000.950.950.910.800.800.800.800.800.901.001.000.601.000.701.001.001.001.001.000.000.000.000.040.000.000.000.000.000.000.000.000.000.000.000.030.000.000.000.000.000.000.000.000.000.000.000.000.000.000.000.020.000.160.180.200.220.030.000.150.420.000.000.000.000.000.190.060.000.190.300.000.000.000.000.000.000.000.000.000.020.020.040.000.000.080.060.530.000.000.000.000.040.000.170.060.060.080.060.060.000.000.000.000.000.000.050.050.050.200.100.100.100.000.000.260.280.200.710.600.400.000.020.000.000.110.280.360.170.280.050.000.000.000.000.000.000.000.050.000.880.860.860.840.820.460.440.240.290.250.181.000.941.000.830.830.470.500.770.530.651.001.001.001.001.000.950.950.900.80Comparison SGPlan4.1 LPG-TD-SPEEDAIRPORT-NONTEMPAIRPORT-TEMPAIRPORT-TEMP-TIMEWINDOWSPIPESWORLD-NOTANKAGE-NONTEMPPIPESWORLD-NOTANKAGE-TEMPPIPESWORLD-TANKAGE-NONTEMPPIPESWORLD-TANKAGE-TEMPPIPESWORLD-NOTANKAGE-TEMP-DEADPROMELA-OPTICAL-TELEGRAPHPROMELA-OPTICAL-TELEGRAPH-DPPROMELA-PHILOSOPHERSPROMELA-PHILOSOPHERS-DPPSR-SMALLPSR-MIDDLESATELLITE-STRIPSSATELLITE-TIMESATELLITE-TIME-TIMEWINDOWSSATELLITE-NUMERICSATELLITE-COMPLEXSATELLITE-COMPLEX-TIMEWINDOWSSETTLERSUMTS-TEMPUMTS-TEMP-TIMEWINDOWSUMTS-FLAW-TEMPUMTS-FLAW-TEMP-TIMEWINDOWSDEPOTS-STRIPSDEPOTS-SIMPLETIMEDEPOTS-TIMEDEPOTS-NUMERICDRIVERLOG-STRIPS0.16 0.140.14 0.200.08 0.220.30 0.060.44 0.000.22 0.120.24 0.060.07 0.030.29 0.000.25 0.000.19 0.001.00 0.000.40 0.540.14 0.640.42 0.360.19 0.330.47 0.000.11 0.000.36 0.170.44 0.000.10 0.000.82 0.001.00 0.000.00 0.480.00 0.000.32 0.360.09 0.090.09 0.090.32 0.270.65 0.15Continued . . .3570.000.020.000.260.360.100.120.030.000.000.000.000.000.000.030.170.000.330.170.030.550.180.000.000.000.050.270.090.050.000.100.280.300.160.020.000.020.070.000.000.000.000.000.060.000.080.000.060.080.000.000.000.000.120.000.180.500.680.270.000.480.220.260.020.000.020.000.030.000.000.000.000.000.160.030.030.000.000.000.000.000.000.000.401.000.090.000.000.000.00fiChen, Wah, & HsuTable 3: (continued)Instances Solvable (Fb )FiFqFtFw Fwt FwqDomain VariantDRIVERLOG-SIMPLETIMEDRIVERLOG-TIMEDRIVERLOG-NUMERICDRIVERLOG-HARDNUMERICFREECELL-STRIPSROVERS-STRIPSROVERS-SIMPLETIMEROVERS-NUMERICSATELLITE-SIMPLETIMESATELLITE-HARDNUMERICZENOTRAVEL-STRIPSZENOTRAVEL-SIMPLETIMEZENOTRAVEL-TIMEZENOTRAVEL-NUMERICBLOCKSWORLD0.650.650.600.450.500.700.750.500.000.150.800.600.651.000.460.100.100.200.250.000.250.200.000.000.000.200.150.000.000.460.050.050.000.100.350.050.050.000.700.550.000.150.300.000.030.000.000.000.000.000.000.000.100.250.000.000.050.050.000.030.000.000.000.000.000.000.000.000.000.000.000.000.000.000.00FnInstancesFgFuFb0.000.000.000.000.000.000.000.000.050.000.000.050.000.000.030.000.000.000.000.050.000.000.000.000.000.000.000.000.000.000.200.200.150.200.100.000.000.400.000.000.000.000.000.000.000.000.000.050.000.000.000.000.000.000.300.000.000.000.000.000.800.800.800.800.851.001.000.601.000.701.001.001.001.001.000.000.000.000.000.000.000.000.000.000.000.000.000.000.320.290.040.600.000.000.000.000.000.120.020.060.000.380.000.000.060.000.400.170.000.000.280.710.230.400.000.000.000.380.000.880.980.340.000.350.001.000.941.000.220.830.000.060.000.000.000.000.030.000.000.000.000.050.060.180.000.040.020.000.000.000.140.000.000.400.000.000.020.000.240.000.000.020.170.000.200.050.000.000.000.100.000.100.710.400.040.000.000.000.050.000.000.000.701.000.620.270.600.940.830.860.800.900.601.001.00Comparison SGPlan4.1 DownwardAIRPORT-NONTEMPPIPESWORLD-NOTANKAGE-NONTEMPPIPESWORLD-TANKAGE-NONTEMPPROMELA-OPTICAL-TELEGRAPHPROMELA-OPTICAL-TELEGRAPH-DPPROMELA-PHILOSOPHERSPROMELA-PHILOSOPHERS-DPPSR-SMALLPSR-MIDDLEPSR-LARGESATELLITE-STRIPS0.520.140.160.000.350.001.000.420.320.120.690.000.020.000.000.000.000.000.040.380.040.080.020.200.160.000.000.000.000.000.020.000.030.160.020.000.000.000.000.000.000.060.040.030.180.000.020.000.000.000.000.480.220.020.00Comparison SGPlan4.1 YAHSPAIRPORT-NONTEMPPIPESWORLD-NOTANKAGE-NONTEMPPIPESWORLD-TANKAGE-NONTEMPPROMELA-OPTICAL-TELEGRAPHPROMELA-PHILOSOPHERSPSR-SMALLSATELLITE-STRIPSDEPOTS-STRIPSDRIVERLOG-STRIPSFREECELL-STRIPSROVERS-STRIPSZENOTRAVEL-STRIPSBLOCKSWORLD0.240.140.220.270.130.360.250.500.500.100.350.300.463580.220.520.320.000.000.020.530.320.300.800.250.650.310.020.000.020.000.000.000.000.050.000.000.000.000.090.100.280.020.000.000.020.000.000.000.000.000.000.060.120.000.040.000.480.540.030.000.000.000.000.000.03fiTemporal Planning using Subgoal Partitioning ResolutionKeys: (tn , qn )(tg , qg )FbFiFqFtFwFwtFwqFnFgFu(run time, quality) SGPlan4.1(run time, quality) target planner comparedFraction solved SGPlan4.1 target planner(Fb = Fi + Fq + Ft + Fw + Fwt + Fwq = 1 Fn Fg Fu )Fraction tn tg qn qg (SGPlan4.1 better run time quality)Fraction tn > tg qn < qg (SGPlan4.1 worse run time better quality)Fraction tn < tg qn > qg (SGPlan4.1 worse quality better run time)Fraction tn > tg qn > qg (SGPlan4.1 worse run time worse quality)Fraction tn > tg qn = qg (SGPlan4.1 worse run time quality)Fraction tn = tg qn > qg (SGPlan4.1 worse quality run time)Fraction solved SGPlan4.1 target plannerFraction solved target planner SGPlan4.1Fraction unsolved SGPlan4.1 target plannerFigures 17-20 plot time-quality trade-offs run time (resp. quality)target planner normalized respect corresponding measure SGPlan4.1instances solvable planners. graph, also list six percentages computednormalizing Fi , Ft , Fq , Fw , Fwt , Fwq respect Fb (defined Table 3)domains evaluated.Airport domain, SGPlan4.1 improves performance SGPlan4terms run time quality majority (69.9%) instances (Figure 17a).NONTEMP variant, solution files (not shown) show SGPlan4.1 cannot solvesix (Fg + Fu = 0.12 Table 3) seven largest instances (number 44 50); whereasDownward, leading planner variant, solve 50 instances. SGPlan4.1difficulty instances partitioned subproblems large evaluated embedded Metric-FF planner. also reason SGPlan4.1worse Downward LPG terms run time larger instances. obvioussolution employ efficient basic planner becomes available. fact,one strengths partition-and-resolve approach. Another solutionpartition subproblems reduce complexity extenthandled modified Metric-FF planner. design partitioning methodsstill open time.Pipesworld domain, SGPlan4.1 significant improvements SGPlan4terms makespan NOTANKAGE-TEMP TANKAGE-TEMP variants (Figure 17b). improvements due minimization estimated makespan(Te) (19). However, improvements found NOTANKAGE-NONTEMPTANKAGE-NONTEMP variants (19) term correspondsnumber actions non-temporal variants. respect planners, SGPlan4.1solve instances NOTANKAGE-NONTEMP, NOTANKAGE-TEMP,TANKAGE-TEMP variants (Fn Fg 0 corresponding rows Table 3),consistently shortest solution time NOTANKAGE-TEMP TANKAGETEMP variants. NOTANKAGE-NONTEMP TANKAGE-NONTEMP variants, YAHSP, however, solve number instances shortest solution time cases, although tends produce longer plans. Last, discussedSection 4.2, SGPlan4.1 competitive PIPESWORLD-NOTANKAGE-TEMPDEADLINE variant solve eight 30 instances.359fiChen, Wah, & Hsu469.9%8.5%2.0%5.2%0.7%0.250.251138.6%0.0%0.250.014Normalized qualityNormalized quality81.4%0.0%18.6%0.0%0.0010.0%0.010.0%0.111SMALLMIDDLELARGEMIDDLE-CO0.8%67.2%0.0%0.0%32.0%0.0%0.250.25101Normalized run timec) PROMELAd) PSRNormalized qualityNormalized quality416.2%68.6%13.2%1.5%STRIPSTIME-TIMEWINDOW-COCOMPLEXCOMPLEX-TIMEWINDOWSTIME-TIMEWINDOWSNUMERICTIMECOMPLEX-TIMEWINDOWS-CO0.10.010.0%0.5%0.111SETTLERS10.5%0.0%0.0%0.0%0.250.110110Normalized run timee) SATELLITEf) SETTLERS1029.1%Normalized quality4Normalized quality0.0%89.5%Normalized run time14Normalized run time10110b) PIPESWORLD4OPTICAL-TELEGRAPH-DPOPTICAL-TELEGRAPHPHILOSOPHERSOPTICAL-TELEGRAPH-FLUENTSPHILOSOPHERS-FLUENTSPHILOSOPHERS-DP0.251e-040.0%1Normalized run timea) AIRPORT10.0%0.1Normalized run time445.8%15.7%Normalized qualityNormalized quality13.7%14NOTANKAGE-NONTEMPNOTANKAGE-TEMPTANKAGE-TEMPTANKAGE-NONTEMPTEMPNONTEMPTEMP-TIMEWINDOWS-COTEMP-TIMEWINDOWS62.2%8.7%0.0%0.0%TEMPFLAW-TEMP-TIMEWINDOWS-COTEMP-TIMEWINDOWS-COTEMP-TIMEWINDOWSFLAW-TEMP-TIMEWINDOWSFLAW-TEMP0.0%0.250.11134.5%1.7%5.0%16.8%DEPOTS-STRIPSDEPOTS-SIMPLETIMEDEPOTS-TIMEDEPOTS-NUMERICBLOCKSWORLD0.10.00110Normalized run time42.0%0.010.10.0%110100100010000Normalized run timeg) UMTSh) DEPOTS & BLOCKSWORLDFigure 17: Run time-quality SGPlan4 instance normalized respectcorresponding run time-quality SGPlan4.1 instance instances solvable planners. (Performance values larger onebetter SGPlan4.1 .)360fiTemporal Planning using Subgoal Partitioning ResolutionTEMPNONTEMPTEMP-TIMEWINDOWS21.5%11014.6%Normalized qualityNormalized quality436.9%26.2%0.8%NOTANKAGE-NONTEMPNOTANKAGE-TEMPTANKAGE-TEMPTANKAGE-NONTEMPNOTANKAGE-TEMP-DEADLINES46.9%9.4%11.6%7.8%0.250.010.0%0.11101000.011.6%0.11Normalized run time100.0%0.0%0.0%0.0%0.0%11000SMALLMIDDLE10Normalized qualityNormalized qualityOPTICAL-TELEGRAPH-DPOPTICAL-TELEGRAPHPHILOSOPHERSPHILOSOPHERS-DP0.250.1100b) PIPESWORLD0.0%110Normalized run timea) AIRPORT432.8%101001000160.8%27.8%3.1%0.0%8.2%1e-040.0010.01Normalized run time0.0%0.1110100Normalized run timec) PROMELAd) PSR44SETTLERS51.4%1Normalized qualityNormalized quality22.1%1.4%18.6%STRIPSCOMPLEXCOMPLEX-TIMEWINDOWSTIME-TIMEWINDOWSNUMERICTIME5.7%0.250.10.7%1101001000115.4%0.0%84.6%0.0%0.250.01100000.0%45.5%Normalized qualityNormalized quality1001000f) SETTLERS28.6%12.0%135.0%4.5%3.0%0.0%0.11010TEMPTEMP-TIMEWINDOWSFLAW-TEMP-TIMEWINDOWSFLAW-TEMP0.250.011Normalized run timee) SATELLITE40.0%0.1Normalized run time110100Normalized run time128.6%1.7%31.1%9.2%DEPOTS-STRIPSDEPOTS-SIMPLETIMEDEPOTS-TIMEDEPOTS-NUMERICBLOCKSWORLD0.10.0010.010.10.8%110100Normalized run timeg) UMTSh) DEPOTS & BLOCKSWORLDFigure 18: Run time-quality LPG-TD-SPEED instance normalized respectcorresponding run time-quality SGPlan4.1 instanceinstances solvable planners. (Performance values larger onebetter SGPlan4.1 ).361fiChen, Wah, & Hsu0.0%14NONTEMP59.1%Normalized qualityNormalized quality420.5%18.2%NOTANKAGE-NONTEMPTANKAGE-NONTEMP1.8%11.8%2.3%0.0%0.250.126.3%1.8%68.4%0.0%0.251101001Normalized run timea) AIRPORTOPTICAL-TELEGRAPH-DPPHILOSOPHERS-DP0.0%1100.0%0.0%0.250.10.0%0.0%0.0%1101001SMALLMIDDLELARGE21.3%39.8%4.6%0.9%33.3%0.251e-040.001Normalized run timeNormalized quality10.010.10.0%110100Normalized run timec) PROMELA4100b) PIPESWORLD4Normalized qualityNormalized quality410Normalized run timed) PSRSTRIPS10.0%83.3%3.3%3.3%0.0%0.250.10.0%110100Normalized run timee) SATELLITEFigure 19: Run time-quality Downward instance normalized respectcorresponding run time-quality SGPlan4.1 instanceinstances solvable planners. (Performance values larger onebetter SGPlan4.1 .)Promela domain, SGPlan4.1 improvements SGPlan4 terms qualityimproves terms run time instances solve four six variants(worse OPTICAL-TELEGRAPH-FLUENTS PHILOSOPHERS-FLUENTS vari362fiTemporal Planning using Subgoal Partitioning Resolution4NONTEMPNOTANKAGE-NONTEMPTANKAGE-NONTEMP31.4%134.3%Normalized qualityNormalized quality1017.1%14.3%2.9%0.0%0.250.010.1110100151.9%22.2%18.5%1.2%2.5%0.0010.010.1Normalized run timeOPTICAL-TELEGRAPHPHILOSOPHERS45.2%Normalized qualityNormalized quality154.8%0.0%0.0%0.0%0.250.1110138.3%2.1%0.0%57.4%0.250.0011002.1%0.0110d) PSR1063.3%30.0%Normalized qualityNormalized quality1Normalized run timeSTRIPS3.3%0.250.010.0%0.1c) PROMELA11000SMALLNormalized run time4100b) PIPESWORLD40.0%10Normalized run timea) AIRPORT43.7%10.0%0.10.0%3.3%110100Normalized run time133.3%50.0%3.7%7.4%1.9%DEPOTS-STRIPSBLOCKSWORLD0.10.0010.010.13.7%1101001000Normalized run timee) SATELLITEf) DEPOTS & BLOCKSWORLDFigure 20: Run time-quality YAHSP instance normalized respect corresponding run time-quality SGPlan4.1 instance instancessolvable planners. (Performance values larger one betterSGPlan4.1 .)ants). SGPlan4.1 solve number instances OPTICAL-TELEGRAPHFLUENTS, PHILOSOPHERS, PHILOSOPHERS-DP, PHILOSOPHERS-FLUENTS363fiChen, Wah, & Hsuvariants compared LPG-TD-SPEED, Downward, YAHSP. Further,fastest planner three variants slightly slower YAHSP PHILOSOPHERS variant (Figures 18c, 19c, 20c). OPTICAL-TELEGRAPH OPTICALTELEGRAPH-DP variants, organizer IPC4 provided two versions, one writtenpure STRIPS another ADL. However, 14 (resp., 19) instancesSTRIPS 48 (resp., 48) instances ADL OPTICAL-TELEGRAPH (resp.,OPTICAL-TELEGRAPH-DP) variant. instances available ADL ADL space-efficient problem representation, whereas instances STRIPSrequire large files. (For example, file size OPTICAL-TELEGRAPH-14 38 KbytesADL 8.3 Mbytes STRIPS.) Since SGPlan4.1 SGPlan4 cannot handle ADLtime, solved instances pure STRIPS two variants.able solve instances available STRIPS fastest instances.However, Downward handle instances ADL able solve instancestwo variants. plan extend SGPlan4.1 directly support ADL future.Note SGPlan4.1 SGPlan4 always find plans better qualityinstances solved OPTICAL-TELEGRAPH, OPTICAL-TELEGRAPH-DP,PHILOSOPHERS, PHILOSOPHERS-DP variants compared threeplanners (Edelkamp & Hoffmann, 2004).SGPlan4.1 planner solve instances four variantsPSR domain. Since PSR pure propositional domain, SGPlan4.1 unable improvesolution quality SGPlan4 . Nevertheless, quality SGPlan4.1 consistently betterthree planners (Fi +Fq +Fwt > Ft +Fw +Fwq corresponding rowsTable 3). SMALL variant, SGPlan4.1 LPG comparable run timescannot solve largest instances. Like AIRPORT domain, SGPlan4.1 difficultylargest instances basic planner cannot handle partitionedsubproblems. MIDDLE variant, SGPlan4.1 , LPG, Downward solve 50instances. situation MIDDLE-CO LARGE variants similarOPTICAL-TELEGRAPH OPTICAL-TELEGRAPH-DP variants Promeladomain. variants, Downward handle directly ADL format, SGPlan4.1must expand ADL syntax pure STRIPS exhausted memory evaluatinglarger instances. plan address issue future.Satellite domain, SGPlan4.1 significant improvements quality SGPlan4 .fact, SGPlan4.1 generates solutions better quality plannersinstances solve number instances seven variants. eighthvariant (TIME), able solve largest instances memoryusage exceeded 1 Gbytes. variants except STRIPS, SGPlan4.1 fasterthree planners. STRIPS variant, YAHSP fastest generatemultiple actions instead single action search step. However, finds slightlylonger plans compared SGPlan4.1 .Settlers domain, SGPlan4.1 improve solution quality SGPlan4because, discussed earlier, (19) term corresponds numberactions non-temporal variants. SGPlan4.1 solve instances except eighthinstance, learned IPC4 organizers infeasible instance.also fastest among planners, generates longer plans LPGTD-SPEED. due iterative scheme reducing producible resources.364fiTemporal Planning using Subgoal Partitioning ResolutionTable 4: Summary number instances solved five planners compared (? meansclear whether domain solved object codeavailable testing, means planner supportlanguage features benchmark.)DomainSGPlan4.1 SGPlan4 LPG-TD-SPEED Downward YAHSPAirport1541561345036Pipesworld1741661586093Promela129167838342PSR1221229913148IPC4Satellite2042071573636Settlers191913UMTS300254200Total11021091844360219Depots848887?19DriverLog808799?20FreeCell182019?19Rovers525780?12IPC3Satellite343434ZenoTravel808080?20Total348366399?90IPC2 Blocksworld353535?35Overall148514921243360344optimal amount resources cannot found ahead time, SGPlan4.1 may incurredundant actions producing unused resources.UMTS domain, SGPlan4.1 solve instances six variantsfastest four them. Moreover, makespans greatly improvedSGPlan4 incorporating Te modified heuristic function Metric-FF, althoughimprovements makespan LPG-TD-SPEED small variants. SGPlan4.1 ,however, slower LPG-TD-SPEED FLAW FLAW-TIL variants. performance degradation variants attributed flawed actions leadoverly optimistic heuristic values relaxed-plan-based planners (Edelkamp & Hoffmann,2004) like Metric-FF.IPC3 Depots domain, SGPlan4.1 better quality LPG-TD-SPEEDYAHSP STRIPS NUMERIC variants, whereas makespan SGPlan4.1worse LPG-TD-SPEED majority instances TIMESIMPLETIME variants. LPG-TD-SPEED also faster SGPlan4.1 majorityinstances (Fq + Fw + Fwt > Fi + Ft + Fwq corresponding rows Table 3).Due large fraction initial active global constraints, performance subgoalpartitioning SGPlan4.1 unsatisfactory domain.remaining IPC3 domains, SGPlan4.1 generally improves SGPlan4 qualitybesides Freecell domain STRIPS. Except Satellite domainLPG-TD-SPEED performs better, SGPlan4.1 generates solutions better quality365fiChen, Wah, & Hsuinstances. Further, SGPlan4.1 faster LPG-TD-SPEEDhalf instances, although difference run times among plannersrelatively easy instances usually insignificant.Blocksworld domain, SGPlan4.1 generally finds solutions smaller numberactions SGPlan4 , LPG-TD-SPEED, YAHSP. However, SGPlan4.1 muchslower LPG-TD-SPEED many instances needs time resolvinglarge fraction initial active global constraints (Figure 18h).8. Conclusions Future Workpresented paper partition-and-resolve approach applicationSGPlan4 , planner first prize Suboptimal Temporal Metric Tracksecond prize Suboptimal Propositional Track IPC4. Table 4 summarizesnumber instances solved top planners IPC4 well SGPlan4.1 . resultsshow constraint partitioning employed planners effective solving majorityproblems two competitions.approach based observation fraction active mutex constraintsacross subgoals majority instances IPC3 IPC4 small. observation allows us partition search largely independent subproblems limitamount backtracking resolving violated global constraints across subproblems. improvements also attributed combination techniques introducedreducing search space handling new features PDDL2.2.future, plan study partitioning techniques better exploitconstraint structure planning domains. particular, study fine-grain partitioningorder address cases larger fraction global constraints, develop searchstrategies solving problems difficult-to-satisfy global constraints deadlines.also plan extend method planning uncertainty supportexpressive modeling language features.Acknowledgmentsresearch paper supported National Science Foundation Grant IIS 03-12084.ReferencesBlum, A. L., & Furst, M. L. (1997). Fast planning planning graph analysis. ArtificialIntelligence, 90, 281300.Bonet, B., & Geffner, H. (2001). Planning heuristic search. Artificial Intelligence, Specialissue Heuristic Search, 129 (1).Chen, Y., & Wah, B. W. (2003). Automated planning scheduling using calculus variations discrete space. Proc. Intl Conf. Automated Planning Scheduling,pp. 211.Chien, S., Rabideau, G., Knight, R., Sherwood, R., Engelhardt, B., Mutz, D., Estlin, T.,Smith, B., Fisher, F., Barrett, T., Stebbins, G., & Tran, D. (2000). ASPEN - Au366fiTemporal Planning using Subgoal Partitioning Resolutiontomating space mission operations using automated planning scheduling. Proc.SpaceOps. Space Operations Organization.Doherty, P., & Kvarnstrm, J. (1999). Talplanner: empirical investigation temporallogic-based forward chaining planner.. Proc. Sixth Intl Workshop TemopralLogic-based Forword Chaining Planner, pp. 4754. AIPS.Edelkamp, S. (2002). Mixed propositional numerical planning model checkingintegrated planning system. Proc. Workshop Planning Temporal Domains.AIPS.Edelkamp, S. (2003). Pddl2.2 planning model checking integrated environment.UK Planning Scheduling Special Interest Group (PlanSig). Glasgow.Edelkamp, S., & Hoffmann, J. (2004). Classical part, 4th international planning competition.http://ls5-www.cs.uni-dortmund.de/~edelkamp/ipc-4/.Foulser, D. E., Li, M., & Yang, Q. (1992). Theory algorithms plan merging.. ArtificialIntelligence, 57 (2-3), 143181.Fourman, M. P. (2000). Propositional planning. Proc. Workshop Model TheoreticApproaches Planning. AIPS.Garrido, A., Fox, M., & Long, D. (2002). temporal planning system durative actionspddl2.1. Proc. European Conf. Artificial Intelligence, pp. 586590.Gerevini, A., & Serina, I. (2002). LPG: planner based local search planning graphsaction costs. Proc. Sixth Int. Conf. AI Planning Scheduling, pp.1222. Morgan Kaufman.Hanks, S., & Weld, D. S. (1995). domain-independent algorithm plan adaptation.. J.Artificial Intelligence Research, 2, 319360.Helmert, M., & Richter, S. (2004). Fast downward - making use causal dependenciesproblem representation. Proc. IPC4, ICAPS, pp. 4143.Hoffmann, J. (2003). metric-ff planning system: Translating ignoring delete listsnumeric state variables. Journal Artificial Intelligence Research, 20, 291341.Hoffmann, J., & Nebel, B. (2001). FF planning system: Fast plan generationheuristic search. J. Artificial Intelligence Research, 14, 253302.Jonsson, A. K., Morris, P. H., Muscettola, N., & Rajan, K. (2000). Planning interplanetary space: Theory practice. Proc. 2nd Intl NASA Workshop PlanningScheduling Space. NASA.Kambhampati, S., & Hendler, J. A. (1992). validation-structure-based theory planmodification reuse.. Artificial Intelligence, 55 (2), 193258.Kautz, H., & Selman, B. (1996). Pushing envelope: planning, propositional logic,stochastic search. Proc. 13th National Conference Artificial Intelligence, pp.11941201. AAAI.Kautz, H., & Selman, B. (1999). Unifying SAT-based graph-based planning. Proc.Intl Joint Conf. Artificial Intelligence. IJCAI.367fiChen, Wah, & HsuKautz, H., & Walser, J. P. (2000). Integer optimization models AI planning problems.Knowledge Engineering Review, 15 (1), 101117.Koehler, J., & Hoffmann, J. (2000). reasonable forced goal ordering useagenda-driven planning algorithm. J. AI Research, 12, 339386.Lin, F. (2001). planner called R. AI Magazine, 7376.Long, D., & Fox, M. (1998). Efficient implementation plan graph STAN. J. AIResearch.Nau, D., Muoz-Avila, H., Cao, Y., Lotem, A., & Mitchell, S. (2001). Total-order planningpartially ordered subtasks. Proc. Intl Joint Conf. Artificial Intelligence,pp. 425430. IJCAI.Nebel, B., Dimopoulos, Y., & Koehler, J. (1997). Ignoring irrelevant facts operatorsplan generation. Proc. European Conf. Planning, pp. 338350.Nebel, B., & Koehler, J. (1995). Plan reuse versus plan generation: theoreticalempirical analysis.. Artificial Intelligence, 76 (1-2), 427454.Nigenda, R. S., Nguyen, X., & Kambhampati, S. (2000). AltAlt: Combining advantagesGraphplan heuristic state search. Tech. rep., Arizona State University.Penberethy, J., & Weld, D. (1992). UCPOP: sound, complete, partial order plannerADL. Proc. 3rd Intl Conf. Principles Knowledge RepresentationReasoning, pp. 103114. KR Inc.Penberethy, J., & Weld, D. (1994). Temporal planning continuous change. Proc.12th National Conf. AI, pp. 10101015. AAAI.Porteous, J., Sebastia, L., & Hoffmann, J. (2001). extraction, ordering, usagelandmarks planning. Proc. European Conf. Planning, pp. 3748.Refanidis, I., & Vlahavas, I. (2001). GRT planner. AI Magazine, 6366.Refanidis, I., & Vlahavas, I. (2002). MO-GRT system: Heuristic planning multiplecriteria. Proc. Workshop Planning Scheduling Multiple Criteria. AIPS.Subbarao, M. B. D., & Kambhampati, S. (2002). Sapa: domain-independent heuristicmetric temporal planner. Tech. rep., Arizona State University.Tate, A., Drabble, B., & Kirby, R. (1994). O-Plan2: open architecture command,planning control. Intelligent Scheduling, 213239.Tsamardinos, I., Pollack, M. E., & Horty, J. F. (2000). Merging plans quantitativetemporal constraints, temporally extended actions, conditional branches.. Proc.Intl Conf. AI Planning Scheduling (AIPS), pp. 264272.Wah, B., & Chen, Y. (2006). Constraint partitioning penalty formulations solvingtemporal planning problems. Artificial Intelligence, 170 (3), 187231.Wah, B. W., & Chen, Y. (2003). Partitioning temporal planning problems mixed spaceusing theory extended saddle points. Proc. IEEE Intl Conf. ToolsArtificial Intelligence, pp. 266273.368fiTemporal Planning using Subgoal Partitioning ResolutionWah, B. W., & Chen, Y. (2004). Subgoal partitioning global search solving temporalplanning problems mixed space. Intl J. Artificial Intelligence Tools, 13 (4), 767790.Wilkins, D. (1990). AI planners solve practical problems?. Computational Intelligence,232246.Wolfman, S., & Weld, D. (2000). Combining linear programming satisfiability solvingresource planning. Knowledge Engineering Review, 15 (1).Yang, Q. (1997). Intelligent planning: decomposition abstraction based approach.Springer-Verlag, London, UK.369fiJournal Artificial Intelligence Research 26 (2006) 101-126Submitted 8/05; published 5/06Domain Adaptation Statistical ClassifiersHal Daume IIIDaniel Marcuhdaume@isi.edumarcu@isi.eduInformation Sciences InstituteUniversity Southern California4676 Admiralty Way, Suite 1001Marina del Rey, CA 90292 USAAbstractbasic assumption used statistical learning theory training datatest data drawn underlying distribution. Unfortunately, manyapplications, in-domain test data drawn distribution related,identical, out-of-domain distribution training data. considercommon case labeled out-of-domain data plentiful, labeled in-domain datascarce. introduce statistical formulation problem terms simple mixturemodel present instantiation framework maximum entropy classifierslinear chain counterparts. present efficient inference algorithms specialcase based technique conditional expectation maximization. experimentalresults show approach leads improved performance three real world tasksfour different data sets natural language processing domain.1. Introductiongeneralization properties current statistical learning techniques predicatedassumption training data test data come underlyingprobability distribution. Unfortunately, many applications, assumption inaccurate.often case plentiful labeled data exists one domain (or coming onedistribution), one desires statistical model performs well another related,identical domain. Hand labeling data new domain costly enterprise, oneoften wishes able leverage original, out-of-domain data building modelnew, in-domain data. seek eliminate annotation in-domaindata, instead seek minimize amount new annotation effort required achievegood performance. problem known domain adaptation transfer.paper, present novel framework understanding domain adaptationproblem. key idea framework treat in-domain data drawnmixture two distributions: truly in-domain distribution general domaindistribution. Similarly, out-of-domain data treated drawn mixturetruly out-of-domain distribution general domain distribution. applyframework context conditional classification models conditional linear-chainsequence labeling models, inference may efficiently solved using techniqueconditional expectation maximization. apply model four data sets varying degrees divergence in-domain out-of-domain data obtainc2006AI Access Foundation. rights reserved.fiDaume III & Marcupredictive accuracies higher large number baseline systems secondmodel proposed literature problem.domain adaptation problem arises frequently natural language processing domain, millions dollars spent annotating text resourcesmorphological, syntactic semantic information. However, resourcesbased text news domain (in cases, Wall Street Journal). sortlanguage appears text Wall Street Journal highly specialized is,circumstances, poor match domains. instance,recent surge interest performing summarization (Elhadad, Kan, Klavans, & McKeown, 2005) information extraction (Hobbs, 2002) biomedical texts, summarizationelectronic mail (Rambow, Shrestha, Chen, & Lauridsen, 2004), information extractiontranscriptions meetings, conversations voice-mail (Huang, Zweig, & Padmanabhan,2001), among others. Conversely, machine translation domain, parallelresources machine translation system depend parameter estimation drawntranscripts political meetings, yet translation systems often targeted newsdata (Munteanu & Marcu, 2005).2. Statistical Domain Adaptationmulticlass classification problem, one typically assumes existence training set= {(xn , yn ) X : 1 n N }, X input space finite set.assumed (xn , yn ) drawn fixed, unknown base distribution ptraining set independent identically distributed, given p. learning problemfind function f : X obtains high predictive accuracy (this typicallydone either explicitly minimizing regularized empirical error, maximizingprobabilities model parameters).2.1 Domain Adaptationcontext domain adaptation, situation becomes complicated. assumegiven two sets training data, (o) D(i) , out-of-domain indomain data sets, respectively. longer assume single fixed,known distribution drawn, rather assume (o) drawndistribution p(o) D(i) drawn distribution p(i) . learning problemfind function f obtains high predictive accuracy data drawn p (i) . (Indeed,model turn symmetric respect (i) D(o) , contextsconsider obtaining good predictive model (i) makes intuitive sense.)assume |D (o) | = N (o) |D(i) | = N (i) , typically N (i) N (o) .before, assume N (o) out-of-domain data points drawn iid p(o)N (i) in-domain data points drawn iid p(i) .Obtaining good adaptation model requires careful modeling relationshipp(i) p(o) . two distributions independent (in obvious intuitivesense), out-of-domain data (o) useless building model p(i) maywell ignore it. hand, p(i) p(o) identical, adaptationnecessary simply use standard learning algorithm. practical problems,though, p(i) p(o) neither identical independent.102fiDomain Adaptation Statistical Classifiers2.2 Prior Workrelatively little prior work problem, nearly focusedspecific problem domains, n-gram language models generative syntactic parsingmodels. standard approach used treat out-of-domain data prior knowledgeestimate maximum posterior values model parameters priordistribution. approach applied successfully language modeling (Bacchiani& Roark, 2003) parsing (Roark & Bacchiani, 2003). Also parsing domain, Hwa(1999) Gildea (2001) shown simple techniques based using carefully chosensubsets data parameter pruning improve performance adaptedparser. models assume data distribution p (D | ) parameters priordistribution parameters p ( | ) hyper-parameters . estimatehyperparameters out-of-domain data find maximum posterioriparameters in-domain data, prior fixed.context conditional discriminative models, domain adaptationwork aware model Chelba Acero (2004). modeluses out-of-domain data estimate prior distribution, contextmaximum entropy model. Specifically, maximum entropy model trainedout-of-domain data, yielding optimal weights problem. weights usedmean weights Gaussian prior learned weights in-domain data.Though effective experimentally, practice estimating prior distributionout-of-domain data fixing estimation in-domain data leaves muchdesired. Theoretically, strange estimate fix prior distribution data;made apparent considering form models. Denoting in-domain dataparameters (i) , respectively, out-of-domain data parametersD(o) , obtain following form prior estimation models:= arg max p | arg max p () p(o)(i)|p |(1)One would difficult time rationalizing optimization problem anythingexperimental performance. Moreover, models unusualtreat in-domain data out-of-domain data identically. Intuitively,difference two sets data; simply come different, related distributions.Yet, prior-based models highly asymmetric respect two data sets.also makes generalization one domain data set difficult. Finally,see, model propose paper, alleviates problems,outperforms experimentally.second generic approach domain adaptation problem builddomain model use predictions features domain data.successfully used context named entity tagging (?). approach attractivemakes assumptions underlying classifier; fact, multiple classifiersused.103fiDaume III & Marcu2.3 Frameworkpaper, propose following relationship in-domain out-ofdomain distributions. assume instead two underlying distributions,actually three underlying distributions, denote q (o) , q (g) q (i) .consider p(o) mixture q (o) q (g) , consider p(i) mixture q (i)q (g) . One intuitively view q (o) distribution distribution data trulyout-of-domain, q (i) distribution data truly in-domain q (g) distributiondata general domains. Thus, knowing q (g) q (i) sufficient buildmodel in-domain data. out-of-domain data help us providinginformation q (g) available considering in-domain data.example, part-of-speech tagging, assignment tag determiner (DT)word likely general decision, independent domain. However,Wall Street Journal, monitor almost always verb (VB), technical documentationlikely noun. q (g) distribution account case the/DT,q (o) account monitor/VB q (i) account monitor/NN.3. Domain Adaptation Maximum Entropy Modelsdomain adaptation framework outlined Section 2.3 completely generalapplied statistical learning model. section apply loglinear conditional maximum entropy models linear chain counterparts, sincemodels proved quite effective many learning tasks. first review maximumentropy framework, extend domain adaptation problem; finallydiscuss domain adaptation linear chain maximum entropy models.3.1 Maximum Entropy Modelsmaximum entropy framework seeks conditional distribution p (y | x) closest(in sense KL divergence) uniform distribution also matches set training data respect feature function expectations (Della Pietra, Della Pietra, &Lafferty, 1997). introducing one Lagrange multiplier feature function fi ,optimization problem results probability distribution form:p (y | x ; ) =1Z,xhexp > f (x, y)(2)PHere, u> v denotes scalar product two vectors u v, given by: u> v = ui vi .normalization constant Eq (2), Z,x , obtained summing exponentialpossible classes 0 Y. probability distribution also known exponentialdistribution Gibbs distribution. learning (or optimization) problem findvector maximizes likelihood Eq (2). practice, prevent over-fitting, onetypically optimizes penalized (log) likelihood, isotropic Gaussian prior mean0 covariance matrix 2 placed parameters (Chen & Rosenfeld, 1999).graphical model standard maximum entropy model depicted leftFigure 1. figure, circular nodes correspond random variables square nodes104fiDomain Adaptation Statistical Classifierscorrespond fixed variables. Shaded nodes observed training data emptynodes hidden unobserved. Arrows denote conditional dependencies.general, feature functions f (x, y) may arbitrary real-valued functions; however,paper restrict attention binary features. practice, harshrestriction: many problems natural language domain naturally employ binaryfeatures (for real valued features, binning techniques applied). Additionally,notational convenience, assume features fi (x, y) written productform gi (y)hi (x) arbitrary binary functions g outputs binary features hinputs. latter assumption means consider x binary vectorxi = hi (x); following simplify notation significantly (the extension fullcase straightforward, messy, therefore considered remainderpaper). considering x vector, may move class dependence parametersconsider matrix y,i weight hi class y. writerefer column vector corresponding class y. x also consideredcolumn vector, write > x shorthand dot product x weightsclass y. modified notation, may rewrite Eq (2) as:p (y | x ; ) =1Z,xhexp > x(3)Combining Gaussian prior weights, obtain following formlog posterior data set:NhXX1yn > xn logl = log p ( | D, ) = 2 > +exp y0 > xn + const20n=1(4)parameters estimated using convex optimization technique; practice,limited memory BFGS (Nash & Nocedal, 1991; Averick & More, 1994) seems goodchoice (Malouf, 2002; Minka, 2003) use algorithm experimentsdescribed paper. order perform calculations, one must able computegradient Eq (4) respect , available closed form.3.2 Maximum Entropy Genre Adaptation ModelExtending maximum entropy model account in-domain out-of-domaindata framework described earlier requires addition several extra model param(i) (i)eters. particular, in-domain data point (xn , yn ), assume existence(i)(i)(i) (i)binary indicator variable zn . value zn = 1 indicates (xn , yn ) drawn q (i)(i)(the truly in-domain distribution), value zn = 0 indicates drawn q (g)(o) (o)(the general-domain distribution). Similarly, out-of-domain data point (x n , yn ),(o)(o)assume binary indicator variable zn , zn = 1 means data point drawn(o)q (the truly out-of-domain distribution) value 0 means drawnq (g) (the general-domain distribution). course, indicator variablesobserved data, must infer values automatically.105fiDaume III & Marcu22gyniynxnxnizniNynoNxnoznogNFigure 1: (Left) standard logistic regression model; (Right) Mega Model.According model, zn binary random variables assumedrawn Bernoulli distribution parameter (i) (for in-domain) (o) (for outof-domain). Furthermore, assume three vectors, (i) , (o) (g)corresponding q (i) , q (o) q (g) , respectively. instance, zn = 1, assume(i)xn classified using (i) . Finally, model binary vectors xn (respec(o)tively xn s) drawn independently Bernoulli distributions parameterized(i)(g) (respectively, (o) (g) ). Again, zn = 1, assume xndrawn according (i) . corresponds nave Bayes assumption generativeprobabilities xn vectors. Finally, place common Beta prior nave Bayesparameters, . Allowing range {i, o, g}, full hierarchical model is:()f | a, b(i)zn | (i)(i)(i)(i)(i)(i)(g)xnf | zn , f , f(i)() | 2(o)zn | (o)Bet(a, b)Ber( (i) )z (i)Ber( fn )(i)(i)yn | xn , zn , (i) , (g) Gibbs(xn , zn )(o)(o)(o)(o)(o)(g)xnf | zn , f , f(o)Nor(0, 2 I)Ber( (o) )(5)z (o)Ber( fn )(o)(o)yn | xn , zn , (o) , (g) Gibbs(xn , zn )term model Maximum Entropy Genre Adaptation Model (the MegaModel). corresponding graphical model shown right Figure 1. generative story in-domain data point x(i) follows:1. Select whether x(i) truly in-domain general-domain indicatez (i) {i, g}. Choose z (i) = probability (i) z (i) = g probability1 (i) .(i)2. component f x(i) , choose xf 1 probability zfz (i)probability 1 f .(i)3. Choose class according Eq (3) using parameter vector z .106(i)0fiDomain Adaptation Statistical Classifiersstory out-of-domain data points identical, uses truly out-of-domaingeneral-domain parameters, rather truly in-domain parameters generaldomain parameters.3.3 Linear Chain Modelsstraightforward extension maximum entropy classification model maximumentropy Markov model (MEMM) (McCallum, Freitag, & Pereira, 2000) obtainedassuming targets yn sequences labels. canonical example modelpart speech tagging: word sequence assigned part speech tag.introducing first order Markov assumption tag sequence, one obtains linear chainmodel viewed discriminative counterpart standard (generative)hidden Markov model. parameters models estimated usinglimited memory BFGS. extension Mega Model linear chain frameworksimilarly straightforward, assumption label (part speech tag)indicator variable z (versus global indicator variable z entire tag sequence).techniques described herein may also applied conditional random fieldframework Lafferty, McCallum, Pereira (2001), fixes bias problemMEMM performing global normalization rather per-state normalization. is,however, subtle difficulty direct application CRFs. Specifically, one would needdecide single z variable would assigned entire sentence, wordindividually. MEMM case, natural one z per word. However,CRF would computationally expensive. remainder, continueuse MEMM model efficiency purposes.4. Conditional Expectation MaximizationInference Mega Model slightly complex standard maximum entropy models. However, inference solved efficiently using conditional expectationmaximization (CEM), variant standard expectation maximization (EM) algorithm(Dempster, Laird, & Rubin, 1977), due Jebara Pentland (1998). high level, EMuseful computing generative models hidden variables, CEM usefulcomputing discriminative models hidden variables; Mega Model belongslatter family, CEM appropriate choice.standard EM family algorithms maximizes joint likelihood data.particular, (xn , yn )Nn=1 data z (discrete) hidden variable, M-step EMproceeds maximizing bound given Eq (6)log p (x, | ) = logXzp (z, x, | ) = log Ezp( | x;) p (x, | z; )(6)Eq (6), Ez denotes expectation. One may apply Jensens inequalityequation, states f (E{x}) E{f (x)} whenever f convex. Taking f = log,able decompose log expectation expectation log. typicallyseparates terms makes taking derivatives solving resolution optimization problem tractable. Unfortunately, EM cannot directly applied conditional models (such107fiDaume III & MarcuMega Model) form Eq (7) models result M-steprequires maximization equation form given Eq (8).log p (y | x; ) = logl = logXzXzp (z, | x; ) = log Ezp( | x,) p (y | x, z; )p (z, x, | ) logXzp (z, x | )(7)(8)Jensens inequality applied first term Eq (8), maximizedreadily standard EM. However, applying Jensens inequality second term wouldlead upper bound likelihood, since term appears negated.conditional EM solution (Jebara & Pentland, 1998) bound changelog-likelihood iterations, rather log-likelihood itself. change loglikelihood written Eq (9), denotes parameters iteration t.lc = log p | x; log p | x; t1(9)rewriting conditional distribution p (y | x) p (x, y) divided p (x),express lc log joint distribution difference minus log marginaldistribution. Here, apply Jensens inequality first term (the joint difference),second (because appears negated). Fortunately, Jensensbound employ. standard variational upper bound logarithm function is:log x x 1; leads lower bound negation, exactly desired.bound attractive reasons: (1) tangent logarithm; (2) tight;(3) makes contact current operating point (according maximizationprevious time step); (4) simply linear function; (5) terminologycalculus variations, variational dual logarithm; see (Smith, 1998).Applying Jensens inequality first term Eq (9) variational dualsecond term, obtain change log-likelihood moving model parameterst1 time 1 time (which shall denote Qt ) bounded l Qt ,Qt defined Eq (10), h = E{z | x; } z = 1 1 E{z | x; }z = 0, expectations taken respect parameters previous iteration.PXp z, x, |z p z, x |+1(10)Q =hz logPt1 )p (z, x, | t1 )z p (z, x |zZapplying two bounds (Jensens inequality variational bound),removed sums logs, hard deal analytically. full derivationgiven Appendix A. remaining expression lower bound change likelihood,maximization result maximization likelihood.MAP variant standard EM, change E-step priorsplaced parameters. assumption standard EM wish maximizep ( | x, y) p () p (y | , x) prior probability ignored, leavinglikelihood term parameters given data. MAP estimation, makeassumption instead use true prior p (). so, need add factorlog p () definition Qt Eq (10).108fiDomain Adaptation Statistical Classifierst1jn,znmt1n= log p xn , yn , zn | t1n,znPt1 1=n,zn ,f 0z n p x n , zn |xnf1xnfznzn1f =1ffxnf1xnfQznzn=10f 6=fff=QFTable 1: Notation used Mega Model equations.important note although make use full joint distribution p (x, y, z),objective function model conditional. joint distribution usedprocess creating bound: overall optimization maximize conditional likelihood labels given input. particular, bound using full joint likelihoodholds parameters marginal.5. Parameter Estimation Mega Modelmade explicit Eq (10), relevant distributions performing CEM full jointdistributions input variables x, output variables y, hidden variables z.Additionally, require marginal distribution x variables z variables.Finally, need compute expectations z variables. derive expectationstep section present final solution maximization step classvariables. derivation equations maximization given Appendix B.Q bound complete conditional likelihood Mega Modelis given below:P(i)(i)(i)(i)(i)z,xppz,x,(i)Xnnnnnz+ 1P nh(i)Qt =n log(i)(i)(i)0 z (i) , x(i)0p z n , x n , yn(i) pnnn=1 z (i)znnP(o)(o) (o)(o)(o)(o)Np zn , xn , yn(o) p zn , xnX Xz+ 1P nh(o)+n log(o)(o)(o)(o)(o)00p z n , x n , ynz n , xn(o) pn=1 z (o)zN (i)X(11)nnequation, p0 () probability distribution previous iteration. firstterm Eq (11) bound in-domain data, second term boundout-of-domain data. optimizations described section, nearlyidentical terms in-domain parameters out-of-domain parameters. brevity,explicitly write equations in-domain parameters; correspondingout-of-domain equations easily derived these. Moreover, reduce notationaloverload, elide superscripts denoting in-domain out-of-domain obviouscontext. notational brevity, use notation depicted Table 1.5.1 Expectation StepE-step concerned calculating hn given current model parameters. Since zn{0, 1}, easily find hn = p (zn = 1|), calculated follows:109fiDaume III & Marcup (zn = z | xn , yn , , , )p (zn = z | ) p (xn | , zn = z) p (yn | , zn = z)= Pz p (zn = z | ) p (xn | , zn = z) p (yn | , zn = z)h1z (1 )1z n,zexp zyn > xnZxn ,z(12)Here, Z partition function before. easily calculated z {0, 1}expectation found dividing value z = 1 sum both.5.2 M-Stepshown Appendix B.1, directly computevalue solving simplequadratic equation. compute + a2 b, where:=b =5.3 M-StepPNt1n=1 2hn mn (n,0 n,1 )PN2 n=1 mt1n (n,0 n,1 )PNn=1 hnPNt1n=1 mn (n,0 n,1 )1Viewing Qt function , easy see optimization variable convex.analytical solution available, gradient Qt respect (i)seen identical gradient standard maximum entropy posterior, Eq (4),data point weighted according posterior probability, (1 h n ). maythus use identical optimization techniques computing optimal variables standardmaximum entropy models; difference data points weighted.similar story holds (o) . case (g) , obtain standard maximum entropy(i)gradient, computed N (i) + N (o) data points, xn weighted hn(o)(o)xn weighted hn . shown Appendix B.2.5.4 M-StepLike case , cannot obtain analytical solution finding maximizesQt . However, compute simple derivatives Qt respect single component(i)f maximized analytically. shown Appendix B.3, compute f+ a2 b, where:PNn=1 1 hn + jn,0 (1 )n,0,f=P2 Nn=1 jn,0 (1 )n,0,fPN(1 hn ) xnf1+b = PN n=1n=1 jn,0 (1 )n,0,f110fiDomain Adaptation Statistical ClassifiersAlgorithm MegaCEM()()Initialize f = 0.5, f = 0, () = 0.5 {g, i, o} f .parameters havent converged iterations remain{- Expectation Step -}n = 1..N (i)(i)Compute in-domain marginal probabilities, mn(i)Compute in-domain expectations, hn , Eq (12)endn = 1..N (o)(o)Compute out-of-domain marginal probabilities, mn(o)Compute out-of-domain expectations, hn Eq (12)end{- Maximization Step -}Analytically update (i) (o) according equations shown Section 5.2Optimize (i) , (o) (g) using BFGSIterations remain and/or havent convergedUpdate according derivation Section 5.4endendreturn , ,Figure 2: full training algorithm Mega Model.case (o) identical. (g) , difference must replacesum data points two sums, one in-domain out-of-domainpoints; and, before, 1 hn must replaced hn ; made explicitAppendix. Thus, optimize variables, simply iterate optimizecomponent analytically, given above, convergence.5.5 Training Algorithmfull training algorithm depicted Figure 2. Convergence properties CEMalgorithm ensure converge (local) maximum posterior space. localoptima become problem practice, one alternatively use stochastic optimizationalgorithm, temperature applied enabling optimization jump localoptima early on. However, explore idea work. contextapplication, extension required.5.6 CEM ConvergenceOne immediate question conditional EM model described manyEM iterations required model converge. experiments, 5 iterations111fiDaume III & MarcuConvergence CEM Optimization222018Negative Log Likelihood (*1e6)16141210864200123Number Iterations45Figure 3: Convergence training algorithm.CEM sufficient, often 2 3 necessary. make clear,Figure 3, plotted negative complete log likelihood model firstdata set, described Section 6.2. three separate maximizations fulltraining algorithm (see Figure 2); first involves updating variables, secondinvolves optimizing variables third involves optimizing variables.compute likelihood steps.Running total 5 CEM iterations still relatively efficient model. dominating expense weighted maximum entropy optimization, which, 5 CEM iterations,must computed 15 times (each iteration requires optimization threesets variables). worst take 15 times amount time train modelcomplete data set (the union in-domain out-of-domain data), practiceresume optimization ending point previous iteration, causessubsequent optimizations take much less time.5.7 Predictiontraining supplied us model parameters, subsequent task applyparameters unseen data obtain class predictions. assume test data indomain (i.e., drawn either Q(i) Q(g) notation introduction),obtain decision rule form given Eq (13) new test point x.= arg max p (y | x; )yYX= arg maxp (z | x; ) p (y | x, z; )yY= arg maxyYzXzp (z | ) p (x | z; ) p (y | x, z; )112fiDomain Adaptation Statistical Classifiers= arg maxF(g)fxf(g)1 f1xfh(g)exp > xZx,(g)h>xFxf1xf exp (i)(i)(i)+ (1 )f1 fZx,(i)yYf =1(13)f =1Thus, decision rule simply select class highest probability according maximum entropy classifiers, weighted linearly marginal probabilitiesnew data point drawn Q(i) versus Q(g) . sense, modelseen linearly interpolating in-domain model general-domain model,interpolation parameter input specific.6. Experimental Resultssection, describe result applying Mega Model several datasetsvarying degrees divergence in-domain out-of-domain data. However,describing data results, discuss systems compare.6.1 Baseline SystemsThough little literature problem thus real systemscompare, several obvious baselines, describe section.OnlyI: model obtained simply training standard maximum entropy modelin-domain data. completely ignores out-of-domain data servesbaseline case data unavailable.OnlyO: model obtained training standard maximum entropy modelout-of-domain data, completely ignoring in-domain data. serves baselineexpected performance without annotating new data. also gives sense closeout-of-domain distribution in-domain distribution.LinI: model obtained linearly interpolating OnlyI OnlyO systems.interpolation parameter estimated held-out (development) in-domain data.means that, practice, extra in-domain data would need annotated order createdevelopment set; alternatively, cross-validation could used.Mix: model obtained training maximum entropy model unionout-of-domain in-domain data sets.MixW: model also obtained training maximum entropy model unionout-of-domain in-domain data sets, out-of-domain data downweighted effectively equinumerous in-domain data.Feats: model uses out-of-domain data build one classifier usesclassifiers predictions features in-domain data, described ? (?).113fiDaume III & MarcuPrior: adaptation model described Section 2.2, out-of-domaindata used estimate prior in-domain classifier. case maximumentropy models consider here, weights learned out-of-domain data usedmean Gaussian prior distribution placed weights trainingin-domain data, described Chelba Acero (2004).cases, tune model hyperparameters using performance development data.development data taken random 20% training data cases.appropriate hyperparameters found, 20% folded back training set.6.2 Data Setsevaluate models three different problems. first two problems comeAutomatic Content Extraction (ACE) data task. data selected ACEprogram specifically looks data different domains. third problemtackled Chelba Acero (2004), required annotate data themselves.6.2.1 Mention Type Classificationfirst problem, Mention Type, subcomponent entity mention detectiontask (an extension named entity tagging task, wherein pronouns nominalsmarked, addition simple names). assume extents mentionsmarked simply need identify type, one of: Person, Geo-political Entity,Organization, Location, Weapon Vehicle. out-of-domain data, use newswirebroadcast news portions ACE 2005 training data; in-domain data, useFisher conversations data. example out-of-domain sentence is:again, prime battleground constitutional allocation powernomnamfederal governmentnomgpe statesgpe , Congressorgbarfederal regulatory agenciesorg .example in-domain sentence is:nompronommyproper wifeper Iper transported across continent gpewhq prowhereloc Iper bornuse 23k out-of-domain examples (each mention corresponds one example), 1kin-domain examples 456 test examples. Accuracy computed 0/1 loss. usestandard feature functions employed named entity models, include lexical items,stems, prefixes suffixes, capitalization patterns, part-of-speech tags, membershipinformation gazetteers locations, businesses people. accuracies reportedresult running ten fold cross-validation.6.2.2 Mention Taggingsecond problem, Mention Tagging precursor Mention Type task,attempt tag entity mentions raw text. use standard Begin/In/Outencoding use maximum entropy Markov model perform tagging (McCallumet al., 2000). out-of-domain data, use newswire broadcast news114fiDomain Adaptation Statistical Classifiersdata; in-domain data, use broadcast news data transcribedautomatic speech recognition. in-domain data lacks capitalization, punctuation, etc.,also contains transcription errors (speech recognition word error rate approximately15%). tagging task, 112k out-of-domain examples (in context tagging,example single word), 5k in-domain examples 11k test examples.Accuracy F-measure across segmentation. use features mentiontype identification task. scores reported ten fold cross-validation.6.2.3 Recapitalizationfinal problem, Recap, task recapitalizing text. Following Chelba Acero(2004), use maximum entropy Markov model, possible tags are:Lowercase, Capitalized, Upper Case, Punctuation Mixed case. out-of-domaindata task comes Wall Street Journal, two separate in-domain data setscome broadcast news text CNN/NPR ABC Primetime, respectively. use3.5m out-of-domain examples (one example one word). CNN/NPR data, use146k in-domain training examples 73k test examples; ABC Primetime data,use 33k in-domain training examples 8k test examples. use identical featuresChelba Acero (2004). order maintain comparability results describedChelba Acero (2004), perform cross-validation experiments: usetrain/test split described paper.6.3 Feature Selectionmaximum entropy models used classification adept dealingmany irrelevant and/or redundant features, nave Bayes generative model, usemodel distribution input variables, overfit features. turnedproblem Mention Type Mention Tagging problems,Recap problems, caused errors. alleviate problem, Recapproblem only, applied feature selection algorithm features used naveBayes model (the entire feature set used maximum entropy model). Specifically,took 10k top features according information gain criteria predict indomain versus out-of-domain (as opposed feature selection class label); Forman(2003) provides overview different selection techniques.16.4 Resultsresults shown Table 2, see training in-domain dataalways outperforms training out-of-domain data. linearly interpolated modelimprove base models significantly. Placing data one bag helps,clear advantage re-weighting domain data. Prior modelFeats model perform roughly comparably, Prior model edgingsmall margin.2 model outperforms Prior model Feats model.1. value 10k selected arbitrarily initial run model development data;tuned optimize either development test performance.2. numbers result Prior model data Chelba Acero (2004) differ slightlyreported paper. two potential reasons this. First, numbers115fiDaume III & Marcu|D(o) ||D(i) |AccuracyOnlyOOnlyILinIMixMixWFeatsPriorMegaM% ReductionMixPriorMentionType23k1kMentionTagging112k5kRecapABC3.5m8kRecapCNN3.5m73kAverage-57.681.281.584.981.387.887.992.178.383.583.880.981.084.285.188.295.597.497.796.497.697.897.998.194.694.794.995.093.596.195.996.881.589.289.589.388.891.591.793.947.734.738.220.852.819.036.022.043.026.5Table 2: Experimental results; first set rows show sizes in-domainout-of-domain training data sets. second set rows (Accuracy) showperformance various models four tasks. last two rows (%Reduction) show percentage reduction error rate using Mega Modelbaseline model (Mix) best alternative method (Prior).applied McNemars test (Gibbons & Chakraborti, 2003, section 14.5) gage statistical significance results, comparing results Prior modelMega Model (for mention tagging experiment, compute McNemars test simpleHamming accuracy rather F-score; suboptimal, knowcompute statistical significance F-score). mention type task, differencestatistical significant p 0.03 level; mention tagging task, p 0.001;recapitalization tasks, difference ABC data significant p 0.06level, CNN/NPR data significant p 0.004 level.mention type task, improved baseline model trained in-domaindata accuracy 81.2% 92.1%, relative improvement 13.4%. mentiontagging, improve 83.5% F-measure 88.2%, relative improvement 5.6%.ABC recapitalization task (for much in-domain data available), increaseperformance 95.5% 98.1%, relative improvement 2.9%. CNN/NPRrecapitalization task (with little in-domain data), increase performance 94.6%96.8%, relative improvement 2.3%.reported based using 20m examples; consider 3.5m example case. Second,likely subtle differences training algorithms used. Nevertheless, whole, relativeimprovements agree paper.116fiDomain Adaptation Statistical ClassifiersMention Type Identification TaskMention Tagging Task9095OnlyOutChelbaMegaM90OnlyOutChelbaMegaM858580AccuracyFmeasure8075757065706065010110231010Amount Domain Data Used (log scale)55110410210Amount Domain Data Used (log scale)Figure 4: Learning curves Prior MegaM models.6.5 Learning Curvesparticular interest amount annotated in-domain data needed see markedimprovement OnlyO baseline well adapted system. show Figure 4learning curves Mention Type Mention Tagging problems. Along x-axis,plot amount in-domain data used; along y-axis, plot accuracy. plotthree lines: flat line OnlyO model use in-domain data,curves Prior MegaM models. see, model maintains accuracymodels, Prior curve actually falls baselinetype identification task.37. Model Introspectionseen previous sections Mega Model routinely outperforms competing models. Despite clear performance improvement, question remains open regardinginternal workings models. (i) variable captures degree indomain data set truly in-domain. z variables model aim capture,test data point, whether general domain in-domain. section, discussparticular values parameters model learns variables.present two analyses. first (Section 7.1), inspect models inner workingsMention Type task Section 6.2.1. analysis, look specificallyexpected values hidden variables found model. second analysis(Section 7.2), look ability model judge degree relatedness, definedvariables.3. Fisher data personal conversations. hence much higher degree firstsecond person pronouns news. (The baseline always guesses person achieves 77.8%accuracy.) able intelligently use out-of-domain data in-domain modelunsure, performance drops, observed Prior model.117310fiDaume III & MarcuPre-contexthome trentonveterans administrationknow americangivescapable gettingfisher thing calling. . . Entity . . .. . . new jersey . . .. . . hospital . . .. . . government. . ........ . . anything . . ..........kid...Post-contextthatschillsha ha screwedTrueGPEORGORGPERWEAPERPERHypGPELOCORGPERPERPERPERp (z = I)0.020.110.170.710.920.930.98Table 3: Examples test data Mention Type task. True columncorrect entity type Hyp column models prediction. finalcolumn probability example truly in-domain model.7.1 Model Expectationsfocus discussion, consider Mention Type task, Section 6.2.1.Table 3, shown seven test-data examples Mention Type task. Precontext text appears entity post-context textappears after. report true class class model hypothesizes. Finally,report probability example truly in-domain, according model.see, three examples model thinks general domain newjersey, hospital government. believes me, anything kidin-domain. general, probabilities tend skewed toward 0 1,uncommon nave Bayes models. shown two errors data. first,model thinks hospital location truly organization.difficult distinction make: training data, hospitals often used locations.second example error anything capable getting anythinghere. long-distance context example discussion biological warfareSaddam Hussein, anything supposed refer type biological warhead.model mistakingly thinks person. error likely due factmodel identifies word anything likely truly in-domain (the wordcommon newswire). also learned truly in-domain entities people.Thus, lacking evidence otherwise, model incorrectly guesses anything person.interesting observe model believes entity giveschills closer general domain fisher thing calling ha hascrewed up. likely occurs context ha ha occurred anywhereout-of-domain training data, twice in-domain training data. unlikelyexample would misclassified otherwise (me fairly clearly person),example shows model able take context account deciding domain.decisions made model, shown Table 3 seem qualitatively reasonable.numbers perhaps excessively skewed, ranking believable. in-domaindata primarily conversations random (not necessarily news worthy) topics,hence highly colloquial. Contrastively, out-of-domain data formal news.model able learn entities like new jersey governmentnews words like kid.118fiDomain Adaptation Statistical Classifiers(i)(o)MentionType0.140.11MentionTagging0.410.45RecapCNN0.360.40RecapABC0.510.69Table 4: Values variables discovered Mega Model algorithm.7.2 Degree Relatednesssection, analyze values found model. Low values (i)(o) mean in-domain data significantly different out-of-domain data;high values mean similar. high value meansgeneral domain model used cases. tasks Mention Type,values middling around 0.4. Mention Type, (i) 0.14 (o) 0.11,indicating significant difference in-domain out-of-domaindata. exact values tasks shown Table 4.values make intuitive sense. distinction conversation datanews data (for Mention Type task) significantly stronger differencemanually automatically transcribed newswire (for Mention Tagging task).values reflect qualitative distinction. rather strong differencevalues recapitalization tasks expected priori. However, post hocanalysis shows result reasonable. compute KL divergence unigramlanguage model out-of-domain data set in-domain data sets.KL divergence CNN data 0.07, divergence ABC data 0.11.confirms ABC data perhaps different baseline out-of-domainCNN data, reflected values.also interested cases little difference in-domainout-of-domain data. simulate case, performed following experiment.consider Mention Type task, use training portion out-ofdomain data. randomly split data half, assigning half in-domainout-of-domain. theory, model learn may rely generaldomain model. performed experiment ten fold cross-validation foundaverage value selected model 0.94. strictly lessone, show model able identify similar domains.8. Conclusion Discussionpaper, presented Mega Model domain adaptation discriminative (conditional) learning framework. described efficient optimization algorithmsbased conditional EM technique. experimentally shown, four data sets,model outperforms large number baseline systems, including current stateart model, requiring significantly less in-domain data.Although focused specifically discriminative modeling maximum entropyframework, believe novel, basic idea work foundedto breakin-domain distribution p(i) out-of-domain distribution p(o) three distributions, q (i) ,119fiDaume III & Marcuq (o) q (g) general. particular, one could perform similar analysis casegenerative models obtain similar algorithms (though case generative model,standard EM could used). model could applied domain adaptationlanguage modeling machine translation.exception work described Section 2.2, previous work in-domain adaptation quite rare, especially discriminative learning framework. substantial literature language modeling/speech community, adaptationconcerned based adapting new speakers (Iyer, Ostendorf, & Gish,1997; Kalai, Chen, Blum, & Rosenfeld, 1999). learning perspective, MegaModel similar mixture experts model. model seen constrained experts model, three experts, constraints specify in-domaindata come one two experts, out-of-domain data comeone two experts (with single expert overlapping two). attemptsbuild discriminative mixture experts models make heuristic approximations orderperform necessary optimization (Jordan & Jacobs, 1994), rather apply conditionalEM, gives us strict guarantees monotonically increase data (incomplete)log likelihood iteration training.domain adaptation problem also closely related multitask learning (also knownlearning learn inductive transfer). multitask learning, one attempts learnfunction solves many machine learning problems simultaneously. related problemdiscussed Thrun (1996), Caruana (1997) Baxter (2000), among others.similarity multitask learning domain adaptation dealdata drawn related, distinct distributions. primary difference domainadaptation cares predicting one label type, multitask learning carespredicting many.various sub-communities natural language processing family begin continue branch domains newswire, importance developing modelsnew domains without annotating much new data become important.Mega Model first step toward able migrate simple classification-style models (classifiers maximum entropy Markov models) across domains. Continued researcharea adaptation likely benefit work done active learninglearning large amounts unannotated data.Acknowledgmentsthank Ciprian Chelba Alex Acero making data available. thank RyanMcDonald pointing Feats baseline, previously considered.also thank Kevin Knight Dragos Munteanu discussions related project.paper greatly improved suggestions reviewers, including reviewersprevious, shorter version. work partially supported DARPA-ITO grant N6600100-1-9814, NSF grant IIS-0097846, NSF grant IIS-0326276, USC Dean FellowshipHal Daume III.120fiDomain Adaptation Statistical ClassifiersAppendix A. Conditional Expectation Maximizationappendix, derive Eq (10) Eq (7) making use Jensens inequalityvariational bound. interested reader referred work Jebara Pentland(1998) details. discussion consider bound change loglikelihood iteration 1 iteration t, l c , given Eq (14):p | x;p x, | /p |l = log= logp (y | x; t1 )p (x, | t1 ) /p (y | t1 )p x, y;p x;= loglogp (x, y; t1 )p (x; t1 )c(14)(15)Here, effectively rewritten log-change ratio conditionalsdifference log-change ratio joints log-change ratiomarginals. may rewrite Eq (15) introducing hidden variables z as:PPz p x, z;z p x, y, z;log Pl = log Pt1 )t1 )z p (x, y, z;z p (x, z;c(16)apply Jensens inequality first term Eq (16) obtain:clXz"#Pp x, y, z | t1p x, y, z;z p x, z;Ploglog Pt1 )t1 )p (x, y, z; t1 )z 0 p (x, y, z |z p (x, z;|{z}(17)hx,y,z,t1Eq (17), expression denoted hx,y,z,t1 joint expectation zprevious iterations parameter settings. Unfortunately, cannot also apply Jensens inequality remaining term Eq (17) appears negated. applyingvariational dual (log x x 1) term, obtain following, final bound:cl Q =XzPp x, y, z;z p x, z;+1hx,y,z,t1 logPt1 )p (x, y, z; t1 )z p (x, z;(18)Applying bound Eq (18) distributions chosen model yields Eq (10).Appendix B. Derivation Estimation EquationsGiven model structure parameterization Mega Modelgiven Section 3.2,Eq (5), obtain following expression joint probability data:121fiDaume III & Marcup x, y, z | () , () ,FNznznBer(xnf | f )Gibbs(yn | xn , )Ber(zn | )=n=1f =1FN1xnfxnf=1 zfnzfnzn (1 )1znn=1f =1!hX1hexp zynn > xnexp zcn > xnc(19)marginal distribution obtained removing last two terms (the expsum exps) final equation. Plugging Eq (19) Eq (10) using notationEq (12), obtain following expression Qt :Qt =X+NXn=1log Nor(() ; 0, 2 I) +"Xzn(FXf =1()log Bet( f ; a, b)hn zn log + (1 zn ) log(1 ) + log n,zn+FXxnf log zynnf =1znmt1n (1)1znlogXcn,zn + 1#exphzcn > xnjn,zn)(20)well analogous term out-of-domain data. j defined Table 1.B.1 M-Stepcomputing , simply differentiate Qt (see Eq (20)) respect , obtaining:NQt X hn 1 hn=++ mt1n (n,0 n,1 )1(21)n=1solving 0 leads directly quadratic expression form:0 =2""NXmt1n (n,0n=1+ 1 1 +NXn=1n,1 )#2hn mt1n (n,0 n,1 )122#fiDomain Adaptation Statistical Classifiers+0"NXn=1hn#(22)Solving directly gives desired update equation.B.2 M-Stepoptimizing (i) , rewrite Qt , Eq (20), neglecting irrelevant terms, as:Qt [] =NXn=1(1 hn )FXf =1xnf yn ,f logXchexp c > xn+ log Nor(; 0, 2 I)(23)Eq (23), bracketed expression exactly log-likelihood term obtainedstandard logistic regression models. Thus, optimization Q respect (i)(o) performed using weighted version standard logistic regression optimization,weights defined (1hn ). case (g) , obtain weighted logistic regressionmodel, N (i) + N (o) data points, weights defined hn .B.3 M-Stepcase (i) (o) , rewrite Eq (20) remove irrelevant terms, as:Qt [ (i) ] =FXf =1log Bet(f ; a, b) +NXn=1(1 hn ) log n,0 mt1n (1 )n,0(24)Due presence product term , cannot compute analytical solutionmaximization problem. However, take derivatives component-wise (in F )obtain analytical solutions (when combined prior). admits iterativesolution maximizing Qt maximizing component separately convergence.Computing derivatives Qt respect f requires differentiating n,0 respectf ; convenient form (recalling notation Table 1:n,0 = [n,0,f ]{xnf f + (1 xnf )(1 f )} = n,0,fff(25)Using result, maximize Qt respect f solving:"NXxnf (1 f ) (1 xnf )fh(1 hn )Qf=ff (1 f )n=1#(26)1f (1 f )#NXjn,0 (1 )n,0,ff )jn,0 (1 )n,0,f +="NX1(1 hn ) (xnf1+f (1 f )n=1123n=1fiDaume III & MarcuEquating zero yields quadratic expression form:0 = (f )2+ (f )1"NXjn,0 (1n=1" NX")n,0,f#1 hn + jn,0 (1 )n,0,fn=1NX+ (f )0 1 +n=1(1 hn ) xnf##(27)(o)final equation solved analytically. similar expression arises f .(g)case f , obtain quadratic form sums entire data sethn replacing occurrences (1 hn ):0 =++N (i)2 X(i)(i)(g)jn,1 (i)n,1,ffn=1n=1(g)f1"(g) 0f+(o)NXN (i)Xn=1Xn=1jn,1 (o) n,1,f(i)h(i)n + jn,1 n,1,fN (i)1+(i)(i)(o)(o)N (o)(i)h(i)n xnf +Xn=1(o)h(o)n xnf(o)NXn=1(o)(o)(o)h(o)n + jn,1 n,1,f#(28)Again, solved analytically. values j, m, , ,, defined Table 1.ReferencesAverick, B. M., & More, J. J. (1994). Evaluation large-scale optimization problemsvector parallel architectures. SIAM Journal Optimization, 4.Baxter, J. (2000). model inductive bias learning. Journal Artificial IntelligenceResearch, 12 , 149198.Bacchiani, M., & Roark, B. (2003). Unsupervised langauge model adaptation. ProceedingsInternational Conference Acoustics, Speech Signal Processing (ICASSP).Caruana, R. (1997). Multitask learning: knowledge-based source inductive bias. Machine Learning, 28 , 4175.Chelba, C., & Acero, A. (2004). Adaptation maximum entropy classifier: Little datahelp lot. Proceedings Conference Empirical Methods NaturalLanguage Processing (EMNLP), Barcelona, Spain.Chen, S., & Rosenfeld, R. (1999). Gaussian prior smoothing maximum entropymodels. Tech. rep. CMUCS 99-108, Carnegie Mellon University, Computer ScienceDepartment.124fiDomain Adaptation Statistical ClassifiersDella Pietra, S., Della Pietra, V. J., & Lafferty, J. D. (1997). Inducing features randomfields. IEEE Transactions Pattern Analysis Machine Intelligence, 19 (4), 380393.Dempster, A., Laird, N., & Rubin, D. (1977). Maximum likelihood incomplete datavia EM algorithm. Journal Royal Statistical Society, B39.Elhadad, N., Kan, M.-Y., Klavans, J., & McKeown, K. (2005). Customization unifiedframework summarizing medical literature. Journal Artificial IntelligenceMedicine, 33 (2), 179198.Forman, G. (2003). extensive empirical study feature selection metrics text classification. Journal Machine Learning Research, 3, 12891305.Gibbons, J. D., & Chakraborti, S. (2003). Nonparametric Statistical Inference. MarcelDekker, Inc.Gildea, D. (2001). Corpus variation parser performance. Proceedings Conference Empirical Methods Natural Language Processing (EMNLP).Hobbs, J. R. (2002). Information extraction biomedical text. Journal BiomedicalInformatics, 35 (4), 260264.Huang, J., Zweig, G., & Padmanabhan, M. (2001). Information extraction voicemail.Proceedings Conference Association Computational Linguistics(ACL).Hwa, R. (1999). Supervised grammar induction using training data limited constituentinformation. Proceedings Conference Association ComputationalLinguistics (ACL), pp. 7379.Iyer, R., Ostendorf, M., & Gish, H. (1997). Using out-of-domain data improve in-domainlanguage models. IEEE Signal Processing, 4 (8).Jebara, T., & Pentland, A. (1998). Maximum conditional likelihood via bound maximizationCEM algorithm. Advances Neural Information Processing Systems(NIPS).Jordan, M., & Jacobs, R. (1994). Hierarchical mixtures experts EM algorithm.Neural Computation, 6, 181214.Kalai, A., Chen, S., Blum, A., & Rosenfeld, R. (1999). On-line algorithms combininglanguage models. ICASSP.Lafferty, J., McCallum, A., & Pereira, F. (2001). Conditional random fields: Probabilisticmodels segmenting labeling sequence data. Proceedings InternationalConference Machine Learning (ICML).Malouf, R. (2002). comparison algorithms maximum entropy parameter estimation.Proceedings CoNLL.McCallum, A., Freitag, D., & Pereira, F. (2000). Maximum entropy Markov modelsinformation extraction segmentation. Proceedings International Conference Machine Learning (ICML).125fiDaume III & MarcuMinka, T. P. (2003). comparison numerical optimizers logistic regression. http://www.stat.cmu.edu/~minka/papers/logreg/.Munteanu, D., & Marcu, D. (2005). Improving machine translation performance exploiting non-parallel corpora. Computational Linguistics, appear.Nash, S., & Nocedal, J. (1991). numerical study limited memory BFGS methodtruncated Newton method large scale optimization. SIAM JournalOptimization, 1, 358372.Rambow, O., Shrestha, L., Chen, J., & Lauridsen, C. (2004). Summarizing email threads.Proceedings Conference North American Chapter AssociationComputational Linguistics (NAACL) Short Paper Section.Roark, B., & Bacchiani, M. (2003). Supervised unsupervised PCFG adaptationnovel domains. Proceedings Conference North American ChapterAssociation Computational Linguistics Human Language Technology(NAACL/HLT).Smith, D. R. (1998). Variational Methods Optimization. Dover Publications, Inc., Mineola, New York.Thrun, S. (1996). learning n-th thing easier learning first. AdvancesNeural Information Processing Systems (NIPS).126fiJournal Artificial Intelligence Research 26 (2006) 191246Submitted 01/05; published 07/06Fast Downward Planning SystemMalte HelmertHELMERT @ INFORMATIK . UNI - FREIBURG . DEInstitut fur InformatikAlbert-Ludwigs-Universitat FreiburgGeorges-Kohler-Allee, Gebaude 05279110 Freiburg, GermanyAbstractFast Downward classical planning system based heuristic search. deal general deterministic planning problems encoded propositional fragment PDDL2.2, includingadvanced features like ADL conditions effects derived predicates (axioms). Likewell-known planners HSP FF, Fast Downward progression planner, searchingspace world states planning task forward direction. However, unlike PDDL planning systems, Fast Downward use propositional PDDL representation planningtask directly. Instead, input first translated alternative representation called multivalued planning tasks, makes many implicit constraints propositional planningtask explicit. Exploiting alternative representation, Fast Downward uses hierarchical decompositions planning tasks computing heuristic function, called causal graph heuristic,different traditional HSP-like heuristics based ignoring negative interactionsoperators.article, give full account Fast Downwards approach solving multi-valuedplanning tasks. extend earlier discussion causal graph heuristic tasks involvingaxioms conditional effects present novel techniques search control usedwithin Fast Downwards best-first search algorithm: preferred operators transfer idea helpful actions local search global best-first search, deferred evaluation heuristic functionsmitigates negative effect large branching factors search performance, multi-heuristicbest-first search combines several heuristic evaluation functions within single search algorithmorthogonal way. also describe efficient data structures fast state expansion (successorgenerators axiom evaluators) present new non-heuristic search algorithm called focusediterative-broadening search, utilizes information encoded causal graphs novelway.Fast Downward proven remarkably successful: classical (i. e., propositional,non-optimising) track 4th International Planning Competition ICAPS 2004, followingfootsteps planners FF LPG. experiments show also performswell benchmarks earlier planning competitions provide insightsusefulness new search enhancements.1. IntroductionConsider typical transportation planning task: postal service must deliver number parcelsrespective destinations using vehicle fleet cars trucks. Let us assume carserves locations one city, different cities connected via highwaysserved trucks. sake simplicity, let us assume travelling segmentroad highway incurs cost. highly realistic assumption, purposesexposition do. number parcels, posted arbitrary locationsc2006AI Access Foundation. rights reserved.fiH ELMERTp2BFc2Ec1GCc3p1Figure 1: transportation planning task. Deliver parcel p 1 C G parcel p2 F E,using cars c1 , c2 , c3 truck t. cars may use inner-city roads (thin edges),truck may use highway (thick edge).arbitrary destinations. Moreover, cities varying size, one several carswithin city, one several trucks connecting cities. Cars never leavecity. Fig. 1 shows example task kind two cities, three cars single truck.two parcels delivered, one (p 1 ) must moved two cities,(p2 ) stay within initial city.astute reader familiar planning literature noticedessentially describing L OGISTICS domain, standard benchmark classical planning systems,extended roadmaps complete graphs. (Part of) propositional STRIPS-like encodingtask shown Fig. 2.would human planners go solving tasks kind? likely, would usehierarchical approach: p1 , clear parcel needs moved cities,possible using truck. Since example city access highway onelocation, see must first load parcel car initial location, dropfirst citys highway access location, load truck, drop citys highwayaccess location, load car city, finally drop destination.commit high-level plan delivering p 1 without worrying lower-level aspectspath planning cars. obvious us good solution structure,since parcel change location clearly defined ways (Fig. 3). figureshows reasonable plans getting p 2 destination require loading carinitial city dropping target location. point ever loadingtruck cars left city.say committed (partially ordered, movements two parcelsinterleaved) high-level plan shown Fig. 5. need complete plan chooselinearization high-level steps fill movements vehicle fleet them.thus decomposed planning task number subproblems. parcel schedulingproblem (where, vehicles, parcel loaded unloaded) separatedpath planning problem vehicle fleet (how move point X Y).192fiT FAST OWNWARD P LANNING YSTEMVariables:at-p1-a, at-p1-b, at-p1-c, at-p1-d, at-p1-e,at-p2-a, at-p2-b, at-p2-c, at-p2-d, at-p2-e,at-c1-a, at-c1-b, at-c1-c, at-c1-d,at-c2-a, at-c2-b, at-c2-c, at-c2-d,at-c3-e, at-c3-f, at-c3-g,at-t-d, at-t-e,in-p1-c1, in-p1-c2, in-p1-c3, in-p1-t,in-p2-c1, in-p2-c2, in-p2-c3, in-p2-tInit:at-p1-c, at-p2-f, at-c1-a, at-c2-b, at-c3-g,Goal:at-p1-g, at-p2-eOperator drive-c1-a-d:PRE: at-c1-a ADD: at-c1-d DEL: at-c1-aOperator drive-c1-b-d:PRE: at-c1-b ADD: at-c1-d DEL: at-c1-bOperator drive-c1-c-d:PRE: at-c1-c ADD: at-c1-d DEL: at-c1-c...Operator load-c1-p1-a:PRE: at-c1-a, at-p1-a ADD: in-p1-c1 DEL:Operator load-c1-p1-b:PRE: at-c1-b, at-p1-b ADD: in-p1-c1 DEL:Operator load-c1-p1-c:PRE: at-c1-c, at-p1-c ADD: in-p1-c1 DEL:...Operator unload-c1-p1-a:PRE: at-c1-a, in-p1-c1 ADD: at-p1-a DEL:Operator unload-c1-p1-b:PRE: at-c1-b, in-p1-c1 ADD: at-p1-b DEL:Operator unload-c1-p1-c:PRE: at-c1-c, in-p1-c1 ADD: at-p1-c DEL:...at-p1-f, at-p1-g,at-p2-f, at-p2-g,at-t-eat-p1-aat-p1-bat-p1-cin-p1-c1in-p1-c1in-p1-c1Figure 2: Part typical propositional encoding transportation planning task (no actualPDDL syntax).193fiH ELMERTc1BCEFc2Gc3Figure 3: Domain transition graph parcels p 1 p2 . Indicates parcel changestate. example, arcs correspond actionsloading/unloading parcel location truck t.BFEEGCFigure 4: Domain transition graphs cars c 1 c2 (left), truck (centre), car c3 (right).Note graph corresponds part roadmap traversedrespective vehicle.loadc1-p1-cunloadc1-p1-dloadt-p1-dunloadt-p1-eloadc3-p2-funloadc3-p2-eloadc3-p1-eFigure 5: High-level plan transportation planning task.194unloadc3-p1-gfiT FAST OWNWARD P LANNING YSTEMc1c2c3p1p2Figure 6: Causal dependencies transportation planning task.graph search problems, corresponding graphs shown Fig. 3 Fig. 4.Graphs kind formally introduced domain transition graphs Section 5.course graph search problems interact, limited ways: Statetransitions parcels associated conditions regarding vehicle fleet, needconsidered addition actual path planning Fig. 3. example, parcel changestate location inside car c 1 car c1 location A. However, state transitionsvehicles associated conditions parts planning task, hencemoving vehicle one location another indeed easy finding path associateddomain transition graph. say parcels causal dependencies vehiclesoperators change state parcels preconditions statevehicles. Indeed, causal dependencies task, since parcels dependparcels vehicles depend anything except (Fig. 6). set causaldependencies planning task visualized causal graph.argue humans often solve planning tasks hierarchical fashion outlined preceding paragraphs, algorithmic approaches action planning usefully apply similarideas. Indeed, show following section, first introduce domain transition graphs causal graphs. However, earlier work almost exclusively focused acycliccausal graphs, good reason: causal graph planning task exhibits cycle, hierarchical decomposition possible, subproblems must solved achieveoperator precondition necessarily smaller original task. far aware,first (Helmert, 2004) present general planning algorithm focuses exploiting hierarchical information causal graphs. However, causal graph heuristic also requiresacyclicity; general case, considers relaxed planning problem operatorpreconditions ignored break causal cycles.Knowing cycles causal graphs undesirable, take closer look transportationplanning task. Let us recall informal definition causal graphs: causal graph planningtask contains vertex state variable arcs variables occur preconditionsvariables occur effects operator. far, may given impressioncausal graph example task well-behaved shape shown Fig. 6. Unfortunately,closer look STRIPS encoding Fig. 2, see case: correctcausal graph, shown Fig. 7, looks messy. discrepancy intuitive actualgraph due fact informal account human-style problem solving, madeuse (non-binary) state variables like location car c 1 state parcel p1 ,STRIPS-level state variables correspond (binary) object-location propositions like parcel p 1195fiH ELMERTFigure 7: Causal graph STRIPS encoding transportation planning task.location A. would much nicer given multi-valued encoding planningtask explicitly contains variable location car c 1 similar properties. Indeed,nice looking acyclic graph Fig. 6 causal graph multi-valued encoding shownFig. 8.provided intuition underlying concepts, let us state design goalFast Downward planning system: develop algorithm efficiently solves generalpropositional planning tasks exploiting hierarchical structure inherent causal graphs.need overcome three major obstacles undertaking:First, propositionally encoded planning tasks usually unstructured causal graphs.However, intuitive dependencies often become visible encodings multi-valuedstate variables. exploit fact automated PDDL planning system, devisedautomatic algorithm translating (or reformulating) propositional tasks multi-valuedones. translation algorithm considered independently rest planner; fact, also used part planning systems (van den Briel, Vossen, &Kambhampati, 2005). keep article focused, discuss translation algorithmhere, referring earlier work central ideas (Edelkamp & Helmert, 1999).Instead, consider output, multi-valued planning task, base formalism.Second, matter clever encoding is, planning tasks completely hierarchical nature. deal causal cycles, consider relaxations causaldependencies ignored use solutions relaxed problem within heuristic searchalgorithm.Third, even planning tasks solved hierarchically, finding solution difficult (indeed, still PSPACE-complete). reason, heuristic function considersfragment task time, namely subproblems induced single state variablepredecessors causal graph. Even planning problem still NP-complete,196fiT FAST OWNWARD P LANNING YSTEMVariables:p1, p2 {at-a, at-b, at-c, at-d, at-e, at-f, at-g,in-c1, in-c2, in-c3, in-t}c1, c2 {at-a, at-b, at-c, at-d}c3{at-e, at-f, at-g}{at-d, at-e}Init:p1 = at-c, p2 = at-fc1 = at-a, c2 = at-b, c3 = at-g, = at-eGoal:p1 = at-g, p2 = at-eOperator drive-c1-a-d:PRE: c1 = at-a EFF: c1 = at-dOperator drive-c1-b-d:PRE: c1 = at-b EFF: c1 = at-dOperator drive-c1-c-d:PRE: c1 = at-c EFF: c1 = at-d...Operator load-c1-p1-a:PRE: c1 = at-a, p1 = at-a EFF: p1 = in-c1Operator load-c1-p1-b:PRE: c1 = at-b, p1 = at-b EFF: p1 = in-c1Operator load-c1-p1-c:PRE: c1 = at-c, p1 = at-c EFF: p1 = in-c1...Operator unload-c1-p1-a:PRE: c1 = at-a, p1 = in-c1 EFF: p1 = at-aOperator unload-c1-p1-b:PRE: c1 = at-b, p1 = in-c1 EFF: p1 = at-bOperator unload-c1-p1-c:PRE: c1 = at-c, p1 = in-c1 EFF: p1 = at-c...Figure 8: Part encoding transportation planning task multi-valued state variables.197fiH ELMERTcontent incomplete solution algorithm within heuristic solver. solutionalgorithm theoretical shortcomings never failed us practice.introduced rationale approach, discuss related work next section.followed overview general architecture Fast Downward planning systemSection 3. planning system consists three components: translation, knowledge compilation,search. translation component converts PDDL2.2 tasks multi-valued planning tasks,formally introduce Section 4. knowledge compilation component discussedSection 5, search component Section 6. conclude presentation experimentalresults Section 7 discussion Section 8.2. Related Workplanning system based heuristic forward search, Fast Downward clearly relatedheuristic planners HSP (Bonet & Geffner, 2001) FF (Hoffmann & Nebel, 2001)architectural level. However, section focus work related conceptual level,i. e., work uses similar forms hierarchical decomposition causal graphs work usessimilar forms search domain transition graphs.2.1 Causal Graphs Abstractionterm causal graph first appears literature work Williams Nayak (1997),general idea considerably older. approach hierarchically decomposing planning tasksarguably old field AI Planning itself, first surfaced Newell Simons(1963) work General Problem Solver.Still, took long time notions evolve modern form. Sacerdotis (1974)ABSTRIPS algorithm introduced concept abstraction spaces STRIPS-like planning tasks.abstraction space STRIPS task state space abstracted task, obtainedremoving preconditions operators original task belong given setpropositions (which abstracted away). 1 solve planning task, ABSTRIPS first generatesplan abstracted task, refines plan inserting concrete plans abstractplan steps bridge gap abstract states satisfying operator preconditionsignored abstract level. idea easily generalized several levels abstraction forming abstraction hierarchy, abstract level top almostpreconditions ignored, successively introducing preconditions every layer finallayer hierarchy equals original planning task.One problem approach planning general guaranteeabstract plans bear resemblance reasonable concrete plans. example, abstraction spaceschosen badly, quite possible finding concrete plan satisfies preconditionfirst operator abstract plan difficult solving original goal concrete level.shortcomings spawned large amount research properties abstraction hierarchiesgenerated automatically.1. later work authors, propositions abstracted away also removed operator effects.makes difference subtle cases require presence axioms; distinguishtwo kinds abstraction here.198fiT FAST OWNWARD P LANNING YSTEMTenenberg (1991) gives one first formal accounts properties different kindsabstraction. Among contributions, defines so-called upward solution property,informally stated as: exists concrete solution, also exists abstractsolution. Rather surprisingly, abstractions considered time satisfied basicproperty, without one would loathe call given state space abstraction anotherstate space.limitation upward solution property states relationship concreteabstract plan all. ABSTRIPS-style hierarchical planning successful, abstractplan must bear resemblance concrete one; otherwise little point tryingrefine it. Indeed, Tenenberg introduces stronger versions upward solution property,relevant Fast Downward Knoblocks (1994) work ordered monotonicity property.abstraction space satisfies ordered monotonicity property if, roughly speaking, concretesolution derived abstract solution leaving actions abstract planintact relevant concrete plan. Clearly, important property ABSTRIPSlike hierarchical planning.Knoblocks article causal graphs first surface (although introduce namethem). Translated terminology, Knoblock proves following relationshipuseful abstractions causal graphs: causal graph contains path variableabstracted away variable abstracted away, abstraction orderedmonotonicity property. particular, means acyclic causal graphs, possible deviseabstraction hierarchy one new variable introduced level.Besides theoretical contributions, Knoblock presents planning system called ALPINEcomputes abstraction hierarchy planning task causal graph exploitswithin hierarchical refinement planner. Although planning method different,derivation abstraction hierarchy similar Fast Downwards method generatinghierarchical decompositions planning tasks (Section 5.2).itself, ordered monotonicity property sufficient guarantee good performancehierarchical planning approach. guarantees every concrete solution obtainednatural way abstract solution, guarantee abstract solutionsrefined concrete ones. guarantee provided downward refinement property,introduced Bacchus Yang (1994).downward refinement property rarely guaranteed actual planning domains,Bacchus Yang develop analytical model performance hierarchical planning situations given abstract plan refined certain probability p < 1. Basedanalysis, present extension ALPINE called HIGHPOINT, selects abstraction hierarchy high refinement probability among satisfy ordered monotonicityproperty. practice, feasible compute refinement probability, HIGHPOINT approximates value based notion k-ary necessary connectivity.2.2 Causal Graphs Unary STRIPS OperatorsCausal graphs first given name Jonsson Backstrom (1995, 1998b), calldependency graphs. study fragment propositional STRIPS negative conditionsinteresting property plan existence decided polynomial time, minimalsolutions task exponentially long, polynomial planning algorithm exists.199fiH ELMERTpresent incremental planning algorithm polynomial delay, i. e., planning algorithmdecides within polynomial time whether given task solution, and, so, generatessolution step step, requiring polynomial time two subsequent steps. 2fragment STRIPS covered Jonsson Backstroms algorithm called 3Sdefined requirement causal graph task acyclic state variablesstatic, symmetrically reversible, splitting. Static variables easyguarantee never change value solution plan. variables detectedcompiled away easily. Symmetrically reversible variables operatormakes true corresponding operator identical preconditions makesfalse, vice versa. words, variable symmetrically reversible iff domaintransition graph undirected. Finally, variable v splitting iff removal causal graphweakly disconnects positive successors (those variables appear effects operatorsv precondition) negative successors (those variables appear effectsoperators v precondition).Williams Nayak (1997) independently prove incremental (or, setting, reactive)planning polynomial problem STRIPS-like setting causal graphs acyclicoperators reversible. operators reversible (according definition WilliamsNayak), variables symmetrically reversible (according definition JonssonBackstrom), actually special case previous result. However, Williams Nayakswork applies general formalism propositional STRIPS, approachesdirectly comparable.recently, Domshlak Brafman provide detailed account complexity finding plans propositional STRIPS (with negation) formalism unary operators acyclicgraphs (Domshlak & Brafman, 2002; Brafman & Domshlak, 2003). 3 Among results,prove restriction unary operators acyclic graphs reduce complexityplan existence: problem PSPACE-complete, like unrestricted propositional STRIPSplanning (Bylander, 1994). also show singly connected causal graphs, shortest planscannot exponentially long, problem still NP-complete. even restricted classcausal graphs, namely polytrees bounded indegree, present polynomial planning algorithm. generally, analysis relates complexity STRIPS planning unary domainsnumber paths causal graph.2.3 Multi-Valued Planning Tasksexception Williams Nayaks paper, work discussed far exclusively dealspropositional planning problems, state variables assume values binary domain. observed introduction, question propositional vs. multi-valued encodingsusually strong impact connectivity causal graph task. fact, aparttrivial OVIE domain, none common planning benchmarks exhibits acyclic causal graph2. However, guarantee length generated solution polynomially related lengthoptimal solution; might exponentially longer. Therefore, algorithm might spend exponential time taskssolved polynomial time.3. According formal definition causal graphs Section 5.2, operators several effects always inducecycles causal graph, acyclic causal graph implies unary operators. researchers define causal graphsdifferently, name properties explicitly here.200fiT FAST OWNWARD P LANNING YSTEMconsidering propositional representation. contrast, multi-valued encodingintroductory example acyclic causal graph.Due dominance PDDL (and previously, STRIPS) formalism, non-binary state variables studied often classical planning literature. One important exceptions rule work SAS + planning formalism, papers BackstromNebel (1995) Jonsson Backstrom (1998a) relevant Fast Downward.SAS+ planning formalism basically equivalent multi-valued planning tasks introduceSection 4 apart fact include derived variables (axioms) conditionaleffects. Backstrom Nebel analyse complexity various subclasses SAS + formalism discover three properties (unariness, post-uniqueness single-valuedness) togetherallow optimal planning polynomial time. One three properties (unariness) relatedacyclicity causal graphs, one (post-uniqueness) implies particularly simple shape domaintransition graphs (namely, post-unique tasks, domain transition graphs must simple cyclestrees).Backstrom Nebel analyse domain transition graphs formally. Indeed, termintroduced later article Jonsson Backstrom (1998a), refines earlier resultsintroducing five additional restrictions SAS + tasks, related propertiesdomain transition graphs.Neither two articles discusses notion causal graphs. Indeed, earlier workaware includes causal graphs domain transition graphs central conceptsarticle Domshlak Dinitz (2001) state-transition support (STS) problem,essentially equivalent SAS+ planning unary operators. context STS, domaintransition graphs called strategy graphs causal graphs called dependence graphs,apart minor details, semantics two formalisms identical. Domshlak Dinitzprovide map complexity STS problem terms shape causal graph,showing problem NP-complete worse almost non-trivial cases. One interestingresult causal graph simple chain n nodes variables three-valued,length minimal plans already grow (2 n ). contrast, propositional taskscausal graph shape admit polynomial planning algorithms according result BrafmanDomshlak (2003), causal graphs polytrees constant indegree bound(namely, bound 1).summarize conclude discussion related work, observe central concepts Fast Downward causal graph heuristic, causal graphs domain transitiongraphs, firmly rooted previous work. However, Fast Downward first attempt marryhierarchical problem decomposition use multi-valued state variables within general planning framework. also first attempt apply techniques similar Knoblock (1994)Bacchus Yang (1994) within heuristic search planner.significance latter point underestimated: classical approacheshierarchical problem decomposition, imperative abstraction satisfies ordered monotonicity property, important probability able refine abstract planconcrete plan high, analysis Bacchus Yang shows. Unfortunately, non-trivialabstraction hierarchies rarely ordered monotonic, even rarely guarantee high refinement probabilities. Within heuristic approach, must-haves turn nice-to-haves:abstraction hierarchy ordered monotonic abstract plan considered heuristicevaluator refinable, merely reduces quality heuristic estimate, rather caus201fiH ELMERTTranslationNormalizationInvariant synthesisGroundingTranslation MPTKnowledgeCompilationDomain transitiongraphsCausal graphSuccessor generatorAxiom evaluatorSearchCausal graph heuristicFF heuristicGreedy best-first searchMulti-heuristic best-first searchFocused iterative-broadening searchFigure 9: three phases Fast Downwards execution.ing search fail (in worst case) spend long time trying salvage non-refinable abstractplans (in much better case).3. Fast Downwarddescribe overall architecture planner. Fast Downward classical planningsystem based ideas heuristic forward search hierarchical problem decomposition.deal full range propositional PDDL2.2 (Fox & Long, 2003; Edelkamp & Hoffmann,2004), i. e., addition STRIPS planning, supports arbitrary formulae operator preconditionsgoal conditions, deal conditional universally quantified effects derivedpredicates (axioms).name planner derives two sources: course, one sources Hoffmanns successful FF (Fast Forward) planner (Hoffmann & Nebel, 2001). Like FF, FastDownward heuristic progression planner, i. e., computes plans heuristic search spaceworld states reachable initial situation. However, compared FF, Fast Downward usesdifferent heuristic evaluation function called causal graph heuristic. heuristic evaluator proceeds downward far tries solve planning tasks hierarchical fashionoutlined introduction. Starting top-level goals, algorithm recursescausal graph remaining subproblems basic graph search tasks.Similar FF, planner shown excellent performance: original implementationcausal graph heuristic, plugged standard best-first search algorithm, outperformed previous champions area, FF LPG (Gerevini, Saetti, & Serina, 2003), set STRIPSbenchmarks first three international planning competitions (Helmert, 2004). Fast Downward followed footsteps FF LPG winning propositional, non-optimizingtrack 4th International Planning Competition ICAPS 2004 (referred IPC4on).mentioned introduction, Fast Downward solves planning task three phases (Fig. 9):translation component responsible transforming PDDL2.2 input nonbinary form amenable hierarchical planning approaches. applies number normalizations compile away syntactic constructs like disjunctionsdirectly supported causal graph heuristic performs grounding axioms operators. importantly, uses invariant synthesis methods find groups related propo202fiT FAST OWNWARD P LANNING YSTEMsitions encoded single multi-valued variable. output translationcomponent multi-valued planning task, defined following section.knowledge compilation component generates four kinds data structures playcentral role search: Domain transition graphs encode how, conditions,state variables change values. causal graph represents hierarchical dependencies different state variables. successor generator efficient datastructure determining set applicable operators given state. Finally, axiomevaluator efficient data structure computing values derived variables.knowledge compilation component described Section 5.search component implements three different search algorithms actual planning.Two algorithms make use heuristic evaluation functions: One well-knowngreedy best-first search algorithm, using causal graph heuristic. called multiheuristic best-first search, variant greedy best-first search tries combine severalheuristic evaluators orthogonal way; case Fast Downward, uses causalgraph FF heuristics. third search algorithm called focused iterative-broadeningsearch; closely related Ginsberg Harveys (1992) iterative broadening.heuristic search algorithm sense use explicit heuristic evaluationfunction. Instead, uses information encoded causal graph estimate usefulness operators towards satisfying goals task. search component describedSection 6.4. Multi-Valued Planning TasksLet us formally introduce problem planning multi-valued state variables.formalism based SAS+ planning model (Backstrom & Nebel, 1995; Jonsson & Backstrom,1998a), extends axioms conditional effects.Definition 1 Multi-valued planning tasks (MPTs)multi-valued planning task (MPT) given 5-tuple = hV, 0 , s? , A, Oi followingcomponents:V finite set state variables, associated finite domain v . State variables partitioned fluents (affected operators) derived variables (computedevaluating axioms). domains derived variables must contain undefined value .partial variable assignment partial state V function subset Vs(v) Dv wherever s(v) defined. partial state called extended statedefined variables V reduced state state defined fluents V.context partial variable assignments, write v = variable-value pairing(v, d) v 7 d.s0 state V called initial state.s? partial variable assignment V called goal.finite set (MPT) axioms V. Axioms triples form hcond, v, di,cond partial variable assignment called condition body axiom, v derived203fiH ELMERTvariable called affected variable, v called derived value v. pair(v, d) called head axiom written v := d.axiom set partitioned totally ordered set axiom layers 1 Akwithin layer, affected variable may associated singlevalue axiom heads bodies. words, within layer, axiomsaffected variable different derived values forbidden, variable appearsaxiom head, may appear different value body. calledlayering property.finite set (MPT) operators V. operator hpre, effi consists partialvariable assignment pre V called precondition, finite set effects eff. Effectstriples hcond, v, di, cond (possibly empty) partial variable assignment calledeffect condition, v fluent called affected variable, v called newvalue v.axioms effects, also use notation cond v := place hcond, v, di.provide formal semantics MPT planning, first need formalize axioms:Definition 2 Extended states defined stateLet state MPT axioms A, layered 1 Ak . extended state defineds, written A(s), result 0 following algorithm:algorithm evaluate-axioms(A1 , . . . , Ak , s):variable( v:s(v) v fluent variables0 (v) :=v derived variable{1, . . . , k}:exists axiom (cond v := d) cond s0 s0 (v) 6= d:Choose axiom cond v := d.s0 (v) :=words, axioms evaluated layer-by-layer fashion using fixed point computations,similar semantics stratified logic programs. easy see layeringproperty Definition 1 guarantees algorithm terminates produces deterministicresult. defined semantics axioms, define state space MPT:Definition 3 MPT state spacesstate space MPT = hV, s0 , s? , A, Oi, denoted S(), directed graph. vertexset set states V, contains arc (s, 0 ) iff exists operator hpre, effithat:pre A(s),s0 (v) = effects cond v := eff cond A(s),s0 (v) = s(v) fluents.204fiT FAST OWNWARD P LANNING YSTEMFinally, define MPT planning problem:Definition 4 MPT planningMPT-P LAN E X following decision problem: Given MPT initial state 0 goals? , S() contain path s0 state s0 s? A(s0 )?MPT-P LANNING following search problem: Given MPT initial state 0 goals? , compute path S() s0 state s0 s? A(s0 ), prove none exists.MPT-P LAN E X problem easily shown PSPACE-hard generalizes planexistence problem propositional STRIPS, known PSPACE-complete (Bylander,1994). also easy see addition multi-valued domains, axioms conditional effectsincrease theoretical complexity MPT planning beyond propositional STRIPS. Thus,conclude formal introduction MPT planning stating MPT-P LAN E X PSPACEcomplete, turn practical side things following section.5. Knowledge Compilationpurpose knowledge compilation component set stage search algorithmscompiling critical information planning task number data structures efficient access. contexts, computations kind often called preprocessing. However,preprocessing nondescript word mean basically anything. reason,prefer term puts stronger emphasis role module: rephrase critical information planning task way directly useful search algorithms.three building blocks Fast Downward (translation, knowledge compilation, search),least time-critical part, always requiring less time translation dominated searchtrivial tasks.Knowledge compilation comprises three items. First foremost, compute domaintransition graph state variable. domain transition graph state variable encodescircumstances variable change value, i. e., values domaintransitions values, operators axioms responsible transition, conditions state variables associated transition. Domaintransition graphs described Section 5.1. central concept computationcausal graph heuristic, described Section 6.1.Second, compute causal graph planning task. domain transition graphs encode dependencies values given state variable, causal graph encodes dependenciesdifferent state variables. example, given location planning task unlockedmeans key carried agent, variable representing lock statelocation dependent variable represents whether key carried.dependency encoded arc causal graph. Like domain transition graphs, causalgraphs central concept computation causal graph heuristic, giving name.causal graph heuristic requires causal graphs acyclic. reason, knowledge compilation component also generates acyclic subgraph real causal graph cycles occur.amounts relaxation planning task operator preconditions ignored.addition usefulness causal graph heuristic, causal graphs also key conceptfocused iterative-broadening search algorithm introduced Section 6.5. discuss causalgraphs Section 5.2.205fiH ELMERTThird, compute two data structures useful forward-searching algorithmMPTs, called successor generators axiom evaluators. Successor generators compute setapplicable operators given world state, axiom evaluators compute values derivedvariables given reduced state. designed job quickly possible,especially important focused iterative-broadening search algorithm, compute heuristic estimates thus requires basic operations expanding search nodeimplemented efficiently. data structures discussed Section 5.3.5.1 Domain Transition Graphsdomain transition graph state variable representation ways variablechange value, conditions must satisfied value changes allowed. Domain transition graphs introduced Jonsson Backstrom (1998a) contextSAS+ planning. formalization domain transition graphs generalizes original definitionplanning tasks involving axioms conditional effects.Definition 5 Domain transition graphsLet = hV, s0 , s? , A, Oi multi-valued planning task, let v V state variable .domain transition graph v, symbols DTG(v), labelled directed graph vertexset Dv . v fluent, DTG(v) contains following arcs:effect cond v := d0 operator precondition pre pre condcontains condition v = d, arc 0 labelled pre cond \ {v = d}.effect cond v := d0 operator precondition pre pre condcontain condition v = v , arc Dv \ {d0 } d0labelled pre cond.v derived variable, DTG(v) contains following arcs:axiom cond v := d0 cond contains condition v = d, arcd0 labelled cond \ {v = d}.axiom cond v := d0 cond contain condition v =Dv , arc Dv \ {d0 } d0 labelled cond.Arcs domain transition graphs called transitions. labels referredconditions transition.Domain transition graphs weighted, case transition associatednon-negative integer weight. Unless stated otherwise, assume transitions derivedoperators weight 1 transitions derived axioms weight 0.definition somewhat lengthy, informal content easy grasp: domain transition graph v contains transition 0 exists operator axiomchange value v d0 . transition labelled conditions statevariables must true transition shall applied. Multiple transitionsvalues using different conditions allowed occur frequently.already seen domain transition graphs introductory section (Figs. 3 4), although introduced informally show arc labels usually associated206fiT FAST OWNWARD P LANNING YSTEM= open(1, 1)(2, 1)(3, 1)(2, 1)(2, 2)(3, 2)r=r = (1, 1), k = carriedclosedr = (2, 2), k = carriedr=open(1, 2)(1,1)1)2)(1,r=2)(1,r = (2, 1)(1,r = (2, 1)r=r=r=(3, 1)1)(3,r=r=carriedr = (2, 2)(1, 2)(1, 1)r = (2, 2)= open= open(3,(3,(3,2)1)2)(3, 2)r = (3, 1), k = carried(2, 2)Figure 10: Domain transition graphs G RID task. Top left: DTG(r) (robot); right: DTG(k)(key); bottom left: DTG(d) (door).transitions. Fig. 10 shows examples simple task G RID domain, featuring 3 2 grid single initially locked location centre upper row, unlockablesingle key. MPT encoding task, three state variables: variable rDr = { (x, y) | x {1, 2, 3}, {1, 2} } encodes location robot, variable kDk = Dr {carried} encodes state key, variable = {closed, open}encodes state initially locked grid location.operators MPT unary (i. e., single effect) leave aside axiomsmoment, strong correspondence state space MPTdomain transition graphs. Since vertices domain transition graphs correspond values statevariables, given state represented selecting one vertex domain transition graph, calledactive vertex state variable. Applying operator means changing active vertexstate variable performing transition corresponding domain transition graph.Whether transition allowed depends condition, checkedactive vertices domain transition graphs.Let us use G RID example illustrate correspondence. Consider initial staterobot location (1, 1), key location (3, 2), door locked. representplacing pebbles appropriate vertices three domain transition graphs. wantmove pebble domain transition graph key location (2, 1). donemoving robot pebble vertex (1, 2), (2, 2), (3, 2), moving key pebble vertexcarried, moving robot pebble back vertex (2, 2), moving door pebble open, movingrobot pebble vertex (2, 1) finally moving key pebble vertex (2, 1).207fiH ELMERT= open, r = (1, 1)= open, r = (3, 1)= open, r = (1, 1)= closed>r = (2, 1)>r = (1, 2)= open, r = (3, 1)r = (2, 2)r = (3, 2)Figure 11: Domain transition graphs freezing variable G RID task, normal (left)extended (right). Note extended graph shows change statefreezing (>) freezing ().example shows plan execution viewed simultaneous traversal domaintransition graphs (cf. Domshlak & Dinitz, 2001). important notion Fast Downwardcausal graph heuristic computes heuristic estimates solving subproblemsplanning task looking paths domain transition graphs basically way described.mentioned before, view MPT planning completely accurate unary taskswithout axioms, domain transition graphs indeed complete representationstate space. non-unary operators, would need link certain transitions different domaintransition graphs belong operator. could executed together.axioms, would need mark certain transitions mandatory, requiring takenwhenever possible. (This intended rough analogy leaves details like layeredaxioms.)previous work (Helmert, 2004), successfully applied view planningSTRIPS tasks. Extending notion plans conditional effects provides challenges domain transition graphs always consider planning operators one effect time,case effect condition simply seen part operator precondition. However, axiomsprovide challenge easily overlooked. want change value fluentd0 , domain transition graph contains important information; find path 0try find associated conditions achieved. Consider problemderived state variable. Let us assume unlocking location G RID example leadsdrought, causing robot freeze enters horizontally adjacent location. could encodenew derived variable f (for freezing) domain f = {>, }, defined axioms= open, r = (1, 1) f := > = open, r = (3, 1) f := >. domain transition graphDTG(f ) depicted Fig. 11 (left).problem domain transition graph tell us changestate variable f > . general, MPTs derived STRIPS tasks derivedpredicates occur negatively condition, domain transition graph contain sufficientinformation changing value derived variable true false. Derived variables208fiT FAST OWNWARD P LANNING YSTEMnever assume value due derivation value; negation failure semantics,assume value default value derived. want reasonways setting value derived variable , need make information explicit.logical notation, whether derived variable assumes given value triggeringaxiom given layer determined formula disjunctive normal form, one disjunctaxiom setting value. example, axioms = open, r = (1, 1) f := >= open, r = (3, 1) f := > correspond DNF formula (d = open r = (1, 1)) (d =open r = (3, 1)). want know rules trigger, must negate formula,leading CNF formula (d 6= open r 6= (1, 1))(d 6= open r 6= (3, 1)). able encodeinformation domain transition graph, need replace inequalities equalitiestranslate formula back DNF. Since transformations increase formula sizedramatically, apply simplifications along way, removing duplicated dominated disjuncts.result case DNF formula = closed r = (2, 1) r = (1, 2) r = (2, 2) r =(3, 2).domain transition graph derived variable enriched contain possibleways causing variable assume value called extended domain transition graph,shown G RID example Fig. 11 (right). Since computing extended domain transitiongraph costly always necessary, knowledge compilation component scansconditions planning task (axioms, operator preconditions effect conditions, goal)occurrences pairings type v = derived variables v. Extended domain transitiongraphs computed derived variables required.Note negative occurrences derived variables cascade: u, v w derivedvariables domain {>, } condition v = present operator precondition,moreover v defined axiom u = >, w = > v := >, v assumes valuewhenever u w do, would require extended domain transition graphs u w well.hand, multiple layers negation failure cancel out: derivedvariable v occurs conditions form v = never positive form definedaxiom u = , w = v := >, necessarily require extended domain transitiongraphs u w.general, whether need extended domain transition graphs derived variabledetermined following rules:v derived variable condition v = 6= appears operatorprecondition, effect condition goal, v used positively.v derived variable condition v = appears operator precondition,effect condition goal, v used negatively.v derived variable condition v = 6= appears bodyaxiom whose head used positively (negatively), v used positively (negatively).v derived variable condition v = appears body axiomwhose head used positively (negatively), v used negatively (positively).knowledge compilation component computes extended domain transition graphs derived variables used negatively (standard) domain transition graphs statevariables. Normal domain transition graphs computed going set axioms209fiH ELMERTset operator effects following Definition 5, reasonably straight-forward; computation extended domain transition graphs outlined above. Therefore, algorithmicaspects topic require discussion.5.2 Causal GraphsCausal graphs introduced informally introduction. formal definition.Definition 6 Causal graphsLet multi-valued planning task variable set V. causal graph , symbolsCG(), directed graph vertex set V containing arc (v, v 0 ) iff v 6= v 0 onefollowing conditions true:domain transition graph v 0 transition condition v.set affected variables effect list operator includes v v 0 .first case, say arc induced transition condition. second case sayinduced co-occurring effects.course, arcs induced transition conditions arcs induced co-occurring effectsmutually exclusive. causal graph arc generated reasons.Informally, causal graph contains arc source variable target variable changesvalue target variable depend value source variable. arcsincluded also dependency form effect source variable. agreesdefinition dependency graphs Jonsson Backstrom (1998b), although authorsdistinguish two different ways arc graph introduced usinglabelled arcs.Whether co-occurring effects induce arcs causal graph depends intended semantics: arcs included, set causal graph ancestors anc(v) variablev precisely variables relevant goal change value v. Plansgoal computed without considering variables outside anc(v), eliminating variables outside anc(v) planning task simplifying axioms operators accordingly.call achievability definition causal graphs, causal graphs encode variablesimportant achieving given assignment state variable.However, achievability definition, planner considers anc(v) generatingaction sequence achieves given valuation v may modify variables outside anc(v), i. e.,generated plans side effects could destroy previously achieved goals otherwisenegative impact overall planning. Therefore, prefer definition, callseparability definition causal graphs.5.2.1 ACYCLIC C AUSAL G RAPHSFollowing separability definition causal graphs, solving subproblem variables anc(v)always possible without changing values outside anc(v). leads us followingobservation.210fiT FAST OWNWARD P LANNING YSTEMObservation 7 Acyclic causal graphs strongly connected domain transition graphsLet MPT CG() acyclic, domain transition graphs strongly connected,derived variables, trivially false conditions occur operators goals.solution.trivially false conditions, mean conditions kind {v = d, v = 0 } 6= d0 .Note similarity Observation 7 results Williams Nayak (1997) planning domains unary operators, acyclic causal graphs reversible transitions. separabilitydefinition causal graphs, acyclic causal graphs imply unariness operators operatorsseveral effects introduce causal cycles. Moreover, strong connectedness domain transitiongraphs closely related Williams Nayaks reversibility property, although weakerrequirement.truth observation easily seen inductively: planning task one statevariable domain transition graph strongly connected, state (of one variable)transformed state applying graph search techniques. planning taskseveral state variables causal graph acyclic, pick sink causal graph, i. e.,variable v without outgoing arcs, check goal defined variable. not,remove variable task, thus reducing problem one fewer state variables,solved recursively. yes, search path 0 (v) s? (v) domain transition graphv, guaranteed exist graph strongly connected. yields high-levelplan setting v s? (v) fleshed recursively inserting plans settingvariables predecessors v causal graph values required transitionsform high-level plan. desired value v set, v eliminatedplanning task remaining problem solved recursively.algorithm shown Fig. 12. Although backtrack-free, require exponentialtime execute generated plans exponentially long. unavoidable; evenMPTs satisfy conditions Observation 7, shortest plans exponentially long.family planning tasks property given proof Theorem 4.4 articleBackstrom Nebel (1995).method solving multi-valued planning tasks essentially planning refinement:begin constructing abstract skeleton plan, merely path domain transitiongraph, lower level abstraction adding operators satisfy preconditions requiredtransitions taken path. Strong connectedness domain transition graphs guaranteesevery abstract plan actually refined concrete plan. precisely BacchusYangs (1994) downward refinement property (cf. Section 2.1).5.2.2 G ENERATINGP RUNING C AUSAL G RAPHSusefulness causal graphs planning refinement limited acyclic case. Consider subset V 0 task variables contains causal graph descendants. general,restrict task V 0 removing occurrences variables initial state, goal,operators axioms, obtain abstraction original problem satisfies Knoblocks(1994) ordered monotonicity property (Section 2.1).Unfortunately, one major problem approach requirement include causalgraph descendants quite limiting. uncommon causal graph planning taskstrongly connected, case technique allow us abstract away variables211fiH ELMERTalgorithm solve-easy-MPT(V, s0 , s? , O):s? = :{ goal empty: empty plan solution. }return hi.else:Let v V variable occurring preconditions effect conditions O.{ variable always exists causal graph task acyclic. }V 0 := V \ {v}.0 := { | affect v }.plan := his? (v) defined:Let t1 , . . . , tk path transitions DTG(v) 0 (v) s? (v).{ t1 , . . . , tk high-level plan reaches goal v,ignores preconditions variables. }{t1 , . . . , tk }:{ Recursively find plan achieves conditions t. }Let cond condition operator associated t.Let s00 state reached executing plan, restricted V 0 .Extend plan solve-easy-MPT(V 0 , s00 , cond, 0 ).Extend plan o.{ dealing v, recursively plan goals remaining variables. }Let s00 state reached executing plan, restricted V 0 .s0? := s? restricted V 0 .Extend plan solve-easy-MPT(V 0 , s00 , s0? , 0 ).return planFigure 12: Planning algorithm MPTs acyclic causal graph strongly connected domaintransition graphs.212fiT FAST OWNWARD P LANNING YSTEMall. However, heuristic approach, free simplify planning task. particular,ignoring operator preconditions purposes heuristic evaluation, makearbitrary causal graph acyclic. Clearly, aspects real task ignore, worseexpect heuristic approximate actual goal distance. Considering this, aim ignorelittle information possible. explain done.knowledge compilation component begins causal graph processing generatingfull causal graph (Definition 6). One consequence separability definition causal graphsstate variables ancestors variables mentioned goal completelyirrelevant. Therefore, computed graph, compute causal graph ancestorsvariables goal. state variables found goal ancestors eliminated planning task causal graph, associated operators axioms removed. 4Afterwards, compute pruned causal graph, acyclic subgraph causal graphvertex set. try fashion important causal dependencies retainedwhenever possible. specifically, apply following algorithm.First, compute strongly connected components causal graph. Cycles occurwithin strongly connected components, component dealt separately. Second,connected component, compute total order vertices, retainingarcs (v, v 0 ) v v 0 . v v 0 , say v 0 higher level v. total ordercomputed following way:1. assign weight arc causal graph. weight arc n inducedn axioms operators. lower cumulated weight incoming arcs vertex,fewer conditions ignored assigning low level vertex.2. pick vertex v minimal cumulated weight incoming arcs selectlowest level, i. e., set v v 0 vertices v 0 strongly connected component.3. Since v dealt with, remove vertex incident arcs considerationrest ordering algorithm.4. remaining problem solved iteratively applying technique ordervertices single vertex remains.reader notice pruning choices within strongly connected componentperformed greedy algorithm. could also try find sets arcs minimal total weighteliminating arcs results acyclic graph. However, NP-equivalent problem,even case unweighted graphs (Garey & Johnson, 1979, problem GT8).generating pruned causal graph, also prune domain transition graphs removing transition labels DTG(v) conditions variables v 0 v v 0 .conditions ignored heuristic computation. Finally, simplify domain transitiongraphs removing dominated transitions: 0 transitions two valuesvariable, condition proper subset condition 0 , transitioneasier apply t0 , remove t0 . Similarly, several transitions identicalconditions, keep one them.4. simplification closely related Knoblocks criterion problem-specific ordered monotonicity property(Knoblock, 1994).213fiH ELMERTt1t2a1p1p2a2Figure 13: Causal graph L OGISTICS task. State variables ai encode locationstrucks airplanes, state variables p locations packages.f1p1f2f3f1f2f3l1l2l1l2c1c2c1c2p2p1p2Figure 14: Causal graph YSTERY task (left) relaxed version task (right). Statevariables fi encode fuel location, state variables l ci encode locationsremaining capacities trucks, state variables p encode locations packages.5.2.3 C AUSAL G RAPH E XAMPLESgive impression types causal graphs typically found standard benchmarkseffects pruning, show examples increasing graph complexity.first simplest example, Fig. 13 shows causal graph task L OGISTICSdomain, featuring two trucks, two airplanes two packages. seen, graph acyclic,requires pruning causal graph heuristic. Since L OGISTICS tasks also feature stronglyconnected domain transition graphs, even solved polynomial solve-easy-MPTalgorithm.slightly complicated example, next figure, Fig. 14, shows task YS TERY domain three locations, two trucks two packages. causal graph containsnumber cycles, mostly local. pruning arcs vertices l fj , ignore214fiT FAST OWNWARD P LANNING YSTEMrrlk1k2lk1k2Figure 15: Causal graph G RID task (left) relaxed version task (right). Statevariable r encodes location robot, encodes status robot arm (emptycarrying key), l encodes status locked location (locked open), k 1k2 encode locations two keys.fact must move trucks certain locations want use fuel location.using fuel useful thing do, big loss information. pruning arcsvertices pi cj , ignore fact vehicles increase decrease currentcapacity unloading loading packages. Compared heuristics based ignoring delete effects, great loss information, since ignoring delete effects YSTERY domainalmost amounts ignoring capacity fuel constraints altogether. pruning arcs,eliminate cycles causal graph, YSTERY domain considered fairlywell-behaved.worse case shown Fig. 15, shows example G RID domainarbitrary number locations, single one locked. two keys, oneunlock locked location. Eliminating cycles requires minor relaxations regardingstatus robot arm (empty non-empty), also one major simplification, namelyelimination arc l r representing fact robot enter lockedlocation unlocked.(nearly) worst-case example, consider task B LOCKSWORLD domain (no figure).typical MPT encoding uses one state variable h encoding whether hand emptytwo state variables per block task: i-th block, encodes whether blocklying table, bi encodes block lying top it, clear heldarm. causal graph task, variable h ingoing arcs outgoing arcsstate variables, state variables b connected directions.state variables ti slightly simpler connection structure, connected hbi value i. relaxation problem eliminates cycles causalgraph loses large amount information, surprising EPOT domain,includes B LOCKSWORLD subproblem, one precursor Fast Downward faredworst (Helmert, 2004). Still, pointed planners ignore delete effectssimilar problems B LOCKSWORLD-like domains, comparison FF causalgraph heuristics article shows.215fiH ELMERT5.3 Successor Generators Axiom Evaluatorsaddition good heuristic guidance, forward searching planning system needs efficient methodsgenerating successor states applied benchmark suite internationalplanning competitions. domains, causal graph heuristic popular methodslike FF heuristic provide excellent goal estimates, yet still planning time-consuminglong plans vast branching factors.variant best-first search implemented Fast Downward compute heuristicestimate state generated. Essentially, heuristic evaluations computedclosed nodes, computation deferred nodes search frontier. domainsstrong heuristic guidance large branching factors, number nodes frontierfar dominate number nodes closed set. case point, consider problem instanceATELLITE #29. solving task, default configuration Fast Downward computesheuristic estimates 67 597 world states adding 107 233 381 states frontier. Clearly,determining set applicable operators quickly critical importance scenario.ATELLITE tasks, almost 1 000 000 ground operators, tryavoid individually checking operator applicability. Similarly, biggest PSR tasks,100 000 axioms must evaluated state compute values derivedvariables, computation must made efficient. purposes, Fast Downward uses twodata structures called successor generators axiom evaluators.5.3.1 UCCESSOR G ENERATORSSuccessor generators recursive data structures similar decision trees. internal nodesassociated conditions, likened decisions decision tree, leavesassociated operator lists likened set classified samples decision treeleaf. formally defined follows.Definition 8 Successor generatorssuccessor generator MPT = hV, 0 , s? , A, Oi tree consisting selector nodesgenerator nodes.selector node internal node tree. associated variable v V calledselection variable. Moreover, |D v |+1 children accessed via labelled edges, one edge labelledv = value Dv , one edge labelled >. latter edge called dont careedge selector.generator node leaf node tree. associated set operators calledset generated operators.operator must occur exactly one generator node, set edge labelsleading root node (excluding dont care edges) must equal precondition o.Given successor generator MPT state , compute setapplicable operators traversing successor generator follows, starting root:selector node selection variable v, follow edge v = s(v) dont care edge.generator node, report generated operators applicable.216fiT FAST OWNWARD P LANNING YSTEMalgorithm evaluate-axiom-layer(s, ):axiom Ai :a.counter := |a.cond|variable v:axiom Ai condition v = s(v) body:a.counter := a.counter 1exists axiom Ai a.counter = 0 yet considered:Let hv, di head axiom.s(v) 6= d:s(v) :=axiom Ai condition v = body:a.counter := a.counter 1Figure 16: Computing values derived variables given planning state.build successor generator , apply top-down algorithm considers taskvariables arbitrary order v1 v2 vn . root node, choose v1 selection variable classify set operators according preconditions respect v 1 . Operatorsprecondition v1 = represented child root accessed edgecorresponding label, operators without preconditions v 1 represented childroot accessed dont care edge. children root, choose v 2 selectionvariable, grandchildren v3 , on.one exception rule avoid creating unnecessary selection nodes: operatorcertain branch tree condition v , vi considered selection variablebranch. construction branch ends variables considered,stage generator node created operators associated branch.5.3.2 XIOM E VALUATORSAxiom evaluators simple data structure used efficiently implementing well-knownmarking algorithm propositional Horn logic (Dowling & Gallier, 1984), extended modifiedlayered logic programs correspond axioms MPT. consist two parts.Firstly, indexing data structure maps given variable/value pairing given axiom layerset axioms given layer whose body pairing appears. Secondly, set counters,one axiom, counts number conditions axiom yet derived.Within Fast Downward, axioms evaluated two steps. First, derived variables setdefault value . Second, algorithm evaluate-axiom-layer (Fig. 16) executed axiomlayer sequence determine final values derived variables.assume reader familiar enough marking algorithm require muchexplanation, point test whether axiom ready trigger implemented means queue axioms put soon counter reaches 0. actualimplementation evaluate-axiom-layer within Fast Downward initializes axiom counters slightlyefficiently indicated pseudo-code. However, minor technical detail,turn remaining piece Fast Downwards architecture, search component.217fiH ELMERT6. SearchUnlike translation knowledge compilation components, singlemode execution, search component Fast Downward perform work various alternative ways. three basic search algorithms choose from:1. Greedy best-first search: standard textbook algorithm (Russell & Norvig, 2003),modified technique called deferred heuristic evaluation mitigate negative influence wide branching. also extended algorithm deal preferred operators, similar FFs helpful actions (Hoffmann & Nebel, 2001). discuss greedy best-firstsearch Section 6.3. Fast Downward uses algorithm together causal graphheuristic, discussed Section 6.1.2. Multi-heuristic best-first search: variation greedy best-first search evaluatessearch states using multiple heuristic estimators, maintaining separate open lists each.Like variant greedy best-first search, supports use preferred operators. Multiheuristic best-first search discussed Section 6.4. Fast Downward uses algorithmtogether causal graph FF heuristics, discussed Sections 6.1 6.2.3. Focused iterative-broadening search: simple search algorithm useheuristic estimators, instead reduces vast set search possibilities focusinglimited operator set derived causal graph. experimental algorithm;future, hope develop basic idea algorithm robust method.Focused iterative-broadening search discussed Section 6.5.two heuristic search algorithms, second choice must made regarding usepreferred operators. five options supported planner:1. use preferred operators.2. Use helpful transitions causal graph heuristic preferred operators.3. Use helpful actions FF heuristic preferred operators.4. Use helpful transitions preferred operators, falling back helpful actionshelpful transitions current search state.5. Use helpful transitions helpful actions preferred operators.five options combined two heuristic search algorithms,total eleven possible settings search component, ten using oneheuristic algorithms one using focused iterative-broadening search.addition basic settings, search component configured execute severalalternative configurations parallel making use internal scheduler. configurationsFast Downward participated IPC4 made use feature running one configurationheuristic search algorithms parallel focused iterative-broadening search. heuristicsearch algorithm, configuration Fast Downward employed greedy best-first search helpfultransitions, falling back helpful actions necessary (option 4.). configuration Fast Diagonally Downward employed multi-heuristic best-first search using helpful transitions helpfulactions preferred operators (option 5.).218fiT FAST OWNWARD P LANNING YSTEMavoid confusion complete Fast Downward planning system particularconfiguration called Fast Downward, refer IPC4 planner configurations FDFDD rest paper. name planning system whole never abbreviated.6.1 Causal Graph Heuristiccausal graph heuristic centrepiece Fast Downwards heuristic search engine. estimates cost reaching goal given search state solving number subproblemsplanning task derived looking small windows (pruned) causal graph.additional intuitions design heuristic discussion theoretical aspects,refer article heuristic first introduced (Helmert, 2004).6.1.1 C ONCEPTUAL V IEWC AUSAL G RAPH H EURISTICstate variable v pair values d, 0 Dv , causal graph heuristic computesheuristic estimate cost v (d, d0 ) cost changing value v 0 , assumingstate variables carry values current state. (This simplification. Costestimates computed state variables v values never required.ignore fact discussing heuristic conceptual level.) heuristic estimategiven state sum costs cost v (s(v), s? (v)) variables v goalcondition s? (v) defined.Conceptually, cost estimates computed one variable other, traversing (pruned)causal graph bottom-up fashion. bottom-up, mean start variablespredecessors causal graphs; call order computation bottom-upconsider variables change state accord low-level, variableswhose state transitions require help variables complex transition semanticsthus considered high-level. Note figures depicting causal graphs, high-levelvariables typically displayed near bottom.variables without predecessors causal graph, cost v (d, d0 ) simply equals costshortest path d0 (pruned) domain transition graph DTG(v). variables,cost estimates also computed graph search domain transition graph. However,conditions transitions must taken account path planning, additioncounting number transitions required reach destination value, also consider costsachieving value changes variables necessary set transition conditions.important point computing values cost v (d, d0 ), completely considerinteractions state variable v predecessors causal graph. changingvalue d0 requires several steps steps associated conditionvariable v 0 , realize v 0 must assume values required conditions sequence.example, v represents package transportation task must moved Bmeans vehicle located C, recognize vehicle must first move CB order drop package B. different way HSPor FF-based heuristics work examples. However, consider interactionsimmediate predecessors v causal graph. Interactions occur via several graph layerscaptured heuristic estimator.essence, compute cost v (d, d0 ) solving particular subproblem MPT, inducedvariable v predecessors pruned causal graph. subproblem, assume219fiH ELMERTalgorithm compute-costs-bottom-up(, s):variable v , traversing pruned causal graph bottom-up order:Let V 0 set immediate predecessors v pruned causal graph.pair values (d, d0 ) Dv Dv :Generate planning task v,d,d0 following components:Variables: V 0 {v}.Initial state: v = v 0 = s(v 0 ) v 0 V 0 .Goal: v = d0 .Axioms operators:1. corresponding transitions pruned DTG v.2. variables v 0 V 0 values e, e0 Dv0 , operatorprecondition v 0 = e, effect v 0 = e0 cost cost0v (e, e0 ).{ Note variables v 0 V 0 considered previously,cost values known. }Set costv (d, d0 ) cost plan solves v,d,d0 .Figure 17: compute-costs-bottom-up algorithm, high-level description causal graphheuristic.v initially set d, want v assume value 0 , state variables carryvalue current state. call planning problem local subproblem v, 0 ,local subproblem v leave target value 0 open.formalization intuitive notions cost estimates generated, considerpseudo-code Fig. 17. reflect way heuristic values actually computedwithin Fast Downward; algorithm figure would far expensive evaluatesearch state. However, computes cost values Fast Downward does, providedalgorithm generating plans last line algorithm one one usedreal cost estimator.6.1.2 C OMPUTATIONC AUSAL G RAPH H EURISTICactual computation causal graph heuristic traverses causal graph top-down direction starting goal variables, rather bottom-up starting variables without causalpredecessors. fact, top-down traversal causal graph reason Fast Downwardsname.Computing cost estimates top-down traversal implies algorithm computingplans local subproblems given variable, typically yet know costs changingstate causal predecessors. algorithm compute-costs addresses evaluatingcost values dependent variables recursive invocations itself.given variable-value pairing v = d, always compute costs cost v (d, d0 ) valuesd0 Dv time, similar way Dijkstras algorithm computes shortest pathsingle source single destination vertex, single source possible destinationvertices. Computing costs values 0 (much) expensive computing220fiT FAST OWNWARD P LANNING YSTEMone values, cost values determined, cache re-useneeded later parts computation heuristic valuecurrent state.fact, similarity shortest path problems superficial runs quite deeply.ignore recursive calls computing cost values dependent variables, compute-costs basically implementation Dijkstras algorithm single-source shortest path problemdomain transition graphs. difference regular algorithm lies factknow cost using arc advance. Transitions derived variables base cost0 transitions fluents base cost 1, addition base cost, must paycost achieving conditions associated transition. However, cost achievinggiven condition v 0 = e0 depends current value e state variable time transitiontaken. Thus, compute real cost transition know valuesdependent state variables relevant situation.course, many different ways taking transitions domain transition graphs,potentially leading different values dependent state variables. first introducedcausal graph heuristic, showed deciding plan existence local subproblems NPcomplete (Helmert, 2004), content approach lead completeplanning algorithm, long works well subproblems face practice.approach chosen achieve value state variable v local subproblemv quickly possible, following greedy policy. context Dijkstra algorithm,means start finding cheapest possible plan make transitionvalue d0 . found cheapest possible plan d0 , commit it, annotatingvertex d0 domain transition graph local state obtained applying plan d0current state. next step, look cheapest possible plan achieve another value 00 ,either considering transitions start initial value d, considering transitionscontinue plan d0 moving neighbour d0 . process iterated verticesdomain transition graph reached progress possible.implementation follows Dijkstras algorithm (Fig. 18). implemented priority queue vector buckets maximal speed use cache avoid generatingcostv (d, d0 ) value twice state. addition this, use global cache sharedthroughout whole planning process need compute values cost v (d, d0 ) variables v ancestors pruned causal graph once. (Note cost v (d, d0 ) dependscurrent values ancestors v.)Apart technical considerations, Fig. 18 gives accurate accountFast Downwards implementation causal graph heuristic. details, includingcomplexity considerations worked-out example, refer original descriptionalgorithm (Helmert, 2004).6.1.3 TATESNFINITEH EURISTIC VALUEnoted Fast Downward uses incomplete planning algorithm determining solutionslocal planning problems. Therefore, states cost v (s(v), s? (v)) = even thoughgoal condition v = s? (v) still reached. means cannot trust infinite valuesreturned causal graph heuristic. experience, states infinite heuristic evaluationstill possible reach goal rare, indeed treat states dead ends.221fiH ELMERTalgorithm compute-costs(, s, v, d):Let V 0 set immediate predecessors v pruned causal graph .Let DTG pruned domain transition graph v.costv (d, d) := 0costv (d, d0 ) := d0 Dv \ {d}local-state := restricted V 0unreached := Dvunreached contains value d0 Dv cost v (d, d0 ) < :Choose value d0 unreached minimizing cost v (d, d0 ).unreached := unreached \ {d0 }transition DTG leading 0 d00 unreached:transition-cost := 0 v derived variable; 1 v fluentpair v 0 = e0 condition t:e := local-state d0 (v 0 )call compute-costs(, s, v 0 , e).transition-cost := transition-cost + cost v0 (e, e0 )costv (d, d0 ) + transition-cost < cost v (d, d00 ):costv (d, d00 ) := costv (d, d0 ) + transition-costlocal-state d00 := local-state d0pair v 0 = e0 condition t:local-state d00 (v 0 ) := e0Figure 18: Fast Downwards implementation causal graph heuristic: compute-costs algorithm computing estimates cost v (d, d0 ) values d0 Dv stateMPT .222fiT FAST OWNWARD P LANNING YSTEMturns states search frontier dead ends, cannot make progresscausal graph heuristic. case, use sound dead-end detection routine verifyheuristic assessment. turns frontier states indeed dead ends, reportproblem unsolvable. Otherwise, search restarted FF heuristic (cf. Section 6.2),sound purposes dead-end detection. 5dead-end detection routine originally developed STRIPS-like tasks. However,extending full MPTs easy; fact, changes core algorithm required, workslevel domain transition graphs still sound applied tasks conditionaleffects axioms. Since central aspect Fast Downward, discuss here,referring earlier work instead (Helmert, 2004).6.1.4 H ELPFUL RANSITIONSInspired Hoffmanns successful use helpful actions within FF planner (Hoffmann &Nebel, 2001), extended algorithm computing causal graph heuristicaddition heuristic estimate, also generates set applicable operators considered usefulsteering search towards goal.compute helpful actions FF, Hoffmanns algorithm generates plan relaxed planning task defined current search state considers operators helpful belongrelaxed plan applicable current state.approach follows similar idea. computing heuristic estimate cost v (s(v), s? (v))variable v goal condition defined, look domain transition graphv trace path transitions leading s(v) ? (v) gave rise cost estimate.particular, consider first transition path, starting s(v). transition correspondsapplicable operator, consider operator helpful transition continue checknext goal. transition correspond applicable operator associatedconditions form v 0 = e0 currently satisfied, recursively look helpful transitions domain transition graph variable v 0 , checking pathgenerated computation cost v0 (s(v 0 ), e0 ).recursive process continues found helpful transitions. Unlike caseFF, helpful actions found non-goal states, might find helpfultransition all. may case transition correspond applicable operatoreven though associated conditions; happen operator preconditionsrepresented pruned domain transition graph due cycles causal graph. Even so,found helpful transitions useful tool guiding best-first search algorithms.6.2 FF HeuristicFF heuristic named Hoffmanns planning algorithm name, contextoriginally introduced (Hoffmann & Nebel, 2001). based notionrelaxed planning tasks ignore negative interactions. context MPTs, ignoring negativeinteractions means assume state variable hold several values simultaneously.operator effect axiom sets variable v value original task corresponds5. practice, never observed causal graph heuristic fail solvable task. Therefore, fallbackmechanism used unsolvable tasks ICONIC -F ULL ADL domain recognizeddead-end detection technique.223fiH ELMERTeffect axiom adds value range values assumed v relaxed task.condition v = original task corresponds condition requiring elementset values currently assumed v relaxed task.easy see applying operator solvable relaxed planning task never renderunsolvable. lead operators applicable goals true,significant effect all. reason, relaxed planning tasks solved efficiently, eventhough optimal solutions still NP-hard compute (Bylander, 1994). plan relaxationplanning task called relaxed plan task.FF heuristic estimates goal distance world state generating relaxed plantask reaching goal world state. number operators generated planused heuristic estimate. implementation FF heuristic necessarilygenerate same, even equally long, relaxed plan FF. experiments, turnproblematic, implementations appear equally informative.FF heuristic originally introduced ADL domains, extending tasks involving derived predicates straight-forward. One possible extension simply assumederived predicate initially set default value treat axioms relaxed operators cost0. slightly complicated, also accurate approach, derived variables initializedactual value given world state, allowing relaxed planner achieve value(or values) applying transitions extended domain transition graph derivedvariable. followed second approach.addition heuristic estimates, FF heuristic also exploited restricting biasingchoice operators apply given world state s. set helpful actions consistsoperators relaxed plan computed applicable state. mentionedintroduction section, Fast Downward configured treat helpful actionspreferred operators.wealth work FF heuristic literature, discuss further.thorough treatment, point references (Hoffmann & Nebel, 2001; Hoffmann,2001, 2002, 2005).6.3 Greedy Best-First Search Fast DownwardFast Downward uses greedy best-first search closed list default search algorithm.assume reader familiar algorithm refer literature details (Russell &Norvig, 2003).implementation greedy best-first search differs textbook algorithm two ways.First, treat helpful transitions computed causal graph heuristic helpful actions computed FF heuristic preferred operators. Second, performs deferred heuristic evaluationreduce influence large branching factors. turn describing two searchenhancements.6.3.1 P REFERRED PERATORSmake use helpful transitions computed causal graph heuristic helpful actions computed FF heuristic, variant greedy best-first search supports use so-called preferred operators. set preferred operators given state subset set applicableoperators state. operators considered preferred depends settings224fiT FAST OWNWARD P LANNING YSTEMsearch component, discussed earlier. intuition behind preferred operators randomlypicked successor state likely closer goal generated preferred operator, case call preferred successor. Preferred successors considerednon-preferred ones average.search algorithm implements preference maintaining two separate open lists, onecontaining successors expanded states one containing preferred successors exclusively.search algorithm alternates expanding regular successor preferred successor.even iterations consider one open list, odd iterations other. matteropen list state taken from, successors placed first open list, preferredsuccessors additionally placed second open list. (Of course could limit first openlist contain non-preferred successors; however, typically total number successorsvast number preferred successors tiny. Therefore, cheaper add successorsfirst open list detect duplicates upon expansion scan list successorsdetermining element whether preferred.)Since number preferred successors smaller total number successors,means preferred successors typically expanded much earlier others. especiallyimportant domains heuristic guidance weak lot time spent exploring plateaus.faced plateaus, Fast Downwards open lists operate first-in-first-out fashion. (Inwords: constant heuristic function, search algorithm behaves like breadth-firstsearch.) Preferred operators typically offer much better chances escaping plateaus sincelead significantly lower effective branching factors.6.3.2 EFERRED H EURISTIC E VALUATIONUpon expanding state s, textbook version greedy best-first search computes heuristicevaluation successor states sorts open list accordingly.wasteful many successors heuristic evaluations costly, two conditions oftentrue heuristic search approaches planning.second modification comes play. successor better heuristicestimate generated early leads promising path towards goal, would likeavoid generating successors. Let us assume 1000 successors, 0 ,10th successor generated, better heuristic estimate s. Furthermore, let usassume goal reached 0 path non-increasing heuristic estimates.would like avoid computing heuristic values 990 later successors altogether.Deferred heuristic evaluation achieves computing heuristic estimates successors expanded state immediately. Instead, successors placed open listtogether heuristic estimate state s, heuristic estimates computedexpanded, time used sorting successors openlist, on. general, state sorted open list according heuristic evaluationparent, initial state exception. fact, need put successorstate open list, since require representation want evaluateheuristic estimate. Instead, save memory storing reference parent stateoperator transforming parent state successor state open list.might clear approach lead significant savings time, since deferredevaluation also means information available later. potential savings become225fiH ELMERTapparent considering deferred heuristic evaluation together use preferred operators:improving successor s0 state reached preferred operator, likelyexpanded (via second open list) long successors even siblingss. situation described above, exists non-increasing path 0 goal,heuristic evaluations never computed successors s. fact, deferred heuristicevaluation significantly improve search performance even preferred operatorsused, especially tasks branching factors large heuristic estimate informative.first glance, deferred heuristic evaluation might appear related another technique reducing effort expanding node within best-first search algorithm, namely PartialExpansion (Yoshizumi, Miura, & Ishida, 2000). However, algorithm designed reducingspace requirements best-first search expense additional heuristic evaluations:expanding node, Partial Expansion computes heuristic value successors,stores open queue whose heuristic values fall certain relevance threshold.later iterations, might turn threshold chosen low, case nodeneeds re-expanded heuristic values successors re-evaluated. general,Partial Expansion never compute fewer heuristic estimates standard , usuallyrequire less memory.However, heuristic search approaches planning (and certainly Fast Downward), heuristic evaluations usually costly time memory storing open closed listslimiting factor. thus willing trade memory time opposite way: Deferredheuristic evaluation normally leads node expansions higher space requirementsstandard best-first search heuristic values used guiding search less informative (they evaluate predecessor search node rather node itself). However, heuristiccomputations required nodes actually removed open queue rathernodes fringe, latter usually significantly numerous.6.4 Multi-Heuristic Best-First Searchalternative greedy best-first search, Fast Downward supports extended algorithm calledmulti-heuristic best-first search. algorithm differs greedy best-first search usemultiple heuristic estimators, based observation different heuristic estimators different weaknesses. may case given heuristic sufficient directing searchtowards goal except one part plan, gets stuck plateau. Another heuristicmight similar characteristics, get stuck another part search space.Various ways combining heuristics proposed literature, typically addingtogether taking maximum individual heuristic estimates. believe oftenbeneficial combine different heuristic estimates single numerical value. Instead,propose maintaining separate open list heuristic estimator, sorted accordingrespective heuristic. search algorithm alternates expanding stateopen list. Whenever state expanded, estimates calculated according heuristic,successors put open list.Fast Downward configured use multi-heuristic best-first search, computes estimates causal graph heuristic FF heuristic, maintaining two open lists. course,approach combined use preferred operators; case, search algorithmmaintains four open lists, heuristic distinguishes normal preferred successors.226fiT FAST OWNWARD P LANNING YSTEMalgorithm reach-one-goal(, v, d, cond):{0, 1, . . . , max-threshold}:Let set operators whose modification distance respect v.Assign cost c operator modification distance crespect v.Call uniform-cost-search algorithm closed list, using operator set ,find state satisfying {v = d} cond.return plan uniform-cost-search succeeded.Figure 19: reach-one-goal procedure reaching state v = d. value max-thresholdequal maximal modification distance operator respect v.6.5 Focused Iterative-Broadening Searchfocused iterative-broadening search algorithm experimental piece Fast Downwards search arsenal. present form, algorithm unsuitable many planning domains,especially containing comparatively different goals. Yet think might containnucleus successful approach domain-independent planning differentcurrent methods, include completeness source inspiration.algorithm intended first step towards developing search techniques emphasizeidea using heuristic criteria locally, limiting set operators apply, rather globally,choosing states expand global set open states. made first experimentsdirection observing large boost performance obtained using preferredoperators heuristic search. algorithm performed surprisingly well standardbenchmark domains, performing badly others.name suggests, algorithm focuses search concentrating one goal time,restricting attention operators supposedly important reaching goal:Definition 9 Modification distancesLet MPT, let operator , let v variable .modification distance respect v defined minimum, variablesv 0 occur affected variables effect list o, distance v 0 v CG().example, operators modify v directly modification distance 0 respectv, operators modify variables occur preconditions operators modifying vmodification distance 1, on. assume order change value variable,operators low modification distance respect variable useful.Fig. 19 shows reach-one-goal procedure achieving single goal MPT. timebeing, assume cond parameter always . procedure makes use assumptionhigh modification distance implies low usefulness two ways. First, operators highmodification distance respect goal variable considered higher associatedcost, hence applied less frequently. Second, operators whose modification distance beyondcertain threshold forbidden completely. Instead choosing threshold priori, algorithm227fiH ELMERTfirst tries find solution lowest possible threshold 0, increasing threshold 1whenever previous search failed. uniform-cost-search algorithm mentioned Fig. 19standard textbook method (Russell & Norvig, 2003).Although ignorant fact time algorithm conceived, core ideareach-one-goal new: Ginsberg Harvey (1992) present search technique called iterativebroadening, also based idea repeatedly sequence uninformed searchesever-growing set operators. work demonstrates superiority iterative broadening standard depth-bounded search empirically analytically reasonableassumption choices made branching point equally important. 6 original iterative broadening algorithm applies scenarios without knowledge problem domain,chooses set operators may applied every search node randomly, ratherusing heuristic information causal graph case. However, Ginsberg Harveyalready discuss potential incorporation heuristics operator selection. introduction operator costs (in form modification distances) new, fairly straightforwardextension heuristic information available.focused iterative-broadening search algorithm based reach-one-goal method;idea achieve goals planning task one other, using reach-one-goalalgorithm core subroutine satisfying individual goals. Since obvious goodorder achieving goals would be, one invocation reach-one-goal started goalparallel. one-goal solver focuses (supposedly) relevant operators reachingparticular goal, hope number states considered goal reached small.one one-goal solvers reaches goal, resulting plan reported sub-searchesstopped. overall search algorithm commits part plan; situationfirst goal reached considered new initial state.situation, try satisfy second goal, starting parallel invocationsreach-one-goal possible second goal. course, lead situationsearch algorithm oscillates goals, first achieving goal a, abandoning favour goalb, without sign making real progress. Therefore, demand reach-one-goal achievessecond goal addition one reached first, setting cond argument accordingly.two goals reached, sub-searches stopped, sub-searches thirdgoal started, on, goals reached.sense, focusing technique similar beam search algorithm (Lowerre, 1976),also performs fixed number concurrent searches avoid committing particular pathsearch space early. Beam search uses heuristic function evaluate branchessearch abandoned new branches spawned. focused iterativebroadening search appear use heuristic evaluations first glance, number satisfiedgoals state used evaluation criterion essentially way. One important difference beam search use modification distances relative particular goal, meansdifferent beams explore state space qualitatively different ways.one final twist: motivate reach-one-goal needlessly wander away satisfied goals, forbid applying operators undo previously achieved goals cond.old idea called goal protection (Joslin & Roach, 1989). well-known protecting6. See original analysis precise definition equally important (Ginsberg & Harvey, 1992). GinsbergHarveys assumption certainly valid practice, find much convincing competing modelgoal states uniformly distributed across search fringe.228fiT FAST OWNWARD P LANNING YSTEMalgorithm reach-one-goal(, v, d, cond):{0, 1, . . . , max-threshold}:Let set operators whose modification distance respect vaffect state variable occurring cond.Assign cost c operator modification distance crespect v.Call uniform-cost-search algorithm closed list, using operator set ,find state satisfying {v = d} cond.return plan uniform-cost-search succeeded.{0, 1, . . . , max-threshold}:Let set operators whose modification distance respect v.Assign cost c operator modification distance crespect v.Call uniform-cost-search algorithm closed list, using operator set ,find state satisfying {v = d} cond.return plan uniform-cost-search succeeded.Figure 20: reach-one-goal procedure reaching state v = (corrected).goals renders search algorithm incomplete, even state spaces operators reversiblelocal search approaches like focused iterative-broadening search would otherwise complete.particular, search must fail planning tasks serializable (Korf, 1987). Therefore,first solution attempt fails, algorithm restarted without goal protection. completeprocedure shown Fig. 20, concludes discussion Fast Downwards search component.7. Experimentsevaluate performance Fast Downward, specifically differences variousconfigurations search component, performed number experiments setbenchmarks previous international planning competitions. purpose experiments compare Fast Downward state art PDDL planning, contrastperformance different search algorithms Fast Downward (greedy best-first searchwithout preferred operators, multi-heuristic best-first search without preferred operators,focused iterative-broadening search).clearly state purpose experiments, let us also point two areas worthy studychoose investigate here:compare causal graph heuristic heuristics, FF HSPheuristics. comparison would require evaluating different heuristics within otherwise identical planning systems. performed experiment (Helmert,2004) thus prefer dedicate section evaluation complete Fast Downwardplanning system, rather heuristic function.229fiH ELMERTgive final answer question Fast Downward performs well badlydomains analyse. observe bad performance, try give plausibleexplanation this, conduct full-blown study heuristic quality spiritHoffmanns work FF h+ heuristics (Hoffmann, 2005). believemuch could learned investigation, major undertaking would gobeyond scope article.aim section evaluate Fast Downward planner whole,number algorithmic questions address. example, one might wonder (ifany) speed-up obtained using successor generators simpler methods testoperator applicability whenever node expanded. Another question concerns extentdeferred heuristic evaluation affects search performance. keep section reasonablelength, discuss either questions here. However, conducted experimentsaddressing them, include results electronic appendix paper. 77.1 Benchmark Setbenchmark set use consists propositional planning tasks fully automatedtracks first four international planning competitions hosted AIPS 1998, AIPS 2000, AIPS2002 ICAPS 2004. set benchmark domains shown Fig. 21. Altogether, benchmark suite comprises 1442 tasks. (The numbers Fig. 21 add 1462, 20 ATELLITEinstances introduced IPC3 also part benchmark set IPC4,count once.)distinguish three classes domains:STRIPS domains: domains feature derived predicates conditional effects,conditions appearing goal operators conjunctions positive literals.ADL domains: domains make use conditional effects operator and/or containgeneral conditions simple conjunctions goals operators. However,require axioms.PDDL2.2 domains: domains use full range propositional PDDL2.2, includingfeatures present ADL domains axioms.IPC4, domains presented different formulations, meaning realworld task encoded several different ways. Participants asked work oneformulation per domain, able choose preferred formulation given domain freely.example, IRPORT domain available STRIPS formulation ADL formulation.However, organizers strictly follow rule considering different encodingsreal-world task different formulations, rather different domains proper. Namely,PSR-M IDDLE P ROMELA domains, encodings without axioms available,considered different domains grounds encodings without axioms7. See http://www.jair.org/. short summary successor generators speed search twoorders magnitude extreme cases like largest ATELLITE tasks, little impact performancetime. Deferred heuristic evaluation beneficial domains, speed-ups one ordermagnitude common, somewhat beneficial majority domains, speed-ups 2 4,rarely detrimental performance.230fiT FAST OWNWARD P LANNING YSTEMCompetitionDomainClassNumber tasksIPC1 (AIPS 1998)SSEMBLYG RIDG RIPPERL OGISTICSOVIEYSTERYMP RIMEADLSTRIPSSTRIPSSTRIPSSTRIPSSTRIPSSTRIPS3052035303035IPC2 (AIPS 2000)B LOCKSWORLDF REECELLL OGISTICSICONIC -STRIPSICONIC -S IMPLE ADLICONIC -F ULL ADLCHEDULESTRIPSSTRIPSSTRIPSSTRIPSADLADLADL356028150150150150IPC3 (AIPS 2002)EPOTRIVERLOGF REECELLROVERSATELLITEZ ENOTRAVELSTRIPSSTRIPSSTRIPSSTRIPSSTRIPSSTRIPS222020202020IPC4 (ICAPS 2004)IRPORTP ROMELA -O PTICALT ELEGRAPHP ROMELA -P HILOSOPHERSP IPESWORLD -N OTANKAGEP IPESWORLD -TANKAGEPSR-S MALLPSR-M IDDLEPSR-L ARGEATELLITESTRIPSPDDL2.2PDDL2.2STRIPSSTRIPSSTRIPSPDDL2.2PDDL2.2STRIPS504848505050505036Figure 21: Planning domains first four international planning competitions.231fiH ELMERTmuch larger hence likely difficult solve. apply formulation vs. encoding viewstrictly thus consider one PSR-M IDDLE domain one domain twoP ROMELA variants, P ROMELA -P HILOSOPHERS P ROMELA -O PTICALT ELEGRAPH.IPC1 benchmark set, tasks solvable except 11 YSTERY instances.IPC2 benchmark set, tasks solvable except 11 ICONIC -F ULL ADL instances.IPC3 benchmarks solvable. IPC4, checked instances P IPESWORLD TANKAGE domain, assume tasks solvable.run heuristic search modes, Fast Downward proves unsolvabilityunsolvable YSTERY ICONIC -F ULL ADL tasks using dead-end detection routine described earlier article causal graph heuristic (Helmert, 2004), casesICONIC -F ULL ADL domain exhaustively searching states finite FF heuristic.course, unsolvable task proved unsolvable planner, report successfullysolved instance experimental results.7.2 Experimental Setupdiscussed Section 6, eleven possible configurations Fast Downwards searchcomponent. However, equally reasonable. example, use FFs helpfulactions, would seem wasteful use FF heuristic estimate, since two calculatedtogether. Therefore, greedy best-first search setup, exclude configurations FFhelpful actions always computed. multi-heuristic best-first search setup, excludeconfigurations one type preferred operators considered, other, sincewould seem arbitrary choice. leaves us six different configurationsplanner:1. G: Use greedy best-first search without preferred operators.2. G + P: Use greedy best-first search helpful transitions preferred operators.3. G + P+ : Use greedy best-first search helpful transitions preferred operators. Usehelpful actions preferred operators states helpful transitions.4. M: Use multi-heuristic best-first search without preferred operators.5. + P: Use multi-heuristic best-first search helpful transitions helpful actionspreferred operators.6. F: Use focused iterative-broadening search.apply planner configurations 1442 benchmark tasks, usingcomputer 3.066 GHz Intel Xeon CPU machine used IPC4 setmemory limit 1 GB timeout 300 seconds.compare Fast Downward state art, try solve benchmarkbest-performing planners literature. Unfortunately, involves intricacies:planners publicly available, others cover restricted subset PDDL2.2.main experiment, thus partition benchmark domains three sets dependingplanners available comparison.232fiT FAST OWNWARD P LANNING YSTEMDomainTaskConfigurationF REECELL (IPC2)G RIDMP RIMEPSR-L ARGEATELLITE (IPC4)probfreecell-10-1prob05prob14p30-s179-n30-l3-f30p33-HC-pfile13M+PG+PM+PPreprocessing9.3010.0422.3843.43180.74Search298.64291.01291.67265.29169.09Figure 22: Tasks could solved configuration Fast Downward searchtimeout 300 seconds, total processing timeout 300 seconds.column preprocessing shows total time translation knowledge compilation.7.3 Translation Knowledge Compilation vs. Searchcourse, results report Fast Downward include time spent three componentsplanner: translation, knowledge compilation, search. Therefore, following presentation results, consider task solved total processing time 300 seconds.However, also investigated tasks solved timeout 300 secondssearch component alone, allowing components use arbitrary amount resources.turns makes difference five cases, could solvedtotal time 310 seconds (Fig. 22). one five cases, ATELLITE instanceexorbitant size, search take less time two phases combined. results showsearch component time-critical part Fast Downward practice. Therefore,report separate performance results individual components.7.4 STRIPS Domains IPC13Let us present results main experiment. abstain listing runtimes individual planning tasks due prohibitively large amount data. available electronicappendix article.8 Instead, report following information:Tables showing number tasks solved planner within 300 second timeout.Here, present individual results domain.Graphs showing number tasks solved given time planner. Here,present separate results domain, would require many graphs.discuss plan lengths; observations regard similar madeoriginal implementation causal graph heuristic (Helmert, 2004).Fig. 23 shows number unsolved tasks STRIPS domains IPC13.Figs. 24 25 show number tasks solved planner within given time bound0 300 seconds. addition six configurations Fast Downward consideration,table includes four columns.heading Any, include results hypothetical meta-planner guessesbest six configuration Fast Downward input task executes Fast Downward8. http://www.jair.org/233fiH ELMERTDomain#TasksGB LOCKSWORLDEPOTRIVERLOGF REECELL (IPC2)F REECELL (IPC3)G RIDG RIPPERL OGISTICS (IPC1)L OGISTICS (IPC2)ICONIC -STRIPSOVIEYSTERYMP RIMEROVERSATELLITE (IPC3)Z ENOTRAVEL352220602052035281503030352020200122401010001021001304020000020000013012510000010000Total550242132G+P G+P+M+PFCGFFLPG01211111040000200008012200000000000171114014402600013142600703000000000000014320100000113004353200000012300000055191040001570003222148102532101Figure 23: Number unsolved tasks STRIPS domains IPC1, IPC2, IPC3.PSfrag replacementsFDD (Fast Downward)550 (100%)FD (Fast Downward)YAHSPMacro-FF495 (90%)LPG-TDCGFFLPGSolved TasksSGPlan440 (80%)(Fast Downward)G + P (Fast Downward)+ P (Fast Downward)G (Fast Downward)G + P+ (Fast Downward)(Fast Downward)F (Fast Downward)385 (70%)0s50s100s150sSearch Time200s250s300sFigure 24: Number tasks solved vs. runtime STRIPS domains IPC1, IPC2 IPC3.graph shows results various configurations Fast Downward.234fiT FAST OWNWARD P LANNING YSTEMPSfrag replacementsFDD (Fast Downward)550 (100%)FD (Fast Downward)YAHSPMacro-FF495 (90%)LPG-TDSolved TasksSGPlan440 (80%)G + P+ (Fast Downward)(Fast Downward)G + P (Fast Downward)CGFFLPG385 (70%)G (Fast Downward)+ P (Fast Downward)(Fast Downward)F (Fast Downward)0s50s100s150sSearch Time200s250s300sFigure 25: Number tasks solved vs. runtime STRIPS domains IPC1, IPC2 IPC3.graph shows results CG, FF LPG hypothetical planneralways chooses best configuration Fast Downward. result greedybest-first search helpful transitions repeated ease comparison Fig. 24.235fiH ELMERTsetting. heading CG, report results first implementationcausal graph heuristic (Helmert, 2004). 9 Finally, FF LPG refer well-known planners(Hoffmann & Nebel, 2001; Gerevini et al., 2003) fully-automated tracks IPC2IPC3. chosen comparison benchmark set showed bestperformance far publicly available planners experimented with. LPG, usesrandomized search strategy, attempted solve task five times report median result.results show excellent performance Fast Downward set benchmarks. Compared CG, already shown solve tasks FF LPG benchmark set(Helmert, 2004), get another slight improvement half planner configurations. Oneconfigurations, multi-heuristic best-first search using preferred operators, solves benchmarksdomains except EPOT F REECELL. Even importantly, number taskssolved Fast Downward configurations small 10. Note planning competitions typically allowed planner spend 30 minutes task; time constraints,could allocate five minutes six configurations Fast Downward, getting resultsleast good reported planner. Results might even bettercleverer allocation scheme.Even configuration using focused iterative-broadening search performs comparatively wellbenchmarks, although cannot compete planners. surprisingly,version planner difficulties domains many dead ends (F REECELL, YSTERY,MP RIME) goal ordering important (B LOCKSWORLD, EPOT). also fares comparatively badly domains large instances, namely L OGISTICS (IPC1) ATELLITE.reader keep mind FF LPG excellent planning systems;planners experimented with, including awarded prizes first three planning competitions, none solved benchmarks group focused iterative-broadeningsearch.one domain proves quite resistant Fast Downwards solution attempts configuration EPOT. already observed initial experiments causal graph heuristic(Helmert, 2004), believe one key problem Fast Downward, unlike FF,use goal ordering techniques, important domain. fact domainincludes B LOCKSWORLD-like subproblem also problematic, gives rise dense causalgraphs demonstrated Section 5.2.3.7.5 ADL Domains IPC13Second, present results ADL domains first three planning competitions.much smaller group previous, including four domains. time, cannot considerCG LPG, since neither CG publicly available version LPG supports ADL domains.Therefore, compare FF exclusively. Again, report number unsolved tasksdomain (Fig. 26) present graphs showing quickly tasks solved (Figs. 27 28).results look good first group domains. Results ICONICdomains good, even improving FF. However, greedy best-first search performsbadly SSEMBLY domain, configurations perform badly CHEDULE domain.9. Apart missing support ADL axioms, CG similar Fast Downward using greedy best-first searchpreferred operators (configuration G). translation knowledge compilation components essentiallyidentical. older search component mainly differs Fast Downward use deferred heuristicevaluation.236fiT FAST OWNWARD P LANNING YSTEMDomain#TasksSSEMBLYICONIC -S IMPLE ADLICONIC -F ULL ADLCHEDULETotal30150150150480G2809134171G+P G+P+270893128250993127309132144M+PFFF00828363009011323300625310012012Figure 26: Number unsolved tasks ADL domains IPC1, IPC2 IPC3.PSfrag replacementsFDD (Fast Downward)480 (100%)FD (Fast Downward)432 (90%)YAHSPMacro-FF384 (80%)LPG-TDCGFFLPGSolved TasksSGPlan336 (70%)288 (60%)(Fast Downward)240 (50%)+ P (Fast Downward)G + P+ (Fast Downward)G + P (Fast Downward)(Fast Downward)G (Fast Downward)F (Fast Downward)192 (40%)144 (30%)0s50s100s150sSearch Time200s250s300sFigure 27: Number tasks solved vs. runtime ADL domains IPC1, IPC2 IPC3.graph shows results various configurations Fast Downward.237fiH ELMERTPSfrag replacementsFDD (Fast Downward)480 (100%)FD (Fast Downward)432 (90%)YAHSPMacro-FF384 (80%)LPG-TDCGLPGG + P+ (Fast Downward)Solved TasksSGPlan336 (70%)288 (60%)240 (50%)G + P (Fast Downward)G (Fast Downward)(Fast Downward)F (Fast Downward)192 (40%)FF(Fast Downward)+ P (Fast Downward)144 (30%)0s50s100s150sSearch Time200s250s300sFigure 28: Number tasks solved vs. runtime ADL domains IPC1, IPC2 IPC3.graph shows results FF hypothetical planner alwayschooses best configuration Fast Downward. result multi-heuristic bestfirst search preferred operators repeated ease comparison Fig. 27.238fiT FAST OWNWARD P LANNING YSTEMCurrently, good explanation SSEMBLY behaviour. CHEDULE domain, weak performance seems related missing goal ordering techniques:many CHEDULE tasks, several goals defined object satisfiedcertain order. instance, objects cylindrical, polished painted,three goals must satisfied precisely order: making object cylindrical reverts effectspolishing painting, polishing reverts effect painting. recognising constraints, heuristic search algorithm assumes close goal object alreadypolished painted cylindrical, loathe transform object cylindrical shapewould undo already achieved goals. rudimentary manual goal ordering,ignoring painting goals goals satisfied, number tasks solvedmulti-heuristic best-first search preferred operators drops 28 3. three failuresappear due remaining ordering problems regard cylindrical polished objects.7.6 Domains IPC4Third finally, present results IPC4 domains. Here, compare FF:benchmarks, FF perform well best planners competition. Besides, severalIPC4 competitors extensions FF hybrids using FF part bigger system, FFbased planning well-represented even limit attention IPC4 planners.comparison, chose four successful competition participants besides Fast Downward,namely LPG-TD, SGPlan, Macro-FF YAHSP (cf. results Hoffmann & Edelkamp, 2005).Similar previous two experiments, report number unsolved tasks domain(Fig. 29) present graphs showing quickly tasks solved (Figs. 30 31).Fast Downward competitive planners across domains, better otherssome. P IPESWORLD domains ones planners noticeably better two competition versions Fast Downward. case YAHSPP IPESWORLD domain variants SGPlan P IPESWORLD -N OTANKAGE . P IPESWORLDdomain hierarchical nature; might domain decomposition approach causal graph heuristic appropriate. results heuristic searchconfigurations P ROMELA -O PTICALT ELEGRAPH domain extremely bad require investigation.Interestingly, focused iterative-broadening search performs well benchmarks suite. One reasons many tasks IPC4 suite,many individual goals easy serialize solved mostly independently. 10Comparing configuration G G + P + especially + P, also observe using preferred operators useful benchmarks, even two previousexperiments.final remark, observe implemented meta-planner calling sixFast Downward configurations round-robin fashion, would obtain planning systemcould solve 54 IPC4 benchmarks within 6 5 = 30 minute timeout. almostpar top performer IPC4, Fast Diagonally Downward, solved 52 IPC4benchmarks timeout. Thus, benchmark set exploring differentplanner configurations definitely pays off.10. devised experiment shows property artificially violated simple goal reformulation,performance algorithm degrades quickly; see electronic appendix details.239fiH ELMERTDomain#TasksIRPORTP IPESWORLD -N OTANKAGEP IPESWORLD -TANKAGEP ROMELA -O PTICALT ELEGRAPHP ROMELA -P HILOSOPHERSPSR-S MALLPSR-M IDDLEPSR-L ARGEATELLITE (IPC4)Total505050484850505036432G28243648000228166G+P G+P+M+PF30253647000200158147174600022310901034132112239221620714130002005417233648000220146181434471600238160DomainFDFDDLPG-TDMacro-FFSGPlanYAHSPIRPORTP IPESWORLD -N OTANKAGEP IPESWORLD -TANKAGEP ROMELA -O PTICALT ELEGRAPHP ROMELA -P HILOSOPHERSPSR-S MALLPSR-M IDDLEPSR-L ARGEATELLITE (IPC4)Total01134220002208907192200022373710293712050113730122931365019500257602029064396110170133619350500188Figure 29: Number unsolved tasks IPC4 domains. Results various configurationsFast Downward listed upper part, results competition participantslower part. FD FDD denote versions Fast Downward participated IPC4 names Fast Downward Fast Diagonally Downward(cf. Section 6).240fiT FAST OWNWARD P LANNING YSTEMPSfrag replacementsFDD (Fast Downward)432 (100%)FD (Fast Downward)389 (90%)YAHSP346 (80%)Macro-FFSGPlanCGFFLPG302 (70%)Solved TasksLPG-TD(Fast Downward)259 (60%)216 (50%)173 (40%)130 (30%)+ P (Fast Downward)G + P+ (Fast Downward)G + P (Fast Downward)(Fast Downward)F (Fast Downward)G (Fast Downward)86 (20%)43 (10%)0s50s100s150sSearch Time200s250s300sFigure 30: Number tasks solved vs. runtime IPC4 domains. graph shows resultsvarious configurations Fast Downward.PSfrag replacements432 (100%)389 (90%)346 (80%)CGFFLPGG + P+ (Fast Downward)G + P (Fast Downward)G (Fast Downward)Solved Tasks302 (70%)259 (60%)216 (50%)173 (40%)130 (30%)(Fast Downward)FDD (Fast Downward)FD (Fast Downward)SGPlanLPG-TDYAHSPMacro-FF86 (20%)43 (10%)+ P (Fast Downward)(Fast Downward)F (Fast Downward)0s50s100s150sSearch Time200s250s300sFigure 31: Number tasks solved vs. runtime IPC4 domains. graph shows resultshypothetical planner always chooses best configuration FastDownward, competition configurations Fast Downward best fourparticipants.241fiH ELMERT7.7 Conclusions Experimentinterpret experimental results? first conclusion Fast Downwardclearly competitive state art. especially true configuration usingmulti-heuristic best-first search preferred operators (M+P), outperforms competingplanning systems set STRIPS domains IPC13 domains IPC4.problems CHEDULE domain, would true remaininggroup benchmarks, ADL domains IPC13.regard second objective investigation, evaluating relative strengthsdifferent planner configurations, M+P configuration emerges clear-cut winner. 2329 domains, configuration solves tasks, unlike configurations,one domain (P ROMELA -O PTICALT ELEGRAPH) performs badly. concludemulti-heuristic best-first search use preferred operators promising extensionsheuristic planners.particularly true preferred operators. Indeed, M+P configuration, twovariants greedy best-first search preferred operators show next best overall performance,terms number domains among top performers termstotal number tasks solved. Comparing G G+P, ten domains variantusing preferred operators solves tasks one using them; opposite true fivedomains. Comparing M+P, difference even striking, preferred operatorvariant outperforming fifteen domains, worse two (insolves one task less). convincing arguments use preferred operators.8. Summary Discussionturn discussion, let us briefly summarize contributions article. motivating starting point, explained planning tasks often exhibit simpler structure expressedmulti-valued state variables, rather traditional propositional representations.introduced Fast Downward, planning system based idea converting tasks multivalued formalism exploiting causal information underlying encodings.Fast Downward processes PDDL planning tasks three stages. skipped firststages, translation, automatically transforms PDDL task equivalent multi-valuedplanning task nicer causal structure. explained inner workings second stage,knowledge compilation, demonstrating depth kind knowledge planner extractsproblem representation, discussing causal graphs, domain transition graphs, successor generators axiom evaluators. discussion Fast Downwards search component,introduced heuristic search algorithms, use technique deferred heuristic evaluationreduce number states heuristic goal distance estimate must computed.addition greedy best-first search, Fast Downward employs multi-heuristic best-first searchalgorithm usefully integrate information two heuristic estimators, namely causal graphheuristic FF heuristic. heuristic search algorithms utilize preference informationoperators. also introduced Fast Downwards experimental focused iterative-broadeningsearch algorithm, based idea pruning set operators considersuccessor states likely lead towards specific goal.thus tried give complete account Fast Downward planning systems approachsolving multi-valued planning tasks, including motivation, architecture, algorithmic founda242fiT FAST OWNWARD P LANNING YSTEMtions. previous section, demonstrated empirical behaviour, showing good performanceacross whole range propositional benchmarks previous planning competitions.Among novel algorithms search enhancements discussed article, twoaspects Fast Downward consider central importance would likeemphasize. One use multi-valued state variables PDDL-style planning.believe multi-valued representations much structured hence much amenableautomated reasoning purposes heuristic evaluation, problem decomposition,aspects planning goal ordering extraction landmarks. centralidea use hierarchical decompositions within heuristic planning framework. Hierarchicalapproaches domain-independent planning considerable potential, since workKnoblock (1994) Bacchus Yang (1994), little work published. Fast Downward, hope renew interest area, believe promising groundadvances automated planning.future, several aspects Fast Downward would like investigatefurther. First, intend experiment search techniques along lines focusediterative-broadening search, emphasize heuristically evaluating operator usefulness ratherheuristically evaluating states.Second, would like come efficient heuristic multi-valued planning tasksrequire pruning cycles causal graph. Initial experiments directionshown difficult achieve goal without losing performance Fast Downwardsheuristic estimator, perhaps better heuristic accuracy outweigh worse per-state performancemany cases.Third, want investigate far performance planner could improvedencoding domains differently. cases, merging set state variablesclosely interrelated single state variable whose domain product domainsoriginal state variables might beneficial. Also, want test hand-tailored encodings leadbetter performance automatically derived ones, so, large performance gap is.Fourth finally, would like evaluate behaviour causal graph heuristicspecific planning domains empirically theoretically, following Hoffmanns work FFheuristic (Hoffmann, 2001, 2002, 2005). Hopefully, give indicationexpect good performance causal graph heuristic advisable lookapproaches.Acknowledgementsauthor wishes thank Silvia Richter, member Fast Downward team4th International Planning Competition, part implementing planner valuableadvice before, throughout, competition. also deserves thanks helpingexperiments, proof-reading article, suggesting number improvements.anonymous reviewers article handling editor, Maria Fox, made numberuseful suggestions led significant improvements.work partly supported German Research Council (DFG) within GraduateProgramme Mathematical Logic Applications part Transregional CollaborativeResearch Centre Automatic Verification Analysis Complex Systems (SFB/TR 14 AVACS).See www.avacs.org information.243fiH ELMERTReferencesBacchus, F., & Yang, Q. (1994). Downward refinement efficiency hierarchical problemsolving. Artificial Intelligence, 71(1), 43100.Backstrom, C., & Nebel, B. (1995). Complexity results SAS + planning. Computational Intelligence, 11(4), 625655.Bonet, B., & Geffner, H. (2001). Planning heuristic search. Artificial Intelligence, 129(1), 533.Brafman, R. I., & Domshlak, C. (2003). Structure complexity planning unary operators.Journal Artificial Intelligence Research, 18, 315349.Bylander, T. (1994). computational complexity propositional STRIPS planning. ArtificialIntelligence, 69(12), 165204.Domshlak, C., & Brafman, R. I. (2002). Structure complexity planning unary operators.Ghallab, M., Hertzberg, J., & Traverso, P. (Eds.), Proceedings Sixth InternationalConference Artificial Intelligence Planning Scheduling (AIPS 2002), pp. 3443. AAAIPress.Domshlak, C., & Dinitz, Y. (2001). Multi-agent off-line coordination: Structure complexity.Cesta, A., & Borrajo, D. (Eds.), Pre-proceedings Sixth European ConferencePlanning (ECP01), pp. 277288, Toledo, Spain.Dowling, W. F., & Gallier, J. H. (1984). Linear-time algorithms testing satisfiabilitypropositional Horn formulae. Journal Logic Programming, 1(3), 367383.Edelkamp, S., & Helmert, M. (1999). Exhibiting knowledge planning problems minimizestate encoding length. Fox, M., & Biundo, S. (Eds.), Recent Advances AI Planning.5th European Conference Planning (ECP99), Vol. 1809 Lecture Notes ArtificialIntelligence, pp. 135147, New York. Springer-Verlag.Edelkamp, S., & Hoffmann, J. (2004). PDDL2.2: language classical part 4thInternational Planning Competition. Tech. rep. 195, Albert-Ludwigs-Universitat Freiburg,Institut fur Informatik.Fox, M., & Long, D. (2003). PDDL2.1: extension PDDL expressing temporal planningdomains. Journal Artificial Intelligence Research, 20, 61124.Garey, M. R., & Johnson, D. S. (1979). Computers Intractability Guide TheoryNP-Completeness. Freeman.Gerevini, A., Saetti, A., & Serina, I. (2003). Planning stochastic local search temporalaction graphs LPG. Journal Artificial Intelligence Research, 20, 239290.Ginsberg, M. L., & Harvey, W. D. (1992). Iterative broadening. Artificial Intelligence, 55, 367383.Helmert, M. (2004). planning heuristic based causal graph analysis. Zilberstein, S., Koehler,J., & Koenig, S. (Eds.), Proceedings Fourteenth International Conference AutomatedPlanning Scheduling (ICAPS 2004), pp. 161170. AAAI Press.Hoffmann, J. (2001). Local search topology planning benchmarks: empirical analysis.Nebel, B. (Ed.), Proceedings 17th International Joint Conference Artificial Intelligence (IJCAI01), pp. 453458. Morgan Kaufmann.244fiT FAST OWNWARD P LANNING YSTEMHoffmann, J. (2002). Local search topology planning benchmarks: theoretical analysis.Ghallab, M., Hertzberg, J., & Traverso, P. (Eds.), Proceedings Sixth International Conference Artificial Intelligence Planning Scheduling (AIPS 2002), pp. 92100. AAAIPress.Hoffmann, J. (2005). ignoring delete lists works: Local search topology planning benchmarks. Journal Artificial Intelligence Research, 24, 685758.Hoffmann, J., & Edelkamp, S. (2005). deterministic part IPC-4: overview. JournalArtificial Intelligence Research, 24, 519579.Hoffmann, J., & Nebel, B. (2001). FF planning system: Fast plan generation heuristicsearch. Journal Artificial Intelligence Research, 14, 253302.Jonsson, P., & Backstrom, C. (1995). Incremental planning. Ghallab, M., & Milani, A. (Eds.),New Directions AI Planning: EWSP 95 3rd European Workshop Planning, Vol. 31Frontiers Artificial Intelligence Applications, pp. 7990, Amsterdam. IOS Press.Jonsson, P., & Backstrom, C. (1998a). State-variable planning structural restrictions: Algorithms complexity. Artificial Intelligence, 100(12), 125176.Jonsson, P., & Backstrom, C. (1998b). Tractable plan existence imply tractable plan generation. Annals Mathematics Artificial Intelligence, 22(3), 281296.Joslin, D., & Roach, J. (1989). theoretical analysis conjunctive-goal problems. ArtificialIntelligence, 41(1), 97106. Research Note.Knoblock, C. A. (1994). Automatically generating abstractions planning. Artificial Intelligence,68(2), 243302.Korf, R. E. (1987). Planning search: quantitative approach. Artificial Intelligence, 33(1),6588.Lowerre, B. T. (1976). HARPY Speech Recognition System. Ph.D. thesis, Computer ScienceDepartment, Carnegie-Mellon University, Pittsburgh, Pennsylvania.Newell, A., & Simon, H. A. (1963). GPS: program simulates human thought. Feigenbaum,E. A., & Feldman, J. (Eds.), Computers Thought, pp. 279293. Oldenbourg.Russell, S., & Norvig, P. (2003). Artificial Intelligence Modern Approach. Prentice Hall.Sacerdoti, E. D. (1974). Planning hierarchy abstraction spaces. Artificial Intelligence, 5,115135.Tenenberg, J. D. (1991). Abstraction planning. Allen, J. F., Kautz, H. A., Pelavin, R. N.,& Tenenberg, J. D., Reasoning Plans, chap. 4, pp. 213283. Morgan Kaufmann, SanMateo.van den Briel, M., Vossen, T., & Kambhampati, S. (2005). Reviving integer programming approaches AI planning: branch-and-cut framework. Biundo, S., Myers, K., & Rajan,K. (Eds.), Proceedings Fifteenth International Conference Automated PlanningScheduling (ICAPS 2005), pp. 310319. AAAI Press.Williams, B. C., & Nayak, P. P. (1997). reactive planner model-based executive. Pollack,M. E. (Ed.), Proceedings 15th International Joint Conference Artificial Intelligence(IJCAI97), pp. 11781195. Morgan Kaufmann.245fiH ELMERTYoshizumi, T., Miura, T., & Ishida, T. (2000). partial expansion large branching factor problems. Kautz, H., & Porter, B. (Eds.), Proceedings Seventeenth NationalConference Artificial Intelligence (AAAI-2000), pp. 923929. AAAI Press.246fiJournal Artificial Intelligence Research 26 (2006) 453-541Submitted 12/05; published 08/06Engineering Benchmarks Planning: Domains UsedDeterministic Part IPC-4Jorg HoffmannHOFFMANN @ MPI - SB . MPG . DEMax Planck Institute Computer Science,Saarbrucken, GermanyStefan EdelkampSTEFAN . EDELKAMP @ CS . UNI - DORTMUND . DEFachbereich Informatik,Universitat Dortmund, GermanySylvie ThiebauxYLVIE .T HIEBAUX @ ANU . EDU . AUNational ICT Australia & Computer Sciences Laboratory,Australian National University, Canberra, AustraliaRoman EnglertROMAN .E NGLERT @ TELEKOM . DEDeutsche Telekom Laboratories,Berlin, GermanyFrederico dos Santos LiporaceLIPORACE @ INF. PUC - RIO . BRDepartamento de Informatica, PUC-Rio,Rio de Janeiro, BrazilSebastian TrugTRUEG @ INFORMATIK . UNI - FREIBURG . DEInstitut fur Informatik,Universitat Freiburg, GermanyAbstractfield research general reasoning mechanisms, essential appropriatebenchmarks. Ideally, benchmarks reflect possible applications developed technology. AI Planning, researchers tend draw testing examplesbenchmark collections used International Planning Competition (IPC). organization(the deterministic part of) fourth IPC, IPC-4, authors therefore invested significant effortcreate useful set benchmarks. come five different (potential) real-world applications planning: airport ground traffic control, oil derivative transportation pipeline networks,model-checking safety properties, power supply restoration, UMTS call setup. Adaptingpreparing application use benchmark IPC involves, time, inevitable(often drastic) simplifications, well careful choice between, engineering of, domain encodings. first time IPC, used compilations formulate complex domain featuressimple languages STRIPS, rather dropping interesting problem constraints simpler language subsets. article explains discusses five applicationdomains adaptation form PDDL test suites used IPC-4. summarize knowntheoretical results structural properties domains, regarding computational complexityprovable properties topology h+ function (an idealized version relaxedplan heuristic). present new (empirical) results illuminating properties qualitywide-spread heuristic functions (planning graph, serial planning graph, relaxed plan),growth propositional representations instance size, number actions availableachieve fact; discuss data conjunction best results achieveddifferent kinds planners participating IPC-4.c2006AI Access Foundation. rights reserved.fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UG1. IntroductionToday, large extent research discipline AI planning concerned improving performance domain independent generative planning systems. domain independent generativeplanning system (planner) must able fully automatically find plans: solution sequencesdeclaratively specified transition systems. simplest planning formalism deterministic planning. There, planner given input set state variables (often Booleans, called facts),initial state (a value assignment variables), goal (a formula), set actions (withprecondition formula describing applicability, effect specifying action changesstate). plan time-stamped sequence actions maps initial state statesatisfies goal. sort formalism called deterministic since initial state fully specified effects actions non-ambiguous. restrictions may weakened obtainnon-deterministic probabilistic planning.Performance planners measured testing benchmark example instancesplanning problem. best algorithm point time is, generally, considered onesolves examples efficiently. particular, idea International Planning Competition (IPC), biennial event aimed showcasing capabilities current planningsystems.first IPC took place 1998, time writing four events. Providing details IPC beyond scope paper, refer reader overviewarticles written organizers respective IPC editions (McDermott, 2000; Bacchus, 2001;Long & Fox, 2003; Hoffmann & Edelkamp, 2005). particular, Hoffmann Edelkamp (2005)provide details 4th IPC, overall organization, different tracks, evaluation, participating planners, results. Basic information included paper, readerable follow main discussion without detailed background. language used describeplanning problems IPC called PDDL: Planning Domain Definition Language. introduced McDermott (1998) first IPC, IPC-1, 1998. subset languageselected Bacchus (2000) IPC-2 2000. language extended temporal numerical constructs Fox Long (2003) form language PDDL2.1 IPC-3 2002.extended two additional constructs, timed initial literals derived predicates,Hoffmann Edelkamp (2005) form language PDDL2.2 IPC-4 2004.Since, even simplest forms, AI planning computationally hard problem, systemwork efficiently problem instances (Bylander, 1994; Helmert, 2003). Thus, crucial importance kinds examples used testing. Today, more, AI Planningresearchers draw testing examples collections used IPC. makes IPCbenchmarks important instrument field. organization deterministic part4th IPC (there also probabilistic part, see Younes, Littman, Weissman, & Asmuth,2005), authors therefore invested considerable effort creating set useful benchmarksplanning.first question answer precisely meant word useful.easy question. widely accepted mathematical definition deciding whetherset benchmarks considered useful. are, however, widely accepted intuitionscase. Benchmarks be:1. Oriented applications benchmark reflect application technology developed field.454fiE NGINEERING B ENCHMARKSP LANNING2. Diverse structure set benchmarks cover different kinds structure, ratherre-state similar tasks.first usually considered particularly important indeed, AI planning frequently criticized obsession toy examples. recent years, performancestate-of-the-art systems improved dramatically, realistic examples comewithin reach. made another step direction orienting IPC-4 benchmarksapplication domains. traditionally planning benchmarks less fantasy productscreated real scenario mind,1 took actual (possible) applications planningtechnology, turned something suitable competition. considered five different application domains: airport ground traffic control (Airport), oil derivative transportationpipeline networks (Pipesworld), model checking safety properties (Promela), power supply restoration (PSR), setup mobile communication UMTS (UMTS). course, adaptationapplication use IPC, simplifications need made. get back below.Diverse structure benchmarks traditionally given less attention realism,believe less important. structure underlying testing example determinesperformance applied solving mechanism. particularly true solving mechanismswhose performance rises falls quality heuristic use. Hoffmanns (2001, 2002,2005) results suggest much spectacular performance modern heuristic search plannersdue structural similarities traditional planning benchmarks.imply modern heuristic search planners arent useful, certainly showscreation benchmarks risk introducing bias towards one specific way solvingthem. selecting benchmark domains IPC-4, tried cover range intuitivelydifferent kinds problem structure. get back below.one hand, creator planning benchmarks noble goal realistic, structurally diverse, benchmark domains. hand, he/she pragmatic goalcome version/representation benchmarks attacked existing planning systems. Given still quite restricted capabilities systems, obviously two goalsconflict. make matters worse, isnt arbitrarily large supply planning applicationspublicly available, and/or whose developers agree application used basisbenchmark. IPC organizer, top this, final benchmarks must accessiblelarge enough number competing systems, means must formulated languageunderstood systems. Further, benchmarks must show differences scalability planners, i.e., must easy hard, thus straddling boundary currentsystem capabilities.solution difficulties, least solution organization IPC-4, involved slow tedious interleaved process contacting application developers, choosing domains,exploring domain versions, engineering domain version representations. article presents,motivates, discusses choice benchmark domains IPC-4; explains engineeringprocesses led finally used domain versions instances. Further, report about,present new data determining certain structural properties resulting benchmarks(more details below). main contribution work set benchmarks, provided1. course, exceptions rule. One important one, context here, Satellite domain, usedIPC-3, refined use IPC-4. later.455fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UGIPC-4.2 contributions article are: first, providing necessary documentationbenchmarks; second, describing technical processes used creation; third, providingextensive discussion structural properties benchmarks. Apart technical contributions, believe work value example large-scale attemptengineering useful set benchmarks classical planning.difficult make formal claim created set benchmarks,way better previous benchmarks. working this, intentovercome certain shortcomings many benchmarks, though one would hard pressed comeformal proof improvements indeed made. all, judging qualityset benchmarks rather complex matter guided mostly intuitions, and, worse, personalopinions.3 was, best create realistic, structurally diverse, accessiblebenchmarks possible IPC-4. belief succeeded so. benchmarksdefinitely differ certain ways previous benchmarks. thinkdifferences advantageous; discuss places point differences.Regarding realism benchmarks, pointed above, main step took designbenchmarks top-down, i.e., start actual possible applications planning technology,turn something suitable competition rather traditional bottomup approach artificially creating domain real scenario mind. course,modelling application PDDL, particularly modelling way making suitableuse IPC, simplifications need made. cases, e.g., airport ground trafficcontrol, simplifications overly drastic, preserved overall properties intuitivestructure domain. cases, e.g., oil derivative transportation pipeline networks,simplifications needed make drastic domains could wellcreated traditional bottom-up way. Still, even greatly simplified, domain generatedtop-down better chance capture structure relevant real application. Moreover,top-down domain advantage since derived real application, providesclear guideline towards realism; future challenge make planners workrealistic encodings application. previous competitions, domains generatedtop-down sense Elevator domain used IPC-2 (Koehler & Schuster, 2000;Bacchus, 2001), Satellite Rovers domains used IPC-3 (Long & Fox, 2003).Regarding diverse structure benchmarks, contrast previous competitions,IPC-4 domains common theme underlying many benchmarks. IPC-1, 57 domains variants transportation; IPC-2, 4 7 domains variants transportation; IPC-3, 3 6 domains variants transportation, 2 gatheringdata space. variants fact interesting use constructslocked locations, fuel units, road map graphs, stackable objects, complex side constraints.However, certainly intuitive similarity structure relationships domains.extent similarity even automatically detectable (Long & Fox, 2000). IPC4: airport ground traffic control, oil derivative transportation pipeline networks, model checkingsafety properties, power supply restoration, UMTS call setup rather different topics.2. benchmarks downloaded IPC-4 web page http://ipc.icaps-conference.org/3. Consider example Movie domain used IPC-1. instances domain, matter size is,share space reachable states; thing increases connectivity states, i.e.number actions effect. Still one argue Movie useful benchmark, sensehighlight systems/approaches have/have difficulties attacking problem characteristics.456fiE NGINEERING B ENCHMARKSP LANNINGone could claim airport ground traffic control UMTS call setup scheduling nature. see, however, IPC-4 version airport ground traffic control allowsconsiderably freedom classical scheduling formulations, making PSPACE-completedecision problem. particulars domains overviewed Section 3.Approaching structure formal point view difficult. largely unclearwhat, precisely, relevant structure planning domain/instance is, general sense.Hoffmann (2001, 2002, 2005) provides one possible definition search space surface topology certain heuristic function many possible options. particular, Hoffmannsresults relevant heuristic search planners generate heuristic functions basedignoring delete lists relaxation (McDermott, 1996, 1999; Bonet, Loerincs, & Geffner, 1997;Bonet & Geffner, 2001; Hoffmann & Nebel, 2001). lack better formal handle, usedHoffmanns definitions qualify structure domains. selected domains cover different regions Hoffmanns planning domain taxonomy, particular lie regionsless coverage traditional benchmarks. interesting contextpaper hand, summarize Hoffmanns (2005) results 30 domains including domainsused previous competitions. also summarize Helmerts (2006b) results computational complexity satisficing optimal planning IPC-4 domains. turnscomplexity covers wide range widest possible range, propositional planning formalismsPSPACE-hard polynomial. finally provide new data analyze structuralrelationships differences domains. Amongst things, instance,measure: number (parallel sequential) steps needed achieve goal, estimatedsmallest plan found IPC-4 participant; number estimated planning graphsrelaxed plans; distribution number possible achieving actions fact.results examined comparison different domains, taking accountruntime performance exhibited different kinds planners IPC-4.Apart realism diverse structure, main quest creation IPC-4 benchmarkspromote accessibility. Applications are, typically, modelledPDDL, naturally modelled using rather complex language constructs time, numericvariables, logical formulas, conditional effects. existing systems handle subsetsthis, fact half systems entered IPC-4 (precisely, 11 19) could handlesimple STRIPS language, slight extensions it.4 previous competitions, doneexample Elevator, Satellite, Rovers domains, handled simply droppinginteresting domain constraints simpler languages, i.e., removing respectivelanguage constructs domain/instance descriptions. contrast, first time IPC,compiled much domain semantics possible simpler language formats.compilation hard, sometimes impossible, do. done ADL constructs,well two new constructs introduced IPC-4 language PDDL2.2, derived predicatestimed initial literals. implemented, applied, compilation methods cases.4. STRIPS (Stanford Research Institute Problem Solver) name simplest time widespread planning language. form language used today, state variables Boolean, formulasconjunctions positive atoms, action effects either atomic positive (make fact true/add it) atomic negative(make fact false/delete it) (Fikes & Nilsson, 1971). languages selected IPC-2 (Bacchus, 2000),PDDL2.1 PDDL2.2 derived, STRIPS ADL. ADL prominent, expressive, alternativeSTRIPS, extending arbitrary first-order formulas preconditions goal, conditional effects,i.e., effects occur individual effect condition (a first-order formula) met state execution(Pednault, 1989).457fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UGcompilations serve preserve original domain structure, simpler languageclasses. example, STRIPS version Elevator domain IPC-2 simplifiedoriginal ADL version bears marginal similarity real elevator control particular,planner explicitly tell passengers get lift.5 contrast, STRIPSformulation airport ground traffic domain is, semantically, identical ADL formulationexpresses things, awkward fashion.compiled domain versions offered competitors alternative domain version formulations, yielding 2-step hierarchy domain. is, domain IPC-4could contain several different domain versions, differing terms number domain constraints/properties considered. Within domain version, could several domain version formulations, differing terms language used formulate (same) semantics.competitors could choose, within version, whichever formulation planners could handlebest/handle all, results within domain version evaluated together.way, intended make competition accessible possible time keepingnumber separation lines data number distinctions need madeevaluating data acceptable level.are, course, aware encoding details significant impact system performance.6 Particularly, compiling ADL STRIPS, cases revert fullygrounded encodings. certainly isnt desirable, believe acceptable pricepay benefit accessibility. current systems ground operators pre-processanyway. cases considered compiled domain formulations differentoriginal ones allow fair comparison typically plan length increased significantlydue compilation compiled formulation posed competitors separate domain version.article organized follows. main body text contains general information.Section 2, give detailed explanation compilation methods used. Section 3, givesummary domains, short application description, motivation includingdomain, brief explanation main simplifications made, brief explanationdifferent domain versions formulations. Section 4, summarize Hoffmanns (2005)Helmerts (2006b) theoretical results structure IPC-4 domains. Section 5, provideempirical analysis structural properties. Section 6 discusses achieved,provides summary main issues left open. IPC-4 domains, includeseparate section Appendix A, providing detailed information application, adaptationIPC-4, domain versions, example instances used, future directions. Althoughdetails appendix, emphasize secondary importance.contrary, describe main body work did. presentation appendixseems suitable since expect reader to, typically, examine domains detailselective non-chronological manner.5. passengers wont get (out) floors origin (destination); however, explicit control,planner choose let someone (out). accurate encoding via conditional effects actionstopping lift floor.6. detailed account matters provided Howe Dahlman (2002).458fiE NGINEERING B ENCHMARKSP LANNING2. PDDL Compilationsused three kinds compilation methods:ADL SIMPLE-ADL (STRIPS conditional effects) STRIPS;PDDL derived predicates PDDL without them;PDDL timed initial literals PDDL without them.consider compilation methods order, explaining, each, compilationworks, main difficulties possible solutions are, giving outlineused compilation competition. Note ADL, SIMPLE-ADL, STRIPS subsetsPDDL. compilation methods published elsewhere already (see citationstext). section serves overview article, since coherent summary techniques,behavior practice, appeared elsewhere literature.2.1 Compilations ADL SIMPLE-ADL STRIPSADL constructs compiled away methods first proposed Gazen Knoblock (1997).Suppose given planning instance constant (object) set C, initial state I, goal G,operator set O. operator precondition pre(o), conditional effects e, taking formcon(e), add(e), del(e) add(e) del(e) lists atoms. Preconditions, effect conditions,G first order logic formulas (effect conditions RU E unconditional effects). Sincedomain discourse set constants finite, formulas equivalently transformed propositional logic.(1) Quantifiers turned conjunctionsVand disjunctions, simply expandingWavailable objects: x : (x) turns cC (c) x : (x) turns cC (c). Iteratequantifiers left.Since STRIPS allows conjunctions positive atoms, transformations necessary.(2) Formulas brought negation normal form: ( ) turns ( )turns . Iterate negation front atoms only.(3) x occurs formula: introduce new predicate not-x; set not-x iffx 6 I; effects e: set not-x add(e) iff x del(e) not-x del(e) iff x add(e);formulas, replace x not-x. Iterate negations left.(4) Transform formulas DNF: (1 2 ) (1 2 ) turns (1 1 ) (1 2 )(2 1 ) (2 2 ). Iterate conjunctions occur disjunctions.operator precondition pre(o) n > 1 disjuncts, create n copies onedisjunct precondition. effect condition con(e) n > 1 disjuncts, create ncopies e one disjunct condition. G n > 1 disjuncts, introducenew fact goal-reached, set G := goal-reached, create n new operators onedisjunct precondition single unconditional effect adding goal-reached.459fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UG(:action move:parameters(?a - airplane ?t - airplanetype ?d1 - direction ?s1 ?s2 - segment ?d2 - direction):precondition(and (has-type ?a ?t) (is-moving ?a) (not (= ?s1 ?s2)) (facing ?a ?d1) (can-move ?s1 ?s2 ?d1)(move-dir ?s1 ?s2 ?d2) (at-segment ?a ?s1)(not (exists (?a1 - airplane) (and (not (= ?a1 ?a)) (blocked ?s2 ?a1))))(forall (?s - segment) (imply (and (is-blocked ?s ?t ?s2 ?d2) (not (= ?s ?s1))) (not (occupied ?s))))):effect(and (occupied ?s2) (blocked ?s2 ?a) (not (occupied ?s1)) (not (at-segment ?a ?s1)) (at-segment ?a ?s2)(when (not (is-blocked ?s1 ?t ?s2 ?d2)) (not (blocked ?s1 ?a)))(when (not (= ?d1 ?d2)) (and (not (facing ?a ?d1)) (facing ?a ?d2)))(forall (?s - segment) (when (is-blocked ?s ?t ?s2 ?d2) (blocked ?s ?a)))(forall (?s - segment) (when(and (is-blocked ?s ?t ?s1 ?d1) (not (= ?s ?s2)) (not (is-blocked ?s ?t ?s2 ?d2)))(not (blocked ?s ?a))))))Figure 1: operator airport ground traffic control.illustrative example, consider operator description Figure 1, taken domainencoding airport ground traffic control. operator moves airplane one airport segmentanother. Consider specifically precondition formula (not (exists (?a1 - airplane) (and (not (=?a1 ?a)) (blocked ?s2 ?a1)))), saying airplane different ?a allowed block segment?s2, segment moving into. Say set airplanes a1 , . . . , . step (1)turn formula (not (or (and (not (= a1 ?a)) (blocked ?s2 a1 )) . . . (and (not (= ?a)) (blocked ?s2)))). Step (2) yields (and (or (= a1 ?a) (not (blocked ?s2 a1 ))) . . . (or (= ?a) (not (blocked ?s2 )))).Step (3) yields (and (or (= a1 ?a) (not-blocked ?s2 a1 )) . . . (or (= ?a) (not-blocked ?s2 ))). Step (4),finally, (naively) transform (or (and (= a1 ?a) . . . (= ?a)) . . . (and (not-blocked ?s2 a1 ). . . (not-blocked ?s2 ))), i.e., mathematically notated:_^x.x{(= a1?a),(not-blocked ?s2 a1 )}...{(= ?a),(not-blocked ?s2 )}words, transforming formula DNF requires enumerating n-vectors atomsvector position selected one two possible atoms regarding airplane ai .yields exponential blow-up DNF 2n disjuncts. DNF split singledisjuncts, one yielding new copy operator.reader noticed exponential blow-up also inherent compilation step(1), quantifier may expanded |C| sub-formulas, k nested quantifiersexpanded |C|k sub-formulas. Obviously, general way around eitherblow-ups, deal complex formulas allowed STRIPS. practice,however, blow-ups typically dealt reasonably well, thanks relative simplicityoperator descriptions, frequent occurrence static predicates, explained shortly.quantifiers arent deeply nested, like Figure 1, blow-up inherent step (1)matter. Transformation DNF often problem like example here. keysuccessful application compilation practice, least far personal experiencegoes, exploitation static predicates. idea described, example, Koehler460fiE NGINEERING B ENCHMARKSP LANNINGHoffmann (2000). Static predicates arent affected operator effect. predicateseasily found, truth value fully determined initial state soon fullyinstantiated. transformation step (4), operator parameters still variables,even knew = (of course) static predicate, would help uswouldnt know ?a is. instantiate ?a, however, then, instantiationoperator, (= ?a1 ?a) atoms trivialize TRUE FALSE, large DNF collapsesVsingle conjunction 6= ?a1 airplane (not-blocked ?s2 ?a1), instantiation?a. Similarly, expansion quantifiers often made much easier first instantiatingoperator parameters, inserting TRUE FALSE static predicate soonparameters grounded. Inserting TRUE FALSE often simplifies formulas significantlyinformation propagated upwards (e.g., disjunction TRUE element becomesTRUE itself).Assuming compilation succeeded thus far, steps (1) (4) processedSTRIPS description conditional effects, i.e., actions still conditional effects con(e),add(e), del(e) con(e) conjunction atoms. subset ADL termedSIMPLE-ADL Fahiem Bacchus, used encoding one versionsElevator domain used IPC-2 (i.e. 2000 competition). choose leavelanguage, necessitating planning algorithm deal conditional effects directly.Several existing planning systems, example FF (Hoffmann & Nebel, 2001) IPP (Koehler,Nebel, Hoffmann, & Dimopoulos, 1997), this. sensible approach since, Nebel (2000)proved, conditional effects cannot compiled STRIPS without either exponential blowup task description, linear increase plan length. One might suspect that, likesteps (1) (4) above, exponential blow-up mostly avoided practice.airport move operator Figure 1 provides example this. effect conditions staticconditional effects disappear completely instantiate parameters anothergood reason instantiation prior compilation. However, conditional effectsdisappear many other, even simple, natural domains. Consider following effect, takenclassical Briefcaseworld domain:(forall (?o) (when (in ?o) (and (at ?o ?to) (not (at ?o ?from)))))effect says object ?o currently briefcase moves along briefcase.Obviously, effect condition static, outcome operator truly dependcontents briefcase. Note forall means actually set (distinct)conditional effects, one object.basically two known methods compile conditional effects away, correspondingtwo options left open Nebels (2000) result. first option enumerate possiblecombinations effect outcomes, preserves plan length cost exponential blow-updescription size exponential number different conditional effects single action.Consider Briefcaseworld operator, say object set o1 , . . . , . everysubset o01 , . . . o0k o1 , . . . , , o0k+1 , . . . , o0n complement subset, get distinctoperator precondition contains of:(in o01 ) . . . (in o0k ) (not-in o0k+1 ) . . . (not-in o0n )effect objects is:(at o01 ?to) . . . (at o0k ?to) (not (at o01 ?from)) . . . (not (at o0k ?from))461fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UGwords, operator applied (only) exactly o01 , . . . o0k briefcase,moves exactly objects. Since (in deterministic planning considered here) neveruncertainty objects inside briefcase not, exactly one newoperators applied whenever original operator applied. compilation methodpreserves size (nodes) form (edges) state space. However, wont abletransformation, planner wont able deal resulting task, n grows beyond, say,maximally 10 . . . 20. Often, real-world operators contain distinct conditional effects that.alternative method, first proposed Nebel (2000), introduce artificial actionsfacts enforce, application normal action, effect-evaluation phaseconditional effects action must tried, whose condition satisfiedmust applied. Briefcaseworld example, would look follows. First,conditional effect gets removed, new fact evaluate-effects inserted add list,new fact normal inserted precondition delete list. 2n new operators,two object oi . One means move-along-oi , means leave-oi . formerin(oi ) precondition, latter not-in(oi ). former (at oi ?to) (not (at oi?from) effect. evaluate-effects precondition, new fact tried-oiadd effect. final new operator stops evaluation, whose preconditionconjunction evaluate-effects tried-o1 , . . . , tried-on , whose add effect normal,whose delete effect evaluate-effects. conditional effects several operators compiledaway method, evaluate-effects tried-oi facts made specificoperator; normal remain single fact used operators. effect k > 1 factscondition, k leave-oi actions must created, negation one factsprecondition.Nebels (2000) method increases plan length number distinct conditional effectsoperators. Note benign are, say, 20 effects. searchprocedure recognizes new constructs do, search space essentially remainscompilation. But, artificial constructs easily decipheredhuman, necessarily true (is likely case) computersearches general-purpose search procedure. example, naive forwardsearch space choice order application conditional effects (whichcould avoided enforcing order yet artificial constructs). Probablyimportantly, standard search heuristics unlikely recognize nature constructs.example, without delete lists suffices achieve tried-o1 , . . . , tried-on once,later apply conditional effects needed.conclude necessary eliminate conditional effects, whenever feasible, onecompile conditional effects away first method, enumerating effect outcomes.IPC-4. took FFs pre-processor, implements transformation steps (1) (4) above,extended code compiles conditional effects away, optionally either two described methods. call resulting tool adl2strips.7 cases domainversion formulated ADL, used adl2strips generate STRIPS formulation domainversion. one case, version power supply restoration, also generated SIMPLE-ADL7. Executables adl2strips downloaded IPC-4 web page http://ipc.icaps-conference.org.also download tool named Ground, based code Mips system (Edelkamp, 2003b), takesfull syntax PDDL2.2 (Hoffmann & Edelkamp, 2005) puts grounded representation (weuse tool IPC-4 since temporal numeric planners pre-processing steps implemented).462fiE NGINEERING B ENCHMARKSP LANNINGformulation. cases one, enumerating effect outcomes feasible. single exception another version power supply restoration forced use Nebels (2000)method. Details process, exceptions use adl2stripsdomain-specific method, described sections individual domains Appendix A.2.2 Compilations Derived Predicatesseveral proposals literature compile derived predicates away, certain restrictions form use rest domain description (Gazen &Knoblock, 1997; Garagnani, 2000). compilation scheme works general proposedThiebaux, Hoffmann, Nebel (2003, 2005). Thiebaux et al. also proved compilation scheme works general not, worst case, involve exponentialblow-up either domain description size length plans. Note exponential refers also increase plan length, description blow-up, unlikecompilation conditional effects discussed above. makes compilation derived predicates rather difficult task. IPC-4, compilation schemes oriented approaches takenGazen Knoblock (1997), Thiebaux et al. (2003, 2005), used. detail below.First, let us explain derived predicates are, compilations work.Derived predicates predicates affected operators, whose truthvalue derived set derivation rules. rules take form (x) P (x).basic intuition that, (x) satisfied instantiation c variable vector x, P (c)concluded. formally, semantics derivation rules defined negationfailure: starting empty extension, instances P (c) derived fixpoint reached;instances lie outside fixpoint assumed FALSE. Consider following example:(:derived (trans ?x ?y) (or (edge ?x ?y ) (exists (?z) (and (edge ?x ?z) (trans ?z ?y)))))derivation rule defines transitive closure edges graph. typicalapplication derived predicates. example, Blocksworld naturally formalizedpredicate; power supply restoration domain, transitive closure models powerflow paths network electric lines. Obviously, pairs ?x ?ytransitively connected appear fixpoint negation failure.Matters become interesting think derived predicates allowed referother, may used rest task description. important distinctionsare: derived predicate appear antecedent derivation rule? derived predicateappear negated antecedent derivation rule? derived predicate appear negatedaction precondition goal?derived predicates appear antecedents derivation rules, merelynon-recursive macros, serving syntactic sugar. One simply replace derived predicatesdefinitions.8 derived predicate P appears negated (negation normal form the)antecedent derivation rule predicate Q, fixpoints P Q computedinterleaved way: extension Q may differ depending order individualinstances derived. Say rule P A(x) P (x), basic predicate, ruleQ P (x) Q(x). Say objects b, current state satisfies (only) A(a).8. derived predicates recursive cycle-free, replaced definitions may incureexponential blow-up.463fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UGComputing derived predicates interleaved way, may derive A(a) P (a), A(b)Q(b), stop; may also derive P (a) Q(a), A(b) Q(b), A(a) P (a).non-monotonic behavior, making non-trivial define extension B is. keepthings simple extensions derived predicates must computed every newworld state Thiebaux et al. (2003, 2005) propose simply order Q P . is, computeP extension first compute Q based that. Generalized, one ends semanticscorresponding stratified logic programs (Apt, Blair, & Walker, 1988). contextIPC-4, i.e., PDDL2.2 (Hoffmann & Edelkamp, 2005), sake simplicity use negatedderived predicates antecedents derivation rules allowed.Whether derived predicates appear negated action preconditions goal makesdifference Gazen Knoblocks (1997) compilation scheme. idea schemesimply replace derivation rules actions. rule (x) P (x) replaced newoperator parameters x, precondition (x) (add) effect P (x). Actions influencetruth value affect atoms mentioned delete instances P . words,new actions allow derivation P , normal action applied may influencevalue P , extension P re-initialized.derived predicates used negated, Gazen Knoblocks (1997) compilationscheme works. However, say P (c) contained action precondition. compiledversion, planner achieve precondition simply applying derivation ruleaction adds P (c). is, planner choice predicate instances derive,course negation failure semantics. reader may pointwonder compile negations away first, thereafter use Gazen Knoblocks(1997) compilation. problem would need inverse derivation rules worknegation failure semantics. clear done. Say, example,want define negated version (trans ?x ?y) predicate above. One would temptedtake negation derivation rule antecedent:(:derived (not-trans ?x ?y) (and (not-edge ?x ?y) (forall (?z) (or (not-edge ?x ?z) (not-trans ?z ?y)))))work, however. Say every node graph least one adjacent edge. Startingempty extension (not-trans ?x ?y), single instantiation derived: givenx edge, z edge x would(not-trans z y) first place.One possible solution difficulties extend Gazen Knoblocks (1997) compilation constructs force planner compute entire extension derived predicatesresuming normal planning. full description this, dealing arbitrary derivation rules,described Thiebaux et al. (2003, 2005). nutshell, compilation works follows. Oneintroduces flags saying one normal fixpoint mode. Normal actions invoke fixpoint mode affect predicates relevant derivation rules. fixpoint mode, actionapplied one conditional effect derivation rule: effect condition true,respective derived predicate instance false, predicate instance added, plusflag changes-made. Another action tests whether fixpoint: changes-madetrue, action resets false; changes-made false, action switches backnormal mode. reduce domain STRIPS, compilation derived predicates,negations conditional effects must compiled away techniques explained earlier.464fiE NGINEERING B ENCHMARKSP LANNINGOne would imagine Thiebaux et al.s (2003, 2005) compilation, making use rather complicated constructs, tends confuse domain independent search techniques. Indeed, Thiebauxet al. (2003, 2005) report even completely naive explicit treatment derived predicatesFF performs lot better, benchmark domains, standard version FF appliedcompiled benchmarks. Gazen Knoblocks (1997) compilation makes use less artificialconstructs, thus preferable whenever safely applied. Note, however, compilations imply potentially exponential blow-up plan length: exponential arity derivedpredicates. worst case every action affects derivation rules, every re-computationextension derived predicates go predicates instantiations.situation, every pair normal actions planner apply order |C|aactions, maximum arity derived predicate. typically smallpower supply restoration domain aware features derived predicatetwo (four, namely) arguments even plan length increase linear numberobjects mean quite significant decrease planner performance.IPC-4 benchmarks, derived predicates occur (only) power supply restoration (Appendix A.4) model checking safety properties (Appendix A.3). latter, derivedpredicates occur negated, Stefan Edelkamp encoded domain version without derived predicates hand, using method along lines one described Gazen Knoblock (1997).power supply restoration, derived predicates occur negated, used variationmethod described Thiebaux et al. (2003, 2005). cases, due increase plan lengthconsidered resulting domain formulation different original formulation directly compared it, terms planner performance. compiled formulations posedcompetitors distinct domain versions, instead alternative domain version formulations.Indeed, expected, planner results IPC-4 much worse compiled encodings.2.3 Compilations Timed Initial LiteralsTimed initial literals literals known become true time points pre-specifiedinitial state. literals compiled durational PDDL relatively easily, costplan length domain description size blowing linearly number timed initialliterals. compilation proposed brought attention Fox, Long, Halsey(2004). idea use wrapper action must applied action,whose duration occurrence time last timed initial literal. planner must also applysequence literal actions achieve timed initial literals order occurrence,durations time intervals occurrences. wrapper actionterminated, literal actions longer applied. planner forced applydirect sequence. suffices encode desired semantics. Consider following example:(:init(at 9 (have-to-work))(at 19 (not (have-to-work)))(at 19 (bar-open))(at 23 (not (bar-open))))encode standard durational PDDL, wrapper be:(:action wrapper465fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UG:parameters ():duration (= ?duration 23):condition(at start (no-wrapper)):effect(and (at start (not (no-wrapper)))(at start (wrapper-started))(at start (wrapper-active))(at start (literal-1-started))(at end (not (wrapper-active)))))Here, no-wrapper ensures one wrapper action executed; wrapper-started insertedprecondition every normal action thus ensures wrapper startedaction executed; wrapper-active precondition literal actions. Precisely,be:(:action literal-1:parameters ():duration (= ?duration 9):condition(and (over (wrapper-active))(over (literal-1-started))):effect(and (at end (not (literal-1-started)))(at end (literal-2-started))(at end (have-to-work))))(:action literal-2:parameters ():duration (= ?duration 10):condition(and (over (wrapper-active))(over (literal-2-started))):effect(and (at end (not (literal-2-started)))(at end (literal-3-started))(at end (not (have-to-work)))(at end (bar-open))))(:action literal-3:parameters ():duration (= ?duration 4):condition(and (over (wrapper-active))(over (literal-3-started))):effect(and (at end (not (literal-3-started)))(at end (not (bar-open)))(at end (literals-done))))466fiE NGINEERING B ENCHMARKSP LANNINGfact literals-done made goal, planner must actually apply literal actions.Note need three actions here, since two timed initial literalslonger work opening bar scheduled occur time. Notealso that, Nebels (2000) compilation conditional effects Thiebaux et al.s (2003,2005) compilation derived predicates, compiled encoding likely confusing domainindependent search methods.Many IPC-4 domains made use timed initial literals (in versions) encodevarious kinds time windows (see Appendix A). compiled domain versions pure(durational) PDDL above, provided resulting encodings additional domain versions.Due increase number actions needed plans, figured compilationconstructs much change direct comparison. Indeed, derived predicates,planner results IPC-4 much worse domain versions compiled way.3. Summary Domainssection provide brief summary IPC-4 domains. domain, provide:short description application; motivation inclusion domain; brief explanationmain simplifications made IPC-4; brief explanation different domain versionsformulations used IPC-4. proceed alphabetical order.3.1 Airportcontact person application domain, Wolfgang Hatzack, workingapplication area several years. domain adapted IPC-4 Jorg HoffmannSebastian TrugApplication. task control ground traffic airport. Timed travel routes mustassigned airplanes reach targets. inbound outbound traffic;former airplanes must take off, latter airplanes landed park.main problem constraint is, course, ensure safety airplanes. means avoidcollisions, also prevent airplanes entering unsafe zones behind large airplanesengines running. optimization criterion minimize summed travel time (onsurface airport) airplanes.9 usually standard routes, i.e., routesairplane must take outbound certain parking area, inbound certain runway.reason introducing routes reduce complexity human ground controllers, sincesignificant computer support yet available real airports. Solving instances optimally (thecorresponding decision problem) PSPACE-hard without standard routes (Helmert, 2006b)NP-complete routes standardized (Hatzack & Nebel, 2001). latter case,pure scheduling problem. former case, complicated unrealistic airport traffic situationslead exponentially long solutions, see Section 4.1.Motivation. main motivation including domain able modelapplication quite accurately, and, particular, generate quite realistic instances. fact,able generate instances based real airport. made possible contactWolfgang Hatzack, completed PhD application (Hatzack, 2002). Apart9. alternative criterion would minimize summed squared delay airplanes. interestairlines; minimizing summed travel time interest airport. Neither two easilymodelled PDDL2.2, discuss Simplifications, below.467fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UGdeveloping domain-specific solutions (Hatzack & Nebel, 2001), developed realistic simulationtool, kindly supplied us purpose generating IPC-4 domain versions testinstances. Sebastian Trug implemented options inside simulator allowed it, pointtime simulation traffic flow, output current traffic situation PDDL format.simulator included real airports Frankfurt, Zurich, Munich. Frankfurt Zurich provedlarge purposes, able devise competition instances based Munich airport.Simplifications. make two simplifications. first amounts discretization space(location) airport, making domain amenable PDDL style discrete actions.continuous space representation, one would need actions continuous choice farmove. discretization loses precision, believe distort natureproblem much. Due amount expected conflicting traffic different pointsairport, high parking positions, relatively easy choose discretizationsegments different length precise small enough time. secondsimplification severe: drop original optimization criterion,awkward express current PDDL. model travel times airplanes, one needs accesstimes plans wait, i.e., nothing.10 aware way expresscurrent PDDL. IPC-4 committee voted introduction additional languageconstruct, look clock, since didnt seem relevant anywhere else. Another optionwould introduce explicit waiting actions, causes lot trouble because, similarcontinuous space, must continuous choice long wait. end, decideddrop criterion now, ask planners optimize standard makespan instead,11corresponding arrival time last airplane (meaning, arrival destinationairport). ideal, reasonable optimization criterion. planning system participatingIPC-4, single exception LPG-td (Gerevini, Saetti, & Serina, 2006), able takeaccount general optimization criteria built-in ones (like makespan). usefull standard routes, thus allowing airplanes choice move. use standardsroutes, particularly regions near runways large airports. one thing, servedkeep large airports manageable PDDL encoding planners; another thing, seemsgood compromise exploiting capabilities computers time remainingclose existing practice.Versions Formulations. generated four versions airport domain: non-temporalone; temporal one; temporal one time windows, fact planes landfuture block certain runways modeled using timed initial literals; latter version,timed initial literals compiled away. versions, constraints ensuring airplane safetymodelled ADL logical formulas. compilation partially grounded STRIPSprovides, version, alternative formulation: domain version one ADL formulationone STRIPS formulation.3.2 PipesworldFrederico Liporace working application area several years; submitted paperearly domain version workshop competition ICAPS03. domainadapted IPC-4 Frederico Liporace Jorg Hoffmann.10. difficulty arises modelling delay, one must also compute travel times.11. Makespan, Planning, means amount time start plan last action stops executing.468fiE NGINEERING B ENCHMARKSP LANNINGApplication. task control flow different oil derivatives pipelinenetwork, certain product amounts transported destinations. Pipeline networksgraphs consisting areas (nodes) pipes (edges), pipes differ length.available actions pump liquid ends pipes, effect liquid endpipe gets ejected. application rich additional constraints, like, constraintstypes products may interface within pipe, restricted tankage space areas, deadlinesarrival products.Motivation. main motivation including domain original structure. one insertssomething pipe one end, something possibly completely different comes pipeend. way, changing position one object directly results changingposition several objects namely, objects inside affected pipeline.case transportation domain aware of, fact reminiscent complicatedsingle-player games Rubiks Cube. Indeed, strong interaction objects leadseveral subtle phenomena. example, instances solution must pump liquidring pipeline segments cyclic fashion.Simplifications. severely simplify domain order able solve reasonablycomplex instances current planners. importantly, encoding heavily based assuming smallest indivisible unit liquid, batch. Every amount liquid encoding modelledterms number batches. capture continuous nature real application, meansone choose batch size trade-off encoding size accuracy. trade-offless well-behaved one Airport (choosing segments sizes) since unit size cannotmade flexible: every batch may pass every pipeline, smallest batch governsdiscretization pipelines. contrast Airport, segments may vary size.another important simplification, used personalized goals, i.e. goals referred specificbatch objects rather product amounts. serves avoid large disjunctions enumeratingpossible combinations individual batches. simplifications quite severe indeedseems unlikely realistic representation Pipesworld, particular real-valued product amounts instead batches, could solved efficiently planners without introducingspecialized language constructs sort queue data structure PDDL, see Appendix A.2.5.Versions Formulations. created six different versions Pipesworld: four versions /without temporal actions, with/without tankage restrictions, respectively; one temporal versionwithout tankage restrictions arrival deadlines goal batches; one version identicallast one except timed initial literals compiled away.3.3 Promeladomain created IPC-4 Stefan Edelkamp.Application. task validate properties systems communicating processes (oftencommunication protocols), encoded Promela language. Promela (PROcess MEta LAnguage)input language model checker SPIN (Holzmann, 2003). language loosely basedDijkstras guarded command language, borrowing notation Hoares CSP language.One important property check detect deadlock states, none processes applytransition. example, process may blocked trying read data emptycommunication channel. Edelkamp (2003a) developed automatic translation PromelaPDDL, extended generate competition examples.469fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UGMotivation. main motivation including domain promote make visible important connection Planning Model Checking. Model Checking (Clarke,Grumberg, & Peled, 1999) automated formal method basically consists threephases: modeling, specification checking. first two phases system correctness specification modeled using formalism. last step automatically checksmodel satisfies specification. Roughly speaking, step analyzes state space modelcheck validity specification. Especially concurrent systems, several componentsinteract, state spaces grow exponentially size components system. twomain research branches model checking: explicit-state model checking, implemented SPIN,exploits automata theory stores explored state individually, symbolic model checkingdescribes sets states properties using binary decision diagrams (BDDs) efficientrepresentations Boolean formulas.Checking validity reachability property, property asks system state certain property reachable, similar question plan existence. use model checking approaches solve planning problems explored depth, e.g. Cimatti, Roveri,Traverso (1998), Bertoli, Cimatti, Roveri, Traverso (2001), Lago, Pistore, Traverso(2002), Kvarnstrom, Doherty, Haslum (2000), Bacchus Kabanza (2000), HolldoblerStor (2000), Fourman (2000), Edelkamp (2003b), Dierks (2005), Kabanza Thiebaux (2005).However, much done inverse direction, applying planners model checkingproblems. Running IPC-4 planners planning encodings Promela specifications first stepthat.Promela domain also contributes unusual structural properties domain set; computational complexity local search topology quite different discussed Section 4.Simplifications. main simplification make use simple example classescommunicating processes. PDDL models refer fixed-length state vectors, couldinclude process construction calls. therefore considered active processes, i.e., processescalled initialization time. PDDL also support temporally extendedgoals, consider reachability properties only. Moreover, prototypical naturelanguage compiler, many features Promela rendezvous communication supported. Although limited support shared variables, competition chosesimple message passing protocols only; experimented reachability properties, PDDL goals competition event deadlock detection only. Concretely,IPC-4 instances come two toy examples used area Model-Checking: well-knownDining Philosophers problem, Optical Telegraph problem viewedversion Dining Philosophers philosophers complex inner life, exchanging datatwo hands (each separate process). both, goal reach deadlockstate.Versions Formulations. created eight different versions domain. differPromela example class encoded (two options), whether use numeric variablesencoding, whether use derived predicates encoding. four encodingsPromela example class semantically equivalent sense 1-to-1 correspondence plans. decided make different versions, rather formulations,derived predicates make large difference plan length, numeric variables makelarge difference applicability planning algorithms/systems. translation Promela470fiE NGINEERING B ENCHMARKSP LANNINGPDDL makes use ADL constructs, domain version contains one ADL formulationone (fully grounded) compiled STRIPS formulation.3.4 PSRSylvie Thiebaux others worked application domain. domain adaptedIPC-4 Sylvie Thiebaux Jorg Hoffmann.Application. task PSR (power supply restoration) reconfigure faulty power distribution network resupply customers affected faults. network consists electriclines connected switches fed via number power sources equipped circuitbreakers. faults occur, circuit-breakers sources feeding faulty lines openprotect network, leaving lines also many healthy ones un-supplied. network needs reconfigured opening closing switches circuit-breakers wayresupply healthy portions. Unreliable fault sensors switches lead uncertaintystate network. Furthermore, breakdown costs depend various parameters needoptimized constraints capacity sources lines. application topic ongoing interest field power distribution, investigated AI communitylong time, including AI planning standpoint (Thiebaux, Cordier, Jehl, & Krivine, 1996;Thiebaux & Cordier, 2001; Bertoli, Cimatti, Slaney, & Thiebaux, 2002; Bonet & Thiebaux, 2003).Motivation. motivation including PSR twofold. First, well-researched interestingapplication domain. Second, original structure rarely found previous benchmarks.natural encoding models power propagation using recursive derived predicates compute transitive closure connectivity relation network. contrastplanning benchmarks, number actions needed optimal plan necessarily growinstance size: available actions alter position switches, even largenetwork altering position switches may suffice reconfiguration. difficultquestion answer is, switches.Simplifications. Three major simplifications made. First, deterministic planningassume network state fully observable, i.e., initial state descriptioncomplete, actions always succeed. Second, ignored numerical optimizationaspects PSR. Third, used personalized goals sense lines supplied namedexplicitly goal. Note that, even simplified form, domain exhibits structureexplained above.Versions Formulations. created four domain versions, differing primarily sizeavailable formulations. natural domain formulation ADL derived predicates.Though experimented many combinations PDDL encodings compilation strategies,size instances could compile simpler languages quite restricted. Precisely,versions are: large version ADL plus derived predicates; middle versioncould devise also SIMPLE-ADL plus derived predicates STRIPS plus derived predicates;middle-compiled version ADL, identical middle version except derivedpredicates compiled away; small version pure STRIPS. instances latterdomain version particularly small, since extremely difficult comeencoding pure STRIPS either yield prohibitively long plans, prohibitively largePDDL descriptions. fact, obtain small version applied pre-computation step (Bertoliet al., 2002) obviates need reasoning power propagation and, consequently,471fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UGneed derived predicates. resulting tasks, opening closing switch directly withoutdetour power propagation affects parts network. Thus planner longer needscompute flow power network, left issue configureflow.3.5 Satellitedomain introduced Long Fox (2003) IPC-3; adapted IPC-4 JorgHoffmann. domain comes NASA space application, satellites take imagesspatial phenomena. motivation inclusion IPC-4 domain applicationoriented similar sense new domains. Also, wanted immediate comparison performance achieved IPC-3, achieved IPC-4. top 5 domainversions used IPC-3, added 4 new versions, introducing additional time windows (formulatedalternatively timed initial literals compilation) sending data earth.3.6 Settlersdomain also introduced Long Fox (2003) IPC-3. task buildinfrastructure unsettled area, involving building housing, railway tracks, sawmills, etc.distinguishing feature domain domain semantics encoded numeric variables. makes domain important benchmark numeric planning.reason, IPC-3 participant could solve smallest instances, includeddomain IPC-4. modification made except compiled away universallyquantified preconditions order improve accessibility.3.7 UMTSRoman Englert working application area several years. domain adaptedIPC-4 Stefan Edelkamp Roman Englert.Application. third generation mobile communication, so-called UMTS (Holma &Toskala, 2000), makes available broad variety applications mobile terminals.comes challenge maintain several applications one terminal. First, due limited resources, radio bearers restrictions quality service (QoS) applications. Second,cell setup execution several mobile applications may lead unacceptable waiting periodsuser. Third, QoS may insufficient call setup case executionmobile application shut down. Thus arises call setup problem several mobile applications. main requirement is, course, setup minimum possible amounttime. (pure) scheduling problem necessitates ordering optimizing executionmodules needed setup. many scheduling problems, finding some, necessarilyoptimal, solution trivial; main challenge find good-quality solutions, optimal ones ideally.Motivation. main motivation modelling pure scheduling problem planning domainstrong industrial need flexible solution procedures UMTS call setup,due rapidly evolving nature domain, particularly sorts mobile applicationsavailable. ideal solution would put automatic planner mobile device,let compute optimized schedules on-the-fly. sense, UMTS call setupnatural promising field real-world application automatic planners. also interesting472fiE NGINEERING B ENCHMARKSP LANNINGsense scheduling problems far central competitive AI planning,domain serves advertise usefulness PDDL addressing certain kinds schedulingproblems.Simplifications. setup model chose considers coarse parts network environmentpresent UMTS applications invoked. Action duration fixed rather computedbased network traffic. inter-operational restrictions different concurrent devicesalso neglected. considered plausible timings instances rather real-applicationdata running certain applications UMTS device. designed domain10 applications single device. challenge optimal planners computing minimummakespan solutions, much challenge satisficing planners.Versions Formulations. created six domain versions; arise two groupsthree versions each. first group, standard UMTS domain, comes without timingconstraints. latter represented either using timed initial literals, compilation;before, separated two options different domain versions (rather domain versionformulations) due increase plan size. second group domain versions similarstructure. difference three domain versions includes additional flawaction. single step, action achieves one needed fact, where, normally, several stepsrequired. However, action useless reality deletes another fact needed,cannot re-achieved. flaw action added see happens intentionallystressed planners: beside increasing branching factor, flaw action look usefulperspective heuristic function ignores delete lists.4. Known (Theoretical) Results Domain Structuresection, start structural analysis IPC-4 domains summarizing knownresults literature. Helmert (2006b) analyzes domains perspective domainspecific computational complexity. Hoffmann (2005) analyzes domains used IPCs far,plus standard benchmarks literature, identifying topological properties searchspace surface relaxed plan heuristic introduced FF system (Hoffmann& Nebel, 2001), variants used many modern planning systems. studiesexclusively concerned purely propositional non-temporal STRIPS ADL planning.follows, domain names refer respective (non-temporal) domain versions.124.1 Computational ComplexityHelmert (2006b) studied complexity plan existence bounded plan existenceIPC-4 benchmark problems. Plan existence asks whether given planning task solvable. Boundedplan existence asks whether given planning task solvable given numberactions. Helmert established following results.Airport, plan existence bounded plan existence PSPACE-complete, evenaircraft inbound need taxi park goal location, map planarsymmetric, safety constraints simply prevent planes occupying adjacent segments.12. UMTS domain, temporal versions, treated either studies. computationalcomplexity, easy see deciding plan existence P deciding bounded plan existence (optimizingmakespan) NP-complete UMTS. Topological properties relaxed plan heuristic havent yet definedtemporal setting.473fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UGproof reduction Sliding Tokens puzzle, set tokens must reach goalassignment vertices graph, moving adjacent vertices ensuring twotokens ever find adjacent vertices. length optimal sequential plansexponential number tokens, likewise airport domain. Even parallel plansshorter linear amount, since plane move per time step. proofSliding Tokens puzzle quite complicated involves construction instancesexponentially long optimal plans. one would expect, constructions usedunlikely occur real airport; particular true necessary density conflictingtraffic graph structure. consider interesting since makes Airport benchmarkextremely high worst-case complexity, much good-natured typical casebehavior. Typically, ample space airport (comparatively) airplanes movingacross it.Pipesworld, whether without tankage, plan existence bounded plan existence NP-hard. unknown whether NP, however. NP-hardness proofreduction SAT four literals per clause variable occurs3 clauses. SAT instance reduced network way parts network (variable subnetworks) represent choice assignment variables, parts(clause subnetworks) represent satisfaction clauses. content areas pipesinitialized batches way interface restrictions guarantee goal areareached certain batch clause subnetwork iff clause satisfied assignment.general Promela planning, defined Edelkamp (2003a), plan existence boundedplan existence PSPACE-complete. PSPACE-hardness proof reduction halting problem space-restricted Turing Machines (TM). cells machines tapemapped onto process queue unit capacity, states TM form set Promelamessages, TMs alphabet form set Promela states processes, Promela transitions encode TMs transitions. shown TM halts iff Promela task reachesdeadlock.Dining Philosophers, hand, particular structure one process perphilosopher, transition graph. Optimal plans generated linear timenumber philosophers making constant number transitions reach known stategraphs. Similar considerations apply Optical Telegraph.PSR tasks also solved optimally polynomial time, requires rather complexalgorithm. plans start wait action opens circuit-breakers affected fault.simplest form, optimal plans follow prescribing series actions opening switchesconnecting feedable line faulty one. necessary also sufficient ensurenetwork safe state faulty line re-supplied. minimal set devices(disjoint previous one) must closed resupply rest network.achieved generating minimal spanning tree healthy part network,done polynomial time.Figure 2 gives overview results summarizes Helmerts (2003) resultsstandard benchmarks. domain set displayed set investigated Hoffmann (2005),minor differences explained shortly. Blocksworld-no-arm, Briefcaseworld, Ferry, Fridge,Simple-TSP, Tireworld traditional planning benchmarks never used IPC.1313. Blocksworld-no-arm version Blocksworld blocks moved directly destination, withoutreferring robot arm. Simple-TSP used (Fox & Long, 1999) demonstrate potential symmetry474fiFORP LANNINGPSPACEE NGINEERING B ENCHMARKSPromelaAirportPPlan ExistenceNPPipesworldMysteryMprimeMiconicADLFreecellTireworldSimpleTSPSchedulePSROpticalTelegraphMovieGripperFridgeFerryDiningPhil.ZenotravelSatelliteRoversMiconicSTRIPSMiconicSIMPLELogisticsGridDriverlogDepotsBriefcaseworldBlocksworldnoarmBlocksworldarmPNPPSPACEBounded Plan ExistenceFigure 2: overview Helmerts results computational complexity benchmarks.IPC-1 benchmarks Assembly, Grid, Gripper, Logistics, Movie, Mprime, Mystery.IPC-2 benchmarks Blocksworld-arm, Freecell, Logistics, Miconic-ADL, Miconic-SIMPLE,Miconic-STRIPS (Miconic Schindler Lifts name elevator domain), Schedule.IPC-3 benchmarks Depots, Driverlog, Freecell, Rovers, Satellite, Zenotravel. IPC-4benchmarks displayed bold face, including (hypothetical) general Promela domain.table Figure 2 organized along two axes, x axis shows complexitydeciding bounded plan existence, axis shows complexity deciding (unbounded) planexistence. Membership table entry means, NP PSPACE rows columns,respective problem complete respective complexity class. exception Pipesworlddomain, which, stated above, still unknown whether two decision problems alsomembers NP. Assembly domain displayed since, there, Helmert (2003) provedexistence exponentially long optimal plans, showing plan generation quite harddomain. table sectors diagonal crossed unbounded plan existencepolynomially reduced bounded plan existence set bound 2n , nnumber distinct actions, or, ADL, number distinct conditional effects.striking new feature IPC-4 introduction PSPACE-complete benchmarkdomains, filling top right corner Figure 2. Thus, benchmarks cover four inhabitedsectors table. previous IPCs, IPC-1 IPC-2 cover three sectors inhabiteddetection. One simply visit n nodes, using move action applied two nodes,permutation nodes optimal tour. Hoffmann (2005) also investigates Towers Hanoi domain.475fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UGsectors except top right corner IPC-3 benchmarks cover two sectors namely,bounded plan existence NP-complete domains, domains except Freecellpolynomial time algorithm deciding unbounded plan existence.IPC-4 benchmarks exceptional aspects visible Figure 2. particularly, explained above, polynomial decision algorithm PSR highly non-obvious.benchmarks important since, one hand, principle allow planners provide efficient solutions, while, hand, necessitating employ interesting techniquesso.14 Schedule polynomial benchmark bounded plan generationrequires non-obvious algorithm. 20 domains left bottom middle bottomsectors table, polynomial algorithms deciding bounded unbounded plan existencecompletely trivial, mostly addressing one subgoal time.pointed already, final exception lies extraordinarily large differenceworst-case typical-case behavior Airport. see Section 5, even fully automatedmethods (the IPC-4 planners) are, least unbounded plan existence (generation), quite efficienttypical instances domain. large differences worst-case typical-casebehavior unusual, believe extent phenomenon Airport really unusual.example, planners tend find PSR much harder Airport.4.2 Topology h+Hoffmann (2005) considers state spaces (the forward search spaces) STRIPS ADL taskstaken standard benchmark domains. defines, given task world state s, h+ (s)length shortest possible relaxed plan, relaxed plan. relaxed planplan achieves goal one assumes delete lists empty. Computingh+ (the corresponding decision problem) NP-hard (Bylander, 1994). Many modern planners,e.g., HSP (Bonet & Geffner, 2001), FF (Hoffmann & Nebel, 2001), SGPlan (Wah & Chen, 2004;Chen, Hsu, & Wah, 2004), YAHSP (Vidal, 2004), Fast-Diagonally-Downward (Helmert, 2004,2006a), interpreted sort heuristic search approximation h+ , plustechniques like problem decomposition (Wah & Chen, 2004), lookahead techniques (Vidal,2004), additional different heuristic functions (Helmert, 2004). context, questiongreat practical interest quality underlying heuristic function addressed domains.Heuristic quality measured terms topological properties search space surface:many local minima there? large they? flat regions? Hoffmann (2005)investigates questions h+ function, topological properties search spacesurface proven.Hoffmann defines topological phenomena following Frank, Cheeseman, Stutz (1997).identifies several parameters show particularly interesting behavior planning benchmarks.dead end world state reachable initial state goal state cannotreached. unrecognized dead end dead end h+ (s) < . exit distancestate length shortest path state space leading state s0 ,h+ (s) = h+ (s0 ), s0 direct neighbor state s00 h+ (s00 ) < h+ (s0 ). is,exit distance number steps need go order find better state (s00 ),14. Helmerts (2005) words: think domains solved polynomial time polynomialalgorithms obvious extraordinarily interesting. Deterministic PSR definitely domain kindregard optimization. NP-hard problems cannot solved without strong reliance search, polynomialproblems can, planners capture important concepts.476fiE NGINEERING B ENCHMARKSP LANNINGminus 1 since distance s0 measured. Here, s0 plays role exit state usedFrank et al. (1997). state lies local minimum paths exit temporary increaseheuristic value; otherwise state lies bench. maximal local minimum exit distance(mlmed), state space, maximum exit distances states lying local minimastate space. Similarly, maximal bench exit distance (mbed) maximum exitdistances states lying benches. core results Hoffmanns (2005) investigationdisplayed Figure 3.BlocksworldarmDepotsDriverlogPipesworldPSRRoversOpticalTelegraphMysteryMprimeMiconicADLFreecellAssemblyAirportmbed <= cmlmed <= cHanoi [0]Blocksworldnoarm [0]Fridge [0]Briefcaseworld [0]Grid [0]Logistics [0,1]Ferry [0,1]Gripper [0,1]undirectedTireworld [0,6]Satellite [4,4]Zenotravel [2,2]MiconicSIMPLE [0,1]MiconicSTRIPS [0,1]Movie [0,1]SimpleTSP [0,0]harmlessDiningPhil. [31,31]Schedule [5,5]recognizedunrecognizedFigure 3: overview Hoffmanns results topology h+ benchmarks.x-axis Figure 3 corresponds properties regarding dead ends. y-axis correspondsproperties regarding exit distance local minima benches. domains assignedappropriate table sectors classes domains depending worst-case behavior possiblethem. detail, meaning table following. state space undirectedevery transition (action) directly inverted; state space harmless inversionpossible, dead ends anyway; recognized means dead ends,h+ them; unrecognized means least one unrecognized dead end.domain falls class worst-case instance: example, single instance whosestate space contains single unrecognized dead end, domain considered unrecognized.results proved, i.e., domain is, example, considered harmless, meansprovably instance domain contains dead ends.y-axis Figure 3, distinction lines correspond existence non-existenceconstant upper bounds maximal local minimum exit distance (upper line) maximalbench exit distance (lower line). Note constant upper bounds maximal local minimumexit distance exist domains upper line domains lower line,bounds exist.15 constant, meant bound valid every instance15. presentation assumes domains bounded bench exit distance subset boundedlocal minimum exit distance. true general, hold considered benchmark domains.477fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UGdomain, regardless size. actual bounds proved displayed brackets; local minimumbound precedes bench bound cases both. right bottom part tablecrossed since unrecognized dead ends infinite exit distance domain classesempty.16obvious intuition behind Figure 3 transition easy hardplanning systems based heuristic search approximating h+ one moves left bottomside top right side table. Indeed, table does, sense, coincide wellempirical behavior of, least, FF system. Note extreme topological behaviormany domains. upper bound local minimum exit distance 0 meanslocal minima all. case 13 30 investigated domains. severaldomains, widely used Logistics benchmark, top single step suffices reachexit benches. Hoffmann (2005) shows FF would polynomial bottom classestable, provided oracle computing h+ .Considering table perspective benchmark development, one notices particularly older benchmarks tend lie left bottom side; consider example Ferry, Briefcaseworld, Fridge, Simple-TSP, Tireworld. distribution IPC-1 benchmarks Gripper,Logistics, Movie, Grid, Assembly, Mystery, Mprime somewhat extreme: first fourlist belong simple classes, last three belong hardest class (until today,Mystery Mprime domains amongst causing planners trouble).IPC-2 benchmarks Logistics, Blocksworld-arm, Miconic-STRIPS, Miconic-SIMPLE, Schedule,Freecell, Miconic-ADL again, many simple challenging domains.notable exceptions respect Blocksworld-arm, left top side table,Schedule, contain dead ends local minima. IPC-3 benchmarks, distribution starts get varied. domains Zenotravel, Satellite, Depots, Driverlog, Rovers,Freecell span three four top classes table, plus one bottom classes.IPC-4 domains, shown bold face, obviously continue development. twosharing class Pipesworld PSR.17 continue emphasis spanning top classestable; new domain one bottom classes Dining Philosophers,highly exceptional exceedingly large bound, making bound practically uselessexploitation planning.18 Satellite domain adopted IPC-3 benchmarks servesrepresent (a interesting instance of) easier classes. Note Satellite simpletalking STRIPS version, drops challenging problem constraints formulated numeric variables. Airport domain exceptional top right classthat, again, worst-case place Figure 3 differs lot typical case. deadend Airport situation two airplanes completely block others paths.19 course,practical airports designed way doesnt usually happen. mentioned earlier,usually non-overlapping, far possible standard routes, placeblocking occur densely populated areas near parking positions.16. One could skip unrecognized dead ends definition maximum exit distances, Hoffmann (2005)argues un-intuitive, plus making things unnecessarily complicated.17. Actually, Pipesworld invertible sense every two-step sequence (starting ending pumping operation)directly undone. considered harmless since single actions cannot inverted.18. Indeed, h+ bad heuristic Dining Philosophers. basically comes counting numberunsatisfied goals.19. relaxed plan use free space planes make move across other.478fiE NGINEERING B ENCHMARKSP LANNING5. New (Empirical) Results Domain Structureprovide empirical analysis various structural parameters IPC-4 domains.sake readability conciseness, focus non-temporal domain versions only.types data measure, results temporal domain versions quite similar.extent, visible tables showing numbers actions facts, domain versions,individual domain descriptions Appendix A.empirical analysis aimed highlighting characteristics of, differences between, IPC-4 domains. Apart focussing practical parameters, analysiscompared theoretical results cited previous section big advantage tells ussomething actual instances run competition. Note choice instancesmake huge difference example, stated earlier, real-world airport likelyexponentially long plans, neither likely provoke many dead-end situations. possible all, instances used IPC-4 chosen relatively realistic (details Appendix A).analysis structured three sub-sections. Section 5.1 shows how, individualdomains, size grounded encoding grows instance size. Section 5.2 assessescorrespondence quality standard heuristic functions, runtime achievedIPC-4. Section 5.3, finally, assesses fact connectivity instance size, meaning numberchoices one achieve fact, number actions fact required for.5.1 Encoding Sizecurrent STRIPS ADL planners, far authors aware, ground parametersvariables pre-process, ending task representation consisting ground factsground actions. obvious question ask large grounded encodings are. Figure 4shows data, numbers facts actions plotted instance size (selected versions of)different domains. numbers measured using FFs pre-processor. filters static factsfacts added deleted action unreachable actions, meaning actionsappear relaxed planning graph (a planning graph without mutex reasoning) initialstate (Hoffmann & Nebel, 2001); formulas compiled simple STRIPS-like conjunctionsfacts, along lines Gazen Knoblock (1997) outlined Section 2.1000001e+06AirportPipesworldDining PhilosophersOptical TelegraphPSR smallPSR largeSatelliteUMTS10000AirportPipesworldDining PhilosophersOptical TelegraphPSR smallPSR largeSatelliteUMTS100000Nr. ActionsNr. Facts1000010001000100100101051015202530Nr. Instance3540455051015202530Nr. Instance35404550(a)(b)Figure 4: Numbers (a) ground facts (b) ground actions, plotted instance number,selected versions IPC-4 domains.479fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UGcases except UMTS (that temporal versions), domain version selectedFigure 4 non-temporal. Let us consider domains one one. Airport, onenon-temporal version. plots Figure 4 (a) (b) show us quite nicely instancesscaled, sharp drops curves corresponding steps new underlying airport. Precisely,instances 1 3, 4 9, 10 20, 21 35, 36 50 based growing airports, respectively,within airport number travelling airplanes grows 1 2 15 (ininstance 50). example, instance 35 instance 36 step one half Munich airport,12 airplanes, full Munich airport, 2 airplanes.Pipesworld, two non-temporal versions, without tankage restrictions. Figure 4 shows data former, challenging one (the IPC-4 planners fared muchworse it); without tankage restrictions, slightly fewer facts, factor 510 fewer actions. Pipesworld instances scaled similar way Airport ones: fivegrowing pipeline networks feature growing number travelling liquid batches. networks underlie instances 1 10, 11 20, 21 30, 31 40, 41 50, respectively.Corresponding drops observed stepping instance 30 31, and, less significantly,stepping 20 21 40 41. major difference Airport visiblemuch crippled nature (featuring much variance) curve number actions.because, Airport, objects move big spacious structure, while, Pipesworld,many objects move within rather dense space.20 fundamental difference AirportPipesworld also manifests order curves reversed numbers factsactions: Airport, extraordinarily many facts required describe huge airport structure,Pipesworld fewer facts smaller structure, many actions describingthings move along structure. stated earlier, Pipesworld, different objects affectothers position moving.Promela domains, Dining Philosophers Optical Telegraph, data domainversions without derived predicates identical, derivation rule deriving factcounted action achieving fact. main difference seen liesextremely smooth scaling. domains single size parameter, numbersground facts actions grow linear functions parameter functions Optical Telegraph order magnitude higher Dining Philosophers. curvesOptical Telegraph stop instance 17 able compute groundedrepresentation much time memory needed simplification precondition formulas. Note artifact data presentation, rather constitutes seriouslimitation planner tries perform pre-processing.PSR, interesting domain versions small, since could formulatedSTRIPS, large, since goes instances realistic size (in largest instances,is). name small suggests, numbers quite small able compile STRIPS,indicated earlier make instances small.21 Essentially compilationproblem also visible curves large, huge number ground facts actionsrelatively early instances already. curves stop instance 20 beyond that, simplifying20. much objects cannot move affects also number ground actions due mentioned filteringunreachable actions.21. notable exception instance nr. 25, number actions peaks 9400. due exceedingly complex goal formula, 9216 disjuncts DNF, yields extra goal-achievement action,c.f. Section 2.480fiE NGINEERING B ENCHMARKSP LANNINGformulas becomes extremely costly. versions, note high degree variancenumbers facts actions, somewhat corresponds huge degree varianceobserved planner performance domain (see Figure 8). Part variance, leastpace oscillations amplitude, explained way instancesscaled. given number sources (the instance size), generated instances increasingminimal number switches originally fed given source, given number switches,generated instances increasing percentage faulty lines ranging 10% 70%.Intuitively, larger number switches per source, larger harder expect instancebe. Furthermore, percentage faulty lines tends induce easy-hard-easy pattern.lines faulty, small part network resupplied devices needswitched. Similarly, faulty lines exist, network resuppliedswitching operations. intermediate percentage, effects actions becomecomplex conditioned positions many switches instancesbecome critically constrained harder solve.Satellite, main observation made extremely steep ascent curvesinstance 20, particularly growth extremely high numbers actions. two reasonsthis. First, one action Satellite (take-image) 4 parameters reachable almostcombination objects correct types (most time, actions 2 3 parameters).Second, size instances grows sharply beyond instance 20 which, simply,instances 21 36, used IPC-4, correspond 16 instances posed IPC-3challenge hand-tailored planners.consider Settlers ease readability graphs, since domainquite obviously exceptional anyway, relies almost completely numeric variables.UMTS, Figure 4 shows data plain domain version without time windows flaw action.obvious characteristic numbers facts actions constants. truedomain versions, numbers vary slightly. reason that, way UMTS instancesscaled, every instance describes applications requirements; changes (only)goal, specifying applications actually need set up. Independent effectparticular scaling method used, observe numbers facts actions relativelylow around 100 even largest instances, applications must set up,plans contain actions.5.2 Quality Heuristics, Runtimesection, measure length best (sequential parallel) plans foundplanner, (sequential parallel) plan length estimates returned common heuristicfunctions, runtime taken planners. Precisely, optimal planners, measure:optimal makespan, found IPC-4 parallel optimal planners (planners optimizingmakespan).length standard plan graph (Blum & Furst, 1997), i.e., index first plangraph layer contains goals without mutexes.best runtime taken parallel optimal planner IPC-4.optimal sequential plan length, found IPC-4 sequential optimal planners.481fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UGlength serialized plan graph, pair non-NOOP actions made mutex.best runtime taken sequential optimal planner IPC-4.satisficing planners, measure:best (shortest) plan length, found planner IPC-4.length relaxed plan initial state (an action sequence solves task oneassumes delete lists empty; computed FF (Hoffmann & Nebel, 2001)).best runtime taken satisficing planner IPC-4.main goal identify characteristic behavior domains, identify characteristiceffects heuristic quality performance. reader note that, selection measurements, make several simplifying assumptions. Optimal planners exclusively basedplan graph estimates. Satisficing planners exclusively based relaxed plan estimates. Further, satisficing planners minimize makespan, sequential plan length. chosetake account latter since potentially over-estimating (non-admissible) heuristicspecifically estimating parallel plan length; best knowledge, satisficing plannersminimizing makespan actually use heuristic estimating number remaining actions, employ method greedily arrange chosen actions parallel plan. said,wish imply simplifying assumptions safe sense lose importantinformation. simplifying assumptions necessary make analysis presentationfeasible. data show definitely capture many crucial aspects IPC-4 heuristic qualityplanner runtime. show data individual domains, proceeding alphabetical order.(IPC-4) runtime results obtained Linux machine running two Pentium-4 CPUs 3GHz,6 GB main memory; time memory cutoffs 30 minutes 1 GB, per instance.Consider Figure 5, showing data Airport domain. Note axis two differentmeanings, runtime left hand side, number (parallel sequential) plan stepsright hand side. applies figures sub-section. Airport, observeclear correlation quality plan length estimation, runtime. optimal parallelplanners, Figure 5 (a), best observed instances nr. 15 20. There, differencemakespan estimate plan graph grows, grows achieved runtime,exponential scale. may look like counter example that, instance nr. 20, plangraph estimate exact (coincides real makespan), runtime get lower again.Note however, instance 20 based much larger airport previous instances.instance 20 onwards, instances solved parallel planner exact plan graphestimate. optimal sequential planners, Figure 5 (b), get similar behaviorinstances nr. 14 18. behavior also strong instances nr. 35 36: planlength grows lot 35 36, serial plan graph becomes little shorter; correspondingly,runtime goes two orders magnitude. true instances 20 21.satisficing planners, Figure 5 (c), striking observation lengthreal plan coincides, instances, exactly length relaxed plan (for respectiveinitial state). actually quite easy explain: optimal plan moves airplanes waynever block paths; plan optimal even ignoring delete lists.Moving airplanes without blocking always possible start. situation changes482fiE NGINEERING B ENCHMARKS100070P LANNING10000160Optimal MakeSpanPlanGraphBest Parallel RuntimeOptimal NrActionsSerialPlanGraphBest Sequential Runtime140601000100120503011080Nr. Steps40Runtime (sec.)10010Nr. StepsRuntime (sec.)10060120400.10.1100.01200510152025303540450.01050510152025Nr. InstanceNr. Instance(a)(b)10003035404550700Best NrActionsRelaxedPlanBest Runtime600100104003001Nr. StepsRuntime (sec.)5002000.11000.0105101520253035404550Nr. Instance(c)Figure 5: Airport domain. Plots (parallel) plan length, heuristic estimation, runtime,(a) optimal parallel planners, (b) optimal sequential planners, (c) satisficing planners.wrong decision made, additional moves become necessary reality,without delete lists avoid blocking situation. Apart this, Figure 5 shows quite nicelyruntime taken corresponds closely length plan found. Note latterhuge, 694 largest instance.Pipesworld domain, two non-temporal domain versions: with/without tankagerestrictions, i.e., restrictions amount liquid stored network areas.Figure 6 shows data version without restrictions; observations madedomain version similar, except sorts planners scale much worse, thus providingus less data. optimal planners, Figure 6 (a) (b), striking differenceAirport domain Figure 5 (a) (b) quality even parallel plan graph heuristicbad: underestimates real makespan much larger extent Airport.underestimation grows instance size, and, naturally, runtime grows well. Noteplanners fail scale much earlier Figure 5 (a) (b). one slight exceptionrule poorer heuristic estimate leads longer runtime: instance number 10 11,optimal sequential plan length grows 19 20, length serial plan graph remains 9,runtime drops 1400 150 secs.483fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UG1000161000020Optimal MakeSpanPlanGraphBest Parallel RuntimeOptimal NrActionsSerialPlanGraphBest Sequential Runtime181410001001612811012Nr. Steps10Runtime (sec.)1410Nr. StepsRuntime (sec.)100101680.10.140.0162510152025303540450.01450510152025Nr. InstanceNr. Instance(a)(b)103035404550160Best NrActionsRelaxedPlanBest Runtime140120180Nr. StepsRuntime (sec.)100600.140200.0105101520253035404550Nr. Instance(c)Figure 6: Pipesworld domain without tankage restrictions. Plots (parallel) plan length, heuristic estimation, runtime, (a) optimal parallel planners, (b) optimal sequential planners, (c) satisficing planners.Similarly situation optimal planners, satisficing planners, Figure 6 (c),main difference Figure 5 (c) much worse quality heuristic function:relaxed plan length differs greatly length real plans found, particularlylarger instances. curiously, despite worse quality heuristic, runtimes muchlower. longest time taken instance 10 seconds. goes show, first,shortcomings analysis here: give heuristic quality initial state, maydiffer lot situation rest state space. example, Airport planner usingrelaxed plans may get lost huge dead ends wrong decision made early on. Second,course, techniques satisficing planners use also relevant. runtime dataFigure 5 (b) exclusively due SGPlan (Wah & Chen, 2004) YAHSP (Vidal, 2004), whoseproblem decomposition/greedy lookahead techniques appear work extremely well domain.satisficing planners perform much worse, failing solve largest instances. notePipesworld, overall runtime curves (for planners) characteristically jaggedshow considerable variance comparison to, e.g., Airport. information gets lostbest-of presentation chosen figures here. seems hardness domain comes484fiE NGINEERING B ENCHMARKSP LANNINGinteractions subtle seen rather high-level parameters measured here. reiterate domain version tankage restrictions much challenging planners,planner getting anywhere close largest instances YAHSP.1000010001350Optimal MakeSpanPlanGraphBest Parallel RuntimeOptimal NrActionsSerialPlanGraphBest Sequential Runtime1000Best NrActionsRelaxedPlanBest Runtime300250102000.1150Nr. StepsRuntime (sec.)100Nr. StepsRuntime (sec.)100101001500.115101520253035400.014505Nr. Instance1015202530354045Nr. Instance(a)(b)Figure 7: Dining Philosophers domain without derived predicates. Plots (parallel) plan length,heuristic estimation, runtime, (a) optimal planners (b) satisficing planners.Figure 7 shows data Promela/Dining Philosophers without derived predicates.show two separate figures optimal planners since curves quite easy read. evenquick glance, one sees domain characteristic behavior differentdomains. optimal makespan, plan graph length, serial plan graph length constantacross instance size. contrast, optimal sequential plan length grows linear functionsize; note logarithmic scale right hand side axis Figure 7 (a), usemake figure (the values plan step measures) readable. best plans foundsatisficing planners optimal, i.e., NrActions data identical sidesfigure. Figure 7 (a), see effect heuristic quality search performance:parallel planners scale linear function instance size, sequential planners,heuristic function becomes worse worse, scale highly exponentially. latter mightalso true satisficing planners; bit hard tell since solved instances solvedextremely quickly. reason instance index higher 29 solved that,instances, similarly discussed (Section 5.1), simplifying precondition formulasbecame prohibitively costly, instances available ADL only. two satisficingplanners scaled well Dining Philosophers (without derived predicates) SGPlanYAHSP neither could handle ADL formulation domain. Similarly,optimal planners SATPLAN04 Optiplan scaled well, neither could handleADL formulation. Note inability planners handle formulas without pre-simplificationtechniques thus constitutes serious limitation.Optical Telegraph without derived predicates (no figure shown) observations similarones Figure 7, except planners scale much worse. particularly, optimalsequential planners solve single smallest instance, best satisficing runtime clearlyexponential instance size, taking 1500 seconds solve instance number 25. Promeladomain versions derived predicates, results optimal planners since nonecould handle derived predicates. observations satisficing planners similar485fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UGabove: NrActions grows linear function instance size, relaxed plan length growslinear function significantly lower gradient. planners fast Dining Philosophersneed lot time (> 1000 sec) solve largest Optical Telegraph instances (someremain unsolved). omit results Promela domain versions using numeric variables,since two planners participated domain versions.100040100035Optimal MakeSpanPlanGraphBest Parallel RuntimeOptimal NrActionsSerialPlanGraphBest Sequential Runtime3530100100301151020151Nr. Steps20Runtime (sec.)10Nr. StepsRuntime (sec.)252510100.10.1550.010510152025303540450.015005101520Nr. Instance253035404550Nr. Instance(a)(b)1050Best NrActionsRelaxedPlanBest Runtime100060Best NrActionsRelaxedPlanBest Runtime45504010035Nr. StepsRuntime (sec.)252010300.1Nr. Steps4030Runtime (sec.)120151101050.010510152025303540450.15005Nr. Instance101520253035404550Nr. Instance(c)(d)Figure 8: PSR domain. Plots (parallel) plan length, heuristic estimation, runtime, (a)parallel optimal planners PSR small (STRIPS version), (b) sequential optimal planners PSR small, (c) satisficing planners PSR small, (d) satisficing plannersPSR large (featuring ADL derived predicates).Figure 8 shows results PSR domain. Figure 8 (a), (b) (c) show plots domain version PSR small, comes pure STRIPS addressed IPC-4 planners;Figure 8 (d) shows plots PSR large, comes ADL derived predicatesaddressed four satisficing planners only. show data PSR middle-compiledPSR middle: former, two satisficing planners participated; latter, six satisficingplanners participated, scaled quite well less challenging instances resultsless interesting PSR large.486fiE NGINEERING B ENCHMARKSP LANNINGFirst, note curves PSR small show large amount zig-zagging, quiteunusual cannot simply accounted way instances scaled.22 ConsiderFigure 8 (a). main observation made real optimal makespan much largerestimation plan graph, particularly larger instances. Still, optimal parallel plannersquite efficient, least solve instances. runtime data entirely dueSATPLAN04, whose search techniques apparently quite efficient domain evenbad plan graph lower bound. optimal planners least one order magnitudeslower, cant solve largest instances; example, none solve instances 4849. optimal sequential planners Figure 8 (b), results pretty similar exceptruntime scaling somewhat worse. kinds optimal planners, runtime clearlycorrelated length optimal plans, which, since plan graph bounds almostconstant, coincides difference real plan length estimate.Figure 8 (c), observe relaxed plan bad estimator plan length PSRsmall (at least respective initial states), planners solve instances quite efficiently anyway. runtime data entirely due YAHSP Fast Downward; particularlyFast Downward extremely efficient, showing slight increase runtime instancesize, satisficing planner capable solving instances 48 49. Note YAHSP(Vidal, 2004) uses powerful techniques besides relaxed plan heuristic, Fast Downward(Helmert, 2004) uses involved (and apparently powerful, case) heuristic function. Note also that, least terms solved instances, optimal satisficing planners are,unusually, equally good (or bad) domain: exactly one group solves instances,planners cannot solve instances 48 49. difficulty planners experiencingdomain also remarkable since instances, least grounded encodings, actuallysmall compared instances domains, c.f. Figure 4. indicatesdomain fundamental characteristic yet captured well searchheuristics/techniques (most of) planners nicely complements saidnon-obvious polynomial algorithm PSR Section 4.1.Figure 8 (d), see relaxed plan (computed version FF handling derivedpredicates, see Thiebaux et al., 2003, 2005) rather useless estimator PSR domainexpressed natural way using ADL derived predicates. relaxed plan constantlycontains 0 steps, meaning over-approximation semantics derived predicates makesinitial state look like goal state; happens PSR middle. situation maydifferent parts state space heuristic value constantly 0 this, apparently,causes serious trouble satisficing planners except Fast Downward. planner except FastDownward solve instance higher number 16. Fast Downward seems profit, again,involved heuristic function, reaching scaling limit instance number 31.Satellite domain, many temporal numeric domain versions, select,presentation here, single pure STRIPS version. Figure 9 (a) (b), observe that,like Pipesworld Promela, unlike Airport PSR, Satellite domain serialplan graph provides much worse heuristic values (for sequential planning) parallel planninggraph (for parallel planning). instances solved optimal planners, parallel planlength (serial parallel) plan graph length grow much, sequential plan lengthdoes. Consequently, sequentially optimal planners scale much worse parallel ones.22. true runtime curves individual planners. fact, planners even disagree widelyinstances solved easily take lot time.487fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UG1000121000030Optimal MakeSpanPlanGraphBest Parallel RuntimeOptimal NrActionsSerialPlanGraphBest Sequential Runtime1110001002510120Nr. StepsRuntime (sec.)8Nr. StepsRuntime (sec.)10091010157160.1100.150.014510152025300.013555101520Nr. InstanceNr. Instance(a)(b)100253035500Best NrActionsRelaxedPlanBest Runtime450400103001250Nr. StepsRuntime (sec.)3502001500.1100500.0105101520253035Nr. Instance(c)Figure 9: Satellite domain. Plots (parallel) plan length, heuristic estimation, runtime,(a) optimal parallel planners, (b) optimal sequential planners, (c) satisficing planners.Figure 9 (a), also nicely see how, instances 8, 9, 10, parallel plan lengthdown-up movement (8, 6, 8) constant parallel plan graph length (4), resultingmovement pretty much shape logarithmic scale! best parallel runtime.Figure 9 (c), observe that, like Airport unlike domains,relaxed plans initial states almost length real plans (there actuallyslight over-estimation time). seen earlier, c.f. Section 4.2, Hoffmann(2005) shown that, Satellite, relaxed plan length is, fact, bound close real planlength states (in contrast Airport, unrecognized dead ends possible principle).Indeed, Satellite easy tackle almost satisficing planners IPC-4.runtime shown Figure 9 (c) appears non-trivial, remember instances huge, seeparticular number ground actions Figure 4 (b). instance 20, satisficing IPC-4planners could solve instance within minute.skip Settlers domain since relies almost exclusively numeric variables encodedomain semantics, makes rather incomparable domains. Figure 10 showsdata UMTS domain. temporal numeric versions, half featurealso time windows. consider versions without time windows; Figure 10 (a) (b) concern488fiE NGINEERING B ENCHMARKS10000720Optimal MakeSpanPlanGraphBest Parallel Runtime0.180Best NrActionsRelaxedPlanBest Runtime7001000P LANNING7068060620106005040Nr. Steps640Runtime (sec.)100Temporal MakeSpanRuntime (sec.)66030580201560105400.1520510152025303540450.015005101520Nr. Instance253035404550Nr. Instance(a)(b)1000720Optimal MakeSpanPlanGraphBest Parallel Runtime190Best NrActionsRelaxedPlanBest Runtime7008068070100660600500.140Nr. Steps620Runtime (sec.)10Temporal MakeSpanRuntime (sec.)6064030580120560105400.152051015202530Nr. Instance3540450.0150051015202530Nr. Instance35404550(c)(d)Figure 10: UMTS domain. Plots (durational) plan length, heuristic estimation, runtime,(a) optimal (b) satisficing planners plain temporal version, (c) optimal (d) satisficing planners temporal version flaw action.plain domain version, Figure 10 (c) (d) flaw action. Let us first consider optimalplanners, left hand side overall figure. optimal planners could tackledomain i.e., domains syntax TP4 HSPa (Haslum & Geffner, 2001).makespan-minimizing planners, data sequentially optimal planners (whichwouldnt make lot sense temporal setting anyway). PlanGraph curves Figure 10(a) (c) correspond makespan estimation delivered initial state TP4s temporalnumeric extension heuristic. effect heuristic quality runtime, observestrong correlation. Figure 10 (a), instance 21 makespan estimateclose real makespan time, two actually coincide runtimesgood. Starting instance 22, real makespan makes sudden leap upwardsfollowed estimation, runtimes shoot upwards. phenomenon also clearinstances 18, 19, 20, makespan estimation exhibits good, bad, good pattern,runtime same. Figure 10 (c), sort behavior observed,meaning particular flaw action effect makespan estimationTP4. fact, makespan estimation exactly instances solveddomain versions. contained implicitly latter sentence, flaw action affect runtime489fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UGset solved instances. runtime flaw action consistentlyfactor 2 larger without flaw action. challenging instances planners failflaw action present. decrease performance presumably due larger statespace incurred flaw action.Consider satisficing planners, Figure 10 (b) (d). first observe that, more,facing individual characteristic behavior, domain challengesatisficing planners. latter shows domain useful benchmark satisficingplanners; also shows heterogeneous benchmark set is: commonsatisficing planners faster optimal ones except PSR domainpicture extreme UMTS. stated earlier, domain pure scheduling problem,obviously satisficing planners provide runtime-efficient greedy solutions problem.23Looking plots little detail, find Figure 10 (b) sequential plan length (theplans found optimal) simple stepwise linear function instances, relaxed planlength initial state coincides real plan length isnt surprise givenexcellent runtimes satisficing planners, fact scheduling domain. (Insequentialized schedule harmful delete effects occur.) picture changes lot Figure 10 (d).real plan length stays basically (is increased constant 2), relaxed planlength becomes lot shorter due flaw action. satisficing planners unaffected, largelykeeping excellent runtime behavior. Apparently, planners incorporate techniquerecognizing uselessness flaw action (this done simple domain analysistechniques), getting rid influence. suspicion confirmed fact onesatisficing planner get affected flaw action way one expect. CRIKEY,heuristic search forward state space planner using relaxed plan heuristic, solves task within70 seconds without flaw action, sometimes takes 1000 seconds flaw action.Let us briefly summarize overall observations:presented data, time performance planners correlates wellquality relevant heuristic function. notable exceptions rule farobserved data Fast Downward PSR large, relaxed planspretty much devoid information, SGPlan YAHSP (to extent also FastDownward) Pipesworld, relaxed plans provide poor estimates plannersexperience (much more) serious difficulties.Usually, known benchmarks general, satisficing planners several ordersmagnitude faster optimal ones. Exceptions PSR groups performalmost equally UMTS satisficing planners hardly need time all.Usually, known benchmarks general, parallel plan graph length muchbetter estimator parallel plan length serial plan graph length sequential planlength. exceptions Airport often huge differencelengths two kinds plan graphs and, extent, PSR smalldifference parallel sequential plan length big. Note nonedomains purely sequential, i.e. parallelism possible them.23. terms quality solutions found, satisficing planners also reasonably well. example, LPG-td,minimizes makespan domain, finds, version optimized speed, plans take maximally10% time optimal ones found TP4. version LPG-td optimized plan quality, goes1%.490fiE NGINEERING B ENCHMARKSP LANNINGUsually, known benchmarks general, considerable differencelength relaxed plan initial state, length real planinitial state. Exceptions Airport, Satellite, UMTS, lengthsidentical nearly so.Usually, known benchmarks general, largest instances solvedwithin given particular time memory (30 minutes 1GB) plans aroundhundred steps more. PSR exceptional Fast Downward planner ablefind plan 35 (namely, 57) steps.indicates diversity IPC-4 domains almost every one appearsleast exceptions listed here. domains dont appear Promeladomains Pipesworld. sort exception itself, meaning domains contributetypical benchmark behaviors overall set.take existence mentioned distinguishing features evidenceIPC-4 domains indeed several novel aspects, besides oriented applicationsstructurally diverse. particular, behavior PSR domain stands one typically observes. Note that, typically easy construct artificial domains provokeunusual behavior, domains oriented applications, exhibited behavior, particularly PSR domain, unusual, also relevantconcrete sense.5.3 Fact Connectivityconclude empirical analysis data aimed assessing sort connectivityfacts. fact p, measure number adders: actions p add list (inADL case, effect p adds list). gives indication branching factoraction choices comes fact. measure number requirers: actionsp precondition (in ADL case, effect p condition).gives indication central fact task. given planning task, measureparameters distribution adders(p) requirers(p), set facts p: minimum(min), mean (mean), maximum (max), standard deviation (dev). Within domain versions,plot data instance size (number).data abstract allow deep conclusions reasons planner performance,able highlight characteristic features domains. particular, seeabstract measurements behave characteristically different IPC-4 domainsIPC-3 domains. Figure 11 shows plots IPC-4 domains Airport, Pipesworld, DiningPhilosophers, Satellite. picture PSR relatively complicated shown separatelyFigure 12. Settlers left exceptional. picture UMTS extremely simple,explained text below.Consider Figure 11 (a), (non-temporal) Airport domain. min curves shownsince constantly 0: is-pushing-back(airplane) never added since pushback requests (ofoutbound traffic) modelled; occupied(segment) required negation. maxcurves step functions since follow size underlying airports: is-moving(airplane)many adders segments, since start-up-engine done segment; ispushing-back(airplane) required every action, leading overall similar form491fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UG100010000#Adders, max#Adders, mean#Adders, deviation#Requirers, max#Requirers, mean#Requirers, deviation1001000101001100.1#Adders, max#Adders, mean#Adders, deviation#Adders, min#Requirers, max#Requirers, mean#Requirers, deviation#Requirers, min151015202530354045505101520Nr. Instance253035404550Nr. Instance(a)(b)10001000#Adders, max#Adders, mean#Adders, deviation#Requirers, max#Requirers, mean#Requirers, deviation#Adders, max#Adders, mean#Adders, deviation#Requirers, max#Requirers, mean#Requirers, deviation100100101011510152025Nr. Instance303540455101520Nr. Instance253035(c)(d)Figure 11: Distributions numbers actions adding fact, actions requiring fact,selected versions IPC-4 domains: (a) Airport, (b) Pipesworld, (c) DiningPhilosophers, (d) Satellite.max requirers curve. mean adders curve flattened facts ismoving(airplane) added certain places airport. mean requirers curve, interestingly, shows similar downwards step behavior numbers facts actions shownFigure 4. reason lies not-occupied facts, exist every segment,needed every action moving (any) airplane across segment. number factsincreases number airplanes. Since many facts, stronginfluence mean. much correspondence runtime data,trivial one tend grow instance size.Data Pipesworld, tankage non-temporal, shown Figure 11 (b). Several observationsmade: 1. max mean curves clearly follow scaling pattern, growing traffic5 growing underlying networks. 2. min curves non-zero. 3. characteristicdifference curves instance 10, afterwards. 4. curves addersrequirers almost (but exactly) coincide. Apart 1, also present Airport data,observations clearly distinguish Pipesworld domains. observation2, sometimes larger instances min number adders drop 0. dueinteractions complex networks, certain configurations inside pipes true initially492fiE NGINEERING B ENCHMARKSP LANNINGre-achieved later interactions recognized reachabilitypre-process made FF actions, c.f. explanation Section 5.1. Observation 3 duelarge contrast smallest network larger ones: smallest networkunitary pipelines (containing single batch), others pipelines least length 2.Observation 4 particularly odds domains, large differencesadders requirers. fact, measuring distribution difference addersrequirers, found numbers (not distribution parameters) extremelyclose together: instance 50, max adders 1524 max requirers 1520, maxdifference 29, mean 1.63 dev 5.31. Pipesworld tankage restrictions,phenomenon somewhat less extreme still there. Another characteristic enormouslylarge max number adders requirers, order magnitude largerdomains. max adders requirers come do-normal facts, control statusindividual pipelines, affected action moving combination batchesrespective pipeline; facts depend single batches (not combinations them),flattens mean curves two orders magnitude. Regarding runtime, mentionedearlier, Pipesworld scaling pattern clear correlation runtime; neitherfact connectivity measure here.Consider Promela domain Figure 11 (c), data shown Dining Philosophers derivedpredicates. again, extreme characteristics domain recognizable first glance.data Dining Philosophers without derived predicates identical, data Optical Telegraph differ numbers higher. min curves 0, adders constant,requirers linear. exist facts without adders due oddity encoding,certain start-up transitions put forks table first place; facts without requirersblocked-philosopher, needed goal. number adders dependinstance size due static sort domain structure, size increases numberparallel processes (philosophers), form processes stays fixed, every processinteracts exactly two processes. number requirers linear (non-constant, particular) due technicality encoding, activating (requesting) performing(executing) transition requires communication channels neutral state; respectiveflags required transitions, number course grows size. factsrequired locally, resulting much lower (easily two orders magnitude) mean. onewould expect domain simple scaling pattern, planner performance pretty muchfunction size.Data Satellite (STRIPS version) shown Figure 11 (d). characteristic feature,comparison domains, extremely smooth parallel close-together growthcurves. curve stands little max requirers; max adders due pointing(satellite, direction) facts added turning direction; maxrequirers due power-on(instrument) facts, needed every take-imageinstrument, done every combination direction image mode supportedinstrument. Note that, contrast domains max curves twoorders magnitude higher mean, max requirers one order magnitudecurves, curves roughly order. min curvesshown since constantly 1 adders power-on(instrument) addedswitch-on(instrument) constantly 0 requirers have-image(direction) needed493fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UGgoal. runtime performance IPC-4 planners scales relatively smoothly sizeSatellite, like parameters do.UMTS, parameters constants. another consequence aforementionedscaling pattern, number specified applications instances,changes (only) goal, specifying applications shall actually scheduled. Precisely, plain domain version, number adders 1 facts, nicely showingscheduling-domain characteristic choice accomplish tasks,accomplish them. another illustration satisficing planners finddomain trivial, whereas optimal planner like TP4 (Haslum & Geffner, 2001) spend longtime searching optimal schedule. number requirers minimum 0, maximum 2, mean0.89, standard deviation 0.57. domain version flaw action, notable differencemax adders 2 due alternative provided flaw action (min 0,mean 1.2, deviation 0.5). interesting note context that, mentioned above,domain version satisficing planner, CRIKEY, experiences serious trouble.1000010000maxmeandeviationmaxmeandeviation10001000#Adders#Required100100101010.1151015202530354045505101520Nr. Instance253035404550Nr. Instance(a)(b)351000maxmeandeviationmaxmeandeviation3025#Required#Adders10020151010501510152025Nr. Instance303540455051015202530Nr. Instance35404550(c)(d)Figure 12: Distributions numbers actions adding fact, actions requiring fact,PSR small large: (a) adders small, (b) requirers small, (c) adders large,(d) requirers large.Data PSR shown Figure 12. Here, show plots adders requirers separatelymakes much readable. Since data contain particularly interesting494fiE NGINEERING B ENCHMARKSP LANNINGphenomena, show two domain versions, small large. obvious featuresmall, Figure 12 (a) (b), is, again, huge amount variance data. clearlydiscernible peaks curves (instance nrs. 15, 25, 31, 40) coincide peaks sizemeasured numbers facts actions Figure 4. also note largerange values, spanning four orders magnitude, even though instances (except number25) small comparison domains shown Figure 4. minimum numbersadders requirers constantly 1: updated(breaker) added wait(breaker) action,not-closed(breaker) needed one wants close it.24 Regarding maximum addersrequirers, instance 25, far highest (9400) total number actions, max adders(9216) due goal-reached fact, i.e., 9216 disjuncts DNF goal formula;max requirers (9251) due do-normal, flag needed every goal-reached action,plus actions opening closing breakers. remark facts responsiblepeaks curves, i.e., happens also instances 15, 31, 40.highly characteristic PSR small max numbers adders requirers approach sometimes exceed two thirds total number actions. casedomain, even domain version PSR (see below). intuitive reason liesone pre-compilation steps employed order able formulate reasonablylarge PSR instances pure STRIPS: compilation step (Bertoli et al., 2002) removes networkreasoning (and it, need derived predicates) basically enumerating breaker configurations effects flow current network. result dense structureend network directly affects every end, explaining high degreefact connectivity, particular explaining extremely complex goal formulas four peakcases mentioned above.pre-compilation step also key understanding huge difference behavior small, large. latter shown Figure 12 (c) (d). There, maxadders curve small linear function note non-logarithmic scale axis spite(mostly) much larger numbers actions. example, instance highest number (7498)actions derivation rules number 20, max number adders 31, less halfpercent total number actions. natural high-level domain encoding here,flow current network modelled transitive closure derivation rulespropagate current based local status network. particular breakerconfigurations effects flow current implicit structure network.again, PSR large, min curves constantly 0 adders requirers; notaffected(breaker) negation derived predicate (needed precondition open closeactions), isnt added inverse rule, given meaning negation failuresemantics derived predicates; fed(line) required goal. mean devadders completely flattened numerous (5029 5237, instance 20) upstream(x,y)facts, true currently path open side node x side node y, addedlocal derivation rule relies predicate neighbors y. SimilarlySatellite, max number requirers generally lot larger max number adders.example, 542 vs. 31 instance 20, max requirers due fact closed(device)required derivation rules talking pairs devices; instance 20, 7360 7498 actionsrules; 46 devices.24. Sometimes 0 minimum requirers due artificial goal-reached fact, introduced get rid complexgoal formulas, c.f. Section 2.495fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UG10001000#Adders, max#Adders, mean#Adders, deviation#Requirers, max#Requirers, mean#Requirers, deviation#Adders, max#Adders, mean#Adders, deviation#Requirers, max#Requirers, mean#Requirers, deviation10010010101124681012141618202468Nr. Instance1012141618201012Nr. Instance14161820Nr. Instance(a)(b)100001000#Adders, max#Adders, mean#Adders, deviation#Requirers, max#Requirers, mean#Requirers, deviation#Adders, max#Adders, mean#Adders, deviation#Requirers, max#Requirers, mean#Requirers, deviation100010010010101124681012Nr. Instance141618202468(c)(d)Figure 13: Distributions numbers actions adding fact, actions requiring fact,STRIPS versions IPC-3 domains except Freecell Satellite: (a) Depots, (b)Driverlog, (c) Rovers, (d) Zenotravel.sum sub-section, data are, generally, abstract really tightly interconnectedperformance exhibited planners. hand, certain characteristics visible.particularly: Pipesworld, numbers adders requirers almost identical.Promela, adders constant requirers linear. Satellite, curves closetogether. PSR small lot variance, max numbers adders requirersapproach sometimes exceed two thirds total number actions. contrast, PSRlarge max adders decline less half percent total number actions. UMTS,parameters constant. Except PSR UMTS, phenomena somewhat hardinterpret. nothing else, certainly show us domains rather differentcharacteristics. Interestingly, differences significant IPC-3 benchmarks shownFigure 13. Clearly, behavior characteristically diverse seenIPC-4 domains. four domains Figure 13, basically observe mostly parallel linespretty close together except max lines, order magnitude higherothers. striking feature zig-zag nature curves Depots. duescaling pattern: smallest instances, number crates (blocks) grows continually15 crates instance 6. Thereafter, come blocks 3 instances each, first 6496fiE NGINEERING B ENCHMARKSP LANNINGcrates, second 10 crates, third 15 crates (across blocks, instance size parametersgrow). means zig-zag shape curves corresponds exactly zig-zag shapecrate numbers.Note behavior plots Figure 13 similar behavior plot SatelliteFigure 11 (d), particular first 20 instances. instances posed fullyautomated planners IPC-3, also shown Figure 13. IPC-3 domain truly standsterms behavior curves Freecell.25 There, observe phenomenon similarPipesworld Figure 11 (b), curves adders requirers almost coincide.phenomenon little weaker Pipesworld: largest Freecell instance, number 20,max (both) adders requirers 1638, max difference 102, mean14.30 dev 24.86. comparison, largest Pipesworld instance, max adders 1524,max requirers 1520, max difference 29, mean 1.63 dev 5.31.sum overall empirical analysis, data certainly dont solve mysterybehind performance every planner every domain (and instance). do, however, provideinteresting insights instances scaled domains, certain subtletiespeculiarities encodings, standard heuristic methods, groups planners,react them. observe large characteristic differences domains. senseresults nicely complement technical descriptions Appendix A, well known theoreticalresults Section 4.6. Conclusionfield research general reasoning mechanisms, AI planning, essentialuseful benchmarks: benchmarks reflect possible applications developed technology,help drive research new fruitful directions. development benchmarkdomains instances IPC-4, authors invested significant effort creating setuseful benchmarks AI planning.explained introduction, three main goals tried achieve 1. realism, 2.structural diversity, 3. accessibility benchmarks. debatable extent goalsachieved. extent, inherent conflicting nature goals. Accessibilitybenchmark formulation simple possible PDDL dialects obviously conflictrealism. Structural diversity also conflict realism since, time window availablecreate competition benchmark set, may (and been, case) large setsuitable applications choose from. One must make whats available. stressedrealism since lack realism traditionally considered one main weaknessesAI Planning achieving structural diversity accessibility would, fact,comparatively easy (see also below). said, adapt applications IPCmake many significant simplifications. Still, derived domains applications, oneexpect capture important features even simplification; top that,clear path towards realism.believe domains constitute best possible compromise IPC-4. namedistinguishing features domain set:25. somehow makes sense precisely domain stands out, also intuitively differentdomains. notably, deciding plan existence Freecell NP-hard easy domains, c.f.Section 4.1.497fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UG1. Airport, Pipesworld, PSR, UMTS derived directly applications (Promelaspecial case since model checking instances could encode simplistic).previously case Elevator domain (IPC-2) Rovers Satellitedomains (IPC-3).2. complexity satisficing optimal planning STRIPS domain versions coversentire range P, NP, PSPACE deciding (bounded) plan existence P PSRPSPACE-complete Airport general Promela. aware previousPSPACE-complete STRIPS benchmark; polynomial algorithm finding plans PSRis, contrast STRIPS benchmarks algorithms, quite nontrivial.3. Hoffmanns (2005) taxonomy domain classes different h+ topology, IPC-4domains lie classes sparse coverage previous benchmarks. particular, nonenew domains nearly simple topology proved Hoffmanntraditional benchmarks. taking account Pipesworld actions inverted(not one but) two steps, domains lies different class Hoffmanns taxonomy,covering classes (6) previous IPC benchmark set (3, 5, 4 IPC-1, IPC-2,IPC-3, respectively). Dining Philosophers exceptional lies simple classdoesnt simple topology; Airport exceptional lies hard classtypically (in real-world instances) easy.4. behavior different kinds planners IPC-4 shows lot characteristicpatterns individual domains. Airport, sheer size main obstacle. Pipesworld,particularly tankage restrictions, known heuristic functions badly.Promela domains, main obstacle is, lot cases, impossibility compilingPDDL description fully grounded simpler representation. PSR, extremelylarge amount variance, optimal planners perform well (or poorly) satisficingplanners. UMTS, satisficing planners need time all.5. abstract level looks numbers actions adding/needing fact,behavior domains characteristically diverse IPC-3 domains.6. Last least, STRIPS versions domains preserve much originaldomain structure previously case. IPC-2 STRIPS version Elevatorhardly elevator problem anymore, IPC-3 STRIPS versions Satellite Roversdevoid interesting problem constraints. contrast, STRIPS versionsAirport Promela semantically identical ADL versions, PSR STRIPSversion, pre-compiled lot, still preserves much original difficulty domain(judging, e.g., behavior IPC-4 planners it).Feature 1 is, obviously, point realism. Features 2 5 points diverse structure; particularly Feature 4 shows domains pose different challenges (current) planningtechnology. Feature 6 point realism combined accessibility. would like stressaccessibility respect really quite important. 19 planners entered IPC-4,8 could handle (some) ADL features. compilation approach enabled us confront 11planners reasonably realistic problems. said, certainly debatable role STRIPS498fiE NGINEERING B ENCHMARKSP LANNINGplays play community. people may say many core algorithms,e.g., planning graphs (Blum & Furst, 1997) relaxed plan heuristics (McDermott, 1999; Bonet& Geffner, 2001; Hoffmann & Nebel, 2001), invented STRIPS. Others may sayfocus STRIPS-like languages algorithms distracts us considering temporal numerical problems truly different nature. notwithstanding, STRIPS still widelyused language among research community. cannot ignored competition organizers.pointed advantages benchmark set, also pointdisadvantages. explained detail individual sections Appendix A, makemany simplifications order make applications fit use IPC-4. extent, whethersimplifications preserve original domain structure debatable matter. feelAirport encoding close real physical thing. able representreal optimization criterion bad, ameliorated fact that, 19 planners,single one (LPG-td) could actually deal user-defined optimization criteria.26 Pipesworld,simplifications severe. IPC-4 domain still resembles core difficulties,reminiscent (complicated) toy example software could used controlreal pipelines. Promela examples go show toy examples model checking areabetter traditional toy examples planning. PSR, removing uncertaintynumerical optimization renders IPC-4 domain unsuitable practical use.course, domain set exhaustive, meaning presumably numerous applications whose essential structure similar IPC-4 domains. examplesspring mind action choice autonomous robots, detecting security holes computer networks (Boddy, Gohde, Haigh, & Harp, 2005), online manufacturing (Ruml, Do, & Fromherz,2005). structural diversity, would easy construct set artificial domainsexplore possible extreme cases. domains would probably completely infeasible current planners, thus posing strong challenges. think of, example, RubiksCube, Sokoban, Rintanens (2004) purely randomly generated instance distributions. again,domain set would devoid realism. point preparation IPC-4,considered introducing separate class domains, called Diverse Structure, wouldcontained domains sort. decided since competition event alreadylarge without it. Also, felt applications already quite diverse structuralside. pointed above, several theoretical empirical phenomena suggest latterindeed case.work, experienced various successes failures accurately formulatingapplication domains PDDL. People asked us if, this, obtained picturesuitable PDDL is, current form, formulate applications, sorts domainsworks well. answer is, dont feel like obtained many insights mattersparticularly deep havent known before. lessons learned these. Firstforemost, formulating application STRIPS takes huge amount engineering expertise unlessone drops problem constraints; simplifications unavoidable. Second, discretenature action instantiations previous IPC PDDL dialects seriously impedes formulationdomains continuous aspects. discretization must chosen, sometimes easy(Airport) sometimes hard (Pipesworld) do. good way seems adoptduration inequalities suggested Fox Long (2003). Third, community pay26. good example case PDDL moving faster actual planning technology.499fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UGattention lifted encodings, deal modern planning algorithms: one lessoncompilation activities grounding parameters often simply possible(Promela, PSR). Since compiling away ADL constructs often feasible without grounding (c.f.Section 2), also relevant ADL/STRIPS context. final lesson, (the AIPlanning community) still, mostly, far away as-is applicability planners real world.right track.conclude, spent significant time effort creating useful set planning benchmarksIPC-4. hope become standard benchmarks coming years.Acknowledgements. would like thank competitors detailed commentsbugs found domains, would like thank Malte Helmert various useful toolshelped remove bugs.thank Malte Helmert providing us yet unpublished, timewriting results computational complexity (Helmert, 2005, 2006b). thank Patrik Haslumproviding us TP4 temporal numerical plan graph estimates makespan UMTSdomain. indebted anonymous reviewers, much David Smith MariaFox, whose detailed extensive comments contributed greatly development paper.finally thank David Smith extensive advice language, including corrections evenacknowledgements.Jorg Hoffmann thanks Wolfgang Hatzack support development Airportdomain benchmark instances.Frederico dos Santos Liporace supported Conselho Nacional de Desenvolvimento Cientficoe Tecnologico, Brazil. would like acknowledge support PhD supervisor, Ruy Milidiu, development Pipesworld application.Sylvie Thiebaux thanks Piergiorgio Bertoli, Blai Bonet, John Slaney contributionsdevelopment PSR domain instances. also would like acknowledgesupport National ICT Australia. NICTA funded Australian Governments backingAustralias Ability initiative, part Australian Research Council.Appendix A. Detailed Domain Descriptionsprovide detailed descriptions domains, alphabetical order. section (exceptSatellite Settlers domains, adapted IPC-3) organizedsub-sections follows. first give outline application domain. explainmain adaptations made model application PDDL domain IPC-4, explain IPC-4domain structure, i.e., domain versions formulations used IPC-4, explaingenerated example instances IPC-4 test suites. Finally, discuss possible futureextensions.A.1 Airportcontact person application domain, Wolfgang Hatzack, workingapplication area several years. domain adapted IPC-4 Jorg HoffmannSebastian Trug.500fiE NGINEERING B ENCHMARKSP LANNINGA.1.1 PPLICATION OMAINtask control ground traffic airport. Timed travel routes must assignedairplanes reach targets. inbound outbound traffic; formerairplanes must take (reach certain runway), latter airplaneslanded get parked (reach certain parking position). main problem constraint is,course, ensure safety airplanes. means avoid collisions, also preventairplanes entering unsafe zones behind large airplanes engines running.optimization criterion minimize summed travel time (on surface airport)airplanes.27 usually standard routes, i.e., routes airplane outboundcertain park position area, inbound certain runway, must take. reason introducingroutes is, simply, sheer complexity managing situation otherwise, without significantcomputer support (which yet available real airports). see whetherstandard routes present makes big difference also computationally.airplanes move airport infrastructure, consists runways, taxiways,parking positions. runways taxiways sub-divided smaller segments. positionairplane given segment currently located in, plus directionprecise position within segment several airplanes segment time.Airplanes generally divided three categories, light, medium, heavy, classifyaccording engine exhaust (jet blast). airplane moved either inbound out-bound. In-bound airplanes recently landed way runwayparking position, usually gate. Out-bound airplanes ready departure, meaningway departure runway. Since airplanes cannot move backwards, needpushed back gate onto taxiway, start engines. airports alsoprovide different park positions allow airplane start engines directly.ensure safety, airplane must get close back another airplane whose enginesrunning. far safety distance depends category (jet blast) secondairplane.ground controller planner communicate airplanes waysshall take stop. guidance given purely reactively, pays basedecisions anticipating future. Otherwise may happen airplanes blockneed time necessary reach destinations airport. objective is, said,minimize overall summed traveling times airplanes.instances domain, one considers traffic situation given point time,time horizon of, say, one hour. new airplanes known land given time slots insidetime horizon, time slots respective runways considered blocked,planner make sure runways free times. course, situationchanges continually (new planes moved plans cannot executed intended), continuous re-planning, i.e., consideration domain instance describing new traffic situation,necessary. Solving instances optimally (the corresponding decision problem) PSPACE-completewithout standard routes (Helmert, 2006b) NP-complete routes standardized (Hatzack& Nebel, 2001). latter case, pure scheduling problem. former case, compli27. criterion airport wants minimize, order maximize throughput. point viewairlines, would better minimize delay, e.g., minimizing summed squared delay airplanes.two criteria may conflict. Neither two easily modelled PDDL2.2, see below.501fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UGcated (highly unrealistic, course) airport topologies lead exponentially long solutions, c.f.Section 4.1.A.1.2 IPC-4 PDDL DAPTATIONPDDL encoding (as well example instance generation process, see below) basedsoftware Wolfgang Hatzack, namely system called Astras: Airport Surface ground TRAfficSimulator. software package originally designed training platformairport controllers. Astras provides two-dimensional view airport, allowing usercontrol airplanes means point click. Astras also simulate traffic flowairport course specified time window.made three simplifications, one benign, airport model. benignsimplification: model park positions airplane start engines directly,without pushed back taxiway first. difficult model park positionsPDDL, seldom occur reality relevant application. firstimportant simplification assume somewhat cruder notion airplane locatedness,requiring single airplane located segment time. is, useterm segment meaning smallest indivisible unit space. minimize lossprecision, (some of) original segments sub-divided several new smaller segments.safety distance behind back airplane whose engines running also measuredterms number segments. discretization makes us lose precision, believedistort nature problem much: due amount expected conflictingtraffic different points airport (high near parking positions), relatively easychoose discretization segments different length precise small enoughtime.28 last simplification severe. give real optimizationcriterion. say rather strong simplification below. use full standardroutes, thus allowing airplanes choice move. use standardsroutes, particularly regions near runways large airports. one thing, served keeplarge airports manageable PDDL encoding planners; another thing, seems goodcompromise exploiting capabilities computers time keeping closetraditions airports. get back matter Section A.1.5.full PDDL description domain encoding downloaded IPC-4 web pagehttp://ipc.icaps-conference.org/. Briefly, encoding works follows. available actionspushback (move plane away backwards parking position), startup engines,move segments, park (turning engines), takeoff (which amountsremoving plane airport). semantics actions encoded basedpredicates defining current state airplane. point time, airplane eithermoving, pushed, parked, airborne. airplane always occupies one segment and, enginesrunning, may block several segments depending size occupied segmentcategory airplane. action preconditions ensure blocked segments neveroccupied another airplane. initial state, plane either parked, moving. parkedplane pushed back, starting engines, moving. moving airplane28. need smallest indivisible units (of space, case) fundamental consequence discrete naturePDDL2.2; said Section A.1.5.502fiE NGINEERING B ENCHMARKSP LANNINGeither move current segment neighboring segment, park parking positiontake runway.example, look PDDL encoding (non-durational) move action (onepreconditions used example Section 2 already):(:action move:parameters(?a - airplane ?t - airplanetype ?d1 - direction ?s1 ?s2 - segment ?d2 - direction):precondition(and (has-type ?a ?t) (is-moving ?a) (not (= ?s1 ?s2)) (facing ?a ?d1) (can-move ?s1 ?s2 ?d1)(move-dir ?s1 ?s2 ?d2) (at-segment ?a ?s1)(not (exists (?a1 - airplane) (and (not (= ?a1 ?a)) (blocked ?s2 ?a1))))(forall (?s - segment) (imply (and (is-blocked ?s ?t ?s2 ?d2) (not (= ?s ?s1))) (not (occupied ?s))))):effect(and (occupied ?s2) (blocked ?s2 ?a) (not (occupied ?s1)) (not (at-segment ?a ?s1)) (at-segment ?a ?s2)(when (not (is-blocked ?s1 ?t ?s2 ?d2)) (not (blocked ?s1 ?a)))(when (not (= ?d1 ?d2)) (and (not (facing ?a ?d1)) (facing ?a ?d2)))(forall (?s - segment) (when (is-blocked ?s ?t ?s2 ?d2) (blocked ?s ?a)))(forall (?s - segment) (when(and (is-blocked ?s ?t ?s1 ?d1) (not (= ?s ?s2)) (not (is-blocked ?s ?t ?s2 ?d2)))(not (blocked ?s ?a))))))six parameters lot compared usual benchmarks causeprohibitive explosion instantiations since lot restriction static predicates.Airplane ?a moves; type (category) ?t; segment ?s1 facing direction ?d1,?s2 facing direction ?d2 move. Direction simple conceptsays end segment airplane facing. course, moves ?s1 ?d1?s2 ?d2 possible specified static topology airport (can-move,move-dir). first two complex preconditions says ?s2 must currentlyblocked airplane ?a itself. second complex precondition makes surethat, move, ?a block segment currently occupied (by another airplane,necessarily): (is-blocked ?s ?t ?s2 ?d2) static predicate true iff ?s endangeredblocked plane type ?t ?s2w facing direction ?d2. effects selfexplanatory; simply update at, occupied, blocked information. effectlooks little complicated last one says segments blockedmove, longer blocked move, become un-blocked. Note conditionsconditional effects static, conditions disappear parameter instantiation chosen.durational PDDL, actions take time according simple computations. timetaken move across segment depends, naturally, segment length speed.assumed airplanes move speed regardless category. time taken startengines proportional number engines. actions fixed duration.planes known land near future, blocking runways, modelblocking time windows using timed initial literals, respectively compilationartificial (temporal) PDDL constructs. timed literals simply instances usual blockedpredicate, becoming true respective time window starts, becoming falseends.able model real optimization criterion airport ground traffic control.standard criterion PDDL minimize execution time, i.e., makespan, plan.503fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UGencoding domain comes minimizing arrival time (meaning, arrivaldestination airport) last airplane. real objective is, said above, minimizeoverall summed travel time airplanes. appears good way modelingcriterion current PDDL. difficulty lies accessing waiting times planes, i.e.times stay segment waiting plane pass.29way (we could think of) get access waiting times, current PDDL,introduce explicit waiting action. one must able tell planner, i.e., encodeaction, long plane supposed wait. One option use duration inequalitiesproposed Fox Long (2003). action imposes constraints duration,planner can/has choose actual duration action, point usedplan, additional (rational-valued) parameter. potential disadvantage approachchoice waiting time introduces, principle, infinite branching factorstate space, may thus make problem much harder automated planners. Moreover, duration inequalities put use IPC-3, part PDDL2.1. usingduration inequalities, way encode requested waiting time action usediscretization time. One introduce new objects representing every considered timeinterval, give waiting action parameter ranging objects. Apart lossprecision involved discretization, approach also likely cause huge performanceproblems automated planners. alternative way out, considered introducing specialcurrent-time variable PDDL2.2, returning time evaluation plan execution.Using look clock, one could make plane record arrival time, thus formulate true optimization criterion without major changes domain structure. IPC-4organizing committee decided introduction current-time variable seemedproblematic algorithmic point view (it implies commitment precise time pointsplanning time), didnt seem relevant anywhere except Airport.all, IPC-4 PDDL encoding Airport domain realistic except optimization criterion, demands minimize maximal arrival time makespan instead summedtravel time. remains remark one (LPG-td) IPC-4 planners ignoredoptimization criterion anyway. Also, minimizing latest arrival time appear useful (ifideal) objective.A.1.3 IPC-4 OMAIN TRUCTUREAirport domain versions used IPC-4 non-temporal, temporal, temporal-timewindows,temporal-timewindows-compiled. first versions is, name suggests, nondurational PDDL. second version, actions take time explained above. third fourthversions also consider runways blocked future planes known land given timewindows. third version encodes time windows using timed initial literals, fourthversion uses literals compilation standard temporal PDDL constructs, c.f. Section 2.domain versions, problem constraints modeled using ADL, i.e., complex preconditions conditional effects. compiled ADL encodings STRIPS domainspecific software implemented purpose. grounded operatorparameters, precisely, parameters except, action, one giving name29. Modelling summed (squared) delay airplanes, optimization criterion airlines, would pose essentiallydifficulty: also involves computing arrival time (in order compute delay).504fiE NGINEERING B ENCHMARKSversionnon-temporalnon-temporaltemporaltemporaltemporal-twtemporal-twtemporal-twctemporal-twcformulationADLSTRIPSADLSTRIPSADLSTRIPSADLSTRIPSmax-#op514085140851408141429P LANNINGmax-#act(1048) 989(21120) 13100(1408) 989(21120) 13100(995) 854(22038) 13100(911) 861(21141) 13121Table 1: Overview different domain versions formulations Airport. Abbreviations used: temporal-tw temporal-timewindows, temporal-twc temporaltimewindows-compiled; max-#op maximum number (parameterized) PDDLoperators instance, max-#act maximum number ground actionsinstance. ADL formulations, set ground actions could generatedlargest instances; data shown largest instances could handled. Dataparentheses collected FFs reachability pre-process (see text).affected individual airplane. parameters fixed, formulas conditionaleffects simplified usual STRIPS constructs. Airport domain version containsoriginal ADL formulation, well compilation STRIPS. result groundingprocess depends specific airport considered instance, set airplanestravelling. So, STRIPS formulations, instance individual domain file(the applies STRIPS compilations domains described later).domain versions, well blow-up incurred compilation, overviewedTable 1.30 numbers shown table indicate numbers PDDL operators, numbersgrounded actions. domain version/formulation, maximum numberinstance shown. Note that, ADL formulations except temporal-timewindows-compiled,single domain file number operators identical instances.STRIPS formulations, number operators high because, explained, operatorparameters grounded. difference number ground actions STRIPSADL formulations because, automated software, able generateground actions larger ADL instances; data shown largest instancescould handle. numbers shown parentheses refer situation FFs reachabilitypre-process; said before, builds relaxed planning graph initial state, removesactions appear graph. difference numbers inside outsideparentheses indicates much simple pre-process helps. see helps quite lothere, pruning almost half actions (which would never become applicable, forward searchleast, blow representation regardless algorithm used).30. instantiation process is, course, planner-dependent. Similarly Section 5, data basedFFs pre-processor. extended pre-processor (precisely, one Metric-FF (Hoffmann, 2003)) dealtemporal constructs.505fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UGA.1.4 IPC-4 E XAMPLE NSTANCESAirport example instances generated Sebastian Trug, implementation basedaforementioned airport simulation tool Astras. Five scaling airport topologies designed,used basis instance generation. airports named Minimal, Mintoy,Toy, Half-MUC, MUC. smallest airports smallest possible airportAstras handle. two largest airports correspond one half Munich Airport (MUC),full MUC airport. Figure 14 shows sketches Minimal airport, MUCairport.(a)(b)Figure 14: smallest (a), largest (b) IPC-4 Airport topologies. Park positionsegments marked black (e.g., top part (a)), segments airplanestakeoff marked white (e.g., left bottom side part (a)). linesshow road network airport. Topology (b) corresponds MUC airport.Sebastian Trug implemented PDDL instance generation software inside Astras. simulation traffic flow airport, desired user software exports current trafficsituation various PDDL encodings explained above. simulator run differentairports, 50 scaling traffic situations exported (3 Minimal, 6 Mintoy, 11Toy, 15 Half-MUC, 15 MUC). airport, instances scale termsnumber travelling airplanes. largest instance features 15 planes moved destinations Munich airport, 10 planes landing future considered (in respectivedomain versions). considered realistically sized traffic situation, airport.506fiE NGINEERING B ENCHMARKSP LANNINGA.1.5 F UTURE W ORKremains explore relax simplifications make. importantly,overcome discrete model space (locatedness), model real optimizationcriterion. difficulties are, partly described already, mostly due discretenature PDDL2.2, allow continuous choice instantiation action.continuous choice would natural way saying far plane movinglong waiting. best way go direction is, probably, assumeduration inequalities proposed Fox Long (2003), together numeric variablesalready contained PDDL2.2. easy modelling side. main problemprobably technology side, i.e., develop planners deal efficientlycontinuous choice points. time IPC-4, said, continuous choice appeared muchdemand planners.One interesting topic future work arises one restricts airplanes completely standardroutes, i.e., leaves choice route take destination. said, first,usually done real airports, sheer complexity managing situation otherwise,without significant computer support (which yet available real airports). Second, IPC4 made limited use feature, retain flexibility could offeredautomatized methods. Third, restriction turns PSPACE-complete ground traffic controlproblem pure, NP-complete (Hatzack & Nebel, 2001), scheduling problem,question planes move across segment. One could exploit create muchconcise PDDL encoding. restricted problem comes resolving conflictsarise two planes need cross airport segment. One could thus try encodePDDL physical airport, conflicts possible solutions, ideally connectionreal optimization criterion. expected planners much efficientsimpler concisely encoded problem.A.2 PipesworldFrederico Liporace working application area several years; submitted paperearly domain version workshop competition ICAPS03. domainadapted IPC-4 Frederico Liporace Jorg Hoffmann.A.2.1 PPLICATION OMAINPipelines play important role transportation Petroleum derivatives, sinceeffective way transport large volumes large distances. application domainconsider deals complex problems arise transporting oil derivative productsmulti-commodity pipeline system. Note that, many planning benchmarksdealing variants transportation problems, transporting oil derivatives pipelinesystem different characteristic kind structure, since uses stationary carrierswhose cargo moves rather usual moving carriers stationary cargo. particular,changing position one object directly results changing position several objects.less reminiscent transportation domains complicated single-player gamesRubics Cube. lead several subtle phenomena. example, may happen solutionmust reverse flow liquid pipeline segment several times. may also happen507fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UGliquid must pumped ring pipeline segments cyclic fashion, achieve goal(we see example later).detail, application domain following. pipeline network graph operational areas connected pipeline segments. Operational areas may harbors, distribution centersrefineries. may connected one pipeline segments. oil derivativesmoved areas pipelines.different types petroleum derivative products. area set tanksdefine storage capacity product type. pipeline segment fixed volumespeed. volume depends segments length cross section diameter, speeddepends power pumps move contents. segment may uni-directional, i.e.usable transportation one direction.Pipeline segments always pressurized, is, must always completely filledpetroleum derivative products. that, way move pipeline segments contentspumping amount product adjacent area segment. operationresults, assuming incompressible fluids, amount possibly different productreceived area end segment.pumping operations executed violate interface tankingconstraints. former, distinct products direct contact inside pipeline segment,unavoidable loss due mixture interface them.interface losses major concern pipeline operation, mixed productssimply discarded. must pass special treatment may involve sending backrefinery, may require use special tanks. severity interface losses dependsproducts interface inside pipeline segment. two product types known generatehigh interface losses, pipeline plan must place adjacently segment. pairproduct types said interface restriction.Tanking constraints limits product amounts stored area, arisingrespective tank capacities. constraints may effectively block pipeline segment,room receiving area store product would leave segment processpumping operation.task application bring certain amounts products areasrequired, i.e. one find plan pumping operations shifts positions productamounts way goal specifications met. Sometimes deadline specifyingwhen, latest, product amount arrive destination area. may also casearea (typically, refinery) known produce given amount product given pointtime, plan must make sure enough tank space available respectivearea store new product amount. Similarly, area (typically, harbor distribution center)may known consume given amount product given point time, thereby freeingrespective amount tank space.A.2.2 IPC-4 PDDL DAPTATIONmain adaptations made PDDL encoding unitary batches, split pumping operations,personalized goals (see latter). term batch used oil pipelineindustry refer amount product must transported pipeline. Batchesthus associated single product predefined volume. Batches also indivisible.508fiE NGINEERING B ENCHMARKSP LANNINGbatch Bi pumped area Aj segment Sj,k , possible another batchpumped Aj Sj,k Bi volume pumped. course, reality productamount batch rational number. Using numeric encoding IPC-4 seemed completelyinfeasible due complications modeling, expected capabilities participatingplanners (see Section A.2.5). Instead, based encoding concept calledunitary batches. smallest considered indivisible portions product. pumpingoperations refer unitary batches. pipeline segments volumes volumes tanksalso defined terms unitary batches. encoding real-world instance domain,actual volume associated unitary batch choice variable. Smaller unitary batches decreaserounding error PDDL encoding, cost larger encoding size. Note that, likesmallest units space Airport domain, discretization need duenon-continuous nature actions PDDL2.2; get back Section A.2.5.modeled pipe segments directional fashion, i.e. default direction assigningone area role, area role. pumping operations accordinglydistinguish push actions, move liquid respective segments default direction,pop actions, move liquid opposite direction. simply technical deviceenable encoding pipe segment contents predicates defining first lastbatches segments (as well successor relation). push pop actions receive(amongst things) arguments pipeline segment whose contents moved,batch inserted segment. batch leaves segment dependssegment content action executed. Figure 15 shows example.A1B1A2B2B4B5B6A1fifififififififififififififiB1fiB2A2fiB3B4B6B5B3(b)(a)A1!!!!!!!!!!""""#"#B1!!"#"#"#"#"#$$%$%$$%$%"#B2"#A2B4()()()()()()()()()()()()()B6()()()()()'()&()'()B5&&'''&&&&$%&B3P1'ffffffffffffffffffffffff$%P2P3(c)Figure 15: small example. A1 plays role. fill pattern batch representsproduct. (a) shows initial state, (b) shows state (a) push operationB3 inserted segment, (c) shows state (b) pop operation B6inserted segment.Apart pipe segment batch inserted, push pop actionstake several parameters regarding, e.g., product types tank slots. particular, orderable update segment contents correctly, actions also need parameters givingrespective first, last, second last batch current contents segment. Thusaction four parameters ranging batches, yielding least n4 ground instances actionn (unitary) batches considered task. found made domain509fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UGcompletely infeasible planning system grounded actions. Since many unitarybatches needed encode even relatively small Pipesworld examples, planners typicallydied pre-processing phase already.31 avoided phenomenon splitting actionstwo parts, start action taking batch parameters inserted batch first batchpipe, end action taking batch parameters last second last batchespipe. make concrete, split push action:(:action PUSH-START:parameters(?pipe - pipe ?batch-atom-in - batch-atom ?from-area - area ?to-area - area?first-batch-atom - batch-atom ?product-batch-atom-in - product?product-first-batch - product):precondition(and (normal ?pipe) (first ?first-batch-atom ?pipe) (connect ?from-area ?to-area ?pipe)(on ?batch-atom-in ?from-area) (not-unitary ?pipe)(is-product ?batch-atom-in ?product-batch-atom-in)(is-product ?first-batch-atom ?product-first-batch)(may-interface ?product-batch-atom-in ?product-first-batch)):effect(and (push-updating ?pipe) (not (normal ?pipe)) (first ?batch-atom-in ?pipe)(not (first ?first-batch-atom ?pipe)) (follow ?first-batch-atom ?batch-atom-in)(not (on ?batch-atom-in ?from-area))))(:action PUSH-END:parameters(?pipe - pipe ?from-area - area ?to-area - area ?last-batch-atom - batch-atom?next-last-batch-atom - batch-atom):precondition(and (push-updating ?pipe) (last ?last-batch-atom ?pipe) (connect ?from-area ?to-area ?pipe)(not-unitary ?pipe) (follow ?last-batch-atom ?next-last-batch-atom)):effect(and (not (push-updating ?pipe)) (normal ?pipe)(not (follow ?last-batch-atom ?next-last-batch-atom))(last ?next-last-batch-atom ?pipe) (not (last ?last-batch-atom ?pipe))(on ?last-batch-atom ?to-area)))constructs largely self-explanatory. static predicates used are: connect,encoding topology network; is-product, encoding types liquid; may-interface,encoding interface restrictions;32 not-unitary, saying whether pipe segment containsone batch case push pop actions much simpler needsplit (the first last elements pipe identical). predicates normal pushupdating ensure, obvious way, two parts split action usedintended. Finally, on, first, follow, last encode relevant batches are.role clear, encodes locatedness areas. pipe contents,modelled queue-like fashion, head first, tail last, successor function follow.two parts push action update representation accordingly.31. Matters may easier planning systems ground actions pre-process. didnt affectdesign decision since large majority systems around time IPC-4 employ pre-process.32. Note model interface loss products may interface.510fiE NGINEERING B ENCHMARKSP LANNINGencode uni-directional pipe segments, i.e. segments push popactions available IPC-4 encodings. modeled tankage restrictions simple constructs involving tank slots located areas, slot capacity store one unitary batchgiven product type is, push pop actions also specify tank slotinserted/outgoing batch comes from/is inserted into. simple examples regarding interfacetankage restrictions, re-consider Figure 15. storage capacity P2 A2 equal zero,transition state (a) state (b) becomes invalid. forbid interface P1P3 , transition state (b) state (c) becomes invalid.Pipe segment speed easily taken account (in durational PDDL). speedsegment s, simply assign push/pop actions regarding segment duration proportional 1s . (In IPC-4 encoding, start/end action takes exactly time,non-split actions regarding length-1 segments take time 2s .)reality, outlined goals refer amounts product requested certaindestination areas. encoding based batches, formulating goal would mean introduce potentially large disjunction conjunctive goals. one wants say, e.g., three unitary batches product P requested area A, needed goal condition disjunctionW{b1 ,b2 ,b3 }B (atb1 A) (atb2 A) (atb3 A) respective conjunctive goal three-subsets{b1 , b2 , b3 } batches B type P . avoid exponential blow-ups kind, encodingused personalized goals instead, referring specific batches instead product amounts. Basically, comes pre-selecting one {b1 , b2 , b3 } subsets disjunction.33One could also avoid blow-up replacing disjunction existential quantification;step would undone compilation STRIPS anyway.Deadlines arrival batches are, durational PDDL, easily modeled compilationtimed initial literals. goal deadline literal saying respective batchstill ejected end pipe segment. literal initially true, becomes falsetime deadline. described above, application also pre-specified timepoints area produces consumes given amount product. modelIPC-4 domain (see also Section A.2.5).mentioned above, structure Pipesworld domain lead several subtle phenomena possible plans. example plans perform cyclic sequence pumpingoperations depicted Figure 16. goal place B8 A3. shortest plan following (for readability, action parameters batches going pipesshown): 0: PUSH S1,4 B8 B2, 1: POP S2,4 B2 B3, 2: POP S1,2 B3 B1, 3: PUSH S1,4 B1 B8, 4:PUSH S4,3 B8 B7, 5: POP S2,3 B7 B4, 6: PUSH S2,4 B4 B2, 7: PUSH S4,3 B2 B8. Observeplan contains two cyclic patterns. Action 0 inserts B8 S14. Actions 1, 2, 3 formcycle {S2,4 , S1,2 , S1,4 } brings B8 A4. Thereafter, action 4 inserts B8 S43, actions5, 6, 7 form another cycle {S2,3 , S2,4 , S4,3 } bringing B8 goal position A3.3433. Note bad choice {b1 , b2 , b3 } make task harder solve. are, however, currently investigating computational complexity different variants Pipesworld, preliminary results suggestallowing/disallowing personalized goals affect complexity.34. Note need cyclic patterns oddity introduced encoding. something may (butprobably likely to) happen reality: like example, becomes necessary isnt enough liquidorigin area (here, A1 A4) push needed amount liquid (here, B8) destination.511fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UGA2S1,2B1A1B4S2,4 B3B8S2,3B5B6B2S1,4A4B7A3S4,3Figure 16: example cycling required achieve goal (place B8 A3). Pipe segmentSi, j directed Ai Aj.versionnotankage-nontemporalnotankage-temporalnotankage-temporal-dnotankage-temporal-dctankage-nontemporaltankage-temporalformulationSTRIPSSTRIPSSTRIPSSTRIPSSTRIPSSTRIPSmax-#op666966max-#act(14800) 13696(14800) 13696(8172) 7740(8175) 7742(107120) 101192(107120) 101192Table 2: Overview different domain versions Pipesworld. Abbreviations used:temporal-d temporal-deadlines, temporal-dc deadlines-compiled; max-#opmaximum number (parameterized) PDDL operators instance, max-#actmaximum number ground actions instance. Data parentheses collected FFs reachability pre-process (see text).A.2.3 IPC-4 OMAIN TRUCTUREPipesworld domain versions used IPC-4 notankage-nontemporal, tankage-nontemporal,notankage-temporal, tankage-temporal, notankage-temporal-deadlines, notankage-temporaldeadlines-compiled. versions include interface restrictions. versions tankagename include tankage restrictions. versions temporal name, actions takedifferent amounts time depending pipeline segment moved, explainedabove. versions deadlines name include deadlines arrival goalbatches. One versions models deadlines using timed initial literals, version(naturally, compiled name) literals compiled artificial (temporal) PDDLconstructs. None encodings uses ADL constructs, version one(STRIPS) formulation.domain versions numbers ground actions overviewed Table 2. before,data measured using (a temporal extension of) FFs pre-processor. numbers shown512fiE NGINEERING B ENCHMARKSP LANNINGparentheses refer situation pre-processors reachability pre-process,builds relaxed planning graph initial state removes actions appeargraph. observe numbers ground actions low domain versionsdeadlines, extremely high versions tankage restrictions. former simplybecause, due complicated generation process (explained next sub-section), examplesdeadlines generated smaller size. latter high numbers actionspresence tankage restriction due additional blow-up incurred choice tankslots draw/in put batches. note effect reachabilitypruning relatively moderate, particular much lower than, e.g., Airport, c.f. Section A.1.3.A.2.4 IPC-4 E XAMPLE NSTANCESPipesworld example instances generated Frederico Liporace, process goingrandom generators XML files PDDL files.35 Five scaling network topologies designedused basis instance generation. Figure 17 shows network topologies, wellreal-world network topology comparison. one see, largest network topologyused IPC-4 quite yet ballpark real network; neither triviallysmall comparison. volumes pipeline segments connect areas realworld example necessarily segments may different cross sectiondiameters.domain versions without tankage restrictions deadlines, networktopologies 10 scaling random instances generated. Within network, instances scaledterms total number batches number batches goal location.instances featuring tankage restrictions deadlines, generation process complicatedwanted make sure obtain solvable instances. tankage restriction examples, ran Mips (Edelkamp, 2003b) respective notankage instances, incrementallygrowing tankage.36 chose instance random point first instance solvedMips, maximum needed tankage (enough tankage area accommodate instancebatches). instances could solved Mips even given several days runtime,inserted maximum tankage. deadline examples, ran Mipscorresponding instances without deadlines, arranged deadline goal batch random point interval arrival time batch Mipss plan, end timeMipss plan. instances solved Mips left out.A.2.5 C URRENTF UTURE W ORKongoing work developing Pipesworld specific solver, named Plumber (Milidiu & dosSantos Liporace, 2004a; Milidiu & dos Santos Liporace, 2004b). Plumber incorporates pipelinesimulator, domain specific heuristics, procedures reducing branching factor symmetryelimination. also lets user choose different search strategies, enforced hillclimbing (Hoffmann & Nebel, 2001) learning real time A*(Korf, 1990). Currentlyextended support temporal planning well.35. XML file mapped different PDDL files depending kind encoding used; lottrial error came final IPC-4 encoding.36. Mips convenient choice since one planners, also deal temporal constructs.513fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UGA1A1S1,211,3S1S1,2,3S11A3A2Network 12A2Network 2A321,3S1S1,2A1S3,4A3A2A4Network 3146233A114252S2,394S3,4A3A253138BA1,22S1UT13RC512213513,3S1TB124S3,4A410Network 5BA13SZ1157A5A3A2RD621541GU347S1,5A137530Network 41RV843A43S2,383GA2,3S1S1,2RP7110176(a)312033SB93RB42(b)Figure 17: IPC-4 Pipesworld network topologies (a), real network topology (b).segment volumes latter annotated 100m3 units.availability solver enable extension Pipesworld benchmark, sinceeasier overcome aforementioned difficulties generating large feasible instances.hope able generate feasible instances real-world pipeline topologies, like one shownFigure 17.addition generating larger instances, Pipesworld benchmark may extended manyways make closer real application scenario. relevant possible extensions include:Defining pipeline segments single flow direction, is, segmentspush pop actions allowed. Note introduces dead ends/critical choicesproblem.Un-personalized goals. could accomplished, e.g., imposing desired tank volume goal products respective areas. planner also decidebatches used bring tank volume desired level.Modeling production consumption products pre-specified points time, described above.514fiE NGINEERING B ENCHMARKSP LANNINGUsing rational numbers model tank capacities current volumes, instead encodingbased unitary tank slots. Apart precise model real world (whencombined rational-valued batch sizes, see below), encoding would avoid unnecessary symmetries currently arise availability several non-distinguishabletank slots (in area, product).important shortcoming encoding use unitary batches. would muchappropriate base encoding product amounts given real numbers. One problematicaspect encoding would, naturally, demand continuous choicemuch liquid pump pipeline. Like Airport (c.f. Section A.1.5), choice couldnaturally modelled using Fox Longs (2003) duration inequalities, uncleardevelop planners deal reasonably well. Unlike Airport, implementingchoice end difficulties modelling side. model continuouscontents pipeline? number distinct regions liquid pipeline grow arbitrarilyhigh, principle. One solution might fix upper bound, simply disallow pumpingoperation would result many distinct regions. may bearable loss precision,given upper bound high enough. even then, bound awkward correctly updatecontents pipeline amount x product pushed in: number differentproducts leaving pipe depends x. option may use complicated constructconditional effects.all, impression pipeline scheduling wont realistically modelled PDDL,successfully solved planners, unless one introduces language data structuresuitable modelling contents pipes. Basically, would queues whose elementsannotated real numbers, whose basic operations usual push pop.semantics pipes could explicitly computed inside planner, rather awkwardlymodelled using language constructs likely disturb general search mechanism.A.3 Promeladomain created IPC-4 Stefan Edelkamp.A.3.1 PPLICATION OMAINdropping Promela domain, briefly recall origin.model checker SPIN (Holzmann, 2003) targets efficient software verification.used trace logical design errors distributed systems design, operating systems, datacommunications protocols, switching systems, concurrent algorithms, railway signaling protocols,etc. tool checks logical consistency specification. SPIN reports deadlocks, unspecified receptions identifies race conditions, unwarranted assumptions relative speedsprocesses. SPIN (starting Version 4) provides support use embedded C codepart model specifications. makes possible directly verify implementation level softwarespecifications, using SPIN driver logic engine verify high level temporal properties. SPIN works on-the-fly, means avoids need construct global state graphprerequisite verification system properties. SPIN supports property checking linear temporal logic (LTL). LTL expresses state trajectory constraints, using temporal modalities like515fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UGeventually, always, until37 . SPIN uses specific mechanisms specifying deadlock-freenesssafety properties, addition general LTL specifications. explore state spaceordinary nested search algorithm applied, depending whether state-based (a.k.a.safety) property verified.Promela SPINs input specification language. computational model asynchronouscommunicating finite state machines. Promela allows define classes finite processes. specialprocess called init started first usually governs instantiation processessystem. possible process invoke another one, Promela allows modeling systemsdynamic creation state components. Communication Promela achieved via shared variablesmessage channels. Two kind message channels distinguished synchronous asynchronous communication. asynchronous channel basically FIFO queue, synchronouschannels imply rendezvous communication transition system involves two processes, one reading message channel another sending message it. Here,consider asynchronous communication. body process class basically sequencestatements. statement interpreted transition process. Typical statements include assignments, numerical boolean expressions channel operations. Promela also allowsdefine atomic regions, whose sequence transitions treated atomicaction. interpreted weighted transitions whose costs number steps withinregions.38IPC-4, used two example communication protocols formulated Promela: DijkstrasDining Philosophers problem, so-called Optical Telegraph protocol. briefly describelatter protocol Section A.3.4. illustrate Promela language, let us consider DiningPhilosophers problem, n philosophers sit around table lunch. n plates,one philosopher, n forks located left right plate. Since twoforks required eat spaghetti plates, philosopher eat time. Moreover,communication except taking releasing forks allowed. task devise localstrategy philosopher lets philosophers eventually eat. simplest solutionaccess left fork followed right one, obvious problem. philosophers waitsecond fork released possible progress; deadlock occurred.difficult probably insightful derive bottom-up PDDL encoding DiningPhilosophers domain, using actions like eat, wait think. motivation, however, cometop-down encoding, starting Promela specification, automatically translatingPDDL.deadlock model Dining Philosophers specified Promela shown Figure 18.first lines define macros declare array N boolean variables representavailability forks. following lines define behavior process type philosopher.process iterates indefinitely endless loop (do) one unique entry marked symbol::. Statements separated semicolon. first transition left!fork consists sendoperation tag fork channel left, macro address forks currentprocess id pid. represents availability left fork philosopher. access transition left?fork executed reading tag fork channel left successful.37. Note fragments LTL likely included PDDL language next international planningcompetition (Gerevini & Long, 2005)38. documentation Promela specification language found web site SPINhttp://netlib.bell-labs.com/netlib/spin/whatispin.html516fiE NGINEERING B ENCHMARKSP LANNING#define MAX PHILOSOPHERS Nmtype=fork#define left forks[ pid]#define right forks[( pid+1) % MAX PHILOSOPHERS]chan forks[MAX PHILOSOPHERS] = [1] bit;active [MAX PHILOSOPHERS] proctype philosopher(){left!fork;::left?fork -> /* try get left fork */right?fork; /* try get right fork *//* eat... */left!fork; right!fork /* release forks *//* meditation... */od}Figure 18: Promela specification model Dining Philosophers problem.next transition right?fork similar first, last two ones sends tag fork backchannels left right.A.3.2 IPC-4 PDDL DAPTATIONModel Checking Action Planning closely related, c.f. Section 3. model checkersearches counterexample form sequence transitions falsify given specification, planner searches sequence actions satisfies given goal. cases,basic models (STRIPS Planning, Kripke structures), refer implicit graphs, nodesannotated atomic propositions.automatically generating PDDL model Promela syntax wrote compiler (Edelkamp, 2003a). restricted safety properties, especially deadlocks, assertionsglobal invariances difficult obtain. also concentrated models fixed number processes, since models communication protocols adhere restriction.39compiler parse Promela code itself, takes input intermediaterepresentation problem generated SPIN validation tool40 . Figure 19 showstextual automata representation philosopher process. case, value Ninitialized 10 philosophers. file contains almost necessary information39. dynamic creation processes PDDL would require language extension dynamic object creation.extension dismissed since would involve heavy changes existing planner technology, relevance(beyond Promela) unclear.40. precisely, Promela input file taken, corresponding c-file generated, verifier compiledexecutable run option -d.517fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UGtranslation, number processes queues (i.e., message channels) well queue capacities read original Promela input file41 .proctype philosopherstate 1 -(trans 3)-> state 6 line 11 => forks[ pid]!forkstate 6 -(trans 4)-> state 3 line 12 => forks[ pid]?forkstate 3 -(trans 5)-> state 4 line 14 => forks[(( pid+1)%10)]?forkstate 4 -(trans 3)-> state 5 line 16 => forks[ pid]!forkstate 5 -(tras 6)-> state 6 line 16 => forks[(( pid+1)%10)]!forkFigure 19: Automata representation model 10 Dining Philosophers problem.derive suitable PDDL encoding domain, process represented finite stateautomata. Hence, propositional encoding simulates automaton. propositional atomstrue initial state one process running example problem shown Figure 20 (a)42 .(is-a-process philosopher-0 philosopher)(at-process philosopher-0 state-1)(trans philosopher trans-3 state-1 state-6)(trans philosopher trans-4 state-6 state-3)(trans philosopher trans-5 state-3 state-4)(trans philosopher trans-3 state-4 state-5)(trans philosopher trans-6 state-5 state-6)(is-a-queue forks-0 queue-1)(queue-head forks-0 qs-0)(queue-tail forks-0 qs-0)(queue-next queue-1 qs-0 qs-0)(queue-head-msg forks-0 empty)(queue-size forks-0 zero)(settled forks-0)(a)(b)(writes philosopher-0 forks-0 trans-3) (trans-msg trans-3 fork)(reads philosopher-0 forks-0 trans-4) (trans-msg trans-4 fork)(reads philosopher-0 forks-1 trans-5) (trans-msg trans-5 fork)(writes philosopher-0 forks-1 trans-6) (trans-msg trans-6 fork)(c)Figure 20: Propositional encoding one philosophers process (a), Propositional encoding(single-cell) communication channel (b), Connecting communication local state transitions (c).encoding communication structure represents channels graphs. PDDL encoding additionally exploits cyclic embedding queue array. formally, (FIFO)channel Q represented structure GQ = (SQ , headQ , tailQ , Q , messQ ,contQ ), SQset queue cells, headQ , tailQ SQ head tail cells Q, messQ M|SQ |41. avoid conflicts pre-compiler directives, first invoked c-compiler command line option -E,executes pre-compiler.42. use transition IDs, competition less accessible textual representation label chosen.518fiE NGINEERING B ENCHMARKSP LANNINGvector messages Q (M set messages), contQ IR|SQ | vectorvariable values Q Q : SQ SQ successor relation Q; SQ = s[1], . . . , s[k](s[i]) = s[(i + 1) mod k]. Explicitly modeling head tail positions queue tradesspace time, since queue updates reduce constant time.queue either empty (or full) pointers refer queue state. special case,simple queues (as example) may consist one queue state, successor bucketqueue state 0 queue state 0 itself. case grounded propositional encoding includesoperators add delete lists share atom. make standard assumptiondeletion done first. propositional atoms one queue adaption two queuesone process exemplified Figure 20 (b) (c).Queue content, shared local variables modeled PDDL fluents. differencelocal variables compared shared ones restricted visibility scope, local variablesprefixed process appear in. two benchmark protocols selected IPC-4 relypure message passing, numerical state variables involved. allowed ussupply propositional model problems.(:action activate-trans:parameters (?p - process ?pt - proctype ?t - transition ?s1 ?s2 - state):precondition (and (forall (?q - queue) (settled ?q)) (trans ?pt ?t ?s1 ?s2)(is-a-process ?p ?pt) (at-process ?p ?s1) (pending ?p)):effect (and (activate ?p ?t) (not (pending ?p)))))Figure 21: Testing transition enabled activating it.PDDL domain encoding uses seven operators, named activate-trans, queue-read,queue-write, advance-queue-head, advance-empty-queue-tail, advance-non-empty-queue-tail,process-trans. activation process shown Figure 21. see pending processactivated, queues settled transition matches current process state.Briefly, operators encode protocol semantics follows. Operator activate-trans activatestransition process given type local state s1 s2 . operator sets predicateactivate. boolean flag precondition queue-read queue-write actions, setpropositions initialize reading/writing message. queue Q activated transitionquerying message m, corresponds Promela expression Q?m, respectively Q!m.read/write operation initialized, queue update operators must applied, i.e. advancequeue-head, advance-empty-queue-tail, advance-non-empty-queue-tail appropriate.names indicate, operators respectively update head tail positions, neededimplement requested read/write operation. operators also set settled flag,precondition every queue access action. Action process-trans applied. executestransition local state s1 s2 , i.e. sets new local process state re-sets flags.stored message match query, queue capacity either smalllarge, active local state transition block. active transitions process block,process block. processes blocked, deadlock system. Detectiondeadlocks implemented, different domain versions, either collection specificallyengineered actions or, elegantly, set derived predicates. cases one infer,along lines argumentation outlined above, process/the entire system blocked.519fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UG(:derived (blocked-trans ?p - process ?t - transition)(exists (?q - queue)(exists (?m - message)(exists (?n - number)(and (activate ?p ?t) (reads ?p ?q ?t) (settled ?q)(trans-msg ?t ?m) (queue-size ?q ?n) (is-zero ?n))))))(:derived (blocked ?p - process)(exists (?s - state)(exists (?pt - proctype)(and (at-process ?p ?s) (is-a-process ?p ?pt)(forall (?t - transition)(or (blocked-trans ?p ?t) (forall (?s2 - state) (not (trans ?pt ?t ?s ?s2)))))))))Figure 22: Derivation deadlock.goal condition makes planners detect deadlocks protocols simply conjunctionatoms requiring processes blocked. example derivation rules derivedpredicates, PDDL description derivation deadlock based blocked read accessesshown Figure 22.A.3.3 IPC-4 OMAIN TRUCTUREtwo benchmark protocols IPC-4, created three different domain versions:derivedpredicates, contains derived predicates infer deadlocks; plain, purely propositional specification specific actions applied establish deadlock (the lateractions basically Gazen Knoblock (1997) compilation derived predicates, c.f. Section 2); fluents alternative latter numerical state variables encodes sizequeues messages used access contents. also made version called fluentsderivedpredicates, obvious combination, none IPC-4 competitors participated there,omit herein. Within domain version, one formulation includes ADLconstructs quantification, disjunctive preconditions, negated preconditions. domainversions without fluents, another formulation pure STRIPS, obtained respective ADLencodings using adl2strips compiler (which handle numeric variables). Unfortunately,larger problem instances lead STRIPS files big stored disk(remember adl2strips grounds operator parameters). too-large instances were,course, left respective test suites.kept fluent-domains separated domain versions, rather domain version formulations,order able compare propositional numerical exploration efficiencies, emphasizefluent variables essential real-world model checking treated separately.domain versions numbers operators ground actions overviewed Table 3.Consider rows table top bottom. before, times parentheses valuesFFs reachability pre-process, builds relaxed planning graph initial stateremoves actions appear graph. STRIPS formulation fully groundedusing adl2strips program, derived FFs pre-processor (c.f. Section 2).520fiE NGINEERING B ENCHMARKSversionoptical-telegraphoptical-telegraphoptical-telegraph-dpoptical-telegraph-dpoptical-telegraph-fluentsphilosophersphilosophersphilosophers-dpphilosophers-dpphilosophers-fluentsformulationSTRIPSADLSTRIPS DPADL DPADLSTRIPSADLSTRIPS DPADL DPADLP LANNINGmax-#op334511401411118401113721111max-#act(3345) 3345(5070) 3345(4014) 4014(6084) 4014(1337) 1169(840) 840(930) 840(1372) 1372(1519) 1372(930) 930Table 3: Overview different domain versions Promela. Abbreviations used: dp derived predicates; max-#op maximum number (parameterized) PDDL operatorsinstance, max-#act maximum number ground actions instance.Data parentheses collected FFs reachability pre-process (see text). Derivation rules (ground derivation rules) counted operators (ground actions).reason number operators number ground actions, FFs preprocess identical one run adl2strips effect. ADL formulation, seereachability pruning reduces number actions factor almost 2, similar Airportdomain (c.f. Section A.1.3). picture next two domain versions, derived predicates,similar. fact, since, consistently data Section 5, count derivation rulesactions, data identical. reason identical Table 3 that, using derivedpredicates instead operators, FFs pre-processor scales larger instances (presumably, dueunimportant implementation detail). next domain version, formulated numericvariables, FFs pre-processor scales even worse. However, even instances numbertelegraphs, less ground actions before, due different encoding.observations made Dining Philosophers exactly same, different numbers.notable difference effect FFs reachability pruning weaker, yieldingslight decrease number actions versions without fluents, decreaseversion fluents. Apparently, complex process structure Optical Telegraph leadsuseless action instances.A.3.4 IPC-4 E XAMPLE NSTANCESsaid, selected two simple communication protocols benchmarks IPC-4: encoding Dining Philosopher problem described above, so-called Optical Telegraphprotocol (Holzmann, 1990).Optical Telegraph protocol involves n pairs communicating processes, pair featuring process. pair go fairly long, heavily interactive,sequence operations, implementing possible data exchange two stations.data exchanged, various initializing steps must taken ensure processes workingsynchronously. importantly, process writes token control channel (queue)521fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UGbeginning sequence, reads token end. causes deadlocksituation n control channels, accessed two processes.every pair up/down processes occupied one control channel, overall systemblocked.Dining Philosopher Optical Telegraph benchmark, instances scale viasingle parameter, number philosophers number control stations, respectively.scaled parameter 2 49 competition instances. Promela models benchmarks distributed together experimental model checking tool HSF-SPIN (Edelkamp,Leue, & Lluch-Lafuente, 2004), extends SPIN heuristic search strategies improve errordetection.A.3.5 F UTURE W ORKgeneral terms, see Promela planning benchmark another important step towards exploiting synergies research areas Planning Model Checking (Giunchiglia & Traverso,1999). example, complement recent progress planning, explicit directed model checkingdomain protocol validation (Edelkamp et al., 2004) symbolic directed model checkingdomain hardware validation (Reffel & Edelkamp, 1999) led drastic improvementsstate-of-the-art model checkers. work, e.g., (Yang & Dill, 1998; Bloem, Ravi,& Somenzi, 2000), show model checking growing interest guided exploration,mostly find errors faster blind state space enumeration algorithms. compilationPromela domain model, alternative option applying heuristic search model checkingproblems available. work needed understand planning heuristics work failmodel checking benchmarks.strongly believe communities profit wide-spread availability techniques represent Model Checking problems PDDL. allows direct comparison exploration efficiencies. Based design Promela domain, suitable PDDL domain encodingstwo expressive model checking input languages, Graph Transformation Systems (Edelkamp,Jabbar, & Lluch-Lafuente, 2005) Petri Nets (Edelkamp & Jabbar, 2005), proposed.encodings exploit expressive power PDDL well efficiency current planners.result, state-of-the-art planners often faster compared model checkers benchmarks.A.4 PSRSylvie Thiebaux others worked application domain. domain adaptedIPC-4 Sylvie Thiebaux Jorg Hoffmann.A.4.1 PPLICATION OMAINPower Supply Restoration (PSR) domain consider derived application investigated Sylvie Thiebaux others (Thiebaux et al., 1996; Thiebaux & Cordier, 2001). PSRdeals reconfiguring faulty power distribution system resupply customers affectedfaults. topic ongoing interest field power distribution.detail, power distribution system (see Figure 23), viewed network electric lines connected switches fed via number power sources equippedcircuit-breakers. Switches circuit-breakers two possible positions, open closed,522fiE NGINEERING B ENCHMARKSP LANNINGFigure 23: Sample power distribution system. Sources/circuit-breakers (e.g., CB4) representedlarge squares, switches (e.g., SD3) small squares. Open switches (e.g., SD8)white. area fed CB4 boxed. Gray dark used distinguish adjacentareas fed different sourcesconnected two lines. restriction connectivity lines, extremitiesalso connected earth. circuit-breaker power source closed,power flows source lines downstream, flow stopped open switch.switches used appropriately configure network position initially setline fed exactly one source.Due bad weather conditions, permanent faults affect one lines network.power source feeds faulty line, circuit-breaker fitted source opens protectrest network overloads. leaves lines fed source without power.problem consists planning sequence switching operations (opening closing switchescircuit-breakers) bringing network configuration maximum non-faulty linesresupplied. instance, suppose line l20 becomes faulty. leads circuit-breakerCB4 open boxed area without power. possible restoration plan wouldfollowing: open switches SD16 SD17 isolate faulty line, close SD15 sourceCB7 resupply l19, finally re-close CB4 resupply others.original PSR problem (Thiebaux & Cordier, 2001), maximal capacity sourceslines, well load requested customers taken account. plan must optimizevarious numerical parameters breakdown costs, power margins, distance initialconfiguration, subject capacity constraints. Furthermore, due fault sensors switchesunreliable, location faults current network configuration partiallyobservable. optimizing, leads complex tradeoff acting resupply linesacting (intrusively) reduce uncertainty.523fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UGA.4.2 IPC-4 PDDL DAPTATIONPDDL adaptation, benefited contributions Piergiorgio Bertoli, Blai Bonet, Alessandro Cimatti, John Slaney (Bertoli et al., 2002; Bonet & Thiebaux, 2003). Comparedoriginal PSR domain described above, IPC-4 version underwent 3 major adaptations. Firstly,IPC deals fully observable domains. Hence, partial observability PSR crucialissue (Thiebaux et al., 1996; Bertoli et al., 2002; Bonet & Thiebaux, 2003), IPC version assumescomplete observability. Secondly, given difficulty encoding even basic problem, choseignore numerical optimization aspects PSR (capacities, power margins, . . . ). Thirdly,IPC-4 version set pure goal-achievement problem, goal specifies setlines must (re)-supplied. considered realistic goal asking planner supplyline be. However, unable compile goal STRIPS reasonablespace, opted simpler goal keep STRIPS formulation consistent possibleothers.highest level natural IPC-4 encoding PSR involves ADL constructs derivedpredicates. Briefly, encoding works follows. PSR problem instances specify (1) networktopology, i.e., objects network connections (the lines, switching devices,is, switches sources/circuit-breakers, two side constants side1 side2 denotetwo connection points switching device, connection relations objects),(2) initial configuration, i.e., initial positions (open/closed) switching devices, (3)modes (faulty not) various lines. Among those, devices positions change.number predicates derived basic ones. model propagationcurrent network view determining lines currently fed sourcesaffected fault, i.e. feed fault. closed-world assumption semantics PDDL2.2derived predicates exactly needed elegantly encode relations. requirerecursive traversal network paths naturally represented transitive closureconnection relation network. complex derived predicates, upstream,requires four parameters, two which, however take two possible values, expressespower flows one two sides device (side ?sx device ?x) onesides another (side ?sy device ?y) happens side ?x opposite ?sxdirectly connected ?sy (via line), exists closed device ?z one sideupstream ?sx side connected ?sy:(:derived (upstream ?x - DEVICE ?sx - SIDE ?y - DEVICE ?sy - SIDE)(and (closed ?x)(or (and (= ?sx side1) (con ?x side2 ?y ?sy))(and (= ?sx side2) (con ?x side1 ?y ?sy))(exists (?z - DEVICE)(and (closed ?z)(or (and (con ?z side1 ?y ?sy) (upstream ?x ?sx ?z side2))(and (con ?z side2 ?y ?sy) (upstream ?x ?sx ?z side1))))))))upstream, relatively easy define predicates stating whether given line fed givensource affected.524fiE NGINEERING B ENCHMARKSP LANNINGgoal problem instance asks given lines fed sources unaffected.43available actions closing opening switching device. effect simply setdevice position requested. addition, action wait, models event circuitbreakers opening become affected. Wait applicable affected source exists,applicable action case (the open close actions require preconditionsource affected). This, together goal, ensures wait action appliedsoon source affected. effect wait action open affected circuit-breakers.Concretely, wait close actions follows (note open similar close earthtreated device whose position cannot changed actions):(:action close:parameters (?x - DEVICE):precondition (and (not (= ?x earth))(not (closed ?x))(forall (?b - DEVICE) (not (affected ?b)))):effect (closed ?x))(:action wait:parameters ():precondition (exists (?b - DEVICE) (affected ?b)):effect (forall (?b - DEVICE) (when (affected ?b) (not (closed ?b)))))would possible encode opening affected breakers conditional effectclose action. However, would required complex derived predicates additionaldevice parameter conditional flavor, specifying, e.g., whether circuit-breaker wouldaffected close device.A.4.3 IPC-4 OMAIN TRUCTUREused four domain versions PSR IPC-4. Primarily, versions differ sizeproblem instances encoded. instance size determined languages ableformulate domain version. tried generate instances size appropriate evaluatecurrent planners, i.e, scaled instances push-over everybody impossibly hardcurrent automated planners, got intuitions running version FF enhanceddeal derived predicates. largest instances kind size one typically encountersreal world. instance generation process said Section A.4.4.domain versions named 1. large, 2. middle, 3. middle-compiled, 4. small.Version 1 single formulation adl-derivedpredicates. Version 2 formulations adlderivedpredicates, simpleadl-derivedpredicates, strips-derivedpredicates. Version 3single formulation adl, version 4 single formulation strips. formulation names simply give language used. Version 1 contains largest instances, versions 2 3 contain (thesame) medium instances, version 4 contains smallest instances. adl-derivedpredicates43. Note circuit-breaker affected source opens, source affected more, feedline. Then, circuit-breaker closed again, source stay unaffected unless re-starts feeding faultyline.525fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UGversionlargemiddlemiddlemiddlemiddle-compiledsmallformulationADL DPADL DPSIMPLE-ADL DPSTRIPS DPADLSTRIPSmax-#op773485356059400max-#act(14038) 7498(7055) 3302(3485) 3485(3560) 3560(99) 71(9400) 9400Table 4: Overview different domain versions formulations PSR. Abbreviations used:dp derived predicates; max-#op maximum number (parameterized) PDDLoperators instance, max-#act maximum number ground actionsinstance. Data parentheses collected FFs reachability pre-process (seetext). Derivation rules (ground derivation rules) counted operators (ground actions).formulation inspired Bonet Thiebaux (2003); makes use derived predicates explained above, ADL constructs derived predicate, action, goal definitions.simpleadl-derivedpredicates strips-derivedpredicates formulations, ADL constructs (exceptconditional effects simpleadl case) compiled away. resulting fully grounded encodings significantly larger original, hand length plans remainsnearly unaffected44 . pure adl formulation obtained adl-derivedpredicates formulation compiling derived predicates away, using method described Thiebaux et al. (2003,2005). significant increase domain size, compilation method leadincrease plan length exponential arity derived predicates (no compilationmethod avoid blow-up worst case, see Thiebaux et al., 2003, 2005). Indeed,particular PSR example instances, observed considerable blow plan length. feltblow much allow useful direct comparison data generated adlderivedpredicates opposed adl, separated adl formulation domain version3 listed above.strips domain formulation proved quite challenge. 20 schemes considered compiling derived predicates ADL constructs away led either completelyunmanageable domain descriptions completely unmanageable plans. problem feasible compilations derived predicates create new actions highly conditional effects,compiling away impractical. therefore adopted different fully-grounded encoding inspired Bertoli et al. (2002). encoding generated description problem instancetool performing reasoning power propagation. resulting tasks, effectsclose actions directly specify circuit-breakers open result closing switch givennetwork configuration. derived predicates needed, consequently STRIPS encodingmuch simpler refers positions devices lines, faults, connections. Nevertheless, still able formulate comparatively small instances STRIPS,without prohibitive blow-up encoding size.44. variation due fact existential precondition wait action causes compilation splitaction many wait actions circuit-breakers526fiE NGINEERING B ENCHMARKSP LANNINGdomain versions, formulations, respective numbers operators ground actions, shown Figure 4. Data parentheses collected FFs reachability preprocess, building relaxed planning graph initial state removing actionsappear graph. encodings using ADL derived predicates, reduces number ground actions factor around 2; ADL, factor much smaller;encodings, reduction obtained, simply due fact encodings obtained adl2strips, uses pruning process. interesting observationsmade middle versions formulations. data shown correspond largestinstance FFs pre-processor could handle versions/formulations, enable direct comparison. see that, formulation SIMPLE-ADL STRIPS, need introduceground actions. also see that, curiously, compilation derived predicates (compilationmiddle-compiled), number ground actions decreases dramatically. reason liesdata count ground derivation rules ground actions, subtleties compilation derived predicates. middle formulations, almost ground actions factground derivation rules. compiled away middle-compiled following Thiebaux et al.(2003, 2005), introducing single action one distinct conditional effect derivation rule, c.f. Section 2. means complexity thousands derivation rulesreplaced complexity action thousands conditional effects.A.4.4 IPC-4 E XAMPLE NSTANCESDue contractual agreements, unable use real data competition. Instead, PSRinstances randomly generated using randomnet, special purpose tool implemented JohnSlaney.Power distribution networks often mesh-able structure exploited radially: path takenpower source forms tree whose nodes switches whose arcs electriclines; terminal switches connect various trees together. Randomnet takes input numbersources, percentage faulty lines, range parameters controlling tree depth, branching,tree adjacency, whose default values representative real networks. Randomnet randomlygenerates network topology set faulty lines. turned various PDDLencodings tool called net2pddl, implemented Piergiorgio Bertoli Sylvie Thiebaux.net2pddl computes set lines supplied, makes goal.instances generated make use randomnet default settings, two exceptionscreate problems increasing difficulty. first maximal depth trees takes rangevalues twice default. larger value, harder problem. secondpercentage faulty lines ranges 0.1 0.7. Problems middle range harderaverage, bottom range realistic.instance suite contains 50 instances. small instances feature 1 6 sources,middle instances feature 10 sources, large instances feature 100 sources.large instances size typical real-world instances, even larger. exampleFigure 23 representative difficult instance middle set.A.4.5 F UTURE W ORKPSR around time benchmark planning uncertainty, expectwork done framework IPC-4 facilitate acceptance one standard527fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UGbenchmarks planning. end, developed PSR resource web page giving accessrelevant papers, data, tools (net2pddl, randomnet, . . . ).45 One aspect future workcomplete maintain website, making available number already existing tools,SyDRe (Thiebaux et al., 1996), domain-specific system full PSR problem, Matt Graysnet2jpeg graphically displays networks generated randomnet.Considering future IPCs, potential extending PDDL encoding take numerical optimization aspects benchmark account. PDDL-like encodings partiallyobservable version benchmark exist (Bonet & Thiebaux, 2003) ready usedfuture edition probabilistic part IPC.46A.5 SatelliteSatellite domain introduced IPC-3 Long Fox (2003). motivated NASAspace application: number satellites take images number spatial phenomena,obeying constraints data storage space fuel usage. IPC-3, 5 versionsdomain, corresponding different levels language PDDL2.1: Strips, Numeric, SimpleTime(action durations constants), Time (action durations expressions static variables),Complex (durations numerics, i.e. union Numeric Time).adaptation Satellite domain IPC-4 done Jorg Hoffmann. IPC-3 domainversions example instances re-used, except SimpleTime like IPC-4 domains,didnt want introduce extra version distinction difference constantdurations static durations. top IPC-3 versions, 4 new domain versions added.idea make domain realistic additionally introducing time windowssending image data earth, i.e. antennas visible satellites certainperiods time according Derek Long, lack time windows main shortcomingIPC-3 domain.47extended IPC-3 Time domain version two IPC-4 domain versions, Time-timewindowsTime-timewindows-compiled. extended IPC-3 Complex domain version two IPC-4domain versions Complex-timewindows Complex-timewindows-compiled. cases, introduced new action sending data antenna. antenna receive datasingle satellite time, antenna visible subsets satellites certain timeperiods, sending image takes time proportional size image. timewindows modelled using timed initial literals, -compiled domain versions,literals compiled artificial PDDL constructs. None domain versions uses ADLconstructs, versions single (STRIPS) formulation.instances generated follows. objectives clearly demonstrate effectadditional time windows, produce solvable instances only. accomplish former,re-used IPC-3 instances, difference between, e.g., Time Time-timewindows,lies additional time window constructs. ensure solvability, implemented tool readplans produced one IPC-3 participants, namely TLPlan, arranged timewindows input plan suitable solve enriched instance. important note45. page available http://rsise.anu.edu.au/thiebaux/benchmarks/pds46. probabilistic part IPC-4 feature partially observable domains.47. learned meantime lack time windows gathering data also, even more,essential: often, due occlusion objects due rotation earth, targets visiblerestricted periods time. probably constitutes one important future directions domain.528fiE NGINEERING B ENCHMARKSP LANNINGtime windows arranged exactly meet times extracted IPC-3plan. Rather, introduced one time window per 5 take-image actions, made antennavisible time window respective 5 satellites, let image sizeindividual image random value within certain range time window 5 timeslong sending time resulting maximum possible size.course, generation process arranged rather arbitrarily, resulting instancesmight long way away typical characteristics Satellite problem occursreal world. isnt nice, best could without inside knowledgeapplication domain, advantage enriched instances solvable, directlycomparable IPC-3 ones.new domain versions derived Complex, also introduced utilities timewindow inside image sent earth. image, utility eitherwindows, decreases monotonically start time window, random withincertain interval. image put randomly one classes, optimizationrequirement minimize linear combination makespan, fuel usage, summed negatedimage utility.A.6 SettlersSettlers domain introduced IPC-3 Long Fox (2003). makes extensive usenumeric variables. variables carry domain semantics, buildinginfrastructure unsettled area, involving building housing, railway tracks, sawmills,etc. domain included IPC-4 order pose challenge numeric plannersdomains mostly make much use numeric variables, computing(static) durations actions.48 used exact domain file example instancesIPC-3, except removed universally quantified preconditions improve accessibilityplanners. quantifiers ranged domain constants could easily replacedconjunctions atoms.A.7 UMTSRoman Englert working application area several years. domain adaptedIPC-4 Stefan Edelkamp Roman Englert.A.7.1 PPLICATION OMAINProbably best known feature UMTS (Universal Mobile Telecommunication Standard)higher bit rate (Holma & Toskala, 2000): packet-switched connections reach 2 megabit per second (Mbps) optimal case. Compared existing mobile networks, UMTS providesnew important feature, namely negotiation Quality Service (QoS) transferproperties. attributes define characteristics transfer throughput, transfer delay, data error rate. UMTS bearers generic order provide good supportexisting applications evolution new applications. Applications services divided48. Note that, extent, numeric values abstracted away PDDL encoding,mostly (in Airport Pipesworld, c.f. Sections A.1.5 A.2.5) order obtain discrete encoding suitablePDDL2.2-style actions.529fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UGClassConstraintsExamplesConversationalPreserve timerelationinformation flowstream.Conversationalpattern (low delay)Voice, videotelephony &video gamesStreamingPreserve timerelationinformationentitiesstreamInteractiveRequest response pattern.Preserve dataintegrityBackgroundUndefineddelay.PreservedataintegrityStreamingmultimediaWeb browsing,network gamesBackgrounddownloade-mailsTable 5: UMTS quality service classes characteristics.four traffic classes QoS (TS23107, 2002; Holma & Toskala, 2000). traffic classes,fundamental characteristics, examples applications summarized Table 5.main distinguishing factor classes delay-sensitive traffic is:conversational class delay sensitive (approximately 40 ms time preservation), background class defined maximum delay.UMTS call set-up modularized using perspective Intelligent Software Agents(Appleby & Steward, 1999; Busuioc, 1999), since agents logical units enable discreteperspective continuous signaling process. call set-up partitioned followingmodules executed sequential order (Englert, 2005):TRM initial step initiation application mobile determinationrequired resources execution. resources mobile like display memorychecked Terminal Resource Management (TRM) allocated, possible. Otherwise,execution aborted.CT wireless connection radio network initiated via dedicated control channelGSM (Holma & Toskala, 2000). case success, transmission Ready servicetransferred via node B mobile order ensure Connection Timing (CT)bearer service availability.information mobile like location data handling capabilities sent application server Internet (cf. AEEI). transmission done comfortablyso-called service agent (Farjami, Gorg, & Bell, 2000) controlled Agent Management (AM) CND. advantage service agent is, case failure, e.g.,network resources sufficiently available, agent negotiate terminalsagent another QoS class different quality parameters.AEEM service agent required QoS class execution applicationparameters mobile application sent mobiles Agent Execution EnvironmentMobile (AEEM) application server Internet (cf. AEEI).RRC Radio Resource Controller (RRC) provisions/allocates required QoS logical resources MAC level radio bearer (Holma & Toskala, 2000).530fiE NGINEERING B ENCHMARKSP LANNINGRAB Then, bearer resources supplied physical level Radio Access Bearer(RAB) CND call flow set-up mapping logical QoS parametersphysical QoS resources together.AEEI Agent Execution Environment Internet (AEEI) establishes data transfercore network PDN (e.g., Internet) sends service agent (controlled AM)application PDN order ensure QoS application.BS Finally, Bearer Service (BS) execution mobile application establishedrequired radio bearer resources QoS. Messages sent modules TRMAEEI start execution application.modules executed sequential order set-up call execution mobileapplications. Two modules (AEEM AEEI) executed time windows orderensure agents life network. However, two constraints added: First,intra-application constraint, modules one application ordered. Second, interapplication constraint, modules names different applications cannot executed parallel order ensure required resources available.A.7.2 IPC-4 PDDL DAPTATIONBesides action duration, domain encodes scheduling types resources49 , consumingamount action initialization time releasing amount action ending time. Schedulingtypes resources used planning benchmarks before, good newstemporal PDDL2.1 (Level 3) capable expressing them. fact used similar encodingone found Job- Flow-Shop problems. one feature, actions definedtemporarily produce rather temporarily consume resources. current PDDL waystating resource constraints explicitly, planners want exploit knowledgelook certain patterns increase/decrease effects recognize them. Additionally, resourcemodeling UMTS adaptation constrained important parameters (in total 15).real networks several hundred parameters applied.UMTS, two subsequent actions check update value resources (e.g.,has-mobile-cpu) starting (resp. ending) time points far start (resp. ending) eventsseparated time steps, minimum slack time required two dependentevents. modeling renewable resources construct invariant conditionaction check, start event change. decided best choiceproper temporal action. Consequently, temporal actions require resources availableadding amount used.Finally, time windows two agent-based modules defined using average execution times modules. average times estimated based signaling durationsUMTS network (Holma & Toskala, 2000).Resources may renewable consumable: example renewable resource keyboard mobile. used input data several applications. Consumable resources49. terminology resources planning scheduling varies. job-shop scheduling, machine resource,planning machine would domain object. PDDL, renewable consumable resourcesmodeled using numerical fluents per se distinguished.531fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UGmobile-cpud-availablee-balancemobile-channels-availablenum-mobilesnum-callsmobile-storagelogical-channelscell-updatehandoveractive-set-upggsn-bitratemax-no-pdpmax-no-apnused x per cent per applicationpartition display, e.g., ticker chessenergy balance mobile accumulatorused data transfernumber mobiles tractablenode Bmobile network load node Bmemory S(IM)AT cardnumber logical channels available CNreport UE location RNChandover required get higher bit rateupdate connectioncapacity (kbit/s) GGSN PDNmax. no. packet data protocols per mobilemax. access point names (APN) per mobileTable 6: Scheduling types resources UMTS call set-up.released action execution. resources realized experiments summarizedTable 6 (see 3GPP, 2004 complete list resources UMTS call set-up).PDDL representation planning domain based eight modules UMTScall set-up. eight operators corresponding eight modules. Let us consider,example, BS action, is, final action used establish predicate bs-ok.defined follows:(:durative-action BS:parameters(?A-new - application ?M - mobile ?L - list ?MS1 ?MS2 - message ?a - agent):duration(= ?duration (time-bs ?A-new)):condition(and (at start (initiated ?A-new ?M))(at start (aeei-ok ?A-new ?M ?L ?a))(at start (qos-params ?A-new ?L))(at start (message-trm ?M ?MS1))(at start (message-aeei ?A-new ?MS2))):effect(and (at end (iu-bearer ?A-new ?M ?L)) (at end (bs-ok ?A-new ?M ?L ?a)))))action preconditions successful execution module AEEI callset-up, satisfaction required QoS class parameters (denoted list L), transferedmessages set-up status application mobile PDN. resources alreadyallocated preceding modules. effect bearer network connection mobileapplication set up.532fiE NGINEERING B ENCHMARKSP LANNINGinitiation application starts mobile TRM. Afterwards, CTasked ready-for-service signal. core call set-up radio access bearerprocedure CND. Let us consider latter detail. first step logical resourcesmust allocated (RRC), e.g., required number channels must provided logicallevel radio bearer later logical resources mapped physical channels.PDDL RRC action looks follows:(:durative-action RRC:parameters(?A-new - application ?M - mobile ?L - list ?a - agent):duration(= ?duration (time-rrc ?A-new)):condition(and (at start (ct-ok ?A-new ?M ?L))(at start (aeem-ok ?A-new ?M ?L ?a))(at start (<= (has-logical-channels)(- (max-logical-channels) (app-channels ?A-new ?m))))(at start (<= (has-cell-update) (- (max-cell-update) 2)))(at start (< (has-handover) (max-handover)))(at start (< (has-active-set-up) (max-active-set-up)))):effect(and (at start (increase (has-logical-channels) (app-channels ?A-new ?M)))(at end (decrease (has-logical-channels) (app-channels ?A-new ?M)))(at start (increase (has-cell-update) 2))(at end (decrease (has-cell-update) 2))(at start (increase (has-handover) 1))(at end (decrease (has-handover) 1))(at start (increase (has-active-set-up) 1))(at end (decrease (has-active-set-up) 1))(at end (rrc-ok ?A-new ?M ?L ?a))))requested QoS class available, fact rab-ok true serviceagent must sent mobile order negotiate application user weaker QoSrequirements. case success predicate rab-ok true connection PDN mustchecked. Finally, goal predicate BS fulfilled resources available.A.7.3 IPC-4 OMAIN TRUCTUREused IPC-4, UMTS domain six versions. first three are: temporal, domainversion timing constraints, temporal-timewindows, domain version PDDL2.2 timedinitial facts, temporal-timewindows-compiled, domain version PDDL2.1 wrapper encoding timed initial literals. second domain version set flaw-temporal, flaw-temporaltimewindows, flaw-temporal-timewindows-compiled, includes following flaw action:(:durative-action FLAWparameters(?A-new - application ?M - mobile ?L - list ?a - agent):duration (= ?duration 4):condition533fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UGversiontemporaltemporal-twtemporal-twcflaw-temporalflaw-temporal-twflaw-temporal-twcformulationSTRIPS-TEMPORALSTRIPS-TEMPORAL-TWSTRIPS-TEMPORALSTRIPS-TEMPORALSTRIPS-TEMPORAL-TWSTRIPS-TEMPORALmax-#op88139914max-#act(5120) 80(5120) 80(5125) 85(5310) 90(5310) 90(5315) 95Table 7: Overview different domain versions UMTS. Abbreviations used: temporaltw temporal-timewindows, temporal-twc temporal-timewindows-compiled;max-#op maximum number (parameterized) PDDL operators instance,max-#act maximum number ground actions instance. Data parentheses collected FFs reachability pre-process (see text).(and (at start (initiated ?A-new ?M))(at start (qos-params ?A-new ?L))(at start (trm-ok ?A-new ?M ?L))):effect(and (at end (rab-ok ?A-new ?M ?L ?a))(at start (not (initiated ?A-new ?M)))))action offers shortcut rab-ok predicate, used real solutiondeletes initiated predicate. action used heuristic functionsbased ignoring negative effects. sense, action encodes flaw may disturbheuristic techniques used modern planners. determine action useful, negative interactions considered. idea flaw practically motivated order seeheuristic planners react it. standard form, domain big challengeplanners, seen Section 5. domain versions one formulation, namely stripsfluents-temporal, numerical fluents, - except typing - ADL constructs used.instances, plan objective minimize makespan.domain versions numbers operators ground actions overviewed Table 7.many empirical data UMTS seen before, data quite exceptional,time easy interpret. First, similar seen Section 5.3, dataactually constant across instances within domain version, duefact instances scale specification applications need actually started.Second, numbers operators actions differ versions withouttime windows; increase somewhat, additional artificial actions, compiletimed initial literals away (c.f. Section 2); also increase somewhat, course, introduceflaw action. Third, striking observation huge effect FFs reachability preprocessor, building relaxed planning graph initial state removing actionsappear graph. due technical subtleties encoding, restrictionsfeasible action instantiations are, partly, implicit possible action sequences, ratherexplicit static predicates.534fiE NGINEERING B ENCHMARKSP LANNINGA.7.4 IPC-4 E XAMPLE NSTANCESUMTS call set-up domain following challenges planning task (Englert & Cremers, 2001):Real-time: plans execution mobile applications generated appropriate time?Planning done maximum duration exceed UMTS call set-uptime.Completeness: possible generate plan, i.e. planning result (optimal) planrequired applications minimizes waiting period applications started?PDDL structure basic problem discrete UMTS call set-up (DUCS) domainfollowing:(define (problem DUCS DOMAIN BASIC VERSION)(:domain DUCS DOMAIN BASIC VERSION(:objects MS1 MS2 - messageA1 A2 A3 A4 A5 A6 A7 A8 A9 A10 - applicationM1 M2 M3 M4 M5 M6 M7 M8 M9 M10 - mobileL1 L2 L3 L4 L5 L6 L7 L8 L9 L10 - listae - agent)(:init (= (time-trm A1) 76) (= (time-ct A1) 48)(= (time-am A1) 74) (= (time-aeem A1) 66)(= (time-rrc A1) 202) (= (time-rab A1) 67)(= (time-aeei A1) 36) (= (time-bs A1) 28)[...](location M1) ;; types(authentification M1)[...](= (has-mobile-cpu) 0) ;; current status[...] )(:goal (and (bs-ok A1 M1 L1 ae) [...] )))First PDDL description come objects applications mobiles.come durations modules depending applications, e.g., module TRM requiresless time news ticker chess game, since latter requires terminal resourcesticker. current status resources initialized. Finally, goal defined:bearer establishment execution start initiated mobile applications. total executiontime minimized.IPC-4 time windows varied small perturbations order generate differentinstances. perturbations motivated average execution times modules radionetwork according load. Furthermore, number applications set varied 110. domains assume applications run one mobile terminal. However,also distributed several mobile terminals. 50 different instances per domain version.A.7.5 F UTURE W ORKUMTS domain big challenge modern heuristic, i.e. HSP/FF/LPG-style, plannersplanners satisficing (potentially return sub-optimal plans). objective UMTS535fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UGminimize execution time, one ignores objective task trivializes.optimal planners, UMTS realistic challenge. domain already relatively realisticallymodelled, except left-out additional constraints (many) less important resources.remains seen if, introducing resources, planner (in particular optimal planner)performance gets degraded. option case may introduce explicit language constructsdifferent types (renewable consumable) resources.future following two challenges shall investigated. First, negotiation UMTSQuality Service (QoS) parameters could considered. Assume video application mobileterminal initiated, bearer resources sufficiently available. QoSnegotiated terminal bearer. leads planning negotiationplan execution already initiated applications.Second, approach optimization UMTS call set-up applied WirelessLAN registration. challenge transfer QoS parameters, since current Wireless LANstandard (802.11b) contain QoS. demerit solved applying additionalservice level addresses QoS.References3GPP (2004). 3G Partnership Project, www.3gpp.org.Appleby, S., & Steward, T. (1999). Mobile Software Agents Control TelecommunicationNetworks, chap. 11 Hayzelden, A./Bigham, J. (eds.), Software Agents Future Telecommunication Systems. Springer.Apt, K., Blair, H., & Walker, A. (1988). Towards theory declarative knowledge. FoundationsDeductive Databases Logic Programming, pp. 89148. Morgan Kaufmann.Bacchus, F., & Kabanza, F. (2000). Using temporal logics express search control knowledgeplanning. Artificial Intelligence, 116, 123191.Bacchus, F. (2000). Subset PDDL AIPS2000 Planning Competition. AIPS-00 Planning Competition Comitee. Available http://www.cs.toronto.edu/aips2000/pddl-subset.ps.Bacchus, F. (2001). AIPS00 planning competition. AI Magazine, 22(3), 4756.Bertoli, P., Cimatti, A., Roveri, M., & Traverso, P. (2001). Planning nondeterministic domainspartial observability via symbolic model checking.. Nebel (Nebel, 2001).Bertoli, P., Cimatti, A., Slaney, J., & Thiebaux, S. (2002). Solving power supply restoration problems planning via symbolic model checking. Proceedings 15th EuropeanConference Artificial Intelligence (ECAI-02), pp. 57680, Lyon, France. Wiley.Biundo, S., Myers, K., & Rajan, K. (Eds.)., ICAPS-05 (2005). Proceedings 15th International Conference Automated Planning Scheduling (ICAPS-05), Monterey, CA, USA.Morgan Kaufmann.Bloem, R., Ravi, K., & Somenzi, F. (2000). Symbolic guided search CTL model checking.Conference Design Automation (DAC), pp. 2934.Blum, A. L., & Furst, M. L. (1997). Fast planning planning graph analysis. ArtificialIntelligence, 90(1-2), 279298.536fiE NGINEERING B ENCHMARKSP LANNINGBoddy, M., Gohde, J., Haigh, T., & Harp, S. (2005). Course action generation cyber securityusing classical planning.. Biundo et al. (Biundo, Myers, & Rajan, 2005), pp. 1221.Bonet, B., & Geffner, H. (2001). Planning heuristic search. Artificial Intelligence, 129(12),533.Bonet, B., Loerincs, G., & Geffner, H. (1997). robust fast action selection mechanismplanning. Proceedings 14th National Conference American AssociationArtificial Intelligence (AAAI-97), pp. 714719. MIT Press.Bonet, B., & Thiebaux, S. (2003). GPT meets PSR. Giunchiglia, E., Muscettola, N., & Nau,D. (Eds.), Proceedings 13th International Conference Automated PlanningScheduling (ICAPS-03), pp. 102111, Trento, Italy. Morgan Kaufmann.Busuioc, M. (1999). Distributed Intelligent Agents - Solution Management ComplexTelecommunications Services, chap. 4 Hayzelden, A./Bigham, J. (eds.), Software AgentsFuture Telecommunication Systems. Springer.Bylander, T. (1994). computational complexity propositional STRIPS planning. ArtificialIntelligence, 69(12), 165204.Cesta, A., & Borrajo, D. (Eds.). (2001). Recent Advances AI Planning. 6th European ConferencePlanning (ECP01), Toledo, Spain. Springer-Verlag.Chen, Y., Hsu, C., & Wah, B. (2004). SGPlan: Subgoal partitioning resolution planning.Edelkamp, S., Hoffmann, J., Littman, M., & Younes, H. (Eds.), Proceedings 4thInternational Planning Competition, Whistler, BC, Canada. JPL.Chien, S., Kambhampati, R., & Knoblock, C. (Eds.)., AIPS-00 (2000). Proceedings 5thInternational Conference Artificial Intelligence Planning Systems (AIPS-00). AAAI Press,Menlo Park.Cimatti, A., Roveri, M., & Traverso, P. (1998). Automatic OBDD-based generation universalplans non-deterministic domains. Proceedings 15th National ConferenceAmerican Association Artificial Intelligence (AAAI-98), pp. 875881, Madison, WI. MITPress.Clarke, E. M., Grumberg, O., & Peled, D. A. (1999). Model Checking. MIT Press.Dierks, H. (2005). Finding optimal plans domains restricted continuous effects uppaal cora. ICAPS Workshop Verification Validation Model-Based PlanningScheduling Systems.Edelkamp, S. (2003a). Promela planning. Workshop Model Checking Software (SPIN), Lecture Notes Computer Science, pp. 197212. Springer.Edelkamp, S. (2003b). Taming numbers durations model checking integrated planningsystem. Journal Artificial Intelligence Research, 20, 195238.Edelkamp, S., & Jabbar, S. (2005). Action planning directed model checking Petri nets.Electronic Notes Theoretical Computer Science, 149(2), 318.Edelkamp, S., Jabbar, S., & Lluch-Lafuente, A. (2005). Action planning graph transition systems. ICAPS Workshop Verification Validation Model-Based PlanningScheduling Systems, pp. 4857.537fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UGEdelkamp, S., Leue, S., & Lluch-Lafuente, A. (2004). Directed explicit-state model checkingvalidation communication protocols. International Journal Software Tools Technology, 5, 247 267.Englert, R. (2005). Planning optimize UMTS call set-up execution mobile applications. Int. Journal Applied Artificial Intelligence, 19(2), 99117.Englert, R., & Cremers, A. B. (2001). Configuration Applications 3rd Generation MobileCommunication. KI Workshop AI Planning, Scheduling, Configuration Design(PUK). Vienna, Austria.Farjami, P., Gorg, C., & Bell, F. (2000). Advanced service provisioning based mobile agents.Computer Communications, 23, 754 760.Fikes, R. E., & Nilsson, N. (1971). STRIPS: new approach application theorem provingproblem solving. Artificial Intelligence, 2, 189208.Fourman, M. P. (2000). Propositional planning. AIPS Workshop Model-Theoretic ApproachesPlanning.Fox, M., Long, D., & Halsey, K. (2004). investigation expressive power PDDL2.1.Saitta, L. (Ed.), Proceedings 16th European Conference Artificial Intelligence(ECAI-04), Valencia, Spain. Wiley.Fox, M., & Long, D. (1999). detection exploitation symmetry planning problems.Pollack, M. (Ed.), Proceedings 16th International Joint Conference ArtificialIntelligence (IJCAI-99), pp. 956961, Stockholm, Sweden. Morgan Kaufmann.Fox, M., & Long, D. (2003). PDDL2.1: extension PDDL expressing temporal planningdomains. Journal Artificial Intelligence Research, 20, 61124.Frank, J., Cheeseman, P., & Stutz, J. (1997). gravity fails: Local search topology. JournalArtificial Intelligence Research, 7, 249281.Garagnani, M. (2000). correct algorithm efficient planning preprocessed domain axioms.Research Development Intelligent Systems XVII. Springer-Verlag.Gazen, B. C., & Knoblock, C. (1997). Combining expressiveness UCPOP efficiencyGraphplan.. Steel, & Alami (Steel & Alami, 1997), pp. 221233.Gerevini, A., & Long, D. (2005). Plan Constraints Preferences. AIPS-06 Planning Competition Comitee. Available http://zeus.ing.unibs.it/ipc-5/pddl-ipc5.pdf.Gerevini, A., Saetti, A., & Serina, I. (2006). approach temporal planning schedulingdomains predictable exogenous events. Journal Artificial Intelligence Research, 25,187231.Giunchiglia, F., & Traverso, P. (1999). Planning model checking. Biundo, S., & Fox, M.(Eds.), Recent Advances AI Planning. 5th European Conference Planning (ECP99),Lecture Notes Artificial Intelligence, pp. 119, Durham, UK. Springer-Verlag.Haslum, P., & Geffner, H. (2001). Heuristic planning time resources.. Cesta, & Borrajo(Cesta & Borrajo, 2001), pp. 121132.Hatzack, W. (2002). Entwicklung und Auswertung von Algorithmen zur autonomen Verkehrskoordinierung und Konfliktauflsung Flughfen. Ph.D. thesis, University Freiburg, Freiburg,Germany.538fiE NGINEERING B ENCHMARKSP LANNINGHatzack, W., & Nebel, B. (2001). operational traffic control problem: Computational complexity solutions.. Cesta, & Borrajo (Cesta & Borrajo, 2001), pp. 4960.Helmert, M. (2003). Complexity results standard benchmark domains planning. ArtificialIntelligence, 143, 219262.Helmert, M. (2004). planning heuristic based causal graph analysis.. Koenig et al. (Koenig,Zilberstein, & Koehler, 2004), pp. 161170.Helmert, M. (2005) Personal communication.Helmert, M. (2006a). fast downward planning system. Journal Artificial Intelligence Research, 26. Accepted Publication.Helmert, M. (2006b). New complexity results classical planning benchmarks. Long, D., &Smith, S. (Eds.), Proceedings 16th International Conference Automated PlanningScheduling (ICAPS-06), pp. 5261, English Lake District, UK. Morgan Kaufmann.Hoffmann, J. (2001). Local search topology planning benchmarks: empirical analysis..Nebel (Nebel, 2001), pp. 453458.Hoffmann, J. (2002). Local search topology planning benchmarks: theoretical analysis.Ghallab, M., Hertzberg, J., & Traverso, P. (Eds.), Proceedings 6th International Conference Artificial Intelligence Planning Scheduling (AIPS-02), pp. 92100, Toulouse,France. Morgan Kaufmann.Hoffmann, J. (2003). Metric-FF planning system: Translating ignoring delete lists numericstate variables. Journal Artificial Intelligence Research, 20, 291341.Hoffmann, J. (2005). ignoring delete lists works: Local search topology planning benchmarks. Journal Artificial Intelligence Research, 24, 685758.Hoffmann, J., & Edelkamp, S. (2005). deterministic part IPC-4: overview. JournalArtificial Intelligence Research, 24, 519579.Hoffmann, J., & Nebel, B. (2001). FF planning system: Fast plan generation heuristicsearch. Journal Artificial Intelligence Research, 14, 253302.Holldobler, S., & Stor, H.-P. (2000). Solving entailment problem fluent calculus usingbinary decision diagrams. ICAPS Workshop Model-Theoretic Approaches Planning.Holma, H., & Toskala, A. (2000). WCDMA UMTS - Radio Access 3rd Generation MobileCommunications. Wiley & Sons.Holzmann, G. (2003). Spin Model Checker - Primer Reference Manual. Addison-Wesley.Holzmann, G. J. (1990). Design Validation Computer Protocols. Prentice Hall.Howe, A., & Dahlman, E. (2002). critical assessment benchmark comparison planning.Journal Artificial Intelligence Research, 17, 133.Kabanza, F., & Thiebaux, S. (2005). Search control planning temporally extended goals..Biundo et al. (Biundo et al., 2005), pp. 130139.Koehler, J., & Hoffmann, J. (2000). instantiation ADL operators involving arbitraryfirst-order formulas. ECAI Workshop New Results Planning, Scheduling Design.539fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UGKoehler, J., Nebel, B., Hoffmann, J., & Dimopoulos, Y. (1997). Extending planning graphsADL subset.. Steel, & Alami (Steel & Alami, 1997), pp. 273285.Koehler, J., & Schuster, K. (2000). Elevator control planning problem.. Chien et al. (Chien,Kambhampati, & Knoblock, 2000), pp. 331338.Koenig, S., Zilberstein, S., & Koehler, J. (Eds.)., ICAPS-04 (2004). Proceedings 14th International Conference Automated Planning Scheduling (ICAPS-04), Whistler, Canada.Morgan Kaufmann.Korf, R. E. (1990). Real-time heuristic search. Artificial Intelligence, 42, 189211.Kvarnstrom, J., Doherty, P., & Haslum, P. (2000). Extending TALplanner concurrencyressources. Horn, W. (Ed.), Proceedings 14th European Conference ArtificialIntelligence (ECAI-00), pp. 501505, Berlin, Germany. Wiley.Lago, U. D., Pistore, M., & Traverso, P. (2002). Planning language extended goals.Proceedings 18th National Conference American Association ArtificialIntelligence (AAAI-02), pp. 447454, Edmonton, AL. MIT Press.Long, D., & Fox, M. (2000). Automatic synthesis use generic types planning.. Chienet al. (Chien et al., 2000), pp. 196205.Long, D., & Fox, M. (2003). 3rd international planning competition: Results analysis.Journal Artificial Intelligence Research, 20, 159.McDermott, D. (1996). heuristic estimator means-ends analysis planning. Proceedings3rd International Conference Artificial Intelligence Planning Systems (AIPS-96),pp. 142149. AAAI Press, Menlo Park.McDermott, D. (1998). PDDL Planning Domain Definition Language. AIPS-98 Planning Competition Comitee. Available http://ls5-www.cs.uni-dortmund.de/ edelkamp/ipc4/DOCS/pddl.ps.gz.McDermott, D. (2000). 1998 AI planning systems competition. AI Magazine, 21(2), 3555.McDermott, D. V. (1999). Using regression-match graphs control search planning. ArtificialIntelligence, 109(1-2), 111159.Milidiu, R. L., & dos Santos Liporace, F. (2004a). Plumber, pipeline transportation planner.International Workshop Harbour Maritime Simulation (HMS), pp. 99106, Rio deJaneiro, Brazil.Milidiu, R. L., & dos Santos Liporace, F. (2004b). Pipesworld: Applying planning systemspipeline transportation. Proceedings International Pipeline Conference (IPC), pp.713719.Nebel, B. (Ed.)., IJCAI-01 (2001). Proceedings 17th International Joint Conference Artificial Intelligence (IJCAI-01), Seattle, Washington, USA. Morgan Kaufmann.Nebel, B. (2000). compilability expressive power propositional planning formalisms.Journal Artificial Intelligence Research, 12, 271315.Pednault, E. P. (1989). ADL: Exploring middle ground STRIPS situationcalculus. Brachman, R., Levesque, H. J., & Reiter, R. (Eds.), Principles KnowledgeRepresentation Reasoning: Proceedings 1st International Conference (KR-89), pp.324331, Toronto, ON. Morgan Kaufmann.540fiE NGINEERING B ENCHMARKSP LANNINGReffel, F., & Edelkamp, S. (1999). Error detection directed symbolic model checking. WorldCongress Formal Methods (FM), pp. 195211.Rintanen, J. (2004). Phase transitions classical planning: experimental study.. Koenig et al.(Koenig et al., 2004), pp. 101110.Ruml, W., Do, M., & Fromherz, M. (2005). On-line planning scheduling high-speed manufacturing.. Biundo et al. (Biundo et al., 2005), pp. 3039.Steel, S., & Alami, R. (Eds.). (1997). Recent Advances AI Planning. 4th European ConferencePlanning (ECP97), Vol. 1348 Lecture Notes Artificial Intelligence, Toulouse, France.Springer-Verlag.Thiebaux, S., & Cordier, M.-O. (2001). Supply restoration power distribution systemsbenchmark planning uncertainty.. Cesta, & Borrajo (Cesta & Borrajo, 2001), pp.8595.Thiebaux, S., Cordier, M.-O., Jehl, O., & Krivine, J.-P. (1996). Supply restoration power distribution systems case study integrating model-based diagnosis repair planning.Horvitz, E., & Jensen, F. V. (Eds.), Proceedings 12th International ConferenceUncertainty AI (UAI-96), pp. 525532, Portland, Oregon, USA. Morgan Kaufmann.Thiebaux, S., Hoffmann, J., & Nebel, B. (2003). defense PDDL axioms.. Gottlob, G. (Ed.),Proceedings 18th International Joint Conference Artificial Intelligence (IJCAI-03),pp. 961966, Acapulco, Mexico. Morgan Kaufmann.Thiebaux, S., Hoffmann, J., & Nebel, B. (2005). defense PDDL axioms. Artificial Intelligence,168(12), 3869.TS23107 (2002). 3rd Generation Partnership Project: Technical Specification Group ServiceSystem Aspects: QoS Concept Architecture (Release 5), TS 23.107, V5.3.0, 3GPP.Vidal, V. (2004). lookahead strategy heuristic search planning.. Koenig et al. (Koenig et al.,2004), pp. 150160.Wah, B., & Chen, Y. (2004). Subgoal partitioning global search solving temporal planningproblems mixed space. International Journal Artificial Intelligence Tools, 13(4), 767790.Yang, C. H., & Dill, D. L. (1998). Validation guided search state space. ConferenceDesign Automation (DAC), pp. 599604.Younes, H., Littman, M., Weissman, D., & Asmuth, J. (2005). first probabilistic trackinternational planning competition. Journal Artificial Intelligence Research, 24, 85188.541fiJournal Artificial Intelligence Research 26 (2006) 289-322Submitted 10/04; published 07/06Breaking Instance-Independent SymmetriesExact Graph ColoringArathi RamaniIgor L. MarkovKarem A. Sakallahramania@umich.eduimarkov@eecs.umich.edukarem@eecs.umich.eduDepartment Electrical Engineering Computer ScienceUniversity Michigan, Ann Arbor, USAFadi A. Aloulfaloul@umich.eduDepartment Computer EngineeringAmerican University Sharjah, UAEAbstractCode optimization high level synthesis posed constraint satisfactionoptimization problems, graph coloring used register allocation. Graph coloringalso used model traditional CSPs relevant AI, planning, time-tablingscheduling. Provably optimal solutions may desirable commercial defense applications. Additionally, applications register allocation code optimization,naturally-occurring instances graph coloring often small solved optimally.recent wave improvements algorithms Boolean satisfiability (SAT) 0-1 Integer Linear Programming (ILP) suggests generic problem-reduction methods, ratherproblem-specific heuristics, (1) heuristics may upset new constraints, (2)heuristics tend ignore structure, (3) many relevant problems provably inapproximable.Problem reductions often lead highly symmetric SAT instances, symmetriesknown slow SAT solvers. work, compare several avenues symmetry breaking, particular certain kinds symmetry present generatedinstances. focus reducing CSPs SAT allows us leverage recent dramaticimprovement SAT solvers automatically benefit future progress.use variety black-box SAT solvers without modifying source codesymmetry-breaking techniques static, i.e., detect symmetries add symmetrybreaking predicates (SBPs) pre-processing.important result work among types instance-independent SBPsstudied combinations, simplest least complete constructionseffective. experiments also clearly indicate instance-independent symmetriesmostly processed together instance-specific symmetries ratherspecification level, contrary suggested literature.1. IntroductionDetecting using problem structure, symmetries, often usefulaccelerating search solutions constraint satisfaction problems (CSPs).particularly true algorithms perform exhaustive searches benefit pruning search tree. work conducts theoretical empirical study impactbreaking structural symmetries 0-1 ILP reductions exact graph coloring problemc2006AI Access Foundation. rights reserved.fiRamani, Aloul, Markov, & Sakallahapplications number fields. example, compiler design, many techniques code optimization high-level synthesis operate relatively objectstime. Graph coloring used register allocation program compilation (Chaitin,Auslander, Chandra, Cocke, Hopkins, & Markstein, 1981) limited small numbersregisters embedded processors well number local variables virtualregisters. Graph coloring also relevant AI applications planning, scheduling,map coloring. Recent work graph coloring AI included algorithms basedneural networks (Jagota, 1996), evolutionary algorithms (Galinier & Hao, 1999), scattersearch (J.-P. Hamiez, 2001) several approaches discussed Section 2.many search procedures heuristic, work focuses exact graph coloring,closely related several useful combinatorial problems maximal independent set vertex cover. seek provably optimal solutions maydesirable commercial defense applications competitive reasons, oftenfound. work focuses solving exact graph coloring reduction 0-1 ILP.idea solving N P complete problems reduction well-known, rarely usedpractice algorithms developed standard problems, SAT, maycompetitive domain-specific techniques aware problem structure. However,many applications imply problem-specific constraints non-trivial objective functions.extensions may upset heuristics standard problems. Heuristics, particularlybased local search, often fail use structure problem instances (Prestwich, 2002)inefficient used problem reductions. contrast, exact solvers basedbranch-and-bound back-tracking tend adapt new constraints appliedproblem reduction. growing literature handling structure optimalsolvers (Aloul, Ramani, Markov, & Sakallah, 2003; Crawford, Ginsberg, Luks, & Roy, 1996;Huang & Darwiche, 2003), work falls category well.NP-spec project (Cadoli, Palopoli, Schaerf, & Vasileet, 1999) offers frameworkformulating wide range combinatorial problems automatically reducinginstances instances Boolean satisfiability. approach attractive circumvents problem-specific solvers leverages recent breakthroughs Boolean satisfiability(Moskewicz, Madigan, Zhao, Zhang, & Malik, 2001). However, approach remains unexplored practice, possibly efficiency problem-solving may reduceddomain-specific structure lost problem reductions. drawback addressedrecent work detection structure, particularly symmetry, SAT 0-1 ILPinstances order accelerate exact solvers (Crawford et al., 1996; Aloul et al., 2003;Aloul, Ramani, Markov, & Sakallah, 2004). papers, symmetries SAT/0-1ILP instance detected reduction graph automorphism, i.e. formula represented graph automorphism problem graph solved using graphautomorphism software packages (McKay, 1990; Darga, Liffiton, Sakallah, & Markov, 2004).recently, type symmetry detection frequently inefficient solvingautomorphism problem large graphs time-consuming. However, recent automorphism software (Darga et al., 2004) removed bottleneck largeextent. Moreover, adding simple symmetry breaking predicates new constraints significantly speeds exact SAT solvers (Aloul et al., 2003). work viewed casestudy symmetry breaking problem reductions, focus graph coloringvariants reduced Boolean satisfiability 0-1 ILP. main goals (i)290fiBreaking Instance-Independent Symmetries Exact Graph Coloringaccelerate optimal solving graph coloring instances, (ii) compare different strategiesbreaking instance-independent symmetries. two distinct sources symmetriesgraph-coloring instances: (i) colors arbitrarily permuted (instance-independentsymmetries), (ii) graphs may invariant certain permutations vertices(instance-dependent symmetries). Previous work (Crawford et al., 1996; Aloul et al., 2003,2004) deals instance-dependent symmetries SAT 0-1 ILP instances. Symmetries first detected reduction graph automorphism broken addingsymmetry breaking predicates (SBPs) formulation. advantage strategy every instance-independent symmetry also instance-dependent, whereasreverse hold. Symmetries exist due problem formulation appear everyinstance problem, addition symmetries exist due specific parametervalues instance. Given may many instance-specific symmetries, onemay process symmetries using publicly available symmetry processing packagesShatter (Aloul et al., 2003; Aloul, Markov, & Sakallah, 2003). Alternatively, onemay add symmetry breaking predicates instance-independent symmetries early, hopingspeed-up processing remaining symmetries. type symmetry breakingdiscussed earlier work (Aloul et al., 2003, 2003), paper studyutility graph coloring problem.work deals symmetries problem instance descriptions; distinguish(i) symmetries generic problem specifications (ii) symmetries problem-instancedata. former symmetries translate latter way aroundexample graph coloring given color permutations versus automorphismsspecific graphs. types symmetries detected solving graph automorphism problem, symmetries specifications often captured manually, whereascapturing symmetries problem instances may require large-scale computation nontrivial software. Indeed, specification-level symmetries instantiated, sizesupport (the number objects moved) typically increases dramatically. example,color permutations graph coloring simultaneously applied every vertexgraph question. Detecting symmetries larger support seems like waste computational effort. end, recent work breaking symmetries specifications (Cadoli& Mancini, 2003) prefers instance-independent techniques breaks symmetriesspecification level. approach particularly relevant constraint solverslanguages process problem specifications prior seeing actual problem instancesamortize symmetry-detection effort. Also, general setting, using instanceindependent symmetry breaking rule applying redundant (or complementary)instance-specific techniques later stage.recently automatic symmetry detection serious bottleneck handlingsymmetries. example, graph automorphism solved using program Nauty(McKay, 1990), detecting symmetries often take longer constraint solving withoutsymmetry breaking. observed microprocessor verification SAT instancesAloul et. al. 2002 (Aloul et al., 2003). Therefore, detecting symmetries earlyrepresenting structured way appears attractive, especially givenmay potentially increase efficiency symmetry-breaking. However, symmetrydetection bottleneck recently eliminated many applications softwaretool Saucy (Darga et al., 2004) often finds symmetries practical graphs many times291fiRamani, Aloul, Markov, & Sakallahfaster Nauty. development undermines, extent, potential benefitssymmetry processing specification level puts spotlight symmetry-breaking.end, SBPs added different circumstances may different efficiency,unclear priori approach successful, differences performancemay significant. Since SBPs appear solver additional constraints, mayeither speed frustrate solver (the latter effect clearly visible experimentsCPLEX). Outcomes practical experiments also affected recent dramaticimprovements efficiency symmetry-breaking predicates (Aloul et al., 2003, 2004).seems difficult justify particular expectation empirical performance,fortunate observe clear trends experimental data presented Section 4summarize simple rules.focus graph coloring instances, techniques immediately applicablerelated CSP problems, e.g., produced adding new types constraintseasily expressed SAT 0-1 ILP graph coloring converted genericproblems. also expect conclusions symmetry-breaking carryCSPs economically reduced SAT 0-1 ILP, e.g., maximum independentset, minimum dominating set, etc. Another advantage approach able usevariety existing future SAT 0-1 ILP solvers without modifying source code.Unfortunately, precludes use dynamic symmetry-breaking would requiremodifying source code may adversely affect performance disturbing fragilebalance amount reasoning searching performed modern SAT solvers.Specifically, heuristics variable ordering decision selection may affected, wellrecording learned conflict clauses (nogoods).main contributions work listed below.Using symmetry breaking flow pseudo-Boolean (PB) formulas describedAloul et. al 2004 (Aloul et al., 2004), detect break symmetries DIMACSgraph coloring benchmarks expressed instances 0-1 ILP. show instancedependent symmetry breaking enables many medium-sized instances optimallysolved reasonable time commodity PCspropose instance-independent techniques breaking symmetries problem formulation, assess relative strength completeness, evaluateempirically using well-known academic commercial toolsshow empirically instance-dependent techniques are, general, effectiveinstance-independent symmetry breaking benchmarks question.fact, simplest least complex instance-independent SBPs competitiveremaining part paper organized follows. Section 2 covers backgroundgraph coloring, SAT 0-1 ILP, well previous work symmetry breaking. Instanceindependent symmetry breaking predicates discussed Section 3. Section 4 presentsempirical results Section 5 concludes paper. Appendix gives detailed resultsqueens family instances.292fiBreaking Instance-Independent Symmetries Exact Graph Coloring2. Background Previous Worksection discusses problem definitions applications existing algorithmsexact graph coloring. also discuss previous work symmetry breaking SAT0-1 ILP detail.2.1 Graph ColoringGiven undirected graph G(V, E), vertex coloring graph assignmentlabel (color) node labels adjacent nodes different. minimumcoloring uses smallest possible number colors, known chromatic numbergraph. decision version graph coloring (Kcoloring) asks whether verticesgraph colored using K colors given K.clique undirected graph G(V, E) set mutually adjacent verticesgraph. maximum clique problem consists seeking clique maximal size, i.e.,clique least many vertices clique graph. maximumclique graph coloring problems closely related. Specifically, max-clique sizelower bound chromatic number graph. years, numberdifferent algorithms solving graph coloring developed, fundamental importance computer science. algorithms fall three broad categories:polynomial-time approximation schemes, optimal algorithms, heuristics. brieflydiscuss work categories below. number online resourcesgraph coloring (Trick, 1996; Culberson, 2004) offer detailed bibliographies.far approximation schemes concerned, common technique usedsuccessive augmentation. approach partial coloring found small numbervertices extended vertex vertex entire graph colored. Examplesinclude algorithms Leighton (Leighton, 1979) large scheduling problems,Welsh Powell (Welsh & Powell, 1967) time-tabling. recent work attemptedtighten worst-case bounds chromatic number graph. algorithmproviding currently best worst-case ratio (number colors used divided optimalnumber)dueHaldorsson (Haldorsson, 1990), guarantees ratio2log n), n number vertices. General heuristic methodsn(log(log n)3tried include simulated annealing (Chams, Hertz, & Werra, 1987; Aragon, Johnson,McGeoch, & Schevon, 1991) tabu search (Hertz & Werra, 1987). well-known heuristicstill widely used DSATUR algorithm Brelaz (Brelaz, 1979) colorsvertices according saturation degree. saturation degree vertex numberdifferent colors adjacent. DSATUR heuristic repeatedly picks vertexmaximal saturation degree colors lowest-numbered color possible.heuristic optimal bipartite graphs. Algorithms finding optimal coloringsfrequently based implicit enumeration, discussed detail latersection. graph coloring max-clique problems N P-complete (Garey &Johnson, 1979) even finding near-optimal solutions good approximation guaranteesN P-hard (Feige, Goldwasser, Lovasz, Safra, & Szege, 1991). inapproximabilitygraph coloring suggests may difficult solve heuristically than, say,Traveling Salesman Problem Polynomial-Time Approximation Schemes (PTAS)293fiRamani, Aloul, Markov, & Sakallahknown Euclidean Manhattan graphs. number reasons,study optimal graph coloring many application-derived instances solvablereasonable time. Several applications outlined next.Time-Tabling Scheduling problems involve placing pairwise restrictions jobscannot performed simultaneously. example, two classes taught faculty member cannot scheduled time slot. problem studiedprevious work Leighton (Leighton, 1979) De Werra (Werra, 1985). generally,graph coloring important problem Artificial Intelligence close relationship planning scheduling. Several traditional AI techniques appliedproblem, including parallel algorithms using neural networks (Jagota, 1996). Genetichybrid evolutionary algorithms also developed, notably Galinier et. al.1999 (Galinier & Hao, 1999), addition traditional optimization methodology,scatter search (J.-P. Hamiez, 2001). also studies benchmarkingmodels graph coloring, recent work Walsh (Walsh, 2001), showsgraphs high vertex degrees likely occur real-world applications.Register Allocation active application graph coloring. problemseeks assign variables limited number hardware registers program execution.Accessing variables registers much faster fetching memory. However,number registers limited typically much smaller number variables.Therefore, multiple variables must assigned register. restrictionsassignments. Two variables conflict livetime, i.e. one used within short period time (forinstance, within subroutine). goal assign variables conflictminimize use non-register memory. formalize this, one creates graphnodes represent variables edges represent conflicts variables. coloring mapsconflict-free assignment, number registers exceeds chromatic number,conflict-free register assignment exists (Chaitin et al., 1981).Printed Circuit Board Testing (Garey & Johnson, 1979) involves problemtesting printed circuit boards (PCBs) unintended short circuits (caused stray linessolder). gives rise graph coloring problem vertices correspondnets board edge two vertices potential shortcircuit corresponding nets. Coloring graph corresponds partitioningnets supernets, nets supernet simultaneously testedshorts nets, thereby speeding testing process.Radio frequency assignment broadcast services geographic regions (including commercial radio stations, taxi dispatch, police emergency services). listpossible frequencies fixed government agencies, adjacent geographic regions cannot use overlapping frequencies. reduce frequency assignment graph coloring,geographic region needing K frequencies represented Kclique, N Kpossible bipartite edges introduced two geographically adjacent regions needingN K frequencies respectively.applications graph coloring circuit design layout include circuit clustering, scheduling signal flow graphs, many others. Benchmarks applicationspublicly available, therefore appear paper. However, symmetry breaking techniques described extend instances application.294fiBreaking Instance-Independent Symmetries Exact Graph Coloringbenchmarks use include register allocation, nqueens, several applications discussed detail Section 4. Empirically, observe manyinstances paper optimally solved reasonable time, especially symmetry breaking employed. Since work deals finding optimal solutions graphcoloring, discuss previous work finding exact algorithms problemdetail.literature exact graph coloring includes generic algorithms (Kubale & Jackowski,1985) specialized algorithms particular application, Chaitins register allocation algorithm (Chaitin et al., 1981). moment, appearcomprehensive survey techniques problem. However, online surveys (Trick, 1996;Culberson, 2004) contain reasonably large bibliographies even downloadable sourcecode coloring algorithms cases. Published algorithms finding optimal graphcolorings mainly based implicit enumeration. algorithm proposed Brown(Brown, 1972) enumerates solutions given instance graph coloring checkssolution correctness optimality. algorithm introduces special tree construction avoid redundancy enumerating solutions. work Brelaz (Brelaz, 1979)improves upon algorithm creating initial coloring based cliquegraph considering assignments induced coloring. work KubaleKusz (Kubale & Kusz, 1983) discusses empirical performance implicit enumerationalgorithms, later work Kubale Jackowski (Kubale & Jackowski, 1985) augmentstraditional implicit enumeration techniques sophisticated backtracking methods.work deals solving graph coloring reduction another problem,case 0-1 ILP. type reduction discussed past, notably recentwork Mehrotra Trick (Mehrotra & Trick, 1996), proposes optimal coloringalgorithm expresses graph coloring using ILP-like constraints. relies auxiliaryindependent set formulation, independent set graph representedvariable. prohibitively many variables practical cases number mayreduced column generation, method first tries solve linear relaxation usingsubset variables adds needed. approach inherently breaksproblem symmetries, thus rules use SBPs way speed searchprocess. ILP construction differs considerably one described above, sincerely independent set formulation, assigns colors individual verticesusing indicator variables. construction described detail later section.Solving graph coloring reduction allows exact solutions found using SAT/0-1ILP solvers black boxes. Earlier work Coudert (Coudert, 1997) demonstratedfinding exact solutions application-derived graph coloring benchmarks often takeslonger heuristic approaches, heuristic solutions may differ optimalvalue much 100%. Coudert (Coudert, 1997) proposes algorithm findsexact graph coloring solutions solving max-clique problem. algorithm usestechnique called qcolor pruning, assigns colors vertices systematicallyremoves vertices colored q colors, q greater specified limit.295fiRamani, Aloul, Markov, & Sakallah2.2 Breaking Symmetries CSPsSeveral earlier works addressed importance symmetry breaking searchsolutions CSPs. shown (Krishnamurthy, 1985) symmetry facilitates shortproofs propositions pigeonhole principle, whereas pure-resolution proofsnecessarily exponential size. Finding proofs is, course, difficult problem,performance many CSP techniques lower-bounded best-case proofsize. typical approach use symmetries prevent CSP solver consideringredundant symmetric solutions. called symmetry-breaking accomplishedadding constraints, often called symmetry-breaking predicates (SBPs). Static symmetrybreaking, instance-independent constructions proposed workinstance-dependent predicates literature (Aloul et al., 2003; Crawford et al., 1996),detects symmetries adds SBPs pre-processing branching towardpossible solutions. Symmetry Breaking Dominance Detection (SBDD) proceduredescribed Fahle 2001 (Fahle, Schamberger, & Sellmann, 2001) detects symmetricchoice points search. choice point generated search algorithm checkedpreviously expanded search nodes. equivalent choice pointpreviously expanded, choice point visited again. global cut algorithmproposed Focacci Milano (Focacci & Milano, 2001) records nogoods foundsearch whose symmetric images pruned. set nogoods, called global cutseed used generate global cut constraints prune symmetric images entiresearch tree, ensuring correctness original constraints violated. Laterwork (Puget, 2002) proposed improved methods nogood recording. worksoffer systematic strategy symmetry detection - either require symmetriesknown declared advance, record information search enablessymmetry detection. work outlines implements complete strategy detectbreak symmetries automatically pre-processing, black-box solverused search. context broader justify developmentspecialized solvers. hand, techniques conflict dynamicsymmetry-breaking results potentially reused context.promising new partially-dynamic approach symmetry-breaking, called Group Equivalence (GE) trees proposed Roney et. al. (Roney-Dougal, Gent, Kelsey, & Linton,2004). work aims reduce per-node overhead associated dynamic approaches.GE tree constructed CSP symmetry group G nodestree represent equivalence classes partial assignments group. approachillustrated tracking value symmetries, i.e., simultaneous permutations values CSPvariables. work also shows GE trees empirically outperform several well-knownsymmetry-breaking methodologies, SBDDs. comparison, work compares different ways handle arbitrary compositions variable value symmetries (in graphcoloring, value symmetries seen specification level, whereas variable symmetriesseen problem instances). end, static techniques appear compatible rather competing use GE trees. also many symmetry breaking approaches particular relevance graph coloring. Recent work Gent(Gent, 2001) proposes constraints break symmetry indistinguishable values,evaluate empirically. Like lowest-index ordering (LI) constraints pro296fiBreaking Instance-Independent Symmetries Exact Graph Coloringposed us Section 3, constraints also use pre-existing sequential numberingvertices instance graph coloring enforce distinctions symmetric vertices.construction appears complex compared alternative SBPs effectiveexperiments simpler constructions. Another related work (Hentenryck, Agren, Flener,& Pearson, 2003), proposes constant-time, constant-space algorithm detecting breaking value symmetries class CSPs includes graph coloring.recently, Benhamou (Benhamou, 2004) discusses symmetry breaking CSPs modeled using not-equals constraints (NECSP), uses graph coloring illustrative example.paper defines sufficient condition symmetry certain symmetriesdetected linear time. removal symmetries leads considerable gainsbacktracking search algorithms NECSPs. general, empirical results, reportedSection 4, appear competitive state-of-the-art dynamic approaches. However,designing worlds best graph-colorer goal research. Instead, focusefficient problem reductions SAT 0-1 ILP improving symmetry-breaking.ensure broad applicability results, treat SAT solvers black boxes,perform comprehensive comparison static SBPs report empirical trends.comprehensive comparison existing graph coloring literature wouldgreat value, making rigorous, conclusive revealing requires best staticbest dynamic symmetry-breaking techniques known. end, speculatelikely winner would hybrid. Additional major issues resolved includetuning solvers specific benchmarks (noted work Kirovski Potkonjak (Kirovski & Potkonjak, 1998), differences experimental setup, different softwarehardware platforms, etc. Given comparison completely scopework, better delegated dedicated publication. However, demonstratetechniques competitive related work, provide comparison bestresults recent literature (Benhamou, 2004; Coudert, 1997) Section 4.3.2.3 SAT 0-1 ILPOne solve decision version graph coloring reducing Boolean satisfiability,optimization version reduction 0-1 ILP. Boolean satisfiability (SAT)problem involves finding assignment set 0-1 variables satisfies setconstraints, called clauses, expressed conjunctive normal form (CNF). CNF formulan binary variables, x1 , . . . , xn consists conjunction clauses, 1 , . . . , . clauseconsists disjunction literals. literal l occurrence Boolean variablecomplement. 0-1 ILP problem closely related SAT, allows usepseudo-Boolean (PB) constraints, linear inequalities integer coefficientsexpressed normalized form (Aloul, Ramani, Markov, & Sakallah, 2002) of:a1 x1 + a2 x2 + . . . xn b ai , b Z + xi literals Boolean variables. 1cases single PB constraint replace exponential number CNF clauses(Aloul et al., 2002). general, efficiency CNF reductions encoding-dependent.Earlier work Warners (Warners, 1998) shows linear-overhead conversion existslinear inequalities integer coefficients 0-1 variables CNF. However, CNF1. Using relations (Ax b) (Ax b) xi = (1 xi ), arbitrary PB constraintexpressed normalized form positive coefficients.297fiRamani, Aloul, Markov, & Sakallahencodings use conversion may less efficient. converting CNFPB, single CNF constraint always expressed single 0-1 ILP constraint (byreplacing disjunctions literals constraint + setting right-handside value 1). However, may always suitable since certain operations,disjunction, implication inequality intuitively expressed CNF,efficiently processed SAT solvers Chaff (Moskewicz et al., 2001). conversion0-1 ILP desirable arithmetic operations, counting constraints, whoseCNF equivalent requires polynomially many clauses (and exponentially manyconversions). maximize advantages CNF PB formats, recent0-1 ILP solvers PBS (Aloul et al., 2002) Galena (Chai & Kuehlmann, 2003)allow formula possess CNF PB components. Additionally, 0-1 ILP solvers alsoprovide solution optimization problems. Subject given constraints, one mayrequest minimization (or maximization) objective function must linearcombination problem variables.Exact SAT solvers (Goldberg & Novikov, 2002; Moskewicz et al., 2001; Silva & Sakallah,1999) typically based original Davis-Logemann-Loveland (DLL) backtrack searchalgorithm (Davis, Logemann, & Loveland, 1962). Recently, several powerful methodsproposed expedite backtrack search algorithm, conflict diagnosis (Silva& Sakallah, 1999) watched literal Boolean constraint propagation (BCP) (Moskewiczet al., 2001). improvements, modern SAT solvers (Moskewicz et al., 2001;Goldberg & Novikov, 2002) capable solving instances several million variablesclauses reasonable time. increase scalability scope enabled numberSAT-based applications various domains, including circuit layout (Aloul et al., 2003),microprocessor verification, symbolic model checking, many others. recent workfocused extending advances SAT 0-1 ILP (Aloul et al., 2002; Chai & Kuehlmann,2003). work, focus solving instances exact graph coloring reduction0-1 ILP use SBPs. choice 0-1 ILP motivated following reasons.Firstly, 0-1 ILP permits use general input format CNF, allowinggreater efficiency problem encoding, time similar enough SATallow improved methods SAT-solving used without paying penalty generality.specialized 0-1 ILP solvers PBS (Aloul et al., 2002) Galena (Chai & Kuehlmann,2003) propose sophisticated new techniques 0-1 ILP based recentdecision heuristics (Moskewicz et al., 2001), conflict diagnosis backtracking techniques(Silva & Sakallah, 1999) SAT solvers. result, empirically perform bettergeneric ILP solver CPLEX (ILOG, 2000) leading-edge SAT solver zChaffseveral DIMACS SAT benchmarks application-derived instances FPGA routinginstances circuit layout. Also, since 0-1 ILP optimization problem, unlike SATdecision problem, 0-1 ILP solvers possess ability maximize/minimizeobjective function. can, therefore, directly applied optimization versionexact graph coloring, unlike pure CNF-SAT solvers used kcoloringdecision variant. possible solve optimization version repeatedly solvinginstances kcoloring using SAT solver, value k updatedcall. However, 0-1 ILP solvers require extra step, moreover tend providebetter performance repeated calls SAT solver many Boolean optimizationproblems (Aloul et al., 2002).298fiBreaking Instance-Independent Symmetries Exact Graph Coloringpossible use generic ILP solver, commercial solver CPLEX (ILOG,2000) instead specialized 0-1 ILP solver without changes problem formulation.However, Aloul et al. (Aloul et al., 2002) show generalization alwaysdesirable, particularly case Boolean optimization problems Max-SAT. 0-1ILP also especially useful evaluating effectiveness symmetry breaking graphcoloring, primary purpose work. Detecting breaking symmetries SATformulas shown speed problem-solving process (Crawford et al., 1996;Aloul et al., 2003). Recently, symmetry breaking techniques SAT extended0-1 ILP (Aloul et al., 2004), shown produce search speedupsdomain well. However, similar extension non-binary variables generic ILPpresently exist. evidence (Aloul et al., 2002) advantages symmetrybreaking may depend actual algorithm used search. Specifically, resultscited work suggest generic ILP solver CPLEX actually slowedaddition SBPs. Since CPLEX commercial tool algorithms usedpublicly known, difficult pinpoint reason disparity. However, empiricalresults Section 4 bear observations. remainder section discussesreduction graph coloring 0-1 ILP explains previous work symmetry breakingdetail.2.4 Detecting Breaking Symmetries 0-1 ILPsPrevious work (Crawford et al., 1996; Aloul et al., 2003) shown breaking symmetriesCNF formulas effectively prunes search space lead significant runtimespeedups. Breaking symmetries prevents symmetric images search pathssearched, thus pruning search tree. papers cited work use variantsapproach first described Crawford et al. (Crawford et al., 1996), detectssymmetries CNF formula using graph automorphism. formula expressedundirected graph symmetry group graph isomorphic symmetrygroup CNF formula. Symmetries induce equivalence relations set truthassignments CNF formula. assignments equivalence class resulttruth value formula (satisfying not). Therefore, necessary considerone assignment class.Techniques symmetry breaking proposed literature follow following steps:(i) construction colored graph CNF formula (ii) detection symmetriesgraph using graph automorphism software (iii) use detected symmetries construct symmetry breaking predicates (SBPs) appended additional clausesCNF formula (iv) solution new CNF formula thus created using SAT solver.Crawfords construction (Crawford, 1992) uses 3 colors vertices, one positive literals, one negative literals third clauses. Edges added literalsclause corresponding clause vertex, positive negative literalvertices Boolean consistency. optimization, binary clauses (with two literals)represented adding edge two involved literals, extra vertexneeded. useful runtime graph automorphism programsNauty (McKay, 1990) generally increases number vertices graph.However, optimization Boolean consistency enforced, since binary clausal299fiRamani, Aloul, Markov, & Sakallahedges could confused Boolean consistency edges positive negative literals variable. may improved representing binary clausal edgesdouble edges (Crawford et al., 1996), thus distinguishing two edge types.However, Nauty (and graph automorphism programs) support usesdouble edges, construction useful practice. Furthermore, citedconstructions (Crawford, 1992; Crawford et al., 1996) allow detection phase-shiftsymmetries, variables positive literal mapped negative literal vice versa,since color positive negative literals differently. previous work (Aloul et al.,2003) improves upon constructions giving positive negative literal verticescolor, allowing binary clauses Boolean consistency edges representedway, i.e. single edge two literal vertices. Although constructionmay allow spurious symmetries - clause edges mapped consistency edges -occur formula contains circular chains implications subsetvariables. example, given subset variables x 1 . . . xn , chain collectionclauses (y1 y2 )(y2 y3 ) . . . (yn1 yn ), yi positive negative literalxi . circular chains rarely occur practice, easily checked for. Therefore,efficient graph construction described used practical cases.Graph automorphisms detected Crawfords work (Crawford et al., 1996) wellprevious work (Aloul et al., 2003) using program Nauty (McKay, 1990),part GAP (Groups, Algebra Programming) package. Nauty accepts graphsGAP input format returns list generators automorphism group (theterm generators used mathematical sense, symmetry group partitions setvertex permutations graph equivalence classes permutationsclass equivalent. Nauty returns set generators symmetry group).recent work ((Aloul et al., 2003, 2004)) uses automorphism program Saucy (Dargaet al., 2004), efficient Nauty also process larger graphsvertices. generators symmetry group detected, symmetry breakingpredicates added instance pre-processing step. Crawford et al. (Crawfordet al., 1996) propose addition SBPs choose lexicographically smallest assignments(lex-leaders) equivalence class. refer SBPs instance-dependentSBPs, since symmetries first detected broken, therefore exactnumber nature SBPs added always depends connectivity graph itself.Although detecting symmetries non-trivial, using modern software NautySaucy detection time frequently insignificant compared SAT-solving time.Crawford et. al. (Crawford et al., 1996) construct lex-leader SBPs entire symmetrygroup, using group generators returned Nauty. type symmetry breakingcomplete. However, approach used Aloul et al. TCAD 2003 (Aloul et al.,2003) shows incomplete symmetry breaking, breaks symmetriesgenerators, often effective practice much efficient since requirewhole group reconstructed. SBP construction proposed cited work (Aloulet al., 2003) quadratic number problem variables, compared earlierconstruction (Crawford et al., 1996), could run exponential size. constructionimproved 2003 work Aloul, Sakallah Markov (Aloul et al., 2003),describes efficient, tautology-free SBP construction, whose size linear numberproblem variables. Empirical results Crawfords work (Crawford et al., 1996)300fiBreaking Instance-Independent Symmetries Exact Graph Coloringwell work TCAD 2003 (Aloul et al., 2003) show breaking symmetries produceslarge search speedups number CNF benchmark families, including pigeonholeUrquhart benchmarks, microprocessor verification, FPGA routing ASIC global routingbenchmarks VLSI domain.work symmetry breaking SAT (Aloul et al., 2003) also extendedoptimization problems include CNF PB constraints, objectivefunction (Aloul et al., 2004). before, symmetries detected reduction graphautomorphism. PB formula optimization problem represented undirectedgraph. Graph symmetries detected using graph automorphism tool Saucy (Dargaet al., 2004). Efficient symmetry breaking predicates (Aloul et al., 2003) appendedformula CNF clauses. empirical results work symmetry breaking0-1 ILP (Aloul et al., 2004) show addition symmetry breaking predicatesPB formulas results considerable search speedups specialized 0-1 ILP solverPBS (Aloul et al., 2002). work, use methodology (Aloul et al., 2004)detecting breaking instance-dependent symmetries instances graph coloringexpressed 0-1 ILP. instance-dependent SBPs compared numberinstance-independent SBP constructions described next section.Detecting breaking symmetries application-derived SAT instances amountsrecovery structure original application. loss structure problemreductions one reason reduction-based techniques often competitivedomain-specific algorithms, recent work symmetry breaking useful context.types structure include clusters (Huang & Darwiche, 2003; Aloul, Markov, &Sakallah, 2004). Huang et al. (Huang & Darwiche, 2003) propose algorithm detectsclusters SAT instances uses produce variable orderings, structureaware orderings result considerable empirical improvements SAT solver zChaff(Moskewicz et al., 2001).2.5 Reducing Graph Coloring 0-1 ILPexpress instance minimal graph coloring problem 0-1 ILP optimizationproblem, consisting (i) CNF PB constraints model graph (ii) objectivefunction minimize number colors used.Consider graph G(V, E). Let n = |V | number vertices G, = |E|number edges. instance Kcoloring problem G (i.e., verticesV colored K colors) formulated follows.vertex vi , K indicator variables xi,1 , . . . , xi,K , denote possible color assignments vi . Variable xi,j set 1 indicate vertex vi colored color j,0 otherwisevertex vi , PB constraint formcolored exactly one color.PKj=1 xi,j= 1 ensures vertexedge ei E connects two vertices (va , vb ). edge ei , define CNFVconstraints form Kj=1 (xa,j xb,j ) specify two vertices connectededge given color.301fiRamani, Aloul, Markov, & Sakallahtrack used colors, define K new variables, 1 , . . . , yK . Variable yi trueleast one vertex uses color i. expressed using following CNFVWnconstraints: Kj=1 (yj ( i=1 xi,j )).optimization objective minimize number variables set true, i.e.PMIN Ki=1 yitotal number variables formula nK +K. total number constraintscomputed follows. totally n 0-1 ILP constraints (one per vertex) ensurevertex uses exactly one color. edge, K CNF clauses specifyingtwo vertices connected edge cannot color, giving totalmK CNF clauses. additional nK CNF clauses (K per vertex) settingindicator variables, K CNF clauses, one per color, complete iff condition indicator variables. gives total K (m + n + 1) CNF clauses n 0-1 ILP constraints,plus one objective function, converted formula. dense graphs, |E| |V | 2 ,resulting formula size quadratic number vertices graph, sparsergraphs may linear. key observation instance-dependent symmetries graphcoloring survive reduction 0-1 ILP. instance-independent symmetries (i.e.permutations colors) easy see, since ordering colors changedwithout effect formula producing set constraints.instance-dependent symmetries, consider two vertices v vb symmetricswapped original graph. Clearly, constraints specifyva vb must use exactly one color interchangeable, constraints determine color usage based colors assigned v vb . remains showconnectivity constraints control colors vertices adjacent v vb alsosymmetric. clear fact every edge E incident va , mustcorresponding edge Ej incident vb two vertices symmetric (E Ejedge). Therefore, set K CNF clauses added formularepresent Ei , must symmetric set clauses added E j , thus connectivitypreserved.also clear 0-1 ILP formulation introduce spurious symmetries, i.e.symmetry formula symmetry graph. spurious symmetry arises(i) variables different types mapped other, e.g. vertex color variablesmapped color usage indicator variables (ii) variables type mappedcorresponding vertices actually symmetric.construction 0-1 ILP formula, clear K variables per vertex indicatevertexs color permuted, K color usage variables, since appearexactly constraints. corresponds instance-independent symmetry colors instance graph coloring arbitrarily permuted. However, vertex colorvariables appear constraints restricting number colors vertex use alsoconstraints describe connectivity graph, whereas color usage variables appearconstraints specify set. Therefore, two types variablescannot map one another. Since constraints regarding color connectivityvertex written using K color variables vertex, variables symmetricgroups K, i.e. one variable given vertex v 1 symmetricvariable another vertex v2 , K variables v1 v2 correspondingly302fiBreaking Instance-Independent Symmetries Exact Graph Coloringsymmetric. Additionally, symmetry variables indicates correspondenceclauses occur. possible vertices v 1 v2 symmetricterms connectivity (instance-dependent symmetry). Thus, types symmetriespreserved conversion 0-1 ILP, false symmetries added. Therefore,apply known techniques symmetry detection 0-1 ILP.3. Instance-Independent SBPsquestion addressed work whether instance-independent SBPs addedreduction provide even greater speedups, possibly accelerating detectioninstance-dependent symmetries. answer question, propose three provablycorrect SBP constructions varying strength, one heuristic intended breaksmall number symmetries minimal overhead. construction implementedempirical results reported Section 4.use following notation. Consider instance Kcoloring problem,asks whether graph G(V, E) colored using K colors minimizes numbercolors. Assume colors numbered 1 . . . K. denote valid color assignmentP(n1 , n2 , . . . , nK ) ni number vertices colored color i, |V | = Ki=1 ni .ni color assignment denotes cardinality independent set coloredcolor i. concerned actual composition independent sets here,since instance-dependent issue. Instance-independent symmetriesarbitrary permutations colors different independent sets.effects proposed construction illustrated using example Figure1. figure example 4-coloring problem graph four vertices. Part(a) figure shows graph colored. visual clarity, part (b) shows colorpatterns corresponding different color numbers. clear figurevertices V1 , V2 V3 form clique, must use different colors. However, V 4 givencolor either V1 V2 , therefore 3 colors needed instance.instance partitioned independent sets two ways: {{V 1 , V4 }, {V2 }, {V3 }}{{V1 }, {V2 , V4 }, {V3 }}. SBPs actually address independent setscomposed, instance-dependent issue. However, given partitionindependent sets, colors arbitrarily permuted sets partition.instance-independent SBPs proposed restrict permutation. examples below,assume first partition independent sets i.e. {{V 1 , V4 }, {V2 }, {V3 }}. Resultsproved respect permutation colors partition.3.1 Null-Color Elimination (NU)Consider Kcoloring problem colors 1 . . . K graph G(V, E). Assume Gminimally colored withK 1 colors. Consider optimal solution colorused: (n1 , n2 , ..ni1 , 0, ni+1 , . . . , nK ). assignment equivalent another assignment,(n0 1 , n0 2 , ..n0 j1 , 0, n0 j+1 ...n0 K )6= j n0 = nj . example, assignment (1, 0, 2, 3) equivalent (1, 3, 2, 0),(0, 1, 2, 3), (1, 2, 0, 3). due existence null colors, create symmetries303fiRamani, Aloul, Markov, & Sakallah1:V1V3V42:3:V24:(a)(b)V1V1V3V2V4V3(1,0,2,1)V2V4(1,2,1,0)(c)V1V1V3V2V4V3(1,1,2,0)V2V4(2,1,1,0)(d)V1V1V3V2V3V4(2,1,1,0)V2V4(1,1,2,0)(e)Figure 1: Instance-independent symmetry breaking predicates (SBPs).Part (a) shows original graph vertices colored.Part (b) shows color key. Part (c) shows nullcolor SBPs prevent color 4 used. Part (d) showscardinality based SBPs assign colors order independent set sizes, allowing fewer assignments nullcolor SBPs. Part (e) demonstrates lowest-index orderedSBPs break symmetries undetected typesSBPs.304fiBreaking Instance-Independent Symmetries Exact Graph Coloringinstance Kcoloring color swapped null color. Null colorsextraneous actually required color vertices,inserted anywhere solution, seen above. propose construction enforcesordering null colors: null colors may appear end color assignment,non-null colors. implemented adding K 1 CNF constraints form:yk+1 yk 1 k K 1, original formulation. example above, onefour symmetric assignments (1, 3, 2, 0) would allowed construction. SinceILP formulation defines sets K indicator variables track color usage,extremely easy enforce null color elimination described above. SBPs requireaddition extra variables K 1 new CNF clauses.prove proposed construction correct. Assume originalformulation, optimal solution graph G(V, E) uses colors. Assume solutioncontains null colors non-null colors, null-color elimination, differentoptimal solution uses m0 colors, 6= m0 . colors used solution1 . . . m0 , since null colors cannot occur non-null colors. Since constructionadds SBPs without changing original constraints, legal solution satisfiesSBPs satisfy constraints original formulation. solution originalsatisfies constraints new formulation except SBPs. < 0 , re-ordersolution null colors placed last. satisfy SBPs usecolors, < m0 , violating assumption 0 -color solution optimal.m0 < m, already solution satisfies original constraints uses fewercolors, violates assumptions optimality.illustration use NU predicates example Figure 1 (a) shownFigure 1 (c). figure shows two valid minimal-color assignments graph verticesexample. assignment left uses colors 1, 3 4, one rightuses colors 1, 2 3. assignments symmetric NU predicatesright-hand side assignment permissible.3.2 Cardinality-Based Color Ordering (CA)Null-color elimination useful cases null colors exist. Kcoloringproblem colors needed, construction breaks symmetries. Evennull colors exist, several symmetries go undetected. first example above, nullcolor elimination permits six symmetric color assignments (1, 2, 3, 0), (1, 3, 2, 0), (2, 1, 3, 0),(2, 3, 1, 0) (3, 2, 1, 0) (3, 1, 2, 0). restrictions placed null colors,ordering non-null colors unrestricted. stronger construction would distinguish independent sets themselves. propose alternative construction,assigns colors based cardinality independent sets. subsumes null-colorelimination, since null colors viewed coloring sets cardinality 0. cardinality rule implemented follows: largest independent set assigned color 1,second-largest color 2, etc. example above, assignment (3, 2, 1, 0)PPvalid. enforced adding K 1 PB constraints form: ni=1 xi,k ni=1 xi,k+1 ,1 k K 1. Again, construction fairly simple implement, requiringK 1 additional constraints. However, 0-1 ILP constraints multiple305fiRamani, Aloul, Markov, & Sakallahvariables, unlike simple CNF implication clauses two variables used NUpredicates. Thus, overhead greater completeness.prove CA construction correct follows. Assume optimal solutionconstruction uses < K colors: (n 1 , n2 , . . . , nm ), (n1 n2 . . . nm ). Colors> used vertex, Assume exists optimal solution originalformulation uses m0 colors: (n0 1 , n0 2 , . . . , n0 m0 ), (where n0 1 , etc. arrangeddescending order). Without loss generality, assume 0 < m. sortnumbers n0 1 , . . . , n0 m0 reassign colors descending order. would solutionm0 colors satisfying cardinality constraints. However, 0 < m, possiblemcolor solution optimal. similar argument applies < 0 .example Figure 1 (a), largest independent set partitionconsidering, i.e. {V1 , V4 } given color 1. Therefore, assignment rightFigure 1 (c), assigns largest set color 2 correct NU predicates,incorrect CA construction. left-hand side Figure 1 (d) shows anotherassignment correct NU predicates incorrect CA predicates, sinceassigns set {V1 , V4 } color 3. correct assignment, shown right-hand sideFigure 1 (d), gives largest set color 1 since sets one element each,assigned either color 2 color 3. Thus, several symmetric assignmentssurvive NU predicates prohibited construction.3.3 Lowest Index Color Ordering (LI)complete NU predicates, CA predicates break symmetriesdifferent independent sets cardinality. Consider graph G V ={v1 , . . . , v8 }, optimal solution, satisfying cardinality-based ordering, partitionsV 4 independent sets: S1 = {v4 , v6 , v7 }, S2 = {v1 , v5 }, S3 = {v3 , v8 }, S4 = {v2 }.solution assigns colors 2 3 2 S3 symmetric one assigns colors 23 S3 S2 . legal cardinality-based ordering. order completelybreak symmetries, adequate distinguish sets solely basiscardinality (unless two sets cardinality). necessary constructSBPs based actual composition sets partition, unique. However,distinctions make basis composition confusedinstance-dependent SBPs, since construction implemented symmetriesinstance known, regardless actual composition. SBPs specifybroad guidelines coloring independent sets applicable graphs.improve upon cardinality-based ordering, propose set predicates enforcelowest-index ordering (LI). Consider vertices color i, find lowest index jamong those. require lowest indices color ordered. constraintenforced adding inequalities colors adjacent numbers.Note color unique lowest-index vertex otherwise vertex wouldcolored two colors. example, color assignmentcompatible partitioning vertices independent sets is: color 1 1 , 2 S3 ,3 S4 , 4 S2 .evaluate strength symmetry-breaking technique, consider arbitrarycoloring color permutation remains symmetry LI constraints306fiBreaking Instance-Independent Symmetries Exact Graph Coloringimposed. colors permuted simultaneously vertices, permutelowest indices colors. Since lowest indices different, ordering completely determined ordering colors, thus color permutation chose mustidentity permutation. words, instance-independent symmetries remainsymmetry-breaking LI.implement lowest-index color ordering follows. vertex v , declarenew set K variables, Vi,1 , . . . Vi,K . Variable Vi,k set implies vertex vilowest-indexV vertexcolored color k. enforced following CNF constraints:i1Vi,kj=1 Vj,k . Also, exactly one Vi,j variable must true every color used.WTherefore, add constraints: k ni=1 Vi,k , 1 k K, yk variablesindicate color k used, n = |V | Section 2. Finally,W followingCNFnclause added Vi,k ensure lowest-index ordering: V i,kj=i+1 Vj,k1 , SinceLI ordering completely breaks symmetries independent sets, subsumes earlierconstructions. However, come added cost. NU CA constructionsrequired new variables K 1 constraints, LI construction requires nK newvariables additional 2nK CNF clauses, almost double size originalformula.LI construction proved correct means CA construction.Given optimal assignment colors independent sets, sort independent setsorder lowest-index vertex assign colors 1 K accordingly, without affectingcorrectness.Figure 1 (e) illustrates effect LI SBPs example Figure 1 (a). graphleft, shown correct CA predicates Figure 1 (d) incorrectLI construction, lowest-index vertex color 2 (V 3 )higher index lowest-index vertex color 3, V 2 . graph rightshows correct assignment, LI predicates permissible assignmentpartition {{V3 }, {V2 }, {V1 , V4 }}.addition complex, LI predicates rigid obscure symmetries original instance. example, Figure 1 (a), easily seen verticesV1 V2 symmetric permuted effect resulting graph.symmetry instance-dependent - decided way V 1 V2 connected. Without addition SBPs, apparent legal coloring graph,colors given V1 V2 swapped regardless V3 V4 colored. NUpredicates preserve symmetry, since concerned null colorsdefinition could used V1 V2 . CA predicates also preserve symmetrysince V1 V2 interchangeably used independent set, swappingsets would effect cardinality sets. However,LI predicates, independent set containing V 1 must always given higher-numberedcolor set containing V2 , two cannot interchanged. V 1 givencolor highest color use, would exist independent set whosecolor index 1 greater color assigned V 1 , set, lowest-indexpredicate would satisfied. Thus, LI predicates actually destroy vertex permutations graph. seen empirical results Section 4, addition307fiRamani, Aloul, Markov, & SakallahLI SBPs leaves symmetries benchmarks. unusual ordinarilybenchmarks reasonable size would contain least vertex permutations.3.4 Selective Coloring (SC)noticeable ILP formulation constraints complexcomplete SBPs, LI predicates above, introduce several additional variablesclauses. raises question whether complex construction actuallycounterproductive - may break symmetries, require much effort searchbenefit complete symmetry breaking lost. investigate this, also proposesimple heuristic construction break symmetries vertices addingalmost additional constraints. impact many vertices possible, find vertexvl largest degree vertices graph. color v l color 1.achieved simply adding unary clause x l,1 . search vl neighbors findvertex vl0 highest degree vertices adjacent v l . color vl0 color2, adding unary clause xl0 ,2 . construction effect simplifying colorassignment vertices adjacent v l vl0 . vertex adjacent vl coloredcolor 1, vertex adjacent vl0 colored color 2. Moreover, verticesindependent set vl (vl0 ) must colored color 1 (color 2). v l vl0 sufficientlylarge degree, construction restrict many vertex assignments. even strongerconstruction would find triangular clique fix colors three vertices it;however, clique finding complicated graphs may possess cliques.refer construction selective coloring.extent selective coloring breaks symmetries instance-dependent. failscompletely break symmetries almost graphs. However, simple construction,adding two constraints unary clauses. easily resolved pre-processingSAT solvers, symmetry breaking achieved construction virtuallyoverhead.note instance-independent predicates defined concernedsymmetries colors, exist instance graph coloring. However, additional instance-independent symmetries may introduced reduction graphcoloring certain applications. example, radio frequency assignment application Section 2, adding possible bipartite edges cliques adjacent regionsresult symmetries vertices cliques. Additional predicatesadded instances application break symmetries.4. Empirical Resultssection describes experimental setup, empirical results, performance comparedrelated work.4.1 Experimental Setupused 20 medium-sized instances DIMACS graph coloring benchmark suite.briefly describe family benchmarks used below.308fiBreaking Instance-Independent Symmetries Exact Graph ColoringRandom graphs. Benchmarks randomly created connections vertices,named DSJBook graphs. Edges represent interaction characters book.four benchmarks: anna, david, huck, jeanMileage graphs. represent distances cities map, namedmilesFootball game graphs. Indicate relationships teams must playcollege football games. tables referred gamesnqueens graphs. Instances nqueens problem, named queenRegister allocation graphs. Represent register allocation problem differentsystems. use two families work, named mulsol, zeroinMycielski graphs. Instances triangle-free graphs based Mycielski (Mycielski, 1955) transformation, called mycielTable 1 gives name, size (number vertices edges) chromatic numberbenchmark. use maximum value K = 20 Kcoloring. benchmarkschromatic number > 20, report chromatic number.problem formulation fixed K application-driven. Indeed, many domains useful find exact chromatic number well-knownthreshold. example, graph coloring instances register allocation, cannotcolors processor registers. PC processors often 32 registers, high-endCPUs may more. However, realistic graphs relatively sparse low chromatic numbers. hand, processors embedded cellular phones, automobilespoint-of-sale terminals may registers, leading tighter constraintsacceptable chromatic numbers. value K = 20 used experiments wayspecial, results achieved representative results. Also,apply K = 20 bound instances study trends, reasonable boundsdetermined per-instance basis using following simple procedure.1. Apply heuristic min-coloring determine feasible upper bound2. value relatively small, perform linear search incrementally tighteningcolor constraint, otherwise perform binary searchBenchmark graphs transformed instances 0-1 ILP using conversion described Section 2. solve instances 0-1 ILP, used academic 0-1 ILP solversPBS (Aloul et al., 2002), Galena (Chai & Kuehlmann, 2003), Pueblo (Sheini, 2004),also commercial ILP solver CPLEX version 7.0. Pueblo recent PBSGalena, incorporates Pseudo-Boolean (PB) learning based ILP cutting-planetechniques. use later version PBS, PBS II, enhances original PBS algorithms (Aloul et al., 2002) learning techniques Pueblo solver (Sheini, 2004).include results original version PBS reported (Ramani,309fiRamani, Aloul, Markov, & SakallahInstanceannadavidDSJC125.1DSJC125.9games120huckjeanmiles250mulsol.i.2mulsol.i.4myciel3myciel4myciel5queen5 5queen6 6queen7 7queen8 12zeroin.i.1zeroin.i.2zeroin.i.3#V13887125125120748012818818511234725364996211211206#E98681214721392212766025087743885394620712363205809522736410035413540K11115> 20911108>20>2045657712>20>20>20Table 1: DIMACS graph coloring benchmarksAloul, Markov, & Sakallah, 2004), since retired newer version. However,Appendix report detailed results n queens instances using older versionPBS along results solvers, sake detailed study. PBS IIimplemented C++ compiled using g++. Galena Pueblo binaries providedauthors. PBS run using variable state independent decaying sum (VSIDS)decision heuristic option (Moskewicz et al., 2001). Galena run using default optionslinear search cardinality reduction (CARD) learning. experiments runSun-Blade-1000 workstations 2GB RAM, CPUs clocked 750MHz Solarisoperating system. Time-out limits solvers set 1000 seconds.use symmetry breaking flow first proposed earlier work (Aloul et al.,2004) detect break symmetries original ILP formulation Section 2.flow uses tool Shatter (Aloul et al., 2003), uses Saucy (Darga et al., 2004)graph automorphism program efficient SBP construction (Aloul et al., 2003).also check unbroken symmetries formulations produced instanceindependent constructions described Section 3. runtimes symmetry detectionsolving reduced 0-1 ILP problems reported next section.4.2 Runtimes Symmetry Detection 0-1 ILP SolvingTable 2 shows symmetry detection results runtimes. numbers reported tablesums individual results 20 benchmarks used. report statistics sumsreporting results SBPs benchmarks would space-consuming,310fiBreaking Instance-Independent Symmetries Exact Graph ColoringSBPTypeSBPsNUCALISCNU+SC#V437K437K437K870K437K437KCNF Stats#CL7775057778857775054019980777545777925# PB319331933630319331933193Sym. Stats (SAUCY)#S#G Time1.1e+168 9941855.0e+149 614495.0e+149 614492.0e+010843.0e+164 9411675.0e+148 59747Table 2: CNF formula sizes, symmetry detection resultsruntimes, totaled 20 benchmarksTable 1, K = 20. NU = null-color elimination; CA = cardinality-based; LI = lowest-index;SC = selective coloring. LI SBPs, one instance do-nothing symmetry countedcase, giving total 20 symmetries0 generators. Saucy run Intel Xeon dualprocessor 2 GHz running RedHat Linux 9.0.would also illustrate trends clearly. work concerned characterizingbroad impact symmetry breaking. However, show detailed results queensinstances Appendix.first column table indicates type construction: use SBPsbasic formulation, NU null-color elimination, CA cardinality-based ordering,LI lowest-index ordering, SC selective coloring (the last row shows NU SCcombination). next three columns show number variables, CNF clauses,PB constraints problems. last three columns show number symmetries,number symmetry generators, symmetry detection runtimes Saucy. Henceforth,refer instance-dependent SBPs external, added instance symmetries detected part problem formulation. toprow separated bottom 5 rows represents statistics without instanceindependent SBPs. observe adding instance-independent SBPs problemformulation cut symmetry detection runtime considerably. Saucy total runtime 185 seconds instance-independent SBPs added, runtimesNU, CA, LI NU + SC constructions much smaller. SC constructioncomparable runtime heuristic breaks symmetries.columns showing numbers symmetries generators support observation: NU,CA, LI NU + SC constructions far fewer symmetries top row,SC construction almost number. benchmarks, LI construction,breaks symmetries, even instance-dependent vertex permutations may existgraph. Saucy reports finding symmetries construction (except one instancedo-nothing symmetry graph, trivial). However, Saucy runtimesconstruction larger NU, CA NU + SC constructions (85 secondsapproximately 49 seconds) even though symmetries instances LI311fiRamani, Aloul, Markov, & SakallahSBPTypeSBPsNUCALISCNU+SCPBS II, PB LearningOrig.w/i.-d. SBPsTm.#STm.#S17K8.2K13K15K14K6.9K313666144.2K7.5K12K15K656.8K1613862014CPLEXOrig.w/i.-d. SBPsTm.#STm.#S6.3K5.9K11K16K5.3K4.5K1415114151613K6.5K11K16K12K6.4K715104814GalenaOrig.w/i.-d. SBPsTm.#STm.#S1.7K8.3K19K15K16K6.1K211154143K6.7K17K15K94.46.1K1711352014PuebloOrig.w/i.-d. SBPsTm.#STm.#S18K9.1K9K16K15k7.3K3121255131.6K8.3K10K16K2.1K7.1KTable 3: Runtimes number solutions found SBPs addedconstructions using PBS II (with PB learning), CPLEX, GalenaPueblo; experiments run SunBlade 1000 workstations. Timeoutssolvers set 1000s. maximum color limit set 20,instances k > 20 unsatisfiable formulations.comparison solvers. solve ILP formulations equal optimalvalues using different solvers weed solver-specific issues. Best resultsgiven solver shown boldface. entries, K denotes multiples1000s seconds rounded nearest integer.predicates added. likely reason sharp increase instance size causedLI construction. general, SC construction little effect numbersymmetries - used itself, leaves symmetries intact, usedNU construction, improvement NU construction alone small.Table 3 shows effect symmetry breaking runtimes PBS II (Aloul et al., 2002),CPLEX (ILOG, 2000), Galena (Chai & Kuehlmann, 2003) Pueblo (Sheini, 2004).first column table specifies construction type, followed total runtimesolver (with without addition instance-independent SBPs) numberinstances solved construction. solver, best performance amongconfigurations (largest number instances solved corresponding runtime) boldfaced.Results given first new version PBS, PBS II based (Sheini, 2004), followedCPLEX, Galena Pueblo. Runtimes older version PBS obtainedearlier work (Ramani et al., 2004). compare performance individual solverdifferent constructions, observe runtime solution entries different rowscolumn, compare performance different solvers constructions,observe numbers row across columns. observe following trends.1. benchmarks possess large number symmetries. Different instance-independentSBPs achieve varying degrees completeness: lowest-index ordering (LI) breakssymmetries benchmarks used, selective coloring (SC) SBP breaksfewest symmetries. Saucy runtimes residual symmetry detectionaddition instance-independent SBPs highest SBPs constructionSC construction, since possess largest numbers symmetries2. case SBPs kind added, CPLEX performs well, solving14 20 instances within time limit. However, PBS II, Galena Puebloperform poorly - Galena solves 2 instances PBS II Pueblo solve 331219131251813fiBreaking Instance-Independent Symmetries Exact Graph Coloring3. PBS II, Galena Pueblo benefit considerably instance-dependent symmetrybreaking. instance-dependent SBPs used without instanceindependent constructions propose, PBS II solves 16 instances within timelimit, Galena Pueblo solve 17 19 instances respectively. However,CPLEX hampered addition instance-dependent SBPs, solves 7instances case4. Adding instance-independent SBPs improves performance specialized 01 ILP solvers no-SBP version. best performance PBS II, GalenaPueblo seen NU + SC construction - PBS II Galena solve 14instances, Pueblo solves 13. CPLEX, NU + SC construction showsmarginal improvement no-SBPs case (16 instances solved),complex constructions, CA LI, actually undermine performance - CPLEX solves4 instances LI construction. general, complex SBP constructionsperform much worse simple ones. PBS II, Pueblo Galena also performpoorly CA LI constructions - Galena solves 1 instance CAconstruction help instance-dependent SBPs, instancessolved LI construction solver5. Adding instance-independent SBPs alone solve many instances addinginstance-dependent SBPs SBP-free formulation. best performance seeninstance-independent SBPs 14 instances solved, Galena PBS II,16 instances solved CPLEX, NU + SC construction. instancedependent SBPs added PBS II Galena solve 20 instances SCconstruction. CA LI constructions leave (or none all) symmetriesbroken instance-dependent SBPs. Consequently, almost differenceresults without instance-dependent SBPs constructions. However, achieve performance improvements instance-dependentSBPs, due size complexity6. Using instance-dependent SBPs conjunction SC construction useful.combination, PBS II Galena solve 20 instances within timelimit, Pueblo solves 18. Runtime also considerably improved PBS IIGalena PBS II solves 20 instances total 65 seconds, Galena 94.4seconds. best overall performance, terms number solutions runtime,seen combination. general, however, SC construction dominantown. Results SC construction alone similar resultsSBPs, results NU + SC combination similar achievedusing NU SBPs. SC construction effective boosting performanceconstructions7. three specialized 0-1 ILP solvers - PBS II, Galena Pueblo, exhibitperformance trends respect constructions used, performancescomparable, terms number solutions found runtimeindicates variations performance due different SBPs, duediffering solver implementations. solvers independent implementations based313fiRamani, Aloul, Markov, & SakallahSBPTypePBS II, PB LearningOrig.w/i.-d. SBPsTm.#STm.#SSBPsNUCALISCNU + SC18K9.2K13K15K15K7.1K212755136.2K7.9K13K15K5.3K7.0K1413951513CPLEXOrig.w/i.-d. SBPsTm.#STm.#S11K11K13K19K10K9.7K999210118.2K12K14K19K12K9.9K12882911GalenaOrig.w/i.-d. SBPsTm.#STm.#S19K10K19K16K16K9.2K110154129.1K7.6K17K16K5.3K6.9K1113451514PuebloOrig.w/i.-d. SBPsTm.#STm.#S19K11K11K17K16K8.0K1111134137.5K9.5K13K17K6.0K7.4KTable 4: Total runtimes number solutions found SBPsadded constructions using PBS II (with PB learning), CPLEX,Galena Pueblo. experimental setup usedTable 3 color limit K = 30. Best results solver boldfaced. Fewer instances solved Table 3 higher colorlimit results larger potentially difficult instances.algorithmic framework (the Davis-Logemann-Loveland backtrack searchprocedure), PBS II Galena also learning capabilities8. Adding instance-dependent SBPs construction usually adversely affectsperformance CPLEX. previously noted work (Aloul et al.,2004). Since CPLEX algorithms implementation availablepublic domain, difficult account effect. However, PBS Galenasymmetry breaking significantly outperform CPLEX without symmetry breaking9. report results sum runtimes instances illustrate trends.per-instance basis, trends displayed. example, no-SBPscase top row, PBS II solves 3 instances Galena solves 2, twoinstances solved Galena among solved PBS II. general,instances tend easy difficult 0-1 ILP solvers, although CPLEXbehaves differently. example behavior queens family instancesillustrated AppendixOverall, results suggest graph coloring, adding instance-independent SBPs alonecompetitive use instance-dependent SBPs alone. best resultsachieved using combination types, even here, instance-independent SBPsused simple variety. true even symmetry detection runtimestaken consideration. attribute result complexity instance-independentSBPs use, also fact improvements graph automorphism software(Darga et al., 2004) greatly reduced overhead detecting symmetries reductiongraph automorphism. Previously, static approaches require symmetriesdetected broken advance, task symmetry detection often bottleneckcould actually take longer search itself. bottleneck removed,advantages static symmetry breaking - simple predicates address specific symmetriesrather complex constructions alter problem specification considerably -clearly illustrated. Even among instance-independent predicates, simple constructionseffective complex ones. noted Section 3, simple constructions3141311831513fiBreaking Instance-Independent Symmetries Exact Graph Coloringlike NU SC add additional constraints alter original problemgreatly. However, CA LI constructions add many constraints, mayconfuse specialized 0-1 ILP solvers.important note color permutations, instance-independent, appear instance-specific level. Thus, symmetries targeted instance-independentpredicates subset targeted instance-dependent predicates. instanceindependent constructions intended cover different set symmetries, ratherbreak symmetries problem formulation, thus reducing eliminating overhead instance-dependent methods may follow. factstrategy successful suggests that, set symmetries, instancedependent predicates use efficient easier solvers tackle.verify claims performance trends, show results additional setexperiments increased color limit K = 30 Table 4. instances re-formulatedK = 30 different SBP constructions. experiment intended verifytrends K = 20 case, investigate whether instances chromatic number> 20, unsatisfiable first case, colored 30 colors. ResultsTable 4 validate observations Table 3 best results PBS II, GalenaPueblo achieved NU + SC (with instance-dependent SBPs)SC (with instance-dependent SBPs) constructions. However, formulation fewerinstances solved K = 20 case, possibly K = 30 limit resultslarger instances. Also, instances whose chromatic number much closer 3020, may harder prove optimality, whereas proving unsatisfiability K = 20experiments may simpler.4.3 Comparison Related WorkHere, discuss empirical performance approach compared relatedwork (Coudert, 1997; Benhamou, 2004). note cited works describe algorithmsspecifically developed graph coloring, search procedures cannot used solveproblems. approach, hand, solves hard problems reductiongeneric problems SAT 0-1 ILP, work graph coloring viewedcase study. Consequently, use problem-specific knowledge actual problemformulation (instance-independent SBPs also added reduction),search itself. may useful applications problem-specific solvers cannotdeveloped acquired due limited resources. goal determine whether symmetrybreaking improve performance reduction-based methods, traditionallycompetitive problem-specific methods. Thus, techniques maysuperior problem-specific solvers instances, hope show reasonably strongperformance broad spectrum instances.Common data points work Couderts (Coudert, 1997) include instancesqueens, myciel DSCJ125.1. Referring detailed results queens instancesAppendix, note runtimes competitive Couderts algorithm- example, queen5 5, algorithms runtime 0.01s. larger instances,however, runtimes somewhat slower. myciel instances, obtain bestresults Pueblo solver SC predicates, runtimes 0.01, 0.06, 1.80s315fiRamani, Aloul, Markov, & Sakallahmyciel3, 4, 5, compared 0.01, 0.02 4.17 Couderts algorithm. Therefore,appears approach competitive common data points. Moreover,studies (Kirovski & Potkonjak, 1998) observed Couderts work provideresults several hard real-world problem classes, particularly modeling resultsdense graphs. work general, cannot biased favor certain typesgraphs.algorithm described Benhamou (Benhamou, 2004) shows competitive runtimes number DIMACS benchmarks, particularly instances register allocation.example, DSJC125.1 instance solved Benhamous algorithm 0.01 seconds,best time achieved us 1.12 seconds, using Pueblo solverinstance-dependent SBPs. However, note Benhamous algorithm determinesupper limit chromatic number K using instance-specific knowledge, example, DSCJ125.1, set K = 5. solve instances K = 20, maylarge limit cases. value K affects size resulting 0-1 ILPreductions SBPs, likely affect runtime. also note DIMACSbenchmarks used cited work (Benhamou, 2004) primarily register allocationrandomly generated instances, whereas achieve reasonably good performance widevariety benchmark applications. Moreover, Benhamous approach relies modelinggraph coloring not-equals CSP, bode well generality. Many CSPscannot modeled using not-equals constraints. Additionally, symmetry detection, breaking search procedures described work specific graph coloring,whereas work extended several problems, requiring reductionSAT/0-1 ILP.5. Conclusionswork shows problem reduction 0-1 ILP viable method optimally solvingcombinatorial problems without investing specialized solvers. approach likelyeven successful efficiency 0-1 ILP solvers improves future,able better handle problem structure. particular, problem reductions mayproduce highly-structured instances making ability automatically detect exploitstructure important. case graph coloring demonstrate generic,publicly-available symmetry breaking flow earlier work (Aloul et al., 2004) significantly improves empirical results conjunction academic 0-1 ILP solvers PBS II,new version solver PBS (Aloul et al., 2002), Galena (Chai & Kuehlmann, 2003)Pueblo (Sheini, 2004). specialized 0-1 ILP solvers significantly outperform commercial generic ILP solver CPLEX 7.0 symmetry-breaking used. performanceCPLEX actually deteriorates SBPs added, original instancesSBPs, CPLEX able solve instances 0-1 ILP solvers. However, bestperformance overall obtained 0-1 ILP solvers instances SBPs added.Although techniques tested standard DIMACS benchmarks instances, notesymmetry-breaking flow described applied graph coloring instancesapplication.particularly interested comparing strategies breaking symmetriespresent every ILP instance produced problem reduction (instance-independent sym316fiBreaking Instance-Independent Symmetries Exact Graph Coloringmetries). symmetries may known even first instances originalproblem delivered (i.e., symmetries may detected specification level),one option use problem reduction. Intuitively, may preventdiscovering symmetries every instance thus improve overall CPU time.end, propose four constructions instance-independent symmetry breaking predicates (SBPs). constructions vary terms strength completeness. goalexperiments compare performance four instance-independent SBP constructions relative other, well assess performance comparedinstance-dependent SBPs. Instance-independent SBPs advantage requiringadditional step symmetry detection, since part problem specification.Additionally, designed information problem itself,effect solutions clear - example, know null-color elimination forcelower-numbered colors used solution. Instance-dependent SBPs detectedadded automatically 0-1 ILP reduction instance without understandingsignificance. hand, instance-dependent constructions less complex result compact predicates. empirical data indicate simplicityconstruction powerful factor determining performance - instance-dependentSBPs consistently outperform instance-independent SBPs, complete complex instance-independent constructions (LI) actually weakest performance.clear results symmetry breaking useful graph coloring: addinginstance-dependent SBPs always speeds search no-SBPs case. likelyinstance-independent SBPs less successful due complex construction. Simplerinstance-independent constructions (NU, SC) outperform complex ones (CA, LI).well known syntactic structure CNF PB constraints may dramaticallyaffect efficiency SAT ILP solvers. Shorter clauses PB constraints muchpreferable easier resolve constraints, usefullearning strategies employed exact SAT solvers. Another factor gives instancedependent SBPs advantage ease symmetry detection, previouslybottleneck. Due improved software (Darga et al., 2004), overhead symmetry detection via reduction graph automorphism SAT/0-1 ILP instances almost negligible.also show three specialized 0-1 ILP solvers, PBS II, Galena Pueblo,exhibit similar performance trends different constructions. indicates performance decided solver-specific issues, difficulty instancesSBPs added them. CPLEX display behavior solvers,fact slowed addition instance-dependent SBPs severalinstance-independent constructions. CPLEX commercial solver generic ILP problems, algorithms decision heuristics likely differentused academic solvers. However, since details CPLEX publicly available,possible accurately explain behavior. note CPLEXappear benefit symmetry breaking, performance reduced instancesSBPs kind superior 0-1 ILP solvers. However, SBPs addedspecialized solvers solve instances CPLEX less time.context generic search combinatorial optimization problems definedNP-spec language (Cadoli et al., 1999), empirical data suggest new theoreticalbreakthroughs required make use instance-independent symmetries problem317fiRamani, Aloul, Markov, & Sakallahreductions SAT 0-1 ILP. current level understanding, simple strategyprocessing instance-independent instance-dependent symmetries together producessmallest runtimes graph coloring benchmarks. current future work focuseddeveloping effective SBPs problem, also investigating utilitysymmetry breaking hard search problems. Moreover, work uses instanceindependent predicates color symmetries, results analysis may broaderscope, example, applications radio frequency assignment (Section 2)symmetries introduced reduction graph coloring likelypreserved future reductions. issues involved using instance-dependent vs.instance-independent SBPs relevant applications.6. Acknowledgmentswork funded part NSF ITR Grant #0205288. Also, thank Donald ChaiAndreas Kuehlmann UC Berkeley providing us binaries Galenasolver, Hossein Sheini providing us binaries Pueblo.Appendix A: Performance Analysis Queens Instancessection provides detailed discussion results individual benchmarksqueens family instances. problem posed queens instances whetherqueens placed n chessboard without conflicts. instances useexperiments queens 5 5, 6 6, 7 7 8 12. Table 5 shows resultsqueens family. Results shown every instance SBPs, fourconstructions NU, CA, LI SC, NU + SC combination. constructionstested without instance-dependent SBPs before. report resultsoriginal version PBS, (Aloul et al., 2002), PBS II, CPLEX, GalenaPueblo Section 4. Experiments run Sun Blade 1000 workstations before.table, report solver runtime instance solved, T/O timeout 1000seconds. best results solver particular instance boldfaced.greater variation considering performance per-instance basis,table largely reflects trends reported Section 4. example,instance-dependent SBPs used, PBS, PBS II, Galena Pueblo largely performbest NU + SC construction. instance-dependent SBPs added, bestperformance seen SC construction cases. CPLEX displaybehavior solvers, performance clearly deteriorates instancedependent SBPs added construction. similar effect observed relatedwork (Aloul et al., 2004). Results original version PBS (Aloul et al., 2002),could included Section 4, added section. seenPBS follows trends PBS II, Galena Pueblo, reinforcing claimbehavior solver-dependent.318fiBreaking Instance-Independent Symmetries Exact Graph ColoringInst.Namequeen5 5queen6 6queen7 7queen8 12SBPTypeSBPsNUCALISCNU + SCSBPsNUCALISCNU + SCSBPsNUCALISCNU + SCSBPsNUCALISCNU + SCPBSInst.-dep.SBPs used?YesT/O0.191.84T/OT/OT/O135134.7115.990.198.6312.34T/O3.61331.63521.12T/OT/OT/OT/OT/O0.582.891.72T/O36.560.453.29T/OT/OT/OT/OT/O8.425.6538.07T/O1.31T/OT/OT/OT/OT/OT/OT/O1.05T/OT/OPBS IIInst.-dep.SBPs used?Yes34.520.040.010.020.310.241.481.480.150.0700.01T/O0.2156.6313.5950.6780.57T/OT/OT/O0.11.40.63T/O1.7936.3124.74T/OT/O53.353.438.570.854.375.73T/O0.52T/OT/OT/OT/OT/OT/OT/O0.47787.26 780.14CPLEXInst.-dep.SBPs used?Yes1.11643.931.3823.6739.22.76262.96217.210.45229.790.830.88T/OT/OT/OT/OT/OT/OT/OT/O242.79T/O95.91T/O243.3T/O119.16 459.44271.2T/OT/OT/O38.04T/O119.7T/OT/OT/OT/OT/OT/OT/OT/OT/OT/OT/OT/OT/OGalenaInst.-dep.SBPs used?Yes83.060.350.210.27T/OT/O5.45.40.290.290.31T/O0.87192.1719.11T/OT/OT/OT/OT/O1.011.191.05T/OT/O56.6147.52T/OT/O78.8578.8T/O1.3317.465.16T/OT/OT/O138.61T/OT/OT/OT/OT/O1.952.153.63PuebloInst.-dep.SBPs used?Yes203.090.010.080.10.140.528.488.480.250.190.060.07T/O0.49123.9918.88196.9480.53T/OT/OT/O0.324.852.64T/O1.139.5915.49692.67150.86212.18213.8217.821.2325.7314.04T/OT/OT/OT/OT/OT/OT/OT/OT/O0.98T/OT/OTable 5: Detailed results queens instances. instance, show resultssolvers PBS, PBS II, CPLEX, Galena Pueblo. solversrun SunBlade 1000 workstations. Instances testedinstance-independent SBPs, four proposed constructionsSection 3 combination NU SC constructions.instance-independent SBPs tested alone instance-dependentSBPs added. table shows runtime given instance different construction. T/O indicates timeout 1000 seconds. Best resultsgiven solver instance shown boldface.ReferencesAloul, F. A., Markov, I. L., & Sakallah, K. A. (2003). Shatter: Efficient symmetry-breakingboolean satisfiability. International Joint Conference Artificial Intelligence,pp. 271282.Aloul, F. A., Markov, I. L., & Sakallah, K. A. (2004). MINCE: static global variableordering heuristic sat search bdd manipulation. Journal Universal ComputerScience (JUCS), 10, 15621596.Aloul, F. A., Ramani, A., Markov, I. L., & Sakallah, K. A. (2002). Generic ILP versusspecialized 0-1 ILP: update. International Conference Computer-AidedDesign, pp. 450457.Aloul, F. A., Ramani, A., Markov, I. L., & Sakallah, K. A. (2003). Solving difficult instances319fiRamani, Aloul, Markov, & Sakallahboolean satisfiability presence symmetry. IEEE Transactions CAD,22, 11171137.Aloul, F. A., Ramani, A., Markov, I. L., & Sakallah, K. A. (2004). Symmetry-breakingpseudo-boolean formulas. Asia-Pacific Design Automation Conference, pp. 884887.Aragon, C. R., Johnson, D. S., McGeoch, L. A., & Schevon, C. (1991). Optimizationsimulated annealing: experimental evaluation; part ii, graph coloring numberpartitioning. Operations Research, 39, 378406.Benhamou, B. (2004). Symmetry not-equals binary constraint networks. WorkshopSymmetry CSPs, pp. 28.Brelaz, D. (1979). New methods color vertices graph. Communications ACM,22, 251256.Brown, R. J. (1972). Chromatic scheduling chromatic number problem. Management Science, 19, 451463.Cadoli, M., & Mancini, T. (2003). Detecting breaking symmetries specifications.Third Annual Workshop Symmetry Constraint Satisfaction Problems(SymCon), pp. 1326.Cadoli, M., Palopoli, L., Schaerf, A., & Vasileet, D. (1999). NP-SPEC: executable specification language solving problems NP. Practical Aspects DeclarativeLanguages, pp. 1630.Chai, D., & Kuehlmann, A. (2003). fast pseudo-boolean constraint solver. DesignAutomation Conference, pp. 830835.Chaitin, G. J., Auslander, M., Chandra, A., Cocke, J., Hopkins, M., & Markstein, P. (1981).Register allocation via coloring. Computer Languages, 6, 4757.Chams, M., Hertz, A., & Werra, D. D. (1987). experiments simulated annealingcoloring graphs. European Journal Operations Research, 32, 260266.Coudert, O. (1997). Coloring real-life graphs easy. Design Automation Conference,pp. 121126.Crawford, J. (1992). theoretical analysis reasoning symmetry first-order logic.AAAI Workshop Tractable Reasoning Tenth National ConferenceArtificial Intelligence.Crawford, J., Ginsberg, M., Luks, E., & Roy, A. (1996). Symmetry-breaking predicatessearch problems. 5th International Conference Principles KnowledgeRepresentation Reasoning, pp. 148159.Culberson, J. (2004). Graph coloring page. http://web.cs.ualberta.ca/joe/Coloring/index.html.Darga, P. T., Liffiton, M. H., Sakallah, K. A., & Markov, I. L. (2004). Exploiting structuresymmetry generation cnf. 41st Internation Design Automation Conference,pp. 530534.Davis, M., Logemann, G., & Loveland, D. (1962). machine program theorem proving.Communications ACM, 5, 394397.320fiBreaking Instance-Independent Symmetries Exact Graph ColoringFahle, T., Schamberger, S., & Sellmann, M. (2001). Symmetry breaking. 7th InternationalConference Principles Practice Constraint Programming, pp. 93107.Feige, U., Goldwasser, S., Lovasz, L., Safra, S., & Szege, M. (1991). Approximating cliquealmost NP-complete. IEEE Symposium Foundations Computer Science,pp. 212.Focacci, F., & Milano, M. (2001). Global cut framework removing symmetries.Principles Practice Constraints Programming, pp. 7782.Galinier, P., & Hao, J. (1999). Hybrid evolutionary algorithms graph coloring. JournalCombinatorial Optimization, 3, 379397.Garey, M. R., & Johnson, D. S. (1979). Computers Intractability: Guide TheoryNP-completeness. W. H. Freeman Company.Gent, I. P. (2001). symmetry-breaking constraint indistinguishable values. Workshop Symmetry Constraint Satisfaction Problems.Goldberg, E., & Novikov, Y. (2002). Berkmin: fast robust SAT-solver. DesignAutomation Test Europe, pp. 142149.Haldorsson, M. M. (1990). still better performance guarantee approximate graphcoloring..Hentenryck, P. V., Agren, M., Flener, P., & Pearson, J. (2003). Tractable symmetry breakingCSPs interchangeable values. International Joint ConferenceArtificial Intelligence (IJCAI).Hertz, A., & Werra, D. D. (1987). Using tabu search techniques graph coloring. Computing, 39, 345351.Huang, J., & Darwiche, A. (2003). structure-based variable ordering heuristic SAT.International Joint Conference Artificial Intelligence, pp. 11671172.ILOG (2000). ILOG CPLEX ILP solver, version 7.0. http://www.ilog.com/products/cplex/.J.-P. Hamiez, J.-K. H. (2001). Scatter search graph coloring. 5th EuropeanConference Artificial Evolution, pp. 168179.Jagota, A. (1996). adaptive, multiple restarts neural network algorithm graph coloring. European Journal Operational Research, 93, 257270.Kirovski, D., & Potkonjak, M. (1998). Efficient coloring large spectrum graph.Design Automation Conference.Krishnamurthy, B. (1985). Short proofs tricky formulas. Acta Informatica, 22, 327337.Kubale, M., & Jackowski, B. (1985). generalized implicit enumeration algorithm graphcoloring. Communications ACM, 28, 412418.Kubale, M., & Kusz, E. (1983). Computational experience implicit enumeration algorithms graph coloring. Proceedings WG83 International WorkshopGraph Theoretic Concepts Computer Science, pp. 167176.Leighton, F. (1979). graph coloring algorithm large scheduling problems. JournalResearch National Bureau Standards, 84, 489506.321fiRamani, Aloul, Markov, & SakallahMcKay, B. D. (1990). Nauty users guide (version 1.5). http://cs.anu.edu.au/bdm/nauty/.Mehrotra, A., & Trick, M. A. (1996). column generation approach graph coloring.INFORMS Journal Computing, 8, 344354.Moskewicz, M., Madigan, C., Zhao, Y., Zhang, L., & Malik, S. (2001). Chaff: Engineeringefficient sat solver. Design Automation Conference, pp. 530535.Mycielski, J. (1955). Sur le coloriage des graphs. Colloqium Mathematicum, 3, 161162.Prestwich, S. (2002). Supersymmetric modelling local search. SymCon: WorkshopSymmetries CSPs, pp. 2128.Puget, J. (2002). Symmetry breaking revisited. Principles Practice ConstraintsProgramming, pp. 446461.Ramani, A., Aloul, F. A., Markov, I. L., & Sakallah, K. A. (2004). Breaking instanceindependent symmetries exact graph coloring. Design Automation TestEurope, pp. 324329.Roney-Dougal, C. M., Gent, I. P., Kelsey, T., & Linton, S. (2004). Tractable symmetrybreaking using restricted search trees. European Conference Artificial Intelligence, pp. 211215.Sheini, H. (2004). Pueblo 0-1 ILP solver. http://www.eecs.umich.edu/hsheini/pueblo/.Silva, J. P. M., & Sakallah, K. A. (1999). GRASP: new search algorithm satisfiability.IEEE Transactions Computers, 48, 506521.Trick, M. (1996). Network resources coloring graph.http://mat.gsia.cmu.edu/COLOR/color.html.Walsh, T. (2001). Search high degree graphs. 17th International Joint ConferenceArtificial Intelligence, pp. 266271.Warners, J. P. (1998). linear-time transformation linear inequalities conjunctivenormal form. Information Processing Letters, 68, 6369.Welsh, D. J. A., & Powell, M. B. (1967). upper bound chromatic numbergraph application timetabling problems. Computer Journal, 10, 8586.Werra, D. D. (1985). introduction timetabling. European Journal OperationsResearch, 19, 151162.322fiJournal Artificial Intelligence Research 26 (2006) 127-151Submitted 8/05; published 6/06Admissible Restrained RevisionRichard Boothrichard.b@msu.ac.thFaculty InformaticsMahasarakham UniversityMahasarakham 44150, ThailandThomas MeyerThomas.Meyer@nicta.com.auNational ICT AustraliaUniversity New South Wales223 Anzac ParadeKensington, NSW 2052, AustraliaAbstractpartial justification framework iterated belief revision DarwichePearl convincingly argued Boutiliers natural revision provided prototypicalrevision operator fits scheme. show Darwiche-Pearl argumentslead naturally acceptance smaller class operators refer admissible. Admissible revision ensures penultimate input ignored completely,thereby eliminating natural revision, includes Darwiche-Pearl operator, Nayakslexicographic revision operator, newly introduced operator called restrained revision.demonstrate restrained revision conservative admissible revisionoperators, effecting changes possible, lexicographic revision least conservative, point restrained revision also viewed composite operator,consisting natural revision preceded application backwards revision operator previously studied Papini. Finally, propose establishment principledapproach choosing appropriate revision operator different contexts discussfuture work.1. Introductionability rationally change ones knowledge base face new informationpossibly contradicts currently held beliefs basic characteristic intelligent behaviour. Thus question belief revision crucial importance Artificial Intelligence.last twenty years question received considerable attention, startingwork Alchourron, Gardenfors, Makinson (1985) usually abbreviated AGMproposed set rationality postulates reasonable revision operatorsatisfy. semantic construction revision operators later provided KatsunoMendelzon (1991), according agent mind plausibility orderingtotal preorder set possible worlds, knowledge base associatedordering identified set sentences true plausible worlds.approach dates back work Lewis (1973) counterfactuals. introducedbelief revision literature Grove (1988) Spohn (1988). Given new sentenceepistemic input , revised knowledge base set set sentences trueplausible worlds holds. shown Katsuno Mendelzon (1991),family operators defined construction coincides exactly family opc2006AI Access Foundation Morgan Kaufmann Publishers. rights reserved.fiBooth & Meyererators satisfying AGM postulates. Due intuitive appeal, construction camewidely used area. However, researchers soon began notice deficiencyalthough prescribes obtain new knowledge base, remains silentobtain new plausibility ordering serve target next epistemicinput. Thus rich enough deal adequately problem iterated beliefrevision. paper contribution study problem.iterated revision schemes sensitive history belief changes1 , basedversion recent best argument, newest information higherpriority anything else knowledge base. Arguably extreme caseNayaks lexicographic revision (Nayak, 1994; Nayak, Pagnucco, & Peppas, 2003). However,operators where, admitted knowledge base, rapidly becomesmuch candidate removal anything else set another, newer, pieceinformation comes along, Boutiliers natural revision (1993, 1996) case point.dual Rott (2003) terms radical revision new informationaccepted maximal, irremediable entrenchment see also Segerberg (1998). Anotherissue consider problem termed temporal incoherence (Rott, 2003):comparative recency information translate systematically comparative importance, strength entrenchmentinfluential paper Darwiche Pearl (1997) proposed framework iteratedrevision. proposal characterised terms sets syntactic semantic postulates, also viewed perspective conditional beliefs. extensionformulation Katsuno Mendelzon (1991) AGM revision (Alchourron et al.,1985). justify proposal Darwiche Pearl mount comprehensive argument.argument includes critique natural revision, shown admit changes.addition, provide concrete revision operator shown satisfy postulates. many ways seen prototypical Darwiche-Pearl operator.instructive observe two best-known operators satisfying Darwiche-Pearlpostulates, natural revision lexicographic revision, form opposite extremesDarwiche-Pearl framework: Natural revision conservative Darwiche-Pearl operator, sense effects changes possible, lexicographic revisionleast conservative.paper show Darwiche-Pearl arguments lead naturally acceptance smaller class operators refer admissible. provide characterisations admissible revision, terms syntactic well semantic postulates.Admissible revision ensures penultimate input ignored completely. consequence natural revision eliminated. hand, admissible revisionincludes prototypical Darwiche-Pearl operator well lexicographic revision, latter result also showing lexicographic revision least conservative admissibleoperators. removal natural revision scene leaves gap filledintroduction new operator refer restrained revision. conservativeadmissible revision operators, thus seen appropriate replacementnatural revision. give syntactic semantic characterisation restrained revision,1. external revision scheme like Areces Becher (2001) Freund Lehmann (1994)not.128fiAdmissible Restrained Revisiondemonstrate satisfies desirable properties. particular, unlike lexicographicrevision, ensures older information discarded unnecessarily, showsproblem temporal incoherence dealt with.Although natural revision feature class admissible revision operators,show still role play iterated revision, provided first temperedappropriately. show restrained revision also viewed composite operator,consisting natural revision preceded application backwards revision operatorpreviously studied Papini (2001).paper organised follows. outlining notation, review DarwichePearl framework Section 2. followed discussion admissible revisionSection 3. Section 4 introduce restrained revision, Section 5 showdefined composite operator. Section 6 discusses possibility enrichingepistemic states way determining appropriate admissible revision operatorparticular context. section also conclude briefly discuss future work.1.1 Notationassume finitely generated propositional language L includes constants >, closed usual propositional connectives, equipped classicalmodel-theoretic semantics. V set valuations L [] (or [B]) setmodels L (or B L). Classical entailment denoted logical equivalence. also use Cn denote operation closure classical entailment. Greekletters , , . . . stand arbitrary sentences. examples sometimes use lowercase letters p, q, r propositional atoms, sequences 0s 1s denotevaluations language. example, 01 denotes valuation, language generatedp q, p assigned value 0 q value 1, 011 denotesvaluation, language generated p, q r, p assigned value 0q r value 1. Whenever use term knowledge base always meanset sentences X deductively closed, i.e., X = Cn(X).2. Darwiche-Pearl RevisionDarwiche Pearl (1997) reformulated AGM postulates (Alchourron et al., 1985)compatible suggested approach iterated revision. necessitated moveknowledge bases epistemic states. epistemic state contains, additionknowledge base, information needed coherent reasoning including, particular,strategy belief revision agent wishes employ given time. DarwichePearl consider epistemic states abstract entities, provide single formalrepresentation. thus possible talk two epistemic states E F identical(denoted E = F) , yet syntactically different.2 borne mind below,particularly considering postulate (E 5). Darwiche Pearls reformulatedpostulates belief change operator epistemic states, knowledge bases.denote B(E) knowledge base extracted epistemic state E.(E1) B(E ) = Cn(B(E ))2. Personal communication Adnan Darwiche.129fiBooth & Meyer(E2) B(E )(E3) B(E ) B(E) +(E4)/ B(E) B(E) + B(E )(E5) E = F B(E ) = B(F )(E6) B(E ) iff(E7) B(E ( )) B(E ) +(E8)/ B(E ) B(E ) + B(E ( ))Darwiche Pearl show, via representation result similar KatsunoMendelzon (1991), revision epistemic states represented terms plausibility orderings associated epistemic states.3 specifically, every epistemic stateE associated total preorder E valuations, elements lowerordering deemed plausible. Moreover, two epistemic states E Fidentical (but may syntactically different), case E =F .Let min(, E ) denote minimal models E . knowledge base associated epistemic state obtained considering minimal models E i.e.,[B(E)] = min(>, E ). Observe means B(E) consistent.requirement enables us obtain unique knowledge base total preorder E .Preservation results paper requirement relaxed possible,technically messy.observant reader note assumption consistent B(E) incompatiblesuccessful revision . requires jettison (E6) insist consistentepistemic inputs only. (The left-to-right direction (E6) rendered superfluous (E1)assumption knowledge bases extracted epistemic statesconsistent.) difference original AGM postulates DarwichePearl reformulation first inspired critical observation Freund Lehmann (1994)occurs (E5), states revising logically equivalent sentences resultsepistemic states identical associated knowledge bases. weakeningoriginal AGM postulate, phrased notation follows:(B5) B(E) = B(F) B(E ) = B(F )(B5) states two epistemic states identical associated knowledge bases will,revised equivalent inputs, produce two epistemic states identical associated knowledge bases. stronger (E5) requires equivalent associatedknowledge bases original epistemic states identical. shall referreformulated AGM postulates, (E6) removed, DP-AGM.DP-AGM guarantees unique extracted knowledge base revision performed. sets [B(E )] equal min(, E ) thereby fixes plausible valuations E . However, places restriction rest ordering. purpose3. Alternative frameworks studying iterated revision, based using sequences sentences ratherplausibility orderings, Lehmann (1995) Konieczny Pino-Perez (2000).130fiAdmissible Restrained RevisionDarwiche-Pearl framework constrain remaining part new ordering.done way set postulates iterated revision (Darwiche & Pearl, 1997).(Throughout paper follow convention left associative.)(C1) B(E ) = B(E )(C2) B(E ) = B(E )(C3) B(E ) B(E )(C4)/ B(E )/ B(E )postulate (C1) states two pieces informationone specificotherarrive, first made redundant second. (C2) says twocontradictory epistemic inputs arrive, second one prevails; second evidence aloneyields knowledge base. (C3) says piece evidence retainedaccommodating recent evidence entails given current knowledgebase. (C4) simply says epistemic input act defeater. shall referclass belief revision operators satisfying DP-AGM (C1) (C4) DP-revision.following corresponding semantic versions (with v, w V ):(CR1) v [], w [] v E w iff v E w(CR2) v [], w [] v E w iff v E w(CR3) v [], w [] v E w v E w(CR4) v [], w [] v E w v E w(CR1) states relative ordering -worlds remain unchanged following revision, (CR2) requires -worlds. (CR3) requires that, -worldstrictly plausible -world, relationship retained -revision,(CR4) requires weak plausibility. Darwiche Pearl showed that, givenDP-AGM, precise correspondence obtains (Ci) (CRi) (i = 1, 2, 3, 4).One guiding principles belief revision principle minimal change: changesbelief state ought kept minimum. always clear oughtminimised. AGM theory prevailing wisdom minimal change refers setssentences corresponding knowledge bases. interpretations.move knowledge bases epistemic states, minimal change defined termsfewest possible changes associated plausibility ordering E . followsfrequently opportunity refer latter interpretation minimal change.See also discussion principle Rott (2000).3. Admissible Revisionsection consider two best-known DP-operators, propose three postulates added Darwiche-Pearl framework. first correctionstrengthening. show Darwiche-Pearl representation principleirrelevance syntax weak suggest appropriate strengthened postulate.131fiBooth & Meyersecond suggested arguments advanced Darwiche Pearl themselves. eliminates one operators criticise, satisfied sole operatorprovide instance framework. addition two postulatesDarwiche-Pearl framework leads definition class admissible revisionoperators. Finally, point problem Nayaks well-known lexicographic revisionoperator propose third postulate added. consequences insistingaddition third postulate discussed detail Section 4.mentioned Section 2, Darwiche Pearl replaced original AGM postulate(B 5) (E 5). attempts appropriate formulation principleirrelevance syntax, popularised Dalal (1988). whereas (B 5)shown strong, shown Darwiche Pearl (1997), closer inspection reveals(E 5) weak. precise, fails adequate formulation syntaxirrelevance iterated revision. specifies revision two equivalent sentencesproduce epistemic states identical associated knowledge bases, requireepistemic states, another revision two equivalent sentences, alsoproduce epistemic states identical associated knowledge bases. So, seenfollowing example, DP-AGM (and indeed, even (C1) (C4) added)possible B(E ) differ B(E ) even equivalentequivalent .Example 1 Consider propositional language generated two atoms p q letE epistemic state B(E) = Cn(p q). consider two epistemic statesE0 E00 B(E0 ) = B(E00 ) = Cn(p), 01 E0 00 00 E00 01. Observegives complete descriptions E0 E00 . tedious, difficult, verifysetting Ep =E0 Ep =E00 compatible DP-AGM. observeB(E p p) = Cn(p q), B(E p p) = Cn(p q).consequence this, propose (E5) replaced following postulate:(E50 ) E = F, B(E ) = B(F )semantic equivalent (E50 ) looks like this:(ER50 ) E = F E =F(ER50 ) states revision two identical epistemic states two equivalent sentences result epistemic states identical associated total preorders,epistemic states identical associated knowledge bases.Proposition 1 (E50 ) (ER50 ) equivalent, given DP-AGM.Proof: proof (E50 ) follows (ER50 ) straightfoward. converse, suppose (ER50 ) hold; i.e. F 6=E . meansexist x, V x E F x. let [] = {x, y}.x [B(E )], [B(F )] = {y}, B(E ) 6= B(F );violation (E50 ).132fiAdmissible Restrained Revisionalready clear (E50 ) desirable property. view bolsteredobserving well-known iterated revision operators satisfy it; naturalrevision, Darwiche-Pearl operator , Nayaks lexicographic revision, firstthird discussed detail below. fact, conjecture DarwichePearls intention replace (B5) (E50 ), (E5) proposepermanent replacement.Definition 1 set postulates obtained replacing (E5) (E50 ) DP-AGMdefined RAGM.Observe RAGM, like DP-AGM, guarantees [B(E )] = min(, E ).Rule (E50 ) first new postulates want add Darwiche-Pearlframework. lead second. One oldest known DP-operatorsnatural revision, usually credited Boutilier (1993, 1996), although idea alsofound (Spohn, 1988). main feature application principle minimalchange epistemic states. characterised DP-AGM plus following postulate:(CB) B(E ) B(E ) = B(E )(CB) requires that, whenever B(E ) inconsistent , revising Ecompletely ignore revision . semantic counterpart follows:(CBR) v, w/ [B(E )], v E w iff v E wshown Darwiche Pearl (1997), natural revision minimises changes conditionalbeliefs, | conditional belief epistemic state E iff B(E ).fact, Darwiche Pearl show (Lemma 1, p. 7), keeping E E similarpossible effect minimising changes conditional beliefs revision. So,(CBR) clear natural revision application minimal change epistemicstates. requires that, barring changes mandated DP-AGM, relative orderingvaluations remains unchanged, thus keeping E similar possible E . sensethen, natural revision conservative DP-operators. strict adherenceminimal change inadvisable needs tempered appropriately, issueaddressed Section 5. Darwiche Pearl shown (CB) strong,natural revision natural, sometimes yielding counterintuitive results.Example 2 (Darwiche & Pearl, 1997) encounter strange animal appearsbird, believe one. comes closer, see clearly animal red,believe red bird. remove doubts call bird expert examinesconcludes bird, sort animal. still believeanimal red? (CB) tells us longer believe red. seensubstituting B(E) = Cn() = Cn(bird) red (CB), instructing us totallyignore observation never taken place.Given Example 2, perhaps surprising Darwiche Pearl never considered postulate (P) below. example, argument retaining belief creaturered hinges upon assumption red conflict newly obtainedinformation kind animal. is, learning creature133fiBooth & Meyeranimal automatically disqualify red, reasonable retainbelief red. generally then, whenever consistent revision ,retained -revision inserted -revision.(P)/ B(E ) B(E )Applying (P) Example 2 see that, red consistent B(E bird),red B(E red bird). Put differently, (P) requires retain beliefanimals redness, provided would precluded observationred never occurred. (P) also proposed independently present paperJin Thielscher (2005) named Independence. semantic counterpart(P) looks like this:(PR) v [] w [], v E w v E w(PR) requires -world v least plausible -world w strictlyplausible w -revision. following result also proved independentlyJin Thielscher (2005).Proposition 2 satisfies DP-AGM, satisfies (P) iff also satisfies (PR).Proof: (P)(PR), let v [], w [], v E w, let [] = {v, w}.means/ B(E ) (since [B(E )] either equal {v} {v, w}), so,(P), B(E ). therefore v E w, not, would w E v,follows w [B(E )],/ B(E )].(P)(PR), suppose/ B(E ). means v [] [B(E )];is, v E w every w []. means B(E ). not,means x [] [B(E )]. Now, since x [B(E )], followsDP-AGM x E w every w [], x E v (since v [B(E )] []).also follows DP-AGM x [], therefore v E x, (PR)follows v E x; contradiction.Rule (PR) enforces certain changes ordering E receipt . fact soonexist -world v -world w plausibility level somewhereE (in v E w w E v), (PR) implies E 6=E . Furthermorechanges must also occur even already believed E begin with, i.e., B(E).(Although course B(E) B(E ) = B(E), i.e., knowledge base associatedE remain unchanged follows DP-AGM.) rules (P)/(PR) ensure inputbelieved certain minimal strength belief enough help survive nextrevision. point informed lead increase strengthagents belief , even cases agent already believes begin with,made before, e.g., Friedman Halpern (1999, p.405). Note (P) antecedent(C4) consequent (C3). fact, (P) stronger (C3) (C4) combined.easily seen semantic counterparts postulates. also followsconcrete example iterated revision operator provided Darwiche Pearl,operator refer employs form Spohnian conditioning (Spohn,1988), satisfies (PR), therefore (P) well. Furthermore, adopting (P) explicitly134fiAdmissible Restrained Revisionexclude natural revision permissible operator. accepting (P) move towardsviewpoint information obtained latest input ought discardedunnecessarily.Based analysis section propose strengthening Darwiche-Pearlframework (E5) replaced (E50 ) (C3) (C4) replaced (P).Definition 2 revision operator admissible iff satisfies RAGM, (C1), (C2), (P).Inasmuch Darwiche-Pearl framework visualised one -worldsslide downwards relative -worlds, admissible revision ensures, via (PR),downwards slide strict one.pave way third postulate would like add paperDarwiche-Pearl framework. begin with, note another view (P)significant weakening following property, first introduced Nayak et al. (1996):(Recalcitrance)/ Cn() B(E )Semantically, (Recalcitrance) corresponds following property, pointedBooth (2005) implicitly contained work Nayak et al. (2003):(R) v [], w [], v E w(Recalcitrance) property lexicographic revision operator, second wellknown DP-operators consider, one old natural revision.first introduced Nayak (1993) studied notably Nayak et al. (1994,2003), although, natural revision, idea actually dates back Spohn (1988).fact, lexicographic revision characterised DP-AGM (and also RAGM) together(C1), (C2) (Recalcitrance), result easily proved semantic counterpartsproperties Nayak et al.s semantic characterisation lexicographic revision(2003). Informally, lexicographic revision takes assumption recent best,Success postulate (E 2) based, adds assumption temporalcoherence. combination, leads stronger assumption recentbetter.analysis semantic characterisation lexicographic revision showsleast conservative DP-operators, sense effects changesrelative ordering valuations permitted DP-AGM (or RAGM matter)Darwiche-Pearl postulates. Since also admissible revision operator, followsalso least conservative admissible operator.problem (Recalcitrance) decision whether acceptsubsequent revision completely determined logical relationshipepistemic state E robbed influence. replacement (Recalcitrance)weaker (P) already gives E influence outcome. shortlyconstrain matters giving E much influence allowed postulatesadmissible revision. move ensures greater sensitivity agents epistemic recordmaking changes.Note lexicographic revision assumes recent information takes completeprecedence information obtained previously. Thus, applied Example 2,135fiBooth & Meyerrequires us believe animal, previously assumed bird, indeed red,red recent input conflict recently obtained input.reasonable approach many circumstances, dogmatic adherenceproblematic, following example shows.Example 3 holidaying wildlife park observe creature clearly red,far away determine whether bird land animal. adoptknowledge base B(E) = Cn(red). Next us person knowledge local areadeclares that, since creature red, bird. reason doubt him,adopt belief red bird. creature moves closer becomes clearbird. question is, continue believing red?circumstances described want initial observation take precedence,believe animal red. lexicographic revision allow us so.examples along similar lines speaking rigid acceptance (Recalcitrance)Glaister (1998, p.31) Jin Thielscher (2005, p.482).(P) allows possibility retaining belief animal red,enforce belief. rest section devoted discussion propertyso. help us express property, introduce extra piece terminologynotation.Definition 3 counteract respect epistemic state E, written !E ,iff B(E ) B(E ).use term counteract describe relation taken Nayak et al. (2003).!E means that, viewpoint E, tend exclude other.discuss properties relation. First note !E dependstotal preorder E obtained E. Indeed !E iff min(, E ) []min(, E ) []. turn reformulated following way, providesuseful aid visualise counteracts relation:Proposition 3 !E iff exist v [], w [] v E x w E xx min( , E ).Proof: First note that, since obviously min(, E ) [], min(, E ) [] may rewritten min(, E ) [( )]. Using fact E total preorder, easy seehold iff exists v [] v E x x min( , E ).way may rewrite min(, E ) [] min(, E ) [( )],equivalent saying exists w [] w E x x min( , E ).words, Proposition 3 says !E iff exist -world -worldstrictly plausible plausible ( )-worlds. immediatethings note !E symmetric, syntax-independent, i.e.,!E 0 !E 0 . Furthermore logically inconsistent!E , converse need hold (see short examplenext proposition confirmation). Thus !E seen weak form inconsistency.next result gives two properties !E :136fiAdmissible Restrained RevisionProposition 4 Given RAGM, following properties hold !E :(i) !E !E ( ) !E(ii) 6!E 6!E ( ) 6!EProof: (i) Suppose !E !E . show ( ) !E need showB(E ( )) ( ) B(E ). former alreadyB(E ) B(E ) !E !E respectively. Since followsRAGM B(E ) B(E ) B(E ( )) , L, concludeB(E ( )). latter already B(E )B(E ) !E !E respectively. conclude( ) B(E ), using RAGM (specifically (E 1)).(ii) Suppose 6!E 6!E . Firstly, either 6 B(E ) 6 B(E )must ( ) 6 B(E ) RAGM ( ) 6!E required.suppose B(E ) B(E ). Then, since 6!E 6!E ,means 6 B(E ) 6 B(E ). Since follows RAGMB(E ( )) B(E ) B(E ) , L, follows two6 B(E ( )) also case ( ) 6!E required.first property says counteracts two sentences separately,counteracts disjunction, second says cannot counteractdisjunction without counteracting least one disjuncts. Obviouslyproperties also hold binary relation logical inconsistency. However one departureinconsistency relation possible 6!E () !E .see assume moment L generated three propositional atoms {p, q, r}take = p, = q = r. take E lowest plausibilitylevel contains two valuations 010 100, next plausibility levelvaluation 111.ready introduce third postulate. following:(D) !E B(E )(D) requires that, whenever counteract respect E, disallowed-revision followed -revision. is, -revision E takesplace, information encoded E takes precedence information containedE . Darwiche Pearl (1997) considered property (it rule (C6)) arguedit, citing following example.Example 4 (Darwiche & Pearl, 1997) believe exactly one John Mary committed murder. get persuasive evidence indicating John murderer.followed persuasive information indicating Mary murderer. Let representJohn committed murder Mary committed murder. (D) forcesus conclude Mary, John, involved murder. This, accordingDarwiche Pearl, counterintuitive, since conclude involvedcommitting murder.Darwiche Pearls argument (D) rests upon assumption recentinformation ought take precedence information previously obtained.137fiBooth & Meyerseen Example 3, always valid assumption. fact, application (D)Example 3, = red bird = bird, produces intuitively correct resultbelief observed animal red: red B(E (red bird) bird).Another way gain insight significance (D) consider semanticcounterpart:(DR) v [], w [], w/ [B(E )], v E w v E w(DR) curtails rise plausiblity -worlds -revision. ensures that,exception plausible -worlds, relative ordering -world-worlds plausible remains unchanged.Proposition 5 Whenever revision operator satisfies RAGM, satisfies (D) iffsatisfies (DR).Proof: (D)(DR), suppose v [], w [], w/ [B(E )], v E w, let[] = {v, w}. B(E ) B(E ), so, (D).B(E ). follows v E w. not, wouldw E v, means w [B(E )], therefore/ B(E );contradiction.(D)(DR), suppose B(E ) B(E ), assume/ B(E ). means w [] also [B(E )].observe w/ [B(E )] since w -model. Also, since w [B(E )],follows RAGM w -model, therefore w/ [B(E )]. suppositionB(E ) means min(, E ) []. Since w [] [] thus followsv [] [] v E w. (DR) follows v E w. wcannot model B(E ); contradiction.4. Restrained Revisionstrengthen requirements admissible revision (those operators satisfyingRAGM, (C1), (C2) (P)) insisting (D) satisfied well. so, let usfirst consider semantic definition interesting admissible revision operator. RecallRAGM fixes set (E )-minimal models, setting equal min(, E ),places restriction remaining valuations ordered. followingproperty provides unique relative ordering remaining valuations.v E w or,(RR) v, w/ [B(E )], v E w iffv E w (v [] w [])(RR) says relative ordering valuations (E )-minimal remainsunchanged, except -worlds -worlds plausibility level; splittwo levels -worlds plausible -worlds. RAGM combined(RR) fixes unique ordering valuations.Definition 4 revision operator satisfying RAGM (RR) called restrained revision.138fiAdmissible Restrained Revisionturns within framework provided admissible revision, restrainedrevision satisfies (D). prove help following lemma, assertingequivalence (RR) (CR1), (CR2), (PR) (DR) presence RAGM.Lemma 1 Whenever revision operator satisfies RAGM, satisfies (RR) iffsatisfies (CR1), (CR2), (PR), (DR).Proof: (CR1)(RR), pick v, w []. v [B(E )] (CR1) followsRAGM. not, follows RAGM w/ [B(E )], (CR1) followsdirect application (RR). (CR2)(RR), pick v, w []. RAGM followsv, w/ [B(E )], obtain (CR2) direct application (RR).Observe (RR) rewrittenv E w and,0(RR ) v, w/ [B(E )], v E w iffv E w (v [] w [])Now, (PR)(RR), pick v [] w []. v [B(E )] (PR) followsRAGM. not, follows direct application (RR0 ). (DR)(RR), pickv [], w [], w/ [B(E )]. (PR) follows direct application0(RR ).(CR1), (CR2), (PR), (DR)(RR), let v, w/ [B(E )] suppose v E wv 6E w (i.e. w E v). show v E w either v [] w [].Assume case. w E v v [] w []. Now, secondcase impossible because, together w E v (PR) implies w E v;contradiction. first case also impossible. see why, observe (CR1)implies v w cannot -models, (CR2) v w cannot models, (DR) cannot case w [] v []. (PR) cannotcase w [] v []. concludes first part proof (CR1),(CR2), (PR), (DR)(RR). second part, let v, w/ [B(E )] suppose firstv E w. v, w [] v E w follows (CR1). v, w [] v E wfollows (CR2). v [] w [] v E w follows (PR). v []w [] v E w follows (DR). suppose v E w either v []w []. v [] v E w follows either (CR1) (PR), dependingwhether w [] w []. similarly, w [] v E w follows either(CR2) (PR), depending whether v [] v [].Theorem 2 RAGM, (C1), (C2), (P) (D) provide exact characterisation restrained revision.Proof: proof follows Lemma 1, Proposition 2, Proposition 5, correspondence (C1) (CR1), (C2) (CR2).Another interpretation (RR) maintains relative ordering valuations(E )-minimal, except changes mandated (PR).seen restrained revision conservative admissible revision operators,sense effects least changes relative ordering valuations permitted139fiBooth & Meyeradmissible revision. So, context admissible revision, restrained revision takesrole played natural revision Darwiche-Pearl framework.rest section examine properties restrained revision.Firstly, Examples 3 4 share interesting structural properties. both, initialknowledge base B(E) pairwise consistent subsequent sentencesrevision sequence, sentences revision sequence pairwise inconsistent.examples information contained initial knowledge base B(E)retained revision sequence. commonalities instances importantgeneral result. Let denote non-empty sequence inputs 1 , . . . , n , let Edenote revision sequence E 1 . . . n . Furthermore shall refer epistemicstate E -compatible provided/ B(E) every {1, . . . , n}.(O) E -compatible B(E) B(E )(O) says long B(E) direct conflict inputs sequence1 , . . . , n , entire B(E) propagated knowledge base obtainedrevision sequence E 1 . . . n . preservation property satisfiedrestrained revision.Proposition 6 Restrained revision satisfies (O).Proof: denote E , = 0, . . . , n, revision sequence E 1 , . . . , (withE 0 = E). give inductive proof that, v [B(E)] w/ [B(E)], v Ei w= 0, . . . , n. words, every B(E)-world always strictly every non B(E)world. result follows immediately. = 0 amounts showingv E w follows immediately definition E B(E). pick= 1, . . . , n assume v Ei1 w. consider four cases. v, w [i ]follows (CR1) v Ei w. v, w [i ] follows (CR2) v Ei w.v [i ] w [i ] follows (PR) v Ei w. finally, supposev [i ] w [i ]. -compatibility x [B(E)] [i ],inductive hypothesis, x Ei1 w. w/ [B(E )], follows (DR)v Ei w.Although restrained revision preserves information directly contradicted,dogmatically wedded older information. neither two successive, incompatible, epistemic states conflict inputs sequence = 1 , . . . , n ,prefers latter epistemic state revising .Proposition 7 Restrained revision satisfies following property:(Q) E E -compatible B(E)B(E) , B(E) B(E)B(E) * B(E )Proof: follows immediately Proposition 6 B(E ) B(E ).B(E) * B(E ) follows consistency B(E ).Next consider another preservation property, time, unlike case (O)(Q), look circumstances B(E) incompatible inputsrevision sequence.140fiAdmissible Restrained Revision(S) B(E ) B(E ) B(E ) = B(E )Note that, given RAGM, antecedent (S) implies B(E). Thus (S) statesbelieved initially, subsequent commitment either negationwould change fact, sequence inputs preceded, second input concerning nullified, older input regarding retained.Proposition 8 Restrained revision satisfies (S).Proof: Suppose antecedent holds. !E consequent holds. factseen property (T) Proposition 10 below. suppose 6!E .either 6 B(E ) 6 B(E ). latter doesnt hold oneassumptions together (C2), former must hold. implies B(E )(P). Combining assumption get !E . case getB(E ) = B(E ) (again using (T)), (since 6!E ) B(E ) =B(E ( )) ((T) more), turn equals B(E ( )) (C2). SinceB(E ) turn equal B(E ) RAGM required.provide compact syntactic representation restrained revision. Firstshow (C1) (P) combined single property, (C2) (D).Proposition 9 Given RAGM,1. (C1) (P) together equivalent single rule(C1P) 6 B(E ) B(E ) = B(E ( ))2. (C2) (D) together equivalent single rule(C2D) !E B(E ) = B(E ).Proof: (C1),(P)(C1P), suppose 6 B(E). (P) follows B(E)means, RAGM, B(E ) = B(E ( )). (C1) followsB(E()) = B(E()), thus B(E) = B(E()). (C1)(C1P),suppose . 6 B(E ) RAGM, B(E ) = B(E ( ))(C1P). since follows B(E ) = B(E ). (P)(C1P),suppose 6 B(E ). B(E ) = B(E ( )) (C1P) means,RAGM, B(E ).(C2),(D)(C2D), suppose !E . (D), B(E ).RAGM means B(E ) = B(E ( )). Now, (C2) followsB(E ( )) = B(E ( )). B(E ) = B(E ( )). sinceB(E), get RAGM B(E( )) = B(E), followsB(E ) = B(E ). (C2)(C2D), suppose . !E EB(E ) = B(E ) (C2D). (D)(C2D), suppose !E .B(E) = B(E) (C2D) since B(E), follows B(E).(C1P) (C2D) provide conditions reduction two-step revision sequenceE single-step revision (if regards resulting knowledge base). (C1P)141fiBooth & Meyerreduces ( )-revision consistent -revision. (C2D) reduces-revision, ignoring completely, counteract respect E. Now,follows RAGM consequent (C1P) also obtains 6 B(E ).Putting together get succinct characterisation restrained revision.Proposition 10 restrained revision satisfies RAGM and:B(E )!E(T) B(E ) =B(E ( )) otherwise.Proof: Theorem 2 Proposition 9 sufficient show RAGM, (C1P)(C2D) hold iff RAGM (T) hold. So, suppose satisfies RAGM (T). (C1P)follows bottom part (T), (C2D) follows top part. Conversely,suppose satisfies RAGM, (C1P) (C2D). !E follows (C2D)B(E ) = B(E ). not, consider two cases./ B(E ) follows(C1P) B(E) = B(E()). Otherwise case/ B(E).follows RAGM B(E ) = B(E ( )).replace !E first clause (T) strongerlogically inconsistent, would obtain instead characterisation lexicographicrevision given Nayak et al. (2003).Proposition 10 allows us see clearly another significant property restrained revision.!E know B(E ) directly (D), 6!EProposition 10 tells us B(E ) = B(E ( )) B(E ) RAGM.Thus see state E epistemic status (either accepted rejected)always completely determined, i.e., proved:Proposition 11 Restrained revision satisfies following property:(U) 6 B(E ) B(E )(Given similar characterisation mentioned above, easy see lexicographicrevision satisfies (U) too.) Like (P), property (U) read providing conditionspenultimate revision input believed. antecedent simplysaying B(E ) consistent . Thus (U) saying penultimate inputbelieved long consistent so. chaining (U) together (C4), easilysee (U) actually implies (P) presence (C4). consequence, obtainfollowing alternative axiomatic characterisation restrained revision.Theorem 3 RAGM, (C1), (C2), (C4), (U) (D) provide exact characterisationrestrained revision.(U), also able provide simple semantic counterpart property. correspondsseparating -worlds -worlds total preorder E following-revision, plausibility level E either contains -worlds contains-worlds:Proposition 12 Whenever revision operator satisfies RAGM, satisfies (U) iffsatisfies following property:142fiAdmissible Restrained Revision(UR) v [] w [], either v E w w E vProof: (U)(UR) suppose (UR) doesnt hold, i.e., exist , v [] w []v E w w E v. Letting [] = {v, w} get[B(E )] = {v, w} RAGM thus , 6 B(E ) (becausev, w [B(E )] respectively). Hence (U) doesnt hold.(U)(UR) suppose (U) doesnt hold, i.e., exist , , 6B(E ). exist v [] w [] v, w [B(E )] =(by RAGM) min(, E ). Since v w (E )-minimal -worlds mustv E w w E v. Hence , v, w give counterexample (UR).Finally section turn two properties first mentioned (as far know)Schlecta et al. (1996) (see also work Lehmann et al. (2001)):(Disj1) B(E ) B(E ) B(E ( ) )(Disj2) B(E ( ) ) B(E ) B(E )(Disj1) says sentence believed one two sequences revisionsdiffer step (step one case other), sentencealso believed sequence differs steprevision disjunction . Similarly, (Disj2) says every sentence believed( )--revision believed least one (-) (-).conditions reasonable properties expect revision operators.Proposition 13 Restrained revision satisfies (Disj1) (Disj2).prove result make use properties counteracts relation givenProposition 4, along following lemma.Lemma 4 !E ( ) 6!E B(E (( ) )) = B(E ( ))Proof: Suppose !E ( ) 6!E . first show impliesB(E(())). able conclude required B(E(())) =B(E ( )) using RAGM. suppose contrary 6 B(E (( ) )).exists -world w min(( ) , E ). also w min( , E ). Since!E know Proposition 3 exist -world w1 -world w2wi E w = 1, 2. Clearly w1 also ( )-world, infer ( ) 6!Econtradiction. Hence B(E (( ) )) required.Proof:[of Proposition 13] prove properties simultaneously looking two cases:Case (i): ( ) !E . case B(E ( ) ) = B(E ) property (T)Proposition 10. Meanwhile know Proposition 4(ii) either !E !E ,using (T) know least one B(E ) B(E ) must alsoequal B(E ). Hence see (Disj1) (Disj2) hold case.Case (ii): ( ) 6!E . case (T) tells us B(E ( ) ) = B(E (( ) )) =B(E (( ) ( )). Meanwhile Proposition 4(i) tells us least one 6!E6!E holds. consider two subcases according either143fiBooth & Meyerhold, one holds. hold B(E ) = B(E ( ))B(E ) = B(E ( )), (Disj1) (Disj2) reduce(Disj10 ) B(E ( )) B(E ( )) B(E (( ) ( )))(Disj20 ) B(E (( ) ( ))) B(E ( )) B(E ( ))respectively. consequence RAGM sentences ,(1) B(E ) B(E ) B(E ( )) (2) B(E ( )) B(E ) B(E ).Substituting gives us required (Disj10 ) (from (1))(Disj20 ) (from (2)).lets consider subcase !E 6!E . (A symmetric argumentwork subcase 6!E !E .) 6!E getB(E ) = B(E ( )), !E together ( ) 6!E get alsoB(E()) = B(E()) using Lemma 4. case B(E()) = B(E),(Disj1) (Disj2) follow immediately.end section remarking shown lexicographic revision alsosatisfies (Disj1) (Disj2).5. Restrained Revision Composite Operatorsaw Section 3, Boutiliers natural revision operator let us denote sectionvulnerable damaging counterexamples red bird Example 2,fails satisfy reasonable postulate (P). Although new input acceptednext epistemic state E , way provide preservationsubsequent revisions. Hans Rott (2003, p.128) describes it, [t]he recent inputsentence always embraced without reservation, last one input sentence, however,treated utter disrespect. Thus, seem convincing reasons rejectviable operator performing iterated revision. However, literature epistemic statechange constantly reminds us keeping changes minimal major concern,judged purely minimal change viewpoint, clear cant beaten!find way apparent quandary? section showuse retained, provided application preceded intermediate operationwhich, rather revising E new input , essentially revised E.Given epistemic state E sentence , let us denote E / resultintermediate operation. E / epistemic state. idea forming E / ,information E maintained. is, total preorder E/ satisfyv E w implies v E/ w.(1)rather leaving behind entirely favour E, much informationalcontent preserved E / possible. formalised sayingv [], w [], take v E/ w long conflict (1)above. second requirement guarantee enough presencerevised epistemic state E help survive subsequent revisions allow (P)144fiAdmissible Restrained Revisioncaptured. Taken together, two requirements enough specify E/ uniquely:v E w,v E/ w iff(2)v E w (v [] w []).Thus, E/ lexicographic refinement E two-level total preorderdefined v w iff v [] w []. backwards revision operator new.studied Papini (2001). also viewed backwards versionNayaks lexicographic revision operator. necessarily B(E / ) (thishold 6 B(E)), / satisfy RAGM.Given /, define composite revision operator / settingE / = (E / )(3)reminiscent Levi Identity (Gardenfors, 1988), used AGM theory recipereducing operation revision knowledge bases composite operation consistingcontraction plus expansion. (3), playing role expansion. operator /satisfy RAGM. fact, easily seen comparing (2) condition(RR) start Section 4, / coincides restrained revision.Proposition 14 Let R denote restrained revision operator. R = / .Thus proved restrained revision viewed combination two existingoperators.6. Choose Revision Operatorcontribution paper far summarised follows. arguedreplacement Darwiche-Pearl framework class admissible revision operators, arguing former needs strengthened. eliminatednatural revision, retained lexicographic revision operator DarwichePearl admissible operators. also introduced new admissible revision operator,restrained revision, argued plausibility. argument restrained revision somehow unique, preferred revision operators.contention merely that, epistemic state revision, Darwiche-Pearl frameworkweak replaced admissible revision. restrained revision,admissible revision operator, therefore one many revision operators deemedrational. question admissible revision operator use particular situationone depends number issues, context, strength certainbeliefs held, source information, on. point essentially alsomade Friedman Halpern (1999). example, Example 3 formed partargument use restrained revision, use lexicographic revision.effect used example argue red ought B (E red bird bird),B(E) = Cn(red). change context slightly, becomes examplefavour use lexicographic revision, restrained revision.Example 5 observe creature seems red, far awaydetermine whether bird land animal. adopt knowledge base B(E) =145fiBooth & MeyerCn(red). Next us expert birds remarks that, creature indeed red,must bird. adopt belief red bird. get information someonestanding closer creature bird. Given context, is, reliabilityexpert combined statement creature initially seemed red,reasonable adopt lexicographic approach recent best concludebird red. Formally, red B (E red bird bird), B(E) = Cn(red).case source information dramatically affects outcome, considerfollowing example.Example 6 Consider sequence inputs p followed finite number, say n,instances pair p q, q. (To make concrete, reader might wishsubstitute p red q bird.) Since p direct conflict sentencessequence succeeding it, revision operator satisfying property (and includesrestrained revision) require p contained knowledge base obtainedrevision sequence. Now, pair p q q obtained different source,conclusion clearly unreasonable. all, sequence amounts toldp case, followed n different sources essentially telling p case.hand, pairs p q q come source, caseclear cut anymore. fact, case one would expect resultobtained sequence p, p q, q, sequence formal structureemployed Example 3, restrained revision seen reasonable approach.Another example restrained revision fares less well following:4Example 7 Suppose teaching class students consisting n boys girls,suppose class takes part mathematics competition. = 1, . . . , nj = 1, . . . , let propositional variables pi qj stand boy competitiongirl j competition respectively, suppose initiallyW believe oneboys competition, i.e., B(E) = Cn( ) = pisentence expressing uniqueness competition winner. suppose interviewboys one other, tells us either one girlswon, i.e., obtain sequence inputs ( pi )i . Suppose willingaccept boys testimony. Using revision operator satisfies lead usbelieve boy n competition, seems implausible. Lexicographic revision givesdesired result one girls competition.examples clear agent need not, cases, oughtstick revision operator every time perform revision.means agent keep switching one revision operator anotherprocess iterated revision. course, leads question choose amongavailable (admissible) revision operators particular point. comprehensive answerquestion beyond scope paper, provide cluesaddress problem. brief, contend epistemic states enriched,detailed specification internal structure. Looking back history beliefrevision, see exactly field progressed. initial papers,4. grateful one anonymous referees suggesting example.146fiAdmissible Restrained RevisionAGM revision, epistemic state taken contain nothingknowledge base. So, example, basic AGM revision characterised first sixAGM postulates imposes structure epistemic states all. shall refersimple epistemic states. full AGM belief revision characterised eight AGMpostulates, view still one revision knowledge bases, every revisionoperator knowledge base B uniquely associated B-faithful total preorder; i.e.,total preorder valuations models B minimal elements.small step define epistemic states include ordering, i.e. includetotal preorder E associated epistemic state E part definition E.shall refer complex epistemic states.leads two different views revision process. view revisionoperator simple epistemic states many different revision operators; onecorresponding B-faithful total preorders, way distinguishchoose revision operator. Viewed such, iterated revisionprocess (possibly) different revision operator employed every revisionstep. principal view adopted Nayak et al. (2003). However, viewrevision operator complex epistemic states, every epistemic state contains enoughinformation determine uniquely knowledge base, faithful total preorder,resulting revision. words, enough information encodedepistemic state uniquely determine knowledge base resulting revision,lack information uniquely determine full epistemic state. DarwichePearl framework, also admissible revision, place constraints resultingepistemic state, impose additional structure complex epistemic state.view admissible revision complex epistemic states analogous basic AGMrevision simple epistemic states. next step would thus impose additionalstructure complex epistemic states. could possibly involve addition secondordering valuations done, example, Booth et al. (2004). casesimple epistemic states effect adding two supplementary postulates constrainbasic revision extent revision operator uniquely associated Bfaithful total preorder. sense, addition supplementary postulates allowedimposition additional structure simple epistemic states. Recall one wayinterpreting two supplementary postulates explain interactionrevision two sentences revision conjunction, something basic postulatesaddress.So, seen, addition supplementary postulates leads definitionrevision operators complex epistemic states. conjecture giving additionalstructure complex epistemic states might involve provision postulates analogoustwo AGM supplementary postulates. particular, conjecture postulatesmight explain interaction two sentences conjunction,disjunction, iterated revision. Observe none Darwiche-Pearl postulates,additional postulates admissible revision matter, address issue.fact postulates suggested (of aware) faraddress (Disj1) (Disj2). speculate appropriate set supplementarypostulates iterated revision (which may may include two mentioned)lead definition extra structure complex epistemic states,147fiBooth & Meyerincorporated enriched version complex epistemic states, revisionseen operators enriched entities. Let us refer enriched epistemicstates. Enriched epistemic states enable us determine uniquely complex epistemicstate resulting revision, thereby solving question started with;determining revision complex epistemic states use every particular pointprocess iterated revision. shall briefly discuss possible wayenriching complex epistemic states. note also recent proposalBooth et al. (2006). instructive observe (Disj1) (Disj2) holdframework.proposed outline without pitfalls. obvious problemapproach leaves us meta-version dilemma startedwith. Using enriched epistemic states able uniquely determine complexepistemic states resulting revision, resulting enriched epistemic state.lessen problem constraining permissible resulting enriched epistemic statesway admissible revision constrains permissible complex epistemic states,chances whittling produce single permissible enrichedepistemic state. And, course, bound occur again. is,whenever solve problem uniquely determining epistemic state certainstructure process enrichment, saddled questionuniquely determine enriched epistemic state resulting revision.conjecture level point reached constraining enrichedepistemic states, la admissible revision, eventually lead unique enrichedepistemic state associated every revision. research determine whetherconjecture holds water.conclusion, shown Darwiche-Pearl arguments lead acceptanceadmissible revision operators class worthy study. restrained revisionoperator, particular, exhibits quite desirable properties. Besides taking placenatural revision operator adhering closely principle minimal change,satisfaction properties (O), (Q) (U) shows unnecessarilyremove previously obtained information.future work would also like explore thoroughly class admissiblerevision operators. paper saw restrained revision lexicographic revisionlie opposite ends spectrum admissible operators. represent respectivelyconservative least conservative admissible operators senseeffect changes least changes, respectively, relative orderingvaluations permitted admissible revision. natural question whether existsaxiomatisable class admissible operators represents middle ground. Oneclue finding class found counteracts relation !Ederived epistemic state E. said, relation depends preorderE associated E. fact, given total preorder V define relation!! iff min(, ) [] min(, ) [].clearly !E =!E . Furthermore full relation V V ! reduceslogical inconsistency. counteracts relation stronger !E , still weaker logicalinconsistency found setting !=!0 , 0 lies somewhere E148fiAdmissible Restrained RevisionV V . Hence one avenue worth exploring might assume epistemicstate E extract one two preorders E 0E E 0E . Then,instead requiring !E deduce B(E ), done restrainedrevision (the postulate (D)), could require stronger condition !0Ehold. currently experimenting strategies using second preorderguide manipulation E enable property satisfied. use secondpreorder seen way enriching epistemic state, might thus contributesolution choice revision operators discussed Section 6.future work relates also two extreme cases revision, lookeddifferent angle. mentioned earlier, lexicographic revision formalisationrecent best approach revision taken logical extreme. approachexemplified (E2) postulate, also known Success, requires revisionsuccessful, sense epistemic input provided always containedresulting knowledge base. Given (E2) one postulates admissiblerevision, requirement carries even restrained revision, oppositeend spectrum admissible revision. means admissible revisionoperator differs lexicographic revision still adheres dictumrecent best, raises question recent input givenprominence. relaxation requirement would imply giving (E2)venturing area known non-prioritised revision (Booth, 2001; Chopra, Ghose, &Meyer, 2003; Hansson, 1999). speculate appropriate relaxation admissiblerevision, (E2) removed requirement, lead class (non-prioritised)revision operators strictly containing admissible revision, lexicographic revisionstill one end spectrum, end spectrum occupiedoperator studied Papini (2001) used sub-operation restrained revisionSection 5. operator formalised extreme version recent worst;words, older better.AcknowledgementsMuch first authors work done stints researcher WollongongUniversity Macquarie University, Sydney. wishes thank Aditya Ghose AbhayaNayak making possible enjoy great working environments there,also interesting comments work. Thanks also due Samir Chopracontributed preliminary version paper, Adnan Darwiche clearingmisconceptions definition epistemic states, three anonymous refereesvaluable insightful comments. National ICT Australia funded AustraliaGovernments Department Communications, Information Technology ArtsAustralian Research Council Backing Australias Ability ICTCentre Excellence program. supported members Australian NationalUniversity, University NSW, ACT Government, NSW Government affiliate partnerUniversity Sydney.149fiBooth & MeyerReferencesAlchourron, C. E., Gardenfors, P., & Makinson, D. (1985). logic theory change:Partial meet functions contraction revision. Journal Symbolic Logic, 50,510530.Areces, C., & Becher, V. (2001). Iterable AGM functions. Frontiers belief revision,pp. 261277. Kluwer, Dordrecht.Booth, R. (2001). negotiation-style framework non-prioritised revision. van Benthem, J. (Ed.), Theoretical Aspects Rationality Knowledge: ProceedingsEighth Conference (TARK 2001), pp. 137150, San Francisco, California. MorganKaufmann.Booth, R. (2005). logic iterated non-prioritised revision. Conditionals, Information Inference Selected papers Workshop Conditionals, InformationInference, 2002, Vol. 3301 LNAI, pp. 86107. Springer-Verlag, Berlin.Booth, R., Chopra, S., Ghose, A., & Meyer, T. (2004). unifying semantics beliefchange. Mantaras, R. L. D., & Saitta, L. (Eds.), Sixteenth European ConferenceArtificial Intelligence: ECAI2004, pp. 793797. IOS Press.Booth, R., Meyer, T., & Wong, K.-S. (2006). bad day surfing better good dayworking: revise total preorder. Proceedings KR2006, Tenth International Conference Principles Knowledge Representation Reasoning.Boutilier, C. (1993). Revision sequences nested conditionals. Bajcsy, R. (Ed.), IJCAI93. Proceedings 13th International Joint Conference Artificial Intelligenceheld Chambery, France, August 28 September 3, 1993, Vol. 1, pp. 519525, SanMateo, CA. Morgan Kaufmann.Boutilier, C. (1996). Iterated revision minimal changes conditional beliefs. JournalPhilosophical Logic, 25 (3), 263305.Chopra, S., Ghose, A., & Meyer, T. (2003). Non-prioritized ranked belief change. JournalPhilosophical Logic, 32 (3), 417443.Dalal, M. (1988). Investigations theory knowledge base revision. Proceedings7th National Conference American Association Artificial Intelligence,Saint Paul, Minnesota, pp. 475479.Darwiche, A., & Pearl, J. (1997). logic iterated belief revision. Artificial Intelligence, 89, 129.Freund, M., & Lehmann, D. (1994). Belief revision rational inference. Tech. rep. TR94-16, Leibniz Centre Research Computer Science, Institute ComputerScience, Hebrew University Jerusalem.Friedman, N., & Halpern, J. Y. (1999). Belief revision: critique. Journal Logic,Language Information, 8, 401420.Gardenfors, P. (1988). Knowledge Flux : Modeling Dynamics Epistemic States.MIT Press, Cambridge, Massachusetts.Glaister, S. M. (1998). Symmetry belief revision. Erkenntnis, 49, 2156.150fiAdmissible Restrained RevisionGrove, A. (1988). Two modellings theory change. Journal Philosophical Logic, 17,157170.Hansson, S. O. (1999). survey non-prioritized belief revision. Erkenntnis, 50, 413427.Jin, Y., & Thielscher, M. (2005). Iterated belief revision, revised. ProceedingsNineteenth International Joint Conference Artificial Intelligence (IJCAI 05), pp.478483.Katsuno, H., & Mendelzon, A. O. (1991). Propositional knowledge base revision minimal change. Artificial Intelligence, 52, 263294.Konieczny, S., & Pino Perez, R. (2000). framework iterated revision. JournalApplied Non-Classical Logics, 10(3-4), 339367.Lehmann, D. (1995). Belief revision, revised. Proceedings Fourteenth InternationalJoint Conference Artificial Intelligence (IJCAI95), pp. 15341540.Lehmann, D., Magidor, M., & Schlechta, K. (2001). Distance semantics belief revision.Journal Symbolic Logic, 66, 295317.Lewis, D. K. (1973). Counterfactuals. Journal Philosophy, 70, 556567.Nayak, A. C. (1993). Studies Belief Change. Ph.D. thesis, University Rochester.Nayak, A. C. (1994). Iterated belief change based epistemic entrenchment. Erkenntnis,41, 353390.Nayak, A. C., Foo, N. Y., Pagnucco, M., & Sattar, A. (1996). Changing Conditional BeliefUnconditionally. Shoham, Y. (Ed.), Theoretical Aspects Rationality Knowledge: Proceedings Sixth Conference (TARK 1996), pp. 119136, San Francisco,California. Morgan Kaufmann.Nayak, A. C., Pagnucco, M., & Peppas, P. (2003). Dynamic belief change operators. Artificial Intelligence, 146, 193228.Papini, O. (2001). Iterated revision operations stemming history agentsobservations. Frontiers belief revision, pp. 281303. Kluwer, Dordrecht.Rott, H. (2000). Two dogmas belief revision. Journal Philosophy, 97, 503522.Rott, H. (2003). Coherence conservatism dynamics belief II: Iterated beliefchange without dispositional coherence. Journal Logic Computation, 13 (1),111145.Schlechta, K., Lehmann, D., & Magidor, M. (1996). Distance semantics belief revision.Shoham, Y. (Ed.), Proceedings Sixth Conference Theoretical AspectsRationality Knowledge, pp. 137145. Morgan Kaufmann.Segerberg, K. (1998). Irrevocable belief revision dynamic doxastic logic. Notre DameJournal Formal Logic, 39, 287306.Spohn, W. (1988). Ordinal conditional functions: dynamic theory epistemic states.Harper, W. L., & Skyrms, B. (Eds.), Causation Decision: Belief, ChangeStatistics: Proceedings Irvine Conference Probability Causation: VolumeII, Vol. 42 University Western Ontario Series Philosophy Science, pp.105134, Dordrecht. Kluwer Academic Publishers.151fiJournal Artificial Intelligence Research 26 (2006) 35-99Submitted 8/05; published 5/06Planning Graph Heuristics Belief Space SearchDaniel BryceSubbarao Kambhampati,DAN . BRYCE @ ASU . EDURAO @ ASU . EDUDepartment Computer Science EngineeringIra A. Fulton School EngineeringArizona State University, Brickyard Suite 501699 South Mill Avenue, Tempe, AZ 85281David E. SmithDE 2 SMITH @ EMAIL . ARC . NASA . GOVNASA Ames Research CenterIntelligent Systems Division, MS 269-2Moffett Field, CA 94035-1000Abstractrecent works conditional planning proposed reachability heuristics improveplanner scalability, many lack formal description properties distance estimates.place previous work context extend work heuristics conditional planning,provide formal basis distance estimates belief states. give definitiondistance belief states relies aggregating underlying state distance measures.give several techniques aggregate state distances associated properties. Many existingheuristics exhibit subset properties, order provide standardized comparisonpresent several generalizations planning graph heuristics used single planner.compliment belief state distance estimate framework also investigating efficient planninggraph data structures incorporate BDDs compute effective heuristics.developed two planners serve test-beds investigation. first, CAltAlt,conformant regression planner uses A* search. second, P D, conditionalprogression planner uses AO* search. show relative effectiveness heuristictechniques within planners. also compare performance planners severalstate art approaches conditional planning.1. IntroductionEver since CGP (Smith & Weld, 1998) SGP (Weld, Anderson, & Smith, 1998) series planners developed tackling conformant conditional planning problems includingGPT (Bonet & Geffner, 2000), C-Plan (Castellini, Giunchiglia, & Tacchella, 2001), PKSPlan (Petrick & Bacchus, 2002), Frag-Plan (Kurien, Nayak, & Smith, 2002), MBP (Bertoli, Cimatti, Roveri,& Traverso, 2001b), KACMBP (Bertoli & Cimatti, 2002), CFF (Hoffmann & Brafman, 2004),YKA (Rintanen, 2003b). Several planners extensions heuristic state space plannerssearch space belief states (where belief state set possible states). Withoutfull-observability, agents need belief states capture state uncertainty arising startinguncertain state executing actions uncertain effects known state. focusfirst type uncertainty, agent starts uncertain state deterministic actions.seek strong plans, agent reach goal certainty despite partially knownstate. Many aforementioned planners find strong plans, heuristic search plannersc2006AI Access Foundation. rights reserved.fiB RYCE , K AMBHAMPATI , & MITHcurrently among best. Yet foundation constitutes good distance-based heuristicbelief space adequately investigated.Belief Space Heuristics: Intuitively, argued heuristic merit belief state dependsleast two factorsthe size belief state (i.e., uncertainty current state),distance individual states belief state destination belief state. questioncourse compute measures effective. Many approaches estimatebelief state distances terms individual state state distances states two beliefstates, either lack effective state state distances ways aggregate state distances.instance MBP planner (Bertoli et al., 2001b) counts number states current beliefstate. amounts assuming state distance unit cost, planning statedone independently. GPT planner (Bonet & Geffner, 2000) measures state state distancesexactly takes maximum distance, assuming states belief state positively interact.Heuristic Computation Substrates: characterize several approaches estimating belief statedistance describing terms underlying state state distances. basis investigation adapting classical planning reachability heuristics measure state distancesdeveloping state distance aggregation techniques measure interaction plans statesbelief state. take three fundamental approaches measure distance two beliefstates. first approach involve aggregating state distance measures, rather useclassical planning graph compute representative state distance. second retains distinctionsindividual states belief state using multiple planning graphs, akin CGP (Smith& Weld, 1998), compute many state distance measures aggregated. thirdemploys new planning graph generalization, called Labelled Uncertainty Graph (LU G),blends first two measure single distance two belief states. techniques discuss types heuristics compute special emphasis relaxedplans. present several relaxed plan heuristics differ terms employ state distance aggregation make stronger assumptions states belief state co-achievegoal action sequences independent, positively interact, negatively interact.motivation first three planning graph techniques measuring belief statedistances try minimal extension classical planning heuristics see work us.Noticing use classical planning heuristics ignores distinctions states beliefstate may provide uninformed heuristics, move second approach possiblybuild exponentially many planning graphs get better heuristic. multiple planninggraphs extract heuristic graph aggregate get belief state distancemeasure. assume states belief state independent, aggregate measuressummation. Or, assume positively interact use maximization. However,show, relaxed plans give us unique opportunity measure positive interactionindependence among states essentially taking union several relaxed plans. Moreover,mutexes play role measuring negative interactions states. Despite utilityrobust ways aggregate state distances, still faced exponential blownumber planning graphs needed. Thus, third approach seeks retain ability measureinteraction state distances avoid computing multiple graphs extracting heuristicseach. idea condense symbolically represent multiple planning graphs singleplanning graph, called Labelled Uncertainty Graph (LU G). Loosely speaking, single graphunions causal support information present multiple graphs pushes disjunction,36fiP LANNING G RAPH H EURISTICS B ELIEF PACE EARCHdescribing sets possible worlds (i.e., initial literal layers), labels. planning graphvertices present multiple graphs, redundant representation avoided.instance action present multiple planning graphs would presentLU G labelled indicate applicable planning graph projectionpossible world. describe extract heuristics LU G make implicitassumptions state interaction without explicitly aggregating several state distances.Ideally, planning graph techniques considers every state belief state computeheuristics, belief states grow size could become uninformed costly. example,single classical planning graph ignores distinctions possible states heuristicbased multiple graphs leads construction planning graph state. One waykeep costs base heuristics subset states belief state. evaluateeffect sampling cost heuristics. single graph sample singlestate multiple graphs LU G sample percent states. evaluatestate sampling show appropriate, find dependent computeheuristics states.Standardized Evaluation Heuristics: issue evaluating effectiveness heuristic techniques many architectural differences planners use heuristics. quite hardpinpoint global effect assumptions underlying heuristics performance.example, GPT outperformed MBPbut questionable whether credit efficiency attributable differences heuristics, differences search engines (MBP usesBDD-based search). interest paper systematically evaluate spectrum approachescomputing heuristics belief space planning. Thus implemented heuristics similarGPT MBP use compare new heuristics developed around notionoverlap (multiple world positive interaction independence). implemented heuristicswithin two planners, Conformant-AltAlt planner (CAltAlt) Partially-Observable NonDeterministic planner (P D). P handle search non-deterministic actions,bulk paper discuss deterministic actions. general action formulation,pointed Smith Weld (1998), translated initial state uncertainty. Alternatively,Section 8.2 discuss direct approach reason non-deterministic actionsheuristics.External Evaluation: Although main interest paper evaluate relative advantages spectrum belief space planning heuristics normalized setting, also compareperformance best heuristics work current state art conformantconditional planners. empirical studies show planning graph based heuristics provide effective guidance compared cardinality heuristics well reachability heuristic used GPTCFF, planners competitive BDD-based planners MBP YKA,GraphPlan-based ones CGP SGP. also notice planners gain scalabilityheuristics retain reasonable quality solutions, unlike several planners compareagainst.rest paper organized follows. first present CAltAlt P plannersdescribing state action representations well search algorithms. understandsearch guidance planners, discuss appropriate properties heuristic measuresbelief space planning. follow description three planning graph substrates usedcompute heuristics. carry empirical evaluation next three sections, describing37fiB RYCE , K AMBHAMPATI , & MITHtest setup, presenting standardized internal comparison, finally comparing severalstate art planners. end related research, discussion, prospects future work,various concluding remarks.2. Belief Space Plannersplanning formulation uses regression search find strong conformant plans progressionsearch find strong conformant conditional plans. strong plan guarantees finitenumber actions executed many possible initial states, resulting states goalstates. Conformant plans special case plan conditional plan branches,classical planning. Conditional plans general case plans structured graphinclude conditional actions (i.e. actions causative observational effects).presentation, restrict conditional plans DAGs, conceptual reasoncannot general graphs. plan quality metric maximum plan path length.formulate search space belief states, technique described Bonet Geffner(2000). planning problem P defined tuple D, BSI , BSG , domaindescription, BSI initial belief state, BSG goal belief state (consisting statessatisfying goal). domain tuple F, A, F set fluents setactions.Logical Formula Representation: make extensive use logical formulas F representbelief states, actions, LU G labels, first explain conventions. refer everyfluent F either positive literal negative literal, either denoted l.discussing literal l, opposite polarity literal denoted l. Thus l = at(location1),l = at(location1). reserve symbols denote logical false true, respectively.Throughout paper define conjunction empty set equivalent , disjunctionempty set .Logical formulas propositional sentences comprised literals, disjunction, conjunction,negation. refer set models formula f M(f ). consider disjunctive normal), conjunctive normal form f , (f ). DNF seenform logical formula f , (fdisjunction constituents conjunction literals. Alternatively CNFseen conjunction clauses C disjunction literals.1 find usefulthink DNF CNF represented sets disjunctive set constituents conjunctive setclauses. also refer complete representation (f ) formula f DNF everyconstituent case state model f .Belief State Representation: world state, S, represented complete interpretationfluents. also refer states possible worlds. belief state BS set states symbolically represented propositional formula F . state set states representedbelief state BS M(BS), equivalently |= BS.pedagogical purposes, use bomb toilet clogging sensing problem,BTCS, running example paper.2 BTCS problem includes two packages, one) readily related. Specifically constituent contains k |F | literals,1. easy see M(f ) (fcorresponding 2|F |k models.2. aware negative publicity associated B&T problems fact handle interestingproblems difficult reachability uncertainty (e.g. Logistics Rovers), simplify discussionchoose small problem.38fiP LANNING G RAPH H EURISTICS B ELIEF PACE EARCHcontains bomb, also toilet dunk packages defuse potentialbombs. goal disarm bomb allowable actions dunking packagetoilet (DunkP1, DunkP2), flushing toilet becomes clogged dunking (Flush),using metal-detector sense package contains bomb (DetectMetal). fluents encodingproblem denote bomb armed (arm) not, bomb package (inP1, inP2)not, toilet clogged (clog) not. also consider conformant variation BTCS,called BTC, DetectMetal action.belief state representation BTCS initial condition, clausal representation is:(BSI ) = arm clog (inP1 inP2) (inP1 inP2),constituent representation is:(BS) = (arm clog inP1 inP2) (arm clog inP1 inP2).goal BTCS clausal constituent representation:(BSG ) = (BSG ) = arm.However, goal complete representation:(BSG ) = (arm clog inP1 inP2) (arm clog inP1 inP2)(arm clog inP1 inP2) (arm clog inP1 inP2)(arm clog inP1 inP2) (arm clog inP1 inP2)(arm clog inP1 inP2) (arm clog inP1 inP2).last four states (disjuncts) complete representation unreachable, consistentgoal description.Action Representation: represent actions causative observational effects.actions described tuple e (a), (a), (a) e (a) execution precondition,(a) set causative effects, (a) set observations. execution precondition,e (a), conjunction literals must hold action executable. action executable, apply set causative effects find successor states apply observationspartition successor states observational classes.causative effect j (a) (a) conditional effect form j (a) = j (a),antecedent j (a) consequent j (a) conjunction literals. handle disjunctione (a) j (a) replicating respective action effect different conditions,loss generality assume conjunctive preconditions. However, cannot split disjunctioneffects. Disjunction effect amounts representing set non-deterministic outcomes. Henceallow disjunction effects thereby restricting deterministic effects. convention0 (a) unconditional effect, equivalent conditional effect 0 (a) = .way obtain observations execute action observations. observationformula oj (a) (a) possible sensor reading. example, action observestruth values two fluents p q defines (a) = {p q, p q, p q, p q}. differsslightly conventional description observations conditional planning literature.works (e.g., Rintanen, 2003b) describe observation list observable formulas,define possible sensor readings boolean combinations formulas. directly definepossible sensor readings, illustrated example. note convention helpfulproblems boolean combinations observable formulas never sensor readings.causative sensory actions example BTCS problem are:39fiB RYCE , K AMBHAMPATI , & MITHDunkP1: e = clog, = {0 = clog, 1 = inP1 = arm}, = {},DunkP2: e = clog, = {0 = clog, 1 = inP2 = arm}, = {},Flush: e = , = {0 = clog}, = {},DetectMetal: e = , = , = {o0 = inP1, o1 = inP1}.2.1 Regressionperform regression CAltAlt planner find conformant plans starting goalbelief state regressing non-deterministically relevant actions. action (withoutobservations) relevant regressing belief state (i) unconditional effect consistentevery state belief state (ii) least one effect consequent contains literal presentconstituent belief state. first part relevance requires every state successorbelief state actually reachable predecessor belief state second ensuresaction helps support successor.Following Pednault (1988), regressing belief state BS action a, conditionaleffects, involves finding execution, causation, preservation formulas. define regressionterms clausal representation, generalized arbitrary formulas. regressionbelief state conjunction regression clauses (BS). Formally, result BSregressing belief state BS action defined as:3BS = Regress(BS, a) = (a)((a, l) IP (a, l))C(BS) lCExecution formula ((a)) execution precondition e (a). must hold BSapplicable.Causation formula ((a, l)) literal l w.r.t effects (a) action definedweakest formula must hold state l holds BS. intuitive meaningl already held BS , antecedent (a) must held BS make l hold BS.Formally (a, l) defined as:(a)(a, l) = li:li (a)Preservation formula (IP (a, l)) literal l w.r.t. effects (a) action definedformula must true l violated effect (a). intuitivemeaning antecedent every effect inconsistent l could held BS .Formally IP (a, l) defined as:IP (a, l) =(a)i:li (a)Regression also formalized MBP planner (Cimatti & Roveri, 2000) symbolicpre-image computation BDDs (Bryant, 1986). formulation syntactically different,approaches compute result.3. Note BS may clausal form regression (especially action multiple conditional effects).40fiP LANNING G RAPH H EURISTICS B ELIEF PACE EARCHBSGFlushBS1FlushBS4FlushBS7DunkP1BS8DunkP1BS2DunkP1BS5DunkP2BS3DunkP2BS6DunkP2BS9Figure 1: Illustration regression search path conformant plan BT C problem.2.2 CAltAltCAltAlt planner uses regression operator generate children A* search. Regressionterminates search node expansion generates belief state BS logically entailedinitial belief state BSI . plan sequence actions regressed BSG obtainbelief state entailed BSI .example, BTC problem, Figure 1, have:BS2 =Regress(BSG , DunkP1) = clog (arm inP1).first clause execution formula second clause causation formulaconditional effect DunkP1 arm.Regressing BS2 Flush gives:BS4 = Regress(BS2 , Flush) = (arm inP1).BS4 , execution precondition Flush , causation formula clog = ,(arm inP1) comes persistence causation formula.Finally, regressing BS4 DunkP2 gives:BS9 = Regress(BS4 , DunkP2) = clog (arm inP1 inP2).terminate BS9 BSI |= BS9 . plan DunkP2, Flush, DunkP1.2.3 Progressionprogression handle causative effects observations, general, progressingaction belief state BS generates set successor belief states B. set beliefstates B empty action applicable BS (BS|= e (a)).Progression belief state BS action best understood union resultapplying model BS fact implement BDD images, MBP planner41fiB RYCE , K AMBHAMPATI , & MITH(Bertoli et al., 2001b). Since compute progression two steps, first finding causative successor, second partitioning successor observational classes, explain steps separately.causative successor BS found progressing belief state BS causative effectsaction a. action applicable, causative successor disjunction causativeprogression (Progressc ) state BS a:: BS|= e (a)BS = Progressc (BS, a) =SM(BS) Progressc (S, a) : otherwiseprogression action state conjunction every literal persists (noapplicable effect consequent contains negation literal) every literal giveneffect (an applicable effect consequent contains literal).= Progressc (S, a) =l:lSj S|=j (a)lj (a)ll:jS|=j (a)lj (a)lApplying observations action results set successors B. set found (inProgresss ) individually taking conjunction sensor reading oj (a) causativesuccessor BS . Applying observations (a) belief state BS results set B beliefstates, defined as:: BS ={BS }: (a) =B = Progresss (BS , a) =j{BS |BS = (a) BS } : otherwisefull progression computed as:B = Progress(BS, a) = Progresss (Progressc (BS, a), a).2.4 Puse top AO* search (Nilsson, 1980), P planner generate conformantconditional plans. search graph, nodes belief states hyper-edges actions.need AO* applying action observations belief state divides belief stateobservational classes. use hyper-edges actions actions observationsseveral possible successor belief states, must included solution.AO* search consists two repeated steps: expand current partial solution,revise current partial solution. Search ends every leaf node current solutionbelief state satisfies goal better solution exists (given heuristic function). Expansion involves following current solution unexpanded leaf node generating children.Revision dynamic programming update node current solution selects besthyper-edge (action). update assigns action minimum cost start best solutionrooted given node. cost node cost best action plus average costchildren (the nodes connected best action). expanding leaf node, childrenapplied actions given heuristic value indicate estimated cost.42fiP LANNING G RAPH H EURISTICS B ELIEF PACE EARCHmain differences formulation AO* Nilsson (1980)allow cycles search graph, update costs nodes average rathersummation, use weighted estimate future cost. first difference ensure plansstrong (there finite number steps goal), second guide search toward planslower average path cost, third bias search trust heuristic function.define plan quality metric (maximum plan path length) differently metric searchminimizes two reasons. First, easier compare competing plannersmeasure plan quality metric. Second, search tends efficient using averageinstead maximum cost actions children. using average instead maximum,measured cost plan lower means likely search shallower search graphprove solution best solution.Conformant planning, using actions without observations, special case AO* search,similar A* search. hyper-edges represent actions singletons, leadingsingle successor belief state. Consider BTC problem (BTCS without DetectMetal action)future cost (heuristic value) set zero every search node. show search graphFigure 2 conformant example well conditional example, described shortly.expand initial belief state progressing applicable actions. get:B1 = {BS10 } = Progress(BSI , DunkP1)= {(inP1 inP2 clog arm) (inP1 inP2 clog arm)}B3 = {BS20 } = Progress(BSI , DunkP2)= {(inP1 inP2 clog arm) (inP1 inP2 clog arm)}.Since clog already holds every state initial belief state, applying Flush BSI leadsBSI creating cycle. Hence, hyper-edge Flush added search graph BSI .assign cost zero BS10 BS20 , update internal nodes best solution, addDunkP1 best solution rooted BSI (whose cost one).expand leaf nodes best solution, single node BS10 , applicable actions.applicable action Flush, get:B3 = {BS30 } = Progress(BS10 , Flush)= {(inP1 inP2 clog arm) (inP1 inP2 clog arm)}.assign cost zero BS30 update best solution. choose Flush best actionBS10 (whose cost one), choose DunkP2 best action BSI (whose costone). DunkP2 chosen BSI successor BS20 cost zero, opposedBS10 cost one.Expanding leaf node BS20 applicable action, Flush, get:B4 = {BS40 } = Progress(BS20 , Flush)= {(inP1 inP2 clog arm) (inP1 inP2 clog arm)}.update BS40 (to cost zero) BS20 (to cost one), choose Flush bestaction BS20 . root node BSI two children, cost one, arbitrarily chooseDunkP1 best action.expand BS30 relevant actions get BSG DunkP2 action. DunkP1 createscycle back BS10 added search graph. solution leafnodes terminal. required terminal belief state contains subset43fiB RYCE , K AMBHAMPATI , & MITHBSIDunkP1DetectMetalDunkP2:inP1inP1B1B2BS10B5BS50BS20FlushFlushB3B4BS30DunkP2DunkP1DunkP2DunkP1B6BS40DunkP2BS51B7BS60BS70DunkP1BSGFigure 2: Illustration progression search conformant plan (bold dashed edges) conditional plan (bold solid edges) BTCS problem.states BSG , case terminal belief state contains exactly states BSG . costsolution three because, revision, BS30 cost one, sets BS10 costtwo. However, means BSI cost three best action DunkP1. Instead,revision sets best action BSI DunkP2 cost currently two.expand BS40 DunkP1 find successor BSG . DunkP2 creates cycleback BS20 added search graph. second valid solutioncontains unexpanded leaf nodes. Revision sets cost BS40 one, BS20 two,BSI three. Since solutions starting BSI equal cost (meaning cheapersolutions), terminate plan DunkP2, Flush, DunkP1, shown bold dashed linesFigure 2.example search conditional plan P D, consider BTCS example whosesearch graph also shown Figure 2. Expanding initial belief state, get:B1 = {BS10 } = Progress(BSI , DunkP1),B2 = {BS20 } = Progress(BSI , DunkP2),B5 = {BS50 , BS51 } = Progress(BSI ,DetectMetal)= {inP1 inP2 clog arm, inP1 inP2 clog arm}.leaf nodes assigned cost zero, DunkP1 chosen arbitrarily bestsolution rooted BSI cost solution identical. cost includinghyper-edge average cost children plus cost, cost using DetectMetal (0+0)/2+ 1 = 1. Thus, root BSI cost one.44fiP LANNING G RAPH H EURISTICS B ELIEF PACE EARCHconformant problem expand BS10 , giving child cost zero BS10 costone. changes best solution BSI use DunkP2, expand BS20 , giving childcost zero cost one. choose DetectMetal start best solution BSIgives BSI cost one, using either Dunk action would give BSI cost two.expand first child DetectMetal, BS50 , DunkP1 get:{inP1 inP2 clog arm},goal state, DunkP2 get:B6 = {BS60 } = Progress(BS50 ,DunkP2) = {inP1 inP2 clog arm}.expand second child, BS51 , DunkP2 get:{inP1 inP2 clog arm},also goal state DunkP1 get:B7 = {BS70 } = Progress(BS51 ,DunkP1) = {inP1 inP2 clog arm}.none new belief states equivalent BSG , two entail BSG ,treat terminal connecting hyper-edges actions BSG . chooseDunkP1 DunkP2 best actions BS50 BS51 respectively set cost nodeone. turn sets cost using DetectMetal BSI (1+1)/2 + 1 = 2. terminateplan cost equal possible plans starting BSI leaf nodessatisfy goal. plan shown bold solid lines Figure 2.3. Belief State DistanceCAltAlt P planners need guide search node expansion heuristicsestimate plan distance dist(BS, BS ) two belief states BS BS . convention, assume BS precedes BS (i.e., progression BS search node BS goalbelief state, regression BS initial belief state BS search node). simplicity,limit discussion progression planning. Since strong plan (executed BS) ensuresevery state M(BS) transition state M(BS ), define plan distanceBS BS number actions needed transition every state M(BS)state M(BS ). Naturally, strong plan, actions used transition state S1 M(BS)may affect transition another state S2 M(BS). usually degree positivenegative interaction S1 S2 ignored captured estimating plan distance.4 following explore perform estimates using several intuitionsclassical planning state distance heuristics.start example search scenario Figure 3. three belief states BS1 (containing states S11 S12 ), BS2 (containing state S21 ), BS3 (containing states S31 S32 ).goal belief state BS3 , two progression search nodes BS1 BS2 . wantexpand search node smallest distance BS3 estimating dist(BS1 , BS3 ) denotedbold, dashed line dist(BS2 , BS3 ) denoted bold, solid line. assumeestimates state distance measures dist(S, ) denoted light dashedsolid lines numbers. state distances represented numbers action sequences.example, use following action sequences illustration:4. Interaction states captures notion actions performed transition one state goal may interfere(negatively interact) aid (positively interact) transitioning states goals states.45fiB RYCE , K AMBHAMPATI , & MITHBS1S11BS3145S12S3137BS2S328S2110Figure 3: Conformant Plan Distance Estimation Belief Spacedist(S11 , S32 ) : ({a1 , a2 }, {a5 }, {a6 , a7 }),dist(S12 , S31 ) : ({a1 , a7 }, {a3 }),dist(S21 , S31 ) : ({a3 , a6 }, {a9 , a2 , a1 }, {a0 , a8 }, {a5 }).sequence may several actions step. instance, dist(S21 , S31 ) a3a6 first step, total eight actions sequence meaning distanceeight. Notice example includes several state distance estimates, foundclassical planning techniques. many ways use similar ideas estimate beliefstate distance addressed issue belief states containing several states.Selecting States Distance Estimation: exists considerable body literature estimating plan distance states classical planning (Bonet & Geffner, 1999; Nguyen,Kambhampati, & Nigenda, 2002; Hoffmann & Nebel, 2001), would like apply estimate plan distance two belief states, say BS1 BS3 . identify four possibleoptions using state distance estimates compute distance belief states BS1BS3 :Sample State Pair: sample single state BS1 single state BS3 ,whose plan distance used belief state distance. example, might sample S12BS1 S31 BS3 , define dist(BS1 , BS3 ) = dist(S12 , S31 ).Aggregate States: form aggregate states BS1 BS3 measure plandistance. aggregate state union literals needed express belief state formula,46fiP LANNING G RAPH H EURISTICS B ELIEF PACE EARCHdefine as:S(BS) =ffll:lS,S(BS)Since possible express belief state formula every literal (e.g., using (q q) pexpress belief state p true), assume reasonably succinct representation,ROBDD (Bryant, 1986). quite possible aggregate states inconsistent, many classical planning techniques (such planning graphs) require consistent states. example, aggregate states would compute belief state distancedist(BS1 , BS3 ) = dist(S(BS1 ), S(BS3 )).Choose Subset States: choose set states (e.g., random sampling)BS1 set states BS3 , compute state distances pairs statessets. Upon computing state distances, aggregate state distances (asdescribe shortly). example, might sample S11 S12 BS1 S31BS3 , compute dist(S11 , S31 ) dist(S12 , S31 ), aggregate state distancesdefine dist(BS1 , BS3 ).Use States: use states BS1 BS3 , and, similar sampling subsetstates (above), compute distances state pairs aggregate distances.former two options computing belief state distance reasonably straightforward, givenexisting work classical planning. latter two options compute multiple state distances.multiple state distances two details require consideration order obtainbelief state distance measure. following treat belief states contain statesappropriately replaced subset chosen states.first issue state distances may needed. Since state BS1needs reach state BS3 , consider distance state BS1 stateBS3 . However, dont necessarily need distance every state BS1 every stateBS3 . explore assumptions state distances need computed Section 3.1.second issue, arises computing state distances, need aggregatestate distances belief state distance. notice popular state distance estimatesused classical planning typically measure aggregate costs state features (literals). Sinceplanning belief space, wish estimate belief state distance aggregate costbelief state features (states). Section 3.2, examine several choices aggregating statedistances discuss captures different types state interaction. Section 3.3,conclude summary choices make order compute belief state distances.3.1 State Distance Assumptionschoose compute multiple state distances two belief states BS BS ,whether considering states sampling subsets, state distances important.given state BS need know distance every state BSstate BS need transition one state BS . two assumptions makestates reached BS help us define two different belief state distance measuresterms aggregate state distances:47fiB RYCE , K AMBHAMPATI , & MITHoptimistically assume earlier states M(BS) reach closestlater states M(BS ). assumption compute distance as:dist(BS, BS ) = ffSM(BS)minM(BS )dist(S, ).assume earlier states M(BS) reach later stateM(BS ), aggregate distance minimum. assumption compute distance as:dist(BS, BS ) =minM(BS )ffSM(BS) dist(S, ),ff represents aggregation technique (several discuss shortly).Throughout rest paper use first definition belief state distancerelatively robust easy compute. drawback treats earlier statesindependent fashion, flexible allowing earlier states transition different later states.second definition measures dependencies earlier states, restricts reachlater state. second may sometimes accurate, misinformed casesearlier states cannot reach later state (i.e., measure would infinite).pursue second method may return distance measures infinitefact finite.see Section 4, discuss computing measures planning graphs,implicitly find state BS closest state BS , enumeratestates minimization term first belief state distance (above). Part reason) rather actual states.compute distance terms constituents (BSAlso, consider constituents BS , discuss sampling belief states include distance computation sample BS. also avoid explicit aggregationff using LU G, describe several choices ff understand implicit assumptions madeheuristics computed LU G.3.2 State Distance Aggregationaggregation function ff plays important role measure distance beliefstates. compute one state distance measure, either exhaustively samplingsubset (as previously mentioned), must combine measures means, denoted ff.range options taking state distances aggregating belief statedistance. discuss several assumptions associated potential measures:Positive Interaction States: Positive interaction assumes difficult state BSrequires actions help transition states BS state BS .example, means assume actions used transition S11 S32 help ustransition S12 S31 (assuming state BS1 transitions closest state BS3 ).Inspecting action sequences, see positively interact need actions a1a7 . need know action sequences assume positive interactiondefine aggregation ff maximization numerical state distances:dist(BS, BS ) =maxminSM(BS) M(BS )dist(S, ).48fiP LANNING G RAPH H EURISTICS B ELIEF PACE EARCHbelief state distances dist(BS1 , BS3 ) = max(min(14, 5), min(3, 7)) = 5dist(BS2 , BS3 ) = max(min(8, 10)) = 8. case prefer BS1 BS2 .state distance admissible sample belief states, assuming positiveinteraction also admissible.Independence States: Independence assumes state BS requires actionsdifferent states BS order reach state BS . Previously, foundpositive interaction action sequences transition S11 S32 S12 S31shared actions a1 a7 . also independence sequencesfirst contains a2 , a5 , a6 , second contains a3 . Again, needknow action sequences assume independence define aggregation ffsummation numerical state distances:fimin dist(S, ).dist(BS, BS ) =SM(BS) M(BS )example, dist(BS1 , BS3 ) = min(14, 5) + min(3, 7) = 8, dist(BS2 , BS3 ) =min(8, 10) = 8. case preference BS1 BS2 .notice using cardinality belief state |M(BS)| measure dist(BS, BS )special case assuming state independence, S, dist(S, ) = 1. use cardinality measure distance example, dist(BS1 , BS3 ) = |M(BS1 )| = 2,dist(BS2 , BS3 ) = |M(BS2 )| = 1. cardinality prefer BS2 BS1better knowledge BS2 .Overlap States: Overlap assumes positive interaction independenceactions used states BS reach state BS . intuitionactions often used multiple states BS simultaneously countactions once. example, computed dist(BS1 , BS3 ) assuming positiveinteraction, noticed action sequences dist(S11 , S32 ) dist(S12 , S31 )used a1 a7 . aggregate sequences would like count a1 a7potentially overlap. However, truly combining action sequencesmaximal overlap plan merging problem (Kambhampati, Ihrig, & Srivastava, 1996),difficult planning. Since ultimate intent compute heuristics,take simple approach merging action sequences. introduce plan mergingoperator ff picks step align sequences unions alignedsteps. use size resulting action sequence measure belief state distance:dist(BS, BS ) = SM(BS)minM(BS )dist(S, ).Depending type search, define differently. assume sequences usedprogression search start time used regression end time.Thus, progression sequences aligned first step union steps,regression sequences aligned last step union.example, progression dist(S11 , S32 ) dist(S12 , S31 ) = ({a1 , a2 }, {a5 }, {a6 , a7 })({a1 , a7 }, {a3 }) = ({a1 , a2 , a7 }, {a5 , a3 }, {a6 , a7 }) align sequencesfirst steps, union step. Notice resulting sequence seven actions, giving49fiB RYCE , K AMBHAMPATI , & MITHdist(BS1 , BS3 ) = 7, whereas defining ff maximum gave distance five summation gave distance eight. Compared overlap, positive interaction tendsestimate distance, independence tends estimate distance. see empirical evaluation (in Section 6.5), accounting overlap provides accuratedistance measures many conformant planning domains.Negative Interaction States: Negative interaction states appear exampletransitioning state S11 state S32 makes difficult (or even impossible) transitionstate S12 state S31 . could happen performing action a5 S11 conflicts actiona3 S12 . say BS1 cannot reach BS3 possible action sequences startS11 S12 , respectively, end M(BS3 ) negatively interact.two ways negative interactions play role belief state distances. Negative interactions allow us prove impossible belief state BS reach belief stateBS , meaning dist(BS, BS ) = , potentially increase distance finiteamount. use first, extreme, notion negative interaction computingcross-world mutexes (Smith & Weld, 1998) prune belief states search.cannot prune belief state, use one aforementioned techniques aggregatestate distances. such, provide concrete definition ff measure negativeinteraction.explore ways adjust distance measure negative interactions,mention possibilities. Like work classical planning (Nguyen et al., 2002),penalize distance measure dist(BS1 , BS3 ) reflect additional cost associated serializing conflicting actions. Additionally conditional planning, conflicting actionsconditioned observations execute plan branch. distancemeasure uses observations would reflect added cost obtaining observations,well change cost associated introducing plan branches (e.g., measuring averagebranch cost).techniques belief state distance estimation terms state distances providebasis use multiple planning graphs. show empirical evaluationmeasures affect planner performance differently across standard conformant conditionalplanning domains. quite costly compute several state distance measures, understanding aggregate state distances sets foundation techniques developLU G. already mentioned, LU G conveniently allows us implicitly aggregate statedistances directly measure belief state distance.3.3 Summary Methods Distance EstimationSince explore several methods computing belief state distances planning graphs, provide summary choices must consider, listed Table 1. column headedchoice, containing possible options below. order columns reflects orderconsider options.section covered first two columns relate selecting states beliefstates distance computation, well aggregating multiple state distances belief statedistance. test options choices empirical evaluation.50fiP LANNING G RAPH H EURISTICS B ELIEF PACE EARCHStateSelectionSingleAggregateSubsetState DistanceAggregation+ InteractionIndependenceOverlap- InteractionPlanningGraphSGMGLU GMutexTypeNoneStaticDynamicInducedMutexWorldsIntersectCrossHeuristicMaxSumLevelRelaxed PlanTable 1: Features belief state distance estimation.next section also expand upon aggregate distance measures welldiscuss remaining columns Table 1. present type planning graph: singleplanning graph (SG), multiple planning graphs (M G), labelled uncertainty graph (LU G).Within planning graph describe several types mutex, including static, dynamic,induced mutexes. Additionally, type mutex computed respect differentpossible worlds means mutex involves planning graph elements (e.g., actions)exist world (i.e., mutexes computed within planning graph singlestate), across worlds (i.e., mutexes computed planning graphs different states)two methods (denoted Intersect Cross). Finally, compute many different heuristicsplanning graphs measure state distances max, sum, level, relaxed plan. focusdiscussion planning graphs, same-world mutexes, relaxed plan heuristics nextsection. Cross-world mutexes heuristics described appendices.4. Heuristicssection discusses use planning graph heuristics measure belief state distances.cover several types planning graphs extent used computevarious heuristics. begin brief background planning graphs.Planning Graphs: Planning graphs serve basis belief state distance estimation. Planning graphs initially introduced GraphPlan (Blum & Furst, 1995) representing optimistic, compressed version state space progression tree. compression lies unioningliterals every state subsequent steps initial state. optimism relates underestimating number steps takes support sets literals (by tracking subsetinfeasible tuples literals). GraphPlan searches compressed progression (or planning graph)achieves goal literals level two goal literals marked infeasible. searchtries find actions support top level goal literals, find actions support chosenactions reaching first graph level. basic idea behind using planning graphssearch heuristics find first level planning graph literal stateappears; index level lower bound number actions needed achievestate literal. also techniques estimating number actions requiredachieve sets literals. planning graphs serve way estimate reachability state literals discriminate goodness different search states. work generalizesliteral estimations belief space search considering GraphPlan CGP style planninggraphs plus new generalization planning graphs, called LU G.Planners CGP (Smith & Weld, 1998) SGP (Weld et al., 1998) adapt GraphPlanidea compressing search space planning graph using multiple planning graphs, one51fiB RYCE , K AMBHAMPATI , & MITHOverlapn-distanceshMGRPUhLUGRPState Distance AggregationCFFIndependencePositiveInteractionNoneh cardMBPKACMBPYKAhMGs-RPGPThMGm-RPh0NG1hSGRPUhSGRPSGMGLUGPlanning Graph TypeFigure 4: Taxonomy heuristics respect planning graph type state distance aggregation. Blank entries indicate combination meaningless possible.possible world initial belief state. CGP SGP search planning graphs,similar GraphPlan, find conformant conditional plans. work paper seeksapply idea extracting search heuristics planning graphs, previously used state spacesearch (Nguyen et al., 2002; Hoffmann & Nebel, 2001; Bonet & Geffner, 1999) belief spacesearch.Planning Graphs Belief Space: section proceeds describing four classes heuristicsestimate belief state distance N G, SG, G, LU G. N G heuristics techniques existingliterature based planning graphs, SG heuristics techniques based singleclassical planning graph, G heuristics techniques based multiple planning graphs (similarused CGP) LU G heuristics use new labelled planning graph. LU G combinesadvantages SG G reduce representation size maintain informedness. Noteinclude observations planning graph structures SGP (Weld et al.,1998) would, however include feature future work. conditional planning formulation directly uses planning graph heuristics ignoring observations, results showstill gives good performance.Figure 4 present taxonomy distance measures belief space. taxonomy alsoincludes related planners, whose distance measures characterized section.related planners listed N G group, despite fact actually use planning graphs,clearly fall one planning graph categories. figure shows52fiP LANNING G RAPH H EURISTICS B ELIEF PACE EARCHdifferent substrates (horizontal axis) used compute belief state distance aggregatingstate state distances various assumptions (vertical axis). combinationsconsidered make sense impossible. reasons omissionsdiscussed subsequent sections. wealth different heuristics onecompute using planning graphs, concentrate relaxed plans proveneffective classical planning previous studies (Bryce & Kambhampati, 2004).provide additional descriptions heuristics like max, sum, level Appendix A.Example: illustrate computation heuristic, use example derived BTCcalled Courteous BTC (CBTC) courteous package dunker disarm bombleave toilet unclogged, discourteous person left toilet clogged. initialbelief state CBTC clausal representation is:(BSI ) = arm clog (inP1 inP2) (inP1 inP2),goal is:(BSG ) = clog arm.optimal action sequences reach BSG BSI are:Flush, DunkP1, Flush, DunkP2, Flush,Flush, DunkP2, Flush, DunkP1, Flush.Thus optimal heuristic estimate distance BSI BSG , regression,h (BSG ) = 5 either plan five actions.use planning graphs progression regression search. regression searchheuristic estimates cost current belief state w.r.t. initial belief state progressionsearch heuristic estimates cost goal belief state w.r.t. current belief state. Thus,regression search planning graph(s) built (projected) possible worldsinitial belief state, progression search need built search node.introduce notation BSi denote belief state find heuristic measure, BSPdenote belief state used construct initial layer planning graph(s).following subsections describe computing heuristics regression, generalizedprogression changing BSi BSP appropriately.previous section discussed two important issues involved heuristic computation:sampling states include computation using mutexes capture negative interactionsheuristics. directly address issues section, deferring discussionrespective empirical evaluation sections, 6.4 6.2. heuristics computeddecided set states use, whether sampling not. Also, previouslymentioned, consider sampling states belief state BSP implicitlyfind closest states BSi without sampling. explore computing mutexes planninggraphs regression search. use mutexes determine first level planning graphgoal belief state reachable (via level heuristic described Appendix A) extractrelaxed plan starting level. level heuristic level beliefstate reachable, prune regressed belief state.proceed describing various substrates used computing belief space distance estimates. Within describe prospects various types world aggregation. additionheuristics, mention related work relevant areas.53fiB RYCE , K AMBHAMPATI , & MITH4.1 Non Planning Graph-based Heuristics (N G)group many heuristics planners N G group using SG, G,LU G planning graphs. mention group meanusing planning graphs form.Aggregation: Breadth first search uses simple heuristic, h0 heuristic value setzero. mention heuristic gauge effectiveness search substratesrelative improvements gained using heuristics.Positive Interaction Aggregation: GPT planner (Bonet & Geffner, 2000) measures beliefstate distance maximum minimum state state distance states sourcedestination belief states, assuming optimistic reachability mentioned Section 3. GPT measuresstate distances exactly, terms minimum number transitions state space. Takingmaximum state state distance akin assuming positive interaction states currentbelief state.Independence Aggregation: MBP planner (Bertoli et al., 2001b), KACMBP planner (Bertoli& Cimatti, 2002), YKA planner (Rintanen, 2003b), comparable hcard heuristic measurebelief state distance assuming every state state distance one, taking summationstate distances (i.e. counting number states belief state). measure usefulregression goal belief states partially specified contain many states consistentgoal formula many states consistent goal formula reachableinitial belief state. Throughout regression, many unreachable states removedpredecessor belief states inconsistent preconditions regressed action.Thus, belief states reduce size regression cardinality may indicatecloser initial belief state. Cardinality also useful progression belief statesbecome smaller, agent knowledge easier reach goal state.CBTC, hcard (BSG ) = 4 BSG four states consistent complete representation:(BSG ) = (inP1 inP2clog arm) (inP1 inP2 clog arm)(inP1 inP2 clog arm) (inP1 inP2 clog arm).Notice, may uninformed BSG two states (BSG ) reachable,like: (inP1 inP2 clog arm). n packages, would 2n1 unreachablestates represented (BSG ). Counting unreachable states may overestimate distance estimateneed plan them. general, addition problem counting unreachable states, cardinality accurately reflect distance measures. instance, MBP revertsbreadth first search classical planning problems state distance may large smallstill assigns value one.Overlap Aggregation: Rintanen (2004) describes n-Distances generalize belief statedistance measure GPT consider maximum n-tuple state distance. measure involves,n-sized tuple states belief state, finding length actual plan transitionn-tuple destination belief state. maximum n-tuple distance taken distancemeasure.example, consider belief state four states. n equal two, would definesix belief states, one size two subset four states. belief statesfind real plan, take maximum cost plans measure distance original54fiP LANNING G RAPH H EURISTICS B ELIEF PACE EARCHL0A0E0L1A1E1L2inP1inP1inP1inP1inP1inP1inP2inP2inP2inP2inP2inP2DunkP11(DunkP1)0(DunkP1)armDunkP2armarm1(DunkP2)0(DunkP2)clogclogclogFlush0(Flush)clogarmFlush0(Flush)clogFigure 5: Single planning graph CBTC, relaxed plan components bold. Mutexes omitted.four state belief state. n one, computing measure GPT, nequal size belief state directly solving planning problem. costlycompute measure large values n, informed accounts overlapnegative interactions.CFF planner (Hoffmann & Brafman, 2004) uses version relaxed planning graphextract relaxed plans. relaxed plans measure cost supporting set goal literalsstates belief state. addition traditional notion relaxed planning graph ignoresmutexes, CFF also ignores one antecedent literal conditional effects keep relaxedplan reasoning tractable. CFF relaxed plan capture overlap ignores subgoalsmutexes. way CFF ensures goal supported relaxed problem encoderelaxed planning graph satisfiability problem. encoding satisfiable, chosen numberaction assignments distance measure.4.2 Single Graph Heuristics (SG)simplest approach using planning graphs belief space planning heuristics useclassical planning graph. form initial literal layer projected belief state, couldeither sample single state (denoted SG1 ) use aggregate state (denoted SGU ). example,CBTC (see Figure 5) assuming regression search BSP = BSI , initial level L0planning graph SG1 might be:55fiB RYCE , K AMBHAMPATI , & MITHL0 = {arm, clog, inP1, inP2}SGU defined aggregate state S(BSP ):L0 = {arm, clog, inP1, inP2, inP1, inP2}.Since two versions single planning graph identical semantics, aside initialliteral layer, proceed describing SGU graph point differences SG1arise.Graph construction identical classical planning graphs (including mutex propagation)stops two subsequent literal layers identical (level off). use planning graph formalism used IPP (Koehler, Nebel, Hoffmann, & Dimopoulos, 1997) allow explicit representation conditional effects, meaning literal layer Lk , action layer Ak , effectlayer Ek level k. Persistence literal l, denoted lp , represented actione (lp ) = 0 (lp ) = l. literal Lk effect previous effect layer Ek1 containsliteral consequent. action action layer Ak every one execution preconditionliterals Lk . effect effect layer Ek associated action action layer Akevery one antecedent literals Lk . Using conditional effects planning graph avoidsfactoring action conditional effects possibly exponential number non-conditionalactions, adds extra planning graph layer per level. graph built, extractheuristics.Aggregation: Relaxed plans within single planning graph able measure,optimistic assumptions, distance two belief states. relaxed plan representsdistance subset initial layer literals literals constituent beliefstate. SGU , literals initial layer used support may holdsingle state projected belief state, unlike SG1 . classical relaxed plan heuristic hSGRPfinds set (possibly interfering) actions support goal constituent. relaxed plan RPRPRPRPRPRPsubgraph planning graph, form {ARP0 , E0 , L1 , ..., Ab1 , Eb1 , Lb }.layers contains subset vertices corresponding layer planning graph.formally, find relaxed plan support constituent (BS) reachedSGearliest graph (as found hlevel (BSi ) heuristic Appendix A). Briefly, hSGlevel (BSi )returns first level b constituent BSi literals Lb none markedpair-wise mutex. Notice incorporate negative interactions heuristics.start extraction level b, defining LRPliterals constituent used levelbRPheuristic. literal l Lb , select supporting effect (ignoring mutexes) Eb1RP . prefer persistence literals effects supporting literals.form subset Eb1RPsupporting set effects found, create ARPb1 actions effect Eb1 .RPRP addedneeded preconditions actions antecedents chosen effects Ab1 Eb1list literals support LRPb2 . algorithm repeats find needed actionsA0 . relaxed plans value summation number actions action layer.literal persistence, denoted subscript p, treated action planning graph,|. single graph relaxed planrelaxed plan include final computation | ARPjheuristic computedhSGRP (BSi )=b1j=056| ARP|jfiP LANNING G RAPH H EURISTICS B ELIEF PACE EARCHCBTC problem find relaxed plan SGU , shown Figure 5 boldedges nodes. Since arm clog non mutex level two, use persistenceRP use persistence inP1,support clog DunkP1 support arm LRP2 . L1SGFlush clog. Thus, hRP (BSG ) = 2 relaxed plan is:= {inP1p , Flush},ARP0E0RP = {0 (inP1p ), 0 (Flush)},= {inP1, clog},LRP1= {clogp , DunkP1},ARP1E1RP = {0 (clogp ), 1 (DunkP1)},= {arm, clog}.LRP2relaxed plan use DunkP2 DunkP1 support arm. result armsupported worlds (i.e. supported state inP2 holds initialstate). initial literal layer threw away knowledge inP1 inP2 holding different worlds,relaxed plan extraction ignored fact arm needs supported worlds. EvenSG1 graph, see similar behavior reasoning single world.single, unmodified classical planning graph cannot capture support possible worlds henceexplicit aggregation distance measures states. result, mentionaggregating states measure positive interaction, independence, overlap.4.3 Multiple Graph Heuristics (M G)Single graph heuristics usually uninformed projected belief state BSP often corresponds multiple possible states. lack accuracy single graphs ablecapture propagation multiple world support information. Consider CBTC problemprojected belief state BSI using single graph SGU . DunkP1 actionwould say arm clog reached cost two, fact cost infinite(since DunkP2 support arm possible worlds), strong plan.account lack support possible worlds sharpen heuristic estimate, setmultiple planning graphs considered. single graph, previously discussed.multiple graphs similar graphs used CGP (Smith & Weld, 1998), lackgeneral cross-world mutexes. Mutexes computed within graph, i.e. sameworld mutexes computed. construct initial layer L0 graph different stateM(BSP ). multiple graphs, heuristic value belief state computed termsgraphs. Unlike single graphs, compute different world aggregation measuresmultiple planning graphs.get informed heuristic considering states M(BSP ),certain cases costly compute full set planning graphs extract relaxed plans.describe computing full set planning graphs, later evaluate (in Section 6.4)effect computing smaller proportion these. single graph SG1 extreme casecomputing fewer graphs.illustrate use multiple planning graphs, consider example CBTC. build twographs (Figure 6) projected BSP . respective initial literal layers:L10 = {arm, clog, inP1, inP2}L20 = {arm, clog, inP2, inP2}.57fiB RYCE , K AMBHAMPATI , & MITHL0inP1A0E0inP2A1L1inP1E1inP2inP2DunkP111(DunkP1)0(DunkP1)armclog1(DunkP2)armFlush0(Flush)DunkP2clogFlushcloginP1inP1inP2inP2L2inP1armarm0(DunkP2)0(Flush)clogcloginP1DunkP11(DunkP1)0(DunkP1)2inP2armarmclog1(DunkP2)armFlush0(Flush)DunkP2clogFlushclog0(DunkP2)0(Flush)armclogclogFigure 6: Multiple planning graphs CBTC, relaxed plan components bolded. Mutexesomitted.graph first possible world, arm comes DunkP1 level 2.graph second world, arm comes DunkP2 level 2. Thus, multiplegraphs show actions different worlds contribute support literal.single planning graph sufficient aggregate state measures, following consider compute achievement cost belief state multiple graphsaggregating state distances.Positive Interaction Aggregation: Similar GPT (Bonet & Geffner, 2000), use worstGcase world represent cost belief state BSi using hMmRP heuristic. differenceGPT compute heuristic planning graphs, compute plans statespace. heuristic account number actions used given world, assumepositive interaction across possible worlds.GhMmRP heuristic computed finding relaxed plan RP planning graph ,exactly done single graph hSGRP . difference unlike single graph relaxedplan SGU , like SG1 , initial levels planning graphs states, relaxed planreflect support needed world corresponding . Formally:b 1RPG| Aj |hMmRP (BSi ) = maxj=0b level constituent BSG first reachable.58fiP LANNING G RAPH H EURISTICS B ELIEF PACE EARCHNotice computing state distances states BSP BSi .planning graph corresponds state BSP , extract single relaxed plan.need enumerate states BSi find relaxed plan each. instead supportset literals one constituent BSi . constituent estimated minimum distancestate BSi first constituent reached .GCBTC, computing hMmRP (BSG ) (Figure 6) finds:RP 1 =1= {inP1p , Flush},ARP0E0RP1 = {0 (inP1p ), 0 (Flush)},1= {inP1, clog},LRP11= {clogp , DunkP1},ARP1E1RP1 = {0 (clogp ), 1 (DunkP1)},1= {arm, clog}LRP2RP 2 =2= {inP2p , Flush},ARP0E0RP2 = {0 (inP2p ), 0 (Flush)},2= {inP2, clog},LRP12= {clogp , DunkP2},ARP1E1RP2 = {0 (clogp ), 1 (DunkP2)},2= {arm, clog}.LRP2relaxed plan contains two actions taking maximum two relaxed plan valuesGgives hMmRP (BSG ) = 2. aggregation ignores fact must use different Dunk actionspossible world.GIndependence Aggregation: use hMsRP heuristic assume independence amongworlds belief state. extract relaxed plans exactly described previous heuristicsimply use summation rather maximization relaxed plan costs. Formally:1bRPG| Aj |hMsRP (BSi ) =j=0b level constituent BSG first reachable.GMGCBTC, computing hMsRP (BSG ), find relaxed plans hmRP (BSG )heuristic, sum values get 2 + 2 = 4 heuristic. aggregation ignores factuse Flush action possible worlds.State Overlap Aggregation: notice two previous heuristics either takingmaximization accounting actions, taking summation possibly accountingGextra actions. present hMRP U heuristic balance measure positive interactionindependence worlds. Examining relaxed plans computed two previous heuristicsCBTC example, see relaxed plans extracted graph overlap.12Notice, ARPARPcontain Flush action irrespective package bomb0012contains DunkP1, ARPcontains DunkP2showing positive interaction. Also, ARP1159fiB RYCE , K AMBHAMPATI , & MITHshowing independence. take layer-wise union two relaxed plans, wouldget unioned relaxed plan:RPU =U= {inP1p , Flush},ARP0E0RPU = {0 (inP1p ), 0 (inP2p ), 0 (Flush)},ULRP= {inP1, inP2, clog},1U= {clogp , DunkP1, DunkP2},ARP1E1RPU = {0 (clogp ), 1 (DunkP1), 1 (DunkP2)},ULRP= {arm, clog}.2relaxed plans accounts actions possible worldsactions differ. Notice Flush appears layer zero Dunk actionsappear layer one.order get union relaxed plans, extract relaxed plans ,two previous heuristics. computing heuristics regression search, startlast level (and repeat level) taking union sets actions relaxed planlevel another relaxed plan. relaxed plans end-aligned, hence unioning levelsproceeds last layer relaxed plan create last layer RPU relaxed plan,second last layer relaxed plan unioned on. progression search,relaxed plans start-aligned reflect start time, whereas regressionassume end time. summation number actions actionlevel unioned relaxed plan used heuristic value. Formally:GhMRP U (BSi ) =b1U| ARP|jj=0b greatest level b constituent BSG first reachable.CBTC, found RPU , counting number actions gives us heuristic valueG (BS ) = 3.hMGRP U4.4 Labelled Uncertainty Graph Heuristics (LU G)multiple graph technique advantage heuristics aggregate costs multipleworlds, disadvantage computing redundant information different graphs (c.f.GFigure 6) using every graph compute heuristics (c.f hMRP U ). next approach addresseslimitations condensing multiple planning graphs single planning graph, calledlabelled uncertainty graph (LU G). idea implicitly represent multiple planning graphscollapsing graph connectivity one planning graph, use annotations, called labels (),retain information multiple worlds. could construct LU G generatingmultiple graphs taking union, instead define direct construction procedure.start manner similar unioned single planning graph (SGU ) constructing initiallayer literals source belief state. difference LU G preventloss information multiple worlds keeping label literal recordsworlds relevant. discuss, use simple techniques propagate60fiP LANNING G RAPH H EURISTICS B ELIEF PACE EARCHlabels actions effects label subsequent literal layers. Label propagation reliesexpressing labels propositional formulas using standard propositional logic operations.end product single planning graph labels graph elements; labels indicateexplicit multiple graphs (if build them) contain graph element.trading planning graph structure space label storage space. choice BDDsrepresent labels helps lower storage requirements labels. worst-case complexityLU G equivalent G representation. LU Gs complexity savings realizedprojected possible worlds relevant actions completely disjoint; however,often appear practice. space savings comes two ways: (1) redundant representation actions literals avoided, (2) labels facilitate non-redundant representationstored BDDs. nice feature BDD package (Brace, Rudell, & Bryant, 1990) useefficiently represents many individual BDDs shared BDD leverages common substructure. Hence, practice LU G contains information G much lowerconstruction usage costs.section present construction LU G without mutexes, describeintroduce mutexes, finally discuss extract relaxed plans.4.4.1 L ABEL P ROPAGATIONLike single graph multiple graphs, LU G based IP P (Koehler et al., 1997)planning graph. extend single graph capture multiple world causal support, presentmultiple graphs, adding labels elements action A, effect E, literal L layers.denote label literal l level k k (l). build LU G belief state BSP ,illustrate BSP = BSI CBTC example. label formula describing set states (inBSP ) graph element (optimistically) reachable. say literal l reachableset states, described BS, k levels, BS |= k (l). instance, say armreachable two levels L2 contains arm BSI |= 2 (arm), meaning modelsworlds arm holds two levels superset worlds current belief state.intuitive definition LU G planning graph skeleton, represents causal relations,propagate labels indicate specific possible world support. show skeletonCBTC Figure 7. Constructing graph skeleton largely follows traditional planning graphsemantics, label propagation relies simple rules. initial layer literal labelled,indicate worlds BSP holds, conjunction literal BSP .action labelled, indicate worlds execution preconditions co-achieved,conjunction labels execution preconditions. effect labelled, indicateworlds antecedent literals actions execution preconditions co-achieved,conjunction labels antecedent literals label associated action. Finally,literals labelled, indicate worlds given effect, disjunctionlabels effects previous level affect literal. following describe labelpropagation detail work CBTC example.Initial Literal Layer: LU G initial layer consisting every literal non false ()label. initial layer label 0 (l) literal l identical lBSP , representing statesBSP l holds. labels initial layer literals propagated actionseffects label next literal layer, describe shortly. continue propagationlabel literal changes layers, condition referred level off.61fiB RYCE , K AMBHAMPATI , & MITHL0A0E0L1A1E1L2inP1inP1inP1: inP1: inP1: inP1inP2inP2inP2: inP2: inP2DunkP11(DunkP1)0(DunkP1)DunkP2: inP2: arm1(DunkP2)0(DunkP2)armarmarmclogclogclogFlush0(Flush): clogFlush0(Flush): clogGFigure 7: LU G skeleton CBTC, mutexes. relaxed plan hLURP shownbold.LU G CBTC, shown Figure 7 (without labels), using BSP =BSI initial literallayer:L0 = {inP1, inP2, inP2, inP1, clog, arm}0 (inP1) = 0 (inP2) = (arm clog inP1 inP2),0 (inP2) = 0 (inP1) = (arm clog inP1 inP2),0 (clog) = 0 (arm) = BSPNotice inP1 inP2 labels indicating respective initial states hold,clog arm BSP label hold states BSP .Action Layer: previous literal layer Lk computed, construct label actionlayer Ak . Ak contains causative actions action set A, plus literal persistence. actionincluded Ak label false (i.e. k (a)=). label action level k, equivalentextended label execution precondition:k (a) = k (e (a))Above, introduce notation extended labels k (f ) formula f denote worldsBSP reach f level k. say propositional formula f reachable BS62fiP LANNING G RAPH H EURISTICS B ELIEF PACE EARCHk levels BSi |= k (f ). Since labels literals, substitute labelsliterals literals formula get extended label formula. extended labelpropositional formula f level k, defined:k (f f ) = k (f ) k (f ),k (f f ) = k (f ) k (f ),k ((f f )) = k (f f ),k ((f f )) = k (f f ),k () = BSP ,k () =,k (l) = k (l)zeroth action layer CBTC is:A0 = {Flush, inP1p , inP2p , inP2p , inP1p , clogp , armp }0 (Flush) = BSP ,0 (inP1p ) = 0 (inP2p ) = (arm clog inP1 inP2),0 (inP2p ) = 0 (inP1p ) = (arm clog inP1 inP2),0 (clogp ) = 0 (armp ) = BSPliteral persistence label identical label corresponding literalprevious literal layer. Flush action BSP label always applicable.Effect Layer: effect layer Ek depends literal layer Lk action layer Ak . Ekcontains effect j (a) effect non false label (i.e. k (j (a))=).action effect must applicable world, label effect level kconjunction label associated action extended label antecedentk (j (a)) = k (a) k (j (a))zeroth effect layer CBTC is:E0 = {0 (Flush), 0 (inP1p ), 0 (inP2p ), 0 (inP2p ),0 (inP1p ), 0 (clogp ), 0 (armp )}0 (0 (Flush)) = BSP0 (0 (inP1p )) = 0 (0 (inP2p )) = (arm clog inP1 inP2),0 (0 (inP2p )) = 0 (0 (inP1p )) = (arm clog inP1 inP2),0 (0 (clogp )) = 0 (0 (armp )) = BSPAgain, like action layer, unconditional effect literal persistence label identical corresponding literal previous literal layer. unconditional effect Flushlabel identical label Flush.Literal Layer: literal layer Lk depends previous effect layer Ek1 , containsliterals non false labels (i.e. k (l)=). effect j (a) Ek1 contributes labelliteral l effect consequent contains literal l. label literal disjunctionlabels effect previous effect layer gives literal:k1 (j (a))k (l) =j (a):lj (a),j (a)Ek163fiB RYCE , K AMBHAMPATI , & MITHfirst literal layer CBTC is:L1 = {inP1, inP2, inP2, inP1, clog, clog, arm}1 (inP1) = 1 (inP2) = (arm clog inP1 inP2),1 (inP2) = 1 (inP1) = (arm clog inP1 inP2),1 (clog) = 1 (clog) = 1 (arm) = BSPliteral layer identical initial literal layer, except clog goes falselabel (i.e. existing layer) label BSP .continue level one action layer L1 indicate BSG reachableBSP (armL1 ). Action layer one defined:A1 = {DunkP1, DunkP2, Flush, inP1p , inP2p , inP2p , inP1p , clogp , armp , clogp }1 (DunkP1) = 1 (DunkP2) = 1 (Flush) = BSP ,1 (inP1p ) = 1 (inP2p ) = (arm clog inP1 inP2),1 (inP2p ) = 1 (inP1p ) = (arm clog inP1 inP2),1 (clogp ) = 1 (armp ) = 1 (clogp ) = BSPaction layer similar level zero action layer. adds Dunk actionsexecutable. also add persistence clog. Dunk action gets label identicalexecution precondition label.level one effect layer is:E1 = {0 (DunkP1), 0 (DunkP2), 1 (DunkP1), 1 (DunkP2), 0 (Flush), 0 (inP1p ),0 (inP2p ), 0 (inP2p ), 0 (inP1p ), 0 (clogp ), 0 (armp ), 0 (clogp )}1 (0 (DunkP1)) = 1 (0 (DunkP2)) = 1 (0 (Flush)) = BSP1 (1 (DunkP1)) = (arm clog inP1 inP2),1 (1 (DunkP2)) = (arm clog inP1 inP2),1 (0 (inP2p )) = 1 (0 (inP1p )) = (arm clog inP1 inP2),1 (0 (inP1p )) = 1 (0 (inP2p )) = (arm clog inP1 inP2),1 (0 (clogp )) = 1 (0 (armp )) = 1 (0 (clogp )) = BSPconditional effects Dunk actions CBTC (Figure 7) labels indicatepossible worlds give arm antecedents hold possibleworlds. example, conditional effect 1 (DunkP1) label found taking conjunction actions label BSP antecedent label 1 (inP1) obtain (arm clog inP1inP2).Finally, level two literal layer:L2 = {inP1, inP2, inP2, inP1, clog, clog, arm, arm}2 (inP1) = 2 (inP2) = (arm clog inP1 inP2),2 (inP2) = 2 (inP1) = (arm clog inP1 inP2),2 (clog) = 2 (clog) = 2 (arm) = 2 (arm) = BSPlabels literals level 2 CBTC indicate arm reachable BSP label entailed BSP . label arm found taking disjunctionlabels effects give it, namely, (arm clog inP1 inP2) conditional64fiP LANNING G RAPH H EURISTICS B ELIEF PACE EARCHeffect DunkP1 (arm clog inP1 inP2) conditional effect DunkP2,reduces BSP . Construction could stop BSP entails label goalk (armclog)= k (arm) k (clog) = BSP BSP = BSP . However, level occursnext level change labels literals.level occurs level three example, say BS, BS |=BSP , formula f reachable k steps BS |= k (f ). level k exists, freachable BS. level k, f reachable BS, first klower bound number parallel plan steps needed reach f BS. lower boundsimilar classical planning max heuristic (Nguyen et al., 2002). provideinformed heuristic extracting relaxed plan support f respect BS, described shortly.4.4.2 AME -W ORLD L ABELLED UTEXESseveral types mutexes added LU G. start with, concentrateevolve single possible world same-world mutexes effectivewell relatively easy understand. extend mutex propagation usedmultiple graphs mutexes one planning graph. savings computing mutexesLU G instead multiple graphs reduce computation mutex exitsseveral worlds. Appendix B describe handle cross-world mutexes, despite lackeffectiveness experiments conducted. Cross-world mutexes extend LU G computeset mutexes found CGP (Smith & Weld, 1998).Same-world mutexes represented single label, k (x1 , x2 ), two elements(actions, effect, literals). mutex holds elements x1 x2 worlds|= k (x1 , x2 ). elements mutex world, assume label mutexfalse . discuss labelled mutexes discovered propagatedactions, effect relations, literals.using mutexes, refine means formula f reachable setworlds BSP . must ensure every state BSP , exists state f reachable.state f reachable state BSP two literalsmutex world BSP |= k (S).action, effect, literal layers multiple ways pairelements become mutex (e.g. interference competing needs). Thus, mutex label pairdisjunction labelled mutexes found pair means.Action Mutexes: same-world action mutexes level k set labelled pairs actions.pair labelled formula indicates set possible worlds actionsmutex. possible reasons mutex actions interference competing needs.Interference Two actions a, interfere (1) unconditional effect consequent 0 (a)one inconsistent execution precondition e (a ) other, (2) vice versa.additionally interfere (3) unconditional effect consequents 0 (a) 0 (a )inconsistent, (4) execution preconditions e (a) e (a ) inconsistent. mutexexist possible world projections k (a, ) = BSP . Formally, interfere65fiB RYCE , K AMBHAMPATI , & MITHone following holds:(1) 0 (a) e (a ) =(2) e (a) 0 (a ) =(3) 0 (a) 0 (a ) =(4) e (a) e (a ) =Competing Needs Two actions a, competing needs world pair literalsexecution preconditions mutex world. worldsmutex competing needs described by:k (a) k (a )k (l, l )lj (a),l j (a )formula find worlds pair execution preconditions l e (a), le (a ) mutex actions reachable.Effect Mutexes: effect mutexes set labelled pairs effects. pair labelledformula indicates set possible worlds effects mutex. possible reasonsmutex effects associated action mutexes, interference, competing needs, induced effects.Mutex Actions Two effects (a) (a), j (a ) (a ) mutex worldsassociated actions mutex, k (a, ).Interference Like actions, two effects (a), j (a ) interfere (1) consequent (a)one inconsistent antecedent j (a ) other, (2) vice versa. additionally interfere (3) effect consequents (a) j (a ) inconsistent, (4)antecedents (a) j (a ) inconsistent. mutex exist possible world projections, label mutex k (i (a), j (a )) = BSP . Formally, (a) j (a )interfere one following holds:(1) (a) j (a ) =(2) (a) j (a ) =(3) (a) j (a ) =(4) (a) j (a ) =Competing Needs Like actions, two effects competing needs world pairliterals antecedents mutex world. worlds (a) j (a )competing needs mutex are:k (i (a)) k (j (a ))k (l, l )li (a),l j (a )formula find worlds pair execution preconditions l (a), lj (a ) mutex actions reachable.66fiP LANNING G RAPH H EURISTICS B ELIEF PACE EARCHLklk(p)pEkAklk(a)h(a)lk(h(a))lk(p, q)Induced mutex worlds:lk(j(a),h(a))lk(i(a))lk(j(a), h(a))lk(q)qlk(a)j(a)lk(r)rlk(j(a))i(a) induces j(a) in:lk(i(a))lk(j(a))i(a) lk(i(a))Figure 8: Effect (a) induces effect j (a). j (a) mutex h (a ), (a) induced mutexh (a ).Induced induced effect j (a) effect (a) effect actionmay execute time. effect induced another possible worldsreachable. example, conditional effect action always inducesunconditional effect action.Induced mutexes, involving inducing effect (a), come induced effectj (a) mutex another effect h (a ) (see Figure 8). induced mutex(a) effect h (a ) mutex induced effect j (a) (b) inducing effect(a). label mutex conjunction label mutex k (j (a), h (a ))label induced effect j (a). additional discussion methodology behindinduced mutexes refer Smith Weld (1998).Literal Mutexes: literal mutexes set labelled pairs literals. pair labelledformula indicates set possible worlds literals mutex. reasonmutex literals inconsistent support.Inconsistent Support Two literals inconsistent support possible world level ktwo non-mutex effects support literals world. labelliteral mutex level k disjunction worlds inconsistent support.worlds inconsistent support mutex l l are:67fiB RYCE , K AMBHAMPATI , & MITHS:i (a),j (a )Ek1 ,li (a),l j (a ),S|=k1 (i (a),j (a ))meaning formula two literals mutex worldspairs effects support literals mutex S.4.4.3 LU G H EURISTICSheuristics computed LU G capture measures similar G heuristics,exists new opportunity make use labels improve heuristic computation efficiency. singleplanning graph sufficient state aggregation measured, mentionmeasures LU G.Positive Interaction Aggregation: Unlike G heuristics, compute positive interactionbased relaxed plans LU G. G approach measure positive interaction acrossstate belief state compute multiple relaxed plans take maximum value. getmeasure LU G would still need extract multiple relaxed plans, situationtrying avoid using LU G. graph construction overhead may lowered usingLU G, heuristic computation could take long. Hence, compute relaxed plansLU G measure positive interaction alone, compute relaxed plans measureoverlap (which measures positive interaction).Independence Aggregation: Like positive interaction aggregation, need relaxed plan everystate projected belief state find summation costs. Hence, computerelaxed plans assume independence.GState Overlap Aggregation: relaxed plan extracted LU G get hLURP heuristicGGresembles unioned relaxed plan hRP U heuristic. Recall hRP U heuristic extractsrelaxed plan multiple planning graphs (one possible world) unionsset actions chosen level relaxed plans. LU G relaxed plan heuristicsimilar counts actions positive interaction multiple worldsaccounts independent actions used subsets possible worlds. advantageGhLURP find actions single pass one planning graph.trading cost computing multiple relaxed plans cost manipulating LU Glabels determine lines causal support used worlds. relaxed planwant support goal every state BSP , need track statesBSP use paths planning graph. subgoal may several different (and possiblyoverlapping) paths worlds BSP .RPRPRPRPRPRPLU G relaxed plan set layers: {ARP0 , E0 , L1 , ..., Ab1 , Eb1 , Lb }, ArRPRPset actions, Er set effects, Lr+1 set clauses. elements layerslabelled indicate worlds BSP chosen support. relaxed planGextracted level b = hLUlevel (BSi ) (i.e., first level BSi reachable, also describedAppendix A).Please note extracting relaxed plan BSi terms clauses, literals, different SG G versions relaxed plans. Previously found68fiP LANNING G RAPH H EURISTICS B ELIEF PACE EARCHconstituent BSi first reached planning graph commitone constituent. rationale possibly using different constituentsmultiple graphs, condensed version multiple graphs still want ablesupport different constituents BSi different worlds. could also use constituent representation BSi defining layers relaxed plan, choose clausal representationBSi instead know support clause. However constituentsknow need support one (but dont need know one).relaxed plan, shown bold Figure 7, BSI reach BSG CBTC listed follows:= {inP1p , inP2p , Flush},ARP0RP0 (inP1p ) = (arm clog inP1 inP2),RP0 (inP2p ) = (arm clog inP1 inP2),RP0 (Flush) = BSP ,E0RP = {0 (inP1p ), 0 (inP2p ), 0 (Flush)},0RP0 ( (inP1p )) = (arm clog inP1 inP2),0RP0 ( (inP2p )) = (arm clog inP1 inP2),RP0 (0 (Flush)) = BSP ,= {inP1, inP2, clog},LRP1RP1 (inP1) = (arm clog inP1 inP2),RP1 (inP2) = (arm clog inP1 inP2),RP1 (clog) = BSP ,= {DunkP1, DunkP2, clogp },ARP1RP1 (DunkP1) = (arm clog inP1 inP2),RP1 (DunkP2) = (arm clog inP1 inP2),RP1 (clogp ) = BSP ,E1RP = {1 (DunkP1), 1 (DunkP2), 0 (clogp )},1RP1 ( (DunkP1)) = (arm clog inP1 inP2),1RP1 ( (DunkP2)) = (arm clog inP1 inP2),RP1 (0 (clogp )) = BSP ,= {arm, clog},LRP2RP2 (arm) = BSP ,RP2 (clog) = BSPstart forming LRPclauses (BSG ), namely arm clog; label2clauses BSP need supported states belief state. Next,support clause LRPrelevant effects E1 form E1RP . clog use2persistence supports clog worlds described BSP (this example positiveinteraction worlds). arm relevant effects respective 1 Dunk action.choose effects support arm need support arm worlds BSP ,effect gives support one world (this example independence worlds).appropriate label indicatinginsert actions associated chosen effect ARP169fiB RYCE , K AMBHAMPATI , & MITHworlds needed, general fewer worlds reachable (i.e.RP execution preconditionsalways case RPr () |= r ()). Next form L1actions ARPantecedents effects E1RP , clog, inP1, inP2, labelled1worlds action effect needed them. fashion level two, supportliterals level one, using persistence inP1 inP2, Flush clog. stop here,supported clauses level one.general case, extraction starts level b BSi first reachable BSP .RPRPRP contains clausesfirst relaxed plan layers construct ARPb1 , Eb1 , Lb , LbRPC (BSi ), labelled k (C) = BSP .choosing relevant effectslevel r, 1 r b, support clause LRPrRP . effect j (a) relevant reachable worldsEr1 form Er1need support C (i.e. r1 (j (a)) RPr (C)=) consequent gives literal l C.clause, choose enough supporting effects chosen effect worldssuperset worlds need support clause, formally:RPRPj(C)|=((a))CLRPrr1rj(a):lj (a),lC,j (a)Er1think supporting clause set worlds set cover problem effects coversubsets worlds. algorithm cover worlds clause worlds effects variantwell known greedy algorithm set cover (Cormen, Leiserson, & Rivest, 1990). firstchoose relevant persistence effects cover worlds, choose action effects coverRP labelled newnew worlds. effect choose support added Er1RPworlds covered C. clauses Lr covered, form action layer ARPr1RP . actions ARP labelled indicate worldsactions effect Er1r1RP .effects labelled Er1obtain next subgoal layer, LRPr1 , adding literals execution preconditionsRPRP . literal l LRP labelled indicateactions Ar1 antecedents effects Er1r1worlds action effect requires l. support literals LRPr1 fashion.continuesupportliteralseffects,insertactions,insert action effectLRPrRPpreconditions supported literals L1 .Gget relaxed plan, relaxed plan heuristic, hLURP (BSi ), summationnumber actions action layer, formally:GhLURP (BSi )=b1| ARP|i=0GThus CBTC example hLURP (BSG ) = 3. Notice construct LU Gwithout mutexes CBTC reach goal two layers. included mutexesLU G, would reach goal three layers. way use mutexes changerelaxed plan use mutexes influence relaxed plan extraction. Mutexes helpidentify belief state BSi reachable BSP .70fiP LANNING G RAPH H EURISTICS B ELIEF PACE EARCHProblemPDDL Parser(IPC)ActionsBeliefStatesSearch Engine(HSP-r: CAltAlt/LAO*: POND)HeuristicsBDDs(CUDD)Labels(POND only)PlanningGraph(s)(IPP)Figure 9: implementations CAltAlt P rely many existing technologies.search engine guided heuristics extracted planning graphs.5. Empirical Evaluation: Setupsection presents implementation CAltAlt P planners domainsuse experiments. tests run Linux x86 machine 2.66GHz P4 processor1GB RAM timeout 20 minutes. CAltAlt P used heuristic weightfive the, respective, A* AO* searches. compare competing approaches (CGP,SGP, GPT v1.40, MBP v0.91, KACMBP, YKA, CFF) several domains problems.planners domain problem files compared planners foundonline appendix.5.1 Implementationimplementation CAltAlt uses several off-the-shelf planning software packages. Figure 9shows diagram system architecture CAltAlt P D. CAltAlt extendsname AltAlt, relies limited subset implementation. components CAltAltIPC parser PDDL 2.1 (slightly extended allow uncertain initial conditions), HSPr search engine (Bonet & Geffner, 1999), IPP planning graph (Koehler et al., 1997),CUDD BDD package (Brace et al., 1990) implement LU G labels. custom partsimplementation include action representation, belief state representation, regression operator,heuristic calculation.implementation P similar CAltAlt aside search engine,state action representation. P also uses IPP source code planning graphs. Puses modified LAO* (Hansen & Zilberstein, 2001) source code Eric Hansen perform AO*71fiB RYCE , K AMBHAMPATI , & MITHProblemRovers1Rovers2Rovers3Rovers4Rovers5Rovers6Logistics1Logistics2Logistics3Logistics4Logistics5BT(n)BTC(n)CubeCenter(n)Ring(n)InitialStates1234161224248nnn3n3nGoalLiterals11113312123113nFluents66666666711192936586878n+1n+23n4nCausativeActions888888889721770106282396510nn+164ObservationalActions0 {12}0 {12}0 {12}0 {12}0 {12}0 {18}0 {10}0 {20}0 {21}0 {42}0 {63}0 {n}0 {n}00OptimalParallel5 {5}8 {7}10 {?}13 {?}? {?}? {?}6 {6}6 {?}8 {?}8 {?}? {?}1 {1}2n-1 {2}(3n-3)/23n-1OptimalSerial5 {5}8 {7}10 {8}13 {10}20 {?}? {?}9 {7}15 {12}11 {8}18 {?}28 {?}n {n-1}2n-1 {n-1}(9n-3)/23n-1Table 2: Features test domains problems - Number initial states, Number goal literals, Number fluents, Number causative actions, Number Observational Actions,Optimal number parallel plan steps, Optimal number serial plan steps. Data conditional versions domains braces; plan lengths conditional plans maximumconditional branch length.search, CUDD (Brace et al., 1990) represent belief states actions. Even deterministicactions possible obtain cycles actions observations planningbelief space. P constructs search graph directed acyclic graph employing cyclechecking algorithm. adding hyper-edge search graph creates cycle, hyper-edgecannot represent action strong plan hence added graph.5.2 DomainsTable 2 shows relative features different problems used evaluate approach. table shows number initial states, goal literals, fluents, actions, optimalplan lengths. used guide gauge difficulty problems, wellperformance.Conformant Problems addition standard domains used conformant planningsuchBomb-in-the-Toilet, Ring, Cube Center, also developed two new domains LogisticsRovers. chose new domains difficult subgoals, manyplans varying length.Ring domain involves ring n rooms room connected two adjacentrooms. room window open, closed, locked. goal everywindow locked. Initially, state possible could room window couldconfiguration. four actions: move right, move left, close window current72fiP LANNING G RAPH H EURISTICS B ELIEF PACE EARCHroom, lock window current room. Closing window works windowopen, locking window works window closed. good conformant plan involvesmoving one direction closing locking window room.Cube Center domain involves three-dimensional grid (cube) six actionspossible move two directions along dimension. dimension consists n possiblelocations. Moving direction along grid points leaves oneposition. Using phenomena, possible localize dimension repeatedly movingdirection. Initially possible location cube goal reachcenter. good conformant plan involves localizing corner moving center.Rovers domain conformant adaptation analogous domain classical planningtrack International Planning Competition (Long & Fox, 2003). added uncertaintyinitial state uses conditions determine whether image objective visible variousvantage points due weather, availability rock soil samples. goal uploadimage objective rock soil sample data. Thus conformant plan requires visitingpossible vantage points taking picture, plus visiting possible locations soilrock samples draw samples.first five Rovers problems 4 waypoints. Problems one four onefour locations, respectively, desired imaging objective possibly visible (at least onework, dont know one). Problem 5 adds rock soil samples part goalseveral waypoints one obtained (again, dont know waypointright sample). Problem 6 adds two waypoints, keeps goals Problem5 changes possible locations rock soil samples. cases waypointsconnected tree structure, opposed completely connected.Logistics domain conformant adaptation classical Logistics domain trucksairplanes move packages. uncertainty initial locations packages. Thus, actionsrelating movement packages conditional effect predicated packageactually location. conformant version, drivers pilots cannot sense communicate packages actual whereabouts. problems scale adding packages cities.Logistics problems consist one airplane, cities airport, post office,truck. airplane travel airports trucks travel within cities. firstproblem two cities one package could start either post office, goal getpackage second citys airport. second problem adds another packagepossible starting points destination. third problem three citiesone package could post office reach third airport. fourth problemadds second package third problem starting ending locations. fifthproblem three cities three packages, one two three post officesreach different airports.Conditional Problems conditional planning consider domains literature: Bombin-the-Toilet sensing BTS, Bomb-in-the-Toilet clogging sensing BTCS. alsoextend conformant Logistics Rovers include sensory actions.Rovers problem allows rover, particular waypoint, sense availability image, soil, rock data location. locations collectable data expressedone-of constraints, rover deduce locations collectable data failing sensepossibilities.73fiB RYCE , K AMBHAMPATI , & MITHLogistics observations determine package location exists, observationassumed made driver pilot particular location. Since several driverspilot, different agents make observations. information gained agents assumedautomatically communicated others, planner agent knowledge.56. Empirical Evaluation: Inter-Heuristic Comparisonstart comparing heuristic approaches within planners. next section, continuedescribing planners, using best heuristics, compare state artapproaches. section intend validate claims belief space heuristics measureoverlap perform well across several domains. justify using LU G multipleplanning graphs applying mutexes improve heuristics regression pruning beliefstates.compare many techniques within CAltAlt P conformant planning domains, addition test heuristics P conditional domains. performance metrics include total planning time number search nodes expanded. Additionally, discussing mutexes analyze planning graph construction time. proceedshowing heuristics perform CAltAlt various mutex computation schemesLU G affect performance. present P performs differentheuristics conformant conditional domains, explore effect sampling proportionworlds build SG1 , G, LU G graphs, compare heuristic estimates Poptimal plan length gauge heuristic accuracy. finish summary importantconclusions.compute mutexes planning graphs CAltAlt planning graph(s)built search episode mutexes help prune inconsistent belief states encounteredregression search. abstain computing mutexes P progressionbuild new planning graphs search node want keep graph computation time low.exception discussion sampling worlds construct planning graphs,planning graphs constructed deterministically. means single graph unionedsingle graph SGU , G LU G graphs built possible worlds.6.1 CAltAltresults CAltAlt conformant Rovers, Logistics, BT, BTC domains, termstotal time number expanded search nodes, presented Table 3. show numberexpanded nodes gives indication well heuristic guides planner. totaltime captures amount time computing heuristic searching. high total timehigh number search nodes indicates poor heuristic, high total time low numbersearch nodes indicates expensive informed heuristic.discuss Ring Cube Center domains CAltAlt cannot solveeven smallest instances. Due implementation details planner performs poorlydomains actions several conditional effects hence scale. trouble stems5. problem may interesting investigate multi-agent planning scenario, assuming global communication(e.g. radio dispatcher).74fiP LANNING G RAPH H EURISTICS B ELIEF PACE EARCHProblemRovers 123456Logistics 12345BT 21020304050607080BTC 210203040506070h02255/549426/81108/919/24837/1030/315021/19-hcard18687/144268/914/256/10418/201698/305271/4012859/5026131/6048081/7082250/8016/3161/191052/393823/5911285/7926514/9955687/119125594/140hSGRP543/578419/891672/10198/97722/153324/14141094/1918/25158/1016/315679/19-GhMmRP542/58327/820162/1061521/16183/915491/1570882/1420/28988/1033/341805/19-GhMRP U185/529285/92244/113285/151109/969818/1921/2342/102299/209116/3044741/4023/3614/192652/399352/5951859/79-LU G(F X)hRP15164/532969/816668/1031584/131340/918535/1516458/15178068/1912/271/10569/202517/307734/4018389/5037820/6070538/70188603/8018/31470/1951969/39484878/59-Table 3: Results CAltAlt conformant Rovers, Logistics, BT, BTC. data TotalTime / # Expanded Nodes, indicates time (20 minutes) - indicatesattempt.weak implementation bringing general propositional formulas (obtained regressionseveral conditional effects) CNF.describe results left right Table 3, comparing different planning graphstructures relaxed plans computed planning graph. start non-planninggraph heuristics h0 hcard . expected, h0 , breadth-first search, perform welllarge portion problems, shown large number search nodes inability scalesolve larger problems. notice hcard heuristic performance good BTBTC problems (this confirms results originally seen Bertoli, Cimatti, & Roveri, 2001a).However, hcard perform well Rovers Logistics problems sizebelief state, planning, necessarily indicate belief state good plan.Part reason hcard works well domains measures knowledge, plansdomains largely based increasing knowledge. reason hcard performs poorlydomains finding causal support (which measure) importantknowledge domains.75fiB RYCE , K AMBHAMPATI , & MITHNext, single planning graph (SGU ), CAltAlt reasonably well hSGRP heuristicRovers Logistics domains, fails scale well BT BTC domains. RoversLogistics comparatively fewer initial worlds BT BTC problems. Moreoverdeterministic plans, assuming initial state real state, somewhat similar RoversLogistics, mostly independent BT BTC. Therefore, approximating fully observable plan single graph relaxed plan reasonable plans achieving goalworld high positive interaction. However, without high positive interaction heuristicdegrades quickly number initial worlds increases.multiple planning graphs, CAltAlt able perform better Rovers domain, takesquite bit time Logistics, BT, BTC domains. Rovers, capturing distance estimatesindividual worlds aggregating means tends better aggregatingworlds computing single distance estimate (as single graph). Logistics, partreason computing multiple graphs costly computing mutexesplanning graphs. BT BTC, total time increases quickly number planninggraphs, number relaxed plans every search node increase much problems get larger.GMGComparing two multiple graph heuristics6 CAltAlt namely hMmRP hRP U ,Gsee effect choices state distance aggregation. hmRP relaxed plan heuristicaggregates state distances, found planning graph, taking maximum distance.GhMRP U unions relaxed plans graph, counts number actions unionedGrelaxed plan. single graph relaxed plan, hMmRP relaxed plan essentially measuresone state state distance; thus, performance suffers BT BTC domains. However, usingunioned relaxed plan heuristic, capture independence among multiple worldsscale better BT BTC. Despite usefulness unioned relaxed plan, costlycompute scalability limited, turn LU G version measure.LU G(F X)LU G, use hRPheuristic CAltAlt. heuristic uses LU GGfull cross-world mutexes (denoted F X). similar hMRP U heuristic, measuring overlapimportant, improving speed computing heuristic tends improve scalabilityCAltAlt. CAltAlt slower Rovers BTC domains using LU G, noteadded cost computing cross-world mutexes able improvespeed relaxing mutexes, describe shortly.6.2 MutexesMutexes used help determine belief state unreachable. Mutexes improve pruningpower heuristics accounting negative interactions. mutexes used improveheuristics, reasonable compute subset mutexes. would like knowmutexes cost effective number possible mutexes findquite large.use several schemes compute subset mutexes. schemes combine differenttypes mutexes types cross-world checking. mutex types are: computing mutexes(NX), computing static interference mutexes (StX), computing (StX) plus inconsistent support competing needs mutexes dynamic mutexes (DyX), computing (DyX) plus inducedmutexes full mutexes (FX). cross-world checking (see appendix B) reduction schemes are:G6. show hMsRP P D.76fiProblemRovers 123456Logistics 12345BT 21020304050607080BTC 210203040506070LU G(N X)hRP13/1112/5120/904/4113/8704/3845/868/8110/63699/14331/34/24/72/1019/452/2062/1999/30130/6130/40248/14641/50430/30140/60680/55202/701143/135760/800/62/34/93/1921/546/3958/2311/59133/6889/79260/15942/99435/32201/119742/62192/139LU G(StX)hRP19/1119/5116/903/4117/8972/38410/868/8188/78448/14330/13/24/56/1022/448/2059/1981/30132/6170/40255/14760/50440/29891/60693/55372/701253/140716/801/16/34/77/1932/545/3961/2293/59149/6879/79261/16452/99443/32923/119745/61827/139LU G(DyX)hRP15453/89/613431/138/817545/185/1032645/441/14698575/3569/451250/117/916394/622/1517196/1075/15136702/1035/190/13/213/57/10120/453/20514/1999/301534/6432/403730/14711/507645/30127/6015019/55417/7026478/132603/800/15/314/78/19139/553/39543/2288/591564/6829/79-LU G(F X)hRP15077/87/632822/147/816481/187/1031293/291/141242/98/918114/421/1516085/373/15176995/1073/190/12/213/58/10120/449/20509/2008/301517/6217/403626/14763/507656/30164/6014636/55902/7026368/162235/804/14/31388/82/1951412/557/39482578/2300/59-LU G(DyXSX)hRP15983/87/610318/139/810643/185/1014988/291/1461373/3497/45217507/3544/37791/116/92506/356/1510407/403/1524214/648/1952036/2690/410/16/212/59/10102/450/20421/1994/301217/6326/402866/14707/505966/30017/6011967/55723/7021506/136149/800/16/313/76/19105/546/39427/2294/591211/6798/792890/16184/996045/32348/119LU G(DyXIX)hRP15457/87/610625/134/811098/209/1016772/291/14379230/3457/45565013/3504/37797/117/97087/428/1510399/408/1571964/871/19328114/4668/520/15/214/59/10139/454/20600/2007/301822/6163/404480/14676/509552/30337/6018475/55572/7032221/105654/801/14/316/75/19140/549/39606/2300/591824/6816/794412/16414/999492/32350/119LU G(F XSX)hRP15098/86/610523/138/810700/191/1014726/290/1460985/3388/45225213/3408/37796/115/92499/352/1510214/387/1523792/642/1952109/2672/410/25/213/59/10105/444/20413/1986/301196/6113/402905/14867/505933/30116/6011558/55280/7021053/139079/801/13/314/75/19110/555/39444/2287/591253/6830/792926/16028/996150/32876/119LU G(F XIX)hRP15094/85/614550/138/811023/184/1016907/290/14378869/3427/45588336/3512/37808/115/96968/401/1510441/418/1571099/858/19324508/4194/520/13/214/56/10137/454/20596/2002/301797/6127/404392/14683/509234/29986/6018081/55403/7032693/109508/802/14/3440/81/1919447/568/39199601/2401/591068019/6940/79-P LANNING G RAPH H EURISTICS B ELIEF PACE EARCHGTable 4: Results CAltAlt using hLURP mutex schemes. data Graph ConstructionTime (ms)/All Time (ms)/# Expanded Nodes, indicates time (20 minutes)- indicates attempt.77fiB RYCE , K AMBHAMPATI , & MITHcomputing mutexes across same-worlds (SX) computing mutexes across pairs worldsintersection (conjunction) element labels (IX).Table 4 shows within CAltAlt, using relaxed plan heuristic changing waycompute mutexes LU G drastically alter performance. Often, cross-world mutexesnumerous building LU G takes much time. see could reduce graphGconstruction overhead without hindering performance, evaluated hLURP LUG built(a) considering cross-world relations, schemes (NX), (StX), (DyX), (FX); (b)same-world relations schemes (DyX-SX) (FX-SX), (c) cross-world relationspossible worlds pairs intersection elements labels (DyX-IX) (FX-IX).results show simpler problems like BT BTC benefit much advancedcomputation mutexes beyond static interference. However, Rovers Logistics problems, advanced mutexes play larger role. Mainly, interference, competing needs, inconsistentsupport mutexes important. competing needs inconsistent support mutexes seemlarge impact informedness guidance given LU G, scalability improveshere. Induced mutexes dont improve search time much, add graph computationtime. possible reason induced mutexes dont help much domains actionstwo effects, unconditional conditional effect. Reducing cross-world mutexchecking also helps quite bit. seems checking same-world mutexes sufficientsolve large problems. Interestingly, G graphs compute same-world interference, competingneeds, inconsistent support mutexes within graph, equating scenario (DyXSX), however, LUG provides much faster construction time, evidenced LU Gs abilityout-scale G.6.3 Pshow total time number expanded nodes P solving conformantproblems (including Ring Cube Center) Table 5, P solving conditionalproblems Table 6. CAltAlt show total time number expanded nodesGtest. also add hMsRP heuristic, implemented CAltAlt, takes summationvalues relaxed plans extracted multiple planning graphs. compute mutexesplanning graphs used heuristics P mainly build planninggraphs search node. proceed first commenting performance P D,different heuristics, conformant domains, discuss conditional domains.conformant domains, P generally better CAltAlt. may attributedpart implementation-level details. P makes use existing (highly optimized) BDDpackage belief state generation progression, previously mentioned, CAltAlt reliesless optimized implementation belief state generation regression. see nextsection, regression planners employ sophisticated implementation perform much better,could still benefit heuristics. Aside differences mention, seesimilar trends performance various heuristics CAltAlt P D. Namely,N G SG heuristics limited ability help planner scale, G heuristics helpplanner scale better costly, LU G provides best scalability. differenceG LU G especially pronounced Cube Center Ring, sizeinitial belief state quite large instances scale. Interestingly Ring, breadth firstsearch single graph relaxed plan able scale due reduced heuristic computation time78fiP LANNING G RAPH H EURISTICS B ELIEF PACE EARCHProblemRovers 123456Logistics 12345BT 21020304050607080BTC 210203040506070CubeCenter 35791113Ring 2345678910h0540/36940/2493340/1150560/169450/3760/1023460/51090/204510/184180/31981940/2170320/1520/5930/232160/973880/40575940/1629939120/64657251370/261394hcard520/21790/1572340/75514830/4067530/102460/2590/428460/4970/180630/1420/5840/20370/363230/1010700/259420/720/1120/1520/1930/2340/2740/3150/3570/39hSGRP590/6700/153150/23013480/1004680/46460/31560/1023450/53160/204590/343510/134246620/10316333330/4688130/1570/59350/2322270/97314250/405783360/16299510850/64657-GhMmRP580/61250/323430/7710630/18185370/452180890/416970/582520/3227820/9275740/2742980/59450/26200/428460/418250/18061050/6160460/38280/81500/4151310/77-GhMsRP580/6750/101450/247000/16312470/9915780/38730/216420/1054050/8329180/21151380/152450/2820/106740/2041320/30179930/40726930/50460/3980/19370/911060/55852630/35980/7500/86370/11283780/16-GhMRP U580/6830/131370/232170/3431480/7331950/73650/92310/202000/1553470/382471850/988500/2880/106870/2044260/30183930/40758140/50470/3990/199180/3954140/59251140/791075250/990430/1114780/821183220/44480/8920/1919300/40-GhLURP590/6680/11850/161130/282050/369850/147560/9910/151130/143180/466010/42460/2520/101230/204080/3011680/4028420/5059420/60113110/70202550/80460/3540/191460/394830/5914250/7934220/9971650/119134880/13970/111780/20527900/1774177790/7226609540/1702730/870/10250/24970/444080/9875020/574388300/902-Table 5: Results P conformant Rovers, Logistics, BT, BTC, Cube Center, Ring.data Total Time (ms)/# Expanded Nodes, indicates time - indicatesattempt.low branching factor search. LU G able provide good search guidance, tendstake long time computing heuristics Ring.also able compare choices aggregating distance measures reGlaxed plans multiple graphs. see taking maximum relaxed plans, hMmRP ,assuming positive interaction among worlds useful Logistics Rovers, loses independence worlds BT BTC domains. However, taking summation relaxed plan79fiB RYCE , K AMBHAMPATI , & MITHProblemRovers 123456Logistics 12345BT 21020304050607080BTC 210203040506070h0550/361030/2621700/4675230/1321530/118460/5450/6-hcard480/21550/36590/48620/58460/3470/19510/39620/59850/791310/992240/11924230/13945270/159460/3480/19510/39660/59970/791860/994010/1197580/139hSGRP580/6780/153930/2486760/387740/46450/3111260/7197470/5271410/10842-GhMmRP570/6760/14830/151020/2016360/17531870/173580/101630/301360/204230/5927370/183460/3970/199070/3952410/59207890/79726490/99470/31150/1911520/3962060/59251850/79941220/99-GhMsRP570/6710/12830/151040/2111100/23224840/159570/101300/361250/193820/5719620/178450/3970/199060/3952210/59206830/79719000/99460/31140/19-GhMRP U580/6730/12910/171070/2112810/20930250/198600/101360/361290/193940/5720040/178470/31020/199380/3955750/59233720/79470/31200/1911610/3964290/59274610/79-GhLURP580/6730/13810/16910/217100/17413560/174570/101250/361210/194160/5720170/178460/3550/191610/395970/5917620/7943020/9991990/119170510/139309940/159470/3590/191960/396910/5919830/7949080/99103480/119202040/139Table 6: Results P conditional Rovers, Logistics, BTS, BTCS. data Total Time(ms)/# Expanded Nodes, indicates time (20 minutes) - indicatesattempt.Gvalues different worlds, hMsRP able capture independence BT domain. noticesummation help P BTC domain; overestimateheuristic value nodes counting Flush action world factGneeds done (i.e. miss positive interaction). Finally, using hMRP U heuristicwell every domain, aside cost computing multiple graph heuristics,account positive interaction independence taking overlap relaxed plans.Again, LU G relaxed plan, analogous multiple graph unioned relaxed plan, Pscales well measure overlap lower cost computing heuristic significantly.main change see using P versus CAltAlt direction searchdifferent, hcard heuristic performs unlike before. BT BTC domains cardinalitywork well progression size belief states change get closergoal (it impossible ever know package contains bomb). However, regressionstart belief state containing states consistent goal regressing actions limits80fiP LANNING G RAPH H EURISTICS B ELIEF PACE EARCHbelief state states reach goal actions. Thus regressionsize belief states decreases, progression remains constant.performance P conditional domains exhibits similar trends conformant domains, exceptions. Like conformant domains, G relaxed plans tendoutperform SG relaxed plan, LU G relaxed plan best overall. Unlike conformantGdomains, hMmRP performs much better BTS BTCS BT BTC partlyconditional plans lower average cost. hcard heuristic better BTS BTCSBT BTC belief states actually decrease size partitionedsensory actions.6.4 Sampling Worldsevaluations point considered effectiveness different heuristics, computed respect possible worlds belief state. would like use manypossible worlds can, reduce computation cost hopefully still get reasonableheuristics considering subset worlds. scheme considering subsets worldsheuristics sample single world (SG1 ), sample given percentage worldsbuild multiple graphs, LU G.MGLU Gsampling approaches, use hSGRP , hRP U , hRP relaxed plans. buildG LU G 10%, 30%, 50%, 70%, 90% worlds belief state, sampledrandomly. Figure 10, show total time taken (ms) solve every problem test set(79 problems 10 domains). unsolved problem contributed 20 minutes total time.comparison show previously mentioned heuristics: hSGRP computed unioned singleUgraph SG , denoted Unioned compared sampled single graph SG1 denoted Single,GLU GhMRP U hRP computed worlds denoted 100%. total time heuristicsamples worlds averaged ten runs.two major points see Figure 10. First, hSGRP heuristic much effective1Ucomputed SG versus SG . SG1 less optimistic. buildsplanning graph real world state, opposed union literals possible world states,SGU . Respecting state boundaries considering single state better ignoringstate boundaries naively consider possible states. However, seen GLU G heuristics, respecting state boundaries considering several states much better,bringing us second point.see different performance using possible worlds build multiple graphscompared LU G. better using fewer worlds build multiple graphsbecome costly number worlds increases. contrast, performanceimproves possible worlds use LU G. Using possible worlds computeheuristics good idea, takes efficient substrate exploit them.6.5 Accuracyheuristics account overlap possible worlds accurateheuristics make assumption full positive interaction full independence. checkintuitions, compare heuristic estimates distance initial belief stategoal belief state heuristics used conformant problems solved P D. Figure11 shows ratio heuristic estimate h(BSI ) optimal serial plan length h (BSI )81fiB RYCE , K AMBHAMPATI , & MITHSGMGLUG1614121086420UnionedSingle10%30%50%70%90%100%Figure 10: Total Time (hours) P solve conformant conditional problemssampling worlds use heuristic computation.several problems. points line (where ratio one) under-estimates,over-estimates. problem instances shown optimal planlength known.GMGnote domains hLURP hRP U heuristics close h , confirmingGGintuitions. Interestingly, hsRP hmRP close h Rovers Logistics;whereas former close BT BTC problems, latter close CubeCenterRing. expected, assuming independence (using summation) tends over-estimate,assuming positive interaction (using maximization) tends under-estimate. hSGRP heuristictends under-estimate, cases (CubeCenter Ring) gives value zero (becauseinitial state satisfies goal). hcard heuristic accurate BT BTC,under-estimates Rovers Logistics, over-estimates Cube Center Ring.accuracy heuristics cases disconnected run time performance.instance hcard highly overestimates Ring Cube Center, well domainsGMGexhibit special structure heuristic fast compute. hand, hLURP hRP U82fiP LANNING G RAPH H EURISTICS B ELIEF PACE EARCH1000010001001010.10.010.00133333333333333+333333332222222++222+3322222 ++33333+33+333233 + 3 3+2++22+2++2+2+2+2+2+2+22+2+2+2hcard 3hSGRP +GhmRP2GhsRPGhMRP UGhLURPRv1 Rv4 L1L5 B10B80 BC10BC70 C3ProblemC13 R2R10Figure 11: Ratio heuristic estimates distance BSI BSG optimal plan length.Rv = Rovers, L = Logistics, B = BT, BC = BTC, C = Cube Center, R = Ring.accurate many domains, suffer Ring Cube Center costlycompute.6.6 Inter-Heuristic Conclusionsfindings fall two main categories: one, effective estimates belief state distancesterms state state distances, two, exploit planning graphs supportcomputation distance measures.comparing ways aggregate state distance measures compute belief state distances,found measuring interaction single graph heuristics tends poorly guide planners,measuring independence positive interaction worlds works well specific domains,measuring overlap (i.e. combination positive interaction independence) tends work welllarge variety instances. studying accuracy heuristics foundcases accurate effective. however find accuratebest cases.Comparing graph structures provide basis belief state distance measures, foundheuristics extracted single graph fail systematically account independence positive interaction among different possible worlds. Despite lack distancemeasure, single graphs still identify structure domains like Rovers Logistics.accurately reflect belief state distances, multiple graphs reason reachabilityworld independently. accuracy comes cost computing lot redundant G structure limiting instances large belief states. reduce cost G structure83fiB RYCE , K AMBHAMPATI , & MITHPlannerCAltAltPMBPKACMBPCGPSGPGPTYKACFFSearch SpaceBelief SpaceBelief SpaceBelief SpaceBelief SpacePlanning GraphPlanning GraphBelief SpaceBelief SpaceBelief SpaceSearch DirectionBackwardForwardForward/BackwardForwardBackwardBackwardForwardBackwardForwardConditionalHeuristicPlanning GraphPlanning GraphCardinalityCardinalityPlanning GraphPlanning GraphState Space PlansCardinalityPlanning GraphImplementationCCCCLispLispCCCTable 7: Comparison planner features.sampling worlds used construction. However planners able exhibit better scalabilityconsidering worlds optimizing representation redundant structureLU G. improvement scalability attributed lowering cost heuristic computation, retaining measures multiple state distances. LU G makes trade-off usingexponential time algorithm evaluation labels instead building exponential numberplanning graphs. trade-off justified experiments.7. Empirical Evaluation: Inter-Planner Comparisonfirst compare CAltAlt P several planners conformant domains,compare P conditional planners conditional domains. purposesection identify advantages techniques state art planners. endsection discussion general conclusions drawn evaluation.7.1 Conformant PlanningAlthough work aimed giving general comparison heuristics belief space planning,also present comparison best heuristics within CAltAlt Pleading approaches conformant planning. Table 7 lists several features evaluatedplanners, search space, search direction, whether conditional, typeheuristics, implementation language. Note, since approach uses different planningrepresentation (BDDs, GraphPlan, etc.), even use heuristics, hard getstandardized comparison heuristic effectiveness. Furthermore, planners use PDDLlike input syntax; MBP, KACMBP use AR encodings may give advantagereducing number literals actions. gave MBP planners groundedfiltered action descriptions used CAltAlt P D. also tried, reportresults, giving MBP planners full set ground actions without filtering irrelevant actions.appears MBP planners use sort action pre-processing performancemuch worse full grounded set actions. Nevertheless, Table 8 compares MBP, KACMBP,LU G(DyXSX)GGPT, CGP, YKA, CFF hRPCAltAlt hLURP P respectrun time plan length.MBP: MBP planner uses cardinality heuristic many cases overestimates plan distances(as per implementation hcard ). MBP uses regression search conformant plans,progression search conditional plans. interesting note difficult problem84fiP LANNING G RAPH H EURISTICS B ELIEF PACE EARCHProblemRovers 123456Logistics 12345BT 21020304050607080BTC 210203040506070CubeCenter 357911Ring 2345678CAltAltLU G(DyXSX)hRP U16070/510457/810828/1015279/1364870/29221051/25907/92862/1510810/1524862/1954726/3416/271/10552/202415/307543/4017573/5035983/6067690/70157655/8016/389/19651/392721/598009/7919074/9938393/11965448/139-PONDGhLURP590/5680/9850/111130/162050/258370/25560/9910/151130/143180/226010/29460/2520/101230/204080/3011680/4028420/5059420/60113110/70202550/80460/3540/191460/394820/5914250/7934220/9971650/119134880/13970/91780/1827900/29177790/36609540/4730/670/8250/13970/174080/2275020/30388300/29MBPKACMBPGPTCGPYKACFF66/5141/8484/103252/13OoM727/3237/9486/24408/142881/27OoM6/2119/1080/20170/30160/40300/50480/60730/701080/808/3504/1998/39268/59615/791287/992223/1193625/13910/916/1835/2764/36130/450/50/810/1120/1430/1780/20160/239293/59289/159293/169371/1839773/40127/12451/191578/188865/22226986/4210/216/1084/20244/30533/401090/502123/603529/701090/8018/345/19211/39635/591498/7910821/995506/1192640/13920/920/1870/27120/36230/450/540/830/1150/14120/18230/21600/243139/54365/85842/107393/13399525/20916/91297/151711/119828/18543865/28487/2627/10472174/20465/3715/1940/9363/184782/2742258/3626549/4531/535/860/11635/1451678/17-70/5180/8460/101860/13OoM60/6290/6400/81170/820/1520/13200/110330/124630/149329/187970/1145270/10/339370/1928990/3-1220/72050/101740/122010/167490/2724370/26250/13670/1920280/2117530/27141910/400/20/1020/2080/30160/40250/50420/60620/703310/8010/330/19240/391210/593410/798060/5015370/11927400/1390/90/1920/3480/69190/680/50/820/1180/14110/17300/20480/2370/530/810/1010/1318/2221/2310/912/1514/1212/1825/280/230/104400/204500/3026120/4084730/50233410/60522120/70979400/8010/357/192039/3923629/59116156/79334879/9920/1528540/45360/12-LU G(DyXSX)GTable 8: Results CAltAlt using hRP, P using hLURP , MBP, KACMBP, GPT,CGP, YKA, CFF conformant Rovers, Logistics, BT, BTC, Cube Center, Ring.data Total Time / # Plan Steps, indicates time (20 minutes), OoMindicates memory (1GB), - indicates attempt.instances Rovers Logistics domains MBP KACMBP tend generate much longerplans planners. MBP outperform P cases findsolutions certain instances (like Rovers 5), likely heuristic. noteKACMBP MBP quite fast Cube Center Ring domains, troubledomains like Rovers Logistics. illustrates heuristic modeling knowledge opposedreachability well domains challenge uncertainty reachability.85fiB RYCE , K AMBHAMPATI , & MITHOptimal Planners: optimal approaches (CGP GPT) tend scale well, despitegood solutions. CGP trouble constructing planning graphs parallel conformant plandepth increases. CGP spends quite bit time computing mutexes, increases planningcost plan lengths increase. CGP much better shallow parallel domains like BT,find one step plans dunk every package parallel.GPT performs progression search guided heuristic measures cost fullyobservable plans state space. GPT finds optimal serial plans effective sizesearch space increases. GPT fails scale search space becomes difficulteven compute heuristic (due larger state space well).YKA: YKA, like CAltAlt regression planner, search engine different YKAuses cardinality heuristic. YKA performs well domains search enginebased BDDs. notice difference progression regression comparing PYKA, similar trends found comparison P CAltAlt. Additionally,seems YKA stronger regression search engine CAltAlt. P able betterYKA Rovers Logistics domains, unclear whether searchdirection heuristics.CFF: Conformant FF, progression planner using relaxed plan similar LU G relaxed plan,well Rovers Logistics domains uses highly optimized FF searchengine well cheap compute relaxed plan heuristic. However, CFF wellBT, BTC, Cube Center, Ring problems many literalsentailed belief state. CFF relies implicitly representing belief states terms literalsentailed belief state, initial belief state, action history.literals entailed belief state, reasoning belief state requiresinference action history. Another possible reason CFF suffers encodings.Cube Center Ring domains naturally expressed multi-valued state features,transformation binary state features describe values must hold also valuesmust hold. difficult CFF conditional effect antecedents contain severalliterals heuristic restricted considering one literal. may CFFchoosing wrong literal simply enough literals get effective heuristics. However BTBTC used one literal effect antecedents CFF still performs poorly.7.2 Conditional PlanningTable 9 shows results testing conditional versions domains P D, MBP, GPT,SGP, YKA.MBP: P planner similar MBP uses progression search. Puses AO* search, whereas MBP binary used uses depth first And-Or search. depthfirst search used MBP contributes highly sub-optimal maximum length branches (as muchorder magnitude longer P D). instance, plans generated MBPRovers domain rover navigating back forth locations several timesanything useful; situation beneficial actual mission use. MBP tends scalewell P domains tested. possible reason performance MBPLogistics Rovers domains sensory actions execution preconditions,prevent branching early finding deterministic plan segments branch. experimented86fiP LANNING G RAPH H EURISTICS B ELIEF PACE EARCHProblemRovers 123456Logistics 12345BT 21020304050607080BTC 210203040506070PONDGhLURP580/5730/8810/8910/107100/1913560/22570/71250/121210/94160/1520170/22460/2550/101610/205970/3017620/4043020/5091990/60170510/70309940/80470/2590/101960/206910/3019830/4049080/50103480/60202040/70MBPGPTSGPYKA3312/114713/755500/1195674/14616301/76OoM41/1622660/1772120/45OoM0/2240/10OoM20/2280/10OoM-3148/55334/77434/811430/101023/75348/122010/8510/2155314/10OoM529/2213277/10-70/5760/75490/60/170/1950/14470/113420/132160/190407/1120010/110/2-3210/56400/77490/811210/101390/80/220/1060/20200/30400/40810/501350/602210/703290/800/4210/122540/2213880/3246160/42109620/52221460/6241374/72GTable 9: Results P using hLURP , MBP, GPT, SGP, YKA conditional Rovers, Logistics, BT, BTC. data Total Time / # Maximum possible steps execution,indicates time (20 minutes), OoM indicates memory (1GB), -indicates attempt.MBP using sensory actions without execution preconditions able scale somewhatbetter, plan quality much longer.Optimal Planners: GPT SGP generate better solutions slowly. GPT betterRovers Logistics problems exhibit positive interaction plans,SGP well BT planning graph search well suited shallow, yet broad (highlyparallel) problems.YKA: see YKA fares similar GPT Rovers Logistics, trouble scalingreasons. think YKA may trouble regression sensory actionssince able scale reasonably well conformant version domains. Despite this,YKA proves well BT BTC problems.87fiB RYCE , K AMBHAMPATI , & MITH7.3 Empirical Evaluation Conclusionsinternal comparisons heuristics within CAltAlt P D, well external comparisons several state art conformant conditional planners learned manyinteresting lessons heuristics planning belief space.Distance based heuristics belief space search help control conformant conditional planlength because, opposed cardinality, heuristics model desirable plan quality metrics.Planning graph heuristics belief space search scale better planning graph searchadmissible heuristic search techniques.planning graph heuristics presented, relaxed plans take account overlapindividual plans states source destination belief statesaccurate tend perform well across many domains.LUG effective planning graph regression progression search heuristics.regression search, planning graphs maintain same-world mutexes provide besttrade-off graph construction cost heuristic informedness.Sampling possible worlds construct planning graphs reduce computational cost,considering worlds exploiting planning graph structure common possible worlds(as LU G), efficient informed.LUG heuristics help conditional planner, P D, scale conditional domains,despite fact heuristic computation model observation actions.8. Related Work & Discussiondiscuss connections several related works involve heuristics and/or conditional planning first half section. second part section discuss extendwork directly handle non-deterministic outcomes actions heuristic computation.8.1 Related WorkMuch interest conformant conditional planning traced CGP (Smith & Weld, 1998),conformant version GraphPlan (Blum & Furst, 1995), SGP (Weld et al., 1998), analogousconditional version GraphPlan. graph search conducted several planning graphs,constructed one possible initial states. recent work C-plan (Castelliniet al., 2001) Frag-Plan (Kurien et al., 2002) generalize CGP approach orderingsearches different worlds plan hardest satisfy world found first,extended worlds. Although CAltAlt P utilize planning graphssimilar CGP Frag-plan uses compute reachability estimates. searchconducted space belief states.Another strand work models conformant conditional planning search spacebelief states. started Genesereth Nourbakhsh (1993), concentrated formulating set admissible pruning conditions controlling search. heuristicschoosing among unpruned nodes. GPT (Bonet & Geffner, 2000) extended idea consider88fiP LANNING G RAPH H EURISTICS B ELIEF PACE EARCHsimple form reachability heuristic. Specifically, computing estimated cost belief state,GPT assumes initial state fully observable. cost estimate done termsreachability (with dynamic programming rather planning graphs). GPTs reachability heuristicGsimilar hMmRP heuristic estimate cost farthest (maximum distance) state looking deterministic relaxation problem. comparison GPT, CAltAltP seen using heuristics better job considering cost beliefstate across various possible worlds.Another family planners search belief states MBP-family plannersMBP(Bertoli et al., 2001b), KACMBP (Bertoli & Cimatti, 2002). contrast CAltAlt similar P D, MBP-family planners represent belief states terms binary decisiondiagrams. Action application modeled modifications BDDs. MBP supports progression regression space belief states, KACMBP pure progression planner.computing heuristic estimates, KACMBP pro-actively reduces uncertainty beliefstate preferring uncertainty reducing actions. motivation approach applyingcardinality heuristics belief states containing multiple states may give accurate enough direction search. reducing uncertainty seems effective idea, note (a)domains may contain actions reduce belief state uncertainty (b) need uncertainty reduction may reduced heuristics effectively reason multipleworlds (viz., multiple planning graph heuristics). Nevertheless, could fruitful integrate knowledge goal ideas KACMBP reachability heuristics CAltAlt Phandle domains contain high uncertainty costly goals.contrast domain-independent approaches require models domainphysics, PKSPlan (Petrick & Bacchus, 2002) forward-chaining knowledge-based plannerrequires richer domain knowledge. planner makes use several knowledge bases, opposedsingle knowledge base taking form belief state. knowledge bases separate binarymulti-valued variables, planning execution time knowledge.YKA (Rintanen, 2003b) regression conditional planner using BDDs uses cardinality heuristic. Recently Rintanen also developed related reachability heuristics considerdistances groups states, rely planning graphs (Rintanen, 2004).recently, closely related work heuristics constructing conformantplans within CFF planner (Hoffmann & Brafman, 2004). planner represents belief statesimplicitly set known facts, action history (leading belief state), initialbelief state. CFF builds planning graph forward set known literals goal literalsbackwards initial belief state. planning graph, conditional effects restrictedsingle literals antecedent enable tractable 2-cnf reasoning. planning graph,CFF extracts relaxed plan represents supporting goal belief state statesinitial belief state. biggest differences LU G CFF techniqueLU G reasons forward source belief state (assuming explicit, albeit symbolic, beliefstate), LU G restrict number literals antecedents. result, LU Glose causal information perform backward reasoning initial belief state.handling uncertainty labels label propagation reminiscent relatedde Kleers assumption based truth maintenance system (ATMS) (de Kleer, 1986). ATMSuses labels identify assumptions (contexts) particular statement holds, traditionaltruth maintenance system requires extensive backtracking consistency enforcement identifycontexts. Similarly, reason multiple possible worlds (contexts)89fiB RYCE , K AMBHAMPATI , & MITHLUG simultaneously, MG approach requires, backtracking, reproduction planninggraphs possible worlds.Finally, CAltAlt P also related to, adaptation work reachabilityheuristics classical planning, including AltAlt (Nguyen et al., 2002), FF (Hoffmann & Nebel,2001) HSP-r (Bonet & Geffner, 1999). CAltAlt conformant extension AltAlt usesregression search (similar HSP-r) guided planning graph heuristics. P similar FFuses progression search planning graph heuristics.8.2 Extension Non-Deterministic Actionsscope presentation evaluation restricted planning initial state uncertainty deterministic actions, planning graph techniques extended includenon-deterministic actions type described Rintanen (2003a). Non-deterministic actionseffects described terms set outcomes. simplicity, consider Rintanensconditionality normal form, actions set conditional effects (as before)consequent mutually-exclusive set conjunctions (outcomes) one outcome effectresult randomly. outline generalization single, multiple, labelled planning graphsreason non-deterministic actions.Single Planning Graphs: Single planning graphs, built approximate belief statessampled state, lend straight-forward extension. single graph ignoresuncertainty belief state unioning literals sampling state form initial planninggraph layer. Continuing single graph assumptions uncertainty, makes sense treatnon-deterministic actions deterministic. Similar approximate belief state setliterals form initial literal layer sample state, assume non-deterministic effectadds literals appearing effect samples outcome action deterministic(i.e. gives set literals). Single graph relaxed plan heuristics thus remain unchanged.Multiple Planning Graphs: Multiple planning graphs much like Conformant GraphPlan(Smith & Weld, 1998). generalize splitting non-determinism current belief statemultiple initial literal layers splitting outcomes non-deterministic effects multipleliteral layers. idea root set new planning graphs level,initial literal layer containing literals supported interpretation previous effect layer.interpretations effect layer mean every possible set joint effect outcomes. set effectoutcomes possible two outcomes outcomes effect. Relaxed plan extractionstill involves finding relaxed plan planning graph. However, since planning graphsplit many times (in tree-like structure) relaxed plan extracted path tree.note technique likely scale exponential growth redundantplanning graph structure time. Further, experiments CGP enough trouble initialstate uncertainty. expect able much better LU G.Labelled Uncertainty Graph: multiple planning graphs forced capture nondeterminism splitting planning graphs initial literal layer, alsoliteral layer follows least one non-deterministic effect. saw LU G labelscapture non-determinism drove us split initial literal layer multiple graphs.such, labels took syntactic form describes subsets states source beliefstate. order generalize labels capture non-determinism resulting uncertain effects,90fiP LANNING G RAPH H EURISTICS B ELIEF PACE EARCHneed extend syntactic form. objective label represent sourcesuncertainty (arising source belief state effects) causally support labelled item.also introduce graph layer Ok represent outcomes connect effects literals.might seem natural describe labels outcomes terms affected literals,lead trouble. problem literals effect outcomes describing statesdifferent time literals projected belief state. Further, outcome appears twolevels graph describing random event different times. Using state literals describelabels lead confusion random events (state uncertainty effect outcomesdistinct steps) causally support labelled item. pathological example effectwhose set outcomes matches one-to-one states source belief state. case,using labels defined terms state literals cannot distinguish random event (the stateuncertainty effect uncertainty) described label.two choices describing effect outcomes labels. choices introducenew set label variables describe literal layer split. new variables useddescribe effect outcomes labels confused variables describing initial stateuncertainty. first case, variables one-to-one matching original setliterals, thought time-stamped literals. number variables addlabel function order 2F per level (the number fluent literals assuming booleanfluents). second option describe outcomes labels new set fluents,interpretation fluents matched particular outcome. case, add orderlog |Ok | variables, Ok k th outcome layer. would actually lower manyoutcomes deterministic effects need describe labels.former approach likely introduce fewer variables lot non-deterministiceffects affect quite literals. latter introduce fewer variablesrelatively non-deterministic effects whose outcomes fairly independent.generalized labelling, still say item reachable source beliefstate label entailed source belief state. even though addingvariables labels, implicitly adding fluents source belief state. example, sayadd fluent v describe two outcomes effect. One outcome labelled v, v.express source belief state BSP projected LU G new fluentBSP (v v) = BSP . item labelled BSP v entailed projected beliefstate (i.e. unreachable) one outcome causally supports it. outcomes supportitem, reachable.Given notion reachability, determine level extract relaxedplan. relaxed plan procedure change much terms semanticsextra graph layer outcomes. still ensure literals causally supportedworlds labelled relaxed plan, whether worlds initial stateuncertainty supporting non-deterministic effects.9. Conclusionintent establishing basis belief state distance estimates, have:Discussed heuristic measures aggregate state distance measures capture positiveinteraction, negative interaction, independence, overlap.91fiB RYCE , K AMBHAMPATI , & MITHShown compute heuristic measures planning graphs provided empiricalcomparisons measures.Found exploiting planning graph structure reduce cost considering possiblestates belief state preferable sampling subset states heuristics.Shown labelled uncertainty graph capture support information multiplegraphs, reduces cost heuristic computation.Shown labelled uncertainty graph useful conformant planning and, withoutconsidering observational actions knowledge, perform well conditional planning.intent work provide formal basis measuring distance beliefstates terms underlying state distances. investigated several ways aggregate statedistances reflect various assumptions interaction state state trajectories. bestmeasures turned measure positive interaction independence, calloverlap. saw planners using notion overlap tend well across large varietydomains tend accurate heuristics.Weve also shown planning Labelled Uncertainty planning Graph LU G, condensedversion multiple graphs useful encoding conformant reachability information. maininnovation idea labels labels attached literals, actions, effect relations,mutexes indicate set worlds respective elements hold. experimentalresults show LU G outperform multiple graph approach. comparisonapproaches, weve also able demonstrate utility structured reachability heuristicscontrolling plan length boosting scalability conformant conditional planning.intend investigate three additions work. first, incorporate sensingknowledge heuristics. already promising results without using featuresplanning graphs, hope help approaches scale even better conditionalproblems. second addition consider heuristics stochastic planning problems.major challenges associate probabilities labels indicate likelihoodpossible world integrate reasoning probabilistic action effects.Lastly, recently extended LU G within framework state agnostic planninggraphs (Cushing & Bryce, 2005), hope improve technique. state agnostic planninggraph essentially multiple source planning graph, analogy conventional planninggraph single source. Planning graphs already multiple destination, generalizationstate agnostic planning graph allows us compute distance measure pairstates belief states. LU G seeks avoid redundancy across multiple planning graphsbuilt states belief state. extended notion avoid redundancy planninggraphs built every belief state. shown state agnostic LU G (SLU G)built per search episode (as opposed LU G node) reduce heuristic computationcost without sacrificing informedness.Acknowledgments would like thank Minh B. Do, Romeo Sanchez, Terry Zimmermam,Satish Kumar Thittamaranahalli, Cushing helpful discussions feedback, Jussi Rintanen help YKA planner, Piergiorgio Bertoli help MBP planner.work supported part NASA grants NCC2-1225 NAG2-1461, NSF grant IIS0308139, 2003 NASA RIACS SSRP, ARCS Foundation, IBM faculty award.92fiP LANNING G RAPH H EURISTICS B ELIEF PACE EARCHAppendix A. Additional Heuristicscompleteness, present additional heuristics adapted classical planning reasonbelief state distances type planning graph. Many heuristics appearedprevious work (Bryce & Kambhampati, 2004). show compute max, sum, levelheuristics single graph SG, multiple graphs G, labelled uncertainty graph LU G.heuristics tend less effective relaxed plan heuristics, providereference. Section 4, describe heuristics terms regression search.A.1 Single Planning Graph Heuristics (SG)Like, relaxed plan single unmodified planning graph, cannot aggregate state distancesnotion separate states lost forming initial literal layer, thus computeheuristics aggregate state distances.State Aggregation:Max classical planning, maximum cost literal used get max heuristic, useformulas describe belief states, take maximum cost clause costbelief state find max heuristic hSGmax . maximum cost clause belief state,respect single planning graph, is:hSGmax (BSi ) =maxC(BSi )cost(C)cost clause is:cost(C) = min min klC k:lLkfind cheapest literal cost clause find maximum cost clause.underestimate closest state current belief state.Sum Like classical planning sum heuristic, take sum hSGsum costsclauses belief state estimate belief state distancecost(C)hSGsum (BSi ) =C(BSi )heuristic takes summation costs literals closest estimated statebelief state, inadmissible may single action support everyclause, could count clause.Level mutexes planning graph, compute level heuristic hSGlevel(without mutexes level heuristic equivalent max heuristic). level heuristicmaintains admissibility max heuristic improves lower bound consideringlevel planning graph literals constituent non-pairwise mutex.level heuristic computed taking minimum among (BS), first level(lev(S)) planning graph literals present none markedpairwise mutex. Formally:hSGlevel (BSi ) =93minS(BSi )lev(S)fiB RYCE , K AMBHAMPATI , & MITHA.2 Multiple Planning Graph Heuristics (M G)Similar various relaxed plan heuristics multiple graphs, compute max, sum,level heuristic multiple planning graphs aggregate maximumsummation respectively measure positive interaction independence. reason cannotaggregate individual graph heuristics measure overlap numbers, setsactions. Measuring overlap involves taking union heuristics graph unionnumbers meaningful like union action sets relaxed plans. Like before,reason use multiple graphs state distance aggregation.Positive Interaction Aggregation:GMax max heuristic hMmmax computed multiple planning graphs measure posMGitive interaction hmmax heuristic. heuristic computes maximum cost clause(BSi ) graph , similar hSGmmax (BSi ) computed, takesmaximum. Formally:GhMmmax (BSi ) = max (hmax (BSi ))GhMmmax heuristic considers minimum cost, relevant literals belief state (thosereachable given possible world graph ) get state measures. maximumtaken estimate accounts worst (i.e., plan needed difficultworld achieve subgoals).Sum sum heuristic measures positive interaction multiple planning graphsGhMmsum . computes summation cost clauses (BSi ) graphtakes maximum. Formally:GhMmsum (BSi ) = max (hsum (BSi ))heuristic considers minimum cost, relevant literals belief state (thosereachable given possible worlds represented graph ) get state measures.GhMmmax , maximum taken estimate costly world.GMGMGLevel Similar hMmmax hmsum , hmlevel heuristic found first finding hlevelgraph get state distance measure, taking maximum acrossgraphs. hlevel (BSi ) computed taking minimum among (BS),first level lev (S) planning graph literals present nonemarked mutex. Formally:hlevel (BSi ) =minS(BSi )lev (S)GhMmlevel (BSi ) = max(hlevel (BSi ))Note heuristic admissible. reasoning classical planning, firstlevel subgoals present non-mutex underestimate true coststate. holds graphs. Taking maximum accounts difficult94fiP LANNING G RAPH H EURISTICS B ELIEF PACE EARCHworld achieve constituent BSi thus provable underestimate h .GGPTs max heuristic (Bonet & Geffner, 2000) similar hMmlevel , computeddynamic programming state space rather planning graphs.Independence Aggregation: heuristics mentioned Positive Interaction Aggregationaugmented take summation costs found individual planning graphs ratherGMGMGmaximum. denote as: hMsmax , hssum , hslevel . None heuristicsadmissible action may used worlds, count cost every worldusing summation.A.3 Labelled Uncertainty Graph (LU G)max, sum, level heuristics LU G similar analogous multiple graph heuristics. main difference heuristics LU G much easier computepositive interaction measures independence measures. reason positive interaction easiercompute find cost clause states belief state once, rathermultiple planning graphs. Like before, consider heuristics aggregatestate distances.Positive Interaction Aggregation:GMax max heuristic hLUmmax LU G finds maximum clause cost across clausescurrent belief state BSi . cost clause first level becomes reachable.Formally:GhLUmmax (BSi )=maxC(BSi )mink:BSP |=k (C)kGSum sum heuristic hLUmsum LU G sums individual levels clause(BSi ) first reachable. Formally:GminhLU(BS)=kmsumC(BSi )k:BSP |=k (C)GLevel level heuristic hLUmlevel index first level BSi reachable.Formally:GhLUmlevel (BSi ) =mink:BSP |=k (BSi )Independence Aggregation: heuristics mentioned positive interaction aggregationaugmented take summation costs state belief state. may inefficientdue fact lose benefit LU G evaluating heuristic stateBSP , rather states positive interaction aggregation. casework similar multiple graph heuristic extraction, aside improved graphconstruction time. positive interaction aggregation able implicitly calculate maximumworlds heuristics, whereas sum heuristic need explicitly findGLU GLU Gcost world. denote sum heuristics as: hLUsmax , hssum , hslevel .95fiB RYCE , K AMBHAMPATI , & MITHAppendix B. Cross-World MutexesMutexes develop possible world also two possible worlds,described Smith Weld (1998). Cross-world mutexes useful capture negative interactions belief state distance measures (mentioned Section 3). representation crossworld mutexes requires another generalization labelling mutexes. world mutexesrequire keeping one label mutex signify possible worlds mutex holds. extended representation keeps pair labels, one element mutex;x possible world mutex x possible world , denote mutex pair(k (x) = S, k (x ) = ).compute cross-world mutexes several worlds elements x x . example, k (x) = S1 S2 S3 k (x ) = S2 S3 , check cross-world mutexes needconsider mutexes world pairs (S1 , S2 ), (S1 , S3 ), (S2 , S2 ), (S2 , S3 ), (S3 , S2 ), (S3 , S3 ).also check mutexes intersection element labels k (x) k (x ) = S2 S3 ,meaning cross world pairs check mutexes (S2 , S2 ), (S2 , S3 ), (S3 , S2 ),(S3 , S3 ).say formula f reachable projected belief state BSP , consideringcross-world mutexes, every pair states BSP , f reachable. pair states, f reachable |= k (f ) every pair constituents , f|= k (S ) |= k (S ), two literals either same-worldmutex = , mutex literals , across respectiveworlds= . mutex pair literals l l , respectivelymutex (k (l), k (l )) |= k (l) |= k (l ).computation cross-world mutexes requires changes mutex formulas,outlined next. major change check, instead single possible worlds S, pairspossible worlds mutexes.Action Mutexes: action mutexes hold actions executable differentpossible worlds.Interference Interference mutexes change cross-world mutexes, exceptpair labels (k (a) = BSP , k (a ) = BSP ), instead single label.Competing Needs Competing needs change mutexes cross-world mutexes twoactions , worlds respectively, could competing. Formally, crossworld competing needs mutex ((k (a) = S, k (a ) = ) exists worldsif:le (a),l e (a ) (k (l) = S, k (l ) = )Effect Mutexes: effect mutexes hold effects occur different possible worlds.Interference Effect interference mutexes change cross-world mutexes, exceptpair labels (k (i (a)) = BSP , k (j (a )) = BSP ), instead singlelabel.96fiP LANNING G RAPH H EURISTICS B ELIEF PACE EARCHLklk(p)pEkAklk(a)h(a)lk(h(a))(lk(p), lk(q))Induced mutex across worlds:(lk(j(a))lk(i(a)), lk(h(a)))(lk(j(a)), lk(h(a)))lk(q)qlk(a)j(a)lk(r)rlk(j(a))i(a) induces j(a) in:lk(i(a))lk(j(a))i(a) lk(i(a))Figure 12: Example cross-world induced effect mutex.Competing Needs Effect competing needs mutexes change cross-world mutexestwo effects (a) j (a ), worlds respectively, could competing. Formally,cross-world competing needs mutex (k (i (a)) = S, k (j (a )) = ) exists (a)j (a ) worlds if:li (a),l j (a ) (k (l) = S, k (l ) = )Induced Induced mutexes change slightly cross-world mutexes. worlds oneeffect induces another, remains same, mutex changes slightly.j (a) k (j (a)) mutex h (a ) k (h (a )), (a) induces effect j (a)possible worlds described k (i (a)) k (j (a)), induced mutex(a) k (j (a)) k (i (a)) h (a ) k (h (a )) (see Figure 12).Literal Mutexes: literal mutexes hold literals supported different possible worlds.Inconsistent Support changes cross-world mutexes. mutex (k (l) = S, k (l ) = )holds l l (a), j (a ) Ek1 l (a), l j (a ),mutex k1 (i (a)) = S, k1 (j (a )) = ).97fiB RYCE , K AMBHAMPATI , & MITHReferencesBertoli, P., & Cimatti, A. (2002). Improving heuristics planning search belief space.Proceedings AIPS02.Bertoli, P., Cimatti, A., & Roveri, M. (2001a). Heuristic search + symbolic model checking =efficient conformant planning. Proceedings IJCAI01.Bertoli, P., Cimatti, A., Roveri, M., & Traverso, P. (2001b). Planning nondeterministic domainspartial observability via symbolic model checking. Proceedings IJCAI01.Blum, A., & Furst, M. (1995). Fast planning planning graph analysis. ProceedingsIJCAI95.Bonet, B., & Geffner, H. (1999). Planning heuristic search: New results. ProceedingsECP99.Bonet, B., & Geffner, H. (2000). Planning incomplete information heuristic search beliefspace. Proceedings AIPS00.Brace, K., Rudell, R., & Bryant, R. (1990). Efficient implementation bdd package. Proceedings 27th ACM/IEEE design automation conference.Bryant, R. (1986). Graph-based algorithms Boolean function manipulation. IEEE TransactionsComputers, C-35(8), 677691.Bryce, D., & Kambhampati, S. (2004). Heuristic guidance measures conformant planning.Proceedings ICAPS04.Castellini, C., Giunchiglia, E., & Tacchella, A. (2001). Improvements sat-based conformantplanning. Proceedings ECP01.Cimatti, A., & Roveri, M. (2000). Conformant planning via symbolic model checking. JournalArtificial Intelligence Research, 13, 305338.Cormen, T. H., Leiserson, C. E., & Rivest, R. L. (1990). Introduction Algorithms. McGraw-Hill.Cushing, W., & Bryce, D. (2005). State agnostic planning graphs. Proceedings AAAI05.de Kleer, J. (1986). Assumption-Based TMS. Artificial Intelligence, 28(2), 127162.Genesereth, M. R., & Nourbakhsh, I. R. (1993). Time-saving tips problem solving incomplete information. Proceedings AAAI93.Hansen, E., & Zilberstein, S. (2001). LAO: heuristic-search algorithm finds solutionsloops. Artificial Intelligence, 129(12), 3562.Hoffmann, J., & Brafman, R. (2004). Conformant planning via heuristic forward search: newapproach. Proceedings ICAPS04.Hoffmann, J., & Nebel, B. (2001). FF planning system: Fast plan generation heuristicsearch. Journal Artificial Intelligence Research, 14, 253302.Kambhampati, S., Ihrig, L., & Srivastava, B. (1996). candidate set based analysis subgoalinteractions conjunctive goal planning. Proceedings AIPS96.Koehler, J., Nebel, B., Hoffmann, J., & Dimopoulos, Y. (1997). Extending planning graphsadl subset. Proceedings ECP97.98fiP LANNING G RAPH H EURISTICS B ELIEF PACE EARCHKurien, J., Nayak, P., & Smith, D. (2002). Fragment-based conformant planning. ProceedingsAIPS02.Long, D., & Fox, M. (2003). 3rd international planning competition: Results analysis.Journal Artificial Intelligence Research, 20, 159.Nguyen, X., Kambhampati, S., & Nigenda, R. (2002). Planning graph basis derivingheuristics plan synthesis state space CSP search. Artificial Intelligence, 135(1-2),73123.Nilsson, N. (1980). Principles Artificial Intelligence. Morgan Kaufmann.Pednault, E. P. D. (1988). Synthesizing plans contain actions context-dependent effects.Computational Intelligence, 4, 356372.Petrick, R., & Bacchus, F. (2002). knowledge-based approach planning incomplete information sensing. Proceedings AIPS02.Rintanen, J. (2003a). Expressive equivalence formalisms planning sensing. Proceedings ICAPS03.Rintanen, J. (2003b). Product representation belief spaces planning partial observability.Proceedings IJCAI03.Rintanen, J. (2004). Distance estimates planning discrete belief space. ProceedingsAAAI04.Smith, D., & Weld, D. (1998). Conformant graphplan. Proceedings AAAI98.Weld, D., Anderson, C., & Smith, D. (1998). Extending graphplan handle uncertainty sensingactions. Proceedings AAAI98.99fi