Journal Artificial Intelligence Research 26 (2006) 153-190

Submitted 10/05; published 06/06

Convexity Arguments Efficient Minimization
Bethe Kikuchi Free Energies
Tom Heskes

t.heskes@science.ru.nl

IRIS, Faculty Science, Radboud University Nijmegen
Toernooiveld 1, 6525 ED, Nijmegen, Netherlands

Abstract
Loopy generalized belief propagation popular algorithms approximate inference Markov random fields Bayesian networks. Fixed points algorithms
shown correspond extrema Bethe Kikuchi free energy,
approximations exact Helmholtz free energy. However, belief propagation always converge, motivates approaches explicitly minimize
Kikuchi/Bethe free energy, CCCP UPS.
describe class algorithms solves typically non-convex constrained
minimization problem sequence convex constrained minimizations upper
bounds Kikuchi free energy. Intuitively one would expect tighter bounds lead
faster algorithms, indeed convincingly demonstrated simulations. Several
ideas applied obtain tight convex bounds yield dramatic speed-ups CCCP.

1. Introduction
Pearls belief propagation (Pearl, 1988) popular algorithm inference Bayesian
networks. known exact special cases, e.g., tree-structured (singly connected)
networks Gaussian discrete nodes. also networks containing cycles,
so-called loopy belief propagation empirically often leads good performance (approximate
marginals close exact marginals) (Murphy, Weiss, & Jordan, 1999; McEliece, MacKay,
& Cheng, 1998). notion fixed points loopy belief propagation correspond
extrema so-called Bethe free energy (Yedidia, Freeman, & Weiss, 2001) important
step theoretical understanding success.
Kikuchi free energy (Kikuchi, 1951) generalization Bethe free energy
lead better approximations exact Helmholtz free energy. like fixed points
loopy belief propagation correspond extrema Bethe free energy, fixed points
algorithm called generalized belief propagation (Yedidia et al., 2001) correspond
extrema Kikuchi free energy.
problem loopy generalized belief propagation always
converge stable fixed point. New algorithms (Yuille, 2002; Teh & Welling, 2002)
derived therefore explicitly minimize Bethe Kikuchi free energy.
describe Section 2, minimization Kikuchi free energy corresponds usually nonconvex constrained minimization problem. Non-convex constrained minimization problems
known rather difficult solve, Section 3 first derive sufficient
conditions Kikuchi free energy convex (over set constraints). Section 4
derive class converging double-loop algorithms, inner loop
corresponds constrained minimization convex bound Kikuchi free energy,
c
2006
AI Access Foundation. rights reserved.

fiHeskes

outer-loop step recalculation bound. Based intuition
tightest bound yields fastest algorithm, come several ideas construct
tight bounds. see Yuilles (2002) CCCP algorithm corresponds special
case rather loose bound discuss relationship UPS algorithm Teh
Welling (2002) Section 4.5. simulations Section 5 illustrate use tight
convex bounds several inference problems. Implications issues discussed
Section 6. Technical details treated appendices.

2. Kikuchi Approximation
Exact inference graphical models often intractable. section introduce
Kikuchi approximation particular example variational approach towards approximate inference.
2.1 Graphical Models
undirected graph G = (V, E) consists set nodes vertices V = {1, . . . , N }
joined set edges E. place node variable xi takes values
finite discrete alphabet. vector containing variables denoted x (x1 , . . . , xn ).
Let subset V ; call region. clique fully connected subset V ; C
set cliques. potential, also referred compatibility kernel function, (x )
strictly positive function depends variables part clique
. define probability distribution probability mass function
pexact (x)

1
(x ) ,
Z

(1)

C

Z normalizing constant, often called partition function. HammersleyClifford theorem (Besag, 1974) guarantees us underlying probability process
Markov respect graph and, vice versa, distribution Markov random field G strictly positive expressed form. process
moralization, directed graphical model (Bayesian network) transformed
corresponding undirected model. Consequently, probability distribution corresponding
Bayesian network also written form (1) (Lauritzen, 1996).
Computing partition function Z, well computing marginals subsets variables, principle requires summation exponential number states. circumvent
exponential summation two kinds approaches: sampling techniques
variational methods. sampling, one draws samples exact probability distribution. variational methods try find approximation exact probability
distribution.
2.2 Variational Methods
Variational methods often derived approximation so-called free energy
X
XX
p(x ) log (x ) +
p(x) log p(x) E(p) S(p) .
(2)
F (p) =
C x

x

154

fiEfficient minimization Kikuchi free energy

first term, E(p), referred energy, second term S(p) entropy.
Functional minimization F (p) respect functions p(x) constraint
p(x) properly normalized yields pexact (x). Furthermore, partition function Z
follows
log Z = F (pexact ) .
stick exact free energy (2), really gain anything: entropy
part S(p) still consists sum exponentially many terms. Variational methods
based tractable approximation free energy. roughly divided two
classes, mean-field Kikuchi approximations. mean-field approach one
confines minimization free energy restricted class (tractable) probability
distributions instead considering class P probability distributions:
log Z = F (pexact ) = min F (p) min F (p) .
pP

pT

crux choose class entropy S(p) becomes tractable p .
Note however restriction typically also affects energy term E(p) (Jordan,
Ghahramani, Jaakkola, & Saul, 1998; Jaakkola & Jordan, 1999).
Kikuchi approximation free energy (2) leaves energy term
approximates entropy S(p) combination marginal entropies:
X
X
S(p) =
p(x) log p(x)
c (p)
R

x

=

X

c

R

X

p(x ) log p(x ) .

(3)

x

R denotes collection so-called regions; parameters c called Moebius
overcounting numbers.
2.2.1 Partially Ordered Sets
Following Pakzad Anantharam (2002, 2005), use language partially ordered
sets posets. Specifically, collection R regions viewed poset
ordering defined respect inclusion operator . region includes
region , written , variables also part . use denote
strict inclusion, i.e., 6 . say covers R, written ,
exists R . visualize poset
so-called Hasse diagram region graph (see examples below). Given particular poset
R, Hasse diagram GR directed acyclic graph, whose vertices elements R,
whose edges corresponds cover relationships. is, edge
iff .
2.3 Cluster Variation Method
Kikuchis (1951) original cluster variation method (CVM), collections regions
overcounting numbers constructed follows. start defining collection
outer regions. minimal choice original set cliques C, also choose
155

fiHeskes

combine cliques construct larger ones, similar process triangulation (Lauritzen,
1996). convenience, redefine potentials correspondingly, i.e.,
precisely one potential (x ) per outer region (see example below).
Given outer regions, construct new regions taking intersections
outer regions, intersections intersections, on, intersections
made. refer regions constructed way inner regions, combined
collection I. collection regions R (3) union outer
inner regions: R = I.
overcounting Moebius numbers original CVM follow Moebius
formula
X
(4)
c = 1
c .


definition c = 1 outer regions O.
Bethe free energy considered special case Kikuchi free energy.
Bethe free energy intersectionsPof intersections, i.e., one level
inner regions c = 1 n n O; 1 equals number outer regions
covering inner region .
2.3.1 Alternatives
Several alternatives original CVM, weaker constraints and/or constraints
choice regions overcounting numbers, proposed recently. Yedidia,
Freeman, Weiss (2005) present overview. particular choice inner regions
subsets overcounting numbers junction graphs (Aji & McEliece, 2001) join
graphs (Dechter, Kask, & Mateescu, 2002) leads entropy approximation
overcounting numbers inner regions negative. resulting algorithms
similar junction tree algorithm, applied graph loops.
entropy approximation follows original cluster variation method takes
account entropy contributions level outer regions consistent manner
and, theoretical grounds, seems reason deviate (Pakzad
& Anantharam, 2005). paper, therefore focus original cluster variation
method, analysis holds much generally poset region graph.
2.4 Constrained Minimization
Kikuchi approximation free energy depends marginals p(x )
R. replace minimization exact free energy complete
distribution p(x) minimization Kikuchi free energy
XX
X X
FKikuchi (q) =
q (x ) log (x ) +
c
q (x ) log q (x )
(5)
x

R

156

x

fiEfficient minimization Kikuchi free energy

pseudo-marginals q {q ; R} consistency normalization constraints
q (x ) 0 R x
X

q (x ) = 1 R

(positive)

(6a)

(normalized)

(6b)

(consistent)

(6c)

x

X

q (x ) = q (x ) , R;

x \

Referring class pseudo-marginals satisfying constraints Q,
approximation
log Z min FKikuchi (q) .
qQ

Furthermore, hope pseudo-marginals q (x ) corresponding minimum accurate approximations exact marginals pexact (x ). Kikuchi free
energy corresponding marginals exact Hasse diagram turns singlyconnected (Pakzad & Anantharam, 2005).
2.5 Illustration
illustration main concepts, consider probability model 4 variables
(nodes) pairwise interactions nodes visualized Figure 1(a).
obvious shorthand notation, exact distribution form
1
1
pexact (x) =
ij (xi , xj ) = 12 13 14 23 24 34 .
Z
Z
{i,j}

Note potentials originally defined single nodes always incorporated
definition two-node potentials. region graph corresponding minimal
choice outer regions, i.e., equivalent potential subsets, given Figure 1(b).
outer regions pairs nodes, inner regions subsets single nodes.
fact, case region graph equivalent so-called factor graph (Kschischang,
Frey, & Loeliger, 2001) Kikuchi approximation free energy boils
Bethe approximation:
XX
qij (xi , xj ) log ij (xi , xj )
FKikuchi (q) =
{i,j} xi ,xj

+

XX

qij (xi , xj ) log qij (xi , xj ) +

{i,j} xi ,xj

X
X
(1 ni )
qi (xi ) log qi (xi ) ,


xi

ni = 3 number outer regions containing inner region i.
cluster variation method allows us choose larger outer regions, example,
consisting triples {i, j, k}. redefine factorization potentials
pexact (x) =

1
ijk (xi , xj , xk ) = 123 124 134 234 ,
Z
{i,j,k}

157

fiHeskes

1
x1

x3

EE
EE yyy
EyEy
yy EEE

E
yy

1

1

1

1

1

1, 2 1, 3 1, 4 2, 3
2, 4
3, 4
66 II
II
II
u
u
u
u
II
II
u
66 III
uu
uu
66 III IIII IuIuIuIu
uu
u
66

II uu II uu

IuIu
Iu
6 IIII

uuuII
uu
1
2
3
4

x2

x4

-2

(a) Markov random field.

-2

-2

-2

(b) Hasse diagram Bethe approximation.

1

1

1

1

1, 2, 3I 1, 2, 4I 1, 3, 4I 2, 3, 4

II 66
Iu
Iu

II
uuII
uuII
uuu III uuu III
II 666


II 6


u
u





u
u
II 66

u
u


II
II
uuu
II 6
uu





u

uu


uu

1, 2 1, 3 1, 4 2, 3
2, 4
3, 4
66 II
II
II
u
u
u
u



u
u
II
II uu -1 uu -1
II
-1 666 -1
-1
-1
u
66 IIII IIII uuIuIuII
uu
II
II uuu
II uu
66



uI

II
uuuII
uu
1
2
3
4
1

1

1

1

(c) Region graph Kikuchi approximation.
Figure 1: Region graphs Bethe Kikuchi approximations. Lines nodes
Markov random field (a) indicate edges. region graphs (b) (c),
outer regions drawn highest level. Lines indicate covering
relationship, lower regions covered higher regions. oblique
numbers overcounting numbers follow Moebius formula.
Bethe approximation (b) corresponds minimal approximation
outer regions equivalent cliques graph; pairs nodes.
particular Kikuchi approximation (c) follows taking outer regions
node triples.

158

fiEfficient minimization Kikuchi free energy

example (distribute symmetrically)
1

123 [12 13 23 ] 2

1

124 [12 14 24 ] 2

1

134 [13 14 34 ] 2

1

234 [23 24 34 ] 2 ,
(assign first outer region)
123 12 13 23
124 14 24
134 34
234 1 .
corresponding region graph given Figure 1(c). first-level inner regions
pairs nodes second-level inner regions single nodes, overcounting numbers -1 1, respectively. Kikuchi approximation entropy boils

X
X
X
SKikuchi (q) =
Sijk
Sij +
Si .
{i,j,k}

{i,j}



intuitive reasoning behind approximation follows. sum threenode entropies overcounts two-node interactions (each combination {i, j} appears twice
rather once), therefore discounted once. single-node
interactions much discounted (overcounting number -1 times 3 appearances, compared 3 appearances overcounting number 1 three-node entropies),
yielding overcounting number 1 3 (1) 3 (1) = 1.
2.6 Generalized Loopy Belief Propagation
summarize, finding Kikuchi approximation partition function Z boils
minimization Kikuchi free energy respect set pseudo-marginals
linear constraints them. Introducing Lagrange multipliers constraints,
shown fixed points popular algorithm called loopy belief propagation correspond extrema Bethe free energy and, generally, fixed points generalized
belief propagation extrema Kikuchi free energy (Yedidia et al., 2001). However,
algorithms guaranteed converge minimum practice get stuck
example limit cycles. explains search convergent alternatives directly
minimize Kikuchi free energy, topic rest paper.

3. Convexity Kikuchi Free Energy
section derive sufficient conditions Kikuchi free energy convex
set consistency constraints (6). relevant Kikuchi free
energy indeed convex constraint set, must unique minimum
minimization problem relatively straightforward. Furthermore, argument
159

fiHeskes

use deriving conditions play important role construction efficient
minimization algorithms later on.
3.1 Sufficient Conditions
consider Kikuchi free energy (5) function pseudo-marginals q.
reasoning convexity, disregard energy term linear q.
entropy terms give either convex concave contribution, depending whether
corresponding overcounting numbers positive negative, respectively. Ignoring
constraints (6), free energy (5) convex concave contributions vanish,
i.e., c = 0 R .
However, really care subspace induced constraints (6). Therefore introduce notion convexity set constraints. call free energy
convex set constraints (6)
F (q1 + (1 )q2 ) F (q1 ) + (1 )F (q2 ) 0<<1 q1 ,q2 Q .
Note that, since constraints linear, q1 q2 satisfy constraints (6),
q1 + (1 )q2 . following, talk convexity Kikuchi free
energy, conditioning constraint set implicitly assumed.
One way proceed make use (consistency) constraints express Kikuchi
free energy terms outer region pseudo-marginals study convexity.
approach along lines. particular, replace inner region pseudomarginals correspond concave contributions outer region pseudo-marginals.
pseudo-marginals corresponding convex contributions concern. fact, may
able use convex contributions well compensate concave
contributions.
make reasoning precise, define positive regions (or perhaps better,
nonnegative) R+ , R+ { R; c 0} I+ negative regions R ,
R { R; c < 0} . idea, formulated following theorem,
Kikuchi free energy convex compensate concave contributions
negative regions R convex contributions positive regions R+ .
Theorem 3.1. Kikuchi free energy convex set constraints (6)
exists allocation matrix positive regions R+ negative regions
R satisfying
6= 0

( used compensate )

(7a)

0
X
c

(positivity)

(7b)

(sufficient amount resources)

(7c)

(sufficient compensation)

(7d)

R+



X

|c |

R



160

fiEfficient minimization Kikuchi free energy

Proof First all, note worry energy terms
linear q. words, prove theorem restrict showing
minus entropy


X
X
S(q) =
c (q )
|c |S (q )
R+

R

convex set constraints.
intermediate step, let us consider combination convex entropy contribution
positive region R+ concave entropy contribution negative inner region
R , subset :
X
X
(q) [S (q) (q)] =
q (x ) log q (x )
q (x ) log q (x )
x

=

X

q (x ) log q (x )

=

x

q (x ) log q (x )

x

x

X

x

X



q (x )

X

x\



q (x\ |x ) log q (x\ |x ) ,

used standard definitions
X
q (x )
q (x ) q (x\ |x )
q (x )
.
q (x )
x
\

first step, applied constraint q (x ) = q (x ) extended summation
x second term summation x . second step basically turned
difference two entropies (a weighted sum of) conditional entropies.
difference , depends q , is, Lemma A.1 Appendix A, convex
q . words, concave contribution fully compensated convex
contribution , yielding overall convex term relevant set constraints.
resulting operation matter resource allocation. concave contribution |c |S find convex contributions compensate it. Let denote
amount resources take positive region R+ compensate
negative region R . Obviously, positive region compensate negative regions
contains, = 0 subset , explains condition (7a).
Now, shorthand notation little bit rewriting


X
X
|c |S
c
S(q) =
R+

=

X

R+

=

X

R+



c



c

R

X

+



X



X











X X

R+

161

X

R





X

+



[S ]

X



X

R






|c |

X





|c | .

fiHeskes

P
Convexity first term guaranteed
P c 0 (7c), second term
0 (7b), third term |c | 0 (7d).

3.2 Checking Conditions

Checking conditions Theorem 3.1 cast form linear programming
problem, example follows. define auxiliary variable replacing condition (7c)

X
(8)
= |c | R (variable compensation)


solve linear programming problem attempts maximize single variable constraints implied four conditions. interpretation
try use available resources compensate much concave contributions
can. find solution 1 conditions satisfied: Kikuchi free energy
convex set constraints unique minimum. optimal turns
smaller 1, matrix satisfying constraints convexity
Kikuchi free energy guaranteed Theorem 3.1.
Instead solving linear program, often get away simpler checks.
example, guess particular check whether conditions (7) hold. obvious
choice
X
c
= n
1,

n
R ,


satisfies condition (7c) substituted (7d) yields condition
X

c +

R+ ,

c
0
n


R .

(9)

Similarly, choice
=

X
|c |
+

n

1

n+

R ,
+

satisfies condition (7d) yields condition
X

R ,

c
+ c 0 R+
n+


(10)

substituted (7c). (9) (10) holds, Theorem 3.1 guarantees convexity
Kikuchi free energy.
two conditions sufficient, necessary Theorem 3.1 apply.
necessary condition
X
X
c 0
(11)
c +
R

R+

easily derived summing condition (7d) R substituting condition (7c). condition (11) fails, cannot use Theorem 3.1 prove convexity
Kikuchi free energy.

162

fiEfficient minimization Kikuchi free energy

would like conjecture conditions Theorem 3.1 sufficient,
also necessary convexity Kikuchi free energy. pursue
here, irrelevant current purposes. Furthermore, may
relevant practice either, since convexity sufficient necessary condition unique minimum. Tatikonda Jordan (2002), Heskes (2004), Ihler,
Fisher, Willsky (2005) give conditions convergence loopy belief propagation
uniqueness minimum corresponding Bethe free energy. conditions
depend graphical structure, also (strength the) kernels (x ).
3.3 Related Work
Chiang Forney (2001) present similar ideas, convex entropy terms compensating
concave terms set constraints, derive conditions convexity Bethe
free energy pairwise potentials. resulting conditions formulated terms
single-node marginals, may difficult validate practice generalize
Kikuchi case.
Closely related Theorem 3.1 following theorem Pakzad Anantharam
(2002, 2005).
Theorem 3.2. (Pakzad & Anantharam, 2002, 2005) Kikuchi free energy (5) convex
set consistency constraints imposed collection regions R (and hence
constrained minimization problem unique solution) overcounting numbers c
c satisfy:
X
X
R,
c +
c 0 .
(12)
R\S:
S,



words, subset R, sum overcounting numbers elements
ancestors R must nonnegative.
fact, using Halls (1935) matching theorem, shown conditions (7)
Theorem 3.1 equivalent conditions (12) Theorem 3.2. latter
direct require solution linear program.
Theorem 3.1 Theorem 3.2 used show Bethe free energy
graphs single loop convex set constraints (Heskes, 2004; McEliece &
Yildirim, 2003; Pakzad & Anantharam, 2002, 2005).
3.4 Minimization Convex Kikuchi Free Energy
Kikuchi free energy convex, guaranteed unique minimum,
minimum also relatively easy find message-passing algorithm similar
standard (loopy) belief propagation.
basic idea follows. focus case overcounting numbers
positive. case negative overcounting numbers involved worked
Appendix B. Furthermore, rest paper ignore positivity
constraints (6a). easy check satisfied solutions obtain.
introduce Lagrange multipliers (x ) consistency constraints well

163

fiHeskes

normalization constraints construct Lagrangian


XX
X
L(q, ) = FKikuchi (q) +
q (x )
(x ) q (x )
,


+

X




x \

x

1

X
x



q (x ) .

(13)

Minimization Kikuchi free energy appropriate consistency normalization constraints is, terms Lagrangian, equivalent
min FKikuchi (q) = min max L(q, ) ,
q

qQ



minimization q unconstrained. Standard results constrained
optimization (e.g., Luenberger, 1984) tell us
min max L(q, ) max min L(q, ) ,
q





q

equality convex problems linear equality constraints. is, convex
problems allowed interchange maximum minimum q.
Furthermore, optimal q () corresponding minimum Lagrangian (13)
function unique, since L(q, ) convex q . Substitution solution
yields so-called dual
L () min L(q, ) = L(q (), ) .

(14)

q

dual concave unique maximum.
Many algorithms used find maximum dual (14). particular
one, derived Appendix B, given Algorithm 1. slightly differs presented
Yedidia et al. (2005) Yuille (2002) sending messages (messages directly related
Lagrange multipliers) inner regions outer regions, i.e., never
inner regions subsets inner regions. price one pay update
line 7 depends overcounting number c . Bethe free energy, c = 1 n ,
obtain standard (loopy) belief propagation update rules. particular ordering
Algorithm 1, running inner regions updating messages inner
region neighboring outer regions, guarantees dual (14) increases
iteration1 . local partition functions Z Z lines 10 7 chosen
normalize pseudo-marginals q (x ) q (x ). normalization strictly
necessary, helps prevent numerical instability. Algorithm 1 initialized
setting messages (x ) = 1 skipping lines 3 6 first iteration.
1. positive overcounting numbers c . argumentation negative overcounting numbers
complicated may require damping updates achieve convergence. See Appendix B details.

164

fiEfficient minimization Kikuchi free energy

Algorithm 1 Message-passing algorithm constrained minimization Kikuchi free
energy.
1:

converged

2:



3:
4:

O,
X
q (x ) =
q (x )
x\

5:

(x ) =

q (x )
(x )

6:

end

7:

q (x ) =

8:

O,
q (x )
(x ) =
(x )

1
q (x ) =
(x )
(x )
Z
I,

1
n +c
1
(x )
Z O,


9:
10:



11:

end

12:

end

13:

end

4. Double-Loop Algorithms Guaranteed Convergence
Even Kikuchi free energy convex, still run Algorithm 1
hope converges fixed point. fixed point must correspond
extremum Kikuchi free energy appropriate constraints (Yedidia et al.,
2001). Even better, empirically general Kikuchi free energy provably
Bethe free energy (Heskes, 2003), extremum fact minimum. However, practice
single-loop2 algorithm always converge resort double-loop
algorithms guarantee convergence minimum Kikuchi free energy.
4.1 General Procedure
introduce class double-loop algorithms based following theorem.
2. Note single loop refers message-passing algorithm nothing
notion single loop graphical model.

165

fiHeskes

Theorem 4.1. Given function Fconvex (q; q ) properties
Fconvex (q; q ) FKikuchi (q)

q,q Q

Fconvex (q; q) = FKikuchi (q)
fi
Fconvex (q; q ) fifi
FKikuchi (q)
fi =
q
q
q =q

Fconvex (q; q ) convex q Q

(bound)

(15a)

qQ

(touching)

(15b)

q Q

(convex)

(15c)

algorithm
qn+1 = argmin Fconvex (q; qn ) ,

(16)

qQ

qn pseudo-marginals iteration n, guaranteed converge local minimum
Kikuchi free energy FKikuchi (q) appropriate constraints.
Proof immediate Kikuchi free energy decreases iteration:
FKikuchi (qn+1 ) Fconvex (qn+1 ; qn ) Fconvex (qn ; qn ) = FKikuchi (qn ) ,
first inequality follows condition (15a) (upper bound) second
definition algorithm. gradient property (15b) ensures algorithm
stationary points gradient FKikuchi zero. construction qn Q
n.
See Figure 2 illustration algorithm proof. fact, convexity
Fconvex used establish proof. But, argued Section 3.4,
algorithmic point view constrained minimization convex functional much simpler
constrained minimization non-convex functional. general idea, replacing
minimization complex functional consecutive minimization easier
handle upper bound functional, forms basis popular algorithms
EM algorithm (Dempster, Laird, & Rubin, 1977; Neal & Hinton, 1998) iterative
scaling/iterative proportional fitting (Darroch & Ratcliff, 1972; Jirousek & Preucil, 1995).
Intuitively, tighter bound, faster algorithm.
4.2 Bounding Concave Terms
first step, lay main ideas, build convex bound removing concave
entropy contributions . so, make use linear bound
X
X

q (x ) log q (x )
(17)
q (x ) log q (x ) ,
x

x

directly follows
0

KL(q , q )

=

X
x

"

q (x )
q (x ) log
q (x )

166

#

fiEfficient minimization Kikuchi free energy

(1)
(2)

(3)

Figure 2: Illustration proposed algorithm corresponding convergence proof.
iteration n, Fconvex (q; qn ) (dashed line) convex bound non-convex
FKikuchi (q) (solid line). touch qn , point (1), Fconvex (qn ; qn ) =
FKikuchi (qn ).
minimum, point (2), Fconvex (qn+1 ; qn )
Fconvex (qn ; qn ). corresponding Kikuchi free energy, point (3), obeys
FKikuchi (qn+1 ) Fconvex (qn+1 ; qn ) bounding property.

KL Kullback-Leibler divergence. choice Fconvex reads

X
XX
X
q (x )
(1)

Fconvex (q; q ) =
q (x ) log
q (x ) log q (x )
c
+
(x )
x
x
I+


X
X
X
X
|c | 1
q (x ) . (18)
|c |

q (x ) log q (x ) +




x

x

easy check functional properties (15a) (15c). last term
added fulfill property (15b). Next make crucial observation that, using
(1)
constraints (6) fixed q , rewrite Fconvex normal form (5):

X X
XX
q (x )
(1)
Fconvex
(q; q ) =
q (x ) log
+
c
q (x ) log q (x ) + C(q) , (19)

(x
)


x
x


C(q) evaluates zero q Q , implicitly depends q ,
c defined

X |c |
0

log q (x ) c
.
(20)
log (x ) log (x ) +
c I+
n
,



is, always incorporate terms linear q energy term
redefinition potentials. chosen distribute terms equally
n neighboring outer regions, choices possible well.
167

fiHeskes

term C(q) (19) evaluates zero q Q thus irrelevant
optimization inner loop. consists terms last one (18)
(1)
serve make bound Fconvex satisfy (15b). construction bounds below,
ignore terms: affect algorithm way3 .
(1)
Fconvex convex normal form, use Algorithm 1
solve constrained problem (16). resulting double-loop algorithm described
two lines.
Outer loop: recompute (20) q = qn .
Inner loop: run Algorithm 1 c c, yielding qn+1 .
inner loop, initialize messages converged values previous
inner loop.
4.3 Bounding Convex Terms
section show many cases make algorithm better
simpler. idea bound concave, also convex entropy
contributions inner regions. is, enforce c 0 set


XX
q (x )
(2)

Fconvex (q; q ) =
q (x ) log
,
(21)
(x )
x

log (x ) log (x )

X c
log q (x ) .
n

(22)



(2)

Let us first explain algorithm based Fconvex simpler one based
(21), reference inner regions disappeared. fact, constraints
care outer regions pseudo-marginals agree
intersections. Consequently, inner loop (Algorithm 1), run
inner regions direct intersections outer regions, is,
exist outer regions x = x x . Similar arguments
used algorithm based (19) well, neglecting negative inner regions
correspond direct intersections outer regions. practice, however,
negative inner regions direct intersections outer regions, whereas many positive
inner regions arise next level, intersections intersections. See instance
example Figure 1, six negative inner regions direct intersections outer
regions, contrast four positive inner regions.
(2)
(17), applied positive inner regions, clear Fconvex (q; q )
(1)
(2)
(1)
Fconvex (q; q ): bound, Fconvex tighter bound Fconvex expect
(2)
algorithm based Fconvex perform better. remains shown
(2)
conditions FKikuchi (q) Fconvex (q; q ). following theorem comes in.
(1)
Fconvex .

3. Alternatively, could relax condition (15b) statement gradients Fconvex FKikuchi
equal subspace orthogonal constraints. milder condition, C(q)
well last term (18) longer needed.

168

fiEfficient minimization Kikuchi free energy

Theorem 4.2. functional Fconvex (21) convex bound Kikuchi free energy (5) exists allocation matrix negative inner regions
positive inner regions I+ satisfying
6= 0

( used compensate )

(23a)

0
X
|c |

(positivity)

(23b)

(sufficient amount resources)

(23c)

(sufficient compensation)

(23d)





X

c

I+



Proof surprisingly, proof follows line reasoning proof Theorem 3.1. First consider combination concave entropy contribution
(17) convex entropy contribution I+ , :
X
X

q (x ) log q (x ) +
q (x ) log q (x )
x

x



X

q (x ) log q (x ) +

x

X

q (x ) log q (x ) ,

(24)

x

follows

q (x\ |x )
0
q (x ) q (x\ |x )
q (x\ |x )
x



X
q (x )
q (x ) q (x )
=
q (x )
log
,
(x )
q
(x
)
q
(x
)
q






x
X







recognize term braces Kullback-Leibler divergence two
probability distributions.
(2)
show difference Fconvex FKikuchi nonnegative,
able compensate concave contributions c I+ convex contributions , without exceeding available amount resources
|c |. shorthand notation,
#
"
X
q (x )
K
,
q (x ) log
q (x )
x


decomposition
X
X
X
(2)
Fconvex
FKikuchi =
c K =
c K
|c |K


=

X





|c |

X





K +



X X

I+

(K K ) +



X

I+

169




X





c K 0 ,

fiHeskes

123456
1

2

3

4

5

1237

12457

13467

23567

4567

6

1

2

123
3

1245

1346

2356

456

127

137

237

147

257

457

367

467

567

1 1 1 1 1 1 1 1 1 1 1 1 1 1

4

5

6

12

13

23

14

25

45

36

46

56

17

27

37

47

57

67

1

1

1

1

1

1

1

1

1

1

1

1

1

1

1

7

(a) Outer regions.

7

1

2

3

4

5

6

1

1

1

1

1

1

1

(b) Region graph.
Figure 3: Smallest example showing conditions Theorem 4.2 need always
hold region graph overcounting numbers constructed cluster
variation method. (a) Visualization outer regions: black means
variable (1 7) part outer region (1 6).
(b) Region graph overcounting numbers boldface. positive overcounting numbers third level outweigh negative overcounting numbers second level.

inequality follows since terms guaranteed nonnegative
conditions (23) satisfied.
above, conditions Theorem 4.2 checked linear program.
generated many different sets overcounting numbers resulting Moebius formula (4), started wondering whether conditions (23) perhaps automatically satisfied. However, exhaustively checking possible outer region combinations given fixed
number variables, come counterexample. smallest counterexample
violates conditions Theorem 4.2, illustrated Figure 3.
Even if, counterexample, positive inner regions compensated
negative inner regions, pay get rid many possible. Finding optimal
assignment may complex problem, heuristics easy find (see Appendix C).
4.4 Pulling Tree
(1)

previous section tightened convex bound Fconvex Kikuchi free energy
FKikuchi bounding convex contributions positive regions well. Another way
get tighter bound bound part concave contributions negative

170

fiEfficient minimization Kikuchi free energy

inner regions. first illustrate considering Bethe free energy, i.e.,
non-overlapping negative inner regions (nodes) c = 1 n .
Bethe free energy convex singly-connected structures. Inspired Teh
Welling (2002), choose set nodes Ibound remaining nodes Ifree
become singly-connected take


XX
X
X
q (x )
(3)

Fconvex (q; q ) =
q (x ) log
+
(1 n )
q (x ) log q (x )
(x )
x
x
Ifree
X
X
+
(1 n )
(25)
q (x ) log q (x ) .
Ibound

x

is, bound entropy terms corresponding bounded nodes Ibound
simply keep entropy terms correspond free nodes Ifree . construction
Fconvex satisfies conditions (15). Furthermore, rewritten normal form (5)
definitions

X 1 n
0
Ibound

log (x ) log (x )
.
log q (x ) c
1

n
n
Ifree

,
bound



Note resulting inner-loop algorithm completely equivalent running standard belief propagation tree free nodes: send messages
bounded nodes Ibound well enforce constraints q (x ) = q (x )
, .
Rather pulling single tree, also pull convex combination
trees. is, suppose several bounds, result pulling
particular tree corresponding set overcounting numbers ci .
convex combination
X
X
c =
wi ci wi 0
wi = 1




also corresponds convex bound. generally, combine ideas
previous section choosing c resulting bound convex.
procedure given Appendix C. Basically, first try shield much
concave entropy contributions convex entropy contributions can. Next,
tighten bound incorporating convex contributions linear bounds
concave contributions manage shield first step. steps
cast form easy solve linear programming problem.
4.5 Related Work
(1)

double-loop algorithm described Section 4.2 based Fconvex closely related
Yuilles (2002) CCCP (concave-convex procedure) algorithm. Although originally formulated completely different way, CCCP applied minimization Kikuchi free
energy also understood particular case general procedure outlined
Theorem 4.1. specifically, based bounding concave contributions
X
X
X
|c |
q (x ) log q (x )
q (x ) log q (x ) (|c | 1)
q (x ) log q (x ) , (26)
x

x

x

171

fiHeskes

compared (17). is, bounding concave entropy contributions, part concave terms taken convex side. reason
CCCP algorithm requires functional convex, independent
constraints involved4 . procedure, hand, makes use fact
functional convex set constraints. allows us use
tighter bounds, yielding efficient sometimes simpler algorithms. less important note, inner-loop algorithm particular message-passing scheme applied
Yuille (2002) somewhat different.
(3)
double-loop algorithm based Fconvex (25) inspired Teh Wellings
(2002) UPS (unified propagation scaling) algorithm. difference
bound entropy contributions nodes tree, UPS nodes (and thus
entropy contributions) clamped values resulting previous inner loop.
is, inner loop UPS algorithm corresponds minimizing


XX
X
X
q (x )
UPS

Fconvex (q; q ) =
q (x ) log
+
(1 n )
q (x ) log q (x )
(x )
x
x
Ifree
X
X

(1 n )
q (x ) log q (x ) .
Iclamped

x

constraints
q (x ) = q (x ) Ifree , , yet q (x ) = q (x ) Iclamped , .
boils iterative scaling algorithm, also relatively easy solve.
outer-loop iteration, different choice made Ifree Iclamped . UPS
algorithm understood coordinate descent guaranteed converge
local minimum Bethe free energy (under appropriate conditions choices made
(3)
Ifree Iclamped ). inner loop results Fconvex also allows changes
marginals q (x ) Ibound , i.e., flexible make larger steps. Loosely
(3)
UPS . Furthermore, approach
speaking, Fconvex tighter bound Fconvex
choose different subdivisions bounded free nodes
within inner loop.
Wainwright, Jaakkola, Willsky (2002b, 2002a) present similar ideas, exploiting
convexity Bethe free energy tree structures. Wainwright et al. (2002b) use
tree structure obtain efficient implementation loopy belief propagation, without
however guaranteeing convergence. Wainwright et al. (2002a) show particular convex
combinations convex Bethe free energies lead convex bounds exact Helmholtz
free energy (2). bounds, overcounting numbers inner regions still follow
Moebius relation (4), overcounting numbers outer regions smaller
equal 1. Constrained minimization bound similar constrained
(3)
minimization Fconvex algorithm used Wainwright, Jaakkola, Willsky (2003)
indeed closely related Algorithm 1.
4. procedure described Yuille (2002) often even moves part convex terms concave side.
makes (implicit) bound even worse corresponding algorithm slower. following
stick favorable interpretation CCCP algorithm based implicit
bound (26).

172

fiEfficient minimization Kikuchi free energy

5. Simulations
Intuitively, would expect algorithms based tightest bound converge
fastest terms outer-loop iterations. However, larger steps outer loop,
might need inner-loop iterations achieve convergence inner loop.
following simulations designed check this.
5.1 General Set-up
simulations compare four different algorithms, based different
bound.
convex tightest bound Kikuchi free energy convex. Based
ideas described Section 4.4 Appendix C.
negative zero bound obtained setting negative overcounting numbers
zero, explained Section 4.2.
zero bound described Section 4.3 follows setting overcounting
numbers, negative positive, zero. models considered below,
overcounting numbers satisfy conditions Theorem 4.2, i.e., setting zero
indeed yields bound Kikuchi free energy. Note zero
equivalent negative zero Bethe free energy.
cccp (rather favorable interpretation the) bound implicit Yuilles (2002) CCCP
algorithm, explained Section 4.5.
Algorithm 1 applied inner loop algorithms: difference
setting overcounting numbers c implied bound.
inner loop runs preset convergence criterion met. Specifically, end inner loop
inner region marginals change less 104 . criterion algorithms
happened converge, probably would also case looser criteria.
example, Yuille (2002) reports two inner-loop iterations sufficient obtain
convergence.
simulations report Kullback-Leibler (KL) divergence exact
approximate marginals, either summed nodes subset nodes. Plots
different error functions look much same. Kikuchi/Bethe free energy
somewhat less illustrative: close minimum, marginals
thus KL divergence still change considerably. visualize KL divergence
function outer-loop iterations function floating point operations,
count necessary operations involved inner-loop outer-loop updates (i.e.,
involved convergence checks, computing KL divergence, on).
comparing number inner-loop iterations used different algorithms meet
convergence criterion, scale outer-loop iterations relative outer-loop iterations
convex algorithm. is, number outer-loop iterations used
algorithm reach particular level accuracy, consider corresponding number
outer-loop iterations used convex algorithm reach level.

173

fiHeskes

(a)

(b)
just_convex
negative_to_zero
cccp
kldivergence

kldivergence

just_convex
negative_to_zero
cccp
0

10

2

0

10

2

10

10
0

20

40
60
80
outerloop iterations

100

0

1

2
flops

3

4
6

x 10

Figure 4: Bethe approximation 9 9 Boltzmann grid. Kullback-Leibler divergence
exact approximate single-node marginals function outerloop iterations (a) floating point operations (b) three different algorithms.

done simulations quite number different problems problem instances, involving Markov random fields Bayesian networks. results shown
exemplary meant illustrate general findings summarize
below.
5.2 Bethe Free Energy Boltzmann Grid
first set simulations concerns minimization Bethe free energy Boltzmann grid 9 9 nodes pairwise interactions form


tj
ti
(27)
ij (xi , xj ) = exp wij (2xi 1)(2xj 1) + (2xi 1) + (2xj 1)
ni
nj
ni number neighbors node i, i.e., 2 corner node, 3 nodes
boundary, 4 nodes middle. Weights wij biases ti drawn
random normal distribution mean zero standard deviation 0.5.
Bethe approximation outer regions pairs neighboring nodes.
Figure 4 shows summed KL divergence exact approximate single-node
marginals function number outer loop iterations (a) function
number floating point operations (b) convex, negative zero,
cccp algorithms. seen that, expected, convex algorithms converges
faster negative zero algorithm, converges faster cccp algorithm. speed-up terms outer-loop iterations translates almost equivalent
speed-up terms flops. Indeed, seen Figure 5(a), number inner-loop
iterations required convex algorithm slightly higher
two algorithms.
curves Figure 4(a) mapped onto rough linear scaling
number outer-loop iterations. also suggested straight lines
174

fiEfficient minimization Kikuchi free energy

2

outerloop iterations

10

(b)
number innerloop iterations

(a)
just_convex
negative_to_zero
cccp

1

10

0

10
0
10

8
6
4
2
0

1

just_convex
negative_to_zero
cccp

10
outerloop iterations

10
20
30
40
outerloop iterations (scaled)

Figure 5: Bethe approximation 9 9 Boltzmann grid. (a) Outer loop iterations
convex algorithm versus corresponding outer-loop iterations
two algorithms. (b) Number inner loop iterations needed meet
convergence criterion function outer-loop iterations, scaled according
(a).

Figure 5(a). slope lines relate 0.34, 1 (by definition), 1.35
convex, negative zero cccp, respectively (see also convergence rates
Table 1). following argumentation shows striking correspondence
numbers respective bounds.
negative overcounting numbers
P
Bethe free energy FKikuchi
P add c = 207. respective convex
bounds Fconvex , sums c = 144, 0, 81. translate
fraction negative overcounting mass bounded, i.e.,
P
P
c
c
P
,
c

obtain, respectively 0.30, 1 (by definition), 1.39. is, appears
almost linear relationship tightness bound (here expressed fraction
concave entropy contributions bounded linearly) speed convergence.
noticed almost linear relationship simulations involving
Bethe free energy (no positive overcounting numbers).
5.3 Kikuchi Free Energy Boltzmann Grid

second set simulations also 99 Boltzmann grid, outer regions
chosen squares four neighboring nodes. Potentials form (27)
weights biases drawn normal distribution standard deviation 4 0.5,
respectively. Note size weights much larger previous set
simulations, make problem still bit challenge Kikuchi approximation.
weights, Bethe approximation badly (summed Kullback-Leibler
175

fiHeskes

(a)

(b)

2

2

10
just_convex
negative_to_zero
all_to_zero
cccp

1

10

kldivergence

kldivergence

10

0

10

just_convex
negative_to_zero
all_to_zero
cccp

1

10

0

10

0

200
400
600
outerloop iterations

800

0

5

10
flops

15
7

x 10

Figure 6: Kikuchi approximation 9 9 Boltzmann grid. Kullback-Leibler divergence
exact approximate single-node marginals function outerloop iterations (a) floating point operations (b) four different algorithms.

divergence larger 10). Bethe Kikuchi algorithm, singleloop algorithm convergence problems: Bethe approximation typically gets
stuck limit cycle Kikuchi approximation tends diverge. total
8 8 = 64 outer regions (8 7) 2 = 122 negative inner regions (all node pairs
correspond intersections outer regions) 7 7 = 49 positive inner regions
(all single nodes correspond intersections node pairs).
Figure 6 shows KL divergence approximate exact single-node marginals
four different algorithms terms outer-loop iterations (a) floating point
operations (b). seen ordering (a) expected: tighter
bound, faster algorithm. terms floating point operations, convex
zero algorithm get much closer together.
Part explanation given Figure 7: convex algorithm requires considerably inner-loop iterations meet convergence criterion.
effect zero algorithm inner loop runs 112 negative
inner regions instead 161 positive negative inner regions. makes
inner-loop iteration zero requires factor 1.8 less floating point operations
inner-loop iteration three algorithms.
difficult find quantitative relationship tightness
bounds (asymptotic) convergence rates. One complications
negative, also positive overcounting numbers play role. case,
algorithms still seem converge linearly, faster convergence rates tighter bounds.
convergence rates, expressed time scale corresponding exponential decay
(KL(t) KL() exp[t/ ], outer-loop iterations), summarized
Table 1.

176

fiEfficient minimization Kikuchi free energy

3

outerloop iterations

10

(b)
number innerloop iterations

(a)
just_convex
negative_to_zero
all_to_zero
cccp

2

10

1

10

0

10
0
10

25
20
15
10
5
0

1

10
outerloop iterations

just_convex
negative_to_zero
all_to_zero
cccp

5

10
15
20
25
outerloop iterations (scaled)

Figure 7: Kikuchi approximation 9 9 Boltzmann grid. (a) Outer loop iterations
convex algorithm versus corresponding outer-loop iterations
three algorithms. (b) Number inner loop iterations needed meet
convergence criterion function outer-loop iterations, scaled according
(a).

Figure 8: Graphical structure QMR-like network.
5.4 QMR Network
third set simulations concerns QMR-like (Quick Medical Reference) Bayesian
network (Heckerman, 1989; Jaakkola & Jordan, 1999): bipartite graph layer
disease nodes layer findings. particular network used simulations
generated Bayes Net Toolbox (Murphy, 2001). contains 20 finding nodes,
18 observed (positive), 10 hidden disease nodes; see Figure 8. diseases
Bernoulli probability distributions prior drawn random 0 0.01.
findings noisy-or conditional probability distributions without leakage. Diseases
findings linked randomly probability 0.5. absence leakage, large amount
findings, strong connectivity make relatively difficult inference problem.
outer regions take subsets implied conditional probability distribution, i.e.,
outer region consists disease findings linked it. Figure 9 gives
corresponding region graph.

177

fiHeskes

11111111111111111111111111111111
1 0 0 1 0 0 0 1 1 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
0

0

1

1

0

1

1

1

1

0
2

1

2

2

0

1

1

1

1

1

0

1

1

1

1

1

0

1
1

Figure 9: Region graph resulting QMR-like network.

(a)
just_convex
negative_to_zero
all_to_zero
cccp

10

10

2

10

0

200
400
600
outerloop iterations

just_convex
negative_to_zero
all_to_zero
cccp

0

kldivergence

0

kldivergence

(b)

800

2

10

0

2

4

6
flops

8

10
8

x 10

Figure 10: Kikuchi approximation QMR-like network. Kullback-Leibler divergence
exact approximate single-node marginals function outerloop iterations (a) floating point operations (b) four different algorithms.

178

fiEfficient minimization Kikuchi free energy

(b)
number innerloop iterations

outerloop iterations

(a)
just_convex
negative_to_zero
all_to_zero
cccp

2

10

1

10

0

10
0
10

35

25
20
15
10
5
0

1

10
outerloop iterations

just_convex
negative_to_zero
all_to_zero
cccp

30

5
10
15
outerloop iterations (scaled)

20

Figure 11: Kikuchi approximation QMR-like network. (a) Outer loop iterations
convex algorithm versus corresponding outer-loop iterations
three algorithms. (b) Number inner loop iterations needed meet
convergence criterion function outer-loop iterations, scaled according
(a).

original
convex
negative zero
zero
cccp

-207
-144
0
0
81

Bethe
+

0
0
3.8
0 11.3
0 11.3
0 15.3

Kikuchi
+

-112 49
-64
1
11
0 49
41
0
0
29
112 49 153

-54
-34
0
0
52

QMR
+

35
15
6
35
67
0
17
35 166

Table 1: Summary asymptotic convergence ( time constant, time outerloop iterations, exponential decay) sums negative positive overcounting numbers original Kikuchi/Bethe free energy convex bounds
used different algorithms.

results found Figure 10 11. comparable
Kikuchi approximation Boltzmann grid. Also single-loop algorithm fails
converge. convex algorithm converges much faster three algorithms, requires inner-loop iterations less efficient zero algorithm, makes latter preferable terms floating point operations. However,
relatively straightforward speed-up convex algorithm. First, probably
need many inner-loop iterations outer loop converge properly.
secondly, bound part entropy contribution, efficient choice
would many zero overcounting numbers possible.

179

fiHeskes

5.5 General Findings
summarize points illustrated
encountered many simulations well.
tighter (convex) bound used inner loop, faster convergence
terms outer-loop iterations.
number outer-loop iterations needed meet prespecified convergence criterion tends decrease looser bound, never nearly enough compensate
slower convergence outer loop.
fact, observed strong dependency number inner-loop
iterations tightness bound bound convex problem
hard sense single-loop algorithm would fail converge.
terms floating point operations, looser bound sets overcounting numbers
inner loop zero, beat tighter bound negative overcounting numbers:
slower convergence terms outer-loop iterations compensated
efficient inner loop.
Pelizzola (2005) tests several convergent algorithms Kikuchi approximations problems statistical physics reports similar findings. Also study, convex algorithm, described first time Heskes, Albers, Kappen (2003), clearly outperforms competitors.

6. Discussion
article based perspective interested minima Kikuchi
free energy appropriate constraints. Finding minimum becomes possibly non-convex constrained minimization problem. Here, well studies,
approach solve non-convex problem sequential constrained minimization convex bounds Kikuchi free energy. presumption tighter
bounds yield faster algorithms, worked several ideas construct tight convex
bounds. simulation results article well obtained Pelizzola (2005)
clearly validate presumption show speed-ups significant.
Heskes, Zoeter, Wiegerinck (2004) apply bounds (approximate) parameter
learning directed graphical models.
double-loop algorithms considered article based convex bounds
Kikuchi free energy. principle, necessary: concern
inner-loop algorithm converges might well case tighter bounds. One
practical solution simply choose (tight) bound Kikuchi free, check whether
inner-loop algorithm converge, restart looser bound not. Alternatively,
construct tighter bounds making use conditions guaranteed convergence
belief propagation derived Tatikonda Jordan (2002), Heskes (2004),
Ihler et al. (2005) Bethe approximation.
suggested non-convergence single-loop generalized/loopy belief propagation indication Kikuchi/Bethe approximation inaccurate.
180

fiEfficient minimization Kikuchi free energy

results Section 5.3 5.4 show need always case. Apparently,
exist middle range problems Kikuchi free energy easy minimize, yield decent approximations. problems algorithms
described article useful.

Acknowledgments
author would like thank Wim Wiegerinck, Onno Zoeter, Kees Albers, Bert Kappen fruitful discussions anonymous reviewers constructive comments.
work supported part Dutch Technology Foundation STW.

Appendix A: Convexity Difference Two Entropies
appendix treats two lemmas convexity difference two entropies.
first one used proof Theorem 3.1. similar lemma used McEliece
Yildirim (2003).
Lemma A.1. difference two entropies
X
X
q (x ) log q (x )
q (x ) log q (x )
(q )
x

x

=

X
x

convex q .



q (x )

X

x\



q (x\ |x ) log q (x\ |x )

Proof take step backwards write

(q ) =

X
x





q (x )
.
q (x ) log
X

q (x )

x\

taking derivatives, best interpret table q , specifying value q (x )
possible realization x , vector x playing role index. Taking second
derivatives, obtain
Hx ,x (q )

2 (q )
1
1
.
=
Ix ,x

q (x )q (x )
q (x ) q (x ) x ,x

Ix,x 1 elements x x equal zero otherwise.

181

fiHeskes

Next would like show matrix positive semi-definite, i.e.,
tables q, interpreted vectors indices x ,
0

X

q(x )Hx ,x (q )q(x ) =

x ,x

x


X q 2 (x , x )
X

\

=

q (x\ , x )
x x


\

X q(x )q(x )


q (x )
q (x ) x ,x

x ,x



q(x\ , x )q(x\ , x )

X q 2 (x )

X

x\ ,x\



q (x )


i2
hP



X X q 2 (x\ , x )
x\ q(x\ , x )
=
P
.

q (x\ , x )

x\ q (x\ , x )
x x





\

Cauchys inequality,

X

a2k

k

X

b2k

k

"

X
k

ak bk

#2

,

follows term braces indeed semi-positive q
realization x .
see this, make substitutions x\ k, q(x\ , x )/ q (x\ , x ) ak ,
q
q (x\ , x ) bk find
{. . .}

X
k

a2k

P
2
k ak bk ]
P
0.
2
k bk
[

following related lemma used Appendix B.

Lemma A.2. difference two entropies
X
X
(q , q )
q (x ) log q (x )
q (x ) log q (x )
x

x

convex {q , q }.
Proof Hessian matrix components
Hx ,x



2 (q )
1
=
Ix ,x

q (x )q (x )
q (x )

Hx ,x



2 (q )
1

=


q (x )q (x )
q (x ) x ,x

Hx ,x



q (x )
2 (q )
.

= 2

q (x )q (x )
q (x ) x ,x

182

fiEfficient minimization Kikuchi free energy

Convexity requires q = (q (x ), q (x )),
q (x ) q (x )

0
=

X q2 (x )

2

Hx ,x
Hx ,x

Hx ,x
Hx ,x

X q (x )q (x )

q (x )


X
q (x ) q (x ) 2

.
q (x )
=
q (x ) q (x )
x
x

q (x )



x

+

!

q (x )
q (x )



X q (x )q2 (x )
q2 (x )

x



Appendix B: Minimizing Convex Kikuchi Free Energy
appendix, derive Algorithm 1 minimizing convex Kikuchi free energy
appropriate linear constraints. simplify notation, use convention runs
outer regions, inner regions.
First, note principle necessary explicitly take account
constraints (6), since constraints implied others. Obviously, constraint
two inner region marginals,
q (x ) = q (x ) ,
implied corresponding constraints inner region marginals outer
region subsuming inner regions,
q (x ) = q (x ) q (x ) = q (x ) .
is, take account constraints inner regions
inner regions. Similarly, normalization constraints outer region pseudo-marginals follow
normalization constraints inner region pseudo-marginals. So, sufficient set
constraints
X
q (x )

q (x ) = q (x ) q (x ) =
x\

X

q (x ) = 1

.

x

Introducing Lagrange multipliers (x ) corresponding constraints,
obtain Lagrangian
X X

XX
q (x )
q (x ) log q (x )
+
c
L(q, ) =
q (x ) log
(x )
x
x





XXX
X
X
X
+
(x ) q (x )
q (x ) +
1
q (x ) . (B-1)
x

x\

183



x

fiHeskes

Convex Independent Constraints
Let us first consider case overcounting numbers c strictly positive (c >
0). Then, Lagrangian convex set constraints, convex
q independent constraints. Minimization Lagrangian respect
pseudo-marginals follows setting derivatives zero, yielding

e (x )
(B-2)
q (x ) = (x )e1


q (x )

/c 1

= e



e (x )/c ,

(B-3)



following noted q q functions
Lagrange multipliers . Substituting solution back Lagrangian, obtain
dual
X
XX
X X
L () L(q (), ) =

q (x )
c
q (x ) .
(B-4)




x



x

Now, consider optimizing L () respect subset components corresponding
inner region , collected ( , (x ) ,x ), keeping
6= fixed. concavity dual L (), find maximum
direction setting corresponding derivatives zero. yields
fi
L () fifi
= qnew (x ) qnew (x ) = 0 x ;
(x ) fi=new
fi
X
L () fifi
=
1

qnew (x ) = 0 ,
(B-5)
fi=new
x
q new refers solution (B-2) (B-3) (x ) replaced new
(x )
new
.
Since

(B-2)

new

qnew (x )

e (x )
= (x ) q (x ) ,
e

solution new
(x ) must obey
new
new
(x ) = log q (x ) + (x ) + log q (x ) ,

still solve qnew (x ). Summing expression , substituting (B-3), solving qnew (x ) get
log qnew (x ) =

X
1
1
[log q (x ) (x )] +
(new c ) .
n + c
n + c


Now, obtain exactly updates Algorithm 1 define
(x ) = e (x ) (x ) = q (x )e (x ) ,
184

fiEfficient minimization Kikuchi free energy

properly normalize q (x ), line 7. normalization q (x ) line 10
fact unnecessary, since construction updates ensure q (x ) = q (x )
Z = 1.
bottom line particular ordering Algorithm 1 joint update
messages particular subset interpreted coordinate-wise gradient
ascent dual L (), updating Lagrange multipliers (x ) particular
time. Therefore Algorithm 1 guaranteed converge
unique maximum case positive overcounting numbers c .
Convex Set Constraints
Next, let us consider general case (some of) overcounting numbers
negative, Kikuchi free energy still convex set constraints.
consider case inner region overcounting numbers negative5 .
show that, sufficient damping updates, Algorithm 1 still guaranteed
converge unique minimum Kikuchi free energy set constraints.
Note direct application argumentation fails, solution (B-3)
q (x ) negative c corresponds maximum rather minimum. Consequently, dual L () (B-4) need concave. updates Algorithm 1
follow setting derivatives zero interpreted fixed-point iterations, coordinate ascent L (). Still, practice seem work fine indeed without
always increasing L (). following explain why: argue updates Algorithm 1 correspond coordinate ascent, rather something like
coordinate descent-ascent convex-concave saddle function. sufficient damping,
algorithm converge unique saddle point, corresponds
minimum Kikuchi free energy set constraints.
Convexity set
P according Theorem 3.1, exists
P constraints implies,
matrix = |c | 1. Using q (x ) = q (x ), replace
Lagrangian (B-1)

XX
XX
X
q (x )
L(q, ) =
q (x ) log
q (x ) log q (x )


(x )
x
x





XXX
X
X
X
X
1
+
(x )
q (x )
q (x ) +
1
q (x ) . (B-6)
n
x
x
x








\



since, Lemma A.2 Appendix A,
X
X
q (x ) log q (x )
q (x ) log q (x )
x

x

convex {q (x ), q (x )}, Lagrangian (B-6) indeed convex q independent
constraints. Thus could apply argumentation above: find minimum
5. argumentation hold negative inner region entropy contributions
compensated positive inner region subset entropy contributions prove convexity Kikuchi free
energy. case, might need slightly different algorithm guarantee convergence.

185

fiHeskes

convex Lagrangian respect q, substitute corresponding solution q () back
Lagrangian obtain concave dual L (), maximize dual respect
. problem closed-form expression optimal q ()
thus also closed-form expression dual L (), makes procedure
rather awkward.
Instead, distinguish outer region marginals, collected qO ,
inner region marginals, collected qI . rewritten consistency constraint terms
outer region marginals alone, replace constrained minimization respect
qO unconstrained maximization respect corresponding Lagrange multipliers
, leaving minimization respect qI normalization constraint
is. gives us saddle-point problem type minqI maxO . Even without explicitly
writing equations, tell maximization respect particular
corresponds finding
qnew (x ) = qnew
(x )

, .

Then, minimization respect q given fixed qnew (x ) immediately yields
X
qnew (x ) ,
qnew (x )


properly normalized sum 1. exactly updates particular inner
region Algorithm 1 amount to: yield unique maximum respect
minimum respect q , keeping q 6= fixed.
coordinate descent-ascent procedure works fine saddle function convex minimizing parameter concave maximizing parameter (e.g., Seung,
Richardson, Lagarias, & Hopfield, 1998). concavity immediate, convexity
qI follows convexity Lagrangian (B-6) q = (qO , qI ): minimizing
overall convex function parameters, qO , yields convex function
remaining parameters, qI . Technically, convergence unique solution
saddle-point problem proven construction Lyapunov function
decreases infinitesimal updates parameters descent ascent direction
zero unique saddle point (Seung et al., 1998). Convergence guaranteed
sufficiently damped updates, full ones Algorithm 1. Empirically full updates, correspond full maximization minimization one inner region
moving next one, work fine cases, occasionally indeed require little
damping. Wainwright et al. (2003) successfully apply damping similar algorithm
attempt minimize convexified Bethe free energy.

Appendix C: Constructing Tight Convex Bound
appendix, describe procedure constructing tight convex bound Fconvex
Kikuchi free energy FKikuchi . combines ideas Section 4.3 4.4. is,
first convexify Kikuchi free energy, bounding little concave contributions
negative inner regions possible. Next, terms bound anyways,
try incorporate many convex contributions can. leads following
procedure.
186

fiEfficient minimization Kikuchi free energy

Consider minus entropy
=


X

+





X

c +

X

c

,



I+






choose c c first term



X
X

X
X
c
c +
(c c )S ,
=
+



I+





(just) convex.

corresponding allocation matrix Theorem 3.1, define used resources
X
|c | c ,
c


rewrite
=


X


+





c +




X


X

X

c

I+

(c c )S +

X

I+



construction, first term still convex.





(c c )S




.



guarantee convexity, bound entropy contributions second
term . make bound tighter, include many convex
contributions can, still satisfying conditions Theorem 4.2. Call
corresponding overcounting numbers c c c c put remaining
c c back first term:


X

X
X
c
=
c +
+


I+



X

X

(c c )S .
(c c )S +


I+



Choose Fconvex first term plus linear bound second term.

find c first step similarly c third, use linear program
similar one described Section 3.2 checking conditions Theorem 3.1.
introduce slack variables replace condition (7d)
X
= (variable compensation) ,


187

fiHeskes

similar spirit (8). Furthermore, add inequality constraints |cP
|
(no need compensate |c |) search maximum
(compensate much possible). terms corresponding solution , set c =
c .

References
Aji, S., & McEliece, R. (2001). generalized distributive law free energy minimization. Proceedings Allerton Conference Communication, Control,
Computing.
Besag, J. (1974). Spatial interaction statistical analysis lattice systems. Journal
Royal Statistical Society Series B, 36, 192236.
Chiang, M., & Forney, G. (2001). Statistical physics, convex optimization sum
product algorithm. Tech. rep., Stanford University.
Darroch, J., & Ratcliff, D. (1972). Generalized iterative scaling. Annals Mathematical
Statistics, 43, 14701480.
Dechter, R., Kask, K., & Mateescu, R. (2002). Iterative join-graph propagation. Darwiche, A., & Friedman, N. (Eds.), Proceedings UAI-2002, pp. 128136.
Dempster, A., Laird, N., & Rubin, D. (1977). Maximum likelihood incomplete data
via EM algorithm. Journal Royal Statistical Society B, 39, 138.
Hall, P. (1935). representatives subsets. Journal London Mathematical Society,
10, 2630.
Heckerman, D. (1989). tractable inference algorithm diagnosing multiple diseases.
Kanal, L., Henrion, M., Shachter, R., & Lemmer, J. (Eds.), Proceedings Fifth
Workshop Uncertainty Artificial Intelligence, pp. 163171, Amsterdam. Elsevier.
Heskes, T. (2003). Stable fixed points loopy belief propagation minima Bethe
free energy. Becker, S., Thrun, S., & Obermayer, K. (Eds.), Advances Neural
Information Processing Systems 15, pp. 359366, Cambridge. MIT Press.
Heskes, T. (2004). uniqueness loopy belief propagation fixed points. Neural
Computation, 16, 23792413.
Heskes, T., Albers, K., & Kappen, B. (2003). Approximate inference constrained optimization. Uncertainty Artificial Intelligence: Proceedings Nineteenth
Conference (UAI-2003), pp. 313320, San Francisco, CA. Morgan Kaufmann Publishers.
Heskes, T., Zoeter, O., & Wiegerinck, W. (2004). Approximate Expectation Maximization. Thrun, S., Saul, L., & Scholkopf, B. (Eds.), Advances Neural Information
Processing Systems 16, pp. 353360, Cambridge. MIT Press.
Ihler, A., Fisher, J., & Willsky, A. (2005). Loopy belief propagation: Convergence
effects message errors. Journal Machine Learning Research, 6, 905936.
Jaakkola, T., & Jordan, M. (1999). Variational probabilistic inference QMR-DT
network. Journal Artificial Intelligence Research, 10, 291299.
188

fiEfficient minimization Kikuchi free energy

Jirousek, R., & Preucil, S. (1995). effective implementation iterative proportional fitting procedure. Computational Statistics Data Analysis, 19, 177189.
Jordan, M., Ghahramani, Z., Jaakkola, T., & Saul, L. (1998). introduction variational
methods graphical models. Jordan, M. (Ed.), Learning Graphical Models,
pp. 183233. Kluwer Academic Publishers, Dordrecht.
Kikuchi, R. (1951). theory cooperative phenomena. Physical Review, 81, 9881003.
Kschischang, F., Frey, B., & Loeliger, H. (2001). Factor graphs sum-product algorithm. IEEE Transactions Information Theory, 47 (2), 498519.
Lauritzen, S. (1996). Graphical models. Oxford University Press, Oxford.
Luenberger, D. (1984). Linear Nonlinear Programming. Addison-Wesley, Reading,
Massachusetts.
McEliece, R., MacKay, D., & Cheng, J. (1998). Turbo decoding instance Pearls
belief propagation algorithm. IEEE Journal Selected Areas Communication,
16 (2), 140152.
McEliece, R., & Yildirim, M. (2003). Belief propagation partially ordered sets.
Gilliam, D., & Rosenthal, J. (Eds.), Mathematical Systems Theory Biology, Communications, Computation, Finance, pp. 275300. Springer, New York.
Murphy, K. (2001). Bayes Net toolbox Matlab. Computing Science Statistics,
33, 331350.
Murphy, K., Weiss, Y., & Jordan, M. (1999). Loopy belief propagation approximate
inference: empirical study. Laskey, K., & Prade, H. (Eds.), Proceedings
Fifteenth Conference Uncertainty Articial Intelligence, pp. 467475, San
Francisco, CA. Morgan Kaufmann Publishers.
Neal, R., & Hinton, G. (1998). view EM algorithm justifies incremental,
sparse, variants. Jordan, M. (Ed.), Learning Graphical Models, pp.
355368. Kluwer Academic Publishers, Dordrecht.
Pakzad, P., & Anantharam, V. (2002). Belief propagation statistical physics. 2002
Conference Information Sciences Systems, Princeton University.
Pakzad, P., & Anantharam, V. (2005). Estimation marginalization using Kikuchi approximation methods. Neural Computation, 17, 18361873.
Pearl, J. (1988). Probabilistic Reasoning Intelligent systems: Networks Plausible Inference. Morgan Kaufmann, San Francisco, CA.
Pelizzola, A. (2005). Cluster variation method statistical physics graphical models.
Journal Physics A, 38, R309R339.
Seung, S., Richardson, T., Lagarias, J., & Hopfield, J. (1998). Minimax Hamiltonian
dynamics excitatory-inhibitory networks. Jordan, M., Kearns, M., & Solla, S.
(Eds.), Advances Neural Information Processing Systems 10, pp. 329335. MIT
Press.
Tatikonda, S., & Jordan, M. (2002). Loopy belief propagation Gibbs measures. Darwiche, A., & Friedman, N. (Eds.), Uncertainty Artificial Intelligence: Proceedings
189

fiHeskes

Eighteenth Conference (UAI-2002), pp. 493500, San Francisco, CA. Morgan
Kaufmann Publishers.
Teh, Y., & Welling, M. (2002). unified propagation scaling algorithm. Dietterich,
T., Becker, S., & Ghahramani, Z. (Eds.), Advances Neural Information Processing
Systems 14, pp. 953960, Cambridge. MIT Press.
Wainwright, M., Jaakkola, T., & Willsky, A. (2002a). new class upper bounds log
partition function. Darwiche, A., & Friedman, N. (Eds.), Uncertainty Artificial
Intelligence: Proceedings Eighteenth Conference (UAI-2002), pp. 536543, San
Francisco, CA. Morgan Kaufmann Publishers.
Wainwright, M., Jaakkola, T., & Willsky, A. (2002b). Tree-based reparameterization
approximate estimation loopy graphs. Dietterich, T., Becker, S., & Ghahramani,
Z. (Eds.), Advances Neural Information Processing Systems 14, pp. 10011008,
Cambridge. MIT Press.
Wainwright, M., Jaakkola, T., & Willsky, A. (2003). Tree-reweighted belief propagation
algorithms approximate ML estimation via pseudo-moment matching. Bishop,
C., & Frey, B. (Eds.), Proceedings Ninth International Workshop Artificial
Intelligence Statistics. Society Artificial Intelligence Statistics.
Yedidia, J., Freeman, W., & Weiss, Y. (2001). Generalized belief propagation. Leen,
T., Dietterich, T., & Tresp, V. (Eds.), Advances Neural Information Processing
Systems 13, pp. 689695, Cambridge. MIT Press.
Yedidia, J., Freeman, W., & Weiss, Y. (2005). Constructing free energy approximations
generalized belief propagation algorithms. IEEE Transactions Information
Theory, 51, 22822312.
Yuille, A. (2002). CCCP algorithms minimize Bethe Kikuchi free energies:
Convergent alternatives belief propagation. Neural Computation, 14, 16911722.

190

fiJournal Artificial Intelligence Research 26 (2006) 417-451

Submitted 11/05; published 08/06

Multiple-Goal Heuristic Search
Dmitry Davidov
Shaul Markovitch

dmitry@cs.technion.ac.il
shaulm@cs.technion.ac.il

Computer Science Department
Technion, Haifa 32000, Israel

Abstract
paper presents new framework anytime heuristic search task
achieve many goals possible within allocated resources. show inadequacy
traditional distance-estimation heuristics tasks type present alternative
heuristics appropriate multiple-goal search. particular, introduce
marginal-utility heuristic, estimates cost benet exploring subtree
search node. developed two methods online learning marginal-utility
heuristic. One based local similarity partial marginal utility sibling nodes,
generalizes marginal-utility state feature space. apply adaptive
non-adaptive multiple-goal search algorithms several problems, including focused
crawling, show superiority existing methods.

1. Introduction
Internet search engines build indices using brute-force crawlers attempt scan
large portions Web. Due size Web, crawlers require several weeks
complete one scan, even using high computational power bandwidth (Brin
& Page, 1998; Douglis, Feldmann, Krishnamurthy, & Mogul, 1997), still leave
large part Web uncovered (Lawrence & Giles, 1998; Najork & Wiener, 1998). Many
times, however, necessary retrieve small portion Web pages dealing
specic topic satisfying various user criteria. Using brute-force crawlers task
would require enormous resources, would wasted irrelevant pages.
possible design focused crawler would scan relevant parts Web
retrieve desired pages using far fewer resources exhaustive crawlers?
Since Web viewed large graph (Cooper & Frieze, 2002; Kumar, Raghavan,
Rajagopalan, Sivakumar, Tomkins, & Upfal, 2000; Pandurangan, Raghavan, & Upfal, 2002),
pages nodes links arcs, may look solution problem
eld heuristic graph-search algorithms. quick analysis, however, reveals
problem denition assumed designers heuristic search algorithms inappropriate
focused crawling, uses entirely dierent setup. crucial dierence
heuristic search focused crawling success criterion. setups set
goal states. heuristic search setup, however, search completed soon
single goal state found, focused crawling setup, search continues reach
many goal states possible within given resources.
Changing success criterion existing search algorithms enough. informed search algorithms based heuristic function estimates distance
node nearest goal node. heuristics usually appropriate multiplec
2006
AI Access Foundation. rights reserved.

fiDavidov & Markovitch

goal search. Consider search graph described Figure 1. grey area expanded
Start

B


Figure 1: Using distance-estimation heuristic multiple-goal search problem
graph. Assume evaluate nodes B using distance-based heuristic. Node
better heuristic value therefore selected. indeed right decision
traditional search task nd one goal. multiple-goal search, however,
B looks like much promising direction since leads area high density
goal nodes.
many problems wide variety domains interested nding
set goals rather single goal. genetic engineering, example, want
nd multiple possible alignments several DNA sequences (Yoshizumi, Miura, & Ishida,
2000; Korf & Zhang, 2000). chemistry may want nd multiple substructures
complex molecule. robotics, may want plan paths multiple robots access
multiple objects. cases, possible solution would invoke single-goal
search multiple times. approach, however, likely wasteful, resourcebounded computation, may wish exploit multiple-goal1 nature problem
make search ecient.
specic multiple-goal task focused crawling received much attention (Chakrabarti,
van den Berg, & Dom, 1999; Cho & Garcia-Molina, 2000; Cho, Garca-Molina, & Page, 1998;
Diligenti, Coetzee, Lawrence, Giles, & Gori, 2000; Rennie & McCallum, 1999)
popularity Web domain. works, however, focused Web-specic
techniques tailored particular problem crawling.
goal research described paper establish new domain-independent
framework multiple-goal search problems develop anytime heuristic algorithms
solving eciently. framework focuses mainly problem domains
looking goal states paths goals either irrelevant, cost
concern (except eect search cost).
1. Note term multiple-goal also used planning domain. There, however, task
satisfy set dependent goals possible order constraints.

418

fiMultiple-Goal Heuristic Search

start formal denition multiple-goal search problem. describe
versions existing heuristic search algorithms, modied multiple-goal framework.
main dierences single-goal multiple-goal search heuristic functions
used. describe new set heuristics better suited multiple-goal search.
particular, introduce marginal-utility heuristic, considers expected costs
well expected benets associated search direction. dicult
specify heuristics explicitly. therefore present adaptive methods allow online
learning them. Finally describe extensive empirical study algorithms
various domains, including focused crawling problem.
contributions paper fourfold:
1. identify dene framework multiple-goal heuristic search. novel
framework heuristic search.
2. dene set search algorithms heuristics appropriate multiplegoal search problems.
3. dene utility-based heuristic present method automatic acquisition
via online learning.
4. provide extensive empirical study presented methods various domains
show superiority existing general algorithms.

2. Multiple-Goal Search Problem
Let S, E potentially innite state graph nite degree, set states
E set edges. single-goal search problem dened follows:
1. Input:
set initial states Si
successor function Succ : 2S Succ(s) = {s | s, E}.
(Sometimes Succ given implicitly nite set operators O.)
goal predicate G : {0, 1}. denote Sg = {s | G(s)}. Sometimes Sg
given explicitly.
2. Search objective: Find goal state g Sg directed path S, E
state Si g. Sometimes also interested path itself.
3. Performance evaluation: Although performance evaluation criterion commonly considered integral part problem denition, consider
such. determines class algorithms considered.
common criteria evaluating search solution quality, usually
measured solution path cost, search eciency, mostly evaluated
resources consumed search.
Although multiple-goal search problem bears similarity single-goal search
problem, diers several ways:
419

fiDavidov & Markovitch

1. Input: Includes additional resource limit R. simplicity presentation
assume R given us number generated nodes2 . Later discuss
assumption.
2. Search objective: Find set goal states SgR Sg satises:
SgR , directed path S, E si Si s.
search resources consumed exceed R.
Sometimes also interested set corresponding paths.






3. Performance evaluation: SgR . Obviously, higher values considered better.
looks mix reasoning meta reasoning inserting resource
limit part problem input, many problems much naturally dened
resource limitation. Consider, example, minimax algorithm, maximal
depth search (which determines resources consumed) given part algorithms
input. formulation, resource limit given input, falls
scope problems solved anytime algorithms (Boddy & Dean, 1994; Hovitz, 1990;
Zilberstein, 1996) specically contract algorithms (Russell & Zilberstein, 1991;
Zilberstein, Charpillet, & Chassaing, 1999).
Sometimes resource limit known advance. setup, search
algorithm interrupted time required return current set collected goals. type problem solved interruptible anytime algorithms (Hansen
& Zilberstein, 1996; Russell & Zilberstein, 1991). alternative formalization require
algorithm nd specied number goals evaluate performance
resources consumed search.

3. Multiple-Goal Heuristic Search Algorithms
Assume hmg : heuristic function estimates merit states
respect objective performance evaluation criterion multiple-goal search
problem dened previous section.
objective develop search algorithms exploit heuristics similar
manner heuristic search algorithms developed single-goal search problems.
start describing multiple-goal version greedy best-rst search3 .
two main dierences existing single-goal best-rst search algorithm newly dened multiple-goal version:
1. single-goal best-rst search stops soon encounters goal state,
multi-goal version collects goal continues allocated resources
exhausted.
2. framework therefore applicable resource proportional number generated
nodes. CPU time, internet bandwidth, energy consumption robots, etc. best-first
search algorithms, number also corresponds memory consumption.
3. follow Russell Norvig (2003, page 95) use term describe search algorithm
always expands node estimated closest goal.

420

fiMultiple-Goal Heuristic Search

2. single-goal best-rst search typically uses heuristic function tries
approximate distance nearest goal, multiple-goal version use
dierent type heuristic appropriate multiple-goal task.
heuristic search algorithms also modied nd multiple goals.
algorithm Pearl Kim (1982) converted handle multiple goal search
collecting goal nds focal list. goals optimal paths.
multiple-goal heuristic used select node focal list expansion.
algorithm stops, before, allocated resources exhausted. addition,
stop algorithm nodes focal satisfy f (n) > (1 + )gmin , gmin
minimal g value among collected goals.
Multiple-goal hill-climbing uses multiple-goal heuristic choose best direction.
modify algorithm allow search continue goal found. One possible
method continuing search perform random walk found goal.
Multiple-goal backtracking works similarly single goal version. However,
goal encountered, algorithm simulates failure therefore continues. multiple goal
heuristic used ordering operators node. constraint satisfaction,
means ordering values associated variable.

4. Heuristics Multiple-Goal Problems
introduction, illustrated problem using traditional distance-estimation
heuristic multiple-goal search. One main problems using distanceestimation heuristic take account goal density distance
nearest goal. lead search relatively futile branch,
left branch Figure 1, rather much fruitful right branch. section
consider several alternative heuristic functions appropriate multiple-goal
search.
4.1 Perfect Heuristic
describe analyze heuristic functions multiple-goal search, would like
consider function trying approximate. Assume, example,
perform multiple-goal greedy best-rst search perfect knowledge
search graph. node would like heuristic select next? Assume given
resource limit allows us expand additional nodes, look search forests4
size rooted current list open nodes. perfect heuristic select node
belonging forest largest number goals.
Definition 1 Let Sopen set currently open states. Let R resource limit
Rc resources consumed far. Let Sgf Sg set goals found far. Let
F set possible forests size R Rc starting roots Sopen . forest f F
optimal




f F, (Sg \ Sgf ) f |(Sg \ Sgf ) f | .
4. search space graph, search algorithm expands forest currently open nodes.

421

fiDavidov & Markovitch

state Sopen optimal, denoted OP (s), exists optimal
forest f f .
Definition 2 heuristic function h perfect respect multiple-goal search problem
every possible search stage dened Sopen ,
s1 , s2 Sopen [OPT(s1 ) OPT(s2 ) = h(s1 ) < h(s2 )] .
Thus perfect heuristic never selects expansion state optimal
forest. Using heuristic multiple-goal best-rst search make search optimal.
Note optimality respect search resources respect
cost paths leading goal states.
Obviously, denition lead practical multiple-goal heuristic. Even
simple problems, even perfect knowledge graph, calculating
heuristic hard. number possible forests exponential
resource limit number nodes open list.
4.2 Sum Heuristics
Many search algorithms look single goal state use heuristic function estimates cost cheapest path goal state. Optimizing algorithms
require admissible heuristics (that underestimate real distance goal) satiscing search algorithms, greedy best-rst, use non-admissible heuristics well.
Distance-estimation heuristics therefore developed many domains. addition,
several researchers developed automatic methods inferring admissible heuristics
relaxation (Held & Karp, 1970; Mostow & Prieditis, 1989; Prieditis, 1993) pattern
databases (Culberson & Schaeer, 1998; Gasser, 1995; Korf & Felner, 2002).
Figure 1 illustrates straightforward use distance heuristics multiple-goal
search appropriate. dene method utilizing (admissible nonadmissible) distance heuristics multiple-goal search. Assume set goal states,
Sg , given explicitly, given common distance-estimation heuristic
hdist (s1 , s2 ) estimates graph distance two given states5 . sum-ofdistances heuristic, denoted hsum , estimates sum distances goal set:
hsum (s) =



hdist (s, g).

(1)

gSg

Minimizing heuristic bias search towards larger groups goals, thus selecting
node B example Figure 1. indeed better decision, provided enough
resources left reaching goals subgraph B.
4.3 Progress Heuristics
One problem sum heuristic tendency try progress towards
goals simultaneously. Hence, groups goals scattered around search front,
states around front similar heuristic values. step reduces
5. Assume Euclidean distance figure reflects heuristic distance.

422

fiMultiple-Goal Heuristic Search

distances increases others, leading more-or-less constant sum. constantsum regions, algorithm relies sum heuristic information
node choose expansion. Even distinct groups

Figure 2: behavior sum heuristic vs. progress heuristic. solidline ellipse indicates area covered search using sum heuristic.
dotted-line ellipse marks area searched using progress heuristic.

goals dierent directions, sum heuristic may lead simultaneous progress towards
groups. two groups goal states shown Figure 2 illustrate problem.
sum heuristic strategy work enough enough resources reach
groups. If, however, resources sucient, sum heuristic may waste
available resources trying progress towards groups, reaching none.
avoid problems, dene progress heuristic, takes account
number goals towards progress made average distance them. Thus,
instead trying pursue multiple groups goals, heuristic pursue one group
time, preferring less distant groups.
Let Sopen set currently opened states. before, assume
explicit goal list, Sg = g1 , . . . , gk , distance-estimation heuristic, hdist . Let
mi = minsSopen hdist (s, gi ) minimal estimated distance search frontier
goal gi . Sopen dene Gp (s) = {gi Sg | hdist (s, gi ) = mi } set
goals estimated closest among states thesearch frontier.
gG (s)

hdist (s,g)

p
average estimated distance states Gp Dp (s) =
|Gp (s)|
average distance s. interested states many members
Gp small average distance. Hence, dene progress heuristic

hprogress (s) =

Dp (s)
.
|Gp (s)|

(2)

Minimizing heuristic direct search larger closer groups goals towards
progress made. example, simple space shown Figure 2, progress
heuristic advance correctly, towards group left side indicated dashed
ellipse. Note although right group larger, progress heuristic nonetheless
prefers left one smaller distance. Since progress heuristic considers
goal exactly once, misled multiple paths goal.

423

fiDavidov & Markovitch

4.4 Marginal-Utility Heuristics
comparing two search directions, far considered concentration
goal states, preferring directions lead larger groups goals. Thus, former
heuristics would considered node node B Figure 3 equivalent.
reasoning, however, take account resources invested reach set
goals. example Figure 3, clear visiting set goals node B
requires less search resources node A. Therefore node B preferable.
Start

B


Figure 3: Searching node result number goals searching
node B. consume, however, far greater resources.
account cases suggesting another approach multiple-goal heuristics,
one considers expected benet search also expected cost.
Obviously, prefer subgraphs cost low benet high. words,
would like high return resource investment. call heuristic tries
estimate return marginal-utility heuristic.
Assume (for now) search space S, E tree. Let (s) set states
reachable s. Let Tg (s) = (s) Sg set goal states reachable
s. Let Sv set states visited completed search process. dene
marginal utility state Sv respect Sv
MU(s) =

|Tg (s) Sv |
.
|T (s) Sv |

(3)

Thus, MU measures density goal states subtree expanded search process.
One possible good search strategy select states eventually yield high
values U respect Sv . search process consumed Rc R resources
424

fiMultiple-Goal Heuristic Search

far, largest tree visited size r = R Rc . Section 4.1 dene
perfect heuristic considering possible ways distributing r among open
nodes. approach obviously impractical, take greedy approach
instead. look heuristic function hM U (s, r) tries estimate best marginal
utility s, assuming remaining resources, r, consumed exploring
(s). Let (s, r) set trees size r root s. hM U (s, r) tries estimate
resource-bounded marginal utility, dened
|Tg (s) |
.
r
(s,r)

MU(s, r) = max

(4)

MU(s, r) measures best ratio number goals achieved search
resources used it. Naturally, dicult build heuristics estimate marginal
utility accurately. following sections show heuristics learned.
4.5 Additional Considerations
One possible side-eect stopping discovering goals continuous inuence
already discovered goals search process. found goals continue attract
search front, would preferable search progress towards
undiscovered goals. explicit set goal states given - sum progress
heuristics - disable inuence visited goals simply removing
set. set features states given instead, reduce eect visited goals
preferring nodes farther feature space. Specically, let d(n)
minimal distance n members set visited goals, let h(n)
multiple-goal heuristic value n. modied heuristic h (n) = h(n)(1+c1 ec2 d(n) )
c1 c2 parameters determine magnitude eect d(n).
second term penalty add heuristic value. penalty decays exponentially
distance visited goals.
Note tension tendency search dense groups goals
tendency push search away visited goals. groups goals
dense, method detrimental nding goals
group reduces tendency pursue goals group. eect
controlled ci parameters. domains high goal density, ci set
lower values. Hence, values set dynamically search, according
measurements goal density explored graph.
One problem using marginal utility heuristic non-tree graphs possible
overlap marginal utility. means search algorithm might pursue set
goals dierent directions. One way overcome problem try diversify
search progress measuring feature-based average distance best
nodes set recently expanded nodes, prefer maximal diversity
nodes explored. gives maximal diversity exploration directions
minimize expected overlap visited subtrees.

425

fiDavidov & Markovitch

5. Learning Marginal Utility Heuristics
quite possible marginal-utility heuristics supplied user,
many domains heuristics dicult design. use learning approach
acquire marginal-utility heuristics online search. present two
alternative methods inferring marginal utility. One approach estimates marginal
utility node based partial marginal utility siblings. approach
predicts marginal utility using feature-based induction.
5.1 Inferring Marginal Utility Based Marginal Utility Siblings
rst approach predicting marginal utility based assumption sibling
nodes similar marginal utility. dene partial marginal utility state
step executing multiple-goal search algorithm number goals found far
subtree divided number states visited far subtree. Thus,
Sv (t) set states visited step t, partial marginal utility dened
U (s, t) =

|Tg (s) Sv (t)|
.
|T (s) Sv (t)|

(5)

method estimates marginal utility siblings based partial marginal
utility, marginal utility node based average estimated marginal utility
siblings.
discussed previous subsection, expected marginal utility node strongly
depends resources invested exploring it. Thus, learn heuristic hM U (s, r),
one would need estimate partial marginal utility values dierent values r. One
way reducing complexity two-dimensional estimation divide two
stages: estimating depth tree searchable within r resources,

U depth (s, d)
estimating marginal utility predicted depth. Thus,
estimated marginal utility searching node depth d, compute estimated
r)),


U resources(s, r) =
U depth (s, d(s,
marginal utility node using r resources

d(s, r) estimated depth searching node using r resources.
following subsections show values estimated.
5.1.1 Updating Partial Marginal-Utility Values
maintain node two vectors counters, parameter limits
maximal lookahead partial marginal utility. One vector, N (n), stores current
number visited nodes node n depth 1, . . . , D, Ni (n) contains current
number visited nodes n depth less equal i. vector, G(n)
holds similarly number visited goals.
Whenever new node n generated, appropriate entries ancestors N vectors
incremented. n goal, G vectors updated well. p ancestor
n connected path length l D, Nl (p), . . . ND (p) incremented
one. one path exists n p, consider shortest one.
memory requirements procedure linear number stored nodes.

),
number operations required one marginal-utility update bounded O(Bdegree
Bdegree upper bound maximum indegree graph. Therefore,
426

fiMultiple-Goal Heuristic Search

backward degree bounded, number calculations per node grow
search progresses. depth limit determines complexity update given
search graph; hence, desirable reduce value. value low, however,
make possible infer local marginal-utility values.
5.1.2 Inferring Marginal Utility
inference algorithm estimates marginal utility node basis average
partial marginal utility siblings. nodes sucient statistics partial
marginal utility used predict marginal utility new nodes. call nodes
supported nodes. node supported siblings, base estimate average
estimated marginal utility parents (computed recursively using procedure).
Figure 4 illustrates method. left tree, marginal utility grey node

MU=0.4

MU=0.2

MU=(0.4+0.2)/2=0.3

MU=0.4

MU=0.2

MU=(0.4+0.2)/2=0.3

?

?

MU=0.3

Figure 4: Inferring marginal utility partial marginal utility supported siblings (left)
supported uncles (right). Nodes question mark unsupported.

computed average marginal utility siblings. right tree, marginal
utility grey node computed average marginal utility uncles. input
marginal utility estimation heuristic remaining unconsumed resources, r.
algorithm rst nds largest depth, d, number predicted nodes smaller
r. prediction based supported siblings parent node
described above.
found depth, d, used determine counters used estimate
marginal utilities supported uncles. complete algorithm listed Figure
5.
5.1.3 Sibling Clustering
algorithm described Figure 5, marginal utility node induced averaging
partial marginal utility siblings. dene meaningful similarity metric
nodes, try making prediction less noisy using nodes similar
siblings. One way use similarity metric cluster set siblings
generate virtual node cluster. virtual node linked cluster
members parent parent original sibling set. required
change. existing algorithm described Figure 5 rest. predicting
marginal utility node, algorithm rst looks partial marginal utility
427

fiDavidov & Markovitch

procedure MU(s,d)
Gd (s)
Supported(s,d) return N
(s)
else P Parents(s)
|P | = 0 return 0
SupportedSiblings {c Children(p) | p P, Supported(c)}
|SupportedSiblings
| > 0

Gd (c)
c

SupportedSiblings
return Avg

Nd (c)
else return Avg({M U (p, in(d + 1, D)) | p P })
procedure TreeSize(s,d)
P Parents(s)
Supported(s,d) |P | = 0 return Nd (s)
else
SupportedSiblings {c Children(p) | p P, Supported(c)}
|SupportedSiblings | > 0
return Avg({{N
}) fi


ff (c) | c SupportedSiblings

TreeSize(p,Min(d+1,D)) p P
else return Avg

|Children(p)|

procedure Get-marginal-utility(s,ResourceLimit )
Depth = max(d D|TreeSize(s, d) < ResourceLimit )
return MU(s, Depth)

Figure 5: algorithm marginal-utility estimation
siblings. case members cluster. siblings
unsupported algorithm use information clusters propagated
common parent.
mechanism illustrated Figure 6. Without clustering, predicted marginal
utility unsupported nodes would average three supported siblings,
0.5. Note average large variance associated it. Clustering nodes
B one virtual node, C, D, E another, yields (we hope)
accurate prediction, since based uniform sets. similarity metric
usually Euclidean distance vectors features states corresponding
sibling nodes. domains, try reduce number generated nodes
deciding step node-operator pair proceed with. cases need
similarity measurement operators implement approach.
5.2 Feature-Based Induction Marginal Utility
Unfortunately, partial marginal-utility information allows us predict marginal utility
nodes proximity one another graph structure. addition,
428

fiMultiple-Goal Heuristic Search

Sibling clustering

0.5


B
? 0.1

C
0.7


0.7

0.5

E
?

0.1


?

B
0.1

C
0.7


0.7

E
0.7

?

Figure 6: eect sibling clustering marginal utility estimation
domains local uniformity sibling nodes respect marginal utility
cannot assumed. overcome problems view marginal-utility inference
problem function learning use common induction algorithms. depth
d, induce marginal utility function using set supported nodes (with respect
d) examples. state features domain independent (such in-degree
out-degree node) domain specic, supplied user. induction
problem, quality induced function highly dependent quality
supplied features.
Since learning scheme performed on-line, high cost learning,
using classier directly, reduce utility learning process. One way essentially eliminate learning costs use lazy learner, KNN (Cover & Hart,
1967). approach also advantage incremental: new example contributes immediately learned model. problem approach high
costs associated using classier.
alternative approach would learn ecient classier regression
tree (Breiman, Friedman, Olshen, & Stone, 1984). learned incrementally using
algorithms ID5 (Utgo, 1988). Batch learning algorithms C4.5 usually
yield better classiers incremental algorithms. However, due higher cost
applying batch learning, one decide often call it. Applying node
generation would increase induction cost, yield better classiers earlier - may
improve performance search process. Applying large intervals would reduce
induction costs lead poorer search performance.
on-line learning process gives rise another problem: initial search period
yet sucient examples make learning helpful. One way reduce
eect lack knowledge using classiers induced beforehand, on-line
o-line, goals similar goals current search.

6. Empirical Evaluation
test eectiveness methods described previous sections show
versatility, experimented intensively several domains. challenging domain,
however, focused crawling apply algorithms task collecting target
web pages sub-web millions pages. rst compare anytime behavior
distance-based methods uninformed search best rst search.
429

fiDavidov & Markovitch

test performance marginal utility methods. also test eect
various suggested enhancements algorithms performance. also show realtime
performance algorithm allowing search real web.
6.1 Experimental Methodology
compare performance algorithms two competitors: breadth-rst search
best-rst search using distance estimation heuristics. algorithms adopted
multiple-goal framework allowing continue search nding rst
goal.
basic experiment compares two multiple-goal search algorithms conducted
following way:
1. set initial states goal predicate given.
2. algorithms perform multiple-goal search.
3. resources consumed number goals found execution
monitored.
4. last two steps repeated several times accumulate sucient statistics (all
algorithms contain least one random component).
5. performance two algorithms, measured number goals found
allocated resources, compared.
problem estimating performance algorithm resource allocation
given extensively discussed context anytime algorithms. Measuring
performance anytime algorithms problematic (Hansen & Zilberstein, 1996).
probability distribution resource allocation given, compute
expected performance anytime algorithm basis performance prole.
many cases, however, probability distribution available. therefore measure
performance tested algorithm means obtained quality dierent
resource allocation values. multiple-goal search, quality measured number
goals found allocated resources. know total number goals,
report instead percentage goals found.
Alternatively, anytime algorithms evaluated measuring amount resources required achieve given quality. multiple-goal search, obvious
measurement time. Time, however, overly aected irrelevant factors hardware, software, programming quality. Moreover, Web domain, spent
accessing Web pages. long takes depends many factors, network
server loads, irrelevant research topic.
thus decided measure resource consumption number generated nodes.
Nevertheless, cannot ignore time completely: must make sure overhead
methods described paper outweigh benets. therefore report
time results experiment uses real Web.
Many parameters aect performance algorithms described paper.
Ideally, would like perform factorial analysis (Montgomery, 2001) combination values tested. experimentation, however, infeasible large
430

fiMultiple-Goal Heuristic Search

number variables involved. therefore take one-factor-at-a-time approach,
use default value parameters except one tested. addition, wherever
appropriate, perform several experiments testing two factors together.
6.2 Tasks Domains
experiments conducted context several Web domains. show
generality approach, applied methods several additional domains, including
n-queens, open knight tours, multiple robot path planning multiple sequence alignment.
algorithms applied following tasks:
1. Focused crawling: One main motivations research problem
focused crawling Web (Chakrabarti et al., 1999; Cho & Garcia-Molina, 2000;
Kleinberg, 1999; Menczer, Pant, Srinivasan, & Ruiz, 2001). task nd
many goal pages possible using limited resources, basic resource unit
usually actual retrieval page link. looks task
retrieval information internet could achieved using general-purpose
search engines, several circumstances focused crawling still needed:
(a) search criterion complicated expressible query
language search engines.
(b) one needs updated set goals search engines updated every
weeks due huge space brute-force crawlers cover.
(c) coverage general engines sucient.
Previous work focused crawling concentrated Web-specic techniques directing search. experiments test whether generalization single-goal
heuristic search multiple-goal search contribute task focused crawling.
Performing rigorous empirical research Web problematic. First, Web
dynamic therefore likely modied dierent runs algorithms (Douglis et al., 1997). Second, enormous time required crawling
Web disallows parametric experimentation. solve problems downloaded signicant section Web local storage performed experiments
using local copy (Cho et al., 1998; Hirai, Raghavan, Garcia-Molina, & Paepcke,
2000). Specically, downloaded large part .edu domain, containing, cleanup, approximately 8,000,000 valid accessible HTML pages.
resulting graph average branching factor 10.6 (hyperlinks).
tested performance algorithms entire downloaded domain.
parametric experiments, however, time-consuming even local
copy used. Therefore, used small sub-domains experiments. subdomain generated randomly selecting root page predesignated set
roots extracting sub-graph size 35,000 it.
ensure overhead algorithms signicantly aect performance, also conducted several online experiments real Web.
use three types goal predicates:
431

fiDavidov & Markovitch

(a) Predicates test pages specic topics: robotics, mathematics, football, food, sport. predicates automatically induced applying
decision tree learning set manually supplied examples.
(b) Predicates test certain types pages: pages containing publication lists,
laboratory pages, student home pages, project pages news pages.
predicates also learned examples.
(c) Predicates test home pages people members specic
list. list people generated Web page listed names
people together personal information (such name, aliation area
interest). predicates employ commonly used heuristics determining
whether HTML document home page specic person. use three
predicates corresponding three dierent lists found Web.
limit list contain 100 names.
2. Finding paths multiple goals: Path-nding algorithms usually search single
path one goals. applications, however, get set initial states
set goal states task nd set paths initial state
goal state. paths may used another algorithm evaluates
selects one execute according various criteria. simulated physical environment 500 500 grid random walls inserted obstacles (an
average 210000 nodes average 3.9 branching factor). parameters
controlling maximal length walls desired density grid. Walls
inserted randomly making certain resulting graph remains connected.
set goal states randomly generated using one following two methods:
(a) set states independently uniformly drawn set states.
(b) 10% goal states generated above. randomly uniformly
select K states used cluster centers. rest goal states
randomly generated distances centers normally
distributed.
Figure 7 shows example multiple path-nding problem solution
includes 9 paths.
3. Planning movement multiple robots: Assume given set N robots
located various states, task collect set K > N objects scattered
around. case need plan N paths pass many objects
possible. situation illustrated Figure 8. Although problem appears
resemble one Figure 7, solution two paths (while solution
previous problem 9). many similar planning problems: example,
planning product delivery several starting points multiple customers
using xed number delivery trucks. experiments used
type grids previous problem. robots placed randomly.
assumed collisions harmful.
4. Open knight tours: famous problem task nd path
knight chess board squares visited none visited
432

fiMultiple-Goal Heuristic Search

G

G

G

G
G

G

G

G

G

G

G

G

G

G

G

G

G

G









Figure 7: Searching set paths multiple goals grid
G

G

G
G

G
G

G

G

G

G

G

G

G

G

G

G

G

R1

G

R2

R1

R2

Figure 8: Multiple-robot path planning grid
twice. tried multiple-goal version task nd many
paths possible within allocated resources. experiments, used boards
6 6, 7 7 8 8 squares.
5. N-Queens: constraint satisfaction problem goal place N queens
chessboard two queens row, column diagonal.
multiple goal version problem, want nd many satisfying congurations
possible within allocated resources.
6. Multiple sequence alignment: known bioinformatics problem goal
align several biological sequences optimally respect given cost function.
multiple-goal version interested obtaining many almost optimal
solutions possible within allocated resources.
6.3 Performance Distance-Based Heuristics
experimented rst two multiple-goal heuristic functions based graphdistance estimation: sum heuristic progress heuristic. compare performance multiple-goal best-rst search uses heuristics with:
433

fiDavidov & Markovitch

Domain

Task

BFS

Multiple path
nding
Multiple robot
movement

scattered
clustered
scattered
clustered

8.5(0.1)
10.2(0.1)
7.1(0.8)
10.1(0.9)

Focused
crawling
100-person search

Group 1
Group 2
Group 3

0.1(0.0)
3.2(1.4)
0.3(0.1)

Min. dist.
Sum
Without

Without

disab.
disab.
disab
disab
% goals found 20% resources
21.3(1.2) 29.0(0.5) 21.5(0.9) 28.9(0.3)
34.0(0.5) 45.1(0.4) 32.6(1.1) 59.2(0.3)
20.3(0.8) 26.5(0.4) 22.3(0.6) 25.8(0.2)
31.2(1.4) 47.4(1.2) 42.0(0.6) 64.1(0.7)
% goals found 2% resources
13.5(0.5) 17.3(1.3) 24.1(0.7) 28.0(1.1)
18.4(2.1) 26.2(1.9) 19.7(1.0) 23.5(1.1)
5.8(0.9)
10.7(1.4)
6.4(0.9)
11.7(0.9)

Progress

76.8(2.1)
94(1.2)
89.9(1.8)
98.6(0.9)
51.3(0.8)
78.1(3.1)
60.9(0.8)

Table 1: performance multiple-goal search various heuristics. numbers parentheses standard deviations.

1. Multiple-goal best-rst search uses distance estimation heuristic function.
2. Breadth-rst search (BFS) (shown Najork & Wiener, 2001, perform well
Web crawling).
sum heuristic requires distance estimates individual goals. dene distances three domains. multiple path nding multiple robot planning
use Manhattan distance. focused crawling task experiment personal
home page domain. estimate distance given page home page
list member computing cosine vector distance bag words given
page bag words description text person.
distance heuristic sum heuristic tested without disabling
inuence visited goals. use disabling progress heuristic since
subsumes behavior. two grid-based tasks, goals given, used
complete disabling removing visited goals goal list described Section
4.5.
personal home page search task, set goals explicitly given,
used feature-based method described Section 4.5. features used determining
distance candidate pages visited goals words highest
TFIDF value (Joachims, 1997; Salton & Buckley, 1988).
Table 1 summarizes results experiment. number represents average
50 experiments. measure performance consuming 20% maximal
amount resources, i.e., expanding 20% total number nodes search graph.
100-person home page search proved relatively easy domain methods
able nd goals consuming little resources. Therefore,
domain, measure performance 2% nodes. Figure 9 shows anytime
behavior various methods two domains. graphs domains
show similar patterns.
434

fiMultiple-Goal Heuristic Search

40

100
Breadth First Search
Distance-estimation heuristics
Sum heuristic

35

80
% Goals found

% Goals found

30
25
20
15

60

40

10
20

Breadth First Search
Distance-estimation heuristics
Sum heuristic
Progress heuristic

5
0

0
0

1

2

3

4

5

6

0

% Nodes generated

20

40

60

80

100

% Nodes generated

(a)

(b)

Figure 9: Anytime performance various heuristics: (a)Focused crawling (b)Multiple
path nding

basis table corresponding graphs, make following observations:
1. progress heuristic superior methods tested far.
advantageous case clustered goals comes surprise leads
one cluster pursued time. superior distance estimation
case scattered goals far less obvious: would expect heuristics
pursue one goal another therefore yield similar results. weighted
progress heuristic, however, prefers pursuing goals closer goals, thus
yielding better results.
2. results clearly demonstrate goal inuence phenomenon indeed signicant, method ecient reducing eect.
3. almost every case, heuristic methods signicantly better blind search.
exception using sum heuristic without inuence disabling graphs
scattered goals. cases, behavior marginally better blind
search.
6.4 Performance Marginal-Utility Heuristics
None methods previous subsection take account expected search
resources involved pursuing alternative directions. addition, assume
either knowledge specic set goals heuristic distances each.
subsection test performance two methods based marginal utility
described Section 5.1.2. experiments described subsection performed
focused crawling task 10 goal predicates described Section 6.2.
topic-based goals, cannot compare performance marginal-utility
algorithms sum heuristic access list goals
list heuristic values specic goals previously tested domains. Therefore

435

fiDavidov & Markovitch

use comparison blind BFS common best-rst search using distanceestimation heuristic. heuristic based list words selected induction
algorithm generating goal predicates.
6.4.1 Inferring Marginal Utility Partial Marginal Utility
implemented tested marginal-utility estimation algorithm described Section
5.1. Figure 10 shows performance proles tested algorithms robotics
100

100
Distance-estimation heuristics
Breadth First Search
Marginal-utility inference

Distance-estimation heuristics
Breadth First Search
Marginal-utility inference

90

80

80

70

70
% Goals found

% Goals found

90

60
50
40

60
50
40

30

30

20

20

10

10

0

0
0

5

10

15

20

25

30

35

40

0

% Nodes generated

5

10

15

20

25

30

35

40

% Nodes generated

(a)

(b)

Figure 10: performance multiple-goal best-rst search using marginal-utility
inference method applied focused crawling (with D=4). results shown
(a)Robotics pages (b) Mathematics pages

mathematics goal predicates. graph represents average 5 runs tested
algorithm using 5 starting pages randomly selected xed 200 root pages.
cases see signicant advantage marginal-utility method
two methods. advantage becomes evident initial training period,
sucient statistics accumulated. full data 10 domains resource
allocation 10% 20% available Appendix. Figure 11 shows average
improvement factor (compared distance estimation) 10 domains function
search resources. graph shows nicely initial exploratory stage
statistics accumulated 8% resources consumed,
improvement factor becomes larger 1. improvement factor reaches peak
2.8 17%, starts decline towards value 1 100% search resources,
algorithm necessarily nds goals.
Performance exploratory stage improved combining two methods.
tested hybrid method, uses linear combination marginal-utility prediction heuristic estimation. determine linear coecients part,
conducted 100 experiments small Web subgraph (below 35,000 pages) using dierent
goal predicates (unrelated tested main experiments). Figure 12 shows
results obtained combined method compared individual methods.
see indeed combined method better algorithms alone.

436

fiImprovement factor M.U. vs dist. estim. heuristics

Multiple-Goal Heuristic Search

3

2.5

2

1.5

1

0.5

0
0

10

20

30

40
50
60
70
% Nodes generated

80

90

100

Figure 11: improvement factor best-rst using marginal-utility inference compared
best-rst using distance-estimation heuristic

100

100
Distance-estimation heuristics
Marginal-utility inference
Combined approach

Distance-estimation heuristics
Marginal-utility inference
Combined approach

90

80

80

70

70
% Goals found

% Goals found

90

60
50
40

60
50
40

30

30

20

20

10

10

0

0
0

5

10

15

20

25

30

35

40

0

% Nodes generated

5

10

15

20

25

30

35

40

% Nodes generated

(a)

(b)

Figure 12: Combining marginal-utility inference heuristic search (a)Robotics pages
(b)Mathematics pages.

interesting phenomenon that, points, result combined method better
maximal results two. One possible explanation early
search process distance-estimation heuristic leads sucient number goals
jump-start learning process much earlier. results 10 domains available
Appendix.
6.4.2 Learning Marginal-Utility Features
Section 5 describe method feature-based generalization visited search nodes
order induce marginal-utility values. conducted set experiments test

437

fiDavidov & Markovitch

eciency learning mechanism problem focused crawling 10
goal types previous experiments.
crawling, accumulate supported visited pages tag
marginal utility measured time learning took place (see Section 5). convert
tagged pages feature vectors hand CART algorithm (Breiman et al.,
1984) regression-tree induction6 . used induced tree estimate marginal
utility newly generated nodes.
features, use bag-of-words approach, dominant eld text
categorization classication. value word-feature appearance frequency.
Words appearing HTML title tags given weight. apply feature selection
choose words highest TFIDF (Joachims, 1997; Salton & Buckley, 1988).
100

100
Feature-based marginal-utility induction
Distance-estimation heuristics
Marginal-utility inference

Feature-based marginal-utility induction
Distance-estimation heuristics
Marginal-utility inference

90

80

80

70

70
% Goals found

% Goals found

90

60
50
40

60
50
40

30

30

20

20

10

10

0

0
0

5

10

15

20

25

30

35

40

0

% Nodes generated

5

10

15

20

25

30

35

40

% Nodes generated

(a)

(b)

Figure 13: performance best-rst search marginal utility induced using
regression trees classier. experiments performed problem
focused crawling with: (a)Robotics pages (b) Mathematics pages

Figure 13 shows results obtained robotics mathematics goal predicates.
full report 10 domains available Appendix. cases,
initial period sibling-based inference method outperforms sophisticated feature-based induction. period, however, induction-based method
signicantly outperforms sibling-based method. One possible reason initial inferior performance feature-based induction requires examples simplistic
sibling-based method computes averages therefore needs fewer examples.
inspected produced trees found reect reasonable concepts.
several dozens nodes contain features related searched
goal features correspond hubs (such repository collection).
tested whether choice classier aects performance inductionbased method performing set experiments using KNN classier.
results obtained essentially identical.
6. induction algorithms, SVM Naive Bayes, could used well.

438

fiMultiple-Goal Heuristic Search

6.5 Testing Full System Architecture

100

100

90

90

80

80

70

70
% Goals found

% Goals found

described various enhancements marginal-utility methods, including sibling
clustering, overlap minimization, combining marginal-utility heuristic distanceestimation heuristic, disabling visited goals. Figure 14 shows performance
two marginal-utility methods full set enhancements. full results 10
domains available Appendix.

60
50
40
30

60
50
40
30

Enhanced induction
Inference
Induction
Enhanced inference
Distance-estimation heuristics

20
10

Enhanced induction
Inference
Induction
Enhanced inference
Distance-estimation heuristics

20
10

0

0
0

5

10

15

20

25

30

35

40

0

% Nodes generated

5

10

15

20

25

30

35

40

% Nodes generated

(a)

(b)

Figure 14: performance best-rst search (using dierent marginal-utility heuristics)
enhancements enabled: (a)Robotics pages (b) Mathematics pages.
gure contains 5 plots: one baseline performance (distance estimation), two unenhanced methods (inference induction) two
enhanced methods (enhanced inference enhanced induction).
sibling- feature-based methods indeed improve enhancements
enabled. Furthermore, feature-based method maintains advantage, albeit
slightly decreased magnitude. Although enabling enhancements improve system
performance, recalled true enabling enhancement separately. question thus arises whether improvements least partially cumulative.
words, would performance using enhancements better performance
using enhancements separately? Figure 15 compares graphs
sibling-based method. see fully enhanced method indeed superior
rest.
6.6 Realtime Performance
previous experiments took number generated nodes basic resource
unit. must careful, however, since measurement take account
overhead method. ensure overhead outweigh benets,
conducted realtime evaluation system architecture performing focused crawling
online Web measured resources consumed time elapsed.7 . Figures
7. experiment performed using Pentium-4 2.53 GHz computer 1GB main memory
cable modem.

439

fi100

100

90

90

80

80

70

70
% Goals found

% Goals found

Davidov & Markovitch

60
50
40
30

60
50
40
30

enchancements
Combined approach
Overlap minimization
Siblings clustering
enchancements

20
10

enchancements
Combined approach
Overlap minimization
Siblings clustering
enchancements

20
10

0

0
0

5

10

15

20

25

30

35

40

0

5

10

% Nodes generated

15

20

25

30

35

40

% Nodes generated

(a)

(b)

100

100

90

90

80

80

70

70
% Goals found

% Goals found

Figure 15: performance best-rst search (using marginal-utility inference)
enhancements enabled compared performance algorithm
single option enabled: (a)Robotics pages (b) Mathematics pages. gure
contains 5 plots: One marginal utility inference method enhancements, one method enhanced combined approach, one
enhancement overlap estimation, one enhancement sibling clustering
and, nally, one enhancements together.

60
50
40
30

60
50
40
30

20

20
Distance-estimation heuristics
Marginal-utility inference
Feature-based marginal-utility induction

10

Distance-estimation heuristics
Marginal-utility inference
Feature-based marginal-utility induction

10

0

0
0

10

20

30

40

50

60

70

80

90

0

10

20

30

40

50

Time (hours)

Time (hours)

(a)

(b)

60

70

80

90

Figure 16: performance marginal-utility based methods function real
time: (a)Robotics pages (b)Mathematics pages

16(a),(b) show performance methods enhancements enabled
function real time. comparison graphs graphs shown Figure 14
reveals overhead methods noticeably aect performance.
see overhead increases size visited graph, plotted Figure 17
real time function number generated nodes. graphs show
majority time required focused crawling tasks indeed loading time itself,

440

fiMultiple-Goal Heuristic Search

90
Pure loading time
Distance-estimation heuristics
Marginal-utility inference
Feature-based induction

80
70
Time (hours)

60
50
40
30
20
10
0
0

5

10

15
20
25
% Nodes generated

30

35

40

Figure 17: average real time used marginal-utility methods function
number generated nodes focused crawling domain

even calculations related discussed options enabled. fact, using
distance-estimation increases computation time factor 1.07; using
similarity-based inference marginal utility, 1.1; using feature-based
induction, 1.17. Thus, improvement greater factors,
algorithms benet focused crawling systems.
6.7 Contract Algorithms Versus Interruptible Anytime Algorithms
experiments described far test performance methods interruptible
anytime algorithms. point graph also considered test result
contract algorithm using specic resource allocation. However, multiple-goal
search algorithm called contract mode, utilize additional input (of resource
allocation) improve performance algorithm, described Section 5.1.2.
test eect exploiting resource allocation, repeated experiment
described Section 6.4.1 using algorithm Section 5.1.2. found
using algorithm contract mode, obtained average improvement factor 7.6
(factor 2.8 dropping two extremes) 5% resource allocation 1.4 10% resource
allocation. full results available Appendix.
algorithms also allow contract quality mode input required
quality instead allocated resources. case quality specied percentage goals found. contract algorithm achieved average improvement factor
1.9 5% goals 1.4 20% goals. full results available
Appendix.
6.8 Performance Multiple-Goal Search Algorithms
previous subsections test heuristic methods best-rst search. show
generality approach, test whether marginal-utility heuristics used eciently
dynamic ordering variables backtracking, choosing node focal
441

fiDavidov & Markovitch

group multiple-goal algorithm. cases used sibling clustering
method.
backtracking used multiple-goal version two known problems: open knight
tour n-queens. applied marginal-utility inference algorithm based
similarity siblings sort variable values multiple-goal version backtracking
search. open knight tour problem used 6 6 board (this board contains
524, 486 goal congurations, maximal size collect goals
eciently). n-queens problem used 16 16 board containing 14, 772, 512 goals.
neither case use domain-specic technique increase search eciency. Figure
100

100
90

80

80
% Goals found

% Goals found

70
60

40

60
50
40
30

20

20
10

Simple backtracking
Backtracking marginal utility
0

Simple backtracking
Backtracking marginal utility

0
0

10

20

30

40

50

60

70

80

90

100

0

% Nodes generated

10

20

30

40

50

60

70

80

90

100

% Nodes generated

(a)

(b)

Figure 18: Applying marginal-utility inference backtracking search (a)Open knight tour
(b) N-queens task

18(a),(b) compares performance multiple-goal backtracking search without
marginal utility. see applying marginal-utility inference signicantly increases
performance problems.
also tested multiple-goal algorithm, described Section 3, siblingbased marginal-utility heuristic selects node focal group. experiment
performed known multiple-sequence alignment problem. Since graph degree
large, applied modied version described Yoshizumi, Miura
Ishida (2000). used heuristic, methodology data set described
paper, requiring algorithm collect optimal 1.1-suboptimal solutions.
Figure 19 shows results obtained two data sets. see marginal utility
improves performance search algorithm.

7. Related Work
multiple-goal search framework dened paper novel. previous work
treated heuristic search multiple goals general search framework previous
work provided general algorithms multiple-goal search. planning community
dealt multiple goals entirely dierent setup, goals conjunctive

442

fiMultiple-Goal Heuristic Search

100

100

A* using regular heuristic
A* marginal-utility inference

80
% Goals found

% Goals found

80

A* using regular heuristic
A* marginal-utility inference

60

40

20

60

40

20

0

0
0

10

20

30

40

50

60

70

80

90

100

0

% Nodes generated

10

20

30

40

50

60

70

80

90

100

% Nodes generated

(a)

(b)

Figure 19: Applying marginal-utility inference search (a)Data set 1 (b) Data set 2
possibly conicting. particular, setup easily applicable generic graph
search.
Even domain-specic algorithms multiple-goal heuristic search common.
domains mentioned Section 6.2, one given attention multiplegoal problem domain Web crawling. sequence alignment domain used several
works heuristic search (for example, Korf & Felner, 2002; Zhou & Hansen, 2002, 2003;
Schroedl, 2005), single-goal search problem.
popularity Web led many researchers explore problem multiplegoal search Web graph. problem better known focused crawling. Chakrabarti
et al. (1999) dened focused crawler Web agent selectively seeks pages
relevant pre-dened set topics retrieving links live Web.
agents used building domain-specic Web indices (see example McCallum,
Nigam, Rennie, & Seymore, 1999). Focused Web-crawling algorithms use various methods
breadth-rst search (Najork & Wiener, 2001), best-rst search (Cho & GarciaMolina, 2000; Cho et al., 1998), reinforcement learning (Boyan, Freitag, & Joachims,
1996; Rennie & McCallum, 1999). heuristic methods focused crawling
based Web-specic features. example, page-rank model (Page, Brin, Motwani,
& Winograd, 1998) used Brin Page (1998), Haveliwala (1999).
hubs-and-authorities model (Kleinberg, 1999) used Borodin, Roberts, Rosenthal,
Tsaparas (2001). addition, several theoretical works provide analysis bounds
problem Web crawling (for example Cooper & Frieze, 2002; Kumar et al., 2000).
approach similar taken Rennie McCallum (Rennie &
McCallum, 1999), apply reinforcement learning Web crawling. Like marginalutility induction method, method also estimates reward value generalizes
unvisited nodes. are, however, several important dierences two methods, particulary denition reward. approach based maximal
number goals achieved given resources, method focused immediacy
goal achievement. sooner goal achieved optimal algorithm starting
graph node, contributes reward value node regardless
whether algorithm enough resources collect goal. Thus, setup
443

fiDavidov & Markovitch

allow direct incorporation supplied resource limit input. Furthermore, approach
relies relatively heavy o-line processing training set. propose online update
method estimate update marginal-utility based system. approach
eliminates need fetching xed training set, also gives exibility
algorithm.

8. Discussion
work described paper presents new framework heuristic search.
framework task collect many goals possible within allocated resources.
show traditional distance-estimation heuristic suciently eective
multiple-goal search. introduce sum progress heuristics, take advantage explicitly given goal set estimate direction larger closer groups
goals.
One problem heuristics ignore expected resources required
collect goals alternative directions. introduce marginal-utility heuristic,
attempts estimate cost per goal search direction. Thus, using
lead productive search.
Designing eective marginal-utility heuristic rather dicult task. therefore
developed two methods online learning marginal-utility heuristics. One based
local similarity partial marginal-utility sibling nodes, generalizes
marginal-utility state feature space. methods infer marginal utility
partial marginal-utility values based number visited goal non-goal
nodes partially explored subgraphs.
sibling-based inference method requires basic input search problem:
set starting nodes, successor function, goal predicate. method also
take advantage input resource allocation, demonstrated Section 6.7.
distance-estimation heuristic given, sibling-based method utilize initial
stages search data base inference sucient.
marginal-utility generalization method requires set meaningful features set
states. common requirement learning systems.
applied methodology several tasks, including focused Web crawling,
showed merit various conditions. also applied tasks nding paths
multiple goals, planning movement multiple robots, knight-tour, n-queens, nding
set multiple sequence alignments. experiments show even without prior
knowledge goal type, given goal predicate, algorithm,
initiation period, signicantly outperforms blind best-rst search using distanceestimation heuristic. enhance method regular distance-estimation
heuristic, method shows threefold improvement distanceestimation heuristic alone.
framework proposed algorithms applicable wide variety problems
interested nding many goal states rather one. show,
example, multiple sequence alignment problem, methods allow reach
many nearly-optimal congurations. methods also applied con-

444

fiMultiple-Goal Heuristic Search

straint satisfaction problems may useful nd many solutions apply
another algorithm selecting solution found set.
apply framework new problems, following requirements must fullled:
1. problem domain formulated state space.
2. exists predicate identies goal states.
3. using sum progress heuristics:
(a) traditional function estimates distance two states
given.
(b) set goals states given explicitly.
4. sibling-based method marginal utility inference assume marginal
utility values sibling nodes relatively similar.
5. induction-based marginal utility inference assume availability set
state features informative respect marginal utility.
main message research induction-based method marginal
utility inference used possible. Unlike sum progress heuristics,
takes account resources needed collecting goals. Unlike siblingbased inference method, makes assumptions similarity marginal utility values
siblings. require informative state features; however, many domains,
reach set state features sucient inducing marginal utility estimation function
available.
marginal-utility based heuristic techniques greedy sense always
choose node leading subgraph would expect nd maximal number
goals, use remaining resources subgraph. sophisticated approach would try wisely distribute remaining resources promising
directions, leading, hope, better performance greedy approach.
Although marginal utility approach consider possible overlap subgraphs, propose, Section 4.5, technique reduce it. interesting direction
could predict actual overlap search progresses, using methods
partial marginal-utility calculation.
framework described paper opens new research direction. eld wide
open development new algorithms application multiple-goal search
algorithms tasks.

Acknowledgements
would like thank Adam Darlo, Irena Koifman Yaron Goren, helped us
programming. research supported fund promotion research
Technion Israeli Ministry Science.

445

fiDavidov & Markovitch

Appendix A. Detailed Results
appendix provide breakdown results 10 Web topics. Tables 2
3 refer results described Section 6.4.1. Table 4 refers results described
Section 6.4.2. Table 5 refers results described Section 6.5. Tables 6 7 refers
results described Section 6.7.
Goal type

Robotics
Students
Mathematics
Football
Sports
Laboratories
Food
Publications
Projects
News

% goals found 10% resources
BFS
Distance
Marginal
estimation
utility
2.6(0.9)
7.2(0.2)
18.1(0.6)
10.5(1.0)
11.8(0.5)
17.0(1.3)
7.2(0.1)
17.6(1.6)
15.4(0.8)
0.0(0.0)
31.4(0.5)
25.3(0.9)
3.6(0.4)
37.0(1.1)
45.1(2.7)
0.3(0.1)
12.5(0.7)
33.4(0.8)
7.2(1.2)
25.1(0.9)
30.3(1.7)
0.3(0.1)
12.6(1.0)
35.2(2.6)
0.1(0.0)
30.1(1.4)
41.2(1.6)
8.5(0.9)
23.1(0.8)
22.6(0.8)

% goals found 20% resources
BFS
Distance
Marginal
estimation
utility
9.3(1.6)
30.5(1.1)
58.3(2.0)
12.5(1.5)
23.7(0.6)
54.6(1.3)
13.1(1.2)
28.3(0.8)
60.3(2.9)
0.8(0.0)
42.1(1.7)
71.5(0.7)
16.2(0.7)
41.6(0.7)
68.5(1.4)
5.3(0.5)
18.1(0.8)
80.4(3.1)
26.5(3.9)
48.5(1.7)
92.7(2.2)
3.5(0.1)
18.5(1.1)
64.2(1.9)
0.8(0.2)
32.0(1.4)
80.5(2.0)
15.3(1.0)
40.4(1.2)
88.9(0.7)

Table 2: Marginal-utility distance-estimation heuristics focused crawling (with D=4).
numbers parentheses standard deviations.

Goal type

Robotics
Students
Mathematics
Football
Sports
Laboratories
Food
Publications
Projects
News

% goals found 10% resources
Distance
Marginal Combined
estimation
utility
method
7.2(0.2)
18.1(0.6)
21.1(0.5)
11.8(0.5)
17.0(1.8)
23.3(0.6)
17.6(1.6)
15.4(0.8)
26.5(1.9)
31.4(0.5)
25.3(0.9)
33.9(1.0)
37.0(1.1)
45.1(2.7)
48.3(2.2)
12.5(0.7)
33.4(0.7)
40.2(0.9)
25.1(0.9)
30.3(1.7)
30.1(1.2)
12.6(1.0)
35.2(2.7)
42.0(2.4)
30.1(1.4)
41.2(1.6)
41.3(1.0)
23.1(0.8)
22.6(0.8)
30.5(1.9)

% goals found 20% resources
Distance
Marginal Combined
estimation
utility
method
30.5(1.1)
58.3(2.0)
65.1(2.1)
23.7(0.6)
54.6(1.3)
59.6(1.2)
28.3(0.8)
60.3(2.9)
68.7(2.5)
42.1(1.7)
71.5(0.7)
70.9(1.1)
41.6(0.7)
68.5(1.4)
75.0(1.3)
18.1(0.8)
80.4(3.1)
79.8(1.0)
48.5(1.7)
92.7(2.2)
93.3(1.6)
18.5(1.1)
64.2(1.9)
77.8(1.7)
32.0(1.4)
80.5(2.0)
84.7(1.8)
40.4(1.2)
88.9(0.7)
90.5(0.9)

Table 3: Combining marginal-utility distance-estimation heuristics focused crawling.
numbers parentheses standard deviations.

446

fiMultiple-Goal Heuristic Search

Goal type

Robotics
Students
Mathematics
Football
Sports
Laboratories
Food
Publications
Projects
News

% goals found 10% resources
Distance
Inference
Inference
estim.
siblings induction
7.2(0.2)
18.1(0.7)
8.4(1.2)
11.8(0.6)
17.0(1.9)
12.4(1.1)
17.6(1.6)
15.4(0.9)
10.8(1.5)
31.4(0.5)
25.3(0.4)
21.1(0.7)
37.0(1.2)
45.1(2.7)
36.2(1.6)
12.5(0.7)
33.4(0.8)
19.4(1.3)
25.1(0.9)
30.3(1.7)
27.1(0.6)
12.6(1.0)
35.2(2.7)
21.8(2.4)
30.1(1.4)
41.2(1.8)
35.5(1.2)
23.1(0.8)
22.6(0.8)
23.2(0.9)

% goals found 20% resources
Distance
Inference
Inference
estim.
siblings induction
30.5(1.1)
58.3(2.0)
75.5(2.5)
23.7(0.6)
54.6(1.3)
68.2(1.6)
28.3(0.8)
60.3(2.9)
69.7(2.5)
42.1(1.7)
71.5(0.7)
71.6(1.1)
41.6(0.7)
68.5(1.4)
79.4(1.5)
18.1(0.8)
80.4(3.1)
86.0(1.3)
48.5(1.7)
92.7(2.2)
89.1(1.8)
18.5(1.1)
64.2(1.9)
78.9(1.2)
32.0(1.4)
80.5(2.0)
89.8(2.5)
40.4(1.2)
88.9(0.7)
93.9(1.1)

Table 4: performance two methods marginal-utility estimation focused crawling
task. numbers parentheses standard deviations.

Goal type

Robotics
Students
Mathematics
Football
Sports
Laboratories
Food
Publications
Projects
News
Robotics
Students
Mathematics
Football
Sports
Laboratories
Food
Publications
Projects
News

% goals found 10% resources
Distance-estimation Inference siblings Inference induction
Without

Without

enhanc.
enhanc.
enhanc.
enhanc.
7.2(0.2)
18.1(0.7)
33.4(2.1)
8.4(1.2)
22.1(2.2)
11.8(0.5)
17.0(1.8)
26.5(1.0)
12.4(1.1)
25.2(1.6)
17.6(1.6)
15.4(0.8)
35.2(1.4)
10.8(1.4)
36.8(1.5)
31.4(0.6)
25.3(0.9)
46.9(1.4)
21.1(0.7)
39.5(1.4)
37.0(1.2)
45.1(2.7)
54.9(1.6)
36.2(1.6)
46.8(1.5)
12.5(0.7)
33.4(0.8)
46.4(1.6)
19.4(1.3)
39.5(1.4)
25.1(0.9)
30.3(1.7)
35.1(1.4)
27.1(0.6)
30.4(1.4)
12.6(1.1)
35.2(2.6)
46.8(1.5)
21.8(2.4)
30.0(1.4)
30.1(1.4)
41.2(1.6)
49.2(1.2)
35.5(1.2)
44.6(1.3)
23.1(0.8)
22.6(0.8)
41.1(1.6)
23.2(0.9)
42.3(1.1)
% goals found 20% resources
30.5(1.2)
58.3(2.0)
83.2(1.8)
75.5(2.6)
89.1(1.5)
23.7(0.6)
54.6(1.3)
65.4(1.7)
68.2(1.6)
78.3(1.6)
28.3(0.8)
60.3(2.9)
88.6(1.0)
69.7(2.5)
92.0(1.1)
42.1(1.7)
71.5(0.8)
80.6(1.4)
71.6(1.1)
91.3(1.4)
41.6(0.7)
68.5(1.4)
78.5(1.6)
79.4(1.6)
86.4(1.5)
18.1(0.8)
80.4(3.1)
86.4(2.0)
86.0(1.3)
92.1(1.4)
48.5(1.7)
92.7(2.2)
95.1(1.6)
89.1(1.8)
95.0(1.7)
18.5(1.1)
64.2(1.9)
87.2(2.1)
78.9(1.2)
94.7(1.4)
32.0(1.4)
80.5(2.1)
85.4(1.7)
89.8(2.5)
95.2(1.7)
40.4(1.3)
88.9(0.8)
93.8(1.2)
93.9(1.1)
96.5(1.5)

Table 5: performance two marginal-utility based methods enhancements
enabled. numbers parentheses standard deviations.

447

fiDavidov & Markovitch

Goal type

Robotics
Students
Mathematics
Football
Sports
Laboratories
Food
Publications
Projects
News

% goals found
Contract:
10% Resources
Anytime Contract
18.1(0.7) 24.6(1.1)
17.0(1.9) 29.3(1.5)
15.4(0.8) 24.1(0.9)
25.3(0.9) 29.9(1.0)
45.1(2.7) 60.4(1.1)
33.4(0.7) 40.7(0.9)
30.3(1.7) 32.1(1.6)
35.2(2.6) 55.0(2.3)
41.2(1.6) 48.6(0.9)
22.6(0.8) 41.5(1.0)

Contract:
5% Resources
Anytime Contract
0.1(0.0)
2.9(0.2)
0.7(0.0)
1.9(0.1)
1.8(0.3)
8.4(0.8)
4.2(0.2)
12.1(0.5)
12.7(0.6) 20.4(1.1)
10.5(1.0) 11.3(0.6)
9.4(0.9)
22.1(1.2)
2.1(0.3)
12.9(1.7)
0.9(0.0)
21.5(1.6)
8.8(1.0)
13.2(0.9)

Contract:
20% Resources
Anytime Contract
58.3(2.0) 69.5(2.2)
54.6(1.3) 62.8(1.3)
55.3(0.9) 64.0(1.1)
71.5(0.8) 75.6(0.8)
68.5(1.4) 77.7(1.3)
80.4(3.1) 81.6(0.1)
92.7(2.2) 92.8(2.2)
64.2(1.9) 76.2(1.5)
80.5(2.1) 86.5(1.6)
88.9(0.7) 91.1(1.0)

Table 6: Contract vs. anytime performance supplying resource limit. numbers
parentheses standard deviations.

Goal type

Robotics
Students
Mathematics
Football
Sports
Laboratories
Food
Publications
Projects
News

Contract: 5% Goals
Anytime Contract
8.4(0.6)
6.2(1.1)
9.0(0.5)
7.1(0.4)
5.4(0.4)
3.9(0.2)
5.2(0.2)
2.1(0.2)
3.9(0.2)
1.9(0.1)
4.2(0.1)
1.8(0.1)
5.9(0.3)
2.2(0.3)
6.1(0.5)
3.3(0.4)
4.0(0.3)
1.7(0.1)
7.6(0.7)
5.9(0.4)

% resources consumed
Contract: 20% Goals
Anytime Contract
12.1(0.7)
9.8(0.5)
11.3(0.5)
8.4(0.3)
10.3(0.5)
6.8(0.4)
8.4(0.3)
5.2(0.2)
7.1(0.6)
6.0(0.4)
7.4(0.4)
5.3(0.3)
8.8(0.5)
5.1(0.2)
7.7(1.1)
5.9(0.5)
8.3(0.5)
6.1(0.3)
9.8(0.2)
6.5(0.3)

Contract: 50% Goals
Anytime Contract
17.2(1.2)
10.2(1.0)
18.7(0.6)
9.9(0.5)
14.3(0.6)
9.9(0.4)
14.6(0.5)
9.9(0.7)
12.2(0.6)
6.5(0.6)
14.1(0.6)
6.6(0.2)
14.3(0.5)
7.0(0.6)
18.8(0.7)
10.3(0.6)
16.5(0.6)
9.0(0.5)
16.0(0.4)
10.3(0.6)

Table 7: Contract vs. anytime performance supplying goal requirements. numbers
parentheses standard deviations.

448

fiMultiple-Goal Heuristic Search

References
Boddy, M., & Dean, T. L. (1994). Deliberation scheduling problem solving time
constrained environments. Articial Intelligence, 67 (2), 245285.
Borodin, A., Roberts, G. O., Rosenthal, J. S., & Tsaparas, P. (2001). Finding authorities
hubs link structures www. 10th International WWW conference,
pp. 415429. ACM Press.
Boyan, J., Freitag, D., & Joachims, T. (1996). machine learning architecture optimizing web search engines. Proceedings AAAI Workshop Internet-Based
Information Systems, pp. 324335.
Breiman, L., Friedman, J. H., Olshen, R. A., & Stone, P. J. (1984). Classication
Regression Trees. Wadsworth International, Monterey, CA.
Brin, S., & Page, L. (1998). anatomy large-scale hypertextual Web search engine.
Computer Networks ISDN Systems, 30 (17), 107117.
Chakrabarti, S., van den Berg, M., & Dom, B. (1999). Focused crawling: new approach
topic-specic Web resource discovery. Computer Networks, 31 (1116), 16231640.
Cho, J., & Garcia-Molina, H. (2000). evolution Web implications
incremental crawler. El Abbadi, A., Brodie, M. L., Chakravarthy, S., Dayal, U.,
Kamel, N., Schlageter, G., & Whang, K.-Y. (Eds.), VLDB 2000, pp. 200209, Los
Altos, CA 94022, USA. Morgan Kaufmann Publishers.
Cho, J., Garca-Molina, H., & Page, L. (1998). Ecient crawling URL ordering.
Computer Networks ISDN Systems, 30 (17), 161172.
Cooper, C., & Frieze, A. (2002). Crawling web graphs. Proceedings STOC, pp.
419427.
Cover, T. M., & Hart, P. E. (1967). Nearest neighbor pattern classication. IEEE Transactions Information Theory, 13, 2127.
Culberson, J. C., & Schaeer, J. (1998). Pattern databases. Computational Intelligence,
14 (4), 318334.
Diligenti, M., Coetzee, F., Lawrence, S., Giles, C. L., & Gori, M. (2000). Focused crawling
using context graphs. 26th International Conference Large Databases,
VLDB 2000, pp. 527534, Cairo, Egypt.
Douglis, F., Feldmann, A., Krishnamurthy, B., & Mogul, J. C. (1997). Rate change
metrics: live study www. USENIX Symposium Internet
Technologies Systems, pp. 147158.
Gasser, R. U. (1995). Harnessing Computational Resources Ecient Exhaustive Search.
Ph.D. thesis, ETH, Swiss Federal Institute Technology, Zurich, Switzerland.
Hansen, E. A., & Zilberstein, S. (1996). Monitoring progress anytime problem-solving.
Proceedings Thirteenth National Conference Articial Intelligence (AAAI96), pp. 12291234, Portland, Oregon, USA. AAAI Press / MIT Press.
Haveliwala, T. (1999). Ecient computation PageRank. Tech. rep. 1999-31.

449

fiDavidov & Markovitch

Held, M., & Karp, R. M. (1970). traveling salesman problem minimum spanning
trees. Operations Research, 18, 11381162.
Hirai, J., Raghavan, S., Garcia-Molina, H., & Paepcke, A. (2000). Webbase: repository
web pages. Proceedings 9th International WWW Conference, pp. 277293.
Hovitz, E. (1990). Computation Action Bounded Resources. Ph.D. thesis, Stanford University.
Joachims, T. (1997). probabilistic analysis Rocchio algorithm TFIDF
text categorization. Proc. 14th International Conference Machine Learning, pp.
143151. Morgan Kaufmann.
Kleinberg, J. M. (1999). Authoritative sources hyperlinked environment. Journal
ACM, 46 (5), 604632.
Korf, R. E., & Zhang, W. (2000). Divide-and-conquer frontier search applied optimal
sequence allignment. National Conference Articial Intelligence (AAAI), pp.
910916.
Korf, R. E., & Felner, A. (2002). Disjoint pattern database heuristics. Articial Intelligence,
134 (12), 922.
Kumar, R., Raghavan, P., Rajagopalan, S., Sivakumar, D., Tomkins, A., & Upfal, E. (2000).
Web graph. Proc. 19th Symp. Principles Database Systems, PODS,
pp. 110. ACM Press.
Lawrence, S., & Giles, C. L. (1998). Searching WWW. Science, 280 (5360), 98100.
McCallum, A., Nigam, K., Rennie, J., & Seymore, K. (1999). Building domain-specic
search engines machine learning techniques. Proc. AAAI-99 Spring Symposium
Intelligent Agents Cyberspace, pp. 2839.
Menczer, F., Pant, G., Srinivasan, P., & Ruiz, M. (2001). Evaluating topic-driven web
crawlers. Proceedings SIGIR-01), pp. 241249, New York. ACM Press.
Montgomery, D. C. (2001). Design Analysis Experiments (5 edition). John Wiley
Sons.
Mostow, J., & Prieditis, A. E. (1989). Discovering admissible heuristics abstracting
optimizing: transformational approach. Proceedings IJCAI-89, Vol. 1, pp.
701707.
Najork, M., & Wiener, J. L. (1998). technique measuring relative size overlap
public web search engines. Proceedings Seventh International WWW
Conference[WWW7], pp. 379388.
Najork, M., & Wiener, J. L. (2001). Breadth-rst crawling yields high-quality pages.
Proceedings 10th International WWW Conference, pp. 114118.
Page, L., Brin, S., Motwani, R., & Winograd, T. (1998). pagerank citation ranking:
Bringing order web. Tech. rep., Stanford Digital Library Technologies Project.
Pandurangan, G., Raghavan, P., & Upfal, E. (2002). Using PageRank Characterize Web
Structure. 8th Annual International Computing Combinatorics Conference
(COCOON), pp. 330339.
450

fiMultiple-Goal Heuristic Search

Pearl, J., & Kim, J. H. (1982). Studies semi-admissible heuristics. IEEE Transactions
Pattern Analysis Machine Intelligence, 4 (4), 392399.
Prieditis, A. E. (1993). Machine discovery eective admissible heuristics.. 12 (13),
117141.
Rennie, J., & McCallum, A. K. (1999). Using reinforcement learning spider Web
eciently. Bratko, I., & Dzeroski, S. (Eds.), Proceedings ICML-99, 16th International Conference Machine Learning, pp. 335343, Bled, SL. Morgan Kaufmann
Publishers, San Francisco, US.
Russell, S. J., & Zilberstein, S. (1991). Composing real-time systems. Proceedings
IJCAI-91, pp. 213217, Sydney. Morgan Kaufmann.
Russell, S., & Norvig, P. (2003). Articial Intelligence: Modern Approach (2nd edition
edition). Prentice-Hall, Englewood Clis, NJ.
Salton, G., & Buckley, C. (1988). Term weighting approaches automatic text retrieval.
Information Processing Management, 24 (5), 513523.
Schroedl, S. (2005). improved search algorithm optimal multiple-sequence alignment.
Journal Articial Intelligence Research, 23, 587623.
Utgo, P. (1988). incremental ID3. Fifth International Conference Machine
Learning, pp. 107120. Morgan Kaufmann.
Yoshizumi, T., Miura, T., & Ishida, T. (2000). * partial expansion large branching
factor problems. AAAI/IAAI, pp. 923929.
Zhou, R., & Hansen, E. A. (2002). Multiple sequence alignment using anytime a*.
Proceedings Eighteenth National Conference Articial Intelligence, pp. 975
976, Edmonton, Alberta, Canada.
Zhou, R., & Hansen, E. A. (2003). Sweep a*: Space-ecient heuristic search partially
ordered graphs. Proceedings 15th IEEE International Conference Tools
Articial Intelligence, pp. 427434, Sacramento, CA.
Zilberstein, S. (1996). Using anytime algorithms intelligent systems. AI Magazine, 17 (3),
7383.
Zilberstein, S., Charpillet, F., & Chassaing, P. (1999). Real-time problem-solving
contract algorithms. IJCAI, pp. 10081015.

451

fiJournal Artificial Intelligence Research 26 (2006) 371-416

Submitted 11/05; published 8/06

Clause/Term Resolution Learning Evaluation
Quantified Boolean Formulas
Enrico Giunchiglia
Massimo Narizzano
Armando Tacchella

giunchiglia@unige.it
mox@dist.unige.it
tac@dist.unige.it

DIST - Universita di Genova
Viale Causa 13, 16145 Genova, Italy

Abstract
Resolution rule inference basis procedures automated reasoning. procedures, input formula first translated equisatisfiable
formula conjunctive normal form (CNF) represented set clauses. Deduction starts inferring new clauses resolution, goes empty clause
generated satisfiability set clauses proven, e.g., new clauses
generated.
paper, restrict attention problem evaluating Quantified Boolean
Formulas (QBFs). setting, outlined deduction process known
sound complete given formula CNF form resolution, called Qresolution, used. introduce Q-resolution terms, used formulas disjunctive normal form. show computation performed available
procedures QBFs based Davis-Logemann-Loveland procedure (DLL) propositional satisfiability corresponds tree Q-resolution terms clauses
alternate. poses theoretical bases introduction learning, corresponding
recording Q-resolution formulas associated nodes tree. discuss
problems related introduction learning DLL based procedures, present
solutions extending state-of-the-art proposals coming literature propositional
satisfiability. Finally, show DLL based solver extended learning, performs
significantly better benchmarks used 2003 QBF solvers comparative evaluation.

1. Introduction
Resolution (Robinson, 1965) rule inference basis procedures
automated reasoning (see, e.g., Fermuller, Leitsch, Hustadt, & Tammet, 2001; Bachmair &
Ganzinger, 2001). procedures, input formula first translated equisatisfiable formula conjunctive normal form (CNF) represented set clauses.
Deduction starts inferring new clauses resolution, goes empty clause
generated satisfiability set clauses proven, e.g., new clauses
generated. restrict attention problem evaluating Quantified Boolean
Formulas (QBFs). setting, outlined deduction process known sound
complete given formula CNF form resolution, called Q-resolution,
used (Kleine-Buning, Karpinski, & Flogel, 1995). However, available decision
procedures QBFs based extend Davis-Logemann-Loveland procedure
(DLL) (Davis, Logemann, & Loveland, 1962) propositional satisfiability (SAT).
c
2006
AI Access Foundation. rights reserved.

fiGiunchiglia, Narizzano & Tacchella

propositional case, well known computation performed DLL corresponds
specific form resolution called regular tree resolution (see, e.g., Urquhart, 1995).
paper introduce Q-resolution terms, used formulas disjunctive
normal form. show computation performed DLL based decision procedures
QBFs corresponds tree Q-resolution terms clauses alternate.
correspondence poses theoretical bases introduction learning, corresponding
recording Q-resolution formulas associated nodes tree. particular,
recording Q-resolutions clauses generalizes popular nogood learning constraint satisfaction SAT literatures (see, e.g., Dechter, 1990; Bayardo, Jr. & Schrag,
1997): nogood corresponds set assignments falsifying input formula,
useful pruning assignments existential variables. Recording Q-resolutions
terms corresponds good learning: good corresponds set assignments satisfying input formula, useful pruning assignments universal variables.
discuss problems related introduction learning DLL based procedures
QBFs, present solutions extending state-of-the-art proposals coming literature
SAT. show effectiveness learning QBFs evaluation problem,
implemented QuBE, state-of-the-art QBF solver. Using QuBE, done
experimental tests several real-world QBFs, corresponding planning (Rintanen, 1999;
Castellini, Giunchiglia, & Tacchella, 2003) circuit verification (Scholl & Becker, 2001;
Abdelwaheb & Basin, 2000) problems, two primary application domains
interest. results witness effectiveness learning.
paper structured follows. first review basics Quantified Boolean
Logic, time introducing terminology notation used
throughout paper. Section 3, introduce clause term resolution,
relation DLL based decision procedures QBFs. Then, Section 4, introduce
nogood good learning, show effectively integrated DLL
based decision procedures QBFs. implementation experimental results
presented Section 5. paper ends conclusions related work.
paper builds extends many ways AAAI paper (Giunchiglia, Narizzano, & Tacchella, 2002). respect paper, (i) introduce clause
term resolution; (ii) show correspondence clause/term Q-resolution
computation tree searched DLL based decision procedures; (iii) basis
correspondence, extend basic backtracking search procedure, first backjumping
learning, prove soundness completeness; (iv) discuss
implementation QuBE providing many details, (v) present results
much broader detailed experimental analysis.
on, simply write resolution Q-resolution.

2. Quantified Boolean Logic
Consider set P symbols. variable element P. literal variable
negation variable. following, literal l,
|l| variable occurring l;
l negation l l variable, |l| otherwise.
372

fiClause/Term Resolution Learning Quantified Boolean Formulas

sake simplicity, consider formulas negation normal form (NNF).
Thus, us, propositional formula combination literals using k-ary (k 0)
connectives (for conjunctions) (for disjunctions). following, use True
False abbreviations empty conjunction empty disjunction respectively.
QBF expression form
= Q1 z1 Q2 z2 . . . Qn zn

(n 0)

(1)


every Qi (1 n) quantifier, either existential universal ,
z1 , . . . , zn distinct variables,
propositional formula z1 , . . . , zn .
example,
x1 yx2 ((x1 x2 ) (y x2 ) (x2 ((x1 y) (y x2 ))))

(2)

QBF.
(1), Q1 z1 . . . Qn zn prefix matrix. also say literal l
existential |l| belongs prefix, universal otherwise. Finally, (1),
define
level variable zi , 1 + number expressions Qj zj Qj+1 zj+1
prefix j Qj 6= Qj+1 ;
level literal l, level |l|.
example, (2) x2 existential level 1, universal level 2, x1
existential level 3.
value semantics QBF defined recursively follows:
1. prefix empty, evaluated according truth tables propositional
logic.
2. x, true x true x true.
3. y, true true.
(1) l literal |l| = zi , l QBF
whose matrix obtained substituting
zi True z False l = zi ,
zi False z True l = z .
whose prefix Q1 z1 Q2 z2 . . . Qi1 zi1 Qi+1 zi+1 . . . Qn zn .
easy see QBF without universal quantifiers, problem determining
value reduces SAT problem.
Two QBFs equivalent either true false.
373

fiGiunchiglia, Narizzano & Tacchella

3. Resolution DLL Based Decision Procedures QBFs
section first introduce clause/term resolution DLL based decision procedures
QBFs, show correspondence two.
3.1 Clause Term Resolution
According definition QBF, matrix combination conjunctions
disjunctions literals. However, using common clause form transformations based
renaming first used Tseitin (1970), possible perform linear time conversion
arbitrary QBF equivalent one matrix conjunctive normal form
(CNF). conversions based fact QBF (1) equivalent
Q1 z1 Q2 z2 . . . Qn zn x((x ) [x/])

(n 0)


propositional formula literal;
x variable distinct z1 , z2 , . . . , zn ;
[x/] propositional formula obtained substituting one occurrences x.
Thus,
((x1 y) (y x2 ))
follows (2) equivalent
x1 yx2 x3 ((x1 x2 ) (y x2 ) (x2 x3 ) (x1 x3 ) (y x2 x3 ))

(3)

Thanks conversions, restrict attention QBFs matrix
CNF, represent matrix formula set clauses interpreted
conjunctively, clause finite set literals interpreted disjunctively. Further,
assume clause non-tautological minimal. clause tautological
contains variable negation. clause C minimal literals C
minimum level existential. minimal form clause C clause obtained
C deleting universal literals cause C non-minimal. instance,
(4), clauses non-tautological minimal. assumption clauses
non-tautological minimal restriction, following theorem states.
Theorem 1 Let QBF matrix CNF. Let 0 QBF obtained

1. eliminating tautological clauses;
2. replacing non-tautological non-minimal clause minimal form.
0 equivalent.
374

fiClause/Term Resolution Learning Quantified Boolean Formulas

Proof. Clearly, tautological clauses eliminated result equivalent
QBF. Let C = {l1 , . . . , ln , ln+1 , . . . , lm } non-tautological non-minimal clause
ln+1 , . . . , lm universal literals C \ min(C) (0 n < m). Further, without
loss generality, assume level li less equal level li+1 ,
1 < m. Then, form (p m)
. . . Q1 |l1 | . . . |ln | . . . |ln+1 | . . . |lm |Qm+1 zm+1 . . . Qp zp {{l1 , . . . , ln , ln+1 , . . . , lm }, . . .},
standing
. . . Q1 |l1 | . . . |ln | . . . |ln+1 | . . . |lm |Qm+1 zm+1 . . . Qp zp ((l1 . . . ln ln+1 . . . lm ) ).
Then, applying standard rules quantifiers, rewritten
. . . Q1 |l1 | . . . |ln | . . . |ln+1 | . . . |lm |((l1 . . . ln ln+1 . . . lm ) Qm+1 zm+1 . . . Qp zp ),
equivalent
. . . Q1 |l1 | . . . |ln | . . . |ln+1 | . . . (|lm |(l1 . . .ln ln+1 . . .lm )|lm |Qm+1 zm+1 . . . Qp zp ),
equivalent
. . . Q1 |l1 | . . . |ln | . . . |ln+1 | . . . ((l1 . . . ln ln+1 . . . lm1 ) |lm |Qm+1 zm+1 . . . Qp zp ),
equivalent
. . . Q1 |l1 | . . . |ln | . . . |ln+1 | . . . |lm |Qm+1 zm+1 . . . Qp zp ((l1 . . . ln ln+1 . . . lm1 ) ),
i.e., QBF obtained deleting lm clause C. iterating
reasoning process, literals C \ min(C) eliminated C, hence
thesis.

on, QBF CNF matrix conjunction clauses,
clause minimal non-tautological. represent matrix QBF
set clauses,
empty clause {} stands False;
empty set clauses {} stands True;
formula {{}} equivalent False;
QBF (3) written
x1 yx2 x3 {{x1 , y, x2 }, {y, x2 }, {x2 , x3 }, {x1 , y, x3 }, {y, x2 , x3 }}.

(4)

Clause resolution (Kleine-Buning et al., 1995) similar ordinary resolution
existential literals matched. precisely, clause resolution (on literal l)
rule
C1
C2
(5)
min(C)

375

fiGiunchiglia, Narizzano & Tacchella

(c1)
(c2)
(c3)
(c4)

{x1 , y, x2 }
{y, x2 }
{x2 , x3 }
{x1 , y, x3 }

Input
Input
Input
Input

formula
formula
formula
formula

(c5)
(c6)
(c7)
(c8)

{x1 }
{x3 , y}
{x1 }
{}






(c1),
(c2),
(c4),
(c5),

(c2)
(c3)
(c6)
(c7)

Table 1: clause resolution deduction showing (4) false. prefix x1 yx2 x3 .

l existential literal;
C1 , C2 two clauses {l, l} (C1 C2 ), literal l0 6= l, {l0 , l0 }
(C1 C2 );
C (C1 C2 ) \ {l, l}.
C1 C2 antecedents, min(C) resolvent rule.
Theorem 2 ((Kleine-Buning et al., 1995)) Clause resolution sound complete
proof system deciding QBFs CNF: QBF CNF true empty
clause derivable clause resolution.
instance, fact (4) false follows deduction Table 1.
Alternatively CNF conversion, could converted (2) QBF
matrix disjunctive normal form (DNF), linear time, basis QBF
(1), equivalent
Q1 z1 Q2 z2 . . . Qn zn y((y ) [y/])

(n 0),

assuming propositional formula literal, variable distinct
z1 , z2 , . . . , zn .
simple recursive application equivalence (2) leads following
equivalent QBF:
x1 yx2 y1 y2 y3 y4 y5 y6 ((y1 y2 y3 )
(y 1 x1 ) (y 1 y) (y 1 x2 )
(y 2 y) (y 2 x2 )
(y 3 x2 ) (y 3 y4 )
(y 4 y5 y6 )
(y 5 x1 ) (y 5 y)
(y 6 y) (y 6 x2 )).

(6)

Given QBF matrix DNF, represent matrix set terms
interpreted disjunctively, term finite set literals interpreted
conjunctively. Further, assume term non-contradictory minimal.
term contradictory contains variable negation. term minimal
literals minimum level universal. minimal form term
term obtained deleting existential literals cause non-minimal.
terms (6) non-contradictory minimal. Analogously said
QBFs CNF, QBF DNF assume terms
non-contradictory minimal without loss generality.
376

fiClause/Term Resolution Learning Quantified Boolean Formulas

Theorem 3 Let QBF matrix DNF. Let 0 QBF obtained

1. eliminating contradictory terms;
2. replacing non-contradictory non-minimal term minimal form.
0 equivalent.
Proof. Analogous proof Theorem 1.



before, on, QBF DNF matrix disjunction
terms, term minimal non-contradictory.
introduce term resolution (on literal l) consists rule
T1
T2
min(T )

l universal literal;
T1 , T2 two terms {l, l} (T1 T2 ), literal l0 6= l, {l0 , l0 }
(T1 T2 );
(T1 T2 ) \ {l, l}.
T1 T2 antecedents, min(T ) resolvent rule.
Theorem 4 Term resolution sound complete proof system deciding QBFs
DNF: QBF DNF true empty term derivable term resolution.
Proof. fact term resolution sound complete proof system follows
soundness completeness clause resolution.
Let set sets literals, = Q1 z1 Q2 z2 . . . Qn zn QBF
interpreted set clauses. Without loss generality assume clause
non-tautological minimal. following chain equivalences holds:
exists deduction empty clause using clause resolution

false

QBF = Q1 z1 Q2 z2 . . . Qn zn interpreted set terms true

deduction empty term using term resolution.
chain equivalences, Q Q = , Q = .



example term resolution deduction empty term, consider QBF :
x1 yx2 x3 ((x1 x2 ) (y x2 ) (x2 x3 ) (x1 x3 ) (y x2 x3 ))
377

fiGiunchiglia, Narizzano & Tacchella

i.e., QBF obtained (3) simultaneously replacing , , ,
. Then, deduction Table 1 also deduction empty term
using term resolution.
QBF DNF CNF term resolution cannot applied,
thus term resolution sufficient proving truth falsity . However,
also following model generation rule

min(T )

matrix ;
non-contradictory term clause C , C 6= ,
get sound complete proof system QBFs CNF. Intuitively, model generation rule allows us start minimal form terms propositionally entail
matrix input formula.
Theorem 5 Term resolution model generation sound complete proof system
deciding QBFs CNF: QBF CNF true empty term derivable
term resolution model generation.
Proof. Given QBF CNF matrix , model generation rule derive
set terms form min(T )
term non-contradictory clause C , C 6= ;

disjunction terms propositionally logically equivalent .
Let 0 QBF DNF obtained substituting . 0
value. Hence thesis thanks Theorem 4.


3.2 DLL Based Decision Procedures QBFs
Given said far, arbitrary QBF converted (in linear time)
equivalent QBF CNF. this, end paper, restrict
attention QBFs format. assumption, (1) l literal
|l| = zi , redefine l QBF
whose matrix obtained removing clauses C l C,
removing l clauses;
whose prefix Q1 z1 Q2 z2 . . . Qi1 zi1 Qi+1 zi+1 . . . Qn zn .
378

fiClause/Term Resolution Learning Quantified Boolean Formulas

Further, extend notation sequence literals: = l1 ; l2 ; . . . ; lm (m 0),
defined (. . . ((l1 )l2 ) . . .)lm .
Consider QBF .
simple procedure determining value starts empty assignment
recursively extends current assignment z and/or z, z heuristically
chosen variable highest level , either empty clause empty set
clauses produced . basis values ;z ;z , value
determined according semantics QBFs. value value .
Cadoli, Giovanardi, Giovanardi Schaerf (2002) introduced various improvements
basic procedure.
first improvement directly conclude value
matrix contains contradictory clause (Lemma 2.1 Cadoli et al., 2002). clause
C contradictory contains existential literal. example contradictory clause
empty clause.
second improvement allows us directly extend l l unit monotone
(Lemmas 2.4, 2.5, 2.6 Cadoli et al., 2002). (1), literal l is:
Unit l existential 0,
clause {l, l1 , . . . , lm } belongs ;
literal li (1 m) universal level lower level l.
Monotone pure
either l existential, l belong clause , l occurs ;
l universal, l belong clause , l occurs .
example, QBF form
. . . x1 yx2 . . . {{x1 , y}, {x2 }, . . .},
x1 x2 unit. QBF
y1 x1 y2 x2 {{y 1 , y2 , x2 }, {x1 , 2 , x2 }},
monotone literals y1 x1 .
improvements, resulting procedure, called Q-DLL, essentially one
presented work Cadoli, Giovanardi, Schaerf (1998), extends DLL
order deal QBFs. Figure 1 simple, recursive presentation it. figure,
given QBF ,
1. False returned contradictory clause matrix (line 1); otherwise
2. True returned matrix empty (line 2); otherwise
3. line 3, recursively extended ; l l unit (and say l
assigned unit); otherwise
379

fiGiunchiglia, Narizzano & Tacchella

0 function Q-DLL(, )
1
(ha contradictory clause matrix i) return False;
2
(hthe matrix emptyi) return True;
3
(hl unit i) return Q-DLL(, ; l);
4
(hl monotone i) return Q-DLL(, ; l);
5
l := ha literal highest level i;
6
(hl existentiali) return Q-DLL(, ; l) Q-DLL(, ; l);
7
else return Q-DLL(, ; l) Q-DLL(, ; l).
Figure 1: algorithm Q-DLL.
4. line 4, recursively extended ; l l monotone (and say l
assigned monotone); otherwise
5. literal l highest level chosen
l existential (line 6), extended ; l first (and say l
assigned left split). result False, ; l tried returned (and
case say l assigned right split).
Otherwise (line 7), l universal, extended ; l first (and say l
assigned left split). result True, ; l tried returned
(and case say l assigned right split).
Theorem 6 Q-DLL(, ) returns True true, False otherwise.
Proof. Trivial consequence Lemmas 2.1, 2.4, 2.5, 2.6 work Cadoli, Giovanardi,
Giovanardi, Schaerf (2002) semantics QBFs.

Given said far, clear Q-DLL evaluates generating
semantic tree (Robinson, 1968) node corresponds invocation Q-DLL
thus assignment . us,
assignment (for QBF ) possibly empty sequence = l1 ; l2 ; . . . ; lm (m 0)
literals li , li unit, monotone, highest level
l1 ;l2 ;...;li1 ;
(semantic) tree representing run Q-DLL tree
node call Q-DLL(, );
edge connecting two nodes ; l, l literal.
tree representing run Q-DLL least node .
example run Q-DLL, consider QBF (4). simplicity, assume
literal returned line 5 Figure 1 negation first variable prefix
occurs matrix QBF consideration. Then, tree searched
Q-DLL (4) represented Figure 2. figure:
380

fiClause/Term Resolution Learning Quantified Boolean Formulas

{}{{x1 , y, x2 }, {x1 , y, x3 }, {y, x2 }, {y, x2 , x3 }, {x2 , x3 }}{}
{x1 } hx1 , li {x1 }
{{y, x3 }, {y, x2 }, {y, x2 , x3 }, {x2 , x3 }}

{x1 } hx1 , ri {x1 }
{{y, x2 }, {y, x2 }, {y, x2 , x3 }, {x2 , x3 }}

hy, ri {x1 }
hy, li {y}
hy, li {y}
hy, ri {x1 }
hx2 , pi{y} {x1 , y, x2 }hx2 , ui{x1 }
hx2 , pi{y} {x1 , y, x3 }hx3 , ui{x1 }
{{}} {y, x2 }
{y} {} {y}
{y, x2 }hx2 , ui{y, x3 } {y} {} {y}
{{}} {x2 , x3 }

Figure 2: tree generated Q-DLL (4). matrix (4) shown root
node, prefix x1 yx2 x3 . u, p, l, r stand unit, pure, left
split, right split respectively, obvious meaning.

node labeled literal assigned Q-DLL order extend assignment built far. Thus, assignment corresponding node sequence
labels path root node. instance, assignment corresponding node label x3 x1 ; y; x3 .
literals assigned unit monotone, corresponding nodes aligned
one other. assigned literal l, also show whether l
assigned unit, monotone, left right split marking u, p, l, r
respectively.
l assigned left right split, also show matrix ;l ,
sequence literals assigned l.
node leaf, matrix either empty (in case
write {} node), contains contradictory clause (in case
write {{}} node).
Considering Figure 2, easy see Q-DLL would correctly return False, meaning (4) (and thus also (2)) false.
3.3 Resolution DLL Based Decision Procedures QBFs
well known correspondence SAT semantic trees resolution (see, e.g.,
Urquhart, 1995) gives us starting point analysis, aimed establish correspondence Q-DLL clause/term resolution.
Consider QBF . Let tree explored Q-DLL evaluating .
time being, assume dealing SAT problem, i.e.,
contain universal quantifiers. Then, Q-DLL reduces DLL, false
use generate clause resolution deduction empty clause . basic idea
associate node clause C -falsified, i.e.,
381

fiGiunchiglia, Narizzano & Tacchella

literal l C, l . (We say literal l assigned l1 ; . . . ; lm
l {l1 , . . . , lm }). precisely:
every leaf , associate arbitrarily selected clause matrix
-falsified. least one clause exists contains empty
clause.
C clause associated node ; l,
1. l 6 C C also clause associated . Notice l monotone
l 6 C.
2. l C l unit clause associated resolvent
C arbitrarily selected clause causes l unit .
3. l C l unit consider clause C 0 associated
node ; l. l 6 C 0 C 0 clause associated (as
first case). l C 0 , clause associated resolvent C C 0 .
Lemma 1 Let QBF without universal quantifiers. Let tree searched
Q-DLL(, ). Let assignment . false, clause associated
node
-falsified;
contain existential literals whose negation assigned monotone
.
Proof. Let set assignments extend . Clearly, assignment
0 S, 0 false ( contain universal quantifiers). S, define partial
order relation according two assignments 0 00 0 00
0 extends 00 . Clearly well founded minimal elements
assignments extending corresponding leaves .
0 extends leaf , 0 contains contradictory clause C. Since
contain universal quantifiers, C 0 -falsified associated node
0 . Clearly, C contain existential literals whose negation assigned
monotone.
induction hypothesis, assignment 0 = 00 ; l 00 0 -falsified
clause containing existential literals whose negation assigned monotone.
show thesis 00 . three cases:
1. l assigned unit. Let C1 clause associated 00 ; l. induction
hypothesis, thesis holds C1 . C1 contain l, thesis trivially follows.
Otherwise, clause associated 00 resolvent C C1 clause C2
causes l unit 00 . C2 00 ; l-falsified contain existential
literals whose negation assigned monotone. C = C1 C2 \ {l, l} thus
thesis trivially holds.
2. l assigned monotone. case clause C associated 00
clause associated 00 ; l. induction hypothesis C contain l
thus C 00 -falsified.
382

fiClause/Term Resolution Learning Quantified Boolean Formulas

3. l split. case clause C1 associated 00 ; l clause C2
associated 00 ; l. thesis holds C1 C2 induction hypothesis.
C1 contain l, clause associated 00 C1 thesis
trivially holds. Otherwise, C2 contain l, clause associated
00 C2 thesis trivially holds. Otherwise, clause associated 00
C1 C2 \ {l, l} thesis trivially holds.

Theorem 7 Let false QBF without universal quantifiers. tree searched QDLL(, ) corresponds clause resolution deduction empty clause.
Proof. Let sequence clauses obtained listing clauses matrix
according arbitrary order, followed clauses associated internal nodes
tree searched Q-DLL(, ), assuming visited post order. Clearly,
deduction. deduction empty clause node associated
-falsified clause (Lemma 1), i.e., empty clause.

theorem points close correspondence computation Q-DLL
clause resolution, assuming input formula false contain
universal quantifiers. input formula contain universal quantifiers true,
still tree explored Q-DLL generating path ending empty matrix
corresponds sequence clause resolutions, one maximal subtree whose leaves
contains empty clause.
longer assume input formula contain universal quantifiers,
consider case arbitrary QBF, situation gets complicated,
also possibility assigning unit literals highest level.
So, assume literal l assigned unit node , l highest
level .
Then, input formula false, use tree searched Q-DLL
generate clause resolution deduction empty clause. construction analogous
one described before. difference restrict attention
minimal false subtree , i.e., tree obtained deleting subtrees
starting left split universal literal: subtrees originated wrong
choices deciding branch explore first. minimal false subtree 0 ,
leaves terminate empty clause, associate node 0
clause exactly way described SAT case. instance, (4),
Q-DLL assigns unit literals highest level. Figure 3 shows
minimal false subtree Q-DLLs computation, associated clause resolution
deduction empty clause. figure,
clause associated node written red right node
itself;
node corresponds assignment unit literal l, clause
causes l unit node (used corresponding clause resolution) written
red left node.
383

fiGiunchiglia, Narizzano & Tacchella

{}{{x1 , y, x2 }, {x1 , y, x3 }, {y, x2 }, {y, x2 , x3 }, {x2 , x3 }}{}
{x1 } hx1 , li {x1 }
{{y, x3 }, {y, x2 }, {y, x2 , x3 }, {x2 , x3 }}

{x1 } hx1 , ri {x1 }
{{y, x2 }, {y, x2 }, {y, x2 , x3 }, {x2 , x3 }}

hy, ri {x1 }
{x1 , y, x3 }hx3 , ui{x1 }
{y, x2 }hx2 , ui{y, x3 }
{{}} {x2 , x3 }

hy, ri {x1 }
{x1 , y, x2 }hx2 , ui{x1 }
{{}} {y, x2 }

Figure 3: clause resolution corresponding tree generated Q-DLL (4).
prefix x1 yx2 x3 .

Lemma 2 Let false QBF. Let minimal false subtree tree searched
Q-DLL(, ) assume node ; l , l unit l also
highest level . Let assignment . false, clause associated
node
-falsified;
contain existential literals whose negation assigned monotone
.
Proof. Trivial extension proof Lemma 1. assumption node ; l
, l unit l highest level , ensures clause associated
node -falsified.


Theorem 8 Let false QBF. Let minimal false subtree tree searched
Q-DLL(, ) assume node ; l , l unit l also
highest level . corresponds clause resolution deduction empty
clause.
Proof. Given Lemma 2, proof analogous one Theorem 7.



Regardless whether input formula true false, tree explored Q-DLL
may contain (exponentially many) subtrees whose nodes false.
procedure described above, allows us associate clause resolution deduction
subtrees.
input formula true, situation simpler far unit
universal literals, use tree searched Q-DLL generate deduction
empty term . Intuitively, process analogous one described
false, except leaves term resolution deduction terms corresponding
assignments computed Q-DLL entailing matrix . details:
384

fiClause/Term Resolution Learning Quantified Boolean Formulas

First, restrict attention minimal true subtree , i.e., tree
obtained deleting subtrees starting left split existential
literal: Analogously case false, leaf minimal true
subtree terminates empty matrix.
Second, associate node term, represented set, follows:
term associated leaf minimal term min(T ) 1
1. contain universal literals assigned monotone,
2. propositionally entail matrix, i.e., clause C matrix
, C 6= ,
3. subset literals , i.e., {l : l }.
term associated node ; l,
1. l 6 term associated node . Notice l
either existential universal monotone , l 6 .
2. l consider also term 0 associated node
; l. l 6 0 0 term associated (as first case).
l 0 , term associated resolvent 0 .
easy see term associated node -entailed: literal
also .
Lemma 3 Let true QBF. Let minimal true subtree tree searched
Q-DLL(, ). Let assignment . true, term associated
node
-entailed;
contain universal literals assigned monotone.
Proof. Analogous proof Lemma 2.



Theorem 9 Let true QBF. Let minimal true subtree tree searched
Q-DLL(, ). corresponds model generation term resolution deduction
empty term.
Proof. Let sequence terms obtained listing terms associated
nodes visited post order. Clearly, model generation term resolution deduction. deduction empty term node associated -entailed
term (Lemma 3), i.e., empty term.

before, regardless whether input formula true false, tree explored
Q-DLL may contain (exponentially many) subtrees whose nodes associated
1. sake efficiency, also important term satisfies properties. However,
necessary time being, discussed next section.

385

fiGiunchiglia, Narizzano & Tacchella

{}{{x1 , y, x2 }, {x1 , y, x3 }, {y, x2 }, {y, x2 , x3 }, {x2 , x3 }}{}
{x1 } hx1 , ri {x1 }
{x1 } hx1 , li {x1 }
{{y, x3 }, {y, x2 }, {y, x2 , x3 }, {x2 , x3 }} {{y, x2 }, {y, x2 }, {y, x2 , x3 }, {x2 , x3 }}
hy, li {y}
hx2 , pi{y}
{y, x2 } {} {y}

hy, li {y}
hx2 , pi{y}
{y, x2 } {} {y}

Figure 4: term resolutions corresponding tree generated Q-DLL (4).
prefix x1 yx2 x3 .

{}{{x1 , y, x2 }, {x1 , y, x3 }, {y, x2 }, {y, x2 , x3 }, {x2 , x3 }}{}
{x1 } hx1 , li {x1 }
{{y, x3 }, {y, x2 }, {y, x2 , x3 }, {x2 , x3 }}

{x1 } hx1 , ri {x1 }
{{y, x2 }, {y, x2 }, {y, x2 , x3 }, {x2 , x3 }}

hy, li {y}
hy, li {y}
hy, ri {x1 }
hy, ri {x1 }
hx2 , pi{y} {x1 , y, x3 }hx3 , ui{x1 }
hx2 , pi{y} {x1 , y, x2 }hx2 , ui{x1 }
{y, x2 } {} {y}
{{}} {y, x2 }
{y, x2 }hx2 , ui{y, x3 } {y, x2 } {} {y}
{{}} {x2 , x3 }

Figure 5: resolution corresponding tree generated Q-DLL (4). prefix
x1 yx2 x3 .

assignments true. described procedure allows us associate
term resolution deduction subtrees. instance, (4)
two maximal subtrees, roots x1 ; x1 ; y. associated deductions
represented Figure 4. figure,
represent also nodes along path root subtrees,
term associated node written green right node
itself,
leaf, non-contradictory term entailing matrix whose minimal
form min(T ) associated , written green left .
Merging trees Figures 3 4 obtain whole tree deductions corresponding search tree explored Q-DLL (represented Figure 5) clause
term resolutions intermixed.
386

fiClause/Term Resolution Learning Quantified Boolean Formulas

consider case input QBF false longer assume
literals assigned unit highest level. restrict
attention minimal false subtree tree searched Q-DLL(, ). Then,
procedure described associating clause node may longer work.
one thing, given leaf , may -falsified clauses matrix input
formula. However, guaranteed existence -contradicted clause
matrix input formula. clause C -contradicted if2
literal l C, l ;
existential literal l C, l .
long associate node -contradicted clause (either belonging
matrix obtained clause resolution) corresponds clause resolution
deduction empty clause: Indeed clause associated root
empty (remember resolvent clause resolution minimal form). Thus,
obvious solution try associate
1. leaf -contradicted clause input formula,
2. internal node -contradicted clause obtained resolving input clauses
and/or previously deduced clauses along lines outlined before.
cases process runs smoothly. Consider instance, QBF form:
x1 x2 yx3 {{x1 , x3 }, {x2 , x3 }, {x2 , y, x3 }, . . .}.

(7)

Then, assume split x1 occurs first, following path explored (we
using conventions Figure 3):
hx1 , li
hx3 , ui
hx2 , ui
{{}}

(8)

clause associated node are:
hx1 , li
{x1 , x3 } hx3 , ui
{x2 , x3 } hx2 , ui
{{}}

{x1 }
{x1 }
{y, x3 }
{x2 , y, x3 }

see that:
1. clause associated leaf = x1 ; x3 ; x2 -falsified -contradicted;

2. respect definition contradictory clause given Section 3.2, clear clause C
contradictory -contradicted. Further, QBF assignment , exists
-contradicted clause matrix , contains -contradicted clause,
contains contradictory clause.

387

fiGiunchiglia, Narizzano & Tacchella

0 function Rec-C-Resolve(, C1 , C2 , l, )
1
:= {l : l C1 , l C2 };
2
(S = ) return C-Resolve(C1 , C2 );
3
l1 := han existential literal C1 level level literals C1 }i;
4
C := ha clause causes l1 unit 0 , 0 ; l1 prefix i;
5
C3 := C-Resolve(C1 , C);
6
return Rec-C-Resolve(, C3 , C2 , l, ).
Figure 6: algorithm Rec-C-Resolve.
2. able associate node -contradicted clause.
Unfortunately, cases things run smoothly, i.e., may possible
associate clause internal node simple single resolution input and/or
previously deduced clauses. Indeed, clause resolutions may blocked
universal variables occurring clauses used resolution.
Consider instance QBF form (obtained (7) replacing clause {x2 , x3 }
{x2 , y, x3 }):
x1 x2 yx3 {{x1 , x3 }, {x2 , y, x3 }, {x2 , y, x3 }, . . .}.

(9)

Then, (8) would still valid path, corresponding clause resolutions would be:
hx1 , li
{x1 , x3 } hx3 , ui
{x2 , y, x3 } hx2 , ui . . .
{{}} {x2 , y, x3 }

(10)

possible perform clause resolution associated node
label hx2 , ui. example, clause resolution (5) may blocked
blocking universal literal l
l l ,
l C1 l C2 .
Since C1 C2 minimal form, possible C1 C2 contain
existential literal l0
level less equal level literals clause;
assigned unit.
Then, obvious solution get rid, e.g., blocking literals l C1 resolving
away C1 existential literals level lower level l.
idea behind procedure Rec-C-Resolve Figure 6. figure,
assume
1. input QBF;
388

fiClause/Term Resolution Learning Quantified Boolean Formulas

2. ; l assignment;
3. l existential literal either unit highest level ;
4. C1 clause containing l, minimal form ; l-contradicted;
5. C2 clause containing l, minimal form ; l-contradicted. Further, l unit
, C2 clause causes l unit ;
6. C-Resolve(C1 , C2 ) returns resolvent clause resolution two
clauses C1 C2 .
on, h, C1 , C2 , l, satisfies first 5 conditions, say
pair hC1 , C2 ; l-Rec-C-Resolved (in ). Given two clauses hC1 , C2
; l-Rec-C-Resolved:
1. set universal literals blocking clause resolution C1 C2
computed (line 1).
2. empty, simply return resolvent C1 C2 (line 2);
otherwise
3. pick existential literal l1 C1 minimum level C1 (line 3): l1
assigned unit earlier search, consider clause C caused
l1 assigned unit (line 4). C3 resolvent C1 C (line 5),
Rec-C-Resolve(, C3 , C2 , l, ) returned (line 6).
hC1 , C2 ; l-Rec-C-Resolved , Rec-C-Resolve(, C1 , C2 , l, ) returns minimal clause -contradicted without existential literals whose negation
assigned monotone . formally stated following lemma.
Lemma 4 Let C1 C2 two clauses hC1 , C2 ; l-Rec-C-Resolved
QBF . Rec-C-Resolve(, C1 , C2 , l, ) terminates returns clause
minimal form -contradicted;
contain existential literals whose negation assigned monotone .
proof lemma quite long reported appendix.
Assuming input QBF false, construction deduction empty
clause (associated minimal false subtree tree searched Q-DLL)
following:
every leaf , associate clause C input formula contradicted.
C clause associated node ; l,
1. l 6 C l universal C clause associated parent
; l, i.e., node . Notice l existential monotone
l 6 C.
389

fiGiunchiglia, Narizzano & Tacchella

2. l C l unit clause associated node
result Rec-C-Resolve(, C, C 0 , l, ), C 0 clause causes l
unit .
3. l C, l existential unit , consider also
clause C 0 associated node ; l. l 6 C 0 C 0 clause associated
(as first case). l C 0 , clause associated node
result Rec-C-Resolve(, C, C 0 , l, ).
example, (9) reference deduction (10), blocked resolution one associated node x1 ; x3 ; x2 . Rec-C-Resolve(, {x2 , y, x3 }, {x2 , y, x3 }, x2 , x1 ; x3 )
1. line 5, resolves {x2 , y, x3 } {x1 ; x3 }, resolvent C3 min({x1 , x2 , y}) =
{x1 , x2 };
2. following recursive call Rec-C-Resolve(, {x1 , x2 }, {x2 , y, x3 }, x2 , x1 ; x3 ) line 6
returns {x1 , y, x3 }.
Thus, clause associated node are:
hx1 , li
{x1 , x3 } hx3 , ui
{x2 , y, x3 } hx2 , ui
{{}}

{x1 }
{x1 }
{x1 , y, x3 }
{x2 , y, x3 }

Notice that, reference Figure 6, choice eliminating blocking literals
C1 maintaining C2 invariant, arbitrary. Indeed, could eliminate blocking
literals C2 maintain C1 invariant. case deduction (10), amounts
eliminate universal literal {x2 , y, x3 }: resolving clause {x1 , x3 }
x3 , get resolvent {x1 , x2 }, leads following legal deduction:
hx1 , li
{x1 , x3 } hx3 , ui
(From {x2 , y, x3 }, {x1 , x3 }) {x1 , x2 } hx2 , ui
{{}}

{x1 }
{x1 }
{x1 , y, x3 }
{x2 , y, x3 }

Lemma 5 Let false QBF. Let minimal false subtree tree searched
Q-DLL(, ). Let assignment . false, clause associated
node
minimal form -contradicted;
contain existential literals whose negation assigned monotone
.
Proof. construction, clause associated leaf -contradicted.
show also clause C associated internal node -contradicted,
assuming clause C 0 associated child ; l ; l-contradicted. also
child ; l, also assume clause C 00 associated child ; l ; l-contradicted.
390

fiClause/Term Resolution Learning Quantified Boolean Formulas

1. l 6 C 0 l universal C = C 0 . Hence, C minimal form. Since l 6 C 0
l universal, C 0 ; l-contradicted C 0 -contradicted. thesis
follows C = C 0 .
2. l C 0 l unit C =Rec-C-Resolve(, C 0 , C 00 , l, ), C 00
clause causes l unit . thesis follows Lemma 4.
3. l C, l existential unit , consider also clause
C 0 associated node ; l. Assuming l C 0 (otherwise would first
case), clause associated result Rec-C-Resolve(, C, C 0 , l, ).
previous case, thesis follows Lemma 4.

Theorem 10 Let false QBF. Let minimal false subtree tree searched
Q-DLL(, ). corresponds clause resolution deduction empty clause.
Proof. Given Lemma 5, proof analogous one Theorem 7.



4. Backjumping Learning DLL Based Procedures QBFs
section first show computing resolvent associated node allows
backjump branches backtracking (Subsection 4.1). Then, show
learning resolvents allows prune search tree branches different ones
resolvents computed learned (Subsection 4.2).
4.1 Conflict Solution Directed Backjumping
procedure described Section 3.2 uses standard backtracking schema whenever
empty clause (resp. matrix) generated: Q-DLL backtrack first existential
(resp. universal) literal assigned left split. instance, given QBF
y1 x1 y2 x2 x3 {{y1 , y2 , x2 }, {y1 , 2 , x2 , x3 }, {y1 , x2 , x3 },
{y 1 , x1 , x3 }, {y 1 , y2 , x2 }, {y 1 , y2 , x2 }, {y 1 , x1 , 2 , x3 }},

(11)

tree searched Q-DLL represented Figure 7, use conventions
Section 3.
2001 work Giunchiglia, Narizzano, Tacchella (2001), shown
exploration branches necessary. particular, input QBF
assignment, show possible compute reason (un)satisfiability
backtracking. Intuitively speaking, reason result subset
literals assignment 0
assigns true false literals assigned (i.e., {|l| :
l 0 } = {|l| : l });
extends (i.e., {l : l 0 }),
391

fiGiunchiglia, Narizzano & Tacchella

{}
{{y1 , y2 , x2 }, {y1 , 2 , x2 , x3 }, {y1 , x2 , x3 },
{y 1 , x1 , x3 }, {y 1 , y2 , x2 }, {y 1 , y2 , x2 }, {y 1 , x1 , 2 , x3 }}
{y 1 }hy 1 , li{y 1 }
{{y2 , x2 }, {y 2 , x2 , x3 }, {x2 , x3 }}
hy 2 , li{y 1 }
{y 1 , 2 , x2 }hx2 , ui{y 1 }
{y 1 , x2 , x3 }hx3 , ui{y 1 }
{y 1 , x2 , x3 } {} {y 1 }

hy1 , ri{}
{{x1 , x3 }, {y2 , x2 }, {y2 , x2 }, {x1 , 2 , x3 }}

hy2 , ri
{{x2 , x3 }, {x2 , x3 }}
hx2 , li
hx3 , ui
{}

hx1 , li{}
{y 1 , x1 , x3 }hx3 , ui{}
hy 2 , pi{}
{y 1 , y2 , x2 }hx2 , ui{}
{{}} {y 1 , y2 , x2 }

hx1 , ri
hx3 , pi
hy 2 , pi
hx2 , ui
{{}}

Figure 7: resolution corresponding tree generated Q-DLL (11). prefix
y1 x1 y2 x2 x3 .

0 equivalent . Then, computing reasons, avoid right split
literal l l reason: assigning l false would change result.
resulting procedure generalization QBF popular Conflict-directed Backjumping
(CBJ) (Prosser, 1993b), also introduces concept Solution-directed Backjumping
(SBJ), avoiding useless splits universal variables.
later paper, Giunchiglia, Narizzano, Tacchella (2003) show possible
optimize computation reasons. particular, paper, shown
assuming unsatisfiable, consider reasons subset existential
literals ,
assuming satisfiable, consider reasons subset universal
literals .
Apart optimizations, tree searched procedures described former
latter papers same, and, case (11), exploration branches
starting hy2 , ri, hx1 , ri skipped (see Figure 7).
show computation resolutions corresponding Q-DLL allows
avoid exploration branches pretty much CBJ SBJ do: case
QBF (11), branches skipped skipped CBJ SBJ.
key point think Q-DLL procedure producing clause (resp. term)
deduction empty clause (resp. term), proving unsatisfiable (resp. satisfiable).
Then, according rules use associating deduction tree searched QDLL, that:
C clause associated node ; l l 6 C, clause associated
node C, even l existential assigned left split.
392

fiClause/Term Resolution Learning Quantified Boolean Formulas

0 function Q-DLL-BJ(, )
1
(ha clause C -contradictedi)
2
return C;
3
(hthe matrix emptyi) return ModelGenerate();
4
(hl unit i)
5
C := ha clause matrix causes l unit i;
6
W R := Q-DLL-BJ(, ; l);
7
(hW R termi l 6 W R) return W R;
8
return Rec-C-Resolve(, W R, C, l, );
9
(hl monotone i) return Q-DLL-BJ(, ; l);
10
l := ha literal highest level i;
11
W R := Q-DLL-BJ(, ; l);
12
(hl existentiali (hW R termi l 6 W R)) return W R;
13
(hl universali (hW R clausei l 6 W R)) return W R;
14
W R0 := Q-DLL-BJ(, ; l);
15
(hl existentiali (hW R0 termi l 6 W R0 )) return W R0 ;
16
(hl universali (hW R0 clausei l 6 W R0 )) return W R0 ;
17
(hl existentiali) return Rec-C-Resolve(, W R0 , W R, l, );
18
return T-Resolve(W R0 , W R, l, ).
Figure 8: algorithm Q-DLL-BJ.
Analogously, term associated node ; l l 6 , term
associated node , even l universal assigned left
split.
rules take account clause/term associated node ; l,
thus need explore branch starting ; l.
Consider example Figure 7, use standard conventions and, e.g., write
clause (resp. term) associated node red (resp. green) right
node. reference figure, clear considering term {y 1 } associated
node 1 ; 2 , need explore branch starting hy2 , ri order
associate 1 -entailed term node 1 . Similarly, considering empty clause {}
associated node y1 ; x1 , need explore branch starting
hx1 , ri order associate y1 -contradicted clause node y1 .
procedure Q-DLL-BJ(, ) Figure 8 incorporates ideas. figure,
ModelGenerate() returns minimal form non-contradictory -entailed
term
clause C , C 6= ;
universal literal l assigned monotone, l 6 .
Rec-C-Resolve(, C1 , C2 , l, ) Figure 6.
393

fiGiunchiglia, Narizzano & Tacchella

T-Resolve(T1 , T2 ) returns resolvent term resolution two terms
T1 T2 .
behavior Q-DLL-BJ illustrated words saying Q-DLL-BJ(, )
computes returns clause/term would associated node tree
explored Q-DLL. particular, assuming
W R clause (resp. term) returned Q-DLL-BJ(, ; l);
l existential (resp. universal);
l assigned left split,
Q-DLL-BJ(, ) explore branch starting ; l l 6 W R (resp. l 6 W R),
see line 12 (resp. line 13) Q-DLL-BJ.
far, reference Figure 7, interpret clause (resp. term) red
(resp. green) right node value returned Q-DLL-BJ(, ). Then,
considering term {y 1 } associated node 1 ; 2 , Q-DLL-BJ explore
branch starting hy2 , ri. Similarly, considering empty clause {} associated
node y1 ; x1 , Q-DLL-BJ explore branch starting hx1 , ri.
Theorem 11 Q-DLL-BJ(, ) returns empty clause false, empty term
true.
Proof.(Sketch) enough notice that:
node associated clause C, C -contradicted, C result
sequence clause resolutions.
node associated term , -entailed, result
sequence model generations term resolutions.
Then, previous section:
empty clause associated initial node , false.
empty term associated initial node , true.



4.2 Learning
Learning well known technique SAT avoiding useless traversal branches.
SAT, learning amounts storing (clause) resolvents associated nodes tree
explored DLL: resolvents called nogoods simply added set
input clauses.
case QBFs, situation different complicated. Indeed,
two types resolutions (term clause), resolvents clause resolutions
added conjunctively matrix, resolvents term resolutions (that
call goods) considered disjunction matrix.
practice, handle three sets formulas:
394

fiClause/Term Resolution Learning Quantified Boolean Formulas

set terms corresponding goods learned search;
set clauses corresponding matrix input QBF;
set clauses corresponding nogoods learned search.
Formally, QBF form (1), QBF Extended Learning (EQBF)
expression form
Q1 z1 . . . Qn zn h, ,
(n 0)
(12)

set terms, also called goods, interpreted disjunctively. good
obtained model generation and/or term resolution ;
set clauses, also called nogoods, interpreted conjunctively. nogood
obtained clause resolution .
Clearly,
Q1 z1 . . . Qn zn ( )

Q1 z1 . . . Qn zn ( )
equivalent (1).
Initially empty set, input set clauses. search
proceeds,
Nogoods determined backtracking contradiction (i.e., assignment unsatisfiable) possibly added ;
Goods determined backtracking solution (i.e., assignment
satisfiable) possibly added .
following, use term constraints want refer goods
nogoods indifferently.
Consider EQBF (12). constraints and/or , search
pruned considerably. Indeed, descending search tree, literal assigned
long guaranteed reconstruct valid clause/term deduction
backtracking empty clause/term. availability already derived clauses/terms
allows prune search constraints : Given assignment ,
exists -contradicted clause C (resp. -satisfied term ) stop
search return C (resp. ). term -satisfied
literal l , l ;
universal literal l , l .
Clearly, -entailed term also -satisfied. Further, extend notion unit
take account constraints and/or . literal l
unit EQBF (12)
395

fiGiunchiglia, Narizzano & Tacchella

0 function Rec-Resolve(, W1 , W2 , l, )
1
:= {l : l W1 , l W2 };
2
(S = ) return Resolve(W1 , W2 );
3
l0 := ha literal W1 level level literals W1 }i;
4
W := ha constraint causes l0 unit 0 , 0 ; l0 prefix i;
5
W3 := Resolve(W1 , W );
6
return Rec-Resolve(, W3 , W2 , l, ).
Figure 9: algorithm Rec-Resolve.
either l existential 0,
clause {l, l1 , . . . , lm } belongs ,
expression |li | (1 m) occurs right |l| prefix
(12).
l universal 0,
term {l, l1 , . . . , lm } belongs ,
expression |li | (1 m) occurs right |l| prefix
(12).
definition monotone literals, crucial property ensured
dealing EQBFs, existential (resp. universal) literal l assigned monotone
; l never enter nogood (resp. good) associated node extending ; l.
guaranteed defining literal l monotone pure if3
either l existential l belong constraint ;
l universal l belong constraint .
possibility assigning also universal literals unit, may case
term resolutions may blocked existential literals l l,
occurring one terms used antecedents term resolution. However,
procedure Rec-C-Resolve presented Subsection 3.3 easily generalized
work also case constraints resolved terms. result
procedure Rec-Resolve(, W1 , W2 , l, ) Figure 9, assumed
1. EQBF;
3. various ways guarantee existential literal l assigned monotone ; l
enter nogood associated node extending ; l. Another one
keep definition existential monotone literal unchanged: existential literal assigned
monotone (12) l belong clause ;
update (or proceed search updated to) \ {C : C , l C}.
Analogously universal monotone literals. See work Giunchiglia, Narizzano Tacchella (2004a) details possibilities, including discussion interaction
monotone rule learning.

396

fiClause/Term Resolution Learning Quantified Boolean Formulas

2. ; l assignment;
3. l existential (resp. universal) literal either unit highest level
;
4. W1 clause (resp. term) containing l (resp. l), minimal form ; l-contradicted
(resp. ; l-satisfied);
5. W2 clause (resp. term) containing l (resp. l), minimal form ; l-contradicted
(resp. ; l-satisfied). Further, l unit , W2 clause (resp. term)
causes l unit ;
6. existential (resp. universal) literal l0 assigned unit 0 ; l0 , 0 ; l0
prefix ; l, clause (resp. term) causes l0 unit
0 .
7. Resolve(W1 , W2 ) returns C-Resolve(W1 , W2 ) (resp. T-Resolve(W1 , W2 )).
h, W1 , W2 , l, satisfy first 6 7 conditions, say pair hW1 , W2
; l-Rec-Resolved (in ).
above, (12), l defined EQBF obtained
removing (resp. ) clauses C (resp. terms ) l C (resp.
l ), removing l (resp. l) clauses (resp. terms
);
removing Q|l| prefix.
= l1 ; l2 ; . . . ; lm (m 0), defined (. . . ((l1 )l2 ) . . .)lm .
hW1 , W2 ; l-Rec-Resolved , Rec-Resolve(, W1 , W2 , l, ) returns
constraint minimal form -contradicted -satisfied, stated following
lemma.
Lemma 6 Let W1 W2 two clauses (resp. terms) hW1 , W2 ; lRec-Resolved EQBF . Rec-Resolve(, W1 , W2 , l, ) terminates returns minimal
clause (resp. term)
-contradicted (resp. -satisfied);
contain existential literals whose negation (resp. universal literals
been) assigned monotone .
Proof.(Sketch) proof equal (resp. analogous to) proof Lemma 4 l
existential (resp. universal).

procedure Q-DLL-LN (, ) incorporates new definitions ideas,
represented Figure 10. Considering figure,
definition ModelGenerate() relaxed respect definition provided Subsection 4.1 order return minimal form non-contradictory
-satisfied term
397

fiGiunchiglia, Narizzano & Tacchella

0 := {};
1 := {};
2 function Q-DLL-LN (, )
3
Q := hthe prefix i;
4
:= hthe matrix i;
5
(ha -contradicted clause C i)
6
return C;
7
(ha -satisfied term i)
8
return ;
9
(hthe matrix emptyi) return ModelGenerate();
10
(hl unit (Qh, , i) i)
11
W := ha constraint causes l unit (Qh, , i) i;
12
W R := Q-DLL-LN (, ; l);
13
(hl existentiali (hW R termi l 6 W R)) return W R;
14
(hl universali (hW R clausei l 6 W R)) return W R;
15
W R := Rec-Resolve(Qh, , i, W R, W, l, );
16
Learn(, W R);
17
return W R;
18
(hl monotone (Qh, , i) i) return Q-DLL-LN (, ; l);
19
l := ha literal highest level i;
20
W R := Q-DLL-LN (, ; l);
21
(hl existentiali (hW R termi l 6 W R)) return W R;
22
(hl universali (hW R clausei l 6 W R)) return W R;
23
W R0 := Q-DLL-LN (, ; l);
24
(hl existentiali (hW R0 termi l 6 W R0 )) return W R0 ;
25
(hl universali (hW R0 clausei l 6 W R0 )) return W R0 ;
26
W R := Rec-Resolve(Qh, , i, W R0 , W R, l, );
27
Learn(, W R);
28
return W R.
Figure 10: algorithm Q-DLL-LN.
clause C , C 6= ;
universal literal l assigned monotone, l 6 .
Learn(, W R) updates set goods nogoods according given policy.
simply assume Learn(, W R) updates 0 0 respectively,
0 0 satisfy following conditions:
0 subset {W R} W R term, otherwise;
0 subset {W R} W R clause, otherwise;
existential (resp. universal) literal l assigned unit initial prefix
0 ; l , 0 (resp. 0 ) still contains clause (resp. term) causes l
assigned unit (Qh0 , , 0 i)0 .
398

fiClause/Term Resolution Learning Quantified Boolean Formulas

reference Figure 9, last condition necessary order guarantee
existence constraint W satisfying condition line 4.
conditions Learn(, W R) general ensure soundness
completeness Q-DLL-LN.
Theorem 12 Q-DLL-LN(, ) returns empty clause false, empty term
true.
Proof. Analogous proof Theorem 11.



understand benefits learning, assume input QBF (4). corresponding
EQBF
x1 yx2 x3 h{}, {{x1 , y, x2 }, {y, x2 }, {x2 , x3 }, {x1 , y, x3 }, {y, x2 , x3 }}, {}i,
search proceeds Figure 2, first path leading empty matrix,
starts term resolution process. Assuming term min({y, x2 }) = {y} added
set goods checking value x1 ;y , soon x1 assigned true,
detected unit correspondingly assigned;
path corresponding assignment x1 ; explored.
example shows, (good) learning avoid useless exploration branches
would explored backtracking backjumping schema. Indeed,
assuming deduced term learned backtracking. policy according
Learn(, W R) simply adds W R
W R clause;
otherwise,
easily implemented. However, simple policy may easily lead store exponential number goods and/or nogoods (notice call Learn(, W R)
literal assigned unit right split). Thus, practical implementations incorporate
policies guaranteed space bounded, i.e., ones store polynomial number goods
nogoods most. SAT, three popular space bounded learning schemes are:
Size learning order n (Dechter, 1990): nogood added
cardinality less equal n. added, never deleted.
Relevance learning order n (Ginsberg, 1993): given current assignment ,
nogood C always added , deleted soon number
literals l C l 6 bigger n.
Unique Implication Point (UIP) based learning (Marques-Silva & Sakallah, 1996):
nogood C stored C contains one literal maximum decision
level. Given assignment , decision level literal l number
splits done l . UIP based learning, set added clauses
periodically inspected clauses deleted according various criteria.
399

fiGiunchiglia, Narizzano & Tacchella

Thus, size learning, nogood stored, never deleted. relevance UIP
based learning, nogoods dynamically added deleted depending current assignment. See work Bayardo (1996) details related size relevance
learning (including complexity analysis), work Zhang, Madigan, Moskewicz
Malik (2001) discussion various UIP based learning mechanisms SAT. Size,
relevance, UIP based learning various possibilities limiting
number stored clauses, one generalized various ways considering
QBFs instead SAT formulas. next section, present particular learning
schema implemented QuBE.

5. Implementation Experimental Analysis
section first describe details implementation nogood good
learning QuBE, report experimental analysis conducted order
evaluate (separate) benefits nogood good learning, also relative efficiency
solver compared state-of-the-art QBF solvers.
5.1 Implementation QuBE
evaluate benefits deriving learning, implemented good nogood learning QuBE. QuBE QBF solver based search which, non-random
instances, compares well respect state-of-the-art solvers based search, like
semprop (Letz, 2002), yquaffle (Zhang & Malik, 2002a), i.e., best solvers based
search non-random instances according (Le Berre, Simon, & Tacchella, 2003),
see (Giunchiglia, Narizzano, & Tacchella, 2004c) details.
Besides learning, version QuBE used features
efficient detection unit monotone literals using lazy data structures (Gent,
Giunchiglia, Narizzano, Rowley, & Tacchella, 2004);
branching strategy exploits information gleaned input formula initially,
leverages information extracted learning phase.
See (Giunchiglia, Narizzano, & Tacchella, 2004b) description characteristics.
learning, computation nogoods goods corresponding internal
nodes search tree carried clause term resolution
working reason initialized backtracking starts, reasons stored
descending search tree
unit literal, stored reason constraint literal unit;
literal assigned right split, stored reason constraint computed
backtracking left branch;
monotone literals, way working reasons initialized ensures existential
(resp. universal) monotone literals never belong working reason computed
backtracking contradiction (resp. solution).
400

fiClause/Term Resolution Learning Quantified Boolean Formulas

Assume = l1 ; l2 ; . . . ; lm assignment corresponding leaf consideration. Considering problem initializing working reason, way
QuBE
return -contradicted clause matrix input QBF set learned
nogoods, contradiction;
compute minimal form -satisfied prime implicant matrix contains universal literals possible, solution.
second case, computation prime implicant important order short
reasons, possible universal literals important order backjump
nodes. requirements met recursively removing irrelevant literals
set literals , starting universals ones. Given set literals, say
literal irrelevant clause C matrix l C exists another
literal l0 l0 V
C. prime() set literals result recursive
procedure, term prime()
satisfied ;
prime implicant matrix input QBF;
exist another term satisfying first two properties
smaller (under set inclusion) set universal literals.
order reduce number universal literals initial goods, take
advantage fact assignment may partial: literal l may
case neither l l . Then, use existential literals ,
level lower level universal literals assigned left split,
order reduce number universals prime(). fact, sequence 0
literals extending existential literals, set universals prime(0 ) subset
prime(). instance, considering QBF (11) = 1 ; y2 ; x2 ; x3 ,
prime() {y 1 , y2 , x2 , x3 };
extend 0 = ; x1 prime(0 ) {x1 , y2 , x2 , x3 }.
Finally, evaluating universal literals irrelevant, follow reverse
order assigned, order try backjump high possible
search tree.
said previous section, besides problem setting initial working
reason, another problem learning unconstrained storage clauses (resp. terms)
obtained reasons conflicts (resp. solutions) may lead exponential memory
blow up. practice, necessary introduce criteria
1. limiting constraints learned; and/or
2. unlearning them.
401

fiGiunchiglia, Narizzano & Tacchella

implementation learning QuBE works follows. Assume backtracking
literal l assigned decision level n. constraint corresponding reason
current conflict (resp. solution) learned following conditions satisfied:
1. l existential (resp. universal);
2. assigned literals reason except l, decision level strictly lower
n;
3. open universal (resp. existential) literals reason l
prefix.
Notice three conditions ensure l unit constraint corresponding
reason. QuBE learned constraint, backjumps node
maximum decision level among literals reason, excluding l. say l
Unique Implication Point (UIP) therefore lookback QuBE UIP based.
Notice definition UIP generalizes QBF concepts first described Silva
Sakallah (1996) used SAT solver grasp. SAT instance, QuBE lookback
scheme behaves similarly 1-UIP-learning scheme used zCHAFF (and described
Zhang et al., 2001). Even QuBE guaranteed learn one clause (resp. term)
per conflict (resp. solution), still number learned constraints may blow up,
number backtracks exponential. stop course, QuBE scans periodically
set learned constraints search became irrelevant, i.e., clauses (resp.
terms) number open literals exceeds parameter n, corresponding
relevance order. Thus, implementation uses UIP based learning decide store
constraint, relevance based criteria decide forget constraint.
experimental analysis presented next subsection, parameter n set 20
set learned constraints scanned every 5000 nodes.
Besides learning mechanism, current version QuBE features lazy data
structures unit literal detection propagation (as described Gent et al., 2004),
monotone literal fixing (as described Giunchiglia et al., 2004a), Variable State Independent Decaying Sum heuristic (VSIDS) (as introduced SAT Moskewicz, Madigan,
Zhao, Zhang, & Malik, 2001). SAT, basic ideas heuristic (i) initially
rank literals basis occurrences matrix, (ii) increment weight
literals learned constraints, (iii) periodically divide constant weight
literal.
5.2 Experimental Results
evaluate effectiveness implementation, considered 450 formal verification planning benchmarks constituted part 2003 QBF solvers comparative
evaluation4 : 25% instances comes verification problems (described Scholl
& Becker, 2001; Abdelwaheb & Basin, 2000), remaining planning domains (described Rintanen, 1999; Castellini, Giunchiglia, & Tacchella, 2001). start
analysis considering QuBE without learning enabled. versions QuBE
4. respect non-random instances used 2003 QBF comparative evaluation, test set
include QBF encodings modal K formulas submitted Pan Vardi (2003).

402

fiClause/Term Resolution Learning Quantified Boolean Formulas

Figure 11: Effectiveness learning: QuBE versus QuBE(cbj,sbj). CPU time (left)
number backtracks instances solved solvers (right).

compute goods nogoods order backjump irrelevant existential universal
branching nodes. differ treatment computed goods nogoods:
learning enabled, QuBE records goods nogoods;
learning disabled, QuBE records neither nogoods goods.
call two versions QuBE(cln,sln) QuBE(cbj,sbj) respectively, order
specify type look-back used two systems. Notice consider
QuBE backtracking (i.e., version computes neither nogoods goods
performs simple chronological backtracking) competitive
solvers.
experiments run farm identical PCs, one equipped
Pentium 4, 3.2GHz processor, 1GB RAM, running Linux Debian (sarge). Finally,
system timeout value 900s per instance.
Figure 11 left shows performances QuBE(cln,sln) versus QuBE(cbj,sbj).
plot, x-axis CPU-time QuBE(cln,sln) y-axis CPU-time
QuBE(cbj,sbj). plotted point hx, yi represents benchmark QuBE(cln,sln)
QuBE(cbj,sbj) take x seconds respectively.5 convenience, also plot
points hx, xi, representing benchmarks solved QuBE(cln,sln) x seconds.
first observation learning pays off:
5. principle, one point hx, yi could correspond many benchmarks solved QuBE(cln,sln)
QuBE(cbj,sbj) x seconds respectively. However, scatter diagrams
present, point (except point h900, 900i, representing instances solvers
time-out) corresponds single instance cases.

403

fiGiunchiglia, Narizzano & Tacchella

Figure 12: Effectiveness learning random heuristic: QuBE(rnd,cln,sln)[3] versus QuBE(rnd,cbj,sbj)[3]. CPU time (left) number backtracks
instances solved solvers (right).

QuBE(cln,sln) (resp. QuBE(cbj,sbj)) able solve 16 (resp. 1) instances
solved QuBE(cbj,sbj) (resp. QuBE(cln,sln));
among instances solved solvers, QuBE(cln,sln) (resp. QuBE(cbj,sbj))
least one order magnitude faster QuBE(cbj,sbj) (resp. QuBE(cln,sln))
39 (resp. 0) instances.
order implementation-quality independent measure pruning introduced
learning, right plot figure shows number backtracks (i.e., number
solutions conflicts found) QuBE(cbj,sbj) versus QuBE(cln,sln) 358
problems solved systems. plotted point hx, yi represents benchmark
solved QuBE(cln,sln) QuBE(cbj,sbj) performing x backtracks
respectively. seen, learning substantially prunes search space:
point diagonal, meaning never case QuBE(cbj,sbj) performs
less backtracks QuBE(cln,sln).6 Still, learning overhead, thus
pruning caused learning always pays terms speed, proved
points diagonal left plot.
experimental data entirely satisfactory two reasons.
First, learning heuristic tightly coupled QuBE: Whenever QuBE learns
constraint, also increments score literals it. QuBE(cbj,sbj) constraint
6. imply tree searched QuBE(cln,sln) subtree tree searched
QuBE(cbj,sbj): Indeed, literal selected branching node two systems guaranteed
same.

404

fiClause/Term Resolution Learning Quantified Boolean Formulas

Figure 13: Effectiveness conflict learning:
QuBE(rnd,cln,sln)[3] versus
QuBE(rnd,cbj,sln)[3]. CPU time (left) number conflict backtracks
instances solved solvers (right).

ever learned. consequence, QuBE(cbj,sbj), (i) literals initially sorted
basis occurrences input QBF, (ii) score literal periodically
halved becomes 0. literals score 0, literals prefix
level chosen according lexicographic order.
Second, independently heuristic used, plot showing performances
QuBE without learning, say two learning schemes (conflict,
solution) effective (Gent & Rowley, 2004).
address first problem, consider QuBE random heuristic, i.e., heuristic
randomly selects literal among maximum level yet assigned.
call resulting systems QuBE(rnd,cln,sln) QuBE(rnd,cbj,sbj) respectively:
names suggest, first learning enabled, second learning
disabled. randomness, run solver 5 times instance. Then,
define QuBE(rnd,cln,sln)[i] system whose performances are,
instance, i-th best among 5 results obtained running QuBE(rnd,cln,sln)
instance. QuBE(rnd,cbj,sbj)[i] defined analogously.
Figure 12 shows CPU time (left) number backtracks solved instances
(right) QuBE(rnd,cln,sln)[3] QuBE(rnd,cbj,sbj)[3]. plots, easy
see QuBE(rnd,cln,sln)[3] faster QuBE(rnd,cbj,sbj)[3] cases.
witness fact
QuBE(rnd,cln,sln) (resp. QuBE(rnd,cbj,sbj)) able solve 21 (resp. 2) instances solved QuBE(cbj,sbj) (resp. QuBE);
405

fiGiunchiglia, Narizzano & Tacchella

Figure 14: Effectiveness solution learning:
QuBE(rnd,cln,sln)[3] versus
QuBE(rnd,cln,sbj)[3]. CPU time (left) number solution backtracks instances solved solvers (right).

among instances solved solvers, QuBE (resp. QuBE(cbj,sbj)) least
one order magnitude faster QuBE(cbj,sbj) (resp. QuBE) 68 (resp. 2)
instances.
Still, longer case enabling learning always causes reduction number
backtracks. different literals selected branching node,
also pruning node may prevent long backjump (Prosser, 1993a)
would cause vast reduction search space. Interestingly, comparing results
Figure 11, seems random heuristic learning becomes important.
fact witnesses also setting well known tension look-ahead look-back
techniques: smart look-ahead makes look-back less important, viceversa.
address second problem, considered systems QuBE(rnd,cbj,sln)
QuBE(rnd,cln,sbj), i.e., systems obtained QuBE(rnd,cln,sln) disabling
conflict learning solution learning respectively. usual, system run 5 times
instance, QuBE(rnd,cbj,sln)[i] QuBE(rnd,cln,sbj)[i] (1 5)
defined before. left plots Figures 13 14 show performances
QuBE(rnd,cln,sln)[3] versus QuBE(rnd,cbj,sln)[3] QuBE(rnd,cln,sbj)[3] respectively. also measured number backtracks. However, order better
highlight pruning due conflict (resp. solution) learning, right plot Figure 13
(resp. 14) shows number conflict (resp. solution) backtracks QuBE(rnd,cbj,sln)[3]
(resp. QuBE(rnd,cln,sbj)[3]). plots, see conflict solution
learning prune search space pay off: plot, points
well diagonal. Comparing two left plots, also see that, test set
406

fiClause/Term Resolution Learning Quantified Boolean Formulas

QuBE(rnd,cln,sln)[3]
QuBE(rnd,cln,sln)[1]
QuBE(rnd,cln,sln)[2]
QuBE(rnd,cln,sln)[4]
QuBE(rnd,cln,sln)[5]
QuBE(rnd,cbj,sbj)[1]
QuBE(rnd,cbj,sbj)[2]
QuBE(rnd,cbj,sbj)[3]
QuBE(rnd,cbj,sbj)[4]
QuBE(rnd,cbj,sbj)[5]
QuBE(rnd,cbj,sln)[1]
QuBE(rnd,cbj,sln)[2]
QuBE(rnd,cbj,sln)[3]
QuBE(rnd,cbj,sln)[4]
QuBE(rnd,cbj,sln)[5]
QuBE(rnd,cln,sbj)[1]
QuBE(rnd,cln,sbj)[2]
QuBE(rnd,cln,sbj)[3]
QuBE(rnd,cln,sbj)[4]
QuBE(rnd,cln,sbj)[5]

=
136
169
156
109
131
137
123
110
84
130
133
129
115
86
135
151
169
141
103

<
0
0
203
244
145
164
192
205
222
96
134
169
209
245
78
110
134
183
218

>
225
192
0
0
72
43
25
17
10
128
82
48
20
6
142
90
39
11
2


0
0
2
8
13
17
21
29
45
7
12
15
17
24
6
10
19
26
38


3
1
0
0
7
2
2
2
2
5
5
3
1
1
4
4
1
0
0

./
86
88
89
89
82
87
87
87
87
84
84
86
88
88
85
85
88
89
89

10<
0
0
27
61
27
43
68
83
99
20
27
40
54
87
7
15
29
51
69

0.1>
43
19
0
0
20
7
2
1
1
26
14
5
1
0
36
15
5
0
0


86
88
91
97
95
104
108
116
132
91
96
101
105
112
91
95
107
115
127

Table 2: Comparison among various versions QuBE. row compares system written first column respect QuBE(rnd,cln,sln)[3] taken reference.
QuBE(rnd,cln,sln)[3] B solver first column,
columns report number problems that: =, B solve
time; <, B solve takes less time B; >, B solve
takes time B; , solves B not; , solve
B does; ./, B solve; 10<, B solve
least one order magnitude faster; 0.1<, B solve
least one order magnitude slower; TO, B solve.
number timeouts QuBE(rnd,cln,sln)[3] 89.
considered, solution learning helps solving problems conflict learning:
QuBE(rnd,cbj,sln)[3] times 101 QuBE(rnd,cln,sbj)[3] times 107.
hand, two right plots suggest conflict learning prunes solution learning, conclusion correct. Indeed, plot shows either number
conflicts number solutions: Pruning node (no matter whether existential
universal) may avoid finding (exponentially many) solutions and/or conflicts. particular,
given instances CNF thus form
. . . yx1 x2 . . . xn
(n 1) pruning variable {x1 , x2 , . . . , xn } potential prune 2n conflicts.
407

fiGiunchiglia, Narizzano & Tacchella

detailed quantitative information CPU times reported Table 2. last column table see that, indicate TO(S)
number timeouts system S, then, {1, 2, 3, 4, 5},
TO(QuBE(rnd,cln,sln)[i]) <

TO(QuBE(rnd,cbj,sln)[i])
< TO(QuBE(rnd,cbj,sbj)[i]).
TO(QuBE(rnd,cln,sbj)[i])

gives indication capacity solvers, i.e., ability solve
problems. order get indication productivity, i.e., considering problems
solve, ability solve quickly, consider number FS(S)
difference 0.1 > 10 < columns: lower FS(S) is, better
is.
FS(QuBE(rnd,cln,sln)[i]) <

FS(QuBE(rnd,cbj,sln)[i])
< FS(QuBE(rnd,cbj,sbj)[i])
FS(QuBE(rnd,cln,sbj)[i])

{1, 2, 3, 4, 5}. above, clear conflict solution learning
allow improve capacity productivity. experimental results thus seem
contradict negative results reported Gents Rowleys work (2004) solution
based look-back mechanisms. However, results comparable ours, given
different mechanisms implemented respective solvers (e.g., computing
initial solution monotone literal fixing), different experimental setting (e.g.,
testset).

6. Conclusions Related Work
paper based extends (Giunchiglia et al., 2002) introduces nogood
good learning QBFs satisfiability. show correspondence computation trees searched DLL based QBF solvers clause/term resolution deductions.
Nogoods goods clauses terms respectively resolution deductions.
perspective, learning simply amounts storing nogoods goods. show
incorporate nogoods goods learning DLL based QBF solvers considering
EQBFs (QBFs extended learning), illustrate means examples
computation nogoods goods:
allows solution conflict directed backjumping spirit (Giunchiglia et al.,
2001, 2003);
stored, allows pruning branches parts search tree.
present high level description algorithms incorporating ideas, formally
prove soundness completeness. also discuss problems related effective
implementations DLL based QBF solvers, present (in details) implementation QuBE, state-of-the-art QBF solver. experimental analysis shows QuBE
enhanced nogood good learning effective, considering selection
nonrandom problems consisting planning formal verification benchmarks. also
show QuBE competitive respect state art.
408

fiClause/Term Resolution Learning Quantified Boolean Formulas

already said, work builds (Giunchiglia et al., 2002). papers dealing
learning QBFs satisfiability (Letz, 2002), (Zhang & Malik, 2002a) (Gent &
Rowley, 2004). particular, (Letz, 2002) conflict solution learning called lemma
model caching. paper also proposes technique based model caching dealing QBFs variable-independent subformulas. Zhang Malik (2002a) propose
conflict learning (which extended solution learning Zhang & Malik, 2002b).
second paper, terms called cubes. Gent Rowley (2004) introduce new form
solution learning: new technique revisits less solutions standard techniques,
experimental results reported paper positive. works share
intuitions thus propose similar techniques. Though difficult establish
precise relation among works due differences terminology and/or
different level detail presentations,7 believe main differences
implementation level, i.e., way solution conflict learning implemented.
therefore quite difficult impossible compare different alternatives, without
re-implementing recasting different learning mechanisms even different solvers
common framework. Indeed, specific learning mechanism implemented within
solver may motivated characteristics solver, e.g., data structures used heuristic. instance, watched data structures (used, e.g.,
QuBE, yquaffle semprop) allow efficient detection propagation unit pure literals (Gent et al., 2004). consequence, solvers watched
data structures may profitably maintain huge databases goods nogoods. solvers
standard data structures, costs involved managing huge databases may
overwhelm advantages. Considering solvers whole, experimental
analysis conducted (Giunchiglia et al., 2004c) shows solver QuBE compares
well respect semprop yquaffle 450 formal verification planning
benchmarks considered also paper.

Acknowledgments
would like thank Ian Gent Andrew Rowley discussions related subject
paper, anonymous reviewers suggestions corrections. work
partially supported MIUR.

Appendix A. Proof Lemma 4
proof well founded induction. Thus, steps follow are:
1. definition well founded order tuples hC1 , C2 , l, i;
2. proof thesis holds minimal elements partial order;
3. assuming thesis holds tuples hC3 , C2 , l, hC3 , C2 , l,
hC1 , C2 , l, i, proof thesis holds also hC1 , C2 , l, i.
7. instance, (Letz, 2002; Zhang & Malik, 2002b) also initial work (Giunchiglia et al.,
2002), method used computing initial working reason corresponding solution (procedure
ModelGenerate Figure 10) detailed.

409

fiGiunchiglia, Narizzano & Tacchella

deliberately omitted properties elements tuples
hC1 , C2 , l, partial order satisfy. Indeed, standard assumption would
C1 C2 two clauses hC1 , C2 ; l-Rec-C-Resolved. However,
sufficient. Indeed, may happen starting two clauses hC1 , C2
; l-Rec-C-Resolved (line numbers refer Figure 6)
1. set {l : l C1 , l C2 , l universal} empty (see line 1);
2. clause C3 computed line 5 Figure 6 ; l-contradicted; thus
3. tuple hC3 , C2 , l, element partial order.
better understand problem, consider following simple example:
x1 y1 x2 y2 x3 x4 {{x4 }, {x2 , y2 , x4 }, {y 2 , x3 }, {x1 , 1 , x3 }, {x1 , y1 , x2 , x3 }}.

(13)

QBF :
1. x4 ; x2 ; y2 ; x3 ; x1 assignment producing contradictory clause;
2. h{x1 , y1 , x2 , x3 },{x1 , 1 , x3 }i x4 ; x2 ; y2 ; x3 ; x1 -Rec-C-Resolved;
3. Rec-C-Resolve(, {x1 , y1 , x2 , x3 }, {x1 , 1 , x3 }, x1 , x4 ; x2 ; y2 ; x3 ; x1 ),
(a) causes call Rec-C-Resolve(, {x1 , y1 , y2 , x4 }, {x1 , 1 , x3 }, x1 , x4 ; x2 ; y2 ; x3 ; x1 ),
clause C3 = {x1 , y1 , y2 , x4 } x4 ; x2 ; y2 ; x3 ; x1 -contradicted;
(b) returns clause {y 1 , x3 } x4 ; x2 ; y2 ; x3 ; x1 -contradicted, expected.
fact universal literal y2 causes C3 x4 ; x2 ; y2 ; x3 ; x1 -contradicted
appear clause returned Rec-C-Resolve due following two facts:
1. y2 lower level blocking literal y1 ;
2. negation existential literals C3 level lower y2 assigned
y2 x4 ; x2 ; y2 ; x3 ; x1 .
formally define notions, need additional notation. First, consider
given clause C2 . ResC2 (C1 ) set literals C1 level lower literal
blocking resolution C1 C2 . Formally:
ResC2 (C1 ) = {l : l C1 , l0 BlockingC2 (C1 )level(l) < level(l0 )},

literal l, level(l) prefix level l;
BlockingC2 () function defined
BlockingC2 (C1 ) = {l : l C1 , l C2 , l universal}
Let assignment. say clause C1 -contradictable (with respect
C2 )
410

fiClause/Term Resolution Learning Quantified Boolean Formulas

1. existential literal l C1 , l ;
2. universal literal l C1 , l
(a) l ResC2 (C1 );
(b) existential literal l0 C1 , level(l0 ) < level(l) l0 left l
.
Clearly, clause -contradicted also -contradictable. Considering QBF
(13), clause {x1 , y1 , y2 , x4 } x4 ; x2 ; y2 ; x3 ; x1 -contradicted; x4 ; x2 ; y2 ; x3 ; x1 contradictable (with respect {x1 , 1 , x3 }).
well founded order induction set tuples hC1 , C2 , l,
C1 ; l-contradictable. preliminary step, first define well founded order
literals according l0 l00 either l0 = l00 l0 l00
l0 assigned l00 (i.e., l0 left l00 ).
extend partial order relation literals clauses (i) minimal form, (ii)
containing l, (iii) ; l-contradictable, saying two clauses C1 C3 ,
C3 C1
either C3 = C1 ;
E
E
E
E
00
0
00
l0 (ResE
C2 (C1 )\ResC2 (C3 ))l (ResC2 (C3 )\ResC2 (C1 ))l l , ResC2 (C1 )
subset existential literals ResC2 (C1 ), similarly ResE
C2 (C3 ).

order well founded, minimal elements ResC2 (C) (or,
equivalently, BlockingC2 (C)) empty.
Finally, consider set W tuples hC1 , C2 , l,
1. ; l assignment;
2. l existential literal either unit highest level ;
3. C1 clause containing l, minimal form ; l-contradictable respect
C2 ;
4. C2 contains l, minimal form ; l-contradicted. Further, l unit ,
C2 clause causes l unit .
set, define well founded order according hC3 , C2 , l, hC1 , C2 , l,
C3 C1 .
consider procedure Rec-C-Resolve Figure 6. prove well founded
induction that, tuple hC1 , C2 , l, W , Rec-C-Resolve(, C1 , C2 , l, ) terminates
returns clause C minimal form -contradicted. end, also show
assume C1 ; l-contradicted (and simply ; l-contradictable),
C contain existential literals whose negation assigned monotone
.
base case, C1 ResC2 (C1 ) empty. Hence, universal literal
l C1 , l thus C1 ; l-contradicted. Since ResC2 (C1 ) empty, set
computed line 1 empty thus Rec-C-Resolve(, C1 , C2 , l, ) terminates returning
411

fiGiunchiglia, Narizzano & Tacchella

resolvent C C1 C2 . Clearly C minimal form, easy show C
-contradicted.
step case, induction hypothesis, thesis holds Rec-CResolve(, C3 , C2 , l, ) show holds also Rec-C-Resolve(, C1 , C2 , l, ),
assuming hC3 , C2 , l, hC1 , C2 , l, i. set ResC2 (C1 ) empty, see base case.
Assume ResC2 (C1 ) empty, thus also BlockingC2 (C1 ) empty.
on, let l0 literal BlockingC2 (C1 ) highest level. l0
l0 6 ResC2 (C1 ) C1 ; l-contradictable. l0 l0 C2 C2
; l-contradicted. Further, level(l0 ) < level(l). see why, consider two possible
cases:
1. l unit : Since l0 C2 , C2 clause causes l unit ,
must level(l0 ) = level(l0 ) < level(l).
2. l highest level : Since l0 l0 l highest
level , level(l0 ) level(l). hand, level(l0 ) 6= level(l) l0
universal l existential.
Since C1 minimal form, exists existential literal l00 l00 C1 , l00
, level(l00 ) < level(l0 ) < level(l). on, let l1 existential literal
C1 (not necessarily distinct l00 ) level less equal level
literals C1 (see line 3). Since
level(l1 ) < level(l0 ) < level(l)

(14)

l1 (because C1 ; l-contradictable), follows l1 assigned unit,
thus exists clause C causes l1 unit 0 , 0 ; l1
initial prefix (see line 4).
Consider set
BlockingC (C1 ) = {l : l C1 , l C, l universal}.
BlockingC (C1 ) empty. fact, universal literal l00 C
level(l00 ) < level(l1 ) l00 6 C1 since C1 minimal form;
level(l00 ) > level(l1 ) l00 l1 . Assume l00 C1 . Since C1 ; l-contradictable,
l1 l00 . However, l00 l1 l1 l00 possible l1 6= l00 (l1 existential
l00 universal).
Since BlockingC (C1 ) empty, resolve C C1 l1 , obtaining
C3 = min((C1 C) \ {l1 , l1 })
resolvent. C3 minimal form contains l.
show C3 C1 remains showed C3 ; l-contradictable. Indeed,
existential literal l C3 , l , universal literals C3 , consider
two cases:
412

fiClause/Term Resolution Learning Quantified Boolean Formulas

1. BlockingC2 (C3 ) empty. case, l0 BlockingC2 (C3 ). easy
consequence following facts:
(a) literal l00 BlockingC2 (C), level(l00 ) < level(l0 ): l00 C2 definition
BlockingC2 (C), hence l00 C2 ; l-contradicted, therefore
level(l00 ) < level(l1 ), thus thesis (see (14));
(b) BlockingC2 (C3 ) = (BlockingC2 (C1 )BlockingC2 (C))C3 thus (BlockingC2 (C1 )
BlockingC2 (C)) \ BlockingC2 (C3 ) = (C C1 ) \ ({l1 , l1 } C3 ), i.e., literals
BlockingC2 (C1 ) BlockingC2 (C) BlockingC2 (C3 )
omitted minimal form C3 ;
(c) BlockingC2 (C3 ) empty.
Since l0 BlockingC2 (C3 ), literals ResC2 (C1 ) also C3 , also belong
ResC2 (C3 ), i.e.,
ResC2 (C3 ) ResC2 (C1 ) C3 .
(15)
consider universal literal l00 C1 C3 . l00
(a) l00 ResC2 (C1 ) C1 ; l-contradictable, hence l00 ResC2 (C3 )
(see (15));
(b) existential literal l000 C1 , level(l000 ) < level(l00 ) l000 l00
C1 ; l-contradictable;
(c) existential literal l000 6= l1 C, l000 l00 . fact, level(l1 ) < level(l00 ),
l1 l00 C1 ; l-contradictable, existential literal l000 6= l1
C, l000 l1 .
Finally, consider universal literal l00 C C3 . l00 level(l00 ) < level(l1 )
hence
(a) l00 ResC2 (C3 ) level(l1 ) < level(l0 ) (see (14));
(b) existential literal l000 C3 , level(l000 ) < level(l00 ) l000 C hence
l000 l00 .
2. BlockingC2 (C3 ) empty. Let lowest among level literals C3 .
level(l0 ) < since l0 6 C3 . Then, universal literal l00 C3 , l00 , i.e.,
C3 ; l-contradicted. fact, assume exists universal literal l00 C3
. Then, level(l00 ) > either l00 C1 l00 C. Consider first case l00 C1 .
Then, l00 ResC2 (C1 ) C1 ; l-contradictable, level(l00 ) < level(l0 ).
possible level(l00 ) > level(l0 ) < m. Consider case
l00 C. Then, level(l00 ) < level(l1 ) hence level(l00 ) < level(l0 ) (see (14))
possible.
Since C3 C1 , hC3 , C2 , l, hC1 , C2 , l, i, conclude induction hypothesis
Rec-C-Resolve(, C3 , C2 , l, ) returns clause minimal form -contradicted.
413

fiGiunchiglia, Narizzano & Tacchella

make assumption input clause C1 ; l-contradicted.
Then, C1 contain existential literals whose negation assigned monotone, holds C2 clause C used line 5. Hence, Rec-CResolve(, C3 , C2 , l, ) returns clause without existential literals whose negation
assigned monotone .

References
Abdelwaheb, A., & Basin, D. (2000). Bounded model construction monadic second-order
logics. 12th International Conference Computer-Aided Verification (CAV00),
No. 1855 Lecture Notes Computer Science, pp. 99113, Chicago, USA. SpringerVerlag.
Bachmair, L., & Ganzinger, H. (2001). Resolution theorem proving. Robinson, A., &
Voronkov, A. (Eds.), Handbook Automated Reasoning, Vol. I, chap. 2, pp. 1999.
Elsevier Science.
Bayardo, Jr., R. J., & Schrag, R. C. (1997). Using CSP look-back techniques solve
real-world SAT instances. Proceedings 14th National Conference Artificial Intelligence 9th Innovative Applications Artificial Intelligence Conference
(AAAI-97/IAAI-97), pp. 203208, Menlo Park. AAAI Press.
Bayardo, Jr., Roberto J., & Miranker, D. P. (1996). complexity analysis space-bounded
learning algorithms constraint satisfaction problem. Proceedings
Thirteenth National Conference Artificial Intelligence Eighth Innovative
Applications Artificial Intelligence Conference, pp. 298304, Menlo Park. AAAI
Press / MIT Press.
Cadoli, M., Schaerf, M., Giovanardi, A., & Giovanardi, M. (2002). algorithm evaluate
quantified Boolean formulae experimental evaluation. Journal Automated
Reasoning, 28, 101142.
Cadoli, M., Giovanardi, A., & Schaerf, M. (1998). algorithm evaluate Quantified
Boolean Formulae. Proceedings 15th National Conference Artificial Intelligence (AAAI-98) 10th Conference Innovative Applications Artificial
Intelligence (IAAI-98), pp. 262267, Menlo Park. AAAI Press.
Castellini, C., Giunchiglia, E., & Tacchella, A. (2001). Improvements SAT-based conformant planning. Proc. ECP.
Castellini, C., Giunchiglia, E., & Tacchella, A. (2003). SAT-based planning complex
domains: Concurrency, constraints nondeterminism. Artificial Intelligence, 147 (12), 85117.
Davis, M., Logemann, G., & Loveland, D. W. (1962). machine program theorem
proving. Communication ACM, 5 (7), 394397.
de la Tour, T. B. (1990). Minimizing Number Clauses Renaming. Proc.
10th Conference Automated Deduction, pp. 558572. Springer-Verlag.
Dechter, R. (1990). Enhancement schemes constraint processing: Backjumping, learning,
cutset decomposition. Artificial Intelligence, 41 (3), 273312.
414

fiClause/Term Resolution Learning Quantified Boolean Formulas

Fermuller, C. G., Leitsch, A., Hustadt, U., & Tammet, T. (2001). Resolution decision procedures. Robinson, A., & Voronkov, A. (Eds.), Handbook Automated Reasoning,
Vol. II, chap. 25, pp. 17911849. Elsevier Science B.V.
Gent, I., Giunchiglia, E., Narizzano, M., Rowley, A., & Tacchella, A. (2004). Watched data
structures QBF solvers. Giunchiglia, E., & Tacchella, A. (Eds.), Theory
Applications Satisfiability Testing, 6th International Conference, SAT 2003. Santa
Margherita Ligure, Italy, May 5-8, 2003 Selected Revised Papers, Vol. 2919 Lecture
Notes Computer Science, pp. 2536. Springer.
Gent, I. P., & Rowley, A. G. (2004). Solution learning solution directed backjumping revisited. Tech. rep. APES-80-2004, APES Research Group. Available
http://www.dcs.st-and.ac.uk/apes/apesreports.html.
Ginsberg, M. L. (1993). Dynamic backtracking. Journal Artificial Intelligence Research,
1, 2546.
Giunchiglia, E., Narizzano, M., & Tacchella, A. (2001). Backjumping quantified Boolean
logic satisfiability. Proc. International Joint Conference Artificial Intelligence (IJCAI2001).
Giunchiglia, E., Narizzano, M., & Tacchella, A. (2002). Learning Quantified Boolean
Logic Satisfiability. Proceedings Eighteenth National Conference Artificial
Intelligence Fourteenth Conference Innovative Applications Artificial Intelligence, July 28 - August 1, 2002, Edmonton, Alberta, Canada. AAAI Press, 2002,
pp. 649654.
Giunchiglia, E., Narizzano, M., & Tacchella, A. (2003). Backjumping Quantified Boolean
Logic Satisfiability. Artificial Intelligence, 145, 99120.
Giunchiglia, E., Narizzano, M., & Tacchella, A. (2004a). Monotone literals learning
QBF reasoning. Tenth International Conference Principles Practice
Constraint Programming, CP 2004, pp. 260273.
Giunchiglia, E., Narizzano, M., & Tacchella, A. (2004b). Qbf reasoning real-world instances. Theory Applications Satisfiability Testing, 7th International Conference, SAT 2004, Vancouver, BC, Canada, May 10-13, 2004, Revised Selected Papers,
pp. 105121.
Giunchiglia, E., Narizzano, M., & Tacchella, A. (2004c). Qube++: efficient qbf solver.
5th International Conference Formal Methods Computer-Aided Design, FMCAD
2004, pp. 201213.
Kleine-Buning, H., Karpinski, M., & Flogel, A. (1995). Resolution quantified Boolean
formulas. Information Computation, 117 (1), 1218.
Le Berre, D., Simon, L., & Tacchella, A. (2003). Challenges QBF arena: SAT03
evaluation QBF solvers. Sixth International Conference Theory Applications Satisfiability Testing (SAT 2003), Vol. 2919 LNCS. Springer Verlag.
Letz, R. (2002). Lemma model caching decision procedures quantified Boolean
formulas. Proceedings Tableaux 2002, LNAI 2381, pp. 160175. Springer.
415

fiGiunchiglia, Narizzano & Tacchella

Marques-Silva, J. P., & Sakallah, K. A. (1996). GRASP - New Search Algorithm
Satisfiability. Proceedings IEEE/ACM International Conference ComputerAided Design, pp. 220227.
Moskewicz, M. W., Madigan, C. F., Zhao, Y., Zhang, L., & Malik, S. (2001). Chaff: Engineering Efficient SAT Solver. Proceedings 38th Design Automation
Conference (DAC01), pp. 530535.
Pan, G., & Vardi, M. Y. (2003). Optimizing BDD-based modal solver. Automated
Deduction - CADE-19, 19th International Conference Automated Deduction Miami
Beach, FL, USA, July 28 - August 2, 2003, Proceedings, pp. 7589.
Plaisted, D., & Greenbaum, S. (1986). Structure-preserving Clause Form Translation.
Journal Symbolic Computation, 2, 293304.
Prosser, P. (1993a). Domain filtering degrade intelligent backjumping search. Proceedings 13th International Joint Conference Artificial Intelligence (IJCAI99-Vol2), pp. 262267.
Prosser, P. (1993b). Hybrid algorithms constraint satisfaction problem. Computational Intelligence, 9 (3), 268299.
Rintanen, J. (1999). Constructing conditional plans theorem prover. Journal Artificial Intelligence Research, 10, 323352.
Robinson, A. (1965). machine-oriented logic based resolution principle. Journal
ACM, 12 (1), 2341.
Robinson, A. (1968). generalized resolution principle. Machine Intelligence, Vol. 3,
pp. 7793. Oliver Boyd, Edinburgh.
Scholl, C., & Becker, B. (2001). Checking equivalence partial implementations.
Proceedings 38th Design Automation Conference (DAC01), pp. 238243.
Tseitin, G. (1970). complexity proofs propositional logics. Seminars Mathematics, 8.
Urquhart, A. (1995). complexity propositional proofs. Bulletin Symbolic
Logic, 1 (4), 425467.
Zhang, L., Madigan, C. F., Moskewicz, M. W., & Malik, S. (2001). Efficient conflict driven
learning Boolean satisfiability solver. International Conference ComputerAided Design (ICCAD01), pp. 279285.
Zhang, L., & Malik, S. (2002a). Conflict driven learning quantified Boolean satisfiability solver. Proceedings International Conference Computer Aided Design
(ICCAD02).
Zhang, L., & Malik, S. (2002b). Towards symmetric treatment satisfaction conflicts
quantified Boolean formula evaluation. Proceedings Eighth International
Conference Principles Practice Constraint Programming, pp. 200215.

416

fiJournal Artificial Intelligence Research 26 (2006) 247287

Submitted 01/06; published 07/06

Landscape Random Job Shop Scheduling
Instances Depends Ratio Jobs Machines
Matthew J. Streeter
Stephen F. Smith

matts@cs.cmu.edu
sfs@cs.cmu.edu

Carnegie Mellon University
5000 Forbes Avenue, Pittsburgh, PA, 15213 USA

Abstract
characterize search landscape random instances job shop scheduling
problem (JSP). Specifically, investigate expected values (1) backbone size,
(2) distance near-optimal schedules, (3) makespan random schedules vary
N
N
N
function job machine ratio (
). limiting cases
0

N
provide analytical results, intermediate values perform experiments.
N
N
prove
0, backbone size approaches 100%,
backbone
N
N
vanishes. process show 0 (resp. ), simple priority rules
almost surely generate optimal schedule, providing theoretical evidence easyhard-easy pattern typical-case instance difficulty job shop scheduling. also draw
connections theoretical results big valley picture JSP landscapes.

1. Introduction
1.1 Motivations
goal work provide picture typical landscape random instance
job shop scheduling problem (JSP), determine picture changes
N
function job machine ratio (
). picture potentially useful (1)
N
understanding typical-case instance difficulty varies function
(2) designing
selecting search heuristics take advantage regularities typical instances
JSP.

1.1.1 Understanding instance difficulty function

N


job shop scheduling literature contains much empirical evidence square JSPs (those
N

= 1) difficult solve rectangular instances (Fisher & Thompson,
1963). work makes theoretical empirical contributions toward understanding
phenomenon. Empirically, show random schedules random local
N
1. Analytically, prove two
optima furthest optimality
N
N
limiting cases ( 0 ) exist simple priority rules almost surely
produce optimal schedule, providing theoretical evidence easy-hard-easy pattern
instance difficulty JSP.
c
2006
AI Access Foundation. rights reserved.

fiStreeter & Smith

1.1.2 Informing design search heuristics
Heuristics based local search, example tabu search (Glover & Laguna, 1997; Nowicki
& Smutnicki, 1996) iterated local search (Lourenco, Martin, Stutzle, 2003),
shown excellent performance benchmark instances job shop scheduling problem
(Jain & Meeran, 1998; Jones & Rabelo, 1998). order design effective heuristic, one
must (explicitly implicitly) make assumptions search landscape instances
heuristic applied. example, Nowicki Smutnicki motivate
use path relinking state-of-the-art i-TSAB algorithm citing evidence
JSP big valley distribution local optima (Nowicki & Smutnicki, 2005). One
conclusions work typical landscape random instances
N
N
thought big valley values
close 1; larger values
(including values
common benchmark instances), landscape breaks many big valleys, suggesting
modifications i-TSAB may allow better handle case (we discuss i-TSAB
9.3).
1.2 Contributions
contributions paper twofold. First, design novel set experiments
run experiments random instances JSP. Second, derive analytical results
confirm provide insight trends suggested experiments.
main contributions empirical work follows.
N
, show low-makespan schedules clustered small
low values
region search space many attributes (i.e., directed disjunctive graph edges)
N
common low-makespan schedules.
increases, low-makespan schedules
become dispersed throughout search space attributes common
low-makespan schedules.

introduce statistic (neighborhood exactness) used quantitatively
measure smoothness search landscape, estimate expected value
statistic random instances JSP. results, combination
results clustering, suggest landscape typical instances JSP
N
N
described big valley low values
; high values

many separate big valleys.
limiting cases
prove

N


0

N


, derive analytical results. Specifically,

N

0, expected size backbone (i.e., set problem variables
N
common value global optima) approaches 100%,
,
expected backbone size approaches 0%;
N
N

0 (resp.
), randomly generated schedule almost surely (a)
located close search space optimal schedule (b) near-optimal
makespan.

248

fiThe Landscape Random Job Shop Scheduling Instances

2. Related Work
least three threads research conducted search space analyses
related ones conduct here. include literature big valley distribution
common number combinatorial optimization problems, studies backbone size
Boolean satisfiability, statistical mechanical analysis TSP. briefly review
three areas below, well relevant work phase transitions easy-hardeasy pattern instance difficulty.
2.1 Big Valley
term big valley originated paper Boese et al. (1994) examined
distribution local optima Traveling Salesman Problem (TSP). Based sample
local optima obtained next-descent starting random TSP tours, Boese calculated
two correlations:
1. correlation cost locally optimal tour average distance
locally optimal tours,
2. correlation cost locally optimal tour distance
tour best tour sample.
distance two TSP tours defined total number edges minus
number edges common two tours. Based fact
correlations surprisingly high, Boese conjectured local optima TSP
arranged big valley. Adapted work Boese et al. (1994), Figure 1
gives intuitive picture big valley, set local minima appears convex
one central global minimum (Boese et al., 1994). offer formal definition
big valley landscape 6.
Boeses analysis applied combinatorial problems (Kim & Moon, 2004),
including permutation flow shop scheduling problem (Watson, Barbulescu, Whitley, &
Howe, 2002; Reeves & Yamada, 1998) JSP (Nowicki & Smutnicki, 2001). Correlations observed JSP generally weaker observed TSP.
related study, Mattfeld (1996) examined cost-distance correlations famous
JSP instance ft10 (Beasley, 1990) found evidence Massif Central. . . many
near optimal solutions reside laying closer together local optima. 4 contains
related results backbone size ft10.
2.2 Backbone Size
backbone problem instance set variables assigned common value
globally optimal solutions instance. example, Boolean satisfiability
problem (SAT), backbone set variables assigned fixed truth value
satisfying assignments. JSP, backbone defined number
disjunctive edges (3.2) common orientation globally optimal schedules
(a formal definition given 4).
large literature backbones combinatorial optimization problems, including many empirical analytical results (Slaney & Walsh, 2001; Monasson, Zecchina,
249

fiStreeter & Smith

Figure 1: intuitive picture big valley landscape.
Kirkpatrick, Selman, & Troyansky, 1999). analysis problem difficulty JSP,
Watson et al. (2001) present histograms backbone size random 6x6 (6 job, 6 machine)
6x4 (6 job, 4 machine) JSP instances. Summarizing experiments reported
paper, Watson et al. note [job:machine ratios] > 1.5, bias toward small backbones becomes pronounced, ratios < 1, bias toward larger backbones
magnified. 4 generalizes observations proves two theorems give
insight phenomenon occurs.
2.3 Statistical Mechanical Analyses
large growing literature applies techniques statistical mechanics analysis
combinatorial optimization problems (Martin, Monasson, & Zecchina, 2001). least
one result obtained literature concerns clustering low-cost solutions. study
TSP, Mezard Parisi (1986) obtain expression expected overlap (number
common edges) random TSP tours drawn Boltzmann distribution.
show temperature parameter Boltzmann distribution lowered (placing
probability mass low-cost TSP tours), expected overlap approaches 100%. Though
use Boltzmann weighting, 5 paper examines expected overlap
random JSP schedules changes probability mass placed low-makespan
schedules.
2.4 Phase Transitions Easy-hard-easy Pattern
Loosely speaking, phase transition occurs system expected value
statistic varies discontinuously (asymptotically) function parameter.
example, > 0 holds random instances 2-SAT problem satisfiable
probability asymptotically approaching 1 clause variable ratio (
n ) 1,
satisfiable probability approaching 0 clause variable ratio 1 + .
similar statement conjectured hold 3-SAT; critical value k
n (if exists)
must satisfy 3.42 k 4.51 (Achlioptas & Peres, 2004).
problems exhibit phase transitions (notably 3-SAT), average-case instance
difficulty (for typical solvers) appears first increase decrease one increases
relevant parameter, hardest instances appearing close threshold value
250

fiThe Landscape Random Job Shop Scheduling Instances

(A) JSP instance

J1 :

J11

J 12

J 2 : J12









J 13

J 22


(B) JSP schedule

J 14

J 32

J 42





J11





J12

J 22



time

(C) Disjunctive

graph




J11

J 12

J 13

J 32

J 14
J 42



J 14
o*



J12


J 12 J 13



J 22


J 32


J 42

Figure 2: (A) JSP instance,

(B) afeasibleschedule instance, (C) disjunctive graph representation schedule. Boxes represent operations; operation
durations proportional width box; machine
operation performed represented texture. (C), solid arrows represent
conjunctive arcs dashed arrows represent disjunctive arcs (arc weights
proportional duration operation arc points of).

(Cheeseman, Kanefsky, & Taylor, 1991; Yokoo, 1997). phenomenon referred
easy-hard-easy pattern instance difficulty (Mammen & Hogg, 1997). 7.4
discuss evidence easy-hard-easy pattern instance difficulty JSP, though
(to knowledge) associated phase transition.
results 4-5 empirical results 6 previously presented conference paper (Streeter & Smith, 2005a).

3. Job Shop Scheduling Problem
adopt notation [n] {1, 2, . . . , n}.
3.1 Problem Definition
Definition (JSP instance). N JSP instance = {J 1 ,J 2 , . . . , J N } set N
k ) sequence operations. operation
jobs , job J k = (J1k , J2k , . . . , JM
k
= Ji associated duration (o) (0, max ] machine m(o) [M ]. require
job uses machine exactly (i.e., J k [M ],
exactly one [M ] m(Jik ) = m). define
1. ops(I) {Jik : k [N ], [M ]},
2. (J k )

PM

k
i=1 (Ji ),


251

fiStreeter & Smith

3. job-predecessor J (Jik ) operation Jik
J (Jik )




k
Ji1


> 1
otherwise

fictitious operation (o ) = 0 m(o ) undefined.
Definition (JSP schedule). JSP schedule instance function : ops(I)
<+ associates operation ops(I) start time S(o) (operation performed
machine m(o) time S(o) time S(o) + (o); preemption allowed). make
following definitions.
1. completion time operation + (o) S(o) + (o).
2. machine-predecessor M(o) operation ops(I)

M(o)

arg maxoOprev (o) S(o)


Oprev (o) 6=
otherwise.

Oprev (o) = {o ops(I) : m(o) = m(o), S(o) < S(o)} set operations
scheduled run os machine.
3. feasible schedule S(o) max(S + (J (o)), + (M(o))) ops(I).
4. quantity
`(S) max + (o)
oops(I)

called makespan S.
consider makespan-minimization version JSP, goal find
schedule minimizes makespan.
remainder paper, whenever refer JSP schedule shall adopt
convention S(o ) = 0 shall assume
S(o) = max(S + (J (o)), + (M(o))) ops(I)

(3.1)

(i.e., so-called semi-active schedule, French, 1982). words, ignore schedules
superfluous idle time start schedule end one operation
start another.
Figure 2 (A) (B) depict, respectively, JSP instance feasible schedule
instance.
3.2 Disjunctive Graphs
schedule satisfying (3.1) uniquely represented weighted, directed graph called
disjunctive graph. disjunctive graph representation schedule JSP
instance I, operation ops(I) vertex directed edge (o1 , o2 ) indicates
operation o1 completes o2 starts.
252

fiThe Landscape Random Job Shop Scheduling Instances

Definition (disjunctive graph). disjunctive graph G = G(I, S) schedule
~ w) defined follows.
JSP instance weighted, directed graph G = (V, E,
V = ops(I) {o , }, (like ) fictitious operation (o ) = 0
m(o ) undefined.
~ =C
~ D,
~
E


~ = {(J (o), o) : ops(I)} (J k , ) : k [N ] called set conjunctive
C

arcs (which specify cannot start J (o) completes),
~ = {(o1 , o2 ) : {o1 , o2 } ops(I), m(o1 ) = m(o2 ), S(o1 ) < S(o2 )} called set

disjunctive arcs (which specify, pair operations performed
machine, two operations performed first).
w((o1 , o2 )) = (o1 ).
Figure 2 (C) depicts disjunctive graph schedule depicted Figure 2 (B).
connection schedule disjunctive graph established following
proposition (Roy & Sussmann, 1964).
Proposition 1. Let feasible schedule satisfying (3.1), let G = G(I, S)
corresponding disjunctive graph. `(S) equal length longest weighted
path G.
Proof. operation o, let L(o) denote length longest weighted path
G. suffices show ops(I), S(o) = L(o). follows
induction number edges path, base case S(o ) = L(o ) = 0.
undirected version disjunctive arc called disjunctive edge.
Definition (disjunctive edge). Let JSP instance. disjunctive edge set
{o1 , o2 } ops(I) m(o1 ) = m(o2 ). define following notation.
E(I) set disjunctive edges I.
Let schedule let e = {o1 , o2 } disjunctive edge. denote
~e(S) unique arc {(o1 , o2 ), (o2 , o1 )} appears disjunctive graph G(I, S)
(this arc called orientation e S).
measure distance two schedules S1 S2 JSP instance
counting number disjunctive edges oriented opposite directions G(I, S1 )
G(I, S2 ).
Definition (disjunctive graph distance). disjunctive graph distance kS1 S2 k
two schedules S1 S2 JSP instance defined
kS1 S2 k |{e E(I) : ~e(S1 ) 6= ~e(S2 )}| .
253

fiStreeter & Smith

3.3 Random Schedules Instances
define uniform distribution JSP instances follows. distribution identical
one used Taillard (1993).
Definition (random JSP instance). random N JSP instance generated
follows.
1. Let 1 , 2 , . . . , N random permutations [M ].
2. Let G probability distribution (0, max ] mean variance 2 > 0.
3. Define = {J 1 , J 2 , . . . , J N }, m(Jik ) = k (i) (Jik ) drawn (independently random) G.
Note definition (and likewise, theoretical results) assumes maximum
operation duration max , makes assumptions form distribution
operation durations. empirical results reported paper, choose operation
durations uniform distribution {1, 2, . . . , 100}.
proofs frequently make use priority rules. priority rule greedy schedulebuilding algorithm assigns priority operation and, step greedy
algorithm, assigns earliest possible start time operation minimum priority.
Definition (priority rule). priority rule function that, given instance
operation ops(I), returns priority (I, o) <. schedule = S(, I) associated
defined following procedure.
1. U nscheduled ops(I), S(o ) 0.
2. |U nscheduled| > 0 do:
(a) Ready {o U nscheduled : J (o)
/ U nscheduled}.
(b) element Ready least priority.
(c) S(o) max(S + (J (o)), + (M(o))).
(d) Remove U nscheduled.
priority rule called instance-independent if, N JSP instance
integers k [N ], [M ], value (I, Jik ) depends k, i, N , .
obtain random schedule assigning random priorities operation.
resulting distribution equivalent one used Mattfeld (1996).
Definition (random schedule). random schedule N JSP instance
generated performing following steps.
1. Create list L containing occurrences integer k k [N ] (we think
occurrences k representing operations job J k ).
2. Shuffle L (obtaining permutation equal probability).
3. Return schedule S(rand , I) rand (I, Jik ) = index ith occurrence
k L.
254

fiThe Landscape Random Job Shop Scheduling Instances

4. Number Common Attributes Function Makespan
backbone JSP instance set disjunctive edges common orientation schedules whose makespan globally optimal. 1, define
backbone set disjunctive edges common orientation schedules whose makespan within factor optimal (a related definition appears Slaney
& Walsh, 2001).
Definition ( backbone). Let JSP instance optimal makespan `min (I).
1, let opt(I) {S : `(S) `min (I)} set schedules whose makespan
within factor optimal.
backbone(I) {e E(I) : ~e(S1 ) = ~e(S2 ) {S1 , S2 } opt(I)} .
section compute expected value | backbone| function
random N JSP instances, examine shape curve changes
N
function
.
4.1 Computing backbone
compute backbone use following proposition.
Proposition 2. Let JSP instance optimal makespan `min (I). Let e = {o1 , o2 }
disjunctive edge orientations a1 = (o1 , o2 ) a2 = (o2 , o1 ). disjunctive
arc a, let `min (I|a) denote optimum makespan among schedules whose disjunctive graph
contains arc a.
e backbone(I) max {`min (I|a1 ), `min (I|a2 )} > lmin (I) .
Proof. e backbone, e must common orientation (say a1 ) schedules
`(S) `min (I), implies `min (I|a2 ) > `min (I). e
/ backbone,
must {S1 , S2 } opt(I) ~e(S1 ) = a1 ~e(S2 ) = a2 , implies
max{`min (I|a1 ), `min (I|a2 )} `min (I).

Thus compute backbone(I) need compute `min (I|a) 2M N2
possible choices a. Given disjunctive arc a, compute `min (I|a) using branch
bound. branch bound algorithms JSP, nodes search tree represent
choices orientations subset disjunctive edges. constructing root search
tree node fixed arc, determine `min (I|a). use branch bound
algorithm due Brucker et al. (1994) efficient code
freely available via ORSEP (Brucker, Jurisch,
& Sievers, 1992).


N
Computing `min (I|a) 2M 2 possible choices requires 1 + N2 runs
branch bound. first run used find globally optimal schedule,
gives

N
N
value `min (I|a) 2 possible choices (namely, 2 disjunctive arcs

present globally optimal schedule). separate run used
N2 remaining choices a.
Figure 3 graphs fraction disjunctive edges belong backbone
function instance ft10 (a 10 job, 10 machine instance) library (Beasley,
255

fiStreeter & Smith

Instance ft10
Normalized |-backbone|

1
0.8
0.6
0.4
0.2
0
1.00

1.02

1.04

1.06

1.09

1.11

1.13



Figure 3: Normalized | backbone| function library instance ft10.
1990). Note definition curve non-increasing respect ,
curve exact . noteworthy among schedules whose makespan within
factor 1.005 optimal, 80% disjunctive edges fixed orientation. see
N
behavior typical JSP instances
= 1.
4.2 Results
plotted | backbone| function instances library 10
fewer jobs 10 fewer machines. results available online (Streeter & Smith,
2005b). Inspection graphs revealed shape curve largely function
job:machine ratio. investigate further, repeat experiments large
number randomly generated JSP instances.
use randomly generated instances 7 different combinations N study
N
N
instances
equal 1, 2, 3.
= 1 use 6x6, 7x7, 8x8 instances;
N
N
=
2

use
8x4

10x5
instances;



= 3 use 9x3 12x4 instances.
generate 1000 random instances combination N .
Figure 4 parts (A), (B), (C) graph expected fraction edges belonging
N
-backbone function combination N , grouped according
.
N
Figure 4 (D) compares curves different values , plots 0.25 0.75
quantiles. purposes study two important observations Figure
4 follows.
curves depend size instance (i.e., N ) shape (i.e.,
N
N
). two factors, far stronger influence shape
curves.
values , expected fraction edges belonging backbone decreases
N

increases.
256

fiThe Landscape Random Job Shop Scheduling Instances

(A) Job:machine ratio 1:1

(B) Job:machine ratio 2:1
1

0.8

E[frac. edges -backbone]

E[frac. edges -backbone]

1

6x6 instances
7x7 instances

0.6

8x8 instances
0.4

0.2

0.8

8x4 instances
10x5 instances

0.6
0.4
0.2
0

0
1

1.1

1.2



1.3

1.4

1

1.5

1.1

1.3

1.4

1.5



(C) Job:machine ratio 3:1

(D) Comparison

1

1

0.8

Frac. edges -backbone

E[frac. edges -backbone]

1.2

9x3 instances

0.6

12x4 instances

0.4
0.2
0

0.8

8x8 instances
10x5 instances

0.6

12x4 instances
0.4

0.2

0

1

1.1

1.2

1.3

1.4

1.5



1

1.1

1.2



1.3

1.4

1.5

Figure 4: Expected fraction edges -backbone function random JSP
N
instances. Graphs (A), (B), (C) depict curves random instances
= 1, 2, 3, respectively. Graph (D) compares curves depicted (A), (B),
(C) (only curves largest instance sizes shown (D)). (D),
top bottom error bars represent 0.75 0.25 quantiles, respectively.

257

fiStreeter & Smith

4.3 Analysis
give insight Figure 4 analyzing two limiting cases. prove
N
0, expected fraction disjunctive edges belong backbone approaches
N
1,
expected fraction approaches 0.
N
Intuitively, happens follows.
0 (i.e., N held constant )
jobs becomes long. Individual disjunctive edges represent precedence
relations among operations performed far apart time. example,
10,000 machines (and job consists 10,000 operations), disjunctive
edge might specify whether operation 1,200 job performed operation
8,500 job B. Clearly, waiting job B complete 8,500 operations allowing
job complete 12% operations likely produce inefficient schedule. Thus,
orienting single disjunctive edge wrong direction likely prevent schedule
optimal, particular edge likely common orientation
globally optimal schedules.
N
contrast,
, workloads machines become long.
order jobs processed particular machine matter much
long machine longest workload kept busy, fact particular
edge oriented particular way unlikely prevent schedule optimal.
formalized below.
make use following well-known definition.
Definition (whp). sequence events n occurs high probability (whp) limn
P[n ] = 1.
Lemma 1 Theorem 1 show constant N , randomly chosen edge random
N JSP instance backbone whp (as ). Lemma 2 Theorem 2
show constant , randomly chosen edge random N JSP instance
backbone whp (as N ).
Lemma 1. Let random N JSP instance, let = S(, I) schedule
obtained using instance-independent priority rule . arbitrary job J I,
define SJ + (JM ) (J). E[SJ ] O(N ).
Proof. assume N = 2 > 1. generalization larger N straightforward,
cases N = 1 = 1 trivial. Let = {J 1 , J 2 } let J = J 1 .
Let = (o1 , o2 , . . . , ) sequence operations selected Ready (in line
2(b) definition priority rule 3.3) constructing S. say operation
Ji1 overlaps operation Jj2
1. Jj2 appears Ji1 ,
1 ), + (J 1 ) + (J 1 )] 6= .
2. [S(Jj2 ), + (Jj2 )] [S + (Ji1
i1


additionally m(Ji1 ) = m(Jj2 ), say Ji1 contends Jj2 . Intuitively, Ji1
overlaps o0 Jj2 start time might delayed os machine
used o0 . contends o0 , start time actually delayed.
258

fiThe Landscape Random Job Shop Scheduling Instances

Let i,j (resp. i,j ) indicator event Ji1 overlaps (resp. contends)
2
2
1
Let
Ci {Jj : i,j = 1} set operations J Ji overlaps with.
|Ci i0 >i Ci0 | 1. Thus

Jj2 .

X


|Ci | =

X


|Ci \

[

Ci0 | +

i0 >i

X


|Ci

[

Ci0 | 2M .

(4.1)

i0 >i

Let = IN,M 1 random N 1 JSP instance, define i,j , i,j , Ci
analogously above. i, j 1,




P i,j = 1|m(Ji1 ) = m(Jj2 ) = P i,j = 1 .
true P[i,j = 1] function joint distribution operations
set {Ji10 : i0 < i} {Jj20 : j 0 < j}; and, far joint distribution concerned,
conditioning event m(Ji1 ) = m(Jj2 ) like deleting operations use
machine m(Ji1 ).
h




1
1
1
P i,j = 1|m(Ji1 ) = m(Jj2 ) =
P i,j = 1 =
E i,j .
Thus E [i,j ] = P [i,j = 1] =
Therefore,
PM 1 PM 1
PM PM
j=1 E[i,j ]
i=1
j=1 E[i,j ] 2 +
i=1
1 PM 1 PM 1
= 2 + i=1
j=1 E[i,j ]
1 PM 1
= 2 + i=1 E[|Ci |]
4
last step used (4.1). follows E[SJ ] 4max (max
maximum operation duration defined 3). consider arbitrary N , get E[SJ ]
4max (N 1).
corollary Lemma 1, show simple priority rule (0 ) almost surely
N
0.
generates optimal schedule case
Definition (priority rule 0 ). Given N JSP instance I, let k = arg maxk[N ]
(J k ) index longest job. priority rule 0 first schedules operations

J k , schedules remaining operations fixed order.


k = k
k
0 (I, Ji ) =
k + otherwise.
Corollary 1. Let random N JSP instance. fixed N , holds
whp (as ) schedule = S(0 , i) optimal makespan `(S) =
maxk[N ] (J k ).
Proof. Define priority rule k k (I, Jik ) = k = k; k + otherwise. k
instance-independent, 0 equivalent k . Thus J
E[J0 ]

X

E[Jk ] = O(N 2 )

k

259

fiStreeter & Smith

S(,I)

define J J
, second step uses Lemma 1. Markovs inequal1
0
ity, J < 4 J whp. Central Limit Theorem,
(J) asymptotically
normally distributed mean standard deviation . follows whp,
1



(J k ) (J k ) > 4 k 6= k . implies `(S) = (J k ). (J k ) lower
bound makespan schedule, corollary follows.
Theorem 1. Let random N JSP instance, let e randomly selected
element E(I). fixed N , holds whp (as ) e 1 backbone(I).
Proof. Let e = {Ji , Jj0 } j let = (Jj0 , Ji ). Proposition 1 Corollary 1,
suffices show whp, disjunctive graphs containing contain path
weighted length > maxk[N ] (J k ).
3

Assume j 4 (this holds whp j selected uniformly
random [M ]), consider path
P = (o , J10 , J20 , . . . , Jj0 , Ji , Ji+1 , . . . , JM , )
3

passes |P | 3+M +M 4 vertices weighted length w(P ). want
show w(P ) > maxJI (J) whp. Central Limit Theorem, (1) fixed
j, w(P
p) asymptotically normally distributed mean (|P | 2) standard
deviation (|P | 2) (2)
J, (J) asymptotically normally distributed
mean standard deviation . w(P ) > maxJI (J) whp follows
Chebyshevs inequality.
N
, simple priority rule ( ) almost surely generates
Lemma 2 shows
schedule machine idle operations performed machine
completed (a schedule property clearly optimal).

Definition (priority rule ). Given N JSP instance I, priority rule
first schedules first operation job (taking jobs order ascending indices),
second operation job, forth. defined (I, Jik ) = + k.
Lemma 2. Let random N JSP instance. fixed , holds whp (as
N ) schedule = S( , I) property
S(o) = + (M(o)) ops(I) .
Proof. Suppose executing replace line S(o) max(S + (J (o)),
+ (M(o))) (line 2(c) definition priority rule given 3.3) S(o) + (M(o)).
resulting feasible replacement must effect. Thus suffices
show resulting feasible whp. Equivalently, want show whp,
S(o) + (J (o)) ops(I) constructed using modified version line 2
(c).
Let ops2+ (I) = {Jik ops(I) : > 1} set operations first
job. suffices show S(o) S(J (o)) max ops2+ (I). end, consider
arbitrary operation = Jik ops2+ (I). , number operations lower
260

fiThe Landscape Random Job Shop Scheduling Instances

priority (i 1)N + (k 1). number operations lower priority
1
Jik run machine m(o) is, expectation, equal
[(i 1)(N 1) + (k 1)]
(where switch N N 1 due fact operation job J k
uses machine m(o)). follows
E[S(o)] =


[(i 1)(N 1) + (k 1)]



k
)] =
E[S(o) S(J (o))] = E[S(Jik ) S(Ji1

N 1
.


Appendix use martingale tail inequality establish following claim.
Claim 2.1. high probability, ops2+ (I)
1
S(o) S(J (o)) E[S(o) S(J (o))] .
2
Lemma follows fact 21 E[S(o) S(J (o))] > max N sufficiently
large.
Based results computational experiments, Taillard (1994) conjectured
optimal makespan almost surely equal maximum machine workload.
following corollary Lemma 2 confirms conjecture.
N


Corollary 2. Let random N JSP instance optimal makespan `min (I).
Let (m) ({o ops(I) : m(o) = m}) denote workload machine m. fixed
, holds whp (as N ) `min (I) = maxm[M ] (m).
Theorem 2. Let random N JSP instance, let e randomly selected
element E(I). fixed , holds whp (as N ) e
/ 1 backbone(I).
Proof. Let e = {Ji , Jj0 }. Remove J J 0 create N 2 instance
comes distribution random N 2 JSP instance. Lemma
I,
2 shows whp exists optimal schedule property described
statement lemma.
: m(o) = m}) denote workload machine
Let (m) ({o ops(I)

instance I. Central Limit Theorem,
(m) asymptotically normally distributed
mean (N 2) standard deviation N 2. follows whp, | (m) (m0 )| >
1
N 4 6= m0 .
Thus whp one machine still processing operations interval
1
[`(S) N 4 , `(S)]. max( (J), (J 0 )) max = O(1), use interval
construct optimal schedules containing disjunctive arc (Ji , Jj0 ) well optimal
schedules containing disjunctive arc (Jj0 , Ji ).
261

fiStreeter & Smith

5. Clustering Function Makespan
section estimate expected distance random schedules whose makespan
within factor optimal, function various combinations N .
N
examine shape curve changes function
. formally,
random N JSP instance optimal makespan `min (I),
opt(I) {S : `(S) `min (I)},
S1 S2 drawn independently random opt(I),
wish compute E[kS1 S2 k].
Note experiments 4 provide upper bound quantity:

N
E [| backbone|]
E [kS1 S2 k]
2
provide lower bound (a low backbone size evidence mean distance
global optima large). experiments section viewed test
degree upper bound provided 4 tight.
5.1 Methodology
generate random samples opt(I) running simulated annealing algorithm
van Laarhoven et al. (1992) finds schedule. precisely, procedure
sampling distances follows.
1. Generate random N JSP instance I.
2. Using branch bound algorithm Brucker et al. (1994), determine optimal
makespan I.
3. Perform k runs, R1 , R2 , . . . , Rk , van Laarhoven et al. (1992) simulated annealing
algorithm. Restart run many times necessary find schedule whose
makespan optimal.
4. {1, 1.01, 1.02, . . . , 1.5}, find first schedule, call Si (), run
Ri whose makespan within factor optimal. k2 pairs
runs (Ri , Rj ), add distance Si () Sj () sample distances
associated .
ran procedure random JSP instances 7 combinations N
used 4.2. smallest instance sizes ratio (i.e., 6x6, 8x4
9x3 instances) generate 100 random JSP instances run procedure k = 100.
Setting k = 100 allows us measure variation instance-specific expected values.
4 combinations N , performing 10,000 simulated annealing runs
computationally expensive, instead generate 1000 random JSP instances run
procedure k = 2.
262

fiThe Landscape Random Job Shop Scheduling Instances

Figure 5 (A), (B), (C) plot expected distance random -optimal schedN
ules function three values
. Figure 5 (D) shows 0.75
0.25 quantiles 100 instance-specific sample means three smallest
instance sizes. Examining Figure 5 (D), see variation among random instances
N small relative differences curves different
N
values
.
5.2 Discussion
examining Figure 5 see , expected distance random N
optimal schedules increases
increases. Indeed, global optima dispersed widely
N
N
throughout search space
= 3, true lesser extent
= 2.
immediate implication Figure 5 whether exhibit two
correlations operational definition big valley, typical landscapes JSP
N
= 3 cannot expected big valleys sense central
instances
cluster optimal near-optimal solutions. anything, one might posit existence
multiple big valleys, leading separate global optimum. next section expands
upon observations.

6. Big Valley
section define formal properties big valley landscape, conduct experiments determine extent random JSP instances exhibit properties
N
N
N
vary
, present analytical results limiting cases
0
.
Considering intuitive picture given Figure 1, take following
necessary (though perhaps sufficient) conditions function f (x) big valley.
1. Small improving moves. x global minimum f , must exist nearby
x0 f (x0 ) < f (x).
2. Clustering global optima. maximum distance two global minima
f small.
Note direct relationship two properties cost-distance
correlations considered Boese et al. (1994).
6.1 Formalization
following four definitions allow us formalize notion big valley landscape.
Definition (Neighborhood Nr ). Let arbitrary JSP instance, let U set
schedules I. Let r positive integer. neighborhood Nr : U 2U defined

Nr (S) {S 0 U : kS 0 k r} .
Definition (local optimum L(S, N )). Let U above; let N : U 2U
arbitrary neighborhood function; let schedule I. L(S, N ) schedule
returned following procedure (which finds local optimum performing next-descent
starting using neighborhood N ).
263

fiStreeter & Smith

(A) Job:machine ratio 1:1

(B) Job:machine ratio 2:1
0.5

E[dist. schedules]

E[dist. schedules]

0.5
0.4
0.3
0.2

6x6 instances
7x7 instances

0.1

8x8 instances
0

0.4
0.3
0.2

8x4 instances

0.1

10x5 instances

0
1

1.1

1.2

1.3

1.4

1.5

1

1.1

1.2



(C) Job:machine ratio 3:1

1.4

1.5

(D) Comparison

0.5

0.5

E[dist. schedules]

E[dist. schedules]

1.3



0.4
0.3
0.2

9x3 instances

0.1

12x4 instances

0.4
0.3
0.2

6x6 instances
8x4 instances

0.1

9x3 instances
0

0
1

1.1

1.2

1.3

1.4

1.5

1

1.1

1.2

1.3

1.4

1.5





Figure 5: Expected distance random schedules within factor optimal,
function . Graphs (A), (B), (C) depict curves random instances
N
= 1, 2, 3, respectively. Graph (D) compares curves depicted (A),
(B), (C) (only curves smallest instance sizes shown (D)).
(D), top bottom error bars represent 0.75 0.25 quantiles (respectively)
instance-specific sample means.

264

fiThe Landscape Random Job Shop Scheduling Instances

(A) (r,)-valley

(B) Three (r,)-valleys

r

r

r

r









Figure 6: Two landscapes comprised (r, )-valleys. (A) single (r, ) valley (for
values r shown figure), (B) either viewed three
distinct (r, ) valleys single (r, 0 )-valley. (The values r shown
figure slightly larger necessary.)

1. Let N (S) = {S1 , S2 , . . . , S|N (S)| } (where elements N (S) indexed fixed
arbitrary manner).
2. Find least `(Si ) < `(S). exists, return S; otherwise set
Si go 1.
Definition ((r, )-valley). Let U above, let r non-negative
integers. set V U (r, )-valley V following two properties.
1. V , schedule L(S, Nr ) V globally optimal.
2. two globally optimal schedules S1 S2 V , kS1 S2 k .
Figure 6 illustrates definition (r, )-valley. would say landscape
depicted Figure 6 (A) big valley, depicted 6 (B) comprised three
big valleys.
Definition ((r, , p) landscape). Let U above, let random schedule
I. (r, , p) landscape exists V U
1. V (r, )-valley,
2. P[S V ] p.



JSP instance trivially (M N2 , N2 , 1) landscape (because r = N2

Nr includes possible schedules). JSP instance (r, N2 , 1) landscape,
globally optimal schedule always found starting random schedule
applying next-descent using neighborhood Nr .
say JSP instance big valley landscape (r, , p) landscape
small r combination p near 1. contrast, small r combination
p near 1 require large , say landscape consists multiple big valleys.
265

fiStreeter & Smith

6.2 Neighborhood Exactness
section seek determine extent random JSP instances
small improving moves property. require following definition.
Definition (neighborhood exactness). Let I, U , N above, let
random schedule I. exactness neighborhood N instance
probability L(S, N ) global optimum.

exactness Nr p, (r, N2 , p) landscape (let V consist
schedules L(S, N ) global optimum). estimate expected exactness
Nr function r various combinations N . examining resulting
curves, able draw conclusions extent landscapes
random N JSP instance typically small improving moves property.
N
determine presence absence property depends
.

fixed N , compute expected exactness Nr 1 r N2
repeatedly executing following procedure.
1. Generate random N JSP instance I.
2. Using algorithm Brucker et al. (1994), compute optimal makespan I.
3. Repeat k times:
(a) random feasible schedule, r 1, opt f alse.
(b) opt = f alse do:
L(S, Nr ).
global optimum, opt true.
Record pair (r, opt).
r r + 1.
(c) r0 r r0

N
2



record pair (r0 , true).

pairs recorded procedure (in step 3(c) third bullet point 3 (b))
used obvious way estimate expected exactness. Specifically, r
estimated expected exactness Nr fraction pairs (r, x) x = true.
implementation first bullet point step 3 (b) deserves discussion.
determine L(S, Nr ), step next-descent must able determine best schedule
{S 0 : kS 0 k r}. large r impractical brute force. Instead
developed radius-limited branch bound algorithm that, given arbitrary center
schedule Sc radius r, finds schedule arg min{S 0 :kSc 0 kr} `(S 0 ). radius-limited
branch bound algorithm uses branching rule Balas (1969) combined
lower bounds branch ordering heuristic Brucker et al. (1994).
266

fiThe Landscape Random Job Shop Scheduling Instances

6.3 Results
N
use three combinations N
= 15 (3x15, 4x20, 5x25 instances), three
N
N
combinations = 1 (6x6, 7x7, 8x8 instances) two combinations
=5
(15x3 20x4 instances). smallest instance sizes ratio (i.e., 3x15, 6x6,
15x3 instances) generate 100 random JSP instances run procedure
k = 100. Otherwise, generate 1000 random JSP instances run procedure
k = 1.

Figure 7 (A), (B), (C) plot expected exactness function neighborhood radius
N
(normalized number disjunctive edges) three values
. Figure
7 (D) shows 0.75 0.25 quantiles 100 instance-specific sample means
three smallest instance sizes.
6.4 Discussion
Examining Figure 7, see normalized neighborhood radius, neighborhood
N
exactness lowest instances
= 1 higher two extreme ratios
1
N
N
( = 5 = 5). view neighborhood exactness measuring smoothness
landscape, data suggest typical JSP landscapes least smooth
N
N
N
intermediate value
, become smooth
0
.
suggests easy-hard-easy pattern typical-case instance difficulty JSP,
phenomenon explored fully next section.
Using methodology 4-5, found expected proportions backbone
edges 3x15, 4x20, 5x25 instances 0.94, 0.93, 0.92, respectively,
expected distance global optima 0.02 three cases. contrast,
expected proportions backbone edges 15x3 20x4 instances near-zero,
expected distances global optima 0.33 0.28, respectively. conclude
landcapes random N JSP instances typically clustering global
N
N
optima property
= 51
= 5. However, Figure 7 suggests small
N
N
improving moves property present
= 51
= 5. Accordingly, would
N
1
N
say typical landscapes = 5 big valleys,
= 5 landscape
comprised many big valleys rather one.
N
data 4-5 show
= 1, typical landscapes clustering
global optima property. Examining Figure 7 (B), see able descend
random schedule globally optimal schedule probability 12 (normalized)
neighborhood radius 6%. reason, think landscapes random
N
JSP instances
= 1 small improving moves property extent.
This, combination curve Figure 5 (A) (which shows expected distance
random -optimal schedules function ) leads us say typical landscapes
N
random JSP instances
= 1 still roughly described big valleys. However,
valley much rougher (meaning larger steps required move random
schedule global optimum via sequence improving moves) extreme
N
values
.

Table 1 summarizes empirical findings discussed.
267

fiStreeter & Smith

(A) Job:machine ratio 1:5

(B) Job:machine ratio 1:1

1

1
0.8

3x15 instances
0.6

E[exactness]

E[exactness]

0.8

4x20 instances

0.4

5x25 instances

6x6 instances
0.6

7x7 instances
8x8 instances

0.4
0.2

0.2

0

0
0

0.1

0.2

0

0.3

0.2

0.3

Normalized radius

Normalized radius

(C) Job:machine ratio 5:1

(D) Comparison

1

1
0.8

E[exactness]

0.8

E[exactness]

0.1

15x3 instances
0.6

20x4 instances
0.4

3x15 instances

0.6

6x6 instances
0.4

15x3 instances

0.2

0.2

0

0
0

0.1

0.2

0.3

0

0.1

0.2

0.3

Normalized radius

Normalized radius

Figure 7: Expected exactness Nr function (normalized) neighborhood radius
N
r. Graphs (A), (B), (C) depict curves random instances
= 15 ,
1, 5, respectively. Graph (D) compares curves depicted (A), (B),
(C) (only curves largest instances shown (D)). (D), top
bottom error bars represent 0.75 0.25 quantiles (respectively) instancespecific exactness.

268

fiThe Landscape Random Job Shop Scheduling Instances

N

1
5

N
Table 1. Landscape attributes three values
.
Clustering
Small
improving
Description
global optima? moves?
Yes
Yes
Big valley

1

Yes

Somewhat

(Rough) big valley

5



Yes

Multiple big valleys

6.5 Analysis
first establish behavior curves depicted Figure 7 limiting cases
N
N
0 . use results characterize landscapes random
JSP instances using (r, , p) notation introduced 6.1.
N
N
following two lemmas show
0 (resp.
), random schedule
almost surely close optimal schedule. proofs given Appendix A.
Lemma 3. Let random N JSP instance, let random schedule
I. Let optimal schedule kS Sk minimal. Let f (M )
unbounded, increasing function . fixed N , holds whp (as )
kS Sk < f (M ).
Lemma 4. Let random N JSP instance, let random schedule I,
let optimal schedule kS Sk minimal. fixed
> 0, holds whp (as N ) kS Sk < N 1+ .
following immediate corollaries Lemmas 3 4.
Corollary 3. fixed N , expected exactness Nf (M ) approaches 1 ,
f (M ) unbounded, increasing function .
Corollary 4. fixed > 0, expected exactness NN 1+ approaches 1
N .

total number disjunctive edges N2 , two corollaries imply
N
N
0 (resp.
), curve depicted Figure 7 approaches horizontal line

height 1.
Using Lemmas 3 4, Theorems 3 4 characterize landscape random JSP
instances using (r, , p) notation 6.1. presenting theorems, slight
disclaimer order. Lemmas 3 4 (the proofs fairly involved) indicate
N
N
extreme cases
0
jump random schedule
globally optimal schedule via single small move. strongly believe cases
also possible go random schedule global optimum sequence many
(smaller) improving moves, although proving seems difficult. Nevertheless,
understood theoretical results strictly imply existence landscapes
like depicted Figure 6 (where starting points sequence two
small improving moves leading global optimum).
N
Theorem 3 shows
0, random JSP instance almost certainly

(r, , p) landscape r grows arbitrarily slowly function , o(M N2 ),
269

fiStreeter & Smith

N
p arbitrarily close 1. words,
0 landscape small
improving move(s) property clustering global optima property. contrast,
N
Theorem 4 shows
, random JSP instance almost surely
(r, , p) landscape unless (N 2 ). Instead, landscape contains (N !) (r, 1)-valleys,
N
r o(M N2 ). Thus,
, landscape small improving move(s)
property clustering global optima property. analytical results confirm
trend suggested Figure 7 discussed 6.4.

Theorem 3. Let random N JSP instance. Let f (M ) unbounded,
increasing function . fixed N > 0, holds whp (as )
(r, , p) landscape r = f (M ), = N2 p = 1 .
Proof. Let V set schedules L(S, Nr ) global optimum. follows
Corollary 3 whp, exactness r least p, means V
probability least p. remains show V (r, )-valley whp. Part 1
definition (r, )-valley satisfied definition V . Part 2 follows Theorem
1.
Theorem 4. Let random N JSP instance, let random schedule
I. exists set V (I) = ni=1 Vi schedules fixed > 0, V
following properties whp:
1. V ;
2. Vi (r, )-valley r = N 1+ = 1 [n];
3. n > N !(1 );
4. max{S1 ,S2 }V kS1 S2 k > (N 2 ).
Proof. Let {S1 , S2 , . . . , Sn } set globally optimal schedules I, define Vi
{S : L(S, N 1+ ) = Si }. Property 1 holds whp Lemma 4. Property 2 holds definition
Vi .
fact property 3 holds whp consequence Lemma 2. Recall Lemma
N
2 showed
, priority rule generates optimal schedule whp,
k
(I, Ji ) = + k. indices assigned jobs arbitrary, Lemma 2 also
applies priority rule (I, Jik ) = + (k), permutation [N ].
N ! possible choices . Let f number choices fail yield globally
optimal schedule. Property 3 fail hold f N !. Lemma 1, E[f ]
o(1)N !; hence f < N ! whp Markovs inequality.
establish property 4, choose permutations 1 2 list elements [N ]
reverse order (i.e., 1 (i) = 2 (N i) [N ]). Lemma 2, schedules S1 =
S( 1 , I) S2 = S( 2 , I) globally optimal whp. disjunctive edge
e = {J1 , J10 } must ~e(S1 ) 6= ~e(S2 ), hence kS1 S2 k |{{J, J 0 } : m(J1 ) =
1
1
m(J10 )}| N M2
= (N 2 ), obtain expression N M2
using pigeonhole
principle.
270

fiThe Landscape Random Job Shop Scheduling Instances

7. Quality Random Schedules
7.1 Methodology
section examine quality randomly generated schedules changes
function job:machine ratio. Specifically, various combinations N ,
estimate expected value following four quantities:
(A) makespan random schedule,
(B) makespan locally optimal schedule obtained starting random schedule
applying next-descent using N1 move operator,
(C) makespan optimal schedule,
(D) lower bound makespan optimal schedule given maximum
maximum job duration maximum machine workload:


X
max max (J), max
(o) .
JI

m[M ]

oops(I):m(o)=m

N
considered experiments set R = { 17 , 16 , 51 , 14 ,
values
1 1 2
3
3 , 2 ,S3 , 1, 2 , 2, 3, 4, 5, 6, 7 }. consider combinations N set
N
rR Sr , Sr {(N, ) :
= r, min(N, ) 2, max(N, ) 6, N < 1000}.
(N, ) S, estimate expected value (A) (resp. (B)) generating 100
random N JSP instances and, instance, generating 100 random schedules
(resp. local optima). estimate (D) generating 1000 random JSP instances
(N, ) S. combinations (N, ) Ssmall S, also practical compute
N
quantity (C). Let nr = |Ssmall Sr | number combinations (N, )
= r
3
computed (C). chose Ssmall nr 4 r 6= 2 n 3 = 3.
2
(N, ) Ssmall , estimate (C) using 1000 random JSP instances.

7.2 Results
Figure 8 plots mean values (A), (B), (C), respectively, mean value
(D), various combinations N . data points combination N
N
assigned symbol based value
. Top bottom error bars represent 0.75
0.25 quantiles (respectively) instance-specific sample means. Note width
error bars small relative differences curves different values
N

.
N
Examining Figure 8, see set data points value
approximately (though exactly) collinear. Furthermore, three graphs slope line
N
formed data points
= r maximized r = 1, decreases r gets
away 1 (see also Figure 9 (A)).
investigate trend, performed least squares linear regression set
N
data points value
. slopes resulting lines shown function
N
Figure 9 (A).
examination Figure 9 (A), apparent
271

fiStreeter & Smith

(A) Random schedules
Mean makespan

7000
6000
5000
4000

Ratio 1:5

3000

Ratio 1:3
Ratio 1:1

2000

Ratio 3:1

1000

Ratio 5:1

0
0

1000 2000 3000 4000 5000 6000 7000

Mean lower bound

(B) Random local optima
Mean makespan

7000
6000
5000
4000

Ratio 1:5

3000

Ratio 1:3
Ratio 1:1

2000

Ratio 3:1

1000

Ratio 5:1

0
0

1000 2000 3000 4000 5000 6000 7000

Mean lower bound

(C) Optimal schedules
Mean makespan

7000
6000
5000
4000

Ratio 1:5

3000

Ratio 1:3
Ratio 1:1

2000

Ratio 3:1

1000

Ratio 5:1

0
0

1000 2000 3000 4000 5000 6000 7000

Mean lower bound

Figure 8: Expected makespan (A) random schedules, (B) random local optima, (C)
optimal schedules vs. expected lower bound, various combinations N
N
(grouped symbol according
). Top bottom error bars represent
0.75 0.25 quantiles (respectively) instance-specific sample means.
272

fiThe Landscape Random Job Shop Scheduling Instances

(A) Results least squares regression

Slope E[makespan]
vs. E[lower bound]

4

Random schedules
Random local optima
Optimal schedules

3

2

1
0.1

1

10

Job:machine ratio

(B) Branch bound search cost
2:1

Num. tree nodes

10000

3:2

1:1
2:3

1000
1:2

100

1:3

3:1
4:1
5:1

10

1:4

1:5
1:6

6:1

1:7
7:1

1
0

500

1000

1500

log(search space size)

1:7
1:6
1:5
1:4
1:3
1:2
2:3
1:1
3:2
2:1
3:1
4:1
5:1
6:1
7:1

2000

Figure 9: (A) graphs slope least squares fits data Figure 8 (A), (B),
N
N
(C) function
(includes values
depicted Figure 8). (B)
th
graphs number search tree nodes (90 percentile) used branch
bound algorithm Brucker et al. (1994) find optimal schedule.

273

fiStreeter & Smith

N
value
becomes extreme (i.e., approaches either 0 ), expected makespan random schedules (resp. random local optima) comes closer
expected value lower bound makespan;

difference expected makespan random schedules (resp. random
local optima) expected value lower bound makespan maximized
N
value
1.
N
first two observations suggests
approaches either 0 ,
random schedule almost certainly near-optimal. 7.3 contains two theorems confirm
this.
second two observations suggests expected difference
makespan random schedule makespan optimal schedule maximized
N
value
somewhere neighborhood 1. observation particularly interesting
N
light empirical fact square instances JSP (i.e.,
= 1)
harder solve rectangular ones (Fisher & Thompson, 1963).
Figure 9 (B) graphs number search tree nodes (90th percentile) required
branch bound algorithm Brucker et al. (1994) optimally solve random N
instances, function log (base 10) search space size. take size
search space N JSP instance number possible disjunctive graphs,

namely 2N ( 2 ) . Note disjunctive graphs contain cycles therefore
correspond feasible schedules, expression overestimates size search
space. Data points given combination N could afford
run branch bound (i.e., combination N computed quantity
N
(C)). data points grouped curves according
.
Examining Figure 9 (B), see curves steepest ratios 23 , 1, 32 , 2,
N
3, curves substantially less steep extreme values
17
7. Thus, least point view particular branch bound algorithm,
random JSP instances exhibit easy-hard-easy pattern instance difficulty. discuss
pattern 7.4.

7.3 Analysis
following two theorems show that,
almost surely near-optimal.

N


approaches either 0 , random schedule

Theorem 5. Let random N JSP instance optimal makespan `min (I)
let random schedule I. fixed N > 0, holds whp (as )
`(S) (1 + )`min (I).
Proof. priority rule rand associates priority operation ops(I). Let
sequence contain elements ops(I), sorted ascending order priority.
schedule = S(rand , I) depends , N ! possible choices . Thus
rand seen choosing random set N ! instance-independent priority
rules. instance-independent priority rules subject Lemma 1, rand

Lemma 1 thus J, E[SJ ] O(N ). Thus E[`(S) `min (I)]
Palso subject

2
J E[J ] = O(N ), `(S) `min (I) exceed `min (I) = (M ) whp Markovs
inequality.
274

fiThe Landscape Random Job Shop Scheduling Instances

Theorem 6. Let random N JSP instance optimal makespan `min (I)
let random schedule I. fixed > 0, holds whp (as N )
`(S) (1 + )`min (I).
Proof. See Appendix A.
idea behind proof Theorem 6 following. shown Lemma 2,
priority rule almost surely generates optimal schedule. relevant property
that, operations sorted order ascending priority, number
operations J (o) (N ). key proof Theorem 6
expectation, rand shares property operations ops(I).
7.4 Easy-hard-easy Pattern Instance Difficulty
N
N
proofs Corollary 1 (resp. Lemma 2) show
0 (resp.
) exist
simple priority rules almost surely produce optimal schedule. Moreover, Theorems
5 6 show two limiting cases, even random schedule almost surely
N
N
makespan close optimal. Thus,
0
, almost
JSP instances easy.
N
contrast,
1, Figure 9 (A) suggests random schedules (as well random
local optima) far optimal. literature JSP (as well results depicted
N
Figure 9 (B)) attests fact random JSP instances
1 hard.
Thus conjecture that, 3-SAT, typical instance difficulty JSP follows easyhard-easy pattern function certain parameter. contrast 3-SAT, easyhard-easy pattern JSP (to knowledge) associated phase transition
N
(i.e., identified quantity undergoes sharp threshold
1).
Furthermore, although empirical results Figures 9 (A) (B) support idea
typical-case instance difficulty JSP follows easy-hard-easy pattern,
N
claim isolated particular value
point maximum
difficulty. shown Figure 9 (B), random JSP N JSP instances difficult
N
branch bound algorithm Brucker et. al (1994)
2, may
true branch bound algorithms JSP heuristics based local search.
leave task characterizing easy-hard-easy pattern precisely future
work.
related work, Beck (1997) studied constraint-satisfaction (as opposed makespanminimization) version JSP, gave empirical evidence probability
random JSP instance satisfiable undergoes sharp threshold function quantity
called constrainedness instance.

8. Limitations Extensions
primary limitation work reported paper theoretical
empirical results apply random instances job shop scheduling problem.
guarantee observations generalize instances drawn distributions
interesting structure (Watson et al., 2002). difficulty extending
analysis distributions analytical results similar ones presented
275

fiStreeter & Smith

paper may become much difficult derive. However, least three
distributions studied scheduling literature believe
difficult adapt proofs (the conclusions may change part
adaptation process).
Random workflow JSP instances. workflow JSP instance, set machines
partitioned sets (say M1 , M2 , . . . , Mk ). < j, job must use
machines Mi using machines Mj . Mattfeld et al. (1999) define
random distribution workflow JSPs generalizes natural way
distribution defined 3.3 (the difference permutations 1 , 2 , . . . , N
chosen uniformly random set permutations satisfy workflow
constraints).
Random instances (permutation) flow shop scheduling problem. instance
flow shop scheduling problem (FSP) JSP instance jobs use
machines order (equivalently, FSP instance workflow JSP instance
k = ). permutation flow shop problem (PFSP) special case FSP
which, additionally, machine must process jobs order.
large literature (P)FSP; Framinan et al. (2004) Hejazi Saghafian
(2005) provide relevant surveys.
Job-correlated machine-correlated JSP instances. job-correlated JSP instance,
distribution operation durations drawn depends job
operation belongs. Similarly, machine-correlated JSP instance distribution
depends machine operation performed. Watson et al. (2002)
studied job-correlated machine-correlated instances PFSP.
Regarding difficulty instances drawn three distributions, computational
experience shows (i) random workflow JSPs harder random JSPs; (ii) random PFSPs easier random JSPs; (iii) job-correlated machine-correlated
PFSPs easier random PFSPs. Extending theoretical analysis
distributions may give insight relevant differences them.
8.1 Big Valley vs. Cost-Distance Correlations
6, defined big valley landscape one exhibits two properties: small improving moves clustering global optima. analytical experimental results
based definition. Although believe definition captures properties JSP
landscapes important designers heuristics understand, properties
(e.g., cost-distance correlations) likely important well. particular, may
possible algorithms exploit cost-distance correlations landscapes neither
small improving moves clustering global optima properties.
existing literature, term big valley used amorphously mean either
(1) landscape like depicted Figure 1 (2) landscape exhibits high costdistance correlations. making sharper distinction two distinct concepts,
improve understanding JSP landscapes well landscapes
combinatorial problems.
276

fiThe Landscape Random Job Shop Scheduling Instances

9. Conclusions
9.1 Summary Experimental Results
N
Empirically, demonstrated low values job machine ratio (
), lowmakespan schedules clustered small region search space backbone
N
size high.
increases, low-makespan schedules become dispersed throughout
N
search space backbone vanishes. function
, smoothness
landscape (as measured statistic called neighborhood exactness) starts small
N
N
N
low values
(e.g.,
= 15 ), relatively high
1, becomes small
N
N
N
high values (e.g., = 5). extremely low extremely high values
,
expected makespan random schedules comes close optimal schedules.
quality random schedules (resp. random local optima) appears worst
N
value
1.
6.4 discussed implications results big valley picture JSP search
N
1, concluded typical landscape described big
landscapes.
N
N
valley, larger values
(e.g.,
3) many big valleys. 7.4 discussed
data support idea JSP instance difficulty exhibits easy-hard-easy
N
.
pattern function

9.2 Summary Theoretical Results
Table 2 shows asymptotic expected values various attributes random N
N
N
JSP instance limiting cases
0
.
Table 2. Attributes random JSP instances.
Fixed N , Fixed , N
Optimum makespan

Max. job length
(Corollary 1)

Max. machine workload
(Corollary 2)

Normalized backbone size

1 (Theorem 1)

0 (Theorem 2)

Normalized maximum distance global optima
Normalized distance random
schedule nearest global optimum
Ratio makespan random schedule
optimum makespan

0 (Theorem 1)

(1) (Theorem 4)

0 (Lemma 3)

0 (Lemma 4)

1 (Theorem 5)

1 (Theorem 6)

9.3 Rules Thumb Designing JSP Heuristics
Though claim deep insights solve random instances
JSP, results suggest two general rules thumb:
N
N

low (say,
1 lower), algorithm attempt locate
cluster global optima exploit it;

277

fiStreeter & Smith

N
N

high (say,
3) algorithm attempt isolate one
clusters global optima deal separately them.

briefly discuss ideas relation two recent algorithms: backbone-guided local
search (Zhang, 2004) i-TSAB (Nowicki & Smutnicki, 2005).
9.3.1 Backbone-guided local search
Several recent algorithms attempt use backbone information bias move operator employed local search. example, Zhang (2004) describes approach called
backbone-guided local search frequency attribute (e.g., assignment particular value particular variable Boolean formula) appears
random local optima used proxy frequency attribute appears global optima. approach improved performance WalkSAT algorithm
(Selman, Kautz, & Cohen, 1994) large instances SATLIB (Hoos & Stutzle, 2000).
similar algorithm successfully applied TSP (Zhang & Looks, 2005)
improve performance iterated Lin-Kernighan algorithm (Martin, Otto, & Felten,
1991). Zhang writes:
method built upon following working hypothesis: problem
whose optimal near optimal solutions form cluster, local search algorithm reach close vicinities solutions, algorithm effective
finding information solution structures, backbone particular.
(Zhang, 2004, p. 3)
Based results 4-5, working hypothesis satisfied random JSPs
1 lower. seems plausible backbone-guided local search could used boost
performance early local search heuristics JSP van Laarhoven
et al. (1992) Taillard (1994) (whether results would competitive
recent algorithms i-TSAB separate question).
N
hypothesis typically violated random JSP instances larger values
.
cases makes sense attempt exploit local clustering optimal
near-optimal schedules.
N


9.3.2 i-TSAB
Nowicki Smutnicki (2005) present JSP heuristic called i-TSAB employs multiple
runs tabu search algorithm TSAB (Nowicki & Smutnicki, 1996). i-TSAB employs path
relinking localize center BV [big valley], probably close global minimum
(Nowicki & Smutnicki, 2005). words, i-TSAB designed based intuitive
picture depicted Figure 6 (A), inaccurate typical random JSP instances
N
N
3. Note although random JSP instances become easy , instances
N

3 means easy, evidenced Figure 9 (B).
concreteness, briefly describe i-TSAB works. Initially, i-TSAB performs
number independent runs TSAB adds best-of-run schedule pool elite
solutions. performs additional runs TSAB uses best-of-run schedules
additional runs replace schedules pool elite solutions. Starting points
278

fiThe Landscape Random Job Shop Scheduling Instances

additional TSAB runs either (i) random elite solutions (ii) schedules obtained
performing path relinking random pair elite solutions. Given two schedules S1
S2 , path relinking uses move operator generate new schedule midway
(in terms disjunctive graph distance) S1 S2 . pool elite solutions
thought cloud particles hovers search space (hopefully)
converges region space containing global optimum.
N
random JSP instances
1, results consistent idea
cloud elite solutions converges center big valley. random JSP
N
instances
3, however, cloud must either converge one many big valleys
converge all. alternate approach one imagine using multiple clouds,
intention cloud specializes particular big valley. seems plausible
ideas could improve performance i-TSAB random JSP instances
N
larger values
.

Appendix A: Additional Proofs
P
proofs section, define (O) oO (o), set operations.
make use following inequality (Spencer, 2005).
Azumas Perimetric Inequality (A.P.I.). Let X = (X1 , X2 , . . . , Xn ) vector n independent random variables. Let function f (x) take input vector x = (x1 , x2 , . . . , xn ),
xi realization Xi [n], produce output real number. Suppose
> 0 holds two vectors x x0 differ one
component,
|f (x) f (x0 )| .
> 0,




2
P X > E[X] + n exp 2 .
2

inequality holds P [X E[X] n].
Lemma 2. Let random N JSP instance. fixed , holds whp (as
N ) schedule = S( , I) property
S(o) = + (M(o)) ops(I) .
Proof. remains prove Claim 2.1 proof 4, says whp,
ops2+ (I)
1
S(o) S(J (o)) E[S(o) S(J (o))] .
2
Pick arbitrary operation ops2+ (I), suppose random choices used
construct made following order:
1. Randomly choose m1 = m(o) m2 = J (o).
2. k 1 N :
279

fiStreeter & Smith

(a) Randomly choose order job J k uses machines (if J k
part choice already made step 1).
(b) Randomly choose (Jik ) [M ].
Let random variable Xk denote sequence random bits used steps (a)
(b) k th iteration loop. Define S(o) S(J (o)). Then, fixed choices
m1 m2 , function N independent events X1 , X2 , . . . , XN , easy
check altering particular Xi changes value 2max . Thus
h



1)
P < 12 E[o ] = P < E[o ] (N
2M
h

N
P < E[o ] 2M


2
exp 2(4M N
2
max )
first step used fact (from proof 4) E[o ] = (NM1)
last step used A.P.I. Taking union bound N (M 1) operations
ops2+ (I) proves claim.

Lemma 3. Let random N JSP instance, let random schedule
I. Let optimal schedule kS Sk minimal. Let f (M )
unbounded, increasing function . fixed N , holds whp (as )
kS Sk < f (M ).
Proof. Let = S(0 , I). proof Corollary 1 showed J, E[SJ ] O(N 2 ).
Thus holds whp SJ < log(f (M )) J. proof Theorem 5, procedure
used produce mixture instance-independent priority rules, subject
Lemma 1. Thus J, E[SJ ] O(N ), whp SJ < log(f (M )) J.
P
P
Let Onear (Ji ) = {Jj0 : J 0 6= J, | i0 <i (Ji0 ) j 0 <j (Jj0 0 )| < log(f (M ))}. (Onear (Ji )
set operations would scheduled near time Ji ignored fact
machine may perform one operation time.) Let Enear = {e = {Ji , Jj0 }
E(I) : Jj0 Onear (Ji )}. assumptions previous paragraph (each
hold whp), kS Sk |Enear |. Ji , E[|Onear (Ji )|] O(N log f (M )). Thus


E kS Sk E [|Enear |] =

X
oops(I)


1
E [|Onear (o)|] = N 2 log(f (M ))


kS Sk < f (M ) whp Markovs inequality.
purpose remaining proofs, convenient introduce additional
notation. Let = (T1 , T2 , . . . , T|T | ) sequence operations. define
T(i1 ,i2 ] {Ti : i1 < i2 },
T(im1 ,i2 ] {Ti T(i1 ,i2 ] : m(Ti ) = m} .
280

fiThe Landscape Random Job Shop Scheduling Instances

Lemma 4. Let random N JSP instance, let random schedule I,
let optimal schedule kS Sk minimal. fixed
> 0, holds whp (as N ) kS Sk < N 1+ .
Proof. Let sequence operations ops(I), sorted ascending order priority
rand (I, o) (where rand random priority rule used create S). Note
ops(I) J (o) 6= , J (o) must appear . Let Ti denote ith operation
.
Consider schedule defined following procedure:
1. S(o) ops(I).
2. Q (). Let Qj denote j th operation Q.
3. Let function ready(o) return true + (M(o)) + (J (o)), false otherwise.
4. 1 N do:
(a) ready(Ti ), set S(o) + (M(Ti )). Otherwise append Ti onto Q.
(b) j 1 |Q| do:
i. ready(Qj ), set S(Qj ) + (M(Qj )) remove Qj Q.
5. Schedule remaining operations Q manner specified (in last
paragraph proof).
construction like construction S, except manipulations
involving Q. purpose Q delay scheduling operation that,
scheduled immediately, might produce schedule S(o) > + (M(o)). first
show kS Sk < N 1+ whp; show optimal whp.
Let P
Qi denote Q exists iterations step 4 performed. Let
NM

iterations Q. claim
q(o) =
i=1
P |o Q | number
N

|. Letting E 6= = {e E(I) : ~e(S) 6= ~e(S)},
kS Sk oops(I) q(o) + (N 1)|Q
kS Sk = |{e E 6= : e QN = }| + |{e E 6= : e QN 6= }|
|{e E 6= : e QN = }| + (N 1)|QN |
P
suffices show |{e E 6= : e QN = }|
oops(I) q(o). see this, let
=
6
N

e = {o1 , o2 } E e Q
= . must q(o1 ) + q(o2 ) > 0. charge e
operation {o1 , o2 } inserted Q first. easy see operation
charged one edge perPiteration spends Q, establishing claim.
Thus suffices show kS Sk oops(I) q(o) + (N 1)|QN | N 1+ whp.
1

0

1

0

divide construction n = N 2 epochs, consisting N 2 +
iterations step 4, to-be-specified 0 > 0. Let zj denote number iterations
step 4 occur end j th epoch, zj = 0 j 0 convention. Let

Cjm T(0,z
\ Qzj set operations scheduled run
j]

end j th epoch;
281

fiStreeter & Smith


Onear j[n] {o T(zj1 ,zj ] : J (o) T(zj(M +2) ,zj ] } set operations whose
job-predecessor belongs nearby epoch.
1

0

[N ], P[Ti Onear ] (M + 2)N 2 + . Thus j [n], E[|Onear
0
T(zj1 ,zj ] |] (M + 2)N 2 . Using A.P.I. straightforward show whp,
|Onear T(zj1 ,zj ] | N

1+0
2

j [n] .

(9.1)

claim whp, following statements hold j [n]:
[

Qi Onear ,

(9.2)

izj

J Qzj1 6= |J Qzj1 Qzj | < |J Qzj1 |
zj

zjM

Q Q

|Qzj | N

J ,

(9.3)

= ,
1+0
2

(9.4)

.

(9.5)

prove induction, step induction fails exponentially
small probability. j = 0, (9.3) (9.4) hold trivially. (9.2) true
operations T(0,z1 ] \ Onear first operations jobs, hence cannot added
Q. (9.5) follows (9.2) (9.1).
Consider case j > 0. show (9.2), let arbitrary operation T(zj1 ,zj ] \Onear .
m(J (o))

induction hypothesis (specifically, equation (9.4)), J (o) Cj2




m(J (o))
m(o)
0 Cj2
> Cj1 . induction hypothesis,

. Thus q(o) >









1+0
m(o)
m(J (o))
m(o)
m(J (o))
.
Cj1 Cj2
T(0,zj1 ] N 2 T(0,zj2 ]
Letting denote right hand side inequality, E[] =
1+0
2

1
+0
1
2
MN



, A.P.I. used show K > 0 independent N , P[ <
MN
0
0
0] exp( K1 N ). Thus (9.2) holds probability least 1 exp( K1 N ).
show (9.3), let J J Qzj1 6= , let Ji Qzj1
chosenso
m(J (Ji ))
m(J (Ji ))
m(J )
minimal. J (Ji ) Cj1
. Thus Ji Qzj Cj1
> Cj . (9.1),
1+0

(9.2), induction hypothesis (equation (9.5)), |Qzj | (M + 1)N 2 . Using
0
technique above, show (9.3) holds probability least 1 exp( K1 N )
K > 0 independent N .
(9.3) implies (9.4). (9.2) (9.4) together (9.1) imply (9.5). Thus whp, (9.2)
(9.5) hold j [n].
(9.2) (9.4),


X
1
0
0
E
q(o) E[|Onear |]M N 2 + 2 (M + 2)N 1+2
oops(I)

also
282

fiThe Landscape Random Job Shop Scheduling Instances

0

E[|QN |] E[|T(znM ,zn ] Onear |] (M + 2)N 2
P
setting 0 = 3 gives kS Sk oops(I) q(o) + (N 1)|QN | N 1+ whp.
remains show optimal whp. first prove following claim.
Claim 4.1. non-negative integers b, probability T(a,b] contains
two operations job

(ba)2
N .

Proof Claim 4.1. Let X denote number pairs operations T(a,b] belong
1
(ba)2
job. P[X > 0] E[X] ba
2 N
N .
see optimal whp, note operations scheduled prior step 5
cause idle time machine, operations QN cause
sub-optimal. Let (m) ({o ops(I) : m(o) = m}) denote workload machine
m. Let = arg maxm[M ] (m). following hold whp.
set Z
last. (It holds

1

consists operations belonging jobs use

(N 2M N 4 ,N ]
whp Z Z,

Z

1

(N N 3 ,N ]

. Z contains

operation job use last, Z must contain two operations
job. Claim 4.1, probability happens
1
(N 3 )2 N1 = o(1).)
1

N 4 (Z ) (Z ) (m) (m) 6= m. (This follows applying
Central Limit Theorem (Z ), (m), (m)).
Thus whp holds prior execution step 5, contains period length
1
least (Z ) N 4 operations processed Z ,
0
{o ops(I) : J (o) Z } = . Assuming |QN | < N 3 (holds whp), always
schedule operations QN guarantee `(S) = (m), implies optimal.

Theorem 6. Let random N JSP instance optimal makespan `min (I)
let random schedule I. fixed > 0, holds whp (as N )
`(S) (1 + )`min (I).
Proof. proof Lemma 4, let sequence operations ops(I), sorted
ascending order priority rand (I, o) (where rand random priority rule used
create S). Note ops(I) J (o) 6= , J (o) must appear .
Let Ti denote ith operation .
Rather analyze directly, analyze schedule defined following procedure:
1. 0.
2. 1 N do:
283

fiStreeter & Smith

(a) Set S(Ti ) = max(t, + (J (Ti )), + (M(Ti ))) .
(b) + (J (Ti )) > + (M(Ti )), set = maxi0 + (Ti0 ).
procedure identical one used construct S, except that, whenever
operation Ti assigned start time S(Ti ) > + (M(Ti )), procedure inserts artificial
delays schedule order re-synchronize machines. , clear
`(S) `(S). Thus, suffices show `(S) (1 + )`min (I) whp.
divide construction n epochs, update (in step 2(b)) defines beginning new epoch. Let zi number operations scheduled
end ith epoch, z0 = 0 convention. Let ti = maxi0 zi + (oi0 ) (updated)
P
+
0
value end ofPthe ith epoch. Define
m=1
Ptni maxi0 <i,m(Ti0 )=m (Ti ).
n
`(S) `min (I) i=1 , suffices show i=1 `min (I) whp.
P
2
Let P
= [n], let L = {i : zi zi1 N 7 }. first consider iL ;
consider iI\L .
2

Let i1 i2 arbitrary integers 0 i1 , i2 N i2 i1 N 7 . Let
i1
. , function outcome
= (T(im1 ,i2 ] ). E[ ] = i2M
i2 i1 events (namely, definition jobs {J : J T(i1 ,i2 ] 6= }),
alters value max . follows A.P.I.
!
0

N 2
0
P[| E[ ]| > N
i2 i1 ] 2 exp 2
2max

0 > 0. Thus, holds whp | E[ ]| N i2 i1 possible choices
0
i1 i2 . particular, whp
2M N zi zi1 L, implies
p
P
5 P
6
2
0
0
7
2M N N 7 = 2M N 7 + .
iL N
iL
P
consider
iI\L . shown proof Lemma 4 (Claim 4.1),
non-negative integers b probability T(a,b] contains two operations
2

2

7
job (ba)
N . Thus probability arbitrary subsequence size N
4
37
contains two operations job N , E[|I \ L|] N 7 . Clearly
P
2
6
max N 7 \ L, E[ iI\L ] O(N 7 ).
P
P
6
6
0
0
Thus E[ iI ] O(N 7 + ) 0 > 0, iI N 7 +2 whp, easy
see `min (I) N2 whp.

References
Achlioptas, D., & Peres, Y. (2004). threshold random k-SAT 2k log 2 O(k).
Journal AMS, 17, 947973.
Balas, E. (1969). Machine sequencing via disjunctive graphs: implicit enumeration
algorithm. Operations Research, 17, 110.
Beasley, J. E. (1990). OR-library: Distributing test problems electronic mail. Journal
Operational Research Society, 41(11), 10691072.
284

fiThe Landscape Random Job Shop Scheduling Instances

Beck, J. C., & Jackson, W. K. (1997). Constrainedness phase transition job
shop scheduling. Tech. rep. CMPT97-21, School Computing Science, Simon Fraser
University.
Boese, K. D., Kahng, A. B., & Muddu, S. (1994). new adaptive multi-start technique
combinatorial global optimizations. Operations Research Letters, 16, 101113.
Brucker, P., Jurisch, B., & Sievers, B. (1992). Job-shop (C-codes). European Journal Operational Research, 57, 132133. Code available http://optimierung.
mathematik.uni-kl.de/ORSEP/contents.html.
Brucker, P., Jurisch, B., & Sievers, B. (1994). branch bound algorithm
job-shop scheduling problem. Discrete Applied Mathematics, 49(1-3), 107127.
Cheeseman, P., Kanefsky, B., & Taylor, W. M. (1991). really hard problems are.
Proceedings Twelfth International Joint Conference Artificial Intelligence,
IJCAI-91, Sidney, Australia, pp. 331337.
Fisher, H., & Thompson, G. L. (1963). Probabilistic learning combinations local job-shop
scheduling rules. Muth, J. F., & Thompson, G. L. (Eds.), Industrial Scheduling,
pp. 225251. Prentice-Hall, Englewood Cliffs, NJ.
Framinan, J. M., Gupta, J. N. D., & Leisten, R. (2004). review classification
heuristics permutation flow-shop scheduling makespan objective. Journal
Operational Research Society, 55(12), 12431255.
French, S. (1982). Sequencing Scheduling: Introduction Mathematics
Job-Shop. Wiley, New York.
Glover, F., & Laguna, M. (1997). Tabu Search. Kluwer Academic Publishers, Boston, MA.
Hejazi, S. R., & Saghafian, S. (2005). Flowshop-scheduling problems makespan criterion: review. International Journal Production Research, 43(14), 28952929.
Hoos, H. H., & Stutzle, T. (2000). SATLIB: online resource research SAT.
Gent, I. P., v. Maaren, H., & Walsh, T. (Eds.), Proceedings SAT 2000, pp. 283292.
SATLIB available online www.satlib.org.
Jain, A., & Meeran, S. (1998). state-of-the-art review job-shop scheduling techniques.
Tech. rep., Department Applied Physics, Electronic Mechanical Engineering,
University Dundee, Dundee, Scotland.
Jones, A., & Rabelo, L. C. (1998). Survey job shop scheduling techniques. Tech. rep.,
National Institute Standards Technology, Gaithersburg, MD.
Kim, Y.-H., & Moon, B.-R. (2004). Investigation fitness landscapes graph bipartitioning: empirical study. Journal Heuristics, 10, 111133.
Lourenco, H., Martin, O., & Stutzle, T. (2003). Iterated local search. Glover, F., &
Kochenberger, G. (Eds.), Handbook Metaheuristics. Kluwer Academic Publishers,
Boston, MA.
Mammen, D. L., & Hogg, T. (1997). new look easy-hard-easy pattern combinatorial search difficulty. Journal Artificial Intelligence Research, 7, 4766.
285

fiStreeter & Smith

Martin, O. C., Otto, S. W., & Felten, E. W. (1991). Large-step Markov chains
traveling salesman problem. Complex Systems, 5, 299326.
Martin, O. C., Monasson, R., & Zecchina, R. (2001). Statistical mechanics methods
phase transitions combinatorial problems. Theoretical Computer Science, 265(1-2),
367.
Mattfeld, D. C. (1996). Evolutionary Search Job Shop: Investigations Genetic
Algorithms Production Scheduling. Physica-Verlag, Heidelberg.
Mattfeld, D. C., Bierwirth, C., & Kopfer, H. (1999). search space analysis job shop
scheduling problem. Annals Operations Research, 86, 441453.
Mezard, M., & Parisi, G. (1986). replica analysis traveling salesman problem.
Journal de Physique, 47, 12851296.
Monasson, R., Zecchina, R., Kirkpatrick, S., Selman, B., & Troyansky, L. (1999). Determining computational complexity characteristic phase transitions. Nature, 400,
133137.
Nowicki, E., & Smutnicki, C. (1996). fast taboo search algorithm job-shop problem.
Management Science, 42(6), 797813.
Nowicki, E., & Smutnicki, C. (2001). new ideas TS job shop scheduling. Tech.
rep. 50/2001, University Wroclaw.
Nowicki, E., & Smutnicki, C. (2005). advanced tabu search algorithm job shop
problem. Journal Scheduling, 8, 145159.
Reeves, C. R., & Yamada, T. (1998). Genetic algorithms, path relinking, flowshop
sequencing problem. Evolutionary Computation, 6, 4560.
Roy, B., & Sussmann, B. (1964). Les problemes dordonnancement avec contraintes disjonctives. Note D.S. no. 9 bis, SEMA, Paris, France, Decembre.
Selman, B., Kautz, H., & Cohen, B. (1994). Noise strategies local search. Proceedings
AAAI-94, pp. 337343.
Slaney, J., & Walsh, T. (2001). Backbones optimization approximation. Proceedings 17th International Joint Conference Artificial Intelligence (IJCAI2001), pp. 254259.
Spencer, J. (2005). Modern probabilistic methods combinatorics. http://www.cs.nyu.
edu/cs/faculty/spencer/papers/stirlingtalk.pdf.
Streeter, M. J., & Smith, S. F. (2005a). Characterizing distribution low-makespan
schedules job shop scheduling problem. Biundo, S., Myers, K., & Rajan, K.
(Eds.), Proceedings ICAPS 2005, pp. 6170.
Streeter, M. J., & Smith, S. F. (2005b). Supplemental material ICAPS 2005 paper
Characterizing distribution low-makespan schedules job shop scheduling
problem. http://www.cs.cmu.edu/~matts/icaps_2005.
Taillard, E. (1993). Benchmarks basic scheduling problems. European Journal Operational Research, 64, 278285.
286

fiThe Landscape Random Job Shop Scheduling Instances

Taillard, E. (1994). Parallel taboo search techniques job shop scheduling problem.
ORSA Journal Computing, 6, 108117.
van Laarhoven, P., Aarts, E., & Lenstra, J. (1992). Job shop scheduling simulated
annealing. Operations Research, 40(1), 113125.
Watson, J.-P., Barbulescu, L., Whitley, L. D., & Howe, A. (2002). Contrasting structured
random permutation flow-shop scheduling problems: search-space topology
algorithm performance. INFORMS Journal Computing, 14(2), 98123.
Watson, J.-P., Beck, J. C., Howe, A. E., & Whitley, L. D. (2001). Toward understanding
local search cost job-shop scheduling. Cesta, A. (Ed.), Proceedings Sixth
European Conference Planning.
Yokoo, M. (1997). adding constraints makes problem easier hill-climbing
algorithms: Analyzing landscapes CSPs. Principles Practice Constraint
Programming, pp. 356370.
Zhang, W. (2004). Configuartion landscape analysis backbone guided local search: Part
I: Satisfiability maximum satisfiability. Artificial Intelligence, 158(1), 126.
Zhang, W., & Looks, M. (2005). novel local search algorithm traveling salesman problem exploits backbones. Proceedings 19th International Joint
Conference Artificial Intelligence, pp. 343350.

287

fiJournal Artificial Intelligence Research 26 (2006) 134

Submitted 11/05; published 05/06

Logic Reasoning Evidence
Joseph Y. Halpern

halpern@cs.cornell.edu

Cornell University, Ithaca, NY 14853 USA

Riccardo Pucella

riccardo@ccs.neu.edu

Northeastern University, Boston, 02115 USA

Abstract
introduce logic reasoning evidence essentially views evidence
function prior beliefs (before making observation) posterior beliefs (after
making observation). provide sound complete axiomatization logic,
consider complexity decision problem. Although reasoning logic
mainly propositional, allow variables representing numbers quantification
them. expressive power seems necessary capture important properties evidence.

1. Introduction
Consider following situation, essentially taken Halpern Tuttle (1993)
Fagin Halpern (1994). coin tossed, either fair double-headed. coin
lands heads. likely coin double-headed? coin tossed
20 times lands heads time? Intuitively, much likely coin
double-headed latter case former. likelihood
measured? cannot simply compute probability coin double-headed;
assigning probability event requires prior probability coin
double-headed. example, coin chosen random barrel
one billion fair coins one double-headed coin, still overwhelmingly likely
coin fair, sequence 20 heads unlucky. However, problem
statement, prior probability given. show given prior probability
coin double-headed increases significantly result seeing 20 heads. But,
intuitively, seems able say seeing 20 heads row provides
great deal evidence favor coin double-headed without invoking prior.
great deal work trying make intuition precise,
review.
main feature coin example involves combination probabilistic outcomes (e.g., coin tosses) nonprobabilistic outcomes (e.g., choice
coin). great deal work reasoning systems combine
probabilistic nondeterministic choices; see, example, Vardi (1985), Fischer Zuck
(1988), Halpern, Moses, Tuttle (1988), Halpern Tuttle (1993), de Alfaro (1998),
He, Seidel, McIver (1997). However, observations suggest attempt
formally analyze situation one frameworks, essentially permit
modeling probabilities, able directly capture intuition
increasing likelihood. see plays out, consider formal analysis situation
Halpern-Tuttle (1993) framework. Suppose Alice nonprobabilistically chooses

c
2006
AI Access Foundation. rights reserved.

fiHalpern & Pucella

one two coins: fair coin probability 1/2 landing heads, double-headed coin
probability 1 landing heads. Alice tosses coin repeatedly. Let k formula
stating: kth coin toss lands heads. probability k according Bob,
know coin Alice chose, even probability Alices choice?
According Halpern-Tuttle framework, modeled considering
set runs describing states system point time, partitioning
set two subsets, one coin used. set runs fair coin
used, probability k 1/2; set runs double-headed coin
used, probability k 1. setting, conclusion drawn
(PrB (k ) = 1/2) (PrB (k ) = 1). (This course probability Bobs point
view; Alice presumably knows coin using.) Intuitively, seems reasonable:
fair coin chosen, probability kth coin toss lands heads, according
Bob, 1/2; double-headed coin chosen, probability 1. Since Bob
know coins used, said.
suppose that, 101st coin toss, Bob learns result first 100
tosses. Suppose, moreover, landed heads. probability
101st coin toss lands heads? analysis, still either 1/2 1, depending
coin used.
hardly useful. make matters worse, matter many coin tosses Bob
witnesses, probability next toss lands heads remains unchanged.
answer misses important information. fact first 100 coin
tosses heads strong evidence coin fact double-headed. Indeed,
straightforward computation using Bayes Rule shows prior probability
coin double-headed , observing 100 tosses land heads,
probability coin double-headed becomes

+ 2100 (1 )

=

2100
.
2100 + (1 )

However, note possible determine posterior probability coin
double-headed (or 101st coin toss heads) without prior probability .
all, Alice chooses double-headed coin probability 10100 , still
overwhelmingly likely coin used fact fair, Bob unlucky
see unrepresentative sequence coin tosses.
None frameworks described reasoning nondeterminism probability takes issue evidence account. hand, evidence
discussed extensively philosophical literature. Much discussion occurs
philosophy science, specifically confirmation theory, concern historically assess support evidence obtained experimentation lends various
scientific theories (Carnap, 1962; Popper, 1959; Good, 1950; Milne, 1996). (Kyburg (1983)
provides good overview literature.)
paper, introduce logic reasoning evidence. logic extends
logic defined Fagin, Halpern Megiddo (1990) (FHM on) reasoning
likelihood expressed either probability belief. logic first-order quantification
reals (so includes theory real closed fields), FHM logic,
reasons shortly become clear. add observations states, provide
2

fiA Logic Reasoning Evidence

additional operator talk evidence provided particular observations. also
refine language talk prior probability hypotheses posterior
probability hypotheses, taking account observation states. lets us
write formulas talk relationship prior probabilities, posterior
probabilities, evidence provided observations.
provide sound complete axiomatization logic. obtain
axiomatization, seem need first-order quantification fundamental way. Roughly
speaking, ensuring evidence operator appropriate properties
requires us assert existence suitable probability measures. seem possible without existential quantification. Finally, consider complexity
satisfiability problem. complexity problem full language requires exponential
space, since incorporates theory real closed fields, exponential-space
lower bound known (Ben-Or, Kozen, & Reif, 1986). However, show satisfiability problem propositional fragment language, still strong enough
allow us express many properties interest, decidable polynomial space.
reasonable ask point bother logic evidence.
claim many decisions practical applications made basis evidence.
take example security, consider enforcement mechanism used detect
react intrusions computer system. enforcement mechanism analyzes
behavior users attempts recognize intruders. Clearly mechanism wants
make sensible decisions based observations user behaviors. this?
One way think enforcement mechanism accumulating evidence
hypothesis user intruder. accumulated evidence used
basis decision quarantine user. context, clear
reasonable way assign prior probability whether user intruder. want
specify behavior systems prove meet specifications,
helpful logic allows us this. believe logic propose
first so.
rest paper organized follows. next section, formalize notion
evidence captures intuitions outlined above. Section 3, introduce logic
reasoning evidence. Section 4, present axiomatization logic
show sound complete respect intended models. Section 5,
discuss complexity decision problem logic. Section 6, examine
alternatives definition weight evidence use. ease exposition,
paper, consider system two time points:
observation. Section 7, extend work dynamic systems,
multiple pieces evidence, obtained different points time. proofs technical
results found appendix.

2. Measures Confirmation Evidence
order develop logic reasoning evidence, need first formalize
appropriate notion evidence. section, review various formalizations
literature, discuss formalization use. Evidence studied depth
philosophical literature, name confirmation theory. Confirmation theory aims

3

fiHalpern & Pucella

determining measuring support piece evidence provides hypothesis.
mentioned introduction, many different measures confirmation proposed
literature. Typically, proposal judged degree satisfies
various properties considered appropriate confirmation. example, may
required piece evidence e confirms hypothesis h e makes h
probable. desire enter debate class measures confirmation
appropriate. purposes, confirmation functions inappropriate:
assume given prior set hypotheses observations.
marginalization, also prior hypotheses, exactly information
want assume. One exception measures evidence use
log-likelihood ratio. case, rather prior hypotheses observations,
suffices probability h observations hypothesis h: intuitively,
h (ob) probability observing ob h holds. Given observation ob, degree
confirmation provides hypothesis h


h (ob)
,
l(ob, h) = log
h (ob)
h represents hypothesis h (recall approach applies
two hypotheses). Thus, degree confirmation ratio
two probabilities. use logarithm critical here. Using ensures
likelihood positive observation confirms hypothesis. approach
advocated Good (1950, 1960), among others.1
One problem log-likelihood ratio measure l defined
used reason evidence discriminating two competing hypotheses,
namely hypothesis h holding hypothesis h holding. would like
measure confirmation along lines log-likelihood ratio measure,
handle multiple competing hypotheses. number generalizations,
example, Pearl (1988) Chan Darwiche (2005). focus generalization given Shafer (1982) context Dempster-Shafer theory evidence
based belief functions (Shafer, 1976); studied Walley (1987).
description taken mostly Halpern Fagin (1992). measure
confirmation number nice properties take advantage, much work
presented paper adapted different measures confirmation.
start finite set H mutually exclusive exhaustive hypotheses; thus,
exactly one hypothesis holds given time. Let set possible observations
(or pieces evidence). simplicity, assume finite. case loglikelihood, also assume that, hypotheses h H, probability measure
h h (ob) probability ob hypothesis h holds. Furthermore,
assume observations relevant hypotheses: every observation
ob O, must hypothesis h h (ob) > 0. (The measures h often
called likelihood functions literature.) define evidence space (over H O)
1. Another related approach, Bayes factor approach, based taking ratio odds rather
likelihoods (Good, 1950; Jeffrey, 1992). remark literature, confirmation usually taken
respect background knowledge. ease exposition, ignore background knowledge
here, although easily incorporated framework present.

4

fiA Logic Reasoning Evidence

tuple E = (H, O, ), function assigns every hypothesis h H
likelihood function (h) = h . (For simplicity, usually write h (h),
function clear context.)
Given evidence space E, define weight observation ob lends hypothesis h, written (ob, h),
h (ob)
.
h0 H h0 (ob)

(ob, h) = P

(1)

measure always lies 0 1; intuitively, (ob, h) = 1, ob fully
confirms h (i.e., h certainly true ob observed), (ob, h) = 0, ob
disconfirms h (i.e.,
P h certainly false
P ob observed). Moreover, fixed observation
ob

(ob)
>
0,
h
hH
hH (ob, h) = 1, thus weight evidence
looks like probability measure ob. useful technical
consequences, one interpret probability measure. Roughly speaking,
weight (ob, h) likelihood h right hypothesis light observation
ob.2 advantages known measures confirmation (a)
applicable given prior probability distribution hypotheses, (b)
applicable two competing hypotheses, (c) fairly
intuitive probabilistic interpretation.
important problem statistical inference (Casella & Berger, 2001) choosing
best parameter (i.e., hypothesis) explains observed data. prior
parameters, best parameter typically taken one maximizes
likelihood data given parameter. Since normalized likelihood
function, parameter maximizes likelihood also maximize . Thus,
interested maximizing likelihood, need normalize evidence
do. return issue normalization Section 6.3
Note H = {h1 , h2 }, sense generalizes log-likelihood ratio
measure. precisely, fixed observation ob, (ob, ) induces relative order
hypotheses l(ob, ), fixed hypothesis h, (, h) induces relative
order observations l(, h).
Proposition 2.1: ob, (ob, hi ) (ob, h3i ) l(ob, hi )
l(ob, h3i ), = 1, 2, h, ob, ob 0 , (ob, h) (ob 0 , h)
l(ob, h) l(ob 0 , h).
2. could taken log ratio make parallel log-likelihood ratio l defined earlier,
technical advantages weight evidence number 0 1.
3. Another representation evidence similar characteristics Shafers original representation evidence via belief functions (Shafer, 1976), defined
wES (ob, h) =

h (ob)
.
maxhH h (ob)

measure known statistical hypothesis testing generalized likelihood-ratio statistic.
another generalization log-likelihood ratio measure l. main difference wES
behave one considers combination evidence, discuss later section.
Walley (1987) Halpern Fagin (1992) point out, gives intuitive results case.
remark parameter (hypothesis) maximized likelihood also maximizes wES , wES
also used statistical inference.

5

fiHalpern & Pucella

Although (ob, ) behaves like probability measure hypotheses every observation ob, one think probability; weight evidence combined
hypothesis, instance, generally sum weights individual hypotheses (Halpern & Pucella, 2005a). Rather, (ob, ) encoding evidence.
evidence? Halpern Fagin (1992) suggested evidence thought
function mapping prior probability hypotheses posterior probability, based
observation made. precise sense viewed function
maps prior probability 0 hypotheses H posterior probability ob based
observing ob, applying Dempsters Rule Combination (Shafer, 1976). is,
ob = 0 (ob, ),

(2)

combines two probability distributions H get new probability distribution
H defined follows:
P
1 (h)2 (h)
(1 2 )(H) = PhH
.
hH 1 (h)2 (h)
(Dempsters Rule Combination used combine belief functions. definition
complicated considering arbitrary belief functions, special case
belief functions fact probability measures, takes form give here.)
Bayes Rule standard way updating prior probability based observation,
applicable joint probability distribution hypotheses
observations (or, equivalently, prior hypotheses together likelihood
functions h h H), something want assume given.
particular, willing assume given likelihood functions,
willing assume given prior hypotheses. Dempsters Rule
Combination essentially simulates effects Bayes Rule. relationship
Dempsters Rule Bayes Rule made precise following well-known theorem.
Proposition 2.2: (Halpern & Fagin, 1992) Let E = (H, O, ) evidence space. Suppose
P probability H P (H {ob} | {h} O) = h (ob) h H
ob O. Let 0 probability H induced marginalizing P ; is, 0 (h) =
P ({h} O). ob O, let ob = 0 (ob, ). ob (h) = P ({h} | H {ob}).
words, joint probability hypotheses observations, Dempsters Rule Combination gives us result straightforward
application Bayes Rule.
Example 2.3: get feel measure evidence used, consider
variation two-coins example introduction. Assume coin chosen
Alice either double-headed fair, consider sequences hundred tosses coin.
Let = {m : 0 100} (the number heads observed), let H = {F, D}, F
coin fair, coin double-headed. probability spaces associated
hypotheses generated following probabilities simple observations m:



1 100
1 = 100
F (m) = 100
(m) =
0 otherwise.
2

6

fiA Logic Reasoning Evidence

(We extend additivity whole set O.) Take E = (H, O, ), (F ) = F
(D) = . observation 6= 100, weight favor F given

1 100
(m, F ) =

0

2100

1 100
+ 2100


= 1,

means support unconditionally provided F ; indeed,
sequence tosses cannot appear double-headed coin. Thus, 6= 100, get

0
= 0.
(m, D) =
1 100
0 + 2100
happens hundred coin tosses heads? straightforward check

1
1
1
2100
100
w
(100,
D)
=
;
(100, F ) = 2 1 =
=
E
1
1 + 2100
1 + 2100
1 + 2100
1 + 2100

time overwhelmingly evidence favor F .
Note assumed prior probability. Thus, cannot talk
probability coin fair double-headed. quantitative assessment
evidence favor one hypotheses. However, assume prior probability
coin fair heads observed 100 tosses, probability
coin fair 1 6= 100; = 100 then, applying rule combination,
posterior probability coin fair /( + (1 )2100 ).

u
characterize weight functions using small number properties? precisely,
given sets H O, function f H [0, 1], properties f
ensure likelihood functions f = E = (H, O, )?
saw earlier, fixed observation ob, f essentially acts like probability measure
H. However, sufficient guarantee f weight function. Consider
following example, = {ob 1 , ob 2 } H = {h1 , h2 , h3 }:
f (ob 1 , h1 ) = 1/4
f (ob 1 , h2 ) = 1/4
f (ob 1 , h3 ) = 1/2

f (ob 2 , h1 ) = 1/4
f (ob 2 , h2 ) = 1/2
f (ob 2 , h3 ) = 1/4.

straightforward check f (ob 1 , ) f (ob 2 , ) probability measures H,
evidence space E = (H, O, ) f = . Indeed, assume
h1 , h2 , h3 . definition weight evidence, fact f
weight evidence, get following system equations:
h1 (ob 1 )
h1 (ob 1 )+h2 (ob 1 )+h3 (ob 1 )
h2 (ob 1 )
h1 (ob 1 )+h2 (ob 1 )+h3 (ob 1 )
h3 (ob 1 )
h1 (ob 1 )+h2 (ob 1 )+h3 (ob 1 )

h1 (ob 2 )
h1 (ob 2 )+h2 (ob 2 )+h3 (ob 2 )
h2 (ob 2 )
h1 (ob 2 )+h2 (ob 2 )+h3 (ob 2 )
h3 (ob 2 )
h1 (ob 2 )+h2 (ob 2 )+h3 (ob 2 )

= 1/4
= 1/4
= 1/2

= 1/4
= 1/2
= 1/4.

immediate exist 1 2 hi (ob j ) = j f (ob j , hi ),
= 1, 2, 3. Indeed, j = h1 (ob j ) + h2 (ob j ) + h3 (ob j ), j = 1, 2. Moreover, since hi
probability measure, must
hi (ob 1 ) + hi (ob 2 ) = 1 f (ob 1 , hi ) + 2 f (ob 2 , hi ) = 1,
7

fiHalpern & Pucella

= 1, 2, 3. Thus,
1 /4 + 2 /4 = 1 /4 + 2 /2 = 1 /2 + 4 /4 = 1.
constraints easily seen unsatisfiable.
argument generalizes arbitrary functions f ; thus, necessary condition f
weight function exists observation ob h (ob ) =
f (ob , h) hypothesis h probability measure, is, 1 f (ob 1 , h) + +
k f (ob k , h) = 1. fact, combined constraint f (ob, ) probability
measure fixed ob, condition turns sufficient, following theorem
establishes.
Theorem 2.4: Let H = {h1 , . . . , hm } = {ob 1 , . . . , ob n }, let f real-valued
function domain H f (ob, h) [0, 1]. exists evidence space
E = (H, O, ) f = f satisfies following properties:
WF1. every ob O, f (ob, ) probability measure H.
P
WF2. exists x1 , . . . , xn > 0 that, h H, ni=1 f (ob , h)xi = 1.
characterization fundamental completeness axiomatization
logic introduce next section. characterization complicated fact
weight evidence essentially normalized likelihood: likelihood
observation given particular hypothesis normalized using sum likelihoods
observation, possible hypotheses. One consequence this, already
mentioned above, weight evidence always 0 1, superficially
behaves like probability measure. Section 6, examine issue normalization
carefully, describe changes framework would occur
take unnormalized likelihoods weight evidence.
Let E = (H, O, ) evidence space. Let set sequences observations
hob 1 , . . . , ob k O.4 Assume observations independent, is, basic
hypothesis h, take h (hob 1 , . . . , ob k i), probability observing particular sequence
observations given h, h (ob 1 ) h (ob k ), product probability making
observation sequence. Let E = (H, , ). assumption, well
known Dempsters Rule Combination used combine evidence setting;
is,
(hob 1 , . . . , ob k i, ) = (ob 1 , ) (ob k , )
(Halpern & Fagin, 1992, Theorem 4.3). easy exercise check weight
provided sequence observations hob 1 , . . . , ob k expressed terms
weight individual observations:
(ob 1 , h) (ob k , h)
.
1 0
k 0
h0 H (ob , h ) (ob , h )

(hob 1 , . . . , ob k i, h) = P

(3)

4. use superscript rather subscripts index observations sequence observations
confused basic observations ob 1 , . . . , ob n O.

8

fiA Logic Reasoning Evidence

let 0 prior probability hypotheses, hob 1 ,...,ob k probability
hypotheses observing ob 1 , . . . , ob k , verify
hob1 ,...,ob k = 0 (hob 1 , . . . , ob k i, ).
Example 2.5: Consider variant Example 2.3, take coin tosses individual observations, rather number heads turn one hundred coin
tosses. before, assume coin chosen Alice either double-headed fair. Let
= {H, }, result individual coin toss, H coin landed heads
coin landed tails. Let H = {F, D}, F coin fair,
coin double-headed. Let E = (H, , ). probability measure h associated
hypothesis h generated following probabilities simple observations:
F (H) =

1
2

(H) = 1.

Thus, example, F (hH, H, T, Hi) = 1/16, (hH, H, Hi) = 1, H (hH, H, T, Hi) =
0.
easily verify results similar obtained Example 2.3.
instance, weight observing favor F given
(T, F ) =

1
2

0+

1
2

= 1,

indicates observing provides unconditional support F ; doubleheaded coin cannot land tails.
sequences observations? weight provided sequence hob 1 , . . . , ob k
hypothesis h given Equation (3). Thus, H = hH, . . . , Hi, sequence hundred
coin tosses, check
(H, F ) =

1

1
2100
1
+ 2100

=

1
1 + 2100

(H, D) =

Unsurprisingly, result Example 2.3.

2100
1
.
=
1
1 + 2100
1 + 2100

u

3. Reasoning Evidence
introduce logic Lfo -ev reasoning evidence, inspired logic introduced
FHM reasoning probability. logic lets us reason weight evidence
observations hypotheses; moreover, able talk relationship
prior probabilities, evidence, posterior probabilities, provide operators reason
prior posterior probabilities hypotheses. remark
somewhat agnostic whether priors exist given (or
known) whether prior exist all. beyond scope paper
enter debate whether always appropriate assume existence prior.
Although definition evidence makes sense even priors exist, logic
implicitly assumes priors (although may known), since provide
9

fiHalpern & Pucella

operators reasoning prior. make use operators
examples below. However, fragment logic use operators
appropriate prior-free reasoning.
logic propositional features first-order features. take probability propositions weight evidence observations hypotheses, view
probability evidence propositions, allow first-order quantification numerical quantities, probabilities evidence. logic essentially considers two
time periods, thought time observation made
time observation made. section, assume exactly one observation
made. (We consider sequences observations Section 7.) Thus, talk
probability formula observation made, denoted Pr0 (), probability
observation, denoted Pr(), evidence provided observation ob
hypothesis h, denoted w(ob, h). course, want able use logic
relate quantities.
Formally, start two finite sets primitive propositions, h = {h1 , . . . , hnh }
representing hypotheses, = {ob 1 , . . . , ob } representing observations. Let
Lh (h ) propositional sublanguage hypothesis formulas obtained taking primitive propositions h closing negation conjunction; use range
formulas sublanguage.
basic term form Pr0 (), Pr(), w(ob, h), hypothesis formula,
ob observation, h hypothesis. said, interpret Pr0 ()
prior probability , Pr() posterior probability , w(ob, h) weight
evidence observation ob hypothesis h. may seem strange allow language
talk prior probability hypotheses, although said
want assume prior known. could, course, simplify syntax
include formulas form Pr0 () Pr(). advantage
that, even prior known, given view evidence function priors
posteriors, make statements prior probability h 2/3, ob
observed, weight evidence ob h 3/4, posterior probability h
6/7;
Pr0 (h) = 1/2 ob w(ob, h) = 3/4 Pr(h) = 6/7.
polynomial term form t1 + + tn , term ti product integers,
basic terms, variables (which range reals). polynomial inequality formula
form p c, p polynomial term c integer. Let Lfo -ev (h , )
language obtained starting primitive propositions h
polynomial inequality formulas, closing conjunction, negation, firstorder quantification. Let true abbreviation arbitrary propositional tautology
involving hypotheses, h1 h1 ; let false abbreviation true.
definition, true false considered part sublanguage Lh (h ).
clear allow integer coefficients appear polynomial
terms, fact express polynomial terms rational coefficients crossmultiplying.
instance, 31 Pr() + 12 Pr(0 ) 1 represented polynomial inequality formula
2Pr() + 3Pr(0 ) 6. difficulty giving semantics polynomial terms
use arbitrary real coefficients, need restriction integers order make use
10

fiA Logic Reasoning Evidence

results theory real closed fields axiomatization Section 4
complexity results Section 5.
use obvious abbreviations needed, ( ),
, x x(), Pr() Pr() c Pr() + (1)Pr() c, Pr() Pr()
Pr() Pr() 0, Pr() c Pr() c, Pr() < c (Pr() c), Pr() = c
(Pr() c) (Pr() c) (and analogous abbreviations inequalities involving Pr0
w).
Example 3.1: Consider situation given Example 2.3. Let , observations,
consist primitive propositions form heads[m], integer 0
100, indicating heads 100 tosses appeared. Let h consist two
primitive propositions fair doubleheaded. computations Example 2.3
written follows:
w(heads[100], fair) = 1/(1 + 2100 ) w(heads[100], doubleheaded) = 2100 /(1 + 2100 ).
also capture fact weight evidence observation maps prior
probability posterior probability Dempsters Rule Combination. example,
following formula captures update prior probability hypothesis fair
upon observation hundred coin tosses landing heads:
Pr0 (fair) = w(heads[100], fair) = 1/(1 + 2100 ) Pr(fair) = /( + (1 )2100 ).
develop deductive system derive conclusions next section.


u

consider semantics. formula interpreted world specifies
hypothesis true observation made, well evidence space interpret
weight evidence observations probability distribution hypotheses
interpret prior probabilities talk updating based evidence. (We need
include posterior probability distribution, since computed prior
weights evidence using Equation (2).) evidential world tuple w = (h, ob, , E),
h hypothesis, ob observation, probability distribution h , E
evidence space h .
interpret propositional formulas Lh (h ), associate hypothesis formula
set [[]] hypotheses, induction structure :
[[h]] = {h}
[[]] = h [[]]
[[1 2 ]] = [[1 ]] [[2 ]].
interpret first-order formulas may contain variables, need valuation v
assigns real number every variable. Given evidential world w = (h, ob, , E)
valuation v, assign polynomial term p real number [p]w,v straightforward way:
[x]w,v = v(x)
[a]w,v =
[Pr0 ()]w,v = ([[]])
11

fiHalpern & Pucella

[Pr()]w,v = ( (ob, ))([[]])
[w(ob 0 , h0 )]w,v = (ob 0 , h0 )
[t1 t2 ]w,v = [t1 ]w,v [t2 ]w,v
[p1 + p2 ]w,v = [p1 ]w,v + [p2 ]w,v .
Note that, interpret Pr(), posterior probability observed ob (the
observation world w), use Equation (2), says posterior obtained
combining prior probability (ob, ).
define means formula true (or satisfied) evidential world
w valuation v, written (w, v) |= , follows:
(w, v) |= h w = (h, ob, , E) ob, , E
(w, v) |= ob w = (h, ob, , E) h, , E
(w, v) |= (w, v) 6|=
(w, v) |= (w, v) |= (w, v) |=
(w, v) |= p c [p]w,v c
(w, v) |= x (w, v 0 ) |= v 0 agree v variables x.
(w, v) |= true v, write simply w |= . easy check
closed formula (that is, one free variables), (w, v) |=
(w, v 0 ) |= , v, v 0 . Therefore, given closed formula , (M, w, v) |= , fact
w |= . typically concerned closed formulas. Finally, w |=
evidential worlds w, write |= say valid. next section,
characterize axiomatically valid formulas logic.
Example 3.2: following formula valid, is, true evidential worlds:
|= (w(ob, h1 ) = 2/3 w(ob, h2 ) = 1/3) (Pr0 (h1 ) 1/100 ob) Pr(h1 ) 2/101.
words, evidential worlds weight evidence observation ob
hypothesis h1 2/3 weight evidence observation ob hypothesis h2 1/3,
must case prior probability h1 least 1/100 ob actually
observed, posterior probability h1 least 2/101. shows extent
reason evidence independently prior probabilities.

u
logic imposes restriction prior probabilities used models.
implies, instance, formula
fair Pr0 (fair) = 0
satisfiable: exists evidential world w formula true w.
words, consistent hypothesis true, despite prior probability
true 0. simple matter impose restriction models
h true world, (h) > 0 prior world.
12

fiA Logic Reasoning Evidence

conclude section remarks concerning semantic model. semantic model implicitly assumes prior probability known likelihood
functions (i.e., measures h ) known. course, many situations
uncertainty both. Indeed, motivation focusing evidence precisely deal
situations prior known. Handling uncertainty prior easy
framework, since notion evidence independent prior hypotheses.
straightforward extend model allowing set possible worlds, different
prior each, using evidence space them. extend
logic knowledge operator, statement known true true
worlds. allows us make statements like know prior hypothesis
h . Since observation ob provides evidence 3/4 h, know
posterior h given ob (3)/(2 + 1) (3)/(2 + 1).
Dealing uncertainty likelihood functions somewhat subtle.
understand issue, suppose one two coins chosen tossed. bias
coin 1 (i.e., probability coin 1 lands heads) 2/3 3/4; bias coin
2 1/4 1/3. uncertainty probability coin 1
picked (this uncertainty prior) uncertainty bias
coin (this uncertainty likelihood functions). problem that,
deal this, must consider possible worlds possibly different evidence
space world. obvious define weight evidence. explore
issue detail companion paper (Halpern & Pucella, 2005a).

4. Axiomatizing Evidence
section present sound complete axiomatization AX(h , ) logic.
axiomatization divided four parts. first part, consisting
following axiom inference rule, accounts first-order reasoning:
Taut. substitution instances valid formulas first-order logic equality.
MP. infer .
Instances Taut include, example, formulas form ,
arbitrary formula logic. also includes formulas (x) x free
. particular, (x(h)) h hypotheses h , similarly observations
. Note Taut includes substitution instances valid formulas first-order logic
equality; words, valid formula first-order logic equality
free variables replaced arbitrary terms language (including Pr0 (), Pr(),
w(ob, h)) instance Taut. Axiom Taut replaced sound complete
axiomatization first-order logic equality, given, instance, Shoenfield (1967)
Enderton (1972).
second set axioms accounts reasoning polynomial inequalities, relying
theory real closed fields:
RCF. instances formulas valid real closed fields (and, thus, true reals),
nonlogical symbols +, , <, 0, 1, 1, 2, 2, 3, 3, . . . .

13

fiHalpern & Pucella

Formulas valid real closed fields include, example, fact addition
reals associative, xyz((x+y)+z = x+(y +z)), 1 identity multiplication,
x(x1 = x), formulas relating constant symbols, k = 1+ +1 (k times)
1 + 1 = 0. Taut, could replace RCF sound complete axiomatization
real closed fields (cf. Fagin et al., 1990; Shoenfield, 1967; Tarski, 1951).
third set axioms essentially captures fact single hypothesis
single observation holds per state.
H1. h1 hnh .
H2. hi hj 6= j.
O1. ob 1 ob .
O2. ob ob j 6= j.
axioms illustrate subtlety logic. Like propositional logics,
parameterized primitive propositions, case, h . However, axiomatizations propositional logics typically depend exact set primitive
propositions, does. Clearly, axiom H1 sound hypothesis primitives
exactly h1 , . . . , hnh . Similarly, axiom O1 sound observation primitives
exactly ob 1 , . . . , ob . therefore important us identify primitive propositions
talking axiomatization AX(h , ).
last set axioms concerns reasoning probabilities evidence proper.
axioms probability taken FHM.
Pr1. Pr0 (true) = 1.
Pr2. Pr0 () 0.
Pr3. Pr0 (1 2 ) + Pr0 (1 2 ) = Pr0 (1 ).
Pr4. Pr0 (1 ) = Pr0 (2 ) 1 2 propositional tautology.
Axiom Pr1 simply says event true probability 1. Axiom Pr2 says probability nonnegative. Axiom Pr3 captures finite additivity. possible express
countable additivity logic. hand, FHM, need
axiom countable additivity. Roughly speaking, establish next section,
formula satisfiable all, satisfiable finite structure. Similar axioms capture
posterior probability formulas:
Po1. Pr(true) = 1.
Po2. Pr() 0.
Po3. Pr(1 2 ) + Pr(1 2 ) = Pr(1 ).
Po4. Pr(1 ) = Pr(2 ) 1 2 propositional tautology.

14

fiA Logic Reasoning Evidence

Finally, need axioms account behavior evidence operator w.
properties? one thing, weight function acts essentially like probability
hypotheses, fixed observation, except restricted taking weight
evidence basic hypotheses only. gives following axioms:
E1. w(ob, h) 0.
E2. w(ob, h1 ) + + w(ob, hnh ) = 1.
Second, evidence connects prior posterior beliefs via Dempsters Rule Combination, (2). captured following axiom. (Note that, since
division language, crossmultiply clear denominator.)
E3. ob (Pr0 (h)w(ob, h) = Pr(h)Pr0 (h1 )w(ob, h1 ) + + Pr(h)Pr0 (hnh )w(ob, hnh )).
quite enough. saw Section 2, property WF2 Theorem 2.4
required function evidence function. following axiom captures WF2
logic:
E4. x1 . . . xno (x1 > 0 xno > 0 w(ob 1 , h1 )x1 + + w(ob , h1 )xno = 1
w(ob 1 , hnh )x1 + + w(ob , hnh )xno = 1).
Note axiom E4 axiom requires quantification. Moreover, axioms E3
E4 depend h .
example, show h h0 distinct hypotheses h , formula
(w(ob, h) = 2/3 w(ob, h0 ) = 2/3)
provable. First, RCF, following valid formula theory real closed fields
provable:
xy(x = 2/3 = 2/3 x + > 1).
Moreover, (x, y) first-order logic formula two free variables x y,
(xy((x, y))) (w(ob, h), w(ob, h0 ))
substitution instance valid formula first-order logic equality, hence
instance Taut. Thus, MP, prove
w(ob, h) = 2/3 w(ob, h0 ) = 2/3 w(ob, h) + w(ob, h0 ) > 1,
provably equivalent (by Taut MP) contrapositive
w(ob, h) + w(ob, h0 ) 1 (w(ob, h) = 2/3 w(ob, h0 ) = 2/3).
argument similar above, using RCF, Taut, MP, E1, E2, derive
w(ob, h) + w(ob, h0 ) 1,
MP, obtain desired conclusion: (w(ob, h) = 2/3 w(ob, h0 ) = 2/3).
15

fiHalpern & Pucella

Theorem 4.1: AX(h , ) sound complete axiomatization Lfo -ev (h , )
respect evidential worlds.
usual, soundness straightforward, prove completeness, suffices show
formula consistent AX(h , ), satisfiable evidential structure. However, usual approach proving completeness modal logic, involves
considering maximal consistent sets canonical structures work. problem
maximal consistent sets formulas satisfiable. example,
maximal consistent set formulas includes Pr() > 0 Pr() 1/n
n = 1, 2, . . . . clearly unsatisfiable. proof follows techniques developed
FHM.
express axiom E4, needed quantification logic.
fact representation evidence normalized nontrivial effect logic: E4
corresponds property WF2, essentially says function weight evidence
function one find normalization factor. interesting question whether
possible find sound complete axiomatization propositional fragment
logic (without quantification variables). this, need give quantifier-free
axioms replace axiom E4. amounts asking whether simpler property
WF2 Theorem 2.4 characterizes weight evidence functions. remains
open question.

5. Decision Procedures
section, consider decision problem logic, is, problem
deciding whether given formula satisfiable. order state problem precisely,
however, need deal carefully fact logic parameterized sets
h primitive propositions representing hypotheses observations.
logics, choice underlying primitive propositions essentially irrelevant. example,
propositional formula contains primitive propositions set
true respect truth assignments , remains true respect
truth assignments set 0 . monotonicity property hold here.
example, already observed, axiom H1 clearly depends set hypotheses
observations; longer valid set changed. true O1, E3,
E4.
means careful, stating decision problems, role
h algorithm. straightforward way deal assume
satisfiability algorithm gets input h , , formula Lfo -ev (h , ).
Lfo -ev (h , ) contains full theory real closed fields, unsurprisingly difficult
decide. decision procedure, use exponential-space algorithm Ben-Or,
Kozen, Reif (1986) decide satisfiability real closed field formulas. define
length || number symbols required write , count
length coefficient 1. Similarly, define kk length longest
coefficient appearing f , written binary.
Theorem 5.1: procedure runs space exponential || kk deciding,
given h , whether formula Lfo -ev (h , ) satisfiable evidential world.
16

fiA Logic Reasoning Evidence

essentially best do, since Ben-Or, Kozen, Reif (1986) prove
decision problem real closed fields complete exponential space, logic
contains full language real closed fields.
assumed algorithm takes input set primitive propositions
h , really affect complexity algorithm. precisely,
given formula Lfo -ev set hypotheses observations,
still decide whether satisfiable, is, whether sets h primitive
propositions containing primitive propositions evidential world w
satisfies .
Theorem 5.2: procedure runs space exponential || kk deciding
whether exists sets primitive propositions h Lfo -ev (h , )
satisfiable evidential world.
main culprit exponential-space complexity theory real closed fields,
add logic able even write axiom E4 axiomatization AX(h , ).5 However, interested axiomatizations, simply
verifying properties probabilities weights evidence, consider following
propositional (quantifier-free) fragment logic. before, start sets h
hypothesis observation primitives, form sublanguage Lh hypothesis
formulas. Basic terms form Pr0 (), Pr(), w(ob, h), hypothesis
formula, ob observation, h hypothesis. quantifier-free polynomial term
form a1 t1 + + tn , ai integer ti product
basic terms. quantifier-free polynomial inequality formula form p c,
p quantifier-free polynomial term, c integer. instance, quantifier-free
polynomial inequality formula takes form Pr0 () + 3w(ob, h) + 5Pr0 ()Pr(0 ) 7.
Let Lev (h , ) language obtained starting primitive propositions
h quantifier-free polynomial inequality formulas, closing conjunction negation. Since quantifier-free polynomial inequality formulas polynomial
inequality formulas, Lev (h , ) sublanguage Lfo -ev (h , ). logic Lev (h , )
sufficiently expressive express many properties interest; instance, certainly
express general connection priors, posteriors, evidence captured axiom
E3, well specific relationships prior probability posterior probability
weight evidence particular observation, Example 3.1. Reasoning
propositional fragment logic Lev (h , ) easier full language.6
5. Recall axiom E4 requires existential quantification. Thus, restrict sublanguage
consisting formulas single block existential quantifiers prefix position. satisfiability
problem sublanguage shown decidable time exponential size formula
(Renegar, 1992).
6. preliminary version paper (Halpern & Pucella, 2003), examined quantifier-free fragment
Lfo -ev (h , ) uses linear inequality formulas, form a1 t1 + + tn c,
ti basic term. claimed problem deciding, given h , whether formula
fragment satisfiable evidential world NP-complete. claimed result
followed small-model theorem: satisfiable, satisfiable evidential world
small number hypotheses observations. small-model theorem true, argument
satisfiability problem NP also implicitly assumed numbers associated
probability measure evidence space evidential world small. true

17

fiHalpern & Pucella

Theorem 5.3: procedure runs space polynomial || kk deciding,
given h , whether formula Lev (h , ) satisfiable evidential world.
Theorem 5.3 relies Cannys (1988) procedure deciding validity quantifierfree formulas theory real closed fields. general case, complexity
unaffected whether decision problem takes input sets h
primitive propositions.
Theorem 5.4: procedure runs space polynomial || kk deciding
whether exists sets primitive propositions h Lev (h , )
satisfiable evidential world.

6. Normalized Versus Unnormalized Likelihoods
weight evidence used throughout paper generalization log-likelihood
ratio advocated Good (1950, 1960). pointed earlier, measure confirmation essentially normalized likelihood: likelihood observation given particular
hypothesis normalized sum likelihoods observation, possible hypotheses. would change take (unnormalized) likelihoods h
weight evidence? things would simplify. example, WF2
consequence normalization, corresponding axiom E4, axiom
requires quantification.
main argument normalizing likelihood normalizing probability measures. like probability, using normalized likelihood, weight
evidence always 0 1, provides absolute scale judge
reports evidence. impact psychologicalit permits one use
rules thumb situations, since numbers obtained independent context use. Thus, instance, weight evidence 0.95 one situation corresponds
amount evidence weight evidence 0.95 different situation;
acceptable decision based weight evidence first situation ought
acceptable situation well. importance uniform scale
depends, course, intended applications.
sake completeness, describe changes framework required
use unnormalized likelihoods weight evidence. Define wEu (ob, h) = h (ob).
general. Even though formula involves linear inequality formulas, every evidential world
satisfies axiom E3. constraint enables us write formulas exist models
probabilities weights evidence rational. example, consider formula
Pr0 (h1 ) = w(ob 1 , h1 ) Pr0 (h2 ) = 1 Pr0 (h1 ) Pr(h1 ) = 1/2 w(ob 1 , h2 ) = 1/4
evidential world satisfying formula must satisfy
Pr0 (h1 ) = w(ob 1 , h1 ) = 1/8(1


17)

irrational. exact complexity fragment remains open. use techniques
show PSPACE, matching lower bound. (In particular, may indeed
NP.) re-examine fragment logic Section 6, different interpretation weights
evidence.

18

fiA Logic Reasoning Evidence

First, note update prior probability 0 via set likelihood functions h
using form Dempsters Rule Combination. precisely, define 0 wEu (ob, )
probability measure defined
0 (h)h (ob)
.
0
h0 H 0 (h )h0 (ob)

(0 wEu (ob, ))(h) = P

logic introduced Section 3 applies well new interpretation
weights evidence. syntax remains unchanged, models remain evidential worlds,
semantics formulas simply take new interpretation weight evidence
account. particular, assignment [p]w,v uses definition wEu ,
becomes
[Pr()]w,v = ( wEu (ob, ))([[]])
[w(ob 0 , h0 )]w,v = wEu (ob 0 , h0 ).
axiomatization new logic slightly different somewhat simpler
one Section 3. particular, E1 E2, say w(ob, h) acts probability
measure fixed ob, replaced axioms say w(ob, h) acts probability
measure fixed h:
E10 . w(ob, h) 0.
E20 . w(ob 1 , h) + + w(ob , h) = 1.
Axiom E3 unchanged, since wEu updated essentially way . Axiom E4
becomes unnecessary.
complexity decision procedure? Section 5, complexity
decision problem full logic Lfo -ev (h , ) remains dominated complexity reasoning real closed fields. course, now, express full axiomatization
unnormalized likelihood interpretation weight evidence Lev (h , ) fragment, decided polynomial space. advantage unnormalized
likelihood interpretation weight evidence, however, leads useful fragment
Lev (h , ) perhaps easier decide.
Suppose interested reasoning exclusively weights evidence,
prior posterior probability. kind reasoning actually underlies
many computer science applications involving randomized algorithms (Halpern & Pucella,
2005b). before, start sets h hypothesis observation primitives,
form sublanguage Lh hypothesis formulas. quantifier-free linear term
form a1 w(ob 1 , h1 ) + + w(ob n , hn ), ai integer, ob
observation, hi hypothesis. quantifier-free linear inequality formula
form p c, p quantifier-free linear term c integer. example,
w(ob 0 , h) + 3w(ob, h) 7 quantifier-free linear inequality formula.
Let Lw (h , ) language obtained starting primitive propositions
h quantifier-free linear inequality formulas, closing conjunction
negation. Since quantifier-free linear inequality formulas polynomial inequality
formulas, Lw (h , ) sublanguage Lfo -ev (h , ). Reasoning Lw (h , )
easier full language, possibly easier Lev (h , ) fragment.
19

fiHalpern & Pucella

Theorem 6.1: problem deciding, given h , whether formula Lw (h , )
satisfiable evidential world NP-complete.
general case, complexity unaffected whether decision
problem takes input sets h primitive propositions.
Theorem 6.2: problem deciding, formula , whether exists sets
primitive propositions h Lw (h , ) satisfiable
evidential world NP-complete.

7. Evidence Dynamic Systems
evidential worlds considered essentially static, model
situation single observation made. Considering static worlds lets
us focus relationship prior posterior probabilities hypotheses
weight evidence single observation. related paper (Halpern & Pucella,
2005b), consider evidence context randomized algorithms; use evidence
characterize information provided by, example, randomized algorithm primality
says number prime. framework work dynamic; sequences
observations made time. section, extend logic reason
evidence sequences observations, using approach combining evidence described
Section 2.
subtleties involved trying find appropriate logic reasoning
situations like Example 2.5. important one relationship
observations time. way illustration, consider following example. Bob
expecting email Alice stating rendezvous take place. Calm
pressure, Bob reading waits. assume Bob concerned
time. purposes example, one three things occur given point
time:
(1) Bob check received email;
(2) Bob checks received email, notices received email
Alice;
(3) Bob checks received email, notices received email Alice.
view world affected events? (1), clear that,
things equal, Bobs view world change: observation made.
Contrast (2) (3). (2), Bob make observation, namely
yet received Alices email. fact checks indicates wants observe
result. (3), also makes observation, namely received email Alice.
cases, check yields observation, use update view
world. case (2), essentially observed nothing happened, emphasize
observation, distinguished case Bob
even check whether email arrived, explicit set evidence
space.

20

fiA Logic Reasoning Evidence

discussion motivates models use section. characterize
agents state observations made, including possibly nothing
happened observation. Although explicitly model time, easy incorporate
time framework, since agent observe times clock ticks. models
section admittedly simple, already highlight issues involved reasoning
evidence dynamic systems. long agents forget observations,
loss generality associating agents state sequence observations. do,
however, make simplifying assumption evidence space used
observations sequence. words, assume evidence space fixed
evolution system. many situations interest, external world changes.
possible observations may depend state world, may likelihood functions.
intrinsic difficulties extending model handle state changes,
additional details would obscure presentation.
ways, considering dynamic setting simplifies things. Rather talking
prior posterior probability using different operators, need single
probability operator represents probability hypothesis current time.
express analogue axiom E3 logic, need able talk
probability next time step. done adding next-time operator
logic, holds current time holds next time step.7
extend logic talk weight evidence sequence observations.
-ev
define logic Lfo
dyn follows. Section 3, start set primitive
propositions h , respectively representing hypotheses observations.
Again, let Lh (h ) propositional sublanguage hypotheses formulas obtained
taking primitive propositions h closing negation conjunction; use
range formulas sublanguage.
basic term form Pr() w(ob, h), hypothesis formula,
ob = hob 1 , . . . , ob k nonempty sequence observations, h hypothesis.
ob = hob 1 i, write w(ob 1 , h) rather w(hob 1 i, h). before, polynomial term
form t1 + + tn , term ti product integers, basic terms, variables
(which intuitively range reals). polynomial inequality formula form
-ev
p c, p polynomial term c integer. Let Lfo
dyn (h , ) language
obtained starting primitive propositions h polynomial
inequality formulas, closing conjunction, negation, first-order quantification,
application operator. use abbreviations Section 3.
semantics logic involves models dynamic behavior. Rather
considering individual worlds, consider sequences worlds,
call runs, representing evolution system time. model infinite
run, run describes possible dynamic evolution system. before, run
records observations made hypothesis true run, well
probability distribution describing prior probability hypothesis initial
state run, evidence space E h interpret w. define
evidential run r map natural numbers (representing time) histories
7. Following discussion above, time steps associated new observations. Thus, means
true next time step, is, next observation. simplifies presentation
logic.

21

fiHalpern & Pucella

system time. history time records relevant information
runthe hypothesis true, prior probability hypotheses, evidence
space E observations made time m. Hence, history
form h(h, , E ), ob 1 , . . . , ob k i. assume r(0) = h(h, , E )i h, ,
E , r(m) = h(h, , E ), ob 1 , . . . , ob > 0. define point run
pair (r, m) consisting run r time m.
associate propositional formula Lh (h ) set [[]] hypotheses,
Section 3.
order ascribe semantics first-order formulas may contain variables,
need valuation v assigns real number every variable. Given valuation v,
evidential run r, point (r, m), r(m) = h(h, , E ), ob 1 , . . . , ob i, assign
polynomial term p real number [p]r,m,v using essentially approach
Section 3:
[x]r,m,v = v(x)
[a]r,m,v =
[Pr()]r,m,v = ( (hob 1 , . . . , ob i, )))([[]])
r(m) = h(h, , E ), ob 1 , . . . , ob
[w(ob, h0 )]r,m,v = (ob, h0 )
r(m) = h(h, , E ), ob 1 , . . . , ob
[t1 t2 ]r,m,v = [t1 ]r,m,v [t2 ]r,m,v
[p1 + p2 ]r,m,,v = [p1 ]r,m,v + [p2 ]r,m,v .
define means formula true (or satisfied) point (r, m)
evidential run r valuation v, written (r, m, v) |= , using essentially
approach Section 3:
(r, m, v) |= h r(m) = h(h, , E ), . . .i
(r, m, v) |= ob r(m) = h(h, , E ), . . . , obi
(r, m, v) |= (r, m, v) 6|=
(r, m, v) |= (r, m, v) |= (r, m, v) |=
(r, m, v) |= p c [p]r,m,v c
(r, m, v) |= (r, + 1, v) |=
(r, m, v) |= x (r, m, v 0 ) |= valuations v 0 agree v variables
x.
(r, m, v) |= true v, simply write (r, m) |= . (r, m) |= points
(r, m) r, write r |= say valid r. Finally, r |=
evidential runs r, write |= say valid.
straightforward axiomatize new logic. axiomatization shows
capture combination evidence directly logic, pleasant property.
22

fiA Logic Reasoning Evidence

axioms Section 3 carry immediately. Let axiomatization AXdyn (h , )
consists following axioms inference rules: first-order reasoning (Taut, MP), reasoning polynomial inequalities (RCF), reasoning hypotheses observations
(H1,H2,O1,O2), reasoning probabilities (Po14 only, since Pr0
language), reasoning weights evidence (E1, E2, E4), well new axioms
present.
Basically, axiom needs replacing E3, links prior posterior
probabilities, since needs expressed using operator. Moreover,
need axiom relate weight evidence sequence observation weight
evidence individual observations, given Equation (3).
E5. ob x( (Pr(h) = x)
Pr(h)w(ob, h) = xPr(h1 )w(ob, h1 ) + + xPr(hnh )w(ob, hnh )).
E6. w(ob 1 , h) w(ob k , h) = w(hob 1 , . . . , ob k i, h)w(ob 1 , h1 ) w(ob k , h1 ) + +
w(hob 1 , . . . , ob k i, h)w(ob 1 , hnh ) w(ob k , hnh ).
get complete axiomatization, also need axioms inference rules capture
properties temporal operator .
T1. ( ) .
T2. .
T3. infer .
Finally, need axioms say truth hypotheses well value polynomial
terms containing occurrences Pr time-independent:
T4. .
T5. (p c) p c p contain occurrence Pr.
T6. (x) x( ).
-ev
Theorem 7.1: AXdyn (h , ) sound complete axiomatization Lfo
dyn (h , )
respect evidential runs.

8. Conclusion
literature, reasoning effect observations typically done context
prior probability set hypotheses condition
observations made obtain new probability hypotheses reflects effect
observations. paper, presented logic evidence lets us reason
weight evidence observations, independently prior probability
hypotheses. logic expressive enough capture logical form relationship
prior probability hypotheses, weight evidence observations,
result posterior probability hypotheses. also capture reasoning
involve prior probabilities.
23

fiHalpern & Pucella

logic essentially propositional, obtaining sound complete axiomatization seems require quantification reals. adds complexity
logicthe decision problem full logic exponential space. However, interesting potentially useful fragment, propositional fragment, decidable polynomial
space.
Acknowledgments. preliminary version paper appeared Proceedings
Nineteenth Conference Uncertainty Artificial Intelligence, pp. 297304, 2003.
work mainly done second author Cornell University. thank Dexter
Kozen Nimrod Megiddo useful discussions. Special thanks Manfred Jaeger
careful reading paper subsequent comments. Manfred found bug
proof satisfiability problem quantifier-free fragment Lfo -ev (h , )
uses linear inequality formulas NP-complete. comments also led us discuss
issue normalization. also thank reviewers, whose comments greatly improved
paper. work supported part NSF grants CTC-0208535, ITR-0325453,
IIS-0534064, ONR grant N00014-01-10-511, DoD Multidisciplinary
University Research Initiative (MURI) program administered ONR grants
N00014-01-1-0795 N00014-04-1-0725, AFOSR grant F49620-02-1-0101.

Appendix A. Proofs
Proposition 2.1: ob, (ob, hi ) (ob, h3i ) l(ob, hi )
l(ob, h3i ), = 1, 2, h, ob, ob 0 , (ob, h) (ob 0 , h)
l(ob, h) l(ob 0 , h).
Proof. Let ob arbitrary observation. result follows following argument:
(ob, hi ) (ob, h3i )
iff hi (ob)/(hi (ob) + h3i (ob)) h3i (ob)/(hi (ob) + h3i (ob))
iff hi (ob)hi (ob) h3i (ob)h3i (ob)
iff hi (ob)/h3i (ob) h3i (ob)/hi (ob)
iff l(ob, hi ) l(ob, h3i ).
similar argument establishes result hypotheses.


u

Theorem 2.4: Let H = {h1 , . . . , hm } = {ob 1 , . . . , ob n }, let f real-valued
function domain H f (ob, h) [0, 1]. exists evidence space
E = (H, O, h1 , . . . , hm ) f = f satisfies following properties:
WF1. every ob O, f (ob, ) probability measure H.
P
WF2. exists x1 , . . . , xn > 0 that, h H, ni=1 f (ob , h)xi = 1.
Proof. () Assume f = evidence space E = (H, O, h1 , . . . , hm ).
routine verify WF1, fixed ob O, wEP
(ob, ) probability measure H.
verify WF2, note simply take xi = h0 H h0 (ob ).
() Let f function H [0, 1] satisfies WF1 WF2. Let
x1 , . . . , xnh positive reals guaranteed WF2. straightforward verify
24

fiA Logic Reasoning Evidence

taking h (ob ) = f (ob , h)/xi h H yields evidence space E f =
.
u

following lemmas useful prove completeness axiomatizations
paper. results depend soundness axiomatization AX(h , ).
Lemma A.1: AX(h , ) sound axiomatization logic Lfo -ev (h , ) respect evidential worlds.
Proof. easy see axiom valid evidential worlds.


u

Lemma A.2: hypothesis formulas , h1 hk provable AX(h , ),
[[]] = {h1 , . . . , hk }.
Proof. Using Taut, show provably equivalent formula 0 disjunctive
normal form. Moreover, axiom H2, assume without loss generality
disjuncts 0 consists single hypothesis. Thus, h1 hk . easy
induction structure shows hypothesis formula evidential world w,
w |= iff w |= h h [[]]. Moreover, follows immediately
soundness axiomatization (Lemma A.1) h1 . . . hk provable iff
evidential worlds w, w |= iff w |= hi {1, . . . , k}. Thus, h1 . . . hk
provable iff [[]] = {h1 , . . . , hk }.

u
easy consequence Lemma A.2 1 provably equivalent 2
[[1 ]] = [[2 ]].
Lemma A.3: Let hypothesis formula. formulas
P
Pr(h)
Pr() =
h[[]]

Pr0 () =

P

Pr0 (h)

h[[]]

provable AX(h , ).
Proof. Let h = {h1 , . . . , hnh } = {ob 1 , . . . , ob }. prove result Pr.
proceed induction size [[]]. base case, assume |[[]]| = 0.
Lemma A.2, implies provably equivalent false. Po4, Pr() =
Pr(false), easy check Pr(false) = 0 provable using Po1, Po3, Po4,
thus Pr() = 0, required. |[[]]| = n + 1 > 0, [[]] = {hi1 , . . . , hin+1 },
Lemma A.2, provably equivalent hi1 hin+1 . Po4, Pr() = Pr( hin+1 ) +
Pr( hin+1 ). easy check hin+1 provably equivalent hin+1 (using
H2), similarly hin+1 provably equivalent hi1 hin . Thus, Pr() =
hin ]]| = n, induction
provable. Since |[[hi1P
Pr(hin+1 ) + Pr(hi1 hin ) P
hypothesis, Pr(hi1 hin ) = h{hi ,...,hin } Pr(h) = h[[]]{hi } Pr(h). Thus, Pr() =
1
n+1
P
P
Pr(hin+1 ) + h[[]]{hi } Pr(h), is, Pr() = h[[]] Pr(h), required.
n+1

argument applies mutatis mutandis Pr0 , using axioms Pr14 instead
Po14.
u

25

fiHalpern & Pucella

Theorem 4.1: AX(h , ) sound complete axiomatization logic respect evidential worlds.
Proof. Soundness established Lemma A.1. prove completeness, recall following definitions. formula consistent axiom system AX(h , )
provable AX(h , ). prove completeness, sufficient show
consistent, satisfiable, is, exists evidential world w valuation v
(w, v) |= .
body paper, let h = {h1 , . . . , hnh } = {ob 1 , . . . , ob }. Let
consistent formula. way contradiction, assume unsatisfiable. reduce
formula equivalent formula language real closed fields. Let u1 , . . . , unh ,
v1 , . . . , vno , x1 , . . . , xnh , y1 , . . . , yno , z11 , . . . , zn1 h , . . . , z1no , . . . , znnho new variables, where,
intuitively,
ui gets value 1 hypothesis hi holds, 0 otherwise;
vi gets value 1 observation ob holds, 0 otherwise;
xi represents Pr0 (hi );
yi represents Pr(hi );
zi,j represents w(ob , hj ).
Let v represent list new variables. Consider following formulas. Let h
formula saying exactly one hypothesis holds:
(u1 = 0 u1 = 1) (unh = 0 unh = 1) u1 + + unh = 1.
Similarly, let formula saying exactly one observation holds:
(v1 = 0 v1 = 1) (vno = 0 vnh = 1) v1 + + vnh = 1.
Let pr formula expresses Pr0 probability measure:
pr = x1 0 xnh 0 x1 + + xnh = 1.
Similarly, let po formula expresses Pr probability measure:
po = y1 0 ynh 0 y1 + + ynh = 1.
Finally, need formulas saying w weight evidence function. formula
w ,p simply says w satisfies WF1, is, acts probability measure fixed
observation:
z1,1 0 z1,nh 0 zno ,1 0 zno ,nh 0
z1,1 + + z1,nh = 1 zno ,1 + + zno ,nh = 1.
formula w ,f says w satisfies WF2:
w1 , . . . , wno (w1 > 0 wno > 0 z1,1 w1 + + zno ,1 wno = 1
z1,nh w1 + + zno ,nh wno = 1)
26

fiA Logic Reasoning Evidence

w1 , . . . , wno new variables.
Finally, formula w ,up captures fact weights evidence viewed updating prior probability posterior probability, via Dempsters Rule Combination:
(v1 = 1 (x1 z1,1 = y1 x1 z1,1 + + y1 xnh z1,nh
xnh z1,nh = ynh x1 z1,1 + + ynh xnh z1,nh ))

(vno = 1 (x1 zno ,1 = y1 x1 zno ,1 + + y1 xnh zno ,nh
xnh zno ,nh = ynh x1 zno ,1 + . . . ynh xnh zno ,nh )).
Let formula language real closed fields obtained replacing
occurrence primitive proposition
hi ui = 1, occurrencePof ob vi =
P
1, occurrence Pr0 () hi [[]] xi , occurrence Pr() hi [[]] yi ,
occurrence w(ob , hj ) zi,j , occurrence integer coefficient k 1 + + 1
(k times). Finally, let 0 formula v(h pr po w ,p w ,f w ,up ).
easy see unsatisfiable evidential worlds, 0 false
interpreted real numbers. Therefore, 0 must formula valid real closed
fields, hence instance RCF. Thus, 0 provable. straightforward show,
using Lemma A.3, provable, contradicting fact consistent.
Thus, must satisfiable, establishing completeness.

u
mentioned beginning Section 5, Lfo -ev monotone respect
validity: axiom H1 depends set hypotheses observations, general
longer valid set changed. true O1, E3, E4. do,
however, form monotonicity respect satisfiability, following lemma
shows.
Lemma A.4: Given h , let formula Lfo -ev (h , ), let H h
hypotheses observations occur . satisfiable
evidential world h , satisfiable evidential world 0h
0o , |0h | = |H| + 1 |0o | = |O| + 1.
Proof. two steps, clarify presentation. First, show
add single hypothesis observation h preserve satisfiability .
means second step assume h 6= H 6= O. Assume
satisfied evidential world w = (h, ob, , E) h , exists
v (w, v) |= . Let 0h = h {h }, h new hypothesis h ,
let 0o = {ob }, ob new observation . Define evidential
world w0 = (h, ob, 0 , E 0 ) 0h 0o , E 0 0 defined follows. Define
probability measure 0 taking:
(
(h) h h
0
(h) =
0
h = h .

27

fiHalpern & Pucella

Similarly, define evidence space E 0 = (0h , 0o , 0 ) derived E = (h , , ) taking:


h (ob) h h ob



0
h h ob = ob
0h (ob) =

0
h = h ob



1
h = h ob ob .
Thus, 0h extends existing h assigning probability 0 new observation ob ;
contrast, new probability 0h assigns probability 1 new observation ob .
check (w0 , v) |= .
second step collapse hypotheses observations appear
one hypotheses appear H O, previous step
guaranteed exist. previous step, assume h 6= H 6= O.
Assume satisfiable evidential world w = (h, ob, , E) h , is,
exists v (w, v) |= . Pick hypothesis observation h
follows, depending hypothesis h observation ob w. Let h h h 6 H,
otherwise, let h arbitrary element h H; let 0h = H {h }. Similarly, let ob
ob ob 6 O, otherwise, let ob arbitrary element O; let 0o = {ob }.
Let w0 = (h, ob, 0 , E 0 ) evidential world 0h 0o obtained w follows.
Define probability measure 0 taking:
(
(h)
h H
0 (h) = P
0

h0 h H (h ) h = h .
Define E 0 = (0h , 0o , 0 ) derived E = (h , , ) taking:


h H ob

h (ob)


P 0
0
h (ob )
h H ob = ob
0h (ob) = Pob

h = h ob

h0 h H h0 (ob)

P

P
0


h0 h H
ob 0 h0 (ob ) h = h ob = ob .
check induction (w0 , v) |= .


u

Theorem 5.1: procedure runs space exponential || kk deciding,
given h , whether formula Lfo -ev (h , ) satisfiable evidential world.
Proof. Let formula Lfo -ev (h , ). Lemma A.4, satisfiable
construct probability measure 0h = H {h } (where H set hypotheses
appearing , h 6 H) probability measures h1 , . . . , hm 0o = {ob }
(where set observations appearing ob 6 O) E = (0h , 0o , ),
w = (h, ob, , E) (w, v) |= h, ob, v.
aim derive formula 0 language real closed fields asserts
existence probability measures. precisely, adapt construction
formula 0 proof Theorem 4.1. one change need make
ensure 0 polynomial size , construction proof
28

fiA Logic Reasoning Evidence

Theorem 4.1 guarantee. culprit fact encode integer constants k
1+ +1. straightforward modify construction use efficient
representation integer constants, namely, binary representation. example,
write 42 2(1 + 22 (1 + 22 )), expressed language real closed fields
(1 + 1)(1 + (1 + 1)(1 + 1)(1 + (1 + 1)(1 + 1))). check k coefficient length
k (when written binary), written term length O(k) language
real closed fields. Thus, modify construction 0 proof Theorem 4.1
integer constants k represented using binary encoding. easy see
|0 | polynomial || kk (since |0h | |0o | polynomial ||).
use exponential-space algorithm Ben-Or, Kozen, Reif (1986) 0 : 0
satisfiable, construct required probability measures, satisfiable;
otherwise, probability measures exist, unsatisfiable.

u
Theorem 5.2: procedure runs space exponential || kk deciding
whether exist sets primitive propositions h Lfo -ev (h , )
satisfiable evidential world.
Proof. Let h1 , . . . , hm hypotheses appearing , ob 1 , . . . , ob n hypotheses
appearing . Let h = {h1 , . . . , hm , h } = {ob 1 , . . . , ob n , ob }, h ob
hypothesis observation appearing . Clearly, |h | |o | polynomial
||. Lemma A.4, satisfiable evidential world, satisfiable evidential
world h . Theorem 5.1, algorithm determine satisfied
evidential world h runs space exponential || kk.

u
Theorem 5.3: procedure runs space polynomial || kk deciding,
given h , whether formula Lev (h , ) satisfiable evidential world.
Proof. proof result similar Theorem 5.1. Let formula
Lev (h , ). Lemma A.4, satisfiable exists probability measure
0h = H {h } (where H set hypotheses appearing , h 6 H), probability
measures h1 , . . . , hm 0o = {ob } (where set observations appearing
ob 6 O), hypothesis h, observation o, valuation v (w, v) |= ,
w = (h, ob, , E) E = (0h , 0o , ).
derive formula 0 language real closed fields asserts existence
probability measures adapting construction formula 0
proof Theorem 4.1. proof Theorem 5.1, need make sure 0
polynomial size , construction proof Theorem 4.1
guarantee. modify construction use efficient representation
integer constants, namely, binary representation. example, write 42
2(1 + 22 (1 + 22 )), expressed language real closed fields (1 + 1)(1 +
(1 + 1)(1 + 1)(1 + (1 + 1)(1 + 1))). check k coefficient length k
(when written binary), written term length O(k) language
real closed fields. modify construction 0 proof Theorem 4.1
integer constants k represented using binary encoding. easy see |0 |
polynomial || kk (since |0h | |0o | polynomial ||). key
notice resulting formula 0 written x1 . . . xn (00 ) quantifierfree formula 00 . form, apply polynomial space algorithm Canny (1988)
29

fiHalpern & Pucella

00 : 00 satisfiable, construct required probability measures,
satisfiable; otherwise, probability measures exist, unsatisfiable.
u

Theorem 5.4: procedure runs space polynomial || kk deciding
whether exists sets primitive propositions h Lev (h , )
satisfiable evidential world.
Proof. Let h1 , . . . , hm hypotheses appearing , ob 1 , . . . , ob n hypotheses
appearing . Let h = {h1 , . . . , hm , h } = {ob 1 , . . . , ob n , ob }, h ob
hypothesis observation appearing . Clearly, |h | |o | polynomial
||. Lemma A.4, satisfiable evidential world, satisfiable evidential
world h . Theorem 5.3, algorithm determine satisfied
evidential world h runs space polynomial || kk.

u
proofs Theorem 6.1 6.2 rely following small model result, variation
Lemma A.4.
Lemma A.5: Given h , let formula Lfo -ev (h , ), let H h
hypotheses observations occur . satisfiable
evidential world h , satisfiable evidential world 0h
0o |0h | = |H| + 1 |0o | = |O| + 1, where, h 0h ob 0o ,
likelihood h (ob) rational number size O(|| kk + || log(||)).
Proof. Let formula satisfiable evidential world h . Lemma A.4,
satisfiable evidential world 0h 0o , |0h | = |H|+1 |0o | = |O|+1.
force likelihoods small, adapt Theorem 2.6 FHM, says
formula f FHM logic satisfiable, satisfiable structure
probability assigned state structure rational number size O(|f | kf k+
|f | log(|f |)). formulas Lw (0h , 0o ) formulas FHM logic. result
adapts immediately, yields required bounds size likelihoods.

u
Theorem 6.1: problem deciding, given h , whether formula Lw (h , )
satisfiable evidential world NP-complete.
Proof. establish lower bound, observe reduce propositional satisfiability
satisfiability Lw (h , ). precisely, let f propositional formula,
p1 , . . . , pn primitive propositions appearing f . Let = {ob 1 , . . . , ob n , ob }
set observations, observation ob corresponds primitive proposition pi ,
ob another (distinct) observation; let h arbitrary set hypotheses, let
h arbitrary hypothesis h . Consider formula f obtained replacing every
occurrence pi f w(ob , h) > 0. straightforward verify f satisfiable
f satisfiable Lw (h , ). (We need extra observation ob take
care case f satisfiable model p1 , . . . , pn false. case,
w(ob 1 , h) = w(ob n , h) = 0, take w(ob , h) = 1.) establishes lower
bound,
upper bound straightforward. Lemma A.5, evidential world h
guessed time polynomial |h | + |o | + || kk, since prior probability
world requires assigning value |h | hypotheses, evidence space requires
30

fiA Logic Reasoning Evidence

|h | likelihood functions, assigning value |o | observations, size polynomial
|| kk. verify world satisfies time polynomial || kk + |h | + |h |.
establishes problem NP.

u
Theorem 6.2: problem deciding, formula , whether exists sets
primitive propositions h Lw (h , ) satisfiable
evidential world NP-complete.
Proof. lower bound, reduce decision problem Lw (h , ) fixed
h . Let h = {h1 , . . . , hm } = {ob 1 , . . . , ob n }, let formula
Lw (h , ). check satisfiable evidential world h
(h1 hm ) (ob 1 ob n ) satisfiable evidential world arbitrary
0h 0o . Thus, Theorem 6.1, get lower bound.
upper bound, Lemma A.5, satisfiable, satisfiable evidential
world h , h = H {h }, H consists hypotheses appearing ,
= {ob }, consists observations appearing , h ob new
hypotheses observations. Thus, |h | || + 1, |o | || + 1. proof
Theorem 6.1, world guessed time polynomial || kk + |h | + |o |,
therefore time polynomial || kk. verify world satisfies time
polynomial || kk, establishing problem NP.

u
-ev
Theorem 7.1: AXdyn (h , ) sound complete axiomatization Lfo
dyn (h , )
respect evidential runs.
Proof. easy see axiom valid evidential runs. prove completeness,
follow procedure proof Theorem 4.1, showing consistent,
satisfiable, is, exists evidential run r valuation v
(r, m, v) |= point (r, m) r.
body paper, let h = {h1 , . . . , hnh } = {ob 1 , . . . , ob }. Let
consistent formula. first step process reduce formula canonical
form respect operator. Intuitively, push every occurrence
polynomial inequality formulas present formula. easy see axioms
inference rules T1T6 used establish provably equivalent formula
0 every occurrence form subformulas n (ob) n (p c),
p polynomial term contains least one occurrence Pr operator. use
notation n . . . , n-fold application . write 0 . Let N
maximum coefficient 0 .
way contradiction, assume 0 (and hence ) unsatisfiable. proof
Theorem 4.1, reduce formula 0 equivalent formula language real
closed fields. Let u1 , . . . , unh , v10 , . . . , vn0 , . . . , v1N , . . . , vnNo , y10 , . . . , yn0 , . . . , y1N , . . . , ynNo ,
zhi1 ,...,ik i,1 , . . . , zhi1 ,...,ik i,nh (for every sequence hi1 , . . . , ik i) new variables, where, intuitively,
ui gets value 1 hypothesis hi holds, 0 otherwise;
vin gets value 1 observation ob holds time n, 0 otherwise;
yin represents Pr(hi ) time n;
31

fiHalpern & Pucella

zhi1 ,...,ik i,j represents w(hob i1 , . . . , ob ik i, hj ).
main difference construction proof Theorem 4.1 variables vin representing observations every time step n, rather variables representing observations time step, variables yin representing hypothesis probability
every time step, rather variables representing prior posterior probabilities,
variables zhi1 ,...,ik i,j representing weight evidence sequences observations, rather
variables representing weight evidence single observations. Let v represent
list new variables. consider formulas proof Theorem 4.1,
modified account new variables, fact reasoning multiple
time steps. specifically, formula h unchanged. Instead , consider formulas 1o , . . . , N
saying exactly one observation holds time time step,
given by:
(v1n = 0 v1n = 1) (vnno = 0 vnnh = 1) v1n + + vnnh = 1.
Let 0o = 1o N
.
Similarly, instead pr po , consider formulas 1p , . . . , N
p expressing Pr
probability measure time step, np given by:
y1n 0 ynnh 0 y1n + + ynnh = 1.
Let p = 1p N
p .
Similarly, consider w ,p w ,f , except replace variables zi,j zhii,j ,
reflect fact consider sequences observations. formula w ,up , capturing
update prior probability posterior probability given E5, replaced
formulas 1w ,up , . . . , N
w ,up representing update probability time step,
n
w ,up given obvious generalization w ,up :
z1,nh
(v1n = 1 (y1n1 z1,1 = y1n y1n1 z1,1 + + y1n ynn1
h
n1
n1
n
z1,nh ))
ynh z1,nh = ynh y1 z1,1 + + ynnh ynn1
h

zno ,nh
(vnno = 1 (y1n1 zno ,1 = y1n y1n1 zno ,1 + + y1n ynn1
h
n n1 z
n n1 z
ynn1
z
+
.
.
.

=

,nh
,1
,nh )).
nh 1
nh nh
h
Let 0w ,up = 1w ,up N
w ,up .
Finally, need new formula w ,c capturing relationship weight
evidence sequence observations, weight evidence individual
observations, capture axiom E6:
^
zhi1 i,h1 zhik i,h1 = zhi1 ,...,ik i,h1 zhi1 i,h1 zhik i,h1
1kN
+ + zhi1 ,...,ik i,h1 zhi1 i,hnh zhik i,hnh
1i1 ,...,ik
^

zhi1 i,hnh zhik i,hnh = zhi1 ,...,ik i,hnh zhi1 i,h1 zhik i,h1
1kN
+ + zhi1 ,...,ik i,hnh zhi1 i,hnh zhik i,hnh .
1i1 ,...,ik

32

fiA Logic Reasoning Evidence

Let formula language real closed fields obtained replacing
occurrence primitive proposition hi ui = 1, occurrence n ob
vin = 1, within
polynomial inequality formula n (p c), replacing occurrence
P
Pr() hi [[]] yin , occurrence w(hob i1 , . . . , ob ik i, hj ) zhi1 ,...,ik i,j ,
occurrence integer coefficient k 1 + + 1 (k times). Finally, let 0 formula
v(h 0o p w ,p w ,f 0w ,up w ,c ).
easy see unsatisfiable evidential systems, 0 false
real numbers. Therefore, 0 must formula valid real closed fields, hence
instance RCF. Thus, 0 provable. straightforward show, using obvious
variant Lemma A.3 provable, contradicting fact consistent.
Thus, must satisfiable, establishing completeness.

u

References
Ben-Or, M., Kozen, D., & Reif, J. H. (1986). complexity elementary algebra
geometry. Journal Computer System Sciences, 32 (1), 251264.
Canny, J. F. (1988). algebraic geometric computations PSPACE. Proc. 20th
Annual ACM Symposium Theory Computing (STOC88), pp. 460467.
Carnap, R. (1962). Logical Foundations Probability (Second edition). University
Chicago Press.
Casella, G., & Berger, R. L. (2001). Statistical Inference (Second edition). Duxbury.
Chan, H., & Darwiche, A. (2005). revision probabilistic beliefs using uncertain
evidence. Artificial Intelligence, 163, 6790.
de Alfaro, L. (1998). Formal Verification Probabilistic Systems. Ph.D. thesis, Stanford
University. Available Technical Report STAN-CS-TR-98-1601.
Enderton, H. B. (1972). Mathematical Introduction Logic. Academic Press.
Fagin, R., & Halpern, J. Y. (1994). Reasoning knowledge probability. Journal
ACM, 41 (2), 340367.
Fagin, R., Halpern, J. Y., & Megiddo, N. (1990). logic reasoning probabilities.
Information Computation, 87 (1/2), 78128.
Fischer, M. J., & Zuck, L. D. (1988). Reasoning uncertainty fault-tolerant distributed systems. Technical report YALEU/DCS/TR643, Yale University.
Good, I. J. (1950). Probability Weighing Evidence. Charles Griffin & Co. Ltd.
Good, I. J. (1960). Weights evidence, corroboration, explanatory power, information
utility experiments. Journal Royal Statistical Society, Series B, 22,
319331.
Halpern, J. Y., & Fagin, R. (1992). Two views belief: belief generalized probability
belief evidence. Artificial Intelligence, 54, 275317.
Halpern, J. Y., Moses, Y., & Tuttle, M. R. (1988). knowledge-based analysis zero
knowledge. Proc. 20th Annual ACM Symposium Theory Computing
(STOC88), pp. 132147.
33

fiHalpern & Pucella

Halpern, J. Y., & Pucella, R. (2003). logic reasoning evidence. Proc. 19th
Conference Uncertainty Artificial Intelligence (UAI03), pp. 297304.
Halpern, J. Y., & Pucella, R. (2005a). Evidence uncertain likelihoods. Proc. 21th
Conference Uncertainty Artificial Intelligence (UAI05), pp. 243250.
Halpern, J. Y., & Pucella, R. (2005b). Probabilistic algorithmic knowledge. Logical Methods
Computer Science, 1 (3:1).
Halpern, J. Y., & Tuttle, M. R. (1993). Knowledge, probability, adversaries. Journal
ACM, 40 (4), 917962.
He, J., Seidel, K., & McIver, A. (1997). Probabilistic models guarded command
language. Science Computer Programming, 28 (23), 171192.
Jeffrey, R. C. (1992). Probability Art Judgement. Cambridge University Press.
Kyburg, Jr., H. E. (1983). Recent work inductive logic. Machan, T., & Lucey, K.
(Eds.), Recent Work Philosophy, pp. 87150. Rowman & Allanheld.
Milne, P. (1996). log[p(h|eb)/p(h|b)] one true measure confirmation. Philosophy
Science, 63, 2126.
Pearl, J. (1988). Probabilistic Reasoning Intelligent Systems: Networks Plausible Inference. Morgan Kaufmann.
Popper, K. R. (1959). Logic Scientific Discovery. Hutchinson.
Renegar, J. (1992). computational complexity geometry first order theory
reals. Journal Symbolic Computation, 13 (3), 255352.
Shafer, G. (1976). Mathematical Theory Evidence. Princeton University Press.
Shafer, G. (1982). Belief functions parametric models (with commentary). Journal
Royal Statistical Society, Series B, 44, 322352.
Shoenfield, J. R. (1967). Mathematical Logic. Addison-Wesley, Reading, Mass.
Tarski, A. (1951). Decision Method Elementary Algebra Geometry (2nd edition).
Univ. California Press.
Vardi, M. Y. (1985). Automatic verification probabilistic concurrent finite-state programs.
Proc. 26th IEEE Symposium Foundations Computer Science (FOCS85),
pp. 327338.
Walley, P. (1987). Belief function representations statistical evidence. Annals Statistics,
18 (4), 14391465.

34

fiJournal Artificial Intelligence Research 26 (2006) 323-369

Submitted 10/05; published 08/06

Temporal Planning using Subgoal Partitioning
Resolution SGPlan
Yixin Chen

chen@cse.wustl.edu

Department Computer Science Engineering
Washington University St Louis
St Louis, MO 63130 USA

Benjamin W. Wah
Chih-Wei Hsu

wah@manip.crhc.uiuc.edu
chsu@manip.crhc.uiuc.edu

Department Electrical Computer Engineering
Coordinated Science Laboratory
University Illinois Urbana-Champaign
Urbana, IL 61801 USA

Abstract
paper, present partitioning mutual-exclusion (mutex) constraints
temporal planning problems implementation SGPlan4 planner. Based
strong locality mutex constraints observed many benchmarks Fourth International Planning Competition (IPC4), propose partition constraints
planning problem groups based subgoals. Constraint partitioning leads
significantly easier subproblems similar original problem
efficiently solved planner modifications objective function. present partition-and-resolve strategy looks locally optimal subplans
constraint-partitioned temporal planning subproblems resolves inconsistent
global constraints across subproblems. also discuss implementation details
SGPlan4 , include resolution violated global constraints, techniques handling producible resources, landmark analysis, path finding optimization, search-space
reduction, modifications Metric-FF used basic planner SGPlan4 . Last,
show results sensitivity techniques quality-time trade-offs
experimentally demonstrate SGPlan4 effective solving IPC3 IPC4
benchmarks.

1. Introduction
paper, present innovative partition-and-resolve strategy implementation SGPlan4 solving temporal planning problems PDDL2.2. strategy partitions mutual-exclusion (mutex) constraints temporal planning problem
subgoals subproblems, solves subproblems individually using modified Metric-FF
planner, resolves violated global constraints iteratively across subproblems.
evaluate various heuristics resolving global constraints demonstrate performance SGPlan4 solving benchmarks Third (IPC3) Fourth (IPC4)
International Planning Competitions.
general popular methods solving large planning problems, systematic search, heuristic search, transformation methods, viewed recursive
c
2006
AI Access Foundation. rights reserved.

fiChen, Wah, & Hsu

P:
Variable
partitioning
ordered
heuristic
functions



B

C

SP = SA SB SC

Variables:

a) Search-space partitioning

a1

11111111111111111111111111
00000000000000000000000000
00000000000000000000000000
11111111111111111111111111
00000000000000000000000000
11111111111111111111111111
00000000000000000000000000
11111111111111111111111111
00000000000000000000000000
11111111111111111111111111
00000000000000000000000000
11111111111111111111111111
00000000000000000000000000
11111111111111111111111111
00000000000000000000000
11111111111111111111111
00000000000000000000000
11111111111111111111111
00000000000000000000000
11111111111111111111111
00000000000000000000000
11111111111111111111111
00000000000000000000000
11111111111111111111111
00000000000000000000000
11111111111111111111111
a2

a3

b) Complete heuristic searches

Figure 1: Search-space partitioning branches variable assignments order decompose P disjunction () subproblems disjoint search spaces.
complexity subproblem similar P .

partitioning search space independent subproblems iterative evaluation
subproblems feasible solution found. level application
approach, problem subproblem decomposed partitioning variable space
disjunction () subspaces (Figure 1a). reduce search complexity, approach
often combined intelligent backtracking employs variable/value ordering order
subproblems generated, pre-filters partial inconsistent assignments eliminate
infeasible subproblems, prunes subproblems using bounds computed relaxation
approximation.
Search-space partitioning directly applied planning problem transformed version problem. Direct methods include complete heuristic searches.
illustrated Figure 1b, methods partition search space recursively branching
assigned variables (selection actions). difference complete search
heuristic search former enumerates subspaces systematically, whereas
latter prioritizes subspaces heuristic function evaluates selectively. Examples complete planners include UCPOP (Penberethy & Weld, 1992), Graphplan (Blum
& Furst, 1997), STAN (Long & Fox, 1998), PropPLAN (Fourman, 2000), System R (Lin,
2001), SIPE-2 (Wilkins, 1990), O-Plan2 (Tate, Drabble, & Kirby, 1994), ZENO (Penberethy
& Weld, 1994), TALplanner (Doherty & Kvarnstrm, 1999), SHOP2 (Nau, Muoz-Avila,
Cao, Lotem, & Mitchell, 2001); examples heuristic planners include HSP (Bonet &
Geffner, 2001), FF (Hoffmann & Nebel, 2001), AltAlt (Nigenda, Nguyen, & Kambhampati, 2000), GRT (Refanidis & Vlahavas, 2001), MO-GRT (Refanidis & Vlahavas, 2002),
ASPEN (Chien, Rabideau, Knight, Sherwood, Engelhardt, Mutz, Estlin, Smith, Fisher,
Barrett, Stebbins, & Tran, 2000), Metric-FF (Hoffmann & Nebel, 2001), GRT-R (Refanidis
& Vlahavas, 2001), LPG (Gerevini & Serina, 2002), MIPS (Edelkamp, 2002), Sapa (Subbarao & Kambhampati, 2002), Europa (Jonsson, Morris, Muscettola, & Rajan, 2000).
contrast, transformation approach, problem first transformed satisfiability
optimization problem, transformed problem solved SAT integer
programming solver employs search-space partitioning. Notable planners using
324

fiTemporal Planning using Subgoal Partitioning Resolution

10000

P:
Run time

1000

G


B

C

100
10
1
0.1

AIRPORT-NT-20
PIPESWORLD-NT-NT-50

0.01

SP = SA SB SC SG

1

a) Constraint partitioning

2
3
4
5
Number Subgoals Subproblem

b) Exponential behavior solution time

Figure 2: Constraint partitioning decomposes P conjunction () subproblems
disjoint constraints possibly overlapping search spaces, set
global constraints (G) resolved. Since complexity subproblem
substantially smaller P , leads exponential decrease solution
time Metric-FF two IPC4 benchmarks (AIRPORT-NONTEMP-20
PIPESWORLD-NOTANKAGE-NONTEMP-50) number subgoals
subproblem decreased 5 1.

approach include SATPLAN (Kautz & Selman, 1996), Blackbox (Kautz & Selman, 1999),
ILP-PLAN (Kautz & Walser, 2000), LPSAT (Wolfman & Weld, 2000).
One limitations search-space partitioning complexity problem
dramatically reduced partitioning. Although pruning ordering strategies make search efficient requiring search every subspace,
aggregate complexity finding solution one subproblems
original problem.
paper, study constraint-partitioning approach decomposes constraints planning problem conjunction () subproblems disjoint constraints possibly overlapping search spaces (Figure 2a). concept constraints
planning problems studied paper precisely defined Section 2.1. Informally,
(mutex) constraint refers condition two actions overlap
execution. Since constraints must satisfied, subproblems
must solved order solve original problem.
decomposing constraints problem subproblems solving
independently, subproblem require significantly less time solve much
relaxed original problem. illustration, Figure 2b shows exponential
decrease solution time number subgoals subproblem reduced linearly.
Here, subgoal collection conjuncts conjunctive top-level goal problem.
IPC4 instances evaluated, run time 1500 seconds
five subgoals subproblem less one second one. Hence,
aggregate complexity solving decomposed subproblems exponentially smaller
original problem.
325

fiChen, Wah, & Hsu

S0

P1

11
00
00
11

11
00
00
11
11
00
00
11
11
00
00
11

111
000
000
111
000
111
000
111
11
00
S1
00 000
11
111
000
111
000
111
000
111
000
111
000
111
000
111
000
111

11
00
00
11

P2

111
000
000
111

11
00
00
11

11
00
00
11

111
000
000
111
000
111
000
111
000
111
000
111
000
111
000
111

S2
11
00
000
111
11 111
00
000
000
111
000
111

11
00
00
11

P3

11
00
00
11

S3

11
00
11
00

11
00
00
11
11
00
00
11

11
00
00
11

11
00
11
00

Figure 3: Partitioning constraints planning problem along temporal horizon
three stages requires finding suitable intermediate states S1 S2 order
connect subplans three stages together. S0 S3 are, respectively,
initial final states.

Constraint partitioning, however, leads global constraints across subproblems (SG
Figure 2a) need resolved. global constraints include span
across common variables multiple subproblems, relate two actions
different subproblems. Since constraints may satisfied solving
subproblems independently, subproblems may need solved multiple times order
resolve violated global constraints.
general, violated global constraints across subproblems cannot efficiently resolved
brute-force enumeration search space global constraints defined
Cartesian product search spaces across subproblems exponentially
large. Dynamic programming cannot applied global constraints may span across
multiple subproblems. means partial feasible plan dominates another
partial feasible plan one subproblem fail execute dominating plan violates
global constraint another subproblem.
address resolution violated global constraints, summarize Section 3
theory extended saddle points developed previous work (Wah & Chen, 2006).
choosing suitable neighborhood, theory allows mixed-integer nonlinear programming problem (MINLP) partitioned subproblems related necessary
condition global constraints. Further, necessary condition subproblem
significantly prunes Cartesian product search spaces across subproblems
inconsistent global constraints resolved.
addition efficient resolution violated global constraints, success
approach depends strong locality constraints respect actions
relate. observed informally previous work strong locality constraints. Based strong locality, studied two alternatives partitioning
constraints: partitioning time (Wah & Chen, 2006; Chen & Wah, 2003)
partitioning subgoals (Wah & Chen, 2004, 2003).
idea partitioning planning problem time partition constraints
temporal bindings stages. find overall feasible plan, planner need
find subplan initial final states stage satisfy local
well global constraints, final state one stage initial state
326

fiTemporal Planning using Subgoal Partitioning Resolution

next stage. example, partitioning horizon three stages (Figure 3),
planner assigns values intermediate states S1 S2 , solves subproblem
individually, perturbs S1 S2 look another solution feasible subplans cannot
found stages.
major drawback partitioning planning problem temporal horizon
constraint resolutions may sequentially propagate multiple stages.
found partitioning constraints PDDL2.1 benchmarks along temporal
horizon often leads many global constraints relate states adjacent stages.
result, violated subgoal caused incorrect assignment states
early stage horizon, resolution incorrect assignment propagate
sequentially stages. Oftentimes, propagation information may cause
search get stuck infeasible point extended period time (Wah & Chen,
2004). end, expensive enumeration final state stage (S1 S2
Figure 3) may needed order resolve inconsistencies.
second approach studied previous work partition constraints
planning problem subgoals (Wah & Chen, 2004, 2003). evaluating
subproblems, inconsistent global constraints among first identified,
subproblems re-evaluated global constraints satisfied. Partitioning
subgoals eliminates need selecting final state subproblem
initial final states subgoal known. Using approach, previous
work shown improvements time quality MIPS planner solving
IPC3 benchmarks.
respect second approach, made four main contributions
paper.
First, quantitatively evaluate Section 2.2 locality constraints IPC4
benchmarks well benchmarks Blocksworld domain Depots domain.
results show constraint partitioning subgoals consistently leads lower
fraction initial active global constraints constraint partitioning time. results
also explain constraint partitioning work well domains,
Blocksworld Depots.
Second, incorporate Metric-FF (Hoffmann, 2003) basic planner SGPlan4
SGPlan4.1 , instead MIPS previous work (Wah & Chen, 2004). change
non-trivial requires significant extensions Metric-FF order handle
new features PDDL2.2 beyond PDDL2.1. extensions include support
temporal planning, handling derived predicates timed initial literals,
handling wrappers timed initial literals (Section 5.3).
Third, describe new techniques improving search efficiency global-
local-level architectures partition-and-resolve approach (Section 4.1). include
handling producible resources (Section 4.3), subgoal-level decomposition using landmark analysis, path finding path optimization (Section 5.1), subgoal-level planning
using search-space reduction (Section 5.2). explain integration planners
analyze effectiveness.
Last, study Section 4.2 trade-offs solution time quality heuristics updating penalties violated global constraints. trade-offs allow us
generate plans either better quality time (SGPlan4.1 ), lower quality
327

fiChen, Wah, & Hsu

less time (SGPlan4 ). optimization quality requires estimation makespan
multiple subplans enhanced PERT algorithm (Section 5.3). previous work
constraint partitioning subgoals (Wah & Chen, 2004), focused minimizing planning time. Without optimizing quality, violated global constraints often
easier resolve planner always delay one actions order avoid
constraints. Finally, compare Section 7 performance planners
respect planners.

2. Locality Mutex Constraints Temporal Planning
section, define mutex constraints planning problems. Based structure
constraints IPC4 benchmarks, show constraint partitioning subgoals
leads constraints localized better constraint partitioning time.
2.1 Representation Mutex Constraints
following standard notations definitions literature (Hoffmann & Nebel, 2001;
Garrido, Fox, & Long, 2002), summarize section basic definitions mutex
constraints used paper.
Definition 1. planning problem = (O, F, I, G) quadruple, set
possible actions , F set facts, set initial facts, G set
goal facts.


Definition 2. state = f1 , , fnS subset facts F true.
Definition 3. STRIPS action associated following attributes:
a) pre(a), set facts define preconditions action a;
b) add(a), set facts define add effects a;
c) del(a), set facts define delete effects a.
resulting state applying action state defined as:
(
(S add(a))\del(a) pre(a)
Result(S, a) =

pre(a) 6 S.

(1)

resulting state applying sequence actions a1 , , recursively defined
as:
Result(S, (a1 , , )) = Result(Result(S, (a1 , , an1 )), ).

(2)

Next, extend action model temporal planning. durative actions supported
PDDL2.2, precondition fact effective beginning, end,
entire duration action; whereas add effect delete effect effective
beginning end action.
Definition 4. temporal action associated following attributes:
a) s(a) e(a) define, respectively, start time end time a.
328

fiTemporal Planning using Subgoal Partitioning Resolution

action
active mutexes
pre(a1 )

add(a4 )

a1

pre(a5 )

a4

a5

a2
del(a2 )

add(a2 )

a7
a6

a3
del(a3 )

pre(a6 )

del(a7 )

time

Figure 4: example temporal plan, active mutexes actions shown
dashed lines, inactive mutexes dotted lines.

b) preconditions divided three types: prestart (a), set initial
preconditions held s(a); preend (a), set final preconditions held e(a);
preoverall (a), set invariant preconditions open interval (s(a), e(a)).
c) two types add effects: addstart (a), set initial add effects
asserted s(a); addend (a), set final add effects asserted e(a).
d) two type delete effects: delstart (a), set initial delete effects
asserted s(a); delend (a), set final delete effects asserted e(a).
Definition 5. temporal plan P = {a1 , a2 , , } list temporal actions,
ai assigned start time s(ai ) end time e(ai ).
Figure 4 illustrates temporal plan seven actions. action, indicate,
appropriate, preconditions, add effects, delete effects.
Concurrent actions plan must arranged way observes mutual
exclusions (mutexes). notion mutex first proposed GraphPlan (Blum & Furst,
1997). defined planning graph, level-by-level constraint graph
alternates fact level action level. Mutex relationships planning graph
classified transient (level-dependent) persistent (level-independent) (Blum
& Furst, 1997). mutex transient exists certain levels graph
vanishes levels graph built. contrast, mutex persistent holds
every level fix-point level (the last level graph) achieved. paper,
consider level-independent, persistent mutex relationships, transient mutexes
exclusively used searches GraphPlan.
Actions b marked persistently mutual exclusive one following
occurs.
329

fiChen, Wah, & Hsu

a) Actions b persistent competing needs,1 competing needs
represented persistent mutex preconditions b;
b) persistent inconsistent effects, one action deletes add effect
other.
c) persistent interference, one action deletes precondition other.
Two facts p q persistently mutual exclusive possible ways making p true
persistently exclusive possible ways making q true; is, action
p add effect (p add(a)) persistently mutual exclusive action
b q add effect (q add(b)). simplicity, rest paper, mutex
actions facts refer corresponding persistent mutex actions facts.
Given temporal plan, mutex relationship active inactive. example,
actions a1 a2 Figure 4 active mutex two actions overlap
execution persistent interference. However, a2 a3 inactive mutex
overlap execution.
Based discussion, conditions active mutex occur two
actions b summarized four cases (Garrido et al., 2002):
a) Actions b start together, nonempty intersection
initial preconditions (resp. add effects) initial delete effects (resp. delete effects).
b) Actions b end together, nonempty intersection
final preconditions (resp. add effects) final delete effects (resp. delete effects).
c) Action ends b starts, nonempty intersection final
delete effects (resp. delete effects, add effects, preconditions) initial add
effects (resp. preconditions, delete effects, delete effects) b.
d) Action starts (resp. ends) execution b, nonempty intersection initial (resp. final) delete effects invariant preconditions
b.
conditions introduced prevent two mutually exclusive actions
executing simultaneously, may actions block propagation facts
(no-op action) cause unsupported actions later. condition detected
looking actions delete existing facts current plan. respect
conditions mutex due competing needs, need represent explicitly
mutexes due competing needs must accompany two types mutex:
two preconditions mutually exclusive due competing needs, two action
sequences making true also mutually exclusive. example, active mutex
a5 a6 Figure 4 due competing needs caused active mutex
a3 a4 .
mutex constraints studied paper closed form. Instead,
defined discrete procedural function checks pair actions meet one four
conditions above. inputs function start time end time
action, continuous temporal problems discrete propositional problems.
1. terms competing needs, inconsistent effects, interference originally proposed
GraphPlan (Blum & Furst, 1997).

330

fiTemporal Planning using Subgoal Partitioning Resolution

11
00
00
11

11
00
00
11
11
00
11
00

11
00
00
11

11
00
00
11

00
11
00
11
11111111111
00000000000
00
11
00000000000
11111111111
00
11
00000000000
11111111111
00000000000
11111111111
000
111
00000000000
11111111111
000
111

11
00
00
11

Subproblem 2

11
00
00
11

11
00
00
11

Subproblem 1

00
11
00
11
11111111111
00000000000
00
11
00000000000
11111111111
00
11
00000000000
11111111111
00000000000
11111111111
00
11
00000000000
11111111111
00
11

11
00
00
11

11
00
00
11

11
00
00
11

11
00
11
00

11
00
11
00

11
00
00
11

11
00
00
11

11
00
11
00

11
00
11
00
11
00
00
11

11
00
11
00

11
00
00
11
11
00
00
11

11
00
00
11

11
00
00
11

11
00
00
11

Subproblem 3

11
00
00
11

11
00
00
11

11
00
11
00

11
00
11
00
11
00
00
11

11
00
00
11
11
00
00
11

11
00
00
11

a) 63 mutex constraints among actions b) Partitioning mutex constraints subgoals
Figure 5: Mutex constraints IPC4 AIRPORT-TEMP-4 instance. rectangular
box represents action, line joining two actions represents mutex constraint (that may inactive). constraints (52 63 83%) local
constraints partitioning subgoals. Global mutex constraints
shown dashed lines (b).

2.2 Locality Mutex Constraints
section, evaluate partitioning mutex constraints planning benchmarks. analysis shows strong locality constraints partitioned
subgoals compared case partitioned time. study
criteria partitioning may lead subproblems whose initial final states specified. subproblems hard solve existing planners
may require systematic enumeration initial final states
finding feasible plans.
Figure 5a shows 63 mutex constraints solution plan fourth instance
IPC4 AIRPORT-TEMP domain. instance involves moving three planes airport
designated gates. rectangular box figure represents action, whereas line
joining two actions represents mutex constraint (that may inactive). Figure 5b shows
partitioning constraints three subproblems, involving movement
one plane. show local constraints (those relevant actions one subproblem) solid lines global constraints relating actions different subproblems
dashed lines. clear majority (83%) constraints local partitioning
subgoals.
demonstrate localization mutex constraints partitioned subgoals,
analyze IPC4 instances. first modify original Metric-FF planner (Hoffmann,
2003) order support new features PDDL2.2, temporal actions derived
predicates. instance, use modified planner find initial subplan
subproblems. find mutexes among actions, including active
inactive ones. Finally, compute number global constraints related actions
331

firga,G

0.6
0.4
0.2
0

1.0

0.6
0.4
0.2
0

rga,G

0.6
0.4
0.2
0

1.0

rga,G

0.6
0.4
0.2
0

0 5 10 15 20 25 30 35 40 45 50
Instance ID

0

rg,T
rg,G

0.8

rga,G

0.6
0.4
0.2
0
0 5 10 15 20 25 30 35 40 45 50
Instance ID

f) UMTS-TEMP

0.4
0.2
0

5

2

4

1.0

rg,T
rg,G

0.8

rga,G

0.6
0.4
0.2
0
0

5

10
15
Instance ID

g) DEPOTS-TIME

20

12

14

0.6
0.4
0.2
0

10 15 20 25 30 35 40
Instance ID

1.0

6
8 10
Instance ID

rg,T
rg,G
rga,G

0.8

0 2 4 6 8 10 12 14 16 18 20
Instance ID

d) SATELLITE-TIME
Global-constraint fraction

Global-constraint fraction

d) PSR-SMALL
1.0

0.6

c) PROMELA-OPTICALTELEGRAPH

rg,T
rg,G

0.8

rg,T
rg,G
rga,G

0.8

0

Global-constraint fraction

b) PIPESWORLD-NOTANKAGENONTEMP

rg,T
rg,G

0.8

rga,G

1.0

0 5 10 15 20 25 30 35 40 45 50
Instance ID

Global-constraint fraction

Global-constraint fraction

1.0

rg,T
rg,G

0.8

0 5 10 15 20 25 30 35 40 45 50
Instance ID

a) AIRPORT-TEMP

Global-constraint fraction

rg,T
rg,G

0.8

e) SETTLER
Global-constraint fraction

1.0

Global-constraint fraction

Global-constraint fraction

Chen, Wah, & Hsu

25

1.0

rg,T
rg,G

0.8

rga,G

0.6
0.4
0.2
0
0

5

10
15
Instance ID

20

25

h) BLOCKSWORLD

Figure 6: Variations rg,T , rg,G , rga,G across instances seven IPC4 domain
variants well instances DEPOTS-TIME domain variant
IPC3 Blocksworld domain IPC2. (The latter two domains
deemed difficult constraint partitioning.)

different subplans, well number initial active global constraints based
subplan evaluated subproblem. comparison, also evaluate partitioning
constraints temporal horizon.
Figure 6 illustrates results seven IPC4 domain variants, well Blocksworld
domain IPC2 DEPOTS-TIME variant IPC3. Table 1 summarizes
average statistics across instances IPC4 domain variant
Blocksworld domain Depots domain variants. instance partitioning
time, use modified Metric-FF planner find initial plan, set number
temporal stages number subgoals, partition horizon
solution plan evenly multiple stages. count number local constraints
stage number global constraints relating actions different stages.
instance, let Nc total number mutex constraints, NgT number global
constraints constraint partitioning time, NgG number global constraints
332

fiTemporal Planning using Subgoal Partitioning Resolution

Table 1: Average rg,T , rg,G , rga,G across instances IPC4 domains well Depots
domain IPC3 Blocksworld domain IPC2. (The latter two
deemed difficult constraint partitioning.) Boxed numbers less 0.1.
Domain Variant

r g,T

r g,G

AIRPORT-NONTEMP
AIRPORT-TEMP
AIRPORT-TEMP-TIMEWINDOWS
AIRPORT-TEMP-TIMEWINDOWS-CO
PIPESWORLD-NOTANKAGE-NONTEMP
PIPESWORLD-NOTANKAGE-TEMP
PIPESWORLD-NOTANKAGE-TEMP-DEADLINE
PIPESWORLD-TANKAGE-NONTEMP
PIPESWORLD-TANKAGE-TEMP
PIPESWORLD-NOTANKAGE-TEMP-DEADLINE-CO
PROMELA-OPTICAL-TELEGRAPH
PROMELA-OPTICAL-TELEGRAPH-DP
PROMELA-OPTICAL-TELEGRAPH-FL
PROMELA-PHILOSOPHER
PROMELA-PHILOSOPHER-DP
PROMELA-PHILOSOPHER-FL
PSR-SMALL
PSR-MIDDLE
PSR-MIDDLE-CO
PSR-LARGE
SATELLITE-STRIPS
SATELLITE-TIME
SATELLITE-TIME-TIMEWINDOWS
SATELLITE-TIME-TIMEWINDOWS-CO
SATELLITE-NUMERIC
SATELLITE-COMPLEX
SATELLITE-COMPLEX-TIMEWINDOWS
SATELLITE-COMPLEX-TIMEWINDOWS-CO
SETTLERS
UMTS-TEMP
UMTS-TEMP-TIMEWINDOWS
UMTS-TEMP-TIMEWINDOWS-CO
UMTS-FLAW-TEMP
UMTS-FLAW-TEMP-TIMEWINDOWS
UMTS-FLAW-TEMP-TIMEWINDOWS-CO
DEPOTS-STRIPS
DEPOTS-SIMPLETIME
DEPOTS-NUMERIC
DEPOTS-TIME
BLOCKSWORLD

0.557
0.568
0.494
0.495
0.695
0.682
0.674
0.687
0.683
0.682
0.575
0.759
0.799
0.554
0.855
0.822
0.897
0.896
0.882
0.902
0.689
0.686
0.648
0.633
0.288
0.642
0.633
0.698
0.549
0.463
0.437
0.407
0.459
0.428
0.414
0.537
0.572
0.491
0.448
0.549

0.219
0.208
0.184
0.188
0.313
0.301
0.297
0.677
0.459
0.296
0.399
0.265
0.426
0.370
0.576
0.507
0.489
0.504
0.478
0.665
0.288
0.289
0.114
0.307
0.305
0.282
0.124
0.153
0.451
0.157
0.126
0.098
0.136
0.110
0.086
0.418
0.304
0.354
0.237
0.314

333

r
ga,G
0.017
0.014
0.013
0.014
0.044
0.042
0.033
0.070
0.126
0.039
0.052
0.020
0.037
0.066
0.019
0.087
0.114
0.092
0.049
0.096
0.096
0.093
0.027
0.075
0.078
0.069
0.041
0.042
0.100
0.006
0.008
0.008
0.006
0.008
0.007
0.231
0.167
0.188
0.197
0.254

fiChen, Wah, & Hsu

G number initial active global
constraint partitioning subgoals, Nga
constraints constraint partitioning subgoals. compute following ratios:

NgT
: fraction global constraints constraint partitioning time;
Nc
NgG
rg,G =
: fraction global constraints constraint partitioning subgoals;
Nc
G
Nga
rga,G =
: fraction initial active global constraints subgoal partitioning.
Nc

rg,T =

respect instances IPC4 domains, results show constraint partitioning subgoals leads lower rg,G rg,T , fractions vary significantly,
rga,G small instances. Except PSR-SMALL SETTLERS, rga,G
consistently less 0.1. behavior important active constraints
need resolved planning, number constraints decrease
planning progresses. describe Section 4.2 two strategies reducing number
active global constraints planning.
behavior worse instances Blocksworld domain Depots domain variants. two domains, rga,G consistently high (over 20%) constraints
partitioned subgoals. reason actions different subgoals
instance highly related, making difficult cluster constraints leading
larger fraction global constraints. evaluate performance approach
two domains Section 7.

3. Constraint Partitioning using Penalty Formulations
Given constrained formulation planning problem, summarize section
theory extended saddle points mixed space (Wah & Chen, 2006) design
planners based upon.
3.1 Extended Saddle-Point Condition
Consider following MINLP variable z = (x, y), x Rv Dw :
(Pm ) :

min
z

subject

f (z),

(3)

h(z) = 0 g(z) 0,

f continuous differentiable respect x, g = (g1 , . . . , gr )T h =
(h1 , . . . , hm )T general functions necessarily continuous differentiable.
assumptions important constraints planners procedural
functions closed form. assume f lower bounded, g h
unbounded.
goal solving Pm find constrained local minimum z = (x , )
respect Nm (z ), mixed neighborhood z . results published
earlier (Wah & Chen, 2006), summarize high-level concepts without
precise formalism.
334

fiTemporal Planning using Subgoal Partitioning Resolution

mixed neighborhood Nm (z), z = (x, y), mixed space Rv Dw is:




fi
fi

fi
fi
(4)
Nm (z) = (x , y) x Nc (x) (x, ) Nd (y) ,

Definition 6.

Nc (x) = {x : kx xk 0} continuous neighborhood x,
discrete neighborhood Nd (y) finite user-defined set points {y Dw }.
Definition 7. Point z CLMm , constrained local minimum Pm respect
points Nm (z ), z feasible f (z ) f (z) feasible z Nm (z ).
Definition 8.

penalty function Pm penalty vectors Rm Rr is:
Lm (z, , ) = f (z) + |h(z)| + max(0, g(z)).

(5)

Next, define informally constraint-qualification condition needed main theorem (Wah & Chen, 2006). Consider feasible point z = (x , ) neighboring point
z = (x + p~, ) infinitely small perturbation along direction p~ X x subspace. constraint-qualification condition satisfied z , means
~
p rates change equality active inequality constraints
z z zero. see necessary, assume f (z) z decreases along p~
equality active inequality constraints z zero rates change z
z . case, possible find finite penalty values constraints
z way leads local minimum penalty function z respect
z . Hence, scenario true p~ z , possible
local minimum penalty function z . short, constraint qualification
z requires least one equality active inequality constraint non-zero rate
change along direction ~
p z x subspace.
Theorem 1. Necessary sufficient ESPC CLMm Pm (Wah & Chen, 2006).
Assuming z Rv Dw Pm satisfies constraint-qualification condition, z
CLMm Pm iff exist finite 0 0 satisfies following extended
saddle-point condition (ESPC):
Lm (z , , ) Lm (z , , ) Lm (z, , )

(6)

> > z Nm (z ), Rm , Rr .
Note (6) satisfied rather loose conditions requires
larger critical . theorem important
establishes one-to-one correspondence CLMm z Pm ESP
(extended saddle point) corresponding unconstrained penalty function (5)
penalties sufficiently large. theorem also leads easy way finding CLMm .
Since ESP local minimum (5) (but converse), z found gradually
increasing penalties violated constraints (5) repeatedly finding
local minima (5) feasible solution Pm obtained. possible
exist many algorithms locating local minima unconstrained functions.
335

fiChen, Wah, & Hsu

3.2 Partitioned Extended Saddle-Point Condition
important feature ESPC Theorem 1 condition partitioned
way subproblem implementing partitioned condition solved
looking larger .
Consider Pt , version Pm whose constraints partitioned N subproblems:
(Pt ) :

min
z

subject


J(z)
h(t) (z(t)) = 0,
H(z) = 0,

g(t) (z(t)) 0
G(z) 0

(local constraints)

(7)

(global constraints).

Subproblem t, = 1, . . . , N , Pt local state vector z(t) = (z1 (t), . . . , zut (t))T ut
mixed variables, N
t=1 z(t) = z. Here, z(t) includes variables appear
(t)
(t)
mt local equality constraint functions h(t) = (h1 , . . . , hmt )T rt local inequal(t)
(t)
ity constraint functions g(t) = (g1 , . . . , grt )T . Since partitioning constraints,
z(1), . . . , z(N ) may overlap other. H = (H1 , . . . , Hp )T G = (G1 , . . . , Gq )T
global-constraint functions z. assume J continuous differentiable
respect continuous variables, f lower bounded, g, h, G, H
general functions discontinuous, non-differentiable, unbounded.
first define Np (z), mixed neighborhood z Pt , decompose ESPC
(6) set necessary conditions collectively sufficient. partitioned
condition satisfied finding local ESP subproblem, violated global
constraints resolved using appropriate penalties.
Np (z), mixed neighborhood z partitioned problem, is:

N
N fi
[
[
fi
(t)
fi

Np (z) =
Np (z) =
/ z(t) ,
z fi z (t) Nm (z(t)) zi = zi zi

Definition 9.

t=1

(8)

t=1

Nm (z(t)) mixed neighborhood z(t).

Intuitively, Np (z) separated N neighborhoods, tth neighborhood perturbs variables z(t) leaving variables z\z(t) unchanged.
Without showing details, consider Pt MINLP apply Theorem 1
derive ESPC Pt . decompose ESPC N necessary conditions, one
subproblem, overall necessary condition global constraints across
subproblems. first define penalty function Subproblem t.
Definition 10. Let (z, , ) = |H(z)|+ max(0, G(z)) sum transformed
p
global constraint functions weighted penalties, = (1 , . . . , p )T R
q
= (1 , . . . , q )T R penalty vectors global constraints. penalty
function Pt (7) corresponding penalty function Subproblem defined
follows:

N
X
(t)

(t)
Lm (z, , , , ) = J(z) +
(t) |h (z(t))| + (t) max(0, g (z(t)) +(z, , ), (9)
t=1

(z, (t), (t), , ) = J(z) + (t)T |h(t) (z(t))| + (t)T max(0, g(t) (z(t))) + (z, , ), (10)
336

fiTemporal Planning using Subgoal Partitioning Resolution



(t) = (1 (t), . . . , mt (t))T R (t) = (1 (t), . . . , rt (t))T R
penalty vectors local constraints Subproblem t.

rt



Theorem 2. Partitioned necessary sufficient ESPC CLMm Pt (Wah & Chen,
2006). Given Np (z), ESPC (6) rewritten N + 1 necessary conditions that,
collectively, sufficient:
(z , (t), (t), , ) (z , (t) , (t) , , ) (z, (t) , (t) , , ), (11)
Lm (z , , , , ) Lm (z , , , , ),(12)

(t) > (t) 0, (t) > (t) 0, 0, 0,

r
p
q
(t)
z Np (z ), (t) R , (t) R , R , R , = 1, . . . , N .
Theorem 2 shows original ESPC Theorem 1 partitioned N necessary conditions (11) overall necessary condition (12) global constraints
across subproblems. partitioned condition Subproblem satisfied
finding ESPs subproblem. finding ESP equivalent solving
MINLP, reformulate search Subproblem solution following
optimization problem:


(t)
Pt
J(z) + |H(z)| + max(0, G(z))
(13)
:
min
z(t)

subject

h(t) (z(t)) = 0

g(t) (z(t)) 0.
(t)

weighted sum global constraint functions objective Pt important
leads points minimize violations global constraints.
(t)
large enough, solving Pt lead points, exist, satisfy global constraints.
short, finding solutions Pt satisfy (6) reduced solving multiple
subproblems, (13) solved existing solver modifications
objective function optimized, reweighting violated global constraints
defined (12).
3.3 Formulation Partitioned Planning Subproblems PDDL2.2
PDDL2.2 planning problem solved paper, solution plan specified
start time end time action O. Hence, variable vector z =
{s(a), e(a) O}; objective function J(z) optimized depends makespan
(or number actions propositional domains) plan z; constraints
mutex constraints defined Section 2.1:


h(ai , aj ) = mutex s(ai ), e(ai ), s(aj ), e(aj ) = 0,
ai , aj O.
(14)
Here, mutex binary procedure checking whether ai aj satisfy mutex conditions defined Section 2.1. returns one conditions satisfied zero otherwise.
constraints partitioned subgoals N subproblems G1 , , GN ,
variable z partitioned N subsets z(1), , z(N ), z(t) includes start time
337

fiChen, Wah, & Hsu



(1)

Pt

x
Lm(z, , , , ), find

minz(1) J(z) + |H(z)| + max(0, G(z))
:
subject h(1)(z(1)) = 0 g (1)(z(1)) 0



(N)
Pt



minz(N) J(z) + |H(z)| + max(0, G(z))
:
subject h(N) (z(N )) = 0 g (N) (z(N )) 0

a) Partitioned search look points satisfy (11) (12)
1. procedure partition resolve(Pt )
2.
0; 0;
3.
repeat
// increase penalties violated global constraints maximum bounds //
4.
= 1 p (Hi (z) 6= 0 < ) increase end end for;
5.
j = 1 q (Gj (z) 0 j < j ) increase j end end for;
// inner loop solving N subproblems //
6.
= 1 N apply existing solver solve (13) end for;
7.
((i > Hi (z) 6= 0 j > j Gj (z) 0) (a CLMm Pt found))
8. end procedure

b) Implementation finding CLMm Pt satisfies (11) (12)
Figure 7: partition-and-resolve procedure look CLMm Pt .

end time actions Gt . local constraints mutex constraints
relate actions within subproblem, global constraints relate
actions across subproblems.
(t)
Pt defined Gt , objective find feasible plan z(t) satisfies
constraints Gt , minimizing objective function biased violated global
constraints:


(t)
Pt
:

min
z(t)

subject

J(z) +

N
X
k=1
k6=t

t,k mt,k

h(t) (ai , aj ) = 0

(15)

ai , aj z(t),

J(z) defined later Section 5.3. Here, mt,k number global constraints
actions z(t) z(k):
mt,k =

X

h(at , ak ).

(16)

z(t)
ak z(k)
k6=t

limit number penalties characterizing priorities among subproblems,
assigned single penalty t,k pair subproblems Gt Gk , instead
penalty global constraint Gt Gk .
338

fiSubgoal-Level Planning

Global-Level Planning

Temporal Planning using Subgoal Partitioning Resolution

Plan
Evaluation

Techniques
Studied

Penalty-Value
Update Strategy

GlobalConstraint
Resolution

Global Constraints Subgoals

G1

P1,1

G2

Producible
Resources
GN

PN,1

P1,c1

PN,cN

Constraint
Partitioning
Subgoals

Landmark
Analysis

Temporal
Engine
Modified MetricFF

Derived
predicates
engine

Temporal
engine

DerivedPredicates
Engine
Searchspace
reduction

SearchSpace
Reduction

Figure 8: SGPlan4 : planner implementing partition-and-resolve procedure Figure 7.

3.4 Partition-and-Resolve Procedure
Figure 7 presents partition-and-resolve procedure finding points satisfy
conditions Theorem 2. Using fixed specified outer loop, inner loop
Subproblem Figure 7b solves (13) existing solver, results ESP
satisfies (11). possible (13) well-defined MINLP. solving
N subproblems, penalties violated global constraints increased outer
loop. process repeated CLMm Pt found exceed
maximum bounds.
procedure Figure 7 may generate fixed points (9) satisfy (11)
(12). happens ESP local minimum (9) (but converse). One
way escape fixed points allow periodic decreases . goal
decreases lower barrier penalty function order local descents
inner loop escape infeasible region. Note decreased
gradually order help search escape infeasible regions.
reach minimum thresholds, scaled up, search repeated.
339

fiChen, Wah, & Hsu

4. System Architecture SGPlan4
Figure 8 shows design SGPlan4 implements partition-and-resolve procedure.
procedure alternates global-level planning subgoal-level planning.
section, describe techniques implemented global level, leaving
discussion techniques subgoal level next section.
4.1 Partition-and-Resolve Process SGPlan4
global level, SGPlan4 partitions planning problem N subproblems, G1 , , GN ,
Gt corresponds tth subgoal. orders subproblems, evaluates
using techniques subgoal-level planning, identifies violated global constraints,
updates penalties order bias search next iteration towards resolving
them. SGPlan4 , adopted implementation LPG1.2 (Gerevini & Serina,
2002) detecting persistent mutexes.
partition-and-resolve process understood calculating subplans separately
merging consistent plan. goals optimize multiple subplans
ensure consistency merging. Prior work plan merging focuses
merging redundant actions finding optimal composed plan. particular, Foulser,
Li, Yang (1992) developed algorithms merging feasible classic plans
efficient ones. complete evaluation plan-merging algorithms classical domains
conducted Yang (1997). Tsamardinos, Pollack, Horty (2000) extended
concept domains temporal constraints. plan merging means
making infeasible plan feasible, different approach aims resolve
inconsistencies terms mutexes among subplans.
alternative view resolution approach reuse modification
subplans consistent plan. Plan-reuse systems adapt existing plans new initial states goals. approach demonstrated SPA (Hanks & Weld, 1995)
PRIAR (Kambhampati & Hendler, 1992) show improvements efficiency many
domains. major difference current plan-reuse approaches partitionand-resolve process generate candidate subproblems based partitioning
mutex constraints, whereas traditional methods reuse plans generated
means. Since assumption conservative plan modification existing methods
always achievable, may necessary replan feasible plan candidate cannot
found. cases, may expensive planning scratch.
reason complexity analysis empirical study cannot prove plan-reuse approaches
consistent improvements planing scratch (Nebel & Koehler, 1995).
contrast, approach augments search subproblem explicitly penalizing
global inconsistencies forcing solution towards resolving global constraints.
partition-and-resolve approach different incremental planning (Koehler &
Hoffmann, 2000) uses goal agenda. incremental planning, planner maintains
set target facts, adds goal states incrementally target set, extends
solution using new target set. goal state must always satisfied
achieved, ordering goal states important order avoid un-doing
previously achieved goal state planning current goal state. invalidations
occur, planning task point complex planning one
340

fiTemporal Planning using Subgoal Partitioning Resolution

goal state. contrast, SGPlan4 tries achieve one subgoal time allows
subgoals invalidated process. Moreover, subgoal, need
start ending state previous subgoal incremental learning,
need pre-order subgoals order avoid invalidations. show Section 6
performance SGPlan4 sensitive order evaluating subgoals.
4.2 Resolving Violated Global Constraints
section, present two penalty-update strategies resolving violated global constraints. constraints identified finding subplan subproblem independently.
SGPlan4 first initializes penalties global constraints starts. first
iteration, SGPlan4 solves subproblem individually, without considering global
constraints. combines subplans integrated plan order determine initial active global constraints across subproblems. subsequent iterations,
SGPlan4 finds local feasible plan subproblem, minimizing global objective weighted sum violated global constraints. end iteration,
SGPlan4 increases penalty violated global constraint proportion violation.
process ends constraints satisfied.
designed two strategies updating penalty global constraints.
SGPlan4 participated IPC4 sets large initial penalty values updates
rate , whereas SGPlan4.1 studied paper sets initial penalty values zero:
(
0 (for SGPlan4 )
(0)
()
(1)
t,k =
t,k = t,k + mt,k ,
= 1, 2, . . .
(17)
0
(for SGPlan4.1 ),
()

Here, t,k penalty global constraints Gt Gk th iteration,
mt,k defined (16), 0 large initial value, parameter controlling
rate penalty updates. experiments, set 0 = 100 = 0.1.
Figure 9 illustrates planning process SGPlan4 AIRPORT-TEMP-14 instance. Given three subproblems instance, SGPlan4 first evaluates subproblem first iteration order determine initial active global constraints.
figure shows, respectively, subplans active global constraints evaluating
three subproblems second iteration. strategy effective reducing
number active global constraints quickly 14 beginning zero one
iteration.
penalty-update strategy SGPlan4 may lead longer makespans uses
large initial penalty values order reduce number violated global constraints
quickly. Hence, subplans found may poor temporal concurrency. address
issue, implemented new strategy SGPlan4.1 (17) sets initial penalty
values zero.
Figure 10 illustrates time-quality trade-offs SGPlan4 SGPlan4.1 used
solve nine representative instances IPC4, Blocksworld, Depots domains.
number active global constraints changes evaluating subproblem,
plot progress remaining number active global constraints respect
total number subproblems evaluated. results show planners
341

fiChen, Wah, & Hsu

1
0
0
1 0
0
1 1
1
0
1
0
10
0
1
1
0
0
1
1
0
10
1
0
0
1
11
0
1
0
0
1
0
1
0
0
01
1
0 1
1
0
1
0
0
1
1
0
1
0
1
0

G1

1
0
1
0
1 0
0
1
1 0
1
0

G2

1
0
0
1

1
0
0
1

1
0
0
1

1
0
0
1

10
0
1
0
0
1
1
0
1
0
1
1
0
1
0

0
1
0
1
1
0
1
0
00
1
1

1
0
0
1

1
0
0
1

0
1
0
1
1
0
0
1

1
0
1
0
0 0
1
1

1
0
0
1

1
0
0
1

G3

1
0
0
1
1
0
1
0

1
0
0
1

a) start Iteration 2
1
0
0
1 0
0
1 1
1
0
1
0
1
1
0
1
0
0
1 0
0
1
0
1 1
0
0
1
1
0
1
0
1
0
1
0
1
0
0
10
0
0 1
1
1
0
1
1
0
0 1
1
0
0
1
1
0
1
0

1
0
1
0

1
0
0
1

1
0
1
0

0
1
0
1
1
0
1
0
00
1
1

1
0
1
0

1
0
1
1 0
0
0
1
1
0
0
1

1
0
1
0

1
0
1
0

1
0
0
01
1
0
1
1
0
1
0
0
1
0
1

1
0
0
1

1
0
1
0

1
0
0
1
1
0
1
0

1
0
0
1

1
0
0
1

1
0
0
1

1
0
1
0

1
0
1
0

0
1
0
1
1
0
0 0
1
1
0
1

1
0
0
1 0
0
1 1
0
1
0
1
0
1
1
0
0
1 0
0
11
0
1 1
0
0
1
11
0
1
0
0
1
0
1
0
0
01
1
0 1
1
0
1
0
0
1
1
0
1
0
1
0

1
0
0
1

1
0
1
0
0
1
1
0
1
0
0
1

1
0
1
0
1 0
0
1
1 0
1
0
10
0
1
0
0
1
1
0
1
0
1
1
0
1
0

0
1
0
1
1
0
0
01
1
0
1

1
0
0
1

1
0
1
0

0
1
0
1
1
0
0
1

1
0
1
0
0 0
1
1

G2

0
1
0
1
1
0
0 0
1
1
0
1
1
0
0
1

G3

1
0
0
1

1
0
0
1
1
0
1
0

1
0
1
0

1
0
1
0

G1

b) solving Subproblem G1
0
1
1
1 0
0
1 0
0
1
0
1
0
1
0
1
1
0
0
1
1
0
0
1
1
0
0
00
1
0
0
1
11
1 1
0
0
10
0
0 1
1
1
0
1
1
0
0 1
1
0
0
1

G1

G2

0
1
0
1
1
0
0 0
1
1
0
1

1
0
1
0

1
0
1
0

1
0
0
1

1
0
0
1

G3

1
0
1
0

c) solving Subproblem G2

0
1
0
1
1
0
1
0
00
1
1

1
0
1
0

1
0
1
1 0
0
0
1
1
0
0
1

1
0
1
0

1
0
1
0

1
0
0
01
1
0
1
1
0
1
0
0
1
0
1

1
0
0
1

1
0
0
1

1
0
1
0

1
0
0
1
0
1
1
0
1
0
0
1

1
0
1
0

G1

G2

0
1
0
1
1
0
0 0
1
1
0
1
1
0
0
1

G3

1
0
1
0

d) solving Subproblem G3

Figure 9: planning process IPC4 version SGPlan4 second iteration
solving AIRPORT-TEMP-14 instance. box corresponds action
subplan, whereas arrow corresponds active global constraint.
placing emphasis violated global constraints, number violated
constraints quickly reduced expense longer makespan.

resolve remaining number active global constraints almost linear fashion,
SGPlan4 generally faster resolving active global constraints generates
plans worse quality. detailed experimental results Section 7, show
SGPlan4.1 generally leads plans better quality.
planners, however, difficulty solving PIPESWORLD-NOTANKAGETEMP-DEADLINE-10 instance (Figure 10c). domain, SGPlan4 cannot solve
instances, whereas SGPlan4.1 solve eight instances (1, 2, 5, 6, 8, 14, 22, 30). Although fraction initial active global constraints constraints 3.3%
average (Table 1), planners may get stuck infeasible solutions cannot make progress afterward. reason basic planner SGPlan4
SGPlan4.1 enough backtracking generate new candidate subplans
subproblem. Hence, basic planner keeps generating subplan point,
regardless violated constraints penalized.
4.3 Handling Producible Resources
planning problems, may facts made true numerical
resources produced anytime needed. example, Settlers domain,
342

fi15
10
5
0

60

SGPlang (Q=61.73)
SGPlang2 (Q=52.00)

50
40
30
20
10
0

8 10 12 14 16 18 20 22 24 26 28

8

total # subproblems evaluated

100
50
0
30

40

50

60

70

100
90
80
70
60
50
40
30
20
10
0

80

10
8
6
4
2
0
15

20

25

30

total # subproblems evaluated

g) UMTS-TEMP-50

20

22

5

10

15

20

25

30

35

40

total # subproblems evaluated

35

45
40
35
30
25
20
15
10
5
0

SGPlang (Q=544.00)
SGPlang2 (Q=541.00)

15 20 25 30 35 40 45 50 55 60

total # subproblems evaluated

total # subproblems evaluated

e) SATELLITE-TIME-20
# active global constraints

# active global constraints

SGPlang (Q=2230.40)
SGPlang2 (Q=818.00)

10

18

40 60 80 100 120 140 160 180 200

d) PROMELA-OPTICALTELEGRAPH-10
12

16

SGPlang (Q=704.26)
SGPlang2 (Q=645.01)

total # subproblems evaluated

14

14

# active global constraints

# active global constraints

# active global constraints

150

20

12

SGPlang (Q=N/A)
SGPlang2 (Q=N/A)

total # subproblems evaluated

SGPlang (Q=198.98)
SGPlang2 (Q=197.34)

200

10

50
45
40
35
30
25
20
15
10
5
0

b) PIPESWORLD-NOTANKAGE- c) PIPESWORLD-NOTANKAGENONTEMP-30
TEMP-DEADLINE-10

a) AIRPORT-TEMP-30
250

# active global constraints

SGPlang (Q=708.10)
SGPlang2 (Q=705.03)

20

70

# active global constraints

25

# active global constraints

# active global constraints

Temporal Planning using Subgoal Partitioning Resolution

SGPlang (Q=56.00)
SGPlang2 (Q=42.00)

60
50
40
30
20
10
0
15

20

25

30

35

total # subproblems evaluated

h) BLOCKSWORLD-17-0

40

f) SETTLERS-20
200
180
160
140
120
100
80
60
40
20
0

SGPlang (Q=107.00)
SGPlang2 (Q=103.00)

10

15

20

25

30

35

40

total # subproblems evaluated

i) DEPOTS-TIME-20

Figure 10: Resolution active global constraints nine benchmark instances original penalty-update strategy SGPlan4 new penalty-update strategy
SGPlan4.1 . x axis includes number subproblems evaluated,
corresponding subgoal, first iteration order determine initial
active global constraints.

coal always produced mine. define producible logical numerical
resources follows.
a) fact producible add effect either action without preconditions
action whose preconditions always producible.
b) numerical resource producible increased either action without
preconditions action whose preconditions always producible.
planning tasks significantly easier producible facts resources
detected preprocessing phase made available planning. first
identifying facts resources, SGPlan4 derives relaxed initial state setting
producible facts true producible numerical resources large enough.
Every time producible fact turned false, made true again. finding feasible
plan relaxed initial state, SGPlan4 removes unused numerical resources
343

fiChen, Wah, & Hsu

initial state plans again. process repeated redundant initial
resources. point, SGPlan4 inserts necessary actions beginning plan
generate minimum initial producible resources needed.
example, suppose timber detected producible resource one always
fell trees get timber. SGPlan4 initially set large number, say 1000 units,
timber available. solving problem, suppose 900 units left unused,
reduces initial timber 100 units plans again. process repeated
either unused timber final state problem becomes unsolvable
reducing initial resource.
Note approach may incur redundant actions producing unused resources, optimal amount resources needed cannot predicted ahead time.

5. Subgoal-Level Planning
subgoal level, SGPlan4 applies landmark analysis partition subproblem,
performs path finding optimization, carries subspace-reduction analysis prune
irrelevant facts actions subproblem, calls modified Metric-FF planner
solve subproblem.
5.1 Subgoal-Level Decomposition Techniques
a) Landmark analysis. First proposed Porteous, Sebastia, Hoffmann (Porteous,
Sebastia, & Hoffmann, 2001), landmark analysis allows large planning problem
decomposed series simpler subproblems. Given initial state, aims find
intermediate facts must true feasible plan reaching goal state.
example, assume object delivered D, path
B C D. (O, B) (O, C) landmark facts,
since feasible plan must make true reaching goal state (O, D).
planning problem first partitioned subgoals subproblems,
apply landmark analysis subproblem order find intermediate facts
reaching corresponding subgoal. Landmark analysis important SGPlan4
allows subproblem decomposed simpler subproblems
solved easily.
subproblem, find landmarks relaxed planning approach. Given planning subproblem = (O, F, I, G), first construct relaxed planning graph
initial state ignoring delete effects actions. force f F level
graph false (even made true actions). result, actions
preconditioned f pruned. exists goal fact G cannot reached
f false, f landmark fact must reached plan relaxed
problem. finding partial order landmarks, SGPlan4 builds sequential
list subproblems joined landmarks found applies basic planner solve
subproblem order. Note landmark analysis expensive, SGPlan4
detects landmarks beginning every iteration.
landmarks found relaxed planning graph necessary solution
plan original problem also solution plan relaxed problem. Hence,
feasible plan original problem must reach landmark found relaxed ap344

fiTemporal Planning using Subgoal Partitioning Resolution

Initial State
ON(B10 A2)

ON(B0 A1)

LAST(B10 S12)

ON(B4 A2)

ON(B6 A2)

ON(B12 A3)

LAST(B4 S12)
FIRST(B0 S12)

LAST(B12 S13)

ON(B9 A2)

ON(B8 A3)

LAST(B9 S12)

LAST(B8 S13)

LAST(B6 S12)

ON(B10 A1)

ON(B5 A2)

LAST(B5 S12)

ON(B12 A1)

ON(B9 A1)

ON(B8 A1)

FIRST(B9 S13)

FIRST(B10 S13)

ON(B10 A3)

FIRST(B12 S12)

ON(B0 A2)

ON(B4 A1)

ON(B6 A1)

ON(B12 A2)

FIRST(B8 S12)

ON(B9 A3)

ON(B8 A2)

ON(B5 A1)

Goal State

Figure 11: Landmarks partial orders PIPESWORLD-NOTANKAGENONTEMP-10 instance.
proach least once. However, landmarks found sufficient test goal
reachability relaxed approach, may exist undetected landmarks even
every fact tested.
Figure 11 shows landmarks found IPC4 PIPESWORLD-NOTANKAGENONTEMP-10 instance. considering first goal fact (B10, A3), LAST (B10, S12)
landmark also landmarks (B10, A1) F IRST (B10, S13).
means LAST (B10, S12) must ordered (B10, A1) F IRST (B10, S13).
way, decompose subproblem (B10, A3) 4 smaller tasks
must carried sequence, namely, LAST (B10, S12), (B10, A1), F IRST (B10, S13),
(B10, A3).
b) Landmarks identified path finding. Landmark analysis may sometimes produce
landmark facts decomposing subproblem. example, gates
along path Airport instance identified landmark facts (that is,
must-visit points) usually multiple paths given source destination. Consider airport topology Figure 12a goal move A1
SG1 SG8. two alternative paths none facts
(A1, SG2), (A1, SG3), , (A1, SG7) true reaching SG8, cannot detect landmark facts.
identify landmark facts decomposing subproblem, developed
SGPlan4 new path-finding technique. technique based concept fact
groups used existing planners, MIPS (Edelkamp, 2002)
Downward (Helmert & Richter, 2004). fact group includes group mutually exclusive
facts one true time, typically involves multiple possible
states object. example Airport instance discussed above, fact group includes
different locations A1 at:


Fg =
(A1, SG1), (A1, SG2), , (A1, SG7), (A1, SG8) .
(18)
345

fiChen, Wah, & Hsu

(A1, SG4)

(A1, SG2)

(A1, SG3)

(A1, SG3)

(A1, SG8)

(A1, SG1)

(A1, SG4)

(A1, SG2)

(A1, SG8)

(A1, SG1)

(A1, SG5) (A1, SG6)

(A1, SG7)

(A1, SG5) (A1, SG6)

a) Transition graph Fg

(A1, SG7)

b) Path finding

Figure 12: Illustration transition graph Fact Group Fg path finding algorithm. Shaded nodes (b) new landmark facts detected path finding.

SGPlan4 , adopted approach MIPS based analysis static mutex
groups finding fact groups subgoal facts.
apply path finding Subproblem Gt none landmarks
detected landmark analysis. Assuming subgoal reached gt , first find
fact group belongs to. previous example, subgoal gt = (A1, G8),
fact group Fg (18).
fact group two facts, determine transition relations
constructing directed graph. Given two facts f1 f2 fact group, add edge
f1 f2 exists action f1 precondition f2
add effect (which implies f1 delete effect since f1 f2 mutually
exclusive). Figure 12a illustrates transition graph airport example discussed
above.
Last, find path, look facts immediate predecessors gt
graph. arbitrarily select one must-visit landmark disable others.
perform landmark analysis initial fact gt . analysis return
landmark facts.
example airport instance, (A1, SG4) (A1, SG7) two immediate
predecessor facts Subgoal gt = (A1, SG8). disable (A1, SG7) landmark
analysis, one path (A1, SG1) gt , (A1, SG2),
(A1, SG3), (A1, SG4) detected landmark facts. Figure 12b illustrates
process.
c) Path optimization used find better landmark facts problems timed
initial literals numerical effects. invoked deadline
dynamically changing numerical resource appears preconditions actions.
conditions satisfied IPC4 Satellite instances technique found
useful.
technique works choosing path optimizes time duration usage
numerical resource multiple paths different quality, setting
nodes along optimal path landmark facts. Given subproblem trying reach
Subgoal gt , construct transition graph fact group gt apply Dijkstras
algorithm find shortest path initial fact gt . weight edge
either time duration problems time windows, usage numerical resource
346

fiTemporal Planning using Subgoal Partitioning Resolution

0

G1

a3

a1
a2

G2

Time

a5
a4

a6
S6

G3
S4
S2
S0

S5

S3

S1

Figure 13: Generating multiple starting states Subproblem G3 , given initial state
S0 Si , = 1, . . . , 6, state action ai finished. SGPlan4 calls
basic planner generate local subplan starting state picks
first one improves objective (15).

problems numerical preconditions. set facts along optimal path
landmark facts force planner choose path others. landmarks along
optimal path allows us decompose problem subproblems.
two limitations current implementation path optimization. First,
since needs path initial fact goal fact transition graph,
cannot apply technique initial goals facts disconnected. Second,
studied case one dynamically changing numerical resource appears
preconditions actions studied optimization multiple numerical
resources.
5.2 Subgoal-Level Planning Techniques
a) Evaluating multiple subplans subproblem. finding local feasible subplan
subproblem improves objective (15), SGPlan4 generates number subplans
multiple starting states. Since active global constraints exist two identical
subplans, generate multiple starting states given subproblem applying possible prefix actions subproblems. example, given six actions
planned G1 G2 Figure 13, six possible starting states developing
subplan G3 . starting state, SGPlan4 calls basic planner generate local
feasible subplan accepts subplan improves objective (15). better
subplans found possible starting states, SGPlan4 leaves local subplan
unchanged moves next subproblem.
b) Search-space reduction. solving partitioned subproblem, often eliminate search space many irrelevant actions related facts subgoals
subproblems. reductions useful planning problems
partitioned actions generally relevant.
347

fiChen, Wah, & Hsu

example, consider transportation domain whose goal move packages,
drivers, trucks various locations initial configuration. Suppose problem instance, goal set {AT (D1, S1), (T 1, S1), (P 1, S0), (P 2, S0)} two
packages P 1 P 2, one driver D1, one truck 1, two locations S1 S2. Without partitioning, actions relevant resolving subgoals. contrast,
partitioning, actions moving P 2 around irrelevant subproblem resolving (P 1, S0) eliminated. Similarly, actions moving P 1 P 2
irrelevant subproblem resolving (D1, S1).
designed backward relevance analysis eliminate irrelevant actions
subproblem solving basic planner. analysis, maintain
open list unsupported facts, close list relevant facts, relevance list relevant
actions. beginning, open list contains subgoal facts subproblem,
relevance list empty. iteration, fact open list, find
actions support fact already relevance list. add
actions relevance list add action preconditions close list
open list. move fact open list close list processed.
analysis ends open list empty. point, relevance list contain
possible relevant actions. analysis takes polynomial time.
Note relevance analysis complete stops, since relevance list
may still contain irrelevant actions. example, reduce relevance
list forward analysis finding applicable actions initial states
backward analysis. However, analysis may cost effective reducing
overhead planning.
reduction method belongs family heuristics proposed Nebel, Dimopoulos
Koehler (1997). Since select possible supporting actions processing fact,
approach indeed one selects union elements possibility
set according classification. conservatively reduce irrelevant information, number tighter reductions approximately minimize use
initial facts (Nebel et al., 1997). However, aggressive heuristics may solution
preserving solution-length preserving.
5.3 Modified Metric-FF Basic Planner
decomposing subproblem associated subgoal smaller subproblems bounded
landmark facts, SGPlan4 solves subproblem identified (or original subproblem
case landmark facts identified) modified Metric-FF planner. modifications consist two components: adaptation original Metric-FF (Hoffmann,
2003) order entertain new features PDDL2.2, support planning
mutex constraints partitioned. fact, lot efforts embedding Metric-FF
SGPlan4 spent first component.
original Metric-FF solve problems PDDL2.1 propositional actions
support temporal features. extended parser Metric-FF
support full PDDL2.2 syntax definition actions atomic logical durational temporal. planning process also extended sequential propositional
planning parallel temporal planning. Specifically, extended sequential actions
348

fiTemporal Planning using Subgoal Partitioning Resolution

Fixed
Subproblems

G1
.....

Components Objective Function
(15)
Estimated makespan Te

Gt1
Gt+1

PN

.....

k=1
k6=t

GN

t,k
e t,k

Weighted sum global mutex
constraint violations

Gt
Current Plan

Heuristic value (z(t)) original Metric-FF

Relaxed Plan

Figure 14: Temporal planning partitioned search context incorporates objective
function (15) makespan Te estimated enhanced PERT algorithm
heuristic value Metric-FF planner.
atomic length original Metric-FF actions predefined durations
scheduled parallel.
extended Metric-FF support new feature called derived predicates introduced PDDL2.2. Derived predicates define axioms whose facts derived set
precondition facts. example, domain boxes, B B
C, derived predicate C generated. Derived predicates
appear preconditions goals effects. modified Metric-FF,
implemented technique proposed MIPS 2.2 (Edelkamp, 2003) handling derived
predicates. encode derived predicate special action a, precondition
facts preconditions facts d, add effects derived facts d,
delete effect empty. planning, derived-predicate actions
included relaxed plan. However, heuristic function computed Metric-FF
counts number real actions relaxed plan number derivedpredicate actions, real actions considered candidates forward expansion
state. state, expand set true facts applying applicable derived
predicates iteratively reach fixed-point state true facts
added.
second component modifications Metric-FF involves support
partitioned search context solving subproblem, say Gt . case, Metric-FF needs
incorporate objective aggregate state schedulable actions G1 , , GN
planning actions Gt . Referring Figure 14, aggregate state represented
estimated makespan Te actions evaluated enhanced PERT
algorithm.
PERT originally developed generate parallel plan scheduling action
early possible blocked dependency mutex relation. Previous
PERT algorithms detect propositional conflict two actions checking one
action adds/deletes anothers precondition, detect numerical conflict two actions
modify numerical variable. latter case, two actions would allowed
349

fiChen, Wah, & Hsu

overlap execution consume resource, even total
amount required exceed amount available. Obviously, resulting schedule
suboptimal.
developed enhanced PERT algorithm considers resource constraints
schedule. algorithm assigns action early possible long
propositional conflicts violations numerical/resource constraints. Besides
maintaining operator dependency original PERT, also keep track changes
numerical variables. algorithm greedy schedules applicable actions
early possible without backtracking.
general, PERT schedule valid sequential plan parallel plan without mutex
conflicts. However, enhanced PERT may generate parallel plan mutex conflicts.
reason subproblem solved initial state sequentially
state previous subproblem. Hence, actions multiple subplans
combined, one action may delete precondition another causes mutex conflict.
example, consider sequential plans two subproblems G1 G2 scheduled
initial state Blocksworld domain: a) MOVE (A, B) MOVE (B, C);
b) MOVE (D, E) MOVE (E, C), MOVE (x, y) places x top y,
precondition CLEAR (y) (y clear nothing it). example, PERT cannot
generate parallel plan mutex conflict MOVE (E, C) MOVE (B, C),
regardless two actions scheduled. conflict occurs action
deletes CLEAR (C) precondition other.
modified Metric-FF planner carries search heuristically looks plans
minimize (15) rewritten follows:



PN




N
e t,k
minz(t) (z(t)) + k=1 t,k
X
k6=t


min J(z) +
t,j
e t,j =
PN

z(t)
e

j=1
e t,k
minz(t) (z(t)) + + k=1 t,k
j6=t

k6=t

(for SGPlan4 )
(19)
(for SGPlan4.1 ),

(z(t)) heuristic value original Metric-FF solving Gt ;
e t,k
estimated number active mutexes plan Gk relaxed plan Gt
obtained ignoring delete effects unscheduled actions; Te makespan estimated
enhanced PERT algorithm composing relaxed plan Gt plans
subproblems; t,k penalty value dynamically updated global-level planning;
constant fixed 0.0001. Although search guarantee optimality,
always resolve global mutual-exclusion constraints between, say z(t) z(k),
move one subplan backward order avoid overlapping another conflicting
subplan penalty t,k large enough.
implementation (19) modified Metric-FF planner, set
SGPlan4.1 small penalty term due makespan dominate
terms. fact, since Te much smaller one test problems, main
purpose break ties among states close heuristic values.
hand, implementation (19) SGPlan4 IPC4 include Te objective
function. result, focuses eliminating mutual-exclusion conflicts tends
generate plans longer makespan.
350

fiTemporal Planning using Subgoal Partitioning Resolution

1. procedure SGPlan(problem file)
2.
parse problem file instantiate facts actions;
3.
detect encode timed initial literals (TIL);
4.
detect encode derived predicates;
5.
detect TIL wrappers translate regular TILs;
6.
detect producible resources;
7.
(there producible resources) set maximum possible end if;
8.
repeat
9.
subgoal fact goal list
10.
call search-space reduction eliminate irrelevant actions;
11.
call basic planner (modified Metric-FF) reach subgoal;
12.
(the basic planner times out)
13.
perform landmark analysis generate list subproblems;
14.
subproblem list
15.
call basic planner solve subproblem;
16.
(solution found time limit)
17.
(problem TIL numerical fluents) perform path optimization
18.
else perform path finding decompose subproblem end if;
19.
call basic planner solve decomposed subproblem;
20.
end
21.
end
22.
end
23.
end
24.
evaluate plan z update penalty values violated global constraints;
25.
feasible solution plan found time limit exceeded;
26.
((new solution found) && (there unused producible resources))
27.
reduce initial producible resources goto step 8;
28.
end
29. end procedure

Figure 15: high-level pseudo code common SGPlan4 SGPlan4.1 .

general, embedding basic planner partition-and-resolve framework requires
modifications objective function basic planner order implement
(15). Hence, cannot done without source code basic planner.
5.4 Putting Pieces Together
Figure 15 shows high-level code common SGPlan4 SGPlan4.1 .
preprocessing phase parses problem file instantiates facts actions (Line
2), detects encodes timed initial literals (TIL) derived predicates, (Lines 3
4), translates problem regular TIL problem problem compiled TIL
problem (Line 5), detects producible resources sets always available (Lines
6 7).
major loop Lines 8 28. subgoal, SGPlan4 uses search-space
reduction eliminate irrelevant actions (Line 10) solves using basic planner
(Line 11). basic planner fails find feasible plan within time limit (3000 node
351

fiChen, Wah, & Hsu

Table 2: Summary useful techniques domain variant. check mark indicates
technique found useful domain variant class domain
variants.

Domain Variant
AIRPORT-*
AIRPORT-TEMP-TIMEWINDOWS-CO
PIPESWORLD-*
PROMELA-*
PROMELA-*-DP
PSR-SMALL
PSR-MIDDLE
PSR-MIDDLE-CO
PSR-LARGE
SATELLITE-STRIPS
SATELLITE-TIME
SATELLITE-NUMERIC
SATELLITE-COMPLEX
SATELLITE-TIME-TIMEWINDOWS
SATELLITE-TIME-TIMEWINDOWS-CO
SETTLERS
UMTS-TEMP
UMTS-TEMP-TIMEWINDOWS
UMTS-TEMP-TIMEWINDOWS-CO
UMTS-FLAW-TEMP
UMTS-FLAW-TEMP-TIMEWINDOWS
UMTS-FLAW-TEMP-TIMEWINDOWS-CO

SG























LM




PF




PO

TIL

TIL-w

DP

PR

SR

































Keys SG: subgoal partitioning
LM: landmark analysis
PF:
path finding
PO: path optimization
TIL: timed initial literals handling TIL-w: TIL wrapper detection
DP: derived predicates handling PR: producible resources
SR:
search-space reduction

expansions Metric-FF), SGPlan4 aborts run Metric-FF tries decompose
problem further. first applies landmark analysis decompose solve subproblem
(Lines 13-15). unsuccessful solving subproblem, tries path optimization
numerical TIL problems (Line 17) path finding (Line 18) partition
subproblem. subgoals evaluated, composes solution, evaluates
global constraints, updates penalty values (Line 24). Finally, new solution
found unused producible resources, reduces initial producible
resources (Lines 26-28) repeats major loop again.

6. Sensitivity Analysis Techniques SGPlan4
section describe ablation study various techniques SGPlan4 order
test effectiveness. Table 2 lists techniques useful IPC4
domain variant. defer discussion performance improvement SGPlan4.1
SGPlan4 Section 7.
352

fiTemporal Planning using Subgoal Partitioning Resolution

Airport variants, useful techniques include subgoal partitioning, landmark analysis, path finding. addition, TIL wrapper detection needed
TIMEWINDOWS-CO variant. ablation study, applied SGPlan4 subgoal
partitioning alone. case, SGPlan4 solve 107 200 (53.5%) instances
cannot solve numbered higher 28 (namely, P29, P30, etc.). reason
subproblems without landmark analysis path finding large
Metric-FF difficulty solving them. contrast, SGPlan4 landmark analysis
path finding solve 159 (79.5%) instances.
Pipesworld variants, useful techniques include subgoal partitioning, landmark analysis, path finding, search-space reduction. Although search-space reduction
slightly reduce run time 5.3% average, landmark analysis path finding
significant effects performance. SGPlan4 without landmark analysis path
finding solve 102 200 (51%) instances, whereas SGPlan4 landmark
analysis path finding solve 186 instances (93%). Landmark analysis path finding also leads 8% average improvement run time instances versions
solve.
Promela domain, subgoal partitioning found useful, besides applying
derived-predicate handling corresponding variants.
PSR variants except PSR-SMALL, search-space reduction particularly
useful addition subgoal partitioning. three variants, SGPlan4 searchspace reduction solve, respectively, 50, 14, 11 instances; whereas SGPlan4 without
search-space reduction solve, respectively, 47, 8, 6 instances. addition, average run-time improvements due search-space reduction are, respectively, 34.1%, 46.9%,
62.5%. PSR-SMALL variant, search-space reduction significant effects
run time solution quality. Last, derived-predicate handling important PSRMIDDLE, encoded using derived predicates.
Satellite domain, subgoal partitioning found useful solving
TIME, STRIPS, COMPLEX variants. NUMERIC, TIME-TIMEWINDOWS,
TIME-TIMEWINDOWS-CO variants, landmark analysis path optimization
also useful. three variants, SGPlan4 solve, respectively, 25, 25, and, 21
instances, whereas SGPlan4 without landmark analysis path optimization solve,
respectively, 16, 16, 13 instances.
Settlers domain, subgoal partitioning well techniques handling producible resources important solving one instances. (The eighth instance
infeasible.) Without detecting producible resources, SGPlan4 solve nine
20 instances.
UMTS domain, subgoal partitioning found useful, besides applying
TIL handling TIL wrapper detection corresponding variants. Landmark analysis
help domain detect none landmark facts
300 instances. Also, search-space reduction prune facts little effects
performance.
also studied effects subgoal ordering SGPlan4.1 eighteen representative variants IPC4 domains well Depots domain (Figure 16).
instance, test SGPlan4.1 using five random subgoal orders normalize run time
(resp. quality) respect corresponding measure SGPlan4.1 run using
353

fiChen, Wah, & Hsu

AIRPORT: NONTEMP, TEMP
PIPESWORLD: NOTANKAGE-NONTEMP, NOTANKAGE-TEMP
PROMELA: OPT-TELEGRAPH, OPT-TELGRAPH-DP, PHIL, PHIL-DP
PSR: SMALL, MIDDLE
SATELLITE: STRIPS, TIME
SETTLERS: SETTLERS
UMTS: TEMP, FLAW-TEMP
DEPOTS: STRIPS, SIMPLETIME, TIME

Normalized Quality

10

1

0.1
0.01

0.1

1

10

100

Normalized Run Time
Figure 16: Run time-quality distribution SGPlan4.1 run using different random subgoal
orders selected IPC4 Depots domain variants. results normalized respect run time quality SGPlan4.1 run using default
subgoal order. (Performance values larger one better SGPlan4.1 .)

original order problem definition. use makespan quality measure
temporal domains number actions propositional domains (even
objective specified problem definition).
results show performance SGPlan4.1 quite insensitive subgoal ordering Airport, Promela, Settlers, UMTS domains. However, significant
variations run time quality Pipesworld PSR domains, although
definitive trend random subgoal order better. Depots domain,
exist smaller variations run time quality. common feature among
Pipesworld, PSR, Depots domains intensive subgoal interactions,
make sensitive order subgoals evaluated. example,
354

fiTemporal Planning using Subgoal Partitioning Resolution

PSR-MIDDLE variant, number subgoals large, different subgoals
highly related derived predicates. Last, note using original subgoal order
leads better run time quality Satellite domain. reason original
order avoid unnecessary subgoal invalidations finding local feasible subplans, since
starting states generated applying prefix subplans subgoals.
clear advantage using random subgoal orders original
subgoal order, SGPlan4 SGPlan4.1 use original subgoal order implementations.

7. Experimental Results
section, experimentally compare performance SGPlan4 , SGPlan4.1 (their
differences (17) (19)) planners solving IPC3 IPC4 benchmark
suites well Blocksworld domain IPC2. suite contains multiple domains,
several variants each. variants IPC4 address different features
PDDL2.2, include versions STRIPS, STRIPS DP (derived predicates), temporal, temporal TIL (deadlines), numeric, complex (temporal numeric).
complete description variant problem files found Web site
competitions2
runs carried AMD Athlon MP2800 PC Redhat Linux AS3
2-Gbyte main memory unless otherwise noted. Following rules IPC4, random
planners set fixed random seed, all, throughout experiments. Moreover,
planners must fully automated, run parameter setting
instances attempted, execute CPU time limit 30 minutes main memory
limit 1 Gbytes.
Table 3 summarizes performance SGPlan4 , SGPlan4.1 , Downward (Helmert &
Richter, 2004), LPG-TD-SPEED-1.0 seed 2004, YAHSP-1.1.3 use makespan
quality metric temporal domains number actions propositional domains. Since code Downward unavailable, report IPC4 results adjusting
run times factor governed difference speeds computer used
IPC4 competition computer used SGPlan4.1 . Likewise, unable
evaluate Downward IPC2 IPC3 benchmarks.
Table 3 include results domain variants target planner cannot
handle. example, LPG-TD-SPEED cannot solve compiled domains
support grammatical features PSR-LARGE two FLUENTS variants
PROMELA domain; YAHSP cannot handle derived predicates. contrast,
SGPlan4 SGPlan4.1 designed solve variants except ROVERS-TIME
variant dynamic durations. Note since Satellite Settlers domains
exist IPC3 IPC4 benchmarks, table include results
2. URL competitions http://ls5-www.cs.uni-dortmund.de/~edelkamp/ipc-4/
IPC4, http://planning.cis.strath.ac.uk/competition/ IPC3, http://www.cs.toronto.
edu/aips2000/ IPC2.
3. object code LPG-TD downloaded http://zeus.ing.unibs.it/lpg/register-lpg-td.
html, object code YAHSP-1.1 downloaded http://www.cril.univ-artois.fr/
~vidal/Yahsp/yahsp.linux.x86.gz. object code Downward unavailable testing
time paper revised.

355

fiChen, Wah, & Hsu

IPC3 Settlers domain variants IPC3 Satellite domain
reported IPC4.
Table 3:
Performance comparison SGPlan4.1 planners.

table comparing SGPlan4.1 SGPlan4 , four missing variants (PIPESWORLDNOTANKAGE-TEMP-DEADLINES-CO, PROMELA-OPTICAL-TELEGRAPH-FLUENTS-DP,
PROMELA-PHILOSOPHERS-FLUENTS-DP, ROVERS-TIME) cannot solved
planners. table comparing SGPlan4.1 LPG-TD-SPEED, missing variants except ROVERS-TIME cannot solved LPG-TD-SPEED. ROVERS-TIME variant,
LPG-TD-SPEED solve instances planners cannot. tables
comparing SGPlan4.1 , Downward, YAHSP, missing variants cannot solved
target planners compared.
Domain Variant

Instances Solvable (Fb )
Fi
Fq
Ft
Fw Fwt Fwq

Fn

Instances
Fg
Fu
Fb

Comparison SGPlan4.1 SGPlan4
AIRPORT-NONTEMP
0.78 0.00
AIRPORT-TEMP
0.60 0.28
AIRPORT-TEMP-TIMEWINDOWS
0.48 0.14
AIRPORT-TEMP-TIMEWINDOWS-CO
0.28 0.00
PIPESWORLD-NOTANKAGE-NONTEMP
0.16 0.10
PIPESWORLD-NOTANKAGE-TEMP
0.72 0.28
PIPESWORLD-TANKAGE-NONTEMP
0.12 0.00
PIPESWORLD-TANKAGE-TEMP
0.52 0.14
PIPESWORLD-NOTANKAGE-TEMP-DEAD
0.00 0.00
PROMELA-OPTICAL-TELEGRAPH
0.19 0.00
PROMELA-OPTICAL-TELEGRAPH-DP
0.40 0.00
PROMELA-OPTICAL-TELEGRAPH-FLUENTS 0.06 0.00
PROMELA-PHILOSOPHERS
0.58 0.00
PROMELA-PHILOSOPHERS-DP
0.94 0.00
PROMELA-PHILOSOPHERS-FLUENTS
0.02 0.00
PSR-SMALL
0.24 0.00
PSR-MIDDLE
0.98 0.02
PSR-MIDDLE-CO
0.26 0.00
PSR-LARGE
0.16 0.00
SATELLITE-STRIPS
0.53 0.06
SATELLITE-TIME
0.39 0.44
SATELLITE-TIME-TIMEWINDOWS
0.58 0.00
SATELLITE-TIME-TIMEWINDOWS-CO
0.53 0.03
SATELLITE-NUMERIC
0.44 0.00
SATELLITE-COMPLEX
0.36 0.22
SATELLITE-COMPLEX-TIMEWINDOWS
0.50 0.14
SATELLITE-COMPLEX-TIMEWINDOWS-CO
0.56 0.03
SETTLERS
0.10 0.00
UMTS-TEMP
0.96 0.04
UMTS-TEMP-TIMEWINDOWS
0.88 0.12
UMTS-TEMP-TIMEWINDOWS-CO
0.76 0.00
UMTS-FLAW-TEMP
0.02 0.88
UMTS-FLAW-TEMP-TIMEWINDOWS
0.00 0.44
UMTS-FLAW-TEMP-TIMEWINDOWS-CO
0.54 0.00
DEPOTS-STRIPS
0.27 0.27
Continued . . .

356

0.00
0.00
0.16
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.05

0.00
0.00
0.06
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.08
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00

0.10
0.00
0.00
0.16
0.74
0.00
0.54
0.00
0.00
0.10
0.00
0.13
0.02
0.06
0.19
0.70
0.00
0.02
0.06
0.25
0.00
0.08
0.11
0.11
0.08
0.03
0.08
0.85
0.00
0.00
0.24
0.10
0.10
0.00
0.41

0.00
0.00
0.02
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.03
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00

0.00
0.00
0.00
0.02
0.00
0.00
0.00
0.00
0.27
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.46
0.46
0.00

0.00
0.00
0.02
0.04
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.79
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.03
0.06
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00

0.12
0.12
0.12
0.50
0.00
0.00
0.34
0.34
0.73
0.71
0.60
0.81
0.40
0.00
0.00
0.06
0.00
0.72
0.78
0.17
0.17
0.33
0.33
0.42
0.17
0.33
0.33
0.05
0.00
0.00
0.00
0.00
0.00
0.00
0.00

0.88
0.88
0.86
0.44
1.00
1.00
0.66
0.66
0.00
0.29
0.40
0.19
0.60
1.00
0.21
0.94
1.00
0.28
0.22
0.83
0.83
0.67
0.67
0.55
0.77
0.67
0.67
0.95
1.00
1.00
1.00
1.00
0.54
0.54
1.00

fiTemporal Planning using Subgoal Partitioning Resolution

Table 3: (continued)
Domain Variant
DEPOTS-SIMPLETIME
DEPOTS-TIME
DEPOTS-NUMERIC
DRIVERLOG-STRIPS
DRIVERLOG-SIMPLETIME
DRIVERLOG-TIME
DRIVERLOG-NUMERIC
DRIVERLOG-HARDNUMERIC
FREECELL-STRIPS
ROVERS-STRIPS
ROVERS-SIMPLETIME
ROVERS-NUMERIC
SATELLITE-SIMPLETIME
SATELLITE-HARDNUMERIC
ZENOTRAVEL-STRIPS
ZENOTRAVEL-SIMPLETIME
ZENOTRAVEL-TIME
ZENOTRAVEL-NUMERIC
BLOCKSWORLD

Instances Solvable (Fb )
Fi
Fq
Ft
Fw Fwt Fwq
0.23
0.27
0.18
0.70
0.60
0.45
0.60
0.55
0.05
0.70
0.55
0.45
0.75
0.50
0.85
0.80
0.45
0.65
0.57

0.68
0.59
0.27
0.10
0.20
0.35
0.15
0.20
0.10
0.00
0.45
0.05
0.00
0.00
0.00
0.20
0.55
0.00
0.29

0.05
0.05
0.05
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.06

0.00
0.05
0.00
0.00
0.00
0.00
0.00
0.00
0.05
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.03

0.00
0.00
0.41
0.00
0.00
0.00
0.05
0.05
0.70
0.30
0.00
0.10
0.25
0.20
0.15
0.00
0.00
0.35
0.06

Fn

Instances
Fg
Fu
Fb

0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00

0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00

0.05
0.05
0.09
0.10
0.10
0.05
0.05
0.05
0.10
0.00
0.00
0.25
0.00
0.00
0.00
0.00
0.00
0.00
0.00

0.00
0.00
0.00
0.10
0.10
0.15
0.15
0.15
0.00
0.00
0.00
0.15
0.00
0.30
0.00
0.00
0.00
0.00
0.00

0.95
0.95
0.91
0.80
0.80
0.80
0.80
0.80
0.90
1.00
1.00
0.60
1.00
0.70
1.00
1.00
1.00
1.00
1.00

0.00
0.00
0.00
0.04
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.03
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00

0.00
0.02
0.00
0.16
0.18
0.20
0.22
0.03
0.00
0.15
0.42
0.00
0.00
0.00
0.00
0.00
0.19
0.06
0.00
0.19
0.30
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00

0.02
0.02
0.04
0.00
0.00
0.08
0.06
0.53
0.00
0.00
0.00
0.00
0.04
0.00
0.17
0.06
0.06
0.08
0.06
0.06
0.00
0.00
0.00
0.00
0.00
0.00
0.05
0.05
0.05
0.20

0.10
0.10
0.10
0.00
0.00
0.26
0.28
0.20
0.71
0.60
0.40
0.00
0.02
0.00
0.00
0.11
0.28
0.36
0.17
0.28
0.05
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.05
0.00

0.88
0.86
0.86
0.84
0.82
0.46
0.44
0.24
0.29
0.25
0.18
1.00
0.94
1.00
0.83
0.83
0.47
0.50
0.77
0.53
0.65
1.00
1.00
1.00
1.00
1.00
0.95
0.95
0.90
0.80

Comparison SGPlan4.1 LPG-TD-SPEED
AIRPORT-NONTEMP
AIRPORT-TEMP
AIRPORT-TEMP-TIMEWINDOWS
PIPESWORLD-NOTANKAGE-NONTEMP
PIPESWORLD-NOTANKAGE-TEMP
PIPESWORLD-TANKAGE-NONTEMP
PIPESWORLD-TANKAGE-TEMP
PIPESWORLD-NOTANKAGE-TEMP-DEAD
PROMELA-OPTICAL-TELEGRAPH
PROMELA-OPTICAL-TELEGRAPH-DP
PROMELA-PHILOSOPHERS
PROMELA-PHILOSOPHERS-DP
PSR-SMALL
PSR-MIDDLE
SATELLITE-STRIPS
SATELLITE-TIME
SATELLITE-TIME-TIMEWINDOWS
SATELLITE-NUMERIC
SATELLITE-COMPLEX
SATELLITE-COMPLEX-TIMEWINDOWS
SETTLERS
UMTS-TEMP
UMTS-TEMP-TIMEWINDOWS
UMTS-FLAW-TEMP
UMTS-FLAW-TEMP-TIMEWINDOWS
DEPOTS-STRIPS
DEPOTS-SIMPLETIME
DEPOTS-TIME
DEPOTS-NUMERIC
DRIVERLOG-STRIPS

0.16 0.14
0.14 0.20
0.08 0.22
0.30 0.06
0.44 0.00
0.22 0.12
0.24 0.06
0.07 0.03
0.29 0.00
0.25 0.00
0.19 0.00
1.00 0.00
0.40 0.54
0.14 0.64
0.42 0.36
0.19 0.33
0.47 0.00
0.11 0.00
0.36 0.17
0.44 0.00
0.10 0.00
0.82 0.00
1.00 0.00
0.00 0.48
0.00 0.00
0.32 0.36
0.09 0.09
0.09 0.09
0.32 0.27
0.65 0.15
Continued . . .

357

0.00
0.02
0.00
0.26
0.36
0.10
0.12
0.03
0.00
0.00
0.00
0.00
0.00
0.00
0.03
0.17
0.00
0.33
0.17
0.03
0.55
0.18
0.00
0.00
0.00
0.05
0.27
0.09
0.05
0.00

0.10
0.28
0.30
0.16
0.02
0.00
0.02
0.07
0.00
0.00
0.00
0.00
0.00
0.06
0.00
0.08
0.00
0.06
0.08
0.00
0.00
0.00
0.00
0.12
0.00
0.18
0.50
0.68
0.27
0.00

0.48
0.22
0.26
0.02
0.00
0.02
0.00
0.03
0.00
0.00
0.00
0.00
0.00
0.16
0.03
0.03
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.40
1.00
0.09
0.00
0.00
0.00
0.00

fiChen, Wah, & Hsu

Table 3: (continued)
Instances Solvable (Fb )
Fi
Fq
Ft
Fw Fwt Fwq

Domain Variant
DRIVERLOG-SIMPLETIME
DRIVERLOG-TIME
DRIVERLOG-NUMERIC
DRIVERLOG-HARDNUMERIC
FREECELL-STRIPS
ROVERS-STRIPS
ROVERS-SIMPLETIME
ROVERS-NUMERIC
SATELLITE-SIMPLETIME
SATELLITE-HARDNUMERIC
ZENOTRAVEL-STRIPS
ZENOTRAVEL-SIMPLETIME
ZENOTRAVEL-TIME
ZENOTRAVEL-NUMERIC
BLOCKSWORLD

0.65
0.65
0.60
0.45
0.50
0.70
0.75
0.50
0.00
0.15
0.80
0.60
0.65
1.00
0.46

0.10
0.10
0.20
0.25
0.00
0.25
0.20
0.00
0.00
0.00
0.20
0.15
0.00
0.00
0.46

0.05
0.05
0.00
0.10
0.35
0.05
0.05
0.00
0.70
0.55
0.00
0.15
0.30
0.00
0.03

0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.10
0.25
0.00
0.00
0.05
0.05
0.00
0.03

0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00

Fn

Instances
Fg
Fu
Fb

0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.05
0.00
0.00
0.05
0.00
0.00
0.03

0.00
0.00
0.00
0.00
0.05
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00

0.20
0.20
0.15
0.20
0.10
0.00
0.00
0.40
0.00
0.00
0.00
0.00
0.00
0.00
0.00

0.00
0.00
0.05
0.00
0.00
0.00
0.00
0.00
0.00
0.30
0.00
0.00
0.00
0.00
0.00

0.80
0.80
0.80
0.80
0.85
1.00
1.00
0.60
1.00
0.70
1.00
1.00
1.00
1.00
1.00

0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00
0.00

0.00
0.00
0.32
0.29
0.04
0.60
0.00
0.00
0.00
0.00
0.00

0.12
0.02
0.06
0.00
0.38
0.00
0.00
0.06
0.00
0.40
0.17

0.00
0.00
0.28
0.71
0.23
0.40
0.00
0.00
0.00
0.38
0.00

0.88
0.98
0.34
0.00
0.35
0.00
1.00
0.94
1.00
0.22
0.83

0.00
0.06
0.00
0.00
0.00
0.00
0.03
0.00
0.00
0.00
0.00
0.05
0.06

0.18
0.00
0.04
0.02
0.00
0.00
0.00
0.14
0.00
0.00
0.40
0.00
0.00

0.02
0.00
0.24
0.00
0.00
0.02
0.17
0.00
0.20
0.05
0.00
0.00
0.00

0.10
0.00
0.10
0.71
0.40
0.04
0.00
0.00
0.00
0.05
0.00
0.00
0.00

0.70
1.00
0.62
0.27
0.60
0.94
0.83
0.86
0.80
0.90
0.60
1.00
1.00

Comparison SGPlan4.1 Downward
AIRPORT-NONTEMP
PIPESWORLD-NOTANKAGE-NONTEMP
PIPESWORLD-TANKAGE-NONTEMP
PROMELA-OPTICAL-TELEGRAPH
PROMELA-OPTICAL-TELEGRAPH-DP
PROMELA-PHILOSOPHERS
PROMELA-PHILOSOPHERS-DP
PSR-SMALL
PSR-MIDDLE
PSR-LARGE
SATELLITE-STRIPS

0.52
0.14
0.16
0.00
0.35
0.00
1.00
0.42
0.32
0.12
0.69

0.00
0.02
0.00
0.00
0.00
0.00
0.00
0.04
0.38
0.04
0.08

0.02
0.20
0.16
0.00
0.00
0.00
0.00
0.00
0.02
0.00
0.03

0.16
0.02
0.00
0.00
0.00
0.00
0.00
0.00
0.06
0.04
0.03

0.18
0.00
0.02
0.00
0.00
0.00
0.00
0.48
0.22
0.02
0.00

Comparison SGPlan4.1 YAHSP
AIRPORT-NONTEMP
PIPESWORLD-NOTANKAGE-NONTEMP
PIPESWORLD-TANKAGE-NONTEMP
PROMELA-OPTICAL-TELEGRAPH
PROMELA-PHILOSOPHERS
PSR-SMALL
SATELLITE-STRIPS
DEPOTS-STRIPS
DRIVERLOG-STRIPS
FREECELL-STRIPS
ROVERS-STRIPS
ZENOTRAVEL-STRIPS
BLOCKSWORLD

0.24
0.14
0.22
0.27
0.13
0.36
0.25
0.50
0.50
0.10
0.35
0.30
0.46

358

0.22
0.52
0.32
0.00
0.00
0.02
0.53
0.32
0.30
0.80
0.25
0.65
0.31

0.02
0.00
0.02
0.00
0.00
0.00
0.00
0.05
0.00
0.00
0.00
0.00
0.09

0.10
0.28
0.02
0.00
0.00
0.02
0.00
0.00
0.00
0.00
0.00
0.00
0.06

0.12
0.00
0.04
0.00
0.48
0.54
0.03
0.00
0.00
0.00
0.00
0.00
0.03

fiTemporal Planning using Subgoal Partitioning Resolution

Keys: (tn , qn )
(tg , qg )
Fb
Fi
Fq
Ft
Fw
Fwt
Fwq
Fn
Fg
Fu

(run time, quality) SGPlan4.1
(run time, quality) target planner compared
Fraction solved SGPlan4.1 target planner
(Fb = Fi + Fq + Ft + Fw + Fwt + Fwq = 1 Fn Fg Fu )
Fraction tn tg qn qg (SGPlan4.1 better run time quality)
Fraction tn > tg qn < qg (SGPlan4.1 worse run time better quality)
Fraction tn < tg qn > qg (SGPlan4.1 worse quality better run time)
Fraction tn > tg qn > qg (SGPlan4.1 worse run time worse quality)
Fraction tn > tg qn = qg (SGPlan4.1 worse run time quality)
Fraction tn = tg qn > qg (SGPlan4.1 worse quality run time)
Fraction solved SGPlan4.1 target planner
Fraction solved target planner SGPlan4.1
Fraction unsolved SGPlan4.1 target planner

Figures 17-20 plot time-quality trade-offs run time (resp. quality)
target planner normalized respect corresponding measure SGPlan4.1
instances solvable planners. graph, also list six percentages computed
normalizing Fi , Ft , Fq , Fw , Fwt , Fwq respect Fb (defined Table 3)
domains evaluated.
Airport domain, SGPlan4.1 improves performance SGPlan4
terms run time quality majority (69.9%) instances (Figure 17a).
NONTEMP variant, solution files (not shown) show SGPlan4.1 cannot solve
six (Fg + Fu = 0.12 Table 3) seven largest instances (number 44 50); whereas
Downward, leading planner variant, solve 50 instances. SGPlan4.1
difficulty instances partitioned subproblems large evaluated embedded Metric-FF planner. also reason SGPlan4.1
worse Downward LPG terms run time larger instances. obvious
solution employ efficient basic planner becomes available. fact,
one strengths partition-and-resolve approach. Another solution
partition subproblems reduce complexity extent
handled modified Metric-FF planner. design partitioning methods
still open time.
Pipesworld domain, SGPlan4.1 significant improvements SGPlan4
terms makespan NOTANKAGE-TEMP TANKAGE-TEMP variants (Figure 17b). improvements due minimization estimated makespan
(Te) (19). However, improvements found NOTANKAGE-NONTEMP
TANKAGE-NONTEMP variants (19) term corresponds
number actions non-temporal variants. respect planners, SGPlan4.1
solve instances NOTANKAGE-NONTEMP, NOTANKAGE-TEMP,
TANKAGE-TEMP variants (Fn Fg 0 corresponding rows Table 3),
consistently shortest solution time NOTANKAGE-TEMP TANKAGETEMP variants. NOTANKAGE-NONTEMP TANKAGE-NONTEMP variants, YAHSP, however, solve number instances shortest solution time cases, although tends produce longer plans. Last, discussed
Section 4.2, SGPlan4.1 competitive PIPESWORLD-NOTANKAGE-TEMPDEADLINE variant solve eight 30 instances.
359

fiChen, Wah, & Hsu

4

69.9%

8.5%

2.0%

5.2%

0.7%

0.25
0.25

1

1

38.6%

0.0%

0.25
0.01

4

Normalized quality

Normalized quality

81.4%

0.0%
18.6%

0.0%

0.001

0.0%

0.01

0.0%

0.1

1

1

SMALL
MIDDLE
LARGE
MIDDLE-CO

0.8%

67.2%

0.0%

0.0%

32.0%

0.0%

0.25
0.25

10

1

Normalized run time

c) PROMELA

d) PSR

Normalized quality

Normalized quality

4

16.2%

68.6%

13.2%
1.5%

STRIPS
TIME-TIMEWINDOW-CO
COMPLEX
COMPLEX-TIMEWINDOWS
TIME-TIMEWINDOWS
NUMERIC
TIME
COMPLEX-TIMEWINDOWS-CO

0.1
0.01

0.0%

0.5%

0.1

1

1

SETTLERS

10.5%

0.0%

0.0%

0.0%

0.25
0.1

10

1

10

Normalized run time

e) SATELLITE

f) SETTLERS
10

29.1%

Normalized quality

4

Normalized quality

0.0%

89.5%

Normalized run time

1

4

Normalized run time

10

1

10

b) PIPESWORLD
4

OPTICAL-TELEGRAPH-DP
OPTICAL-TELEGRAPH
PHILOSOPHERS
OPTICAL-TELEGRAPH-FLUENTS
PHILOSOPHERS-FLUENTS
PHILOSOPHERS-DP

0.25
1e-04

0.0%

1

Normalized run time

a) AIRPORT

1

0.0%

0.1

Normalized run time

4

45.8%

15.7%

Normalized quality

Normalized quality

13.7%

1

4
NOTANKAGE-NONTEMP
NOTANKAGE-TEMP
TANKAGE-TEMP
TANKAGE-NONTEMP

TEMP
NONTEMP
TEMP-TIMEWINDOWS-CO
TEMP-TIMEWINDOWS

62.2%

8.7%
0.0%

0.0%

TEMP
FLAW-TEMP-TIMEWINDOWS-CO
TEMP-TIMEWINDOWS-CO
TEMP-TIMEWINDOWS
FLAW-TEMP-TIMEWINDOWS
FLAW-TEMP

0.0%

0.25
0.1

1

1

34.5%

1.7%

5.0%

16.8%

DEPOTS-STRIPS
DEPOTS-SIMPLETIME
DEPOTS-TIME
DEPOTS-NUMERIC
BLOCKSWORLD

0.1
0.001

10

Normalized run time

42.0%

0.01

0.1

0.0%

1

10

100

1000

10000

Normalized run time

g) UMTS

h) DEPOTS & BLOCKSWORLD

Figure 17: Run time-quality SGPlan4 instance normalized respect
corresponding run time-quality SGPlan4.1 instance instances solvable planners. (Performance values larger one
better SGPlan4.1 .)
360

fiTemporal Planning using Subgoal Partitioning Resolution

TEMP
NONTEMP
TEMP-TIMEWINDOWS

21.5%

1

10

14.6%

Normalized quality

Normalized quality

4

36.9%

26.2%

0.8%

NOTANKAGE-NONTEMP
NOTANKAGE-TEMP
TANKAGE-TEMP
TANKAGE-NONTEMP
NOTANKAGE-TEMP-DEADLINES

46.9%

9.4%

1

1.6%
7.8%

0.25
0.01

0.0%

0.1

1

10

100

0.01

1.6%

0.1

1

Normalized run time

100.0%

0.0%

0.0%

0.0%

0.0%

1

1000

SMALL
MIDDLE

10

Normalized quality

Normalized quality

OPTICAL-TELEGRAPH-DP
OPTICAL-TELEGRAPH
PHILOSOPHERS
PHILOSOPHERS-DP

0.25
0.1

100

b) PIPESWORLD

0.0%

1

10

Normalized run time

a) AIRPORT
4

32.8%

10

100

1000

1

60.8%

27.8%

3.1%

0.0%

8.2%

1e-04

0.001

0.01

Normalized run time

0.0%

0.1

1

10

100

Normalized run time

c) PROMELA

d) PSR

4

4

SETTLERS

51.4%

1

Normalized quality

Normalized quality

22.1%

1.4%
18.6%
STRIPS
COMPLEX
COMPLEX-TIMEWINDOWS
TIME-TIMEWINDOWS
NUMERIC
TIME

5.7%

0.25
0.1

0.7%

1

10

100

1000

1

15.4%

0.0%

84.6%

0.0%

0.25
0.01

10000

0.0%

45.5%

Normalized quality

Normalized quality

100

1000

f) SETTLERS
28.6%

12.0%

1

35.0%
4.5%

3.0%

0.0%

0.1

10

10

TEMP
TEMP-TIMEWINDOWS
FLAW-TEMP-TIMEWINDOWS
FLAW-TEMP

0.25
0.01

1

Normalized run time

e) SATELLITE
4

0.0%

0.1

Normalized run time

1

10

100

Normalized run time

1

28.6%

1.7%
31.1%

9.2%

DEPOTS-STRIPS
DEPOTS-SIMPLETIME
DEPOTS-TIME
DEPOTS-NUMERIC
BLOCKSWORLD

0.1
0.001

0.01

0.1

0.8%

1

10

100

Normalized run time

g) UMTS

h) DEPOTS & BLOCKSWORLD

Figure 18: Run time-quality LPG-TD-SPEED instance normalized respect
corresponding run time-quality SGPlan4.1 instance
instances solvable planners. (Performance values larger one
better SGPlan4.1 ).
361

fiChen, Wah, & Hsu

0.0%

1

4

NONTEMP

59.1%

Normalized quality

Normalized quality

4

20.5%

18.2%

NOTANKAGE-NONTEMP
TANKAGE-NONTEMP

1.8%

1

1.8%

2.3%
0.0%

0.25
0.1

26.3%

1.8%

68.4%
0.0%

0.25

1

10

100

1

Normalized run time

a) AIRPORT
OPTICAL-TELEGRAPH-DP
PHILOSOPHERS-DP

0.0%

1

100.0%

0.0%

0.25
0.1

0.0%

0.0%

0.0%

1

10

100

1

SMALL
MIDDLE
LARGE

21.3%

39.8%

4.6%

0.9%

33.3%

0.25
1e-04

0.001

Normalized run time

Normalized quality

1

0.01

0.1

0.0%

1

10

100

Normalized run time

c) PROMELA
4

100

b) PIPESWORLD
4

Normalized quality

Normalized quality

4

10

Normalized run time

d) PSR

STRIPS

10.0%

83.3%

3.3%

3.3%

0.0%

0.25
0.1

0.0%

1

10

100

Normalized run time

e) SATELLITE

Figure 19: Run time-quality Downward instance normalized respect
corresponding run time-quality SGPlan4.1 instance
instances solvable planners. (Performance values larger one
better SGPlan4.1 .)

Promela domain, SGPlan4.1 improvements SGPlan4 terms quality
improves terms run time instances solve four six variants
(worse OPTICAL-TELEGRAPH-FLUENTS PHILOSOPHERS-FLUENTS vari362

fiTemporal Planning using Subgoal Partitioning Resolution

4

NONTEMP

NOTANKAGE-NONTEMP
TANKAGE-NONTEMP

31.4%

1

34.3%

Normalized quality

Normalized quality

10

17.1%

14.3%

2.9%

0.0%

0.25
0.01

0.1

1

10

100

1

51.9%

22.2%

18.5%

1.2%

2.5%

0.001

0.01

0.1

Normalized run time

OPTICAL-TELEGRAPH
PHILOSOPHERS

45.2%

Normalized quality

Normalized quality

1

54.8%

0.0%

0.0%

0.0%

0.25
0.1

1

10

1

38.3%

2.1%

0.0%

57.4%

0.25
0.001

100

2.1%

0.01

10

d) PSR
10

63.3%

30.0%

Normalized quality

Normalized quality

1

Normalized run time

STRIPS

3.3%

0.25
0.01

0.0%

0.1

c) PROMELA

1

1000

SMALL

Normalized run time
4

100

b) PIPESWORLD
4

0.0%

10

Normalized run time

a) AIRPORT
4

3.7%

1

0.0%

0.1

0.0%

3.3%

1

10

100

Normalized run time

1

33.3%

50.0%

3.7%

7.4%

1.9%

DEPOTS-STRIPS
BLOCKSWORLD
0.1
0.001
0.01

0.1

3.7%

1

10

100

1000

Normalized run time

e) SATELLITE

f) DEPOTS & BLOCKSWORLD

Figure 20: Run time-quality YAHSP instance normalized respect corresponding run time-quality SGPlan4.1 instance instances
solvable planners. (Performance values larger one better
SGPlan4.1 .)

ants). SGPlan4.1 solve number instances OPTICAL-TELEGRAPHFLUENTS, PHILOSOPHERS, PHILOSOPHERS-DP, PHILOSOPHERS-FLUENTS
363

fiChen, Wah, & Hsu

variants compared LPG-TD-SPEED, Downward, YAHSP. Further,
fastest planner three variants slightly slower YAHSP PHILOSOPHERS variant (Figures 18c, 19c, 20c). OPTICAL-TELEGRAPH OPTICALTELEGRAPH-DP variants, organizer IPC4 provided two versions, one written
pure STRIPS another ADL. However, 14 (resp., 19) instances
STRIPS 48 (resp., 48) instances ADL OPTICAL-TELEGRAPH (resp.,
OPTICAL-TELEGRAPH-DP) variant. instances available ADL ADL space-efficient problem representation, whereas instances STRIPS
require large files. (For example, file size OPTICAL-TELEGRAPH-14 38 Kbytes
ADL 8.3 Mbytes STRIPS.) Since SGPlan4.1 SGPlan4 cannot handle ADL
time, solved instances pure STRIPS two variants.
able solve instances available STRIPS fastest instances.
However, Downward handle instances ADL able solve instances
two variants. plan extend SGPlan4.1 directly support ADL future.
Note SGPlan4.1 SGPlan4 always find plans better quality
instances solved OPTICAL-TELEGRAPH, OPTICAL-TELEGRAPH-DP,
PHILOSOPHERS, PHILOSOPHERS-DP variants compared three
planners (Edelkamp & Hoffmann, 2004).
SGPlan4.1 planner solve instances four variants
PSR domain. Since PSR pure propositional domain, SGPlan4.1 unable improve
solution quality SGPlan4 . Nevertheless, quality SGPlan4.1 consistently better
three planners (Fi +Fq +Fwt > Ft +Fw +Fwq corresponding rows
Table 3). SMALL variant, SGPlan4.1 LPG comparable run times
cannot solve largest instances. Like AIRPORT domain, SGPlan4.1 difficulty
largest instances basic planner cannot handle partitioned
subproblems. MIDDLE variant, SGPlan4.1 , LPG, Downward solve 50
instances. situation MIDDLE-CO LARGE variants similar
OPTICAL-TELEGRAPH OPTICAL-TELEGRAPH-DP variants Promela
domain. variants, Downward handle directly ADL format, SGPlan4.1
must expand ADL syntax pure STRIPS exhausted memory evaluating
larger instances. plan address issue future.
Satellite domain, SGPlan4.1 significant improvements quality SGPlan4 .
fact, SGPlan4.1 generates solutions better quality planners
instances solve number instances seven variants. eighth
variant (TIME), able solve largest instances memory
usage exceeded 1 Gbytes. variants except STRIPS, SGPlan4.1 faster
three planners. STRIPS variant, YAHSP fastest generate
multiple actions instead single action search step. However, finds slightly
longer plans compared SGPlan4.1 .
Settlers domain, SGPlan4.1 improve solution quality SGPlan4
because, discussed earlier, (19) term corresponds number
actions non-temporal variants. SGPlan4.1 solve instances except eighth
instance, learned IPC4 organizers infeasible instance.
also fastest among planners, generates longer plans LPGTD-SPEED. due iterative scheme reducing producible resources.
364

fiTemporal Planning using Subgoal Partitioning Resolution

Table 4: Summary number instances solved five planners compared (? means
clear whether domain solved object code
available testing, means planner support
language features benchmark.)
Domain
SGPlan4.1 SGPlan4 LPG-TD-SPEED Downward YAHSP
Airport
154
156
134
50
36
Pipesworld
174
166
158
60
93
Promela
129
167
83
83
42
PSR
122
122
99
131
48
IPC4
Satellite
204
207
157
36
36
Settlers
19
19
13


UMTS
300
254
200


Total
1102
1091
844
360
219
Depots
84
88
87
?
19
DriverLog
80
87
99
?
20
FreeCell
18
20
19
?
19
Rovers
52
57
80
?
12
IPC3
Satellite
34
34
34


ZenoTravel
80
80
80
?
20
Total
348
366
399
?
90
IPC2 Blocksworld
35
35
35
?
35
Overall
1485
1492
1243
360
344
optimal amount resources cannot found ahead time, SGPlan4.1 may incur
redundant actions producing unused resources.
UMTS domain, SGPlan4.1 solve instances six variants
fastest four them. Moreover, makespans greatly improved
SGPlan4 incorporating Te modified heuristic function Metric-FF, although
improvements makespan LPG-TD-SPEED small variants. SGPlan4.1 ,
however, slower LPG-TD-SPEED FLAW FLAW-TIL variants. performance degradation variants attributed flawed actions lead
overly optimistic heuristic values relaxed-plan-based planners (Edelkamp & Hoffmann,
2004) like Metric-FF.
IPC3 Depots domain, SGPlan4.1 better quality LPG-TD-SPEED
YAHSP STRIPS NUMERIC variants, whereas makespan SGPlan4.1
worse LPG-TD-SPEED majority instances TIME
SIMPLETIME variants. LPG-TD-SPEED also faster SGPlan4.1 majority
instances (Fq + Fw + Fwt > Fi + Ft + Fwq corresponding rows Table 3).
Due large fraction initial active global constraints, performance subgoal
partitioning SGPlan4.1 unsatisfactory domain.
remaining IPC3 domains, SGPlan4.1 generally improves SGPlan4 quality
besides Freecell domain STRIPS. Except Satellite domain
LPG-TD-SPEED performs better, SGPlan4.1 generates solutions better quality
365

fiChen, Wah, & Hsu

instances. Further, SGPlan4.1 faster LPG-TD-SPEED
half instances, although difference run times among planners
relatively easy instances usually insignificant.
Blocksworld domain, SGPlan4.1 generally finds solutions smaller number
actions SGPlan4 , LPG-TD-SPEED, YAHSP. However, SGPlan4.1 much
slower LPG-TD-SPEED many instances needs time resolving
large fraction initial active global constraints (Figure 18h).

8. Conclusions Future Work
presented paper partition-and-resolve approach application
SGPlan4 , planner first prize Suboptimal Temporal Metric Track
second prize Suboptimal Propositional Track IPC4. Table 4 summarizes
number instances solved top planners IPC4 well SGPlan4.1 . results
show constraint partitioning employed planners effective solving majority
problems two competitions.
approach based observation fraction active mutex constraints
across subgoals majority instances IPC3 IPC4 small. observation allows us partition search largely independent subproblems limit
amount backtracking resolving violated global constraints across subproblems. improvements also attributed combination techniques introduced
reducing search space handling new features PDDL2.2.
future, plan study partitioning techniques better exploit
constraint structure planning domains. particular, study fine-grain partitioning
order address cases larger fraction global constraints, develop search
strategies solving problems difficult-to-satisfy global constraints deadlines.
also plan extend method planning uncertainty support
expressive modeling language features.

Acknowledgments
research paper supported National Science Foundation Grant IIS 03-12084.

References
Blum, A. L., & Furst, M. L. (1997). Fast planning planning graph analysis. Artificial
Intelligence, 90, 281300.
Bonet, B., & Geffner, H. (2001). Planning heuristic search. Artificial Intelligence, Special
issue Heuristic Search, 129 (1).
Chen, Y., & Wah, B. W. (2003). Automated planning scheduling using calculus variations discrete space. Proc. Intl Conf. Automated Planning Scheduling,
pp. 211.
Chien, S., Rabideau, G., Knight, R., Sherwood, R., Engelhardt, B., Mutz, D., Estlin, T.,
Smith, B., Fisher, F., Barrett, T., Stebbins, G., & Tran, D. (2000). ASPEN - Au366

fiTemporal Planning using Subgoal Partitioning Resolution

tomating space mission operations using automated planning scheduling. Proc.
SpaceOps. Space Operations Organization.
Doherty, P., & Kvarnstrm, J. (1999). Talplanner: empirical investigation temporal
logic-based forward chaining planner.. Proc. Sixth Intl Workshop Temopral
Logic-based Forword Chaining Planner, pp. 4754. AIPS.
Edelkamp, S. (2002). Mixed propositional numerical planning model checking
integrated planning system. Proc. Workshop Planning Temporal Domains.
AIPS.
Edelkamp, S. (2003). Pddl2.2 planning model checking integrated environment.
UK Planning Scheduling Special Interest Group (PlanSig). Glasgow.
Edelkamp, S., & Hoffmann, J. (2004). Classical part, 4th international planning competition.
http://ls5-www.cs.uni-dortmund.de/~edelkamp/ipc-4/.
Foulser, D. E., Li, M., & Yang, Q. (1992). Theory algorithms plan merging.. Artificial
Intelligence, 57 (2-3), 143181.
Fourman, M. P. (2000). Propositional planning. Proc. Workshop Model Theoretic
Approaches Planning. AIPS.
Garrido, A., Fox, M., & Long, D. (2002). temporal planning system durative actions
pddl2.1. Proc. European Conf. Artificial Intelligence, pp. 586590.
Gerevini, A., & Serina, I. (2002). LPG: planner based local search planning graphs
action costs. Proc. Sixth Int. Conf. AI Planning Scheduling, pp.
1222. Morgan Kaufman.
Hanks, S., & Weld, D. S. (1995). domain-independent algorithm plan adaptation.. J.
Artificial Intelligence Research, 2, 319360.
Helmert, M., & Richter, S. (2004). Fast downward - making use causal dependencies
problem representation. Proc. IPC4, ICAPS, pp. 4143.
Hoffmann, J. (2003). metric-ff planning system: Translating ignoring delete lists
numeric state variables. Journal Artificial Intelligence Research, 20, 291341.
Hoffmann, J., & Nebel, B. (2001). FF planning system: Fast plan generation
heuristic search. J. Artificial Intelligence Research, 14, 253302.
Jonsson, A. K., Morris, P. H., Muscettola, N., & Rajan, K. (2000). Planning interplanetary space: Theory practice. Proc. 2nd Intl NASA Workshop Planning
Scheduling Space. NASA.
Kambhampati, S., & Hendler, J. A. (1992). validation-structure-based theory plan
modification reuse.. Artificial Intelligence, 55 (2), 193258.
Kautz, H., & Selman, B. (1996). Pushing envelope: planning, propositional logic,
stochastic search. Proc. 13th National Conference Artificial Intelligence, pp.
11941201. AAAI.
Kautz, H., & Selman, B. (1999). Unifying SAT-based graph-based planning. Proc.
Intl Joint Conf. Artificial Intelligence. IJCAI.
367

fiChen, Wah, & Hsu

Kautz, H., & Walser, J. P. (2000). Integer optimization models AI planning problems.
Knowledge Engineering Review, 15 (1), 101117.
Koehler, J., & Hoffmann, J. (2000). reasonable forced goal ordering use
agenda-driven planning algorithm. J. AI Research, 12, 339386.
Lin, F. (2001). planner called R. AI Magazine, 7376.
Long, D., & Fox, M. (1998). Efficient implementation plan graph STAN. J. AI
Research.
Nau, D., Muoz-Avila, H., Cao, Y., Lotem, A., & Mitchell, S. (2001). Total-order planning
partially ordered subtasks. Proc. Intl Joint Conf. Artificial Intelligence,
pp. 425430. IJCAI.
Nebel, B., Dimopoulos, Y., & Koehler, J. (1997). Ignoring irrelevant facts operators
plan generation. Proc. European Conf. Planning, pp. 338350.
Nebel, B., & Koehler, J. (1995). Plan reuse versus plan generation: theoretical
empirical analysis.. Artificial Intelligence, 76 (1-2), 427454.
Nigenda, R. S., Nguyen, X., & Kambhampati, S. (2000). AltAlt: Combining advantages
Graphplan heuristic state search. Tech. rep., Arizona State University.
Penberethy, J., & Weld, D. (1992). UCPOP: sound, complete, partial order planner
ADL. Proc. 3rd Intl Conf. Principles Knowledge Representation
Reasoning, pp. 103114. KR Inc.
Penberethy, J., & Weld, D. (1994). Temporal planning continuous change. Proc.
12th National Conf. AI, pp. 10101015. AAAI.
Porteous, J., Sebastia, L., & Hoffmann, J. (2001). extraction, ordering, usage
landmarks planning. Proc. European Conf. Planning, pp. 3748.
Refanidis, I., & Vlahavas, I. (2001). GRT planner. AI Magazine, 6366.
Refanidis, I., & Vlahavas, I. (2002). MO-GRT system: Heuristic planning multiple
criteria. Proc. Workshop Planning Scheduling Multiple Criteria. AIPS.
Subbarao, M. B. D., & Kambhampati, S. (2002). Sapa: domain-independent heuristic
metric temporal planner. Tech. rep., Arizona State University.
Tate, A., Drabble, B., & Kirby, R. (1994). O-Plan2: open architecture command,
planning control. Intelligent Scheduling, 213239.
Tsamardinos, I., Pollack, M. E., & Horty, J. F. (2000). Merging plans quantitative
temporal constraints, temporally extended actions, conditional branches.. Proc.
Intl Conf. AI Planning Scheduling (AIPS), pp. 264272.
Wah, B., & Chen, Y. (2006). Constraint partitioning penalty formulations solving
temporal planning problems. Artificial Intelligence, 170 (3), 187231.
Wah, B. W., & Chen, Y. (2003). Partitioning temporal planning problems mixed space
using theory extended saddle points. Proc. IEEE Intl Conf. Tools
Artificial Intelligence, pp. 266273.
368

fiTemporal Planning using Subgoal Partitioning Resolution

Wah, B. W., & Chen, Y. (2004). Subgoal partitioning global search solving temporal
planning problems mixed space. Intl J. Artificial Intelligence Tools, 13 (4), 767
790.
Wilkins, D. (1990). AI planners solve practical problems?. Computational Intelligence,
232246.
Wolfman, S., & Weld, D. (2000). Combining linear programming satisfiability solving
resource planning. Knowledge Engineering Review, 15 (1).
Yang, Q. (1997). Intelligent planning: decomposition abstraction based approach.
Springer-Verlag, London, UK.

369

fiJournal Artificial Intelligence Research 26 (2006) 101-126

Submitted 8/05; published 5/06

Domain Adaptation Statistical Classifiers
Hal Daume III
Daniel Marcu

hdaume@isi.edu
marcu@isi.edu

Information Sciences Institute
University Southern California
4676 Admiralty Way, Suite 1001
Marina del Rey, CA 90292 USA

Abstract
basic assumption used statistical learning theory training data
test data drawn underlying distribution. Unfortunately, many
applications, in-domain test data drawn distribution related,
identical, out-of-domain distribution training data. consider
common case labeled out-of-domain data plentiful, labeled in-domain data
scarce. introduce statistical formulation problem terms simple mixture
model present instantiation framework maximum entropy classifiers
linear chain counterparts. present efficient inference algorithms special
case based technique conditional expectation maximization. experimental
results show approach leads improved performance three real world tasks
four different data sets natural language processing domain.

1. Introduction
generalization properties current statistical learning techniques predicated
assumption training data test data come underlying
probability distribution. Unfortunately, many applications, assumption inaccurate.
often case plentiful labeled data exists one domain (or coming one
distribution), one desires statistical model performs well another related,
identical domain. Hand labeling data new domain costly enterprise, one
often wishes able leverage original, out-of-domain data building model
new, in-domain data. seek eliminate annotation in-domain
data, instead seek minimize amount new annotation effort required achieve
good performance. problem known domain adaptation transfer.
paper, present novel framework understanding domain adaptation
problem. key idea framework treat in-domain data drawn
mixture two distributions: truly in-domain distribution general domain
distribution. Similarly, out-of-domain data treated drawn mixture
truly out-of-domain distribution general domain distribution. apply
framework context conditional classification models conditional linear-chain
sequence labeling models, inference may efficiently solved using technique
conditional expectation maximization. apply model four data sets varying degrees divergence in-domain out-of-domain data obtain
c
2006
AI Access Foundation. rights reserved.

fiDaume III & Marcu

predictive accuracies higher large number baseline systems second
model proposed literature problem.
domain adaptation problem arises frequently natural language processing domain, millions dollars spent annotating text resources
morphological, syntactic semantic information. However, resources
based text news domain (in cases, Wall Street Journal). sort
language appears text Wall Street Journal highly specialized is,
circumstances, poor match domains. instance,
recent surge interest performing summarization (Elhadad, Kan, Klavans, & McKeown, 2005) information extraction (Hobbs, 2002) biomedical texts, summarization
electronic mail (Rambow, Shrestha, Chen, & Lauridsen, 2004), information extraction
transcriptions meetings, conversations voice-mail (Huang, Zweig, & Padmanabhan,
2001), among others. Conversely, machine translation domain, parallel
resources machine translation system depend parameter estimation drawn
transcripts political meetings, yet translation systems often targeted news
data (Munteanu & Marcu, 2005).

2. Statistical Domain Adaptation
multiclass classification problem, one typically assumes existence training set
= {(xn , yn ) X : 1 n N }, X input space finite set.
assumed (xn , yn ) drawn fixed, unknown base distribution p
training set independent identically distributed, given p. learning problem
find function f : X obtains high predictive accuracy (this typically
done either explicitly minimizing regularized empirical error, maximizing
probabilities model parameters).
2.1 Domain Adaptation
context domain adaptation, situation becomes complicated. assume
given two sets training data, (o) D(i) , out-of-domain indomain data sets, respectively. longer assume single fixed,
known distribution drawn, rather assume (o) drawn
distribution p(o) D(i) drawn distribution p(i) . learning problem
find function f obtains high predictive accuracy data drawn p (i) . (Indeed,
model turn symmetric respect (i) D(o) , contexts
consider obtaining good predictive model (i) makes intuitive sense.)
assume |D (o) | = N (o) |D(i) | = N (i) , typically N (i) N (o) .
before, assume N (o) out-of-domain data points drawn iid p(o)
N (i) in-domain data points drawn iid p(i) .
Obtaining good adaptation model requires careful modeling relationship
p(i) p(o) . two distributions independent (in obvious intuitive
sense), out-of-domain data (o) useless building model p(i) may
well ignore it. hand, p(i) p(o) identical, adaptation
necessary simply use standard learning algorithm. practical problems,
though, p(i) p(o) neither identical independent.
102

fiDomain Adaptation Statistical Classifiers

2.2 Prior Work
relatively little prior work problem, nearly focused
specific problem domains, n-gram language models generative syntactic parsing
models. standard approach used treat out-of-domain data prior knowledge
estimate maximum posterior values model parameters prior
distribution. approach applied successfully language modeling (Bacchiani
& Roark, 2003) parsing (Roark & Bacchiani, 2003). Also parsing domain, Hwa
(1999) Gildea (2001) shown simple techniques based using carefully chosen
subsets data parameter pruning improve performance adapted
parser. models assume data distribution p (D | ) parameters prior
distribution parameters p ( | ) hyper-parameters . estimate
hyperparameters out-of-domain data find maximum posteriori
parameters in-domain data, prior fixed.
context conditional discriminative models, domain adaptation
work aware model Chelba Acero (2004). model
uses out-of-domain data estimate prior distribution, context
maximum entropy model. Specifically, maximum entropy model trained
out-of-domain data, yielding optimal weights problem. weights used
mean weights Gaussian prior learned weights in-domain data.
Though effective experimentally, practice estimating prior distribution
out-of-domain data fixing estimation in-domain data leaves much
desired. Theoretically, strange estimate fix prior distribution data;
made apparent considering form models. Denoting in-domain data
parameters (i) , respectively, out-of-domain data parameters
D(o) , obtain following form prior estimation models:




= arg max p | arg max p () p




(o)



(i)
|
p |

(1)

One would difficult time rationalizing optimization problem anything
experimental performance. Moreover, models unusual
treat in-domain data out-of-domain data identically. Intuitively,
difference two sets data; simply come different, related distributions.
Yet, prior-based models highly asymmetric respect two data sets.
also makes generalization one domain data set difficult. Finally,
see, model propose paper, alleviates problems,
outperforms experimentally.
second generic approach domain adaptation problem build
domain model use predictions features domain data.
successfully used context named entity tagging (?). approach attractive
makes assumptions underlying classifier; fact, multiple classifiers
used.
103

fiDaume III & Marcu

2.3 Framework
paper, propose following relationship in-domain out-ofdomain distributions. assume instead two underlying distributions,
actually three underlying distributions, denote q (o) , q (g) q (i) .
consider p(o) mixture q (o) q (g) , consider p(i) mixture q (i)
q (g) . One intuitively view q (o) distribution distribution data truly
out-of-domain, q (i) distribution data truly in-domain q (g) distribution
data general domains. Thus, knowing q (g) q (i) sufficient build
model in-domain data. out-of-domain data help us providing
information q (g) available considering in-domain data.
example, part-of-speech tagging, assignment tag determiner (DT)
word likely general decision, independent domain. However,
Wall Street Journal, monitor almost always verb (VB), technical documentation
likely noun. q (g) distribution account case the/DT,
q (o) account monitor/VB q (i) account monitor/NN.

3. Domain Adaptation Maximum Entropy Models
domain adaptation framework outlined Section 2.3 completely general
applied statistical learning model. section apply loglinear conditional maximum entropy models linear chain counterparts, since
models proved quite effective many learning tasks. first review maximum
entropy framework, extend domain adaptation problem; finally
discuss domain adaptation linear chain maximum entropy models.
3.1 Maximum Entropy Models
maximum entropy framework seeks conditional distribution p (y | x) closest
(in sense KL divergence) uniform distribution also matches set training data respect feature function expectations (Della Pietra, Della Pietra, &
Lafferty, 1997). introducing one Lagrange multiplier feature function fi ,
optimization problem results probability distribution form:
p (y | x ; ) =

1
Z,x

h

exp > f (x, y)

(2)

P
Here, u> v denotes scalar product two vectors u v, given by: u> v = ui vi .
normalization constant Eq (2), Z,x , obtained summing exponential
possible classes 0 Y. probability distribution also known exponential
distribution Gibbs distribution. learning (or optimization) problem find
vector maximizes likelihood Eq (2). practice, prevent over-fitting, one
typically optimizes penalized (log) likelihood, isotropic Gaussian prior mean
0 covariance matrix 2 placed parameters (Chen & Rosenfeld, 1999).
graphical model standard maximum entropy model depicted left
Figure 1. figure, circular nodes correspond random variables square nodes
104

fiDomain Adaptation Statistical Classifiers

correspond fixed variables. Shaded nodes observed training data empty
nodes hidden unobserved. Arrows denote conditional dependencies.
general, feature functions f (x, y) may arbitrary real-valued functions; however,
paper restrict attention binary features. practice, harsh
restriction: many problems natural language domain naturally employ binary
features (for real valued features, binning techniques applied). Additionally,
notational convenience, assume features fi (x, y) written product
form gi (y)hi (x) arbitrary binary functions g outputs binary features h
inputs. latter assumption means consider x binary vector
xi = hi (x); following simplify notation significantly (the extension full
case straightforward, messy, therefore considered remainder
paper). considering x vector, may move class dependence parameters
consider matrix y,i weight hi class y. write
refer column vector corresponding class y. x also considered
column vector, write > x shorthand dot product x weights
class y. modified notation, may rewrite Eq (2) as:
p (y | x ; ) =

1
Z,x

h

exp > x

(3)

Combining Gaussian prior weights, obtain following form
log posterior data set:


N

h
X
X
1
yn > xn log
l = log p ( | D, ) = 2 > +
exp y0 > xn + const
2
0
n=1

(4)



parameters estimated using convex optimization technique; practice,
limited memory BFGS (Nash & Nocedal, 1991; Averick & More, 1994) seems good
choice (Malouf, 2002; Minka, 2003) use algorithm experiments
described paper. order perform calculations, one must able compute
gradient Eq (4) respect , available closed form.
3.2 Maximum Entropy Genre Adaptation Model
Extending maximum entropy model account in-domain out-of-domain
data framework described earlier requires addition several extra model param(i) (i)
eters. particular, in-domain data point (xn , yn ), assume existence
(i)
(i)
(i) (i)
binary indicator variable zn . value zn = 1 indicates (xn , yn ) drawn q (i)
(i)
(the truly in-domain distribution), value zn = 0 indicates drawn q (g)
(o) (o)
(the general-domain distribution). Similarly, out-of-domain data point (x n , yn ),
(o)
(o)
assume binary indicator variable zn , zn = 1 means data point drawn
(o)
q (the truly out-of-domain distribution) value 0 means drawn
q (g) (the general-domain distribution). course, indicator variables
observed data, must infer values automatically.
105

fiDaume III & Marcu

2

2





g

yni

yn
xn

xni

zni

N





yno

N

xno









zno



g





N





Figure 1: (Left) standard logistic regression model; (Right) Mega Model.
According model, zn binary random variables assume
drawn Bernoulli distribution parameter (i) (for in-domain) (o) (for outof-domain). Furthermore, assume three vectors, (i) , (o) (g)
corresponding q (i) , q (o) q (g) , respectively. instance, zn = 1, assume
(i)
xn classified using (i) . Finally, model binary vectors xn (respec(o)
tively xn s) drawn independently Bernoulli distributions parameterized
(i)
(g) (respectively, (o) (g) ). Again, zn = 1, assume xn
drawn according (i) . corresponds nave Bayes assumption generative
probabilities xn vectors. Finally, place common Beta prior nave Bayes
parameters, . Allowing range {i, o, g}, full hierarchical model is:
()

f | a, b
(i)
zn | (i)
(i)

(i)

(i)

(i)

(i)

(g)

xnf | zn , f , f
(i)

() | 2
(o)
zn | (o)

Bet(a, b)
Ber( (i) )
z (i)

Ber( fn )

(i)

(i)

yn | xn , zn , (i) , (g) Gibbs(xn , zn )

(o)

(o)

(o)

(o)

(o)

(g)

xnf | zn , f , f
(o)

Nor(0, 2 I)
Ber( (o) )

(5)

z (o)

Ber( fn )

(o)

(o)

yn | xn , zn , (o) , (g) Gibbs(xn , zn )

term model Maximum Entropy Genre Adaptation Model (the Mega
Model). corresponding graphical model shown right Figure 1. generative story in-domain data point x(i) follows:
1. Select whether x(i) truly in-domain general-domain indicate
z (i) {i, g}. Choose z (i) = probability (i) z (i) = g probability
1 (i) .
(i)

2. component f x(i) , choose xf 1 probability zf
z (i)

probability 1 f .
(i)

3. Choose class according Eq (3) using parameter vector z .
106

(i)

0

fiDomain Adaptation Statistical Classifiers

story out-of-domain data points identical, uses truly out-of-domain
general-domain parameters, rather truly in-domain parameters generaldomain parameters.
3.3 Linear Chain Models
straightforward extension maximum entropy classification model maximum
entropy Markov model (MEMM) (McCallum, Freitag, & Pereira, 2000) obtained
assuming targets yn sequences labels. canonical example model
part speech tagging: word sequence assigned part speech tag.
introducing first order Markov assumption tag sequence, one obtains linear chain
model viewed discriminative counterpart standard (generative)
hidden Markov model. parameters models estimated using
limited memory BFGS. extension Mega Model linear chain framework
similarly straightforward, assumption label (part speech tag)
indicator variable z (versus global indicator variable z entire tag sequence).
techniques described herein may also applied conditional random field
framework Lafferty, McCallum, Pereira (2001), fixes bias problem
MEMM performing global normalization rather per-state normalization. is,
however, subtle difficulty direct application CRFs. Specifically, one would need
decide single z variable would assigned entire sentence, word
individually. MEMM case, natural one z per word. However,
CRF would computationally expensive. remainder, continue
use MEMM model efficiency purposes.

4. Conditional Expectation Maximization
Inference Mega Model slightly complex standard maximum entropy models. However, inference solved efficiently using conditional expectation
maximization (CEM), variant standard expectation maximization (EM) algorithm
(Dempster, Laird, & Rubin, 1977), due Jebara Pentland (1998). high level, EM
useful computing generative models hidden variables, CEM useful
computing discriminative models hidden variables; Mega Model belongs
latter family, CEM appropriate choice.
standard EM family algorithms maximizes joint likelihood data.
particular, (xn , yn )N
n=1 data z (discrete) hidden variable, M-step EM
proceeds maximizing bound given Eq (6)
log p (x, | ) = log

X
z

p (z, x, | ) = log Ezp( | x;) p (x, | z; )

(6)

Eq (6), Ez denotes expectation. One may apply Jensens inequality
equation, states f (E{x}) E{f (x)} whenever f convex. Taking f = log,
able decompose log expectation expectation log. typically
separates terms makes taking derivatives solving resolution optimization problem tractable. Unfortunately, EM cannot directly applied conditional models (such
107

fiDaume III & Marcu

Mega Model) form Eq (7) models result M-step
requires maximization equation form given Eq (8).
log p (y | x; ) = log
l = log

X
z

X
z

p (z, | x; ) = log Ezp( | x,) p (y | x, z; )
p (z, x, | ) log

X
z

p (z, x | )

(7)
(8)

Jensens inequality applied first term Eq (8), maximized
readily standard EM. However, applying Jensens inequality second term would
lead upper bound likelihood, since term appears negated.
conditional EM solution (Jebara & Pentland, 1998) bound change
log-likelihood iterations, rather log-likelihood itself. change loglikelihood written Eq (9), denotes parameters iteration t.


lc = log p | x; log p | x; t1

(9)

rewriting conditional distribution p (y | x) p (x, y) divided p (x),
express lc log joint distribution difference minus log marginal
distribution. Here, apply Jensens inequality first term (the joint difference),
second (because appears negated). Fortunately, Jensens
bound employ. standard variational upper bound logarithm function is:
log x x 1; leads lower bound negation, exactly desired.
bound attractive reasons: (1) tangent logarithm; (2) tight;
(3) makes contact current operating point (according maximization
previous time step); (4) simply linear function; (5) terminology
calculus variations, variational dual logarithm; see (Smith, 1998).
Applying Jensens inequality first term Eq (9) variational dual
second term, obtain change log-likelihood moving model parameters
t1 time 1 time (which shall denote Qt ) bounded l Qt ,
Qt defined Eq (10), h = E{z | x; } z = 1 1 E{z | x; }
z = 0, expectations taken respect parameters previous iteration.


P

X
p z, x, |
z p z, x |

+1
(10)
Q =
hz log
P
t1 )
p (z, x, | t1 )
z p (z, x |
zZ

applying two bounds (Jensens inequality variational bound),
removed sums logs, hard deal analytically. full derivation
given Appendix A. remaining expression lower bound change likelihood,
maximization result maximization likelihood.
MAP variant standard EM, change E-step priors
placed parameters. assumption standard EM wish maximize
p ( | x, y) p () p (y | , x) prior probability ignored, leaving
likelihood term parameters given data. MAP estimation, make
assumption instead use true prior p (). so, need add factor
log p () definition Qt Eq (10).
108

fiDomain Adaptation Statistical Classifiers

t1
jn,z
n

mt1
n


= log p xn , yn , zn | t1
n,zn

P
t1 1
=
n,zn ,f 0
z n p x n , zn |



xnf
1xnf
zn
zn

1


f =1
f
f

xnf
1xnf
Q
zn
zn

=
1


0
f 6=f
f
f

=

QF

Table 1: Notation used Mega Model equations.
important note although make use full joint distribution p (x, y, z),
objective function model conditional. joint distribution used
process creating bound: overall optimization maximize conditional likelihood labels given input. particular, bound using full joint likelihood
holds parameters marginal.

5. Parameter Estimation Mega Model
made explicit Eq (10), relevant distributions performing CEM full joint
distributions input variables x, output variables y, hidden variables z.
Additionally, require marginal distribution x variables z variables.
Finally, need compute expectations z variables. derive expectation
step section present final solution maximization step class
variables. derivation equations maximization given Appendix B.
Q bound complete conditional likelihood Mega Modelis given below:





P
(i)
(i)
(i)
(i)
(i)
z
,
x
p
p
z
,
x
,

(i)
X
n
n
n
n
n
z


+ 1
P n
h(i)
Qt =
n log
(i)
(i)
(i)
0 z (i) , x(i)
0
p z n , x n , yn
(i) p
n
n
n=1 z (i)
zn
n






P
(o)
(o) (o)
(o)
(o)
(o)
N
p zn , xn , yn
(o) p zn , xn
X X
z

+ 1

P n
h(o)
+
n log
(o)
(o)
(o)
(o)
(o)
0
0
p z n , x n , yn
z n , xn
(o) p
n=1 z (o)
z
N (i)
X



(11)

n

n

equation, p0 () probability distribution previous iteration. first
term Eq (11) bound in-domain data, second term bound
out-of-domain data. optimizations described section, nearly
identical terms in-domain parameters out-of-domain parameters. brevity,
explicitly write equations in-domain parameters; corresponding
out-of-domain equations easily derived these. Moreover, reduce notational
overload, elide superscripts denoting in-domain out-of-domain obvious
context. notational brevity, use notation depicted Table 1.
5.1 Expectation Step
E-step concerned calculating hn given current model parameters. Since zn
{0, 1}, easily find hn = p (zn = 1|), calculated follows:
109

fiDaume III & Marcu

p (zn = z | xn , yn , , , )
p (zn = z | ) p (xn | , zn = z) p (yn | , zn = z)
= P
z p (zn = z | ) p (xn | , zn = z) p (yn | , zn = z)

h
1
z (1 )1z n,z
exp zyn > xn
Zxn ,z

(12)

Here, Z partition function before. easily calculated z {0, 1}
expectation found dividing value z = 1 sum both.
5.2 M-Step
shown Appendix B.1, directly compute
value solving simple

quadratic equation. compute + a2 b, where:
=
b =
5.3 M-Step

PN

t1
n=1 2hn mn (n,0 n,1 )
PN
2 n=1 mt1
n (n,0 n,1 )
PN
n=1 hn
PN
t1
n=1 mn (n,0 n,1 )

1



Viewing Qt function , easy see optimization variable convex.
analytical solution available, gradient Qt respect (i)
seen identical gradient standard maximum entropy posterior, Eq (4),
data point weighted according posterior probability, (1 h n ). may
thus use identical optimization techniques computing optimal variables standard
maximum entropy models; difference data points weighted.
similar story holds (o) . case (g) , obtain standard maximum entropy
(i)
gradient, computed N (i) + N (o) data points, xn weighted hn
(o)
(o)
xn weighted hn . shown Appendix B.2.
5.4 M-Step
Like case , cannot obtain analytical solution finding maximizes
Qt . However, compute simple derivatives Qt respect single component
(i)
f maximized analytically. shown Appendix B.3, compute f

+ a2 b, where:
PN



n=1 1 hn + jn,0 (1 )n,0,f
=
P
2 N
n=1 jn,0 (1 )n,0,f
PN
(1 hn ) xnf
1+
b = PN n=1
n=1 jn,0 (1 )n,0,f

110



fiDomain Adaptation Statistical Classifiers

Algorithm MegaCEM
()
()
Initialize f = 0.5, f = 0, () = 0.5 {g, i, o} f .
parameters havent converged iterations remain
{- Expectation Step -}
n = 1..N (i)
(i)
Compute in-domain marginal probabilities, mn
(i)
Compute in-domain expectations, hn , Eq (12)
end
n = 1..N (o)
(o)
Compute out-of-domain marginal probabilities, mn
(o)
Compute out-of-domain expectations, hn Eq (12)
end
{- Maximization Step -}
Analytically update (i) (o) according equations shown Section 5.2
Optimize (i) , (o) (g) using BFGS
Iterations remain and/or havent converged
Update according derivation Section 5.4
end
end
return , ,
Figure 2: full training algorithm Mega Model.
case (o) identical. (g) , difference must replace
sum data points two sums, one in-domain out-of-domain
points; and, before, 1 hn must replaced hn ; made explicit
Appendix. Thus, optimize variables, simply iterate optimize
component analytically, given above, convergence.
5.5 Training Algorithm
full training algorithm depicted Figure 2. Convergence properties CEM
algorithm ensure converge (local) maximum posterior space. local
optima become problem practice, one alternatively use stochastic optimization
algorithm, temperature applied enabling optimization jump local
optima early on. However, explore idea work. context
application, extension required.
5.6 CEM Convergence
One immediate question conditional EM model described many
EM iterations required model converge. experiments, 5 iterations
111

fiDaume III & Marcu

Convergence CEM Optimization
22
20
18

Negative Log Likelihood (*1e6)

16
14
12
10
8
6
4
2
0

0

1

2
3
Number Iterations

4

5

Figure 3: Convergence training algorithm.

CEM sufficient, often 2 3 necessary. make clear,
Figure 3, plotted negative complete log likelihood model first
data set, described Section 6.2. three separate maximizations full
training algorithm (see Figure 2); first involves updating variables, second
involves optimizing variables third involves optimizing variables.
compute likelihood steps.
Running total 5 CEM iterations still relatively efficient model. dominating expense weighted maximum entropy optimization, which, 5 CEM iterations,
must computed 15 times (each iteration requires optimization three
sets variables). worst take 15 times amount time train model
complete data set (the union in-domain out-of-domain data), practice
resume optimization ending point previous iteration, causes
subsequent optimizations take much less time.
5.7 Prediction
training supplied us model parameters, subsequent task apply
parameters unseen data obtain class predictions. assume test data indomain (i.e., drawn either Q(i) Q(g) notation introduction),
obtain decision rule form given Eq (13) new test point x.

= arg max p (y | x; )
yY
X
= arg max
p (z | x; ) p (y | x, z; )
yY

= arg max
yY

z

X
z

p (z | ) p (x | z; ) p (y | x, z; )
112

fiDomain Adaptation Statistical Classifiers



= arg max

F




(g)

f

xf

(g)

1 f

1xf





h
(g)
exp > x

Zx,(g)
h



>x
F
xf
1xf exp (i)


(i)
(i)

+ (1 )
f
1 f
Zx,(i)
yY

f =1

(13)

f =1

Thus, decision rule simply select class highest probability according maximum entropy classifiers, weighted linearly marginal probabilities
new data point drawn Q(i) versus Q(g) . sense, model
seen linearly interpolating in-domain model general-domain model,
interpolation parameter input specific.

6. Experimental Results
section, describe result applying Mega Model several datasets
varying degrees divergence in-domain out-of-domain data. However,
describing data results, discuss systems compare.
6.1 Baseline Systems
Though little literature problem thus real systems
compare, several obvious baselines, describe section.
OnlyI: model obtained simply training standard maximum entropy model
in-domain data. completely ignores out-of-domain data serves
baseline case data unavailable.
OnlyO: model obtained training standard maximum entropy model
out-of-domain data, completely ignoring in-domain data. serves baseline
expected performance without annotating new data. also gives sense close
out-of-domain distribution in-domain distribution.
LinI: model obtained linearly interpolating OnlyI OnlyO systems.
interpolation parameter estimated held-out (development) in-domain data.
means that, practice, extra in-domain data would need annotated order create
development set; alternatively, cross-validation could used.
Mix: model obtained training maximum entropy model union
out-of-domain in-domain data sets.
MixW: model also obtained training maximum entropy model union
out-of-domain in-domain data sets, out-of-domain data downweighted effectively equinumerous in-domain data.
Feats: model uses out-of-domain data build one classifier uses
classifiers predictions features in-domain data, described ? (?).
113

fiDaume III & Marcu

Prior: adaptation model described Section 2.2, out-of-domain
data used estimate prior in-domain classifier. case maximum
entropy models consider here, weights learned out-of-domain data used
mean Gaussian prior distribution placed weights training
in-domain data, described Chelba Acero (2004).
cases, tune model hyperparameters using performance development data.
development data taken random 20% training data cases.
appropriate hyperparameters found, 20% folded back training set.
6.2 Data Sets
evaluate models three different problems. first two problems come
Automatic Content Extraction (ACE) data task. data selected ACE
program specifically looks data different domains. third problem
tackled Chelba Acero (2004), required annotate data themselves.
6.2.1 Mention Type Classification
first problem, Mention Type, subcomponent entity mention detection
task (an extension named entity tagging task, wherein pronouns nominals
marked, addition simple names). assume extents mentions
marked simply need identify type, one of: Person, Geo-political Entity,
Organization, Location, Weapon Vehicle. out-of-domain data, use newswire
broadcast news portions ACE 2005 training data; in-domain data, use
Fisher conversations data. example out-of-domain sentence is:
again, prime battleground constitutional allocation power
nom
nam
federal governmentnom
gpe statesgpe , Congressorg
bar
federal regulatory agenciesorg .
example in-domain sentence is:
nom
pro
nom
mypro
per wifeper Iper transported across continent gpe
whq pro
whereloc Iper born

use 23k out-of-domain examples (each mention corresponds one example), 1k
in-domain examples 456 test examples. Accuracy computed 0/1 loss. use
standard feature functions employed named entity models, include lexical items,
stems, prefixes suffixes, capitalization patterns, part-of-speech tags, membership
information gazetteers locations, businesses people. accuracies reported
result running ten fold cross-validation.
6.2.2 Mention Tagging
second problem, Mention Tagging precursor Mention Type task,
attempt tag entity mentions raw text. use standard Begin/In/Out
encoding use maximum entropy Markov model perform tagging (McCallum
et al., 2000). out-of-domain data, use newswire broadcast news
114

fiDomain Adaptation Statistical Classifiers

data; in-domain data, use broadcast news data transcribed
automatic speech recognition. in-domain data lacks capitalization, punctuation, etc.,
also contains transcription errors (speech recognition word error rate approximately
15%). tagging task, 112k out-of-domain examples (in context tagging,
example single word), 5k in-domain examples 11k test examples.
Accuracy F-measure across segmentation. use features mention
type identification task. scores reported ten fold cross-validation.
6.2.3 Recapitalization
final problem, Recap, task recapitalizing text. Following Chelba Acero
(2004), use maximum entropy Markov model, possible tags are:
Lowercase, Capitalized, Upper Case, Punctuation Mixed case. out-of-domain
data task comes Wall Street Journal, two separate in-domain data sets
come broadcast news text CNN/NPR ABC Primetime, respectively. use
3.5m out-of-domain examples (one example one word). CNN/NPR data, use
146k in-domain training examples 73k test examples; ABC Primetime data,
use 33k in-domain training examples 8k test examples. use identical features
Chelba Acero (2004). order maintain comparability results described
Chelba Acero (2004), perform cross-validation experiments: use
train/test split described paper.
6.3 Feature Selection
maximum entropy models used classification adept dealing
many irrelevant and/or redundant features, nave Bayes generative model, use
model distribution input variables, overfit features. turned
problem Mention Type Mention Tagging problems,
Recap problems, caused errors. alleviate problem, Recap
problem only, applied feature selection algorithm features used nave
Bayes model (the entire feature set used maximum entropy model). Specifically,
took 10k top features according information gain criteria predict indomain versus out-of-domain (as opposed feature selection class label); Forman
(2003) provides overview different selection techniques.1
6.4 Results
results shown Table 2, see training in-domain data
always outperforms training out-of-domain data. linearly interpolated model
improve base models significantly. Placing data one bag helps,
clear advantage re-weighting domain data. Prior model
Feats model perform roughly comparably, Prior model edging
small margin.2 model outperforms Prior model Feats model.
1. value 10k selected arbitrarily initial run model development data;
tuned optimize either development test performance.
2. numbers result Prior model data Chelba Acero (2004) differ slightly
reported paper. two potential reasons this. First, numbers

115

fiDaume III & Marcu

|D(o) |

|D(i) |
Accuracy
OnlyO
OnlyI
LinI
Mix
MixW
Feats
Prior
MegaM
% Reduction
Mix
Prior

Mention
Type
23k
1k

Mention
Tagging
112k
5k

Recap
ABC
3.5m
8k

Recap
CNN
3.5m
73k

Average
-

57.6
81.2
81.5
84.9
81.3
87.8
87.9
92.1

78.3
83.5
83.8
80.9
81.0
84.2
85.1
88.2

95.5
97.4
97.7
96.4
97.6
97.8
97.9
98.1

94.6
94.7
94.9
95.0
93.5
96.1
95.9
96.8

81.5
89.2
89.5
89.3
88.8
91.5
91.7
93.9

47.7
34.7

38.2
20.8

52.8
19.0

36.0
22.0

43.0
26.5

Table 2: Experimental results; first set rows show sizes in-domain
out-of-domain training data sets. second set rows (Accuracy) show
performance various models four tasks. last two rows (%
Reduction) show percentage reduction error rate using Mega Model
baseline model (Mix) best alternative method (Prior).

applied McNemars test (Gibbons & Chakraborti, 2003, section 14.5) gage statistical significance results, comparing results Prior model
Mega Model (for mention tagging experiment, compute McNemars test simple
Hamming accuracy rather F-score; suboptimal, know
compute statistical significance F-score). mention type task, difference
statistical significant p 0.03 level; mention tagging task, p 0.001;
recapitalization tasks, difference ABC data significant p 0.06
level, CNN/NPR data significant p 0.004 level.
mention type task, improved baseline model trained in-domain
data accuracy 81.2% 92.1%, relative improvement 13.4%. mention
tagging, improve 83.5% F-measure 88.2%, relative improvement 5.6%.
ABC recapitalization task (for much in-domain data available), increase
performance 95.5% 98.1%, relative improvement 2.9%. CNN/NPR
recapitalization task (with little in-domain data), increase performance 94.6%
96.8%, relative improvement 2.3%.

reported based using 20m examples; consider 3.5m example case. Second,
likely subtle differences training algorithms used. Nevertheless, whole, relative
improvements agree paper.

116

fiDomain Adaptation Statistical Classifiers

Mention Type Identification Task

Mention Tagging Task

90

95

OnlyOut
Chelba
MegaM

90

OnlyOut
Chelba
MegaM

85
85

80

Accuracy

Fmeasure

80

75

75
70

65

70
60

65
0
10

1

10

2

3

10
10
Amount Domain Data Used (log scale)

55
1
10

4

10

2

10
Amount Domain Data Used (log scale)

Figure 4: Learning curves Prior MegaM models.
6.5 Learning Curves
particular interest amount annotated in-domain data needed see marked
improvement OnlyO baseline well adapted system. show Figure 4
learning curves Mention Type Mention Tagging problems. Along x-axis,
plot amount in-domain data used; along y-axis, plot accuracy. plot
three lines: flat line OnlyO model use in-domain data,
curves Prior MegaM models. see, model maintains accuracy
models, Prior curve actually falls baseline
type identification task.3

7. Model Introspection
seen previous sections Mega Model routinely outperforms competing models. Despite clear performance improvement, question remains open regarding
internal workings models. (i) variable captures degree indomain data set truly in-domain. z variables model aim capture,
test data point, whether general domain in-domain. section, discuss
particular values parameters model learns variables.
present two analyses. first (Section 7.1), inspect models inner workings
Mention Type task Section 6.2.1. analysis, look specifically
expected values hidden variables found model. second analysis
(Section 7.2), look ability model judge degree relatedness, defined
variables.
3. Fisher data personal conversations. hence much higher degree first
second person pronouns news. (The baseline always guesses person achieves 77.8%
accuracy.) able intelligently use out-of-domain data in-domain model
unsure, performance drops, observed Prior model.

117

3

10

fiDaume III & Marcu

Pre-context
home trenton
veterans administration
know american
gives
capable getting
fisher thing calling


. . . Entity . . .
. . . new jersey . . .
. . . hospital . . .
. . . government. . .
...

...
. . . anything . . .
...

...
...
kid
...

Post-context
thats

chills

ha ha screwed


True
GPE
ORG
ORG
PER
WEA
PER
PER

Hyp
GPE
LOC
ORG
PER
PER
PER
PER

p (z = I)
0.02
0.11
0.17
0.71
0.92
0.93
0.98

Table 3: Examples test data Mention Type task. True column
correct entity type Hyp column models prediction. final
column probability example truly in-domain model.

7.1 Model Expectations
focus discussion, consider Mention Type task, Section 6.2.1.
Table 3, shown seven test-data examples Mention Type task. Precontext text appears entity post-context text
appears after. report true class class model hypothesizes. Finally,
report probability example truly in-domain, according model.
see, three examples model thinks general domain new
jersey, hospital government. believes me, anything kid
in-domain. general, probabilities tend skewed toward 0 1,
uncommon nave Bayes models. shown two errors data. first,
model thinks hospital location truly organization.
difficult distinction make: training data, hospitals often used locations.
second example error anything capable getting anything
here. long-distance context example discussion biological warfare
Saddam Hussein, anything supposed refer type biological warhead.
model mistakingly thinks person. error likely due fact
model identifies word anything likely truly in-domain (the word
common newswire). also learned truly in-domain entities people.
Thus, lacking evidence otherwise, model incorrectly guesses anything person.
interesting observe model believes entity gives
chills closer general domain fisher thing calling ha ha
screwed up. likely occurs context ha ha occurred anywhere
out-of-domain training data, twice in-domain training data. unlikely
example would misclassified otherwise (me fairly clearly person),
example shows model able take context account deciding domain.
decisions made model, shown Table 3 seem qualitatively reasonable.
numbers perhaps excessively skewed, ranking believable. in-domain
data primarily conversations random (not necessarily news worthy) topics,
hence highly colloquial. Contrastively, out-of-domain data formal news.
model able learn entities like new jersey government
news words like kid.
118

fiDomain Adaptation Statistical Classifiers

(i)
(o)

Mention
Type
0.14
0.11

Mention
Tagging
0.41
0.45

Recap
CNN
0.36
0.40

Recap
ABC
0.51
0.69

Table 4: Values variables discovered Mega Model algorithm.
7.2 Degree Relatedness
section, analyze values found model. Low values (i)
(o) mean in-domain data significantly different out-of-domain data;
high values mean similar. high value means
general domain model used cases. tasks Mention Type,
values middling around 0.4. Mention Type, (i) 0.14 (o) 0.11,
indicating significant difference in-domain out-of-domain
data. exact values tasks shown Table 4.
values make intuitive sense. distinction conversation data
news data (for Mention Type task) significantly stronger difference
manually automatically transcribed newswire (for Mention Tagging task).
values reflect qualitative distinction. rather strong difference
values recapitalization tasks expected priori. However, post hoc
analysis shows result reasonable. compute KL divergence unigram
language model out-of-domain data set in-domain data sets.
KL divergence CNN data 0.07, divergence ABC data 0.11.
confirms ABC data perhaps different baseline out-of-domain
CNN data, reflected values.
also interested cases little difference in-domain
out-of-domain data. simulate case, performed following experiment.
consider Mention Type task, use training portion out-ofdomain data. randomly split data half, assigning half in-domain
out-of-domain. theory, model learn may rely general
domain model. performed experiment ten fold cross-validation found
average value selected model 0.94. strictly less
one, show model able identify similar domains.

8. Conclusion Discussion
paper, presented Mega Model domain adaptation discriminative (conditional) learning framework. described efficient optimization algorithms
based conditional EM technique. experimentally shown, four data sets,
model outperforms large number baseline systems, including current state
art model, requiring significantly less in-domain data.
Although focused specifically discriminative modeling maximum entropy
framework, believe novel, basic idea work foundedto break
in-domain distribution p(i) out-of-domain distribution p(o) three distributions, q (i) ,
119

fiDaume III & Marcu

q (o) q (g) general. particular, one could perform similar analysis case
generative models obtain similar algorithms (though case generative model,
standard EM could used). model could applied domain adaptation
language modeling machine translation.
exception work described Section 2.2, previous work in-domain adaptation quite rare, especially discriminative learning framework. substantial literature language modeling/speech community, adaptation
concerned based adapting new speakers (Iyer, Ostendorf, & Gish,
1997; Kalai, Chen, Blum, & Rosenfeld, 1999). learning perspective, Mega
Model similar mixture experts model. model seen constrained experts model, three experts, constraints specify in-domain
data come one two experts, out-of-domain data come
one two experts (with single expert overlapping two). attempts
build discriminative mixture experts models make heuristic approximations order
perform necessary optimization (Jordan & Jacobs, 1994), rather apply conditional
EM, gives us strict guarantees monotonically increase data (incomplete)
log likelihood iteration training.
domain adaptation problem also closely related multitask learning (also known
learning learn inductive transfer). multitask learning, one attempts learn
function solves many machine learning problems simultaneously. related problem
discussed Thrun (1996), Caruana (1997) Baxter (2000), among others.
similarity multitask learning domain adaptation deal
data drawn related, distinct distributions. primary difference domain
adaptation cares predicting one label type, multitask learning cares
predicting many.
various sub-communities natural language processing family begin continue branch domains newswire, importance developing models
new domains without annotating much new data become important.
Mega Model first step toward able migrate simple classification-style models (classifiers maximum entropy Markov models) across domains. Continued research
area adaptation likely benefit work done active learning
learning large amounts unannotated data.

Acknowledgments
thank Ciprian Chelba Alex Acero making data available. thank Ryan
McDonald pointing Feats baseline, previously considered.
also thank Kevin Knight Dragos Munteanu discussions related project.
paper greatly improved suggestions reviewers, including reviewers
previous, shorter version. work partially supported DARPA-ITO grant N6600100-1-9814, NSF grant IIS-0097846, NSF grant IIS-0326276, USC Dean Fellowship
Hal Daume III.
120

fiDomain Adaptation Statistical Classifiers

Appendix A. Conditional Expectation Maximization
appendix, derive Eq (10) Eq (7) making use Jensens inequality
variational bound. interested reader referred work Jebara Pentland
(1998) details. discussion consider bound change log
likelihood iteration 1 iteration t, l c , given Eq (14):



p | x;
p x, | /p |
l = log
= log
p (y | x; t1 )
p (x, | t1 ) /p (y | t1 )



p x, y;
p x;
= log
log
p (x, y; t1 )
p (x; t1 )
c

(14)
(15)

Here, effectively rewritten log-change ratio conditionals
difference log-change ratio joints log-change ratio
marginals. may rewrite Eq (15) introducing hidden variables z as:


P
P


z p x, z;
z p x, y, z;
log P
l = log P
t1 )
t1 )
z p (x, y, z;
z p (x, z;
c

(16)

apply Jensens inequality first term Eq (16) obtain:

c

l

X
z

"

#


P

p x, y, z | t1
p x, y, z;
z p x, z;
P
log
log P
t1 )
t1 )
p (x, y, z; t1 )
z 0 p (x, y, z |
z p (x, z;
|
{z
}

(17)

hx,y,z,t1

Eq (17), expression denoted hx,y,z,t1 joint expectation z
previous iterations parameter settings. Unfortunately, cannot also apply Jensens inequality remaining term Eq (17) appears negated. applying
variational dual (log x x 1) term, obtain following, final bound:
c



l Q =

X
z



P

p x, y, z;
z p x, z;
+1
hx,y,z,t1 log
P
t1 )
p (x, y, z; t1 )
z p (x, z;

(18)

Applying bound Eq (18) distributions chosen model yields Eq (10).

Appendix B. Derivation Estimation Equations
Given model structure parameterization Mega Modelgiven Section 3.2,
Eq (5), obtain following expression joint probability data:
121

fiDaume III & Marcu



p x, y, z | () , () ,


F
N



zn
zn
Ber(xnf | f )Gibbs(yn | xn , )
Ber(zn | )
=


n=1
f =1



F
N

1xnf
xnf


=
1 zfn
zfn
zn (1 )1zn

n=1
f =1
!
h
X
1
h
exp zynn > xn
exp zcn > xn

c

(19)

marginal distribution obtained removing last two terms (the exp
sum exps) final equation. Plugging Eq (19) Eq (10) using notation
Eq (12), obtain following expression Qt :
Qt =

X


+

N
X

n=1



log Nor(() ; 0, 2 I) +
"

X
zn

(

F
X

f =1



()

log Bet( f ; a, b)

hn zn log + (1 zn ) log(1 ) + log n,zn
+

F
X

xnf log zynn

f =1

zn
mt1
n (1

)

1zn

log

X
c

n,zn + 1

#

exp

h

zcn > xn






jn,z
n

)
(20)

well analogous term out-of-domain data. j defined Table 1.
B.1 M-Step
computing , simply differentiate Qt (see Eq (20)) respect , obtaining:
N

Qt X hn 1 hn
=
+
+ mt1
n (n,0 n,1 )


1

(21)

n=1

solving 0 leads directly quadratic expression form:

0 =

2

"
"

N
X

mt1
n (n,0

n=1

+ 1 1 +

N
X

n=1

n,1 )

#

2hn mt1
n (n,0 n,1 )
122



#

fiDomain Adaptation Statistical Classifiers

+

0

"



N
X

n=1

hn

#

(22)

Solving directly gives desired update equation.
B.2 M-Step
optimizing (i) , rewrite Qt , Eq (20), neglecting irrelevant terms, as:

Qt [] =

N
X

n=1

(1 hn )


F
X


f =1

xnf yn ,f log

X
c

h

exp c > xn





+ log Nor(; 0, 2 I)

(23)

Eq (23), bracketed expression exactly log-likelihood term obtained
standard logistic regression models. Thus, optimization Q respect (i)
(o) performed using weighted version standard logistic regression optimization,
weights defined (1hn ). case (g) , obtain weighted logistic regression
model, N (i) + N (o) data points, weights defined hn .
B.3 M-Step
case (i) (o) , rewrite Eq (20) remove irrelevant terms, as:
Qt [ (i) ] =

F
X

f =1

log Bet(f ; a, b) +

N
X


n=1

(1 hn ) log n,0 mt1
n (1 )n,0



(24)

Due presence product term , cannot compute analytical solution
maximization problem. However, take derivatives component-wise (in F )
obtain analytical solutions (when combined prior). admits iterative
solution maximizing Qt maximizing component separately convergence.
Computing derivatives Qt respect f requires differentiating n,0 respect
f ; convenient form (recalling notation Table 1:


n,0 = [n,0,f ]
{xnf f + (1 xnf )(1 f )} = n,0,f
f
f

(25)

Using result, maximize Qt respect f solving:
"
N
X
xnf (1 f ) (1 xnf )f
h
(1 hn )
Qf
=
f
f (1 f )
n=1
#

(26)

1
f (1 f )
#
N
X
jn,0 (1 )n,0,f
f )

jn,0 (1 )n,0,f +

=

"
N
X
1
(1 hn ) (xnf
1+
f (1 f )
n=1

123

n=1

fiDaume III & Marcu

Equating zero yields quadratic expression form:

0 = (f )2
+ (f )

1

"

N
X

jn,0 (1
n=1
" N
X


"

)n,0,f

#

1 hn + jn,0 (1 )n,0,f

n=1
N
X

+ (f )0 1 +

n=1

(1 hn ) xnf

#



#
(27)
(o)

final equation solved analytically. similar expression arises f .
(g)

case f , obtain quadratic form sums entire data set
hn replacing occurrences (1 hn ):

0 =

+

+






N (i)
2 X
(i)
(i)
(g)

jn,1 (i)


n,1,f

f

n=1

n=1

(g)

f

1

"




(g) 0

f

+

(o)
N
X

N (i)



X

n=1

X

n=1

jn,1 (o) n,1,f


(i)
h(i)
n + jn,1 n,1,f

N (i)

1+

(i)

(i)

(o)

(o)

N (o)
(i)

h(i)
n xnf +

X

n=1



(o)




h(o)
n xnf

(o)
N
X

n=1

(o)

(o)

(o)
h(o)
n + jn,1 n,1,f



#
(28)

Again, solved analytically. values j, m, , ,, defined Table 1.

References
Averick, B. M., & More, J. J. (1994). Evaluation large-scale optimization problems
vector parallel architectures. SIAM Journal Optimization, 4.
Baxter, J. (2000). model inductive bias learning. Journal Artificial Intelligence
Research, 12 , 149198.
Bacchiani, M., & Roark, B. (2003). Unsupervised langauge model adaptation. Proceedings
International Conference Acoustics, Speech Signal Processing (ICASSP).
Caruana, R. (1997). Multitask learning: knowledge-based source inductive bias. Machine Learning, 28 , 4175.
Chelba, C., & Acero, A. (2004). Adaptation maximum entropy classifier: Little data
help lot. Proceedings Conference Empirical Methods Natural
Language Processing (EMNLP), Barcelona, Spain.
Chen, S., & Rosenfeld, R. (1999). Gaussian prior smoothing maximum entropy
models. Tech. rep. CMUCS 99-108, Carnegie Mellon University, Computer Science
Department.
124

fiDomain Adaptation Statistical Classifiers

Della Pietra, S., Della Pietra, V. J., & Lafferty, J. D. (1997). Inducing features random
fields. IEEE Transactions Pattern Analysis Machine Intelligence, 19 (4), 380
393.
Dempster, A., Laird, N., & Rubin, D. (1977). Maximum likelihood incomplete data
via EM algorithm. Journal Royal Statistical Society, B39.
Elhadad, N., Kan, M.-Y., Klavans, J., & McKeown, K. (2005). Customization unified
framework summarizing medical literature. Journal Artificial Intelligence
Medicine, 33 (2), 179198.
Forman, G. (2003). extensive empirical study feature selection metrics text classification. Journal Machine Learning Research, 3, 12891305.
Gibbons, J. D., & Chakraborti, S. (2003). Nonparametric Statistical Inference. Marcel
Dekker, Inc.
Gildea, D. (2001). Corpus variation parser performance. Proceedings Conference Empirical Methods Natural Language Processing (EMNLP).
Hobbs, J. R. (2002). Information extraction biomedical text. Journal Biomedical
Informatics, 35 (4), 260264.
Huang, J., Zweig, G., & Padmanabhan, M. (2001). Information extraction voicemail.
Proceedings Conference Association Computational Linguistics
(ACL).
Hwa, R. (1999). Supervised grammar induction using training data limited constituent
information. Proceedings Conference Association Computational
Linguistics (ACL), pp. 7379.
Iyer, R., Ostendorf, M., & Gish, H. (1997). Using out-of-domain data improve in-domain
language models. IEEE Signal Processing, 4 (8).
Jebara, T., & Pentland, A. (1998). Maximum conditional likelihood via bound maximization
CEM algorithm. Advances Neural Information Processing Systems
(NIPS).
Jordan, M., & Jacobs, R. (1994). Hierarchical mixtures experts EM algorithm.
Neural Computation, 6, 181214.
Kalai, A., Chen, S., Blum, A., & Rosenfeld, R. (1999). On-line algorithms combining
language models. ICASSP.
Lafferty, J., McCallum, A., & Pereira, F. (2001). Conditional random fields: Probabilistic
models segmenting labeling sequence data. Proceedings International
Conference Machine Learning (ICML).
Malouf, R. (2002). comparison algorithms maximum entropy parameter estimation.
Proceedings CoNLL.
McCallum, A., Freitag, D., & Pereira, F. (2000). Maximum entropy Markov models
information extraction segmentation. Proceedings International Conference Machine Learning (ICML).
125

fiDaume III & Marcu

Minka, T. P. (2003). comparison numerical optimizers logistic regression. http:
//www.stat.cmu.edu/~minka/papers/logreg/.
Munteanu, D., & Marcu, D. (2005). Improving machine translation performance exploiting non-parallel corpora. Computational Linguistics, appear.
Nash, S., & Nocedal, J. (1991). numerical study limited memory BFGS method
truncated Newton method large scale optimization. SIAM Journal
Optimization, 1, 358372.
Rambow, O., Shrestha, L., Chen, J., & Lauridsen, C. (2004). Summarizing email threads.
Proceedings Conference North American Chapter Association
Computational Linguistics (NAACL) Short Paper Section.
Roark, B., & Bacchiani, M. (2003). Supervised unsupervised PCFG adaptation
novel domains. Proceedings Conference North American Chapter
Association Computational Linguistics Human Language Technology
(NAACL/HLT).
Smith, D. R. (1998). Variational Methods Optimization. Dover Publications, Inc., Mineola, New York.
Thrun, S. (1996). learning n-th thing easier learning first. Advances
Neural Information Processing Systems (NIPS).

126

fiJournal Artificial Intelligence Research 26 (2006) 191246

Submitted 01/05; published 07/06

Fast Downward Planning System
Malte Helmert

HELMERT @ INFORMATIK . UNI - FREIBURG . DE

Institut fur Informatik
Albert-Ludwigs-Universitat Freiburg
Georges-Kohler-Allee, Gebaude 052
79110 Freiburg, Germany

Abstract
Fast Downward classical planning system based heuristic search. deal general deterministic planning problems encoded propositional fragment PDDL2.2, including
advanced features like ADL conditions effects derived predicates (axioms). Like
well-known planners HSP FF, Fast Downward progression planner, searching
space world states planning task forward direction. However, unlike PDDL planning systems, Fast Downward use propositional PDDL representation planning
task directly. Instead, input first translated alternative representation called multivalued planning tasks, makes many implicit constraints propositional planning
task explicit. Exploiting alternative representation, Fast Downward uses hierarchical decompositions planning tasks computing heuristic function, called causal graph heuristic,
different traditional HSP-like heuristics based ignoring negative interactions
operators.
article, give full account Fast Downwards approach solving multi-valued
planning tasks. extend earlier discussion causal graph heuristic tasks involving
axioms conditional effects present novel techniques search control used
within Fast Downwards best-first search algorithm: preferred operators transfer idea helpful actions local search global best-first search, deferred evaluation heuristic functions
mitigates negative effect large branching factors search performance, multi-heuristic
best-first search combines several heuristic evaluation functions within single search algorithm
orthogonal way. also describe efficient data structures fast state expansion (successor
generators axiom evaluators) present new non-heuristic search algorithm called focused
iterative-broadening search, utilizes information encoded causal graphs novel
way.
Fast Downward proven remarkably successful: classical (i. e., propositional,
non-optimising) track 4th International Planning Competition ICAPS 2004, following
footsteps planners FF LPG. experiments show also performs
well benchmarks earlier planning competitions provide insights
usefulness new search enhancements.

1. Introduction
Consider typical transportation planning task: postal service must deliver number parcels
respective destinations using vehicle fleet cars trucks. Let us assume car
serves locations one city, different cities connected via highways
served trucks. sake simplicity, let us assume travelling segment
road highway incurs cost. highly realistic assumption, purposes
exposition do. number parcels, posted arbitrary locations
c
2006
AI Access Foundation. rights reserved.

fiH ELMERT

p2

B

F

c2





E

c1

G

C

c3

p1

Figure 1: transportation planning task. Deliver parcel p 1 C G parcel p2 F E,
using cars c1 , c2 , c3 truck t. cars may use inner-city roads (thin edges),
truck may use highway (thick edge).

arbitrary destinations. Moreover, cities varying size, one several cars
within city, one several trucks connecting cities. Cars never leave
city. Fig. 1 shows example task kind two cities, three cars single truck.
two parcels delivered, one (p 1 ) must moved two cities,
(p2 ) stay within initial city.
astute reader familiar planning literature noticed
essentially describing L OGISTICS domain, standard benchmark classical planning systems,
extended roadmaps complete graphs. (Part of) propositional STRIPS-like encoding
task shown Fig. 2.
would human planners go solving tasks kind? likely, would use
hierarchical approach: p1 , clear parcel needs moved cities,
possible using truck. Since example city access highway one
location, see must first load parcel car initial location, drop
first citys highway access location, load truck, drop citys highway
access location, load car city, finally drop destination.
commit high-level plan delivering p 1 without worrying lower-level aspects
path planning cars. obvious us good solution structure,
since parcel change location clearly defined ways (Fig. 3). figure
shows reasonable plans getting p 2 destination require loading car
initial city dropping target location. point ever loading
truck cars left city.
say committed (partially ordered, movements two parcels
interleaved) high-level plan shown Fig. 5. need complete plan choose
linearization high-level steps fill movements vehicle fleet them.
thus decomposed planning task number subproblems. parcel scheduling
problem (where, vehicles, parcel loaded unloaded) separated
path planning problem vehicle fleet (how move point X Y).
192

fiT FAST OWNWARD P LANNING YSTEM

Variables:
at-p1-a, at-p1-b, at-p1-c, at-p1-d, at-p1-e,
at-p2-a, at-p2-b, at-p2-c, at-p2-d, at-p2-e,
at-c1-a, at-c1-b, at-c1-c, at-c1-d,
at-c2-a, at-c2-b, at-c2-c, at-c2-d,
at-c3-e, at-c3-f, at-c3-g,
at-t-d, at-t-e,
in-p1-c1, in-p1-c2, in-p1-c3, in-p1-t,
in-p2-c1, in-p2-c2, in-p2-c3, in-p2-t
Init:
at-p1-c, at-p2-f, at-c1-a, at-c2-b, at-c3-g,
Goal:
at-p1-g, at-p2-e
Operator drive-c1-a-d:
PRE: at-c1-a ADD: at-c1-d DEL: at-c1-a
Operator drive-c1-b-d:
PRE: at-c1-b ADD: at-c1-d DEL: at-c1-b
Operator drive-c1-c-d:
PRE: at-c1-c ADD: at-c1-d DEL: at-c1-c
...
Operator load-c1-p1-a:
PRE: at-c1-a, at-p1-a ADD: in-p1-c1 DEL:
Operator load-c1-p1-b:
PRE: at-c1-b, at-p1-b ADD: in-p1-c1 DEL:
Operator load-c1-p1-c:
PRE: at-c1-c, at-p1-c ADD: in-p1-c1 DEL:
...
Operator unload-c1-p1-a:
PRE: at-c1-a, in-p1-c1 ADD: at-p1-a DEL:
Operator unload-c1-p1-b:
PRE: at-c1-b, in-p1-c1 ADD: at-p1-b DEL:
Operator unload-c1-p1-c:
PRE: at-c1-c, in-p1-c1 ADD: at-p1-c DEL:
...

at-p1-f, at-p1-g,
at-p2-f, at-p2-g,

at-t-e

at-p1-a
at-p1-b
at-p1-c

in-p1-c1
in-p1-c1
in-p1-c1

Figure 2: Part typical propositional encoding transportation planning task (no actual
PDDL syntax).

193

fiH ELMERT

c1




B

C



E

F

c2

G

c3

Figure 3: Domain transition graph parcels p 1 p2 . Indicates parcel change
state. example, arcs correspond actions
loading/unloading parcel location truck t.

B
F







E

E

G
C

Figure 4: Domain transition graphs cars c 1 c2 (left), truck (centre), car c3 (right).
Note graph corresponds part roadmap traversed
respective vehicle.

load
c1-p1-c

unload
c1-p1-d

load
t-p1-d

unload
t-p1-e

load
c3-p2-f

unload
c3-p2-e

load
c3-p1-e

Figure 5: High-level plan transportation planning task.
194

unload
c3-p1-g

fiT FAST OWNWARD P LANNING YSTEM

c1

c2

c3

p1

p2



Figure 6: Causal dependencies transportation planning task.
graph search problems, corresponding graphs shown Fig. 3 Fig. 4.
Graphs kind formally introduced domain transition graphs Section 5.
course graph search problems interact, limited ways: State
transitions parcels associated conditions regarding vehicle fleet, need
considered addition actual path planning Fig. 3. example, parcel change
state location inside car c 1 car c1 location A. However, state transitions
vehicles associated conditions parts planning task, hence
moving vehicle one location another indeed easy finding path associated
domain transition graph. say parcels causal dependencies vehicles
operators change state parcels preconditions state
vehicles. Indeed, causal dependencies task, since parcels depend
parcels vehicles depend anything except (Fig. 6). set causal
dependencies planning task visualized causal graph.
argue humans often solve planning tasks hierarchical fashion outlined preceding paragraphs, algorithmic approaches action planning usefully apply similar
ideas. Indeed, show following section, first introduce domain transition graphs causal graphs. However, earlier work almost exclusively focused acyclic
causal graphs, good reason: causal graph planning task exhibits cycle, hierarchical decomposition possible, subproblems must solved achieve
operator precondition necessarily smaller original task. far aware,
first (Helmert, 2004) present general planning algorithm focuses exploiting hierarchical information causal graphs. However, causal graph heuristic also requires
acyclicity; general case, considers relaxed planning problem operator
preconditions ignored break causal cycles.
Knowing cycles causal graphs undesirable, take closer look transportation
planning task. Let us recall informal definition causal graphs: causal graph planning
task contains vertex state variable arcs variables occur preconditions
variables occur effects operator. far, may given impression
causal graph example task well-behaved shape shown Fig. 6. Unfortunately,
closer look STRIPS encoding Fig. 2, see case: correct
causal graph, shown Fig. 7, looks messy. discrepancy intuitive actual
graph due fact informal account human-style problem solving, made
use (non-binary) state variables like location car c 1 state parcel p1 ,
STRIPS-level state variables correspond (binary) object-location propositions like parcel p 1
195

fiH ELMERT

Figure 7: Causal graph STRIPS encoding transportation planning task.
location A. would much nicer given multi-valued encoding planning
task explicitly contains variable location car c 1 similar properties. Indeed,
nice looking acyclic graph Fig. 6 causal graph multi-valued encoding shown
Fig. 8.
provided intuition underlying concepts, let us state design goal
Fast Downward planning system: develop algorithm efficiently solves general
propositional planning tasks exploiting hierarchical structure inherent causal graphs.
need overcome three major obstacles undertaking:
First, propositionally encoded planning tasks usually unstructured causal graphs.
However, intuitive dependencies often become visible encodings multi-valued
state variables. exploit fact automated PDDL planning system, devised
automatic algorithm translating (or reformulating) propositional tasks multi-valued
ones. translation algorithm considered independently rest planner; fact, also used part planning systems (van den Briel, Vossen, &
Kambhampati, 2005). keep article focused, discuss translation algorithm
here, referring earlier work central ideas (Edelkamp & Helmert, 1999).
Instead, consider output, multi-valued planning task, base formalism.
Second, matter clever encoding is, planning tasks completely hierarchical nature. deal causal cycles, consider relaxations causal
dependencies ignored use solutions relaxed problem within heuristic search
algorithm.
Third, even planning tasks solved hierarchically, finding solution difficult (indeed, still PSPACE-complete). reason, heuristic function considers
fragment task time, namely subproblems induced single state variable
predecessors causal graph. Even planning problem still NP-complete,
196

fiT FAST OWNWARD P LANNING YSTEM

Variables:
p1, p2 {at-a, at-b, at-c, at-d, at-e, at-f, at-g,
in-c1, in-c2, in-c3, in-t}
c1, c2 {at-a, at-b, at-c, at-d}
c3
{at-e, at-f, at-g}

{at-d, at-e}
Init:
p1 = at-c, p2 = at-f
c1 = at-a, c2 = at-b, c3 = at-g, = at-e
Goal:
p1 = at-g, p2 = at-e
Operator drive-c1-a-d:
PRE: c1 = at-a EFF: c1 = at-d
Operator drive-c1-b-d:
PRE: c1 = at-b EFF: c1 = at-d
Operator drive-c1-c-d:
PRE: c1 = at-c EFF: c1 = at-d
...
Operator load-c1-p1-a:
PRE: c1 = at-a, p1 = at-a EFF: p1 = in-c1
Operator load-c1-p1-b:
PRE: c1 = at-b, p1 = at-b EFF: p1 = in-c1
Operator load-c1-p1-c:
PRE: c1 = at-c, p1 = at-c EFF: p1 = in-c1
...
Operator unload-c1-p1-a:
PRE: c1 = at-a, p1 = in-c1 EFF: p1 = at-a
Operator unload-c1-p1-b:
PRE: c1 = at-b, p1 = in-c1 EFF: p1 = at-b
Operator unload-c1-p1-c:
PRE: c1 = at-c, p1 = in-c1 EFF: p1 = at-c
...
Figure 8: Part encoding transportation planning task multi-valued state variables.

197

fiH ELMERT

content incomplete solution algorithm within heuristic solver. solution
algorithm theoretical shortcomings never failed us practice.
introduced rationale approach, discuss related work next section.
followed overview general architecture Fast Downward planning system
Section 3. planning system consists three components: translation, knowledge compilation,
search. translation component converts PDDL2.2 tasks multi-valued planning tasks,
formally introduce Section 4. knowledge compilation component discussed
Section 5, search component Section 6. conclude presentation experimental
results Section 7 discussion Section 8.

2. Related Work
planning system based heuristic forward search, Fast Downward clearly related
heuristic planners HSP (Bonet & Geffner, 2001) FF (Hoffmann & Nebel, 2001)
architectural level. However, section focus work related conceptual level,
i. e., work uses similar forms hierarchical decomposition causal graphs work uses
similar forms search domain transition graphs.
2.1 Causal Graphs Abstraction
term causal graph first appears literature work Williams Nayak (1997),
general idea considerably older. approach hierarchically decomposing planning tasks
arguably old field AI Planning itself, first surfaced Newell Simons
(1963) work General Problem Solver.
Still, took long time notions evolve modern form. Sacerdotis (1974)
ABSTRIPS algorithm introduced concept abstraction spaces STRIPS-like planning tasks.
abstraction space STRIPS task state space abstracted task, obtained
removing preconditions operators original task belong given set
propositions (which abstracted away). 1 solve planning task, ABSTRIPS first generates
plan abstracted task, refines plan inserting concrete plans abstract
plan steps bridge gap abstract states satisfying operator preconditions
ignored abstract level. idea easily generalized several levels abstraction forming abstraction hierarchy, abstract level top almost
preconditions ignored, successively introducing preconditions every layer final
layer hierarchy equals original planning task.
One problem approach planning general guarantee
abstract plans bear resemblance reasonable concrete plans. example, abstraction spaces
chosen badly, quite possible finding concrete plan satisfies precondition
first operator abstract plan difficult solving original goal concrete level.
shortcomings spawned large amount research properties abstraction hierarchies
generated automatically.
1. later work authors, propositions abstracted away also removed operator effects.
makes difference subtle cases require presence axioms; distinguish
two kinds abstraction here.

198

fiT FAST OWNWARD P LANNING YSTEM

Tenenberg (1991) gives one first formal accounts properties different kinds
abstraction. Among contributions, defines so-called upward solution property,
informally stated as: exists concrete solution, also exists abstract
solution. Rather surprisingly, abstractions considered time satisfied basic
property, without one would loathe call given state space abstraction another
state space.
limitation upward solution property states relationship concrete
abstract plan all. ABSTRIPS-style hierarchical planning successful, abstract
plan must bear resemblance concrete one; otherwise little point trying
refine it. Indeed, Tenenberg introduces stronger versions upward solution property,
relevant Fast Downward Knoblocks (1994) work ordered monotonicity property.
abstraction space satisfies ordered monotonicity property if, roughly speaking, concrete
solution derived abstract solution leaving actions abstract plan
intact relevant concrete plan. Clearly, important property ABSTRIPSlike hierarchical planning.
Knoblocks article causal graphs first surface (although introduce name
them). Translated terminology, Knoblock proves following relationship
useful abstractions causal graphs: causal graph contains path variable
abstracted away variable abstracted away, abstraction ordered
monotonicity property. particular, means acyclic causal graphs, possible devise
abstraction hierarchy one new variable introduced level.
Besides theoretical contributions, Knoblock presents planning system called ALPINE
computes abstraction hierarchy planning task causal graph exploits
within hierarchical refinement planner. Although planning method different,
derivation abstraction hierarchy similar Fast Downwards method generating
hierarchical decompositions planning tasks (Section 5.2).
itself, ordered monotonicity property sufficient guarantee good performance
hierarchical planning approach. guarantees every concrete solution obtained
natural way abstract solution, guarantee abstract solutions
refined concrete ones. guarantee provided downward refinement property,
introduced Bacchus Yang (1994).
downward refinement property rarely guaranteed actual planning domains,
Bacchus Yang develop analytical model performance hierarchical planning situations given abstract plan refined certain probability p < 1. Based
analysis, present extension ALPINE called HIGHPOINT, selects abstraction hierarchy high refinement probability among satisfy ordered monotonicity
property. practice, feasible compute refinement probability, HIGHPOINT approximates value based notion k-ary necessary connectivity.
2.2 Causal Graphs Unary STRIPS Operators
Causal graphs first given name Jonsson Backstrom (1995, 1998b), call
dependency graphs. study fragment propositional STRIPS negative conditions
interesting property plan existence decided polynomial time, minimal
solutions task exponentially long, polynomial planning algorithm exists.
199

fiH ELMERT

present incremental planning algorithm polynomial delay, i. e., planning algorithm
decides within polynomial time whether given task solution, and, so, generates
solution step step, requiring polynomial time two subsequent steps. 2
fragment STRIPS covered Jonsson Backstroms algorithm called 3S
defined requirement causal graph task acyclic state variables
static, symmetrically reversible, splitting. Static variables easy
guarantee never change value solution plan. variables detected
compiled away easily. Symmetrically reversible variables operator
makes true corresponding operator identical preconditions makes
false, vice versa. words, variable symmetrically reversible iff domain
transition graph undirected. Finally, variable v splitting iff removal causal graph
weakly disconnects positive successors (those variables appear effects operators
v precondition) negative successors (those variables appear effects
operators v precondition).
Williams Nayak (1997) independently prove incremental (or, setting, reactive)
planning polynomial problem STRIPS-like setting causal graphs acyclic
operators reversible. operators reversible (according definition Williams
Nayak), variables symmetrically reversible (according definition Jonsson
Backstrom), actually special case previous result. However, Williams Nayaks
work applies general formalism propositional STRIPS, approaches
directly comparable.
recently, Domshlak Brafman provide detailed account complexity finding plans propositional STRIPS (with negation) formalism unary operators acyclic
graphs (Domshlak & Brafman, 2002; Brafman & Domshlak, 2003). 3 Among results,
prove restriction unary operators acyclic graphs reduce complexity
plan existence: problem PSPACE-complete, like unrestricted propositional STRIPS
planning (Bylander, 1994). also show singly connected causal graphs, shortest plans
cannot exponentially long, problem still NP-complete. even restricted class
causal graphs, namely polytrees bounded indegree, present polynomial planning algorithm. generally, analysis relates complexity STRIPS planning unary domains
number paths causal graph.
2.3 Multi-Valued Planning Tasks
exception Williams Nayaks paper, work discussed far exclusively deals
propositional planning problems, state variables assume values binary domain. observed introduction, question propositional vs. multi-valued encodings
usually strong impact connectivity causal graph task. fact, apart
trivial OVIE domain, none common planning benchmarks exhibits acyclic causal graph
2. However, guarantee length generated solution polynomially related length
optimal solution; might exponentially longer. Therefore, algorithm might spend exponential time tasks
solved polynomial time.
3. According formal definition causal graphs Section 5.2, operators several effects always induce
cycles causal graph, acyclic causal graph implies unary operators. researchers define causal graphs
differently, name properties explicitly here.

200

fiT FAST OWNWARD P LANNING YSTEM

considering propositional representation. contrast, multi-valued encoding
introductory example acyclic causal graph.
Due dominance PDDL (and previously, STRIPS) formalism, non-binary state variables studied often classical planning literature. One important exceptions rule work SAS + planning formalism, papers Backstrom
Nebel (1995) Jonsson Backstrom (1998a) relevant Fast Downward.
SAS+ planning formalism basically equivalent multi-valued planning tasks introduce
Section 4 apart fact include derived variables (axioms) conditional
effects. Backstrom Nebel analyse complexity various subclasses SAS + formalism discover three properties (unariness, post-uniqueness single-valuedness) together
allow optimal planning polynomial time. One three properties (unariness) related
acyclicity causal graphs, one (post-uniqueness) implies particularly simple shape domain
transition graphs (namely, post-unique tasks, domain transition graphs must simple cycles
trees).
Backstrom Nebel analyse domain transition graphs formally. Indeed, term
introduced later article Jonsson Backstrom (1998a), refines earlier results
introducing five additional restrictions SAS + tasks, related properties
domain transition graphs.
Neither two articles discusses notion causal graphs. Indeed, earlier work
aware includes causal graphs domain transition graphs central concepts
article Domshlak Dinitz (2001) state-transition support (STS) problem,
essentially equivalent SAS+ planning unary operators. context STS, domain
transition graphs called strategy graphs causal graphs called dependence graphs,
apart minor details, semantics two formalisms identical. Domshlak Dinitz
provide map complexity STS problem terms shape causal graph,
showing problem NP-complete worse almost non-trivial cases. One interesting
result causal graph simple chain n nodes variables three-valued,
length minimal plans already grow (2 n ). contrast, propositional tasks
causal graph shape admit polynomial planning algorithms according result Brafman
Domshlak (2003), causal graphs polytrees constant indegree bound
(namely, bound 1).
summarize conclude discussion related work, observe central concepts Fast Downward causal graph heuristic, causal graphs domain transition
graphs, firmly rooted previous work. However, Fast Downward first attempt marry
hierarchical problem decomposition use multi-valued state variables within general planning framework. also first attempt apply techniques similar Knoblock (1994)
Bacchus Yang (1994) within heuristic search planner.
significance latter point underestimated: classical approaches
hierarchical problem decomposition, imperative abstraction satisfies ordered monotonicity property, important probability able refine abstract plan
concrete plan high, analysis Bacchus Yang shows. Unfortunately, non-trivial
abstraction hierarchies rarely ordered monotonic, even rarely guarantee high refinement probabilities. Within heuristic approach, must-haves turn nice-to-haves:
abstraction hierarchy ordered monotonic abstract plan considered heuristic
evaluator refinable, merely reduces quality heuristic estimate, rather caus201

fiH ELMERT

Translation





Normalization
Invariant synthesis
Grounding
Translation MPT

Knowledge
Compilation
Domain transition
graphs
Causal graph
Successor generator
Axiom evaluator

Search






Causal graph heuristic
FF heuristic
Greedy best-first search
Multi-heuristic best-first search
Focused iterative-broadening search

Figure 9: three phases Fast Downwards execution.
ing search fail (in worst case) spend long time trying salvage non-refinable abstract
plans (in much better case).

3. Fast Downward
describe overall architecture planner. Fast Downward classical planning
system based ideas heuristic forward search hierarchical problem decomposition.
deal full range propositional PDDL2.2 (Fox & Long, 2003; Edelkamp & Hoffmann,
2004), i. e., addition STRIPS planning, supports arbitrary formulae operator preconditions
goal conditions, deal conditional universally quantified effects derived
predicates (axioms).
name planner derives two sources: course, one sources Hoffmanns successful FF (Fast Forward) planner (Hoffmann & Nebel, 2001). Like FF, Fast
Downward heuristic progression planner, i. e., computes plans heuristic search space
world states reachable initial situation. However, compared FF, Fast Downward uses
different heuristic evaluation function called causal graph heuristic. heuristic evaluator proceeds downward far tries solve planning tasks hierarchical fashion
outlined introduction. Starting top-level goals, algorithm recurses
causal graph remaining subproblems basic graph search tasks.
Similar FF, planner shown excellent performance: original implementation
causal graph heuristic, plugged standard best-first search algorithm, outperformed previous champions area, FF LPG (Gerevini, Saetti, & Serina, 2003), set STRIPS
benchmarks first three international planning competitions (Helmert, 2004). Fast Downward followed footsteps FF LPG winning propositional, non-optimizing
track 4th International Planning Competition ICAPS 2004 (referred IPC4
on).
mentioned introduction, Fast Downward solves planning task three phases (Fig. 9):
translation component responsible transforming PDDL2.2 input nonbinary form amenable hierarchical planning approaches. applies number normalizations compile away syntactic constructs like disjunctions
directly supported causal graph heuristic performs grounding axioms operators. importantly, uses invariant synthesis methods find groups related propo202

fiT FAST OWNWARD P LANNING YSTEM

sitions encoded single multi-valued variable. output translation
component multi-valued planning task, defined following section.
knowledge compilation component generates four kinds data structures play
central role search: Domain transition graphs encode how, conditions,
state variables change values. causal graph represents hierarchical dependencies different state variables. successor generator efficient data
structure determining set applicable operators given state. Finally, axiom
evaluator efficient data structure computing values derived variables.
knowledge compilation component described Section 5.
search component implements three different search algorithms actual planning.
Two algorithms make use heuristic evaluation functions: One well-known
greedy best-first search algorithm, using causal graph heuristic. called multiheuristic best-first search, variant greedy best-first search tries combine several
heuristic evaluators orthogonal way; case Fast Downward, uses causal
graph FF heuristics. third search algorithm called focused iterative-broadening
search; closely related Ginsberg Harveys (1992) iterative broadening.
heuristic search algorithm sense use explicit heuristic evaluation
function. Instead, uses information encoded causal graph estimate usefulness operators towards satisfying goals task. search component described
Section 6.

4. Multi-Valued Planning Tasks
Let us formally introduce problem planning multi-valued state variables.
formalism based SAS+ planning model (Backstrom & Nebel, 1995; Jonsson & Backstrom,
1998a), extends axioms conditional effects.
Definition 1 Multi-valued planning tasks (MPTs)
multi-valued planning task (MPT) given 5-tuple = hV, 0 , s? , A, Oi following
components:
V finite set state variables, associated finite domain v . State variables partitioned fluents (affected operators) derived variables (computed
evaluating axioms). domains derived variables must contain undefined value .
partial variable assignment partial state V function subset V
s(v) Dv wherever s(v) defined. partial state called extended state
defined variables V reduced state state defined fluents V.
context partial variable assignments, write v = variable-value pairing
(v, d) v 7 d.
s0 state V called initial state.
s? partial variable assignment V called goal.
finite set (MPT) axioms V. Axioms triples form hcond, v, di,
cond partial variable assignment called condition body axiom, v derived
203

fiH ELMERT

variable called affected variable, v called derived value v. pair
(v, d) called head axiom written v := d.
axiom set partitioned totally ordered set axiom layers 1 Ak
within layer, affected variable may associated single
value axiom heads bodies. words, within layer, axioms
affected variable different derived values forbidden, variable appears
axiom head, may appear different value body. called
layering property.
finite set (MPT) operators V. operator hpre, effi consists partial
variable assignment pre V called precondition, finite set effects eff. Effects
triples hcond, v, di, cond (possibly empty) partial variable assignment called
effect condition, v fluent called affected variable, v called new
value v.
axioms effects, also use notation cond v := place hcond, v, di.
provide formal semantics MPT planning, first need formalize axioms:
Definition 2 Extended states defined state
Let state MPT axioms A, layered 1 Ak . extended state defined
s, written A(s), result 0 following algorithm:
algorithm evaluate-axioms(A1 , . . . , Ak , s):
variable
( v:
s(v) v fluent variable
s0 (v) :=

v derived variable
{1, . . . , k}:
exists axiom (cond v := d) cond s0 s0 (v) 6= d:
Choose axiom cond v := d.
s0 (v) :=
words, axioms evaluated layer-by-layer fashion using fixed point computations,
similar semantics stratified logic programs. easy see layering
property Definition 1 guarantees algorithm terminates produces deterministic
result. defined semantics axioms, define state space MPT:
Definition 3 MPT state spaces
state space MPT = hV, s0 , s? , A, Oi, denoted S(), directed graph. vertex
set set states V, contains arc (s, 0 ) iff exists operator hpre, effi
that:
pre A(s),
s0 (v) = effects cond v := eff cond A(s),
s0 (v) = s(v) fluents.
204

fiT FAST OWNWARD P LANNING YSTEM

Finally, define MPT planning problem:
Definition 4 MPT planning
MPT-P LAN E X following decision problem: Given MPT initial state 0 goal
s? , S() contain path s0 state s0 s? A(s0 )?
MPT-P LANNING following search problem: Given MPT initial state 0 goal
s? , compute path S() s0 state s0 s? A(s0 ), prove none exists.
MPT-P LAN E X problem easily shown PSPACE-hard generalizes plan
existence problem propositional STRIPS, known PSPACE-complete (Bylander,
1994). also easy see addition multi-valued domains, axioms conditional effects
increase theoretical complexity MPT planning beyond propositional STRIPS. Thus,
conclude formal introduction MPT planning stating MPT-P LAN E X PSPACEcomplete, turn practical side things following section.

5. Knowledge Compilation
purpose knowledge compilation component set stage search algorithms
compiling critical information planning task number data structures efficient access. contexts, computations kind often called preprocessing. However,
preprocessing nondescript word mean basically anything. reason,
prefer term puts stronger emphasis role module: rephrase critical information planning task way directly useful search algorithms.
three building blocks Fast Downward (translation, knowledge compilation, search),
least time-critical part, always requiring less time translation dominated search
trivial tasks.
Knowledge compilation comprises three items. First foremost, compute domain
transition graph state variable. domain transition graph state variable encodes
circumstances variable change value, i. e., values domain
transitions values, operators axioms responsible transition, conditions state variables associated transition. Domain
transition graphs described Section 5.1. central concept computation
causal graph heuristic, described Section 6.1.
Second, compute causal graph planning task. domain transition graphs encode dependencies values given state variable, causal graph encodes dependencies
different state variables. example, given location planning task unlocked
means key carried agent, variable representing lock state
location dependent variable represents whether key carried.
dependency encoded arc causal graph. Like domain transition graphs, causal
graphs central concept computation causal graph heuristic, giving name.
causal graph heuristic requires causal graphs acyclic. reason, knowledge compilation component also generates acyclic subgraph real causal graph cycles occur.
amounts relaxation planning task operator preconditions ignored.
addition usefulness causal graph heuristic, causal graphs also key concept
focused iterative-broadening search algorithm introduced Section 6.5. discuss causal
graphs Section 5.2.
205

fiH ELMERT

Third, compute two data structures useful forward-searching algorithm
MPTs, called successor generators axiom evaluators. Successor generators compute set
applicable operators given world state, axiom evaluators compute values derived
variables given reduced state. designed job quickly possible,
especially important focused iterative-broadening search algorithm, compute heuristic estimates thus requires basic operations expanding search node
implemented efficiently. data structures discussed Section 5.3.
5.1 Domain Transition Graphs
domain transition graph state variable representation ways variable
change value, conditions must satisfied value changes allowed. Domain transition graphs introduced Jonsson Backstrom (1998a) context
SAS+ planning. formalization domain transition graphs generalizes original definition
planning tasks involving axioms conditional effects.
Definition 5 Domain transition graphs
Let = hV, s0 , s? , A, Oi multi-valued planning task, let v V state variable .
domain transition graph v, symbols DTG(v), labelled directed graph vertex
set Dv . v fluent, DTG(v) contains following arcs:
effect cond v := d0 operator precondition pre pre cond
contains condition v = d, arc 0 labelled pre cond \ {v = d}.
effect cond v := d0 operator precondition pre pre cond
contain condition v = v , arc Dv \ {d0 } d0
labelled pre cond.
v derived variable, DTG(v) contains following arcs:
axiom cond v := d0 cond contains condition v = d, arc
d0 labelled cond \ {v = d}.
axiom cond v := d0 cond contain condition v =
Dv , arc Dv \ {d0 } d0 labelled cond.
Arcs domain transition graphs called transitions. labels referred
conditions transition.
Domain transition graphs weighted, case transition associated
non-negative integer weight. Unless stated otherwise, assume transitions derived
operators weight 1 transitions derived axioms weight 0.
definition somewhat lengthy, informal content easy grasp: domain transition graph v contains transition 0 exists operator axiom
change value v d0 . transition labelled conditions state
variables must true transition shall applied. Multiple transitions
values using different conditions allowed occur frequently.
already seen domain transition graphs introductory section (Figs. 3 4), although introduced informally show arc labels usually associated
206

fiT FAST OWNWARD P LANNING YSTEM

= open
(1, 1)

(2, 1)

(3, 1)

(2, 1)

(2, 2)

(3, 2)

r=

r = (1, 1), k = carried

closed

r = (2, 2), k = carried

r=

open

(1, 2)

(1,

1)

1)

2)
(1,
r=

2)
(1,

r = (2, 1)

(1,

r = (2, 1)

r=

r=

r=

(3, 1)

1)
(3,

r=
r=

carried

r = (2, 2)

(1, 2)

(1, 1)

r = (2, 2)

= open

= open

(3,

(3,
(3,

2)

1)

2)

(3, 2)

r = (3, 1), k = carried
(2, 2)

Figure 10: Domain transition graphs G RID task. Top left: DTG(r) (robot); right: DTG(k)
(key); bottom left: DTG(d) (door).

transitions. Fig. 10 shows examples simple task G RID domain, featuring 3 2 grid single initially locked location centre upper row, unlockable
single key. MPT encoding task, three state variables: variable r
Dr = { (x, y) | x {1, 2, 3}, {1, 2} } encodes location robot, variable k
Dk = Dr {carried} encodes state key, variable = {closed, open}
encodes state initially locked grid location.
operators MPT unary (i. e., single effect) leave aside axioms
moment, strong correspondence state space MPT
domain transition graphs. Since vertices domain transition graphs correspond values state
variables, given state represented selecting one vertex domain transition graph, called
active vertex state variable. Applying operator means changing active vertex
state variable performing transition corresponding domain transition graph.
Whether transition allowed depends condition, checked
active vertices domain transition graphs.
Let us use G RID example illustrate correspondence. Consider initial state
robot location (1, 1), key location (3, 2), door locked. represent
placing pebbles appropriate vertices three domain transition graphs. want
move pebble domain transition graph key location (2, 1). done
moving robot pebble vertex (1, 2), (2, 2), (3, 2), moving key pebble vertex
carried, moving robot pebble back vertex (2, 2), moving door pebble open, moving
robot pebble vertex (2, 1) finally moving key pebble vertex (2, 1).
207

fiH ELMERT

= open, r = (1, 1)
= open, r = (3, 1)

= open, r = (1, 1)

= closed


>



r = (2, 1)

>

r = (1, 2)

= open, r = (3, 1)

r = (2, 2)
r = (3, 2)

Figure 11: Domain transition graphs freezing variable G RID task, normal (left)
extended (right). Note extended graph shows change state
freezing (>) freezing ().

example shows plan execution viewed simultaneous traversal domain
transition graphs (cf. Domshlak & Dinitz, 2001). important notion Fast Downward
causal graph heuristic computes heuristic estimates solving subproblems
planning task looking paths domain transition graphs basically way described.
mentioned before, view MPT planning completely accurate unary tasks
without axioms, domain transition graphs indeed complete representation
state space. non-unary operators, would need link certain transitions different domain
transition graphs belong operator. could executed together.
axioms, would need mark certain transitions mandatory, requiring taken
whenever possible. (This intended rough analogy leaves details like layered
axioms.)
previous work (Helmert, 2004), successfully applied view planning
STRIPS tasks. Extending notion plans conditional effects provides challenges domain transition graphs always consider planning operators one effect time,
case effect condition simply seen part operator precondition. However, axioms
provide challenge easily overlooked. want change value fluent
d0 , domain transition graph contains important information; find path 0
try find associated conditions achieved. Consider problem
derived state variable. Let us assume unlocking location G RID example leads
drought, causing robot freeze enters horizontally adjacent location. could encode
new derived variable f (for freezing) domain f = {>, }, defined axioms
= open, r = (1, 1) f := > = open, r = (3, 1) f := >. domain transition graph
DTG(f ) depicted Fig. 11 (left).
problem domain transition graph tell us change
state variable f > . general, MPTs derived STRIPS tasks derived
predicates occur negatively condition, domain transition graph contain sufficient
information changing value derived variable true false. Derived variables
208

fiT FAST OWNWARD P LANNING YSTEM

never assume value due derivation value; negation failure semantics,
assume value default value derived. want reason
ways setting value derived variable , need make information explicit.
logical notation, whether derived variable assumes given value triggering
axiom given layer determined formula disjunctive normal form, one disjunct
axiom setting value. example, axioms = open, r = (1, 1) f := >
= open, r = (3, 1) f := > correspond DNF formula (d = open r = (1, 1)) (d =
open r = (3, 1)). want know rules trigger, must negate formula,
leading CNF formula (d 6= open r 6= (1, 1))(d 6= open r 6= (3, 1)). able encode
information domain transition graph, need replace inequalities equalities
translate formula back DNF. Since transformations increase formula size
dramatically, apply simplifications along way, removing duplicated dominated disjuncts.
result case DNF formula = closed r = (2, 1) r = (1, 2) r = (2, 2) r =
(3, 2).
domain transition graph derived variable enriched contain possible
ways causing variable assume value called extended domain transition graph,
shown G RID example Fig. 11 (right). Since computing extended domain transition
graph costly always necessary, knowledge compilation component scans
conditions planning task (axioms, operator preconditions effect conditions, goal)
occurrences pairings type v = derived variables v. Extended domain transition
graphs computed derived variables required.
Note negative occurrences derived variables cascade: u, v w derived
variables domain {>, } condition v = present operator precondition,
moreover v defined axiom u = >, w = > v := >, v assumes value
whenever u w do, would require extended domain transition graphs u w well.
hand, multiple layers negation failure cancel out: derived
variable v occurs conditions form v = never positive form defined
axiom u = , w = v := >, necessarily require extended domain transition
graphs u w.
general, whether need extended domain transition graphs derived variable
determined following rules:
v derived variable condition v = 6= appears operator
precondition, effect condition goal, v used positively.
v derived variable condition v = appears operator precondition,
effect condition goal, v used negatively.
v derived variable condition v = 6= appears body
axiom whose head used positively (negatively), v used positively (negatively).
v derived variable condition v = appears body axiom
whose head used positively (negatively), v used negatively (positively).
knowledge compilation component computes extended domain transition graphs derived variables used negatively (standard) domain transition graphs state
variables. Normal domain transition graphs computed going set axioms
209

fiH ELMERT

set operator effects following Definition 5, reasonably straight-forward; computation extended domain transition graphs outlined above. Therefore, algorithmic
aspects topic require discussion.
5.2 Causal Graphs
Causal graphs introduced informally introduction. formal definition.
Definition 6 Causal graphs
Let multi-valued planning task variable set V. causal graph , symbols
CG(), directed graph vertex set V containing arc (v, v 0 ) iff v 6= v 0 one
following conditions true:
domain transition graph v 0 transition condition v.
set affected variables effect list operator includes v v 0 .
first case, say arc induced transition condition. second case say
induced co-occurring effects.
course, arcs induced transition conditions arcs induced co-occurring effects
mutually exclusive. causal graph arc generated reasons.
Informally, causal graph contains arc source variable target variable changes
value target variable depend value source variable. arcs
included also dependency form effect source variable. agrees
definition dependency graphs Jonsson Backstrom (1998b), although authors
distinguish two different ways arc graph introduced using
labelled arcs.
Whether co-occurring effects induce arcs causal graph depends intended semantics: arcs included, set causal graph ancestors anc(v) variable
v precisely variables relevant goal change value v. Plans
goal computed without considering variables outside anc(v), eliminating variables outside anc(v) planning task simplifying axioms operators accordingly.
call achievability definition causal graphs, causal graphs encode variables
important achieving given assignment state variable.
However, achievability definition, planner considers anc(v) generating
action sequence achieves given valuation v may modify variables outside anc(v), i. e.,
generated plans side effects could destroy previously achieved goals otherwise
negative impact overall planning. Therefore, prefer definition, call
separability definition causal graphs.
5.2.1 ACYCLIC C AUSAL G RAPHS
Following separability definition causal graphs, solving subproblem variables anc(v)
always possible without changing values outside anc(v). leads us following
observation.
210

fiT FAST OWNWARD P LANNING YSTEM

Observation 7 Acyclic causal graphs strongly connected domain transition graphs
Let MPT CG() acyclic, domain transition graphs strongly connected,
derived variables, trivially false conditions occur operators goals.
solution.
trivially false conditions, mean conditions kind {v = d, v = 0 } 6= d0 .
Note similarity Observation 7 results Williams Nayak (1997) planning domains unary operators, acyclic causal graphs reversible transitions. separability
definition causal graphs, acyclic causal graphs imply unariness operators operators
several effects introduce causal cycles. Moreover, strong connectedness domain transition
graphs closely related Williams Nayaks reversibility property, although weaker
requirement.
truth observation easily seen inductively: planning task one state
variable domain transition graph strongly connected, state (of one variable)
transformed state applying graph search techniques. planning task
several state variables causal graph acyclic, pick sink causal graph, i. e.,
variable v without outgoing arcs, check goal defined variable. not,
remove variable task, thus reducing problem one fewer state variables,
solved recursively. yes, search path 0 (v) s? (v) domain transition graph
v, guaranteed exist graph strongly connected. yields high-level
plan setting v s? (v) fleshed recursively inserting plans setting
variables predecessors v causal graph values required transitions
form high-level plan. desired value v set, v eliminated
planning task remaining problem solved recursively.
algorithm shown Fig. 12. Although backtrack-free, require exponential
time execute generated plans exponentially long. unavoidable; even
MPTs satisfy conditions Observation 7, shortest plans exponentially long.
family planning tasks property given proof Theorem 4.4 article
Backstrom Nebel (1995).
method solving multi-valued planning tasks essentially planning refinement:
begin constructing abstract skeleton plan, merely path domain transition
graph, lower level abstraction adding operators satisfy preconditions required
transitions taken path. Strong connectedness domain transition graphs guarantees
every abstract plan actually refined concrete plan. precisely Bacchus
Yangs (1994) downward refinement property (cf. Section 2.1).
5.2.2 G ENERATING



P RUNING C AUSAL G RAPHS

usefulness causal graphs planning refinement limited acyclic case. Consider subset V 0 task variables contains causal graph descendants. general,
restrict task V 0 removing occurrences variables initial state, goal,
operators axioms, obtain abstraction original problem satisfies Knoblocks
(1994) ordered monotonicity property (Section 2.1).
Unfortunately, one major problem approach requirement include causal
graph descendants quite limiting. uncommon causal graph planning task
strongly connected, case technique allow us abstract away variables
211

fiH ELMERT

algorithm solve-easy-MPT(V, s0 , s? , O):
s? = :
{ goal empty: empty plan solution. }
return hi.
else:
Let v V variable occurring preconditions effect conditions O.
{ variable always exists causal graph task acyclic. }
V 0 := V \ {v}.
0 := { | affect v }.
plan := hi
s? (v) defined:
Let t1 , . . . , tk path transitions DTG(v) 0 (v) s? (v).
{ t1 , . . . , tk high-level plan reaches goal v,
ignores preconditions variables. }
{t1 , . . . , tk }:
{ Recursively find plan achieves conditions t. }
Let cond condition operator associated t.
Let s00 state reached executing plan, restricted V 0 .
Extend plan solve-easy-MPT(V 0 , s00 , cond, 0 ).
Extend plan o.
{ dealing v, recursively plan goals remaining variables. }
Let s00 state reached executing plan, restricted V 0 .
s0? := s? restricted V 0 .
Extend plan solve-easy-MPT(V 0 , s00 , s0? , 0 ).
return plan
Figure 12: Planning algorithm MPTs acyclic causal graph strongly connected domain
transition graphs.

212

fiT FAST OWNWARD P LANNING YSTEM

all. However, heuristic approach, free simplify planning task. particular,
ignoring operator preconditions purposes heuristic evaluation, make
arbitrary causal graph acyclic. Clearly, aspects real task ignore, worse
expect heuristic approximate actual goal distance. Considering this, aim ignore
little information possible. explain done.
knowledge compilation component begins causal graph processing generating
full causal graph (Definition 6). One consequence separability definition causal graphs
state variables ancestors variables mentioned goal completely
irrelevant. Therefore, computed graph, compute causal graph ancestors
variables goal. state variables found goal ancestors eliminated planning task causal graph, associated operators axioms removed. 4
Afterwards, compute pruned causal graph, acyclic subgraph causal graph
vertex set. try fashion important causal dependencies retained
whenever possible. specifically, apply following algorithm.
First, compute strongly connected components causal graph. Cycles occur
within strongly connected components, component dealt separately. Second,
connected component, compute total order vertices, retaining
arcs (v, v 0 ) v v 0 . v v 0 , say v 0 higher level v. total order
computed following way:
1. assign weight arc causal graph. weight arc n induced
n axioms operators. lower cumulated weight incoming arcs vertex,
fewer conditions ignored assigning low level vertex.
2. pick vertex v minimal cumulated weight incoming arcs select
lowest level, i. e., set v v 0 vertices v 0 strongly connected component.
3. Since v dealt with, remove vertex incident arcs consideration
rest ordering algorithm.
4. remaining problem solved iteratively applying technique order
vertices single vertex remains.
reader notice pruning choices within strongly connected component
performed greedy algorithm. could also try find sets arcs minimal total weight
eliminating arcs results acyclic graph. However, NP-equivalent problem,
even case unweighted graphs (Garey & Johnson, 1979, problem GT8).
generating pruned causal graph, also prune domain transition graphs removing transition labels DTG(v) conditions variables v 0 v v 0 .
conditions ignored heuristic computation. Finally, simplify domain transition
graphs removing dominated transitions: 0 transitions two values
variable, condition proper subset condition 0 , transition
easier apply t0 , remove t0 . Similarly, several transitions identical
conditions, keep one them.
4. simplification closely related Knoblocks criterion problem-specific ordered monotonicity property
(Knoblock, 1994).

213

fiH ELMERT

t1

t2

a1

p1

p2

a2

Figure 13: Causal graph L OGISTICS task. State variables ai encode locations
trucks airplanes, state variables p locations packages.

f1

p1

f2

f3

f1

f2

f3

l1

l2

l1

l2

c1

c2

c1

c2

p2

p1

p2

Figure 14: Causal graph YSTERY task (left) relaxed version task (right). State
variables fi encode fuel location, state variables l ci encode locations
remaining capacities trucks, state variables p encode locations packages.

5.2.3 C AUSAL G RAPH E XAMPLES
give impression types causal graphs typically found standard benchmarks
effects pruning, show examples increasing graph complexity.
first simplest example, Fig. 13 shows causal graph task L OGISTICS
domain, featuring two trucks, two airplanes two packages. seen, graph acyclic,
requires pruning causal graph heuristic. Since L OGISTICS tasks also feature strongly
connected domain transition graphs, even solved polynomial solve-easy-MPT
algorithm.
slightly complicated example, next figure, Fig. 14, shows task YS TERY domain three locations, two trucks two packages. causal graph contains
number cycles, mostly local. pruning arcs vertices l fj , ignore
214

fiT FAST OWNWARD P LANNING YSTEM

r

r



l



k1

k2

l

k1

k2

Figure 15: Causal graph G RID task (left) relaxed version task (right). State
variable r encodes location robot, encodes status robot arm (empty
carrying key), l encodes status locked location (locked open), k 1
k2 encode locations two keys.

fact must move trucks certain locations want use fuel location.
using fuel useful thing do, big loss information. pruning arcs
vertices pi cj , ignore fact vehicles increase decrease current
capacity unloading loading packages. Compared heuristics based ignoring delete effects, great loss information, since ignoring delete effects YSTERY domain
almost amounts ignoring capacity fuel constraints altogether. pruning arcs,
eliminate cycles causal graph, YSTERY domain considered fairly
well-behaved.
worse case shown Fig. 15, shows example G RID domain
arbitrary number locations, single one locked. two keys, one
unlock locked location. Eliminating cycles requires minor relaxations regarding
status robot arm (empty non-empty), also one major simplification, namely
elimination arc l r representing fact robot enter locked
location unlocked.
(nearly) worst-case example, consider task B LOCKSWORLD domain (no figure).
typical MPT encoding uses one state variable h encoding whether hand empty
two state variables per block task: i-th block, encodes whether block
lying table, bi encodes block lying top it, clear held
arm. causal graph task, variable h ingoing arcs outgoing arcs
state variables, state variables b connected directions.
state variables ti slightly simpler connection structure, connected h
bi value i. relaxation problem eliminates cycles causal
graph loses large amount information, surprising EPOT domain,
includes B LOCKSWORLD subproblem, one precursor Fast Downward fared
worst (Helmert, 2004). Still, pointed planners ignore delete effects
similar problems B LOCKSWORLD-like domains, comparison FF causal
graph heuristics article shows.
215

fiH ELMERT

5.3 Successor Generators Axiom Evaluators
addition good heuristic guidance, forward searching planning system needs efficient methods
generating successor states applied benchmark suite international
planning competitions. domains, causal graph heuristic popular methods
like FF heuristic provide excellent goal estimates, yet still planning time-consuming
long plans vast branching factors.
variant best-first search implemented Fast Downward compute heuristic
estimate state generated. Essentially, heuristic evaluations computed
closed nodes, computation deferred nodes search frontier. domains
strong heuristic guidance large branching factors, number nodes frontier
far dominate number nodes closed set. case point, consider problem instance
ATELLITE #29. solving task, default configuration Fast Downward computes
heuristic estimates 67 597 world states adding 107 233 381 states frontier. Clearly,
determining set applicable operators quickly critical importance scenario.
ATELLITE tasks, almost 1 000 000 ground operators, try
avoid individually checking operator applicability. Similarly, biggest PSR tasks,
100 000 axioms must evaluated state compute values derived
variables, computation must made efficient. purposes, Fast Downward uses two
data structures called successor generators axiom evaluators.
5.3.1 UCCESSOR G ENERATORS
Successor generators recursive data structures similar decision trees. internal nodes
associated conditions, likened decisions decision tree, leaves
associated operator lists likened set classified samples decision tree
leaf. formally defined follows.
Definition 8 Successor generators
successor generator MPT = hV, 0 , s? , A, Oi tree consisting selector nodes
generator nodes.
selector node internal node tree. associated variable v V called
selection variable. Moreover, |D v |+1 children accessed via labelled edges, one edge labelled
v = value Dv , one edge labelled >. latter edge called dont care
edge selector.
generator node leaf node tree. associated set operators called
set generated operators.
operator must occur exactly one generator node, set edge labels
leading root node (excluding dont care edges) must equal precondition o.
Given successor generator MPT state , compute set
applicable operators traversing successor generator follows, starting root:
selector node selection variable v, follow edge v = s(v) dont care edge.
generator node, report generated operators applicable.
216

fiT FAST OWNWARD P LANNING YSTEM

algorithm evaluate-axiom-layer(s, ):
axiom Ai :
a.counter := |a.cond|
variable v:
axiom Ai condition v = s(v) body:
a.counter := a.counter 1
exists axiom Ai a.counter = 0 yet considered:
Let hv, di head axiom.
s(v) 6= d:
s(v) :=
axiom Ai condition v = body:
a.counter := a.counter 1
Figure 16: Computing values derived variables given planning state.
build successor generator , apply top-down algorithm considers task
variables arbitrary order v1 v2 vn . root node, choose v1 selection variable classify set operators according preconditions respect v 1 . Operators
precondition v1 = represented child root accessed edge
corresponding label, operators without preconditions v 1 represented child
root accessed dont care edge. children root, choose v 2 selection
variable, grandchildren v3 , on.
one exception rule avoid creating unnecessary selection nodes: operator
certain branch tree condition v , vi considered selection variable
branch. construction branch ends variables considered,
stage generator node created operators associated branch.
5.3.2 XIOM E VALUATORS
Axiom evaluators simple data structure used efficiently implementing well-known
marking algorithm propositional Horn logic (Dowling & Gallier, 1984), extended modified
layered logic programs correspond axioms MPT. consist two parts.
Firstly, indexing data structure maps given variable/value pairing given axiom layer
set axioms given layer whose body pairing appears. Secondly, set counters,
one axiom, counts number conditions axiom yet derived.
Within Fast Downward, axioms evaluated two steps. First, derived variables set
default value . Second, algorithm evaluate-axiom-layer (Fig. 16) executed axiom
layer sequence determine final values derived variables.
assume reader familiar enough marking algorithm require much
explanation, point test whether axiom ready trigger implemented means queue axioms put soon counter reaches 0. actual
implementation evaluate-axiom-layer within Fast Downward initializes axiom counters slightly
efficiently indicated pseudo-code. However, minor technical detail,
turn remaining piece Fast Downwards architecture, search component.
217

fiH ELMERT

6. Search
Unlike translation knowledge compilation components, single
mode execution, search component Fast Downward perform work various alternative ways. three basic search algorithms choose from:
1. Greedy best-first search: standard textbook algorithm (Russell & Norvig, 2003),
modified technique called deferred heuristic evaluation mitigate negative influence wide branching. also extended algorithm deal preferred operators, similar FFs helpful actions (Hoffmann & Nebel, 2001). discuss greedy best-first
search Section 6.3. Fast Downward uses algorithm together causal graph
heuristic, discussed Section 6.1.
2. Multi-heuristic best-first search: variation greedy best-first search evaluates
search states using multiple heuristic estimators, maintaining separate open lists each.
Like variant greedy best-first search, supports use preferred operators. Multiheuristic best-first search discussed Section 6.4. Fast Downward uses algorithm
together causal graph FF heuristics, discussed Sections 6.1 6.2.
3. Focused iterative-broadening search: simple search algorithm use
heuristic estimators, instead reduces vast set search possibilities focusing
limited operator set derived causal graph. experimental algorithm;
future, hope develop basic idea algorithm robust method.
Focused iterative-broadening search discussed Section 6.5.
two heuristic search algorithms, second choice must made regarding use
preferred operators. five options supported planner:
1. use preferred operators.
2. Use helpful transitions causal graph heuristic preferred operators.
3. Use helpful actions FF heuristic preferred operators.
4. Use helpful transitions preferred operators, falling back helpful actions
helpful transitions current search state.
5. Use helpful transitions helpful actions preferred operators.
five options combined two heuristic search algorithms,
total eleven possible settings search component, ten using one
heuristic algorithms one using focused iterative-broadening search.
addition basic settings, search component configured execute several
alternative configurations parallel making use internal scheduler. configurations
Fast Downward participated IPC4 made use feature running one configuration
heuristic search algorithms parallel focused iterative-broadening search. heuristic
search algorithm, configuration Fast Downward employed greedy best-first search helpful
transitions, falling back helpful actions necessary (option 4.). configuration Fast Diagonally Downward employed multi-heuristic best-first search using helpful transitions helpful
actions preferred operators (option 5.).
218

fiT FAST OWNWARD P LANNING YSTEM

avoid confusion complete Fast Downward planning system particular
configuration called Fast Downward, refer IPC4 planner configurations FD
FDD rest paper. name planning system whole never abbreviated.
6.1 Causal Graph Heuristic
causal graph heuristic centrepiece Fast Downwards heuristic search engine. estimates cost reaching goal given search state solving number subproblems
planning task derived looking small windows (pruned) causal graph.
additional intuitions design heuristic discussion theoretical aspects,
refer article heuristic first introduced (Helmert, 2004).
6.1.1 C ONCEPTUAL V IEW



C AUSAL G RAPH H EURISTIC

state variable v pair values d, 0 Dv , causal graph heuristic computes
heuristic estimate cost v (d, d0 ) cost changing value v 0 , assuming
state variables carry values current state. (This simplification. Cost
estimates computed state variables v values never required.
ignore fact discussing heuristic conceptual level.) heuristic estimate
given state sum costs cost v (s(v), s? (v)) variables v goal
condition s? (v) defined.
Conceptually, cost estimates computed one variable other, traversing (pruned)
causal graph bottom-up fashion. bottom-up, mean start variables
predecessors causal graphs; call order computation bottom-up
consider variables change state accord low-level, variables
whose state transitions require help variables complex transition semantics
thus considered high-level. Note figures depicting causal graphs, high-level
variables typically displayed near bottom.
variables without predecessors causal graph, cost v (d, d0 ) simply equals cost
shortest path d0 (pruned) domain transition graph DTG(v). variables,
cost estimates also computed graph search domain transition graph. However,
conditions transitions must taken account path planning, addition
counting number transitions required reach destination value, also consider costs
achieving value changes variables necessary set transition conditions.
important point computing values cost v (d, d0 ), completely consider
interactions state variable v predecessors causal graph. changing
value d0 requires several steps steps associated condition
variable v 0 , realize v 0 must assume values required conditions sequence.
example, v represents package transportation task must moved B
means vehicle located C, recognize vehicle must first move C
B order drop package B. different way HSPor FF-based heuristics work examples. However, consider interactions
immediate predecessors v causal graph. Interactions occur via several graph layers
captured heuristic estimator.
essence, compute cost v (d, d0 ) solving particular subproblem MPT, induced
variable v predecessors pruned causal graph. subproblem, assume
219

fiH ELMERT

algorithm compute-costs-bottom-up(, s):
variable v , traversing pruned causal graph bottom-up order:
Let V 0 set immediate predecessors v pruned causal graph.
pair values (d, d0 ) Dv Dv :
Generate planning task v,d,d0 following components:
Variables: V 0 {v}.
Initial state: v = v 0 = s(v 0 ) v 0 V 0 .
Goal: v = d0 .
Axioms operators:
1. corresponding transitions pruned DTG v.
2. variables v 0 V 0 values e, e0 Dv0 , operator
precondition v 0 = e, effect v 0 = e0 cost cost0v (e, e0 ).
{ Note variables v 0 V 0 considered previously,
cost values known. }
Set costv (d, d0 ) cost plan solves v,d,d0 .
Figure 17: compute-costs-bottom-up algorithm, high-level description causal graph
heuristic.

v initially set d, want v assume value 0 , state variables carry
value current state. call planning problem local subproblem v, 0 ,
local subproblem v leave target value 0 open.
formalization intuitive notions cost estimates generated, consider
pseudo-code Fig. 17. reflect way heuristic values actually computed
within Fast Downward; algorithm figure would far expensive evaluate
search state. However, computes cost values Fast Downward does, provided
algorithm generating plans last line algorithm one one used
real cost estimator.
6.1.2 C OMPUTATION



C AUSAL G RAPH H EURISTIC

actual computation causal graph heuristic traverses causal graph top-down direction starting goal variables, rather bottom-up starting variables without causal
predecessors. fact, top-down traversal causal graph reason Fast Downwards
name.
Computing cost estimates top-down traversal implies algorithm computing
plans local subproblems given variable, typically yet know costs changing
state causal predecessors. algorithm compute-costs addresses evaluating
cost values dependent variables recursive invocations itself.
given variable-value pairing v = d, always compute costs cost v (d, d0 ) values
d0 Dv time, similar way Dijkstras algorithm computes shortest path
single source single destination vertex, single source possible destination
vertices. Computing costs values 0 (much) expensive computing
220

fiT FAST OWNWARD P LANNING YSTEM

one values, cost values determined, cache re-use
needed later parts computation heuristic value
current state.
fact, similarity shortest path problems superficial runs quite deeply.
ignore recursive calls computing cost values dependent variables, compute-costs basically implementation Dijkstras algorithm single-source shortest path problem
domain transition graphs. difference regular algorithm lies fact
know cost using arc advance. Transitions derived variables base cost
0 transitions fluents base cost 1, addition base cost, must pay
cost achieving conditions associated transition. However, cost achieving
given condition v 0 = e0 depends current value e state variable time transition
taken. Thus, compute real cost transition know values
dependent state variables relevant situation.
course, many different ways taking transitions domain transition graphs,
potentially leading different values dependent state variables. first introduced
causal graph heuristic, showed deciding plan existence local subproblems NPcomplete (Helmert, 2004), content approach lead complete
planning algorithm, long works well subproblems face practice.
approach chosen achieve value state variable v local subproblem
v quickly possible, following greedy policy. context Dijkstra algorithm,
means start finding cheapest possible plan make transition
value d0 . found cheapest possible plan d0 , commit it, annotating
vertex d0 domain transition graph local state obtained applying plan d0
current state. next step, look cheapest possible plan achieve another value 00 ,
either considering transitions start initial value d, considering transitions
continue plan d0 moving neighbour d0 . process iterated vertices
domain transition graph reached progress possible.
implementation follows Dijkstras algorithm (Fig. 18). implemented priority queue vector buckets maximal speed use cache avoid generating
costv (d, d0 ) value twice state. addition this, use global cache shared
throughout whole planning process need compute values cost v (d, d0 ) variables v ancestors pruned causal graph once. (Note cost v (d, d0 ) depends
current values ancestors v.)
Apart technical considerations, Fig. 18 gives accurate account
Fast Downwards implementation causal graph heuristic. details, including
complexity considerations worked-out example, refer original description
algorithm (Helmert, 2004).
6.1.3 TATES

NFINITE

H EURISTIC VALUE

noted Fast Downward uses incomplete planning algorithm determining solutions
local planning problems. Therefore, states cost v (s(v), s? (v)) = even though
goal condition v = s? (v) still reached. means cannot trust infinite values
returned causal graph heuristic. experience, states infinite heuristic evaluation
still possible reach goal rare, indeed treat states dead ends.
221

fiH ELMERT

algorithm compute-costs(, s, v, d):
Let V 0 set immediate predecessors v pruned causal graph .
Let DTG pruned domain transition graph v.
costv (d, d) := 0
costv (d, d0 ) := d0 Dv \ {d}
local-state := restricted V 0
unreached := Dv
unreached contains value d0 Dv cost v (d, d0 ) < :
Choose value d0 unreached minimizing cost v (d, d0 ).
unreached := unreached \ {d0 }
transition DTG leading 0 d00 unreached:
transition-cost := 0 v derived variable; 1 v fluent
pair v 0 = e0 condition t:
e := local-state d0 (v 0 )
call compute-costs(, s, v 0 , e).
transition-cost := transition-cost + cost v0 (e, e0 )
costv (d, d0 ) + transition-cost < cost v (d, d00 ):
costv (d, d00 ) := costv (d, d0 ) + transition-cost
local-state d00 := local-state d0
pair v 0 = e0 condition t:
local-state d00 (v 0 ) := e0
Figure 18: Fast Downwards implementation causal graph heuristic: compute-costs algorithm computing estimates cost v (d, d0 ) values d0 Dv state
MPT .

222

fiT FAST OWNWARD P LANNING YSTEM

turns states search frontier dead ends, cannot make progress
causal graph heuristic. case, use sound dead-end detection routine verify
heuristic assessment. turns frontier states indeed dead ends, report
problem unsolvable. Otherwise, search restarted FF heuristic (cf. Section 6.2),
sound purposes dead-end detection. 5
dead-end detection routine originally developed STRIPS-like tasks. However,
extending full MPTs easy; fact, changes core algorithm required, works
level domain transition graphs still sound applied tasks conditional
effects axioms. Since central aspect Fast Downward, discuss here,
referring earlier work instead (Helmert, 2004).
6.1.4 H ELPFUL RANSITIONS
Inspired Hoffmanns successful use helpful actions within FF planner (Hoffmann &
Nebel, 2001), extended algorithm computing causal graph heuristic
addition heuristic estimate, also generates set applicable operators considered useful
steering search towards goal.
compute helpful actions FF, Hoffmanns algorithm generates plan relaxed planning task defined current search state considers operators helpful belong
relaxed plan applicable current state.
approach follows similar idea. computing heuristic estimate cost v (s(v), s? (v))
variable v goal condition defined, look domain transition graph
v trace path transitions leading s(v) ? (v) gave rise cost estimate.
particular, consider first transition path, starting s(v). transition corresponds
applicable operator, consider operator helpful transition continue check
next goal. transition correspond applicable operator associated
conditions form v 0 = e0 currently satisfied, recursively look helpful transitions domain transition graph variable v 0 , checking path
generated computation cost v0 (s(v 0 ), e0 ).
recursive process continues found helpful transitions. Unlike case
FF, helpful actions found non-goal states, might find helpful
transition all. may case transition correspond applicable operator
even though associated conditions; happen operator preconditions
represented pruned domain transition graph due cycles causal graph. Even so,
found helpful transitions useful tool guiding best-first search algorithms.
6.2 FF Heuristic
FF heuristic named Hoffmanns planning algorithm name, context
originally introduced (Hoffmann & Nebel, 2001). based notion
relaxed planning tasks ignore negative interactions. context MPTs, ignoring negative
interactions means assume state variable hold several values simultaneously.
operator effect axiom sets variable v value original task corresponds
5. practice, never observed causal graph heuristic fail solvable task. Therefore, fallback
mechanism used unsolvable tasks ICONIC -F ULL ADL domain recognized
dead-end detection technique.

223

fiH ELMERT

effect axiom adds value range values assumed v relaxed task.
condition v = original task corresponds condition requiring element
set values currently assumed v relaxed task.
easy see applying operator solvable relaxed planning task never render
unsolvable. lead operators applicable goals true,
significant effect all. reason, relaxed planning tasks solved efficiently, even
though optimal solutions still NP-hard compute (Bylander, 1994). plan relaxation
planning task called relaxed plan task.
FF heuristic estimates goal distance world state generating relaxed plan
task reaching goal world state. number operators generated plan
used heuristic estimate. implementation FF heuristic necessarily
generate same, even equally long, relaxed plan FF. experiments, turn
problematic, implementations appear equally informative.
FF heuristic originally introduced ADL domains, extending tasks involving derived predicates straight-forward. One possible extension simply assume
derived predicate initially set default value treat axioms relaxed operators cost
0. slightly complicated, also accurate approach, derived variables initialized
actual value given world state, allowing relaxed planner achieve value
(or values) applying transitions extended domain transition graph derived
variable. followed second approach.
addition heuristic estimates, FF heuristic also exploited restricting biasing
choice operators apply given world state s. set helpful actions consists
operators relaxed plan computed applicable state. mentioned
introduction section, Fast Downward configured treat helpful actions
preferred operators.
wealth work FF heuristic literature, discuss further.
thorough treatment, point references (Hoffmann & Nebel, 2001; Hoffmann,
2001, 2002, 2005).
6.3 Greedy Best-First Search Fast Downward
Fast Downward uses greedy best-first search closed list default search algorithm.
assume reader familiar algorithm refer literature details (Russell &
Norvig, 2003).
implementation greedy best-first search differs textbook algorithm two ways.
First, treat helpful transitions computed causal graph heuristic helpful actions computed FF heuristic preferred operators. Second, performs deferred heuristic evaluation
reduce influence large branching factors. turn describing two search
enhancements.
6.3.1 P REFERRED PERATORS
make use helpful transitions computed causal graph heuristic helpful actions computed FF heuristic, variant greedy best-first search supports use so-called preferred operators. set preferred operators given state subset set applicable
operators state. operators considered preferred depends settings
224

fiT FAST OWNWARD P LANNING YSTEM

search component, discussed earlier. intuition behind preferred operators randomly
picked successor state likely closer goal generated preferred operator, case call preferred successor. Preferred successors considered
non-preferred ones average.
search algorithm implements preference maintaining two separate open lists, one
containing successors expanded states one containing preferred successors exclusively.
search algorithm alternates expanding regular successor preferred successor.
even iterations consider one open list, odd iterations other. matter
open list state taken from, successors placed first open list, preferred
successors additionally placed second open list. (Of course could limit first open
list contain non-preferred successors; however, typically total number successors
vast number preferred successors tiny. Therefore, cheaper add successors
first open list detect duplicates upon expansion scan list successors
determining element whether preferred.)
Since number preferred successors smaller total number successors,
means preferred successors typically expanded much earlier others. especially
important domains heuristic guidance weak lot time spent exploring plateaus.
faced plateaus, Fast Downwards open lists operate first-in-first-out fashion. (In
words: constant heuristic function, search algorithm behaves like breadth-first
search.) Preferred operators typically offer much better chances escaping plateaus since
lead significantly lower effective branching factors.
6.3.2 EFERRED H EURISTIC E VALUATION
Upon expanding state s, textbook version greedy best-first search computes heuristic
evaluation successor states sorts open list accordingly.
wasteful many successors heuristic evaluations costly, two conditions often
true heuristic search approaches planning.
second modification comes play. successor better heuristic
estimate generated early leads promising path towards goal, would like
avoid generating successors. Let us assume 1000 successors, 0 ,
10th successor generated, better heuristic estimate s. Furthermore, let us
assume goal reached 0 path non-increasing heuristic estimates.
would like avoid computing heuristic values 990 later successors altogether.
Deferred heuristic evaluation achieves computing heuristic estimates successors expanded state immediately. Instead, successors placed open list
together heuristic estimate state s, heuristic estimates computed
expanded, time used sorting successors open
list, on. general, state sorted open list according heuristic evaluation
parent, initial state exception. fact, need put successor
state open list, since require representation want evaluate
heuristic estimate. Instead, save memory storing reference parent state
operator transforming parent state successor state open list.
might clear approach lead significant savings time, since deferred
evaluation also means information available later. potential savings become
225

fiH ELMERT

apparent considering deferred heuristic evaluation together use preferred operators:
improving successor s0 state reached preferred operator, likely
expanded (via second open list) long successors even siblings
s. situation described above, exists non-increasing path 0 goal,
heuristic evaluations never computed successors s. fact, deferred heuristic
evaluation significantly improve search performance even preferred operators
used, especially tasks branching factors large heuristic estimate informative.
first glance, deferred heuristic evaluation might appear related another technique reducing effort expanding node within best-first search algorithm, namely Partial
Expansion (Yoshizumi, Miura, & Ishida, 2000). However, algorithm designed reducing
space requirements best-first search expense additional heuristic evaluations:
expanding node, Partial Expansion computes heuristic value successors,
stores open queue whose heuristic values fall certain relevance threshold.
later iterations, might turn threshold chosen low, case node
needs re-expanded heuristic values successors re-evaluated. general,
Partial Expansion never compute fewer heuristic estimates standard , usually
require less memory.
However, heuristic search approaches planning (and certainly Fast Downward), heuristic evaluations usually costly time memory storing open closed lists
limiting factor. thus willing trade memory time opposite way: Deferred
heuristic evaluation normally leads node expansions higher space requirements
standard best-first search heuristic values used guiding search less informative (they evaluate predecessor search node rather node itself). However, heuristic
computations required nodes actually removed open queue rather
nodes fringe, latter usually significantly numerous.
6.4 Multi-Heuristic Best-First Search
alternative greedy best-first search, Fast Downward supports extended algorithm called
multi-heuristic best-first search. algorithm differs greedy best-first search use
multiple heuristic estimators, based observation different heuristic estimators different weaknesses. may case given heuristic sufficient directing search
towards goal except one part plan, gets stuck plateau. Another heuristic
might similar characteristics, get stuck another part search space.
Various ways combining heuristics proposed literature, typically adding
together taking maximum individual heuristic estimates. believe often
beneficial combine different heuristic estimates single numerical value. Instead,
propose maintaining separate open list heuristic estimator, sorted according
respective heuristic. search algorithm alternates expanding state
open list. Whenever state expanded, estimates calculated according heuristic,
successors put open list.
Fast Downward configured use multi-heuristic best-first search, computes estimates causal graph heuristic FF heuristic, maintaining two open lists. course,
approach combined use preferred operators; case, search algorithm
maintains four open lists, heuristic distinguishes normal preferred successors.
226

fiT FAST OWNWARD P LANNING YSTEM

algorithm reach-one-goal(, v, d, cond):
{0, 1, . . . , max-threshold}:
Let set operators whose modification distance respect v
.
Assign cost c operator modification distance c
respect v.
Call uniform-cost-search algorithm closed list, using operator set ,
find state satisfying {v = d} cond.
return plan uniform-cost-search succeeded.
Figure 19: reach-one-goal procedure reaching state v = d. value max-threshold
equal maximal modification distance operator respect v.

6.5 Focused Iterative-Broadening Search
focused iterative-broadening search algorithm experimental piece Fast Downwards search arsenal. present form, algorithm unsuitable many planning domains,
especially containing comparatively different goals. Yet think might contain
nucleus successful approach domain-independent planning different
current methods, include completeness source inspiration.
algorithm intended first step towards developing search techniques emphasize
idea using heuristic criteria locally, limiting set operators apply, rather globally,
choosing states expand global set open states. made first experiments
direction observing large boost performance obtained using preferred
operators heuristic search. algorithm performed surprisingly well standard
benchmark domains, performing badly others.
name suggests, algorithm focuses search concentrating one goal time,
restricting attention operators supposedly important reaching goal:
Definition 9 Modification distances
Let MPT, let operator , let v variable .
modification distance respect v defined minimum, variables
v 0 occur affected variables effect list o, distance v 0 v CG().
example, operators modify v directly modification distance 0 respect
v, operators modify variables occur preconditions operators modifying v
modification distance 1, on. assume order change value variable,
operators low modification distance respect variable useful.
Fig. 19 shows reach-one-goal procedure achieving single goal MPT. time
being, assume cond parameter always . procedure makes use assumption
high modification distance implies low usefulness two ways. First, operators high
modification distance respect goal variable considered higher associated
cost, hence applied less frequently. Second, operators whose modification distance beyond
certain threshold forbidden completely. Instead choosing threshold priori, algorithm
227

fiH ELMERT

first tries find solution lowest possible threshold 0, increasing threshold 1
whenever previous search failed. uniform-cost-search algorithm mentioned Fig. 19
standard textbook method (Russell & Norvig, 2003).
Although ignorant fact time algorithm conceived, core idea
reach-one-goal new: Ginsberg Harvey (1992) present search technique called iterative
broadening, also based idea repeatedly sequence uninformed searches
ever-growing set operators. work demonstrates superiority iterative broadening standard depth-bounded search empirically analytically reasonable
assumption choices made branching point equally important. 6 original iterative broadening algorithm applies scenarios without knowledge problem domain,
chooses set operators may applied every search node randomly, rather
using heuristic information causal graph case. However, Ginsberg Harvey
already discuss potential incorporation heuristics operator selection. introduction operator costs (in form modification distances) new, fairly straightforward
extension heuristic information available.
focused iterative-broadening search algorithm based reach-one-goal method;
idea achieve goals planning task one other, using reach-one-goal
algorithm core subroutine satisfying individual goals. Since obvious good
order achieving goals would be, one invocation reach-one-goal started goal
parallel. one-goal solver focuses (supposedly) relevant operators reaching
particular goal, hope number states considered goal reached small.
one one-goal solvers reaches goal, resulting plan reported sub-searches
stopped. overall search algorithm commits part plan; situation
first goal reached considered new initial state.
situation, try satisfy second goal, starting parallel invocations
reach-one-goal possible second goal. course, lead situation
search algorithm oscillates goals, first achieving goal a, abandoning favour goal
b, without sign making real progress. Therefore, demand reach-one-goal achieves
second goal addition one reached first, setting cond argument accordingly.
two goals reached, sub-searches stopped, sub-searches third
goal started, on, goals reached.
sense, focusing technique similar beam search algorithm (Lowerre, 1976),
also performs fixed number concurrent searches avoid committing particular path
search space early. Beam search uses heuristic function evaluate branches
search abandoned new branches spawned. focused iterativebroadening search appear use heuristic evaluations first glance, number satisfied
goals state used evaluation criterion essentially way. One important difference beam search use modification distances relative particular goal, means
different beams explore state space qualitatively different ways.
one final twist: motivate reach-one-goal needlessly wander away satisfied goals, forbid applying operators undo previously achieved goals cond.
old idea called goal protection (Joslin & Roach, 1989). well-known protecting
6. See original analysis precise definition equally important (Ginsberg & Harvey, 1992). Ginsberg
Harveys assumption certainly valid practice, find much convincing competing model
goal states uniformly distributed across search fringe.

228

fiT FAST OWNWARD P LANNING YSTEM

algorithm reach-one-goal(, v, d, cond):
{0, 1, . . . , max-threshold}:
Let set operators whose modification distance respect v
affect state variable occurring cond.
Assign cost c operator modification distance c
respect v.
Call uniform-cost-search algorithm closed list, using operator set ,
find state satisfying {v = d} cond.
return plan uniform-cost-search succeeded.
{0, 1, . . . , max-threshold}:
Let set operators whose modification distance respect v
.
Assign cost c operator modification distance c
respect v.
Call uniform-cost-search algorithm closed list, using operator set ,
find state satisfying {v = d} cond.
return plan uniform-cost-search succeeded.
Figure 20: reach-one-goal procedure reaching state v = (corrected).
goals renders search algorithm incomplete, even state spaces operators reversible
local search approaches like focused iterative-broadening search would otherwise complete.
particular, search must fail planning tasks serializable (Korf, 1987). Therefore,
first solution attempt fails, algorithm restarted without goal protection. complete
procedure shown Fig. 20, concludes discussion Fast Downwards search component.

7. Experiments
evaluate performance Fast Downward, specifically differences various
configurations search component, performed number experiments set
benchmarks previous international planning competitions. purpose experiments compare Fast Downward state art PDDL planning, contrast
performance different search algorithms Fast Downward (greedy best-first search
without preferred operators, multi-heuristic best-first search without preferred operators,
focused iterative-broadening search).
clearly state purpose experiments, let us also point two areas worthy study
choose investigate here:
compare causal graph heuristic heuristics, FF HSP
heuristics. comparison would require evaluating different heuristics within otherwise identical planning systems. performed experiment (Helmert,
2004) thus prefer dedicate section evaluation complete Fast Downward
planning system, rather heuristic function.
229

fiH ELMERT

give final answer question Fast Downward performs well badly
domains analyse. observe bad performance, try give plausible
explanation this, conduct full-blown study heuristic quality spirit
Hoffmanns work FF h+ heuristics (Hoffmann, 2005). believe
much could learned investigation, major undertaking would go
beyond scope article.
aim section evaluate Fast Downward planner whole,
number algorithmic questions address. example, one might wonder (if
any) speed-up obtained using successor generators simpler methods test
operator applicability whenever node expanded. Another question concerns extent
deferred heuristic evaluation affects search performance. keep section reasonable
length, discuss either questions here. However, conducted experiments
addressing them, include results electronic appendix paper. 7
7.1 Benchmark Set
benchmark set use consists propositional planning tasks fully automated
tracks first four international planning competitions hosted AIPS 1998, AIPS 2000, AIPS
2002 ICAPS 2004. set benchmark domains shown Fig. 21. Altogether, benchmark suite comprises 1442 tasks. (The numbers Fig. 21 add 1462, 20 ATELLITE
instances introduced IPC3 also part benchmark set IPC4,
count once.)
distinguish three classes domains:
STRIPS domains: domains feature derived predicates conditional effects,
conditions appearing goal operators conjunctions positive literals.
ADL domains: domains make use conditional effects operator and/or contain
general conditions simple conjunctions goals operators. However,
require axioms.
PDDL2.2 domains: domains use full range propositional PDDL2.2, including
features present ADL domains axioms.
IPC4, domains presented different formulations, meaning realworld task encoded several different ways. Participants asked work one
formulation per domain, able choose preferred formulation given domain freely.
example, IRPORT domain available STRIPS formulation ADL formulation.
However, organizers strictly follow rule considering different encodings
real-world task different formulations, rather different domains proper. Namely,
PSR-M IDDLE P ROMELA domains, encodings without axioms available,
considered different domains grounds encodings without axioms
7. See http://www.jair.org/. short summary successor generators speed search two
orders magnitude extreme cases like largest ATELLITE tasks, little impact performance
time. Deferred heuristic evaluation beneficial domains, speed-ups one order
magnitude common, somewhat beneficial majority domains, speed-ups 2 4,
rarely detrimental performance.

230

fiT FAST OWNWARD P LANNING YSTEM

Competition

Domain

Class

Number tasks

IPC1 (AIPS 1998)

SSEMBLY
G RID
G RIPPER
L OGISTICS
OVIE
YSTERY
MP RIME

ADL
STRIPS
STRIPS
STRIPS
STRIPS
STRIPS
STRIPS

30
5
20
35
30
30
35

IPC2 (AIPS 2000)

B LOCKSWORLD
F REECELL
L OGISTICS
ICONIC -STRIPS
ICONIC -S IMPLE ADL
ICONIC -F ULL ADL
CHEDULE

STRIPS
STRIPS
STRIPS
STRIPS
ADL
ADL
ADL

35
60
28
150
150
150
150

IPC3 (AIPS 2002)

EPOT
RIVERLOG
F REECELL
ROVERS
ATELLITE
Z ENOTRAVEL

STRIPS
STRIPS
STRIPS
STRIPS
STRIPS
STRIPS

22
20
20
20
20
20

IPC4 (ICAPS 2004)

IRPORT
P ROMELA -O PTICALT ELEGRAPH
P ROMELA -P HILOSOPHERS
P IPESWORLD -N OTANKAGE
P IPESWORLD -TANKAGE
PSR-S MALL
PSR-M IDDLE
PSR-L ARGE
ATELLITE

STRIPS
PDDL2.2
PDDL2.2
STRIPS
STRIPS
STRIPS
PDDL2.2
PDDL2.2
STRIPS

50
48
48
50
50
50
50
50
36

Figure 21: Planning domains first four international planning competitions.

231

fiH ELMERT

much larger hence likely difficult solve. apply formulation vs. encoding view
strictly thus consider one PSR-M IDDLE domain one domain two
P ROMELA variants, P ROMELA -P HILOSOPHERS P ROMELA -O PTICALT ELEGRAPH.
IPC1 benchmark set, tasks solvable except 11 YSTERY instances.
IPC2 benchmark set, tasks solvable except 11 ICONIC -F ULL ADL instances.
IPC3 benchmarks solvable. IPC4, checked instances P IPESWORLD TANKAGE domain, assume tasks solvable.
run heuristic search modes, Fast Downward proves unsolvability
unsolvable YSTERY ICONIC -F ULL ADL tasks using dead-end detection routine described earlier article causal graph heuristic (Helmert, 2004), cases
ICONIC -F ULL ADL domain exhaustively searching states finite FF heuristic.
course, unsolvable task proved unsolvable planner, report successfully
solved instance experimental results.
7.2 Experimental Setup
discussed Section 6, eleven possible configurations Fast Downwards search
component. However, equally reasonable. example, use FFs helpful
actions, would seem wasteful use FF heuristic estimate, since two calculated
together. Therefore, greedy best-first search setup, exclude configurations FF
helpful actions always computed. multi-heuristic best-first search setup, exclude
configurations one type preferred operators considered, other, since
would seem arbitrary choice. leaves us six different configurations
planner:
1. G: Use greedy best-first search without preferred operators.
2. G + P: Use greedy best-first search helpful transitions preferred operators.
3. G + P+ : Use greedy best-first search helpful transitions preferred operators. Use
helpful actions preferred operators states helpful transitions.
4. M: Use multi-heuristic best-first search without preferred operators.
5. + P: Use multi-heuristic best-first search helpful transitions helpful actions
preferred operators.
6. F: Use focused iterative-broadening search.
apply planner configurations 1442 benchmark tasks, using
computer 3.066 GHz Intel Xeon CPU machine used IPC4 set
memory limit 1 GB timeout 300 seconds.
compare Fast Downward state art, try solve benchmark
best-performing planners literature. Unfortunately, involves intricacies:
planners publicly available, others cover restricted subset PDDL2.2.
main experiment, thus partition benchmark domains three sets depending
planners available comparison.
232

fiT FAST OWNWARD P LANNING YSTEM

Domain

Task

Configuration

F REECELL (IPC2)
G RID
MP RIME
PSR-L ARGE
ATELLITE (IPC4)

probfreecell-10-1
prob05
prob14
p30-s179-n30-l3-f30
p33-HC-pfile13

M+P


G+P
M+P

Preprocessing
9.30
10.04
22.38
43.43
180.74

Search
298.64
291.01
291.67
265.29
169.09

Figure 22: Tasks could solved configuration Fast Downward search
timeout 300 seconds, total processing timeout 300 seconds.
column preprocessing shows total time translation knowledge compilation.

7.3 Translation Knowledge Compilation vs. Search
course, results report Fast Downward include time spent three components
planner: translation, knowledge compilation, search. Therefore, following presentation results, consider task solved total processing time 300 seconds.
However, also investigated tasks solved timeout 300 seconds
search component alone, allowing components use arbitrary amount resources.
turns makes difference five cases, could solved
total time 310 seconds (Fig. 22). one five cases, ATELLITE instance
exorbitant size, search take less time two phases combined. results show
search component time-critical part Fast Downward practice. Therefore,
report separate performance results individual components.
7.4 STRIPS Domains IPC13
Let us present results main experiment. abstain listing runtimes individual planning tasks due prohibitively large amount data. available electronic
appendix article.8 Instead, report following information:
Tables showing number tasks solved planner within 300 second timeout.
Here, present individual results domain.
Graphs showing number tasks solved given time planner. Here,
present separate results domain, would require many graphs.
discuss plan lengths; observations regard similar made
original implementation causal graph heuristic (Helmert, 2004).
Fig. 23 shows number unsolved tasks STRIPS domains IPC13.
Figs. 24 25 show number tasks solved planner within given time bound
0 300 seconds. addition six configurations Fast Downward consideration,
table includes four columns.
heading Any, include results hypothetical meta-planner guesses
best six configuration Fast Downward input task executes Fast Downward
8. http://www.jair.org/

233

fiH ELMERT

Domain

#Tasks

G

B LOCKSWORLD
EPOT
RIVERLOG
F REECELL (IPC2)
F REECELL (IPC3)
G RID
G RIPPER
L OGISTICS (IPC1)
L OGISTICS (IPC2)
ICONIC -STRIPS
OVIE
YSTERY
MP RIME
ROVERS
ATELLITE (IPC3)
Z ENOTRAVEL

35
22
20
60
20
5
20
35
28
150
30
30
35
20
20
20

0
12
2
4
0
1
0
1
0
0
0
1
0
2
1
0

0
13
0
4
0
2
0
0
0
0
0
2
0
0
0
0

0
13
0
12
5
1
0
0
0
0
0
1
0
0
0
0

Total

550

24

21

32

G+P G+P+

M+P

F



CG

FF

LPG

0
12
1
11
1
1
0
4
0
0
0
0
2
0
0
0

0
8
0
12
2
0
0
0
0
0
0
0
0
0
0
0

17
11
1
40
14
4
0
26
0
0
0
13
14
2
6
0

0
7
0
3
0
0
0
0
0
0
0
0
0
0
0
0

0
14
3
2
0
1
0
0
0
0
0
1
1
3
0
0

4
3
5
3
2
0
0
0
0
0
0
12
3
0
0
0

0
0
0
55
19
1
0
4
0
0
0
15
7
0
0
0

32

22

148

10

25

32

101

Figure 23: Number unsolved tasks STRIPS domains IPC1, IPC2, IPC3.
PSfrag replacements
FDD (Fast Downward)

550 (100%)

FD (Fast Downward)
YAHSP
Macro-FF

495 (90%)

LPG-TD
CG
FF
LPG

Solved Tasks

SGPlan

440 (80%)

(Fast Downward)
G + P (Fast Downward)
+ P (Fast Downward)
G (Fast Downward)
G + P+ (Fast Downward)
(Fast Downward)
F (Fast Downward)

385 (70%)

0s

50s

100s

150s
Search Time

200s

250s

300s

Figure 24: Number tasks solved vs. runtime STRIPS domains IPC1, IPC2 IPC3.
graph shows results various configurations Fast Downward.

234

fiT FAST OWNWARD P LANNING YSTEM

PSfrag replacements
FDD (Fast Downward)

550 (100%)

FD (Fast Downward)
YAHSP
Macro-FF

495 (90%)

LPG-TD

Solved Tasks

SGPlan

440 (80%)

G + P+ (Fast Downward)
(Fast Downward)
G + P (Fast Downward)
CG
FF
LPG

385 (70%)
G (Fast Downward)
+ P (Fast Downward)
(Fast Downward)
F (Fast Downward)

0s

50s

100s

150s
Search Time

200s

250s

300s

Figure 25: Number tasks solved vs. runtime STRIPS domains IPC1, IPC2 IPC3.
graph shows results CG, FF LPG hypothetical planner
always chooses best configuration Fast Downward. result greedy
best-first search helpful transitions repeated ease comparison Fig. 24.

235

fiH ELMERT

setting. heading CG, report results first implementation
causal graph heuristic (Helmert, 2004). 9 Finally, FF LPG refer well-known planners
(Hoffmann & Nebel, 2001; Gerevini et al., 2003) fully-automated tracks IPC2
IPC3. chosen comparison benchmark set showed best
performance far publicly available planners experimented with. LPG, uses
randomized search strategy, attempted solve task five times report median result.
results show excellent performance Fast Downward set benchmarks. Compared CG, already shown solve tasks FF LPG benchmark set
(Helmert, 2004), get another slight improvement half planner configurations. One
configurations, multi-heuristic best-first search using preferred operators, solves benchmarks
domains except EPOT F REECELL. Even importantly, number tasks
solved Fast Downward configurations small 10. Note planning competitions typically allowed planner spend 30 minutes task; time constraints,
could allocate five minutes six configurations Fast Downward, getting results
least good reported planner. Results might even better
cleverer allocation scheme.
Even configuration using focused iterative-broadening search performs comparatively well
benchmarks, although cannot compete planners. surprisingly,
version planner difficulties domains many dead ends (F REECELL, YSTERY,
MP RIME) goal ordering important (B LOCKSWORLD, EPOT). also fares comparatively badly domains large instances, namely L OGISTICS (IPC1) ATELLITE.
reader keep mind FF LPG excellent planning systems;
planners experimented with, including awarded prizes first three planning competitions, none solved benchmarks group focused iterative-broadening
search.
one domain proves quite resistant Fast Downwards solution attempts configuration EPOT. already observed initial experiments causal graph heuristic
(Helmert, 2004), believe one key problem Fast Downward, unlike FF,
use goal ordering techniques, important domain. fact domain
includes B LOCKSWORLD-like subproblem also problematic, gives rise dense causal
graphs demonstrated Section 5.2.3.
7.5 ADL Domains IPC13
Second, present results ADL domains first three planning competitions.
much smaller group previous, including four domains. time, cannot consider
CG LPG, since neither CG publicly available version LPG supports ADL domains.
Therefore, compare FF exclusively. Again, report number unsolved tasks
domain (Fig. 26) present graphs showing quickly tasks solved (Figs. 27 28).
results look good first group domains. Results ICONIC
domains good, even improving FF. However, greedy best-first search performs
badly SSEMBLY domain, configurations perform badly CHEDULE domain.
9. Apart missing support ADL axioms, CG similar Fast Downward using greedy best-first search
preferred operators (configuration G). translation knowledge compilation components essentially
identical. older search component mainly differs Fast Downward use deferred heuristic
evaluation.

236

fiT FAST OWNWARD P LANNING YSTEM

Domain

#Tasks

SSEMBLY
ICONIC -S IMPLE ADL
ICONIC -F ULL ADL
CHEDULE
Total

30
150
150
150
480

G
28
0
9
134
171

G+P G+P+
27
0
8
93
128

25
0
9
93
127

3
0
9
132
144

M+P

F



FF

0
0
8
28
36

30
0
90
113
233

0
0
6
25
31

0
0
12
0
12

Figure 26: Number unsolved tasks ADL domains IPC1, IPC2 IPC3.

PSfrag replacements
FDD (Fast Downward)

480 (100%)

FD (Fast Downward)

432 (90%)

YAHSP
Macro-FF

384 (80%)

LPG-TD
CG
FF
LPG

Solved Tasks

SGPlan

336 (70%)
288 (60%)

(Fast Downward)

240 (50%)
+ P (Fast Downward)
G + P+ (Fast Downward)
G + P (Fast Downward)
(Fast Downward)
G (Fast Downward)
F (Fast Downward)

192 (40%)
144 (30%)
0s

50s

100s

150s
Search Time

200s

250s

300s

Figure 27: Number tasks solved vs. runtime ADL domains IPC1, IPC2 IPC3.
graph shows results various configurations Fast Downward.

237

fiH ELMERT

PSfrag replacements
FDD (Fast Downward)

480 (100%)

FD (Fast Downward)

432 (90%)

YAHSP
Macro-FF

384 (80%)

LPG-TD
CG

LPG

G + P+ (Fast Downward)

Solved Tasks

SGPlan

336 (70%)
288 (60%)
240 (50%)

G + P (Fast Downward)
G (Fast Downward)

(Fast Downward)
F (Fast Downward)

192 (40%)

FF
(Fast Downward)
+ P (Fast Downward)

144 (30%)
0s

50s

100s

150s
Search Time

200s

250s

300s

Figure 28: Number tasks solved vs. runtime ADL domains IPC1, IPC2 IPC3.
graph shows results FF hypothetical planner always
chooses best configuration Fast Downward. result multi-heuristic bestfirst search preferred operators repeated ease comparison Fig. 27.

238

fiT FAST OWNWARD P LANNING YSTEM

Currently, good explanation SSEMBLY behaviour. CHEDULE domain, weak performance seems related missing goal ordering techniques:
many CHEDULE tasks, several goals defined object satisfied
certain order. instance, objects cylindrical, polished painted,
three goals must satisfied precisely order: making object cylindrical reverts effects
polishing painting, polishing reverts effect painting. recognising constraints, heuristic search algorithm assumes close goal object already
polished painted cylindrical, loathe transform object cylindrical shape
would undo already achieved goals. rudimentary manual goal ordering,
ignoring painting goals goals satisfied, number tasks solved
multi-heuristic best-first search preferred operators drops 28 3. three failures
appear due remaining ordering problems regard cylindrical polished objects.
7.6 Domains IPC4
Third finally, present results IPC4 domains. Here, compare FF:
benchmarks, FF perform well best planners competition. Besides, several
IPC4 competitors extensions FF hybrids using FF part bigger system, FFbased planning well-represented even limit attention IPC4 planners.
comparison, chose four successful competition participants besides Fast Downward,
namely LPG-TD, SGPlan, Macro-FF YAHSP (cf. results Hoffmann & Edelkamp, 2005).
Similar previous two experiments, report number unsolved tasks domain
(Fig. 29) present graphs showing quickly tasks solved (Figs. 30 31).
Fast Downward competitive planners across domains, better others
some. P IPESWORLD domains ones planners noticeably better two competition versions Fast Downward. case YAHSP
P IPESWORLD domain variants SGPlan P IPESWORLD -N OTANKAGE . P IPESWORLD
domain hierarchical nature; might domain decomposition approach causal graph heuristic appropriate. results heuristic search
configurations P ROMELA -O PTICALT ELEGRAPH domain extremely bad require investigation.
Interestingly, focused iterative-broadening search performs well benchmarks suite. One reasons many tasks IPC4 suite,
many individual goals easy serialize solved mostly independently. 10
Comparing configuration G G + P + especially + P, also observe using preferred operators useful benchmarks, even two previous
experiments.
final remark, observe implemented meta-planner calling six
Fast Downward configurations round-robin fashion, would obtain planning system
could solve 54 IPC4 benchmarks within 6 5 = 30 minute timeout. almost
par top performer IPC4, Fast Diagonally Downward, solved 52 IPC4
benchmarks timeout. Thus, benchmark set exploring different
planner configurations definitely pays off.
10. devised experiment shows property artificially violated simple goal reformulation,
performance algorithm degrades quickly; see electronic appendix details.

239

fiH ELMERT

Domain

#Tasks

IRPORT
P IPESWORLD -N OTANKAGE
P IPESWORLD -TANKAGE
P ROMELA -O PTICALT ELEGRAPH
P ROMELA -P HILOSOPHERS
PSR-S MALL
PSR-M IDDLE
PSR-L ARGE
ATELLITE (IPC4)
Total

50
50
50
48
48
50
50
50
36
432

G
28
24
36
48
0
0
0
22
8
166

G+P G+P+

M+P

F



30
25
36
47
0
0
0
20
0
158

14
7
17
46
0
0
0
22
3
109

0
10
34
13
21
1
22
39
22
162

0
7
14
13
0
0
0
20
0
54

17
23
36
48
0
0
0
22
0
146

18
14
34
47
16
0
0
23
8
160

Domain

FD

FDD

LPG-TD

Macro-FF

SGPlan

YAHSP

IRPORT
P IPESWORLD -N OTANKAGE
P IPESWORLD -TANKAGE
P ROMELA -O PTICALT ELEGRAPH
P ROMELA -P HILOSOPHERS
PSR-S MALL
PSR-M IDDLE
PSR-L ARGE
ATELLITE (IPC4)
Total

0
11
34
22
0
0
0
22
0
89

0
7
19
22
0
0
0
22
3
73

7
10
29
37
1
2
0
50
1
137

30
12
29
31
36
50
19
50
0
257

6
0
20
29
0
6
4
39
6
110

17
0
13
36
19
3
50
50
0
188

Figure 29: Number unsolved tasks IPC4 domains. Results various configurations
Fast Downward listed upper part, results competition participants
lower part. FD FDD denote versions Fast Downward participated IPC4 names Fast Downward Fast Diagonally Downward
(cf. Section 6).

240

fiT FAST OWNWARD P LANNING YSTEM

PSfrag replacements
FDD (Fast Downward)

432 (100%)

FD (Fast Downward)

389 (90%)

YAHSP

346 (80%)

Macro-FF
SGPlan

CG
FF
LPG

302 (70%)
Solved Tasks

LPG-TD

(Fast Downward)

259 (60%)
216 (50%)
173 (40%)
130 (30%)
+ P (Fast Downward)
G + P+ (Fast Downward)
G + P (Fast Downward)
(Fast Downward)
F (Fast Downward)
G (Fast Downward)

86 (20%)
43 (10%)
0s

50s

100s

150s
Search Time

200s

250s

300s

Figure 30: Number tasks solved vs. runtime IPC4 domains. graph shows results
various configurations Fast Downward.
PSfrag replacements
432 (100%)
389 (90%)
346 (80%)

CG
FF
LPG

G + P+ (Fast Downward)
G + P (Fast Downward)
G (Fast Downward)

Solved Tasks

302 (70%)
259 (60%)
216 (50%)
173 (40%)
130 (30%)

(Fast Downward)
FDD (Fast Downward)
FD (Fast Downward)
SGPlan
LPG-TD
YAHSP
Macro-FF

86 (20%)
43 (10%)

+ P (Fast Downward)
(Fast Downward)
F (Fast Downward)

0s

50s

100s

150s
Search Time

200s

250s

300s

Figure 31: Number tasks solved vs. runtime IPC4 domains. graph shows results
hypothetical planner always chooses best configuration Fast
Downward, competition configurations Fast Downward best four
participants.

241

fiH ELMERT

7.7 Conclusions Experiment
interpret experimental results? first conclusion Fast Downward
clearly competitive state art. especially true configuration using
multi-heuristic best-first search preferred operators (M+P), outperforms competing
planning systems set STRIPS domains IPC13 domains IPC4.
problems CHEDULE domain, would true remaining
group benchmarks, ADL domains IPC13.
regard second objective investigation, evaluating relative strengths
different planner configurations, M+P configuration emerges clear-cut winner. 23
29 domains, configuration solves tasks, unlike configurations,
one domain (P ROMELA -O PTICALT ELEGRAPH) performs badly. conclude
multi-heuristic best-first search use preferred operators promising extensions
heuristic planners.
particularly true preferred operators. Indeed, M+P configuration, two
variants greedy best-first search preferred operators show next best overall performance,
terms number domains among top performers terms
total number tasks solved. Comparing G G+P, ten domains variant
using preferred operators solves tasks one using them; opposite true five
domains. Comparing M+P, difference even striking, preferred operator
variant outperforming fifteen domains, worse two (in
solves one task less). convincing arguments use preferred operators.

8. Summary Discussion
turn discussion, let us briefly summarize contributions article. motivating starting point, explained planning tasks often exhibit simpler structure expressed
multi-valued state variables, rather traditional propositional representations.
introduced Fast Downward, planning system based idea converting tasks multivalued formalism exploiting causal information underlying encodings.
Fast Downward processes PDDL planning tasks three stages. skipped first
stages, translation, automatically transforms PDDL task equivalent multi-valued
planning task nicer causal structure. explained inner workings second stage,
knowledge compilation, demonstrating depth kind knowledge planner extracts
problem representation, discussing causal graphs, domain transition graphs, successor generators axiom evaluators. discussion Fast Downwards search component,
introduced heuristic search algorithms, use technique deferred heuristic evaluation
reduce number states heuristic goal distance estimate must computed.
addition greedy best-first search, Fast Downward employs multi-heuristic best-first search
algorithm usefully integrate information two heuristic estimators, namely causal graph
heuristic FF heuristic. heuristic search algorithms utilize preference information
operators. also introduced Fast Downwards experimental focused iterative-broadening
search algorithm, based idea pruning set operators consider
successor states likely lead towards specific goal.
thus tried give complete account Fast Downward planning systems approach
solving multi-valued planning tasks, including motivation, architecture, algorithmic founda242

fiT FAST OWNWARD P LANNING YSTEM

tions. previous section, demonstrated empirical behaviour, showing good performance
across whole range propositional benchmarks previous planning competitions.
Among novel algorithms search enhancements discussed article, two
aspects Fast Downward consider central importance would like
emphasize. One use multi-valued state variables PDDL-style planning.
believe multi-valued representations much structured hence much amenable
automated reasoning purposes heuristic evaluation, problem decomposition,
aspects planning goal ordering extraction landmarks. central
idea use hierarchical decompositions within heuristic planning framework. Hierarchical
approaches domain-independent planning considerable potential, since work
Knoblock (1994) Bacchus Yang (1994), little work published. Fast Downward, hope renew interest area, believe promising ground
advances automated planning.
future, several aspects Fast Downward would like investigate
further. First, intend experiment search techniques along lines focused
iterative-broadening search, emphasize heuristically evaluating operator usefulness rather
heuristically evaluating states.
Second, would like come efficient heuristic multi-valued planning tasks
require pruning cycles causal graph. Initial experiments direction
shown difficult achieve goal without losing performance Fast Downwards
heuristic estimator, perhaps better heuristic accuracy outweigh worse per-state performance
many cases.
Third, want investigate far performance planner could improved
encoding domains differently. cases, merging set state variables
closely interrelated single state variable whose domain product domains
original state variables might beneficial. Also, want test hand-tailored encodings lead
better performance automatically derived ones, so, large performance gap is.
Fourth finally, would like evaluate behaviour causal graph heuristic
specific planning domains empirically theoretically, following Hoffmanns work FF
heuristic (Hoffmann, 2001, 2002, 2005). Hopefully, give indication
expect good performance causal graph heuristic advisable look
approaches.

Acknowledgements
author wishes thank Silvia Richter, member Fast Downward team
4th International Planning Competition, part implementing planner valuable
advice before, throughout, competition. also deserves thanks helping
experiments, proof-reading article, suggesting number improvements.
anonymous reviewers article handling editor, Maria Fox, made number
useful suggestions led significant improvements.
work partly supported German Research Council (DFG) within Graduate
Programme Mathematical Logic Applications part Transregional Collaborative
Research Centre Automatic Verification Analysis Complex Systems (SFB/TR 14 AVACS).
See www.avacs.org information.
243

fiH ELMERT

References
Bacchus, F., & Yang, Q. (1994). Downward refinement efficiency hierarchical problem
solving. Artificial Intelligence, 71(1), 43100.
Backstrom, C., & Nebel, B. (1995). Complexity results SAS + planning. Computational Intelligence, 11(4), 625655.
Bonet, B., & Geffner, H. (2001). Planning heuristic search. Artificial Intelligence, 129(1), 533.
Brafman, R. I., & Domshlak, C. (2003). Structure complexity planning unary operators.
Journal Artificial Intelligence Research, 18, 315349.
Bylander, T. (1994). computational complexity propositional STRIPS planning. Artificial
Intelligence, 69(12), 165204.
Domshlak, C., & Brafman, R. I. (2002). Structure complexity planning unary operators.
Ghallab, M., Hertzberg, J., & Traverso, P. (Eds.), Proceedings Sixth International
Conference Artificial Intelligence Planning Scheduling (AIPS 2002), pp. 3443. AAAI
Press.
Domshlak, C., & Dinitz, Y. (2001). Multi-agent off-line coordination: Structure complexity.
Cesta, A., & Borrajo, D. (Eds.), Pre-proceedings Sixth European Conference
Planning (ECP01), pp. 277288, Toledo, Spain.
Dowling, W. F., & Gallier, J. H. (1984). Linear-time algorithms testing satisfiability
propositional Horn formulae. Journal Logic Programming, 1(3), 367383.
Edelkamp, S., & Helmert, M. (1999). Exhibiting knowledge planning problems minimize
state encoding length. Fox, M., & Biundo, S. (Eds.), Recent Advances AI Planning.
5th European Conference Planning (ECP99), Vol. 1809 Lecture Notes Artificial
Intelligence, pp. 135147, New York. Springer-Verlag.
Edelkamp, S., & Hoffmann, J. (2004). PDDL2.2: language classical part 4th
International Planning Competition. Tech. rep. 195, Albert-Ludwigs-Universitat Freiburg,
Institut fur Informatik.
Fox, M., & Long, D. (2003). PDDL2.1: extension PDDL expressing temporal planning
domains. Journal Artificial Intelligence Research, 20, 61124.
Garey, M. R., & Johnson, D. S. (1979). Computers Intractability Guide Theory
NP-Completeness. Freeman.
Gerevini, A., Saetti, A., & Serina, I. (2003). Planning stochastic local search temporal
action graphs LPG. Journal Artificial Intelligence Research, 20, 239290.
Ginsberg, M. L., & Harvey, W. D. (1992). Iterative broadening. Artificial Intelligence, 55, 367383.
Helmert, M. (2004). planning heuristic based causal graph analysis. Zilberstein, S., Koehler,
J., & Koenig, S. (Eds.), Proceedings Fourteenth International Conference Automated
Planning Scheduling (ICAPS 2004), pp. 161170. AAAI Press.
Hoffmann, J. (2001). Local search topology planning benchmarks: empirical analysis.
Nebel, B. (Ed.), Proceedings 17th International Joint Conference Artificial Intelligence (IJCAI01), pp. 453458. Morgan Kaufmann.
244

fiT FAST OWNWARD P LANNING YSTEM

Hoffmann, J. (2002). Local search topology planning benchmarks: theoretical analysis.
Ghallab, M., Hertzberg, J., & Traverso, P. (Eds.), Proceedings Sixth International Conference Artificial Intelligence Planning Scheduling (AIPS 2002), pp. 92100. AAAI
Press.
Hoffmann, J. (2005). ignoring delete lists works: Local search topology planning benchmarks. Journal Artificial Intelligence Research, 24, 685758.
Hoffmann, J., & Edelkamp, S. (2005). deterministic part IPC-4: overview. Journal
Artificial Intelligence Research, 24, 519579.
Hoffmann, J., & Nebel, B. (2001). FF planning system: Fast plan generation heuristic
search. Journal Artificial Intelligence Research, 14, 253302.
Jonsson, P., & Backstrom, C. (1995). Incremental planning. Ghallab, M., & Milani, A. (Eds.),
New Directions AI Planning: EWSP 95 3rd European Workshop Planning, Vol. 31
Frontiers Artificial Intelligence Applications, pp. 7990, Amsterdam. IOS Press.
Jonsson, P., & Backstrom, C. (1998a). State-variable planning structural restrictions: Algorithms complexity. Artificial Intelligence, 100(12), 125176.
Jonsson, P., & Backstrom, C. (1998b). Tractable plan existence imply tractable plan generation. Annals Mathematics Artificial Intelligence, 22(3), 281296.
Joslin, D., & Roach, J. (1989). theoretical analysis conjunctive-goal problems. Artificial
Intelligence, 41(1), 97106. Research Note.
Knoblock, C. A. (1994). Automatically generating abstractions planning. Artificial Intelligence,
68(2), 243302.
Korf, R. E. (1987). Planning search: quantitative approach. Artificial Intelligence, 33(1),
6588.
Lowerre, B. T. (1976). HARPY Speech Recognition System. Ph.D. thesis, Computer Science
Department, Carnegie-Mellon University, Pittsburgh, Pennsylvania.
Newell, A., & Simon, H. A. (1963). GPS: program simulates human thought. Feigenbaum,
E. A., & Feldman, J. (Eds.), Computers Thought, pp. 279293. Oldenbourg.
Russell, S., & Norvig, P. (2003). Artificial Intelligence Modern Approach. Prentice Hall.
Sacerdoti, E. D. (1974). Planning hierarchy abstraction spaces. Artificial Intelligence, 5,
115135.
Tenenberg, J. D. (1991). Abstraction planning. Allen, J. F., Kautz, H. A., Pelavin, R. N.,
& Tenenberg, J. D., Reasoning Plans, chap. 4, pp. 213283. Morgan Kaufmann, San
Mateo.
van den Briel, M., Vossen, T., & Kambhampati, S. (2005). Reviving integer programming approaches AI planning: branch-and-cut framework. Biundo, S., Myers, K., & Rajan,
K. (Eds.), Proceedings Fifteenth International Conference Automated Planning
Scheduling (ICAPS 2005), pp. 310319. AAAI Press.
Williams, B. C., & Nayak, P. P. (1997). reactive planner model-based executive. Pollack,
M. E. (Ed.), Proceedings 15th International Joint Conference Artificial Intelligence
(IJCAI97), pp. 11781195. Morgan Kaufmann.
245

fiH ELMERT

Yoshizumi, T., Miura, T., & Ishida, T. (2000). partial expansion large branching factor problems. Kautz, H., & Porter, B. (Eds.), Proceedings Seventeenth National
Conference Artificial Intelligence (AAAI-2000), pp. 923929. AAAI Press.

246

fiJournal Artificial Intelligence Research 26 (2006) 453-541

Submitted 12/05; published 08/06

Engineering Benchmarks Planning: Domains Used
Deterministic Part IPC-4
Jorg Hoffmann

HOFFMANN @ MPI - SB . MPG . DE

Max Planck Institute Computer Science,
Saarbrucken, Germany

Stefan Edelkamp

STEFAN . EDELKAMP @ CS . UNI - DORTMUND . DE

Fachbereich Informatik,
Universitat Dortmund, Germany

Sylvie Thiebaux

YLVIE .T HIEBAUX @ ANU . EDU . AU

National ICT Australia & Computer Sciences Laboratory,
Australian National University, Canberra, Australia

Roman Englert

ROMAN .E NGLERT @ TELEKOM . DE

Deutsche Telekom Laboratories,
Berlin, Germany

Frederico dos Santos Liporace

LIPORACE @ INF. PUC - RIO . BR

Departamento de Informatica, PUC-Rio,
Rio de Janeiro, Brazil

Sebastian Trug

TRUEG @ INFORMATIK . UNI - FREIBURG . DE

Institut fur Informatik,
Universitat Freiburg, Germany

Abstract
field research general reasoning mechanisms, essential appropriate
benchmarks. Ideally, benchmarks reflect possible applications developed technology. AI Planning, researchers tend draw testing examples
benchmark collections used International Planning Competition (IPC). organization
(the deterministic part of) fourth IPC, IPC-4, authors therefore invested significant effort
create useful set benchmarks. come five different (potential) real-world applications planning: airport ground traffic control, oil derivative transportation pipeline networks,
model-checking safety properties, power supply restoration, UMTS call setup. Adapting
preparing application use benchmark IPC involves, time, inevitable
(often drastic) simplifications, well careful choice between, engineering of, domain encodings. first time IPC, used compilations formulate complex domain features
simple languages STRIPS, rather dropping interesting problem constraints simpler language subsets. article explains discusses five application
domains adaptation form PDDL test suites used IPC-4. summarize known
theoretical results structural properties domains, regarding computational complexity
provable properties topology h+ function (an idealized version relaxed
plan heuristic). present new (empirical) results illuminating properties quality
wide-spread heuristic functions (planning graph, serial planning graph, relaxed plan),
growth propositional representations instance size, number actions available
achieve fact; discuss data conjunction best results achieved
different kinds planners participating IPC-4.

c
2006
AI Access Foundation. rights reserved.

fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UG

1. Introduction
Today, large extent research discipline AI planning concerned improving performance domain independent generative planning systems. domain independent generative
planning system (planner) must able fully automatically find plans: solution sequences
declaratively specified transition systems. simplest planning formalism deterministic planning. There, planner given input set state variables (often Booleans, called facts),
initial state (a value assignment variables), goal (a formula), set actions (with
precondition formula describing applicability, effect specifying action changes
state). plan time-stamped sequence actions maps initial state state
satisfies goal. sort formalism called deterministic since initial state fully specified effects actions non-ambiguous. restrictions may weakened obtain
non-deterministic probabilistic planning.
Performance planners measured testing benchmark example instances
planning problem. best algorithm point time is, generally, considered one
solves examples efficiently. particular, idea International Planning Competition (IPC), biennial event aimed showcasing capabilities current planning
systems.
first IPC took place 1998, time writing four events. Providing details IPC beyond scope paper, refer reader overview
articles written organizers respective IPC editions (McDermott, 2000; Bacchus, 2001;
Long & Fox, 2003; Hoffmann & Edelkamp, 2005). particular, Hoffmann Edelkamp (2005)
provide details 4th IPC, overall organization, different tracks, evaluation, participating planners, results. Basic information included paper, reader
able follow main discussion without detailed background. language used describe
planning problems IPC called PDDL: Planning Domain Definition Language. introduced McDermott (1998) first IPC, IPC-1, 1998. subset language
selected Bacchus (2000) IPC-2 2000. language extended temporal numerical constructs Fox Long (2003) form language PDDL2.1 IPC-3 2002.
extended two additional constructs, timed initial literals derived predicates,
Hoffmann Edelkamp (2005) form language PDDL2.2 IPC-4 2004.
Since, even simplest forms, AI planning computationally hard problem, system
work efficiently problem instances (Bylander, 1994; Helmert, 2003). Thus, crucial importance kinds examples used testing. Today, more, AI Planning
researchers draw testing examples collections used IPC. makes IPC
benchmarks important instrument field. organization deterministic part
4th IPC (there also probabilistic part, see Younes, Littman, Weissman, & Asmuth,
2005), authors therefore invested considerable effort creating set useful benchmarks
planning.
first question answer precisely meant word useful.
easy question. widely accepted mathematical definition deciding whether
set benchmarks considered useful. are, however, widely accepted intuitions
case. Benchmarks be:
1. Oriented applications benchmark reflect application technology developed field.
454

fiE NGINEERING B ENCHMARKS



P LANNING

2. Diverse structure set benchmarks cover different kinds structure, rather
re-state similar tasks.
first usually considered particularly important indeed, AI planning frequently criticized obsession toy examples. recent years, performance
state-of-the-art systems improved dramatically, realistic examples come
within reach. made another step direction orienting IPC-4 benchmarks
application domains. traditionally planning benchmarks less fantasy products
created real scenario mind,1 took actual (possible) applications planning
technology, turned something suitable competition. considered five different application domains: airport ground traffic control (Airport), oil derivative transportation
pipeline networks (Pipesworld), model checking safety properties (Promela), power supply restoration (PSR), setup mobile communication UMTS (UMTS). course, adaptation
application use IPC, simplifications need made. get back below.
Diverse structure benchmarks traditionally given less attention realism,
believe less important. structure underlying testing example determines
performance applied solving mechanism. particularly true solving mechanisms
whose performance rises falls quality heuristic use. Hoffmanns (2001, 2002,
2005) results suggest much spectacular performance modern heuristic search planners
due structural similarities traditional planning benchmarks.
imply modern heuristic search planners arent useful, certainly shows
creation benchmarks risk introducing bias towards one specific way solving
them. selecting benchmark domains IPC-4, tried cover range intuitively
different kinds problem structure. get back below.
one hand, creator planning benchmarks noble goal realistic, structurally diverse, benchmark domains. hand, he/she pragmatic goal
come version/representation benchmarks attacked existing planning systems. Given still quite restricted capabilities systems, obviously two goals
conflict. make matters worse, isnt arbitrarily large supply planning applications
publicly available, and/or whose developers agree application used basis
benchmark. IPC organizer, top this, final benchmarks must accessible
large enough number competing systems, means must formulated language
understood systems. Further, benchmarks must show differences scalability planners, i.e., must easy hard, thus straddling boundary current
system capabilities.
solution difficulties, least solution organization IPC-4, involved slow tedious interleaved process contacting application developers, choosing domains,
exploring domain versions, engineering domain version representations. article presents,
motivates, discusses choice benchmark domains IPC-4; explains engineering
processes led finally used domain versions instances. Further, report about,
present new data determining certain structural properties resulting benchmarks
(more details below). main contribution work set benchmarks, provided
1. course, exceptions rule. One important one, context here, Satellite domain, used
IPC-3, refined use IPC-4. later.

455

fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UG

IPC-4.2 contributions article are: first, providing necessary documentation
benchmarks; second, describing technical processes used creation; third, providing
extensive discussion structural properties benchmarks. Apart technical contributions, believe work value example large-scale attempt
engineering useful set benchmarks classical planning.
difficult make formal claim created set benchmarks,
way better previous benchmarks. working this, intent
overcome certain shortcomings many benchmarks, though one would hard pressed come
formal proof improvements indeed made. all, judging quality
set benchmarks rather complex matter guided mostly intuitions, and, worse, personal
opinions.3 was, best create realistic, structurally diverse, accessible
benchmarks possible IPC-4. belief succeeded so. benchmarks
definitely differ certain ways previous benchmarks. think
differences advantageous; discuss places point differences.
Regarding realism benchmarks, pointed above, main step took design
benchmarks top-down, i.e., start actual possible applications planning technology,
turn something suitable competition rather traditional bottomup approach artificially creating domain real scenario mind. course,
modelling application PDDL, particularly modelling way making suitable
use IPC, simplifications need made. cases, e.g., airport ground traffic
control, simplifications overly drastic, preserved overall properties intuitive
structure domain. cases, e.g., oil derivative transportation pipeline networks,
simplifications needed make drastic domains could well
created traditional bottom-up way. Still, even greatly simplified, domain generated
top-down better chance capture structure relevant real application. Moreover,
top-down domain advantage since derived real application, provides
clear guideline towards realism; future challenge make planners work
realistic encodings application. previous competitions, domains generated
top-down sense Elevator domain used IPC-2 (Koehler & Schuster, 2000;
Bacchus, 2001), Satellite Rovers domains used IPC-3 (Long & Fox, 2003).
Regarding diverse structure benchmarks, contrast previous competitions,
IPC-4 domains common theme underlying many benchmarks. IPC-1, 5
7 domains variants transportation; IPC-2, 4 7 domains variants transportation; IPC-3, 3 6 domains variants transportation, 2 gathering
data space. variants fact interesting use constructs
locked locations, fuel units, road map graphs, stackable objects, complex side constraints.
However, certainly intuitive similarity structure relationships domains.
extent similarity even automatically detectable (Long & Fox, 2000). IPC4: airport ground traffic control, oil derivative transportation pipeline networks, model checking
safety properties, power supply restoration, UMTS call setup rather different topics.
2. benchmarks downloaded IPC-4 web page http://ipc.icaps-conference.org/
3. Consider example Movie domain used IPC-1. instances domain, matter size is,
share space reachable states; thing increases connectivity states, i.e.
number actions effect. Still one argue Movie useful benchmark, sense
highlight systems/approaches have/have difficulties attacking problem characteristics.

456

fiE NGINEERING B ENCHMARKS



P LANNING

one could claim airport ground traffic control UMTS call setup scheduling nature. see, however, IPC-4 version airport ground traffic control allows
considerably freedom classical scheduling formulations, making PSPACE-complete
decision problem. particulars domains overviewed Section 3.
Approaching structure formal point view difficult. largely unclear
what, precisely, relevant structure planning domain/instance is, general sense.
Hoffmann (2001, 2002, 2005) provides one possible definition search space surface topology certain heuristic function many possible options. particular, Hoffmanns
results relevant heuristic search planners generate heuristic functions based
ignoring delete lists relaxation (McDermott, 1996, 1999; Bonet, Loerincs, & Geffner, 1997;
Bonet & Geffner, 2001; Hoffmann & Nebel, 2001). lack better formal handle, used
Hoffmanns definitions qualify structure domains. selected domains cover different regions Hoffmanns planning domain taxonomy, particular lie regions
less coverage traditional benchmarks. interesting context
paper hand, summarize Hoffmanns (2005) results 30 domains including domains
used previous competitions. also summarize Helmerts (2006b) results computational complexity satisficing optimal planning IPC-4 domains. turns
complexity covers wide range widest possible range, propositional planning formalisms
PSPACE-hard polynomial. finally provide new data analyze structural
relationships differences domains. Amongst things, instance,
measure: number (parallel sequential) steps needed achieve goal, estimated
smallest plan found IPC-4 participant; number estimated planning graphs
relaxed plans; distribution number possible achieving actions fact.
results examined comparison different domains, taking account
runtime performance exhibited different kinds planners IPC-4.
Apart realism diverse structure, main quest creation IPC-4 benchmarks
promote accessibility. Applications are, typically, modelled
PDDL, naturally modelled using rather complex language constructs time, numeric
variables, logical formulas, conditional effects. existing systems handle subsets
this, fact half systems entered IPC-4 (precisely, 11 19) could handle
simple STRIPS language, slight extensions it.4 previous competitions, done
example Elevator, Satellite, Rovers domains, handled simply dropping
interesting domain constraints simpler languages, i.e., removing respective
language constructs domain/instance descriptions. contrast, first time IPC,
compiled much domain semantics possible simpler language formats.
compilation hard, sometimes impossible, do. done ADL constructs,
well two new constructs introduced IPC-4 language PDDL2.2, derived predicates
timed initial literals. implemented, applied, compilation methods cases.
4. STRIPS (Stanford Research Institute Problem Solver) name simplest time widespread planning language. form language used today, state variables Boolean, formulas
conjunctions positive atoms, action effects either atomic positive (make fact true/add it) atomic negative
(make fact false/delete it) (Fikes & Nilsson, 1971). languages selected IPC-2 (Bacchus, 2000),
PDDL2.1 PDDL2.2 derived, STRIPS ADL. ADL prominent, expressive, alternative
STRIPS, extending arbitrary first-order formulas preconditions goal, conditional effects,
i.e., effects occur individual effect condition (a first-order formula) met state execution
(Pednault, 1989).

457

fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UG

compilations serve preserve original domain structure, simpler language
classes. example, STRIPS version Elevator domain IPC-2 simplified
original ADL version bears marginal similarity real elevator control particular,
planner explicitly tell passengers get lift.5 contrast, STRIPS
formulation airport ground traffic domain is, semantically, identical ADL formulation
expresses things, awkward fashion.
compiled domain versions offered competitors alternative domain version formulations, yielding 2-step hierarchy domain. is, domain IPC-4
could contain several different domain versions, differing terms number domain constraints/properties considered. Within domain version, could several domain version formulations, differing terms language used formulate (same) semantics.
competitors could choose, within version, whichever formulation planners could handle
best/handle all, results within domain version evaluated together.
way, intended make competition accessible possible time keeping
number separation lines data number distinctions need made
evaluating data acceptable level.
are, course, aware encoding details significant impact system performance.6 Particularly, compiling ADL STRIPS, cases revert fully
grounded encodings. certainly isnt desirable, believe acceptable price
pay benefit accessibility. current systems ground operators pre-process
anyway. cases considered compiled domain formulations different
original ones allow fair comparison typically plan length increased significantly
due compilation compiled formulation posed competitors separate domain version.
article organized follows. main body text contains general information.
Section 2, give detailed explanation compilation methods used. Section 3, give
summary domains, short application description, motivation including
domain, brief explanation main simplifications made, brief explanation
different domain versions formulations. Section 4, summarize Hoffmanns (2005)
Helmerts (2006b) theoretical results structure IPC-4 domains. Section 5, provide
empirical analysis structural properties. Section 6 discusses achieved,
provides summary main issues left open. IPC-4 domains, include
separate section Appendix A, providing detailed information application, adaptation
IPC-4, domain versions, example instances used, future directions. Although
details appendix, emphasize secondary importance.
contrary, describe main body work did. presentation appendix
seems suitable since expect reader to, typically, examine domains detail
selective non-chronological manner.
5. passengers wont get (out) floors origin (destination); however, explicit control,
planner choose let someone (out). accurate encoding via conditional effects action
stopping lift floor.
6. detailed account matters provided Howe Dahlman (2002).

458

fiE NGINEERING B ENCHMARKS



P LANNING

2. PDDL Compilations
used three kinds compilation methods:
ADL SIMPLE-ADL (STRIPS conditional effects) STRIPS;
PDDL derived predicates PDDL without them;
PDDL timed initial literals PDDL without them.
consider compilation methods order, explaining, each, compilation
works, main difficulties possible solutions are, giving outline
used compilation competition. Note ADL, SIMPLE-ADL, STRIPS subsets
PDDL. compilation methods published elsewhere already (see citations
text). section serves overview article, since coherent summary techniques,
behavior practice, appeared elsewhere literature.
2.1 Compilations ADL SIMPLE-ADL STRIPS
ADL constructs compiled away methods first proposed Gazen Knoblock (1997).
Suppose given planning instance constant (object) set C, initial state I, goal G,
operator set O. operator precondition pre(o), conditional effects e, taking form
con(e), add(e), del(e) add(e) del(e) lists atoms. Preconditions, effect conditions,
G first order logic formulas (effect conditions RU E unconditional effects). Since
domain discourse set constants finite, formulas equivalently transformed propositional logic.
(1) Quantifiers turned conjunctionsVand disjunctions, simply expanding
W
available objects: x : (x) turns cC (c) x : (x) turns cC (c). Iterate
quantifiers left.
Since STRIPS allows conjunctions positive atoms, transformations necessary.
(2) Formulas brought negation normal form: ( ) turns ( )
turns . Iterate negation front atoms only.
(3) x occurs formula: introduce new predicate not-x; set not-x iff
x 6 I; effects e: set not-x add(e) iff x del(e) not-x del(e) iff x add(e);
formulas, replace x not-x. Iterate negations left.
(4) Transform formulas DNF: (1 2 ) (1 2 ) turns (1 1 ) (1 2 )
(2 1 ) (2 2 ). Iterate conjunctions occur disjunctions.
operator precondition pre(o) n > 1 disjuncts, create n copies one
disjunct precondition. effect condition con(e) n > 1 disjuncts, create n
copies e one disjunct condition. G n > 1 disjuncts, introduce
new fact goal-reached, set G := goal-reached, create n new operators one
disjunct precondition single unconditional effect adding goal-reached.

459

fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UG

(:action move
:parameters
(?a - airplane ?t - airplanetype ?d1 - direction ?s1 ?s2 - segment ?d2 - direction)
:precondition
(and (has-type ?a ?t) (is-moving ?a) (not (= ?s1 ?s2)) (facing ?a ?d1) (can-move ?s1 ?s2 ?d1)
(move-dir ?s1 ?s2 ?d2) (at-segment ?a ?s1)
(not (exists (?a1 - airplane) (and (not (= ?a1 ?a)) (blocked ?s2 ?a1))))
(forall (?s - segment) (imply (and (is-blocked ?s ?t ?s2 ?d2) (not (= ?s ?s1))) (not (occupied ?s)))))
:effect
(and (occupied ?s2) (blocked ?s2 ?a) (not (occupied ?s1)) (not (at-segment ?a ?s1)) (at-segment ?a ?s2)
(when (not (is-blocked ?s1 ?t ?s2 ?d2)) (not (blocked ?s1 ?a)))
(when (not (= ?d1 ?d2)) (and (not (facing ?a ?d1)) (facing ?a ?d2)))
(forall (?s - segment) (when (is-blocked ?s ?t ?s2 ?d2) (blocked ?s ?a)))
(forall (?s - segment) (when
(and (is-blocked ?s ?t ?s1 ?d1) (not (= ?s ?s2)) (not (is-blocked ?s ?t ?s2 ?d2)))
(not (blocked ?s ?a))))))

Figure 1: operator airport ground traffic control.
illustrative example, consider operator description Figure 1, taken domain
encoding airport ground traffic control. operator moves airplane one airport segment
another. Consider specifically precondition formula (not (exists (?a1 - airplane) (and (not (=
?a1 ?a)) (blocked ?s2 ?a1)))), saying airplane different ?a allowed block segment
?s2, segment moving into. Say set airplanes a1 , . . . , . step (1)
turn formula (not (or (and (not (= a1 ?a)) (blocked ?s2 a1 )) . . . (and (not (= ?a)) (blocked ?s2
)))). Step (2) yields (and (or (= a1 ?a) (not (blocked ?s2 a1 ))) . . . (or (= ?a) (not (blocked ?s2 )))).
Step (3) yields (and (or (= a1 ?a) (not-blocked ?s2 a1 )) . . . (or (= ?a) (not-blocked ?s2 ))). Step (4),
finally, (naively) transform (or (and (= a1 ?a) . . . (= ?a)) . . . (and (not-blocked ?s2 a1 )
. . . (not-blocked ?s2 ))), i.e., mathematically notated:
_
^
x.
x{(= a1

?a),(not-blocked ?s2 a1 )}...{(= ?a),(not-blocked ?s2 )}

words, transforming formula DNF requires enumerating n-vectors atoms
vector position selected one two possible atoms regarding airplane ai .
yields exponential blow-up DNF 2n disjuncts. DNF split single
disjuncts, one yielding new copy operator.
reader noticed exponential blow-up also inherent compilation step
(1), quantifier may expanded |C| sub-formulas, k nested quantifiers
expanded |C|k sub-formulas. Obviously, general way around either
blow-ups, deal complex formulas allowed STRIPS. practice,
however, blow-ups typically dealt reasonably well, thanks relative simplicity
operator descriptions, frequent occurrence static predicates, explained shortly.
quantifiers arent deeply nested, like Figure 1, blow-up inherent step (1)
matter. Transformation DNF often problem like example here. key
successful application compilation practice, least far personal experience
goes, exploitation static predicates. idea described, example, Koehler
460

fiE NGINEERING B ENCHMARKS



P LANNING

Hoffmann (2000). Static predicates arent affected operator effect. predicates
easily found, truth value fully determined initial state soon fully
instantiated. transformation step (4), operator parameters still variables,
even knew = (of course) static predicate, would help us
wouldnt know ?a is. instantiate ?a, however, then, instantiation
operator, (= ?a1 ?a) atoms trivialize TRUE FALSE, large DNF collapses
V
single conjunction 6= ?a1 airplane (not-blocked ?s2 ?a1), instantiation
?a. Similarly, expansion quantifiers often made much easier first instantiating
operator parameters, inserting TRUE FALSE static predicate soon
parameters grounded. Inserting TRUE FALSE often simplifies formulas significantly
information propagated upwards (e.g., disjunction TRUE element becomes
TRUE itself).
Assuming compilation succeeded thus far, steps (1) (4) processed
STRIPS description conditional effects, i.e., actions still conditional effects con(e),
add(e), del(e) con(e) conjunction atoms. subset ADL termed
SIMPLE-ADL Fahiem Bacchus, used encoding one versions
Elevator domain used IPC-2 (i.e. 2000 competition). choose leave
language, necessitating planning algorithm deal conditional effects directly.
Several existing planning systems, example FF (Hoffmann & Nebel, 2001) IPP (Koehler,
Nebel, Hoffmann, & Dimopoulos, 1997), this. sensible approach since, Nebel (2000)
proved, conditional effects cannot compiled STRIPS without either exponential blowup task description, linear increase plan length. One might suspect that, like
steps (1) (4) above, exponential blow-up mostly avoided practice.
airport move operator Figure 1 provides example this. effect conditions static
conditional effects disappear completely instantiate parameters another
good reason instantiation prior compilation. However, conditional effects
disappear many other, even simple, natural domains. Consider following effect, taken
classical Briefcaseworld domain:
(forall (?o) (when (in ?o) (and (at ?o ?to) (not (at ?o ?from)))))

effect says object ?o currently briefcase moves along briefcase.
Obviously, effect condition static, outcome operator truly depend
contents briefcase. Note forall means actually set (distinct)
conditional effects, one object.
basically two known methods compile conditional effects away, corresponding
two options left open Nebels (2000) result. first option enumerate possible
combinations effect outcomes, preserves plan length cost exponential blow-up
description size exponential number different conditional effects single action.
Consider Briefcaseworld operator, say object set o1 , . . . , . every
subset o01 , . . . o0k o1 , . . . , , o0k+1 , . . . , o0n complement subset, get distinct
operator precondition contains of:
(in o01 ) . . . (in o0k ) (not-in o0k+1 ) . . . (not-in o0n )

effect objects is:
(at o01 ?to) . . . (at o0k ?to) (not (at o01 ?from)) . . . (not (at o0k ?from))
461

fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UG

words, operator applied (only) exactly o01 , . . . o0k briefcase,
moves exactly objects. Since (in deterministic planning considered here) never
uncertainty objects inside briefcase not, exactly one new
operators applied whenever original operator applied. compilation method
preserves size (nodes) form (edges) state space. However, wont able
transformation, planner wont able deal resulting task, n grows beyond, say,
maximally 10 . . . 20. Often, real-world operators contain distinct conditional effects that.
alternative method, first proposed Nebel (2000), introduce artificial actions
facts enforce, application normal action, effect-evaluation phase
conditional effects action must tried, whose condition satisfied
must applied. Briefcaseworld example, would look follows. First,
conditional effect gets removed, new fact evaluate-effects inserted add list,
new fact normal inserted precondition delete list. 2n new operators,
two object oi . One means move-along-oi , means leave-oi . former
in(oi ) precondition, latter not-in(oi ). former (at oi ?to) (not (at oi
?from) effect. evaluate-effects precondition, new fact tried-oi
add effect. final new operator stops evaluation, whose precondition
conjunction evaluate-effects tried-o1 , . . . , tried-on , whose add effect normal,
whose delete effect evaluate-effects. conditional effects several operators compiled
away method, evaluate-effects tried-oi facts made specific
operator; normal remain single fact used operators. effect k > 1 facts
condition, k leave-oi actions must created, negation one facts
precondition.
Nebels (2000) method increases plan length number distinct conditional effects
operators. Note benign are, say, 20 effects. search
procedure recognizes new constructs do, search space essentially remains
compilation. But, artificial constructs easily deciphered
human, necessarily true (is likely case) computer
searches general-purpose search procedure. example, naive forward
search space choice order application conditional effects (which
could avoided enforcing order yet artificial constructs). Probably
importantly, standard search heuristics unlikely recognize nature constructs.
example, without delete lists suffices achieve tried-o1 , . . . , tried-on once,
later apply conditional effects needed.
conclude necessary eliminate conditional effects, whenever feasible, one
compile conditional effects away first method, enumerating effect outcomes.
IPC-4. took FFs pre-processor, implements transformation steps (1) (4) above,
extended code compiles conditional effects away, optionally either two described methods. call resulting tool adl2strips.7 cases domain
version formulated ADL, used adl2strips generate STRIPS formulation domain
version. one case, version power supply restoration, also generated SIMPLE-ADL
7. Executables adl2strips downloaded IPC-4 web page http://ipc.icaps-conference.org.
also download tool named Ground, based code Mips system (Edelkamp, 2003b), takes
full syntax PDDL2.2 (Hoffmann & Edelkamp, 2005) puts grounded representation (we
use tool IPC-4 since temporal numeric planners pre-processing steps implemented).

462

fiE NGINEERING B ENCHMARKS



P LANNING

formulation. cases one, enumerating effect outcomes feasible. single exception another version power supply restoration forced use Nebels (2000)
method. Details process, exceptions use adl2strips
domain-specific method, described sections individual domains Appendix A.
2.2 Compilations Derived Predicates
several proposals literature compile derived predicates away, certain restrictions form use rest domain description (Gazen &
Knoblock, 1997; Garagnani, 2000). compilation scheme works general proposed
Thiebaux, Hoffmann, Nebel (2003, 2005). Thiebaux et al. also proved compilation scheme works general not, worst case, involve exponential
blow-up either domain description size length plans. Note exponential refers also increase plan length, description blow-up, unlike
compilation conditional effects discussed above. makes compilation derived predicates rather difficult task. IPC-4, compilation schemes oriented approaches taken
Gazen Knoblock (1997), Thiebaux et al. (2003, 2005), used. detail below.
First, let us explain derived predicates are, compilations work.
Derived predicates predicates affected operators, whose truth
value derived set derivation rules. rules take form (x) P (x).
basic intuition that, (x) satisfied instantiation c variable vector x, P (c)
concluded. formally, semantics derivation rules defined negation
failure: starting empty extension, instances P (c) derived fixpoint reached;
instances lie outside fixpoint assumed FALSE. Consider following example:
(:derived (trans ?x ?y) (or (edge ?x ?y ) (exists (?z) (and (edge ?x ?z) (trans ?z ?y)))))

derivation rule defines transitive closure edges graph. typical
application derived predicates. example, Blocksworld naturally formalized
predicate; power supply restoration domain, transitive closure models power
flow paths network electric lines. Obviously, pairs ?x ?y
transitively connected appear fixpoint negation failure.
Matters become interesting think derived predicates allowed refer
other, may used rest task description. important distinctions
are: derived predicate appear antecedent derivation rule? derived predicate
appear negated antecedent derivation rule? derived predicate appear negated
action precondition goal?
derived predicates appear antecedents derivation rules, merely
non-recursive macros, serving syntactic sugar. One simply replace derived predicates
definitions.8 derived predicate P appears negated (negation normal form the)
antecedent derivation rule predicate Q, fixpoints P Q computed
interleaved way: extension Q may differ depending order individual
instances derived. Say rule P A(x) P (x), basic predicate, rule
Q P (x) Q(x). Say objects b, current state satisfies (only) A(a).
8. derived predicates recursive cycle-free, replaced definitions may incure
exponential blow-up.

463

fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UG

Computing derived predicates interleaved way, may derive A(a) P (a), A(b)
Q(b), stop; may also derive P (a) Q(a), A(b) Q(b), A(a) P (a).
non-monotonic behavior, making non-trivial define extension B is. keep
things simple extensions derived predicates must computed every new
world state Thiebaux et al. (2003, 2005) propose simply order Q P . is, compute
P extension first compute Q based that. Generalized, one ends semantics
corresponding stratified logic programs (Apt, Blair, & Walker, 1988). context
IPC-4, i.e., PDDL2.2 (Hoffmann & Edelkamp, 2005), sake simplicity use negated
derived predicates antecedents derivation rules allowed.
Whether derived predicates appear negated action preconditions goal makes
difference Gazen Knoblocks (1997) compilation scheme. idea scheme
simply replace derivation rules actions. rule (x) P (x) replaced new
operator parameters x, precondition (x) (add) effect P (x). Actions influence
truth value affect atoms mentioned delete instances P . words,
new actions allow derivation P , normal action applied may influence
value P , extension P re-initialized.
derived predicates used negated, Gazen Knoblocks (1997) compilation
scheme works. However, say P (c) contained action precondition. compiled
version, planner achieve precondition simply applying derivation rule
action adds P (c). is, planner choice predicate instances derive,
course negation failure semantics. reader may point
wonder compile negations away first, thereafter use Gazen Knoblocks
(1997) compilation. problem would need inverse derivation rules work
negation failure semantics. clear done. Say, example,
want define negated version (trans ?x ?y) predicate above. One would tempted
take negation derivation rule antecedent:
(:derived (not-trans ?x ?y) (and (not-edge ?x ?y) (forall (?z) (or (not-edge ?x ?z) (not-trans ?z ?y)))))

work, however. Say every node graph least one adjacent edge. Starting
empty extension (not-trans ?x ?y), single instantiation derived: given
x edge, z edge x would
(not-trans z y) first place.
One possible solution difficulties extend Gazen Knoblocks (1997) compilation constructs force planner compute entire extension derived predicates
resuming normal planning. full description this, dealing arbitrary derivation rules,
described Thiebaux et al. (2003, 2005). nutshell, compilation works follows. One
introduces flags saying one normal fixpoint mode. Normal actions invoke fixpoint mode affect predicates relevant derivation rules. fixpoint mode, action
applied one conditional effect derivation rule: effect condition true,
respective derived predicate instance false, predicate instance added, plus
flag changes-made. Another action tests whether fixpoint: changes-made
true, action resets false; changes-made false, action switches back
normal mode. reduce domain STRIPS, compilation derived predicates,
negations conditional effects must compiled away techniques explained earlier.

464

fiE NGINEERING B ENCHMARKS



P LANNING

One would imagine Thiebaux et al.s (2003, 2005) compilation, making use rather complicated constructs, tends confuse domain independent search techniques. Indeed, Thiebaux
et al. (2003, 2005) report even completely naive explicit treatment derived predicates
FF performs lot better, benchmark domains, standard version FF applied
compiled benchmarks. Gazen Knoblocks (1997) compilation makes use less artificial
constructs, thus preferable whenever safely applied. Note, however, compilations imply potentially exponential blow-up plan length: exponential arity derived
predicates. worst case every action affects derivation rules, every re-computation
extension derived predicates go predicates instantiations.
situation, every pair normal actions planner apply order |C|a
actions, maximum arity derived predicate. typically small
power supply restoration domain aware features derived predicate
two (four, namely) arguments even plan length increase linear number
objects mean quite significant decrease planner performance.
IPC-4 benchmarks, derived predicates occur (only) power supply restoration (Appendix A.4) model checking safety properties (Appendix A.3). latter, derived
predicates occur negated, Stefan Edelkamp encoded domain version without derived predicates hand, using method along lines one described Gazen Knoblock (1997).
power supply restoration, derived predicates occur negated, used variation
method described Thiebaux et al. (2003, 2005). cases, due increase plan length
considered resulting domain formulation different original formulation directly compared it, terms planner performance. compiled formulations posed
competitors distinct domain versions, instead alternative domain version formulations.
Indeed, expected, planner results IPC-4 much worse compiled encodings.
2.3 Compilations Timed Initial Literals
Timed initial literals literals known become true time points pre-specified
initial state. literals compiled durational PDDL relatively easily, cost
plan length domain description size blowing linearly number timed initial
literals. compilation proposed brought attention Fox, Long, Halsey
(2004). idea use wrapper action must applied action,
whose duration occurrence time last timed initial literal. planner must also apply
sequence literal actions achieve timed initial literals order occurrence,
durations time intervals occurrences. wrapper action
terminated, literal actions longer applied. planner forced apply
direct sequence. suffices encode desired semantics. Consider following example:
(:init
(at 9 (have-to-work))
(at 19 (not (have-to-work)))
(at 19 (bar-open))
(at 23 (not (bar-open))))

encode standard durational PDDL, wrapper be:
(:action wrapper
465

fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UG

:parameters ()
:duration (= ?duration 23)
:condition
(at start (no-wrapper))
:effect
(and (at start (not (no-wrapper)))
(at start (wrapper-started))
(at start (wrapper-active))
(at start (literal-1-started))
(at end (not (wrapper-active)))))

Here, no-wrapper ensures one wrapper action executed; wrapper-started inserted
precondition every normal action thus ensures wrapper started
action executed; wrapper-active precondition literal actions. Precisely,
be:
(:action literal-1
:parameters ()
:duration (= ?duration 9)
:condition
(and (over (wrapper-active))
(over (literal-1-started)))
:effect
(and (at end (not (literal-1-started)))
(at end (literal-2-started))
(at end (have-to-work))))
(:action literal-2
:parameters ()
:duration (= ?duration 10)
:condition
(and (over (wrapper-active))
(over (literal-2-started)))
:effect
(and (at end (not (literal-2-started)))
(at end (literal-3-started))
(at end (not (have-to-work)))
(at end (bar-open))))
(:action literal-3
:parameters ()
:duration (= ?duration 4)
:condition
(and (over (wrapper-active))
(over (literal-3-started)))
:effect
(and (at end (not (literal-3-started)))
(at end (not (bar-open)))
(at end (literals-done))))

466

fiE NGINEERING B ENCHMARKS



P LANNING

fact literals-done made goal, planner must actually apply literal actions.
Note need three actions here, since two timed initial literals
longer work opening bar scheduled occur time. Note
also that, Nebels (2000) compilation conditional effects Thiebaux et al.s (2003,
2005) compilation derived predicates, compiled encoding likely confusing domain
independent search methods.
Many IPC-4 domains made use timed initial literals (in versions) encode
various kinds time windows (see Appendix A). compiled domain versions pure
(durational) PDDL above, provided resulting encodings additional domain versions.
Due increase number actions needed plans, figured compilation
constructs much change direct comparison. Indeed, derived predicates,
planner results IPC-4 much worse domain versions compiled way.

3. Summary Domains
section provide brief summary IPC-4 domains. domain, provide:
short description application; motivation inclusion domain; brief explanation
main simplifications made IPC-4; brief explanation different domain versions
formulations used IPC-4. proceed alphabetical order.
3.1 Airport
contact person application domain, Wolfgang Hatzack, working
application area several years. domain adapted IPC-4 Jorg Hoffmann
Sebastian Trug
Application. task control ground traffic airport. Timed travel routes must
assigned airplanes reach targets. inbound outbound traffic;
former airplanes must take off, latter airplanes landed park.
main problem constraint is, course, ensure safety airplanes. means avoid
collisions, also prevent airplanes entering unsafe zones behind large airplanes
engines running. optimization criterion minimize summed travel time (on
surface airport) airplanes.9 usually standard routes, i.e., routes
airplane must take outbound certain parking area, inbound certain runway.
reason introducing routes reduce complexity human ground controllers, since
significant computer support yet available real airports. Solving instances optimally (the
corresponding decision problem) PSPACE-hard without standard routes (Helmert, 2006b)
NP-complete routes standardized (Hatzack & Nebel, 2001). latter case,
pure scheduling problem. former case, complicated unrealistic airport traffic situations
lead exponentially long solutions, see Section 4.1.
Motivation. main motivation including domain able model
application quite accurately, and, particular, generate quite realistic instances. fact,
able generate instances based real airport. made possible contact
Wolfgang Hatzack, completed PhD application (Hatzack, 2002). Apart
9. alternative criterion would minimize summed squared delay airplanes. interest
airlines; minimizing summed travel time interest airport. Neither two easily
modelled PDDL2.2, discuss Simplifications, below.

467

fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UG

developing domain-specific solutions (Hatzack & Nebel, 2001), developed realistic simulation
tool, kindly supplied us purpose generating IPC-4 domain versions test
instances. Sebastian Trug implemented options inside simulator allowed it, point
time simulation traffic flow, output current traffic situation PDDL format.
simulator included real airports Frankfurt, Zurich, Munich. Frankfurt Zurich proved
large purposes, able devise competition instances based Munich airport.
Simplifications. make two simplifications. first amounts discretization space
(location) airport, making domain amenable PDDL style discrete actions.
continuous space representation, one would need actions continuous choice far
move. discretization loses precision, believe distort nature
problem much. Due amount expected conflicting traffic different points
airport, high parking positions, relatively easy choose discretization
segments different length precise small enough time. second
simplification severe: drop original optimization criterion,
awkward express current PDDL. model travel times airplanes, one needs access
times plans wait, i.e., nothing.10 aware way express
current PDDL. IPC-4 committee voted introduction additional language
construct, look clock, since didnt seem relevant anywhere else. Another option
would introduce explicit waiting actions, causes lot trouble because, similar
continuous space, must continuous choice long wait. end, decided
drop criterion now, ask planners optimize standard makespan instead,11
corresponding arrival time last airplane (meaning, arrival destination
airport). ideal, reasonable optimization criterion. planning system participating
IPC-4, single exception LPG-td (Gerevini, Saetti, & Serina, 2006), able take
account general optimization criteria built-in ones (like makespan). use
full standard routes, thus allowing airplanes choice move. use standards
routes, particularly regions near runways large airports. one thing, served
keep large airports manageable PDDL encoding planners; another thing, seems
good compromise exploiting capabilities computers time remaining
close existing practice.
Versions Formulations. generated four versions airport domain: non-temporal
one; temporal one; temporal one time windows, fact planes land
future block certain runways modeled using timed initial literals; latter version,
timed initial literals compiled away. versions, constraints ensuring airplane safety
modelled ADL logical formulas. compilation partially grounded STRIPS
provides, version, alternative formulation: domain version one ADL formulation
one STRIPS formulation.
3.2 Pipesworld
Frederico Liporace working application area several years; submitted paper
early domain version workshop competition ICAPS03. domain
adapted IPC-4 Frederico Liporace Jorg Hoffmann.
10. difficulty arises modelling delay, one must also compute travel times.
11. Makespan, Planning, means amount time start plan last action stops executing.

468

fiE NGINEERING B ENCHMARKS



P LANNING

Application. task control flow different oil derivatives pipeline
network, certain product amounts transported destinations. Pipeline networks
graphs consisting areas (nodes) pipes (edges), pipes differ length.
available actions pump liquid ends pipes, effect liquid end
pipe gets ejected. application rich additional constraints, like, constraints
types products may interface within pipe, restricted tankage space areas, deadlines
arrival products.
Motivation. main motivation including domain original structure. one inserts
something pipe one end, something possibly completely different comes pipe
end. way, changing position one object directly results changing
position several objects namely, objects inside affected pipeline.
case transportation domain aware of, fact reminiscent complicated
single-player games Rubiks Cube. Indeed, strong interaction objects lead
several subtle phenomena. example, instances solution must pump liquid
ring pipeline segments cyclic fashion.
Simplifications. severely simplify domain order able solve reasonably
complex instances current planners. importantly, encoding heavily based assuming smallest indivisible unit liquid, batch. Every amount liquid encoding modelled
terms number batches. capture continuous nature real application, means
one choose batch size trade-off encoding size accuracy. trade-off
less well-behaved one Airport (choosing segments sizes) since unit size cannot
made flexible: every batch may pass every pipeline, smallest batch governs
discretization pipelines. contrast Airport, segments may vary size.
another important simplification, used personalized goals, i.e. goals referred specific
batch objects rather product amounts. serves avoid large disjunctions enumerating
possible combinations individual batches. simplifications quite severe indeed
seems unlikely realistic representation Pipesworld, particular real-valued product amounts instead batches, could solved efficiently planners without introducing
specialized language constructs sort queue data structure PDDL, see Appendix A.2.5.
Versions Formulations. created six different versions Pipesworld: four versions /
without temporal actions, with/without tankage restrictions, respectively; one temporal version
without tankage restrictions arrival deadlines goal batches; one version identical
last one except timed initial literals compiled away.
3.3 Promela
domain created IPC-4 Stefan Edelkamp.
Application. task validate properties systems communicating processes (often
communication protocols), encoded Promela language. Promela (PROcess MEta LAnguage)
input language model checker SPIN (Holzmann, 2003). language loosely based
Dijkstras guarded command language, borrowing notation Hoares CSP language.
One important property check detect deadlock states, none processes apply
transition. example, process may blocked trying read data empty
communication channel. Edelkamp (2003a) developed automatic translation Promela
PDDL, extended generate competition examples.

469

fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UG

Motivation. main motivation including domain promote make visible important connection Planning Model Checking. Model Checking (Clarke,
Grumberg, & Peled, 1999) automated formal method basically consists three
phases: modeling, specification checking. first two phases system correctness specification modeled using formalism. last step automatically checks
model satisfies specification. Roughly speaking, step analyzes state space model
check validity specification. Especially concurrent systems, several components
interact, state spaces grow exponentially size components system. two
main research branches model checking: explicit-state model checking, implemented SPIN,
exploits automata theory stores explored state individually, symbolic model checking
describes sets states properties using binary decision diagrams (BDDs) efficient
representations Boolean formulas.
Checking validity reachability property, property asks system state certain property reachable, similar question plan existence. use model checking approaches solve planning problems explored depth, e.g. Cimatti, Roveri,
Traverso (1998), Bertoli, Cimatti, Roveri, Traverso (2001), Lago, Pistore, Traverso
(2002), Kvarnstrom, Doherty, Haslum (2000), Bacchus Kabanza (2000), Holldobler
Stor (2000), Fourman (2000), Edelkamp (2003b), Dierks (2005), Kabanza Thiebaux (2005).
However, much done inverse direction, applying planners model checking
problems. Running IPC-4 planners planning encodings Promela specifications first step
that.
Promela domain also contributes unusual structural properties domain set; computational complexity local search topology quite different discussed Section 4.
Simplifications. main simplification make use simple example classes
communicating processes. PDDL models refer fixed-length state vectors, could
include process construction calls. therefore considered active processes, i.e., processes
called initialization time. PDDL also support temporally extended
goals, consider reachability properties only. Moreover, prototypical nature
language compiler, many features Promela rendezvous communication supported. Although limited support shared variables, competition chose
simple message passing protocols only; experimented reachability properties, PDDL goals competition event deadlock detection only. Concretely,
IPC-4 instances come two toy examples used area Model-Checking: well-known
Dining Philosophers problem, Optical Telegraph problem viewed
version Dining Philosophers philosophers complex inner life, exchanging data
two hands (each separate process). both, goal reach deadlock
state.
Versions Formulations. created eight different versions domain. differ
Promela example class encoded (two options), whether use numeric variables
encoding, whether use derived predicates encoding. four encodings
Promela example class semantically equivalent sense 1-to-1 correspondence plans. decided make different versions, rather formulations,
derived predicates make large difference plan length, numeric variables make
large difference applicability planning algorithms/systems. translation Promela

470

fiE NGINEERING B ENCHMARKS



P LANNING

PDDL makes use ADL constructs, domain version contains one ADL formulation
one (fully grounded) compiled STRIPS formulation.
3.4 PSR
Sylvie Thiebaux others worked application domain. domain adapted
IPC-4 Sylvie Thiebaux Jorg Hoffmann.
Application. task PSR (power supply restoration) reconfigure faulty power distribution network resupply customers affected faults. network consists electric
lines connected switches fed via number power sources equipped circuitbreakers. faults occur, circuit-breakers sources feeding faulty lines open
protect network, leaving lines also many healthy ones un-supplied. network needs reconfigured opening closing switches circuit-breakers way
resupply healthy portions. Unreliable fault sensors switches lead uncertainty
state network. Furthermore, breakdown costs depend various parameters need
optimized constraints capacity sources lines. application topic ongoing interest field power distribution, investigated AI community
long time, including AI planning standpoint (Thiebaux, Cordier, Jehl, & Krivine, 1996;
Thiebaux & Cordier, 2001; Bertoli, Cimatti, Slaney, & Thiebaux, 2002; Bonet & Thiebaux, 2003).
Motivation. motivation including PSR twofold. First, well-researched interesting
application domain. Second, original structure rarely found previous benchmarks.
natural encoding models power propagation using recursive derived predicates compute transitive closure connectivity relation network. contrast
planning benchmarks, number actions needed optimal plan necessarily grow
instance size: available actions alter position switches, even large
network altering position switches may suffice reconfiguration. difficult
question answer is, switches.
Simplifications. Three major simplifications made. First, deterministic planning
assume network state fully observable, i.e., initial state description
complete, actions always succeed. Second, ignored numerical optimization
aspects PSR. Third, used personalized goals sense lines supplied named
explicitly goal. Note that, even simplified form, domain exhibits structure
explained above.
Versions Formulations. created four domain versions, differing primarily size
available formulations. natural domain formulation ADL derived predicates.
Though experimented many combinations PDDL encodings compilation strategies,
size instances could compile simpler languages quite restricted. Precisely,
versions are: large version ADL plus derived predicates; middle version
could devise also SIMPLE-ADL plus derived predicates STRIPS plus derived predicates;
middle-compiled version ADL, identical middle version except derived
predicates compiled away; small version pure STRIPS. instances latter
domain version particularly small, since extremely difficult come
encoding pure STRIPS either yield prohibitively long plans, prohibitively large
PDDL descriptions. fact, obtain small version applied pre-computation step (Bertoli
et al., 2002) obviates need reasoning power propagation and, consequently,

471

fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UG

need derived predicates. resulting tasks, opening closing switch directly without
detour power propagation affects parts network. Thus planner longer needs
compute flow power network, left issue configure
flow.
3.5 Satellite
domain introduced Long Fox (2003) IPC-3; adapted IPC-4 Jorg
Hoffmann. domain comes NASA space application, satellites take images
spatial phenomena. motivation inclusion IPC-4 domain applicationoriented similar sense new domains. Also, wanted immediate comparison performance achieved IPC-3, achieved IPC-4. top 5 domain
versions used IPC-3, added 4 new versions, introducing additional time windows (formulated
alternatively timed initial literals compilation) sending data earth.
3.6 Settlers
domain also introduced Long Fox (2003) IPC-3. task build
infrastructure unsettled area, involving building housing, railway tracks, sawmills, etc.
distinguishing feature domain domain semantics encoded numeric variables. makes domain important benchmark numeric planning.
reason, IPC-3 participant could solve smallest instances, included
domain IPC-4. modification made except compiled away universally
quantified preconditions order improve accessibility.
3.7 UMTS
Roman Englert working application area several years. domain adapted
IPC-4 Stefan Edelkamp Roman Englert.
Application. third generation mobile communication, so-called UMTS (Holma &
Toskala, 2000), makes available broad variety applications mobile terminals.
comes challenge maintain several applications one terminal. First, due limited resources, radio bearers restrictions quality service (QoS) applications. Second,
cell setup execution several mobile applications may lead unacceptable waiting periods
user. Third, QoS may insufficient call setup case execution
mobile application shut down. Thus arises call setup problem several mobile applications. main requirement is, course, setup minimum possible amount
time. (pure) scheduling problem necessitates ordering optimizing execution
modules needed setup. many scheduling problems, finding some, necessarily
optimal, solution trivial; main challenge find good-quality solutions, optimal ones ideally.
Motivation. main motivation modelling pure scheduling problem planning domain
strong industrial need flexible solution procedures UMTS call setup,
due rapidly evolving nature domain, particularly sorts mobile applications
available. ideal solution would put automatic planner mobile device,
let compute optimized schedules on-the-fly. sense, UMTS call setup
natural promising field real-world application automatic planners. also interesting

472

fiE NGINEERING B ENCHMARKS



P LANNING

sense scheduling problems far central competitive AI planning,
domain serves advertise usefulness PDDL addressing certain kinds scheduling
problems.
Simplifications. setup model chose considers coarse parts network environment
present UMTS applications invoked. Action duration fixed rather computed
based network traffic. inter-operational restrictions different concurrent devices
also neglected. considered plausible timings instances rather real-application
data running certain applications UMTS device. designed domain
10 applications single device. challenge optimal planners computing minimum
makespan solutions, much challenge satisficing planners.
Versions Formulations. created six domain versions; arise two groups
three versions each. first group, standard UMTS domain, comes without timing
constraints. latter represented either using timed initial literals, compilation;
before, separated two options different domain versions (rather domain version
formulations) due increase plan size. second group domain versions similar
structure. difference three domain versions includes additional flaw
action. single step, action achieves one needed fact, where, normally, several steps
required. However, action useless reality deletes another fact needed,
cannot re-achieved. flaw action added see happens intentionally
stressed planners: beside increasing branching factor, flaw action look useful
perspective heuristic function ignores delete lists.

4. Known (Theoretical) Results Domain Structure
section, start structural analysis IPC-4 domains summarizing known
results literature. Helmert (2006b) analyzes domains perspective domainspecific computational complexity. Hoffmann (2005) analyzes domains used IPCs far,
plus standard benchmarks literature, identifying topological properties search
space surface relaxed plan heuristic introduced FF system (Hoffmann
& Nebel, 2001), variants used many modern planning systems. studies
exclusively concerned purely propositional non-temporal STRIPS ADL planning.
follows, domain names refer respective (non-temporal) domain versions.12
4.1 Computational Complexity
Helmert (2006b) studied complexity plan existence bounded plan existence
IPC-4 benchmark problems. Plan existence asks whether given planning task solvable. Bounded
plan existence asks whether given planning task solvable given number
actions. Helmert established following results.
Airport, plan existence bounded plan existence PSPACE-complete, even
aircraft inbound need taxi park goal location, map planar
symmetric, safety constraints simply prevent planes occupying adjacent segments.
12. UMTS domain, temporal versions, treated either studies. computational
complexity, easy see deciding plan existence P deciding bounded plan existence (optimizing
makespan) NP-complete UMTS. Topological properties relaxed plan heuristic havent yet defined
temporal setting.

473

fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UG

proof reduction Sliding Tokens puzzle, set tokens must reach goal
assignment vertices graph, moving adjacent vertices ensuring two
tokens ever find adjacent vertices. length optimal sequential plans
exponential number tokens, likewise airport domain. Even parallel plans
shorter linear amount, since plane move per time step. proof
Sliding Tokens puzzle quite complicated involves construction instances
exponentially long optimal plans. one would expect, constructions used
unlikely occur real airport; particular true necessary density conflicting
traffic graph structure. consider interesting since makes Airport benchmark
extremely high worst-case complexity, much good-natured typical case
behavior. Typically, ample space airport (comparatively) airplanes moving
across it.
Pipesworld, whether without tankage, plan existence bounded plan existence NP-hard. unknown whether NP, however. NP-hardness proof
reduction SAT four literals per clause variable occurs
3 clauses. SAT instance reduced network way parts network (variable subnetworks) represent choice assignment variables, parts
(clause subnetworks) represent satisfaction clauses. content areas pipes
initialized batches way interface restrictions guarantee goal area
reached certain batch clause subnetwork iff clause satisfied assignment.
general Promela planning, defined Edelkamp (2003a), plan existence bounded
plan existence PSPACE-complete. PSPACE-hardness proof reduction halting problem space-restricted Turing Machines (TM). cells machines tape
mapped onto process queue unit capacity, states TM form set Promela
messages, TMs alphabet form set Promela states processes, Promela transitions encode TMs transitions. shown TM halts iff Promela task reaches
deadlock.
Dining Philosophers, hand, particular structure one process per
philosopher, transition graph. Optimal plans generated linear time
number philosophers making constant number transitions reach known state
graphs. Similar considerations apply Optical Telegraph.
PSR tasks also solved optimally polynomial time, requires rather complex
algorithm. plans start wait action opens circuit-breakers affected fault.
simplest form, optimal plans follow prescribing series actions opening switches
connecting feedable line faulty one. necessary also sufficient ensure
network safe state faulty line re-supplied. minimal set devices
(disjoint previous one) must closed resupply rest network.
achieved generating minimal spanning tree healthy part network,
done polynomial time.
Figure 2 gives overview results summarizes Helmerts (2003) results
standard benchmarks. domain set displayed set investigated Hoffmann (2005),
minor differences explained shortly. Blocksworld-no-arm, Briefcaseworld, Ferry, Fridge,
Simple-TSP, Tireworld traditional planning benchmarks never used IPC.13
13. Blocksworld-no-arm version Blocksworld blocks moved directly destination, without
referring robot arm. Simple-TSP used (Fox & Long, 1999) demonstrate potential symmetry

474

fiFOR

P LANNING

PSPACE

E NGINEERING B ENCHMARKS

Promela
Airport

P

Plan Existence

NP

Pipesworld
Mystery
Mprime
MiconicADL
Freecell

Tireworld
SimpleTSP
Schedule
PSR
OpticalTelegraph
Movie
Gripper
Fridge
Ferry
DiningPhil.

Zenotravel
Satellite
Rovers
MiconicSTRIPS
MiconicSIMPLE
Logistics
Grid
Driverlog
Depots
Briefcaseworld
Blocksworldnoarm
Blocksworldarm

P

NP

PSPACE

Bounded Plan Existence

Figure 2: overview Helmerts results computational complexity benchmarks.
IPC-1 benchmarks Assembly, Grid, Gripper, Logistics, Movie, Mprime, Mystery.
IPC-2 benchmarks Blocksworld-arm, Freecell, Logistics, Miconic-ADL, Miconic-SIMPLE,
Miconic-STRIPS (Miconic Schindler Lifts name elevator domain), Schedule.
IPC-3 benchmarks Depots, Driverlog, Freecell, Rovers, Satellite, Zenotravel. IPC-4
benchmarks displayed bold face, including (hypothetical) general Promela domain.
table Figure 2 organized along two axes, x axis shows complexity
deciding bounded plan existence, axis shows complexity deciding (unbounded) plan
existence. Membership table entry means, NP PSPACE rows columns,
respective problem complete respective complexity class. exception Pipesworld
domain, which, stated above, still unknown whether two decision problems also
members NP. Assembly domain displayed since, there, Helmert (2003) proved
existence exponentially long optimal plans, showing plan generation quite hard
domain. table sectors diagonal crossed unbounded plan existence
polynomially reduced bounded plan existence set bound 2n , n
number distinct actions, or, ADL, number distinct conditional effects.
striking new feature IPC-4 introduction PSPACE-complete benchmark
domains, filling top right corner Figure 2. Thus, benchmarks cover four inhabited
sectors table. previous IPCs, IPC-1 IPC-2 cover three sectors inhabited
detection. One simply visit n nodes, using move action applied two nodes,
permutation nodes optimal tour. Hoffmann (2005) also investigates Towers Hanoi domain.

475

fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UG

sectors except top right corner IPC-3 benchmarks cover two sectors namely,
bounded plan existence NP-complete domains, domains except Freecell
polynomial time algorithm deciding unbounded plan existence.
IPC-4 benchmarks exceptional aspects visible Figure 2. particularly, explained above, polynomial decision algorithm PSR highly non-obvious.
benchmarks important since, one hand, principle allow planners provide efficient solutions, while, hand, necessitating employ interesting techniques
so.14 Schedule polynomial benchmark bounded plan generation
requires non-obvious algorithm. 20 domains left bottom middle bottom
sectors table, polynomial algorithms deciding bounded unbounded plan existence
completely trivial, mostly addressing one subgoal time.
pointed already, final exception lies extraordinarily large difference
worst-case typical-case behavior Airport. see Section 5, even fully automated
methods (the IPC-4 planners) are, least unbounded plan existence (generation), quite efficient
typical instances domain. large differences worst-case typical-case
behavior unusual, believe extent phenomenon Airport really unusual.
example, planners tend find PSR much harder Airport.
4.2 Topology h+
Hoffmann (2005) considers state spaces (the forward search spaces) STRIPS ADL tasks
taken standard benchmark domains. defines, given task world state s, h+ (s)
length shortest possible relaxed plan, relaxed plan. relaxed plan
plan achieves goal one assumes delete lists empty. Computing
h+ (the corresponding decision problem) NP-hard (Bylander, 1994). Many modern planners,
e.g., HSP (Bonet & Geffner, 2001), FF (Hoffmann & Nebel, 2001), SGPlan (Wah & Chen, 2004;
Chen, Hsu, & Wah, 2004), YAHSP (Vidal, 2004), Fast-Diagonally-Downward (Helmert, 2004,
2006a), interpreted sort heuristic search approximation h+ , plus
techniques like problem decomposition (Wah & Chen, 2004), lookahead techniques (Vidal,
2004), additional different heuristic functions (Helmert, 2004). context, question
great practical interest quality underlying heuristic function addressed domains.
Heuristic quality measured terms topological properties search space surface:
many local minima there? large they? flat regions? Hoffmann (2005)
investigates questions h+ function, topological properties search space
surface proven.
Hoffmann defines topological phenomena following Frank, Cheeseman, Stutz (1997).
identifies several parameters show particularly interesting behavior planning benchmarks.
dead end world state reachable initial state goal state cannot
reached. unrecognized dead end dead end h+ (s) < . exit distance
state length shortest path state space leading state s0 ,
h+ (s) = h+ (s0 ), s0 direct neighbor state s00 h+ (s00 ) < h+ (s0 ). is,
exit distance number steps need go order find better state (s00 ),
14. Helmerts (2005) words: think domains solved polynomial time polynomial
algorithms obvious extraordinarily interesting. Deterministic PSR definitely domain kind
regard optimization. NP-hard problems cannot solved without strong reliance search, polynomial
problems can, planners capture important concepts.

476

fiE NGINEERING B ENCHMARKS



P LANNING

minus 1 since distance s0 measured. Here, s0 plays role exit state used
Frank et al. (1997). state lies local minimum paths exit temporary increase
heuristic value; otherwise state lies bench. maximal local minimum exit distance
(mlmed), state space, maximum exit distances states lying local minima
state space. Similarly, maximal bench exit distance (mbed) maximum exit
distances states lying benches. core results Hoffmanns (2005) investigation
displayed Figure 3.

Blocksworldarm
Depots
Driverlog

Pipesworld
PSR

Rovers
OpticalTelegraph

Mystery
Mprime
MiconicADL
Freecell
Assembly
Airport

mbed <= c

mlmed <= c

Hanoi [0]
Blocksworldnoarm [0]
Fridge [0]
Briefcaseworld [0]
Grid [0]

Logistics [0,1]
Ferry [0,1]
Gripper [0,1]
undirected

Tireworld [0,6]
Satellite [4,4]
Zenotravel [2,2]
MiconicSIMPLE [0,1]
MiconicSTRIPS [0,1]
Movie [0,1]
SimpleTSP [0,0]
harmless

DiningPhil. [31,31]
Schedule [5,5]

recognized

unrecognized

Figure 3: overview Hoffmanns results topology h+ benchmarks.
x-axis Figure 3 corresponds properties regarding dead ends. y-axis corresponds
properties regarding exit distance local minima benches. domains assigned
appropriate table sectors classes domains depending worst-case behavior possible
them. detail, meaning table following. state space undirected
every transition (action) directly inverted; state space harmless inversion
possible, dead ends anyway; recognized means dead ends,
h+ them; unrecognized means least one unrecognized dead end.
domain falls class worst-case instance: example, single instance whose
state space contains single unrecognized dead end, domain considered unrecognized.
results proved, i.e., domain is, example, considered harmless, means
provably instance domain contains dead ends.
y-axis Figure 3, distinction lines correspond existence non-existence
constant upper bounds maximal local minimum exit distance (upper line) maximal
bench exit distance (lower line). Note constant upper bounds maximal local minimum
exit distance exist domains upper line domains lower line,
bounds exist.15 constant, meant bound valid every instance
15. presentation assumes domains bounded bench exit distance subset bounded
local minimum exit distance. true general, hold considered benchmark domains.

477

fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UG

domain, regardless size. actual bounds proved displayed brackets; local minimum
bound precedes bench bound cases both. right bottom part table
crossed since unrecognized dead ends infinite exit distance domain classes
empty.16
obvious intuition behind Figure 3 transition easy hard
planning systems based heuristic search approximating h+ one moves left bottom
side top right side table. Indeed, table does, sense, coincide well
empirical behavior of, least, FF system. Note extreme topological behavior
many domains. upper bound local minimum exit distance 0 means
local minima all. case 13 30 investigated domains. several
domains, widely used Logistics benchmark, top single step suffices reach
exit benches. Hoffmann (2005) shows FF would polynomial bottom classes
table, provided oracle computing h+ .
Considering table perspective benchmark development, one notices particularly older benchmarks tend lie left bottom side; consider example Ferry, Briefcaseworld, Fridge, Simple-TSP, Tireworld. distribution IPC-1 benchmarks Gripper,
Logistics, Movie, Grid, Assembly, Mystery, Mprime somewhat extreme: first four
list belong simple classes, last three belong hardest class (until today,
Mystery Mprime domains amongst causing planners trouble).
IPC-2 benchmarks Logistics, Blocksworld-arm, Miconic-STRIPS, Miconic-SIMPLE, Schedule,
Freecell, Miconic-ADL again, many simple challenging domains.
notable exceptions respect Blocksworld-arm, left top side table,
Schedule, contain dead ends local minima. IPC-3 benchmarks, distribution starts get varied. domains Zenotravel, Satellite, Depots, Driverlog, Rovers,
Freecell span three four top classes table, plus one bottom classes.
IPC-4 domains, shown bold face, obviously continue development. two
sharing class Pipesworld PSR.17 continue emphasis spanning top classes
table; new domain one bottom classes Dining Philosophers,
highly exceptional exceedingly large bound, making bound practically useless
exploitation planning.18 Satellite domain adopted IPC-3 benchmarks serves
represent (a interesting instance of) easier classes. Note Satellite simple
talking STRIPS version, drops challenging problem constraints formulated numeric variables. Airport domain exceptional top right class
that, again, worst-case place Figure 3 differs lot typical case. dead
end Airport situation two airplanes completely block others paths.19 course,
practical airports designed way doesnt usually happen. mentioned earlier,
usually non-overlapping, far possible standard routes, place
blocking occur densely populated areas near parking positions.
16. One could skip unrecognized dead ends definition maximum exit distances, Hoffmann (2005)
argues un-intuitive, plus making things unnecessarily complicated.
17. Actually, Pipesworld invertible sense every two-step sequence (starting ending pumping operation)
directly undone. considered harmless since single actions cannot inverted.
18. Indeed, h+ bad heuristic Dining Philosophers. basically comes counting number
unsatisfied goals.
19. relaxed plan use free space planes make move across other.

478

fiE NGINEERING B ENCHMARKS



P LANNING

5. New (Empirical) Results Domain Structure
provide empirical analysis various structural parameters IPC-4 domains.
sake readability conciseness, focus non-temporal domain versions only.
types data measure, results temporal domain versions quite similar.
extent, visible tables showing numbers actions facts, domain versions,
individual domain descriptions Appendix A.
empirical analysis aimed highlighting characteristics of, differences between, IPC-4 domains. Apart focussing practical parameters, analysis
compared theoretical results cited previous section big advantage tells us
something actual instances run competition. Note choice instances
make huge difference example, stated earlier, real-world airport likely
exponentially long plans, neither likely provoke many dead-end situations. possible all, instances used IPC-4 chosen relatively realistic (details Appendix A).
analysis structured three sub-sections. Section 5.1 shows how, individual
domains, size grounded encoding grows instance size. Section 5.2 assesses
correspondence quality standard heuristic functions, runtime achieved
IPC-4. Section 5.3, finally, assesses fact connectivity instance size, meaning number
choices one achieve fact, number actions fact required for.
5.1 Encoding Size
current STRIPS ADL planners, far authors aware, ground parameters
variables pre-process, ending task representation consisting ground facts
ground actions. obvious question ask large grounded encodings are. Figure 4
shows data, numbers facts actions plotted instance size (selected versions of)
different domains. numbers measured using FFs pre-processor. filters static facts
facts added deleted action unreachable actions, meaning actions
appear relaxed planning graph (a planning graph without mutex reasoning) initial
state (Hoffmann & Nebel, 2001); formulas compiled simple STRIPS-like conjunctions
facts, along lines Gazen Knoblock (1997) outlined Section 2.
100000

1e+06
Airport
Pipesworld
Dining Philosophers
Optical Telegraph
PSR small
PSR large
Satellite
UMTS

10000

Airport
Pipesworld
Dining Philosophers
Optical Telegraph
PSR small
PSR large
Satellite
UMTS

100000

Nr. Actions

Nr. Facts

10000
1000

1000

100
100

10

10
5

10

15

20

25
30
Nr. Instance

35

40

45

50

5

10

15

20

25
30
Nr. Instance

35

40

45

50

(a)
(b)
Figure 4: Numbers (a) ground facts (b) ground actions, plotted instance number,
selected versions IPC-4 domains.
479

fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UG

cases except UMTS (that temporal versions), domain version selected
Figure 4 non-temporal. Let us consider domains one one. Airport, one
non-temporal version. plots Figure 4 (a) (b) show us quite nicely instances
scaled, sharp drops curves corresponding steps new underlying airport. Precisely,
instances 1 3, 4 9, 10 20, 21 35, 36 50 based growing airports, respectively,
within airport number travelling airplanes grows 1 2 15 (in
instance 50). example, instance 35 instance 36 step one half Munich airport,
12 airplanes, full Munich airport, 2 airplanes.
Pipesworld, two non-temporal versions, without tankage restrictions. Figure 4 shows data former, challenging one (the IPC-4 planners fared much
worse it); without tankage restrictions, slightly fewer facts, factor 510 fewer actions. Pipesworld instances scaled similar way Airport ones: five
growing pipeline networks feature growing number travelling liquid batches. networks underlie instances 1 10, 11 20, 21 30, 31 40, 41 50, respectively.
Corresponding drops observed stepping instance 30 31, and, less significantly,
stepping 20 21 40 41. major difference Airport visible
much crippled nature (featuring much variance) curve number actions.
because, Airport, objects move big spacious structure, while, Pipesworld,
many objects move within rather dense space.20 fundamental difference Airport
Pipesworld also manifests order curves reversed numbers facts
actions: Airport, extraordinarily many facts required describe huge airport structure,
Pipesworld fewer facts smaller structure, many actions describing
things move along structure. stated earlier, Pipesworld, different objects affect
others position moving.
Promela domains, Dining Philosophers Optical Telegraph, data domain
versions without derived predicates identical, derivation rule deriving fact
counted action achieving fact. main difference seen lies
extremely smooth scaling. domains single size parameter, numbers
ground facts actions grow linear functions parameter functions Optical Telegraph order magnitude higher Dining Philosophers. curves
Optical Telegraph stop instance 17 able compute grounded
representation much time memory needed simplification precondition formulas. Note artifact data presentation, rather constitutes serious
limitation planner tries perform pre-processing.
PSR, interesting domain versions small, since could formulated
STRIPS, large, since goes instances realistic size (in largest instances,
is). name small suggests, numbers quite small able compile STRIPS,
indicated earlier make instances small.21 Essentially compilation
problem also visible curves large, huge number ground facts actions
relatively early instances already. curves stop instance 20 beyond that, simplifying
20. much objects cannot move affects also number ground actions due mentioned filtering
unreachable actions.
21. notable exception instance nr. 25, number actions peaks 9400. due exceedingly complex goal formula, 9216 disjuncts DNF, yields extra goal-achievement action,
c.f. Section 2.

480

fiE NGINEERING B ENCHMARKS



P LANNING

formulas becomes extremely costly. versions, note high degree variance
numbers facts actions, somewhat corresponds huge degree variance
observed planner performance domain (see Figure 8). Part variance, least
pace oscillations amplitude, explained way instances
scaled. given number sources (the instance size), generated instances increasing
minimal number switches originally fed given source, given number switches,
generated instances increasing percentage faulty lines ranging 10% 70%.
Intuitively, larger number switches per source, larger harder expect instance
be. Furthermore, percentage faulty lines tends induce easy-hard-easy pattern.
lines faulty, small part network resupplied devices need
switched. Similarly, faulty lines exist, network resupplied
switching operations. intermediate percentage, effects actions become
complex conditioned positions many switches instances
become critically constrained harder solve.
Satellite, main observation made extremely steep ascent curves
instance 20, particularly growth extremely high numbers actions. two reasons
this. First, one action Satellite (take-image) 4 parameters reachable almost
combination objects correct types (most time, actions 2 3 parameters).
Second, size instances grows sharply beyond instance 20 which, simply,
instances 21 36, used IPC-4, correspond 16 instances posed IPC-3
challenge hand-tailored planners.
consider Settlers ease readability graphs, since domain
quite obviously exceptional anyway, relies almost completely numeric variables.
UMTS, Figure 4 shows data plain domain version without time windows flaw action.
obvious characteristic numbers facts actions constants. true
domain versions, numbers vary slightly. reason that, way UMTS instances
scaled, every instance describes applications requirements; changes (only)
goal, specifying applications actually need set up. Independent effect
particular scaling method used, observe numbers facts actions relatively
low around 100 even largest instances, applications must set up,
plans contain actions.
5.2 Quality Heuristics, Runtime
section, measure length best (sequential parallel) plans found
planner, (sequential parallel) plan length estimates returned common heuristic
functions, runtime taken planners. Precisely, optimal planners, measure:
optimal makespan, found IPC-4 parallel optimal planners (planners optimizing
makespan).
length standard plan graph (Blum & Furst, 1997), i.e., index first plan
graph layer contains goals without mutexes.
best runtime taken parallel optimal planner IPC-4.
optimal sequential plan length, found IPC-4 sequential optimal planners.
481

fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UG

length serialized plan graph, pair non-NOOP actions made mutex.
best runtime taken sequential optimal planner IPC-4.
satisficing planners, measure:
best (shortest) plan length, found planner IPC-4.
length relaxed plan initial state (an action sequence solves task one
assumes delete lists empty; computed FF (Hoffmann & Nebel, 2001)).
best runtime taken satisficing planner IPC-4.
main goal identify characteristic behavior domains, identify characteristic
effects heuristic quality performance. reader note that, selection measurements, make several simplifying assumptions. Optimal planners exclusively based
plan graph estimates. Satisficing planners exclusively based relaxed plan estimates. Further, satisficing planners minimize makespan, sequential plan length. chose
take account latter since potentially over-estimating (non-admissible) heuristic
specifically estimating parallel plan length; best knowledge, satisficing planners
minimizing makespan actually use heuristic estimating number remaining actions, employ method greedily arrange chosen actions parallel plan. said,
wish imply simplifying assumptions safe sense lose important
information. simplifying assumptions necessary make analysis presentation
feasible. data show definitely capture many crucial aspects IPC-4 heuristic quality
planner runtime. show data individual domains, proceeding alphabetical order.
(IPC-4) runtime results obtained Linux machine running two Pentium-4 CPUs 3GHz,
6 GB main memory; time memory cutoffs 30 minutes 1 GB, per instance.
Consider Figure 5, showing data Airport domain. Note axis two different
meanings, runtime left hand side, number (parallel sequential) plan steps
right hand side. applies figures sub-section. Airport, observe
clear correlation quality plan length estimation, runtime. optimal parallel
planners, Figure 5 (a), best observed instances nr. 15 20. There, difference
makespan estimate plan graph grows, grows achieved runtime,
exponential scale. may look like counter example that, instance nr. 20, plan
graph estimate exact (coincides real makespan), runtime get lower again.
Note however, instance 20 based much larger airport previous instances.
instance 20 onwards, instances solved parallel planner exact plan graph
estimate. optimal sequential planners, Figure 5 (b), get similar behavior
instances nr. 14 18. behavior also strong instances nr. 35 36: plan
length grows lot 35 36, serial plan graph becomes little shorter; correspondingly,
runtime goes two orders magnitude. true instances 20 21.
satisficing planners, Figure 5 (c), striking observation length
real plan coincides, instances, exactly length relaxed plan (for respective
initial state). actually quite easy explain: optimal plan moves airplanes way
never block paths; plan optimal even ignoring delete lists.
Moving airplanes without blocking always possible start. situation changes
482

fiE NGINEERING B ENCHMARKS

1000

70



P LANNING

10000

160

Optimal MakeSpan
PlanGraph
Best Parallel Runtime

Optimal NrActions
SerialPlanGraph
Best Sequential Runtime
140

60

1000

100
120
50

30

1

10

80

Nr. Steps

40

Runtime (sec.)

100

10

Nr. Steps

Runtime (sec.)

100

60
1
20
40

0.1
0.1

10

0.01

20

0
5

10

15

20

25

30

35

40

45

0.01

0

50

5

10

15

20

25

Nr. Instance

Nr. Instance

(a)

(b)
1000

30

35

40

45

50

700
Best NrActions
RelaxedPlan
Best Runtime
600

100

10

400

300

1

Nr. Steps

Runtime (sec.)

500

200
0.1
100

0.01

0
5

10

15

20

25

30

35

40

45

50

Nr. Instance

(c)
Figure 5: Airport domain. Plots (parallel) plan length, heuristic estimation, runtime,
(a) optimal parallel planners, (b) optimal sequential planners, (c) satisficing planners.
wrong decision made, additional moves become necessary reality,
without delete lists avoid blocking situation. Apart this, Figure 5 shows quite nicely
runtime taken corresponds closely length plan found. Note latter
huge, 694 largest instance.
Pipesworld domain, two non-temporal domain versions: with/without tankage
restrictions, i.e., restrictions amount liquid stored network areas.
Figure 6 shows data version without restrictions; observations made
domain version similar, except sorts planners scale much worse, thus providing
us less data. optimal planners, Figure 6 (a) (b), striking difference
Airport domain Figure 5 (a) (b) quality even parallel plan graph heuristic
bad: underestimates real makespan much larger extent Airport.
underestimation grows instance size, and, naturally, runtime grows well. Note
planners fail scale much earlier Figure 5 (a) (b). one slight exception
rule poorer heuristic estimate leads longer runtime: instance number 10 11,
optimal sequential plan length grows 19 20, length serial plan graph remains 9,
runtime drops 1400 150 secs.

483

fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UG

1000

16

10000

20

Optimal MakeSpan
PlanGraph
Best Parallel Runtime

Optimal NrActions
SerialPlanGraph
Best Sequential Runtime
18

14

1000

100
16
12

8

1

10

12

Nr. Steps

10

Runtime (sec.)

14

10

Nr. Steps

Runtime (sec.)

100

10
1
6
8

0.1
0.1

4

0.01

6

2
5

10

15

20

25

30

35

40

45

0.01

4

50

5

10

15

20

25

Nr. Instance

Nr. Instance

(a)

(b)
10

30

35

40

45

50

160
Best NrActions
RelaxedPlan
Best Runtime
140

120
1

80

Nr. Steps

Runtime (sec.)

100

60
0.1
40

20

0.01

0
5

10

15

20

25

30

35

40

45

50

Nr. Instance

(c)
Figure 6: Pipesworld domain without tankage restrictions. Plots (parallel) plan length, heuristic estimation, runtime, (a) optimal parallel planners, (b) optimal sequential planners, (c) satisficing planners.
Similarly situation optimal planners, satisficing planners, Figure 6 (c),
main difference Figure 5 (c) much worse quality heuristic function:
relaxed plan length differs greatly length real plans found, particularly
larger instances. curiously, despite worse quality heuristic, runtimes much
lower. longest time taken instance 10 seconds. goes show, first,
shortcomings analysis here: give heuristic quality initial state, may
differ lot situation rest state space. example, Airport planner using
relaxed plans may get lost huge dead ends wrong decision made early on. Second,
course, techniques satisficing planners use also relevant. runtime data
Figure 5 (b) exclusively due SGPlan (Wah & Chen, 2004) YAHSP (Vidal, 2004), whose
problem decomposition/greedy lookahead techniques appear work extremely well domain.
satisficing planners perform much worse, failing solve largest instances. note
Pipesworld, overall runtime curves (for planners) characteristically jagged
show considerable variance comparison to, e.g., Airport. information gets lost
best-of presentation chosen figures here. seems hardness domain comes

484

fiE NGINEERING B ENCHMARKS



P LANNING

interactions subtle seen rather high-level parameters measured here. reiterate domain version tankage restrictions much challenging planners,
planner getting anywhere close largest instances YAHSP.
10000

1000

1

350

Optimal MakeSpan
PlanGraph
Best Parallel Runtime
Optimal NrActions
SerialPlanGraph
Best Sequential Runtime

1000

Best NrActions
RelaxedPlan
Best Runtime
300

250

10

200
0.1
150

Nr. Steps

Runtime (sec.)

100
Nr. Steps

Runtime (sec.)

100

10
100
1
50

0.1

1
5

10

15

20

25

30

35

40

0.01

45

0
5

Nr. Instance

10

15

20

25

30

35

40

45

Nr. Instance

(a)
(b)
Figure 7: Dining Philosophers domain without derived predicates. Plots (parallel) plan length,
heuristic estimation, runtime, (a) optimal planners (b) satisficing planners.
Figure 7 shows data Promela/Dining Philosophers without derived predicates.
show two separate figures optimal planners since curves quite easy read. even
quick glance, one sees domain characteristic behavior different
domains. optimal makespan, plan graph length, serial plan graph length constant
across instance size. contrast, optimal sequential plan length grows linear function
size; note logarithmic scale right hand side axis Figure 7 (a), use
make figure (the values plan step measures) readable. best plans found
satisficing planners optimal, i.e., NrActions data identical sides
figure. Figure 7 (a), see effect heuristic quality search performance:
parallel planners scale linear function instance size, sequential planners,
heuristic function becomes worse worse, scale highly exponentially. latter might
also true satisficing planners; bit hard tell since solved instances solved
extremely quickly. reason instance index higher 29 solved that,
instances, similarly discussed (Section 5.1), simplifying precondition formulas
became prohibitively costly, instances available ADL only. two satisficing
planners scaled well Dining Philosophers (without derived predicates) SGPlan
YAHSP neither could handle ADL formulation domain. Similarly,
optimal planners SATPLAN04 Optiplan scaled well, neither could handle
ADL formulation. Note inability planners handle formulas without pre-simplification
techniques thus constitutes serious limitation.
Optical Telegraph without derived predicates (no figure shown) observations similar
ones Figure 7, except planners scale much worse. particularly, optimal
sequential planners solve single smallest instance, best satisficing runtime clearly
exponential instance size, taking 1500 seconds solve instance number 25. Promela
domain versions derived predicates, results optimal planners since none
could handle derived predicates. observations satisficing planners similar
485

fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UG

above: NrActions grows linear function instance size, relaxed plan length grows
linear function significantly lower gradient. planners fast Dining Philosophers
need lot time (> 1000 sec) solve largest Optical Telegraph instances (some
remain unsolved). omit results Promela domain versions using numeric variables,
since two planners participated domain versions.
1000

40

1000

35

Optimal MakeSpan
PlanGraph
Best Parallel Runtime

Optimal NrActions
SerialPlanGraph
Best Sequential Runtime
35

30

100

100
30

1

15

10

20

15

1

Nr. Steps

20

Runtime (sec.)

10

Nr. Steps

Runtime (sec.)

25
25

10
10
0.1

0.1
5

5

0.01

0
5

10

15

20

25

30

35

40

45

0.01

50

0
5

10

15

20

Nr. Instance

25

30

35

40

45

50

Nr. Instance

(a)

(b)

10

50
Best NrActions
RelaxedPlan
Best Runtime

1000

60
Best NrActions
RelaxedPlan
Best Runtime

45

50
40
100
35

Nr. Steps

Runtime (sec.)

25
20

10

30

0.1

Nr. Steps

40
30

Runtime (sec.)

1

20
15
1
10
10
5

0.01

0
5

10

15

20

25

30

35

40

45

0.1

50

0
5

Nr. Instance

10

15

20

25

30

35

40

45

50

Nr. Instance

(c)
(d)
Figure 8: PSR domain. Plots (parallel) plan length, heuristic estimation, runtime, (a)
parallel optimal planners PSR small (STRIPS version), (b) sequential optimal planners PSR small, (c) satisficing planners PSR small, (d) satisficing planners
PSR large (featuring ADL derived predicates).
Figure 8 shows results PSR domain. Figure 8 (a), (b) (c) show plots domain version PSR small, comes pure STRIPS addressed IPC-4 planners;
Figure 8 (d) shows plots PSR large, comes ADL derived predicates
addressed four satisficing planners only. show data PSR middle-compiled
PSR middle: former, two satisficing planners participated; latter, six satisficing
planners participated, scaled quite well less challenging instances results
less interesting PSR large.

486

fiE NGINEERING B ENCHMARKS



P LANNING

First, note curves PSR small show large amount zig-zagging, quite
unusual cannot simply accounted way instances scaled.22 Consider
Figure 8 (a). main observation made real optimal makespan much larger
estimation plan graph, particularly larger instances. Still, optimal parallel planners
quite efficient, least solve instances. runtime data entirely due
SATPLAN04, whose search techniques apparently quite efficient domain even
bad plan graph lower bound. optimal planners least one order magnitude
slower, cant solve largest instances; example, none solve instances 48
49. optimal sequential planners Figure 8 (b), results pretty similar except
runtime scaling somewhat worse. kinds optimal planners, runtime clearly
correlated length optimal plans, which, since plan graph bounds almost
constant, coincides difference real plan length estimate.
Figure 8 (c), observe relaxed plan bad estimator plan length PSR
small (at least respective initial states), planners solve instances quite efficiently anyway. runtime data entirely due YAHSP Fast Downward; particularly
Fast Downward extremely efficient, showing slight increase runtime instance
size, satisficing planner capable solving instances 48 49. Note YAHSP
(Vidal, 2004) uses powerful techniques besides relaxed plan heuristic, Fast Downward
(Helmert, 2004) uses involved (and apparently powerful, case) heuristic function. Note also that, least terms solved instances, optimal satisficing planners are,
unusually, equally good (or bad) domain: exactly one group solves instances,
planners cannot solve instances 48 49. difficulty planners experiencing
domain also remarkable since instances, least grounded encodings, actually
small compared instances domains, c.f. Figure 4. indicates
domain fundamental characteristic yet captured well search
heuristics/techniques (most of) planners nicely complements said
non-obvious polynomial algorithm PSR Section 4.1.
Figure 8 (d), see relaxed plan (computed version FF handling derived
predicates, see Thiebaux et al., 2003, 2005) rather useless estimator PSR domain
expressed natural way using ADL derived predicates. relaxed plan constantly
contains 0 steps, meaning over-approximation semantics derived predicates makes
initial state look like goal state; happens PSR middle. situation may
different parts state space heuristic value constantly 0 this, apparently,
causes serious trouble satisficing planners except Fast Downward. planner except Fast
Downward solve instance higher number 16. Fast Downward seems profit, again,
involved heuristic function, reaching scaling limit instance number 31.
Satellite domain, many temporal numeric domain versions, select,
presentation here, single pure STRIPS version. Figure 9 (a) (b), observe that,
like Pipesworld Promela, unlike Airport PSR, Satellite domain serial
plan graph provides much worse heuristic values (for sequential planning) parallel planning
graph (for parallel planning). instances solved optimal planners, parallel plan
length (serial parallel) plan graph length grow much, sequential plan length
does. Consequently, sequentially optimal planners scale much worse parallel ones.
22. true runtime curves individual planners. fact, planners even disagree widely
instances solved easily take lot time.

487

fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UG

1000

12

10000

30

Optimal MakeSpan
PlanGraph
Best Parallel Runtime

Optimal NrActions
SerialPlanGraph
Best Sequential Runtime
11
1000

100

25
10

1

20
Nr. Steps

Runtime (sec.)

8

Nr. Steps

Runtime (sec.)

100
9

10

10
15

7
1
6

0.1

10
0.1
5

0.01

4
5

10

15

20

25

30

0.01

35

5
5

10

15

20

Nr. Instance

Nr. Instance

(a)

(b)
100

25

30

35

500
Best NrActions
RelaxedPlan
Best Runtime

450
400

10

300
1

250

Nr. Steps

Runtime (sec.)

350

200
150
0.1
100
50
0.01

0
5

10

15

20

25

30

35

Nr. Instance

(c)
Figure 9: Satellite domain. Plots (parallel) plan length, heuristic estimation, runtime,
(a) optimal parallel planners, (b) optimal sequential planners, (c) satisficing planners.
Figure 9 (a), also nicely see how, instances 8, 9, 10, parallel plan length
down-up movement (8, 6, 8) constant parallel plan graph length (4), resulting
movement pretty much shape logarithmic scale! best parallel runtime.
Figure 9 (c), observe that, like Airport unlike domains,
relaxed plans initial states almost length real plans (there actually
slight over-estimation time). seen earlier, c.f. Section 4.2, Hoffmann
(2005) shown that, Satellite, relaxed plan length is, fact, bound close real plan
length states (in contrast Airport, unrecognized dead ends possible principle).
Indeed, Satellite easy tackle almost satisficing planners IPC-4.
runtime shown Figure 9 (c) appears non-trivial, remember instances huge, see
particular number ground actions Figure 4 (b). instance 20, satisficing IPC-4
planners could solve instance within minute.
skip Settlers domain since relies almost exclusively numeric variables encode
domain semantics, makes rather incomparable domains. Figure 10 shows
data UMTS domain. temporal numeric versions, half feature
also time windows. consider versions without time windows; Figure 10 (a) (b) concern

488

fiE NGINEERING B ENCHMARKS

10000

720
Optimal MakeSpan
PlanGraph
Best Parallel Runtime



0.1

80
Best NrActions
RelaxedPlan
Best Runtime

700

1000

P LANNING

70

680
60

620
10

600

50

40

Nr. Steps

640

Runtime (sec.)

100

Temporal MakeSpan

Runtime (sec.)

660

30

580
20
1

560
10

540
0.1

520
5

10

15

20

25

30

35

40

45

0.01

50

0
5

10

15

20

Nr. Instance

25

30

35

40

45

50

Nr. Instance

(a)

(b)

1000

720
Optimal MakeSpan
PlanGraph
Best Parallel Runtime

1

90
Best NrActions
RelaxedPlan
Best Runtime

700

80

680

70

100
660

600

50
0.1
40

Nr. Steps

620

Runtime (sec.)

10

Temporal MakeSpan

Runtime (sec.)

60
640

30
580
1

20

560

10

540
0.1

520
5

10

15

20

25
30
Nr. Instance

35

40

45

0.01

50

0
5

10

15

20

25
30
Nr. Instance

35

40

45

50

(c)
(d)
Figure 10: UMTS domain. Plots (durational) plan length, heuristic estimation, runtime,
(a) optimal (b) satisficing planners plain temporal version, (c) optimal (d) satisficing planners temporal version flaw action.
plain domain version, Figure 10 (c) (d) flaw action. Let us first consider optimal
planners, left hand side overall figure. optimal planners could tackle
domain i.e., domains syntax TP4 HSPa (Haslum & Geffner, 2001).
makespan-minimizing planners, data sequentially optimal planners (which
wouldnt make lot sense temporal setting anyway). PlanGraph curves Figure 10
(a) (c) correspond makespan estimation delivered initial state TP4s temporal
numeric extension heuristic. effect heuristic quality runtime, observe
strong correlation. Figure 10 (a), instance 21 makespan estimate
close real makespan time, two actually coincide runtimes
good. Starting instance 22, real makespan makes sudden leap upwards
followed estimation, runtimes shoot upwards. phenomenon also clear
instances 18, 19, 20, makespan estimation exhibits good, bad, good pattern,
runtime same. Figure 10 (c), sort behavior observed,
meaning particular flaw action effect makespan estimation
TP4. fact, makespan estimation exactly instances solved
domain versions. contained implicitly latter sentence, flaw action affect runtime

489

fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UG

set solved instances. runtime flaw action consistently
factor 2 larger without flaw action. challenging instances planners fail
flaw action present. decrease performance presumably due larger state
space incurred flaw action.
Consider satisficing planners, Figure 10 (b) (d). first observe that, more,
facing individual characteristic behavior, domain challenge
satisficing planners. latter shows domain useful benchmark satisficing
planners; also shows heterogeneous benchmark set is: common
satisficing planners faster optimal ones except PSR domain
picture extreme UMTS. stated earlier, domain pure scheduling problem,
obviously satisficing planners provide runtime-efficient greedy solutions problem.23
Looking plots little detail, find Figure 10 (b) sequential plan length (the
plans found optimal) simple stepwise linear function instances, relaxed plan
length initial state coincides real plan length isnt surprise given
excellent runtimes satisficing planners, fact scheduling domain. (In
sequentialized schedule harmful delete effects occur.) picture changes lot Figure 10 (d).
real plan length stays basically (is increased constant 2), relaxed plan
length becomes lot shorter due flaw action. satisficing planners unaffected, largely
keeping excellent runtime behavior. Apparently, planners incorporate technique
recognizing uselessness flaw action (this done simple domain analysis
techniques), getting rid influence. suspicion confirmed fact one
satisficing planner get affected flaw action way one expect. CRIKEY,
heuristic search forward state space planner using relaxed plan heuristic, solves task within
70 seconds without flaw action, sometimes takes 1000 seconds flaw action.
Let us briefly summarize overall observations:
presented data, time performance planners correlates well
quality relevant heuristic function. notable exceptions rule far
observed data Fast Downward PSR large, relaxed plans
pretty much devoid information, SGPlan YAHSP (to extent also Fast
Downward) Pipesworld, relaxed plans provide poor estimates planners
experience (much more) serious difficulties.
Usually, known benchmarks general, satisficing planners several orders
magnitude faster optimal ones. Exceptions PSR groups perform
almost equally UMTS satisficing planners hardly need time all.
Usually, known benchmarks general, parallel plan graph length much
better estimator parallel plan length serial plan graph length sequential plan
length. exceptions Airport often huge difference
lengths two kinds plan graphs and, extent, PSR small
difference parallel sequential plan length big. Note none
domains purely sequential, i.e. parallelism possible them.
23. terms quality solutions found, satisficing planners also reasonably well. example, LPG-td,
minimizes makespan domain, finds, version optimized speed, plans take maximally
10% time optimal ones found TP4. version LPG-td optimized plan quality, goes
1%.

490

fiE NGINEERING B ENCHMARKS



P LANNING

Usually, known benchmarks general, considerable difference
length relaxed plan initial state, length real plan
initial state. Exceptions Airport, Satellite, UMTS, lengths
identical nearly so.
Usually, known benchmarks general, largest instances solved
within given particular time memory (30 minutes 1GB) plans around
hundred steps more. PSR exceptional Fast Downward planner able
find plan 35 (namely, 57) steps.
indicates diversity IPC-4 domains almost every one appears
least exceptions listed here. domains dont appear Promela
domains Pipesworld. sort exception itself, meaning domains contribute
typical benchmark behaviors overall set.
take existence mentioned distinguishing features evidence
IPC-4 domains indeed several novel aspects, besides oriented applications
structurally diverse. particular, behavior PSR domain stands one typically observes. Note that, typically easy construct artificial domains provoke
unusual behavior, domains oriented applications, exhibited behavior, particularly PSR domain, unusual, also relevant
concrete sense.
5.3 Fact Connectivity
conclude empirical analysis data aimed assessing sort connectivity
facts. fact p, measure number adders: actions p add list (in
ADL case, effect p adds list). gives indication branching factor
action choices comes fact. measure number requirers: actions
p precondition (in ADL case, effect p condition).
gives indication central fact task. given planning task, measure
parameters distribution adders(p) requirers(p), set facts p: minimum
(min), mean (mean), maximum (max), standard deviation (dev). Within domain versions,
plot data instance size (number).
data abstract allow deep conclusions reasons planner performance,
able highlight characteristic features domains. particular, see
abstract measurements behave characteristically different IPC-4 domains
IPC-3 domains. Figure 11 shows plots IPC-4 domains Airport, Pipesworld, Dining
Philosophers, Satellite. picture PSR relatively complicated shown separately
Figure 12. Settlers left exceptional. picture UMTS extremely simple,
explained text below.
Consider Figure 11 (a), (non-temporal) Airport domain. min curves shown
since constantly 0: is-pushing-back(airplane) never added since pushback requests (of
outbound traffic) modelled; occupied(segment) required negation. max
curves step functions since follow size underlying airports: is-moving(airplane)
many adders segments, since start-up-engine done segment; ispushing-back(airplane) required every action, leading overall similar form

491

fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UG

1000

10000
#Adders, max
#Adders, mean
#Adders, deviation
#Requirers, max
#Requirers, mean
#Requirers, deviation

100

1000

10

100

1

10

0.1

#Adders, max
#Adders, mean
#Adders, deviation
#Adders, min
#Requirers, max
#Requirers, mean
#Requirers, deviation
#Requirers, min

1
5

10

15

20

25

30

35

40

45

50

5

10

15

20

Nr. Instance

25

30

35

40

45

50

Nr. Instance

(a)

(b)

1000

1000
#Adders, max
#Adders, mean
#Adders, deviation
#Requirers, max
#Requirers, mean
#Requirers, deviation

#Adders, max
#Adders, mean
#Adders, deviation
#Requirers, max
#Requirers, mean
#Requirers, deviation

100

100

10

10

1

1
5

10

15

20

25
Nr. Instance

30

35

40

45

5

10

15

20
Nr. Instance

25

30

35

(c)
(d)
Figure 11: Distributions numbers actions adding fact, actions requiring fact,
selected versions IPC-4 domains: (a) Airport, (b) Pipesworld, (c) Dining
Philosophers, (d) Satellite.
max requirers curve. mean adders curve flattened facts ismoving(airplane) added certain places airport. mean requirers curve, interestingly, shows similar downwards step behavior numbers facts actions shown
Figure 4. reason lies not-occupied facts, exist every segment,
needed every action moving (any) airplane across segment. number facts
increases number airplanes. Since many facts, strong
influence mean. much correspondence runtime data,
trivial one tend grow instance size.
Data Pipesworld, tankage non-temporal, shown Figure 11 (b). Several observations
made: 1. max mean curves clearly follow scaling pattern, growing traffic
5 growing underlying networks. 2. min curves non-zero. 3. characteristic
difference curves instance 10, afterwards. 4. curves adders
requirers almost (but exactly) coincide. Apart 1, also present Airport data,
observations clearly distinguish Pipesworld domains. observation
2, sometimes larger instances min number adders drop 0. due
interactions complex networks, certain configurations inside pipes true initially

492

fiE NGINEERING B ENCHMARKS



P LANNING

re-achieved later interactions recognized reachability
pre-process made FF actions, c.f. explanation Section 5.1. Observation 3 due
large contrast smallest network larger ones: smallest network
unitary pipelines (containing single batch), others pipelines least length 2.
Observation 4 particularly odds domains, large differences
adders requirers. fact, measuring distribution difference adders
requirers, found numbers (not distribution parameters) extremely
close together: instance 50, max adders 1524 max requirers 1520, max
difference 29, mean 1.63 dev 5.31. Pipesworld tankage restrictions,
phenomenon somewhat less extreme still there. Another characteristic enormously
large max number adders requirers, order magnitude larger
domains. max adders requirers come do-normal facts, control status
individual pipelines, affected action moving combination batches
respective pipeline; facts depend single batches (not combinations them),
flattens mean curves two orders magnitude. Regarding runtime, mentioned
earlier, Pipesworld scaling pattern clear correlation runtime; neither
fact connectivity measure here.
Consider Promela domain Figure 11 (c), data shown Dining Philosophers derived
predicates. again, extreme characteristics domain recognizable first glance.
data Dining Philosophers without derived predicates identical, data Optical Telegraph differ numbers higher. min curves 0, adders constant,
requirers linear. exist facts without adders due oddity encoding,
certain start-up transitions put forks table first place; facts without requirers
blocked-philosopher, needed goal. number adders depend
instance size due static sort domain structure, size increases number
parallel processes (philosophers), form processes stays fixed, every process
interacts exactly two processes. number requirers linear (non-constant, particular) due technicality encoding, activating (requesting) performing
(executing) transition requires communication channels neutral state; respective
flags required transitions, number course grows size. facts
required locally, resulting much lower (easily two orders magnitude) mean. one
would expect domain simple scaling pattern, planner performance pretty much
function size.
Data Satellite (STRIPS version) shown Figure 11 (d). characteristic feature,
comparison domains, extremely smooth parallel close-together growth
curves. curve stands little max requirers; max adders due pointing(satellite, direction) facts added turning direction; max
requirers due power-on(instrument) facts, needed every take-image
instrument, done every combination direction image mode supported
instrument. Note that, contrast domains max curves two
orders magnitude higher mean, max requirers one order magnitude
curves, curves roughly order. min curves
shown since constantly 1 adders power-on(instrument) added
switch-on(instrument) constantly 0 requirers have-image(direction) needed

493

fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UG

goal. runtime performance IPC-4 planners scales relatively smoothly size
Satellite, like parameters do.
UMTS, parameters constants. another consequence aforementioned
scaling pattern, number specified applications instances,
changes (only) goal, specifying applications shall actually scheduled. Precisely, plain domain version, number adders 1 facts, nicely showing
scheduling-domain characteristic choice accomplish tasks,
accomplish them. another illustration satisficing planners find
domain trivial, whereas optimal planner like TP4 (Haslum & Geffner, 2001) spend long
time searching optimal schedule. number requirers minimum 0, maximum 2, mean
0.89, standard deviation 0.57. domain version flaw action, notable difference
max adders 2 due alternative provided flaw action (min 0,
mean 1.2, deviation 0.5). interesting note context that, mentioned above,
domain version satisficing planner, CRIKEY, experiences serious trouble.
10000

10000
max
mean
deviation

max
mean
deviation

1000
1000

#Adders

#Required

100
100

10

10
1

0.1

1
5

10

15

20

25

30

35

40

45

50

5

10

15

20

Nr. Instance

25

30

35

40

45

50

Nr. Instance

(a)

(b)

35

1000
max
mean
deviation

max
mean
deviation

30

25

#Required

#Adders

100
20

15

10
10

5

0

1
5

10

15

20

25
Nr. Instance

30

35

40

45

50

5

10

15

20

25
30
Nr. Instance

35

40

45

50

(c)
(d)
Figure 12: Distributions numbers actions adding fact, actions requiring fact,
PSR small large: (a) adders small, (b) requirers small, (c) adders large,
(d) requirers large.
Data PSR shown Figure 12. Here, show plots adders requirers separately
makes much readable. Since data contain particularly interesting
494

fiE NGINEERING B ENCHMARKS



P LANNING

phenomena, show two domain versions, small large. obvious feature
small, Figure 12 (a) (b), is, again, huge amount variance data. clearly
discernible peaks curves (instance nrs. 15, 25, 31, 40) coincide peaks size
measured numbers facts actions Figure 4. also note large
range values, spanning four orders magnitude, even though instances (except number
25) small comparison domains shown Figure 4. minimum numbers
adders requirers constantly 1: updated(breaker) added wait(breaker) action,
not-closed(breaker) needed one wants close it.24 Regarding maximum adders
requirers, instance 25, far highest (9400) total number actions, max adders
(9216) due goal-reached fact, i.e., 9216 disjuncts DNF goal formula;
max requirers (9251) due do-normal, flag needed every goal-reached action,
plus actions opening closing breakers. remark facts responsible
peaks curves, i.e., happens also instances 15, 31, 40.
highly characteristic PSR small max numbers adders requirers approach sometimes exceed two thirds total number actions. case
domain, even domain version PSR (see below). intuitive reason lies
one pre-compilation steps employed order able formulate reasonably
large PSR instances pure STRIPS: compilation step (Bertoli et al., 2002) removes network
reasoning (and it, need derived predicates) basically enumerating breaker configurations effects flow current network. result dense structure
end network directly affects every end, explaining high degree
fact connectivity, particular explaining extremely complex goal formulas four peak
cases mentioned above.
pre-compilation step also key understanding huge difference behavior small, large. latter shown Figure 12 (c) (d). There, max
adders curve small linear function note non-logarithmic scale axis spite
(mostly) much larger numbers actions. example, instance highest number (7498)
actions derivation rules number 20, max number adders 31, less half
percent total number actions. natural high-level domain encoding here,
flow current network modelled transitive closure derivation rules
propagate current based local status network. particular breaker
configurations effects flow current implicit structure network.
again, PSR large, min curves constantly 0 adders requirers; notaffected(breaker) negation derived predicate (needed precondition open close
actions), isnt added inverse rule, given meaning negation failure
semantics derived predicates; fed(line) required goal. mean dev
adders completely flattened numerous (5029 5237, instance 20) upstream(x,y)
facts, true currently path open side node x side node y, added
local derivation rule relies predicate neighbors y. Similarly
Satellite, max number requirers generally lot larger max number adders.
example, 542 vs. 31 instance 20, max requirers due fact closed(device)
required derivation rules talking pairs devices; instance 20, 7360 7498 actions
rules; 46 devices.
24. Sometimes 0 minimum requirers due artificial goal-reached fact, introduced get rid complex
goal formulas, c.f. Section 2.

495

fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UG

1000

1000
#Adders, max
#Adders, mean
#Adders, deviation
#Requirers, max
#Requirers, mean
#Requirers, deviation

#Adders, max
#Adders, mean
#Adders, deviation
#Requirers, max
#Requirers, mean
#Requirers, deviation

100

100

10

10

1

1
2

4

6

8

10

12

14

16

18

20

2

4

6

8

Nr. Instance

10

12

14

16

18

20

10
12
Nr. Instance

14

16

18

20

Nr. Instance

(a)

(b)

10000

1000
#Adders, max
#Adders, mean
#Adders, deviation
#Requirers, max
#Requirers, mean
#Requirers, deviation

#Adders, max
#Adders, mean
#Adders, deviation
#Requirers, max
#Requirers, mean
#Requirers, deviation

1000
100

100

10
10

1

1
2

4

6

8

10
12
Nr. Instance

14

16

18

20

2

4

6

8

(c)
(d)
Figure 13: Distributions numbers actions adding fact, actions requiring fact,
STRIPS versions IPC-3 domains except Freecell Satellite: (a) Depots, (b)
Driverlog, (c) Rovers, (d) Zenotravel.
sum sub-section, data are, generally, abstract really tightly interconnected
performance exhibited planners. hand, certain characteristics visible.
particularly: Pipesworld, numbers adders requirers almost identical.
Promela, adders constant requirers linear. Satellite, curves close
together. PSR small lot variance, max numbers adders requirers
approach sometimes exceed two thirds total number actions. contrast, PSR
large max adders decline less half percent total number actions. UMTS,
parameters constant. Except PSR UMTS, phenomena somewhat hard
interpret. nothing else, certainly show us domains rather different
characteristics. Interestingly, differences significant IPC-3 benchmarks shown
Figure 13. Clearly, behavior characteristically diverse seen
IPC-4 domains. four domains Figure 13, basically observe mostly parallel lines
pretty close together except max lines, order magnitude higher
others. striking feature zig-zag nature curves Depots. due
scaling pattern: smallest instances, number crates (blocks) grows continually
15 crates instance 6. Thereafter, come blocks 3 instances each, first 6

496

fiE NGINEERING B ENCHMARKS



P LANNING

crates, second 10 crates, third 15 crates (across blocks, instance size parameters
grow). means zig-zag shape curves corresponds exactly zig-zag shape
crate numbers.
Note behavior plots Figure 13 similar behavior plot Satellite
Figure 11 (d), particular first 20 instances. instances posed fully
automated planners IPC-3, also shown Figure 13. IPC-3 domain truly stands
terms behavior curves Freecell.25 There, observe phenomenon similar
Pipesworld Figure 11 (b), curves adders requirers almost coincide.
phenomenon little weaker Pipesworld: largest Freecell instance, number 20,
max (both) adders requirers 1638, max difference 102, mean
14.30 dev 24.86. comparison, largest Pipesworld instance, max adders 1524,
max requirers 1520, max difference 29, mean 1.63 dev 5.31.
sum overall empirical analysis, data certainly dont solve mystery
behind performance every planner every domain (and instance). do, however, provide
interesting insights instances scaled domains, certain subtleties
peculiarities encodings, standard heuristic methods, groups planners,
react them. observe large characteristic differences domains. sense
results nicely complement technical descriptions Appendix A, well known theoretical
results Section 4.

6. Conclusion
field research general reasoning mechanisms, AI planning, essential
useful benchmarks: benchmarks reflect possible applications developed technology,
help drive research new fruitful directions. development benchmark
domains instances IPC-4, authors invested significant effort creating set
useful benchmarks AI planning.
explained introduction, three main goals tried achieve 1. realism, 2.
structural diversity, 3. accessibility benchmarks. debatable extent goals
achieved. extent, inherent conflicting nature goals. Accessibility
benchmark formulation simple possible PDDL dialects obviously conflict
realism. Structural diversity also conflict realism since, time window available
create competition benchmark set, may (and been, case) large set
suitable applications choose from. One must make whats available. stressed
realism since lack realism traditionally considered one main weaknesses
AI Planning achieving structural diversity accessibility would, fact,
comparatively easy (see also below). said, adapt applications IPC
make many significant simplifications. Still, derived domains applications, one
expect capture important features even simplification; top that,
clear path towards realism.
believe domains constitute best possible compromise IPC-4. name
distinguishing features domain set:
25. somehow makes sense precisely domain stands out, also intuitively different
domains. notably, deciding plan existence Freecell NP-hard easy domains, c.f.
Section 4.1.

497

fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UG

1. Airport, Pipesworld, PSR, UMTS derived directly applications (Promela
special case since model checking instances could encode simplistic).
previously case Elevator domain (IPC-2) Rovers Satellite
domains (IPC-3).
2. complexity satisficing optimal planning STRIPS domain versions covers
entire range P, NP, PSPACE deciding (bounded) plan existence P PSR
PSPACE-complete Airport general Promela. aware previous
PSPACE-complete STRIPS benchmark; polynomial algorithm finding plans PSR
is, contrast STRIPS benchmarks algorithms, quite nontrivial.
3. Hoffmanns (2005) taxonomy domain classes different h+ topology, IPC-4
domains lie classes sparse coverage previous benchmarks. particular, none
new domains nearly simple topology proved Hoffmann
traditional benchmarks. taking account Pipesworld actions inverted
(not one but) two steps, domains lies different class Hoffmanns taxonomy,
covering classes (6) previous IPC benchmark set (3, 5, 4 IPC-1, IPC-2,
IPC-3, respectively). Dining Philosophers exceptional lies simple class
doesnt simple topology; Airport exceptional lies hard class
typically (in real-world instances) easy.
4. behavior different kinds planners IPC-4 shows lot characteristic
patterns individual domains. Airport, sheer size main obstacle. Pipesworld,
particularly tankage restrictions, known heuristic functions badly.
Promela domains, main obstacle is, lot cases, impossibility compiling
PDDL description fully grounded simpler representation. PSR, extremely
large amount variance, optimal planners perform well (or poorly) satisficing
planners. UMTS, satisficing planners need time all.
5. abstract level looks numbers actions adding/needing fact,
behavior domains characteristically diverse IPC-3 domains.
6. Last least, STRIPS versions domains preserve much original
domain structure previously case. IPC-2 STRIPS version Elevator
hardly elevator problem anymore, IPC-3 STRIPS versions Satellite Rovers
devoid interesting problem constraints. contrast, STRIPS versions
Airport Promela semantically identical ADL versions, PSR STRIPS
version, pre-compiled lot, still preserves much original difficulty domain
(judging, e.g., behavior IPC-4 planners it).
Feature 1 is, obviously, point realism. Features 2 5 points diverse structure; particularly Feature 4 shows domains pose different challenges (current) planning
technology. Feature 6 point realism combined accessibility. would like stress
accessibility respect really quite important. 19 planners entered IPC-4,
8 could handle (some) ADL features. compilation approach enabled us confront 11
planners reasonably realistic problems. said, certainly debatable role STRIPS
498

fiE NGINEERING B ENCHMARKS



P LANNING

plays play community. people may say many core algorithms,
e.g., planning graphs (Blum & Furst, 1997) relaxed plan heuristics (McDermott, 1999; Bonet
& Geffner, 2001; Hoffmann & Nebel, 2001), invented STRIPS. Others may say
focus STRIPS-like languages algorithms distracts us considering temporal numerical problems truly different nature. notwithstanding, STRIPS still widely
used language among research community. cannot ignored competition organizers.
pointed advantages benchmark set, also point
disadvantages. explained detail individual sections Appendix A, make
many simplifications order make applications fit use IPC-4. extent, whether
simplifications preserve original domain structure debatable matter. feel
Airport encoding close real physical thing. able represent
real optimization criterion bad, ameliorated fact that, 19 planners,
single one (LPG-td) could actually deal user-defined optimization criteria.26 Pipesworld,
simplifications severe. IPC-4 domain still resembles core difficulties,
reminiscent (complicated) toy example software could used control
real pipelines. Promela examples go show toy examples model checking area
better traditional toy examples planning. PSR, removing uncertainty
numerical optimization renders IPC-4 domain unsuitable practical use.
course, domain set exhaustive, meaning presumably numerous applications whose essential structure similar IPC-4 domains. examples
spring mind action choice autonomous robots, detecting security holes computer networks (Boddy, Gohde, Haigh, & Harp, 2005), online manufacturing (Ruml, Do, & Fromherz,
2005). structural diversity, would easy construct set artificial domains
explore possible extreme cases. domains would probably completely infeasible current planners, thus posing strong challenges. think of, example, Rubiks
Cube, Sokoban, Rintanens (2004) purely randomly generated instance distributions. again,
domain set would devoid realism. point preparation IPC-4,
considered introducing separate class domains, called Diverse Structure, would
contained domains sort. decided since competition event already
large without it. Also, felt applications already quite diverse structural
side. pointed above, several theoretical empirical phenomena suggest latter
indeed case.
work, experienced various successes failures accurately formulating
application domains PDDL. People asked us if, this, obtained picture
suitable PDDL is, current form, formulate applications, sorts domains
works well. answer is, dont feel like obtained many insights matters
particularly deep havent known before. lessons learned these. First
foremost, formulating application STRIPS takes huge amount engineering expertise unless
one drops problem constraints; simplifications unavoidable. Second, discrete
nature action instantiations previous IPC PDDL dialects seriously impedes formulation
domains continuous aspects. discretization must chosen, sometimes easy
(Airport) sometimes hard (Pipesworld) do. good way seems adopt
duration inequalities suggested Fox Long (2003). Third, community pay
26. good example case PDDL moving faster actual planning technology.

499

fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UG

attention lifted encodings, deal modern planning algorithms: one lesson
compilation activities grounding parameters often simply possible
(Promela, PSR). Since compiling away ADL constructs often feasible without grounding (c.f.
Section 2), also relevant ADL/STRIPS context. final lesson, (the AI
Planning community) still, mostly, far away as-is applicability planners real world.
right track.
conclude, spent significant time effort creating useful set planning benchmarks
IPC-4. hope become standard benchmarks coming years.

Acknowledgements. would like thank competitors detailed comments
bugs found domains, would like thank Malte Helmert various useful tools
helped remove bugs.
thank Malte Helmert providing us yet unpublished, time
writing results computational complexity (Helmert, 2005, 2006b). thank Patrik Haslum
providing us TP4 temporal numerical plan graph estimates makespan UMTS
domain. indebted anonymous reviewers, much David Smith Maria
Fox, whose detailed extensive comments contributed greatly development paper.
finally thank David Smith extensive advice language, including corrections even
acknowledgements.
Jorg Hoffmann thanks Wolfgang Hatzack support development Airport
domain benchmark instances.
Frederico dos Santos Liporace supported Conselho Nacional de Desenvolvimento Cientfico
e Tecnologico, Brazil. would like acknowledge support PhD supervisor, Ruy Milidiu, development Pipesworld application.
Sylvie Thiebaux thanks Piergiorgio Bertoli, Blai Bonet, John Slaney contributions
development PSR domain instances. also would like acknowledge
support National ICT Australia. NICTA funded Australian Governments backing
Australias Ability initiative, part Australian Research Council.

Appendix A. Detailed Domain Descriptions
provide detailed descriptions domains, alphabetical order. section (except
Satellite Settlers domains, adapted IPC-3) organized
sub-sections follows. first give outline application domain. explain
main adaptations made model application PDDL domain IPC-4, explain IPC-4
domain structure, i.e., domain versions formulations used IPC-4, explain
generated example instances IPC-4 test suites. Finally, discuss possible future
extensions.
A.1 Airport
contact person application domain, Wolfgang Hatzack, working
application area several years. domain adapted IPC-4 Jorg Hoffmann
Sebastian Trug.

500

fiE NGINEERING B ENCHMARKS



P LANNING

A.1.1 PPLICATION OMAIN
task control ground traffic airport. Timed travel routes must assigned
airplanes reach targets. inbound outbound traffic; former
airplanes must take (reach certain runway), latter airplanes
landed get parked (reach certain parking position). main problem constraint is,
course, ensure safety airplanes. means avoid collisions, also prevent
airplanes entering unsafe zones behind large airplanes engines running.
optimization criterion minimize summed travel time (on surface airport)
airplanes.27 usually standard routes, i.e., routes airplane outbound
certain park position area, inbound certain runway, must take. reason introducing
routes is, simply, sheer complexity managing situation otherwise, without significant
computer support (which yet available real airports). see whether
standard routes present makes big difference also computationally.
airplanes move airport infrastructure, consists runways, taxiways,
parking positions. runways taxiways sub-divided smaller segments. position
airplane given segment currently located in, plus direction
precise position within segment several airplanes segment time.
Airplanes generally divided three categories, light, medium, heavy, classify
according engine exhaust (jet blast). airplane moved either inbound out-bound. In-bound airplanes recently landed way runway
parking position, usually gate. Out-bound airplanes ready departure, meaning
way departure runway. Since airplanes cannot move backwards, need
pushed back gate onto taxiway, start engines. airports also
provide different park positions allow airplane start engines directly.
ensure safety, airplane must get close back another airplane whose engines
running. far safety distance depends category (jet blast) second
airplane.
ground controller planner communicate airplanes ways
shall take stop. guidance given purely reactively, pays base
decisions anticipating future. Otherwise may happen airplanes block
need time necessary reach destinations airport. objective is, said,
minimize overall summed traveling times airplanes.
instances domain, one considers traffic situation given point time,
time horizon of, say, one hour. new airplanes known land given time slots inside
time horizon, time slots respective runways considered blocked,
planner make sure runways free times. course, situation
changes continually (new planes moved plans cannot executed intended), continuous re-planning, i.e., consideration domain instance describing new traffic situation,
necessary. Solving instances optimally (the corresponding decision problem) PSPACE-complete
without standard routes (Helmert, 2006b) NP-complete routes standardized (Hatzack
& Nebel, 2001). latter case, pure scheduling problem. former case, compli27. criterion airport wants minimize, order maximize throughput. point view
airlines, would better minimize delay, e.g., minimizing summed squared delay airplanes.
two criteria may conflict. Neither two easily modelled PDDL2.2, see below.

501

fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UG

cated (highly unrealistic, course) airport topologies lead exponentially long solutions, c.f.
Section 4.1.
A.1.2 IPC-4 PDDL DAPTATION
PDDL encoding (as well example instance generation process, see below) based
software Wolfgang Hatzack, namely system called Astras: Airport Surface ground TRAffic
Simulator. software package originally designed training platform
airport controllers. Astras provides two-dimensional view airport, allowing user
control airplanes means point click. Astras also simulate traffic flow
airport course specified time window.
made three simplifications, one benign, airport model. benign
simplification: model park positions airplane start engines directly,
without pushed back taxiway first. difficult model park positions
PDDL, seldom occur reality relevant application. first
important simplification assume somewhat cruder notion airplane locatedness,
requiring single airplane located segment time. is, use
term segment meaning smallest indivisible unit space. minimize loss
precision, (some of) original segments sub-divided several new smaller segments.
safety distance behind back airplane whose engines running also measured
terms number segments. discretization makes us lose precision, believe
distort nature problem much: due amount expected conflicting
traffic different points airport (high near parking positions), relatively easy
choose discretization segments different length precise small enough
time.28 last simplification severe. give real optimization
criterion. say rather strong simplification below. use full standard
routes, thus allowing airplanes choice move. use standards
routes, particularly regions near runways large airports. one thing, served keep
large airports manageable PDDL encoding planners; another thing, seems good
compromise exploiting capabilities computers time keeping close
traditions airports. get back matter Section A.1.5.
full PDDL description domain encoding downloaded IPC-4 web page
http://ipc.icaps-conference.org/. Briefly, encoding works follows. available actions
pushback (move plane away backwards parking position), startup engines,
move segments, park (turning engines), takeoff (which amounts
removing plane airport). semantics actions encoded based
predicates defining current state airplane. point time, airplane either
moving, pushed, parked, airborne. airplane always occupies one segment and, engines
running, may block several segments depending size occupied segment
category airplane. action preconditions ensure blocked segments never
occupied another airplane. initial state, plane either parked, moving. parked
plane pushed back, starting engines, moving. moving airplane
28. need smallest indivisible units (of space, case) fundamental consequence discrete nature
PDDL2.2; said Section A.1.5.

502

fiE NGINEERING B ENCHMARKS



P LANNING

either move current segment neighboring segment, park parking position
take runway.
example, look PDDL encoding (non-durational) move action (one
preconditions used example Section 2 already):
(:action move
:parameters
(?a - airplane ?t - airplanetype ?d1 - direction ?s1 ?s2 - segment ?d2 - direction)
:precondition
(and (has-type ?a ?t) (is-moving ?a) (not (= ?s1 ?s2)) (facing ?a ?d1) (can-move ?s1 ?s2 ?d1)
(move-dir ?s1 ?s2 ?d2) (at-segment ?a ?s1)
(not (exists (?a1 - airplane) (and (not (= ?a1 ?a)) (blocked ?s2 ?a1))))
(forall (?s - segment) (imply (and (is-blocked ?s ?t ?s2 ?d2) (not (= ?s ?s1))) (not (occupied ?s)))))
:effect
(and (occupied ?s2) (blocked ?s2 ?a) (not (occupied ?s1)) (not (at-segment ?a ?s1)) (at-segment ?a ?s2)
(when (not (is-blocked ?s1 ?t ?s2 ?d2)) (not (blocked ?s1 ?a)))
(when (not (= ?d1 ?d2)) (and (not (facing ?a ?d1)) (facing ?a ?d2)))
(forall (?s - segment) (when (is-blocked ?s ?t ?s2 ?d2) (blocked ?s ?a)))
(forall (?s - segment) (when
(and (is-blocked ?s ?t ?s1 ?d1) (not (= ?s ?s2)) (not (is-blocked ?s ?t ?s2 ?d2)))
(not (blocked ?s ?a))))))

six parameters lot compared usual benchmarks cause
prohibitive explosion instantiations since lot restriction static predicates.
Airplane ?a moves; type (category) ?t; segment ?s1 facing direction ?d1,
?s2 facing direction ?d2 move. Direction simple concept
says end segment airplane facing. course, moves ?s1 ?d1
?s2 ?d2 possible specified static topology airport (can-move,
move-dir). first two complex preconditions says ?s2 must currently
blocked airplane ?a itself. second complex precondition makes sure
that, move, ?a block segment currently occupied (by another airplane,
necessarily): (is-blocked ?s ?t ?s2 ?d2) static predicate true iff ?s endangered
blocked plane type ?t ?s2w facing direction ?d2. effects selfexplanatory; simply update at, occupied, blocked information. effect
looks little complicated last one says segments blocked
move, longer blocked move, become un-blocked. Note conditions
conditional effects static, conditions disappear parameter instantiation chosen.
durational PDDL, actions take time according simple computations. time
taken move across segment depends, naturally, segment length speed.
assumed airplanes move speed regardless category. time taken start
engines proportional number engines. actions fixed duration.
planes known land near future, blocking runways, model
blocking time windows using timed initial literals, respectively compilation
artificial (temporal) PDDL constructs. timed literals simply instances usual blocked
predicate, becoming true respective time window starts, becoming false
ends.
able model real optimization criterion airport ground traffic control.
standard criterion PDDL minimize execution time, i.e., makespan, plan.
503

fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UG

encoding domain comes minimizing arrival time (meaning, arrival
destination airport) last airplane. real objective is, said above, minimize
overall summed travel time airplanes. appears good way modeling
criterion current PDDL. difficulty lies accessing waiting times planes, i.e.
times stay segment waiting plane pass.29
way (we could think of) get access waiting times, current PDDL,
introduce explicit waiting action. one must able tell planner, i.e., encode
action, long plane supposed wait. One option use duration inequalities
proposed Fox Long (2003). action imposes constraints duration,
planner can/has choose actual duration action, point used
plan, additional (rational-valued) parameter. potential disadvantage approach
choice waiting time introduces, principle, infinite branching factor
state space, may thus make problem much harder automated planners. Moreover, duration inequalities put use IPC-3, part PDDL2.1. using
duration inequalities, way encode requested waiting time action use
discretization time. One introduce new objects representing every considered time
interval, give waiting action parameter ranging objects. Apart loss
precision involved discretization, approach also likely cause huge performance
problems automated planners. alternative way out, considered introducing special
current-time variable PDDL2.2, returning time evaluation plan execution.
Using look clock, one could make plane record arrival time, thus formulate true optimization criterion without major changes domain structure. IPC-4
organizing committee decided introduction current-time variable seemed
problematic algorithmic point view (it implies commitment precise time points
planning time), didnt seem relevant anywhere except Airport.
all, IPC-4 PDDL encoding Airport domain realistic except optimization criterion, demands minimize maximal arrival time makespan instead summed
travel time. remains remark one (LPG-td) IPC-4 planners ignored
optimization criterion anyway. Also, minimizing latest arrival time appear useful (if
ideal) objective.
A.1.3 IPC-4 OMAIN TRUCTURE
Airport domain versions used IPC-4 non-temporal, temporal, temporal-timewindows,
temporal-timewindows-compiled. first versions is, name suggests, nondurational PDDL. second version, actions take time explained above. third fourth
versions also consider runways blocked future planes known land given time
windows. third version encodes time windows using timed initial literals, fourth
version uses literals compilation standard temporal PDDL constructs, c.f. Section 2.
domain versions, problem constraints modeled using ADL, i.e., complex preconditions conditional effects. compiled ADL encodings STRIPS domainspecific software implemented purpose. grounded operator
parameters, precisely, parameters except, action, one giving name
29. Modelling summed (squared) delay airplanes, optimization criterion airlines, would pose essentially
difficulty: also involves computing arrival time (in order compute delay).

504

fiE NGINEERING B ENCHMARKS

version
non-temporal
non-temporal
temporal
temporal
temporal-tw
temporal-tw
temporal-twc
temporal-twc

formulation
ADL
STRIPS
ADL
STRIPS
ADL
STRIPS
ADL
STRIPS



max-#op
5
1408
5
1408
5
1408
14
1429

P LANNING

max-#act
(1048) 989
(21120) 13100
(1408) 989
(21120) 13100
(995) 854
(22038) 13100
(911) 861
(21141) 13121

Table 1: Overview different domain versions formulations Airport. Abbreviations used: temporal-tw temporal-timewindows, temporal-twc temporaltimewindows-compiled; max-#op maximum number (parameterized) PDDL
operators instance, max-#act maximum number ground actions
instance. ADL formulations, set ground actions could generated
largest instances; data shown largest instances could handled. Data
parentheses collected FFs reachability pre-process (see text).
affected individual airplane. parameters fixed, formulas conditional
effects simplified usual STRIPS constructs. Airport domain version contains
original ADL formulation, well compilation STRIPS. result grounding
process depends specific airport considered instance, set airplanes
travelling. So, STRIPS formulations, instance individual domain file
(the applies STRIPS compilations domains described later).
domain versions, well blow-up incurred compilation, overviewed
Table 1.30 numbers shown table indicate numbers PDDL operators, numbers
grounded actions. domain version/formulation, maximum number
instance shown. Note that, ADL formulations except temporal-timewindows-compiled,
single domain file number operators identical instances.
STRIPS formulations, number operators high because, explained, operator
parameters grounded. difference number ground actions STRIPS
ADL formulations because, automated software, able generate
ground actions larger ADL instances; data shown largest instances
could handle. numbers shown parentheses refer situation FFs reachability
pre-process; said before, builds relaxed planning graph initial state, removes
actions appear graph. difference numbers inside outside
parentheses indicates much simple pre-process helps. see helps quite lot
here, pruning almost half actions (which would never become applicable, forward search
least, blow representation regardless algorithm used).
30. instantiation process is, course, planner-dependent. Similarly Section 5, data based
FFs pre-processor. extended pre-processor (precisely, one Metric-FF (Hoffmann, 2003)) deal
temporal constructs.

505

fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UG

A.1.4 IPC-4 E XAMPLE NSTANCES
Airport example instances generated Sebastian Trug, implementation based
aforementioned airport simulation tool Astras. Five scaling airport topologies designed,
used basis instance generation. airports named Minimal, Mintoy,
Toy, Half-MUC, MUC. smallest airports smallest possible airport
Astras handle. two largest airports correspond one half Munich Airport (MUC),
full MUC airport. Figure 14 shows sketches Minimal airport, MUC
airport.

(a)

(b)
Figure 14: smallest (a), largest (b) IPC-4 Airport topologies. Park position
segments marked black (e.g., top part (a)), segments airplanes
takeoff marked white (e.g., left bottom side part (a)). lines
show road network airport. Topology (b) corresponds MUC airport.
Sebastian Trug implemented PDDL instance generation software inside Astras. simulation traffic flow airport, desired user software exports current traffic
situation various PDDL encodings explained above. simulator run different
airports, 50 scaling traffic situations exported (3 Minimal, 6 Mintoy, 11
Toy, 15 Half-MUC, 15 MUC). airport, instances scale terms
number travelling airplanes. largest instance features 15 planes moved destinations Munich airport, 10 planes landing future considered (in respective
domain versions). considered realistically sized traffic situation, airport.

506

fiE NGINEERING B ENCHMARKS



P LANNING

A.1.5 F UTURE W ORK
remains explore relax simplifications make. importantly,
overcome discrete model space (locatedness), model real optimization
criterion. difficulties are, partly described already, mostly due discrete
nature PDDL2.2, allow continuous choice instantiation action.
continuous choice would natural way saying far plane moving
long waiting. best way go direction is, probably, assume
duration inequalities proposed Fox Long (2003), together numeric variables
already contained PDDL2.2. easy modelling side. main problem
probably technology side, i.e., develop planners deal efficiently
continuous choice points. time IPC-4, said, continuous choice appeared much
demand planners.
One interesting topic future work arises one restricts airplanes completely standard
routes, i.e., leaves choice route take destination. said, first,
usually done real airports, sheer complexity managing situation otherwise,
without significant computer support (which yet available real airports). Second, IPC4 made limited use feature, retain flexibility could offered
automatized methods. Third, restriction turns PSPACE-complete ground traffic control
problem pure, NP-complete (Hatzack & Nebel, 2001), scheduling problem,
question planes move across segment. One could exploit create much
concise PDDL encoding. restricted problem comes resolving conflicts
arise two planes need cross airport segment. One could thus try encode
PDDL physical airport, conflicts possible solutions, ideally connection
real optimization criterion. expected planners much efficient
simpler concisely encoded problem.
A.2 Pipesworld
Frederico Liporace working application area several years; submitted paper
early domain version workshop competition ICAPS03. domain
adapted IPC-4 Frederico Liporace Jorg Hoffmann.
A.2.1 PPLICATION OMAIN
Pipelines play important role transportation Petroleum derivatives, since
effective way transport large volumes large distances. application domain
consider deals complex problems arise transporting oil derivative products
multi-commodity pipeline system. Note that, many planning benchmarks
dealing variants transportation problems, transporting oil derivatives pipeline
system different characteristic kind structure, since uses stationary carriers
whose cargo moves rather usual moving carriers stationary cargo. particular,
changing position one object directly results changing position several objects.
less reminiscent transportation domains complicated single-player games
Rubics Cube. lead several subtle phenomena. example, may happen solution
must reverse flow liquid pipeline segment several times. may also happen

507

fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UG

liquid must pumped ring pipeline segments cyclic fashion, achieve goal
(we see example later).
detail, application domain following. pipeline network graph operational areas connected pipeline segments. Operational areas may harbors, distribution centers
refineries. may connected one pipeline segments. oil derivatives
moved areas pipelines.
different types petroleum derivative products. area set tanks
define storage capacity product type. pipeline segment fixed volume
speed. volume depends segments length cross section diameter, speed
depends power pumps move contents. segment may uni-directional, i.e.
usable transportation one direction.
Pipeline segments always pressurized, is, must always completely filled
petroleum derivative products. that, way move pipeline segments contents
pumping amount product adjacent area segment. operation
results, assuming incompressible fluids, amount possibly different product
received area end segment.
pumping operations executed violate interface tanking
constraints. former, distinct products direct contact inside pipeline segment,
unavoidable loss due mixture interface them.
interface losses major concern pipeline operation, mixed products
simply discarded. must pass special treatment may involve sending back
refinery, may require use special tanks. severity interface losses depends
products interface inside pipeline segment. two product types known generate
high interface losses, pipeline plan must place adjacently segment. pair
product types said interface restriction.
Tanking constraints limits product amounts stored area, arising
respective tank capacities. constraints may effectively block pipeline segment,
room receiving area store product would leave segment process
pumping operation.
task application bring certain amounts products areas
required, i.e. one find plan pumping operations shifts positions product
amounts way goal specifications met. Sometimes deadline specifying
when, latest, product amount arrive destination area. may also case
area (typically, refinery) known produce given amount product given point
time, plan must make sure enough tank space available respective
area store new product amount. Similarly, area (typically, harbor distribution center)
may known consume given amount product given point time, thereby freeing
respective amount tank space.
A.2.2 IPC-4 PDDL DAPTATION
main adaptations made PDDL encoding unitary batches, split pumping operations,
personalized goals (see latter). term batch used oil pipeline
industry refer amount product must transported pipeline. Batches
thus associated single product predefined volume. Batches also indivisible.

508

fiE NGINEERING B ENCHMARKS



P LANNING

batch Bi pumped area Aj segment Sj,k , possible another batch
pumped Aj Sj,k Bi volume pumped. course, reality product
amount batch rational number. Using numeric encoding IPC-4 seemed completely
infeasible due complications modeling, expected capabilities participating
planners (see Section A.2.5). Instead, based encoding concept called
unitary batches. smallest considered indivisible portions product. pumping
operations refer unitary batches. pipeline segments volumes volumes tanks
also defined terms unitary batches. encoding real-world instance domain,
actual volume associated unitary batch choice variable. Smaller unitary batches decrease
rounding error PDDL encoding, cost larger encoding size. Note that, like
smallest units space Airport domain, discretization need due
non-continuous nature actions PDDL2.2; get back Section A.2.5.
modeled pipe segments directional fashion, i.e. default direction assigning
one area role, area role. pumping operations accordingly
distinguish push actions, move liquid respective segments default direction,
pop actions, move liquid opposite direction. simply technical device
enable encoding pipe segment contents predicates defining first last
batches segments (as well successor relation). push pop actions receive
(amongst things) arguments pipeline segment whose contents moved,
batch inserted segment. batch leaves segment depends
segment content action executed. Figure 15 shows example.



A1




























B1



A2




















































B2




B4












































B5

















































































































B6

A1

fi

fi


fi

fi


fi

fi


fi

fi


fi


fi


fi


fi


fi


fi


B1
fi


































B2

A2

fi
















B3

























B4























































































































B6





B5












B3



(b)

(a)
A1

!

!

!

!

!

!

!

!

!

!

"

"

"

"
#

"
#

B1
!

!

"
#

"
#

"
#

"
#

"
#

$

$
%

$
%

$

$
%

$
%

"
#

B2

"
#

A2
























B4




(

)


(

)


(

)

(
)

(
)

(
)

(
)

(
)

(
)

(
)

(
)

(
)

(
)

B6
(
)

(
)

(
)

(
)

(
)

'

(
)
&

(
)

'

(
)














































B5

&

&

'

'

'

&

&

&

&




$
%

&

B3

P1

'









ff



ff



ff



ff



ff



ff



ff



ff



ff



ff



ff



ff

$
%

P2
P3

(c)

Figure 15: small example. A1 plays role. fill pattern batch represents
product. (a) shows initial state, (b) shows state (a) push operation
B3 inserted segment, (c) shows state (b) pop operation B6
inserted segment.
Apart pipe segment batch inserted, push pop actions
take several parameters regarding, e.g., product types tank slots. particular, order
able update segment contents correctly, actions also need parameters giving
respective first, last, second last batch current contents segment. Thus
action four parameters ranging batches, yielding least n4 ground instances action
n (unitary) batches considered task. found made domain
509

fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UG

completely infeasible planning system grounded actions. Since many unitary
batches needed encode even relatively small Pipesworld examples, planners typically
died pre-processing phase already.31 avoided phenomenon splitting actions
two parts, start action taking batch parameters inserted batch first batch
pipe, end action taking batch parameters last second last batches
pipe. make concrete, split push action:
(:action PUSH-START
:parameters
(?pipe - pipe ?batch-atom-in - batch-atom ?from-area - area ?to-area - area
?first-batch-atom - batch-atom ?product-batch-atom-in - product
?product-first-batch - product)
:precondition
(and (normal ?pipe) (first ?first-batch-atom ?pipe) (connect ?from-area ?to-area ?pipe)
(on ?batch-atom-in ?from-area) (not-unitary ?pipe)
(is-product ?batch-atom-in ?product-batch-atom-in)
(is-product ?first-batch-atom ?product-first-batch)
(may-interface ?product-batch-atom-in ?product-first-batch))
:effect
(and (push-updating ?pipe) (not (normal ?pipe)) (first ?batch-atom-in ?pipe)
(not (first ?first-batch-atom ?pipe)) (follow ?first-batch-atom ?batch-atom-in)
(not (on ?batch-atom-in ?from-area))))
(:action PUSH-END
:parameters
(?pipe - pipe ?from-area - area ?to-area - area ?last-batch-atom - batch-atom
?next-last-batch-atom - batch-atom)
:precondition
(and (push-updating ?pipe) (last ?last-batch-atom ?pipe) (connect ?from-area ?to-area ?pipe)
(not-unitary ?pipe) (follow ?last-batch-atom ?next-last-batch-atom))
:effect
(and (not (push-updating ?pipe)) (normal ?pipe)
(not (follow ?last-batch-atom ?next-last-batch-atom))
(last ?next-last-batch-atom ?pipe) (not (last ?last-batch-atom ?pipe))
(on ?last-batch-atom ?to-area)))

constructs largely self-explanatory. static predicates used are: connect,
encoding topology network; is-product, encoding types liquid; may-interface,
encoding interface restrictions;32 not-unitary, saying whether pipe segment contains
one batch case push pop actions much simpler need
split (the first last elements pipe identical). predicates normal pushupdating ensure, obvious way, two parts split action used
intended. Finally, on, first, follow, last encode relevant batches are.
role clear, encodes locatedness areas. pipe contents,
modelled queue-like fashion, head first, tail last, successor function follow.
two parts push action update representation accordingly.
31. Matters may easier planning systems ground actions pre-process. didnt affect
design decision since large majority systems around time IPC-4 employ pre-process.
32. Note model interface loss products may interface.

510

fiE NGINEERING B ENCHMARKS



P LANNING

encode uni-directional pipe segments, i.e. segments push pop
actions available IPC-4 encodings. modeled tankage restrictions simple constructs involving tank slots located areas, slot capacity store one unitary batch
given product type is, push pop actions also specify tank slot
inserted/outgoing batch comes from/is inserted into. simple examples regarding interface
tankage restrictions, re-consider Figure 15. storage capacity P2 A2 equal zero,
transition state (a) state (b) becomes invalid. forbid interface P1
P3 , transition state (b) state (c) becomes invalid.
Pipe segment speed easily taken account (in durational PDDL). speed
segment s, simply assign push/pop actions regarding segment duration proportional 1s . (In IPC-4 encoding, start/end action takes exactly time,
non-split actions regarding length-1 segments take time 2s .)
reality, outlined goals refer amounts product requested certain
destination areas. encoding based batches, formulating goal would mean introduce potentially large disjunction conjunctive goals. one wants say, e.g., three unitary batches product P requested area A, needed goal condition disjunction
W
{b1 ,b2 ,b3 }B (atb1 A) (atb2 A) (atb3 A) respective conjunctive goal three-subsets
{b1 , b2 , b3 } batches B type P . avoid exponential blow-ups kind, encoding
used personalized goals instead, referring specific batches instead product amounts. Basically, comes pre-selecting one {b1 , b2 , b3 } subsets disjunction.33
One could also avoid blow-up replacing disjunction existential quantification;
step would undone compilation STRIPS anyway.
Deadlines arrival batches are, durational PDDL, easily modeled compilation
timed initial literals. goal deadline literal saying respective batch
still ejected end pipe segment. literal initially true, becomes false
time deadline. described above, application also pre-specified time
points area produces consumes given amount product. model
IPC-4 domain (see also Section A.2.5).
mentioned above, structure Pipesworld domain lead several subtle phenomena possible plans. example plans perform cyclic sequence pumping
operations depicted Figure 16. goal place B8 A3. shortest plan following (for readability, action parameters batches going pipes
shown): 0: PUSH S1,4 B8 B2, 1: POP S2,4 B2 B3, 2: POP S1,2 B3 B1, 3: PUSH S1,4 B1 B8, 4:
PUSH S4,3 B8 B7, 5: POP S2,3 B7 B4, 6: PUSH S2,4 B4 B2, 7: PUSH S4,3 B2 B8. Observe
plan contains two cyclic patterns. Action 0 inserts B8 S14. Actions 1, 2, 3 form
cycle {S2,4 , S1,2 , S1,4 } brings B8 A4. Thereafter, action 4 inserts B8 S43, actions
5, 6, 7 form another cycle {S2,3 , S2,4 , S4,3 } bringing B8 goal position A3.34
33. Note bad choice {b1 , b2 , b3 } make task harder solve. are, however, currently investigating computational complexity different variants Pipesworld, preliminary results suggest
allowing/disallowing personalized goals affect complexity.
34. Note need cyclic patterns oddity introduced encoding. something may (but
probably likely to) happen reality: like example, becomes necessary isnt enough liquid
origin area (here, A1 A4) push needed amount liquid (here, B8) destination.

511

fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UG

A2
S1,2
B1
A1

B4
S2,4 B3

B8

S2,3
B5
B6

B2
S1,4
A4

B7

A3

S4,3

Figure 16: example cycling required achieve goal (place B8 A3). Pipe segment
Si, j directed Ai Aj.
version
notankage-nontemporal
notankage-temporal
notankage-temporal-d
notankage-temporal-dc
tankage-nontemporal
tankage-temporal

formulation
STRIPS
STRIPS
STRIPS
STRIPS
STRIPS
STRIPS

max-#op
6
6
6
9
6
6

max-#act
(14800) 13696
(14800) 13696
(8172) 7740
(8175) 7742
(107120) 101192
(107120) 101192

Table 2: Overview different domain versions Pipesworld. Abbreviations used:
temporal-d temporal-deadlines, temporal-dc deadlines-compiled; max-#op
maximum number (parameterized) PDDL operators instance, max-#act
maximum number ground actions instance. Data parentheses collected FFs reachability pre-process (see text).

A.2.3 IPC-4 OMAIN TRUCTURE
Pipesworld domain versions used IPC-4 notankage-nontemporal, tankage-nontemporal,
notankage-temporal, tankage-temporal, notankage-temporal-deadlines, notankage-temporaldeadlines-compiled. versions include interface restrictions. versions tankage
name include tankage restrictions. versions temporal name, actions take
different amounts time depending pipeline segment moved, explained
above. versions deadlines name include deadlines arrival goal
batches. One versions models deadlines using timed initial literals, version
(naturally, compiled name) literals compiled artificial (temporal) PDDL
constructs. None encodings uses ADL constructs, version one
(STRIPS) formulation.
domain versions numbers ground actions overviewed Table 2. before,
data measured using (a temporal extension of) FFs pre-processor. numbers shown

512

fiE NGINEERING B ENCHMARKS



P LANNING

parentheses refer situation pre-processors reachability pre-process,
builds relaxed planning graph initial state removes actions appear
graph. observe numbers ground actions low domain versions
deadlines, extremely high versions tankage restrictions. former simply
because, due complicated generation process (explained next sub-section), examples
deadlines generated smaller size. latter high numbers actions
presence tankage restriction due additional blow-up incurred choice tank
slots draw/in put batches. note effect reachability
pruning relatively moderate, particular much lower than, e.g., Airport, c.f. Section A.1.3.
A.2.4 IPC-4 E XAMPLE NSTANCES
Pipesworld example instances generated Frederico Liporace, process going
random generators XML files PDDL files.35 Five scaling network topologies designed
used basis instance generation. Figure 17 shows network topologies, well
real-world network topology comparison. one see, largest network topology
used IPC-4 quite yet ballpark real network; neither trivially
small comparison. volumes pipeline segments connect areas realworld example necessarily segments may different cross section
diameters.
domain versions without tankage restrictions deadlines, network
topologies 10 scaling random instances generated. Within network, instances scaled
terms total number batches number batches goal location.
instances featuring tankage restrictions deadlines, generation process complicated
wanted make sure obtain solvable instances. tankage restriction examples, ran Mips (Edelkamp, 2003b) respective notankage instances, incrementally
growing tankage.36 chose instance random point first instance solved
Mips, maximum needed tankage (enough tankage area accommodate instance
batches). instances could solved Mips even given several days runtime,
inserted maximum tankage. deadline examples, ran Mips
corresponding instances without deadlines, arranged deadline goal batch random point interval arrival time batch Mipss plan, end time
Mipss plan. instances solved Mips left out.
A.2.5 C URRENT



F UTURE W ORK

ongoing work developing Pipesworld specific solver, named Plumber (Milidiu & dos
Santos Liporace, 2004a; Milidiu & dos Santos Liporace, 2004b). Plumber incorporates pipeline
simulator, domain specific heuristics, procedures reducing branching factor symmetry
elimination. also lets user choose different search strategies, enforced hill
climbing (Hoffmann & Nebel, 2001) learning real time A*(Korf, 1990). Currently
extended support temporal planning well.
35. XML file mapped different PDDL files depending kind encoding used; lot
trial error came final IPC-4 encoding.
36. Mips convenient choice since one planners, also deal temporal constructs.

513

fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UG

A1

A1

S1
,2

1

1

,3
S1

S1
,2

,3
S1

1

A3
A2

Network 1

2

A2

Network 2

A3

2

1

,3
S1

S1
,2

A1

S3,4
A3

A2

A4
Network 3

1

46

233

A1

1

425

2

S2,3

94

S3,4
A3

A2

53

138

BA1

,2

2

S1

UT13

RC5

12

2

1

35
13

,3

S1

TB12

4

S3,4
A4

10

Network 5
BA1

3

SZ11

57

A5

A3

A2

RD6

215

41

GU3

47

S1,5
A1

375

30

Network 4

1

RV8

43

A4

3

S2,3

83

GA2

,3
S1

S1
,2

RP7

1

10
17
6

(a)

31

20

3
3

SB9

3

RB4

2

(b)

Figure 17: IPC-4 Pipesworld network topologies (a), real network topology (b).
segment volumes latter annotated 100m3 units.

availability solver enable extension Pipesworld benchmark, since
easier overcome aforementioned difficulties generating large feasible instances.
hope able generate feasible instances real-world pipeline topologies, like one shown
Figure 17.
addition generating larger instances, Pipesworld benchmark may extended many
ways make closer real application scenario. relevant possible extensions include:
Defining pipeline segments single flow direction, is, segments
push pop actions allowed. Note introduces dead ends/critical choices
problem.
Un-personalized goals. could accomplished, e.g., imposing desired tank volume goal products respective areas. planner also decide
batches used bring tank volume desired level.
Modeling production consumption products pre-specified points time, described above.

514

fiE NGINEERING B ENCHMARKS



P LANNING

Using rational numbers model tank capacities current volumes, instead encoding
based unitary tank slots. Apart precise model real world (when
combined rational-valued batch sizes, see below), encoding would avoid unnecessary symmetries currently arise availability several non-distinguishable
tank slots (in area, product).
important shortcoming encoding use unitary batches. would much
appropriate base encoding product amounts given real numbers. One problematic
aspect encoding would, naturally, demand continuous choice
much liquid pump pipeline. Like Airport (c.f. Section A.1.5), choice could
naturally modelled using Fox Longs (2003) duration inequalities, unclear
develop planners deal reasonably well. Unlike Airport, implementing
choice end difficulties modelling side. model continuous
contents pipeline? number distinct regions liquid pipeline grow arbitrarily
high, principle. One solution might fix upper bound, simply disallow pumping
operation would result many distinct regions. may bearable loss precision,
given upper bound high enough. even then, bound awkward correctly update
contents pipeline amount x product pushed in: number different
products leaving pipe depends x. option may use complicated construct
conditional effects.
all, impression pipeline scheduling wont realistically modelled PDDL,
successfully solved planners, unless one introduces language data structure
suitable modelling contents pipes. Basically, would queues whose elements
annotated real numbers, whose basic operations usual push pop.
semantics pipes could explicitly computed inside planner, rather awkwardly
modelled using language constructs likely disturb general search mechanism.
A.3 Promela
domain created IPC-4 Stefan Edelkamp.
A.3.1 PPLICATION OMAIN
dropping Promela domain, briefly recall origin.
model checker SPIN (Holzmann, 2003) targets efficient software verification.
used trace logical design errors distributed systems design, operating systems, data
communications protocols, switching systems, concurrent algorithms, railway signaling protocols,
etc. tool checks logical consistency specification. SPIN reports deadlocks, unspecified receptions identifies race conditions, unwarranted assumptions relative speeds
processes. SPIN (starting Version 4) provides support use embedded C code
part model specifications. makes possible directly verify implementation level software
specifications, using SPIN driver logic engine verify high level temporal properties. SPIN works on-the-fly, means avoids need construct global state graph
prerequisite verification system properties. SPIN supports property checking linear temporal logic (LTL). LTL expresses state trajectory constraints, using temporal modalities like

515

fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UG

eventually, always, until37 . SPIN uses specific mechanisms specifying deadlock-freeness
safety properties, addition general LTL specifications. explore state space
ordinary nested search algorithm applied, depending whether state-based (a.k.a.
safety) property verified.
Promela SPINs input specification language. computational model asynchronous
communicating finite state machines. Promela allows define classes finite processes. special
process called init started first usually governs instantiation processes
system. possible process invoke another one, Promela allows modeling systems
dynamic creation state components. Communication Promela achieved via shared variables
message channels. Two kind message channels distinguished synchronous asynchronous communication. asynchronous channel basically FIFO queue, synchronous
channels imply rendezvous communication transition system involves two processes, one reading message channel another sending message it. Here,
consider asynchronous communication. body process class basically sequence
statements. statement interpreted transition process. Typical statements include assignments, numerical boolean expressions channel operations. Promela also allows
define atomic regions, whose sequence transitions treated atomic
action. interpreted weighted transitions whose costs number steps within
regions.38
IPC-4, used two example communication protocols formulated Promela: Dijkstras
Dining Philosophers problem, so-called Optical Telegraph protocol. briefly describe
latter protocol Section A.3.4. illustrate Promela language, let us consider Dining
Philosophers problem, n philosophers sit around table lunch. n plates,
one philosopher, n forks located left right plate. Since two
forks required eat spaghetti plates, philosopher eat time. Moreover,
communication except taking releasing forks allowed. task devise local
strategy philosopher lets philosophers eventually eat. simplest solution
access left fork followed right one, obvious problem. philosophers wait
second fork released possible progress; deadlock occurred.
difficult probably insightful derive bottom-up PDDL encoding Dining
Philosophers domain, using actions like eat, wait think. motivation, however, come
top-down encoding, starting Promela specification, automatically translating
PDDL.
deadlock model Dining Philosophers specified Promela shown Figure 18.
first lines define macros declare array N boolean variables represent
availability forks. following lines define behavior process type philosopher.
process iterates indefinitely endless loop (do) one unique entry marked symbol
::. Statements separated semicolon. first transition left!fork consists send
operation tag fork channel left, macro address forks current
process id pid. represents availability left fork philosopher. access transition left?fork executed reading tag fork channel left successful.
37. Note fragments LTL likely included PDDL language next international planning
competition (Gerevini & Long, 2005)
38. documentation Promela specification language found web site SPIN
http://netlib.bell-labs.com/netlib/spin/whatispin.html

516

fiE NGINEERING B ENCHMARKS



P LANNING

#define MAX PHILOSOPHERS N
mtype=fork
#define left forks[ pid]
#define right forks[( pid+1) % MAX PHILOSOPHERS]
chan forks[MAX PHILOSOPHERS] = [1] bit;
active [MAX PHILOSOPHERS] proctype philosopher()
{
left!fork;

::left?fork -> /* try get left fork */
right?fork; /* try get right fork */
/* eat... */
left!fork; right!fork /* release forks */
/* meditation... */
od
}
Figure 18: Promela specification model Dining Philosophers problem.
next transition right?fork similar first, last two ones sends tag fork back
channels left right.
A.3.2 IPC-4 PDDL DAPTATION
Model Checking Action Planning closely related, c.f. Section 3. model checker
searches counterexample form sequence transitions falsify given specification, planner searches sequence actions satisfies given goal. cases,
basic models (STRIPS Planning, Kripke structures), refer implicit graphs, nodes
annotated atomic propositions.
automatically generating PDDL model Promela syntax wrote compiler (Edelkamp, 2003a). restricted safety properties, especially deadlocks, assertions
global invariances difficult obtain. also concentrated models fixed number processes, since models communication protocols adhere restriction.39
compiler parse Promela code itself, takes input intermediate
representation problem generated SPIN validation tool40 . Figure 19 shows
textual automata representation philosopher process. case, value N
initialized 10 philosophers. file contains almost necessary information
39. dynamic creation processes PDDL would require language extension dynamic object creation.
extension dismissed since would involve heavy changes existing planner technology, relevance
(beyond Promela) unclear.
40. precisely, Promela input file taken, corresponding c-file generated, verifier compiled
executable run option -d.

517

fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UG

translation, number processes queues (i.e., message channels) well queue capacities read original Promela input file41 .
proctype philosopher
state 1 -(trans 3)-> state 6 line 11 => forks[ pid]!fork
state 6 -(trans 4)-> state 3 line 12 => forks[ pid]?fork
state 3 -(trans 5)-> state 4 line 14 => forks[(( pid+1)%10)]?fork
state 4 -(trans 3)-> state 5 line 16 => forks[ pid]!fork
state 5 -(tras 6)-> state 6 line 16 => forks[(( pid+1)%10)]!fork
Figure 19: Automata representation model 10 Dining Philosophers problem.
derive suitable PDDL encoding domain, process represented finite state
automata. Hence, propositional encoding simulates automaton. propositional atoms
true initial state one process running example problem shown Figure 20 (a)42 .

(is-a-process philosopher-0 philosopher)
(at-process philosopher-0 state-1)
(trans philosopher trans-3 state-1 state-6)
(trans philosopher trans-4 state-6 state-3)
(trans philosopher trans-5 state-3 state-4)
(trans philosopher trans-3 state-4 state-5)
(trans philosopher trans-6 state-5 state-6)

(is-a-queue forks-0 queue-1)
(queue-head forks-0 qs-0)
(queue-tail forks-0 qs-0)
(queue-next queue-1 qs-0 qs-0)
(queue-head-msg forks-0 empty)
(queue-size forks-0 zero)
(settled forks-0)

(a)

(b)

(writes philosopher-0 forks-0 trans-3) (trans-msg trans-3 fork)
(reads philosopher-0 forks-0 trans-4) (trans-msg trans-4 fork)
(reads philosopher-0 forks-1 trans-5) (trans-msg trans-5 fork)
(writes philosopher-0 forks-1 trans-6) (trans-msg trans-6 fork)
(c)
Figure 20: Propositional encoding one philosophers process (a), Propositional encoding
(single-cell) communication channel (b), Connecting communication local state transitions (c).
encoding communication structure represents channels graphs. PDDL encoding additionally exploits cyclic embedding queue array. formally, (FIFO)
channel Q represented structure GQ = (SQ , headQ , tailQ , Q , messQ ,contQ ), SQ
set queue cells, headQ , tailQ SQ head tail cells Q, messQ M|SQ |
41. avoid conflicts pre-compiler directives, first invoked c-compiler command line option -E,
executes pre-compiler.
42. use transition IDs, competition less accessible textual representation label chosen.

518

fiE NGINEERING B ENCHMARKS



P LANNING

vector messages Q (M set messages), contQ IR|SQ | vector
variable values Q Q : SQ SQ successor relation Q; SQ = s[1], . . . , s[k]
(s[i]) = s[(i + 1) mod k]. Explicitly modeling head tail positions queue trades
space time, since queue updates reduce constant time.
queue either empty (or full) pointers refer queue state. special case,
simple queues (as example) may consist one queue state, successor bucket
queue state 0 queue state 0 itself. case grounded propositional encoding includes
operators add delete lists share atom. make standard assumption
deletion done first. propositional atoms one queue adaption two queues
one process exemplified Figure 20 (b) (c).
Queue content, shared local variables modeled PDDL fluents. difference
local variables compared shared ones restricted visibility scope, local variables
prefixed process appear in. two benchmark protocols selected IPC-4 rely
pure message passing, numerical state variables involved. allowed us
supply propositional model problems.
(:action activate-trans
:parameters (?p - process ?pt - proctype ?t - transition ?s1 ?s2 - state)
:precondition (and (forall (?q - queue) (settled ?q)) (trans ?pt ?t ?s1 ?s2)
(is-a-process ?p ?pt) (at-process ?p ?s1) (pending ?p))
:effect (and (activate ?p ?t) (not (pending ?p)))))
Figure 21: Testing transition enabled activating it.
PDDL domain encoding uses seven operators, named activate-trans, queue-read,
queue-write, advance-queue-head, advance-empty-queue-tail, advance-non-empty-queue-tail,
process-trans. activation process shown Figure 21. see pending process
activated, queues settled transition matches current process state.
Briefly, operators encode protocol semantics follows. Operator activate-trans activates
transition process given type local state s1 s2 . operator sets predicate
activate. boolean flag precondition queue-read queue-write actions, set
propositions initialize reading/writing message. queue Q activated transition
querying message m, corresponds Promela expression Q?m, respectively Q!m.
read/write operation initialized, queue update operators must applied, i.e. advancequeue-head, advance-empty-queue-tail, advance-non-empty-queue-tail appropriate.
names indicate, operators respectively update head tail positions, needed
implement requested read/write operation. operators also set settled flag,
precondition every queue access action. Action process-trans applied. executes
transition local state s1 s2 , i.e. sets new local process state re-sets flags.
stored message match query, queue capacity either small
large, active local state transition block. active transitions process block,
process block. processes blocked, deadlock system. Detection
deadlocks implemented, different domain versions, either collection specifically
engineered actions or, elegantly, set derived predicates. cases one infer,
along lines argumentation outlined above, process/the entire system blocked.
519

fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UG

(:derived (blocked-trans ?p - process ?t - transition)
(exists (?q - queue)
(exists (?m - message)
(exists (?n - number)
(and (activate ?p ?t) (reads ?p ?q ?t) (settled ?q)
(trans-msg ?t ?m) (queue-size ?q ?n) (is-zero ?n))))))
(:derived (blocked ?p - process)
(exists (?s - state)
(exists (?pt - proctype)
(and (at-process ?p ?s) (is-a-process ?p ?pt)
(forall (?t - transition)
(or (blocked-trans ?p ?t) (forall (?s2 - state) (not (trans ?pt ?t ?s ?s2)))))))))

Figure 22: Derivation deadlock.
goal condition makes planners detect deadlocks protocols simply conjunction
atoms requiring processes blocked. example derivation rules derived
predicates, PDDL description derivation deadlock based blocked read accesses
shown Figure 22.
A.3.3 IPC-4 OMAIN TRUCTURE
two benchmark protocols IPC-4, created three different domain versions:
derivedpredicates, contains derived predicates infer deadlocks; plain, purely propositional specification specific actions applied establish deadlock (the later
actions basically Gazen Knoblock (1997) compilation derived predicates, c.f. Section 2); fluents alternative latter numerical state variables encodes size
queues messages used access contents. also made version called fluentsderivedpredicates, obvious combination, none IPC-4 competitors participated there,
omit herein. Within domain version, one formulation includes ADL
constructs quantification, disjunctive preconditions, negated preconditions. domain
versions without fluents, another formulation pure STRIPS, obtained respective ADL
encodings using adl2strips compiler (which handle numeric variables). Unfortunately,
larger problem instances lead STRIPS files big stored disk
(remember adl2strips grounds operator parameters). too-large instances were,
course, left respective test suites.
kept fluent-domains separated domain versions, rather domain version formulations,
order able compare propositional numerical exploration efficiencies, emphasize
fluent variables essential real-world model checking treated separately.
domain versions numbers operators ground actions overviewed Table 3.
Consider rows table top bottom. before, times parentheses values
FFs reachability pre-process, builds relaxed planning graph initial state
removes actions appear graph. STRIPS formulation fully grounded
using adl2strips program, derived FFs pre-processor (c.f. Section 2).

520

fiE NGINEERING B ENCHMARKS

version
optical-telegraph
optical-telegraph
optical-telegraph-dp
optical-telegraph-dp
optical-telegraph-fluents
philosophers
philosophers
philosophers-dp
philosophers-dp
philosophers-fluents

formulation
STRIPS
ADL
STRIPS DP
ADL DP
ADL
STRIPS
ADL
STRIPS DP
ADL DP
ADL



P LANNING

max-#op
3345
11
4014
11
11
840
11
1372
11
11

max-#act
(3345) 3345
(5070) 3345
(4014) 4014
(6084) 4014
(1337) 1169
(840) 840
(930) 840
(1372) 1372
(1519) 1372
(930) 930

Table 3: Overview different domain versions Promela. Abbreviations used: dp derived predicates; max-#op maximum number (parameterized) PDDL operators
instance, max-#act maximum number ground actions instance.
Data parentheses collected FFs reachability pre-process (see text). Derivation rules (ground derivation rules) counted operators (ground actions).

reason number operators number ground actions, FFs preprocess identical one run adl2strips effect. ADL formulation, see
reachability pruning reduces number actions factor almost 2, similar Airport
domain (c.f. Section A.1.3). picture next two domain versions, derived predicates,
similar. fact, since, consistently data Section 5, count derivation rules
actions, data identical. reason identical Table 3 that, using derived
predicates instead operators, FFs pre-processor scales larger instances (presumably, due
unimportant implementation detail). next domain version, formulated numeric
variables, FFs pre-processor scales even worse. However, even instances number
telegraphs, less ground actions before, due different encoding.
observations made Dining Philosophers exactly same, different numbers.
notable difference effect FFs reachability pruning weaker, yielding
slight decrease number actions versions without fluents, decrease
version fluents. Apparently, complex process structure Optical Telegraph leads
useless action instances.
A.3.4 IPC-4 E XAMPLE NSTANCES
said, selected two simple communication protocols benchmarks IPC-4: encoding Dining Philosopher problem described above, so-called Optical Telegraph
protocol (Holzmann, 1990).
Optical Telegraph protocol involves n pairs communicating processes, pair featuring process. pair go fairly long, heavily interactive,
sequence operations, implementing possible data exchange two stations.
data exchanged, various initializing steps must taken ensure processes working
synchronously. importantly, process writes token control channel (queue)
521

fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UG

beginning sequence, reads token end. causes deadlock
situation n control channels, accessed two processes.
every pair up/down processes occupied one control channel, overall system
blocked.
Dining Philosopher Optical Telegraph benchmark, instances scale via
single parameter, number philosophers number control stations, respectively.
scaled parameter 2 49 competition instances. Promela models benchmarks distributed together experimental model checking tool HSF-SPIN (Edelkamp,
Leue, & Lluch-Lafuente, 2004), extends SPIN heuristic search strategies improve error
detection.
A.3.5 F UTURE W ORK
general terms, see Promela planning benchmark another important step towards exploiting synergies research areas Planning Model Checking (Giunchiglia & Traverso,
1999). example, complement recent progress planning, explicit directed model checking
domain protocol validation (Edelkamp et al., 2004) symbolic directed model checking
domain hardware validation (Reffel & Edelkamp, 1999) led drastic improvements
state-of-the-art model checkers. work, e.g., (Yang & Dill, 1998; Bloem, Ravi,
& Somenzi, 2000), show model checking growing interest guided exploration,
mostly find errors faster blind state space enumeration algorithms. compilation
Promela domain model, alternative option applying heuristic search model checking
problems available. work needed understand planning heuristics work fail
model checking benchmarks.
strongly believe communities profit wide-spread availability techniques represent Model Checking problems PDDL. allows direct comparison exploration efficiencies. Based design Promela domain, suitable PDDL domain encodings
two expressive model checking input languages, Graph Transformation Systems (Edelkamp,
Jabbar, & Lluch-Lafuente, 2005) Petri Nets (Edelkamp & Jabbar, 2005), proposed.
encodings exploit expressive power PDDL well efficiency current planners.
result, state-of-the-art planners often faster compared model checkers benchmarks.
A.4 PSR
Sylvie Thiebaux others worked application domain. domain adapted
IPC-4 Sylvie Thiebaux Jorg Hoffmann.
A.4.1 PPLICATION OMAIN
Power Supply Restoration (PSR) domain consider derived application investigated Sylvie Thiebaux others (Thiebaux et al., 1996; Thiebaux & Cordier, 2001). PSR
deals reconfiguring faulty power distribution system resupply customers affected
faults. topic ongoing interest field power distribution.
detail, power distribution system (see Figure 23), viewed network electric lines connected switches fed via number power sources equipped
circuit-breakers. Switches circuit-breakers two possible positions, open closed,
522

fiE NGINEERING B ENCHMARKS



P LANNING

Figure 23: Sample power distribution system. Sources/circuit-breakers (e.g., CB4) represented
large squares, switches (e.g., SD3) small squares. Open switches (e.g., SD8)
white. area fed CB4 boxed. Gray dark used distinguish adjacent
areas fed different sources

connected two lines. restriction connectivity lines, extremities
also connected earth. circuit-breaker power source closed,
power flows source lines downstream, flow stopped open switch.
switches used appropriately configure network position initially set
line fed exactly one source.
Due bad weather conditions, permanent faults affect one lines network.
power source feeds faulty line, circuit-breaker fitted source opens protect
rest network overloads. leaves lines fed source without power.
problem consists planning sequence switching operations (opening closing switches
circuit-breakers) bringing network configuration maximum non-faulty lines
resupplied. instance, suppose line l20 becomes faulty. leads circuit-breaker
CB4 open boxed area without power. possible restoration plan would
following: open switches SD16 SD17 isolate faulty line, close SD15 source
CB7 resupply l19, finally re-close CB4 resupply others.
original PSR problem (Thiebaux & Cordier, 2001), maximal capacity sources
lines, well load requested customers taken account. plan must optimize
various numerical parameters breakdown costs, power margins, distance initial
configuration, subject capacity constraints. Furthermore, due fault sensors switches
unreliable, location faults current network configuration partially
observable. optimizing, leads complex tradeoff acting resupply lines
acting (intrusively) reduce uncertainty.

523

fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UG

A.4.2 IPC-4 PDDL DAPTATION
PDDL adaptation, benefited contributions Piergiorgio Bertoli, Blai Bonet, Alessandro Cimatti, John Slaney (Bertoli et al., 2002; Bonet & Thiebaux, 2003). Compared
original PSR domain described above, IPC-4 version underwent 3 major adaptations. Firstly,
IPC deals fully observable domains. Hence, partial observability PSR crucial
issue (Thiebaux et al., 1996; Bertoli et al., 2002; Bonet & Thiebaux, 2003), IPC version assumes
complete observability. Secondly, given difficulty encoding even basic problem, chose
ignore numerical optimization aspects PSR (capacities, power margins, . . . ). Thirdly,
IPC-4 version set pure goal-achievement problem, goal specifies set
lines must (re)-supplied. considered realistic goal asking planner supply
line be. However, unable compile goal STRIPS reasonable
space, opted simpler goal keep STRIPS formulation consistent possible
others.
highest level natural IPC-4 encoding PSR involves ADL constructs derived
predicates. Briefly, encoding works follows. PSR problem instances specify (1) network
topology, i.e., objects network connections (the lines, switching devices,
is, switches sources/circuit-breakers, two side constants side1 side2 denote
two connection points switching device, connection relations objects),
(2) initial configuration, i.e., initial positions (open/closed) switching devices, (3)
modes (faulty not) various lines. Among those, devices positions change.
number predicates derived basic ones. model propagation
current network view determining lines currently fed sources
affected fault, i.e. feed fault. closed-world assumption semantics PDDL2.2
derived predicates exactly needed elegantly encode relations. require
recursive traversal network paths naturally represented transitive closure
connection relation network. complex derived predicates, upstream,
requires four parameters, two which, however take two possible values, expresses
power flows one two sides device (side ?sx device ?x) one
sides another (side ?sy device ?y) happens side ?x opposite ?sx
directly connected ?sy (via line), exists closed device ?z one side
upstream ?sx side connected ?sy:
(:derived (upstream ?x - DEVICE ?sx - SIDE ?y - DEVICE ?sy - SIDE)
(and (closed ?x)
(or (and (= ?sx side1) (con ?x side2 ?y ?sy))
(and (= ?sx side2) (con ?x side1 ?y ?sy))
(exists (?z - DEVICE)
(and (closed ?z)
(or (and (con ?z side1 ?y ?sy) (upstream ?x ?sx ?z side2))
(and (con ?z side2 ?y ?sy) (upstream ?x ?sx ?z side1))))))))
upstream, relatively easy define predicates stating whether given line fed given
source affected.

524

fiE NGINEERING B ENCHMARKS



P LANNING

goal problem instance asks given lines fed sources unaffected.43
available actions closing opening switching device. effect simply set
device position requested. addition, action wait, models event circuitbreakers opening become affected. Wait applicable affected source exists,
applicable action case (the open close actions require precondition
source affected). This, together goal, ensures wait action applied
soon source affected. effect wait action open affected circuit-breakers.
Concretely, wait close actions follows (note open similar close earth
treated device whose position cannot changed actions):
(:action close
:parameters (?x - DEVICE)
:precondition (and (not (= ?x earth))
(not (closed ?x))
(forall (?b - DEVICE) (not (affected ?b))))
:effect (closed ?x))
(:action wait
:parameters ()
:precondition (exists (?b - DEVICE) (affected ?b))
:effect (forall (?b - DEVICE) (when (affected ?b) (not (closed ?b)))))
would possible encode opening affected breakers conditional effect
close action. However, would required complex derived predicates additional
device parameter conditional flavor, specifying, e.g., whether circuit-breaker would
affected close device.
A.4.3 IPC-4 OMAIN TRUCTURE
used four domain versions PSR IPC-4. Primarily, versions differ size
problem instances encoded. instance size determined languages able
formulate domain version. tried generate instances size appropriate evaluate
current planners, i.e, scaled instances push-over everybody impossibly hard
current automated planners, got intuitions running version FF enhanced
deal derived predicates. largest instances kind size one typically encounters
real world. instance generation process said Section A.4.4.
domain versions named 1. large, 2. middle, 3. middle-compiled, 4. small.
Version 1 single formulation adl-derivedpredicates. Version 2 formulations adlderivedpredicates, simpleadl-derivedpredicates, strips-derivedpredicates. Version 3
single formulation adl, version 4 single formulation strips. formulation names simply give language used. Version 1 contains largest instances, versions 2 3 contain (the
same) medium instances, version 4 contains smallest instances. adl-derivedpredicates
43. Note circuit-breaker affected source opens, source affected more, feed
line. Then, circuit-breaker closed again, source stay unaffected unless re-starts feeding faulty
line.

525

fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UG

version
large
middle
middle
middle
middle-compiled
small

formulation
ADL DP
ADL DP
SIMPLE-ADL DP
STRIPS DP
ADL
STRIPS

max-#op
7
7
3485
3560
5
9400

max-#act
(14038) 7498
(7055) 3302
(3485) 3485
(3560) 3560
(99) 71
(9400) 9400

Table 4: Overview different domain versions formulations PSR. Abbreviations used:
dp derived predicates; max-#op maximum number (parameterized) PDDL
operators instance, max-#act maximum number ground actions
instance. Data parentheses collected FFs reachability pre-process (see
text). Derivation rules (ground derivation rules) counted operators (ground actions).

formulation inspired Bonet Thiebaux (2003); makes use derived predicates explained above, ADL constructs derived predicate, action, goal definitions.
simpleadl-derivedpredicates strips-derivedpredicates formulations, ADL constructs (except
conditional effects simpleadl case) compiled away. resulting fully grounded encodings significantly larger original, hand length plans remains
nearly unaffected44 . pure adl formulation obtained adl-derivedpredicates formulation compiling derived predicates away, using method described Thiebaux et al. (2003,
2005). significant increase domain size, compilation method lead
increase plan length exponential arity derived predicates (no compilation
method avoid blow-up worst case, see Thiebaux et al., 2003, 2005). Indeed,
particular PSR example instances, observed considerable blow plan length. felt
blow much allow useful direct comparison data generated adlderivedpredicates opposed adl, separated adl formulation domain version
3 listed above.
strips domain formulation proved quite challenge. 20 schemes considered compiling derived predicates ADL constructs away led either completely
unmanageable domain descriptions completely unmanageable plans. problem feasible compilations derived predicates create new actions highly conditional effects,
compiling away impractical. therefore adopted different fully-grounded encoding inspired Bertoli et al. (2002). encoding generated description problem instance
tool performing reasoning power propagation. resulting tasks, effects
close actions directly specify circuit-breakers open result closing switch given
network configuration. derived predicates needed, consequently STRIPS encoding
much simpler refers positions devices lines, faults, connections. Nevertheless, still able formulate comparatively small instances STRIPS,
without prohibitive blow-up encoding size.
44. variation due fact existential precondition wait action causes compilation split
action many wait actions circuit-breakers

526

fiE NGINEERING B ENCHMARKS



P LANNING

domain versions, formulations, respective numbers operators ground actions, shown Figure 4. Data parentheses collected FFs reachability preprocess, building relaxed planning graph initial state removing actions
appear graph. encodings using ADL derived predicates, reduces number ground actions factor around 2; ADL, factor much smaller;
encodings, reduction obtained, simply due fact encodings obtained adl2strips, uses pruning process. interesting observations
made middle versions formulations. data shown correspond largest
instance FFs pre-processor could handle versions/formulations, enable direct comparison. see that, formulation SIMPLE-ADL STRIPS, need introduce
ground actions. also see that, curiously, compilation derived predicates (compilation
middle-compiled), number ground actions decreases dramatically. reason lies
data count ground derivation rules ground actions, subtleties compilation derived predicates. middle formulations, almost ground actions fact
ground derivation rules. compiled away middle-compiled following Thiebaux et al.
(2003, 2005), introducing single action one distinct conditional effect derivation rule, c.f. Section 2. means complexity thousands derivation rules
replaced complexity action thousands conditional effects.
A.4.4 IPC-4 E XAMPLE NSTANCES
Due contractual agreements, unable use real data competition. Instead, PSR
instances randomly generated using randomnet, special purpose tool implemented John
Slaney.
Power distribution networks often mesh-able structure exploited radially: path taken
power source forms tree whose nodes switches whose arcs electric
lines; terminal switches connect various trees together. Randomnet takes input number
sources, percentage faulty lines, range parameters controlling tree depth, branching,
tree adjacency, whose default values representative real networks. Randomnet randomly
generates network topology set faulty lines. turned various PDDL
encodings tool called net2pddl, implemented Piergiorgio Bertoli Sylvie Thiebaux.
net2pddl computes set lines supplied, makes goal.
instances generated make use randomnet default settings, two exceptions
create problems increasing difficulty. first maximal depth trees takes range
values twice default. larger value, harder problem. second
percentage faulty lines ranges 0.1 0.7. Problems middle range harder
average, bottom range realistic.
instance suite contains 50 instances. small instances feature 1 6 sources,
middle instances feature 10 sources, large instances feature 100 sources.
large instances size typical real-world instances, even larger. example
Figure 23 representative difficult instance middle set.
A.4.5 F UTURE W ORK
PSR around time benchmark planning uncertainty, expect
work done framework IPC-4 facilitate acceptance one standard

527

fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UG

benchmarks planning. end, developed PSR resource web page giving access
relevant papers, data, tools (net2pddl, randomnet, . . . ).45 One aspect future work
complete maintain website, making available number already existing tools,
SyDRe (Thiebaux et al., 1996), domain-specific system full PSR problem, Matt Grays
net2jpeg graphically displays networks generated randomnet.
Considering future IPCs, potential extending PDDL encoding take numerical optimization aspects benchmark account. PDDL-like encodings partially
observable version benchmark exist (Bonet & Thiebaux, 2003) ready used
future edition probabilistic part IPC.46
A.5 Satellite
Satellite domain introduced IPC-3 Long Fox (2003). motivated NASA
space application: number satellites take images number spatial phenomena,
obeying constraints data storage space fuel usage. IPC-3, 5 versions
domain, corresponding different levels language PDDL2.1: Strips, Numeric, SimpleTime
(action durations constants), Time (action durations expressions static variables),
Complex (durations numerics, i.e. union Numeric Time).
adaptation Satellite domain IPC-4 done Jorg Hoffmann. IPC-3 domain
versions example instances re-used, except SimpleTime like IPC-4 domains,
didnt want introduce extra version distinction difference constant
durations static durations. top IPC-3 versions, 4 new domain versions added.
idea make domain realistic additionally introducing time windows
sending image data earth, i.e. antennas visible satellites certain
periods time according Derek Long, lack time windows main shortcoming
IPC-3 domain.47
extended IPC-3 Time domain version two IPC-4 domain versions, Time-timewindows
Time-timewindows-compiled. extended IPC-3 Complex domain version two IPC-4
domain versions Complex-timewindows Complex-timewindows-compiled. cases, introduced new action sending data antenna. antenna receive data
single satellite time, antenna visible subsets satellites certain time
periods, sending image takes time proportional size image. time
windows modelled using timed initial literals, -compiled domain versions,
literals compiled artificial PDDL constructs. None domain versions uses ADL
constructs, versions single (STRIPS) formulation.
instances generated follows. objectives clearly demonstrate effect
additional time windows, produce solvable instances only. accomplish former,
re-used IPC-3 instances, difference between, e.g., Time Time-timewindows,
lies additional time window constructs. ensure solvability, implemented tool read
plans produced one IPC-3 participants, namely TLPlan, arranged time
windows input plan suitable solve enriched instance. important note
45. page available http://rsise.anu.edu.au/thiebaux/benchmarks/pds
46. probabilistic part IPC-4 feature partially observable domains.
47. learned meantime lack time windows gathering data also, even more,
essential: often, due occlusion objects due rotation earth, targets visible
restricted periods time. probably constitutes one important future directions domain.

528

fiE NGINEERING B ENCHMARKS



P LANNING

time windows arranged exactly meet times extracted IPC-3
plan. Rather, introduced one time window per 5 take-image actions, made antenna
visible time window respective 5 satellites, let image size
individual image random value within certain range time window 5 times
long sending time resulting maximum possible size.
course, generation process arranged rather arbitrarily, resulting instances
might long way away typical characteristics Satellite problem occurs
real world. isnt nice, best could without inside knowledge
application domain, advantage enriched instances solvable, directly
comparable IPC-3 ones.
new domain versions derived Complex, also introduced utilities time
window inside image sent earth. image, utility either
windows, decreases monotonically start time window, random within
certain interval. image put randomly one classes, optimization
requirement minimize linear combination makespan, fuel usage, summed negated
image utility.
A.6 Settlers
Settlers domain introduced IPC-3 Long Fox (2003). makes extensive use
numeric variables. variables carry domain semantics, building
infrastructure unsettled area, involving building housing, railway tracks, sawmills,
etc. domain included IPC-4 order pose challenge numeric planners
domains mostly make much use numeric variables, computing
(static) durations actions.48 used exact domain file example instances
IPC-3, except removed universally quantified preconditions improve accessibility
planners. quantifiers ranged domain constants could easily replaced
conjunctions atoms.
A.7 UMTS
Roman Englert working application area several years. domain adapted
IPC-4 Stefan Edelkamp Roman Englert.
A.7.1 PPLICATION OMAIN
Probably best known feature UMTS (Universal Mobile Telecommunication Standard)
higher bit rate (Holma & Toskala, 2000): packet-switched connections reach 2 mega
bit per second (Mbps) optimal case. Compared existing mobile networks, UMTS provides
new important feature, namely negotiation Quality Service (QoS) transfer
properties. attributes define characteristics transfer throughput, transfer delay, data error rate. UMTS bearers generic order provide good support
existing applications evolution new applications. Applications services divided
48. Note that, extent, numeric values abstracted away PDDL encoding,
mostly (in Airport Pipesworld, c.f. Sections A.1.5 A.2.5) order obtain discrete encoding suitable
PDDL2.2-style actions.

529

fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UG

Class

Constraints

Examples

Conversational
Preserve time
relation
information flow
stream.
Conversational
pattern (low delay)
Voice, video
telephony &
video games

Streaming
Preserve time
relation
information
entities
stream

Interactive
Request response pattern.
Preserve data
integrity

Background
Undefined
delay.
Preserve
data
integrity

Streaming
multimedia

Web browsing,
network games

Background
download
e-mails

Table 5: UMTS quality service classes characteristics.
four traffic classes QoS (TS23107, 2002; Holma & Toskala, 2000). traffic classes,
fundamental characteristics, examples applications summarized Table 5.
main distinguishing factor classes delay-sensitive traffic is:
conversational class delay sensitive (approximately 40 ms time preservation), background class defined maximum delay.
UMTS call set-up modularized using perspective Intelligent Software Agents
(Appleby & Steward, 1999; Busuioc, 1999), since agents logical units enable discrete
perspective continuous signaling process. call set-up partitioned following
modules executed sequential order (Englert, 2005):
TRM initial step initiation application mobile determination
required resources execution. resources mobile like display memory
checked Terminal Resource Management (TRM) allocated, possible. Otherwise,
execution aborted.
CT wireless connection radio network initiated via dedicated control channel
GSM (Holma & Toskala, 2000). case success, transmission Ready service
transferred via node B mobile order ensure Connection Timing (CT)
bearer service availability.
information mobile like location data handling capabilities sent application server Internet (cf. AEEI). transmission done comfortably
so-called service agent (Farjami, Gorg, & Bell, 2000) controlled Agent Management (AM) CND. advantage service agent is, case failure, e.g.,
network resources sufficiently available, agent negotiate terminals
agent another QoS class different quality parameters.
AEEM service agent required QoS class execution application
parameters mobile application sent mobiles Agent Execution Environment
Mobile (AEEM) application server Internet (cf. AEEI).
RRC Radio Resource Controller (RRC) provisions/allocates required QoS logical resources MAC level radio bearer (Holma & Toskala, 2000).
530

fiE NGINEERING B ENCHMARKS



P LANNING

RAB Then, bearer resources supplied physical level Radio Access Bearer
(RAB) CND call flow set-up mapping logical QoS parameters
physical QoS resources together.
AEEI Agent Execution Environment Internet (AEEI) establishes data transfer
core network PDN (e.g., Internet) sends service agent (controlled AM)
application PDN order ensure QoS application.
BS Finally, Bearer Service (BS) execution mobile application established
required radio bearer resources QoS. Messages sent modules TRM
AEEI start execution application.
modules executed sequential order set-up call execution mobile
applications. Two modules (AEEM AEEI) executed time windows order
ensure agents life network. However, two constraints added: First,
intra-application constraint, modules one application ordered. Second, interapplication constraint, modules names different applications cannot executed parallel order ensure required resources available.
A.7.2 IPC-4 PDDL DAPTATION
Besides action duration, domain encodes scheduling types resources49 , consuming
amount action initialization time releasing amount action ending time. Scheduling
types resources used planning benchmarks before, good news
temporal PDDL2.1 (Level 3) capable expressing them. fact used similar encoding
one found Job- Flow-Shop problems. one feature, actions defined
temporarily produce rather temporarily consume resources. current PDDL way
stating resource constraints explicitly, planners want exploit knowledge
look certain patterns increase/decrease effects recognize them. Additionally, resource
modeling UMTS adaptation constrained important parameters (in total 15).
real networks several hundred parameters applied.
UMTS, two subsequent actions check update value resources (e.g.,
has-mobile-cpu) starting (resp. ending) time points far start (resp. ending) events
separated time steps, minimum slack time required two dependent
events. modeling renewable resources construct invariant condition
action check, start event change. decided best choice
proper temporal action. Consequently, temporal actions require resources available
adding amount used.
Finally, time windows two agent-based modules defined using average execution times modules. average times estimated based signaling durations
UMTS network (Holma & Toskala, 2000).
Resources may renewable consumable: example renewable resource keyboard mobile. used input data several applications. Consumable resources
49. terminology resources planning scheduling varies. job-shop scheduling, machine resource,
planning machine would domain object. PDDL, renewable consumable resources
modeled using numerical fluents per se distinguished.

531

fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UG

mobile-cpu
d-available
e-balance
mobile-channels
-available
num-mobiles
num-calls
mobile-storage
logical-channels
cell-update
handover
active-set-up
ggsn-bitrate
max-no-pdp
max-no-apn

used x per cent per application
partition display, e.g., ticker chess
energy balance mobile accumulator
used data transfer
number mobiles tractable
node B
mobile network load node B
memory S(IM)AT card
number logical channels available CN
report UE location RNC
handover required get higher bit rate
update connection
capacity (kbit/s) GGSN PDN
max. no. packet data protocols per mobile
max. access point names (APN) per mobile

Table 6: Scheduling types resources UMTS call set-up.
released action execution. resources realized experiments summarized
Table 6 (see 3GPP, 2004 complete list resources UMTS call set-up).
PDDL representation planning domain based eight modules UMTS
call set-up. eight operators corresponding eight modules. Let us consider,
example, BS action, is, final action used establish predicate bs-ok.
defined follows:
(:durative-action BS
:parameters
(?A-new - application ?M - mobile ?L - list ?MS1 ?MS2 - message ?a - agent)
:duration
(= ?duration (time-bs ?A-new))
:condition
(and (at start (initiated ?A-new ?M))
(at start (aeei-ok ?A-new ?M ?L ?a))
(at start (qos-params ?A-new ?L))
(at start (message-trm ?M ?MS1))
(at start (message-aeei ?A-new ?MS2)))
:effect
(and (at end (iu-bearer ?A-new ?M ?L)) (at end (bs-ok ?A-new ?M ?L ?a)))))

action preconditions successful execution module AEEI call
set-up, satisfaction required QoS class parameters (denoted list L), transfered
messages set-up status application mobile PDN. resources already
allocated preceding modules. effect bearer network connection mobile
application set up.
532

fiE NGINEERING B ENCHMARKS



P LANNING

initiation application starts mobile TRM. Afterwards, CT
asked ready-for-service signal. core call set-up radio access bearer
procedure CND. Let us consider latter detail. first step logical resources
must allocated (RRC), e.g., required number channels must provided logical
level radio bearer later logical resources mapped physical channels.
PDDL RRC action looks follows:
(:durative-action RRC
:parameters
(?A-new - application ?M - mobile ?L - list ?a - agent)
:duration
(= ?duration (time-rrc ?A-new))
:condition
(and (at start (ct-ok ?A-new ?M ?L))
(at start (aeem-ok ?A-new ?M ?L ?a))
(at start (<= (has-logical-channels)
(- (max-logical-channels) (app-channels ?A-new ?m))))
(at start (<= (has-cell-update) (- (max-cell-update) 2)))
(at start (< (has-handover) (max-handover)))
(at start (< (has-active-set-up) (max-active-set-up))))
:effect
(and (at start (increase (has-logical-channels) (app-channels ?A-new ?M)))
(at end (decrease (has-logical-channels) (app-channels ?A-new ?M)))
(at start (increase (has-cell-update) 2))
(at end (decrease (has-cell-update) 2))
(at start (increase (has-handover) 1))
(at end (decrease (has-handover) 1))
(at start (increase (has-active-set-up) 1))
(at end (decrease (has-active-set-up) 1))
(at end (rrc-ok ?A-new ?M ?L ?a))))

requested QoS class available, fact rab-ok true service
agent must sent mobile order negotiate application user weaker QoS
requirements. case success predicate rab-ok true connection PDN must
checked. Finally, goal predicate BS fulfilled resources available.
A.7.3 IPC-4 OMAIN TRUCTURE
used IPC-4, UMTS domain six versions. first three are: temporal, domain
version timing constraints, temporal-timewindows, domain version PDDL2.2 timed
initial facts, temporal-timewindows-compiled, domain version PDDL2.1 wrapper encoding timed initial literals. second domain version set flaw-temporal, flaw-temporaltimewindows, flaw-temporal-timewindows-compiled, includes following flaw action:
(:durative-action FLAW
parameters
(?A-new - application ?M - mobile ?L - list ?a - agent)
:duration (= ?duration 4)
:condition
533

fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UG

version
temporal
temporal-tw
temporal-twc
flaw-temporal
flaw-temporal-tw
flaw-temporal-twc

formulation
STRIPS-TEMPORAL
STRIPS-TEMPORAL-TW
STRIPS-TEMPORAL
STRIPS-TEMPORAL
STRIPS-TEMPORAL-TW
STRIPS-TEMPORAL

max-#op
8
8
13
9
9
14

max-#act
(5120) 80
(5120) 80
(5125) 85
(5310) 90
(5310) 90
(5315) 95

Table 7: Overview different domain versions UMTS. Abbreviations used: temporaltw temporal-timewindows, temporal-twc temporal-timewindows-compiled;
max-#op maximum number (parameterized) PDDL operators instance,
max-#act maximum number ground actions instance. Data parentheses collected FFs reachability pre-process (see text).

(and (at start (initiated ?A-new ?M))
(at start (qos-params ?A-new ?L))
(at start (trm-ok ?A-new ?M ?L)))
:effect
(and (at end (rab-ok ?A-new ?M ?L ?a))
(at start (not (initiated ?A-new ?M)))))

action offers shortcut rab-ok predicate, used real solution
deletes initiated predicate. action used heuristic functions
based ignoring negative effects. sense, action encodes flaw may disturb
heuristic techniques used modern planners. determine action useful, negative interactions considered. idea flaw practically motivated order see
heuristic planners react it. standard form, domain big challenge
planners, seen Section 5. domain versions one formulation, namely stripsfluents-temporal, numerical fluents, - except typing - ADL constructs used.
instances, plan objective minimize makespan.
domain versions numbers operators ground actions overviewed Table 7.
many empirical data UMTS seen before, data quite exceptional,
time easy interpret. First, similar seen Section 5.3, data
actually constant across instances within domain version, due
fact instances scale specification applications need actually started.
Second, numbers operators actions differ versions without
time windows; increase somewhat, additional artificial actions, compile
timed initial literals away (c.f. Section 2); also increase somewhat, course, introduce
flaw action. Third, striking observation huge effect FFs reachability preprocessor, building relaxed planning graph initial state removing actions
appear graph. due technical subtleties encoding, restrictions
feasible action instantiations are, partly, implicit possible action sequences, rather
explicit static predicates.

534

fiE NGINEERING B ENCHMARKS



P LANNING

A.7.4 IPC-4 E XAMPLE NSTANCES
UMTS call set-up domain following challenges planning task (Englert & Cremers, 2001):
Real-time: plans execution mobile applications generated appropriate time?
Planning done maximum duration exceed UMTS call set-up
time.
Completeness: possible generate plan, i.e. planning result (optimal) plan
required applications minimizes waiting period applications started?
PDDL structure basic problem discrete UMTS call set-up (DUCS) domain
following:
(define (problem DUCS DOMAIN BASIC VERSION)
(:domain DUCS DOMAIN BASIC VERSION
(:objects MS1 MS2 - message
A1 A2 A3 A4 A5 A6 A7 A8 A9 A10 - application
M1 M2 M3 M4 M5 M6 M7 M8 M9 M10 - mobile
L1 L2 L3 L4 L5 L6 L7 L8 L9 L10 - list
ae - agent)
(:init (= (time-trm A1) 76) (= (time-ct A1) 48)
(= (time-am A1) 74) (= (time-aeem A1) 66)
(= (time-rrc A1) 202) (= (time-rab A1) 67)
(= (time-aeei A1) 36) (= (time-bs A1) 28)
[...]
(location M1) ;; types
(authentification M1)
[...]
(= (has-mobile-cpu) 0) ;; current status
[...] )
(:goal (and (bs-ok A1 M1 L1 ae) [...] )))

First PDDL description come objects applications mobiles.
come durations modules depending applications, e.g., module TRM requires
less time news ticker chess game, since latter requires terminal resources
ticker. current status resources initialized. Finally, goal defined:
bearer establishment execution start initiated mobile applications. total execution
time minimized.
IPC-4 time windows varied small perturbations order generate different
instances. perturbations motivated average execution times modules radio
network according load. Furthermore, number applications set varied 1
10. domains assume applications run one mobile terminal. However,
also distributed several mobile terminals. 50 different instances per domain version.
A.7.5 F UTURE W ORK
UMTS domain big challenge modern heuristic, i.e. HSP/FF/LPG-style, planners
planners satisficing (potentially return sub-optimal plans). objective UMTS
535

fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UG

minimize execution time, one ignores objective task trivializes.
optimal planners, UMTS realistic challenge. domain already relatively realistically
modelled, except left-out additional constraints (many) less important resources.
remains seen if, introducing resources, planner (in particular optimal planner)
performance gets degraded. option case may introduce explicit language constructs
different types (renewable consumable) resources.
future following two challenges shall investigated. First, negotiation UMTS
Quality Service (QoS) parameters could considered. Assume video application mobile
terminal initiated, bearer resources sufficiently available. QoS
negotiated terminal bearer. leads planning negotiation
plan execution already initiated applications.
Second, approach optimization UMTS call set-up applied Wireless
LAN registration. challenge transfer QoS parameters, since current Wireless LAN
standard (802.11b) contain QoS. demerit solved applying additional
service level addresses QoS.

References
3GPP (2004). 3G Partnership Project, www.3gpp.org.
Appleby, S., & Steward, T. (1999). Mobile Software Agents Control Telecommunication
Networks, chap. 11 Hayzelden, A./Bigham, J. (eds.), Software Agents Future Telecommunication Systems. Springer.
Apt, K., Blair, H., & Walker, A. (1988). Towards theory declarative knowledge. Foundations
Deductive Databases Logic Programming, pp. 89148. Morgan Kaufmann.
Bacchus, F., & Kabanza, F. (2000). Using temporal logics express search control knowledge
planning. Artificial Intelligence, 116, 123191.
Bacchus, F. (2000). Subset PDDL AIPS2000 Planning Competition. AIPS-00 Planning Competition Comitee. Available http://www.cs.toronto.edu/aips2000/pddl-subset.ps.
Bacchus, F. (2001). AIPS00 planning competition. AI Magazine, 22(3), 4756.
Bertoli, P., Cimatti, A., Roveri, M., & Traverso, P. (2001). Planning nondeterministic domains
partial observability via symbolic model checking.. Nebel (Nebel, 2001).
Bertoli, P., Cimatti, A., Slaney, J., & Thiebaux, S. (2002). Solving power supply restoration problems planning via symbolic model checking. Proceedings 15th European
Conference Artificial Intelligence (ECAI-02), pp. 57680, Lyon, France. Wiley.
Biundo, S., Myers, K., & Rajan, K. (Eds.)., ICAPS-05 (2005). Proceedings 15th International Conference Automated Planning Scheduling (ICAPS-05), Monterey, CA, USA.
Morgan Kaufmann.
Bloem, R., Ravi, K., & Somenzi, F. (2000). Symbolic guided search CTL model checking.
Conference Design Automation (DAC), pp. 2934.
Blum, A. L., & Furst, M. L. (1997). Fast planning planning graph analysis. Artificial
Intelligence, 90(1-2), 279298.

536

fiE NGINEERING B ENCHMARKS



P LANNING

Boddy, M., Gohde, J., Haigh, T., & Harp, S. (2005). Course action generation cyber security
using classical planning.. Biundo et al. (Biundo, Myers, & Rajan, 2005), pp. 1221.
Bonet, B., & Geffner, H. (2001). Planning heuristic search. Artificial Intelligence, 129(12),
533.
Bonet, B., Loerincs, G., & Geffner, H. (1997). robust fast action selection mechanism
planning. Proceedings 14th National Conference American Association
Artificial Intelligence (AAAI-97), pp. 714719. MIT Press.
Bonet, B., & Thiebaux, S. (2003). GPT meets PSR. Giunchiglia, E., Muscettola, N., & Nau,
D. (Eds.), Proceedings 13th International Conference Automated Planning
Scheduling (ICAPS-03), pp. 102111, Trento, Italy. Morgan Kaufmann.
Busuioc, M. (1999). Distributed Intelligent Agents - Solution Management Complex
Telecommunications Services, chap. 4 Hayzelden, A./Bigham, J. (eds.), Software Agents
Future Telecommunication Systems. Springer.
Bylander, T. (1994). computational complexity propositional STRIPS planning. Artificial
Intelligence, 69(12), 165204.
Cesta, A., & Borrajo, D. (Eds.). (2001). Recent Advances AI Planning. 6th European Conference
Planning (ECP01), Toledo, Spain. Springer-Verlag.
Chen, Y., Hsu, C., & Wah, B. (2004). SGPlan: Subgoal partitioning resolution planning.
Edelkamp, S., Hoffmann, J., Littman, M., & Younes, H. (Eds.), Proceedings 4th
International Planning Competition, Whistler, BC, Canada. JPL.
Chien, S., Kambhampati, R., & Knoblock, C. (Eds.)., AIPS-00 (2000). Proceedings 5th
International Conference Artificial Intelligence Planning Systems (AIPS-00). AAAI Press,
Menlo Park.
Cimatti, A., Roveri, M., & Traverso, P. (1998). Automatic OBDD-based generation universal
plans non-deterministic domains. Proceedings 15th National Conference
American Association Artificial Intelligence (AAAI-98), pp. 875881, Madison, WI. MIT
Press.
Clarke, E. M., Grumberg, O., & Peled, D. A. (1999). Model Checking. MIT Press.
Dierks, H. (2005). Finding optimal plans domains restricted continuous effects uppaal cora. ICAPS Workshop Verification Validation Model-Based Planning
Scheduling Systems.
Edelkamp, S. (2003a). Promela planning. Workshop Model Checking Software (SPIN), Lecture Notes Computer Science, pp. 197212. Springer.
Edelkamp, S. (2003b). Taming numbers durations model checking integrated planning
system. Journal Artificial Intelligence Research, 20, 195238.
Edelkamp, S., & Jabbar, S. (2005). Action planning directed model checking Petri nets.
Electronic Notes Theoretical Computer Science, 149(2), 318.
Edelkamp, S., Jabbar, S., & Lluch-Lafuente, A. (2005). Action planning graph transition systems. ICAPS Workshop Verification Validation Model-Based Planning
Scheduling Systems, pp. 4857.
537

fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UG

Edelkamp, S., Leue, S., & Lluch-Lafuente, A. (2004). Directed explicit-state model checking
validation communication protocols. International Journal Software Tools Technology, 5, 247 267.
Englert, R. (2005). Planning optimize UMTS call set-up execution mobile applications. Int. Journal Applied Artificial Intelligence, 19(2), 99117.
Englert, R., & Cremers, A. B. (2001). Configuration Applications 3rd Generation Mobile
Communication. KI Workshop AI Planning, Scheduling, Configuration Design
(PUK). Vienna, Austria.
Farjami, P., Gorg, C., & Bell, F. (2000). Advanced service provisioning based mobile agents.
Computer Communications, 23, 754 760.
Fikes, R. E., & Nilsson, N. (1971). STRIPS: new approach application theorem proving
problem solving. Artificial Intelligence, 2, 189208.
Fourman, M. P. (2000). Propositional planning. AIPS Workshop Model-Theoretic Approaches
Planning.
Fox, M., Long, D., & Halsey, K. (2004). investigation expressive power PDDL2.1.
Saitta, L. (Ed.), Proceedings 16th European Conference Artificial Intelligence
(ECAI-04), Valencia, Spain. Wiley.
Fox, M., & Long, D. (1999). detection exploitation symmetry planning problems.
Pollack, M. (Ed.), Proceedings 16th International Joint Conference Artificial
Intelligence (IJCAI-99), pp. 956961, Stockholm, Sweden. Morgan Kaufmann.
Fox, M., & Long, D. (2003). PDDL2.1: extension PDDL expressing temporal planning
domains. Journal Artificial Intelligence Research, 20, 61124.
Frank, J., Cheeseman, P., & Stutz, J. (1997). gravity fails: Local search topology. Journal
Artificial Intelligence Research, 7, 249281.
Garagnani, M. (2000). correct algorithm efficient planning preprocessed domain axioms.
Research Development Intelligent Systems XVII. Springer-Verlag.
Gazen, B. C., & Knoblock, C. (1997). Combining expressiveness UCPOP efficiency
Graphplan.. Steel, & Alami (Steel & Alami, 1997), pp. 221233.
Gerevini, A., & Long, D. (2005). Plan Constraints Preferences. AIPS-06 Planning Competition Comitee. Available http://zeus.ing.unibs.it/ipc-5/pddl-ipc5.pdf.
Gerevini, A., Saetti, A., & Serina, I. (2006). approach temporal planning scheduling
domains predictable exogenous events. Journal Artificial Intelligence Research, 25,
187231.
Giunchiglia, F., & Traverso, P. (1999). Planning model checking. Biundo, S., & Fox, M.
(Eds.), Recent Advances AI Planning. 5th European Conference Planning (ECP99),
Lecture Notes Artificial Intelligence, pp. 119, Durham, UK. Springer-Verlag.
Haslum, P., & Geffner, H. (2001). Heuristic planning time resources.. Cesta, & Borrajo
(Cesta & Borrajo, 2001), pp. 121132.
Hatzack, W. (2002). Entwicklung und Auswertung von Algorithmen zur autonomen Verkehrskoordinierung und Konfliktauflsung Flughfen. Ph.D. thesis, University Freiburg, Freiburg,
Germany.
538

fiE NGINEERING B ENCHMARKS



P LANNING

Hatzack, W., & Nebel, B. (2001). operational traffic control problem: Computational complexity solutions.. Cesta, & Borrajo (Cesta & Borrajo, 2001), pp. 4960.
Helmert, M. (2003). Complexity results standard benchmark domains planning. Artificial
Intelligence, 143, 219262.
Helmert, M. (2004). planning heuristic based causal graph analysis.. Koenig et al. (Koenig,
Zilberstein, & Koehler, 2004), pp. 161170.
Helmert, M. (2005) Personal communication.
Helmert, M. (2006a). fast downward planning system. Journal Artificial Intelligence Research, 26. Accepted Publication.
Helmert, M. (2006b). New complexity results classical planning benchmarks. Long, D., &
Smith, S. (Eds.), Proceedings 16th International Conference Automated Planning
Scheduling (ICAPS-06), pp. 5261, English Lake District, UK. Morgan Kaufmann.
Hoffmann, J. (2001). Local search topology planning benchmarks: empirical analysis..
Nebel (Nebel, 2001), pp. 453458.
Hoffmann, J. (2002). Local search topology planning benchmarks: theoretical analysis.
Ghallab, M., Hertzberg, J., & Traverso, P. (Eds.), Proceedings 6th International Conference Artificial Intelligence Planning Scheduling (AIPS-02), pp. 92100, Toulouse,
France. Morgan Kaufmann.
Hoffmann, J. (2003). Metric-FF planning system: Translating ignoring delete lists numeric
state variables. Journal Artificial Intelligence Research, 20, 291341.
Hoffmann, J. (2005). ignoring delete lists works: Local search topology planning benchmarks. Journal Artificial Intelligence Research, 24, 685758.
Hoffmann, J., & Edelkamp, S. (2005). deterministic part IPC-4: overview. Journal
Artificial Intelligence Research, 24, 519579.
Hoffmann, J., & Nebel, B. (2001). FF planning system: Fast plan generation heuristic
search. Journal Artificial Intelligence Research, 14, 253302.
Holldobler, S., & Stor, H.-P. (2000). Solving entailment problem fluent calculus using
binary decision diagrams. ICAPS Workshop Model-Theoretic Approaches Planning.
Holma, H., & Toskala, A. (2000). WCDMA UMTS - Radio Access 3rd Generation Mobile
Communications. Wiley & Sons.
Holzmann, G. (2003). Spin Model Checker - Primer Reference Manual. Addison-Wesley.
Holzmann, G. J. (1990). Design Validation Computer Protocols. Prentice Hall.
Howe, A., & Dahlman, E. (2002). critical assessment benchmark comparison planning.
Journal Artificial Intelligence Research, 17, 133.
Kabanza, F., & Thiebaux, S. (2005). Search control planning temporally extended goals..
Biundo et al. (Biundo et al., 2005), pp. 130139.
Koehler, J., & Hoffmann, J. (2000). instantiation ADL operators involving arbitrary
first-order formulas. ECAI Workshop New Results Planning, Scheduling Design.

539

fiH OFFMANN , E DELKAMP, HI EBAUX , E NGLERT, L IPORACE & R UG

Koehler, J., Nebel, B., Hoffmann, J., & Dimopoulos, Y. (1997). Extending planning graphs
ADL subset.. Steel, & Alami (Steel & Alami, 1997), pp. 273285.
Koehler, J., & Schuster, K. (2000). Elevator control planning problem.. Chien et al. (Chien,
Kambhampati, & Knoblock, 2000), pp. 331338.
Koenig, S., Zilberstein, S., & Koehler, J. (Eds.)., ICAPS-04 (2004). Proceedings 14th International Conference Automated Planning Scheduling (ICAPS-04), Whistler, Canada.
Morgan Kaufmann.
Korf, R. E. (1990). Real-time heuristic search. Artificial Intelligence, 42, 189211.
Kvarnstrom, J., Doherty, P., & Haslum, P. (2000). Extending TALplanner concurrency
ressources. Horn, W. (Ed.), Proceedings 14th European Conference Artificial
Intelligence (ECAI-00), pp. 501505, Berlin, Germany. Wiley.
Lago, U. D., Pistore, M., & Traverso, P. (2002). Planning language extended goals.
Proceedings 18th National Conference American Association Artificial
Intelligence (AAAI-02), pp. 447454, Edmonton, AL. MIT Press.
Long, D., & Fox, M. (2000). Automatic synthesis use generic types planning.. Chien
et al. (Chien et al., 2000), pp. 196205.
Long, D., & Fox, M. (2003). 3rd international planning competition: Results analysis.
Journal Artificial Intelligence Research, 20, 159.
McDermott, D. (1996). heuristic estimator means-ends analysis planning. Proceedings
3rd International Conference Artificial Intelligence Planning Systems (AIPS-96),
pp. 142149. AAAI Press, Menlo Park.
McDermott, D. (1998). PDDL Planning Domain Definition Language. AIPS-98 Planning Competition Comitee. Available http://ls5-www.cs.uni-dortmund.de/ edelkamp/ipc4/DOCS/pddl.ps.gz.
McDermott, D. (2000). 1998 AI planning systems competition. AI Magazine, 21(2), 3555.
McDermott, D. V. (1999). Using regression-match graphs control search planning. Artificial
Intelligence, 109(1-2), 111159.
Milidiu, R. L., & dos Santos Liporace, F. (2004a). Plumber, pipeline transportation planner.
International Workshop Harbour Maritime Simulation (HMS), pp. 99106, Rio de
Janeiro, Brazil.
Milidiu, R. L., & dos Santos Liporace, F. (2004b). Pipesworld: Applying planning systems
pipeline transportation. Proceedings International Pipeline Conference (IPC), pp.
713719.
Nebel, B. (Ed.)., IJCAI-01 (2001). Proceedings 17th International Joint Conference Artificial Intelligence (IJCAI-01), Seattle, Washington, USA. Morgan Kaufmann.
Nebel, B. (2000). compilability expressive power propositional planning formalisms.
Journal Artificial Intelligence Research, 12, 271315.
Pednault, E. P. (1989). ADL: Exploring middle ground STRIPS situation
calculus. Brachman, R., Levesque, H. J., & Reiter, R. (Eds.), Principles Knowledge
Representation Reasoning: Proceedings 1st International Conference (KR-89), pp.
324331, Toronto, ON. Morgan Kaufmann.
540

fiE NGINEERING B ENCHMARKS



P LANNING

Reffel, F., & Edelkamp, S. (1999). Error detection directed symbolic model checking. World
Congress Formal Methods (FM), pp. 195211.
Rintanen, J. (2004). Phase transitions classical planning: experimental study.. Koenig et al.
(Koenig et al., 2004), pp. 101110.
Ruml, W., Do, M., & Fromherz, M. (2005). On-line planning scheduling high-speed manufacturing.. Biundo et al. (Biundo et al., 2005), pp. 3039.
Steel, S., & Alami, R. (Eds.). (1997). Recent Advances AI Planning. 4th European Conference
Planning (ECP97), Vol. 1348 Lecture Notes Artificial Intelligence, Toulouse, France.
Springer-Verlag.
Thiebaux, S., & Cordier, M.-O. (2001). Supply restoration power distribution systems
benchmark planning uncertainty.. Cesta, & Borrajo (Cesta & Borrajo, 2001), pp.
8595.
Thiebaux, S., Cordier, M.-O., Jehl, O., & Krivine, J.-P. (1996). Supply restoration power distribution systems case study integrating model-based diagnosis repair planning.
Horvitz, E., & Jensen, F. V. (Eds.), Proceedings 12th International Conference
Uncertainty AI (UAI-96), pp. 525532, Portland, Oregon, USA. Morgan Kaufmann.
Thiebaux, S., Hoffmann, J., & Nebel, B. (2003). defense PDDL axioms.. Gottlob, G. (Ed.),
Proceedings 18th International Joint Conference Artificial Intelligence (IJCAI-03),
pp. 961966, Acapulco, Mexico. Morgan Kaufmann.
Thiebaux, S., Hoffmann, J., & Nebel, B. (2005). defense PDDL axioms. Artificial Intelligence,
168(12), 3869.
TS23107 (2002). 3rd Generation Partnership Project: Technical Specification Group Service
System Aspects: QoS Concept Architecture (Release 5), TS 23.107, V5.3.0, 3GPP.
Vidal, V. (2004). lookahead strategy heuristic search planning.. Koenig et al. (Koenig et al.,
2004), pp. 150160.
Wah, B., & Chen, Y. (2004). Subgoal partitioning global search solving temporal planning
problems mixed space. International Journal Artificial Intelligence Tools, 13(4), 767
790.
Yang, C. H., & Dill, D. L. (1998). Validation guided search state space. Conference
Design Automation (DAC), pp. 599604.
Younes, H., Littman, M., Weissman, D., & Asmuth, J. (2005). first probabilistic track
international planning competition. Journal Artificial Intelligence Research, 24, 85188.

541

fiJournal Artificial Intelligence Research 26 (2006) 289-322

Submitted 10/04; published 07/06

Breaking Instance-Independent Symmetries
Exact Graph Coloring
Arathi Ramani
Igor L. Markov
Karem A. Sakallah

ramania@umich.edu
imarkov@eecs.umich.edu
karem@eecs.umich.edu

Department Electrical Engineering Computer Science
University Michigan, Ann Arbor, USA

Fadi A. Aloul

faloul@umich.edu

Department Computer Engineering
American University Sharjah, UAE

Abstract
Code optimization high level synthesis posed constraint satisfaction
optimization problems, graph coloring used register allocation. Graph coloring
also used model traditional CSPs relevant AI, planning, time-tabling
scheduling. Provably optimal solutions may desirable commercial defense applications. Additionally, applications register allocation code optimization,
naturally-occurring instances graph coloring often small solved optimally.
recent wave improvements algorithms Boolean satisfiability (SAT) 0-1 Integer Linear Programming (ILP) suggests generic problem-reduction methods, rather
problem-specific heuristics, (1) heuristics may upset new constraints, (2)
heuristics tend ignore structure, (3) many relevant problems provably inapproximable.
Problem reductions often lead highly symmetric SAT instances, symmetries
known slow SAT solvers. work, compare several avenues symmetry breaking, particular certain kinds symmetry present generated
instances. focus reducing CSPs SAT allows us leverage recent dramatic
improvement SAT solvers automatically benefit future progress.
use variety black-box SAT solvers without modifying source code
symmetry-breaking techniques static, i.e., detect symmetries add symmetry
breaking predicates (SBPs) pre-processing.
important result work among types instance-independent SBPs
studied combinations, simplest least complete constructions
effective. experiments also clearly indicate instance-independent symmetries
mostly processed together instance-specific symmetries rather
specification level, contrary suggested literature.

1. Introduction
Detecting using problem structure, symmetries, often useful
accelerating search solutions constraint satisfaction problems (CSPs).
particularly true algorithms perform exhaustive searches benefit pruning search tree. work conducts theoretical empirical study impact
breaking structural symmetries 0-1 ILP reductions exact graph coloring problem
c
2006
AI Access Foundation. rights reserved.

fiRamani, Aloul, Markov, & Sakallah

applications number fields. example, compiler design, many techniques code optimization high-level synthesis operate relatively objects
time. Graph coloring used register allocation program compilation (Chaitin,
Auslander, Chandra, Cocke, Hopkins, & Markstein, 1981) limited small numbers
registers embedded processors well number local variables virtual
registers. Graph coloring also relevant AI applications planning, scheduling,
map coloring. Recent work graph coloring AI included algorithms based
neural networks (Jagota, 1996), evolutionary algorithms (Galinier & Hao, 1999), scatter
search (J.-P. Hamiez, 2001) several approaches discussed Section 2.
many search procedures heuristic, work focuses exact graph coloring,
closely related several useful combinatorial problems maximal independent set vertex cover. seek provably optimal solutions may
desirable commercial defense applications competitive reasons, often
found. work focuses solving exact graph coloring reduction 0-1 ILP.
idea solving N P complete problems reduction well-known, rarely used
practice algorithms developed standard problems, SAT, may
competitive domain-specific techniques aware problem structure. However,
many applications imply problem-specific constraints non-trivial objective functions.
extensions may upset heuristics standard problems. Heuristics, particularly
based local search, often fail use structure problem instances (Prestwich, 2002)
inefficient used problem reductions. contrast, exact solvers based
branch-and-bound back-tracking tend adapt new constraints applied
problem reduction. growing literature handling structure optimal
solvers (Aloul, Ramani, Markov, & Sakallah, 2003; Crawford, Ginsberg, Luks, & Roy, 1996;
Huang & Darwiche, 2003), work falls category well.
NP-spec project (Cadoli, Palopoli, Schaerf, & Vasileet, 1999) offers framework
formulating wide range combinatorial problems automatically reducing
instances instances Boolean satisfiability. approach attractive circumvents problem-specific solvers leverages recent breakthroughs Boolean satisfiability
(Moskewicz, Madigan, Zhao, Zhang, & Malik, 2001). However, approach remains unexplored practice, possibly efficiency problem-solving may reduced
domain-specific structure lost problem reductions. drawback addressed
recent work detection structure, particularly symmetry, SAT 0-1 ILP
instances order accelerate exact solvers (Crawford et al., 1996; Aloul et al., 2003;
Aloul, Ramani, Markov, & Sakallah, 2004). papers, symmetries SAT/0-1
ILP instance detected reduction graph automorphism, i.e. formula represented graph automorphism problem graph solved using graph
automorphism software packages (McKay, 1990; Darga, Liffiton, Sakallah, & Markov, 2004).
recently, type symmetry detection frequently inefficient solving
automorphism problem large graphs time-consuming. However, recent automorphism software (Darga et al., 2004) removed bottleneck large
extent. Moreover, adding simple symmetry breaking predicates new constraints significantly speeds exact SAT solvers (Aloul et al., 2003). work viewed case
study symmetry breaking problem reductions, focus graph coloring
variants reduced Boolean satisfiability 0-1 ILP. main goals (i)
290

fiBreaking Instance-Independent Symmetries Exact Graph Coloring

accelerate optimal solving graph coloring instances, (ii) compare different strategies
breaking instance-independent symmetries. two distinct sources symmetries
graph-coloring instances: (i) colors arbitrarily permuted (instance-independent
symmetries), (ii) graphs may invariant certain permutations vertices
(instance-dependent symmetries). Previous work (Crawford et al., 1996; Aloul et al., 2003,
2004) deals instance-dependent symmetries SAT 0-1 ILP instances. Symmetries first detected reduction graph automorphism broken adding
symmetry breaking predicates (SBPs) formulation. advantage strategy every instance-independent symmetry also instance-dependent, whereas
reverse hold. Symmetries exist due problem formulation appear every
instance problem, addition symmetries exist due specific parameter
values instance. Given may many instance-specific symmetries, one
may process symmetries using publicly available symmetry processing packages
Shatter (Aloul et al., 2003; Aloul, Markov, & Sakallah, 2003). Alternatively, one
may add symmetry breaking predicates instance-independent symmetries early, hoping
speed-up processing remaining symmetries. type symmetry breaking
discussed earlier work (Aloul et al., 2003, 2003), paper study
utility graph coloring problem.
work deals symmetries problem instance descriptions; distinguish
(i) symmetries generic problem specifications (ii) symmetries problem-instance
data. former symmetries translate latter way around
example graph coloring given color permutations versus automorphisms
specific graphs. types symmetries detected solving graph automorphism problem, symmetries specifications often captured manually, whereas
capturing symmetries problem instances may require large-scale computation nontrivial software. Indeed, specification-level symmetries instantiated, size
support (the number objects moved) typically increases dramatically. example,
color permutations graph coloring simultaneously applied every vertex
graph question. Detecting symmetries larger support seems like waste computational effort. end, recent work breaking symmetries specifications (Cadoli
& Mancini, 2003) prefers instance-independent techniques breaks symmetries
specification level. approach particularly relevant constraint solvers
languages process problem specifications prior seeing actual problem instances
amortize symmetry-detection effort. Also, general setting, using instanceindependent symmetry breaking rule applying redundant (or complementary)
instance-specific techniques later stage.
recently automatic symmetry detection serious bottleneck handling
symmetries. example, graph automorphism solved using program Nauty
(McKay, 1990), detecting symmetries often take longer constraint solving without
symmetry breaking. observed microprocessor verification SAT instances
Aloul et. al. 2002 (Aloul et al., 2003). Therefore, detecting symmetries early
representing structured way appears attractive, especially given
may potentially increase efficiency symmetry-breaking. However, symmetrydetection bottleneck recently eliminated many applications software
tool Saucy (Darga et al., 2004) often finds symmetries practical graphs many times
291

fiRamani, Aloul, Markov, & Sakallah

faster Nauty. development undermines, extent, potential benefits
symmetry processing specification level puts spotlight symmetry-breaking.
end, SBPs added different circumstances may different efficiency,
unclear priori approach successful, differences performance
may significant. Since SBPs appear solver additional constraints, may
either speed frustrate solver (the latter effect clearly visible experiments
CPLEX). Outcomes practical experiments also affected recent dramatic
improvements efficiency symmetry-breaking predicates (Aloul et al., 2003, 2004).
seems difficult justify particular expectation empirical performance,
fortunate observe clear trends experimental data presented Section 4
summarize simple rules.
focus graph coloring instances, techniques immediately applicable
related CSP problems, e.g., produced adding new types constraints
easily expressed SAT 0-1 ILP graph coloring converted generic
problems. also expect conclusions symmetry-breaking carry
CSPs economically reduced SAT 0-1 ILP, e.g., maximum independent
set, minimum dominating set, etc. Another advantage approach able use
variety existing future SAT 0-1 ILP solvers without modifying source code.
Unfortunately, precludes use dynamic symmetry-breaking would require
modifying source code may adversely affect performance disturbing fragile
balance amount reasoning searching performed modern SAT solvers.
Specifically, heuristics variable ordering decision selection may affected, well
recording learned conflict clauses (nogoods).
main contributions work listed below.
Using symmetry breaking flow pseudo-Boolean (PB) formulas described
Aloul et. al 2004 (Aloul et al., 2004), detect break symmetries DIMACS
graph coloring benchmarks expressed instances 0-1 ILP. show instancedependent symmetry breaking enables many medium-sized instances optimally
solved reasonable time commodity PCs
propose instance-independent techniques breaking symmetries problem formulation, assess relative strength completeness, evaluate
empirically using well-known academic commercial tools
show empirically instance-dependent techniques are, general, effective
instance-independent symmetry breaking benchmarks question.
fact, simplest least complex instance-independent SBPs competitive
remaining part paper organized follows. Section 2 covers background
graph coloring, SAT 0-1 ILP, well previous work symmetry breaking. Instanceindependent symmetry breaking predicates discussed Section 3. Section 4 presents
empirical results Section 5 concludes paper. Appendix gives detailed results
queens family instances.
292

fiBreaking Instance-Independent Symmetries Exact Graph Coloring

2. Background Previous Work
section discusses problem definitions applications existing algorithms
exact graph coloring. also discuss previous work symmetry breaking SAT
0-1 ILP detail.
2.1 Graph Coloring
Given undirected graph G(V, E), vertex coloring graph assignment
label (color) node labels adjacent nodes different. minimum
coloring uses smallest possible number colors, known chromatic number
graph. decision version graph coloring (Kcoloring) asks whether vertices
graph colored using K colors given K.
clique undirected graph G(V, E) set mutually adjacent vertices
graph. maximum clique problem consists seeking clique maximal size, i.e.,
clique least many vertices clique graph. maximum
clique graph coloring problems closely related. Specifically, max-clique size
lower bound chromatic number graph. years, number
different algorithms solving graph coloring developed, fundamental importance computer science. algorithms fall three broad categories:
polynomial-time approximation schemes, optimal algorithms, heuristics. briefly
discuss work categories below. number online resources
graph coloring (Trick, 1996; Culberson, 2004) offer detailed bibliographies.
far approximation schemes concerned, common technique used
successive augmentation. approach partial coloring found small number
vertices extended vertex vertex entire graph colored. Examples
include algorithms Leighton (Leighton, 1979) large scheduling problems,
Welsh Powell (Welsh & Powell, 1967) time-tabling. recent work attempted
tighten worst-case bounds chromatic number graph. algorithm
providing currently best worst-case ratio (number colors used divided optimal
number)
due
Haldorsson (Haldorsson, 1990), guarantees ratio


2

log n)
, n number vertices. General heuristic methods
n(log
(log n)3
tried include simulated annealing (Chams, Hertz, & Werra, 1987; Aragon, Johnson,
McGeoch, & Schevon, 1991) tabu search (Hertz & Werra, 1987). well-known heuristic
still widely used DSATUR algorithm Brelaz (Brelaz, 1979) colors
vertices according saturation degree. saturation degree vertex number
different colors adjacent. DSATUR heuristic repeatedly picks vertex
maximal saturation degree colors lowest-numbered color possible.
heuristic optimal bipartite graphs. Algorithms finding optimal colorings
frequently based implicit enumeration, discussed detail later
section. graph coloring max-clique problems N P-complete (Garey &
Johnson, 1979) even finding near-optimal solutions good approximation guarantees
N P-hard (Feige, Goldwasser, Lovasz, Safra, & Szege, 1991). inapproximability
graph coloring suggests may difficult solve heuristically than, say,
Traveling Salesman Problem Polynomial-Time Approximation Schemes (PTAS)

293

fiRamani, Aloul, Markov, & Sakallah

known Euclidean Manhattan graphs. number reasons,
study optimal graph coloring many application-derived instances solvable
reasonable time. Several applications outlined next.
Time-Tabling Scheduling problems involve placing pairwise restrictions jobs
cannot performed simultaneously. example, two classes taught faculty member cannot scheduled time slot. problem studied
previous work Leighton (Leighton, 1979) De Werra (Werra, 1985). generally,
graph coloring important problem Artificial Intelligence close relationship planning scheduling. Several traditional AI techniques applied
problem, including parallel algorithms using neural networks (Jagota, 1996). Genetic
hybrid evolutionary algorithms also developed, notably Galinier et. al.
1999 (Galinier & Hao, 1999), addition traditional optimization methodology,
scatter search (J.-P. Hamiez, 2001). also studies benchmarking
models graph coloring, recent work Walsh (Walsh, 2001), shows
graphs high vertex degrees likely occur real-world applications.
Register Allocation active application graph coloring. problem
seeks assign variables limited number hardware registers program execution.
Accessing variables registers much faster fetching memory. However,
number registers limited typically much smaller number variables.
Therefore, multiple variables must assigned register. restrictions
assignments. Two variables conflict live
time, i.e. one used within short period time (for
instance, within subroutine). goal assign variables conflict
minimize use non-register memory. formalize this, one creates graph
nodes represent variables edges represent conflicts variables. coloring maps
conflict-free assignment, number registers exceeds chromatic number,
conflict-free register assignment exists (Chaitin et al., 1981).
Printed Circuit Board Testing (Garey & Johnson, 1979) involves problem
testing printed circuit boards (PCBs) unintended short circuits (caused stray lines
solder). gives rise graph coloring problem vertices correspond
nets board edge two vertices potential short
circuit corresponding nets. Coloring graph corresponds partitioning
nets supernets, nets supernet simultaneously tested
shorts nets, thereby speeding testing process.
Radio frequency assignment broadcast services geographic regions (including commercial radio stations, taxi dispatch, police emergency services). list
possible frequencies fixed government agencies, adjacent geographic regions cannot use overlapping frequencies. reduce frequency assignment graph coloring,
geographic region needing K frequencies represented Kclique, N K
possible bipartite edges introduced two geographically adjacent regions needing
N K frequencies respectively.
applications graph coloring circuit design layout include circuit clustering, scheduling signal flow graphs, many others. Benchmarks applications
publicly available, therefore appear paper. However, symmetry breaking techniques described extend instances application.
294

fiBreaking Instance-Independent Symmetries Exact Graph Coloring

benchmarks use include register allocation, nqueens, several applications discussed detail Section 4. Empirically, observe many
instances paper optimally solved reasonable time, especially symmetry breaking employed. Since work deals finding optimal solutions graph
coloring, discuss previous work finding exact algorithms problem
detail.

literature exact graph coloring includes generic algorithms (Kubale & Jackowski,
1985) specialized algorithms particular application, Chaitins register allocation algorithm (Chaitin et al., 1981). moment, appear
comprehensive survey techniques problem. However, online surveys (Trick, 1996;
Culberson, 2004) contain reasonably large bibliographies even downloadable source
code coloring algorithms cases. Published algorithms finding optimal graph
colorings mainly based implicit enumeration. algorithm proposed Brown
(Brown, 1972) enumerates solutions given instance graph coloring checks
solution correctness optimality. algorithm introduces special tree construction avoid redundancy enumerating solutions. work Brelaz (Brelaz, 1979)
improves upon algorithm creating initial coloring based clique
graph considering assignments induced coloring. work Kubale
Kusz (Kubale & Kusz, 1983) discusses empirical performance implicit enumeration
algorithms, later work Kubale Jackowski (Kubale & Jackowski, 1985) augments
traditional implicit enumeration techniques sophisticated backtracking methods.

work deals solving graph coloring reduction another problem,
case 0-1 ILP. type reduction discussed past, notably recent
work Mehrotra Trick (Mehrotra & Trick, 1996), proposes optimal coloring
algorithm expresses graph coloring using ILP-like constraints. relies auxiliary
independent set formulation, independent set graph represented
variable. prohibitively many variables practical cases number may
reduced column generation, method first tries solve linear relaxation using
subset variables adds needed. approach inherently breaks
problem symmetries, thus rules use SBPs way speed search
process. ILP construction differs considerably one described above, since
rely independent set formulation, assigns colors individual vertices
using indicator variables. construction described detail later section.
Solving graph coloring reduction allows exact solutions found using SAT/0-1
ILP solvers black boxes. Earlier work Coudert (Coudert, 1997) demonstrated
finding exact solutions application-derived graph coloring benchmarks often takes
longer heuristic approaches, heuristic solutions may differ optimal
value much 100%. Coudert (Coudert, 1997) proposes algorithm finds
exact graph coloring solutions solving max-clique problem. algorithm uses
technique called qcolor pruning, assigns colors vertices systematically
removes vertices colored q colors, q greater specified limit.
295

fiRamani, Aloul, Markov, & Sakallah

2.2 Breaking Symmetries CSPs
Several earlier works addressed importance symmetry breaking search
solutions CSPs. shown (Krishnamurthy, 1985) symmetry facilitates short
proofs propositions pigeonhole principle, whereas pure-resolution proofs
necessarily exponential size. Finding proofs is, course, difficult problem,
performance many CSP techniques lower-bounded best-case proof
size. typical approach use symmetries prevent CSP solver considering
redundant symmetric solutions. called symmetry-breaking accomplished
adding constraints, often called symmetry-breaking predicates (SBPs). Static symmetrybreaking, instance-independent constructions proposed work
instance-dependent predicates literature (Aloul et al., 2003; Crawford et al., 1996),
detects symmetries adds SBPs pre-processing branching toward
possible solutions. Symmetry Breaking Dominance Detection (SBDD) procedure
described Fahle 2001 (Fahle, Schamberger, & Sellmann, 2001) detects symmetric
choice points search. choice point generated search algorithm checked
previously expanded search nodes. equivalent choice point
previously expanded, choice point visited again. global cut algorithm
proposed Focacci Milano (Focacci & Milano, 2001) records nogoods found
search whose symmetric images pruned. set nogoods, called global cut
seed used generate global cut constraints prune symmetric images entire
search tree, ensuring correctness original constraints violated. Later
work (Puget, 2002) proposed improved methods nogood recording. works
offer systematic strategy symmetry detection - either require symmetries
known declared advance, record information search enables
symmetry detection. work outlines implements complete strategy detect
break symmetries automatically pre-processing, black-box solver
used search. context broader justify development
specialized solvers. hand, techniques conflict dynamic
symmetry-breaking results potentially reused context.
promising new partially-dynamic approach symmetry-breaking, called Group Equivalence (GE) trees proposed Roney et. al. (Roney-Dougal, Gent, Kelsey, & Linton,
2004). work aims reduce per-node overhead associated dynamic approaches.
GE tree constructed CSP symmetry group G nodes
tree represent equivalence classes partial assignments group. approach
illustrated tracking value symmetries, i.e., simultaneous permutations values CSP
variables. work also shows GE trees empirically outperform several well-known
symmetry-breaking methodologies, SBDDs. comparison, work compares different ways handle arbitrary compositions variable value symmetries (in graph
coloring, value symmetries seen specification level, whereas variable symmetries
seen problem instances). end, static techniques appear compatible rather competing use GE trees. also many symmetry breaking approaches particular relevance graph coloring. Recent work Gent
(Gent, 2001) proposes constraints break symmetry indistinguishable values,
evaluate empirically. Like lowest-index ordering (LI) constraints pro296

fiBreaking Instance-Independent Symmetries Exact Graph Coloring

posed us Section 3, constraints also use pre-existing sequential numbering
vertices instance graph coloring enforce distinctions symmetric vertices.
construction appears complex compared alternative SBPs effective
experiments simpler constructions. Another related work (Hentenryck, Agren, Flener,
& Pearson, 2003), proposes constant-time, constant-space algorithm detecting breaking value symmetries class CSPs includes graph coloring.
recently, Benhamou (Benhamou, 2004) discusses symmetry breaking CSPs modeled using not-equals constraints (NECSP), uses graph coloring illustrative example.
paper defines sufficient condition symmetry certain symmetries
detected linear time. removal symmetries leads considerable gains
backtracking search algorithms NECSPs. general, empirical results, reported
Section 4, appear competitive state-of-the-art dynamic approaches. However,
designing worlds best graph-colorer goal research. Instead, focus
efficient problem reductions SAT 0-1 ILP improving symmetry-breaking.
ensure broad applicability results, treat SAT solvers black boxes,
perform comprehensive comparison static SBPs report empirical trends.
comprehensive comparison existing graph coloring literature would
great value, making rigorous, conclusive revealing requires best static
best dynamic symmetry-breaking techniques known. end, speculate
likely winner would hybrid. Additional major issues resolved include
tuning solvers specific benchmarks (noted work Kirovski Potkonjak (Kirovski & Potkonjak, 1998), differences experimental setup, different software
hardware platforms, etc. Given comparison completely scope
work, better delegated dedicated publication. However, demonstrate
techniques competitive related work, provide comparison best
results recent literature (Benhamou, 2004; Coudert, 1997) Section 4.3.
2.3 SAT 0-1 ILP
One solve decision version graph coloring reducing Boolean satisfiability,
optimization version reduction 0-1 ILP. Boolean satisfiability (SAT)
problem involves finding assignment set 0-1 variables satisfies set
constraints, called clauses, expressed conjunctive normal form (CNF). CNF formula
n binary variables, x1 , . . . , xn consists conjunction clauses, 1 , . . . , . clause
consists disjunction literals. literal l occurrence Boolean variable
complement. 0-1 ILP problem closely related SAT, allows use
pseudo-Boolean (PB) constraints, linear inequalities integer coefficients
expressed normalized form (Aloul, Ramani, Markov, & Sakallah, 2002) of:
a1 x1 + a2 x2 + . . . xn b ai , b Z + xi literals Boolean variables. 1
cases single PB constraint replace exponential number CNF clauses
(Aloul et al., 2002). general, efficiency CNF reductions encoding-dependent.
Earlier work Warners (Warners, 1998) shows linear-overhead conversion exists
linear inequalities integer coefficients 0-1 variables CNF. However, CNF
1. Using relations (Ax b) (Ax b) xi = (1 xi ), arbitrary PB constraint
expressed normalized form positive coefficients.

297

fiRamani, Aloul, Markov, & Sakallah

encodings use conversion may less efficient. converting CNF
PB, single CNF constraint always expressed single 0-1 ILP constraint (by
replacing disjunctions literals constraint + setting right-handside value 1). However, may always suitable since certain operations,
disjunction, implication inequality intuitively expressed CNF,
efficiently processed SAT solvers Chaff (Moskewicz et al., 2001). conversion
0-1 ILP desirable arithmetic operations, counting constraints, whose
CNF equivalent requires polynomially many clauses (and exponentially many
conversions). maximize advantages CNF PB formats, recent
0-1 ILP solvers PBS (Aloul et al., 2002) Galena (Chai & Kuehlmann, 2003)
allow formula possess CNF PB components. Additionally, 0-1 ILP solvers also
provide solution optimization problems. Subject given constraints, one may
request minimization (or maximization) objective function must linear
combination problem variables.
Exact SAT solvers (Goldberg & Novikov, 2002; Moskewicz et al., 2001; Silva & Sakallah,
1999) typically based original Davis-Logemann-Loveland (DLL) backtrack search
algorithm (Davis, Logemann, & Loveland, 1962). Recently, several powerful methods
proposed expedite backtrack search algorithm, conflict diagnosis (Silva
& Sakallah, 1999) watched literal Boolean constraint propagation (BCP) (Moskewicz
et al., 2001). improvements, modern SAT solvers (Moskewicz et al., 2001;
Goldberg & Novikov, 2002) capable solving instances several million variables
clauses reasonable time. increase scalability scope enabled number
SAT-based applications various domains, including circuit layout (Aloul et al., 2003),
microprocessor verification, symbolic model checking, many others. recent work
focused extending advances SAT 0-1 ILP (Aloul et al., 2002; Chai & Kuehlmann,
2003). work, focus solving instances exact graph coloring reduction
0-1 ILP use SBPs. choice 0-1 ILP motivated following reasons.
Firstly, 0-1 ILP permits use general input format CNF, allowing
greater efficiency problem encoding, time similar enough SAT
allow improved methods SAT-solving used without paying penalty generality.
specialized 0-1 ILP solvers PBS (Aloul et al., 2002) Galena (Chai & Kuehlmann,
2003) propose sophisticated new techniques 0-1 ILP based recent
decision heuristics (Moskewicz et al., 2001), conflict diagnosis backtracking techniques
(Silva & Sakallah, 1999) SAT solvers. result, empirically perform better
generic ILP solver CPLEX (ILOG, 2000) leading-edge SAT solver zChaff
several DIMACS SAT benchmarks application-derived instances FPGA routing
instances circuit layout. Also, since 0-1 ILP optimization problem, unlike SAT
decision problem, 0-1 ILP solvers possess ability maximize/minimize
objective function. can, therefore, directly applied optimization version
exact graph coloring, unlike pure CNF-SAT solvers used kcoloring
decision variant. possible solve optimization version repeatedly solving
instances kcoloring using SAT solver, value k updated
call. However, 0-1 ILP solvers require extra step, moreover tend provide
better performance repeated calls SAT solver many Boolean optimization
problems (Aloul et al., 2002).
298

fiBreaking Instance-Independent Symmetries Exact Graph Coloring

possible use generic ILP solver, commercial solver CPLEX (ILOG,
2000) instead specialized 0-1 ILP solver without changes problem formulation.
However, Aloul et al. (Aloul et al., 2002) show generalization always
desirable, particularly case Boolean optimization problems Max-SAT. 0-1
ILP also especially useful evaluating effectiveness symmetry breaking graph
coloring, primary purpose work. Detecting breaking symmetries SAT
formulas shown speed problem-solving process (Crawford et al., 1996;
Aloul et al., 2003). Recently, symmetry breaking techniques SAT extended
0-1 ILP (Aloul et al., 2004), shown produce search speedups
domain well. However, similar extension non-binary variables generic ILP
presently exist. evidence (Aloul et al., 2002) advantages symmetry
breaking may depend actual algorithm used search. Specifically, results
cited work suggest generic ILP solver CPLEX actually slowed
addition SBPs. Since CPLEX commercial tool algorithms used
publicly known, difficult pinpoint reason disparity. However, empirical
results Section 4 bear observations. remainder section discusses
reduction graph coloring 0-1 ILP explains previous work symmetry breaking
detail.
2.4 Detecting Breaking Symmetries 0-1 ILPs
Previous work (Crawford et al., 1996; Aloul et al., 2003) shown breaking symmetries
CNF formulas effectively prunes search space lead significant runtime
speedups. Breaking symmetries prevents symmetric images search paths
searched, thus pruning search tree. papers cited work use variants
approach first described Crawford et al. (Crawford et al., 1996), detects
symmetries CNF formula using graph automorphism. formula expressed
undirected graph symmetry group graph isomorphic symmetry
group CNF formula. Symmetries induce equivalence relations set truth
assignments CNF formula. assignments equivalence class result
truth value formula (satisfying not). Therefore, necessary consider
one assignment class.
Techniques symmetry breaking proposed literature follow following steps:
(i) construction colored graph CNF formula (ii) detection symmetries
graph using graph automorphism software (iii) use detected symmetries construct symmetry breaking predicates (SBPs) appended additional clauses
CNF formula (iv) solution new CNF formula thus created using SAT solver.
Crawfords construction (Crawford, 1992) uses 3 colors vertices, one positive literals, one negative literals third clauses. Edges added literals
clause corresponding clause vertex, positive negative literal
vertices Boolean consistency. optimization, binary clauses (with two literals)
represented adding edge two involved literals, extra vertex
needed. useful runtime graph automorphism programs
Nauty (McKay, 1990) generally increases number vertices graph.
However, optimization Boolean consistency enforced, since binary clausal
299

fiRamani, Aloul, Markov, & Sakallah

edges could confused Boolean consistency edges positive negative literals variable. may improved representing binary clausal edges
double edges (Crawford et al., 1996), thus distinguishing two edge types.
However, Nauty (and graph automorphism programs) support uses
double edges, construction useful practice. Furthermore, cited
constructions (Crawford, 1992; Crawford et al., 1996) allow detection phase-shift
symmetries, variables positive literal mapped negative literal vice versa,
since color positive negative literals differently. previous work (Aloul et al.,
2003) improves upon constructions giving positive negative literal vertices
color, allowing binary clauses Boolean consistency edges represented
way, i.e. single edge two literal vertices. Although construction
may allow spurious symmetries - clause edges mapped consistency edges -
occur formula contains circular chains implications subset
variables. example, given subset variables x 1 . . . xn , chain collection
clauses (y1 y2 )(y2 y3 ) . . . (yn1 yn ), yi positive negative literal
xi . circular chains rarely occur practice, easily checked for. Therefore,
efficient graph construction described used practical cases.
Graph automorphisms detected Crawfords work (Crawford et al., 1996) well
previous work (Aloul et al., 2003) using program Nauty (McKay, 1990),
part GAP (Groups, Algebra Programming) package. Nauty accepts graphs
GAP input format returns list generators automorphism group (the
term generators used mathematical sense, symmetry group partitions set
vertex permutations graph equivalence classes permutations
class equivalent. Nauty returns set generators symmetry group).
recent work ((Aloul et al., 2003, 2004)) uses automorphism program Saucy (Darga
et al., 2004), efficient Nauty also process larger graphs
vertices. generators symmetry group detected, symmetry breaking
predicates added instance pre-processing step. Crawford et al. (Crawford
et al., 1996) propose addition SBPs choose lexicographically smallest assignments
(lex-leaders) equivalence class. refer SBPs instance-dependent
SBPs, since symmetries first detected broken, therefore exact
number nature SBPs added always depends connectivity graph itself.
Although detecting symmetries non-trivial, using modern software Nauty
Saucy detection time frequently insignificant compared SAT-solving time.
Crawford et. al. (Crawford et al., 1996) construct lex-leader SBPs entire symmetry
group, using group generators returned Nauty. type symmetry breaking
complete. However, approach used Aloul et al. TCAD 2003 (Aloul et al.,
2003) shows incomplete symmetry breaking, breaks symmetries
generators, often effective practice much efficient since require
whole group reconstructed. SBP construction proposed cited work (Aloul
et al., 2003) quadratic number problem variables, compared earlier
construction (Crawford et al., 1996), could run exponential size. construction
improved 2003 work Aloul, Sakallah Markov (Aloul et al., 2003),
describes efficient, tautology-free SBP construction, whose size linear number
problem variables. Empirical results Crawfords work (Crawford et al., 1996)
300

fiBreaking Instance-Independent Symmetries Exact Graph Coloring

well work TCAD 2003 (Aloul et al., 2003) show breaking symmetries produces
large search speedups number CNF benchmark families, including pigeonhole
Urquhart benchmarks, microprocessor verification, FPGA routing ASIC global routing
benchmarks VLSI domain.
work symmetry breaking SAT (Aloul et al., 2003) also extended
optimization problems include CNF PB constraints, objective
function (Aloul et al., 2004). before, symmetries detected reduction graph
automorphism. PB formula optimization problem represented undirected
graph. Graph symmetries detected using graph automorphism tool Saucy (Darga
et al., 2004). Efficient symmetry breaking predicates (Aloul et al., 2003) appended
formula CNF clauses. empirical results work symmetry breaking
0-1 ILP (Aloul et al., 2004) show addition symmetry breaking predicates
PB formulas results considerable search speedups specialized 0-1 ILP solver
PBS (Aloul et al., 2002). work, use methodology (Aloul et al., 2004)
detecting breaking instance-dependent symmetries instances graph coloring
expressed 0-1 ILP. instance-dependent SBPs compared number
instance-independent SBP constructions described next section.
Detecting breaking symmetries application-derived SAT instances amounts
recovery structure original application. loss structure problem
reductions one reason reduction-based techniques often competitive
domain-specific algorithms, recent work symmetry breaking useful context.
types structure include clusters (Huang & Darwiche, 2003; Aloul, Markov, &
Sakallah, 2004). Huang et al. (Huang & Darwiche, 2003) propose algorithm detects
clusters SAT instances uses produce variable orderings, structureaware orderings result considerable empirical improvements SAT solver zChaff
(Moskewicz et al., 2001).
2.5 Reducing Graph Coloring 0-1 ILP
express instance minimal graph coloring problem 0-1 ILP optimization
problem, consisting (i) CNF PB constraints model graph (ii) objective
function minimize number colors used.
Consider graph G(V, E). Let n = |V | number vertices G, = |E|
number edges. instance Kcoloring problem G (i.e., vertices
V colored K colors) formulated follows.
vertex vi , K indicator variables xi,1 , . . . , xi,K , denote possible color assignments vi . Variable xi,j set 1 indicate vertex vi colored color j,
0 otherwise
vertex vi , PB constraint form
colored exactly one color.

PK

j=1 xi,j

= 1 ensures vertex

edge ei E connects two vertices (va , vb ). edge ei , define CNF
V
constraints form K
j=1 (xa,j xb,j ) specify two vertices connected
edge given color.
301

fiRamani, Aloul, Markov, & Sakallah

track used colors, define K new variables, 1 , . . . , yK . Variable yi true
least one vertex uses color i. expressed using following CNF
V
Wn
constraints: K
j=1 (yj ( i=1 xi,j )).
optimization objective minimize number variables set true, i.e.
P
MIN K
i=1 yi
total number variables formula nK +K. total number constraints
computed follows. totally n 0-1 ILP constraints (one per vertex) ensure
vertex uses exactly one color. edge, K CNF clauses specifying
two vertices connected edge cannot color, giving total
mK CNF clauses. additional nK CNF clauses (K per vertex) setting
indicator variables, K CNF clauses, one per color, complete iff condition indicator variables. gives total K (m + n + 1) CNF clauses n 0-1 ILP constraints,
plus one objective function, converted formula. dense graphs, |E| |V | 2 ,
resulting formula size quadratic number vertices graph, sparser
graphs may linear. key observation instance-dependent symmetries graph
coloring survive reduction 0-1 ILP. instance-independent symmetries (i.e.
permutations colors) easy see, since ordering colors changed
without effect formula producing set constraints.
instance-dependent symmetries, consider two vertices v vb symmetric
swapped original graph. Clearly, constraints specify
va vb must use exactly one color interchangeable, constraints determine color usage based colors assigned v vb . remains show
connectivity constraints control colors vertices adjacent v vb also
symmetric. clear fact every edge E incident va , must
corresponding edge Ej incident vb two vertices symmetric (E Ej
edge). Therefore, set K CNF clauses added formula
represent Ei , must symmetric set clauses added E j , thus connectivity
preserved.
also clear 0-1 ILP formulation introduce spurious symmetries, i.e.
symmetry formula symmetry graph. spurious symmetry arises
(i) variables different types mapped other, e.g. vertex color variables
mapped color usage indicator variables (ii) variables type mapped
corresponding vertices actually symmetric.
construction 0-1 ILP formula, clear K variables per vertex indicate
vertexs color permuted, K color usage variables, since appear
exactly constraints. corresponds instance-independent symmetry colors instance graph coloring arbitrarily permuted. However, vertex color
variables appear constraints restricting number colors vertex use also
constraints describe connectivity graph, whereas color usage variables appear
constraints specify set. Therefore, two types variables
cannot map one another. Since constraints regarding color connectivity
vertex written using K color variables vertex, variables symmetric
groups K, i.e. one variable given vertex v 1 symmetric
variable another vertex v2 , K variables v1 v2 correspondingly
302

fiBreaking Instance-Independent Symmetries Exact Graph Coloring

symmetric. Additionally, symmetry variables indicates correspondence
clauses occur. possible vertices v 1 v2 symmetric
terms connectivity (instance-dependent symmetry). Thus, types symmetries
preserved conversion 0-1 ILP, false symmetries added. Therefore,
apply known techniques symmetry detection 0-1 ILP.

3. Instance-Independent SBPs
question addressed work whether instance-independent SBPs added
reduction provide even greater speedups, possibly accelerating detection
instance-dependent symmetries. answer question, propose three provably
correct SBP constructions varying strength, one heuristic intended break
small number symmetries minimal overhead. construction implemented
empirical results reported Section 4.
use following notation. Consider instance Kcoloring problem,
asks whether graph G(V, E) colored using K colors minimizes number
colors. Assume colors numbered 1 . . . K. denote valid color assignment
P
(n1 , n2 , . . . , nK ) ni number vertices colored color i, |V | = K
i=1 ni .
ni color assignment denotes cardinality independent set colored
color i. concerned actual composition independent sets here,
since instance-dependent issue. Instance-independent symmetries
arbitrary permutations colors different independent sets.
effects proposed construction illustrated using example Figure
1. figure example 4-coloring problem graph four vertices. Part
(a) figure shows graph colored. visual clarity, part (b) shows color
patterns corresponding different color numbers. clear figure
vertices V1 , V2 V3 form clique, must use different colors. However, V 4 given
color either V1 V2 , therefore 3 colors needed instance.
instance partitioned independent sets two ways: {{V 1 , V4 }, {V2 }, {V3 }}
{{V1 }, {V2 , V4 }, {V3 }}. SBPs actually address independent sets
composed, instance-dependent issue. However, given partition
independent sets, colors arbitrarily permuted sets partition.
instance-independent SBPs proposed restrict permutation. examples below,
assume first partition independent sets i.e. {{V 1 , V4 }, {V2 }, {V3 }}. Results
proved respect permutation colors partition.
3.1 Null-Color Elimination (NU)
Consider Kcoloring problem colors 1 . . . K graph G(V, E). Assume G
minimally colored withK 1 colors. Consider optimal solution color
used: (n1 , n2 , ..ni1 , 0, ni+1 , . . . , nK ). assignment equivalent another assignment,
(n0 1 , n0 2 , ..n0 j1 , 0, n0 j+1 ...n0 K )
6= j n0 = nj . example, assignment (1, 0, 2, 3) equivalent (1, 3, 2, 0),
(0, 1, 2, 3), (1, 2, 0, 3). due existence null colors, create symmetries
303

fiRamani, Aloul, Markov, & Sakallah

1:

V1
V3

V4

2:
3:

V2

4:
(a)

(b)

V1

V1
V3

V2

V4

V3

(1,0,2,1)

V2

V4

(1,2,1,0)

(c)

V1

V1
V3

V2

V4

V3

(1,1,2,0)

V2

V4

(2,1,1,0)

(d)

V1

V1
V3

V2

V3

V4

(2,1,1,0)

V2

V4

(1,1,2,0)

(e)

Figure 1: Instance-independent symmetry breaking predicates (SBPs).
Part (a) shows original graph vertices colored.
Part (b) shows color key. Part (c) shows nullcolor SBPs prevent color 4 used. Part (d) shows
cardinality based SBPs assign colors order independent set sizes, allowing fewer assignments nullcolor SBPs. Part (e) demonstrates lowest-index ordered
SBPs break symmetries undetected types
SBPs.
304

fiBreaking Instance-Independent Symmetries Exact Graph Coloring

instance Kcoloring color swapped null color. Null colors
extraneous actually required color vertices,
inserted anywhere solution, seen above. propose construction enforces
ordering null colors: null colors may appear end color assignment,
non-null colors. implemented adding K 1 CNF constraints form:
yk+1 yk 1 k K 1, original formulation. example above, one
four symmetric assignments (1, 3, 2, 0) would allowed construction. Since
ILP formulation defines sets K indicator variables track color usage,
extremely easy enforce null color elimination described above. SBPs require
addition extra variables K 1 new CNF clauses.
prove proposed construction correct. Assume original
formulation, optimal solution graph G(V, E) uses colors. Assume solution
contains null colors non-null colors, null-color elimination, different
optimal solution uses m0 colors, 6= m0 . colors used solution
1 . . . m0 , since null colors cannot occur non-null colors. Since construction
adds SBPs without changing original constraints, legal solution satisfies
SBPs satisfy constraints original formulation. solution original
satisfies constraints new formulation except SBPs. < 0 , re-order
solution null colors placed last. satisfy SBPs use
colors, < m0 , violating assumption 0 -color solution optimal.
m0 < m, already solution satisfies original constraints uses fewer
colors, violates assumptions optimality.
illustration use NU predicates example Figure 1 (a) shown
Figure 1 (c). figure shows two valid minimal-color assignments graph vertices
example. assignment left uses colors 1, 3 4, one right
uses colors 1, 2 3. assignments symmetric NU predicates
right-hand side assignment permissible.
3.2 Cardinality-Based Color Ordering (CA)
Null-color elimination useful cases null colors exist. Kcoloring
problem colors needed, construction breaks symmetries. Even
null colors exist, several symmetries go undetected. first example above, nullcolor elimination permits six symmetric color assignments (1, 2, 3, 0), (1, 3, 2, 0), (2, 1, 3, 0),
(2, 3, 1, 0) (3, 2, 1, 0) (3, 1, 2, 0). restrictions placed null colors,
ordering non-null colors unrestricted. stronger construction would distinguish independent sets themselves. propose alternative construction,
assigns colors based cardinality independent sets. subsumes null-color
elimination, since null colors viewed coloring sets cardinality 0. cardinality rule implemented follows: largest independent set assigned color 1,
second-largest color 2, etc. example above, assignment (3, 2, 1, 0)
P
P
valid. enforced adding K 1 PB constraints form: ni=1 xi,k ni=1 xi,k+1 ,
1 k K 1. Again, construction fairly simple implement, requiring
K 1 additional constraints. However, 0-1 ILP constraints multiple
305

fiRamani, Aloul, Markov, & Sakallah

variables, unlike simple CNF implication clauses two variables used NU
predicates. Thus, overhead greater completeness.
prove CA construction correct follows. Assume optimal solution
construction uses < K colors: (n 1 , n2 , . . . , nm ), (n1 n2 . . . nm ). Colors
> used vertex, Assume exists optimal solution original
formulation uses m0 colors: (n0 1 , n0 2 , . . . , n0 m0 ), (where n0 1 , etc. arranged
descending order). Without loss generality, assume 0 < m. sort
numbers n0 1 , . . . , n0 m0 reassign colors descending order. would solution
m0 colors satisfying cardinality constraints. However, 0 < m, possible
mcolor solution optimal. similar argument applies < 0 .
example Figure 1 (a), largest independent set partition
considering, i.e. {V1 , V4 } given color 1. Therefore, assignment right
Figure 1 (c), assigns largest set color 2 correct NU predicates,
incorrect CA construction. left-hand side Figure 1 (d) shows another
assignment correct NU predicates incorrect CA predicates, since
assigns set {V1 , V4 } color 3. correct assignment, shown right-hand side
Figure 1 (d), gives largest set color 1 since sets one element each,
assigned either color 2 color 3. Thus, several symmetric assignments
survive NU predicates prohibited construction.
3.3 Lowest Index Color Ordering (LI)
complete NU predicates, CA predicates break symmetries
different independent sets cardinality. Consider graph G V =
{v1 , . . . , v8 }, optimal solution, satisfying cardinality-based ordering, partitions
V 4 independent sets: S1 = {v4 , v6 , v7 }, S2 = {v1 , v5 }, S3 = {v3 , v8 }, S4 = {v2 }.
solution assigns colors 2 3 2 S3 symmetric one assigns colors 2
3 S3 S2 . legal cardinality-based ordering. order completely
break symmetries, adequate distinguish sets solely basis
cardinality (unless two sets cardinality). necessary construct
SBPs based actual composition sets partition, unique. However,
distinctions make basis composition confused
instance-dependent SBPs, since construction implemented symmetries
instance known, regardless actual composition. SBPs specify
broad guidelines coloring independent sets applicable graphs.
improve upon cardinality-based ordering, propose set predicates enforce
lowest-index ordering (LI). Consider vertices color i, find lowest index j
among those. require lowest indices color ordered. constraint
enforced adding inequalities colors adjacent numbers.
Note color unique lowest-index vertex otherwise vertex would
colored two colors. example, color assignment
compatible partitioning vertices independent sets is: color 1 1 , 2 S3 ,
3 S4 , 4 S2 .
evaluate strength symmetry-breaking technique, consider arbitrary
coloring color permutation remains symmetry LI constraints
306

fiBreaking Instance-Independent Symmetries Exact Graph Coloring

imposed. colors permuted simultaneously vertices, permute
lowest indices colors. Since lowest indices different, ordering completely determined ordering colors, thus color permutation chose must
identity permutation. words, instance-independent symmetries remain
symmetry-breaking LI.
implement lowest-index color ordering follows. vertex v , declare
new set K variables, Vi,1 , . . . Vi,K . Variable Vi,k set implies vertex vi
lowest-index
V vertexcolored color k. enforced following CNF constraints:
i1
Vi,k
j=1 Vj,k . Also, exactly one Vi,j variable must true every color used.
W
Therefore, add constraints: k ni=1 Vi,k , 1 k K, yk variables
indicate color k used, n = |V | Section 2. Finally,
W following
CNF
n
clause added Vi,k ensure lowest-index ordering: V i,k
j=i+1 Vj,k1 , Since
LI ordering completely breaks symmetries independent sets, subsumes earlier
constructions. However, come added cost. NU CA constructions
required new variables K 1 constraints, LI construction requires nK new
variables additional 2nK CNF clauses, almost double size original
formula.
LI construction proved correct means CA construction.
Given optimal assignment colors independent sets, sort independent sets
order lowest-index vertex assign colors 1 K accordingly, without affecting
correctness.
Figure 1 (e) illustrates effect LI SBPs example Figure 1 (a). graph
left, shown correct CA predicates Figure 1 (d) incorrect
LI construction, lowest-index vertex color 2 (V 3 )
higher index lowest-index vertex color 3, V 2 . graph right
shows correct assignment, LI predicates permissible assignment
partition {{V3 }, {V2 }, {V1 , V4 }}.
addition complex, LI predicates rigid obscure symmetries original instance. example, Figure 1 (a), easily seen vertices
V1 V2 symmetric permuted effect resulting graph.
symmetry instance-dependent - decided way V 1 V2 connected. Without addition SBPs, apparent legal coloring graph,
colors given V1 V2 swapped regardless V3 V4 colored. NU
predicates preserve symmetry, since concerned null colors
definition could used V1 V2 . CA predicates also preserve symmetry
since V1 V2 interchangeably used independent set, swapping
sets would effect cardinality sets. However,
LI predicates, independent set containing V 1 must always given higher-numbered
color set containing V2 , two cannot interchanged. V 1 given
color highest color use, would exist independent set whose
color index 1 greater color assigned V 1 , set, lowest-index
predicate would satisfied. Thus, LI predicates actually destroy vertex permutations graph. seen empirical results Section 4, addition
307

fiRamani, Aloul, Markov, & Sakallah

LI SBPs leaves symmetries benchmarks. unusual ordinarily
benchmarks reasonable size would contain least vertex permutations.
3.4 Selective Coloring (SC)
noticeable ILP formulation constraints complex
complete SBPs, LI predicates above, introduce several additional variables
clauses. raises question whether complex construction actually
counterproductive - may break symmetries, require much effort search
benefit complete symmetry breaking lost. investigate this, also propose
simple heuristic construction break symmetries vertices adding
almost additional constraints. impact many vertices possible, find vertex
vl largest degree vertices graph. color v l color 1.
achieved simply adding unary clause x l,1 . search vl neighbors find
vertex vl0 highest degree vertices adjacent v l . color vl0 color
2, adding unary clause xl0 ,2 . construction effect simplifying color
assignment vertices adjacent v l vl0 . vertex adjacent vl colored
color 1, vertex adjacent vl0 colored color 2. Moreover, vertices
independent set vl (vl0 ) must colored color 1 (color 2). v l vl0 sufficiently
large degree, construction restrict many vertex assignments. even stronger
construction would find triangular clique fix colors three vertices it;
however, clique finding complicated graphs may possess cliques.
refer construction selective coloring.
extent selective coloring breaks symmetries instance-dependent. fails
completely break symmetries almost graphs. However, simple construction,
adding two constraints unary clauses. easily resolved pre-processing
SAT solvers, symmetry breaking achieved construction virtually
overhead.
note instance-independent predicates defined concerned
symmetries colors, exist instance graph coloring. However, additional instance-independent symmetries may introduced reduction graph
coloring certain applications. example, radio frequency assignment application Section 2, adding possible bipartite edges cliques adjacent regions
result symmetries vertices cliques. Additional predicates
added instances application break symmetries.

4. Empirical Results
section describes experimental setup, empirical results, performance compared
related work.
4.1 Experimental Setup
used 20 medium-sized instances DIMACS graph coloring benchmark suite.
briefly describe family benchmarks used below.
308

fiBreaking Instance-Independent Symmetries Exact Graph Coloring

Random graphs. Benchmarks randomly created connections vertices,
named DSJ
Book graphs. Edges represent interaction characters book.
four benchmarks: anna, david, huck, jean
Mileage graphs. represent distances cities map, named
miles
Football game graphs. Indicate relationships teams must play
college football games. tables referred games
nqueens graphs. Instances nqueens problem, named queen
Register allocation graphs. Represent register allocation problem different
systems. use two families work, named mulsol, zeroin
Mycielski graphs. Instances triangle-free graphs based Mycielski (Mycielski, 1955) transformation, called myciel
Table 1 gives name, size (number vertices edges) chromatic number
benchmark. use maximum value K = 20 Kcoloring. benchmarks
chromatic number > 20, report chromatic number.
problem formulation fixed K application-driven. Indeed, many domains useful find exact chromatic number well-known
threshold. example, graph coloring instances register allocation, cannot
colors processor registers. PC processors often 32 registers, high-end
CPUs may more. However, realistic graphs relatively sparse low chromatic numbers. hand, processors embedded cellular phones, automobiles
point-of-sale terminals may registers, leading tighter constraints
acceptable chromatic numbers. value K = 20 used experiments way
special, results achieved representative results. Also,
apply K = 20 bound instances study trends, reasonable bounds
determined per-instance basis using following simple procedure.
1. Apply heuristic min-coloring determine feasible upper bound
2. value relatively small, perform linear search incrementally tightening
color constraint, otherwise perform binary search
Benchmark graphs transformed instances 0-1 ILP using conversion described Section 2. solve instances 0-1 ILP, used academic 0-1 ILP solvers
PBS (Aloul et al., 2002), Galena (Chai & Kuehlmann, 2003), Pueblo (Sheini, 2004),
also commercial ILP solver CPLEX version 7.0. Pueblo recent PBS
Galena, incorporates Pseudo-Boolean (PB) learning based ILP cutting-plane
techniques. use later version PBS, PBS II, enhances original PBS algorithms (Aloul et al., 2002) learning techniques Pueblo solver (Sheini, 2004).
include results original version PBS reported (Ramani,
309

fiRamani, Aloul, Markov, & Sakallah

Instance
anna
david
DSJC125.1
DSJC125.9
games120
huck
jean
miles250
mulsol.i.2
mulsol.i.4
myciel3
myciel4
myciel5
queen5 5
queen6 6
queen7 7
queen8 12
zeroin.i.1
zeroin.i.2
zeroin.i.3

#V
138
87
125
125
120
74
80
128
188
185
11
23
47
25
36
49
96
211
211
206

#E
986
812
1472
13922
1276
602
508
774
3885
3946
20
71
236
320
580
952
2736
4100
3541
3540

K
11
11
5
> 20
9
11
10
8
>20
>20
4
5
6
5
7
7
12
>20
>20
>20

Table 1: DIMACS graph coloring benchmarks

Aloul, Markov, & Sakallah, 2004), since retired newer version. However,
Appendix report detailed results n queens instances using older version
PBS along results solvers, sake detailed study. PBS II
implemented C++ compiled using g++. Galena Pueblo binaries provided
authors. PBS run using variable state independent decaying sum (VSIDS)
decision heuristic option (Moskewicz et al., 2001). Galena run using default options
linear search cardinality reduction (CARD) learning. experiments run
Sun-Blade-1000 workstations 2GB RAM, CPUs clocked 750MHz Solaris
operating system. Time-out limits solvers set 1000 seconds.
use symmetry breaking flow first proposed earlier work (Aloul et al.,
2004) detect break symmetries original ILP formulation Section 2.
flow uses tool Shatter (Aloul et al., 2003), uses Saucy (Darga et al., 2004)
graph automorphism program efficient SBP construction (Aloul et al., 2003).
also check unbroken symmetries formulations produced instanceindependent constructions described Section 3. runtimes symmetry detection
solving reduced 0-1 ILP problems reported next section.
4.2 Runtimes Symmetry Detection 0-1 ILP Solving
Table 2 shows symmetry detection results runtimes. numbers reported table
sums individual results 20 benchmarks used. report statistics sums
reporting results SBPs benchmarks would space-consuming,
310

fiBreaking Instance-Independent Symmetries Exact Graph Coloring

SBP
Type
SBPs
NU
CA
LI
SC
NU+SC

#V
437K
437K
437K
870K
437K
437K

CNF Stats
#CL
777505
777885
777505
4019980
777545
777925

# PB
3193
3193
3630
3193
3193
3193

Sym. Stats (SAUCY)
#S
#G Time
1.1e+168 994
185
5.0e+149 614
49
5.0e+149 614
49
2.0e+01
0
84
3.0e+164 941
167
5.0e+148 597
47

Table 2: CNF formula sizes, symmetry detection results
runtimes, totaled 20 benchmarks
Table 1, K = 20. NU = null-color elimination; CA = cardinality-based; LI = lowest-index;
SC = selective coloring. LI SBPs, one instance do-nothing symmetry counted
case, giving total 20 symmetries
0 generators. Saucy run Intel Xeon dual
processor 2 GHz running RedHat Linux 9.0.

would also illustrate trends clearly. work concerned characterizing
broad impact symmetry breaking. However, show detailed results queens
instances Appendix.
first column table indicates type construction: use SBPs
basic formulation, NU null-color elimination, CA cardinality-based ordering,
LI lowest-index ordering, SC selective coloring (the last row shows NU SC
combination). next three columns show number variables, CNF clauses,
PB constraints problems. last three columns show number symmetries,
number symmetry generators, symmetry detection runtimes Saucy. Henceforth,
refer instance-dependent SBPs external, added instance symmetries detected part problem formulation. top
row separated bottom 5 rows represents statistics without instanceindependent SBPs. observe adding instance-independent SBPs problem
formulation cut symmetry detection runtime considerably. Saucy total runtime 185 seconds instance-independent SBPs added, runtimes
NU, CA, LI NU + SC constructions much smaller. SC construction
comparable runtime heuristic breaks symmetries.
columns showing numbers symmetries generators support observation: NU,
CA, LI NU + SC constructions far fewer symmetries top row,
SC construction almost number. benchmarks, LI construction,
breaks symmetries, even instance-dependent vertex permutations may exist
graph. Saucy reports finding symmetries construction (except one instance
do-nothing symmetry graph, trivial). However, Saucy runtimes
construction larger NU, CA NU + SC constructions (85 seconds
approximately 49 seconds) even though symmetries instances LI
311

fiRamani, Aloul, Markov, & Sakallah

SBP
Type
SBPs
NU
CA
LI
SC
NU+SC

PBS II, PB Learning
Orig.
w/i.-d. SBPs
Tm.
#S
Tm.
#S
17K
8.2K
13K
15K
14K
6.9K

3
13
6
6
6
14

4.2K
7.5K
12K
15K
65
6.8K

16
13
8
6
20
14

CPLEX
Orig.
w/i.-d. SBPs
Tm.
#S
Tm.
#S
6.3K
5.9K
11K
16K
5.3K
4.5K

14
15
11
4
15
16

13K
6.5K
11K
16K
12K
6.4K

7
15
10
4
8
14

Galena
Orig.
w/i.-d. SBPs
Tm.
#S
Tm.
#S
1.7K
8.3K
19K
15K
16K
6.1K

2
11
1
5
4
14

3K
6.7K
17K
15K
94.4
6.1K

17
11
3
5
20
14

Pueblo
Orig.
w/i.-d. SBPs
Tm.
#S
Tm.
#S
18K
9.1K
9K
16K
15k
7.3K

3
12
12
5
5
13

1.6K
8.3K
10K
16K
2.1K
7.1K

Table 3: Runtimes number solutions found SBPs added
constructions using PBS II (with PB learning), CPLEX, Galena
Pueblo; experiments run SunBlade 1000 workstations. Timeouts
solvers set 1000s. maximum color limit set 20,
instances k > 20 unsatisfiable formulations.
comparison solvers. solve ILP formulations equal optimal
values using different solvers weed solver-specific issues. Best results
given solver shown boldface. entries, K denotes multiples
1000s seconds rounded nearest integer.

predicates added. likely reason sharp increase instance size caused
LI construction. general, SC construction little effect number
symmetries - used itself, leaves symmetries intact, used
NU construction, improvement NU construction alone small.
Table 3 shows effect symmetry breaking runtimes PBS II (Aloul et al., 2002),
CPLEX (ILOG, 2000), Galena (Chai & Kuehlmann, 2003) Pueblo (Sheini, 2004).
first column table specifies construction type, followed total runtime
solver (with without addition instance-independent SBPs) number
instances solved construction. solver, best performance among
configurations (largest number instances solved corresponding runtime) boldfaced.
Results given first new version PBS, PBS II based (Sheini, 2004), followed
CPLEX, Galena Pueblo. Runtimes older version PBS obtained
earlier work (Ramani et al., 2004). compare performance individual solver
different constructions, observe runtime solution entries different rows
column, compare performance different solvers constructions,
observe numbers row across columns. observe following trends.
1. benchmarks possess large number symmetries. Different instance-independent
SBPs achieve varying degrees completeness: lowest-index ordering (LI) breaks
symmetries benchmarks used, selective coloring (SC) SBP breaks
fewest symmetries. Saucy runtimes residual symmetry detection
addition instance-independent SBPs highest SBPs construction
SC construction, since possess largest numbers symmetries
2. case SBPs kind added, CPLEX performs well, solving
14 20 instances within time limit. However, PBS II, Galena Pueblo
perform poorly - Galena solves 2 instances PBS II Pueblo solve 3
312

19
13
12
5
18
13

fiBreaking Instance-Independent Symmetries Exact Graph Coloring

3. PBS II, Galena Pueblo benefit considerably instance-dependent symmetry
breaking. instance-dependent SBPs used without instanceindependent constructions propose, PBS II solves 16 instances within time
limit, Galena Pueblo solve 17 19 instances respectively. However,
CPLEX hampered addition instance-dependent SBPs, solves 7
instances case
4. Adding instance-independent SBPs improves performance specialized 01 ILP solvers no-SBP version. best performance PBS II, Galena
Pueblo seen NU + SC construction - PBS II Galena solve 14
instances, Pueblo solves 13. CPLEX, NU + SC construction shows
marginal improvement no-SBPs case (16 instances solved),
complex constructions, CA LI, actually undermine performance - CPLEX solves
4 instances LI construction. general, complex SBP constructions
perform much worse simple ones. PBS II, Pueblo Galena also perform
poorly CA LI constructions - Galena solves 1 instance CA
construction help instance-dependent SBPs, instances
solved LI construction solver
5. Adding instance-independent SBPs alone solve many instances adding
instance-dependent SBPs SBP-free formulation. best performance seen
instance-independent SBPs 14 instances solved, Galena PBS II,
16 instances solved CPLEX, NU + SC construction. instancedependent SBPs added PBS II Galena solve 20 instances SC
construction. CA LI constructions leave (or none all) symmetries
broken instance-dependent SBPs. Consequently, almost difference
results without instance-dependent SBPs constructions. However, achieve performance improvements instance-dependent
SBPs, due size complexity
6. Using instance-dependent SBPs conjunction SC construction useful.
combination, PBS II Galena solve 20 instances within time
limit, Pueblo solves 18. Runtime also considerably improved PBS II
Galena PBS II solves 20 instances total 65 seconds, Galena 94.4
seconds. best overall performance, terms number solutions runtime,
seen combination. general, however, SC construction dominant
own. Results SC construction alone similar results
SBPs, results NU + SC combination similar achieved
using NU SBPs. SC construction effective boosting performance
constructions
7. three specialized 0-1 ILP solvers - PBS II, Galena Pueblo, exhibit
performance trends respect constructions used, performances
comparable, terms number solutions found runtime
indicates variations performance due different SBPs, due
differing solver implementations. solvers independent implementations based
313

fiRamani, Aloul, Markov, & Sakallah

SBP
Type

PBS II, PB Learning
Orig.
w/i.-d. SBPs
Tm.
#S
Tm.
#S

SBPs
NU
CA
LI
SC
NU + SC

18K
9.2K
13K
15K
15K
7.1K

2
12
7
5
5
13

6.2K
7.9K
13K
15K
5.3K
7.0K

14
13
9
5
15
13

CPLEX
Orig.
w/i.-d. SBPs
Tm.
#S
Tm.
#S
11K
11K
13K
19K
10K
9.7K

9
9
9
2
10
11

8.2K
12K
14K
19K
12K
9.9K

12
8
8
2
9
11

Galena
Orig.
w/i.-d. SBPs
Tm.
#S
Tm.
#S
19K
10K
19K
16K
16K
9.2K

1
10
1
5
4
12

9.1K
7.6K
17K
16K
5.3K
6.9K

11
13
4
5
15
14

Pueblo
Orig.
w/i.-d. SBPs
Tm.
#S
Tm.
#S
19K
11K
11K
17K
16K
8.0K

1
11
11
3
4
13

7.5K
9.5K
13K
17K
6.0K
7.4K

Table 4: Total runtimes number solutions found SBPs
added constructions using PBS II (with PB learning), CPLEX,
Galena Pueblo. experimental setup used
Table 3 color limit K = 30. Best results solver boldfaced. Fewer instances solved Table 3 higher color
limit results larger potentially difficult instances.

algorithmic framework (the Davis-Logemann-Loveland backtrack search
procedure), PBS II Galena also learning capabilities
8. Adding instance-dependent SBPs construction usually adversely affects
performance CPLEX. previously noted work (Aloul et al.,
2004). Since CPLEX algorithms implementation available
public domain, difficult account effect. However, PBS Galena
symmetry breaking significantly outperform CPLEX without symmetry breaking
9. report results sum runtimes instances illustrate trends.
per-instance basis, trends displayed. example, no-SBPs
case top row, PBS II solves 3 instances Galena solves 2, two
instances solved Galena among solved PBS II. general,
instances tend easy difficult 0-1 ILP solvers, although CPLEX
behaves differently. example behavior queens family instances
illustrated Appendix
Overall, results suggest graph coloring, adding instance-independent SBPs alone
competitive use instance-dependent SBPs alone. best results
achieved using combination types, even here, instance-independent SBPs
used simple variety. true even symmetry detection runtimes
taken consideration. attribute result complexity instance-independent
SBPs use, also fact improvements graph automorphism software
(Darga et al., 2004) greatly reduced overhead detecting symmetries reduction
graph automorphism. Previously, static approaches require symmetries
detected broken advance, task symmetry detection often bottleneck
could actually take longer search itself. bottleneck removed,
advantages static symmetry breaking - simple predicates address specific symmetries
rather complex constructions alter problem specification considerably -
clearly illustrated. Even among instance-independent predicates, simple constructions
effective complex ones. noted Section 3, simple constructions
314

13
11
8
3
15
13

fiBreaking Instance-Independent Symmetries Exact Graph Coloring

like NU SC add additional constraints alter original problem
greatly. However, CA LI constructions add many constraints, may
confuse specialized 0-1 ILP solvers.
important note color permutations, instance-independent, appear instance-specific level. Thus, symmetries targeted instance-independent
predicates subset targeted instance-dependent predicates. instanceindependent constructions intended cover different set symmetries, rather
break symmetries problem formulation, thus reducing eliminating overhead instance-dependent methods may follow. fact
strategy successful suggests that, set symmetries, instancedependent predicates use efficient easier solvers tackle.
verify claims performance trends, show results additional set
experiments increased color limit K = 30 Table 4. instances re-formulated
K = 30 different SBP constructions. experiment intended verify
trends K = 20 case, investigate whether instances chromatic number
> 20, unsatisfiable first case, colored 30 colors. Results
Table 4 validate observations Table 3 best results PBS II, Galena
Pueblo achieved NU + SC (with instance-dependent SBPs)
SC (with instance-dependent SBPs) constructions. However, formulation fewer
instances solved K = 20 case, possibly K = 30 limit results
larger instances. Also, instances whose chromatic number much closer 30
20, may harder prove optimality, whereas proving unsatisfiability K = 20
experiments may simpler.
4.3 Comparison Related Work
Here, discuss empirical performance approach compared related
work (Coudert, 1997; Benhamou, 2004). note cited works describe algorithms
specifically developed graph coloring, search procedures cannot used solve
problems. approach, hand, solves hard problems reduction
generic problems SAT 0-1 ILP, work graph coloring viewed
case study. Consequently, use problem-specific knowledge actual problem
formulation (instance-independent SBPs also added reduction),
search itself. may useful applications problem-specific solvers cannot
developed acquired due limited resources. goal determine whether symmetry
breaking improve performance reduction-based methods, traditionally
competitive problem-specific methods. Thus, techniques may
superior problem-specific solvers instances, hope show reasonably strong
performance broad spectrum instances.
Common data points work Couderts (Coudert, 1997) include instances
queens, myciel DSCJ125.1. Referring detailed results queens instances
Appendix, note runtimes competitive Couderts algorithm
- example, queen5 5, algorithms runtime 0.01s. larger instances,
however, runtimes somewhat slower. myciel instances, obtain best
results Pueblo solver SC predicates, runtimes 0.01, 0.06, 1.80s
315

fiRamani, Aloul, Markov, & Sakallah

myciel3, 4, 5, compared 0.01, 0.02 4.17 Couderts algorithm. Therefore,
appears approach competitive common data points. Moreover,
studies (Kirovski & Potkonjak, 1998) observed Couderts work provide
results several hard real-world problem classes, particularly modeling results
dense graphs. work general, cannot biased favor certain types
graphs.
algorithm described Benhamou (Benhamou, 2004) shows competitive runtimes number DIMACS benchmarks, particularly instances register allocation.
example, DSJC125.1 instance solved Benhamous algorithm 0.01 seconds,
best time achieved us 1.12 seconds, using Pueblo solver
instance-dependent SBPs. However, note Benhamous algorithm determines
upper limit chromatic number K using instance-specific knowledge, example, DSCJ125.1, set K = 5. solve instances K = 20, may
large limit cases. value K affects size resulting 0-1 ILP
reductions SBPs, likely affect runtime. also note DIMACS
benchmarks used cited work (Benhamou, 2004) primarily register allocation
randomly generated instances, whereas achieve reasonably good performance wide
variety benchmark applications. Moreover, Benhamous approach relies modeling
graph coloring not-equals CSP, bode well generality. Many CSPs
cannot modeled using not-equals constraints. Additionally, symmetry detection, breaking search procedures described work specific graph coloring,
whereas work extended several problems, requiring reduction
SAT/0-1 ILP.

5. Conclusions
work shows problem reduction 0-1 ILP viable method optimally solving
combinatorial problems without investing specialized solvers. approach likely
even successful efficiency 0-1 ILP solvers improves future,
able better handle problem structure. particular, problem reductions may
produce highly-structured instances making ability automatically detect exploit
structure important. case graph coloring demonstrate generic,
publicly-available symmetry breaking flow earlier work (Aloul et al., 2004) significantly improves empirical results conjunction academic 0-1 ILP solvers PBS II,
new version solver PBS (Aloul et al., 2002), Galena (Chai & Kuehlmann, 2003)
Pueblo (Sheini, 2004). specialized 0-1 ILP solvers significantly outperform commercial generic ILP solver CPLEX 7.0 symmetry-breaking used. performance
CPLEX actually deteriorates SBPs added, original instances
SBPs, CPLEX able solve instances 0-1 ILP solvers. However, best
performance overall obtained 0-1 ILP solvers instances SBPs added.
Although techniques tested standard DIMACS benchmarks instances, note
symmetry-breaking flow described applied graph coloring instances
application.
particularly interested comparing strategies breaking symmetries
present every ILP instance produced problem reduction (instance-independent sym316

fiBreaking Instance-Independent Symmetries Exact Graph Coloring

metries). symmetries may known even first instances original
problem delivered (i.e., symmetries may detected specification level),
one option use problem reduction. Intuitively, may prevent
discovering symmetries every instance thus improve overall CPU time.
end, propose four constructions instance-independent symmetry breaking predicates (SBPs). constructions vary terms strength completeness. goal
experiments compare performance four instance-independent SBP constructions relative other, well assess performance compared
instance-dependent SBPs. Instance-independent SBPs advantage requiring
additional step symmetry detection, since part problem specification.
Additionally, designed information problem itself,
effect solutions clear - example, know null-color elimination force
lower-numbered colors used solution. Instance-dependent SBPs detected
added automatically 0-1 ILP reduction instance without understanding
significance. hand, instance-dependent constructions less complex result compact predicates. empirical data indicate simplicity
construction powerful factor determining performance - instance-dependent
SBPs consistently outperform instance-independent SBPs, complete complex instance-independent constructions (LI) actually weakest performance.
clear results symmetry breaking useful graph coloring: adding
instance-dependent SBPs always speeds search no-SBPs case. likely
instance-independent SBPs less successful due complex construction. Simpler
instance-independent constructions (NU, SC) outperform complex ones (CA, LI).
well known syntactic structure CNF PB constraints may dramatically
affect efficiency SAT ILP solvers. Shorter clauses PB constraints much
preferable easier resolve constraints, useful
learning strategies employed exact SAT solvers. Another factor gives instancedependent SBPs advantage ease symmetry detection, previously
bottleneck. Due improved software (Darga et al., 2004), overhead symmetry detection via reduction graph automorphism SAT/0-1 ILP instances almost negligible.
also show three specialized 0-1 ILP solvers, PBS II, Galena Pueblo,
exhibit similar performance trends different constructions. indicates performance decided solver-specific issues, difficulty instances
SBPs added them. CPLEX display behavior solvers,
fact slowed addition instance-dependent SBPs several
instance-independent constructions. CPLEX commercial solver generic ILP problems, algorithms decision heuristics likely different
used academic solvers. However, since details CPLEX publicly available,
possible accurately explain behavior. note CPLEX
appear benefit symmetry breaking, performance reduced instances
SBPs kind superior 0-1 ILP solvers. However, SBPs added
specialized solvers solve instances CPLEX less time.
context generic search combinatorial optimization problems defined
NP-spec language (Cadoli et al., 1999), empirical data suggest new theoretical
breakthroughs required make use instance-independent symmetries problem
317

fiRamani, Aloul, Markov, & Sakallah

reductions SAT 0-1 ILP. current level understanding, simple strategy
processing instance-independent instance-dependent symmetries together produces
smallest runtimes graph coloring benchmarks. current future work focused
developing effective SBPs problem, also investigating utility
symmetry breaking hard search problems. Moreover, work uses instanceindependent predicates color symmetries, results analysis may broader
scope, example, applications radio frequency assignment (Section 2)
symmetries introduced reduction graph coloring likely
preserved future reductions. issues involved using instance-dependent vs.
instance-independent SBPs relevant applications.

6. Acknowledgments
work funded part NSF ITR Grant #0205288. Also, thank Donald Chai
Andreas Kuehlmann UC Berkeley providing us binaries Galena
solver, Hossein Sheini providing us binaries Pueblo.

Appendix A: Performance Analysis Queens Instances
section provides detailed discussion results individual benchmarks
queens family instances. problem posed queens instances whether
queens placed n chessboard without conflicts. instances use
experiments queens 5 5, 6 6, 7 7 8 12. Table 5 shows results
queens family. Results shown every instance SBPs, four
constructions NU, CA, LI SC, NU + SC combination. constructions
tested without instance-dependent SBPs before. report results
original version PBS, (Aloul et al., 2002), PBS II, CPLEX, Galena
Pueblo Section 4. Experiments run Sun Blade 1000 workstations before.
table, report solver runtime instance solved, T/O timeout 1000
seconds. best results solver particular instance boldfaced.
greater variation considering performance per-instance basis,
table largely reflects trends reported Section 4. example,
instance-dependent SBPs used, PBS, PBS II, Galena Pueblo largely perform
best NU + SC construction. instance-dependent SBPs added, best
performance seen SC construction cases. CPLEX display
behavior solvers, performance clearly deteriorates instancedependent SBPs added construction. similar effect observed related
work (Aloul et al., 2004). Results original version PBS (Aloul et al., 2002),
could included Section 4, added section. seen
PBS follows trends PBS II, Galena Pueblo, reinforcing claim
behavior solver-dependent.
318

fiBreaking Instance-Independent Symmetries Exact Graph Coloring

Inst.
Name

queen5 5

queen6 6

queen7 7

queen8 12

SBP
Type
SBPs
NU
CA
LI
SC
NU + SC
SBPs
NU
CA
LI
SC
NU + SC
SBPs
NU
CA
LI
SC
NU + SC
SBPs
NU
CA
LI
SC
NU + SC

PBS
Inst.-dep.
SBPs used?

Yes
T/O
0.19
1.84
T/O
T/O
T/O
135
134.71
15.99
0.19
8.63
12.34
T/O
3.61
331.63
521.12
T/O
T/O
T/O
T/O
T/O
0.58
2.89
1.72
T/O
36.56
0.45
3.29
T/O
T/O
T/O
T/O
T/O
8.42
5.65
38.07
T/O
1.31
T/O
T/O
T/O
T/O
T/O
T/O
T/O
1.05
T/O
T/O

PBS II
Inst.-dep.
SBPs used?

Yes
34.52
0.04
0.01
0.02
0.31
0.24
1.48
1.48
0.15
0.07
0
0.01
T/O
0.21
56.63
13.59
50.6
780.57
T/O
T/O
T/O
0.1
1.4
0.63
T/O
1.79
36.31
24.74
T/O
T/O
53.3
53.4
38.57
0.85
4.37
5.73
T/O
0.52
T/O
T/O
T/O
T/O
T/O
T/O
T/O
0.47
787.26 780.14

CPLEX
Inst.-dep.
SBPs used?

Yes
1.11
643.93
1.38
23.67
39.2
2.76
262.96
217.21
0.45
229.79
0.83
0.88
T/O
T/O
T/O
T/O
T/O
T/O
T/O
T/O
242.79
T/O
95.91
T/O
243.3
T/O
119.16 459.44
271.2
T/O
T/O
T/O
38.04
T/O
119.7
T/O
T/O
T/O
T/O
T/O
T/O
T/O
T/O
T/O
T/O
T/O
T/O
T/O

Galena
Inst.-dep.
SBPs used?

Yes
83.06
0.35
0.21
0.27
T/O
T/O
5.4
5.4
0.29
0.29
0.3
1
T/O
0.87
192.17
19.11
T/O
T/O
T/O
T/O
T/O
1.0
11.19
1.05
T/O
T/O
56.6
147.52
T/O
T/O
78.85
78.8
T/O
1.33
17.46
5.16
T/O
T/O
T/O
138.61
T/O
T/O
T/O
T/O
T/O
1.9
52.1
53.63

Pueblo
Inst.-dep.
SBPs used?

Yes
203.09
0.01
0.08
0.1
0.14
0.52
8.48
8.48
0.25
0.19
0.06
0.07
T/O
0.49
123.99
18.88
196.94
80.53
T/O
T/O
T/O
0.32
4.85
2.64
T/O
1.13
9.59
15.49
692.67
150.86
212.18
213.8
217.82
1.23
25.73
14.04
T/O
T/O
T/O
T/O
T/O
T/O
T/O
T/O
T/O
0.98
T/O
T/O

Table 5: Detailed results queens instances. instance, show results
solvers PBS, PBS II, CPLEX, Galena Pueblo. solvers
run SunBlade 1000 workstations. Instances tested
instance-independent SBPs, four proposed constructions
Section 3 combination NU SC constructions.
instance-independent SBPs tested alone instance-dependent
SBPs added. table shows runtime given instance different construction. T/O indicates timeout 1000 seconds. Best results
given solver instance shown boldface.

References
Aloul, F. A., Markov, I. L., & Sakallah, K. A. (2003). Shatter: Efficient symmetry-breaking
boolean satisfiability. International Joint Conference Artificial Intelligence,
pp. 271282.
Aloul, F. A., Markov, I. L., & Sakallah, K. A. (2004). MINCE: static global variableordering heuristic sat search bdd manipulation. Journal Universal Computer
Science (JUCS), 10, 15621596.
Aloul, F. A., Ramani, A., Markov, I. L., & Sakallah, K. A. (2002). Generic ILP versus
specialized 0-1 ILP: update. International Conference Computer-Aided
Design, pp. 450457.
Aloul, F. A., Ramani, A., Markov, I. L., & Sakallah, K. A. (2003). Solving difficult instances
319

fiRamani, Aloul, Markov, & Sakallah

boolean satisfiability presence symmetry. IEEE Transactions CAD,
22, 11171137.
Aloul, F. A., Ramani, A., Markov, I. L., & Sakallah, K. A. (2004). Symmetry-breaking
pseudo-boolean formulas. Asia-Pacific Design Automation Conference, pp. 884
887.
Aragon, C. R., Johnson, D. S., McGeoch, L. A., & Schevon, C. (1991). Optimization
simulated annealing: experimental evaluation; part ii, graph coloring number
partitioning. Operations Research, 39, 378406.
Benhamou, B. (2004). Symmetry not-equals binary constraint networks. Workshop
Symmetry CSPs, pp. 28.
Brelaz, D. (1979). New methods color vertices graph. Communications ACM,
22, 251256.
Brown, R. J. (1972). Chromatic scheduling chromatic number problem. Management Science, 19, 451463.
Cadoli, M., & Mancini, T. (2003). Detecting breaking symmetries specifications.
Third Annual Workshop Symmetry Constraint Satisfaction Problems
(SymCon), pp. 1326.
Cadoli, M., Palopoli, L., Schaerf, A., & Vasileet, D. (1999). NP-SPEC: executable specification language solving problems NP. Practical Aspects Declarative
Languages, pp. 1630.
Chai, D., & Kuehlmann, A. (2003). fast pseudo-boolean constraint solver. Design
Automation Conference, pp. 830835.
Chaitin, G. J., Auslander, M., Chandra, A., Cocke, J., Hopkins, M., & Markstein, P. (1981).
Register allocation via coloring. Computer Languages, 6, 4757.
Chams, M., Hertz, A., & Werra, D. D. (1987). experiments simulated annealing
coloring graphs. European Journal Operations Research, 32, 260266.
Coudert, O. (1997). Coloring real-life graphs easy. Design Automation Conference,
pp. 121126.
Crawford, J. (1992). theoretical analysis reasoning symmetry first-order logic.
AAAI Workshop Tractable Reasoning Tenth National Conference
Artificial Intelligence.
Crawford, J., Ginsberg, M., Luks, E., & Roy, A. (1996). Symmetry-breaking predicates
search problems. 5th International Conference Principles Knowledge
Representation Reasoning, pp. 148159.
Culberson, J. (2004). Graph coloring page. http://web.cs.ualberta.ca/joe/Coloring/index.html.
Darga, P. T., Liffiton, M. H., Sakallah, K. A., & Markov, I. L. (2004). Exploiting structure
symmetry generation cnf. 41st Internation Design Automation Conference,
pp. 530534.
Davis, M., Logemann, G., & Loveland, D. (1962). machine program theorem proving.
Communications ACM, 5, 394397.
320

fiBreaking Instance-Independent Symmetries Exact Graph Coloring

Fahle, T., Schamberger, S., & Sellmann, M. (2001). Symmetry breaking. 7th International
Conference Principles Practice Constraint Programming, pp. 93107.
Feige, U., Goldwasser, S., Lovasz, L., Safra, S., & Szege, M. (1991). Approximating clique
almost NP-complete. IEEE Symposium Foundations Computer Science,
pp. 212.
Focacci, F., & Milano, M. (2001). Global cut framework removing symmetries.
Principles Practice Constraints Programming, pp. 7782.
Galinier, P., & Hao, J. (1999). Hybrid evolutionary algorithms graph coloring. Journal
Combinatorial Optimization, 3, 379397.
Garey, M. R., & Johnson, D. S. (1979). Computers Intractability: Guide Theory
NP-completeness. W. H. Freeman Company.
Gent, I. P. (2001). symmetry-breaking constraint indistinguishable values. Workshop Symmetry Constraint Satisfaction Problems.
Goldberg, E., & Novikov, Y. (2002). Berkmin: fast robust SAT-solver. Design
Automation Test Europe, pp. 142149.
Haldorsson, M. M. (1990). still better performance guarantee approximate graph
coloring..
Hentenryck, P. V., Agren, M., Flener, P., & Pearson, J. (2003). Tractable symmetry breaking
CSPs interchangeable values. International Joint Conference
Artificial Intelligence (IJCAI).
Hertz, A., & Werra, D. D. (1987). Using tabu search techniques graph coloring. Computing, 39, 345351.
Huang, J., & Darwiche, A. (2003). structure-based variable ordering heuristic SAT.
International Joint Conference Artificial Intelligence, pp. 11671172.
ILOG (2000). ILOG CPLEX ILP solver, version 7.0. http://www.ilog.com/products/cplex/.
J.-P. Hamiez, J.-K. H. (2001). Scatter search graph coloring. 5th European
Conference Artificial Evolution, pp. 168179.
Jagota, A. (1996). adaptive, multiple restarts neural network algorithm graph coloring. European Journal Operational Research, 93, 257270.
Kirovski, D., & Potkonjak, M. (1998). Efficient coloring large spectrum graph.
Design Automation Conference.
Krishnamurthy, B. (1985). Short proofs tricky formulas. Acta Informatica, 22, 327337.
Kubale, M., & Jackowski, B. (1985). generalized implicit enumeration algorithm graph
coloring. Communications ACM, 28, 412418.
Kubale, M., & Kusz, E. (1983). Computational experience implicit enumeration algorithms graph coloring. Proceedings WG83 International Workshop
Graph Theoretic Concepts Computer Science, pp. 167176.
Leighton, F. (1979). graph coloring algorithm large scheduling problems. Journal
Research National Bureau Standards, 84, 489506.
321

fiRamani, Aloul, Markov, & Sakallah

McKay, B. D. (1990). Nauty users guide (version 1.5). http://cs.anu.edu.au/bdm/nauty/.
Mehrotra, A., & Trick, M. A. (1996). column generation approach graph coloring.
INFORMS Journal Computing, 8, 344354.
Moskewicz, M., Madigan, C., Zhao, Y., Zhang, L., & Malik, S. (2001). Chaff: Engineering
efficient sat solver. Design Automation Conference, pp. 530535.
Mycielski, J. (1955). Sur le coloriage des graphs. Colloqium Mathematicum, 3, 161162.
Prestwich, S. (2002). Supersymmetric modelling local search. SymCon: Workshop
Symmetries CSPs, pp. 2128.
Puget, J. (2002). Symmetry breaking revisited. Principles Practice Constraints
Programming, pp. 446461.
Ramani, A., Aloul, F. A., Markov, I. L., & Sakallah, K. A. (2004). Breaking instanceindependent symmetries exact graph coloring. Design Automation Test
Europe, pp. 324329.
Roney-Dougal, C. M., Gent, I. P., Kelsey, T., & Linton, S. (2004). Tractable symmetry
breaking using restricted search trees. European Conference Artificial Intelligence, pp. 211215.
Sheini, H. (2004). Pueblo 0-1 ILP solver. http://www.eecs.umich.edu/hsheini/pueblo/.
Silva, J. P. M., & Sakallah, K. A. (1999). GRASP: new search algorithm satisfiability.
IEEE Transactions Computers, 48, 506521.
Trick, M. (1996). Network resources coloring graph.

http://mat.gsia.cmu.edu/COLOR/color.html

.

Walsh, T. (2001). Search high degree graphs. 17th International Joint Conference
Artificial Intelligence, pp. 266271.
Warners, J. P. (1998). linear-time transformation linear inequalities conjunctive
normal form. Information Processing Letters, 68, 6369.
Welsh, D. J. A., & Powell, M. B. (1967). upper bound chromatic number
graph application timetabling problems. Computer Journal, 10, 8586.
Werra, D. D. (1985). introduction timetabling. European Journal Operations
Research, 19, 151162.

322

fiJournal Artificial Intelligence Research 26 (2006) 127-151

Submitted 8/05; published 6/06

Admissible Restrained Revision
Richard Booth

richard.b@msu.ac.th

Faculty Informatics
Mahasarakham University
Mahasarakham 44150, Thailand

Thomas Meyer

Thomas.Meyer@nicta.com.au

National ICT Australia
University New South Wales
223 Anzac Parade
Kensington, NSW 2052, Australia

Abstract
partial justification framework iterated belief revision Darwiche
Pearl convincingly argued Boutiliers natural revision provided prototypical
revision operator fits scheme. show Darwiche-Pearl arguments
lead naturally acceptance smaller class operators refer admissible. Admissible revision ensures penultimate input ignored completely,
thereby eliminating natural revision, includes Darwiche-Pearl operator, Nayaks
lexicographic revision operator, newly introduced operator called restrained revision.
demonstrate restrained revision conservative admissible revision
operators, effecting changes possible, lexicographic revision least conservative, point restrained revision also viewed composite operator,
consisting natural revision preceded application backwards revision operator previously studied Papini. Finally, propose establishment principled
approach choosing appropriate revision operator different contexts discuss
future work.

1. Introduction
ability rationally change ones knowledge base face new information
possibly contradicts currently held beliefs basic characteristic intelligent behaviour. Thus question belief revision crucial importance Artificial Intelligence.
last twenty years question received considerable attention, starting
work Alchourron, Gardenfors, Makinson (1985) usually abbreviated AGM
proposed set rationality postulates reasonable revision operator
satisfy. semantic construction revision operators later provided Katsuno
Mendelzon (1991), according agent mind plausibility ordering
total preorder set possible worlds, knowledge base associated
ordering identified set sentences true plausible worlds.
approach dates back work Lewis (1973) counterfactuals. introduced
belief revision literature Grove (1988) Spohn (1988). Given new sentence
epistemic input , revised knowledge base set set sentences true
plausible worlds holds. shown Katsuno Mendelzon (1991),
family operators defined construction coincides exactly family opc
2006
AI Access Foundation Morgan Kaufmann Publishers. rights reserved.

fiBooth & Meyer

erators satisfying AGM postulates. Due intuitive appeal, construction came
widely used area. However, researchers soon began notice deficiency
although prescribes obtain new knowledge base, remains silent
obtain new plausibility ordering serve target next epistemic
input. Thus rich enough deal adequately problem iterated belief
revision. paper contribution study problem.
iterated revision schemes sensitive history belief changes1 , based
version recent best argument, newest information higher
priority anything else knowledge base. Arguably extreme case
Nayaks lexicographic revision (Nayak, 1994; Nayak, Pagnucco, & Peppas, 2003). However,
operators where, admitted knowledge base, rapidly becomes
much candidate removal anything else set another, newer, piece
information comes along, Boutiliers natural revision (1993, 1996) case point.
dual Rott (2003) terms radical revision new information
accepted maximal, irremediable entrenchment see also Segerberg (1998). Another
issue consider problem termed temporal incoherence (Rott, 2003):
comparative recency information translate systematically comparative importance, strength entrenchment
influential paper Darwiche Pearl (1997) proposed framework iterated
revision. proposal characterised terms sets syntactic semantic postulates, also viewed perspective conditional beliefs. extension
formulation Katsuno Mendelzon (1991) AGM revision (Alchourron et al.,
1985). justify proposal Darwiche Pearl mount comprehensive argument.
argument includes critique natural revision, shown admit changes.
addition, provide concrete revision operator shown satisfy postulates. many ways seen prototypical Darwiche-Pearl operator.
instructive observe two best-known operators satisfying Darwiche-Pearl
postulates, natural revision lexicographic revision, form opposite extremes
Darwiche-Pearl framework: Natural revision conservative Darwiche-Pearl operator, sense effects changes possible, lexicographic revision
least conservative.
paper show Darwiche-Pearl arguments lead naturally acceptance smaller class operators refer admissible. provide characterisations admissible revision, terms syntactic well semantic postulates.
Admissible revision ensures penultimate input ignored completely. consequence natural revision eliminated. hand, admissible revision
includes prototypical Darwiche-Pearl operator well lexicographic revision, latter result also showing lexicographic revision least conservative admissible
operators. removal natural revision scene leaves gap filled
introduction new operator refer restrained revision. conservative
admissible revision operators, thus seen appropriate replacement
natural revision. give syntactic semantic characterisation restrained revision,
1. external revision scheme like Areces Becher (2001) Freund Lehmann (1994)
not.

128

fiAdmissible Restrained Revision

demonstrate satisfies desirable properties. particular, unlike lexicographic
revision, ensures older information discarded unnecessarily, shows
problem temporal incoherence dealt with.
Although natural revision feature class admissible revision operators,
show still role play iterated revision, provided first tempered
appropriately. show restrained revision also viewed composite operator,
consisting natural revision preceded application backwards revision operator
previously studied Papini (2001).
paper organised follows. outlining notation, review DarwichePearl framework Section 2. followed discussion admissible revision
Section 3. Section 4 introduce restrained revision, Section 5 show
defined composite operator. Section 6 discusses possibility enriching
epistemic states way determining appropriate admissible revision operator
particular context. section also conclude briefly discuss future work.
1.1 Notation
assume finitely generated propositional language L includes constants >
, closed usual propositional connectives, equipped classical
model-theoretic semantics. V set valuations L [] (or [B]) set
models L (or B L). Classical entailment denoted logical equivalence
. also use Cn denote operation closure classical entailment. Greek
letters , , . . . stand arbitrary sentences. examples sometimes use lower
case letters p, q, r propositional atoms, sequences 0s 1s denote
valuations language. example, 01 denotes valuation, language generated
p q, p assigned value 0 q value 1, 011 denotes
valuation, language generated p, q r, p assigned value 0
q r value 1. Whenever use term knowledge base always mean
set sentences X deductively closed, i.e., X = Cn(X).

2. Darwiche-Pearl Revision
Darwiche Pearl (1997) reformulated AGM postulates (Alchourron et al., 1985)
compatible suggested approach iterated revision. necessitated move
knowledge bases epistemic states. epistemic state contains, addition
knowledge base, information needed coherent reasoning including, particular,
strategy belief revision agent wishes employ given time. Darwiche
Pearl consider epistemic states abstract entities, provide single formal
representation. thus possible talk two epistemic states E F identical
(denoted E = F) , yet syntactically different.2 borne mind below,
particularly considering postulate (E 5). Darwiche Pearls reformulated
postulates belief change operator epistemic states, knowledge bases.
denote B(E) knowledge base extracted epistemic state E.
(E1) B(E ) = Cn(B(E ))
2. Personal communication Adnan Darwiche.

129

fiBooth & Meyer

(E2) B(E )
(E3) B(E ) B(E) +
(E4)
/ B(E) B(E) + B(E )
(E5) E = F B(E ) = B(F )
(E6) B(E ) iff
(E7) B(E ( )) B(E ) +
(E8)
/ B(E ) B(E ) + B(E ( ))
Darwiche Pearl show, via representation result similar Katsuno
Mendelzon (1991), revision epistemic states represented terms plausibility orderings associated epistemic states.3 specifically, every epistemic state
E associated total preorder E valuations, elements lower
ordering deemed plausible. Moreover, two epistemic states E F
identical (but may syntactically different), case E =F .
Let min(, E ) denote minimal models E . knowledge base associated epistemic state obtained considering minimal models E i.e.,
[B(E)] = min(>, E ). Observe means B(E) consistent.
requirement enables us obtain unique knowledge base total preorder E .
Preservation results paper requirement relaxed possible,
technically messy.
observant reader note assumption consistent B(E) incompatible
successful revision . requires jettison (E6) insist consistent
epistemic inputs only. (The left-to-right direction (E6) rendered superfluous (E1)
assumption knowledge bases extracted epistemic states
consistent.) difference original AGM postulates DarwichePearl reformulation first inspired critical observation Freund Lehmann (1994)
occurs (E5), states revising logically equivalent sentences results
epistemic states identical associated knowledge bases. weakening
original AGM postulate, phrased notation follows:
(B5) B(E) = B(F) B(E ) = B(F )
(B5) states two epistemic states identical associated knowledge bases will,
revised equivalent inputs, produce two epistemic states identical associated knowledge bases. stronger (E5) requires equivalent associated
knowledge bases original epistemic states identical. shall refer
reformulated AGM postulates, (E6) removed, DP-AGM.
DP-AGM guarantees unique extracted knowledge base revision performed. sets [B(E )] equal min(, E ) thereby fixes plausible valuations E . However, places restriction rest ordering. purpose
3. Alternative frameworks studying iterated revision, based using sequences sentences rather
plausibility orderings, Lehmann (1995) Konieczny Pino-Perez (2000).

130

fiAdmissible Restrained Revision

Darwiche-Pearl framework constrain remaining part new ordering.
done way set postulates iterated revision (Darwiche & Pearl, 1997).
(Throughout paper follow convention left associative.)
(C1) B(E ) = B(E )
(C2) B(E ) = B(E )
(C3) B(E ) B(E )
(C4)
/ B(E )
/ B(E )
postulate (C1) states two pieces informationone specific
otherarrive, first made redundant second. (C2) says two
contradictory epistemic inputs arrive, second one prevails; second evidence alone
yields knowledge base. (C3) says piece evidence retained
accommodating recent evidence entails given current knowledge
base. (C4) simply says epistemic input act defeater. shall refer
class belief revision operators satisfying DP-AGM (C1) (C4) DP-revision.
following corresponding semantic versions (with v, w V ):
(CR1) v [], w [] v E w iff v E w
(CR2) v [], w [] v E w iff v E w
(CR3) v [], w [] v E w v E w
(CR4) v [], w [] v E w v E w
(CR1) states relative ordering -worlds remain unchanged following revision, (CR2) requires -worlds. (CR3) requires that, -world
strictly plausible -world, relationship retained -revision,
(CR4) requires weak plausibility. Darwiche Pearl showed that, given
DP-AGM, precise correspondence obtains (Ci) (CRi) (i = 1, 2, 3, 4).
One guiding principles belief revision principle minimal change: changes
belief state ought kept minimum. always clear ought
minimised. AGM theory prevailing wisdom minimal change refers sets
sentences corresponding knowledge bases. interpretations.
move knowledge bases epistemic states, minimal change defined terms
fewest possible changes associated plausibility ordering E . follows
frequently opportunity refer latter interpretation minimal change.
See also discussion principle Rott (2000).

3. Admissible Revision
section consider two best-known DP-operators, propose three postulates added Darwiche-Pearl framework. first correction
strengthening. show Darwiche-Pearl representation principle
irrelevance syntax weak suggest appropriate strengthened postulate.
131

fiBooth & Meyer

second suggested arguments advanced Darwiche Pearl themselves. eliminates one operators criticise, satisfied sole operator
provide instance framework. addition two postulates
Darwiche-Pearl framework leads definition class admissible revision
operators. Finally, point problem Nayaks well-known lexicographic revision
operator propose third postulate added. consequences insisting
addition third postulate discussed detail Section 4.
mentioned Section 2, Darwiche Pearl replaced original AGM postulate
(B 5) (E 5). attempts appropriate formulation principle
irrelevance syntax, popularised Dalal (1988). whereas (B 5)
shown strong, shown Darwiche Pearl (1997), closer inspection reveals
(E 5) weak. precise, fails adequate formulation syntax
irrelevance iterated revision. specifies revision two equivalent sentences
produce epistemic states identical associated knowledge bases, require
epistemic states, another revision two equivalent sentences, also
produce epistemic states identical associated knowledge bases. So, seen
following example, DP-AGM (and indeed, even (C1) (C4) added)
possible B(E ) differ B(E ) even equivalent
equivalent .
Example 1 Consider propositional language generated two atoms p q let
E epistemic state B(E) = Cn(p q). consider two epistemic states
E0 E00 B(E0 ) = B(E00 ) = Cn(p), 01 E0 00 00 E00 01. Observe
gives complete descriptions E0 E00 . tedious, difficult, verify
setting Ep =E0 Ep =E00 compatible DP-AGM. observe
B(E p p) = Cn(p q), B(E p p) = Cn(p q).
consequence this, propose (E5) replaced following postulate:
(E50 ) E = F, B(E ) = B(F )
semantic equivalent (E50 ) looks like this:
(ER50 ) E = F E =F
(ER50 ) states revision two identical epistemic states two equivalent sentences result epistemic states identical associated total preorders,
epistemic states identical associated knowledge bases.
Proposition 1 (E50 ) (ER50 ) equivalent, given DP-AGM.
Proof: proof (E50 ) follows (ER50 ) straightfoward. converse, suppose (ER50 ) hold; i.e. F 6=E . means
exist x, V x E F x. let [] = {x, y}.
x [B(E )], [B(F )] = {y}, B(E ) 6= B(F );
violation (E50 ).


132

fiAdmissible Restrained Revision

already clear (E50 ) desirable property. view bolstered
observing well-known iterated revision operators satisfy it; natural
revision, Darwiche-Pearl operator , Nayaks lexicographic revision, first
third discussed detail below. fact, conjecture Darwiche
Pearls intention replace (B5) (E50 ), (E5) propose
permanent replacement.
Definition 1 set postulates obtained replacing (E5) (E50 ) DP-AGM
defined RAGM.
Observe RAGM, like DP-AGM, guarantees [B(E )] = min(, E ).
Rule (E50 ) first new postulates want add Darwiche-Pearl
framework. lead second. One oldest known DP-operators
natural revision, usually credited Boutilier (1993, 1996), although idea also
found (Spohn, 1988). main feature application principle minimal
change epistemic states. characterised DP-AGM plus following postulate:
(CB) B(E ) B(E ) = B(E )
(CB) requires that, whenever B(E ) inconsistent , revising E
completely ignore revision . semantic counterpart follows:
(CBR) v, w
/ [B(E )], v E w iff v E w
shown Darwiche Pearl (1997), natural revision minimises changes conditional
beliefs, | conditional belief epistemic state E iff B(E ).
fact, Darwiche Pearl show (Lemma 1, p. 7), keeping E E similar
possible effect minimising changes conditional beliefs revision. So,
(CBR) clear natural revision application minimal change epistemic
states. requires that, barring changes mandated DP-AGM, relative ordering
valuations remains unchanged, thus keeping E similar possible E . sense
then, natural revision conservative DP-operators. strict adherence
minimal change inadvisable needs tempered appropriately, issue
addressed Section 5. Darwiche Pearl shown (CB) strong,
natural revision natural, sometimes yielding counterintuitive results.
Example 2 (Darwiche & Pearl, 1997) encounter strange animal appears
bird, believe one. comes closer, see clearly animal red,
believe red bird. remove doubts call bird expert examines
concludes bird, sort animal. still believe
animal red? (CB) tells us longer believe red. seen
substituting B(E) = Cn() = Cn(bird) red (CB), instructing us totally
ignore observation never taken place.
Given Example 2, perhaps surprising Darwiche Pearl never considered postulate (P) below. example, argument retaining belief creature
red hinges upon assumption red conflict newly obtained
information kind animal. is, learning creature
133

fiBooth & Meyer

animal automatically disqualify red, reasonable retain
belief red. generally then, whenever consistent revision ,
retained -revision inserted -revision.
(P)
/ B(E ) B(E )
Applying (P) Example 2 see that, red consistent B(E bird),
red B(E red bird). Put differently, (P) requires retain belief
animals redness, provided would precluded observation
red never occurred. (P) also proposed independently present paper
Jin Thielscher (2005) named Independence. semantic counterpart
(P) looks like this:
(PR) v [] w [], v E w v E w
(PR) requires -world v least plausible -world w strictly
plausible w -revision. following result also proved independently
Jin Thielscher (2005).
Proposition 2 satisfies DP-AGM, satisfies (P) iff also satisfies (PR).
Proof: (P)(PR), let v [], w [], v E w, let [] = {v, w}.
means
/ B(E ) (since [B(E )] either equal {v} {v, w}), so,
(P), B(E ). therefore v E w, not, would w E v,
follows w [B(E )],
/ B(E )].
(P)(PR), suppose
/ B(E ). means v [] [B(E )];
is, v E w every w []. means B(E ). not,
means x [] [B(E )]. Now, since x [B(E )], follows
DP-AGM x E w every w [], x E v (since v [B(E )] []).
also follows DP-AGM x [], therefore v E x, (PR)
follows v E x; contradiction.

Rule (PR) enforces certain changes ordering E receipt . fact soon
exist -world v -world w plausibility level somewhere
E (in v E w w E v), (PR) implies E 6=E . Furthermore
changes must also occur even already believed E begin with, i.e., B(E).
(Although course B(E) B(E ) = B(E), i.e., knowledge base associated
E remain unchanged follows DP-AGM.) rules (P)/(PR) ensure input
believed certain minimal strength belief enough help survive next
revision. point informed lead increase strength
agents belief , even cases agent already believes begin with,
made before, e.g., Friedman Halpern (1999, p.405). Note (P) antecedent
(C4) consequent (C3). fact, (P) stronger (C3) (C4) combined.
easily seen semantic counterparts postulates. also follows
concrete example iterated revision operator provided Darwiche Pearl,
operator refer employs form Spohnian conditioning (Spohn,
1988), satisfies (PR), therefore (P) well. Furthermore, adopting (P) explicitly
134

fiAdmissible Restrained Revision

exclude natural revision permissible operator. accepting (P) move towards
viewpoint information obtained latest input ought discarded
unnecessarily.
Based analysis section propose strengthening Darwiche-Pearl
framework (E5) replaced (E50 ) (C3) (C4) replaced (P).
Definition 2 revision operator admissible iff satisfies RAGM, (C1), (C2), (P).
Inasmuch Darwiche-Pearl framework visualised one -worlds
slide downwards relative -worlds, admissible revision ensures, via (PR),
downwards slide strict one.
pave way third postulate would like add paper
Darwiche-Pearl framework. begin with, note another view (P)
significant weakening following property, first introduced Nayak et al. (1996):
(Recalcitrance)
/ Cn() B(E )
Semantically, (Recalcitrance) corresponds following property, pointed
Booth (2005) implicitly contained work Nayak et al. (2003):
(R) v [], w [], v E w
(Recalcitrance) property lexicographic revision operator, second wellknown DP-operators consider, one old natural revision.
first introduced Nayak (1993) studied notably Nayak et al. (1994,
2003), although, natural revision, idea actually dates back Spohn (1988).
fact, lexicographic revision characterised DP-AGM (and also RAGM) together
(C1), (C2) (Recalcitrance), result easily proved semantic counterparts
properties Nayak et al.s semantic characterisation lexicographic revision
(2003). Informally, lexicographic revision takes assumption recent best,
Success postulate (E 2) based, adds assumption temporal
coherence. combination, leads stronger assumption recent
better.
analysis semantic characterisation lexicographic revision shows
least conservative DP-operators, sense effects changes
relative ordering valuations permitted DP-AGM (or RAGM matter)
Darwiche-Pearl postulates. Since also admissible revision operator, follows
also least conservative admissible operator.
problem (Recalcitrance) decision whether accept
subsequent revision completely determined logical relationship
epistemic state E robbed influence. replacement (Recalcitrance)
weaker (P) already gives E influence outcome. shortly
constrain matters giving E much influence allowed postulates
admissible revision. move ensures greater sensitivity agents epistemic record
making changes.
Note lexicographic revision assumes recent information takes complete
precedence information obtained previously. Thus, applied Example 2,
135

fiBooth & Meyer

requires us believe animal, previously assumed bird, indeed red,
red recent input conflict recently obtained input.
reasonable approach many circumstances, dogmatic adherence
problematic, following example shows.
Example 3 holidaying wildlife park observe creature clearly red,
far away determine whether bird land animal. adopt
knowledge base B(E) = Cn(red). Next us person knowledge local area
declares that, since creature red, bird. reason doubt him,
adopt belief red bird. creature moves closer becomes clear
bird. question is, continue believing red?
circumstances described want initial observation take precedence,
believe animal red. lexicographic revision allow us so.
examples along similar lines speaking rigid acceptance (Recalcitrance)
Glaister (1998, p.31) Jin Thielscher (2005, p.482).
(P) allows possibility retaining belief animal red,
enforce belief. rest section devoted discussion property
so. help us express property, introduce extra piece terminology
notation.
Definition 3 counteract respect epistemic state E, written !E ,
iff B(E ) B(E ).
use term counteract describe relation taken Nayak et al. (2003).
!E means that, viewpoint E, tend exclude other.
discuss properties relation. First note !E depends
total preorder E obtained E. Indeed !E iff min(, E ) []
min(, E ) []. turn reformulated following way, provides
useful aid visualise counteracts relation:
Proposition 3 !E iff exist v [], w [] v E x w E x
x min( , E ).
Proof: First note that, since obviously min(, E ) [], min(, E ) [] may rewritten min(, E ) [( )]. Using fact E total preorder, easy see
hold iff exists v [] v E x x min( , E ).
way may rewrite min(, E ) [] min(, E ) [( )],
equivalent saying exists w [] w E x x min( , E ).
words, Proposition 3 says !E iff exist -world -world
strictly plausible plausible ( )-worlds. immediate
things note !E symmetric, syntax-independent, i.e.,
!E 0 !E 0 . Furthermore logically inconsistent
!E , converse need hold (see short example
next proposition confirmation). Thus !E seen weak form inconsistency.
next result gives two properties !E :
136

fiAdmissible Restrained Revision

Proposition 4 Given RAGM, following properties hold !E :
(i) !E !E ( ) !E
(ii) 6!E 6!E ( ) 6!E
Proof: (i) Suppose !E !E . show ( ) !E need show
B(E ( )) ( ) B(E ). former already
B(E ) B(E ) !E !E respectively. Since follows
RAGM B(E ) B(E ) B(E ( )) , L, conclude
B(E ( )). latter already B(E )
B(E ) !E !E respectively. conclude
( ) B(E ), using RAGM (specifically (E 1)).
(ii) Suppose 6!E 6!E . Firstly, either 6 B(E ) 6 B(E )
must ( ) 6 B(E ) RAGM ( ) 6!E required.
suppose B(E ) B(E ). Then, since 6!E 6!E ,
means 6 B(E ) 6 B(E ). Since follows RAGM
B(E ( )) B(E ) B(E ) , L, follows two
6 B(E ( )) also case ( ) 6!E required.

first property says counteracts two sentences separately,
counteracts disjunction, second says cannot counteract
disjunction without counteracting least one disjuncts. Obviously
properties also hold binary relation logical inconsistency. However one departure
inconsistency relation possible 6!E () !E .
see assume moment L generated three propositional atoms {p, q, r}
take = p, = q = r. take E lowest plausibility
level contains two valuations 010 100, next plausibility level
valuation 111.
ready introduce third postulate. following:
(D) !E B(E )
(D) requires that, whenever counteract respect E, disallowed
-revision followed -revision. is, -revision E takes
place, information encoded E takes precedence information contained
E . Darwiche Pearl (1997) considered property (it rule (C6)) argued
it, citing following example.
Example 4 (Darwiche & Pearl, 1997) believe exactly one John Mary committed murder. get persuasive evidence indicating John murderer.
followed persuasive information indicating Mary murderer. Let represent
John committed murder Mary committed murder. (D) forces
us conclude Mary, John, involved murder. This, according
Darwiche Pearl, counterintuitive, since conclude involved
committing murder.
Darwiche Pearls argument (D) rests upon assumption recent
information ought take precedence information previously obtained.
137

fiBooth & Meyer

seen Example 3, always valid assumption. fact, application (D)
Example 3, = red bird = bird, produces intuitively correct result
belief observed animal red: red B(E (red bird) bird).
Another way gain insight significance (D) consider semantic
counterpart:
(DR) v [], w [], w
/ [B(E )], v E w v E w
(DR) curtails rise plausiblity -worlds -revision. ensures that,
exception plausible -worlds, relative ordering -world
-worlds plausible remains unchanged.
Proposition 5 Whenever revision operator satisfies RAGM, satisfies (D) iff
satisfies (DR).
Proof: (D)(DR), suppose v [], w [], w
/ [B(E )], v E w, let
[] = {v, w}. B(E ) B(E ), so, (D).
B(E ). follows v E w. not, would
w E v, means w [B(E )], therefore
/ B(E );
contradiction.
(D)(DR), suppose B(E ) B(E ), assume

/ B(E ). means w [] also [B(E )].
observe w
/ [B(E )] since w -model. Also, since w [B(E )],
follows RAGM w -model, therefore w
/ [B(E )]. supposition
B(E ) means min(, E ) []. Since w [] [] thus follows
v [] [] v E w. (DR) follows v E w. w
cannot model B(E ); contradiction.


4. Restrained Revision
strengthen requirements admissible revision (those operators satisfying
RAGM, (C1), (C2) (P)) insisting (D) satisfied well. so, let us
first consider semantic definition interesting admissible revision operator. Recall
RAGM fixes set (E )-minimal models, setting equal min(, E ),
places restriction remaining valuations ordered. following
property provides unique relative ordering remaining valuations.

v E w or,
(RR) v, w
/ [B(E )], v E w iff
v E w (v [] w [])
(RR) says relative ordering valuations (E )-minimal remains
unchanged, except -worlds -worlds plausibility level; split
two levels -worlds plausible -worlds. RAGM combined
(RR) fixes unique ordering valuations.
Definition 4 revision operator satisfying RAGM (RR) called restrained revision.
138

fiAdmissible Restrained Revision

turns within framework provided admissible revision, restrained
revision satisfies (D). prove help following lemma, asserting
equivalence (RR) (CR1), (CR2), (PR) (DR) presence RAGM.
Lemma 1 Whenever revision operator satisfies RAGM, satisfies (RR) iff
satisfies (CR1), (CR2), (PR), (DR).
Proof: (CR1)(RR), pick v, w []. v [B(E )] (CR1) follows
RAGM. not, follows RAGM w
/ [B(E )], (CR1) follows
direct application (RR). (CR2)(RR), pick v, w []. RAGM follows
v, w
/ [B(E )], obtain (CR2) direct application (RR).
Observe (RR) rewritten

v E w and,
0
(RR ) v, w
/ [B(E )], v E w iff
v E w (v [] w [])
Now, (PR)(RR), pick v [] w []. v [B(E )] (PR) follows
RAGM. not, follows direct application (RR0 ). (DR)(RR), pick
v [], w [], w
/ [B(E )]. (PR) follows direct application
0
(RR ).
(CR1), (CR2), (PR), (DR)(RR), let v, w
/ [B(E )] suppose v E w
v 6E w (i.e. w E v). show v E w either v [] w [].
Assume case. w E v v [] w []. Now, second
case impossible because, together w E v (PR) implies w E v;
contradiction. first case also impossible. see why, observe (CR1)
implies v w cannot -models, (CR2) v w cannot models, (DR) cannot case w [] v []. (PR) cannot
case w [] v []. concludes first part proof (CR1),
(CR2), (PR), (DR)(RR). second part, let v, w
/ [B(E )] suppose first
v E w. v, w [] v E w follows (CR1). v, w [] v E w
follows (CR2). v [] w [] v E w follows (PR). v []
w [] v E w follows (DR). suppose v E w either v []
w []. v [] v E w follows either (CR1) (PR), depending
whether w [] w []. similarly, w [] v E w follows either
(CR2) (PR), depending whether v [] v [].


Theorem 2 RAGM, (C1), (C2), (P) (D) provide exact characterisation restrained revision.
Proof: proof follows Lemma 1, Proposition 2, Proposition 5, correspondence (C1) (CR1), (C2) (CR2).

Another interpretation (RR) maintains relative ordering valuations
(E )-minimal, except changes mandated (PR).
seen restrained revision conservative admissible revision operators,
sense effects least changes relative ordering valuations permitted
139

fiBooth & Meyer

admissible revision. So, context admissible revision, restrained revision takes
role played natural revision Darwiche-Pearl framework.
rest section examine properties restrained revision.
Firstly, Examples 3 4 share interesting structural properties. both, initial
knowledge base B(E) pairwise consistent subsequent sentences
revision sequence, sentences revision sequence pairwise inconsistent.
examples information contained initial knowledge base B(E)
retained revision sequence. commonalities instances important
general result. Let denote non-empty sequence inputs 1 , . . . , n , let E
denote revision sequence E 1 . . . n . Furthermore shall refer epistemic
state E -compatible provided
/ B(E) every {1, . . . , n}.
(O) E -compatible B(E) B(E )
(O) says long B(E) direct conflict inputs sequence
1 , . . . , n , entire B(E) propagated knowledge base obtained
revision sequence E 1 . . . n . preservation property satisfied
restrained revision.
Proposition 6 Restrained revision satisfies (O).
Proof: denote E , = 0, . . . , n, revision sequence E 1 , . . . , (with
E 0 = E). give inductive proof that, v [B(E)] w
/ [B(E)], v Ei w
= 0, . . . , n. words, every B(E)-world always strictly every non B(E)world. result follows immediately. = 0 amounts showing
v E w follows immediately definition E B(E). pick
= 1, . . . , n assume v Ei1 w. consider four cases. v, w [i ]
follows (CR1) v Ei w. v, w [i ] follows (CR2) v Ei w.
v [i ] w [i ] follows (PR) v Ei w. finally, suppose
v [i ] w [i ]. -compatibility x [B(E)] [i ],
inductive hypothesis, x Ei1 w. w
/ [B(E )], follows (DR)
v Ei w.

Although restrained revision preserves information directly contradicted,
dogmatically wedded older information. neither two successive, incompatible, epistemic states conflict inputs sequence = 1 , . . . , n ,
prefers latter epistemic state revising .
Proposition 7 Restrained revision satisfies following property:
(Q) E E -compatible B(E)B(E) , B(E) B(E)
B(E) * B(E )
Proof: follows immediately Proposition 6 B(E ) B(E ).
B(E) * B(E ) follows consistency B(E ).

Next consider another preservation property, time, unlike case (O)
(Q), look circumstances B(E) incompatible inputs
revision sequence.
140

fiAdmissible Restrained Revision

(S) B(E ) B(E ) B(E ) = B(E )
Note that, given RAGM, antecedent (S) implies B(E). Thus (S) states
believed initially, subsequent commitment either negation
would change fact, sequence inputs preceded
, second input concerning nullified, older input regarding retained.
Proposition 8 Restrained revision satisfies (S).
Proof: Suppose antecedent holds. !E consequent holds. fact
seen property (T) Proposition 10 below. suppose 6!E .
either 6 B(E ) 6 B(E ). latter doesnt hold one
assumptions together (C2), former must hold. implies B(E )
(P). Combining assumption get !E . case get
B(E ) = B(E ) (again using (T)), (since 6!E ) B(E ) =
B(E ( )) ((T) more), turn equals B(E ( )) (C2). Since
B(E ) turn equal B(E ) RAGM required.

provide compact syntactic representation restrained revision. First
show (C1) (P) combined single property, (C2) (D).
Proposition 9 Given RAGM,
1. (C1) (P) together equivalent single rule
(C1P) 6 B(E ) B(E ) = B(E ( ))
2. (C2) (D) together equivalent single rule
(C2D) !E B(E ) = B(E ).
Proof: (C1),(P)(C1P), suppose 6 B(E). (P) follows B(E)
means, RAGM, B(E ) = B(E ( )). (C1) follows
B(E()) = B(E()), thus B(E) = B(E()). (C1)(C1P),
suppose . 6 B(E ) RAGM, B(E ) = B(E ( ))
(C1P). since follows B(E ) = B(E ). (P)(C1P),
suppose 6 B(E ). B(E ) = B(E ( )) (C1P) means,
RAGM, B(E ).
(C2),(D)(C2D), suppose !E . (D), B(E ).
RAGM means B(E ) = B(E ( )). Now, (C2) follows
B(E ( )) = B(E ( )). B(E ) = B(E ( )). since
B(E), get RAGM B(E( )) = B(E), follows
B(E ) = B(E ). (C2)(C2D), suppose . !E E
B(E ) = B(E ) (C2D). (D)(C2D), suppose !E .
B(E) = B(E) (C2D) since B(E), follows B(E).
(C1P) (C2D) provide conditions reduction two-step revision sequence
E single-step revision (if regards resulting knowledge base). (C1P)
141

fiBooth & Meyer

reduces ( )-revision consistent -revision. (C2D) reduces
-revision, ignoring completely, counteract respect E. Now,
follows RAGM consequent (C1P) also obtains 6 B(E ).
Putting together get succinct characterisation restrained revision.
Proposition 10 restrained revision satisfies RAGM and:

B(E )
!E
(T) B(E ) =
B(E ( )) otherwise.
Proof: Theorem 2 Proposition 9 sufficient show RAGM, (C1P)
(C2D) hold iff RAGM (T) hold. So, suppose satisfies RAGM (T). (C1P)
follows bottom part (T), (C2D) follows top part. Conversely,
suppose satisfies RAGM, (C1P) (C2D). !E follows (C2D)
B(E ) = B(E ). not, consider two cases.
/ B(E ) follows
(C1P) B(E) = B(E()). Otherwise case
/ B(E).
follows RAGM B(E ) = B(E ( )).

replace !E first clause (T) stronger
logically inconsistent, would obtain instead characterisation lexicographic
revision given Nayak et al. (2003).
Proposition 10 allows us see clearly another significant property restrained revision.
!E know B(E ) directly (D), 6!E
Proposition 10 tells us B(E ) = B(E ( )) B(E ) RAGM.
Thus see state E epistemic status (either accepted rejected)
always completely determined, i.e., proved:
Proposition 11 Restrained revision satisfies following property:
(U) 6 B(E ) B(E )
(Given similar characterisation mentioned above, easy see lexicographic
revision satisfies (U) too.) Like (P), property (U) read providing conditions
penultimate revision input believed. antecedent simply
saying B(E ) consistent . Thus (U) saying penultimate input
believed long consistent so. chaining (U) together (C4), easily
see (U) actually implies (P) presence (C4). consequence, obtain
following alternative axiomatic characterisation restrained revision.
Theorem 3 RAGM, (C1), (C2), (C4), (U) (D) provide exact characterisation
restrained revision.
(U), also able provide simple semantic counterpart property. corresponds
separating -worlds -worlds total preorder E following
-revision, plausibility level E either contains -worlds contains
-worlds:
Proposition 12 Whenever revision operator satisfies RAGM, satisfies (U) iff
satisfies following property:
142

fiAdmissible Restrained Revision

(UR) v [] w [], either v E w w E v
Proof: (U)(UR) suppose (UR) doesnt hold, i.e., exist , v [] w []
v E w w E v. Letting [] = {v, w} get
[B(E )] = {v, w} RAGM thus , 6 B(E ) (because
v, w [B(E )] respectively). Hence (U) doesnt hold.
(U)(UR) suppose (U) doesnt hold, i.e., exist , , 6
B(E ). exist v [] w [] v, w [B(E )] =
(by RAGM) min(, E ). Since v w (E )-minimal -worlds must
v E w w E v. Hence , v, w give counterexample (UR).

Finally section turn two properties first mentioned (as far know)
Schlecta et al. (1996) (see also work Lehmann et al. (2001)):
(Disj1) B(E ) B(E ) B(E ( ) )
(Disj2) B(E ( ) ) B(E ) B(E )
(Disj1) says sentence believed one two sequences revisions
differ step (step one case other), sentence
also believed sequence differs step
revision disjunction . Similarly, (Disj2) says every sentence believed
( )--revision believed least one (-) (-).
conditions reasonable properties expect revision operators.
Proposition 13 Restrained revision satisfies (Disj1) (Disj2).
prove result make use properties counteracts relation given
Proposition 4, along following lemma.
Lemma 4 !E ( ) 6!E B(E (( ) )) = B(E ( ))
Proof: Suppose !E ( ) 6!E . first show implies
B(E(())). able conclude required B(E(())) =
B(E ( )) using RAGM. suppose contrary 6 B(E (( ) )).
exists -world w min(( ) , E ). also w min( , E ). Since
!E know Proposition 3 exist -world w1 -world w2
wi E w = 1, 2. Clearly w1 also ( )-world, infer ( ) 6!E
contradiction. Hence B(E (( ) )) required.

Proof:[of Proposition 13] prove properties simultaneously looking two cases:
Case (i): ( ) !E . case B(E ( ) ) = B(E ) property (T)
Proposition 10. Meanwhile know Proposition 4(ii) either !E !E ,
using (T) know least one B(E ) B(E ) must also
equal B(E ). Hence see (Disj1) (Disj2) hold case.
Case (ii): ( ) 6!E . case (T) tells us B(E ( ) ) = B(E (( ) )) =
B(E (( ) ( )). Meanwhile Proposition 4(i) tells us least one 6!E
6!E holds. consider two subcases according either
143

fiBooth & Meyer

hold, one holds. hold B(E ) = B(E ( ))
B(E ) = B(E ( )), (Disj1) (Disj2) reduce
(Disj10 ) B(E ( )) B(E ( )) B(E (( ) ( )))
(Disj20 ) B(E (( ) ( ))) B(E ( )) B(E ( ))
respectively. consequence RAGM sentences ,
(1) B(E ) B(E ) B(E ( )) (2) B(E ( )) B(E ) B(E ).
Substituting gives us required (Disj10 ) (from (1))
(Disj20 ) (from (2)).
lets consider subcase !E 6!E . (A symmetric argument
work subcase 6!E !E .) 6!E get
B(E ) = B(E ( )), !E together ( ) 6!E get also
B(E()) = B(E()) using Lemma 4. case B(E()) = B(E),
(Disj1) (Disj2) follow immediately.

end section remarking shown lexicographic revision also
satisfies (Disj1) (Disj2).

5. Restrained Revision Composite Operator
saw Section 3, Boutiliers natural revision operator let us denote section
vulnerable damaging counterexamples red bird Example 2,
fails satisfy reasonable postulate (P). Although new input accepted
next epistemic state E , way provide preservation
subsequent revisions. Hans Rott (2003, p.128) describes it, [t]he recent input
sentence always embraced without reservation, last one input sentence, however,
treated utter disrespect. Thus, seem convincing reasons reject
viable operator performing iterated revision. However, literature epistemic state
change constantly reminds us keeping changes minimal major concern,
judged purely minimal change viewpoint, clear cant beaten!
find way apparent quandary? section show
use retained, provided application preceded intermediate operation
which, rather revising E new input , essentially revised E.
Given epistemic state E sentence , let us denote E / result
intermediate operation. E / epistemic state. idea forming E / ,
information E maintained. is, total preorder E/ satisfy
v E w implies v E/ w.

(1)

rather leaving behind entirely favour E, much informational
content preserved E / possible. formalised saying
v [], w [], take v E/ w long conflict (1)
above. second requirement guarantee enough presence
revised epistemic state E help survive subsequent revisions allow (P)
144

fiAdmissible Restrained Revision

captured. Taken together, two requirements enough specify E/ uniquely:

v E w,
v E/ w iff
(2)
v E w (v [] w []).
Thus, E/ lexicographic refinement E two-level total preorder
defined v w iff v [] w []. backwards revision operator new.
studied Papini (2001). also viewed backwards version
Nayaks lexicographic revision operator. necessarily B(E / ) (this
hold 6 B(E)), / satisfy RAGM.
Given /, define composite revision operator / setting
E / = (E / )

(3)

reminiscent Levi Identity (Gardenfors, 1988), used AGM theory recipe
reducing operation revision knowledge bases composite operation consisting
contraction plus expansion. (3), playing role expansion. operator /
satisfy RAGM. fact, easily seen comparing (2) condition
(RR) start Section 4, / coincides restrained revision.
Proposition 14 Let R denote restrained revision operator. R = / .
Thus proved restrained revision viewed combination two existing
operators.

6. Choose Revision Operator
contribution paper far summarised follows. argued
replacement Darwiche-Pearl framework class admissible revision operators, arguing former needs strengthened. eliminated
natural revision, retained lexicographic revision operator Darwiche
Pearl admissible operators. also introduced new admissible revision operator,
restrained revision, argued plausibility. argument restrained revision somehow unique, preferred revision operators.
contention merely that, epistemic state revision, Darwiche-Pearl framework
weak replaced admissible revision. restrained revision,
admissible revision operator, therefore one many revision operators deemed
rational. question admissible revision operator use particular situation
one depends number issues, context, strength certain
beliefs held, source information, on. point essentially also
made Friedman Halpern (1999). example, Example 3 formed part
argument use restrained revision, use lexicographic revision.
effect used example argue red ought B (E red bird bird),
B(E) = Cn(red). change context slightly, becomes example
favour use lexicographic revision, restrained revision.
Example 5 observe creature seems red, far away
determine whether bird land animal. adopt knowledge base B(E) =
145

fiBooth & Meyer

Cn(red). Next us expert birds remarks that, creature indeed red,
must bird. adopt belief red bird. get information someone
standing closer creature bird. Given context, is, reliability
expert combined statement creature initially seemed red,
reasonable adopt lexicographic approach recent best conclude
bird red. Formally, red B (E red bird bird), B(E) = Cn(red).
case source information dramatically affects outcome, consider
following example.
Example 6 Consider sequence inputs p followed finite number, say n,
instances pair p q, q. (To make concrete, reader might wish
substitute p red q bird.) Since p direct conflict sentences
sequence succeeding it, revision operator satisfying property (and includes
restrained revision) require p contained knowledge base obtained
revision sequence. Now, pair p q q obtained different source,
conclusion clearly unreasonable. all, sequence amounts told
p case, followed n different sources essentially telling p case.
hand, pairs p q q come source, case
clear cut anymore. fact, case one would expect result
obtained sequence p, p q, q, sequence formal structure
employed Example 3, restrained revision seen reasonable approach.
Another example restrained revision fares less well following:4
Example 7 Suppose teaching class students consisting n boys girls,
suppose class takes part mathematics competition. = 1, . . . , n
j = 1, . . . , let propositional variables pi qj stand boy competition
girl j competition respectively, suppose initially
W believe one
boys competition, i.e., B(E) = Cn( ) = pi
sentence expressing uniqueness competition winner. suppose interview
boys one other, tells us either one girls
won, i.e., obtain sequence inputs ( pi )i . Suppose willing
accept boys testimony. Using revision operator satisfies lead us
believe boy n competition, seems implausible. Lexicographic revision gives
desired result one girls competition.
examples clear agent need not, cases, ought
stick revision operator every time perform revision.
means agent keep switching one revision operator another
process iterated revision. course, leads question choose among
available (admissible) revision operators particular point. comprehensive answer
question beyond scope paper, provide clues
address problem. brief, contend epistemic states enriched,
detailed specification internal structure. Looking back history belief
revision, see exactly field progressed. initial papers,
4. grateful one anonymous referees suggesting example.

146

fiAdmissible Restrained Revision

AGM revision, epistemic state taken contain nothing
knowledge base. So, example, basic AGM revision characterised first six
AGM postulates imposes structure epistemic states all. shall refer
simple epistemic states. full AGM belief revision characterised eight AGM
postulates, view still one revision knowledge bases, every revision
operator knowledge base B uniquely associated B-faithful total preorder; i.e.,
total preorder valuations models B minimal elements.
small step define epistemic states include ordering, i.e. include
total preorder E associated epistemic state E part definition E.
shall refer complex epistemic states.
leads two different views revision process. view revision
operator simple epistemic states many different revision operators; one
corresponding B-faithful total preorders, way distinguish
choose revision operator. Viewed such, iterated revision
process (possibly) different revision operator employed every revision
step. principal view adopted Nayak et al. (2003). However, view
revision operator complex epistemic states, every epistemic state contains enough
information determine uniquely knowledge base, faithful total preorder,
resulting revision. words, enough information encoded
epistemic state uniquely determine knowledge base resulting revision,
lack information uniquely determine full epistemic state. DarwichePearl framework, also admissible revision, place constraints resulting
epistemic state, impose additional structure complex epistemic state.
view admissible revision complex epistemic states analogous basic AGM
revision simple epistemic states. next step would thus impose additional
structure complex epistemic states. could possibly involve addition second
ordering valuations done, example, Booth et al. (2004). case
simple epistemic states effect adding two supplementary postulates constrain
basic revision extent revision operator uniquely associated Bfaithful total preorder. sense, addition supplementary postulates allowed
imposition additional structure simple epistemic states. Recall one way
interpreting two supplementary postulates explain interaction
revision two sentences revision conjunction, something basic postulates
address.
So, seen, addition supplementary postulates leads definition
revision operators complex epistemic states. conjecture giving additional
structure complex epistemic states might involve provision postulates analogous
two AGM supplementary postulates. particular, conjecture postulates
might explain interaction two sentences conjunction,
disjunction, iterated revision. Observe none Darwiche-Pearl postulates,
additional postulates admissible revision matter, address issue.
fact postulates suggested (of aware) far
address (Disj1) (Disj2). speculate appropriate set supplementary
postulates iterated revision (which may may include two mentioned)
lead definition extra structure complex epistemic states,
147

fiBooth & Meyer

incorporated enriched version complex epistemic states, revision
seen operators enriched entities. Let us refer enriched epistemic
states. Enriched epistemic states enable us determine uniquely complex epistemic
state resulting revision, thereby solving question started with;
determining revision complex epistemic states use every particular point
process iterated revision. shall briefly discuss possible way
enriching complex epistemic states. note also recent proposal
Booth et al. (2006). instructive observe (Disj1) (Disj2) hold
framework.
proposed outline without pitfalls. obvious problem
approach leaves us meta-version dilemma started
with. Using enriched epistemic states able uniquely determine complex
epistemic states resulting revision, resulting enriched epistemic state.
lessen problem constraining permissible resulting enriched epistemic states
way admissible revision constrains permissible complex epistemic states,
chances whittling produce single permissible enriched
epistemic state. And, course, bound occur again. is,
whenever solve problem uniquely determining epistemic state certain
structure process enrichment, saddled question
uniquely determine enriched epistemic state resulting revision.
conjecture level point reached constraining enriched
epistemic states, la admissible revision, eventually lead unique enriched
epistemic state associated every revision. research determine whether
conjecture holds water.
conclusion, shown Darwiche-Pearl arguments lead acceptance
admissible revision operators class worthy study. restrained revision
operator, particular, exhibits quite desirable properties. Besides taking place
natural revision operator adhering closely principle minimal change,
satisfaction properties (O), (Q) (U) shows unnecessarily
remove previously obtained information.
future work would also like explore thoroughly class admissible
revision operators. paper saw restrained revision lexicographic revision
lie opposite ends spectrum admissible operators. represent respectively
conservative least conservative admissible operators sense
effect changes least changes, respectively, relative ordering
valuations permitted admissible revision. natural question whether exists
axiomatisable class admissible operators represents middle ground. One
clue finding class found counteracts relation !E
derived epistemic state E. said, relation depends preorder
E associated E. fact, given total preorder V define relation
!
! iff min(, ) [] min(, ) [].
clearly !E =!E . Furthermore full relation V V ! reduces
logical inconsistency. counteracts relation stronger !E , still weaker logical
inconsistency found setting !=!0 , 0 lies somewhere E
148

fiAdmissible Restrained Revision

V V . Hence one avenue worth exploring might assume epistemic
state E extract one two preorders E 0E E 0E . Then,
instead requiring !E deduce B(E ), done restrained
revision (the postulate (D)), could require stronger condition !0E
hold. currently experimenting strategies using second preorder
guide manipulation E enable property satisfied. use second
preorder seen way enriching epistemic state, might thus contribute
solution choice revision operators discussed Section 6.
future work relates also two extreme cases revision, looked
different angle. mentioned earlier, lexicographic revision formalisation
recent best approach revision taken logical extreme. approach
exemplified (E2) postulate, also known Success, requires revision
successful, sense epistemic input provided always contained
resulting knowledge base. Given (E2) one postulates admissible
revision, requirement carries even restrained revision, opposite
end spectrum admissible revision. means admissible revision
operator differs lexicographic revision still adheres dictum
recent best, raises question recent input given
prominence. relaxation requirement would imply giving (E2)
venturing area known non-prioritised revision (Booth, 2001; Chopra, Ghose, &
Meyer, 2003; Hansson, 1999). speculate appropriate relaxation admissible
revision, (E2) removed requirement, lead class (non-prioritised)
revision operators strictly containing admissible revision, lexicographic revision
still one end spectrum, end spectrum occupied
operator studied Papini (2001) used sub-operation restrained revision
Section 5. operator formalised extreme version recent worst;
words, older better.

Acknowledgements
Much first authors work done stints researcher Wollongong
University Macquarie University, Sydney. wishes thank Aditya Ghose Abhaya
Nayak making possible enjoy great working environments there,
also interesting comments work. Thanks also due Samir Chopra
contributed preliminary version paper, Adnan Darwiche clearing
misconceptions definition epistemic states, three anonymous referees
valuable insightful comments. National ICT Australia funded Australia
Governments Department Communications, Information Technology Arts
Australian Research Council Backing Australias Ability ICT
Centre Excellence program. supported members Australian National
University, University NSW, ACT Government, NSW Government affiliate partner
University Sydney.
149

fiBooth & Meyer

References
Alchourron, C. E., Gardenfors, P., & Makinson, D. (1985). logic theory change:
Partial meet functions contraction revision. Journal Symbolic Logic, 50,
510530.
Areces, C., & Becher, V. (2001). Iterable AGM functions. Frontiers belief revision,
pp. 261277. Kluwer, Dordrecht.
Booth, R. (2001). negotiation-style framework non-prioritised revision. van Benthem, J. (Ed.), Theoretical Aspects Rationality Knowledge: Proceedings
Eighth Conference (TARK 2001), pp. 137150, San Francisco, California. Morgan
Kaufmann.
Booth, R. (2005). logic iterated non-prioritised revision. Conditionals, Information Inference Selected papers Workshop Conditionals, Information
Inference, 2002, Vol. 3301 LNAI, pp. 86107. Springer-Verlag, Berlin.
Booth, R., Chopra, S., Ghose, A., & Meyer, T. (2004). unifying semantics belief
change. Mantaras, R. L. D., & Saitta, L. (Eds.), Sixteenth European Conference
Artificial Intelligence: ECAI2004, pp. 793797. IOS Press.
Booth, R., Meyer, T., & Wong, K.-S. (2006). bad day surfing better good day
working: revise total preorder. Proceedings KR2006, Tenth International Conference Principles Knowledge Representation Reasoning.
Boutilier, C. (1993). Revision sequences nested conditionals. Bajcsy, R. (Ed.), IJCAI93. Proceedings 13th International Joint Conference Artificial Intelligence
held Chambery, France, August 28 September 3, 1993, Vol. 1, pp. 519525, San
Mateo, CA. Morgan Kaufmann.
Boutilier, C. (1996). Iterated revision minimal changes conditional beliefs. Journal
Philosophical Logic, 25 (3), 263305.
Chopra, S., Ghose, A., & Meyer, T. (2003). Non-prioritized ranked belief change. Journal
Philosophical Logic, 32 (3), 417443.
Dalal, M. (1988). Investigations theory knowledge base revision. Proceedings
7th National Conference American Association Artificial Intelligence,
Saint Paul, Minnesota, pp. 475479.
Darwiche, A., & Pearl, J. (1997). logic iterated belief revision. Artificial Intelligence, 89, 129.
Freund, M., & Lehmann, D. (1994). Belief revision rational inference. Tech. rep. TR
94-16, Leibniz Centre Research Computer Science, Institute Computer
Science, Hebrew University Jerusalem.
Friedman, N., & Halpern, J. Y. (1999). Belief revision: critique. Journal Logic,
Language Information, 8, 401420.
Gardenfors, P. (1988). Knowledge Flux : Modeling Dynamics Epistemic States.
MIT Press, Cambridge, Massachusetts.
Glaister, S. M. (1998). Symmetry belief revision. Erkenntnis, 49, 2156.
150

fiAdmissible Restrained Revision

Grove, A. (1988). Two modellings theory change. Journal Philosophical Logic, 17,
157170.
Hansson, S. O. (1999). survey non-prioritized belief revision. Erkenntnis, 50, 413427.
Jin, Y., & Thielscher, M. (2005). Iterated belief revision, revised. Proceedings
Nineteenth International Joint Conference Artificial Intelligence (IJCAI 05), pp.
478483.
Katsuno, H., & Mendelzon, A. O. (1991). Propositional knowledge base revision minimal change. Artificial Intelligence, 52, 263294.
Konieczny, S., & Pino Perez, R. (2000). framework iterated revision. Journal
Applied Non-Classical Logics, 10(3-4), 339367.
Lehmann, D. (1995). Belief revision, revised. Proceedings Fourteenth International
Joint Conference Artificial Intelligence (IJCAI95), pp. 15341540.
Lehmann, D., Magidor, M., & Schlechta, K. (2001). Distance semantics belief revision.
Journal Symbolic Logic, 66, 295317.
Lewis, D. K. (1973). Counterfactuals. Journal Philosophy, 70, 556567.
Nayak, A. C. (1993). Studies Belief Change. Ph.D. thesis, University Rochester.
Nayak, A. C. (1994). Iterated belief change based epistemic entrenchment. Erkenntnis,
41, 353390.
Nayak, A. C., Foo, N. Y., Pagnucco, M., & Sattar, A. (1996). Changing Conditional Belief
Unconditionally. Shoham, Y. (Ed.), Theoretical Aspects Rationality Knowledge: Proceedings Sixth Conference (TARK 1996), pp. 119136, San Francisco,
California. Morgan Kaufmann.
Nayak, A. C., Pagnucco, M., & Peppas, P. (2003). Dynamic belief change operators. Artificial Intelligence, 146, 193228.
Papini, O. (2001). Iterated revision operations stemming history agents
observations. Frontiers belief revision, pp. 281303. Kluwer, Dordrecht.
Rott, H. (2000). Two dogmas belief revision. Journal Philosophy, 97, 503522.
Rott, H. (2003). Coherence conservatism dynamics belief II: Iterated belief
change without dispositional coherence. Journal Logic Computation, 13 (1),
111145.
Schlechta, K., Lehmann, D., & Magidor, M. (1996). Distance semantics belief revision.
Shoham, Y. (Ed.), Proceedings Sixth Conference Theoretical Aspects
Rationality Knowledge, pp. 137145. Morgan Kaufmann.
Segerberg, K. (1998). Irrevocable belief revision dynamic doxastic logic. Notre Dame
Journal Formal Logic, 39, 287306.
Spohn, W. (1988). Ordinal conditional functions: dynamic theory epistemic states.
Harper, W. L., & Skyrms, B. (Eds.), Causation Decision: Belief, Change
Statistics: Proceedings Irvine Conference Probability Causation: Volume
II, Vol. 42 University Western Ontario Series Philosophy Science, pp.
105134, Dordrecht. Kluwer Academic Publishers.

151

fiJournal Artificial Intelligence Research 26 (2006) 35-99

Submitted 8/05; published 5/06

Planning Graph Heuristics Belief Space Search
Daniel Bryce
Subbarao Kambhampati,

DAN . BRYCE @ ASU . EDU
RAO @ ASU . EDU

Department Computer Science Engineering
Ira A. Fulton School Engineering
Arizona State University, Brickyard Suite 501
699 South Mill Avenue, Tempe, AZ 85281

David E. Smith

DE 2 SMITH @ EMAIL . ARC . NASA . GOV

NASA Ames Research Center
Intelligent Systems Division, MS 269-2
Moffett Field, CA 94035-1000

Abstract
recent works conditional planning proposed reachability heuristics improve
planner scalability, many lack formal description properties distance estimates.
place previous work context extend work heuristics conditional planning,
provide formal basis distance estimates belief states. give definition
distance belief states relies aggregating underlying state distance measures.
give several techniques aggregate state distances associated properties. Many existing
heuristics exhibit subset properties, order provide standardized comparison
present several generalizations planning graph heuristics used single planner.
compliment belief state distance estimate framework also investigating efficient planning
graph data structures incorporate BDDs compute effective heuristics.
developed two planners serve test-beds investigation. first, CAltAlt,
conformant regression planner uses A* search. second, P D, conditional
progression planner uses AO* search. show relative effectiveness heuristic
techniques within planners. also compare performance planners several
state art approaches conditional planning.

1. Introduction
Ever since CGP (Smith & Weld, 1998) SGP (Weld, Anderson, & Smith, 1998) series planners developed tackling conformant conditional planning problems including
GPT (Bonet & Geffner, 2000), C-Plan (Castellini, Giunchiglia, & Tacchella, 2001), PKSPlan (Petrick & Bacchus, 2002), Frag-Plan (Kurien, Nayak, & Smith, 2002), MBP (Bertoli, Cimatti, Roveri,
& Traverso, 2001b), KACMBP (Bertoli & Cimatti, 2002), CFF (Hoffmann & Brafman, 2004),
YKA (Rintanen, 2003b). Several planners extensions heuristic state space planners
search space belief states (where belief state set possible states). Without
full-observability, agents need belief states capture state uncertainty arising starting
uncertain state executing actions uncertain effects known state. focus
first type uncertainty, agent starts uncertain state deterministic actions.
seek strong plans, agent reach goal certainty despite partially known
state. Many aforementioned planners find strong plans, heuristic search planners
c
2006
AI Access Foundation. rights reserved.

fiB RYCE , K AMBHAMPATI , & MITH

currently among best. Yet foundation constitutes good distance-based heuristic
belief space adequately investigated.
Belief Space Heuristics: Intuitively, argued heuristic merit belief state depends
least two factorsthe size belief state (i.e., uncertainty current state),
distance individual states belief state destination belief state. question
course compute measures effective. Many approaches estimate
belief state distances terms individual state state distances states two belief
states, either lack effective state state distances ways aggregate state distances.
instance MBP planner (Bertoli et al., 2001b) counts number states current belief
state. amounts assuming state distance unit cost, planning state
done independently. GPT planner (Bonet & Geffner, 2000) measures state state distances
exactly takes maximum distance, assuming states belief state positively interact.
Heuristic Computation Substrates: characterize several approaches estimating belief state
distance describing terms underlying state state distances. basis investigation adapting classical planning reachability heuristics measure state distances
developing state distance aggregation techniques measure interaction plans states
belief state. take three fundamental approaches measure distance two belief
states. first approach involve aggregating state distance measures, rather use
classical planning graph compute representative state distance. second retains distinctions
individual states belief state using multiple planning graphs, akin CGP (Smith
& Weld, 1998), compute many state distance measures aggregated. third
employs new planning graph generalization, called Labelled Uncertainty Graph (LU G),
blends first two measure single distance two belief states. techniques discuss types heuristics compute special emphasis relaxed
plans. present several relaxed plan heuristics differ terms employ state distance aggregation make stronger assumptions states belief state co-achieve
goal action sequences independent, positively interact, negatively interact.
motivation first three planning graph techniques measuring belief state
distances try minimal extension classical planning heuristics see work us.
Noticing use classical planning heuristics ignores distinctions states belief
state may provide uninformed heuristics, move second approach possibly
build exponentially many planning graphs get better heuristic. multiple planning
graphs extract heuristic graph aggregate get belief state distance
measure. assume states belief state independent, aggregate measures
summation. Or, assume positively interact use maximization. However,
show, relaxed plans give us unique opportunity measure positive interaction
independence among states essentially taking union several relaxed plans. Moreover,
mutexes play role measuring negative interactions states. Despite utility
robust ways aggregate state distances, still faced exponential blow
number planning graphs needed. Thus, third approach seeks retain ability measure
interaction state distances avoid computing multiple graphs extracting heuristics
each. idea condense symbolically represent multiple planning graphs single
planning graph, called Labelled Uncertainty Graph (LU G). Loosely speaking, single graph
unions causal support information present multiple graphs pushes disjunction,
36

fiP LANNING G RAPH H EURISTICS B ELIEF PACE EARCH

describing sets possible worlds (i.e., initial literal layers), labels. planning graph
vertices present multiple graphs, redundant representation avoided.
instance action present multiple planning graphs would present
LU G labelled indicate applicable planning graph projection
possible world. describe extract heuristics LU G make implicit
assumptions state interaction without explicitly aggregating several state distances.
Ideally, planning graph techniques considers every state belief state compute
heuristics, belief states grow size could become uninformed costly. example,
single classical planning graph ignores distinctions possible states heuristic
based multiple graphs leads construction planning graph state. One way
keep costs base heuristics subset states belief state. evaluate
effect sampling cost heuristics. single graph sample single
state multiple graphs LU G sample percent states. evaluate
state sampling show appropriate, find dependent compute
heuristics states.
Standardized Evaluation Heuristics: issue evaluating effectiveness heuristic techniques many architectural differences planners use heuristics. quite hard
pinpoint global effect assumptions underlying heuristics performance.
example, GPT outperformed MBPbut questionable whether credit efficiency attributable differences heuristics, differences search engines (MBP uses
BDD-based search). interest paper systematically evaluate spectrum approaches
computing heuristics belief space planning. Thus implemented heuristics similar
GPT MBP use compare new heuristics developed around notion
overlap (multiple world positive interaction independence). implemented heuristics
within two planners, Conformant-AltAlt planner (CAltAlt) Partially-Observable NonDeterministic planner (P D). P handle search non-deterministic actions,
bulk paper discuss deterministic actions. general action formulation,
pointed Smith Weld (1998), translated initial state uncertainty. Alternatively,
Section 8.2 discuss direct approach reason non-deterministic actions
heuristics.
External Evaluation: Although main interest paper evaluate relative advantages spectrum belief space planning heuristics normalized setting, also compare
performance best heuristics work current state art conformant
conditional planners. empirical studies show planning graph based heuristics provide effective guidance compared cardinality heuristics well reachability heuristic used GPT
CFF, planners competitive BDD-based planners MBP YKA,
GraphPlan-based ones CGP SGP. also notice planners gain scalability
heuristics retain reasonable quality solutions, unlike several planners compare
against.
rest paper organized follows. first present CAltAlt P planners
describing state action representations well search algorithms. understand
search guidance planners, discuss appropriate properties heuristic measures
belief space planning. follow description three planning graph substrates used
compute heuristics. carry empirical evaluation next three sections, describing
37

fiB RYCE , K AMBHAMPATI , & MITH

test setup, presenting standardized internal comparison, finally comparing several
state art planners. end related research, discussion, prospects future work,
various concluding remarks.

2. Belief Space Planners
planning formulation uses regression search find strong conformant plans progression
search find strong conformant conditional plans. strong plan guarantees finite
number actions executed many possible initial states, resulting states goal
states. Conformant plans special case plan conditional plan branches,
classical planning. Conditional plans general case plans structured graph
include conditional actions (i.e. actions causative observational effects).
presentation, restrict conditional plans DAGs, conceptual reason
cannot general graphs. plan quality metric maximum plan path length.
formulate search space belief states, technique described Bonet Geffner
(2000). planning problem P defined tuple D, BSI , BSG , domain
description, BSI initial belief state, BSG goal belief state (consisting states
satisfying goal). domain tuple F, A, F set fluents set
actions.
Logical Formula Representation: make extensive use logical formulas F represent
belief states, actions, LU G labels, first explain conventions. refer every
fluent F either positive literal negative literal, either denoted l.
discussing literal l, opposite polarity literal denoted l. Thus l = at(location1),
l = at(location1). reserve symbols denote logical false true, respectively.
Throughout paper define conjunction empty set equivalent , disjunction
empty set .
Logical formulas propositional sentences comprised literals, disjunction, conjunction,
negation. refer set models formula f M(f ). consider disjunctive normal
), conjunctive normal form f , (f ). DNF seen
form logical formula f , (f
disjunction constituents conjunction literals. Alternatively CNF
seen conjunction clauses C disjunction literals.1 find useful
think DNF CNF represented sets disjunctive set constituents conjunctive set
clauses. also refer complete representation (f ) formula f DNF every
constituent case state model f .
Belief State Representation: world state, S, represented complete interpretation
fluents. also refer states possible worlds. belief state BS set states symbolically represented propositional formula F . state set states represented
belief state BS M(BS), equivalently |= BS.
pedagogical purposes, use bomb toilet clogging sensing problem,
BTCS, running example paper.2 BTCS problem includes two packages, one
) readily related. Specifically constituent contains k |F | literals,
1. easy see M(f ) (f
corresponding 2|F |k models.
2. aware negative publicity associated B&T problems fact handle interesting
problems difficult reachability uncertainty (e.g. Logistics Rovers), simplify discussion
choose small problem.

38

fiP LANNING G RAPH H EURISTICS B ELIEF PACE EARCH

contains bomb, also toilet dunk packages defuse potential
bombs. goal disarm bomb allowable actions dunking package
toilet (DunkP1, DunkP2), flushing toilet becomes clogged dunking (Flush),
using metal-detector sense package contains bomb (DetectMetal). fluents encoding
problem denote bomb armed (arm) not, bomb package (inP1, inP2)
not, toilet clogged (clog) not. also consider conformant variation BTCS,
called BTC, DetectMetal action.
belief state representation BTCS initial condition, clausal representation is:
(BSI ) = arm clog (inP1 inP2) (inP1 inP2),
constituent representation is:

(BS
) = (arm clog inP1 inP2) (arm clog inP1 inP2).
goal BTCS clausal constituent representation:

(BSG ) = (BS
G ) = arm.
However, goal complete representation:
(BSG ) = (arm clog inP1 inP2) (arm clog inP1 inP2)
(arm clog inP1 inP2) (arm clog inP1 inP2)
(arm clog inP1 inP2) (arm clog inP1 inP2)
(arm clog inP1 inP2) (arm clog inP1 inP2).
last four states (disjuncts) complete representation unreachable, consistent
goal description.
Action Representation: represent actions causative observational effects.
actions described tuple e (a), (a), (a) e (a) execution precondition,
(a) set causative effects, (a) set observations. execution precondition,
e (a), conjunction literals must hold action executable. action executable, apply set causative effects find successor states apply observations
partition successor states observational classes.
causative effect j (a) (a) conditional effect form j (a) = j (a),
antecedent j (a) consequent j (a) conjunction literals. handle disjunction
e (a) j (a) replicating respective action effect different conditions,
loss generality assume conjunctive preconditions. However, cannot split disjunction
effects. Disjunction effect amounts representing set non-deterministic outcomes. Hence
allow disjunction effects thereby restricting deterministic effects. convention
0 (a) unconditional effect, equivalent conditional effect 0 (a) = .
way obtain observations execute action observations. observation
formula oj (a) (a) possible sensor reading. example, action observes
truth values two fluents p q defines (a) = {p q, p q, p q, p q}. differs
slightly conventional description observations conditional planning literature.
works (e.g., Rintanen, 2003b) describe observation list observable formulas,
define possible sensor readings boolean combinations formulas. directly define
possible sensor readings, illustrated example. note convention helpful
problems boolean combinations observable formulas never sensor readings.
causative sensory actions example BTCS problem are:
39

fiB RYCE , K AMBHAMPATI , & MITH

DunkP1: e = clog, = {0 = clog, 1 = inP1 = arm}, = {},
DunkP2: e = clog, = {0 = clog, 1 = inP2 = arm}, = {},
Flush: e = , = {0 = clog}, = {},
DetectMetal: e = , = , = {o0 = inP1, o1 = inP1}.
2.1 Regression
perform regression CAltAlt planner find conformant plans starting goal
belief state regressing non-deterministically relevant actions. action (without
observations) relevant regressing belief state (i) unconditional effect consistent
every state belief state (ii) least one effect consequent contains literal present
constituent belief state. first part relevance requires every state successor
belief state actually reachable predecessor belief state second ensures
action helps support successor.
Following Pednault (1988), regressing belief state BS action a, conditional
effects, involves finding execution, causation, preservation formulas. define regression
terms clausal representation, generalized arbitrary formulas. regression
belief state conjunction regression clauses (BS). Formally, result BS
regressing belief state BS action defined as:3




BS = Regress(BS, a) = (a)




((a, l) IP (a, l))

C(BS) lC

Execution formula ((a)) execution precondition e (a). must hold BS
applicable.
Causation formula ((a, l)) literal l w.r.t effects (a) action defined
weakest formula must hold state l holds BS. intuitive meaning
l already held BS , antecedent (a) must held BS make l hold BS.
Formally (a, l) defined as:

(a)
(a, l) = l
i:li (a)

Preservation formula (IP (a, l)) literal l w.r.t. effects (a) action defined
formula must true l violated effect (a). intuitive
meaning antecedent every effect inconsistent l could held BS .
Formally IP (a, l) defined as:
IP (a, l) =



(a)

i:li (a)

Regression also formalized MBP planner (Cimatti & Roveri, 2000) symbolic
pre-image computation BDDs (Bryant, 1986). formulation syntactically different,
approaches compute result.
3. Note BS may clausal form regression (especially action multiple conditional effects).

40

fiP LANNING G RAPH H EURISTICS B ELIEF PACE EARCH

BSG
Flush

BS1
Flush

BS4
Flush

BS7

DunkP1

BS8

DunkP1

BS2
DunkP1

BS5

DunkP2

BS3
DunkP2

BS6

DunkP2

BS9

Figure 1: Illustration regression search path conformant plan BT C problem.
2.2 CAltAlt
CAltAlt planner uses regression operator generate children A* search. Regression
terminates search node expansion generates belief state BS logically entailed
initial belief state BSI . plan sequence actions regressed BSG obtain
belief state entailed BSI .
example, BTC problem, Figure 1, have:
BS2 =Regress(BSG , DunkP1) = clog (arm inP1).
first clause execution formula second clause causation formula
conditional effect DunkP1 arm.
Regressing BS2 Flush gives:
BS4 = Regress(BS2 , Flush) = (arm inP1).
BS4 , execution precondition Flush , causation formula clog = ,
(arm inP1) comes persistence causation formula.
Finally, regressing BS4 DunkP2 gives:
BS9 = Regress(BS4 , DunkP2) = clog (arm inP1 inP2).
terminate BS9 BSI |= BS9 . plan DunkP2, Flush, DunkP1.
2.3 Progression
progression handle causative effects observations, general, progressing
action belief state BS generates set successor belief states B. set belief
states B empty action applicable BS (BS
|= e (a)).
Progression belief state BS action best understood union result
applying model BS fact implement BDD images, MBP planner
41

fiB RYCE , K AMBHAMPATI , & MITH

(Bertoli et al., 2001b). Since compute progression two steps, first finding causative successor, second partitioning successor observational classes, explain steps separately.
causative successor BS found progressing belief state BS causative effects
action a. action applicable, causative successor disjunction causative
progression (Progressc ) state BS a:



: BS
|= e (a)



BS = Progressc (BS, a) =
SM(BS) Progressc (S, a) : otherwise
progression action state conjunction every literal persists (no
applicable effect consequent contains negation literal) every literal given
effect (an applicable effect consequent contains literal).


= Progressc (S, a) =

l:lS
j S|=j (a)
lj (a)



l
l:j

S|=j (a)
lj (a)

l


Applying observations action results set successors B. set found (in
Progresss ) individually taking conjunction sensor reading oj (a) causative
successor BS . Applying observations (a) belief state BS results set B belief
states, defined as:

: BS =



{BS }
: (a) =
B = Progresss (BS , a) =



j

{BS |BS = (a) BS } : otherwise
full progression computed as:
B = Progress(BS, a) = Progresss (Progressc (BS, a), a).
2.4 P
use top AO* search (Nilsson, 1980), P planner generate conformant
conditional plans. search graph, nodes belief states hyper-edges actions.
need AO* applying action observations belief state divides belief state
observational classes. use hyper-edges actions actions observations
several possible successor belief states, must included solution.
AO* search consists two repeated steps: expand current partial solution,
revise current partial solution. Search ends every leaf node current solution
belief state satisfies goal better solution exists (given heuristic function). Expansion involves following current solution unexpanded leaf node generating children.
Revision dynamic programming update node current solution selects best
hyper-edge (action). update assigns action minimum cost start best solution
rooted given node. cost node cost best action plus average cost
children (the nodes connected best action). expanding leaf node, children
applied actions given heuristic value indicate estimated cost.
42

fiP LANNING G RAPH H EURISTICS B ELIEF PACE EARCH

main differences formulation AO* Nilsson (1980)
allow cycles search graph, update costs nodes average rather
summation, use weighted estimate future cost. first difference ensure plans
strong (there finite number steps goal), second guide search toward plans
lower average path cost, third bias search trust heuristic function.
define plan quality metric (maximum plan path length) differently metric search
minimizes two reasons. First, easier compare competing planners
measure plan quality metric. Second, search tends efficient using average
instead maximum cost actions children. using average instead maximum,
measured cost plan lower means likely search shallower search graph
prove solution best solution.
Conformant planning, using actions without observations, special case AO* search,
similar A* search. hyper-edges represent actions singletons, leading
single successor belief state. Consider BTC problem (BTCS without DetectMetal action)
future cost (heuristic value) set zero every search node. show search graph
Figure 2 conformant example well conditional example, described shortly.
expand initial belief state progressing applicable actions. get:
B1 = {BS10 } = Progress(BSI , DunkP1)
= {(inP1 inP2 clog arm) (inP1 inP2 clog arm)}

B3 = {BS20 } = Progress(BSI , DunkP2)
= {(inP1 inP2 clog arm) (inP1 inP2 clog arm)}.
Since clog already holds every state initial belief state, applying Flush BSI leads
BSI creating cycle. Hence, hyper-edge Flush added search graph BSI .
assign cost zero BS10 BS20 , update internal nodes best solution, add
DunkP1 best solution rooted BSI (whose cost one).
expand leaf nodes best solution, single node BS10 , applicable actions.
applicable action Flush, get:
B3 = {BS30 } = Progress(BS10 , Flush)
= {(inP1 inP2 clog arm) (inP1 inP2 clog arm)}.
assign cost zero BS30 update best solution. choose Flush best action
BS10 (whose cost one), choose DunkP2 best action BSI (whose cost
one). DunkP2 chosen BSI successor BS20 cost zero, opposed
BS10 cost one.
Expanding leaf node BS20 applicable action, Flush, get:
B4 = {BS40 } = Progress(BS20 , Flush)
= {(inP1 inP2 clog arm) (inP1 inP2 clog arm)}.
update BS40 (to cost zero) BS20 (to cost one), choose Flush best
action BS20 . root node BSI two children, cost one, arbitrarily choose
DunkP1 best action.
expand BS30 relevant actions get BSG DunkP2 action. DunkP1 creates
cycle back BS10 added search graph. solution leaf
nodes terminal. required terminal belief state contains subset
43

fiB RYCE , K AMBHAMPATI , & MITH

BSI
DunkP1

Detect
Metal

DunkP2

:inP1

inP1

B1

B2

BS10

B5

BS50

BS20

Flush

Flush

B3

B4

BS30

DunkP2

DunkP1
DunkP2

DunkP1

B6

BS40
DunkP2

BS51

B7

BS60

BS70

DunkP1

BSG

Figure 2: Illustration progression search conformant plan (bold dashed edges) conditional plan (bold solid edges) BTCS problem.

states BSG , case terminal belief state contains exactly states BSG . cost
solution three because, revision, BS30 cost one, sets BS10 cost
two. However, means BSI cost three best action DunkP1. Instead,
revision sets best action BSI DunkP2 cost currently two.
expand BS40 DunkP1 find successor BSG . DunkP2 creates cycle
back BS20 added search graph. second valid solution
contains unexpanded leaf nodes. Revision sets cost BS40 one, BS20 two,
BSI three. Since solutions starting BSI equal cost (meaning cheaper
solutions), terminate plan DunkP2, Flush, DunkP1, shown bold dashed lines
Figure 2.
example search conditional plan P D, consider BTCS example whose
search graph also shown Figure 2. Expanding initial belief state, get:
B1 = {BS10 } = Progress(BSI , DunkP1),
B2 = {BS20 } = Progress(BSI , DunkP2),

B5 = {BS50 , BS51 } = Progress(BSI ,DetectMetal)
= {inP1 inP2 clog arm, inP1 inP2 clog arm}.
leaf nodes assigned cost zero, DunkP1 chosen arbitrarily best
solution rooted BSI cost solution identical. cost including
hyper-edge average cost children plus cost, cost using DetectMetal (0+0)/2
+ 1 = 1. Thus, root BSI cost one.
44

fiP LANNING G RAPH H EURISTICS B ELIEF PACE EARCH

conformant problem expand BS10 , giving child cost zero BS10 cost
one. changes best solution BSI use DunkP2, expand BS20 , giving child
cost zero cost one. choose DetectMetal start best solution BSI
gives BSI cost one, using either Dunk action would give BSI cost two.
expand first child DetectMetal, BS50 , DunkP1 get:
{inP1 inP2 clog arm},
goal state, DunkP2 get:
B6 = {BS60 } = Progress(BS50 ,DunkP2) = {inP1 inP2 clog arm}.
expand second child, BS51 , DunkP2 get:
{inP1 inP2 clog arm},
also goal state DunkP1 get:
B7 = {BS70 } = Progress(BS51 ,DunkP1) = {inP1 inP2 clog arm}.
none new belief states equivalent BSG , two entail BSG ,
treat terminal connecting hyper-edges actions BSG . choose
DunkP1 DunkP2 best actions BS50 BS51 respectively set cost node
one. turn sets cost using DetectMetal BSI (1+1)/2 + 1 = 2. terminate
plan cost equal possible plans starting BSI leaf nodes
satisfy goal. plan shown bold solid lines Figure 2.

3. Belief State Distance
CAltAlt P planners need guide search node expansion heuristics
estimate plan distance dist(BS, BS ) two belief states BS BS . convention, assume BS precedes BS (i.e., progression BS search node BS goal
belief state, regression BS initial belief state BS search node). simplicity,
limit discussion progression planning. Since strong plan (executed BS) ensures
every state M(BS) transition state M(BS ), define plan distance
BS BS number actions needed transition every state M(BS)
state M(BS ). Naturally, strong plan, actions used transition state S1 M(BS)
may affect transition another state S2 M(BS). usually degree positive
negative interaction S1 S2 ignored captured estimating plan distance.4 following explore perform estimates using several intuitions
classical planning state distance heuristics.
start example search scenario Figure 3. three belief states BS1 (containing states S11 S12 ), BS2 (containing state S21 ), BS3 (containing states S31 S32 ).
goal belief state BS3 , two progression search nodes BS1 BS2 . want
expand search node smallest distance BS3 estimating dist(BS1 , BS3 ) denoted
bold, dashed line dist(BS2 , BS3 ) denoted bold, solid line. assume
estimates state distance measures dist(S, ) denoted light dashed
solid lines numbers. state distances represented numbers action sequences.
example, use following action sequences illustration:
4. Interaction states captures notion actions performed transition one state goal may interfere
(negatively interact) aid (positively interact) transitioning states goals states.

45

fiB RYCE , K AMBHAMPATI , & MITH

BS1
S11

BS3

14
5

S12

S31

3
7

BS2

S32

8

S21

10

Figure 3: Conformant Plan Distance Estimation Belief Space
dist(S11 , S32 ) : ({a1 , a2 }, {a5 }, {a6 , a7 }),
dist(S12 , S31 ) : ({a1 , a7 }, {a3 }),
dist(S21 , S31 ) : ({a3 , a6 }, {a9 , a2 , a1 }, {a0 , a8 }, {a5 }).
sequence may several actions step. instance, dist(S21 , S31 ) a3
a6 first step, total eight actions sequence meaning distance
eight. Notice example includes several state distance estimates, found
classical planning techniques. many ways use similar ideas estimate belief
state distance addressed issue belief states containing several states.
Selecting States Distance Estimation: exists considerable body literature estimating plan distance states classical planning (Bonet & Geffner, 1999; Nguyen,
Kambhampati, & Nigenda, 2002; Hoffmann & Nebel, 2001), would like apply estimate plan distance two belief states, say BS1 BS3 . identify four possible
options using state distance estimates compute distance belief states BS1
BS3 :
Sample State Pair: sample single state BS1 single state BS3 ,
whose plan distance used belief state distance. example, might sample S12
BS1 S31 BS3 , define dist(BS1 , BS3 ) = dist(S12 , S31 ).
Aggregate States: form aggregate states BS1 BS3 measure plan
distance. aggregate state union literals needed express belief state formula,
46

fiP LANNING G RAPH H EURISTICS B ELIEF PACE EARCH

define as:
S(BS) =

ff

l

l:lS,S(BS)

Since possible express belief state formula every literal (e.g., using (q q) p
express belief state p true), assume reasonably succinct representation,
ROBDD (Bryant, 1986). quite possible aggregate states inconsistent, many classical planning techniques (such planning graphs) require consistent states. example, aggregate states would compute belief state distance
dist(BS1 , BS3 ) = dist(S(BS1 ), S(BS3 )).
Choose Subset States: choose set states (e.g., random sampling)
BS1 set states BS3 , compute state distances pairs states
sets. Upon computing state distances, aggregate state distances (as
describe shortly). example, might sample S11 S12 BS1 S31
BS3 , compute dist(S11 , S31 ) dist(S12 , S31 ), aggregate state distances
define dist(BS1 , BS3 ).
Use States: use states BS1 BS3 , and, similar sampling subset
states (above), compute distances state pairs aggregate distances.
former two options computing belief state distance reasonably straightforward, given
existing work classical planning. latter two options compute multiple state distances.
multiple state distances two details require consideration order obtain
belief state distance measure. following treat belief states contain states
appropriately replaced subset chosen states.
first issue state distances may needed. Since state BS1
needs reach state BS3 , consider distance state BS1 state
BS3 . However, dont necessarily need distance every state BS1 every state
BS3 . explore assumptions state distances need computed Section 3.1.
second issue, arises computing state distances, need aggregate
state distances belief state distance. notice popular state distance estimates
used classical planning typically measure aggregate costs state features (literals). Since
planning belief space, wish estimate belief state distance aggregate cost
belief state features (states). Section 3.2, examine several choices aggregating state
distances discuss captures different types state interaction. Section 3.3,
conclude summary choices make order compute belief state distances.
3.1 State Distance Assumptions
choose compute multiple state distances two belief states BS BS ,
whether considering states sampling subsets, state distances important.
given state BS need know distance every state BS
state BS need transition one state BS . two assumptions make
states reached BS help us define two different belief state distance measures
terms aggregate state distances:
47

fiB RYCE , K AMBHAMPATI , & MITH

optimistically assume earlier states M(BS) reach closest
later states M(BS ). assumption compute distance as:
dist(BS, BS ) = ffSM(BS)

min

M(BS )

dist(S, ).

assume earlier states M(BS) reach later state
M(BS ), aggregate distance minimum. assumption compute distance as:
dist(BS, BS ) =

min

M(BS )

ffSM(BS) dist(S, ),

ff represents aggregation technique (several discuss shortly).
Throughout rest paper use first definition belief state distance
relatively robust easy compute. drawback treats earlier states
independent fashion, flexible allowing earlier states transition different later states.
second definition measures dependencies earlier states, restricts reach
later state. second may sometimes accurate, misinformed cases
earlier states cannot reach later state (i.e., measure would infinite).
pursue second method may return distance measures infinite
fact finite.
see Section 4, discuss computing measures planning graphs,
implicitly find state BS closest state BS , enumerate
states minimization term first belief state distance (above). Part reason
) rather actual states.

compute distance terms constituents (BS
Also, consider constituents BS , discuss sampling belief states include distance computation sample BS. also avoid explicit aggregation
ff using LU G, describe several choices ff understand implicit assumptions made
heuristics computed LU G.
3.2 State Distance Aggregation
aggregation function ff plays important role measure distance belief
states. compute one state distance measure, either exhaustively sampling
subset (as previously mentioned), must combine measures means, denoted ff.
range options taking state distances aggregating belief state
distance. discuss several assumptions associated potential measures:
Positive Interaction States: Positive interaction assumes difficult state BS
requires actions help transition states BS state BS .
example, means assume actions used transition S11 S32 help us
transition S12 S31 (assuming state BS1 transitions closest state BS3 ).
Inspecting action sequences, see positively interact need actions a1
a7 . need know action sequences assume positive interaction
define aggregation ff maximization numerical state distances:
dist(BS, BS ) =

max

min

SM(BS) M(BS )

dist(S, ).
48

fiP LANNING G RAPH H EURISTICS B ELIEF PACE EARCH

belief state distances dist(BS1 , BS3 ) = max(min(14, 5), min(3, 7)) = 5
dist(BS2 , BS3 ) = max(min(8, 10)) = 8. case prefer BS1 BS2 .
state distance admissible sample belief states, assuming positive
interaction also admissible.
Independence States: Independence assumes state BS requires actions
different states BS order reach state BS . Previously, found
positive interaction action sequences transition S11 S32 S12 S31
shared actions a1 a7 . also independence sequences
first contains a2 , a5 , a6 , second contains a3 . Again, need
know action sequences assume independence define aggregation ff
summation numerical state distances:
fi
min dist(S, ).
dist(BS, BS ) =


SM(BS) M(BS )

example, dist(BS1 , BS3 ) = min(14, 5) + min(3, 7) = 8, dist(BS2 , BS3 ) =
min(8, 10) = 8. case preference BS1 BS2 .
notice using cardinality belief state |M(BS)| measure dist(BS, BS )
special case assuming state independence, S, dist(S, ) = 1. use cardinality measure distance example, dist(BS1 , BS3 ) = |M(BS1 )| = 2,
dist(BS2 , BS3 ) = |M(BS2 )| = 1. cardinality prefer BS2 BS1
better knowledge BS2 .
Overlap States: Overlap assumes positive interaction independence
actions used states BS reach state BS . intuition
actions often used multiple states BS simultaneously count
actions once. example, computed dist(BS1 , BS3 ) assuming positive
interaction, noticed action sequences dist(S11 , S32 ) dist(S12 , S31 )
used a1 a7 . aggregate sequences would like count a1 a7
potentially overlap. However, truly combining action sequences
maximal overlap plan merging problem (Kambhampati, Ihrig, & Srivastava, 1996),
difficult planning. Since ultimate intent compute heuristics,
take simple approach merging action sequences. introduce plan merging
operator ff picks step align sequences unions aligned
steps. use size resulting action sequence measure belief state distance:
dist(BS, BS ) = SM(BS)

min

M(BS )

dist(S, ).

Depending type search, define differently. assume sequences used
progression search start time used regression end time.
Thus, progression sequences aligned first step union steps,
regression sequences aligned last step union.
example, progression dist(S11 , S32 ) dist(S12 , S31 ) = ({a1 , a2 }, {a5 }, {a6 , a7 })
({a1 , a7 }, {a3 }) = ({a1 , a2 , a7 }, {a5 , a3 }, {a6 , a7 }) align sequences
first steps, union step. Notice resulting sequence seven actions, giving
49

fiB RYCE , K AMBHAMPATI , & MITH

dist(BS1 , BS3 ) = 7, whereas defining ff maximum gave distance five summation gave distance eight. Compared overlap, positive interaction tends
estimate distance, independence tends estimate distance. see empirical evaluation (in Section 6.5), accounting overlap provides accurate
distance measures many conformant planning domains.
Negative Interaction States: Negative interaction states appear example
transitioning state S11 state S32 makes difficult (or even impossible) transition
state S12 state S31 . could happen performing action a5 S11 conflicts action
a3 S12 . say BS1 cannot reach BS3 possible action sequences start
S11 S12 , respectively, end M(BS3 ) negatively interact.
two ways negative interactions play role belief state distances. Negative interactions allow us prove impossible belief state BS reach belief state
BS , meaning dist(BS, BS ) = , potentially increase distance finite
amount. use first, extreme, notion negative interaction computing
cross-world mutexes (Smith & Weld, 1998) prune belief states search.
cannot prune belief state, use one aforementioned techniques aggregate
state distances. such, provide concrete definition ff measure negative
interaction.
explore ways adjust distance measure negative interactions,
mention possibilities. Like work classical planning (Nguyen et al., 2002),
penalize distance measure dist(BS1 , BS3 ) reflect additional cost associated serializing conflicting actions. Additionally conditional planning, conflicting actions
conditioned observations execute plan branch. distance
measure uses observations would reflect added cost obtaining observations,
well change cost associated introducing plan branches (e.g., measuring average
branch cost).
techniques belief state distance estimation terms state distances provide
basis use multiple planning graphs. show empirical evaluation
measures affect planner performance differently across standard conformant conditional
planning domains. quite costly compute several state distance measures, understanding aggregate state distances sets foundation techniques develop
LU G. already mentioned, LU G conveniently allows us implicitly aggregate state
distances directly measure belief state distance.
3.3 Summary Methods Distance Estimation
Since explore several methods computing belief state distances planning graphs, provide summary choices must consider, listed Table 1. column headed
choice, containing possible options below. order columns reflects order
consider options.
section covered first two columns relate selecting states belief
states distance computation, well aggregating multiple state distances belief state
distance. test options choices empirical evaluation.
50

fiP LANNING G RAPH H EURISTICS B ELIEF PACE EARCH

State
Selection
Single
Aggregate
Subset


State Distance
Aggregation
+ Interaction
Independence
Overlap
- Interaction

Planning
Graph
SG
MG
LU G

Mutex
Type
None
Static
Dynamic
Induced

Mutex
Worlds

Intersect
Cross

Heuristic
Max
Sum
Level
Relaxed Plan

Table 1: Features belief state distance estimation.
next section also expand upon aggregate distance measures well
discuss remaining columns Table 1. present type planning graph: single
planning graph (SG), multiple planning graphs (M G), labelled uncertainty graph (LU G).
Within planning graph describe several types mutex, including static, dynamic,
induced mutexes. Additionally, type mutex computed respect different
possible worlds means mutex involves planning graph elements (e.g., actions)
exist world (i.e., mutexes computed within planning graph single
state), across worlds (i.e., mutexes computed planning graphs different states)
two methods (denoted Intersect Cross). Finally, compute many different heuristics
planning graphs measure state distances max, sum, level, relaxed plan. focus
discussion planning graphs, same-world mutexes, relaxed plan heuristics next
section. Cross-world mutexes heuristics described appendices.

4. Heuristics
section discusses use planning graph heuristics measure belief state distances.
cover several types planning graphs extent used compute
various heuristics. begin brief background planning graphs.
Planning Graphs: Planning graphs serve basis belief state distance estimation. Planning graphs initially introduced GraphPlan (Blum & Furst, 1995) representing optimistic, compressed version state space progression tree. compression lies unioning
literals every state subsequent steps initial state. optimism relates underestimating number steps takes support sets literals (by tracking subset
infeasible tuples literals). GraphPlan searches compressed progression (or planning graph)
achieves goal literals level two goal literals marked infeasible. search
tries find actions support top level goal literals, find actions support chosen
actions reaching first graph level. basic idea behind using planning graphs
search heuristics find first level planning graph literal state
appears; index level lower bound number actions needed achieve
state literal. also techniques estimating number actions required
achieve sets literals. planning graphs serve way estimate reachability state literals discriminate goodness different search states. work generalizes
literal estimations belief space search considering GraphPlan CGP style planning
graphs plus new generalization planning graphs, called LU G.
Planners CGP (Smith & Weld, 1998) SGP (Weld et al., 1998) adapt GraphPlan
idea compressing search space planning graph using multiple planning graphs, one
51

fiB RYCE , K AMBHAMPATI , & MITH

Overlap

n-distances

hMG
RPU

hLUG
RP

State Distance Aggregation

CFF

Independence

Positive
Interaction

None

h card
MBP
KACMBP
YKA

hMG
s-RP

GPT

hMG
m-RP

h0
NG

1

hSG
RP
U
hSG
RP
SG

MG

LUG

Planning Graph Type

Figure 4: Taxonomy heuristics respect planning graph type state distance aggregation. Blank entries indicate combination meaningless possible.

possible world initial belief state. CGP SGP search planning graphs,
similar GraphPlan, find conformant conditional plans. work paper seeks
apply idea extracting search heuristics planning graphs, previously used state space
search (Nguyen et al., 2002; Hoffmann & Nebel, 2001; Bonet & Geffner, 1999) belief space
search.
Planning Graphs Belief Space: section proceeds describing four classes heuristics
estimate belief state distance N G, SG, G, LU G. N G heuristics techniques existing
literature based planning graphs, SG heuristics techniques based single
classical planning graph, G heuristics techniques based multiple planning graphs (similar
used CGP) LU G heuristics use new labelled planning graph. LU G combines
advantages SG G reduce representation size maintain informedness. Note
include observations planning graph structures SGP (Weld et al.,
1998) would, however include feature future work. conditional planning formulation directly uses planning graph heuristics ignoring observations, results show
still gives good performance.
Figure 4 present taxonomy distance measures belief space. taxonomy also
includes related planners, whose distance measures characterized section.
related planners listed N G group, despite fact actually use planning graphs,
clearly fall one planning graph categories. figure shows
52

fiP LANNING G RAPH H EURISTICS B ELIEF PACE EARCH

different substrates (horizontal axis) used compute belief state distance aggregating
state state distances various assumptions (vertical axis). combinations
considered make sense impossible. reasons omissions
discussed subsequent sections. wealth different heuristics one
compute using planning graphs, concentrate relaxed plans proven
effective classical planning previous studies (Bryce & Kambhampati, 2004).
provide additional descriptions heuristics like max, sum, level Appendix A.
Example: illustrate computation heuristic, use example derived BTC
called Courteous BTC (CBTC) courteous package dunker disarm bomb
leave toilet unclogged, discourteous person left toilet clogged. initial
belief state CBTC clausal representation is:
(BSI ) = arm clog (inP1 inP2) (inP1 inP2),
goal is:
(BSG ) = clog arm.
optimal action sequences reach BSG BSI are:
Flush, DunkP1, Flush, DunkP2, Flush,

Flush, DunkP2, Flush, DunkP1, Flush.
Thus optimal heuristic estimate distance BSI BSG , regression,
h (BSG ) = 5 either plan five actions.
use planning graphs progression regression search. regression search
heuristic estimates cost current belief state w.r.t. initial belief state progression
search heuristic estimates cost goal belief state w.r.t. current belief state. Thus,
regression search planning graph(s) built (projected) possible worlds
initial belief state, progression search need built search node.
introduce notation BSi denote belief state find heuristic measure, BSP
denote belief state used construct initial layer planning graph(s).
following subsections describe computing heuristics regression, generalized
progression changing BSi BSP appropriately.
previous section discussed two important issues involved heuristic computation:
sampling states include computation using mutexes capture negative interactions
heuristics. directly address issues section, deferring discussion
respective empirical evaluation sections, 6.4 6.2. heuristics computed
decided set states use, whether sampling not. Also, previously
mentioned, consider sampling states belief state BSP implicitly
find closest states BSi without sampling. explore computing mutexes planning
graphs regression search. use mutexes determine first level planning graph
goal belief state reachable (via level heuristic described Appendix A) extract
relaxed plan starting level. level heuristic level belief
state reachable, prune regressed belief state.
proceed describing various substrates used computing belief space distance estimates. Within describe prospects various types world aggregation. addition
heuristics, mention related work relevant areas.
53

fiB RYCE , K AMBHAMPATI , & MITH

4.1 Non Planning Graph-based Heuristics (N G)
group many heuristics planners N G group using SG, G,
LU G planning graphs. mention group mean
using planning graphs form.
Aggregation: Breadth first search uses simple heuristic, h0 heuristic value set
zero. mention heuristic gauge effectiveness search substrates
relative improvements gained using heuristics.
Positive Interaction Aggregation: GPT planner (Bonet & Geffner, 2000) measures belief
state distance maximum minimum state state distance states source
destination belief states, assuming optimistic reachability mentioned Section 3. GPT measures
state distances exactly, terms minimum number transitions state space. Taking
maximum state state distance akin assuming positive interaction states current
belief state.
Independence Aggregation: MBP planner (Bertoli et al., 2001b), KACMBP planner (Bertoli
& Cimatti, 2002), YKA planner (Rintanen, 2003b), comparable hcard heuristic measure
belief state distance assuming every state state distance one, taking summation
state distances (i.e. counting number states belief state). measure useful
regression goal belief states partially specified contain many states consistent
goal formula many states consistent goal formula reachable
initial belief state. Throughout regression, many unreachable states removed
predecessor belief states inconsistent preconditions regressed action.
Thus, belief states reduce size regression cardinality may indicate
closer initial belief state. Cardinality also useful progression belief states
become smaller, agent knowledge easier reach goal state.
CBTC, hcard (BSG ) = 4 BSG four states consistent complete representation:
(BSG ) = (inP1 inP2clog arm) (inP1 inP2 clog arm)
(inP1 inP2 clog arm) (inP1 inP2 clog arm).
Notice, may uninformed BSG two states (BSG ) reachable,
like: (inP1 inP2 clog arm). n packages, would 2n1 unreachable
states represented (BSG ). Counting unreachable states may overestimate distance estimate
need plan them. general, addition problem counting unreachable states, cardinality accurately reflect distance measures. instance, MBP reverts
breadth first search classical planning problems state distance may large small
still assigns value one.
Overlap Aggregation: Rintanen (2004) describes n-Distances generalize belief state
distance measure GPT consider maximum n-tuple state distance. measure involves,
n-sized tuple states belief state, finding length actual plan transition
n-tuple destination belief state. maximum n-tuple distance taken distance
measure.
example, consider belief state four states. n equal two, would define
six belief states, one size two subset four states. belief states
find real plan, take maximum cost plans measure distance original
54

fiP LANNING G RAPH H EURISTICS B ELIEF PACE EARCH

L0

A0

E0

L1

A1

E1

L2

inP1

inP1

inP1

inP1

inP1

inP1

inP2

inP2

inP2

inP2

inP2

inP2
DunkP1

1(DunkP1)
0(DunkP1)

arm
DunkP2
arm

arm

1(DunkP2)
0(DunkP2)

clog

clog

clog
Flush

0(Flush)

clog

arm

Flush

0(Flush)

clog

Figure 5: Single planning graph CBTC, relaxed plan components bold. Mutexes omitted.

four state belief state. n one, computing measure GPT, n
equal size belief state directly solving planning problem. costly
compute measure large values n, informed accounts overlap
negative interactions.
CFF planner (Hoffmann & Brafman, 2004) uses version relaxed planning graph
extract relaxed plans. relaxed plans measure cost supporting set goal literals
states belief state. addition traditional notion relaxed planning graph ignores
mutexes, CFF also ignores one antecedent literal conditional effects keep relaxed
plan reasoning tractable. CFF relaxed plan capture overlap ignores subgoals
mutexes. way CFF ensures goal supported relaxed problem encode
relaxed planning graph satisfiability problem. encoding satisfiable, chosen number
action assignments distance measure.
4.2 Single Graph Heuristics (SG)
simplest approach using planning graphs belief space planning heuristics use
classical planning graph. form initial literal layer projected belief state, could
either sample single state (denoted SG1 ) use aggregate state (denoted SGU ). example,
CBTC (see Figure 5) assuming regression search BSP = BSI , initial level L0
planning graph SG1 might be:
55

fiB RYCE , K AMBHAMPATI , & MITH

L0 = {arm, clog, inP1, inP2}
SGU defined aggregate state S(BSP ):
L0 = {arm, clog, inP1, inP2, inP1, inP2}.
Since two versions single planning graph identical semantics, aside initial
literal layer, proceed describing SGU graph point differences SG1
arise.
Graph construction identical classical planning graphs (including mutex propagation)
stops two subsequent literal layers identical (level off). use planning graph formalism used IPP (Koehler, Nebel, Hoffmann, & Dimopoulos, 1997) allow explicit representation conditional effects, meaning literal layer Lk , action layer Ak , effect
layer Ek level k. Persistence literal l, denoted lp , represented action
e (lp ) = 0 (lp ) = l. literal Lk effect previous effect layer Ek1 contains
literal consequent. action action layer Ak every one execution precondition
literals Lk . effect effect layer Ek associated action action layer Ak
every one antecedent literals Lk . Using conditional effects planning graph avoids
factoring action conditional effects possibly exponential number non-conditional
actions, adds extra planning graph layer per level. graph built, extract
heuristics.
Aggregation: Relaxed plans within single planning graph able measure,
optimistic assumptions, distance two belief states. relaxed plan represents
distance subset initial layer literals literals constituent belief
state. SGU , literals initial layer used support may hold
single state projected belief state, unlike SG1 . classical relaxed plan heuristic hSG
RP
finds set (possibly interfering) actions support goal constituent. relaxed plan RP
RP
RP
RP
RP
RP
subgraph planning graph, form {ARP
0 , E0 , L1 , ..., Ab1 , Eb1 , Lb }.
layers contains subset vertices corresponding layer planning graph.

formally, find relaxed plan support constituent (BS
) reached
SG
earliest graph (as found hlevel (BSi ) heuristic Appendix A). Briefly, hSG
level (BSi )
returns first level b constituent BSi literals Lb none marked
pair-wise mutex. Notice incorporate negative interactions heuristics.
start extraction level b, defining LRP
literals constituent used level
b
RP
heuristic. literal l Lb , select supporting effect (ignoring mutexes) Eb1
RP . prefer persistence literals effects supporting literals.
form subset Eb1
RP
supporting set effects found, create ARP
b1 actions effect Eb1 .
RP
RP added
needed preconditions actions antecedents chosen effects Ab1 Eb1
list literals support LRP
b2 . algorithm repeats find needed actions
A0 . relaxed plans value summation number actions action layer.
literal persistence, denoted subscript p, treated action planning graph,
|. single graph relaxed plan
relaxed plan include final computation | ARP
j
heuristic computed
hSG
RP (BSi )

=

b1

j=0

56

| ARP
|
j

fiP LANNING G RAPH H EURISTICS B ELIEF PACE EARCH

CBTC problem find relaxed plan SGU , shown Figure 5 bold
edges nodes. Since arm clog non mutex level two, use persistence
RP use persistence inP1,
support clog DunkP1 support arm LRP
2 . L1
SG
Flush clog. Thus, hRP (BSG ) = 2 relaxed plan is:
= {inP1p , Flush},
ARP
0
E0RP = {0 (inP1p ), 0 (Flush)},

= {inP1, clog},
LRP
1
= {clogp , DunkP1},
ARP
1

E1RP = {0 (clogp ), 1 (DunkP1)},
= {arm, clog}.
LRP
2

relaxed plan use DunkP2 DunkP1 support arm. result arm
supported worlds (i.e. supported state inP2 holds initial
state). initial literal layer threw away knowledge inP1 inP2 holding different worlds,
relaxed plan extraction ignored fact arm needs supported worlds. Even
SG1 graph, see similar behavior reasoning single world.
single, unmodified classical planning graph cannot capture support possible worlds hence
explicit aggregation distance measures states. result, mention
aggregating states measure positive interaction, independence, overlap.
4.3 Multiple Graph Heuristics (M G)
Single graph heuristics usually uninformed projected belief state BSP often corresponds multiple possible states. lack accuracy single graphs able
capture propagation multiple world support information. Consider CBTC problem
projected belief state BSI using single graph SGU . DunkP1 action
would say arm clog reached cost two, fact cost infinite
(since DunkP2 support arm possible worlds), strong plan.
account lack support possible worlds sharpen heuristic estimate, set
multiple planning graphs considered. single graph, previously discussed.
multiple graphs similar graphs used CGP (Smith & Weld, 1998), lack
general cross-world mutexes. Mutexes computed within graph, i.e. sameworld mutexes computed. construct initial layer L0 graph different state
M(BSP ). multiple graphs, heuristic value belief state computed terms
graphs. Unlike single graphs, compute different world aggregation measures
multiple planning graphs.
get informed heuristic considering states M(BSP ),
certain cases costly compute full set planning graphs extract relaxed plans.
describe computing full set planning graphs, later evaluate (in Section 6.4)
effect computing smaller proportion these. single graph SG1 extreme case
computing fewer graphs.
illustrate use multiple planning graphs, consider example CBTC. build two
graphs (Figure 6) projected BSP . respective initial literal layers:
L10 = {arm, clog, inP1, inP2}
L20 = {arm, clog, inP2, inP2}.

57

fiB RYCE , K AMBHAMPATI , & MITH

L0

inP1

A0

E0

inP2

A1

L1

inP1

E1

inP2

inP2
DunkP1

1

1(DunkP1)
0(DunkP1)

arm
clog

1(DunkP2)

arm
Flush

0(Flush)

DunkP2

clog

Flush

clog
inP1

inP1

inP2

inP2

L2

inP1

arm
arm

0(DunkP2)
0(Flush)

clog
clog
inP1

DunkP1

1(DunkP1)
0(DunkP1)

2

inP2

arm
arm
clog

1(DunkP2)

arm
Flush

0(Flush)

DunkP2

clog

Flush

clog

0(DunkP2)
0(Flush)

arm
clog
clog

Figure 6: Multiple planning graphs CBTC, relaxed plan components bolded. Mutexes
omitted.

graph first possible world, arm comes DunkP1 level 2.
graph second world, arm comes DunkP2 level 2. Thus, multiple
graphs show actions different worlds contribute support literal.
single planning graph sufficient aggregate state measures, following consider compute achievement cost belief state multiple graphs
aggregating state distances.
Positive Interaction Aggregation: Similar GPT (Bonet & Geffner, 2000), use worstG
case world represent cost belief state BSi using hM
mRP heuristic. difference
GPT compute heuristic planning graphs, compute plans state
space. heuristic account number actions used given world, assume
positive interaction across possible worlds.
G
hM
mRP heuristic computed finding relaxed plan RP planning graph ,
exactly done single graph hSG
RP . difference unlike single graph relaxed
plan SGU , like SG1 , initial levels planning graphs states, relaxed plan
reflect support needed world corresponding . Formally:


b 1

RP
G

| Aj |
hM
mRP (BSi ) = max


j=0

b level constituent BSG first reachable.
58

fiP LANNING G RAPH H EURISTICS B ELIEF PACE EARCH

Notice computing state distances states BSP BSi .
planning graph corresponds state BSP , extract single relaxed plan.
need enumerate states BSi find relaxed plan each. instead support
set literals one constituent BSi . constituent estimated minimum distance
state BSi first constituent reached .
G
CBTC, computing hM
mRP (BSG ) (Figure 6) finds:
RP 1 =
1
= {inP1p , Flush},
ARP
0

E0RP1 = {0 (inP1p ), 0 (Flush)},
1
= {inP1, clog},
LRP
1
1
= {clogp , DunkP1},
ARP
1

E1RP1 = {0 (clogp ), 1 (DunkP1)},

1
= {arm, clog}
LRP
2

RP 2 =
2
= {inP2p , Flush},
ARP
0

E0RP2 = {0 (inP2p ), 0 (Flush)},

2
= {inP2, clog},
LRP
1
2
= {clogp , DunkP2},
ARP
1

E1RP2 = {0 (clogp ), 1 (DunkP2)},

2
= {arm, clog}.
LRP
2

relaxed plan contains two actions taking maximum two relaxed plan values
G
gives hM
mRP (BSG ) = 2. aggregation ignores fact must use different Dunk actions
possible world.
G
Independence Aggregation: use hM
sRP heuristic assume independence among
worlds belief state. extract relaxed plans exactly described previous heuristic
simply use summation rather maximization relaxed plan costs. Formally:


1
b
RP
G

| Aj |
hM
sRP (BSi ) =


j=0

b level constituent BSG first reachable.
G
MG
CBTC, computing hM
sRP (BSG ), find relaxed plans hmRP (BSG )
heuristic, sum values get 2 + 2 = 4 heuristic. aggregation ignores fact
use Flush action possible worlds.
State Overlap Aggregation: notice two previous heuristics either taking
maximization accounting actions, taking summation possibly accounting
G
extra actions. present hM
RP U heuristic balance measure positive interaction
independence worlds. Examining relaxed plans computed two previous heuristics
CBTC example, see relaxed plans extracted graph overlap.
1
2
Notice, ARP
ARP
contain Flush action irrespective package bomb
0
0
1
2
contains DunkP1, ARP
contains DunkP2
showing positive interaction. Also, ARP
1
1
59

fiB RYCE , K AMBHAMPATI , & MITH

showing independence. take layer-wise union two relaxed plans, would
get unioned relaxed plan:
RPU =
U
= {inP1p , Flush},
ARP
0

E0RPU = {0 (inP1p ), 0 (inP2p ), 0 (Flush)},
U
LRP
= {inP1, inP2, clog},
1
U
= {clogp , DunkP1, DunkP2},
ARP
1

E1RPU = {0 (clogp ), 1 (DunkP1), 1 (DunkP2)},
U
LRP
= {arm, clog}.
2

relaxed plans accounts actions possible worlds
actions differ. Notice Flush appears layer zero Dunk actions
appear layer one.
order get union relaxed plans, extract relaxed plans ,
two previous heuristics. computing heuristics regression search, start
last level (and repeat level) taking union sets actions relaxed plan
level another relaxed plan. relaxed plans end-aligned, hence unioning levels
proceeds last layer relaxed plan create last layer RPU relaxed plan,
second last layer relaxed plan unioned on. progression search,
relaxed plans start-aligned reflect start time, whereas regression
assume end time. summation number actions action
level unioned relaxed plan used heuristic value. Formally:
G
hM
RP U (BSi ) =

b1


U
| ARP
|
j

j=0

b greatest level b constituent BSG first reachable.
CBTC, found RPU , counting number actions gives us heuristic value
G (BS ) = 3.
hM
G
RP U
4.4 Labelled Uncertainty Graph Heuristics (LU G)
multiple graph technique advantage heuristics aggregate costs multiple
worlds, disadvantage computing redundant information different graphs (c.f.
G
Figure 6) using every graph compute heuristics (c.f hM
RP U ). next approach addresses
limitations condensing multiple planning graphs single planning graph, called
labelled uncertainty graph (LU G). idea implicitly represent multiple planning graphs
collapsing graph connectivity one planning graph, use annotations, called labels (),
retain information multiple worlds. could construct LU G generating
multiple graphs taking union, instead define direct construction procedure.
start manner similar unioned single planning graph (SGU ) constructing initial
layer literals source belief state. difference LU G prevent
loss information multiple worlds keeping label literal records
worlds relevant. discuss, use simple techniques propagate
60

fiP LANNING G RAPH H EURISTICS B ELIEF PACE EARCH

labels actions effects label subsequent literal layers. Label propagation relies
expressing labels propositional formulas using standard propositional logic operations.
end product single planning graph labels graph elements; labels indicate
explicit multiple graphs (if build them) contain graph element.
trading planning graph structure space label storage space. choice BDDs
represent labels helps lower storage requirements labels. worst-case complexity
LU G equivalent G representation. LU Gs complexity savings realized
projected possible worlds relevant actions completely disjoint; however,
often appear practice. space savings comes two ways: (1) redundant representation actions literals avoided, (2) labels facilitate non-redundant representation
stored BDDs. nice feature BDD package (Brace, Rudell, & Bryant, 1990) use
efficiently represents many individual BDDs shared BDD leverages common substructure. Hence, practice LU G contains information G much lower
construction usage costs.
section present construction LU G without mutexes, describe
introduce mutexes, finally discuss extract relaxed plans.
4.4.1 L ABEL P ROPAGATION
Like single graph multiple graphs, LU G based IP P (Koehler et al., 1997)
planning graph. extend single graph capture multiple world causal support, present
multiple graphs, adding labels elements action A, effect E, literal L layers.
denote label literal l level k k (l). build LU G belief state BSP ,
illustrate BSP = BSI CBTC example. label formula describing set states (in
BSP ) graph element (optimistically) reachable. say literal l reachable
set states, described BS, k levels, BS |= k (l). instance, say arm
reachable two levels L2 contains arm BSI |= 2 (arm), meaning models
worlds arm holds two levels superset worlds current belief state.
intuitive definition LU G planning graph skeleton, represents causal relations,
propagate labels indicate specific possible world support. show skeleton
CBTC Figure 7. Constructing graph skeleton largely follows traditional planning graph
semantics, label propagation relies simple rules. initial layer literal labelled,
indicate worlds BSP holds, conjunction literal BSP .
action labelled, indicate worlds execution preconditions co-achieved,
conjunction labels execution preconditions. effect labelled, indicate
worlds antecedent literals actions execution preconditions co-achieved,
conjunction labels antecedent literals label associated action. Finally,
literals labelled, indicate worlds given effect, disjunction
labels effects previous level affect literal. following describe label
propagation detail work CBTC example.
Initial Literal Layer: LU G initial layer consisting every literal non false ()
label. initial layer label 0 (l) literal l identical lBSP , representing states
BSP l holds. labels initial layer literals propagated actions
effects label next literal layer, describe shortly. continue propagation
label literal changes layers, condition referred level off.
61

fiB RYCE , K AMBHAMPATI , & MITH

L0

A0

E0

L1

A1

E1

L2

inP1

inP1

inP1

: inP1

: inP1

: inP1

inP2

inP2

inP2

: inP2

: inP2

DunkP1

1(DunkP1)
0(DunkP1)

DunkP2

: inP2
: arm

1(DunkP2)
0(DunkP2)

arm

arm

arm

clog

clog

clog

Flush

0(Flush)

: clog

Flush

0(Flush)

: clog

G
Figure 7: LU G skeleton CBTC, mutexes. relaxed plan hLU
RP shown
bold.

LU G CBTC, shown Figure 7 (without labels), using BSP =BSI initial literal
layer:
L0 = {inP1, inP2, inP2, inP1, clog, arm}
0 (inP1) = 0 (inP2) = (arm clog inP1 inP2),
0 (inP2) = 0 (inP1) = (arm clog inP1 inP2),
0 (clog) = 0 (arm) = BSP
Notice inP1 inP2 labels indicating respective initial states hold,
clog arm BSP label hold states BSP .
Action Layer: previous literal layer Lk computed, construct label action
layer Ak . Ak contains causative actions action set A, plus literal persistence. action
included Ak label false (i.e. k (a)
=). label action level k, equivalent
extended label execution precondition:
k (a) = k (e (a))
Above, introduce notation extended labels k (f ) formula f denote worlds
BSP reach f level k. say propositional formula f reachable BS
62

fiP LANNING G RAPH H EURISTICS B ELIEF PACE EARCH

k levels BSi |= k (f ). Since labels literals, substitute labels
literals literals formula get extended label formula. extended label
propositional formula f level k, defined:
k (f f ) = k (f ) k (f ),
k (f f ) = k (f ) k (f ),

k ((f f )) = k (f f ),
k ((f f )) = k (f f ),
k () = BSP ,
k () =,
k (l) = k (l)
zeroth action layer CBTC is:
A0 = {Flush, inP1p , inP2p , inP2p , inP1p , clogp , armp }
0 (Flush) = BSP ,
0 (inP1p ) = 0 (inP2p ) = (arm clog inP1 inP2),
0 (inP2p ) = 0 (inP1p ) = (arm clog inP1 inP2),
0 (clogp ) = 0 (armp ) = BSP
literal persistence label identical label corresponding literal
previous literal layer. Flush action BSP label always applicable.
Effect Layer: effect layer Ek depends literal layer Lk action layer Ak . Ek
contains effect j (a) effect non false label (i.e. k (j (a))
=).
action effect must applicable world, label effect level k
conjunction label associated action extended label antecedent
k (j (a)) = k (a) k (j (a))
zeroth effect layer CBTC is:
E0 = {0 (Flush), 0 (inP1p ), 0 (inP2p ), 0 (inP2p ),
0 (inP1p ), 0 (clogp ), 0 (armp )}
0 (0 (Flush)) = BSP
0 (0 (inP1p )) = 0 (0 (inP2p )) = (arm clog inP1 inP2),
0 (0 (inP2p )) = 0 (0 (inP1p )) = (arm clog inP1 inP2),
0 (0 (clogp )) = 0 (0 (armp )) = BSP
Again, like action layer, unconditional effect literal persistence label identical corresponding literal previous literal layer. unconditional effect Flush
label identical label Flush.
Literal Layer: literal layer Lk depends previous effect layer Ek1 , contains
literals non false labels (i.e. k (l)
=). effect j (a) Ek1 contributes label
literal l effect consequent contains literal l. label literal disjunction
labels effect previous effect layer gives literal:

k1 (j (a))
k (l) =
j (a):lj (a),
j (a)Ek1

63

fiB RYCE , K AMBHAMPATI , & MITH

first literal layer CBTC is:
L1 = {inP1, inP2, inP2, inP1, clog, clog, arm}
1 (inP1) = 1 (inP2) = (arm clog inP1 inP2),
1 (inP2) = 1 (inP1) = (arm clog inP1 inP2),
1 (clog) = 1 (clog) = 1 (arm) = BSP
literal layer identical initial literal layer, except clog goes false
label (i.e. existing layer) label BSP .
continue level one action layer L1 indicate BSG reachable
BSP (arm
L1 ). Action layer one defined:
A1 = {DunkP1, DunkP2, Flush, inP1p , inP2p , inP2p , inP1p , clogp , armp , clogp }
1 (DunkP1) = 1 (DunkP2) = 1 (Flush) = BSP ,
1 (inP1p ) = 1 (inP2p ) = (arm clog inP1 inP2),
1 (inP2p ) = 1 (inP1p ) = (arm clog inP1 inP2),
1 (clogp ) = 1 (armp ) = 1 (clogp ) = BSP
action layer similar level zero action layer. adds Dunk actions
executable. also add persistence clog. Dunk action gets label identical
execution precondition label.
level one effect layer is:
E1 = {0 (DunkP1), 0 (DunkP2), 1 (DunkP1), 1 (DunkP2), 0 (Flush), 0 (inP1p ),
0 (inP2p ), 0 (inP2p ), 0 (inP1p ), 0 (clogp ), 0 (armp ), 0 (clogp )}
1 (0 (DunkP1)) = 1 (0 (DunkP2)) = 1 (0 (Flush)) = BSP
1 (1 (DunkP1)) = (arm clog inP1 inP2),
1 (1 (DunkP2)) = (arm clog inP1 inP2),
1 (0 (inP2p )) = 1 (0 (inP1p )) = (arm clog inP1 inP2),
1 (0 (inP1p )) = 1 (0 (inP2p )) = (arm clog inP1 inP2),
1 (0 (clogp )) = 1 (0 (armp )) = 1 (0 (clogp )) = BSP
conditional effects Dunk actions CBTC (Figure 7) labels indicate
possible worlds give arm antecedents hold possible
worlds. example, conditional effect 1 (DunkP1) label found taking conjunction actions label BSP antecedent label 1 (inP1) obtain (arm clog inP1
inP2).
Finally, level two literal layer:
L2 = {inP1, inP2, inP2, inP1, clog, clog, arm, arm}
2 (inP1) = 2 (inP2) = (arm clog inP1 inP2),
2 (inP2) = 2 (inP1) = (arm clog inP1 inP2),
2 (clog) = 2 (clog) = 2 (arm) = 2 (arm) = BSP
labels literals level 2 CBTC indicate arm reachable BSP label entailed BSP . label arm found taking disjunction
labels effects give it, namely, (arm clog inP1 inP2) conditional
64

fiP LANNING G RAPH H EURISTICS B ELIEF PACE EARCH

effect DunkP1 (arm clog inP1 inP2) conditional effect DunkP2,
reduces BSP . Construction could stop BSP entails label goal
k (armclog)= k (arm) k (clog) = BSP BSP = BSP . However, level occurs
next level change labels literals.
level occurs level three example, say BS, BS |=
BSP , formula f reachable k steps BS |= k (f ). level k exists, f
reachable BS. level k, f reachable BS, first k
lower bound number parallel plan steps needed reach f BS. lower bound
similar classical planning max heuristic (Nguyen et al., 2002). provide
informed heuristic extracting relaxed plan support f respect BS, described shortly.
4.4.2 AME -W ORLD L ABELLED UTEXES
several types mutexes added LU G. start with, concentrate
evolve single possible world same-world mutexes effective
well relatively easy understand. extend mutex propagation used
multiple graphs mutexes one planning graph. savings computing mutexes
LU G instead multiple graphs reduce computation mutex exits
several worlds. Appendix B describe handle cross-world mutexes, despite lack
effectiveness experiments conducted. Cross-world mutexes extend LU G compute
set mutexes found CGP (Smith & Weld, 1998).
Same-world mutexes represented single label, k (x1 , x2 ), two elements
(actions, effect, literals). mutex holds elements x1 x2 worlds
|= k (x1 , x2 ). elements mutex world, assume label mutex
false . discuss labelled mutexes discovered propagated
actions, effect relations, literals.
using mutexes, refine means formula f reachable set
worlds BSP . must ensure every state BSP , exists state f reachable.
state f reachable state BSP two literals
mutex world BSP |= k (S).
action, effect, literal layers multiple ways pair
elements become mutex (e.g. interference competing needs). Thus, mutex label pair
disjunction labelled mutexes found pair means.
Action Mutexes: same-world action mutexes level k set labelled pairs actions.
pair labelled formula indicates set possible worlds actions
mutex. possible reasons mutex actions interference competing needs.

Interference Two actions a, interfere (1) unconditional effect consequent 0 (a)
one inconsistent execution precondition e (a ) other, (2) vice versa.
additionally interfere (3) unconditional effect consequents 0 (a) 0 (a )
inconsistent, (4) execution preconditions e (a) e (a ) inconsistent. mutex
exist possible world projections k (a, ) = BSP . Formally, interfere
65

fiB RYCE , K AMBHAMPATI , & MITH

one following holds:
(1) 0 (a) e (a ) =
(2) e (a) 0 (a ) =
(3) 0 (a) 0 (a ) =
(4) e (a) e (a ) =
Competing Needs Two actions a, competing needs world pair literals
execution preconditions mutex world. worlds
mutex competing needs described by:


k (a) k (a )

k (l, l )

lj (a),l j (a )

formula find worlds pair execution preconditions l e (a), l
e (a ) mutex actions reachable.
Effect Mutexes: effect mutexes set labelled pairs effects. pair labelled
formula indicates set possible worlds effects mutex. possible reasons
mutex effects associated action mutexes, interference, competing needs, induced effects.
Mutex Actions Two effects (a) (a), j (a ) (a ) mutex worlds
associated actions mutex, k (a, ).
Interference Like actions, two effects (a), j (a ) interfere (1) consequent (a)
one inconsistent antecedent j (a ) other, (2) vice versa. additionally interfere (3) effect consequents (a) j (a ) inconsistent, (4)
antecedents (a) j (a ) inconsistent. mutex exist possible world projections, label mutex k (i (a), j (a )) = BSP . Formally, (a) j (a )
interfere one following holds:
(1) (a) j (a ) =
(2) (a) j (a ) =
(3) (a) j (a ) =
(4) (a) j (a ) =
Competing Needs Like actions, two effects competing needs world pair
literals antecedents mutex world. worlds (a) j (a )
competing needs mutex are:
k (i (a)) k (j (a ))



k (l, l )

li (a),l j (a )

formula find worlds pair execution preconditions l (a), l
j (a ) mutex actions reachable.
66

fiP LANNING G RAPH H EURISTICS B ELIEF PACE EARCH

Lk

lk(p)
p

Ek

Ak

lk(a)


h(a)

lk(h(a))



lk(p, q)
Induced mutex worlds:

lk(j(a),h(a))lk(i(a))



lk(j(a), h(a))

lk(q)
q
lk(a)


j(a)

lk(r)
r

lk(j(a))
i(a) induces j(a) in:
lk(i(a))lk(j(a))

i(a) lk(i(a))

Figure 8: Effect (a) induces effect j (a). j (a) mutex h (a ), (a) induced mutex
h (a ).

Induced induced effect j (a) effect (a) effect action
may execute time. effect induced another possible worlds
reachable. example, conditional effect action always induces
unconditional effect action.
Induced mutexes, involving inducing effect (a), come induced effect
j (a) mutex another effect h (a ) (see Figure 8). induced mutex
(a) effect h (a ) mutex induced effect j (a) (b) inducing effect
(a). label mutex conjunction label mutex k (j (a), h (a ))
label induced effect j (a). additional discussion methodology behind
induced mutexes refer Smith Weld (1998).

Literal Mutexes: literal mutexes set labelled pairs literals. pair labelled
formula indicates set possible worlds literals mutex. reason
mutex literals inconsistent support.
Inconsistent Support Two literals inconsistent support possible world level k
two non-mutex effects support literals world. label
literal mutex level k disjunction worlds inconsistent support.
worlds inconsistent support mutex l l are:
67

fiB RYCE , K AMBHAMPATI , & MITH





S:i (a),j (a )E

k1 ,
li (a),l j (a ),
S|=k1 (i (a),j (a ))

meaning formula two literals mutex worlds
pairs effects support literals mutex S.
4.4.3 LU G H EURISTICS
heuristics computed LU G capture measures similar G heuristics,
exists new opportunity make use labels improve heuristic computation efficiency. single
planning graph sufficient state aggregation measured, mention
measures LU G.
Positive Interaction Aggregation: Unlike G heuristics, compute positive interaction
based relaxed plans LU G. G approach measure positive interaction across
state belief state compute multiple relaxed plans take maximum value. get
measure LU G would still need extract multiple relaxed plans, situation
trying avoid using LU G. graph construction overhead may lowered using
LU G, heuristic computation could take long. Hence, compute relaxed plans
LU G measure positive interaction alone, compute relaxed plans measure
overlap (which measures positive interaction).
Independence Aggregation: Like positive interaction aggregation, need relaxed plan every
state projected belief state find summation costs. Hence, compute
relaxed plans assume independence.
G
State Overlap Aggregation: relaxed plan extracted LU G get hLU
RP heuristic

G

G
resembles unioned relaxed plan hRP U heuristic. Recall hRP U heuristic extracts
relaxed plan multiple planning graphs (one possible world) unions
set actions chosen level relaxed plans. LU G relaxed plan heuristic
similar counts actions positive interaction multiple worlds
accounts independent actions used subsets possible worlds. advantage
G
hLU
RP find actions single pass one planning graph.
trading cost computing multiple relaxed plans cost manipulating LU G
labels determine lines causal support used worlds. relaxed plan
want support goal every state BSP , need track states
BSP use paths planning graph. subgoal may several different (and possibly
overlapping) paths worlds BSP .
RP
RP
RP
RP
RP
RP
LU G relaxed plan set layers: {ARP
0 , E0 , L1 , ..., Ab1 , Eb1 , Lb }, Ar
RP
RP
set actions, Er set effects, Lr+1 set clauses. elements layers
labelled indicate worlds BSP chosen support. relaxed plan
G
extracted level b = hLU
level (BSi ) (i.e., first level BSi reachable, also described
Appendix A).
Please note extracting relaxed plan BSi terms clauses, literals, different SG G versions relaxed plans. Previously found

68

fiP LANNING G RAPH H EURISTICS B ELIEF PACE EARCH

constituent BSi first reached planning graph commit
one constituent. rationale possibly using different constituents
multiple graphs, condensed version multiple graphs still want able
support different constituents BSi different worlds. could also use constituent representation BSi defining layers relaxed plan, choose clausal representation
BSi instead know support clause. However constituents
know need support one (but dont need know one).
relaxed plan, shown bold Figure 7, BSI reach BSG CBTC listed follows:
= {inP1p , inP2p , Flush},
ARP
0
RP
0 (inP1p ) = (arm clog inP1 inP2),
RP
0 (inP2p ) = (arm clog inP1 inP2),
RP
0 (Flush) = BSP ,
E0RP = {0 (inP1p ), 0 (inP2p ), 0 (Flush)},
0
RP
0 ( (inP1p )) = (arm clog inP1 inP2),
0
RP
0 ( (inP2p )) = (arm clog inP1 inP2),
RP
0 (0 (Flush)) = BSP ,
= {inP1, inP2, clog},
LRP
1
RP
1 (inP1) = (arm clog inP1 inP2),
RP
1 (inP2) = (arm clog inP1 inP2),
RP
1 (clog) = BSP ,
= {DunkP1, DunkP2, clogp },
ARP
1
RP
1 (DunkP1) = (arm clog inP1 inP2),
RP
1 (DunkP2) = (arm clog inP1 inP2),
RP
1 (clogp ) = BSP ,
E1RP = {1 (DunkP1), 1 (DunkP2), 0 (clogp )},
1
RP
1 ( (DunkP1)) = (arm clog inP1 inP2),
1
RP
1 ( (DunkP2)) = (arm clog inP1 inP2),
RP
1 (0 (clogp )) = BSP ,
= {arm, clog},
LRP
2
RP
2 (arm) = BSP ,
RP
2 (clog) = BSP
start forming LRP
clauses (BSG ), namely arm clog; label
2
clauses BSP need supported states belief state. Next,
support clause LRP
relevant effects E1 form E1RP . clog use
2
persistence supports clog worlds described BSP (this example positive
interaction worlds). arm relevant effects respective 1 Dunk action.
choose effects support arm need support arm worlds BSP ,
effect gives support one world (this example independence worlds).
appropriate label indicating
insert actions associated chosen effect ARP
1
69

fiB RYCE , K AMBHAMPATI , & MITH

worlds needed, general fewer worlds reachable (i.e.
RP execution preconditions
always case RP
r () |= r ()). Next form L1
actions ARP
antecedents effects E1RP , clog, inP1, inP2, labelled
1
worlds action effect needed them. fashion level two, support
literals level one, using persistence inP1 inP2, Flush clog. stop here,
supported clauses level one.
general case, extraction starts level b BSi first reachable BSP .
RP
RP
RP contains clauses
first relaxed plan layers construct ARP
b1 , Eb1 , Lb , Lb
RP
C (BSi ), labelled k (C) = BSP .
choosing relevant effects
level r, 1 r b, support clause LRP
r
RP . effect j (a) relevant reachable worlds
Er1 form Er1
need support C (i.e. r1 (j (a)) RP
r (C)
=) consequent gives literal l C.
clause, choose enough supporting effects chosen effect worlds
superset worlds need support clause, formally:









RP
RP
j

(C)
|=

(
(a))
CLRP


r
r1
r
j

(a):lj (a),

lC,
j (a)Er1

think supporting clause set worlds set cover problem effects cover
subsets worlds. algorithm cover worlds clause worlds effects variant
well known greedy algorithm set cover (Cormen, Leiserson, & Rivest, 1990). first
choose relevant persistence effects cover worlds, choose action effects cover
RP labelled new
new worlds. effect choose support added Er1
RP
worlds covered C. clauses Lr covered, form action layer ARP
r1
RP . actions ARP labelled indicate worlds
actions effect Er1
r1
RP .
effects labelled Er1
obtain next subgoal layer, LRP
r1 , adding literals execution preconditions
RP
RP . literal l LRP labelled indicate
actions Ar1 antecedents effects Er1
r1
worlds action effect requires l. support literals LRP
r1 fashion
.

continue

support
literals

effects,
insert
actions,

insert action effect
LRP
r
RP
preconditions supported literals L1 .
G
get relaxed plan, relaxed plan heuristic, hLU
RP (BSi ), summation
number actions action layer, formally:
G
hLU
RP (BSi )

=

b1


| ARP
|


i=0
G
Thus CBTC example hLU
RP (BSG ) = 3. Notice construct LU G
without mutexes CBTC reach goal two layers. included mutexes
LU G, would reach goal three layers. way use mutexes change
relaxed plan use mutexes influence relaxed plan extraction. Mutexes help
identify belief state BSi reachable BSP .

70

fiP LANNING G RAPH H EURISTICS B ELIEF PACE EARCH

Problem

PDDL Parser
(IPC)
Actions

Belief
States

Search Engine
(HSP-r: CAltAlt/
LAO*: POND)

Heuristics

BDDs
(CUDD)

Labels

(POND only)

Planning
Graph(s)
(IPP)

Figure 9: implementations CAltAlt P rely many existing technologies.
search engine guided heuristics extracted planning graphs.

5. Empirical Evaluation: Setup
section presents implementation CAltAlt P planners domains
use experiments. tests run Linux x86 machine 2.66GHz P4 processor
1GB RAM timeout 20 minutes. CAltAlt P used heuristic weight
five the, respective, A* AO* searches. compare competing approaches (CGP,
SGP, GPT v1.40, MBP v0.91, KACMBP, YKA, CFF) several domains problems.
planners domain problem files compared planners found
online appendix.
5.1 Implementation
implementation CAltAlt uses several off-the-shelf planning software packages. Figure 9
shows diagram system architecture CAltAlt P D. CAltAlt extends
name AltAlt, relies limited subset implementation. components CAltAlt
IPC parser PDDL 2.1 (slightly extended allow uncertain initial conditions), HSPr search engine (Bonet & Geffner, 1999), IPP planning graph (Koehler et al., 1997),
CUDD BDD package (Brace et al., 1990) implement LU G labels. custom parts
implementation include action representation, belief state representation, regression operator,
heuristic calculation.
implementation P similar CAltAlt aside search engine,
state action representation. P also uses IPP source code planning graphs. P
uses modified LAO* (Hansen & Zilberstein, 2001) source code Eric Hansen perform AO*
71

fiB RYCE , K AMBHAMPATI , & MITH

Problem
Rovers1
Rovers2
Rovers3
Rovers4
Rovers5
Rovers6
Logistics1
Logistics2
Logistics3
Logistics4
Logistics5
BT(n)
BTC(n)
CubeCenter(n)
Ring(n)

Initial
States
1
2
3
4
16
12
2
4
2
4
8
n
n
n3
n3n

Goal
Literals
1
1
1
1
3
3
1
2
1
2
3
1
1
3
n

Fluents
66
66
66
66
71
119
29
36
58
68
78
n+1
n+2
3n
4n

Causative
Actions
88
88
88
88
97
217
70
106
282
396
510
n
n+1
6
4

Observational
Actions
0 {12}
0 {12}
0 {12}
0 {12}
0 {12}
0 {18}
0 {10}
0 {20}
0 {21}
0 {42}
0 {63}
0 {n}
0 {n}
0
0

Optimal
Parallel
5 {5}
8 {7}
10 {?}
13 {?}
? {?}
? {?}
6 {6}
6 {?}
8 {?}
8 {?}
? {?}
1 {1}
2n-1 {2}
(3n-3)/2
3n-1

Optimal
Serial
5 {5}
8 {7}
10 {8}
13 {10}
20 {?}
? {?}
9 {7}
15 {12}
11 {8}
18 {?}
28 {?}
n {n-1}
2n-1 {n-1}
(9n-3)/2
3n-1

Table 2: Features test domains problems - Number initial states, Number goal literals, Number fluents, Number causative actions, Number Observational Actions,
Optimal number parallel plan steps, Optimal number serial plan steps. Data conditional versions domains braces; plan lengths conditional plans maximum
conditional branch length.

search, CUDD (Brace et al., 1990) represent belief states actions. Even deterministic
actions possible obtain cycles actions observations planning
belief space. P constructs search graph directed acyclic graph employing cyclechecking algorithm. adding hyper-edge search graph creates cycle, hyper-edge
cannot represent action strong plan hence added graph.
5.2 Domains
Table 2 shows relative features different problems used evaluate approach. table shows number initial states, goal literals, fluents, actions, optimal
plan lengths. used guide gauge difficulty problems, well
performance.
Conformant Problems addition standard domains used conformant planningsuch
Bomb-in-the-Toilet, Ring, Cube Center, also developed two new domains Logistics
Rovers. chose new domains difficult subgoals, many
plans varying length.
Ring domain involves ring n rooms room connected two adjacent
rooms. room window open, closed, locked. goal every
window locked. Initially, state possible could room window could
configuration. four actions: move right, move left, close window current
72

fiP LANNING G RAPH H EURISTICS B ELIEF PACE EARCH

room, lock window current room. Closing window works window
open, locking window works window closed. good conformant plan involves
moving one direction closing locking window room.
Cube Center domain involves three-dimensional grid (cube) six actions
possible move two directions along dimension. dimension consists n possible
locations. Moving direction along grid points leaves one
position. Using phenomena, possible localize dimension repeatedly moving
direction. Initially possible location cube goal reach
center. good conformant plan involves localizing corner moving center.
Rovers domain conformant adaptation analogous domain classical planning
track International Planning Competition (Long & Fox, 2003). added uncertainty
initial state uses conditions determine whether image objective visible various
vantage points due weather, availability rock soil samples. goal upload
image objective rock soil sample data. Thus conformant plan requires visiting
possible vantage points taking picture, plus visiting possible locations soil
rock samples draw samples.
first five Rovers problems 4 waypoints. Problems one four one
four locations, respectively, desired imaging objective possibly visible (at least one
work, dont know one). Problem 5 adds rock soil samples part goal
several waypoints one obtained (again, dont know waypoint
right sample). Problem 6 adds two waypoints, keeps goals Problem
5 changes possible locations rock soil samples. cases waypoints
connected tree structure, opposed completely connected.
Logistics domain conformant adaptation classical Logistics domain trucks
airplanes move packages. uncertainty initial locations packages. Thus, actions
relating movement packages conditional effect predicated package
actually location. conformant version, drivers pilots cannot sense communicate packages actual whereabouts. problems scale adding packages cities.
Logistics problems consist one airplane, cities airport, post office,
truck. airplane travel airports trucks travel within cities. first
problem two cities one package could start either post office, goal get
package second citys airport. second problem adds another package
possible starting points destination. third problem three cities
one package could post office reach third airport. fourth problem
adds second package third problem starting ending locations. fifth
problem three cities three packages, one two three post offices
reach different airports.
Conditional Problems conditional planning consider domains literature: Bombin-the-Toilet sensing BTS, Bomb-in-the-Toilet clogging sensing BTCS. also
extend conformant Logistics Rovers include sensory actions.
Rovers problem allows rover, particular waypoint, sense availability image, soil, rock data location. locations collectable data expressed
one-of constraints, rover deduce locations collectable data failing sense
possibilities.
73

fiB RYCE , K AMBHAMPATI , & MITH

Logistics observations determine package location exists, observation
assumed made driver pilot particular location. Since several drivers
pilot, different agents make observations. information gained agents assumed
automatically communicated others, planner agent knowledge.5

6. Empirical Evaluation: Inter-Heuristic Comparison
start comparing heuristic approaches within planners. next section, continue
describing planners, using best heuristics, compare state art
approaches. section intend validate claims belief space heuristics measure
overlap perform well across several domains. justify using LU G multiple
planning graphs applying mutexes improve heuristics regression pruning belief
states.
compare many techniques within CAltAlt P conformant planning domains, addition test heuristics P conditional domains. performance metrics include total planning time number search nodes expanded. Additionally, discussing mutexes analyze planning graph construction time. proceed
showing heuristics perform CAltAlt various mutex computation schemes
LU G affect performance. present P performs different
heuristics conformant conditional domains, explore effect sampling proportion
worlds build SG1 , G, LU G graphs, compare heuristic estimates P
optimal plan length gauge heuristic accuracy. finish summary important
conclusions.
compute mutexes planning graphs CAltAlt planning graph(s)
built search episode mutexes help prune inconsistent belief states encountered
regression search. abstain computing mutexes P progression
build new planning graphs search node want keep graph computation time low.
exception discussion sampling worlds construct planning graphs,
planning graphs constructed deterministically. means single graph unioned
single graph SGU , G LU G graphs built possible worlds.
6.1 CAltAlt
results CAltAlt conformant Rovers, Logistics, BT, BTC domains, terms
total time number expanded search nodes, presented Table 3. show number
expanded nodes gives indication well heuristic guides planner. total
time captures amount time computing heuristic searching. high total time
high number search nodes indicates poor heuristic, high total time low number
search nodes indicates expensive informed heuristic.
discuss Ring Cube Center domains CAltAlt cannot solve
even smallest instances. Due implementation details planner performs poorly
domains actions several conditional effects hence scale. trouble stems
5. problem may interesting investigate multi-agent planning scenario, assuming global communication
(e.g. radio dispatcher).

74

fiP LANNING G RAPH H EURISTICS B ELIEF PACE EARCH

Problem
Rovers 1
2
3
4
5
6
Logistics 1
2
3
4
5
BT 2
10
20
30
40
50
60
70
80
BTC 2
10
20
30
40
50
60
70

h0
2255/5
49426/8

1108/9

19/2
4837/10

30/3
15021/19

-

hcard
18687/14

4268/9

14/2
56/10
418/20
1698/30
5271/40
12859/50
26131/60
48081/70
82250/80
16/3
161/19
1052/39
3823/59
11285/79
26514/99
55687/119
125594/140

hSG
RP
543/5
78419/8
91672/10

198/9
7722/15
3324/14
141094/19

18/2
5158/10

16/3
15679/19

-

G
hM
mRP
542/5
8327/8
20162/10
61521/16

183/9
15491/15
70882/14

20/2
8988/10

33/3
41805/19

-

G
hM
RP U
185/5
29285/9
2244/11
3285/15

1109/9
69818/19

21/2
342/10
2299/20
9116/30
44741/40

23/3
614/19
2652/39
9352/59
51859/79

-

LU G(F X)

hRP
15164/5
32969/8
16668/10
31584/13

1340/9
18535/15
16458/15
178068/19

12/2
71/10
569/20
2517/30
7734/40
18389/50
37820/60
70538/70
188603/80
18/3
1470/19
51969/39
484878/59

-

Table 3: Results CAltAlt conformant Rovers, Logistics, BT, BTC. data Total
Time / # Expanded Nodes, indicates time (20 minutes) - indicates
attempt.

weak implementation bringing general propositional formulas (obtained regression
several conditional effects) CNF.
describe results left right Table 3, comparing different planning graph
structures relaxed plans computed planning graph. start non-planning
graph heuristics h0 hcard . expected, h0 , breadth-first search, perform well
large portion problems, shown large number search nodes inability scale
solve larger problems. notice hcard heuristic performance good BT
BTC problems (this confirms results originally seen Bertoli, Cimatti, & Roveri, 2001a).
However, hcard perform well Rovers Logistics problems size
belief state, planning, necessarily indicate belief state good plan.
Part reason hcard works well domains measures knowledge, plans
domains largely based increasing knowledge. reason hcard performs poorly
domains finding causal support (which measure) important
knowledge domains.
75

fiB RYCE , K AMBHAMPATI , & MITH

Next, single planning graph (SGU ), CAltAlt reasonably well hSG
RP heuristic
Rovers Logistics domains, fails scale well BT BTC domains. Rovers
Logistics comparatively fewer initial worlds BT BTC problems. Moreover
deterministic plans, assuming initial state real state, somewhat similar Rovers
Logistics, mostly independent BT BTC. Therefore, approximating fully observable plan single graph relaxed plan reasonable plans achieving goal
world high positive interaction. However, without high positive interaction heuristic
degrades quickly number initial worlds increases.
multiple planning graphs, CAltAlt able perform better Rovers domain, takes
quite bit time Logistics, BT, BTC domains. Rovers, capturing distance estimates
individual worlds aggregating means tends better aggregating
worlds computing single distance estimate (as single graph). Logistics, part
reason computing multiple graphs costly computing mutexes
planning graphs. BT BTC, total time increases quickly number planning
graphs, number relaxed plans every search node increase much problems get larger.
G
MG
Comparing two multiple graph heuristics6 CAltAlt namely hM
mRP hRP U ,

G
see effect choices state distance aggregation. hmRP relaxed plan heuristic
aggregates state distances, found planning graph, taking maximum distance.
G
hM
RP U unions relaxed plans graph, counts number actions unioned
G
relaxed plan. single graph relaxed plan, hM
mRP relaxed plan essentially measures
one state state distance; thus, performance suffers BT BTC domains. However, using
unioned relaxed plan heuristic, capture independence among multiple worlds
scale better BT BTC. Despite usefulness unioned relaxed plan, costly
compute scalability limited, turn LU G version measure.
LU G(F X)

LU G, use hRP
heuristic CAltAlt. heuristic uses LU G
G
full cross-world mutexes (denoted F X). similar hM
RP U heuristic, measuring overlap
important, improving speed computing heuristic tends improve scalability
CAltAlt. CAltAlt slower Rovers BTC domains using LU G, note
added cost computing cross-world mutexes able improve
speed relaxing mutexes, describe shortly.
6.2 Mutexes
Mutexes used help determine belief state unreachable. Mutexes improve pruning
power heuristics accounting negative interactions. mutexes used improve
heuristics, reasonable compute subset mutexes. would like know
mutexes cost effective number possible mutexes find
quite large.
use several schemes compute subset mutexes. schemes combine different
types mutexes types cross-world checking. mutex types are: computing mutexes
(NX), computing static interference mutexes (StX), computing (StX) plus inconsistent support competing needs mutexes dynamic mutexes (DyX), computing (DyX) plus induced
mutexes full mutexes (FX). cross-world checking (see appendix B) reduction schemes are:
G
6. show hM
sRP P D.

76

fiProblem
Rovers 1
2
3
4
5
6
Logistics 1
2
3
4
5
BT 2
10
20
30
40
50
60
70
80
BTC 2
10
20
30
40
50
60
70

LU G(N X)

hRP
13/1112/51
20/904/41
13/8704/384

5/868/81
10/63699/1433

1/34/2
4/72/10
19/452/20
62/1999/30
130/6130/40
248/14641/50
430/30140/60
680/55202/70
1143/135760/80
0/62/3
4/93/19
21/546/39
58/2311/59
133/6889/79
260/15942/99
435/32201/119
742/62192/139

LU G(StX)

hRP
19/1119/51
16/903/41
17/8972/384

10/868/81
88/78448/1433

0/13/2
4/56/10
22/448/20
59/1981/30
132/6170/40
255/14760/50
440/29891/60
693/55372/70
1253/140716/80
1/16/3
4/77/19
32/545/39
61/2293/59
149/6879/79
261/16452/99
443/32923/119
745/61827/139

LU G(DyX)

hRP
15453/89/6
13431/138/8
17545/185/10
32645/441/14
698575/3569/45

1250/117/9
16394/622/15
17196/1075/15
136702/1035/19

0/13/2
13/57/10
120/453/20
514/1999/30
1534/6432/40
3730/14711/50
7645/30127/60
15019/55417/70
26478/132603/80
0/15/3
14/78/19
139/553/39
543/2288/59
1564/6829/79

-

LU G(F X)

hRP
15077/87/6
32822/147/8
16481/187/10
31293/291/14

1242/98/9
18114/421/15
16085/373/15
176995/1073/19

0/12/2
13/58/10
120/449/20
509/2008/30
1517/6217/40
3626/14763/50
7656/30164/60
14636/55902/70
26368/162235/80
4/14/3
1388/82/19
51412/557/39
482578/2300/59

-

LU G(DyXSX)

hRP
15983/87/6
10318/139/8
10643/185/10
14988/291/14
61373/3497/45
217507/3544/37
791/116/9
2506/356/15
10407/403/15
24214/648/19
52036/2690/41
0/16/2
12/59/10
102/450/20
421/1994/30
1217/6326/40
2866/14707/50
5966/30017/60
11967/55723/70
21506/136149/80
0/16/3
13/76/19
105/546/39
427/2294/59
1211/6798/79
2890/16184/99
6045/32348/119


LU G(DyXIX)

hRP
15457/87/6
10625/134/8
11098/209/10
16772/291/14
379230/3457/45
565013/3504/37
797/117/9
7087/428/15
10399/408/15
71964/871/19
328114/4668/52
0/15/2
14/59/10
139/454/20
600/2007/30
1822/6163/40
4480/14676/50
9552/30337/60
18475/55572/70
32221/105654/80
1/14/3
16/75/19
140/549/39
606/2300/59
1824/6816/79
4412/16414/99
9492/32350/119


LU G(F XSX)

hRP
15098/86/6
10523/138/8
10700/191/10
14726/290/14
60985/3388/45
225213/3408/37
796/115/9
2499/352/15
10214/387/15
23792/642/19
52109/2672/41
0/25/2
13/59/10
105/444/20
413/1986/30
1196/6113/40
2905/14867/50
5933/30116/60
11558/55280/70
21053/139079/80
1/13/3
14/75/19
110/555/39
444/2287/59
1253/6830/79
2926/16028/99
6150/32876/119


LU G(F XIX)

hRP
15094/85/6
14550/138/8
11023/184/10
16907/290/14
378869/3427/45
588336/3512/37
808/115/9
6968/401/15
10441/418/15
71099/858/19
324508/4194/52
0/13/2
14/56/10
137/454/20
596/2002/30
1797/6127/40
4392/14683/50
9234/29986/60
18081/55403/70
32693/109508/80
2/14/3
440/81/19
19447/568/39
199601/2401/59
1068019/6940/79

-

P LANNING G RAPH H EURISTICS B ELIEF PACE EARCH

G
Table 4: Results CAltAlt using hLU
RP mutex schemes. data Graph Construction
Time (ms)/All Time (ms)/# Expanded Nodes, indicates time (20 minutes)
- indicates attempt.

77

fiB RYCE , K AMBHAMPATI , & MITH

computing mutexes across same-worlds (SX) computing mutexes across pairs worlds
intersection (conjunction) element labels (IX).
Table 4 shows within CAltAlt, using relaxed plan heuristic changing way
compute mutexes LU G drastically alter performance. Often, cross-world mutexes
numerous building LU G takes much time. see could reduce graph
G
construction overhead without hindering performance, evaluated hLU
RP LUG built
(a) considering cross-world relations, schemes (NX), (StX), (DyX), (FX); (b)
same-world relations schemes (DyX-SX) (FX-SX), (c) cross-world relations
possible worlds pairs intersection elements labels (DyX-IX) (FX-IX).
results show simpler problems like BT BTC benefit much advanced
computation mutexes beyond static interference. However, Rovers Logistics problems, advanced mutexes play larger role. Mainly, interference, competing needs, inconsistent
support mutexes important. competing needs inconsistent support mutexes seem
large impact informedness guidance given LU G, scalability improves
here. Induced mutexes dont improve search time much, add graph computation
time. possible reason induced mutexes dont help much domains actions
two effects, unconditional conditional effect. Reducing cross-world mutex
checking also helps quite bit. seems checking same-world mutexes sufficient
solve large problems. Interestingly, G graphs compute same-world interference, competing
needs, inconsistent support mutexes within graph, equating scenario (DyXSX), however, LUG provides much faster construction time, evidenced LU Gs ability
out-scale G.
6.3 P
show total time number expanded nodes P solving conformant
problems (including Ring Cube Center) Table 5, P solving conditional
problems Table 6. CAltAlt show total time number expanded nodes
G
test. also add hM
sRP heuristic, implemented CAltAlt, takes summation
values relaxed plans extracted multiple planning graphs. compute mutexes
planning graphs used heuristics P mainly build planning
graphs search node. proceed first commenting performance P D,
different heuristics, conformant domains, discuss conditional domains.
conformant domains, P generally better CAltAlt. may attributed
part implementation-level details. P makes use existing (highly optimized) BDD
package belief state generation progression, previously mentioned, CAltAlt relies
less optimized implementation belief state generation regression. see next
section, regression planners employ sophisticated implementation perform much better,
could still benefit heuristics. Aside differences mention, see
similar trends performance various heuristics CAltAlt P D. Namely,
N G SG heuristics limited ability help planner scale, G heuristics help
planner scale better costly, LU G provides best scalability. difference
G LU G especially pronounced Cube Center Ring, size
initial belief state quite large instances scale. Interestingly Ring, breadth first
search single graph relaxed plan able scale due reduced heuristic computation time
78

fiP LANNING G RAPH H EURISTICS B ELIEF PACE EARCH

Problem
Rovers 1
2
3
4
5
6
Logistics 1
2
3
4
5
BT 2
10
20
30
40
50
60
70
80
BTC 2
10
20
30
40
50
60
70
CubeCenter 3
5
7
9
11
13
Ring 2
3
4
5
6
7
8
9
10

h0
540/36
940/249
3340/1150

560/169

450/3
760/1023

460/5
1090/2045

10/184
180/3198
1940/21703

20/15
20/59
30/232
160/973
880/4057
5940/16299
39120/64657
251370/261394


hcard
520/21
790/157
2340/755
14830/4067

530/102

460/2
590/428

460/4
970/1806

30/14
20/58
40/203
70/363
230/1010
700/2594
20/7
20/11
20/15
20/19
30/23
40/27
40/31
50/35
70/39

hSG
RP
590/6
700/15
3150/230
13480/1004

680/46

460/3
1560/1023

450/5
3160/2045

90/34
3510/1342
46620/10316
333330/46881

30/15
70/59
350/232
2270/973
14250/4057
83360/16299
510850/64657

-

G
hM
mRP
580/6
1250/32
3430/77
10630/181
85370/452
180890/416
970/58
2520/32
27820/927
5740/27
42980/59
450/2
6200/428

460/4
18250/1806

1050/61
60460/382

80/8
1500/41
51310/77

-

G
hM
sRP
580/6
750/10
1450/24
7000/163
12470/99
15780/38
730/21
6420/105
4050/83
29180/211
51380/152
450/2
820/10
6740/20
41320/30
179930/40
726930/50

460/3
980/19

370/9
11060/55
852630/359

80/7
500/8
6370/11
283780/16

-

G
hM
RP U
580/6
830/13
1370/23
2170/34
31480/73
31950/73
650/9
2310/20
2000/15
53470/382
471850/988
500/2
880/10
6870/20
44260/30
183930/40
758140/50

470/3
990/19
9180/39
54140/59
251140/79
1075250/99

0430/11
14780/82
1183220/444

80/8
920/19
19300/40

-

G
hLU
RP
590/6
680/11
850/16
1130/28
2050/36
9850/147
560/9
910/15
1130/14
3180/46
6010/42
460/2
520/10
1230/20
4080/30
11680/40
28420/50
59420/60
113110/70
202550/80
460/3
540/19
1460/39
4830/59
14250/79
34220/99
71650/119
134880/139
70/11
1780/205
27900/1774
177790/7226
609540/17027

30/8
70/10
250/24
970/44
4080/98
75020/574
388300/902

-

Table 5: Results P conformant Rovers, Logistics, BT, BTC, Cube Center, Ring.
data Total Time (ms)/# Expanded Nodes, indicates time - indicates
attempt.

low branching factor search. LU G able provide good search guidance, tends
take long time computing heuristics Ring.
also able compare choices aggregating distance measures reG
laxed plans multiple graphs. see taking maximum relaxed plans, hM
mRP ,
assuming positive interaction among worlds useful Logistics Rovers, loses independence worlds BT BTC domains. However, taking summation relaxed plan
79

fiB RYCE , K AMBHAMPATI , & MITH

Problem
Rovers 1
2
3
4
5
6
Logistics 1
2
3
4
5
BT 2
10
20
30
40
50
60
70
80
BTC 2
10
20
30
40
50
60
70

h0
550/36
1030/262
1700/467
5230/1321

530/118

460/5

450/6

-

hcard
480/21
550/36
590/48
620/58


460/3
470/19
510/39
620/59
850/79
1310/99
2240/119
24230/139
45270/159
460/3
480/19
510/39
660/59
970/79
1860/99
4010/119
7580/139

hSG
RP
580/6
780/15
3930/248
6760/387

740/46

450/3
111260/7197

470/5
271410/10842

-

G
hM
mRP
570/6
760/14
830/15
1020/20
16360/175
31870/173
580/10
1630/30
1360/20
4230/59
27370/183
460/3
970/19
9070/39
52410/59
207890/79
726490/99

470/3
1150/19
11520/39
62060/59
251850/79
941220/99

-

G
hM
sRP
570/6
710/12
830/15
1040/21
11100/232
24840/159
570/10
1300/36
1250/19
3820/57
19620/178
450/3
970/19
9060/39
52210/59
206830/79
719000/99

460/3
1140/19

-

G
hM
RP U
580/6
730/12
910/17
1070/21
12810/209
30250/198
600/10
1360/36
1290/19
3940/57
20040/178
470/3
1020/19
9380/39
55750/59
233720/79

470/3
1200/19
11610/39
64290/59
274610/79

-

G
hLU
RP
580/6
730/13
810/16
910/21
7100/174
13560/174
570/10
1250/36
1210/19
4160/57
20170/178
460/3
550/19
1610/39
5970/59
17620/79
43020/99
91990/119
170510/139
309940/159
470/3
590/19
1960/39
6910/59
19830/79
49080/99
103480/119
202040/139

Table 6: Results P conditional Rovers, Logistics, BTS, BTCS. data Total Time
(ms)/# Expanded Nodes, indicates time (20 minutes) - indicates
attempt.

G
values different worlds, hM
sRP able capture independence BT domain. notice
summation help P BTC domain; overestimate
heuristic value nodes counting Flush action world fact
G
needs done (i.e. miss positive interaction). Finally, using hM
RP U heuristic
well every domain, aside cost computing multiple graph heuristics,
account positive interaction independence taking overlap relaxed plans.
Again, LU G relaxed plan, analogous multiple graph unioned relaxed plan, P
scales well measure overlap lower cost computing heuristic significantly.

main change see using P versus CAltAlt direction search
different, hcard heuristic performs unlike before. BT BTC domains cardinality
work well progression size belief states change get closer
goal (it impossible ever know package contains bomb). However, regression
start belief state containing states consistent goal regressing actions limits
80

fiP LANNING G RAPH H EURISTICS B ELIEF PACE EARCH

belief state states reach goal actions. Thus regression
size belief states decreases, progression remains constant.
performance P conditional domains exhibits similar trends conformant domains, exceptions. Like conformant domains, G relaxed plans tend
outperform SG relaxed plan, LU G relaxed plan best overall. Unlike conformant
G
domains, hM
mRP performs much better BTS BTCS BT BTC partly
conditional plans lower average cost. hcard heuristic better BTS BTCS
BT BTC belief states actually decrease size partitioned
sensory actions.
6.4 Sampling Worlds
evaluations point considered effectiveness different heuristics, computed respect possible worlds belief state. would like use many
possible worlds can, reduce computation cost hopefully still get reasonable
heuristics considering subset worlds. scheme considering subsets worlds
heuristics sample single world (SG1 ), sample given percentage worlds
build multiple graphs, LU G.
MG
LU G
sampling approaches, use hSG
RP , hRP U , hRP relaxed plans. build
G LU G 10%, 30%, 50%, 70%, 90% worlds belief state, sampled
randomly. Figure 10, show total time taken (ms) solve every problem test set
(79 problems 10 domains). unsolved problem contributed 20 minutes total time.
comparison show previously mentioned heuristics: hSG
RP computed unioned single
U
graph SG , denoted Unioned compared sampled single graph SG1 denoted Single,
G
LU G
hM
RP U hRP computed worlds denoted 100%. total time heuristic
samples worlds averaged ten runs.
two major points see Figure 10. First, hSG
RP heuristic much effective
1
U
computed SG versus SG . SG1 less optimistic. builds
planning graph real world state, opposed union literals possible world states,
SGU . Respecting state boundaries considering single state better ignoring
state boundaries naively consider possible states. However, seen G
LU G heuristics, respecting state boundaries considering several states much better,
bringing us second point.
see different performance using possible worlds build multiple graphs
compared LU G. better using fewer worlds build multiple graphs
become costly number worlds increases. contrast, performance
improves possible worlds use LU G. Using possible worlds compute
heuristics good idea, takes efficient substrate exploit them.
6.5 Accuracy
heuristics account overlap possible worlds accurate
heuristics make assumption full positive interaction full independence. check
intuitions, compare heuristic estimates distance initial belief state
goal belief state heuristics used conformant problems solved P D. Figure
11 shows ratio heuristic estimate h(BSI ) optimal serial plan length h (BSI )
81

fiB RYCE , K AMBHAMPATI , & MITH

SG
MG
LUG

16

14

12

10

8

6

4

2

0
Unioned

Single

10%

30%

50%

70%

90%

100%

Figure 10: Total Time (hours) P solve conformant conditional problems
sampling worlds use heuristic computation.

several problems. points line (where ratio one) under-estimates,
over-estimates. problem instances shown optimal plan
length known.
G
MG

note domains hLU
RP hRP U heuristics close h , confirming

G

G

intuitions. Interestingly, hsRP hmRP close h Rovers Logistics;
whereas former close BT BTC problems, latter close CubeCenter
Ring. expected, assuming independence (using summation) tends over-estimate,
assuming positive interaction (using maximization) tends under-estimate. hSG
RP heuristic
tends under-estimate, cases (CubeCenter Ring) gives value zero (because
initial state satisfies goal). hcard heuristic accurate BT BTC,
under-estimates Rovers Logistics, over-estimates Cube Center Ring.

accuracy heuristics cases disconnected run time performance.
instance hcard highly overestimates Ring Cube Center, well domains
G
MG
exhibit special structure heuristic fast compute. hand, hLU
RP hRP U
82

fiP LANNING G RAPH H EURISTICS B ELIEF PACE EARCH

10000
1000
100
10
1
0.1
0.01
0.001

3
3
3
3

3
33
3
3
3
3


3
3


3











+





33333333
2
2

2

22
2
2++

2

2
2






+


33
22
222 ++
33333
+
33
+
3
33
2
33 + 3 3
+
2
+
+
2
2+
2+
+
2+
2+
2+
2+
2+
2+
2
2+
2+
2+
2
hcard 3
hSG
RP +

G
hmRP
2

G
hsRP
G
hM
RP U
G
hLU
RP

Rv1 Rv4 L1

L5 B10

B80 BC10
BC70 C3
Problem

C13 R2

R10

Figure 11: Ratio heuristic estimates distance BSI BSG optimal plan length.
Rv = Rovers, L = Logistics, B = BT, BC = BTC, C = Cube Center, R = Ring.

accurate many domains, suffer Ring Cube Center costly
compute.
6.6 Inter-Heuristic Conclusions
findings fall two main categories: one, effective estimates belief state distances
terms state state distances, two, exploit planning graphs support
computation distance measures.
comparing ways aggregate state distance measures compute belief state distances,
found measuring interaction single graph heuristics tends poorly guide planners,
measuring independence positive interaction worlds works well specific domains,
measuring overlap (i.e. combination positive interaction independence) tends work well
large variety instances. studying accuracy heuristics found
cases accurate effective. however find accurate
best cases.
Comparing graph structures provide basis belief state distance measures, found
heuristics extracted single graph fail systematically account independence positive interaction among different possible worlds. Despite lack distance
measure, single graphs still identify structure domains like Rovers Logistics.
accurately reflect belief state distances, multiple graphs reason reachability
world independently. accuracy comes cost computing lot redundant G structure limiting instances large belief states. reduce cost G structure
83

fiB RYCE , K AMBHAMPATI , & MITH

Planner
CAltAlt
P
MBP
KACMBP
CGP
SGP
GPT
YKA
CFF

Search Space
Belief Space
Belief Space
Belief Space
Belief Space
Planning Graph
Planning Graph
Belief Space
Belief Space
Belief Space

Search Direction
Backward
Forward
Forward/Backward
Forward
Backward
Backward
Forward
Backward
Forward

Conditional






Heuristic
Planning Graph
Planning Graph
Cardinality
Cardinality
Planning Graph
Planning Graph
State Space Plans
Cardinality
Planning Graph

Implementation
C
C
C
C
Lisp
Lisp
C
C
C

Table 7: Comparison planner features.
sampling worlds used construction. However planners able exhibit better scalability
considering worlds optimizing representation redundant structure
LU G. improvement scalability attributed lowering cost heuristic computation, retaining measures multiple state distances. LU G makes trade-off using
exponential time algorithm evaluation labels instead building exponential number
planning graphs. trade-off justified experiments.

7. Empirical Evaluation: Inter-Planner Comparison
first compare CAltAlt P several planners conformant domains,
compare P conditional planners conditional domains. purpose
section identify advantages techniques state art planners. end
section discussion general conclusions drawn evaluation.
7.1 Conformant Planning
Although work aimed giving general comparison heuristics belief space planning,
also present comparison best heuristics within CAltAlt P
leading approaches conformant planning. Table 7 lists several features evaluated
planners, search space, search direction, whether conditional, type
heuristics, implementation language. Note, since approach uses different planning
representation (BDDs, GraphPlan, etc.), even use heuristics, hard get
standardized comparison heuristic effectiveness. Furthermore, planners use PDDLlike input syntax; MBP, KACMBP use AR encodings may give advantage
reducing number literals actions. gave MBP planners grounded
filtered action descriptions used CAltAlt P D. also tried, report
results, giving MBP planners full set ground actions without filtering irrelevant actions.
appears MBP planners use sort action pre-processing performance
much worse full grounded set actions. Nevertheless, Table 8 compares MBP, KACMBP,
LU G(DyXSX)
G
GPT, CGP, YKA, CFF hRP
CAltAlt hLU
RP P respect
run time plan length.
MBP: MBP planner uses cardinality heuristic many cases overestimates plan distances
(as per implementation hcard ). MBP uses regression search conformant plans,
progression search conditional plans. interesting note difficult problem
84

fiP LANNING G RAPH H EURISTICS B ELIEF PACE EARCH

Problem
Rovers 1
2
3
4
5
6
Logistics 1
2
3
4
5
BT 2
10
20
30
40
50
60
70
80
BTC 2
10
20
30
40
50
60
70
CubeCenter 3
5
7
9
11
Ring 2
3
4
5
6
7
8

CAltAlt
LU G(DyXSX)
hRP U
16070/5
10457/8
10828/10
15279/13
64870/29
221051/25
907/9
2862/15
10810/15
24862/19
54726/34
16/2
71/10
552/20
2415/30
7543/40
17573/50
35983/60
67690/70
157655/80
16/3
89/19
651/39
2721/59
8009/79
19074/99
38393/119
65448/139


-

POND
G
hLU
RP
590/5
680/9
850/11
1130/16
2050/25
8370/25
560/9
910/15
1130/14
3180/22
6010/29
460/2
520/10
1230/20
4080/30
11680/40
28420/50
59420/60
113110/70
202550/80
460/3
540/19
1460/39
4820/59
14250/79
34220/99
71650/119
134880/139
70/9
1780/18
27900/29
177790/36
609540/47
30/6
70/8
250/13
970/17
4080/22
75020/30
388300/29

MBP

KACMBP

GPT

CGP

YKA

CFF

66/5
141/8
484/10
3252/13
OoM
727/32
37/9
486/24
408/14
2881/27
OoM
6/2
119/10
80/20
170/30
160/40
300/50
480/60
730/70
1080/80
8/3
504/19
98/39
268/59
615/79
1287/99
2223/119
3625/139
10/9
16/18
35/27
64/36
130/45
0/5
0/8
10/11
20/14
30/17
80/20
160/23

9293/5
9289/15
9293/16
9371/18
39773/40

127/12
451/19
1578/18
8865/22
226986/42
10/2
16/10
84/20
244/30
533/40
1090/50
2123/60
3529/70
1090/80
18/3
45/19
211/39
635/59
1498/79
10821/99
5506/119
2640/139
20/9
20/18
70/27
120/36
230/45
0/5
40/8
30/11
50/14
120/18
230/21
600/24

3139/5
4365/8
5842/10
7393/13
399525/20

916/9
1297/15
1711/11
9828/18
543865/28
487/2
627/10
472174/20

465/3
715/19
40/9
363/18
4782/27
42258/36
26549/45
31/5
35/8
60/11
635/14
51678/17

-

70/5
180/8
460/10
1860/13
OoM
60/6
290/6
400/8
1170/8

20/1
520/1
3200/1
10330/1
24630/1
49329/1
87970/1
145270/1

0/3
39370/19
28990/3


-

1220/7
2050/10
1740/12
2010/16
7490/27
24370/26
250/13
670/19
20280/21
17530/27
141910/40
0/2
0/10
20/20
80/30
160/40
250/50
420/60
620/70
3310/80
10/3
30/19
240/39
1210/59
3410/79
8060/50
15370/119
27400/139
0/9
0/19
20/34
80/69
190/68
0/5
0/8
20/11
80/14
110/17
300/20
480/23

70/5
30/8
10/10
10/13
18/22
21/23
10/9
12/15
14/12
12/18
25/28
0/2
30/10
4400/20
4500/30
26120/40
84730/50
233410/60
522120/70
979400/80
10/3
57/19
2039/39
23629/59
116156/79
334879/99

20/15
28540/45

360/12

-

LU G(DyXSX)

G
Table 8: Results CAltAlt using hRP
, P using hLU
RP , MBP, KACMBP, GPT,
CGP, YKA, CFF conformant Rovers, Logistics, BT, BTC, Cube Center, Ring.
data Total Time / # Plan Steps, indicates time (20 minutes), OoM
indicates memory (1GB), - indicates attempt.

instances Rovers Logistics domains MBP KACMBP tend generate much longer
plans planners. MBP outperform P cases find
solutions certain instances (like Rovers 5), likely heuristic. note
KACMBP MBP quite fast Cube Center Ring domains, trouble
domains like Rovers Logistics. illustrates heuristic modeling knowledge opposed
reachability well domains challenge uncertainty reachability.
85

fiB RYCE , K AMBHAMPATI , & MITH

Optimal Planners: optimal approaches (CGP GPT) tend scale well, despite
good solutions. CGP trouble constructing planning graphs parallel conformant plan
depth increases. CGP spends quite bit time computing mutexes, increases planning
cost plan lengths increase. CGP much better shallow parallel domains like BT,
find one step plans dunk every package parallel.
GPT performs progression search guided heuristic measures cost fully
observable plans state space. GPT finds optimal serial plans effective size
search space increases. GPT fails scale search space becomes difficult
even compute heuristic (due larger state space well).
YKA: YKA, like CAltAlt regression planner, search engine different YKA
uses cardinality heuristic. YKA performs well domains search engine
based BDDs. notice difference progression regression comparing P
YKA, similar trends found comparison P CAltAlt. Additionally,
seems YKA stronger regression search engine CAltAlt. P able better
YKA Rovers Logistics domains, unclear whether search
direction heuristics.
CFF: Conformant FF, progression planner using relaxed plan similar LU G relaxed plan,
well Rovers Logistics domains uses highly optimized FF search
engine well cheap compute relaxed plan heuristic. However, CFF well
BT, BTC, Cube Center, Ring problems many literals
entailed belief state. CFF relies implicitly representing belief states terms literals
entailed belief state, initial belief state, action history.
literals entailed belief state, reasoning belief state requires
inference action history. Another possible reason CFF suffers encodings.
Cube Center Ring domains naturally expressed multi-valued state features,
transformation binary state features describe values must hold also values
must hold. difficult CFF conditional effect antecedents contain several
literals heuristic restricted considering one literal. may CFF
choosing wrong literal simply enough literals get effective heuristics. However BT
BTC used one literal effect antecedents CFF still performs poorly.
7.2 Conditional Planning
Table 9 shows results testing conditional versions domains P D, MBP, GPT,
SGP, YKA.
MBP: P planner similar MBP uses progression search. P
uses AO* search, whereas MBP binary used uses depth first And-Or search. depth
first search used MBP contributes highly sub-optimal maximum length branches (as much
order magnitude longer P D). instance, plans generated MBP
Rovers domain rover navigating back forth locations several times
anything useful; situation beneficial actual mission use. MBP tends scale
well P domains tested. possible reason performance MBP
Logistics Rovers domains sensory actions execution preconditions,
prevent branching early finding deterministic plan segments branch. experimented
86

fiP LANNING G RAPH H EURISTICS B ELIEF PACE EARCH

Problem
Rovers 1
2
3
4
5
6
Logistics 1
2
3
4
5
BT 2
10
20
30
40
50
60
70
80
BTC 2
10
20
30
40
50
60
70

POND
G
hLU
RP
580/5
730/8
810/8
910/10
7100/19
13560/22
570/7
1250/12
1210/9
4160/15
20170/22
460/2
550/10
1610/20
5970/30
17620/40
43020/50
91990/60
170510/70
309940/80
470/2
590/10
1960/20
6910/30
19830/40
49080/50
103480/60
202040/70

MBP

GPT

SGP

YKA

3312/11
4713/75
5500/119
5674/146
16301/76
OoM
41/16
22660/177
2120/45
OoM
0/2
240/10
OoM
20/2
280/10
OoM
-

3148/5
5334/7
7434/8
11430/10

1023/7
5348/12
2010/8

510/2
155314/10
OoM
529/2
213277/10

-

70/5
760/7

5490/6

0/1
70/1
950/1
4470/1
13420/1
32160/1
90407/1
120010/1

10/2

-

3210/5
6400/7
7490/8
11210/10

1390/8


0/2
20/10
60/20
200/30
400/40
810/50
1350/60
2210/70
3290/80
0/4
210/12
2540/22
13880/32
46160/42
109620/52
221460/62
41374/72

G
Table 9: Results P using hLU
RP , MBP, GPT, SGP, YKA conditional Rovers, Logistics, BT, BTC. data Total Time / # Maximum possible steps execution,
indicates time (20 minutes), OoM indicates memory (1GB), -
indicates attempt.

MBP using sensory actions without execution preconditions able scale somewhat
better, plan quality much longer.
Optimal Planners: GPT SGP generate better solutions slowly. GPT better
Rovers Logistics problems exhibit positive interaction plans,
SGP well BT planning graph search well suited shallow, yet broad (highly
parallel) problems.
YKA: see YKA fares similar GPT Rovers Logistics, trouble scaling
reasons. think YKA may trouble regression sensory actions
since able scale reasonably well conformant version domains. Despite this,
YKA proves well BT BTC problems.
87

fiB RYCE , K AMBHAMPATI , & MITH

7.3 Empirical Evaluation Conclusions
internal comparisons heuristics within CAltAlt P D, well external comparisons several state art conformant conditional planners learned many
interesting lessons heuristics planning belief space.
Distance based heuristics belief space search help control conformant conditional plan
length because, opposed cardinality, heuristics model desirable plan quality metrics.
Planning graph heuristics belief space search scale better planning graph search
admissible heuristic search techniques.
planning graph heuristics presented, relaxed plans take account overlap
individual plans states source destination belief states
accurate tend perform well across many domains.
LUG effective planning graph regression progression search heuristics.
regression search, planning graphs maintain same-world mutexes provide best
trade-off graph construction cost heuristic informedness.
Sampling possible worlds construct planning graphs reduce computational cost,
considering worlds exploiting planning graph structure common possible worlds
(as LU G), efficient informed.
LUG heuristics help conditional planner, P D, scale conditional domains,
despite fact heuristic computation model observation actions.

8. Related Work & Discussion
discuss connections several related works involve heuristics and/or conditional planning first half section. second part section discuss extend
work directly handle non-deterministic outcomes actions heuristic computation.
8.1 Related Work
Much interest conformant conditional planning traced CGP (Smith & Weld, 1998),
conformant version GraphPlan (Blum & Furst, 1995), SGP (Weld et al., 1998), analogous
conditional version GraphPlan. graph search conducted several planning graphs,
constructed one possible initial states. recent work C-plan (Castellini
et al., 2001) Frag-Plan (Kurien et al., 2002) generalize CGP approach ordering
searches different worlds plan hardest satisfy world found first,
extended worlds. Although CAltAlt P utilize planning graphs
similar CGP Frag-plan uses compute reachability estimates. search
conducted space belief states.
Another strand work models conformant conditional planning search space
belief states. started Genesereth Nourbakhsh (1993), concentrated formulating set admissible pruning conditions controlling search. heuristics
choosing among unpruned nodes. GPT (Bonet & Geffner, 2000) extended idea consider
88

fiP LANNING G RAPH H EURISTICS B ELIEF PACE EARCH

simple form reachability heuristic. Specifically, computing estimated cost belief state,
GPT assumes initial state fully observable. cost estimate done terms
reachability (with dynamic programming rather planning graphs). GPTs reachability heuristic
G
similar hM
mRP heuristic estimate cost farthest (maximum distance) state looking deterministic relaxation problem. comparison GPT, CAltAlt
P seen using heuristics better job considering cost belief
state across various possible worlds.
Another family planners search belief states MBP-family plannersMBP
(Bertoli et al., 2001b), KACMBP (Bertoli & Cimatti, 2002). contrast CAltAlt similar P D, MBP-family planners represent belief states terms binary decision
diagrams. Action application modeled modifications BDDs. MBP supports progression regression space belief states, KACMBP pure progression planner.
computing heuristic estimates, KACMBP pro-actively reduces uncertainty belief
state preferring uncertainty reducing actions. motivation approach applying
cardinality heuristics belief states containing multiple states may give accurate enough direction search. reducing uncertainty seems effective idea, note (a)
domains may contain actions reduce belief state uncertainty (b) need uncertainty reduction may reduced heuristics effectively reason multiple
worlds (viz., multiple planning graph heuristics). Nevertheless, could fruitful integrate knowledge goal ideas KACMBP reachability heuristics CAltAlt P
handle domains contain high uncertainty costly goals.
contrast domain-independent approaches require models domain
physics, PKSPlan (Petrick & Bacchus, 2002) forward-chaining knowledge-based planner
requires richer domain knowledge. planner makes use several knowledge bases, opposed
single knowledge base taking form belief state. knowledge bases separate binary
multi-valued variables, planning execution time knowledge.
YKA (Rintanen, 2003b) regression conditional planner using BDDs uses cardinality heuristic. Recently Rintanen also developed related reachability heuristics consider
distances groups states, rely planning graphs (Rintanen, 2004).
recently, closely related work heuristics constructing conformant
plans within CFF planner (Hoffmann & Brafman, 2004). planner represents belief states
implicitly set known facts, action history (leading belief state), initial
belief state. CFF builds planning graph forward set known literals goal literals
backwards initial belief state. planning graph, conditional effects restricted
single literals antecedent enable tractable 2-cnf reasoning. planning graph,
CFF extracts relaxed plan represents supporting goal belief state states
initial belief state. biggest differences LU G CFF technique
LU G reasons forward source belief state (assuming explicit, albeit symbolic, belief
state), LU G restrict number literals antecedents. result, LU G
lose causal information perform backward reasoning initial belief state.
handling uncertainty labels label propagation reminiscent related
de Kleers assumption based truth maintenance system (ATMS) (de Kleer, 1986). ATMS
uses labels identify assumptions (contexts) particular statement holds, traditional
truth maintenance system requires extensive backtracking consistency enforcement identify
contexts. Similarly, reason multiple possible worlds (contexts)
89

fiB RYCE , K AMBHAMPATI , & MITH

LUG simultaneously, MG approach requires, backtracking, reproduction planning
graphs possible worlds.
Finally, CAltAlt P also related to, adaptation work reachability
heuristics classical planning, including AltAlt (Nguyen et al., 2002), FF (Hoffmann & Nebel,
2001) HSP-r (Bonet & Geffner, 1999). CAltAlt conformant extension AltAlt uses
regression search (similar HSP-r) guided planning graph heuristics. P similar FF
uses progression search planning graph heuristics.
8.2 Extension Non-Deterministic Actions
scope presentation evaluation restricted planning initial state uncertainty deterministic actions, planning graph techniques extended include
non-deterministic actions type described Rintanen (2003a). Non-deterministic actions
effects described terms set outcomes. simplicity, consider Rintanens
conditionality normal form, actions set conditional effects (as before)
consequent mutually-exclusive set conjunctions (outcomes) one outcome effect
result randomly. outline generalization single, multiple, labelled planning graphs
reason non-deterministic actions.
Single Planning Graphs: Single planning graphs, built approximate belief states
sampled state, lend straight-forward extension. single graph ignores
uncertainty belief state unioning literals sampling state form initial planning
graph layer. Continuing single graph assumptions uncertainty, makes sense treat
non-deterministic actions deterministic. Similar approximate belief state set
literals form initial literal layer sample state, assume non-deterministic effect
adds literals appearing effect samples outcome action deterministic
(i.e. gives set literals). Single graph relaxed plan heuristics thus remain unchanged.
Multiple Planning Graphs: Multiple planning graphs much like Conformant GraphPlan
(Smith & Weld, 1998). generalize splitting non-determinism current belief state
multiple initial literal layers splitting outcomes non-deterministic effects multiple
literal layers. idea root set new planning graphs level,
initial literal layer containing literals supported interpretation previous effect layer.
interpretations effect layer mean every possible set joint effect outcomes. set effect
outcomes possible two outcomes outcomes effect. Relaxed plan extraction
still involves finding relaxed plan planning graph. However, since planning graph
split many times (in tree-like structure) relaxed plan extracted path tree.
note technique likely scale exponential growth redundant
planning graph structure time. Further, experiments CGP enough trouble initial
state uncertainty. expect able much better LU G.
Labelled Uncertainty Graph: multiple planning graphs forced capture non
determinism splitting planning graphs initial literal layer, also
literal layer follows least one non-deterministic effect. saw LU G labels
capture non-determinism drove us split initial literal layer multiple graphs.
such, labels took syntactic form describes subsets states source belief
state. order generalize labels capture non-determinism resulting uncertain effects,
90

fiP LANNING G RAPH H EURISTICS B ELIEF PACE EARCH

need extend syntactic form. objective label represent sources
uncertainty (arising source belief state effects) causally support labelled item.
also introduce graph layer Ok represent outcomes connect effects literals.
might seem natural describe labels outcomes terms affected literals,
lead trouble. problem literals effect outcomes describing states
different time literals projected belief state. Further, outcome appears two
levels graph describing random event different times. Using state literals describe
labels lead confusion random events (state uncertainty effect outcomes
distinct steps) causally support labelled item. pathological example effect
whose set outcomes matches one-to-one states source belief state. case,
using labels defined terms state literals cannot distinguish random event (the state
uncertainty effect uncertainty) described label.
two choices describing effect outcomes labels. choices introduce
new set label variables describe literal layer split. new variables used
describe effect outcomes labels confused variables describing initial state
uncertainty. first case, variables one-to-one matching original set
literals, thought time-stamped literals. number variables add
label function order 2F per level (the number fluent literals assuming boolean
fluents). second option describe outcomes labels new set fluents,
interpretation fluents matched particular outcome. case, add order
log |Ok | variables, Ok k th outcome layer. would actually lower many
outcomes deterministic effects need describe labels.
former approach likely introduce fewer variables lot non-deterministic
effects affect quite literals. latter introduce fewer variables
relatively non-deterministic effects whose outcomes fairly independent.
generalized labelling, still say item reachable source belief
state label entailed source belief state. even though adding
variables labels, implicitly adding fluents source belief state. example, say
add fluent v describe two outcomes effect. One outcome labelled v, v.
express source belief state BSP projected LU G new fluent
BSP (v v) = BSP . item labelled BSP v entailed projected belief
state (i.e. unreachable) one outcome causally supports it. outcomes support
item, reachable.
Given notion reachability, determine level extract relaxed
plan. relaxed plan procedure change much terms semantics
extra graph layer outcomes. still ensure literals causally supported
worlds labelled relaxed plan, whether worlds initial state
uncertainty supporting non-deterministic effects.

9. Conclusion
intent establishing basis belief state distance estimates, have:
Discussed heuristic measures aggregate state distance measures capture positive
interaction, negative interaction, independence, overlap.
91

fiB RYCE , K AMBHAMPATI , & MITH

Shown compute heuristic measures planning graphs provided empirical
comparisons measures.
Found exploiting planning graph structure reduce cost considering possible
states belief state preferable sampling subset states heuristics.
Shown labelled uncertainty graph capture support information multiple
graphs, reduces cost heuristic computation.
Shown labelled uncertainty graph useful conformant planning and, without
considering observational actions knowledge, perform well conditional planning.
intent work provide formal basis measuring distance belief
states terms underlying state distances. investigated several ways aggregate state
distances reflect various assumptions interaction state state trajectories. best
measures turned measure positive interaction independence, call
overlap. saw planners using notion overlap tend well across large variety
domains tend accurate heuristics.
Weve also shown planning Labelled Uncertainty planning Graph LU G, condensed
version multiple graphs useful encoding conformant reachability information. main
innovation idea labels labels attached literals, actions, effect relations,
mutexes indicate set worlds respective elements hold. experimental
results show LU G outperform multiple graph approach. comparison
approaches, weve also able demonstrate utility structured reachability heuristics
controlling plan length boosting scalability conformant conditional planning.
intend investigate three additions work. first, incorporate sensing
knowledge heuristics. already promising results without using features
planning graphs, hope help approaches scale even better conditional
problems. second addition consider heuristics stochastic planning problems.
major challenges associate probabilities labels indicate likelihood
possible world integrate reasoning probabilistic action effects.
Lastly, recently extended LU G within framework state agnostic planning
graphs (Cushing & Bryce, 2005), hope improve technique. state agnostic planning
graph essentially multiple source planning graph, analogy conventional planning
graph single source. Planning graphs already multiple destination, generalization
state agnostic planning graph allows us compute distance measure pair
states belief states. LU G seeks avoid redundancy across multiple planning graphs
built states belief state. extended notion avoid redundancy planning
graphs built every belief state. shown state agnostic LU G (SLU G)
built per search episode (as opposed LU G node) reduce heuristic computation
cost without sacrificing informedness.
Acknowledgments would like thank Minh B. Do, Romeo Sanchez, Terry Zimmermam,
Satish Kumar Thittamaranahalli, Cushing helpful discussions feedback, Jussi Rintanen help YKA planner, Piergiorgio Bertoli help MBP planner.
work supported part NASA grants NCC2-1225 NAG2-1461, NSF grant IIS0308139, 2003 NASA RIACS SSRP, ARCS Foundation, IBM faculty award.
92

fiP LANNING G RAPH H EURISTICS B ELIEF PACE EARCH

Appendix A. Additional Heuristics
completeness, present additional heuristics adapted classical planning reason
belief state distances type planning graph. Many heuristics appeared
previous work (Bryce & Kambhampati, 2004). show compute max, sum, level
heuristics single graph SG, multiple graphs G, labelled uncertainty graph LU G.
heuristics tend less effective relaxed plan heuristics, provide
reference. Section 4, describe heuristics terms regression search.
A.1 Single Planning Graph Heuristics (SG)
Like, relaxed plan single unmodified planning graph, cannot aggregate state distances
notion separate states lost forming initial literal layer, thus compute
heuristics aggregate state distances.
State Aggregation:
Max classical planning, maximum cost literal used get max heuristic, use
formulas describe belief states, take maximum cost clause cost
belief state find max heuristic hSG
max . maximum cost clause belief state,
respect single planning graph, is:
hSG
max (BSi ) =

max

C(BSi )

cost(C)

cost clause is:
cost(C) = min min k
lC k:lLk

find cheapest literal cost clause find maximum cost clause.
underestimate closest state current belief state.
Sum Like classical planning sum heuristic, take sum hSG
sum costs
clauses belief state estimate belief state distance

cost(C)
hSG
sum (BSi ) =
C(BSi )

heuristic takes summation costs literals closest estimated state
belief state, inadmissible may single action support every
clause, could count clause.
Level mutexes planning graph, compute level heuristic hSG
level
(without mutexes level heuristic equivalent max heuristic). level heuristic
maintains admissibility max heuristic improves lower bound considering
level planning graph literals constituent non-pairwise mutex.

level heuristic computed taking minimum among (BS
), first level
(lev(S)) planning graph literals present none marked
pairwise mutex. Formally:
hSG
level (BSi ) =

93

min

S(BSi )

lev(S)

fiB RYCE , K AMBHAMPATI , & MITH

A.2 Multiple Planning Graph Heuristics (M G)
Similar various relaxed plan heuristics multiple graphs, compute max, sum,
level heuristic multiple planning graphs aggregate maximum
summation respectively measure positive interaction independence. reason cannot
aggregate individual graph heuristics measure overlap numbers, sets
actions. Measuring overlap involves taking union heuristics graph union
numbers meaningful like union action sets relaxed plans. Like before,
reason use multiple graphs state distance aggregation.
Positive Interaction Aggregation:
G
Max max heuristic hM
mmax computed multiple planning graphs measure posM
G
itive interaction hmmax heuristic. heuristic computes maximum cost clause
(BSi ) graph , similar hSG
mmax (BSi ) computed, takes
maximum. Formally:
G

hM
mmax (BSi ) = max (hmax (BSi ))


G
hM
mmax heuristic considers minimum cost, relevant literals belief state (those
reachable given possible world graph ) get state measures. maximum
taken estimate accounts worst (i.e., plan needed difficult
world achieve subgoals).

Sum sum heuristic measures positive interaction multiple planning graphs
G
hM
msum . computes summation cost clauses (BSi ) graph
takes maximum. Formally:
G

hM
msum (BSi ) = max (hsum (BSi ))


heuristic considers minimum cost, relevant literals belief state (those
reachable given possible worlds represented graph ) get state measures.
G
hM
mmax , maximum taken estimate costly world.

G
MG
MG
Level Similar hM
mmax hmsum , hmlevel heuristic found first finding hlevel
graph get state distance measure, taking maximum across

graphs. hlevel (BSi ) computed taking minimum among (BS
),

first level lev (S) planning graph literals present none
marked mutex. Formally:

hlevel (BSi ) =

min

S(BSi )

lev (S)



G
hM
mlevel (BSi ) = max(hlevel (BSi ))


Note heuristic admissible. reasoning classical planning, first
level subgoals present non-mutex underestimate true cost
state. holds graphs. Taking maximum accounts difficult
94

fiP LANNING G RAPH H EURISTICS B ELIEF PACE EARCH

world achieve constituent BSi thus provable underestimate h .
G
GPTs max heuristic (Bonet & Geffner, 2000) similar hM
mlevel , computed
dynamic programming state space rather planning graphs.
Independence Aggregation: heuristics mentioned Positive Interaction Aggregation
augmented take summation costs found individual planning graphs rather
G
MG
MG
maximum. denote as: hM
smax , hssum , hslevel . None heuristics
admissible action may used worlds, count cost every world
using summation.
A.3 Labelled Uncertainty Graph (LU G)
max, sum, level heuristics LU G similar analogous multiple graph heuristics. main difference heuristics LU G much easier compute
positive interaction measures independence measures. reason positive interaction easier
compute find cost clause states belief state once, rather
multiple planning graphs. Like before, consider heuristics aggregate
state distances.
Positive Interaction Aggregation:
G
Max max heuristic hLU
mmax LU G finds maximum clause cost across clauses
current belief state BSi . cost clause first level becomes reachable.
Formally:


G
hLU
mmax (BSi )

=

max

C(BSi )


min

k:BSP |=k (C)

k

G
Sum sum heuristic hLU
msum LU G sums individual levels clause
(BSi ) first reachable. Formally:


G
min
hLU
(BS
)
=
k

msum

C(BSi )

k:BSP |=k (C)

G
Level level heuristic hLU
mlevel index first level BSi reachable.
Formally:
G
hLU
mlevel (BSi ) =

min

k:BSP |=k (BSi )



Independence Aggregation: heuristics mentioned positive interaction aggregation
augmented take summation costs state belief state. may inefficient
due fact lose benefit LU G evaluating heuristic state
BSP , rather states positive interaction aggregation. case
work similar multiple graph heuristic extraction, aside improved graph
construction time. positive interaction aggregation able implicitly calculate maximum
worlds heuristics, whereas sum heuristic need explicitly find
G
LU G
LU G
cost world. denote sum heuristics as: hLU
smax , hssum , hslevel .
95

fiB RYCE , K AMBHAMPATI , & MITH

Appendix B. Cross-World Mutexes
Mutexes develop possible world also two possible worlds,
described Smith Weld (1998). Cross-world mutexes useful capture negative interactions belief state distance measures (mentioned Section 3). representation crossworld mutexes requires another generalization labelling mutexes. world mutexes
require keeping one label mutex signify possible worlds mutex holds. extended representation keeps pair labels, one element mutex;
x possible world mutex x possible world , denote mutex pair
(k (x) = S, k (x ) = ).
compute cross-world mutexes several worlds elements x x . example, k (x) = S1 S2 S3 k (x ) = S2 S3 , check cross-world mutexes need
consider mutexes world pairs (S1 , S2 ), (S1 , S3 ), (S2 , S2 ), (S2 , S3 ), (S3 , S2 ), (S3 , S3 ).
also check mutexes intersection element labels k (x) k (x ) = S2 S3 ,
meaning cross world pairs check mutexes (S2 , S2 ), (S2 , S3 ), (S3 , S2 ),
(S3 , S3 ).
say formula f reachable projected belief state BSP , considering
cross-world mutexes, every pair states BSP , f reachable. pair states
, f reachable |= k (f ) every pair constituents , f
|= k (S ) |= k (S ), two literals either same-world
mutex = , mutex literals , across respective
worlds
= . mutex pair literals l l , respectively
mutex (k (l), k (l )) |= k (l) |= k (l ).
computation cross-world mutexes requires changes mutex formulas,
outlined next. major change check, instead single possible worlds S, pairs
possible worlds mutexes.
Action Mutexes: action mutexes hold actions executable different
possible worlds.
Interference Interference mutexes change cross-world mutexes, except
pair labels (k (a) = BSP , k (a ) = BSP ), instead single label.
Competing Needs Competing needs change mutexes cross-world mutexes two
actions , worlds respectively, could competing. Formally, crossworld competing needs mutex ((k (a) = S, k (a ) = ) exists worlds
if:
le (a),l e (a ) (k (l) = S, k (l ) = )
Effect Mutexes: effect mutexes hold effects occur different possible worlds.
Interference Effect interference mutexes change cross-world mutexes, except
pair labels (k (i (a)) = BSP , k (j (a )) = BSP ), instead single
label.
96

fiP LANNING G RAPH H EURISTICS B ELIEF PACE EARCH

Lk

lk(p)
p


Ek

Ak

lk(a)


h(a)

lk(h(a))



(lk(p), lk(q))
Induced mutex across worlds:


(lk(j(a))lk(i(a)), lk(h(a)))





(lk(j(a)), lk(h(a)))

lk(q)
q
lk(a)


j(a)

lk(r)
r

lk(j(a))
i(a) induces j(a) in:
lk(i(a))lk(j(a))

i(a) lk(i(a))

Figure 12: Example cross-world induced effect mutex.
Competing Needs Effect competing needs mutexes change cross-world mutexes
two effects (a) j (a ), worlds respectively, could competing. Formally,
cross-world competing needs mutex (k (i (a)) = S, k (j (a )) = ) exists (a)
j (a ) worlds if:
li (a),l j (a ) (k (l) = S, k (l ) = )
Induced Induced mutexes change slightly cross-world mutexes. worlds one
effect induces another, remains same, mutex changes slightly.
j (a) k (j (a)) mutex h (a ) k (h (a )), (a) induces effect j (a)
possible worlds described k (i (a)) k (j (a)), induced mutex
(a) k (j (a)) k (i (a)) h (a ) k (h (a )) (see Figure 12).

Literal Mutexes: literal mutexes hold literals supported different possible worlds.
Inconsistent Support changes cross-world mutexes. mutex (k (l) = S, k (l ) = )
holds l l (a), j (a ) Ek1 l (a), l j (a ),
mutex k1 (i (a)) = S, k1 (j (a )) = ).

97

fiB RYCE , K AMBHAMPATI , & MITH

References
Bertoli, P., & Cimatti, A. (2002). Improving heuristics planning search belief space.
Proceedings AIPS02.
Bertoli, P., Cimatti, A., & Roveri, M. (2001a). Heuristic search + symbolic model checking =
efficient conformant planning. Proceedings IJCAI01.
Bertoli, P., Cimatti, A., Roveri, M., & Traverso, P. (2001b). Planning nondeterministic domains
partial observability via symbolic model checking. Proceedings IJCAI01.
Blum, A., & Furst, M. (1995). Fast planning planning graph analysis. Proceedings
IJCAI95.
Bonet, B., & Geffner, H. (1999). Planning heuristic search: New results. Proceedings
ECP99.
Bonet, B., & Geffner, H. (2000). Planning incomplete information heuristic search belief
space. Proceedings AIPS00.
Brace, K., Rudell, R., & Bryant, R. (1990). Efficient implementation bdd package. Proceedings 27th ACM/IEEE design automation conference.
Bryant, R. (1986). Graph-based algorithms Boolean function manipulation. IEEE Transactions
Computers, C-35(8), 677691.
Bryce, D., & Kambhampati, S. (2004). Heuristic guidance measures conformant planning.
Proceedings ICAPS04.
Castellini, C., Giunchiglia, E., & Tacchella, A. (2001). Improvements sat-based conformant
planning. Proceedings ECP01.
Cimatti, A., & Roveri, M. (2000). Conformant planning via symbolic model checking. Journal
Artificial Intelligence Research, 13, 305338.
Cormen, T. H., Leiserson, C. E., & Rivest, R. L. (1990). Introduction Algorithms. McGraw-Hill.
Cushing, W., & Bryce, D. (2005). State agnostic planning graphs. Proceedings AAAI05.
de Kleer, J. (1986). Assumption-Based TMS. Artificial Intelligence, 28(2), 127162.
Genesereth, M. R., & Nourbakhsh, I. R. (1993). Time-saving tips problem solving incomplete information. Proceedings AAAI93.
Hansen, E., & Zilberstein, S. (2001). LAO: heuristic-search algorithm finds solutions
loops. Artificial Intelligence, 129(12), 3562.
Hoffmann, J., & Brafman, R. (2004). Conformant planning via heuristic forward search: new
approach. Proceedings ICAPS04.
Hoffmann, J., & Nebel, B. (2001). FF planning system: Fast plan generation heuristic
search. Journal Artificial Intelligence Research, 14, 253302.
Kambhampati, S., Ihrig, L., & Srivastava, B. (1996). candidate set based analysis subgoal
interactions conjunctive goal planning. Proceedings AIPS96.
Koehler, J., Nebel, B., Hoffmann, J., & Dimopoulos, Y. (1997). Extending planning graphs
adl subset. Proceedings ECP97.
98

fiP LANNING G RAPH H EURISTICS B ELIEF PACE EARCH

Kurien, J., Nayak, P., & Smith, D. (2002). Fragment-based conformant planning. Proceedings
AIPS02.
Long, D., & Fox, M. (2003). 3rd international planning competition: Results analysis.
Journal Artificial Intelligence Research, 20, 159.
Nguyen, X., Kambhampati, S., & Nigenda, R. (2002). Planning graph basis deriving
heuristics plan synthesis state space CSP search. Artificial Intelligence, 135(1-2),
73123.
Nilsson, N. (1980). Principles Artificial Intelligence. Morgan Kaufmann.
Pednault, E. P. D. (1988). Synthesizing plans contain actions context-dependent effects.
Computational Intelligence, 4, 356372.
Petrick, R., & Bacchus, F. (2002). knowledge-based approach planning incomplete information sensing. Proceedings AIPS02.
Rintanen, J. (2003a). Expressive equivalence formalisms planning sensing. Proceedings ICAPS03.
Rintanen, J. (2003b). Product representation belief spaces planning partial observability.
Proceedings IJCAI03.
Rintanen, J. (2004). Distance estimates planning discrete belief space. Proceedings
AAAI04.
Smith, D., & Weld, D. (1998). Conformant graphplan. Proceedings AAAI98.
Weld, D., Anderson, C., & Smith, D. (1998). Extending graphplan handle uncertainty sensing
actions. Proceedings AAAI98.

99

fi
