Journal Artificial Intelligence Research 44 (2012) 587-632Submitted 03/12; published 07/12SAP Speaks PDDL: Exploiting Software-EngineeringModel Planning Business Process ManagementJorg Hoffmannhoffmann@cs.uni-saarland.deSaarland University, Saarbrucken, GermanyIngo Weberingo.weber@nicta.com.auNICTA, Sydney, AustraliaFrank Michael Kraftfrank.michael.kraft@bpmnforum.netbpmnforum.net, GermanyAbstractPlanning concerned automated solution action sequencing problems described declarative languages giving action preconditions effects. One importantapplication area technology creation new processes Business ProcessManagement (BPM), essential ever dynamic business environment.major obstacle application Planning area lies modeling. Obtaining suitable model plan ideally description PDDL, commonlyused planning language often prohibitively complicated and/or costly. core observation work problem ameliorated leveraging synergiesmodel-based software development. application SAP, one leading vendorsenterprise software, demonstrates even one-to-one model re-use possible.model question called Status Action Management (SAM). describesbehavior Business Objects (BO), i.e., large-scale data structures, level abstraction corresponding language business experts. SAM covers 400kinds BOs, described terms set status variablesvalues required for, affected by, processing steps (actions) atomicbusiness perspective. SAM developed SAP part major model-based softwareengineering effort. show herein one use model planning, thusobtaining BPM planning application incurs modeling overhead all.compile SAM variant PDDL, adapt off-the-shelf planner solvekind problem. Thanks resulting technology, business experts may createnew processes simply specifying desired behavior terms status variable valuechanges: effectively, describing process language.1. IntroductionBusiness processes workflows controlling flow activities within enterprises (Aalst, 1997). Business process management (BPM) concerned, amongstthings, maintenance processes. minimize time-to-market everdynamic business environment, essential able quickly create new processes. involves selecting arranging suitable transactions huge inc2012AI Access Foundation. rights reserved.fiHoffmann, Weber & Kraftfrastructures. difficult costly task. application supports taskwithin software framework SAP1 , one leading vendors enterprise software.well-known idea context, discussed example Jonathan, Moore, Stader,Macintosh, Chung (1999), Biundo, Aylett, Beetz, Borrajo, Cesta, Grant, McCluskey,Milani, Verfaillie (2003), Rodriguez-Moreno, Borrajo, Cesta, Oddi (2007),use technology field planning. long-standing sub-area AI,allows user describe problem solved declarative language.nutshell, planning problems come form initial state, goal, setactions, formulated relative set (typically Boolean least finite-domain) statevariables. solution (or plan) schedule actions transforming initial statestate satisfies goal. planning technology solves (in principle) problemdescribed language. far wide-spread planning language planningdomain definition language (PDDL) (McDermott, Ghallab, Howe, Knoblock, Ram, Veloso,Weld, & Wilkins, 1998).2idea BPM context annotate transaction planning-likedescription formalizing action. enables planning systems compose (partsapproximations of) desired processes fully automatically, i.e., based minimal userinput specifying process start (initial state), achieve(goal). closely related ideas explored name semantic web servicecomposition context Semantic Web community (e.g., Narayanan & McIlraith,2002; Agarwal, Chafle, Dasgupta, Karnik, Kumar, Mittal, & Srivastava, 2005; Sirin, Parsia,& Hendler, 2006; Meyer & Weske, 2006).Runtime performance important application. Typically, user business expert wishing create new process waiting online planning outcome.However mission-critical question, discussed example Kambhampati (2007)Rodriguez-Moreno et al. (2007), is: get planning model? useful,model needs capture relevant properties huge infrastructure, levelabstraction high-level enough usable business experts,time precise enough relevant level. Designing model costly oneneed good arguments indeed persuade manager embark endeavor.present work, demonstrate problem ameliorated leveragingsynergies model-based software development, thus reducing additional modelingoverhead caused planning. fact, show one least particularapplication re-use exactly, one-to-one, models built purpose softwareengineering, thus reduce modeling overhead zero.previously noted, example Turner McCluskey (1994) Kitchin,McCluskey, West (2005), planning languages commonalities softwarespecification languages B (Schneider, 2001) OCL (Object Management Group,2006). Now, typically specification languages mathematically oriented describe1. http://www.sap.com2. many variants planning, PDDL. share concepts similar short descriptionstated. However, description corresponds best classical planning, (amongstthings) uncertainty action effects. discuss details Section 2.1.Throughout paper, unless refer one particular planning formalisms defined here,use term planning general sense targeting particular variant.588fiSAP Speaks PDDLlow-level properties programs. stands contrast abstract modelsneeded work business experts. always so.part major effort developing flexible service-oriented (Krafzig, Banke, & Slama,2005; Bell, 2008) infrastructure, called SAP Business ByDesign, SAP developedmodel called Status Action Management (SAM). SAM describes status variablesBusiness Objects (BO) change values actions transactions affectingBOs executed. BOs full detail vastly complex, containing 1000s datafields numerous technical-level transactions. SAM captures abstract businessperspective, terms smaller number user-level actions (like submit reject),whose behavior described using preconditions effects high-level status properties(like submitted rejected). way, SAM corresponds language business users, close correspondence common planning languages. SAMextensive, covering 404 kinds BOs 2418 transactions. model creationconstitutes work effort spanning several years, involving, amongst things, dedicatedmodeling environments educational training modelers.SAM originally designed purpose model-driven software development,facilitate design Business ByDesign infrastructure, changes thereuntoinitial development afterwards. Business ByDesign covers needsgreat breadth different SAP customer businesses, flexibly configurablecustomers. configuration involves, amongst things, design customerspecific processes, appropriately combining functionalities provided. Describingproperties individual processing steps, rather supplying BO standard lifecycle workflow, SAM well-suited support flexibility. However, business usersdesigning processes typically familiar details infrastructure. UsingSAM planning, obtain technology alleviates problem. output,technology delivers first version desired process, relevant transactionssuitable control-flow. input, technology requires business usersspecify desired status changes language.intended meaning SAM is, large extent, common planningframeworks. subtleties treatment non-deterministic actions. Oneproblem many non-deterministic actions modeled SAM bad outcomespreclude successful processing respective business object (example: BO datainconsistent). problem aggravated fact that, SAMs non-determinism,repeated executions action independent (example: check BO consistency). discuss detail, derive suitable planning formalism. compileSAM PDDL, thus creating side-effect work new planning benchmark.anonymized PDDL version SAM publicly available.algorithmic side, show minimal changes off-the-shelf planner sufficeobtain good empirical performance. adapt well-known deterministic planningsystem FF (Hoffmann & Nebel, 2001) perform suitable variant AO* (Nilsson, 1969,1971). run heuristic function non-deterministic actions simply actingcould choose outcome, i.e., applying all-outcomes determinization (Yoon,Fern, & Givan, 2007). run large-scale experiments modified FF, fullSAM model used SAP. show runtime performance satisfactory vastmajority cases; point remaining challenges.589fiHoffmann, Weber & Kraftalso integrated planning technology two BPM process modeling environments, making planning functionality conveniently accessible non-IT users.Processes (and plans) environments displayed human-readable format.Users specify status variable values, example planning goal, simple intuitivedrop-down menus. One environments integrated research extensioncommercial SAP NetWeaver platform. said that, technology yet partactual SAP product; discuss Section 7.treatment non-deterministic actions, formalism algorithms, specificapplication context. notwithstanding, plausible techniquescould useful also applications dealing actions. generalperspective, contribution work (A) pointing possible leveragesoftware-engineering models planning, (B) demonstrating applicationrealized one major players BPM industry, thus providing largescale case study. principle underlying SAM modeling software artifacts levelabstraction corresponding business users limited SAP. Thus work mayinspire similar approaches related contexts.next give brief background planning BPM. discuss SAM modelSection 3, explaining structure, context SAP, added value usingplanning. design planning formalization Section 4, explain planningalgorithms Section 5, evaluate experimentally Section 6. Section 7 describesprototypes SAP. Section 8 discusses related work, Section 9 concludes.2. Backgroundintroduce basic concepts relevant work. start planning, overviewbusiness process management (BPM) connection planning.2.1 Planningmany variants planning (for overview, see Traverso, Ghallab, & Nau, 2005).handle SAM, build wide-spread classical planning framework, planningfinite-domain variables (e.g., Backstrom & Nebel, 1995; Helmert, 2006, 2009). extend framework particular kind non-deterministic actions, whose semanticsrelates notions planning uncertainty outline below.Definition 1 (Planning Task) finite-domain planning task tuple (X, A, I, G). Xset variables; x X associated finite domain dom(x). setactions, takes form (pre , eff ) pre (the precondition) eff(the effect) partial variable assignment. variable assignment representinginitial state, G partial variable assignment representing goal.fact statement x = c x X c dom(x). identify partial variableassignments conjunctions (sets, sometimes) facts obvious way. statecomplete variable assignment. action applicable iff |= pre . f partialvariable assignment, f variable assignment coincides fvariable f defined, coincides variables f undefined.590fiSAP Speaks PDDLDefinition 2 (Plan) Let (X, A, I, G) finite-domain planning task. Let state,let sequence actions A. say solution iff either:(i) empty |= G;(ii) = hai 0 , |= pre , 0 solution eff .solution I, called plan.One can, course, define plans finite-domain planning tasks simpler way;present formulation makes easier extend definition later on. remark that,despite simplicity formalism, PSPACE-complete decide whetherplan exists (this follows directly results Bylander, 1994).Unlike classical planning, exist disjunctive effects SAM, i.e., actionsone possible outcome. type situation dealt planninguncertainty. model SAMs disjunctive effects appropriately, need mixtureknown non-deterministic actions (e.g., Smith & Weld, 1999) knownobservation actions (e.g., Weld, Anderson, & Smith, 1998).Non-deterministic actions like usual actions except that, place single effecteff , set Ea effects, referred possible outcomes. Wheneverapply plan execution time, one outcomes Ea occur; separateapplications independent. example, might throw dice. plan generationtime, know outcome occur, must cater cases.straightforward framework conformant planning (e.g., Smith & Weld, 1999),plan still sequence actions, required achieve goal matteroutcomes occur execution. Note exploit observability, i.e.,plan make case distinctions based outcomes actually occur.handle SAM, include case distinctions, along lines knowncontingent planning (e.g., Weld et al., 1998). framework, case distinctions madeexplicit observation actions plan. Typically, observation action observespreviously unknown value particular state variable x plan execution time (forexample, value dice throwing it). plan branches possible valuesx, i.e., one successor value dom(x). Thus plan tree actions,requirement goal fulfilled every leaf tree.wide-spread input language planning systems today Planning Domain Definition Language (PDDL), used international planning competitions(IPC) (McDermott et al., 1998; Bacchus, 2000; Fox & Long, 2003; Hoffmann & Edelkamp,2005; Younes, Littman, Weissman, & Asmuth, 2005; Gerevini, Haslum, Long, Saetti, &Dimopoulos, 2009). get details language, since purposesPDDL merely particular syntax implementing formalisms. importantus, regarding usability PDDL encoding SAM, fact PDDLlot variants, varying degrees support existing planning systems. PDDLsyntax SAM PDDL variant used non-deterministic tracks IPC,i.e., tracks dealing non-probabilistic planning uncertainty (Bonet & Givan,2006; Bryce & Buffet, 2008). Specifically, use basic PDDL constructs (often referred STRIPS), except action preconditions use quantifier-freeformulas (Pednault, 1989; Bacchus, 2000). PDDL subset supported existing591fiHoffmann, Weber & Kraftplanners, particular based FF (Hoffmann & Nebel, 2001) Fast Downward(Helmert, 2006). limiting factor planner support non-deterministic actions,use common syntax, namely (oneof eff 1 . . . eff n ) constructnon-deterministic IPC. Non-deterministic actions supported planners.Further, semantics give plans using actions, fits application basedSAM, non-standard supported existing planners. notwithstanding,several existing approaches closely related (cf. Section 8), and, show herein,least one planner Contingent-FF (Hoffmann & Brafman, 2005) adapted quiteeasily successfully deal new semantics.2.2 Business Process ManagementAccording commonly used definition (e.g., Weske, 2007), business process consistsset activities performed coordination organizational technicalenvironment. activities jointly realize business goal. words, businessprocesses enterprises business. Business process models serve abstractionway enterprises business. example, business process model may specifysteps taken, various entities across enterprise, send customer quoteanswering request quotation. atomic steps process model may both,manual steps performed employees, automatic steps executed infrastructure.refer process models simply processes.explicit model processes allows sorts support automation, addressedarea business process management (BPM). Herein, mostly concernedprocess creation adaptation. done BPM modeling environments. Importantly,users environments typically experts, business expertspeople familiar with, taking decisions for, business. dominant paradigmrepresenting business processes workflows, also called control-flows, often formalizedPetri nets (e.g., Aalst, 1997). control-flow defines order executionprocess steps, within certain degrees flexibility implied, example, parallelism.business experts, control-flow displayed human-readable format, typicallyflow diagram. application SAP uses Business Process Modeling Notation (ObjectManagement Group, 2008), short BPMN, illustrate Section 7.alternative paradigm representing business processes, relates SAMmodel consider herein, constraint-based representations (e.g., Wainer & de LimaBezerra, 2003; van der Aalst & Pesic, 2006; Pesic, Schonenberg, Sidorova, & van der Aalst,2007). model processes implicitly desired properties, rather explicitly concrete workflows. kind representation flexible, that,modifying model, modify entire process space. example, mightadd new constraint archive customer quotes follow-ups created.representation also explicit reasons process design, supportinghuman understanding. downside that, actual automated process execution,concrete control-flow design required. One way viewing planning technologyprovides service generating control-flow designs SAM.Processes executed infrastructures, like one provided SAP. execution coordinates individual processing steps, prompting human users appropriate,performing necessary data updating level. realized dedicated592fiSAP Speaks PDDLprocess execution engines (Dumas, ter Hofstede, & van der Aalst, 2005). Clearly, execution poses high demands structure workflow. basic requirementatomic process steps correspond actual steps known infrastructure.3requirements business processes, legal financial regulations,subject frequent updates. people responsible adapting processes businessexperts familiar infrastructure, may come processeswhose atomic steps nowhere near implemented easily, partially overlapwhole sets existing functions, and/or require implementation new functionsalthough existing functions could arranged job. Thus needintensive communication business experts experts, incurring significantcosts human labor increased time-to-market.planning come rescue? indicated, basic (and well-known) ideause planning tool composing (an approximation of) process automatically,helping business expert come process close infrastructure.main novelty work leverage pre-existing model, SAM, getting us aroundone critical issues area: overhead creating planner input.3. SAMexplain structure SAM language, give running example. outlinebackground SAM SAP, explain added value using SAM planning.3.1 SAM Structure ExampleStatus Action Management (SAM) models belong business objects (BOs). BOassociated set finite-domain status variables, set actions.status variable highlights one value variable take new instanceBO created. action described textual label (its name), precondition,effect. precondition effect propositional formulas variable values.Definition 3 (SAM BO) SAM business object triple (X(o), A(o), I(o)). X(o)set status variables; x X(o) associated finite domain dom(x). A(o)set actions, a(o) A(o) takes form (pre a(o) , eff a(o) ); pre a(o) (the precondition) propositional formula atoms {x = c | x X(o), c dom(x)}; eff a(o)(the effect) negation-free propositional formula atoms, disjunctivenormal form (DNF). I(o) variable assignment representing os initial state.structure obvious correspondence Definition 1. differences goal, preconditions effects complex.planning application, goal set user creating new process. discussSection 4 extend Definitions 1 2 handle SAM preconditions effects.Note cross-BO constraints SAM BO refers valuesvariables. shortcoming current version SAM: reality, BOsinteract. get back below.3. Another important requirement appropriate data-flow (van der Aalst, 2003; Dumas et al., 2005),e.g., sending manager documents required decide whether accept customer quote.Since, case, data encapsulated business objects, major issue us.593fiHoffmann, Weber & KraftAction nameCheck CQ CompletenesspreconditionCQ.archiving:notArchivedCheck CQ ConsistencyCQ.archiving:notArchivedCheck CQ Approval StatusCQ.archiving:notArchivedCQ.approval:notCheckedCQ.completeness:completeCQ.consistency:consistentCQ.archiving:notArchivedCQ.approval:necessaryCQ.archiving:notArchived(CQ.approval:notNecessaryCQ.approval:granted)CQ.archiving:notArchivedCQ.submission:submittedCQ.archiving:notArchivedCQ.acceptance:acceptedCQ.archiving:notArchivedDecide CQ ApprovalSubmit CQMark CQ AcceptedCreate Follow-Up CQArchive CQeffectCQ.completeness:completeCQ.completeness:notCompleteCQ.consistency:consistentCQ.consistency:notConsistentCQ.approval:necessaryCQ.approval:notNecessaryCQ.approval:grantedCQ.approval:notGrantedCQ.submission:submittedCQ.acceptance:acceptedCQ.followUp:documentCreatedCQ.archiving:archivedFigure 1: SAM-like running example, modeling behavior customer quotes CQ.illustration, Figure 1 gives SAM-like model BO called customer quote (CQ),running example. confidentiality reasons, shown object modelartificial, i.e., contained SAM used SAP. CQ.x:c denoteproposition x = c, object CQ. initial state I(CQ) is:CQ.archiving:notArchived,CQ.completeness:notComplete,CQ.consistency:notConsistent,CQ.approval:notChecked,CQ.submission:notSubmitted,CQ.acceptance:notAccepted,CQ.followUp:documentNotCreated.using example below, relevant assume goal entereduser CQ.followUp:documentCreated CQ.archiving:archived.reader keep mind merely illustrative example, necessarily simple. particular, intended life-cycle workflow rather obvious, givenaction descriptions Figure 1. much case general. BusinessObjects modeled SAM 15 status variables, yielding 12 million possiblestates (combinations variable values) even single BO. words, SAMflexible model all, main design purpose describes large number combination possibilities compact way. Furthermore, two applicationscenarios planning ((A) (C) Section 3.3 below), actually lookingentire life-cycles process fragments may begin end BO status values.594fiSAP Speaks PDDL3.2 SAM@SAPSAM created SAP part development infrastructure supportingSAP Business ByDesign. infrastructure constitutes fully-fledged SAP application.key advantage traditional SAP applications higher degree flexibility, facilitating use SAP software as-a-service. Individual system functions encapsulatedsoftware services, using service-oriented architectures paradigm (Krafzig et al., 2005;Bell, 2008). software services may accessed standard architectures like BPMprocess execution engines, thus enabling flexible combination services.support flexibility, Business ByDesign infrastructure model-driven. artifacts various system levels described declaratively using SAP-proprietary modelingformats. Business objects one artifact, SAM one format.original purpose SAM facilitate design, managementchanges, development Business ByDesign infrastructure (a formidablyhuge enterprise). course, SAM also serves implementation changes infrastructure later on, changes required. New developments first implementedtested model level. parts program code automatically generatedmodel. Straightforward code skeletons contain status variables, well functionheaders available actions (similar Eclipse Java class definitions).addition, skeletons filled code fragments performing precondition checksupdates status variables. Changes pertaining status variable level thusimplemented SAM models, automatically propagated code. sense,original semantics SAM follows:(I) BO newly created, values status variables set I(o).(II) BO actions a(o) whose precondition pre a(o) fulfilled either disallowed,raise exception executed; one true depends partarchitecture attempting execute action.(III) Upon execution action a, status variables change values prescribedone disjuncts effect DNF eff a(o) . aspect controlled outsideSAM disjunct chosen: choice made based BO data contentreflected SAM.intention behind SAM formulate complex business-level dependenciesindividual processing steps, using simple modeling constructs facilitate easy modification. formulation terms preconditions effects relative high-level statusvariable values adopted natural means meet requirements. course,design also took inspiration traditional software modeling paradigms (Schneider,2001; Object Management Group, 2006).Leveraging SAM planning great opportunity effort takes buildmodel. SAM developed continuously along Business ByDesign, acrosstime span 5 years. Throughout time, around 200 people involved (aspart-time occupation) development. SAP implemented dedicated graphical userinterface development. design patterns typical cases, naming conventions, fully-fledged governance process, even educationaltraining developers. council senior SAP architects supervises development.595fiHoffmann, Weber & Kraft3.3 Applications SAM-Based PlanningBusiness ByDesign infrastructure designed general adaptable, coveringneeds great breadth different SAP customers business domains. adaptinfrastructure practice, SAP customers may choose create processescompositions functionalities provided (as Web services), way tailoredneeds. Indeed, second motivation behind SAM, beside role software development,facilitate flexibility, describing possible process space declarativemanner, rather imposing standard workflows common methodologycontexts artefact-centric business process modeling (e.g., Cohn & Hull, 2009). SAMshares motivation constraint-based process representation languages. also sharesdownside, actual workflows still need created. context,least three application scenarios planning based SAM:(A) Development based SAM. model-driven development based SAM,planning enables developers examine changes affect process space.greatly facilitates experimentation testing. example, planningused debugging, testing whether goal still reached, whetherchanges opened unintended possibilities, like, reaching undesired stateBO (e.g., CQ.consistency:notConsistent CQ.acceptance:accepted).reachability testing (essentially model checking task), planning servesgenerate entire processes, shall see take form BPMN process modelsparallelism conditional splits. Developers examine space processesgenerated way, determining different combinations start/end conditionsconnected. Note generality offered planning approachabsolute requirement process generation tool must least generalSAM, handling propositional formula preconditions effects.(B) Designing extended/customized processes. Individual SAP customers individual requirements processes, thus may use BOs differentways. example, even end state customer quotes (which practicemuch complex illustrative example) always involved archived,different businesses may differ side conditions: one organization archivesPOs follow-ups created; another archives POs successful; third organization archives POs immediately automatically gettingresponse; fourth based explicit user-request. Part motivationbehind SAM provide flexibility. Planning based SAM usedautomatically generate first version desired process.4(C) Process redesign. Sometimes best option design new process scratch.business experts aware underlying infrastructure,incurs huge costs process implementation time. SAM opens possibilitybusiness experts explain individual steps new process termsstatus variable value changes, i.e., terms start/end state correspondingbusiness user considers atomic processing step. Planning showsstatus changes implemented using existing transactions.4. alternative equipping BO standard life-cycle set thereof would comeprize flexibility loss complex BOs, choice made SAP.596fiSAP Speaks PDDLparticular, planner called business object X (e.g., sales order)within process created object (e.g., customer quote).Hence, despite mentioned absence cross-BO constraints current versionSAM, planning help create non-trivial processes spanning several BOs.use cases supported prototype SAP; illustrate use (C),cross-BO situation mentioned, Section 7.3.obvious requirement planner useful instantaneous response. Typicallyuser sitting computer waiting planner answer. Further,functionality must accessible conveniently. particular, time user wants callplanner, needs provide planning goal (and possibly initial state).essential done simple intuitive manner, without in-depth expertiseBO question. Thus limit conjunctive goalssense want status variables values end process, likegoal CQ.followUp:documentCreated CQ.archiving:archived illustrativeexample. prototype, goals specified using simple drop-down menus.SAM originally intended planning, course perfectpurpose. discuss main limitations Section 9, need briefly touchtwo points already. absence cross-BO constraints current versionSAM implications planner setup performance, play roleexperiments.5 Another issue plan quality. duration/cost actions may differvastly, SAM contain information this: relevant SAMsoriginal purpose, software engineering. address plan quality measures herein.planning algorithm course attempts find small plans. gives qualityguarantee regard, practical value guarantee would doubtful.4. Planning Formalizationdesign syntax semantics suitable planning formalism capturing SAM,illustrate formalism using running example.4.1 SAM Planning Tasks: SyntaxGiven close correspondence SAM business objects (Definition 3) finite-domainplanning tasks (Definition 1), straightforward extend latter capture former.Definition 4 (SAM Planning Task) SAM planning task tuple (X, A, I, G) whoseelements finite-domain planning tasks, except action set A.takes form (pre , Ea ) pre propositional formula atoms{x = c | x X, c dom(x)}, Ea set partial variable assignments.members eff Ea outcomes a.discussed above, keep goal simple possible. effects, placenegation-free propositional DNF formulas Definition 3, sets Ea outcomes. action preconditions Definition 3. generalizes partial variable5. shall discuss Section 9, BO interactions exist. according extension SAM planned,could principle tackled using exact planning technology presented herein.597fiHoffmann, Weber & Kraftassignments Definition 1 equivalent negation-free conjunctionsatoms {x = c | x X, c dom(x)} arbitrary propositional formulas atoms.generalization poses issue defining plan semantics; implementation level,current planning systems compile preconditions negation-free conjunctions,using methods originally proposed Gazen Knoblock (1997).obtain SAM planning task (X, A, I, G), given input SAM business object= (X(o), A(o), I(o)) along goal conjunction G(o), first set X := X(o), := I(o),G := G(o). a(o) A(o) include one A, pre := pre a(o) .eff a(o) , create one partial variable assignment eff disjunct DNFformula, define possible outcomes Ea set eff .convention, denote Ad := {a | |Ea | = 1} := {a | |Ea | > 1}sets deterministic non-deterministic actions SAM planning task, respectively.Ad , eff denote single outcome a.4.2 SAM Planning Tasks: SemanticsSAM action preconditions pre a(o) direct correspondence usual planning preconditions, cf. point (II) Section 3.2. contrast, SAMs disjunctive effects eff a(o) requirecreate mix two different kinds planning actions non-deterministic actionsobservation actions literature. understand this, reconsider role SAMaction effects eff a(o) original environment, i.e., point (III) Section 3.2. onedisjuncts occur, plan generation time know one. planexecution time, SAP system executing action observe relevant data content,decide branch take. example Figure 1, Check CQ Completeness answer CQ.completeness:complete BO data complete, answerCQ.completeness:notComplete otherwise. course, SAP system keeps trackoutcomes occured. words, (a) SAMs disjunctive effects correspond observationactions, (b) internally observe environment data modeled planning level.Due (a), makes perfect sense handle actions introducing case distinctionsplan generation time, one outcome. Due (b), direct linkobservation reduction uncertainty planning level. execution, valuesobserved variables known prior observation already, changeresult applying action. example, CQ.completeness.notComplete consideredtrue prior first application Check CQ Completeness, may changedCQ.completeness.complete action. respect, outcomeset (an arbitrary DNF) general domain particular variable, SAMsdisjunctive effects similar common notions non-deterministic actions.simplicity, henceforth refer SAMs disjunctive-effects actions nondeterministic actions. Another important point regarding actions data contentallowed change process running; data filled directly upon creation BO. Thus outcome non-deterministic action throughoutplan execution, makes sense execute actionplan. example, point repeatedly applying Check CQ Completeness.final issue decide plan actually is. Cimatti, Pistore, Roveri, Traverso(2003) describe three common concepts, presence non-deterministic ac598fiSAP Speaks PDDLtions: strong plans, strong cyclic plans, weak plans. discuss latter twobelow; desirable property first one. strong plan guarantees reachgoal matter action outcomes occur. define formally, setting.action tree tree whose nodes actions A, whose edgeslabeled partial variable assignments. action tree exactly |Ea | outgoingedges, one (and labeled with) eff Ea . following definitions,av referssubset non-deterministic actions yet used, thus stillavailable, given state plan execution. Recall seff , defined Section 2,over-writes variable values defined eff , leaves unchanged elsewhere.Definition 5 (Strong SAM Plan) Let (X, A, I, G) SAM planning task =ndAd . Let state, letav , let action tree {STOP }.say strong SAM solution (s,av ) iff either:(i) consists single node STOP , |= G;(ii) root Ad , |= pre , sub-tree rooted child strongSAM solution (s eff ,av );nd(iii) root Aav , |= pre , and, children reached via edgelabeled eff Ea , sub-tree rooted child strong SAM solution(s eff ,av \ {a}).strong solution (I, ), called strong SAM plan.Compare Definition 2. Item (i) present definition essentially same,saying nothing goal already true. difference Definition 2,distinguish deterministic actions (ii) non-deterministic ones (iii). formercase, single child require remainder tree solve child,similarly Definition 2. latter case, several children needsolved respective sub-tree. corresponds desired case distinction observingaction outcomes plan execution time.Note that, throughout plan, uncertainty current variable values.Note also solve, state, pair consisting state subset nondeterministic actions. reflects fact whether action tree solves statedepends state itself, also non-deterministic actions stillavailable. maintenance setav ensures allow non-deterministicaction once, path (but action may occur several timesseparate paths). Thus one execution plan applies action once.problem Definition 5 strong plans typically exist. illustrate this, consider Figure 2, showing weak SAM plan, notion formallydefine, running example Figure 1. Recall goal assumedCQ.followUp:documentCreated CQ.archiving:archived. either Check CQCompleteness Check CQ Consistency, shown top Figure 2, resultnegative outcome (CQ.completeness:notComplete CQ.completeness:notConsistent),goal becomes unreachable. Thus strong plan exist SAM planning task. phenomenon limited illustrative example. experiments,almost 75% large sample SAM planning tasks strong plan.address this, one define complicated goals, weaker notion plans.former option, one could use goals specifying alternatives, preferences, and/or temporal599fiHoffmann, Weber & KraftCheck CQ CompletenessNCheck CQ ConsistencyNCheck CQ Approval StatusnotNecNecDecide CQ ApprovalgrantedSubmit CQnotGrantedMark CQ AcceptedSubmit CQCreate FollowUp CQMark CQ AcceptedArchive CQCreate FollowUp CQArchive CQFigure 2: weak SAM plan running example Figure 1. STOP actionsshown, FAIL actions marked (red) crosses.plan properties (e.g., Pistore & Traverso, 2001; Dal Lago, Pistore, & Traverso, 2002; Shaparau, Pistore, & Traverso, 2006; Gerevini et al., 2009). However, goals specifiedonline business users absolutely essential simple possible.hence decided go second option.6weak plans Cimatti et al. (2003) liberal purposes. guaranteeleast one possible execution plan reaches goal, posing requirements executions. example, Figure 2, would mean allowplan handle left-hand side outcome Check CQ Approval Status, i.e.,CQ.approval:notNecessary, nothing (attach empty tree at)outcome, CQ.approval:necessary.strong cyclic plans? There, plan may cycles, provided everyplan state can, principle, reach goal. allows wait desired outcome, likecycle around dice throw, waiting obtain 6. Alas, repetitions non-deterministicSAM actions always produce outcome. futile insert cycletop Figure 2, waiting desired outcome Check CQ Completeness.plausible prompt user edit BO content repeat check (placeholderscycles could inserted planning post-process), suitable exceptionhandling general. Exception handling depends business context, typicallydepends actual customer using SAP system. impossible reflectmodel maintained centrally SAP.conclusion, perspective SAM-based planning much onehighlight bad outcomes user, exception handling6. works complex goals employed define alternative notions weak plans (byusing trivial fall-back goals). discuss detail Section 8.600fiSAP Speaks PDDLinserted manually afterwards. course, non-deterministic action leastone successful outcome, else would completely displaced process. Further,essential highlight outcomes bad really bad, i.e., markfailed outcomes could actually solved. definition reflects this:Definition 6 (Weak SAM Plan) Let (X, A, I, G) SAM planning task = Adnd. Let state, letav , let action tree {STOP , FAIL}.say weak SAM solution (s,av ) iff either:(i) consists single node STOP , |= G;(ii) root Ad , |= pre , sub-tree rooted child weakSAM solution (s eff ,av );nd(iii) root Aav , |= pre , and, children reached via edgelabeled eff Ea , either: (a) sub-tree rooted childweak SAM solution (s eff ,av \ {a}); (b) sub-tree rootedchild consists single node FAIL, exists action tree 0weak SAM solution (s eff ,av \ {a}); (a) case least onechildren.weak solution (I, ), called weak SAM plan.Compared Definition 5, difference lies item (iii), longer requiresevery child solved. Instead, arrangement options (a) (b) meansfailed nodes leaves tree stop plan without success tolerated,long least one child solved, every failed node actually unsolvable.obvious correspondence discussion above. Figure 2, failed nodes, i.e.,sub-trees consisting special FAIL action, crossed (in red). Notedifference Cimatti et al.s (2003) definition weak plans discussed above:allowed cross right-hand side outcome Check CQ Approval Status,i.e.,CQ.approval:necessary, outcome solvable.remark allowing non-deterministic actions (or, generally,upper bound repetition non-deterministic actions) required Definition 6make sense. item (iii), definition recurses stating childrenmay unsolvable. recursion occurs also points Definitions 5 6,points action tree considered reduced least one node. unsolvablechildren non-deterministic actions Definition 6 (iii) (b), reduction givenquantification action tree 0 may suitable solve child.makes recursion sound, instead, set available non-deterministic actionsdiminished one. Without this, notion weak SAM plan would ill-defined:recursion step may result planning task again, allowing constructionplanning tasks considered solvable unsolvable.77. Concretely, say obtain Definition 6 Definition 6 considering states only, removinghandlingav . Consider example one variable x whose possible values B,initial state : x = A, goal G : x = B, single action two possible outcomes, x =x = B. Say consists a. bad outcome a, i.e., state x = A, identicaloriginal initial state I. unsolvable according Definition 6, outcome qualifiesDefinition 6 (iii) (b), thus overall task state considered solvable.contrast, using Definition 6 above, plan must solve state/available-non-deterministic-actionspair (x = A, {a}), bad outcome different pair (x = A, ). pair unsolvable,hence weak plan.601fiHoffmann, Weber & Kraftfollowing observation holds simply Definition 5 captures special caseDefinition 6:Proposition 1 (Weak SAM Plans Generalize Strong SAM Plans) Let (X, A, I, G)ndSAM planning task = Ad . Let state, letav , letaction tree {STOP }. strong SAM solution (s,av ),weak SAM solution (s,).avwords, strong SAM plan also weak SAM plan, hence particularSAM planning task solvable strong semantics also solvableweak semantics. inverse obviously true. counter-example runningexample Figure 2.remark that, trivially, deciding whether plan exists hard both, Definition 5Definition 6. special case actions deterministic generalizationDefinition 2, mentioned problem PSPACE-complete.4.3 SAM Planning Tasks: Running Exampleillustration, encode running example, Figure 1, SAM planning task(X, A, I, G). set X := {Arch, Compl , Cons, Appr , Subm, Acc, FoUp}, abbreviating status variable names mentioned Figure 1. example, Arch standsvariable CQ.archiving. domain Arch, Compl , Cons, Subm, Acc, FoUp{true, false}. serves abbreviate various names used respective variablevalues Figure 1. domain Appr {notChecked , nec, notNec, granted , notGranted }.follows, brevity write facts, i.e., variable/value pairs, involving true/falsevalued variables like literals. example, write FoUp instead (FoUp, no).initial state SAM BO, thus SAM planning task, is:= {Arch, Compl , Cons, (Appr , notChecked ), Subm, Acc, FoUp}goal CQ.followUp:documentCreated CQ.archiving:archived:G = {FoUp, Arch}deterministic actions Ad are:Mark CQ Accepted: (Arch Subm, {Acc})Create Follow-Up CQ: (Arch Acc, {FoUp})Archive CQ: (Arch, {Arch})Submit CQ: (Arch ((Appr , notNec) (Appr , granted )), {Subm})Note action effects sets partial variable assignments, i.e., sets sets facts.deterministic actions, one partial variable assignment omitsecond pair set parentheses avoid notational clutter. Note alsodelete effects. effects assign new values affected variables, implicitly removingold values, cf. meaning eff defined Section 2.non-deterministic actions are:Check CQ Completeness: (Arch, {{Compl }, {Compl }})Check CQ Consistency: (Arch, {{Cons}, {Cons}})602fiSAP Speaks PDDLCheck CQ Approval Status:(Arch (Appr , notChecked ) Compl Cons, {{(Appr , nec)}, {(Appr , notNec)}})Decide CQ Approval:(Arch (Appr , nec), {{(Appr , granted )}, {(Appr , notGranted )}})Figure 2 shows weak SAM plan example. presentation user, simplepost-process (outlined Section 7.1) transforms plans BPMN workflows.5. Planning Algorithmsdesign adaptation FF (Hoffmann & Nebel, 2001), using variant AO* forward search Contingent-FF (Hoffmann & Brafman, 2005), well nave extensionFFs heuristic function. assume reader familiar heuristic searchgeneral, refer literature (e.g., Pearl, 1984) background.5.1 Searchstrong SAM planning Definition 5 use AO* tree search (Nilsson, 1969, 1971).weak SAM planning Definition 6 use variant search referSAM-AO*. focus follows mainly SAM-AO*, since AO* well-knownbecome clear side effect discussion.Search forward AND-OR tree whose nodes states (OR nodes) actions(AND nodes). ORed children states applicable actions, ANDed childrenactions alternative outcomes (for deterministic actions, single childnode trivializes). Like AO*, propagate node solved node failedmarkers. mechanics usual ones case nodes, i.e., markernode disjunction childrens markers. nodes, SAM-AO* differsusual conjunctive interpretation, implementing weak SAM planning semanticsDefinition 6: amongst things, node failed childrenfailed. Figure 3 provides overview SAM-AO*, highlighting differences AO*.Figure 4 illustrates algorithm simplification running example.One feature algorithm immediately apparent book-keepingnon-deterministic actions still available. Recall that, line Definitions 56, allow non-deterministic action execution plan.search algorithm strong planning (AO*) weak planning (SAM-AO*)means nodes contain state s, pair (s,av ) giving statendwell subset used node. refer pairssearch states on. book-keeping setsav straightforward.initial state, non-deterministic actions still available. Whenever non-deterministicaction applied, outcome states, longer available. illustration, consideraction sets reduced Figure 4 (BD).heuristic function h, assume given here, takes arguments searchstate, i.e., state available non-deterministic actions. actionavailability affects goal distance hence heuristic estimates. h(s) = 0 heuristicindicates goal states, h(s) = may indicate state unsolvable.algorithm trusts heuristic, i.e., assumes h returns values state603fiHoffmann, Weber & Kraftprocedure SAM-AO*input SAM planning task (X, A, I, G) = Ad , heuristic function houtput weak plan (X, A, I, G), unsolvableinitialize consist NI ; content(NI ) := (I, )status(NI ) :=solved h(I, ) = 0, failed h(I, ) = , unknown elsestatus(NI ) =unknownNs := select-open-node(T ); (s,av ) := content(Ns )Adav |= preAd is-direct-duplicate(Ns , eff ,av ) skip endifinsert Na child Ns ; content(Na ) :=0ndnd 0ndav := Aav , else Aav := Aav \ {a}eff Eas0 := eff0insert Ns0 child Na ; content(Ns0 ) := (s0 ,av )000ndstatus(Ns0 ) := solved h(s0 ,av ) = 0, failed h(s , Aav ) = , unknown elseendforstatus(Na ) := SAM-aggregate({status(N 0 ) | N 0 child Na })endforstatus(Ns ) := OR-aggregate({status(N 0 ) | N 0 child Ns })propagate-status-updates-to-I (Ns )endwhilestatus(NI ) =failed return unsolvable endifreturn action tree corresponding subtree 0 s.t. NI 0 and:inner nodes Ns 0 : status(Ns ) = solved Ns exactly one child Na 0 ;nodes Na 0 : children Ns0 Na contained 0is-direct-duplicate(N, s0 ,av ) :=SAM-aggregate(M ) :=solvedtrue predecessor N0 N s.t. content(N0 ) = (s0 ,av )false else: = solved,: (m = solved = failed): = failedelsefailedunknown: = solvedsolvedfailed: = failedOR-aggregate(M ) :=unknown elseFigure 3: Pseudo-code SAM-AO*, highlighting differences AO*.indeed goal state/unsolvable. Detecting unsolvable states within capabilitiesFFs heuristic, paramount importance planning SAM models. behaviorlike heuristic Figure 4 (BD), immediately marks unsolvable nodessuch. get back Section 5.2.2 below.overall structure SAM-AO* AO*. Starting initialsearch state (compare Figure 4 (A)), iteratively use select-open-node select nodeNs tree yet expanded whose status unknown; selectioncriterion based nodes f -value, explained below. expand selected nodeapplicable actions (in Figure 4 (B), omit Check CQ Consistency savespace), insert one new node possible outcome actions (Comp vs.604fiSAP Speaks PDDLBComp, Cons; CheckComp, CheckConsunknown; h=2Comp, Cons; CheckComp, CheckConsunknown; h=2Check CQ Completeness unknownComp, Cons; CheckConsunknown; h=1CComp, Cons; CheckComp, CheckConsunknown; h=2Check CQ Completeness unknownComp, Cons; CheckConssolved; h=1Comp, Cons; CheckComp, CheckConssolved; h=2Check CQ Completeness solvedComp, Cons; CheckConsfailed; h=inftyComp, Cons; CheckConssolved; h=1Check CQ Consistency solvedComp, Cons; (none)solved; h=0Comp, Cons; CheckConsfailed; h=inftyComp, Cons; CheckConsfailed: h=inftyCheck CQ Consistency solvedComp, Cons; (none)failed; h=inftyComp, Cons; (none)solved; h=0Comp, Cons; (none)failed: h=inftyFigure 4: Phases SAM-AO* simplifaction running example (Figure 1),two variables CQ.completeness CQ.consistency,two actions Check CQ Completeness Check CQ Consistency. goal,using abbrevations here, Comp, Cons. search states (s,av ),left-hand side semicolon show state s, right-hand sideshow setav available non-deterministic actions.-Comp Figure 4 (B)); discuss is-direct-duplicate function below. newnode, i.e., corresponding search state, evaluated heuristic function.outcomes action inserted, status updated. actions applicablecurrent node Ns inserted, status Ns updated. latter update, reflectedOR-aggregate equation Figure 3, exactly AO*. key difference AO* liesformer update, reflected SAM-aggregate equation. equation obviouscorrespondence Definition 6. Figure 4 (B), neither updates yields newinformation, status one action outcomes, Comp, Cons; CheckCons,unknown. changes Figure 4 (C), status outcomes becomesdefinite (solved/failed), updates propagate information actionsearch state node Ns applied to.status Ns set, propagate-status-updates-to-I (Ns ) performsbackward iteration starting Ns , updating action search state along wayusing two functions, OR-aggregate SAM-aggregate. necessary sincestatus Ns may changed, may affect status predecessors.happens, example, Figure 4 (D) status first node actionchange unknown solved . algorithm terminates initial node605fiHoffmann, Weber & Kraft(the search tree root) solved failed. former case, solved sub-tree returned.happens Figure 4 (D), sub-tree returned equivalent start (top twoactions) example plan Figure 2.addition status markers, SAM-AO* also annotates search states f values, well current best action. shown Figure 3 since (almost)identical done AO*. f -value search state node minimumchildren, plus 1 accounting cost applying action; minimizing childbest action. f -value action node maximum children, exceptherein lies difference AO* set action value unlesschildren marked failed. select-open-node procedure starts NI keepschoosing best actions arrives non-expanded state, selected Ns .is-direct-duplicate function Figure 3 disallows generation search statesidentical one predecessors tree. refer direct duplicatepruning. Note method prunes duplicates within deterministic partssearch tree. predecessor node N0 Figure 3 found, actionsN0 N deterministic, otherwise content(N0 ) would contain strictlynon-deterministic actions N . Obviously, direct duplicate pruning preserves soundnesscompleteness search algorithm. have:Proposition 2 (SAM-AO* Complete Sound) Let (X, A, I, G) SAM planning task, let h heuristic function. SAM-AO* terminates run taskh. Provided h(s) = 0 iff goal state, h(s) = unsolvable, SAM-AO*terminates success iff exists weak SAM plan task, action treereturned case plan.follows known results AO*, definition, two simple observations. First, eventually, tree path non-deterministic actions availableanymore. Second, direct duplicate pruning allows finitely many nodes SAM planning task without non-deterministic actions.reader might wonder whether stronger duplicate pruning methods could defined, across non-deterministic actions tree. nave approach, askingwhether predecessor N contains state ignoring sets availablenon-deterministic actions work SAM-AO*. renders algorithm unsound. pruning method may mark solvable search states failed,failed nodes part solution weak plan. illustration, considersimple example variable x values A, B, C, initial state A, goalC, three actions: a1 precondition two possible outcomes B C; a2precondition B outcome A; a3 precondition outcome C. Say searchchosen apply a1 first. Consider a1 unfavorable outcome B. point, orderobtain plan, must apply a2 , a3 achieve goal C. However, outcome state: x = a2 initial state. Hence pruned, hence a1 outcome Bmarked failed, hence algorithm wrongly concludes a1 outcomes qualifyDefinition 6 (iii), a1 plan.606fiSAP Speaks PDDL5.2 Heuristic Functioncompute goal distance estimates, use all-outcomes-determinization knownprobabilistic planning (Yoon et al., 2007) get rid non-deterministic actions, runFF heuristic (Hoffmann & Nebel, 2001) off-the-shelf. sake self-containedness,next explain detail. reader familiar FF may skip Section 5.2.2.5.2.1 Relaxed Planning GraphsFFs heuristic function one range general-purpose planning heuristics basedrelaxation widely known ignoring delete lists (McDermott, 1999; Bonet & Geffner,2001). Heuristics kind emerged late 90s still highly successful.follows, assume action preconditions goal conjunctionspositive atoms, thus equivalent sets facts. general formulas,preconditions SAM planning tasks, one apply known transformations (Gazen &Knoblock, 1997) achieve this.name delete lists comes Boolean-variable representation planning tasks.Translated context, relaxation means variables accumulate, ratherchange, values. illustration, say CQ.archiving:notArchived running example, apply action Archive CQ whose effect CQ.archiving:archived.Then, relaxation, resulting state CQ.archiving:notArchived, CQ.archiving:archived, containing old new value variable CQ.archiving. Thusactions BO, require customer quote archived yet,remain applicable, difference plan Figure 2, relaxed plan archiveCQ right start proceed rest processing.Viewing variable assignments sets facts, relaxed action application equivalenttaking set union current state action effect. yields strictlylarger set facts real application action (CQ.archiving:notArchived,CQ.archiving:archived instead CQ.archiving:archived). Satisfaction preconditionsgoal tested asking inclusion set. Bylander (1994) provedthat, within relaxation, plan existence decided polynomial time. alsoproved, however, optimal relaxed planning, i.e., finding length shortest possible relaxed plan, still NP-hard. Therefore, heuristics used practical plannersapproximate length. Specifically, FF heuristic build herein computesnecessarily optimal relaxed plan. algorithm consists two phases. First,builds relaxed planning graph (RPG) approximate forward reachability.extracts relaxed plan RPG.Figure 5 shows RPG computed planner. algorithm gets statewell remaining non-deterministic actions,av . determinizes latteractions, inserting possible outcomes individual new deterministic actionnew action set A0 . following loop simple fixed point operation setsfacts. initial set F0 equal state whose goal distance shall estimated.loop iteration increments Ft effects actions whose preconditionsreached. case goals reached, algorithm stops success returnsiteration index t. fixed point occurs happens, algorithm returns .607fiHoffmann, Weber & Kraftprocedure RPGinput SAM planning task (X, A, I, G) = Ad ,state s, available non-deterministic actionsavoutput Number relaxed parallel steps needed reach goal,A0 := Ad {(pre , {eff }) |av , eff Ea }F0 := s, := 0G 6 FtA0t := {a A0S| pre Ft }Ft+1 := Ft aA0 effFt+1 = Ft return endif:= + 1endwhilereturnFigure 5: Pseudo-code building relaxed planning graph (RPG).RPG returns < , heuristic function algorithm enters second phase,relaxed plan extraction. straightforward backchaining procedure selecting supporting actions goals, iteratively supporting actions preconditions.backchaining makes sure select feasible supporters exploiting reachabilityinformation encoded sets Ft . Note good heuristic estimatorcounts parallel action applications could make transactions 1000 BOsparallel still count single step.Consider simplified example Figure 4. root node Comp, Cons; CheckComp, CheckCons, relaxed plan returned hCheck CQ Completeness+ , CheckCQ Consistency+ i, superscript + indicates actionsdeterminized set A0 Figure 5, choosing positive outcome actions.heuristic value returned 2, Figure 4. Indeed, heuristic valuesFigure 4 would returned FFs heuristic. particular, -Comp, i.e.,CQ.completeness:notComplete, holds state, action Check CQ Completeness longer available, heuristic value returned actionA0 achieve goal Comp, thus RPG fixed point contain goal.Similarly -Cons holds state Check CQ Consistency longer available.5.2.2 Detecting Failed Nodesfollows directly from, e.g., results Hoffmann Nebel (2001), RPG stopssuccess iff exists relaxed plan task (X, A0 , s, G). this, easilyget following result relevant us:Proposition 3 (RPG Dead-End Detection SAM Sound) Let (X, A, I, G)nd setSAM planning task = Ad . Let state, letavnon-deterministic actions. RPG run inputs returns , existsweak SAM solution (s,av ).see this, note action set A0 Figure 5 is, perspective plan0existence, over-approximation actual action set Adav got available.allows us choose, non-deterministic action, outcome want. Thus,0plan using Adav trivially construct plan using . plan using608fiSAP Speaks PDDLA0 exists neither plan using Adav . suffices see nonexistence relaxed plan (based A0 ) implies non-existence real plan (based A0 ).obvious, concluding argument.course strong simplification act one could choose outcomesnon-deterministic actions. Part motivation demonstratenecessary, least application context, dramatically enhance off-the-shelf planningtechniques. simplistic approach presented suffices obtain good performance.particularly true regarding ability detect dead-ends. experimentedtotal 548987 planning instances based SAM. (within limited time/memory)found weak plan 441884 instances. Around half actions plansnon-deterministic, typically yield failed nodes plan. every onefailed nodes, every one 441884 solved instances, RPG returned .5.2.3 Helpful Actions Pruningalso adopt FFs helpful actions pruning. Aside goal distance estimate,relaxed plan used determine promising subset H(s) helpful actionsactions applicable evaluated state s. Essentially, H(s) consists actionsapplicable contained relaxed plan computed describedSection 5.2.1.8 action subset used pruning method simply restricting,search, expansion state consider actions H(s). kindheuristic action pruning paramount importance planner performance (Hoffmann &Nebel, 2001; Richter & Helmert, 2009).SAM setting, one important aspect FFs helpful actions pruningaccurate enough distinguish relevant BOs irrelevant ones. say, BOmentioned goal, action pertaining ever consideredhelpful. simply because, pointed previously, SAM currently modelcross-BO interactions. BO goal relaxed plan extractionnever create sub-goals pertaining .obvious well-known caveat helpful actions pruningpreserve completeness. H(s) may contain actions actually start plans. happens, search may stop unsuccessfully even though plan exists.pertains classical planning pertains AO* SAM-AO* used herein.Importantly, helpful actions pruning SAM-AO*, i.e., weak SAM planning perDefinition 6, another subtle caveat: preserve soundness. Considerexample variable x values A, B, C, initial state A,goal C, three actions action a1 precondition two possibleoutcomes B C, a2 precondition B outcome A, a3 preconditionoutcome C. Say search applied a1 . Say Ns := (s,av ) nodecorresponding a1 unfavorable outcome B. way complete a1 planattach a2 , a3 Ns . Presume helpful actions pruning, node Ns , removes a2 .Ns marked failed, wrongly conclude a1 plan.8. FFs definition H(s) little complicated, adding also actions selectedrelaxed plan achieve relevant sub-goal. omit brevity. Recent variants helpfulactions pruning, different heuristic functions like causal graph heuristic (Helmert, 2006),make additions, selecting H(s) based membership abstract solutions only.609fiHoffmann, Weber & KraftUsing helpful actions pruning, one may incorrectly mark node Ns failed. Nsleaf weak plan , marked failed even though solvable action tree 0 ,valid plan. fixed, plan-correction post-process, attaching 0Ns , 0 found running SAM-AO* without helpful actions pruning Ns .implement post-process because, according experiments, unnecessarypractice: discussed end previous sub-section, failed nodes Ns441884 weak plans heuristic value , thus proved be, indeed, unsolvable.6. Experimentsdescribe prototype SAP next section. follows, evaluateplanning techniques detail scientific point view. experiments aimedunderstanding three issues:(1) applicability strong respectively weak planning SAM?(2) runtime performance planner sufficient envisioned application?(3) interesting SAM planning benchmark?first explain experiments setup. describe experiments FFstrong plans, FF weak plans; summarize findings blind search.experiments consider instances pertaining single BO, finally examinehappens scaling number relevant BOs.6.1 Experiments Setupexperiments run 1.8 GHz CPU, 10 minute time 0.5 GB memorycut-off. planner implemented C modification FF-v2.3. source code,problem generator used experiments, anonymized PDDL encoding SAMavailable download http://www.loria.fr/~hoffmanj/SAP-PDDL.zip. SAMAO* implementation modifed AO* implementation Contingent-FF (Hoffmann& Brafman, 2005). Like planner, weight heuristic values factor 5 (weplay parameter).focus case initial state set specified SAM. Thus SAMplanning instance follows identified goal: subset variable values.number instances finite, enormous; choosing subset variablesconstrained 21110 options. follows, mostly consider goals whosevariables belong single BO. sensible because, previously stated, SAM currentlyreflect interactions across BOs. made instance generator allowscreate instance subsets characterized number |G| variables constrained goal(this parameter relevant business users, shall see also heavily influencesplanner performance). given |G|, generator enumerates possible variable tuples,allows randomly sample given number value tuples.maximum number variables BO, current version SAM, 15. createdpossible instances |G| = 1, 2, 3, 13, 14, 15 number instancesaround 50000. values |G|, chose value got around50000 instances each. total number instances generated 548987.610fiSAP Speaks PDDLSince SAM currently model cross-BO interactions, single-BO goalprinciple supply planner actions pertaining BO.henceforth refer option using BO-relevant actions. Contrasting this,full actions option supplies planner actions (no matter BO pertainto). use BO-relevant actions experiments wish enableplanner prove planning task unsolvable full actions, alwaysimpossible reachable state space much vast. baseline, however,use full actions. motivation helpful actions pruning detectirrelevant actions anyway (cf. Section 5.2), long term, likely SAMmodel cross-BO interactions.6.2 Strong SAM Plansfirst experiment, evaluate performance strong planning SAM, i.e,run FF standard AO* tree search forcing children nodes solved.identify two parameters relevant performance FF: kind BO considered,|G|. Figure 6 shows coverage state evaluations (number calls heuristicfunction) depend parameters.Consider first Figure 6 (a). x-axis ranges BOs, i.e., data point correspondsone kind BO.9 ordering BOs decreasing percentage solved instances.BO, y-axis shows percentage solved, unsolved, failed instances withinBO. overall message mixed. one hand, coverage perfect 194371 BOs, half BOs tested instances strong planfound FF. hand, 177 BOs, coverage rather bad.51 BOs, single instance solved. 126 BOs between, coveragedeclines steeply. Importantly, counting unsolved cases total (across BOs), turns88.83% instances unsolved. words, almost 90%tested cases FFs search space contain strong SAM plan. course,percentage pertains particular distribution test cases used. Stillresult indicates applicability strong planning SAM quite limited.Another interesting aspect Figure 6 (a) failed cases rare: constitute0.8% total instance set. is, due helpful actions pruning, FFs searchspaces typically small enough exhausted within given time memory.Consider Figure 6 (b), shows coverage y-axis |G| x-axis.Again, message mixed. one hand, single goal (|G| = 1), 58.85%instances solved strong plan. hand, number solved casesdeclines monotonically, quite steeply, growing |G|. 2 goals 36.95%,4 goals 29.03%, 5 goals 23.86%. |G| 10, number solved casesless 5%, |G| 13, number less 1%.One may wonder point whether FFs helpful actions pruning responsible frequent non-existence strong plans. answer no. secondexperiment strong planning, ran FF without helpful actions, giving input9. Note include 404 BOs. Precisely, consider 371 them. remaining 33 BOsinteresting planning: variable values true initial state unreachablevalues set procedures encoded SAM.611fi10010090908080707060CoverageCoverageHoffmann, Weber & KraftSOLVEDUNSOLVEDFAILED504060403030202010100SOLVEDUNSOLVEDFAILED500050100150200BO250300350123456710000010000100001000UNSOLVED MAXSOLVED MAXUNSOLVED MEANSOLVED MEAN100910 11 12 13 14 15(b)100000Number evaluated statesNumber evaluated states(a)8|G|101UNSOLVED MAXSOLVED MAXUNSOLVED MEANSOLVED MEAN1000100101050100150200BO250300350123(c)45678|G|910 11 12 13 14 15(d)Figure 6: Strong planning FF full action sets. Coverage (a,b) stateevaluations (c,d) data, plotted individual kinds BOs (a,c) |G| (b,d).SOLVED: plan found. UNSOLVED: search space (with helpful actions pruning) exhausted. FAILED: time memory. Ordering BOs (c)increasing y-value curve individually.BO-relevant actions order enable proofs unsolvability. result clear:number solved cases hardly changes all. total percentage solved casesprevious experiment 10.38%, total percentage new experiment 10.36%.low success rate due unsolvability, prohibitively large search spaces.total, 74.1% instances proved unsolvable; FF fails 15.54%.Given above, applicability strong planning SAM appears limited unlessrestrict attention BOs variables and/or single-goal planning tasks.general perspective, best option appears to:(I) Try find strong SAM plan (using FF AO*).(II) (I) fails, try find weak SAM plan (using FF SAM-AO*).setting, relevant long wait answer (I). Figures 6(c) (d) provide data this. consider instances FF terminated regularly(plan found helpful actions search space exhausted), consider performanceterms number evaluated states, i.e., number calls heuristic function.612fiSAP Speaks PDDLordering BOs Figure 6 (c) increasing y-value curve individually;otherwise plot would unreadable. striking observation that, 351371 BOs, maximum number state evaluations 100. solved instances,even holds 369 BOs, i.e., 2 BOs. maximum number stateevaluations done order find plan 521, taking 0.22 seconds total runtime. meanbehavior even good-natured, peaking 9.85 state evaluations. Waitingtime consuming, peak 42954 evaluations respectively 110.87 seconds.However, since yes answers given quickly, practical use onlinebusiness process modeling environment seems feasible simply give strong planningone second (or less), switch weak planning case successful.Consider Figure 6 (d). Like (c), observe low number state evaluations required solved cases. Somewhat surprisingly, conclusive behavior|G|. reasons entirely clear us. UNSOLVED MAX curve flattop larger search spaces lead failure. discontinuities around |G| = 12presumably due BO structure. BOs 12 variables, variancedata higher region. sharp drops UNSOLVED MAX SOLVEDMAX, additional factor strong plans large goals (cf.Figure 6 (b)): strong plans exist found easily; disproving existencestrong plan easier larger goals, since increases chance relaxedplan identify least one unsolvable goal.Summing findings regarding issue (1) [applicability strong vs. weak planning SAM] wish understand experiments, SAM admit manystrong plans, instances tend solved easily FF.6.3 Weak SAM Planssee weak SAM planning solve 8 times many instancesstrong SAM planning namely around 80% test cases. Precisely, 548987instances, 441884 solved; 43 solved default configurationplanner. average percentage non-deterministic actions, across weak plans,48.29%; maximum percentage 91.67%. Figure 7 shows results, givingfour kinds plots previously shown strong planning Figure 6.Consider first Figure 7 (a). see that, now, coverage perfect 274 371 kindsBOs, opposed 194 BOs true strong planning. latterBOs subset former: wherever strong planning perfect coveragetrue weak planning. Whereas strong planning 0 coverage instance solved51 BOs, cases here. minimum coverage 18.07%, coverage50% 9 BOs. total, strong planning solves 10.38% testcases, solve 80.48%. said, still 17.12% unsolved cases 2.4% failedcases, gets much worse BOs. Per individual BO, fraction unsolvedinstances peaks 81.92%, fraction failed instances peaks 14.98%.Consider Figure 7 (b). |G| = 1 handled perfectly 100% coverage opposed58.85% strong planning followed fairly steady decline |G| grows.explanation discontinuity |G| = 3, 4 could |G| = 3 experiment613fi10010090908080707060CoverageCoverageHoffmann, Weber & KraftSOLVEDUNSOLVEDFAILED504060403030202010100SOLVEDUNSOLVEDFAILED500050100150200BO25030035012345671e+0610000010000010000UNSOLVED MAXSOLVED MAXUNSOLVED MEANSOLVED MEAN1000910 11 12 13 14 15(b)1e+06Number evaluated statesNumber evaluated states(a)8|G|10010UNSOLVED MAXSOLVED MAXUNSOLVED MEANSOLVED MEAN1000010001001011050100150200BO2503003501(c)2345678|G|910 11 12 13 14 15(d)Figure 7: Weak planning FF full action sets. Coverage (a,b) state evaluations (c,d) data, plotted individual kinds BOs (a,c) |G| (b,d).SOLVED: plan found. UNSOLVED: search space (with helpful actions pruning) exhausted. FAILED: time memory. Ordering BOs (c)increasing y-value curve individually.exhaustive |G| = 4 sample. above, higher variance large |G|explained much smaller number BOs region.Figures 7 (c) (d) provide deeper look performance instancesFF terminated regularly (plan found helpful actions search space exhausted). LikeFigure 6 (c), ordering BOs Figure 7 (c) increasing y-value curveindividually. number state evaluations typically low. phenomenon quiteextreme shown strong planning Figure 6 (c). likelygenerous definition plans makes difficult FF prove nodes unsolvable,hence prune large parts search space. detail, 350 371 BOs,maximum number state evaluations 100; solved instances, holds 364BOs (the corresponding numbers strong planning 351 369). maximumnumber state evaluations done order find plan 54386, taking 27.41 seconds totalruntime (521 0.22 strong planning). SOLVED MEAN curve seemaximal case exceptional mean number state evaluations per BO peaks614fiSAP Speaks PDDL47.78. Comparing UNSOLVED MEAN curve, see large numbersearch nodes is, one would expect, much typical unsolved instances.Figure 7 (d), see overall behavior state evaluations |G| largelymirrors coverage, including discontinuity |G| = 3, 4. notableexception fairly consistent decline SOLVED MAX |G| > 3. unclearus reason is.conclusion regarding issues (2) [planner performance] (3) [benchmark challenge] wish understand? issue (2), results look fairly positive.particular, consider solved instances (weak plan found). explained above,number state evaluations largely well-behaved. addition, heuristic functionquite fast. stated, maximum runtime 27.41 seconds. second largest runtime2.6 seconds, third largest runtime 1.69 seconds; plans foundless 0.3 seconds. practical approach could simply apply small cut off, like0.5 seconds, perhaps minute time critical. yields quick step (II)follow-up similarly quick step (I) determined strong planning.strategy leaves us are, total, 17.12% unsolved instances 2.4%failed ones. important benchmark challenge future research? Answeringquestion first entails finding whether solve instancesusing helpful actions pruning, not, whether solvable all.ran FF without helpful actions pruning unsolved failed instancesFigure 7, slightly 100000 instances total. enabled unsolvability proofsgiving input BO-relevant actions, facilitated larger search spacesincreasing time/memory cut-offs 10 minutes 0.5 GB 30 minutes 1.5 GBrespectively. failed instances still failed new configuration. previouslyunsolved instances, 47.43% failed, 52.52% proved unsolvable.10 0.05%43 instances solved (the largest plan contains 140 actions). influence |G|kind BO similar seen. number state evaluationsvastly higher before, mean max 10996.72 respectively 289484 solvedinstances. heuristic extremely fast BO-relevant actions, hencefinding plan takes mean runtime 0.12 seconds. max, second max, thirdmax runtimes 2.94 seconds, 0.7 seconds, 0.53 seconds respectively; plansfound less 0.15 seconds. Thus, above, 6 441884 weakplans experiment found less 0.5 seconds.all, changing planner configuration achieves progress instancessolved default configuration, appears many unsolvableanyway. certainly challenge research.6.4 Blind Searchexplore extent heuristic techniques actually required successfully dealdomain thus extent domain constitutes interesting benchmark10. Unsolvability certain goal value combinations, i.e., partial assignments BOs variables, occursnaturally since variables independent. example, unsolvable instancesrequired BO simultaneously satisfy BO.approval:In Approval BO.release:Released.running example, kind situation arises, e.g., requiring CQ.approval:notChecked togetherCQ.acceptance:accepted.615fiHoffmann, Weber & Krafttechniques ran experiment blind search. used AO* trivialheuristic returns 1 non-goal states 0 goal states. Since weak planningmuch applicable strong planning SAM, used weak planning semantics.provided input BO-relevant actions otherwise, blind forward searchtrivially hopeless due enormous branching factor.sake conciseness, discuss results detail here. summary,blind search quite hopeless. runs time memory 79.36% test instances.solves 19.04% opposed 80.48% solved based FF. Interestingly, dueFFs ability detect dead-ends via relaxed planning graphs, blind search worseheuristic search even proving unsolvability: total, happens 5.99% casesusing FF, 1.60% cases using blind search.said, BOs status variables and/or status values,goals size 1 2, blind search fares well, well heuristic search. interestingbenchmarks lie outside region case 90% test instances.6.5 Scaling Across BOsFF scale gracefully planning tasks several BOs. selected, BO,one solved instance m(BO) maximum number state evaluations. Sinceinterest, include 404 BOs, i.e., also 33 BOs whose planninggoals trivial (either unsolvable true BOs initial state already); m(BO) 1BOs. generated 404 planning tasks COMk , 1 k 404, combining goalsm(BO) BOs number k, arbitrary ordering BOs. compareddata thus obtained data refer ACCk , obtained summingstate evaluations running FF turn individual goals m(BO).comparison valid since BOs mutually independent, plan COMkobtained union plans individual goals. Figure 8 shows data.60000COMBINEDACCUMULATED-INDIVIDUALNumber evaluated states500004000030000200001000001100200Number BOs300400Figure 8: Weak planning FF scaling number relevant BOs.State evaluations plotted number BOs goal specified.COMBINED means FF run conjunction goals (COMktext). ACCUMULATED-INDIVIDUAL gives sum datarunning FF individually single goal (ACCk text).616fiSAP Speaks PDDLFigure 8 shows quite clearly, FF scale gracefully planning tasksseveral BOs.11 largest instance solved k = 103, 38665 evaluations. sumstate evaluations solving 103 sub-tasks individually 529. possible explanationthat, adding goals additional BOs, actions helpful. increasednumber nodes may multiply search depth. Interestingly, disproportionatesearch increase occurs even new goal added trivial. example, ACC981 state evaluation ACC97 , COM98 COM97 difference amounts251 state evaluations. hand, k 14 BOs, ACCk still1000; difficulties arise k becomes quite large.7. Application Prototypes SAPintegrated technology two BPM modeling environments. next brieflyexplain transform planner output BPM format. outlinepositioning prototypes SAP, illustrate business user view technology.close words prototypes evaluated SAP-internally.7.1 Transforming Plans Business ProcessesBusiness users expect get process model human-readable BPM workflow format.use BPMN (Object Management Group, 2008). BPMN process model corresponding Figure 2 depicted Figure 9. process model makes use alternative (x)parallel (+) execution, unifies redundant sub-trees (Submit CQ . . . Archive CQ),removes failed outcomes, highlights red nodes may outcomes.changes obtained using following simple post-process planning.Approval:NecessaryApproval:NecessaryCheck CQCompletenessCheck CQConsistencyDecide CQApprovalSubmit CQMark CQAcceptedCheck CQApprovalStatusCreate FollowUp CQArchive CQFigure 9: Final BPM process created running example.11. vertical part plot ACCk because, noticed before, globally maximal numberstate evaluations, 54386 BO 347, extreme outlier.617fiHoffmann, Weber & KraftFirst, remove failed node together edge leading it. running example, Figure 2, concerns N branches Check CQ CompletenessCheck CQ Consistency, notGranted branch Decide CQ Approval. Next,separate property checking directing control flow. node1 child. replace node process step bearingname, followed XOR split (BPMN control nodes giving choice execution).example, concerns Check CQ Approval Status. re-unite XOR branches usingXOR joins (BPMN control nodes leading alternative executions back together), avoidingredundancies process finding pairs nodes root identical sub-trees. Figure 2, pertains two occurrences Submit CQ. introduce new XOR joinnode taking incoming edges found node pair. attach one copy commonsub-tree XOR join. insert BPMN start node, join leaves via new XORjoin, attach BPMN end node new (only) leaf plan. Finally, introduceparallelism finding non-interacting sub-sequences actions XOR splitsjoins introduced previously. (This heuristic notion parallelism,guarantee detect possible parallelizations process.)7.2 Positioning Prototypes SAPpart effort transfer research results SAP product developmentorganization, integrated planning approach two BPM prototypes.first one, called Maestro BPMN (Born, Hoffmann, Kaczmarek, Kowalkiewicz,Markovic, Scicluna, Weber, & Zhou, 2008, 2009), BPMN process modeling tool developed SAP Research primarily purpose research early prototyping.focus follows prototype, implemented contextcommercial SAP NetWeaver platform (SAP, 2010). NetWeaver one prominent software platforms SAP, central platform SAPs service-orientedarchitecture. encompasses functionalities required run SAP application.prototype implemented research extension SAP NetWeaver BPM.SAP NetWeaver BPM consists different parts process modeling execution.planning functionality integrated SAP NetWeaver BPM Process Composer,NetWeavers BPM modeling environment targeted creation new processes.process modeling done BPMN notation; notation given execution semanticsNetWeaver BPMs process execution engine.7.3 Demonstration NetWeaver BPM Prototypebriefly illustrate using planning functionality look like business users.consider application scenario (C) described Section 3.3, business userredesigns new process scratch. application scenarios (A) (B) Section 3.3using planning SAM-based development, respectively generating process templatebeginning modeling activity interface used.designing new process, user chooses atomic process steps accordinghis/her intuition. level, drawing box inserting descriptivetext. align intuitive design actual infrastructure process runon, planner allows check atomic steps implemented based existing618fiSAP Speaks PDDLFigure 10: Screenshot BPM modeling environment, showing business users specify planning goals.transactions. Say user designed process model shown Figure 10. Amongstothers, process contains step Release Purchase Order, whose intention orderpurchase special part required satisfy customer demand, customerquote accepted. user wishes inflate Release Purchase Orderactual IT-level process fragment intended meaning. double click stepopens shown interface entering planning initial state goal, i.e., desiredstatus variable value changes, associated step. status variable values chosenvia drop-down menus, selecting initial conditions left-hand side goalsright-hand side. present case, goal PO.Status:Ordered, initialcondition PO.Status:Created purchase order (PO) already createdbeforehand shall released.status variable values entered, user clicks Call composer.invokes planner, using specified initial condition/goal define SAM planning619fiHoffmann, Weber & KraftTimeoutNotification:QuoterejectedNotification:QuoteacceptedTimeoutNotification:QuoterejectedNotification:QuoteacceptedProcessPurchase OrderDataMark CustomerQuoteRejectedCreate SalesOrderQuotearchive QuoteReleasePurchaseOrderCancel PurchaseOrder (2)Mark CustomerQuoteRejectedCreate SalesOrderQuotearchive QuoteCheck PurchaseOrder DataCheck PurchaseOrder ApprovalStatusCancel PurchaseOrder (2)Trigger Followup processingPO Approval NecessaryPO ApprovalNecessaryDecide PurchaseOrder ApprovalEndPlace PurchaseOrderTrigger Followup processingEndFigure 11: BPMN process snippet screenshot Figure 10, (left)(right) calling planner. affected steps highlighted bold;actions failed outcomes highlighted red before.task.12 returned plan transformed BPMN, inserted process modelplace atomic step user working on; see illustration Figure 11.shown case, plan process snippet containing five atomic transactionsone XOR split two possibly failed outcomes, showing releasing PO entailsfirst process data, check data, invoke approval process similarillustrative example;13 finally, PO ordered.Note that, cross-BO interactions part SAM model, plannerhelps create process spans multiple BOs, indeed ties together functionalitycuts across departmental boundaries. process snippet shown Figure 10 invokespurchase goods supplier soon customer accepts quote. relevantcompanies sell highly customized goods (e.g., special-purpose ships),12. current implementation prototype, value variable x specified initialcondition given user, planner make use x, i.e., preconditions xassumed false x set action effect. One could course easily makecomfortable, assuming SAMs initial values default, propagating effects earlierSAM transactions (on BO) along process structure. First investigations latterperformed (May & Weber, 2008).13. Indeed, approval one design patterns SAP applied throughout SAM. actual patterncomplicated illustrative version here.620fiSAP Speaks PDDLturn must procure customized parts suppliers (e.g., ship engines). processingrequires combine services BOs belonging Customer Relationship Management(CRM) Supplier Relationship Management (SRM), hence two oppositeends system (and company). designer process typically intimatelyfamiliar one two, making especially helpful able call plannerobtain information one.7.4 Evaluation Prototype SAPprototype part research transfer project NetWeaver BPM group,shaped several feedback rounds developers architects. evaluationwithin SAP consisted mainly prototype demonstrations various SAP events.example, early version tool demonstrated 2008 global meeting SAPResearch BPM SI (SI stands Semantic Integration), included participantsSAP partners development. demonstrations received positive feedbackSAP software architects. perception functionality would significantlystrengthen link SAP BPM underlying software infrastructure, makingmuch easier access services provided effective manner. critical commentsfocused choices user interface design, non-logiciansunderstand meaning symbol, list several hundred BOslong drop-down box.customer evaluation data, foreseeable (or anyoneelse) able obtain data. first prototype became available, partnerorganization committed perform pilot customer evaluation. commitmentretracted context 2008/2009 financial crisis. Anyway, real customer evaluationdata may impossible come by, let alone publish, due privacy reasons.also issues arising positioning prototype inside SAPsoftware architecture. NetWeaver process execution engine currently connectactual services implement SAMs actions. SAM productiveuse within Business ByDesign, NetWeaver BPM built different technology stack.connection could principle established relatively easily all, service-orientationintended exactly sort thing however connection yetSAPs agenda, involves SAP-internal political issues. fact maindrivers presented technology authors paper left companymeantime course help remedy problem.8. Related Workbasic idea explored paper using planning systems help business expertscome processes close infrastructure around quite longtime. example, mentioned decade ago Jonathan et al. (1999).also discussed 2003 roadmap PLANET Network Excellence AI Planning(Biundo et al., 2003). recently, Rodriguez-Moreno et al. (2007) implemented ideaSHAMASH system. SHAMASH knowledge-based BPM tool targeted helpingengineering optimization process models (Aler, Borrajo, Camacho, & SierraAlonso, 2002). tool includes, amongst things, user-friendly interfaces allowing621fiHoffmann, Weber & Kraftusers conveniently annotate processes rich context information, particularform rules roughly correspond planning actions. rules (andinformation) form basis translation PDDL, planning creationnew process models.largest body related work performed last decade differentname, semantic web service composition (SWSC), context Semantic WebCommunity (e.g., Ponnekanti & Fox, 2002; Narayanan & McIlraith, 2002; Srivastava, 2002;Constantinescu, Faltings, & Binder, 2004; Agarwal et al., 2005; Sirin, Parsia, Wu, Hendler,& Nau, 2004; Sirin et al., 2006; Meyer & Weske, 2006; Liu, Ranganathan, & Riabov, 2007).nutshell, idea SWSC (1) annotate web services declarativeabstract explanation functionality, (2) exploit semantic annotationsautomatically combine web services achieving complex functionality.SWSC terminology differs use paper, idea basically(although SWSC works address BPM specifically).key distinguishing feature present work approach obtainingplanning input (the semantic annotations). first attempt addressplanning/SWSC problem based SAM, generally based pre-existingmodel all. Since modeling costly (Kambhampati, 2007; Rodriguez-Moreno et al.,2007), shift focus gets us around one major open problems area.modeling interfaces SHAMASH (Rodriguez-Moreno et al., 2007), related worksattempting support model creation (Gonzalez-Ferrer, Fernandez-Olivares, & Castillo,2009; Cresswell, McCluskey, & West, 2009, 2010), address problem,different ways less radical extent. Whereas works attempt easemodeling overhead, re-use SAM actually removes overhead completely.said that, course relations SAM planning previous work,technical level. particular, planning SWSC literature contains multitudeworks dealing actions that, like SAMs disjunctive effect actions, onepossible outcome. semantics actions, detailed already Section 4.2,straightforward mixture two wide-spread notions planning: observation actions,one list possible observations distinguished, non-deterministic actions,one list possible effects occurs (e.g., Weld et al., 1998; Smith & Weld,1999; Bonet & Geffner, 2000; Cimatti et al., 2003; Bryce & Kambhampari, 2004; Hoffmann& Brafman, 2005; Bonet & Givan, 2006; Bryce, Kambhampati, & Smith, 2006; Bryce &Buffet, 2008; Palacios & Geffner, 2009).prominent line research web service composition, known Roman model(e.g., Berardi, Calvanese, De Giacomo, Lenzerini, & Mecella, 2003, 2005; De Giacomo &Sardina, 2007; Sardina, Patrizi, & De Giacomo, 2008; Calvanese, De Giacomo, Lenzerini,Mecella, & Patrizi, 2008), also deals notion non-determinism componentweb services, however framework different ours. web servicesRoman model stateful. is, service set possible own/internal states,service provides set operations outside world, responsibletransitions services internal state. composition task create scheduler (afunction choosing one service operation demanded) interacting componentservices way implement desired goal transition system. Similarly,web service composition techniques developed Marco Pistore co-workers (e.g.,622fiSAP Speaks PDDLPistore, Marconi, Bertoli, & Traverso, 2005; Bertoli, Pistore, & Traverso, 2006, 2010) dealform non-determinism stateful component services formalized transitionsystems. work, composition task create controller transition systemoverall (controlled) behavior satisfies planning-like goal (expressed EAGLElanguage, cf. below). latter aspect attempting satisfy planning goalframework slightly closer Roman model.main distinguishing feature formalism notion weak SAM plans,allowing failed action outcomes proved unsolvable, leastone outcome action successful. works also proposed notionsplans guarantee achieve goal cases, notionscomplex goals used achieve similar effects. briefly discuss notionsclosest approach.notions weak strong plans, discussed Section 4.2, first introducedCimatti, Giunchiglia, Giunchiglia, Traverso (1997) Cimatti, Roveri, Traverso(1998b), respectively. Strong cyclic plans first introduced Cimatti, Roveri,Traverso (1998a). notion orthogonal weak SAM plans neither impliesother. example, bad action outcome invalidates, side effect, preconditionsactions task, action may form part weak SAM plan, neverstrong cyclic plan. Vice versa, strong cyclic plans general structure,particular allowing non-deterministic actions appear execution.Pistore Traverso (2001) generalize weak, strong, strong cyclic plans handlinggoals taking form CTL formulas. However, pointed Dal Lago et al. (2002),goals unable express plan try achieve goal, givepossible. Dal Lago et al. design goal language EAGLE addresses(amongst others) shortcoming. EAGLE features variety goal operatorsflexibly combined form goal expressions. One expression TryReach G1 FailG2 , G1 G2 alternative goals. intuition plan tryachieve G1 , resort G2 reaching G1 become impossible. precisely, planEAGLE goal optimal if, every state traverses: (1) strong planG1 G2 ; (2) exists strong plan G1 s, strong plan;(3) exists weak plan G1 s, weak plan. Applyingcontext, say restrict plans execute non-deterministic action once.easy see that, within space plans, every weak SAM plan optimalEAGLE goal TryReach G Fail TRUE: weak SAM plans mark failed reachingG impossible. However, every action tree optimal TryReach GFail TRUE weak SAM plan. (2) (3) force every actionleast one solved outcome. tasks weak SAM plan exists,action tree (e.g., empty tree) optimal TryReach G Fail TRUE. tasksweak SAM plan, TryReach G Fail TRUE forces solvable action outcome providesolution, imposes constraints failed outcomes (which may thus continuedarbitrarily complex sub-plans).Shaparau et al. (2006) define framework similar effect context.consider contingent planning presence linear preference order alternativegoals. Action trees plans achieve least one goal every leaf, i.e.,strong plans disjunction goals. Plan least good plan 0 if, every623fiHoffmann, Weber & Kraftstate common both, best possible outcome achievable using least goodachievable using 0 . optimal least good plans. Giventhis, like EAGLE goal TryReach G Fail TRUE discussed above, every weak SAMplan optimal goal preference G, TRUE, inverse true because,unsolvable tasks unsolvable outcomes solvable tasks, G, TRUE permitsplan anything.Mediratta Srivastava (2006) define framework also based contingent planning,user provides additional input number K. Then, (1) plan treeK leaves achieving goal; (2) exist, plantree whose number leaves maximal. Due (1), failed nodes necessarilyunsolvable (the plan may simply stop reached K). Due (2), even taskgoal cannot reached all, i.e., matter action outcomes cannotachieve goal, plan. addition, application context sensibleway, human modeler, choose meaningful value K.Summing up, related notions weak plans exist, none captures exactlywant SAM. algorithmic side, works listed use symbolic search (basedBDDs), thus quite different explicit-state SAM-AO* search. singleexception planner described Mediratta Srivastava (2006), basedvariant A*, different plan semantics described.Research performed also alternative methods, based planning,automatically generating processes. example, Kuster, Ryndina, Gall (2007) describemethod computing synchronized product life-cycles set business objects,generating process corresponding product. process serves basiscustomer-specific modifications. Clearly, motivation relates ours; intendedmeaning output (the generated process), input assumed generation,quite different. input, Kuster et al.s life-cycles state machines describingpossible behaviors object, formulation corresponds spacereachable states. space generated based SAM, huge evensingle BOs, mention product (cf. results blind search scalingacross BOs, Sections 6.4 6.5). Heuristic search gets us around need enumeratestates. output, Kuster et al.s generated processes guaranteecomply BO behaviors (which well), also cover them, essentiallyrepresenting could done. different plans generate,whose intention show specifically move particular start end states.Altogether, methods complementary. Planning computational advantagesinvolved objects many possible states, often case SAM.9. Conclusionpointed SAP built large-scale model software behavior, SAM,whose abstraction level formalization intimately related planning models languages PDDL. shown base promising BPM application planningfact. Getting planner input free, avoid one important obstaclesmaking kind planning application successful practice. solution specificparticular context treatment non-deterministic actions failed outcomes,624fiSAP Speaks PDDLphenomena quite common planning web service composition,novel approach dealing might turn relevant generally.main open issue obtain concrete data evaluating business valueapplication. points are:modification FF successfully handles many SAM instances, finding plans withinruntimes small enough apply realistic online-setting cut-offs. 15%instances encountered still present challenges. instances could serveinteresting benchmark approaches dealing failed outcomes ways related(cf. Section 8).current SAM model reflect dependencies across BOs. dependenciesdo, however, exist various forms. example, BOs form part datacontained another kind BO, actions one kind BO create newinstance another kind BO, actions must taken several BOstogether. ongoing activity SAP Research, aiming enriching SAMreflect interactions, purpose informed model checking.interactions easily modeled terms well-known planning constructs(object creation, preconditions/effects spanning variables several BOs),expect extended model enable us generate accurate plans.results Section 6.5 indicate, additional planning techniques may requiredimprove performance case number interacting BOs becomes large.smaller numbers (up around dozen BOs) performance current toolstill reasonable.SAM currently provides basis automatically creating number additionalprocess aspects. important aspect exception handling, momenthighlight places (failed nodes) needs inserted. Anotherissue data-flow. mostly easy since application data already prepackaged relevant BOs, cases, like security tokens,covered this. open line research determine aspects couldmodeled, way exploited corresponding planning algorithms.may also interesting look methods presenting user setalternative processes. discerning relevant alternatives, methodsrequire extensions SAM, like action duration, action cost, plan preferences.general perspective, key contribution work demonstratingpotential synergy model-based software engineering planning-based processgeneration. Re-using (or even all) required models, human labor requiredrealize planning dramatically reduced. SAMs methodology business-level descriptions individual activities within software architecture specific SAP.Thus, exploiting synergy novel approach may turn fruitful far beyondparticular application described herein.Acknowledgmentsthank anonymous JAIR reviewers, whose comments helped lot improvingpaper.625fiHoffmann, Weber & Kraftwork performed authors employed SAP. Partwork performed Jorg Hoffmann employed INRIA (Nancy, France),Ingo Weber employed University New South Wales (Sydney, Australia).NICTA funded Australian Government represented DepartmentBroadband, Communications Digital Economy Australian Research CouncilICT Centre Excellence program.ReferencesAalst, W. (1997). Verification Workflow Nets. Application Theory Petri Nets1997.Agarwal, V., Chafle, G., Dasgupta, K., Karnik, N., Kumar, A., Mittal, S., & Srivastava, B.(2005). Synthy: system end end composition web services. Journal WebSemantics, 3 (4).Aler, R., Borrajo, D., Camacho, D., & Sierra-Alonso, A. (2002). knowledge-based approach business process reengineering: SHAMASH. Knowledge Based Systems,15 (8), 473483.Bacchus, F. (2000). Subset PDDL AIPS2000 Planning Competition. AIPS-00Planning Competition Comitee.Backstrom, C., & Nebel, B. (1995). Complexity results SAS+ planning. ComputationalIntelligence, 11 (4), 625655.Bell, M. (2008). Service-Oriented Modeling: Service Analysis, Design, Architecture.Wiley & Sons.Berardi, D., Calvanese, D., De Giacomo, G., Lenzerini, M., & Mecella, M. (2003). Automatic composition e-services export behavior. Orlowska, M. E.,Weerawarana, S., Papazoglou, M. P., & Yang, J. (Eds.), Proceedings 1st International Conference Service-Oriented Computing (ICSOC03), Vol. 2910 LectureNotes Computer Science, pp. 4358. Springer.Berardi, D., Calvanese, D., De Giacomo, G., Lenzerini, M., & Mecella, M. (2005). Automatic service composition based behavioral descriptions. International JournalCooperative Information Systems, 14 (4), 333376.Bertoli, P., Pistore, M., & Traverso, P. (2006). Automated web service compositionon-the-fly belief space search. Long, D., & Smith, S. (Eds.), Proceedings16th International Conference Automated Planning Scheduling (ICAPS-06),Ambleside, UK. AAAI.Bertoli, P., Pistore, M., & Traverso, P. (2010). Automated composition web services viaplanning asynchronous domains. Artificial Intelligence, 174 (3-4), 316361.Biundo, S., Aylett, R., Beetz, M., Borrajo, D., Cesta, A., Grant, T., McCluskey, L., Milani,A., & Verfaillie, G. (2003). PLANET Technological Roadmap AI PlanningScheduling. http://planet.dfki.de/service/Resources/Roadmap/Roadmap2.pdf.Bonet, B., & Geffner, H. (2000). Planning incomplete information heuristic searchbelief space. Chien, S., Kambhampati, R., & Knoblock, C. (Eds.), Proceedings626fiSAP Speaks PDDL5th International Conference Artificial Intelligence Planning Systems (AIPS-00),pp. 5261, Breckenridge, CO. AAAI Press, Menlo Park.Bonet, B., & Geffner, H. (2001). Planning heuristic search. Artificial Intelligence, 129 (12), 533.Bonet, B., & Givan, B. (2006). 5th international planning competition: Non-deterministictrack call participation. Proceedings 5th International Planning Competition (IPC06).Born, M., Hoffmann, J., Kaczmarek, T., Kowalkiewicz, M., Markovic, I., Scicluna, J., Weber,I., & Zhou, X. (2008). Semantic annotation composition business processesMaestro. Demonstrations ESWC08: 5th European Semantic Web Conference,pp. 772776, Tenerife, Spain.Born, M., Hoffmann, J., Kaczmarek, T., Kowalkiewicz, M., Markovic, I., Scicluna, J., Weber, I., & Zhou, X. (2009). Supporting execution-level business process modelingsemantic technologies. Demonstrations DASFAA09: Database SystemsAdvanced Applications, pp. 759763, Brisbane, Australia.Bryce, D., & Buffet, O. (2008). 6th international planning competition: Uncertainty part.Proceedings 6th International Planning Competition (IPC08).Bryce, D., & Kambhampari, S. (2004). Heuristic guidance measures conformant planning. Koenig, S., Zilberstein, S., & Koehler, J. (Eds.), Proceedings 14thInternational Conference Automated Planning Scheduling (ICAPS-04), pp.365374, Whistler, Canada. AAAI.Bryce, D., Kambhampati, S., & Smith, D. E. (2006). Planning graph heuristics beliefspace search. Journal Artificial Intelligence Research, 26, 3599.Bylander, T. (1994). computational complexity propositional STRIPS planning.Artificial Intelligence, 69 (12), 165204.Calvanese, D., De Giacomo, G., Lenzerini, M., Mecella, M., & Patrizi, F. (2008). Automaticservice composition synthesis: roman model. IEEE Data Engineering Bulletin,31 (3), 1822.Cimatti, A., Giunchiglia, F., Giunchiglia, E., & Traverso, P. (1997). Planning via modelchecking: decision procedure ar. Steel, S., & Alami, R. (Eds.), Recent AdvancesAI Planning. 4th European Conference Planning (ECP97), Vol. 1348 LectureNotes Artificial Intelligence, pp. 130142, Toulouse, France. Springer-Verlag.Cimatti, A., Pistore, M., Roveri, M., & Traverso, P. (2003). Weak, strong, strong cyclicplanning via symbolic model checking. Artificial Intelligence, 147 (1-2), 3584.Cimatti, A., Roveri, M., & Traverso, P. (1998a). Automatic obdd-based generationuniversal plans non-deterministic domains. Mostow, J., & Rich, C. (Eds.),Proceedings 15th National Conference American Association ArtificialIntelligence (AAAI-98), pp. 875881, Madison, WI, USA. MIT Press.Cimatti, A., Roveri, M., & Traverso, P. (1998b). Strong planning non-deterministic domains via model checking. Simmons, R., Veloso, M., & Smith, S. (Eds.), Proceedings 4th International Conference Artificial Intelligence Planning Systems(AIPS-98), pp. 3643, Pittsburgh, PA. AAAI Press, Menlo Park.627fiHoffmann, Weber & KraftCohn, D., & Hull, R. (2009). Business artifacts: data-centric approach modelingbusiness operations processes. IEEE Data Engineering Bulletin, 39.Constantinescu, I., Faltings, B., & Binder, W. (2004). Large scale, type-compatible servicecomposition. Jain, H., & Liu, L. (Eds.), Proceedings 2nd InternationalConference Web Services (ICWS-04), pp. 506513, San Diego, California, USA.IEEE Computer Society.Cresswell, S., McCluskey, T., & West, M. (2010). Acquiring planning domains models usingLOCM. Knowledge Engineering Review.Cresswell, S., McCluskey, T. L., & West, M. M. (2009). Acquisition object-centred domainmodels planning examples. Gerevini, A., Howe, A. E., Cesta, A., & Refanidis,I. (Eds.), Proceedings 19th International Conference Automated PlanningScheduling (ICAPS-09), Sydney, Australia. AAAI.Dal Lago, U., Pistore, M., & Traverso, P. (2002). Planning language extendedgoals. Dechter, R., Kearns, M., & Sutton, R. (Eds.), Proceedings 18th NationalConference American Association Artificial Intelligence (AAAI-02), pp.447454, Edmonton, AL, USA. MIT Press.De Giacomo, G., & Sardina, S. (2007). Automatic synthesis new behaviors libraryavailable behaviors. Veloso, M. (Ed.), Proceedings 20th International JointConference Artificial Intelligence (IJCAI-07), pp. 18661871, Hyderabad, India.Morgan Kaufmann.Dumas, M., ter Hofstede, A., & van der Aalst, W. (Eds.). (2005). Process Aware Information Systems: Bridging People Software Process Technology. WileyPublishing.Fox, M., & Long, D. (2003). PDDL2.1: extension PDDL expressing temporalplanning domains. Journal Artificial Intelligence Research, 20, 61124.Gazen, B. C., & Knoblock, C. (1997). Combining expressiveness UCPOPefficiency Graphplan. Steel, S., & Alami, R. (Eds.), Recent AdvancesAI Planning. 4th European Conference Planning (ECP97), Vol. 1348 LectureNotes Artificial Intelligence, pp. 221233, Toulouse, France. Springer-Verlag.Gerevini, A., Haslum, P., Long, D., Saetti, A., & Dimopoulos, Y. (2009). Deterministicplanning fifth international planning competition: PDDL3 experimentalevaluation planners. Artificial Intelligence, 173 (5-6), 619668.Gonzalez-Ferrer, A., Fernandez-Olivares, J., & Castillo, L. (2009). JABBAH: Java application framework translation business process models HTN.Proceedings 3rd International Competition Knowledge EngineeringPlanning Scheduling, Thessaloniki, Greece.Helmert, M. (2006). Fast Downward planning system. Journal Artificial IntelligenceResearch, 26, 191246.Helmert, M. (2009). Concise finite-domain representations pddl planning tasks. ArtificialIntelligence, 173 (5-6), 503535.628fiSAP Speaks PDDLHoffmann, J., & Brafman, R. (2005). Contingent planning via heuristic forward searchimplicit belief states. Biundo, S., Myers, K., & Rajan, K. (Eds.), Proceedings15th International Conference Automated Planning Scheduling (ICAPS-05),pp. 7180, Monterey, CA, USA. AAAI.Hoffmann, J., & Edelkamp, S. (2005). deterministic part IPC-4: overview. JournalArtificial Intelligence Research, 24, 519579.Hoffmann, J., & Nebel, B. (2001). FF planning system: Fast plan generationheuristic search. Journal Artificial Intelligence Research, 14, 253302.Jonathan, P. J., Moore, J., Stader, J., Macintosh, A., & Chung, P. (1999). Exploiting aitechnologies realise adaptive workflow systems. Proceedings ot AAAI99Workshop Agent-Based Systems Business Context.Kambhampati, S. (2007). Model-lite planning web age masses: challengesplanning incomplete evolving domain models. Howe, A., & Holte, R. C.(Eds.), Proceedings 22nd National Conference American AssociationArtificial Intelligence (AAAI-07), Vancouver, BC, Canada. MIT Press.Kitchin, D. E., McCluskey, T. L., & West, M. M. (2005). B vs ocl: Comparing specification languages planning domains. Proceedings ICAPS05 WorkshopVerification Validation Model-Based Planning Scheduling Systems.Krafzig, D., Banke, K., & Slama, D. (2005). Enterprise SOA: Service-Oriented ArchitectureBest Practices. Prentice Hall.Kuster, J. M., Ryndina, K., & Gall, H. (2007). Generation business process modelsobject life cycle compliance. Alonso, G., Dadam, P., & Rosemann, M. (Eds.),Proceedings 5th International Conference Business Process Management(BPM07), Vol. 4714 Lecture Notes Computer Science, pp. 165181. Springer.Liu, Z., Ranganathan, A., & Riabov, A. (2007). planning approach message-orientedsemantic web service composition. Howe, A., & Holte, R. C. (Eds.), Proceedings22nd National Conference American Association Artificial Intelligence(AAAI-07), Vancouver, BC, Canada. MIT Press.May, N., & Weber, I. (2008). Information gathering semantic service discoverycomposition business process modeling. CIAO!08: Workshop Cooperation &Interoperability - Architecture & Ontology CAiSE08, Vol. LNBIP 10, pp. 4660,Montpellier, France.McDermott, D., Ghallab, M., Howe, A., Knoblock, C., Ram, A., Veloso, M., Weld, D., &Wilkins, D. (1998). PDDL planning domain definition language. Tech. rep.CVC TR-98-003, Yale Center Computational Vision Control.McDermott, D. V. (1999). Using regression-match graphs control search planning.Artificial Intelligence, 109 (1-2), 111159.Mediratta, A., & Srivastava, B. (2006). Applying planning composition web servicesuser-driven contingent planner. IBM Research Report RI 06002.Meyer, H., & Weske, M. (2006). Automated service composition using heuristic search.Dustdar, S., Fiadeiro, J. L., & Sheth, A. P. (Eds.), Proceedings 4th International629fiHoffmann, Weber & KraftConference Business Process Management (BPM06), Vol. 4102 Lecture NotesComputer Science, pp. 8196. Springer.Narayanan, S., & McIlraith, S. (2002). Simulation, verification automated compositionweb services. Iyengar, A., & Roure, D. D. (Eds.), Proceedings 11th International World Wide Web Conference (WWW-02), Honolulu, Hawaii, USA. ACM.Nilsson, N. J. (1969). Searching problem-solving game-playing trees minimal costsolutions. Information Processing 68 Vol. 2, pp. 15561562, Amsterdam, Netherlands.Nilsson, N. J. (1971). Problem Solving Methods Artificial Intelligence. McGraw-Hill.Object Management Group (2006). Object Constraint Language Specification, Version 2.http://www.omg.org/technology/documents/formal/ocl.htm.Object Management Group (2008).http://www.bpmn.org/.Business Process Modeling Notation, V1.1.Palacios, H., & Geffner, H. (2009). Compiling uncertainty away conformant planningproblems bounded width. Journal Artificial Intelligence Research, 35, 623675.Pearl, J. (1984). Heuristics. Morgan Kaufmann.Pednault, E. P. (1989). ADL: Exploring middle ground STRIPS situation calculus. Brachman, R., Levesque, H. J., & Reiter, R. (Eds.), PrinciplesKnowledge Representation Reasoning: Proceedings 1st International Conference (KR-89), pp. 324331, Toronto, ON. Morgan Kaufmann.Pesic, M., Schonenberg, M. H., Sidorova, N., & van der Aalst, W. M. P. (2007). Constraintbased workflow models: Change made easy. Meersman, R., & Tari, Z. (Eds.), OTMConferences (1), Vol. 4803 Lecture Notes Computer Science, pp. 7794. Springer.Pistore, M., Marconi, A., Bertoli, P., & Traverso, P. (2005). Automated composition webservices planning knowledge level. Kaelbling, L. (Ed.), Proceedings19th International Joint Conference Artificial Intelligence (IJCAI-05), Edinburgh,Scotland. Morgan Kaufmann.Pistore, M., & Traverso, P. (2001). Planning model checking extended goals nondeterministic domains. Nebel, B. (Ed.), Proceedings 17th International JointConference Artificial Intelligence (IJCAI-01), pp. 479486, Seattle, Washington,USA. Morgan Kaufmann.Ponnekanti, S., & Fox, A. (2002). SWORD: developer toolkit web services composition.Iyengar, A., & Roure, D. D. (Eds.), Proceedings 11th International WorldWide Web Conference (WWW-02), Honolulu, Hawaii, USA. ACM.Richter, S., & Helmert, M. (2009). Preferred operators deferred evaluation satisficingplanning. Gerevini, A., Howe, A. E., Cesta, A., & Refanidis, I. (Eds.), Proceedings19th International Conference Automated Planning Scheduling (ICAPS09), Sydney, Australia. AAAI.Rodriguez-Moreno, M. D., Borrajo, D., Cesta, A., & Oddi, A. (2007). Integrating planningscheduling workflow domains. Expert Systems Applications, 33 (2), 389406.630fiSAP Speaks PDDLSAP (2010). SAP NetWeaver.. http://www.sap.com/platform/netweaver/index.epx.Sardina, S., Patrizi, F., & De Giacomo, G. (2008). Behavior composition presencefailure. Brewka, G., & Lang, J. (Eds.), Proceedings 11th InternationalConference Principles Knowledge Representation Reasoning (KR08), pp.640650. AAAI Press.Schneider, S. (2001). B-Method: Introduction. Palgrave.Shaparau, D., Pistore, M., & Traverso, P. (2006). Contingent planning goal preferences.Gil, Y., & Mooney, R. J. (Eds.), Proceedings 21st National ConferenceAmerican Association Artificial Intelligence (AAAI-06), Boston, Massachusetts,USA. MIT Press.Sirin, E., Parsia, B., Wu, D., Hendler, J., & Nau, D. (2004). HTN planning web servicecomposition using SHOP2. Journal Web Semantics, 1 (4).Sirin, E., Parsia, B., & Hendler, J. (2006). Template-based composition semantic webservices. AAAI Fall Symposium Agents Search.Smith, D. E., & Weld, D. S. (1999). Temporal planning mutual exclusion reasoning.Dean, T. (Ed.), Proceedings 16th International Joint Conference ArtificialIntelligence (IJCAI-99), pp. 326337, Stockholm, Sweden. Morgan Kaufmann.Srivastava, B. (2002). Automatic web services composition using planning. KnowledgeBased Computer Systems (KBCS-02), pp. 467477.Traverso, P., Ghallab, M., & Nau, D. (Eds.). (2005). Automated Planning: TheoryPractice. Morgan Kaufmann.Turner, J., & McCluskey, T. L. (1994). Construction Formal Specifications:Introduction Model-Based Algebraic Approaches. McGraw Hill SoftwareEngineering series.van der Aalst, W. (2003). Business process management demystified: tutorial models,systems standards workflow management. Lectures ConcurrencyPetri Nets ACPN04: Advanced Courses Petri Nets, pp. 165.van der Aalst, W. M. P., & Pesic, M. (2006). Decserflow: Towards truly declarative serviceflow language. Bravetti, M., Nunez, M., & Zavattaro, G. (Eds.), WS-FM, Vol. 4184Lecture Notes Computer Science, pp. 123. Springer.Wainer, J., & de Lima Bezerra, F. (2003). Groupware: Design, Implementation, Use,Vol. 2806 LNCS, chap. Constraint-based flexible workflows, pp. 151158. SpringerVerlag.Weld, D. S., Anderson, C. R., & Smith, D. E. (1998). Extending graphplan handleuncertainty & sensing actions. Mostow, J., & Rich, C. (Eds.), Proceedings15th National Conference American Association Artificial Intelligence(AAAI-98), pp. 897904, Madison, WI, USA. MIT Press.Weske, M. (2007). Business Process Management: Concepts, Languages, Architectures.Springer-Verlag.631fiHoffmann, Weber & KraftYoon, S. W., Fern, A., & Givan, R. (2007). FF-Replan: baseline probabilistic planning.Boddy, M., Fox, M., & Thiebaux, S. (Eds.), Proceedings 17th InternationalConference Automated Planning Scheduling (ICAPS-07), Providence, RhodeIsland, USA. AAAI.Younes, H., Littman, M., Weissman, D., & Asmuth, J. (2005). first probabilistic trackinternational planning competition. Journal Artificial Intelligence Research,24, 851887.632fiJournal Artificial Intelligence Research 44 (2012) 223-273Submitted 11/11; published 05/12Modeling Social Causality Responsibility JudgmentMulti-Agent InteractionsWenji MaoWENJI.MAO@IA.AC.CNState Key laboratory Management Control Complex SystemsInstitute Automation, Chinese Academy SciencesNo.95 Zhongguancun East Road, Beijing 100190, ChinaJonathan GratchGRATCH@ICT.USC.EDUInstitute Creative Technologies, University Southern California12015 Waterfront Drive, Playa Vista, CA 90094, U.S.A.AbstractSocial causality inference entity makes social behavior entities self.Besides physical cause effect, social causality involves reasoning epistemic statesagents coercive circumstances. Based inference, responsibility judgmentprocess whereby one singles individuals assign responsibility, credit blame multiagent activities. Social causality responsibility judgment key aspect socialintelligence, model facilitates design development variety multiagent interactive systems. Based psychological attribution theory, paper presentsdomain-independent computational model automate social inference judgment processaccording agents causal knowledge observations interaction. conductexperimental studies empirically validate computational model. experimental resultsshow model predicts human judgments social attributions makes inferencesconsistent people judgments. Therefore, proposed modelgenerically incorporated intelligent system augment social cognitivefunctionality.1. IntroductionRecent years seen explosion research intersection computing humansocial behavior. Topics human-centered (Jaimes, Sebe, & Gatica-Perez, 2006), social(Wang, Zeng, Carley, & Mao, 2007) affective computing (Picard, 1997, 2010) emphasizerole computers partners facilitators human social activity, highlightchallenge computationally understanding participating human social interactions.Traditional artificial intelligence, emphasis individual problem solvingreasoning rational behavior, obviously suitable social, emotional, humanlike characteristics social interaction. paper, demonstrate AI reasoningmethods applied understanding, modeling predicting human social judgments,applications human-centric social interaction.specific challenge focus paper reasoning social causality. Socialcausality refers inference entity makes social behavior entitiesself. inference differs dramatically traditional artificial intelligence methods(e.g., planning) reason physical reality. Besides physical cause effect, social2012 AI Access Foundation. rights reserved.fiMAO & GRATCHcausality includes reasoning mental states (e.g., actor intend causeoutcome? could foresee outcome?) social power (e.g., actorfreedom act coerced circumstances individuals?). Responsibilityjudgment process whereby one forms judgment results responsibility, creditblame based inference social causality. Social causality responsibility judgmentunderlie act make sense social world around us: lead emotionalexpressions praise rage; justify public applause prison terms. short, lieheart social intelligence.advance multi-agent interactive systems, adaptive user interfacesapplications socially interact people, increasingly important model reasonhuman-centric form social intelligence. Social causal reasoning facilitates multiagent planning augmenting classical planners ability reason entitiespower effect changes. facilitates adaptive learning appraising praiseworthyblameworthy behavior, reinforcing praiseworthy. modeling communicativesocial behavior human-like agents, responsibility judgment helps inform models socialemotions characterizing situations evoke anger, guilt praise (Gratch, Mao, &Marsella, 2006). people usually adept taking credit deflecting blame socialdialogue (e.g., negotiation), information helps guide natural language conversation strategies(Martinovski, Mao, Gratch, & Marsella, 2005).Social causal inference helps reason social cognitive states entity,responsibility judgment helps form assessment observed social behavior entity(either human user, computer program agent). thus facilitate various formsinteractions including human-computer, human-agent agent-agent interactions. alsofacilitate human-human interaction identifying underlying cognitive process principleshuman judgments. multi-agent environment, social causality responsibility judgmenthelp share responsibility multi-agent organization (Jennings, 1992), evaluate social powerdependence (Castelfranchi, 1990; Sichman, Conte, Demazeau, & Castelfranchi, 1994), automateafter-action review group training (Gratch & Mao, 2003; Johnson & Gonzalez, 2008),support social simulation agent society.primary goal develop faithful computational framework human-likeintelligent agents drive realistic behavior modeling generation (Swartout et al.,2006). Psychological philosophical studies agree broad features people useeveryday behavioral judgment. work particularly influenced attribution theory,body research social psychology exploring folk explanation behavior. Basedpsychological attribution theory, developed general computational frameworkinferring social causality forming responsibility judgment according agents causalknowledge observations communication task execution, empirically validatedapproach using human data.rest paper organized follows. Section 2, review previous computationalwork social causality, responsibility blame/credit. Section 3, introduce twoinfluential attributional models behavioral judgment, Weiners (1995) model responsibilityjudgment Shavers (1985) model blame attribution. Based attributional models,Section 4 presents computational framework social causality responsibility judgment.224fiMODELING SOCIAL CAUSALITY RESPONSIBILITY JUDGMENT MULTI-AGENT INTERACTIONSprovide computational representation, inferences algorithm proposed model,illustrate approach using example system development. Section 5,report empirical studies model validation. Section 6 discusses researchissues. paper concludes Section 7.2. Related WorkSince rise cognitive science (Newell & Simon, 1972), computational methodsmetaphors applied modeling understanding human behavior. Several linesresearch addressed aspects social cognition, including natural language dialogue (Cassell,Sullivan, Prevost, & Churchill, 2000; Ferguson & Allen, 2007), collaborative problem solving(Rich, Sidner, & Lesh, 2001; Schurr, Marecki, Tambe, & Scerri, 2005), modeling emotions(Marinier & Laird, 2004; Gratch, Marsella, & Petta, 2009), simulating human negotiationprocesses (Kraus, Hoz-Weiss, Wilkenfeld, 2008; Martinovski & Mao, 2009), understandinghuman social networks (Golbeck & Hendler, 2006; Wang et al., 2010). modeling humansocial behavior, useful distinguish normative, descriptive legal perspectives.Normative models attempt prescribe people assign responsibility blame/credit.Descriptive models characterize people practice, may differ considerablynormative prescriptions. Legal models refer formalized processes society usesresponsibility assignment, seen amalgam normative practicalconsiderations. presenting descriptive model social causality responsibilityjudgment, motivate work examining perspectives.2.1 Normative ModelsNormative (or prescriptive) models typically put forward set rational principlesuniversally guide decision-making. example, Bayesian decision theory proposedoptimal method deciding alternative courses actions. Game theory proposedideal method arriving certain social decisions, whether cooperateanother, possibly deceptive, party. game theoretic approaches model group decisionmaking rational way, social causality responsibility judgment model reasoningassessment social causes consequences resulting decision making.judgment causality, responsibility blame/credit, research normative models largelyresides moral philosophy aim identify rational principles governassignment social credit blame. example, Kant (1998) argued that, unlike oftenobserved practice, would rational assign standards responsibility regardlessvalence (i.e., praiseworthy blameworthy) severity social act. Within computerscience artificial intelligence, unaware complete models basednormative principles, exception computational model proposed ChocklerHalpern (2004).2.2 Legal ModelsLegal models attempt formalize responsibility judgment inferences realized within judicialsystems, typically aim automating verifying human legal judgments.fertile research field intersection artificial intelligence law. field225fiMAO & GRATCHcontinuously progressing since development early legal systems TAXMAN& TAXMAN- (McCarty & Sridharan, 1981; McCarty, 1995), HYPO (Rissland & Ashley,1987), CABARET (Rissland & Skalak, 1991) CATO (Aleven & Ashley, 1995).similarities judgments normative legal responsibility, researcherssuggested using legal model direct analogue normative model responsibility judgment(e.g., Fincham & Jaspars, 1980). However, fundamental differences twokinds responsibility judgment. Legal judgment largely depends specific circumstances.legal reasoning systems case-based, whereas evaluating moralresponsibility identifies general theories fall within broad studies cognitivefunctionalism1 (e.g., clarifying roles cause, belief intention explaining behavior).addition case-based legal reasoning systems, researchers proposed logic-basedapproaches focus general reasoning mechanism, typically defeasible inference using nonmonotonic reasoning defeasible argumentation (e.g., Hage, 1997; Prakken, 1997). mainefforts logic-based legal systems representation complex legal rules (e.g.,contradictory, nonmonotonic priority rules), inference rules exceptions,handling conflict rules (Prakken & Sartor, 2002). McCarty (1997) argued whether real cases,judge would apply formal theory evaluate complex rules, thereby arrive correct results.called intuitive version legal rules, would simple clear.Furthermore, argue laymans judgment behavior everyday situations quitemade court. occur richer forms social interaction,follows different set rules.2.3 Descriptive ModelsDescriptive models attempt characterize people form social judgments practice,differ presumed normative principles legal judgments. example,contrast Kants prescription adopt uniform principles, people use different criteriaassigning blame versus credit often form different judgments depending severityoutcome. Descriptive models also differ criteria validation. Whereas normativemodels judged consistency universal principles fairness legalmodels judged consistency past legal decisions, descriptive models assessedagreement judgments people form day-to-day lives. sense,descriptive models relevant field human-centered social computing,goal adapt computation human norms practice, rather forcing humans adaptprescriptive norms behavior. Research descriptive models largely resides socialpsychology (Heider, 1958; Shaver, 1985; Weiner 1995, 2001, 2006) little workwithin artificial intelligence attributing responsibility blame/credit human-likefashion.2.4 Computational ApproachesAI causality research, computational approaches developed address problemextending causal models (Halpern & Pearl, 2001; Chockler & Halpern, 2004). HalpernPearl (2001) presented definition actual cause within framework structural causal1doctrine views theories behavior complex mental states, introduced individualized functionsroles play producing behavior explained.226fiMODELING SOCIAL CAUSALITY RESPONSIBILITY JUDGMENT MULTI-AGENT INTERACTIONSmodels. approach extract complex causal relationships simple ones,model capable inferring indirect causal factors including social cause. causal model (orstructural model) system equations set random variables. two finitesets variables: exogenous (U) endogenous (V). values exogenous variablesdetermined factors outside model, thus corresponding equations.endogenous variable exactly one causal equation (or structural equation) determinesvalue. causal model expressed causal diagram, nodes correspondingvariables, edges parents endogenous variable (indicated causalequations) endogenous variable. Take two-man firing squad example (Pearl, 1999):two-man firing squad; captains order, riflemen shoot simultaneouslyaccurately, prisoner dies.Figure 1 illustrates causal model firing squad example, U={Uc} V={C,R1, R2, D}. vector values exogenous variables U (called context) causalmodel represents specific situation (i.e., causal world). instance, assume Uc=1 (i.e.,captains order true) causal model below, resulting causal world describestwo-man firing squad story above. Causal inference based counterfactual dependencecontingency. Roughly speaking, B counterfactually dependent if,happened B would happened. example, firing squad scenario,given context captain orders, contingency rifleman-2 shoot,prisoners death counterfactually dependent rifleman-1s shooting. rifleman-1s shooting(R1=1) actual cause death. Similarly, rifleman-2s shooting (R2=1) actual causedeath. Besides two riflemen physically cause death, Halpern & Pearls modelfind captains order (C=1) actual cause death well.Context (Uc)Causal equations:Commander orders (C)Uc = CC = R1Rifleman-1shoots (R1)Rifleman-2shoots (R2)C = R2R1 R2 =Prisoners death (D)Figure 1: Causal Model Firing-Squad ExampleChockler Halpern (2004) extended notion causality, account degreeresponsibility. provide definition degree responsibility based considerationcontingencies. Given causal model M, variable XV context , degreeresponsibility formula X=x outcome measured minimal number changesk made order make counterfactually depend X=x. X=xactual cause , degree responsibility X=x 0; Otherwise degreeresponsibility X=x 1/(k+1). counterfactually depends X=x, degreeresponsibility X=x 1. example, person wins election 11-0, voter227fiMAO & GRATCHvotes cause victory, degree responsibility votervictory 1/6. However, 6-5 victory, degree responsibility voter 1.Based notion responsibility, Chockler Halpern (2004) defined degreeblame, using expected degree responsibility weighed epistemic state agent.agents epistemic state represented pair (K, Pr), K situation form(M, ) Pr probability distribution K. degree blame X=x relativeagents epistemic state (K, Pr) computed sum multiplying expected degreeresponsibility X=x possible situation (MXx, ) agents epistemic stateprobability situation. illustrate this, provide ten-man firing squadexample:firing squad consisting ten excellent marksmen. one live bulletsrifle; rest blanks. marksmen know livebullets. marksmen shoot prisoner dies.Suppose agent knows exactly one marksman live bullets rifle,marksmen shoot. agent considers 10 possible situations, dependingbullets. Let {p1, , p10} probability distribution situations, piagents prior probability marksman-i live bullets. Thus, according agentsepistemic state, expected degree responsibility marksman-1s shot death 1situation bullets (and 0 situations), degreeblame marksman-1s shot death p1.Grounded philosophical principle (i.e., counterfactual reasoning), Chockler &Halperns extended definition responsibility accounts better multiple causesextent cause contributes occurrence specific outcome. Anotheradvantage model definition degree blame takes agents epistemicstate consideration. However, consider one epistemic variable, is,agents knowledge prior action performance. Important concepts moral responsibility,intention freedom choice excluded definition. result,model uses one epistemic state determinant blame assignment,inconsistent psychological theories.Chockler & Halperns (2004) model extension counterfactual reasoning withinstructural-model framework, structural-model approach represents eventsrandom variables causal information equations random variables,several limitations model. instance, causal equations directcorrespondence computational systems, hard obtain practicalapplications. communicative events also represented random variables model(which propositional), difficult construct equations communicative actsinfer intermediate beliefs (e.g., beliefs desires, intentions, etc) importantsocial causal reasoning.3. Attribution Theory Behavioral Judgmentcontemporary psychological studies social causality responsibility judgment drawattribution theory (Heider, 1958). 50 years research, attribution theory progressed228fiMODELING SOCIAL CAUSALITY RESPONSIBILITY JUDGMENT MULTI-AGENT INTERACTIONSsignificantly became core area social psychology (Malle, 2001; Weiner, 2006).Attribution research views social perceivers make sense world attributing behaviorevents underlying causes. Attribution therefore refers process ascribingcause event explaining event, well inferences judgments made. Twoinfluential attributional models social causality, responsibility blame (or credit)proposed Shaver (1985) Weiner (1995), identify underlying key factors (i.e.,attribution variables) people use behavioral judgment. summarize theories (weadopt terminology Shavers model paper).assessments physical causality coercion identify responsible party. Physicalcausality refers connection events outcomes produce,includes personal causality (i.e., human agency) impersonal causality (i.e., environmentalfactors). human agency involved, event become relevantinvestigation responsibility blame/credit. absence coercion, actor whoseaction directly produces outcome regarded responsible. However, presencecoercion (as external force, powerful individual sociallysanctioned authority, limits agents freedom choice), responsibilitymay deflected coercive force. example, two-man firing squad example,captains order limit riflemens freedom avoid prisoners death, captaintake responsibility, depending degree coercion.Intention foreseeability determine degree responsibility. Intention generallyconceived commitment work towards certain act outcome. theories viewintention major determinant degree responsibility. Foreseeability refersagents foreknowledge actions effects. example, although riflemenforesaw shooting gun leads prisoners death, may intend shootingkilling prisoner. However, agent intends action achieve certain outcome,agent must foreknowledge action brings outcome. higherdegree intention, greater responsibility assigned. riflemenintention killing prisoner, instance, assigned much less responsibilitycase really intend so.Weiner (2001) distinguished act intentionality outcome intent. agent mayintentionally perform action, may intend action effects. example,riflemen may intentionally shoot enemy, may intend side effect exposingenemy force. outcome intention (i.e., intended action effect), ratheract intention (i.e., intended action) key responsibility behavioral judgment.Similar difference exists outcome coercion (i.e., coerced action effect) act coercion (i.e.,coerced action). Furthermore, agents intentional action action effect may fail.However, long manifests intentions, failed attempt blamed credited almostsuccessful one (Zimmerman, 1988).result judgment process assignment certain blame creditresponsible party. Shavers model blame assignment follows strict sequential process.model, first one assesses physical causality. human agency involved, judgmentprocess proceeds assessing key variables. Finally, perceiver takes possiblemitigating factors (i.e., justifications excuses) consideration assigns proper blame229fiMAO & GRATCHresponsible agent (mitigating factors modeled yet work). Weiners modelsimilar, relaxed sequential processing Shavers modelpresumed (we follow implications Weiners model relax strict sequential featureShavers model). intensity blame credit determined severity positivityoutcome well degree responsibility. latter based assessedvalues attribution variables.4. Proposed Computational ModelAttribution theory identifies general process key variables people use judging socialbehavior. However, process variables directly applicable computationalsystems, described abstract conceptual level insufficiently precisecomputational perspective. hand, current intelligent systems increasinglysophisticated, usually involving natural language communication, multi-agent interactions, goaldirected reasoning generate execute plans, methods explicitly model beliefs, desiresintentions agents (Pollack, 1990; Grosz & Kraus, 1996; Gratch et al., 2006; Ferguson &Allen, 2007; Swartout et al., 2010).bridge gap conceptual descriptions theory actual componentscurrent intelligent systems, need develop computational mechanisms automaticallyconvert implications conceptual descriptions functionally workable model useintelligent systems. computational model functions inferential mechanismderive conceptual variables theory information context available practicalsystems. Ideally, computational model based data structuresrepresentations typically used practical systems, rely little possibleadditional structural representational features.constructing computational model, follow basic dimensions Shavers modelrelax strict sequential feature. follow implications Weiners model, consideringactions agents outcomes produce. adopt plan representation usedintelligent systems, especially agent-based systems. representation providesconcise description causal relationship events states. also provides clearstructure exploring alternative courses actions, recognizing intentions, assessingcoercive situations plan interventions.take advantage artificial intelligence modeling reasoning techniques, particular,Belief-Desire-Intention model (Bratman, 1987; Georgeff & Lansky, 1987) commonsensereasoning (Gordon & Hobbs, 2004; Mueller, 2006). BDI concepts help us map sometimesvague psychological terms widely accepted concepts AI agent research, researchcommonsense reasoning informs design inferential mechanism generallyoperates conceptual representations. use logic formal representation tool,focusing design small number inference rules capture intuitions peoplesjudgments social behavior2.2Note focus definition logical language, rather, aim identifying commonsenseintuitions peoples behavioral judgment come computational modeling social causalityresponsibility attribution.230fiMODELING SOCIAL CAUSALITY RESPONSIBILITY JUDGMENT MULTI-AGENT INTERACTIONSObservationsSourcesPlan ExecutionCausalKnowledgeModuleTaskExecutionInferencesActionSequenceCausalInferenceDialog ModuleSocialInformationInferenceRulesCommunicationSpeech ActSequenceDialogInferenceBeliefsAlgorithmResultsAttributionValuesJudgmentProcessResponsibilityBlame/CreditFigure 2: Overview Computational Modeldeveloped computational model automatically derive judgmentsunderlying responsibility blame attribution knowledge observations socialacts. Figure 2 illustrates overview computational model. Two sources informationcontribute inference process. One source actions performed agents involvedsocial situation (including physical acts communicative acts). generalcausal knowledge actions states world (i.e., causal knowledge), social rolespower relationship agents (i.e., social information). Causal inference derives beliefscausal evidence. Dialog inference derives beliefs communicative evidence.inferences make use commonsense rules generate beliefs attribution variables.beliefs serve inputs judgment process, described algorithm. Finally,algorithm forms overall judgment assigns proper credit blame responsibleagents.4.1 Representationscomputational representation based plan descriptions widely appliedapplications architecture design intelligent systems (e.g., Georgeff & Lansky, 1987;Veloso et al., 1995; Fischer, Mueller, & Pischel, 1996; Rao, 1996; dInverno, Kinny, Luck, &Wooldridge, 1997; Huber, 1999; Gil, Deelman, Blythe, Kesselman, & Tangmurarunkit, 2004;Marsella & Gratch, 2009). specifically, adopt classical STRIPS operators (Fikes &Nilsson, 1971) hierarchical plan representation (Erol, Hendler, & Nau, 1994; Nau, Cao,Lotem, & Muoz-Avila, 1999).4.1.1 C AUS AL K NOWLE DGEapproach, causal knowledge encoded via hierarchical plan representation. actionset propositional preconditions effects (including conditional effects). Actionseither primitive (i.e., directly executable agents) abstract. abstract action maydecomposed multiple ways decomposition one choice executing action.Different choices action execution alternatives other. abstract actiondecomposed multiple ways, decision node (i.e., node) agent must decide231fiMAO & GRATCHamongst alternatives. Otherwise, abstract action decomposed one way,non-decision node (i.e., node) execution action realized via executingsubactions.plan set actions achieve certain intended goal(s). plan may contain abstractactions (i.e., abstract plan), decomposing abstract actions primitive ones abstractplan results set primitive plans (i.e., plans composed primitive actions),directly executable agents. Consequences outcomes (we use exchangeable)desirable undesirable action effects (i.e., effects positive negative significanceagent). desirability action effects represented utility values (Blythe, 1999).represent hierarchical organizational structure social agents, action planassociated performer (i.e., agent capable performing action) agentauthority execution. used model power relationships agents.Troop-at-aaSupport Unit 1-6Performer: lieutenantAuthority: lieutenantTroop-at-aaTroop-at-aaSend One SquadSend Two SquadsPerformer: sergeantAuthority: lieutenantPerformer: sergeantAuthority: lieutenantOne-sqd-at-aaRemaining-at-aaTwo-sqds-at-aaRemaining-at-aaOne Squad ForwardRemaining ForwardTwo Squads ForwardRemaining ForwardPerformer: squad leaderAuthority: sergeantPerformer: squad leaderAuthority: sergeantPerformer: squad leaderAuthority: sergeantPerformer: squad leaderAuthority: sergeant1-6 Supported Unit FracturedFracturedRoute Secured1-6 SupportedFigure 3: Partial Plan Representation Agent TeamFigure 3 illustrates example plan representation team training systemdeveloped (we shall discuss example Section 4.4). example, lieutenant,sergeant squad leaders work team fulfilling task supporting sister unit (i.e.,unit 1-6). lieutenant leader troop. Two alternative ways available supportunit 1-6, either sending one squad sending two squads. alternative performedsergeant authorized. alternatives decomposed subsequent primitiveactions directly executable squad leaders. Action execution brings certaineffects, example, two squads forward (meaning two four squads troop leavescene) fractures unit (meaning troop forces split weakened),undesirable troop. (Unit) 1-6 supported (meaning sister unit reinforceddeparting squads) desirable team goal.4.1.2 C OM UNIC ATIVE E NTSCommunication agents rich source information inferring social causality.represent communicative events sequence speech acts (Austin, 1962; Searle, 1969).purpose, consider speech acts commonly used agent communication,especially help infer dialogue agents desires, intentions, foreknowledge choicesacting. thus focus acts inform, request, order, accept, reject counter-propose.232fiMODELING SOCIAL CAUSALITY RESPONSIBILITY JUDGMENT MULTI-AGENT INTERACTIONS4.1.3 TTR IB UTION V AR IAB LEAttributional models employ set key variables determine social cause responsibility.Causality refers relationship cause effect. investigationresponsibility attribution, involvement human agency required (Weiner, 1995; Shaver,1985). approach, encode causal knowledge actions (i.e., human agency)effects produce via plan representation.consider act intentionality outcome intent agents. Act intentionrepresented using intend do, outcome intention using intend achieve,connection act outcome intentions using intend by. use know bringrepresent foreseeability. Two concepts important modeling coercion 3 . Oneconcept social obligation. (un)willingness. example, authorizingagent commands another agent perform certain action, latter agentobligation so. latter agent actually willing to, voluntary act rathercoercive one. use coerce represent act coercion coerce achieveoutcome coercion.4.1.4 N OTATIONSprovide symbolic expressions notations used model 4.PredicatesLet x different agents, B actions, e action effect, p q propositions,E effect set time. adopt following predicates model:P1.P2.P3.P4.P5.P6.P7.P8.P9.P10.P11.P12.P13.P14.P15.P16.primitive(A): primitive action.and-node(A): action non-decision node plan structure.or-node(A): action decision node plan structure.alternative(A, B): actions B alternatives performing higher-level action.do(x, A): agent x performs action A.achieve(x, e): agent x achieves effect e.bring-about(A, e): action brings effect e.by(A, e): acting achieve effect e.execute(x, A, t): agent x executes action time t.occur(e, t): effect e occurs time t.inform(x, y, p, t): agent x informs agent p time t.request(x, y, p, t): agent x requests agent p time t.order(x, y, p, t): agent x orders agent p time t.accept(x, p, t): agent x accepts p time t.reject(x, p, t): agent x rejects p time t.counter-propose(x, p, q, y, t): agent x counters p proposes q agenttime t.P17. cause(x, e, t): agent x causes effect e time t.34Coercion sometimes means physical coercion, pushing someones hand pull trigger gun.mean psychological coercion, emphasizes impact psychological states agents.Although represent notations first-order predicate calculus, treat semi-formal notationsmodel conduct theorem-proving type inference strict logical sense.233fiMAO & GRATCHP18.P19.P20.P21.P22.P23.P24.P25.assist-cause(x, y, e, t): agent x assists agent achieving effect e time t.know(x, p, t): agent x knows p time t.want(x, p, t): agent x wants p time t.obligation(x, p, y, t): agent x obligation p created agent time t.intend(x, p, t): agent x intends p time t.coerce(x, y, p, t): agent x coerces agent p time t.superior(x, y): agent x superior agent y.enable(x, E, t): agent x makes effect set E true time (enable(x, E, t) means agentx disables effect set E making least one effect E false time t).P26. can-enable(x, E, t): agent x capable making effect set E true time (can-enable(x,E, t) means agent x disable effect set E making least one effect E falsetime t).P27. true(E, t): effect set E true time (this means every effect E true time t,true(E, t) means least one effect E false time t).Predicates P1P10 denote features related plan structure action execution. PredicatesP11P16 represent communicative acts. predicates used express task knowledgeobservations action execution agent communication. Predicates P17P23 describeepistemic variables (including attributions) used inferring intermediate beliefs. PredicatesP24P26 represent power relationship capabilities agents.FunctionsLet action, e action effect DT domain theory5. adopt followingfunctions model:F1.F2.F3.F4.F5.F6.F7.F8.F9.F10.F11.F12.F13.F14.F15.5subaction(A): subaction set abstract action A.choice(A): choice set performing abstract action A.precondition(A): precondition set action A.effect(A): (definite) effect set action A.conditional-effect(A): conditional effect set action A.antecedent(e): antecedent set conditional effect e.consequent(e): consequent conditional effect e.indefinite-effect(A): indefinite effect set action A.relevant-action(e, DT): relevant action set achieve effect e based domaintheory DT.relevant-effect(e, DT): relevant effect set achieve effect e based domain theoryDT.side-effect(e, DT): side effect set achieve effect e based domain theory DT.performer(A): performing agent(s) action A.authority(A): authorizing agent(s) action A.primary-responsible(e): primary responsible agent(s) effect e.secondary-responsible(e): secondary responsible agent(s) effect e.Domain theory general term used planning plan-based systems, specifying actions performeddomain state affairs (typically described preconditions effects) causally linked actions.Domain theory general knowledge domain represented using given plan representation.234fiMODELING SOCIAL CAUSALITY RESPONSIBILITY JUDGMENT MULTI-AGENT INTERACTIONSAmong functions, F1F7 denote generic features (hierarchical) plan representation.Functions F8F11 describe indefinite effect set, relevant action/effect side effect, functionsF12F15 represent agents involved.4.2 Reasoning cial CausalitySocial causality responsibility judgment involve evaluating outcomes eventspersonal significance agent. evaluation always perceiving agentssubjective perspective. perceiver uses knowledge observed agentsobservation behavior infer beliefs social attributions. show automaticmethods causal dialogue reasoning provide mechanism.4.2.1 IALOGUE NFE R E NC EConversation agents rich source information deriving attribution values.Early attribution theorists (Kidd & Amabile, 1981; Hilton, 1990) pointed importancelanguage communication attributing behavior. Within AI research community,much related work intentions agent communication (Cohen & Levesque, 1990; Smith& Cohen, 1996), plan inference (Allen & Perrault, 1980; Litman & Allen, 1990), discoursestructure (Grosz & Sidner, 1986; Lochbaum, Grosz, & Sidner, 2000) speech act theory(Perrault, 1990). Although previous research partially addressed issue inferringintentions different formalism, focus identifying generic commonsensereasoning rules attribution variables well interrelations socialcommunication.Natural language communication seen collaborative activityconversational agents. Successful communication requires participants follow basicconversation principles (Grice, 1975) reach degree common ground (Clark &Schaefer, 1987). Thus assume communication agents grounded (Traum, 1994),conversation conforms Grices maxims Quality6 Relevance7. conversationaldialogue, participating agents exchange information alternatively. perceiving agent (whoone participating agents another agent) forms updates beliefs accordingobserved speech acts previous beliefs.design commonsense rules allow perceiving agent derive beliefsepistemic states observed agents. also take social information (i.e., social rolesrelationship) consideration. example, order successfully issuedsubordinates, request made agent; request performed agentsdifferent social status may lead different belief derivations.Hobbs (1985) proposed first-order logic notation, using eventuality8 reify eventsconditions. avoid expressing higher-order properties first-order logic, formalismadopted notation; simplification ease illustration, still keep higher678quality maxim states one ought provide true information conversation.relevance maxim states ones contribution conversation ought pertinent context.Eventuality extra argument used predication referring condition exists predicationtrue. every predicate P(x), P true x eventuality possible situation e Ptrue x (called P) e really exists, i.e. (x)P(x)(e)P(e,x)Exist(e). work Hobbs (1985) providedexplanation ontological assumptions notation.235fiMAO & GRATCHorder expressions paper (note actually handled using Hobbs notationapproach). Also, simplify logical forms, universal quantifiers omitted rules,substitute e do(x, A) achieve(x, e) respectively, causingconfusion.time t1, speaker (s) informs (or tells) hearer (h) content p, t1,inferred speaker knows proposition p long interveningcontradictory belief (Rule D1). conversations agents grounded,inferred hearer also knows p (Rule D2). simplify expressions rules,introduce predicate etc9 stands absence contradictory situations.Rule D1 [inform]:inform(s, h, p, t1) t1<t2 etc1 know(s, p, t2)Rule D2 [inform-grounded]:inform(s, h, p, t1) t1<t2 etc2 know(h, p, t2)request shows speaker wants (Rule D3). order (or command) showsspeaker intends (Rule D5). order successfully issued someone highersocial status. requested ordered superior, creates social obligation hearerperform content act (Rules D4 & D6).Rule D3 [request]:request(s, h, p, t1) t1<t2 etc3 want(s, p, t2)Rule D4 [superior-request]:request(s, h, p, t1) superior(s, h) t1<t2 etc4 obligation(h, p, s, t2)Rule D5 [order]:order(s, h, p, t1) t1<t2 etc5 intend(s, p, t2)Rule D6 [order]:order(s, h, p, t1) t1<t2 etc6 obligation(h, p, s, t2)hearer may accept, reject counter-propose order (or request). Various inferencesmade depending response hearer social relationshipspeaker hearer. instance, hearer accepts, obligationbeforehand hearer willing (i.e., wants), inferred hearer intends(Rules D7 & D8).Rule D7 [accept]:obligation(h, p, s, t1) accept(h, p, t2) t1<t2<t3 etc7 intend(h, p, t3)Rule D8 [willing-accept]:want(h, p, t1) accept(h, p, t2) t1<t2<t3 etc8 intend(h, p, t3)clear evidence agents willingness, yet agent accepts obl igation,evidence coercion (Rule D9). another case, agent obviously unwilling(i.e., unintended) accepts obligation, clear evidence coercion (Rule D10).Rule D9 [accept-obligation]:9similar notation used work Hobbs, Stickel, Appelt, Martin (1993). essentially meanscontradictory belief between.236fiMODELING SOCIAL CAUSALITY RESPONSIBILITY JUDGMENT MULTI-AGENT INTERACTIONS(t1)(t1<t3 intend(h, p, t1)) obligation(h, p, s, t2) accept(h, p, t3) t2<t3<t4 etc9coerce(s, h, p, t4)Rule D10 [unwilling-accept-obligation]:intend(h, p, t1) obligation(h, p, s, t2) accept(h, p, t3) t1<t3 t2<t3<t4 etc10coerce(s, h, p, t4)hearer rejects, infer hearer intend (Rule D11). hearer countersproposes B instead, speaker hearer believed know Balternatives (Rules D12 & D13). also implies hearer wants intend (RulesD14 & D15).Rule D11 [reject]:reject(h, p, t1) t1<t2 etc11 intend(h, p, t2)Rule D12 [counter-propose]:counter-propose(h, A, B, s, t1) t1<t2 etc12 know(h, alternative(A, B), t2)Rule D13 [counter-propose-grounded]:counter-propose(h, A, B, s, t1) t1<t2 etc13 know(s, alternative(A, B), t2)Rule D14 [counter-propose]:counter-propose(h, p, q, s, t1) t1<t2 etc14 intend(h, p, t2)Rule D15 [counter-propose]:counter-propose(h, p, q, s, t1) t1<t2 etc15 want(h, q, t2)speaker known alternatives still requests (or orders) one them, inferspeaker wants (or intends) chosen action intend alternative (Rules D16 &D17). (Here z h.)Rule D16 [know-alternative-request]:know(s, alternative(A, B), t1) request(s, h, do(z, A), t2) t1<t2<t3 etc16 intend(s, do(z,B), t3)Rule D17 [know-alternative-order]:know(s, alternative(A, B), t1) order(s, h, A, t2) t1<t2<t3 etc17 intend(s, do(h, B), t3)4.2.2 C AUS AL NFE R E NC EPlan representation gives information inferring agency, intention coercion,direct indirect cases. Causal inference plan-based evaluation based causalinformation provided plan representation.Agency. plan execution environment multiple agents inhabit, agents plansinteract various ways. preconditions agents action may establishedactivities agents, thus agents indirectly help cause outcome. Givendomain theory DT, observed executed actions outcome e, performer actiondirectly causes e causal agent (Rule C1). performers relevant actionsachieve e indirect agency (Rule C2). absence coercion, causal agent deemedresponsible e, agents assist causing e share responsibility causalagent. (The computation relevant actions effects achieve e given Appendix A.)Rule C1 [cause-action-effect]:execute(x, A, t1) eeffect(A) occur(e, t2) t1<t2<t3 etc18 cause(x, e, t3)237fiMAO & GRATCHRule C2 [cause-relevant-effect]:cause(y, e, t1) erelevant-effect(e, DT) cause(x, e, t2) t1<t2<t3 etc19 assistcause(y, x, e, t3)Intention. Attribution intention essential peoples explanations behavior (Heider, 1958;Malle & Knobe, 1997). discussed Section 4.2.1, intentions inferredevidence natural language conversation. Causal inference helps infer outcome intentionevidence act intention. example, agent intends action voluntarily, agent mustintend least one action effect (Rule C3).Rule C3 [intend-action]:intend(x, do(z, A), t1) (y)coerce(y, x, A, t1) t1<t2 etc20 e(eeffect(A) intend(x,e, t2))general cases, action multiple effects, order identify whetherspecific outcome intended not, perceiver may examine action alternatives agentintends intend, compare effects intended unintended alternatives.agent intends action voluntarily intend alternative B, inferagent either intends (at least) one action effect occurs intend(at least) one effect occurs B, both. effect set subset B,effect set B subset A, simplified (Rules C4 & C5).Rule C4 [intend-one-alternative]:intend(x, do(z, A), t1) intend(x, do(z, B), t1) (y)coerce(y, x, A, t1) alternative(A, B)effect(A)effect(B) t1<t2 etc21 e(eeffect(A) eeffect(B) intend(x, e, t2))Rule C5 [intend-one-alternative]:intend(x, do(z, A), t1) intend(x, do(z, B), t1) (y)coerce(y, x, A, t1) alternative(A, B)effect(B)effect(A) t1<t2 etc22 e(eeffect(A) eeffect(B) intend(x, e, t2))clear belief intention derived causal dialogue inferences,employ intention recognition general approach detecting intentions. Given observedexecuted actions agent(s) plan library, observed action sequence matchesactions primitive plan, certainly infer primitive plan pursuedagent(s). situations, however, observed action sequence partially matchspecific plan. find hypothesized plan best explains observed actions, intentionrecognition algorithms use probabilistic models inference. developed generalintention recognition algorithm based probabilistic plan inference (Mao, Gratch, & Li,press). algorithm recursively uses causal information plan representationcompute best candidate plan. provide criteria determining intended actionseffects.agent intends certain plan achieve goal plan, agentintend actions effects relevant achieving goal plan context(Rules C6 & C7). goal intended definition. side effectsintended agent (Rule C8). (The computation relevant actions effects wellside effects plan context given Appendix A.)Rule C6 [intend-plan]:intend(x, by(plan, goal), t1) Arelevant-action(goal, plan) t1<t2 etc23 intend(x, A, t2)238fiMODELING SOCIAL CAUSALITY RESPONSIBILITY JUDGMENT MULTI-AGENT INTERACTIONSRule C7 [intend-plan]:intend(x, by(plan, goal), t1) erelevant-effect(goal, plan) t1<t2 etc24 intend(x, e, t2)Rule C8 [intend-plan]:intend(x, by(plan, goal), t1) eside-effect(goal, plan) t1<t2 etc25 intend(x, e, t2)Foreknowledge. foreknowledge belongs agents epistemic state, mainly deriveddialogue inference. Speech act inform tell, gives evidenceconversants know content act. Intention recognition also helps infer agentsforeknowledge, intention entails foreknowledge: agent intends action achieveeffect e A, agent must know brings e (Rule C9).Rule C9 [intent-foreknowledge-relation]:intend(x, by(A, e), t1) t1<t2 etc26 know(x, bring-about(A, e), t2)addition, agent know action would bring about, actioneffects general knowledge plan representation perceivercontradictory belief specific knowledge involved agents (Rules C10 & C11).Rule C10 [foreknowledge-performer]:eeffect(A) etc27 know(performer(A), bring-about(A, e), t1)Rule C11 [foreknowledge-authority]:eeffect(A) etc28 know(authority(A), bring-about(A, e), t1)Coercion. causal agent could absolved responsibility coerced causeoutcome forces. applying coercive force mean outcome coercionactually occurs. really matters whether force truly constrains causal agentsfreedom avoid outcome. Causal inference helps infer outcome coercion evidenceact coercion.agent coerced execute primitive action, agent also coerced achieveaction effects (Rule C12). coerced execute abstract action actionone decomposition (i.e., non-decision node), agent also coerced executesubsequent actions achieve subaction effects (Rules C13 & C14).Rule C12 [coerce-primitive]:coerce(y, x, A, t1) primitive(A) eeffect(A) t1<t2 etc29 coerce(y, x, e, t2)Rule C13 [coerce-non-decision-node]:coerce(y, x, A, t1) and-node(A) Bsubaction(A) t1<t2 etc30 coerce(y, x, B, t2)Rule C14 [coerce-non-decision-node]:coerce(y, x, A, t1) and-node(A) eeffect(A) t1<t2 etc31 coerce(y, x, e, t2)coerced action multiple decompositions (i.e., decision node), subsequentactions coerced (Rule 15). Since agent options, effects appearalternatives unavoidable (i.e., definite), thus effects coerced (Rule 16);effects appear (but all) alternatives avoidable (i.e., indefinite),coerced (Rule 17). (The computation definite indefinite effects givenAppendix B.)Rule C15 [coerce-decision-node]:coerce(y, x, A, t1) or-node(A) Bchoice(A) t1<t2 etc32 coerce(y, x, B, t2)239fiMAO & GRATCHRule C16 [coerce-decision-node]:coerce(y, x, A, t1) or-node(A) eeffect(A) t1<t2 etc33 coerce(y, x, e, t2)Rule C17 [coerce-decision-node]:coerce(y, x, A, t1) or-node(A) eindefinite-effect(A) t1<t2 etc34 coerce(y, x, e,t2)Given conditional effect coerced, antecedents initially true, consequent alsocoerced (Rule C18). Otherwise, antecedents false initially, consequentcoerced (Rule C19). antecedents established self (i.e., performer),consequent coerced, could choose otherwise (Rule C20). agent(s)establish antecedents, agents assist coercing consequent (Rule C21).agent indirectly coerced (e.g., enabling/disabling action preconditions,blocking action alternatives). among choices coerced action, oneexecutable alternative available coerced agent enable one alternative (i.e.,making action preconditions true), agent coerced execute alternative (RulesC22 & C23). available alternative enabled agent(s),agents assist coercing alternative (Rule C24). agent(s) block actionalternatives (by disabling action preconditions), alternative left coercedblocking agents also coercers (Rule C25).Coercion entails intention. Handing ones wallet threat moneylife may well seen intentional: one decides so, albeit unwillingly, goalsaving life.Rule C26 [coerce-intend-relation]:coerce(y, x, p, t1) t1<t2 etc43 intend(x, p, t2)complete inference rules given Appendix C.4.3 Attribution Algorithmbeliefs derived dialogue causal inferences used attribution processform overall judgment. Different perceivers may different observations, differentknowledge preferences, thus may form different beliefs judge situationdifferently. Despite individual differences, posited attribution process general,applies uniformly different perceivers. action performed agent bringspositive negative effect, agent coerced achieve action effect,performer action primary responsible agent. agents indirectly assistperformer secondary responsible agents. presence external coercion,primary responsible agent redirected coercer (Note coercion may occurone level action hierarchy, process may need trace several levelsfind ultimate source responsibility). agents indirectly assist coercersecondary responsible agents. share responsibility primaryresponsible agent.developed algorithm find responsible agent(s) specific outcome(consequence e). First, based speech act (SA) sequence, algorithm infersdialogue evidence (Step 1). applies causal inference rules (Step 2). executed240fiMODELING SOCIAL CAUSALITY RESPONSIBILITY JUDGMENT MULTI-AGENT INTERACTIONSaction potentially leads consequence, action cause outcome occurrenceperformer action intends bring outcome (i.e. failed attempt) (Step 3.1),assign performer primary responsible agent. agents assistperformer (by enabling action preconditions) secondary responsible agents (Step 3.2).trace coercing agent(s), evaluation process starts primitive action (Step 3.3),works action hierarchy (Step 3.4). pass main loop,evidence outcome coercion (Step 3.4.2), authority deemed responsible (Step 3.4.3).current action root node action hierarchy outcome coercion true,algorithm assigns parent node current action (Step 3.4.4) evaluates next level up.outcome intended responsible agent (Step 3.5), degree responsibility high(Step 3.6). outcome intended (Step 3.7), degree assigned low (Step 3.8).Otherwise, assign medium degree responsibility (Step 3.9). last, algorithm returnsprimary secondary responsible agents well degrees responsibility (Step 4).Attribution Algorithm (SA sequence S, domain theory DT, consequence e, observations):1. Based speech act sequence S, apply dialog inference rules2. Based DT plan representation, apply causal inference rules3. executed action observations3.1cause(performer(A), e) intend(performer(A), by(A, e))3.2primary-responsible(e) = performer(A)secondary-responsible(e) = performer(relevant-action(e, DT))3.3P=A3.43.4.1B=P3.4.2coerce(authority(B), performer(B), e)3.4.3primary-responsible(e) = authority(B)3.4.4P = parent node B DTEND-IFB root action hierarchy coerce(authority(B), performer(B), e)3.5intend(primary-responsible(e), e)3.6Assign high degree responsibility3.7ELSE intend(primary-responsible(e), e)3.8Assign low degree responsibility3.9ELSE assign medium degree responsibilityEND-IFEND-FOR4. RETURN primary-responsible(e) secondary-responsible(e); Degrees responsibilityadopted categorical model responsibility assignment. outcome intendedresponsible agent, degree responsibility high (Recall long manifestsintentions, failed attempt blamed credited almost successful one).outcome intended responsible agent, degree responsibility low.Otherwise, clear evidence outcome intention, assign medium degreeresponsibility. intensity credit blame computed multiplying degreeresponsibility utility outcome. Events may lead onedesirable/undesirable outcomes. evaluating multiple outcomes, apply algorithm241fiMAO & GRATCHway, focusing one outcome time execution. Finally, form overalljudgment, results aggregated grouped responsible agents.4.4 Illustrative Exampleuse example Mission Rehearsal Exercise (MRE) leadership training system(Swartout et al., 2006) illustrate model works. MRE system, human traineepractice decision making skills interactions virtual autonomous agents.train students high-stake social situations, virtual agents figuresresemble humans, also make sense perceived social events exhibithuman-like social reasoning ability. training scenario opens lieutenant (playedstudent), lead troop soldiers fulfill peacekeeping mission. wayreinforce another unit, one troops vehicles seriously injured civilian boy.boys mother medic accident area, crowd gathering around.student faced dilemma whether continue mission render aidboy. Many decisions possible, decision makes lead different outcomesscenario unfolds. important question work goodbad outcomes occur, ensure agents make reasonable judgments react likepeople social situations.one training exercise, example, student (i.e. lieutenant) decided split forces.ordered sergeant (acted autonomous agent) send half squads assistanother unit. sergeant informed bad consequence tried negotiate betteralternative. However, student persisted decision, finally, sergeant orderedsquad leader (Lopez) perform act. Three social actors involved example.lieutenant acts authority sergeant. squad leader acts subordinatesergeant. following dialogue extracted actual run system.illustrate attribute responsibility blame based causal knowledgeobservations agents.Student:Sergeant:Student:Sergeant:Lopez:Sergeant, send two squads forward. (Line 1)bad idea, sir. shouldnt split forces. (Line 2) Insteadsend one squad recon forward. (Line 3)Send two squads forward. (Line 4)recommendation, sir. (Line 5) Lopez! Send first fourth squadsEagle 1-6s location. (Line 6)Yes, sir. Squads! Mount up! (Line 7)Within MRE system, conversations agents represented speech actsdialogue history stored. Details negotiation dialogue automatically generatednatural language mapped speech acts found work Traumcolleagues (2003, 2008). dialogue corresponds following speech acts, orderedtime speakers addressed them. (The symbols lt, sgt sld stand lieutenant,sergeant squad leader, respectively. t1<t2<<t7.)Act 1:Act 2:Act 3:order(lt, sgt, do(sgt, send-two-sqds), t1)(Line 1)inform(sgt, lt, bring-about(send-two-sqds, unit-fractured), t2)(Line 2)counter-propose(sgt, do(sgt, send-two-sqds), do(sgt, send-one-sqd), lt, t3) (Line 3)242fiMODELING SOCIAL CAUSALITY RESPONSIBILITY JUDGMENT MULTI-AGENT INTERACTIONSAct 4:Act 5:Act 6:Act 7:order(lt, sgt, do(sgt, send-two-sqds), t4)accept(sgt, do(sgt, send-two-sqds), t5)order(sgt, sld, do(sld, two-sqds-fwd), t6)accept(sld, do(sld, two-sqds-fwd), t7)(Line(Line(Line(Line4)5)6)7)Figure 3 illustrates causal knowledge troop underlying example. Takesergeants perspective example. sergeant access partial plan knowledgetroop, perceives conversation actors task execution. observedphysical action two-squads-forward executed squad leader occurrence actioneffects. Two effects salient sergeant, (unit) 1-6 supported unit fractured.Supporting unit 1-6 desirable team goal. Assume unit fractured undesirablesergeant assigns negative utility it. consequence serves inputalgorithm.Step 1. Based sequence 1-7 dialogue history, sergeant derive numberbeliefs inferring observed speech acts (Here t1<t1<t2<t2<<t7<t7):Belief 1:Belief 2:Belief 3:Belief 4:Belief 5:Belief 6:Belief 7:Belief 8:Belief 9:Belief 10:Belief 11:Belief 12:Belief 13:intend(lt, do(sgt, send-two-sqds), t1)(Act 1, Rule D5)obligation(sgt, do(sgt, send-two-sqds), lt, t1)(Act 1, Rule D6)know(sgt, bring-about(send-two-sqds, unit-fractured), t2)(Act 2, Rule D1)know(lt, bring-about(send-two-sqds, unit-fractured), t2)(Act 2, Rule D2)know(sgt, alternative(send-two-sqds, send-one-sqd), t3)(Act 3, Rule D12)know(lt, alternative(send-two-sqds, send-one-sqd), t3)(Act 3, Rule D13)intend(sgt, do(sgt, send-two-sqds), t3)(Act 3, Rule D14)want(sgt, do(sgt, send-one-sqd), t3)(Act 3, Rule D15)intend(lt, do(sgt, send-one-sqd), t4)(Act 4, Belief 6, Rule D17)coerce(lt, sgt, do(sgt, send-two-sqds), t5)(Act 5, Beliefs 2&7, Rule D10)intend(sgt, do(sld, two-sqds-fwd), t6)(Act 6, Rule D5)obligation(sld, do(sld, two-sqds-fwd), sgt, t6)(Act 6, Rule D6)coerce(sgt, sld, do(sld, two-sqds-fwd), t7)(Act 7, Belief 12, Rule D9)Step 2. Based observations task execution beliefs obtained Step 1, causalinference derives following beliefs sergeant (Here t0 initial time,t0<t0<t1):Belief 14:Belief 15:Belief 16:Belief 17:Belief 18:Belief 19:Belief 20:Belief 21:know(sld, bring-about(two-sqds-fwd, unit-fractured), t0)(Rule C10)know(sgt, bring-about(two-sqds-fwd, unit-fractured), t0)(Rule C11)intend(lt, unit-fractured, t4)(Beliefs 1&9, Rule C5)coerce(lt, sgt, do(sgt, two-sqds-fwd), t5)(Belief 10, Rule C13)coerce(lt, sgt, do(sgt, remaining-fwd), t5)(Belief 10, Rule C13)coerce(lt, sgt, 1-6-supported, t5)(Belief 10, Rule C14)coerce(lt, sgt, unit-fractured, t5)(Belief 10, Rule C14)coerce(sgt, sld, unit-fractured, t7)(Belief 13, Rule C12)Step 3. Steps 3.13.2: action two-squads-forward directly causes evaluated outcome unitfractured, action performed squad leader, initially, assign squad leaderresponsible agent.243fiMAO & GRATCHStep 3.4: Loop 1: algorithm starts primitive action two-squads-forward.sergeant believes coerced squad leader fracture unit (Belief 21). sergeantalso believes squad leader foreseen outcome unit-fractured(Beliefs 14&15). outcome coercion true, sergeant assigned responsible agent.Since outcome coercion true current node root action hierarchy,algorithm enters next loop.Loop 2: action send-two-squads, performed sergeant. sergeant believeslieutenant coerced fracture unit (Belief 20). sergeant also believeslieutenant intended unit-fractured (Belief 16). outcome coercion true, lieutenantassigned responsible agent. Since outcome coercion true current node rootaction hierarchy, algorithm enters next loop.Loop 3: action support-unit-1-6, performed lieutenant. relevantdialogue act history, clear evidence coercion. current node alreadyroot action hierarchy, algorithm exits loop.Steps 3.53.9: sergeant believes lieutenant intended unit-fractured,lieutenant assigned high degree responsibility outcome.5. Evaluationevaluate computational framework, need assess consistency modelpredictions human judgments social cause, responsibility blame/credit. particular,need evaluate consistency models inferential mechanism underlying humanattributions responsibility blame/credit is, whether model uses sourcesevidence draws intermediate conclusions people do. Thus, designexperiment test model performs predicting beliefs intermediate variables(including attribution variables epistemic variables model) evidence usedinference process. claim model predicts human judgments socialattributions makes inferences consistent people judgments.alternative computational approaches incapable inferring beliefs intermediatevariables, directly compare predictions model human data.5.1 MethodParticipants Procedurestudy consisted 48 subjects either computer science graduate students staffUniversity Southern California. ages range 20 35, 30 subjectsmale. Among them, 12 subjects completed four scenarios survey.subjects completed two scenarios. survey composed four small scenariosorder scenarios randomized across subjects. scenario followedquestionnaire, asking questions assessments internal variables includingcharacters foreknowledge, desire, intentions, obligation perceived coercions.answering question, subjects asked mark (multiple) lines scenarioaccording draw answer. end questionnaire, questionasking subjects score much blame characters deserve scenario.244fiMODELING SOCIAL CAUSALITY RESPONSIBILITY JUDGMENT MULTI-AGENT INTERACTIONSMaterialsstarting point, adopt company program scenario first used (Knobe, 2003a).scenario received much attention recent folk psychology experimentalphilosophy research (Jones, 2009). design three variants company program scenarioquestionnaires following scenario. original scenario (Scenario 2), variants(Scenarios 1, 3 4) complete questionnaires given Appendix D.convenience assessing inference rules, descriptions scenario organizedseparate labeled lines evidence (e.g., E1-E6).Scenario 2:E1E2E3E4E5E6chairman Beta Corporation discussing new program vice presidentcorporation.vice president says, new program help us increase profits,according investigation report, also harm environment.chairman answers, want make much profit can. Start newprogram!vice president says, Ok, executes new program.environment harmed new program.Figure 4: Company Program Scenario 2Experimental Designmodel embodies theoretical view people judge social causeresponsibility differently based perception key variables intention,foreknowledge coercion, good experimental design see model performsevidence judgments systematically varied. end, takedescription single social situation systematically vary it, using inference rulesmodel guide. example, model suggests particular evidence supportsinference coercion, obvious variation would add line scenarioencoding evidence. exploring space inference rules generatingscenarios accordingly, able incorporate information needed different inferencepaths predict judgment results systematic way.Based computational framework introduced Section 4, specific informationutilized inference process includes causal knowledge, goal identification,observations speech acts, physical actions occurrence action effects. encodeinformation line scenarios. encoded information serves modelsinputs provides evidence specific inference. example, Scenario 1,following information encoded (vp chm refer vice president chairman,respectively):E1:E2:E3:E4:E5:E6:request(vp, chm, do(vp, new-program), t1)(speech act)inform(vp, chm, bring-about(new-program, profit-increase), t2)(causal knowledge)inform(vp, chm, bring-about(new-program, env-harm), t2)(causal knowledge)accept(chm, do(vp, new-program), t3)(speech act)execute(vp, new-program, t4)(action execution)occur(env-harm, t5)(outcome occurrence)245fiMAO & GRATCHdesign questions test beliefs different variables. question correspondsfiring inference rule. select assess groups dialogue causalinference rules (D1-D17 C1-C17). rules tested virtual training systemSection 4.4. dialogue inference, design questions test speech acts inform,request, order, accept, accept-obligation counter-propose. Know-alternativetested virtual training scenario. causal inference, design questions testintend-action, intend-plan, intent-foreknowledge-relation, coerce-primitivecoerce-decision-node. Intend-one-alternative, foreknowledge coerce-nondecision-node tested virtual training scenario.Scenario 1, manipulate evidence related agents foreknowledge outcome(i.e., foreknowledge). design questions test inference rules foreseeability(Question 4, Rule D1), relation intent foreknowledge (Question 5, Rule C9), connectionact outcome intentions (Question 3, Rule C3), etc. Scenario 2 gives clear evidenceforeknowledge. authoritys goal also stated. Correspondingly, questions designedtest rules intentional action/effect side effect (Questions 3-4, Rules C7&C8),foreknowledge (Question 1, Rule D2), speech acts. Scenario 3, manipulatedegree perceived coercion unwillingness introducing alternative course actionharm environment vice president prefers. Specifically, addone line E3 E4 (and lines remain Scenario 2).Questions designed test agents willingness (Question 2, Rules D14&D15)perceived coercion (Questions 3-4, Rules D10&C12). Scenario 4, manipulatecharacters freedom choice. introduce alternative, preference vicepresident based feature unrelated environment vice president allowedchoose options. design three questions test important rulescoercion (Rules C15-C17).Model Predictionsquestion questionnaire, models prediction belief belief derivationgiven Appendix E.5.2 Resultsprovide experimental results assessing inferred beliefs inference rules.Question 1Question 2Question 3Question 4Question 5YesYesYesYesYes0273291228003003001020229228291219525525Scenario1ModelPeople30Scenario2ModelPeople30Scenario3ModelPeople21Scenario4ModelPeople219Question 6ChairVP303.003.7385.63N/A5.633.234.135.20N/AN/ATable 1: Model Predictions Subject Responses Company Program Scenarios2463.77fiMODELING SOCIAL CAUSALITY RESPONSIBILITY JUDGMENT MULTI-AGENT INTERACTIONS5.2.1 E ING NFE R R E B E LIE FSTable 1 summarizes experimental results. Results questions 1 5 indicate totalnumber subjects gave particular answer. example, Scenario 1, thirtysubjects reported vice president wanted start new program. Question 6 refersamounts blame attributed chairman vice president scale 1 (little)6 (lots), table lists subjects average reported values. models predictionschecked table. data show questions, people agreequite well. certain disagreements exist questions.purpose assess models general agreement people, measureagreement model subject using Kappa statistic. Kappacoefficient de facto standard evaluate agreement raters, factorsexpected agreement due chance (Carletta, 1996). K coefficient computed as:KP( A) P( E )1 P( E )P(A) propositional agreement among raters. P(E) expected agreement, is,probability raters agree chance. Di Eugenio Glass (2004) arguedcomputation K coefficient sensitive skewed distribution categories (i.e.,prevalence). treatment, account prevalence construct contingency tablescalculation, average results Kappa agreement models predictionssubjects answers. average Kappa agreement model subjects0.732. Based scales given Rietveld van Hout (1993), 0.6<K<0.8 indicatessubstantial agreement. empirical results show good consistency modelsgeneration intermediate beliefs human data.5.2.2 E ING NFE R E NC E R ULEmodel, every belief derived specific inference rule, answer questionquestionnaires corresponds firing one rule (with exception three questionsquestionnaires designed test two rules each). condition side rulecomposed set evidence, assess accuracies inference rules, compareconditions rule evidence people use forming answer. Accuracyrule measured using standard confusion matrix (Kohavi & Provost, 1998). everysubjects evidence choice question, build confusion matrix computenumber true positive TP (i.e., evidence rule subject use), true negative TN(i.e., evidence rule subject ignore), false positive (i.e., evidence ruleincorrectly uses), false negative (i.e., evidence rule incorrectly ignores).question Qi, correct selection evidence corresponding rulerespect subjects measured accuracy (AC), Ns total number subjectsNe total number evidence Qi.(TP( j, Qi) TN ( j, Qi ))AC ( j, Qi )AC (Qi )jSubjectsNsjSubjects247Ns NefiMAO & GRATCHTable 2 lists accuracies tested rules. average accuracy rules 0.85.empirical results show evidence model uses inference consistenthuman data. Thus first experimental study generally supports first claim evaluation:model predicts human judgments social attributions makes inferences consistentpeople judgments.Scenario 1Scenario 2Scenario 3Scenario 4QuestionInference RuleAverage Accuracy1D3 [Request]0.762D7 [Accept]0.963C3 [Intend-Action]0.854D1 [Inform]0.945C9 [Intent-Foreknowledge-Relation]0.911D2 [Inform-Grounded]0.922D5 [Order]0.963C7 [Intend-Plan]0.864C8 [Intend-Plan]0.705D6 & D9 [Order; Accept-Obligation]0.841D13 [Counter-Propose-Grounded]0.942D14 & D15 [Counter-Propose]0.883D6 & D10 [Order; Unwilling-Accept-Obligation]0.804C12 [Coerce-Primitive]0.741C16 [Coerce-Decision-Node]0.712C15 [Coerce-Decision-Node]0.843C17 [Coerce-Decision-Node]0.75Table 2: Accuracies Evidence Used Inference Rules5.3 DiscussionAlthough experimental results show fairly good consistency models predictionshuman data respect inferred beliefs inference rules, results alsoreveal several disagreements among subjects accuracies evidence usedseveral inference rules relatively lower. briefly discuss experimental findingsfirst study.Scenario 1, questionnaire specifically queries perceived desire, foreknowledgeintentions characters. accuracy rule tested Question 1 lower othersbecause, addition evidence E1, many people chose E2 well. Post-experiment interviewssubjects uncovered many subjects assumed making profitsdesirable vice president (because role), therefore, want startnew program increase profits (which supported E2).Scenarios 2 3 manipulate degree perceived coercion willingness coercedagent. Question 4 Scenario 2, one-third subjects think chairmans intentionharm environment. Whether side effect intentional controversial philosophy,empirical studies show similar results (Nadelhoffer, 2006). Also Question 5Scenario 2, subjects think vice president coerced start new programchairman, evidence weaker Scenario 3. Half referred evidence E5,248fiMODELING SOCIAL CAUSALITY RESPONSIBILITY JUDGMENT MULTI-AGENT INTERACTIONSindicating expect vice president negotiate chairman rather directlyaccept order.first question Scenario 3, subjects think chairman knowalternative program, though vice president clearly states scenario.subjects (80%) referred evidence E5, showing looked grounding information.model infers grounded information conversation, consideredscenario design. Question 4 Scenario 3, subjects seemed reluctant infer outcomecoercion evidence act coercion. Nonetheless, still assigned high degree blamechairman.Scenario 4, vice president freedom choice. Question 1, subjects thinkvice president coerced increase profits, reason mentioned earlier.think vice presidents job increase profits, must willing so. accuraciesinference rules Question 1 Question 3 relatively low. model, evidenceneeded inference E3, E4 E5. Many subjects ignore knowledge E3 lowersaccuracies two rules (similar reason low accuracies rules used Question 4Scenarios 2&3).Comparing blame assignments Scenarios 2 3, shows one hand, higherdegree coercion, less blame assigned actor result consistentpsychological findings. hand, even perceived coercion strong, people stillassign high degree blame coercer, Scenario 2. Scenario 4, people assignedblame vice president, could done otherwise. result consistentpsychological findings (Shaver, 1985). However, people still assigned considerable blamechairman, though vice presidents choice harm environment.5.4 Additional Experimentsection, design additional experiment compare overall judgment resultsmodel alternative models human data. Section 2, introducedChockler Halperns (2004) model (abbreviated C&H model) responsibility blamejudgments. addition C&H model, also compare model two simple models.simple cause model always assigns responsibility blame actor whose action directlyproduces outcome. approach used current intelligent systems. Insteadpicking actor, slightly sophisticated model captures intuition hierarchicalstructure universal characteristic human society organizations social poweralways flows top organizational structure. simple authority model choosehighest authority responsible blameworthy agent. report experimenthuman data overall judgments compare models predictions resultssimple cause model, simple authority model C&H model.5.4.1 E THODParticipants ProcedureTwenty-seven subjects participated experiment. either staffs graduatestudents University Southern California, ages ranging 20 45, 14subjects female. subjects presented four similar scenarios. scenario249fiMAO & GRATCHfollowed questionnaire, asking questions assessments physical cause,responsibility, blame perceived coercion characters. order scenariosrandomly assigned.Materialstook starting point firing squad scenario typically used causality research.convenience comparing related work, used original firing squad scenariowork Chockler & Halpern (2004) (Scenario 1), designed variants (Scenarios 2, 34). scenario followed questionnaire. questions questionnairesacross scenarios. original scenario, variants wording questionsgiven Appendix F.Experimental Designdesigned variants Scenario 1 systematically vary perception key variablesintention coercion. variant, manipulate evidence perceived coercionintentions agents. Scenario 2 extends example including authority -commander, orders squad shoot. Scenario 3 extends example presentingnegotiation dialogue commander marksmen. marksmen first rejectcommanders order. commander insists orders again. Finally marksmen acceptorder shoot prisoner. Scenario 4, commander still orders, marksmanfreedom choose either using blanks live bullets shooting.Model Predictionsalternative approach represents typical way handling social causality, responsibilityblame judgment. give predictions model (abbreviated M&G model)alternative models.Simple cause model: simple cause model uses physical causality substitute socialcausality. scenario, predicts marksman (or marksmen) bulletsresponsible blameworthy agent.Simple authority model: simple authority model judges social cause responsibilitytop power hierarchy, regards highest authority responsible. assignsresponsibility blame commander Scenarios 2 4.C&H model: marksman real cause outcome, C&H model predictsmarksmen share responsibility blame Scenario 1. similar reason, Scenarios 23, C&H model predicts commander marksmen responsibleblameworthy. models prediction Scenario 4 depends context (We shall discusslater).M&G model: Scenario 1, model predicts result C&H model,judges commander sole responsible blameworthy agent Scenarios 2 3.last scenario, model assigns responsibility blame marksmen bullets.5.4.2 R E ULTSanswering questions, subjects choose responsible blameworthy agents sixcategories. marksmen bullets, marksmen, commander, commander250fiMODELING SOCIAL CAUSALITY RESPONSIBILITY JUDGMENT MULTI-AGENT INTERACTIONSmarksmen bullets, commander marksmen, none (see Appendix E).Figure 5 shows proportion subjects attribute blame responsibility differentcategories agents, corresponding confidence intervals (=0.05) (Rice, 1994).example, scenario 1, three subjects blame marksman live bullets rifle, 19 blamemarksmen rest blame them. analysis sample dataconfidence intervals show small percentage population blame marksmanlive bullets, significant majority blame marksmen, small percentagewont blame any, 0.95 confidence.ResponsibilityResponsibility100 90-80-70-60-50-40-30-20-10-bulletsmarksmen100 -Blame90-Confidence interval80-70-60-50-40-30-20-10-BlameConfidence intervalcommandernonebullets& commanderScenario 1marksmen& commanderScenario 2Responsibility100 -ResponsibilityBlame100 -BlameConfidence interval90 -Confidence interval90-80-80 -70-70 -60-60 -50-50 -40-40 -30-30 -20-20 -10-10 -commanderbullets& commandermarksmen& commanderbulletsmarksmencommanderbullets& commandermarksmen& commanderScenario 4Scenario 3Figure 5: Proportion Population Agreement Responsibility/Blame ScenariosBlameSimple CauseModelSimple AuthorityModelResultsMatchResultsMatchScenario1bulletsN/AScenario2bulletscommanderyesScenario3bulletscommanderyesScenario4bulletsyes(partial)commanderMatchResultsMatchHumanMajorityAgreementyesmarksmenyesmarksmencommanderyescommandercommanderyescommanderbulletsyes(partial)bullets/bullets &commanderC&H ModelResultsmarksmencommander&marksmencommander&marksmencontextdependentM&G ModelTable 3: Comparison Results Different Models Human DataTable 3 summarizes results blame assignment generated different models,compares results dominant proportion (i.e., majority) human agreement. (InScenario 4, however, dominant proportion overlaps another category; case,251fiMAO & GRATCHmodels prediction falls majority category, regard partial match). simplecause model partially matches human agreement Scenario 4, inconsistentdata Scenarios 1 3. simple authority model matches human data Scenarios 23, inconsistent data scenarios. general, simple models use invariantapproaches judgment problem. Therefore, insensitive changing socialsituations specified scenario. C&H model matches human judgments Scenario 1.remaining scenarios, results show blame model match human datawell. empirical findings show model approximates human judgmentsresponsibility blame/credit performs better computational approaches.5.4.3 C OM P ARC US IONbriefly discuss model appraises scenario compare approachC&H model.Scenario 1. Actions plans explicitly represented approach. Scenario 1,marksman performs primitive action, shooting. action conditional effect,antecedent live bullets consequent death. marksmens shooting actions constituteteam plan squad firing, definite (goal) outcome death (Figure 6). shooting actionsobserved executed, outcome death occurs. observed primitive actionsmarksmen match team plan, certainly infer plan pursued squad10 (i.e.,certain case intention recognition). marksmen believed intend actionsplan plan outcome (i.e. death).Squad FiringPerformer: squadAuthority: noneShootingShootingPerformer: marksman-1Authority: nonePerformer: marksman-2Authority: noneLive BulletsDeathLive BulletsDeathShootingPerformer: marksman-10Authority: noneLive BulletsDeathFigure 6: Team Plan Squad Scenario 1marksman bullets sole causal agent death. marksman intendsoutcome, thus deserves high degree responsibility blame. marksmenblanks also intend actions outcome, shooting actions observed executedantecedent conditional effect false, failed attempt detected. Therefore,marksmen also blameworthy attempt (recall unsuccessful attemptblamed credited almost successful one, Section 3).C&H model judges responsibility according actual cause event.marksman bullets cause death, marksman degreeresponsibility 1 death others degree responsibility 0. result10Note intention recognition method generally applied plan library sequences actions.example oversimplified.252fiMODELING SOCIAL CAUSALITY RESPONSIBILITY JUDGMENT MULTI-AGENT INTERACTIONSinconsistent human data. determining blame, C&H model draws conclusionours, approach different. consider marksmans epistemic stateaction performance (corresponding foreknowledge). 10 situations possible,depending bullets. marksman responsible one situation (inmarksman bullets), degree responsibility 1. Given situation equally likelyhappen (i.e., possibility 1/10), marksman degree blame 1/10.notion intention model, C&H model uses foreknowledgedeterminant blame assignment. fine evidence foreknowledge,foreknowledge entails intention (Rule C9). evidence foreknowledge,however, blame assigned high, even intention manifested case.example, context different example, marksman fires gun mistake,without intention causing attempting death, C&H model, marksmanblamed truly intention.Scenarios 2&3. model, take different forms social interactions account.inference process reasons beliefs causal dialogue evidence. Figure 7illustrates team plan squad Scenarios 2 3, commander actsauthority squad.Squad FiringPerformer: squadAuthority: commanderShootingShootingPerformer: marksman-1Authority: commanderPerformer: marksman-2Authority: commanderLive BulletsDeathLive BulletsDeathShootingPerformer: marksman-10Authority: commanderLive BulletsDeathFigure 7: Team Plan Squad Scenarios 2 3intermediate beliefs inferred Scenario 2 given below. (The symbols cmd, sqdmkn stand commander, squad marksman bullets, respectively.t1<t1<t2<t2.)(1)(2)(3)(4)(5)(6)(7)intend(cmd, do(sqd, squad-firing), t1)obligation(sqd, do(sqd, squad-firing), t1)intend(cmd, death, t1)coerce(cmd, sqd, squad-firing, t2)coerce(cmd, sqd, shooting, t2)coerce(cmd, sqd, death, t2)coerce(cmd, mkn, death, t2)(Act order, Rule D5)(Act order, Rule D6)(Belief 1, Rule C3)(Act accept & Belief 2, Rule D9)(Belief 4, Rule C13)(Belief 4, Rule C14)(Belief 5, Rules C14 & C18)Scenario 2, marksman causes death due coercion. commanderresponsible death. commander intends outcome (Belief 3) severityoutcome death high, commander assigned high degree responsibility blamedhigh intensity.253fiMAO & GRATCHScenario 3 includes sequence negotiation acts. derived beliefs thus changefollowing (t4<t4):(1)(2)(3)(4)(5)(6)(7)(8)intend(cmd, do(sqd, squad-firing), t1)obligation(sqd, do(sqd, squad-firing), t1)intend(cmd, death, t1)intend(sqd, do(sqd, squad-firing), t2)coerce(cmd, sqd, squad-firing, t4)coerce(cmd, sqd, shooting, t4)coerce(cmd, sqd, death, t4)coerce(cmd, mkn, death, t4)(Act order, Rule D5)(Act order, Rule D6)(Belief 1, Rule C3)(Act reject, Rule D11)(Act accept & Beliefs 2&4, Rule D10)(Belief 5, Rule C13)(Belief 5, Rule C14)(Belief 6, Rules C14 & C18)Clearly marksmen intend firing (Belief 4). Scenario 3 shows evidence strongcoercion. also reflected data. greater proportion subjects regardcommander responsible blameworthy Scenario 3 Scenario 2.Assume marksman-1 one live bullets. Using C&H approach, outcomecounterfactually dependent marksman-1s shooting, marksman-1s shooting actualcause death. Similarly, commanders order also actual cause death. Basedresponsibility definition C&H model, commander marksman-1responsible death, degree responsibility 111. assigning blame,ten situations altogether, situation, commander expected responsibility 1,commander blame degree 1. marksmen degree blame 1/10. ThusC&H model appraises commander marksmen blameworthyoutcome.C&H model represents relevant events scenarios random variables. Thus,want model communicative acts Scenarios 2 3 using approach, actmust represented separate variable model (or number speech actsclumped together represented one variable). conversational dialogue involves flexiblecontents orders acts, difficult come structural equations representrelationships variables. ignore communicative acts between,intermediate beliefs conveyed lost.Scenario 4. Unlike previous scenarios, Scenario 4, bullets initially setscenario starts. marksmen choose use either bullets blanks shooting.Firing still joint action squad, team plan common goalsquad. commander orders joint action, shooting actions conditional effectscoerced. However, antecedents enabled self agent (i.e., marksmen bullets),consequent death coerced. inferred beliefs follows.(1)(2)(3)(4)11intend(cmd, do(sqd, squad-firing), t1)obligation(sqd, do(sqd, squad-firing), t1)coerce(cmd, sqd, squad-firing, t3)coerce(cmd, sqd, shooting, t3)(Act order, Rule D5)(Act order, Rule D6)(Act accept & Belief 2, Rule D9)(Belief 3, Rule C13)Halpern Pearl (2005) provide refined definition causality, contingencies allowablesettings considered. refined definition, commander responsible agent death.results blame assignment remain scenario.254fiMODELING SOCIAL CAUSALITY RESPONSIBILITY JUDGMENT MULTI-AGENT INTERACTIONS(5) coerce(cmd, mkn, death, t3)(Belief 4, Rules C14 & C20)case, commander responsible outcome, rather, marksmenchoose use bullets cause death responsible blameworthy. Figure 5 showsScenario 4, peoples judgments somehow diffuse. overlap blamingmarksmen bullets blaming commander marksmen bullets.Nonetheless, category model predicts clearly better three.C&H model requires structural equations deterministic. essence,model could handle alternative courses action, inherently nondeterministicproperties. One remedy push nondeterminism setting context (seeSection 2 explanation context). example, Scenario 4, could build causalmodel let context determine whether bullets live blank marksman,probability distribution contexts. that, compute probabilityactual cause. However, since contexts treated background variables whose valuesassigned modeler, approach could construct internal reasoning processautomate inference alternative courses actions.6. General DiscussionBased well-founded psychological attribution theory, built generalcomputational model social causality responsibility judgment. model takesdifferent forms social interaction account considers actions agentsoutcomes produce. make use commonsense reasoning infer beliefs dialoguecommunication task execution. model based general representation commonlyused intelligent systems. Causal inference plan-based evaluation representation.inferences social attributions overall judgments model shownstrong empirical support respect human data comparison alternativeapproaches.Although examples paper focused negative consequences blamejudgment, model capable credit blame judgments. Currently use uniformmodel two types judgments. However, several researchers made distinctionthem. DArcy (1963) pointed criteria judging benefit (i.e., creditassignment) stricter judging harm (i.e., blame assignment). empiricalfindings work Knobe (2003b) also show credit blame asymmetry peoplesjudgments behavior. findings suggest us consider using asymmetry modelcredit blame assignments future extension.Subjects tended assign shared blame individuals involved. firing squad scenario 1,example, portion subjects mentioned think marksmen actually make groupdecisions together, collectively responsible outcome. Sometimestrue even individual causally connected creditworthy blameworthy event(e.g., chairman blamed company program scenario 1). researchers workrelevant this. Norman Reed (2010) provided logic formalism account delegationresponsibility. models representational inferential mechanism potentialincorporate extensions.255fiMAO & GRATCHAlthough attribution theory emphasizes subjective interpretation events, general theorylaymans judgment behavior. start general principles identified attributiontheory. However, also well known responsibility judgment influenced perceiversemotional states, interpersonal goals impression management (Mele, 2001),dispositional differences personality. People notoriously biased describinginvolvement creditworthy blameworthy events (Bradley, 1978). biases reveal subjectiveneeds motivational influence perceiver responsibility judgment. Related work carriedlab explored influence individual difference explanation socialevents modeling different explanatory styles according agents personalities (Oh, Gratch,& Woo, 2007).paper, focused computational modeling social causalityresponsibility judgment context multi-agent interactions. produce first generalcomputational framework social causality responsibility judgment basedpsychological attribution theory. One major contribution work identificationcommonsense knowledge derivation attributions inter-agent communicationtask execution. Another contribution work empirical validation modelusing human data. producing model, also propose computational accountcoercion design algorithm describe attribution process responsibility judgment.interdisciplinary nature work, also takes first step toward cognitivemodeling human social intelligence helps advance understanding processprinciples human social inference.practical applications work, taken semi-formal approachimplemented model mainly production system. Previously, severalversions implementations improvements regarding work. model firstimplemented within Soar architecture context virtual training environment describedearlier. virtual training system, model closely coupled systemcomponents using blackboard representation, belief update handled using SoarsJTMS mechanism. moved general-purpose programming language implementedinference engine Java. inference engine includes three parts: dialogue reasoner,intention recognizer causal reasoner. implemented dialogue inference rulescausal inference rules model (Rules C22-C25 implemented). Intentionrecognizer implemented separately. experimental studies based Javainference engine.implementation improvement efforts include extension basic modelinteractive environment exploring different explanatory styles (Oh et al., 2007)improvement basic model adding model negligence (Melissen, 2008). Tomai (2009)took attribution variables extended basic model using qualitativeprocess theory. work translates attribution theorys implications blame assignment sixviews impose ordinal constraints blame assignment.7. Conclusionsocial nature computing pervasive every aspect software researchdevelopment. advance computer communication technologies, social computing256fiMODELING SOCIAL CAUSALITY RESPONSIBILITY JUDGMENT MULTI-AGENT INTERACTIONSintelligent system design move toward emphasizing social intelligence (Wang et al.,2007). paper, model key aspect social intelligence, formalizing underlyingsocial reasoning process peoples behavioral judgment. show AI knowledgerepresentation reasoning methods utilized automate social inference judgmentprocess. also conduct human experiments empirically validate proposed model.experimental results show models predictions beliefs intermediate variables,inferential mechanism judgment results consistent peoples responses. Therefore,proposed model generally applied modeling human-like social inferencebehavioral judgment intelligent entities.Acknowledgmentsthank Jerry Hobbs, Paul Rosenbloom, Andrew Gordon, David Traum, Stephen Read,Joseph Halpern, Bernard Weiner Joshua Knobe valuable discussions. worksponsored U.S. Army Research, Development, Engineering Command(RDECOM), content necessarily reflect position policyGovernment, official endorsement inferred. work supported partNNSFC grants #61175040, #71025001, #60921061, #70890084 #91024030.Appendix A. Computing Relevant Actions EffectsGiven domain theory DT, executed action set specific outcome e, relevantactions achieve e contain following actions:action causes e relevant.actions enable precondition relevant action achieve e relevant.e enabled consequent conditional effect A, actions establishantecedent conditional effect relevant.precondition relevant action enabled consequent conditional effect,actions establish antecedent conditional effect also relevant.preconditions relevant actions comprise relevant effects achieve e. Excepte, effects relevant actions side effects.domain theory DT confined actions, preconditions effects specific plan(i.e., within plan context), relevant actions effects achieve goal planderived based computation given above.Appendix B. Computing Definite Indefinite EffectsLet action. abstract action one decomposition, let ai subactionA. abstract action multiple decompositions, let ai choice A.definite effect set denoted effect(A), indefinite effect set denotedindefinite-effect(A).257fiMAO & GRATCHdefinite effect set effect(A) composed action effects, occur waydecomposing primitive actions. computed recursively follows:primitive action, effect(A) consists action effects.effect ( ai )abstract action one decomposition, effect ( A)isubaction ( )effect ( ai )abstract action multiple decompositions, effect ( A)aichoice ( A)indefinite effect set indefinite-effect(A) composed action effectsoccur (but all) ways decomposing primitive actions. computedrecursively follows:primitive action, indefinite-effect(A) = .abstract action one decomposition,indefinite effect ( A)indefinite effect (ai )aisubaction ( A)abstract action multiple decompositions,indefinite effect ( A)(effect (ai ) indefinite effect (ai ))aichoice ( A)effect (ai )aichoice ( A)Appendix C. Inference Rulessimplification, universal quantifies omitted. Variables x, z different agents.Let h speaker hearer, p q propositions, t, t1, , t4 time stamps.Let A, B C actions. Variable e state, denoting action precondition, effect,antecedent consequent conditional effect. rules perceiving agentsperspective.Dialogue Inference RulesD1 [inform]:inform(s, h, p, t1) t1<t2 etc1 know(s, p, t2)D2 [inform-grounded]:inform(s, h, p, t1) t1<t2 etc2 know(h, p, t2)D3 [request]:request(s, h, p, t1) t1<t2 etc3 want(s, p, t2)D4 [superior-request]:request(s, h, p, t1) superior(s, h) t1<t2 etc4 obligation(h, p, s, t2)D5 [order]:order(s, h, p, t1) t1<t2 etc5 intend(s, p, t2)D6 [order]:order(s, h, p, t1) t1<t2 etc6 obligation(h, p, s, t2)D7 [accept]:258fiMODELING SOCIAL CAUSALITY RESPONSIBILITY JUDGMENT MULTI-AGENT INTERACTIONSobligation(h, p, s, t1) accept(h, p, t2) t1<t2<t3 etc7 intend(h, p, t3)D8 [willing-accept]:want(h, p, t1) accept(h, p, t2) t1<t2<t3 etc8 intend(h, p, t3)D9 [accept-obligation]:(t1)(t1<t3 intend(h, p, t1)) obligation(h, p, s, t2) accept(h, p, t3) t2<t3<t4 etc9coerce(s, h, p, t4)D10 [unwilling-accept-obligation]:intend(h, p, t1) obligation(h, p, s, t2) accept(h, p, t3) t1<t3 t2<t3<t4 etc10 coerce(s,h, p, t4)D11 [reject]:reject(h, p, t1) t1<t2 etc11 intend(h, p, t2)D12 [counter-propose]:counter-propose(h, A, B, s, t1) t1<t2 etc12 know(h, alternative(A, B), t2)D13 [counter-propose-grounded]:counter-propose(h, A, B, s, t1) t1<t2 etc13 know(s, alternative(A, B), t2)D14 [counter-propose]:counter-propose(h, p, q, s, t1) t1<t2 etc14 intend(h, p, t2)D15 [counter-propose]:counter-propose(h, p, q, s, t1) t1<t2 etc15 want(h, q, t2)D16 [know-alternative-request]:know(s, alternative(A, B), t1) request(s, h, do(z, A), t2) t1<t2<t3 etc16 intend(s, do(z, B),t3)D17 [know-alternative-order]:know(s, alternative(A, B), t1) order(s, h, A, t2) t1<t2<t3 etc17 intend(s, do(h, B), t3)Causal Inference RulesC1 [cause-action-effect]:execute(x, A, t1) eeffect(A) occur(e, t2) t1<t2<t3 etc18 cause(x, e, t3)C2 [cause-relevant-effect]:cause(y, e, t1) erelevant-effect(e, DT) cause(x, e, t2) t1<t2<t3 etc19 assist-cause(y,x, e, t3)C3 [intend-action]:intend(x, do(z, A), t1) (y)coerce(y, x, A, t1) t1<t2 etc20 e(eeffect(A) intend(x, e,t2))C4 [intend-one-alternative]:intend(x, do(z, A), t1) intend(x, do(z, B), t1) (y)coerce(y, x, A, t1) alternative(A, B)effect(A)effect(B) t1<t2 etc21 e(eeffect(A) eeffect(B) intend(x, e, t2))C5 [intend-one-alternative]:259fiMAO & GRATCHintend(x, do(z, A), t1) intend(x, do(z, B), t1) (y)coerce(y, x, A, t1) alternative(A, B)effect(B)effect(A) t1<t2 etc22 e(eeffect(A) eeffect(B) intend(x, e, t2))C6 [intend-plan]:intend(x, by(plan, goal), t1) Arelevant-action(goal, plan) t1<t2 etc23 intend(x, A, t2)C7 [intend-plan]:intend(x, by(plan, goal), t1) erelevant-effect(goal, plan) t1<t2 etc24 intend(x, e, t2)C8 [intend-plan]:intend(x, by(plan, goal), t1) eside-effect(goal, plan) t1<t2 etc25 intend(x, e, t2)C9 [intent-foreknowledge-relation]:intend(x, by(A, e), t1) t1<t2 etc26 know(x, bring-about(A, e), t2)C10 [foreknowledge-performer]:eeffect(A) etc27 know(performer(A), bring-about(A, e), t)C11 [foreknowledge-authority]:eeffect(A) etc28 know(authority(A), bring-about(A, e), t)C12 [coerce-primitive]:coerce(y, x, A, t1) primitive(A) eeffect(A) t1<t2 etc29 coerce(y, x, e, t2)C13 [coerce-non-decision-node]:coerce(y, x, A, t1) and-node(A) Bsubaction(A) t1<t2 etc30 coerce(y, x, B, t2)C14 [coerce-non-decision-node]:coerce(y, x, A, t1) and-node(A) eeffect(A) t1<t2 etc31 coerce(y, x, e, t2)C15 [coerce-decision-node]:coerce(y, x, A, t1) or-node(A) Bchoice(A) t1<t2 etc32 coerce(y, x, B, t2)C16 [coerce-decision-node]:coerce(y, x, A, t1) or-node(A) eeffect(A) t1<t2 etc33 coerce(y, x, e, t2)C17 [coerce-decision-node]:coerce(y, x, A, t1) or-node(A) eindefinite-effect(A) t1<t2 etc34 coerce(y, x, e, t2)C18 [coerce-conditional-effect-initial-antecedent-true]:econditional-effect(A) true(antecedent(e), t1) coerce(y, x, e, t2) t1<t2<t3 etc35coerce(y, x, consequent(e), t3)C19 [coerce-conditional-effect-initial-antecedent-false]:econditional-effect(A) true(antecedent(e), t1) coerce(y, x, e, t2) t1<t2<t3 etc36coerce(y, x, consequent(e), t3)C20 [coerce-conditional-effect-self-establish-antecedent]:econditional-effect(A) coerce(y, x, e, t1) enable(x, antecedent(e), t2) t1<t2<t3 etc37coerce(y, x, consequent(e), t3)C21 [coerce-conditional-effect-other-establish-antecedent]:econditional-effect(A) coerce(y, x, e, t1) enable(z, antecedent(e), t2) can-enable(x,antecedent(e), t2) t1<t2<t3 etc38 coerce(yz, x, consequent(e), t3)260fiMODELING SOCIAL CAUSALITY RESPONSIBILITY JUDGMENT MULTI-AGENT INTERACTIONSC22 [coerce-decision-node-initial-one-alternative]:Achoice(C) true(precondition(A), t1) (Bchoice(C)BA true(precondition(B), t1)can-enable(x, precondition(B), t1)) coerce(y, x, C, t2) t1<t2<t3 etc39 coerce(y, x, A, t3)C23 [coerce-decision-node-self-enable-alternative]:coerce(y, x, C, t1) Achoice(C) enable(x, precondition(A), t2) (Bchoice(C)BAtrue(precondition(B), t2)can-enable(x, precondition(B), t2)) t1<t2<t3 etc40 coerce(y, x,A, t3)C24 [coerce-decision-node-other-enable-alternative]:coerce(y, x, C, t1) Achoice(C) enable(z, precondition(A), t2) (Bchoice(C)BAtrue(precondition(B), t2)can-enable(x, precondition(B), t2)) t1<t2<t3 etc41 coerce(yz,x, A, t3)C25 [coerce-decision-node-disable-other-alternative]:coerce(y, x, C, t1) Achoice(C) true(precondition(A), t2) (Bchoice(C)BA enable(z,precondition(B), t3)can-enable(x, precondition(B), t3)) t1<t3<t4 t2<t4 etc42coerce(yz, x, A, t4)C26 [coerce-intend-relation]:coerce(y, x, p, t1) t1<t2 etc43 intend(x, p, t2)Appendix D. Company Program ScenariosScenario 1:E1E2E3E4E5E6vice president Beta Corporation goes chairman board requests,start new program?vice president continues, new program help us increase profits,according investigation report, harm environment.chairman answers, well.vice president executes new program.However, environment harmed new program.Questions:1. vice president want start new program?answer:confidence:1Yes23Low456E2E356E2E3HighBased information (circle apply)?E1E4E5E6E4E5E62. chairman intend start new program?answer:confidence:1Yes23Low4HighBased information (circle apply)?E1261fiMAO & GRATCH3. chairmans intention increase profits?answer:confidence:1Yes23Low456E2E3HighBased information (circle apply)?E1E4E5E64. vice president know new program harm environment?answer:confidence:1Yes23Low456E2E3HighBased information (circle apply)?E1E4E5E65. vice presidents intention harm environment starting new program?answer:confidence:1Yes23Low456E2E3HighBased information (circle apply)?E1E4E5E66. much would blame individuals harming environment?Blame chairman:Blame vice president: 11223344556Little6LotsScenario 2:E1E2E3E4E5E6chairman Beta Corporation discussing new program vice presidentcorporation.vice president says, new program help us increase profits,according investigation report, also harm environment.chairman answers, want make much profit can. Start newprogram!vice president says, Ok, executes new program.environment harmed new program.Questions:1. chairman know new program harm environment?answer:confidence:1Yes23Low456E2E356E2E3HighBased information (circle apply)?E1E4E5E6E4E5E62. chairman intend start new program?answer:confidence:1Yes23Low4HighBased information (circle apply)?E13. chairmans intention increase profits?262fiMODELING SOCIAL CAUSALITY RESPONSIBILITY JUDGMENT MULTI-AGENT INTERACTIONSanswer:confidence:1Yes23Low456E2E3HighBased information (circle apply)?E1E4E5E6E4E5E64. chairmans intention harm environment?answer:confidence:1Yes23Low456E2E3HighBased information (circle apply)?E15. vice president coerced start new program (i.e. obligation obeyingchairman)?answer:confidence:1Yes23Low456E2E3HighBased information (circle apply)?E1E4E5E66. much would blame individuals harming environment?Blame chairman:Blame vice president: 112233445566LittleLotsScenario 3:E1E2E3E4E5E6E7chairman Beta Corporation discussing new program vice presidentcorporation.vice president says, new program help us increase profits,according investigation report, also harm environment.Instead, run alternative program, gain us fewer profitsnew program, harm environment.chairman answers, want make much profit can. Start newprogram!vice president says, Ok, executes new program.environment harmed new program.Questions:1. chairman know alternative new program?answer:confidence:1Yes23Low456HighBased information (circle apply)?E1E2E3E4E5E6E7E5E6E72. program vice president willing start?answer:confidence:1New program23LowBased information (circle apply)?Alternative program456HighE1263E2E3E4fiMAO & GRATCH3. vice president coerced start new program?answer:confidence:1Yes23Low456HighBased information (circle apply)?E1E2E3E4E5E6E7E4E5E6E74. vice president coerced harm environment?answer:confidence:1Yes23Low456HighBased information (circle apply)?E1E2E35. much would blame individuals harming environment?Blame chairman:Blame vice president: 11223344556Little6LotsScenario 4:E1E2E3E4E5E6chairman Beta Corporation discussing new program vice presidentcorporation.vice president says, two ways run new program, simple waycomplex way.equally help us increase profits, according investigation report,simple way also harm environment.chairman answers, want make much profit can. Start newprogram either way!vice president says, Ok, chooses simple way execute new program.environment harmed.Questions:1. vice president coerced chairman increase profits?answer:confidence:Yes123Low456E2E3HighBased information (circle apply)?E1E4E5E6E5E6E5E62. vice president coerced chairman choose simple way?answer:confidence:Yes123Low456HighBased information (circle apply)?E1E2E3E43. vice president coerced chairman harm environment?answer:confidence:Yes123LowBased information (circle apply)?456E2E3HighE1264E4fiMODELING SOCIAL CAUSALITY RESPONSIBILITY JUDGMENT MULTI-AGENT INTERACTIONS4. much would blame individuals harming environment?Blame chairman:Blame vice president: 11223344556Little6LotsAppendix E. Belief Derivation Company Program Scenariossymbols chm vp refer chairman vice president, respectively. Time stampst1<t1<t2<t2<<t4<t5. severity outcome environmental harm set medium.Scenario 1Information Encoding:E1E2E3E4E5E6request(vp, chm, do(vp, new-program), t1)inform(vp, chm, bring-about(new-program, profit-increase), t2)inform(vp, chm, bring-about(new-program, env-harm), t2)accept(chm, do(vp, new-program), t3)execute(vp, new-program, t4)env-harmeffect(new-program); occur(env-harm, t5)Question 1 (Rule D3 [request]):request(vp, chm, do(vp, new-program), t1)want(vp, do(vp, new-program), t1)Question 2 (Rule D7 [accept]):accept(chm, do(vp, new-program), t3)intend(chm, do(vp, new-program), t3)Question 3 (Rule C3 [intend-action]):intend(chm, do(vp, new-program), t3) coerce(vp, chm, new-program, t3)profit-increaseeffect(new-program) intend(chm, profit-increase, t3)Question 4 (Rule D1 [inform]):inform(vp, chm, bring-about(new-program, env-harm), t2)know(vp, bring-about(new-program, env-harm), t2)know(vp, bring-about(new-program, env-harm), t2)Question 5 (Rule C9 [intent-foreknowledge-relation]):know(vp, bring-about(new-program, env-harm), t2)intend(vp, by(new-program, env-harm), t2)Question 6 (Attribution Algorithm):Primary-responsible agent: vpDegree responsibility/Intensity blame: lowScenario 2Information Encoding:E2inform(vp, chm, bring-about(new-program, profit-increase), t1)265fiMAO & GRATCHE3E4E5E6inform(vp, chm, bring-about( new-program, env-harm), t1)goal(chm, profit-increase); order(chm, vp, do(vp, new-program), t2)accept(vp, do(vp, new-program), t3); execute(vp, new-program, t3)occur(env-harm, t4)Question 1 (Rule D2 [inform-grounded]):inform(vp, chm, bring-about( new-program, env-harm), t1)know(chm, bring-about( new-program, env-harm), t1)Question 2 (Rule D5 [order]):order(chm, vp, do(vp, new-program), t2)intend(chm, do(vp, new-program), t2)Question 3 (Rule C7 [intend-plan]):intend(chm, by(new-program, profit-increase), t2) profit-increaserelevant-effect(profitincrease, new-program)intend(chm, profit-increase, t2)Question 4 (Rule C8 [intend-plan]):intend(chm, by(new-program, profit-increase), t2) env-harmside-effect(profit-increase,new-program)intend(chm, env-harm, t2)Question 5 (Rules D6 [order] & D9 [accept-obligation]):order(chm, vp, do(vp, new-program), t2)obligation(vp, do(vp, new-program), chm, t2)obligation(vp, do(vp, new-program), chm, t2) accept(vp, do(vp, new-program), t3)coerce(chm, vp, do(vp, new-program), t3)Question 6 (Attribution Algorithm):Primary-responsible agent: chmDegree responsibility/Intensity blame: lowScenario 3Information Encoding:E2E3E4E5E6E7inform(vp, chm, bring-about(new-program, profit-increase), t1)inform(vp, chm, bring-about( new-program, env-harm), t1)counter-propose(vp, do(vp, new-program), do(vp, alternative-program), chm, t1)goal(chm, profit-increase); order(chm, vp, do(vp, new-program), t2)accept(vp, do(vp, new-program), t3); execute(vp, new-program, t3)occur(env-harm, t4)Question 1 (Rule D13 [counter-propose-grounded]):counter-propose(vp, do(vp, new-program), do(vp, alternative-program), chm, t1)know(chm, alternative(new-program, alternative-program), t1)Question 2 (Rules D14 & D15 [counter-propose]):counter-propose(vp, do(vp, new-program), do(vp, alternative-program), chm, t1)intend(vp, do(vp, new-program), t1)266fiMODELING SOCIAL CAUSALITY RESPONSIBILITY JUDGMENT MULTI-AGENT INTERACTIONScounter-propose(vp, do(vp, new-program), do(vp, alternative-program), chm, t1)want(vp, do(vp, alternative-program), t1)Question 3 (Rules D6 [order] & D10 [unwilling-accept-obligation]):order(chm, vp, do(vp, new-program), t2)obligation(vp, do(vp, new-program), chm, t2)intend(vp, do(vp, new-program), t1) obligation(vp, do(vp, new-program), chm, t2)accept(vp, do(vp, new-program), t3)coerce(chm, vp, do(vp, new-program), t3)Question 4 (Rule C12 [coerce-primitive]):coerce(chm, vp, do(vp, new-program), t3) primitive(new-program) env-harmeffect(newprogram)coerce(chm, vp, env-harm, t3)Question 5 (Attribution Algorithm):Primary-responsible agent: chmDegree responsibility/Intensity blame: lowScenario 4Information Encoding:E2E3E4E5E6inform(vp, chm, or-node(new-program), t1)inform(vp, chm, simple-waychoice(new-program), t1)inform(vp, chm, complex-waychoice(new-program,), t1)inform(vp, chm, bring-about(simple-way, profit-increase), t1)inform(vp, chm, bring-about(complex-way, profit-increase), t1)inform(vp, chm, bring-about(simple-way, env-harm), t1)goal(chm, profit-increase); order(chm, vp, do(vp, new-program), t2)accept(vp, do(vp, new-program), t3); intend(vp, simple-way, t3); intend(vp, complex-way,t3);execute(vp, simple-way, t4)occur(env-harm, t5)Question 1 (Rule C16 [coerce-decision-node]):order(chm, vp, do(vp, new-program), t2)obligation(vp, do(vp, new-program), chm, t2)obligation(vp, do(vp, new-program), chm, t2) accept(vp, do(vp, new-program), t3)coerce(chm, vp, do(vp, new-program), t3)coerce(chm, vp, do(vp, new-program), t3) or-node(new-program) profit-increaseeffect(newprogram)coerce(chm, vp, profit-increase, t3)Question 2 (Rule C15 [coerce-decision-node]):coerce(chm, vp, do(vp, new-program), t3) or-node(new-program) simple-waychoice(newprogram)coerce(chm, vp, simple-way, t3)Question 3 (Rule C17 [coerce-decision-node]):267fiMAO & GRATCHcoerce(chm, vp, do(vp, new-program), t3) or-node(new-program) env-harmindefiniteeffect(new-program)coerce(chm, vp, env-harm, t3)Question 4 (Attribution Algorithm):Primary-responsible agent: vpDegree responsibility/Intensity blame: highAppendix F. Firing Squad ScenariosScenario 1Suppose firing squad consisting ten excellent marksmen. onelive bullets rifle; rest blanks. marksmen knowlive bullets. marksmen shoot prisoner dies.Scenario 2Suppose firing squad consisting commanding officer ten excellentmarksmen generally abide leaders commands. one live bulletsrifle; rest blanks. commanding officer marksmen knowmarksman live bullets. commander orders marksmen shoot prisoner.marksmen shoot prisoner dies.Scenario 3Suppose firing squad consisting commanding officer ten excellentmarksmen generally abide leaders commands. one live bulletsrifle; rest blanks. commanding officer marksmen knowmarksman live bullets. commander orders marksmen shoot prisoner.marksmen refuse order. commander insists marksmen shoot prisoner.marksmen shoot prisoner dies.Scenario 4Suppose firing squad consisting commanding officer ten excellentmarksmen generally abide leaders commands. commanding officer ordersmarksman shoot prisoner, marksman choose use either blanks livebullets. commander marksmen know whether marksmen livebullets. tradition, prisoner lives (i.e., everyone chooses blanks), set free.marksmen shoot prisoner dies.Questions (in Scenario 1, Questions 1-3 contain selections b):1.physically caused death?a) marksmen live bullets riflesb) marksmen firing squadc) commanding officerd) a) c)268fiMODELING SOCIAL CAUSALITY RESPONSIBILITY JUDGMENT MULTI-AGENT INTERACTIONSe)f)everybodynone2.would think responsible death?a) marksmen live bullets riflesb) marksmen firing squadc) commanding officerd) a) c)e) everybodyf) none3.deserves blame death?a) marksmen live bullets riflesb) marksmen firing squadc) commanding officerd) a) c)e) everybodyf) none4.making judgment, feel marksmen coerced?a) strong coercionb) weak coercionc) coercionReferencesAleven, V., & Ashley, K. D. (1995). Things Factors. Proceedings Fifth InternationalConference Artificial Intelligence Law.Allen, J. F., & Perrault, C. R. (1980). Analyzing Intention Utterances. Artificial Intelligence, 15(3):143178.Austin, J. (1962). Things Words. Harvard University Press.Blythe, J. (1999). Decision-Theoretic Planning. AI Magazine, 20(2):37-54.Bradley, G. W. (1978). Self-Serving Biases Attribution Process: Reexamination FactFiction Question. Journal Personality Social Psychology, 36(1):56-71.Bratman, M. E. (1987). Intention, Plans, Practical Reason. Harvard University Press.Carletta, J. (1996). Assessing Agreement Classification Tasks: Kappa Statistic. ComputationalIntelligence, 22(2):249-254.Cassell, J., Sullivan, J., Prevost, S., & Churchill, E. (Eds.) (2000). Embodied Conversational Agents.Cambridge University Press.Castelfranchi, C. (1990). Social Power. Proceedings First European Workshop ModelingAutonomous Agents Multi-Agent World.Chockler, H., & Halpern, J. Y. (2004). Responsibility Blame: Structural-Model Approach. JournalArtificial Intelligence Research, 22:93-115.Clark, H. H., & Schaefer, E. F. (1987). Collaborating Contributions Conversation. LanguageCognitive Processes, 2:1-23,.269fiMAO & GRATCHCohen, P. R., & Levesque, H. J. (1990). Intention Choice Commitment. Artificial Intelligence, 42(23):213-261.DArcy, E. (1963). Human Acts: Essay Moral Evaluation. Oxford: Clarendon.Di Eugenio, B., & Glass, M. (2004). Kappa Statistic: second Look. Computational Linguistics,30(1):95-101.dInverno, M., Kinny, D., Luck, M., & Wooldridge, M. (1997). Formal Specification dMARS. In: M.P. Singh, A. Rao M. J. Wooldridge (Eds.). Intelligent Agents IV, pp. 155-176. Springer-Verlag.Erol, K., Hendler, J., & Nau, D. S. (1994). UMCP: Sound Complete Procedure HierarchicalTask-Network Planning. Proceedings Second International Conference Artificial IntelligencePlanning Systems.Ferguson, G., & Allen, J. (2007). Mixed-Initiative Dialogue Systems Collaborative Problem-Solving. AIMagazine, 28(2):23-32.Fikes, R.E., & Nilsson, N. J. (1971). STRIPS: New Approach Application Theorem ProvingProblem Solving. Artificial Intelligence, 2(3-4).Fincham, F. D., & Jaspars, J. M. (1980). Attribution Responsibility: Man Scientist ManLawyer. In: L. Berkowitz (Ed.). Advances Experimental Social Psychology (Vol. 13), pp. 81-138.Academic Press.Fischer, K., Mueller, J. P., & Pischel, M. (1996). Pragmatic BDI Architecture. In: M. Wooldridge, J. P.Mueller M. Tambe (Eds.). Intelligent Agents II, pp. 203-218. Springer-Verlag.Georgeff, M. P., & Lansky, A. L. (1987). Reactive Reasoning Planning. Proceedings SixthNational Conference Artificial Intelligence.Gil, Y., Deelman, E., Blythe, J., Kesselman, C., & Tangmurarunkit, H. (2004). Artificial IntelligenceGrids: Workflow Planning Beyond. IEEE Intelligent Systems, 19(1):26-33.Golbeck, J., & Hendler, J. (2006). Inferring Binary Trust Relationships Web-Based Social Networks,ACM Transactions Internet Technology, 6(4):497-529.Gordon, A., & Hobbs, J. R. (2004). Formalizations Commonsense Psychology. AI Magazine, 25(4):4962.Gratch, J., & Mao, W. (2003). Automating Action Review: Attributing Blame Credit TeamTraining. Proceedings Twelfth Conference Behavior Representation ModelingSimulation.Gratch, J., Mao, W., & Marsella, S. (2006). Modeling Social Emotions Social Attributions. In: R. Sun(Ed.). Cognition Multi-Agent Interaction, pp. 219-251. Cambridge University Press.Gratch, J., Marsella, S., & Petta, P. (2009). Modeling Antecedents Consequences Emotion.Journal Cognitive Systems Research, 10(1):1-5.Grice, H. P. (1975). Logic Conversation. In: P. Cole J. Morgan (Eds.). Syntax Semantics: Vol3, Speech Acts. Academic Press.Grosz, B., & Kraus, S. (1996). Collaborative Plans Complex Group Action. Artificial Intelligence,86(2):269-357.Grosz, B. J., & Sidner, C. L. (1986). Attention, Intentions, Structure Discourse. ComputationalLinguistics, 12(3):175-204.Hage, J. C. (1997). Reasoning Rules: Essay Legal Reasoning Underlying logic. KluwerAcademic Publishers.Halpern, J. Y., & Pearl, J. (2001). Causes Explanations: Structural-Model Approach. Part : Causes.Proceedings Seventeenth Conference Uncertainty Artificial Intelligence.Halpern, J. Y., & Pearl, J. (2005). Causes Explanations: Structural-Model Approach. Part : Causes.British Journal Philosophy Science, 56(4):843-887.270fiMODELING SOCIAL CAUSALITY RESPONSIBILITY JUDGMENT MULTI-AGENT INTERACTIONSHeider, F. (1958). Psychology Interpersonal Relations. John Wiley & Sons Inc.Hilton, D. J. (1990). Conversational Processes Causal Explanation. Psychological Bulletin, 107:65-81.Hobbs, J. R. (1985). Ontological Promiscuity. Proceedings Twenty-Third Annual MeetingAssociation Computational Linguistics.Hobbs, J. R., Stickel, M., Appelt, D., & Martin, P. (1993). Interpretation Abduction. ArtificialIntelligence, 63(1-2):69-142.Huber, M. J. (1999). JAM: BDI-Theoretic Mobile Agent Architecture. Proceedings ThirdInternational Conference Autonomous Agents.Jaimes, A., Sebe, N., & Gatica-Perez, D. (2006). Human-Centered Computing: Multimedia Perspective.Proceedings Fourteenth Annual ACM International Conference Multimedia.Jennings, N. R. (1992). Responsible. In: E. Werner Y. Demazeau (Eds.). Decentralized A.I.,pp. 93-102. North Holland Publishers.Johnson, C., & Gonzalez, A. J. (2008). Automated Action Review: State-of-the-Art ReviewTrends. Journal Defense Modeling Simulation: Applications, Methodology, Technology.5(2):108-121.Jones, D. (2009). Good, Bad Intentional. Psychologist, 22(8):666-669, August.Kant, I. (1998). Groundwork metaphysics morals. Cambridge University Press.Kidd, R. F., & Amabile, T. M. (1981). Causal Explanations Social Interaction: DialoguesDialogue. In: J. H. Harvey, W. J. Ickes R. F. Kidd (Eds.). New Directions Attribution Research(Vol. 3), pp. 307-328. Lawrence Erlbaum Associates.Knobe, J. (2003a). Intentional Action Side-Effects Ordinary Language. Analysis, 63:190-193.Knobe, J. (2003b). Intentional Action Folk Psychology: Experimental Investigation. PhilosophicalPsychology, 16:309-324.Kohavi, R., & Provost, F. (1998). Glossary Terms. Machine Learning, 30(2/3):271-274.Kraus, S., Hoz-Weiss, P., & Wilkenfeld, J. (2008), Resolving Crises Automated BilateralNegotiations. Artificial Intelligence, 172(1).Litman, D. J., & Allen, J. F. (1990). Discourse Processing Commonsense Plans. In: P. R. Cohen, J.Morgan M. E. Pollack (Eds.), Intentions Communication, pp.365-388. MIT Press.Lochbaum, K. E., Grosz, B. J., & Sidner, C. L. (2000). Discourse Structure Intention Recognition. In: R.Dale, H. Moisl H. Somers (Eds.), Handbook Natural Language Processing, pp.123-146.Malle, B. F. (2001). Attribution processes. N. J. Smelser P. B. Baltes (Eds.), Internationalencyclopedia social behavioral sciences Vol. 14, pp. 913-917. Elsevier.Malle, B. F., & Knobe, J. (1997). Folk Concept Intentionality. Journal Experimental SocialPsychology, 33:101-121.Mao, W., Gratch, J., & Li, X. (in press). Probabilistic Plan Inference Group Behavior Prediction. IEEEIntelligent Systems.Marinier, R. P., & Laird, J.E. (2004). Towards Comprehensive Computational Model EmotionsFeelings. Proceedings Sixth International Conference Cognitive Modeling.Marsella, S., & Gratch, J. (2009). EMA: Process Model Appraisal Dynamics. Journal CognitiveSystems Research, 10(1): 70-90.Martinovski, B., & Mao, W. (2009). Emotion Argumentation Engine: Modeling Role EmotionNegotiation. Group Decision Negotiation, 18(3):235-259.Martinovski, B., Mao, W., Gratch, J., & Marsella, S. (2005). Mitigation Theory: Integrated Approach.Proceedings Twenty-Seventh Annual Conference Cognitive Science Society.271fiMAO & GRATCHMcCarty, L. T., & Sridharan, N. S. (1981). Representation Evolving System Legal Concepts: .Prototypes Deformations. Proceedings Seventh International Joint Conference ArtificialIntelligence.McCarty, L. T. (1995). Implementation Eisner v. Macomber. Proceedings Fifth InternationalConference Artificial Intelligence Law.McCarty, L. T. (1997). Arguments Legal Arguments. Proceedings Sixth InternationalConference Artificial Intelligence Law.Mele, A. R. (2001). Self-Deception Unmasked. Princeton University Press.Melissen, A. (2008). Exploring Neglected Avenues Modeling Attribution Theory. Master Thesis,Department Human Media Interaction, University Twente.Mueller, E. (2006). Commonsense Reasoning. Morgan Kaufmann Publishers.Nadelhoffer, T. (2006). Trying Save Simple View. Mind & Language, 21(5):565-586, November.Nau, D. S., Cao, Y., Lotem, A., & Muoz-Avila, H. (1999). SHOP: Simple Hierarchical Ordered Planner.Proceedings Sixteenth International Joint Conference Artificial Intelligence.Newell, A., & Simon, H. A. (1972). Human Problem Solving. Prentice-Hall.Norman, T. J., & Reed, C. (2010). Logic Delegation Responsibility. Artificial Intelligence,174(1):51-71.Oh, S., Gratch, J., & Woo, W. (2007). Explanatory Styles Socially Interactive Agents. ProceedingsSecond International Conference Affective Computing Intelligent Interaction.Pearl, J. (1999). Reasoning Cause Effect. Proceedings Sixteenth International JointConference Artificial Intelligence.Perrault, C. R. (1990). Application Default Logic Speech Act Theory. In: P. R. Cohen, J. MorganM. E. Pollack (Eds.), Intentions Communication, pp.161-186. MIT Press.Picard, R. W. (1997). Affective Computing. MIT Press.Picard, R. W. (2010). Affective Computing: Laughter IEEE. IEEE Transactions AffectiveComputing, 1(1):11-17, January-June.Pollack, M. E. (1990). Plans Complex Mental Attitudes. In: P. R. Cohen, J. Morgan M. E. Pollack(Eds.), Intentions Communication, pp.77-103. MIT Press.Prakken, H. (1997). Logic Tools Modeling Legal Argument: Study Defeasible ArgumentationLaw. Kluwer Academic Publishers.Prakken, H., & Sartor, G. (2002). Role Logic Computational Models Legal Argument. In:A.Kakas F. Sadri (eds.). Computational Logic: Logic Programming Beyond, Essays HonorRobert A. Kowalski, Part II, pp. 342-380. Springer-Verlag.Rao, A. S. (1996). AgentSpeak(L): BDI Agents Speak Logical Computable Language. In: W. Vande Velde J. W. Perram (Eds.). Agents Breaking Away: Proceedings Seventh EuropeanWorkshop Modeling Autonomous Agents Multi-Agent World, pp. 42-55. Springer-Verlag.Rice J. A. (1994). Mathematical Statistics Data Analysis (Second Edition). Duxbury Press.Rich, C., Sidner, C. L., & Lesh, N. (2001). COLLAGEN: Applying Collaborative Discourse TheoryHuman-Computer Interaction. AI Magazine, 22(4):15-26.Rietveld, T., & van Hout. R. (1993). Statistical Techniques Study Language LanguageBehavior. Mouton de Gruyter.Rissland, E. L., & Ashley, K. D. (1987). Case-Based System Trade Secrets Law. ProceedingsFirst International Conference Artificial Intelligence Law.Rissland, E. L., & Skalak, D. B. (1991). CABARET: Statutory Interpretation Hybrid Architecture.International Journal Man-Machine Studies, 34:839-887.272fiMODELING SOCIAL CAUSALITY RESPONSIBILITY JUDGMENT MULTI-AGENT INTERACTIONSSchurr, N., Marecki, J., Tambe, M., & Scerri, P. (2005). Towards Flexible Coordination Human-AgentTeams. Multiagent Grid Systems, 1(1):3-16.Searle, J. R. (1969). Speech Acts: Essay Philosophy Language. Cambridge University Press.Shaver, K. G. (1985). Attribution Theory Blame: Causality, Responsibility Blameworthiness.Springer-Verlag.Sichman, J. S., Conte, R., Demazeau, Y., & Castelfranchi, C. (1994). Social Reasoning MechanismBased Dependence Networks. Proceedings Eleventh European Conference AI.Smith, I. A., & Cohen, P. R. (1996). Toward Semantics Agent Communications Language BasedSpeech-Acts. Proceedings Thirteenth National Conference Artificial Intelligence.Swartout, W., Gratch, J., Hill, R., Hovy, E., Marsella, S., Rickel, J., & Traum, D. (2006). Toward VirtualHumans. AI Magazine, 27(2):96-108.Swartout, W., Traum, D., Artstein, R., Noren, D., Debevec, P., Bronnenkant, K., Williams, J., Leuski, A.,Narayanan, S., Piepol, D., Lane, C., Morie, J., Aggarwal, P., Liewer, M., Chiang, J., Gerten, J., Chu, S.,& White, K. (2010). Ada Grace: Toward Realistic Engaging Virtual Museum Guides.Proceedings Tenth International Conference Intelligent Virtual Agents.Tomai, E. (2009). Pragmatic Approach Computational Narrative Understanding. Ph.D. Thesis,Electrical Engineering Computer Science Department, Northwestern University.Traum, D. (1994). Computational Theory Grounding Natural Language Conversation. Ph.D. Thesis,Computer Science Department, University Rochester.Traum, D., Gratch, J., Marsella, S., Lee, J., & Hartholt, A. (2008). Multi-party, Multi-issue, Multi-strategyNegotiation Multi-modal Virtual Agents. Proceedings Eighth International ConferenceIntelligent Virtual Agents.Traum, D., Rickel, J., Gratch, J., & Marsella, S. (2003). Negotiation Tasks Hybrid Human-AgentTeams Simulation-Based Training. Proceedings Second International Joint ConferenceAutonomous Agents Multiagent Systems.Veloso, M., Carbonell, J., Perez, A., Borrajo, D., Fink, E., & Blythe, J. (1995). Integrating PlanningLearning: Prodigy Architecture. Journal Theoretical Experimental Artificial Intelligence,7(1):81-120.Wang, F., Zeng, D., Carley, K., & Mao, W. (2007). Social Computing: Social Informatics SocialIntelligence. IEEE Intelligent Systems, 22(2):79-83.Wang, F., Zeng, D., Hendler, J. A., Zhang Q., Feng, Z., Gao, Y., Wang, H., & Lai, G. (2010). StudyHuman Flesh Search Engine: Crowd-Powered Expansion Online Knowledge. Computer,43(8):45-53.Weiner, B. (1995). Judgment Responsibility: Foundation Theory Social Conduct.Guilford Press.Weiner, B. (2001). Responsibility Social Transgressions: Attributional Analysis. In: B. F. Malle, L. J.Moses D. A. Baldwin (Eds.), Intentions Intentionality: Foundations Social Cognition, pp.331-344. MIT Press.Weiner, B. (2006). Social Motivation, Justice Moral Emotions: Attributional Approach.Lawrence Erlbaum Associates.Zimmerman, M. J. (1988). Essay Moral Responsibility. Rowman & Littlefield.273fiJournal Artificial Intelligence Research 44 (2012) 455-490Submitted 1/12; published 7/12Tractable Triangles Cross-Free ConvexityDiscrete OptimisationMartin C. Coopercooper@irit.frIRIT, University Toulouse IIIToulouse, FranceStanislav Zivnystanda.zivny@cs.ox.ac.ukDepartment Computer Science, University OxfordOxford, UKAbstractminimisation problem sum unary pairwise functions discrete variablesgeneral NP-hard problem wide applications computing MAP configurationsMarkov Random Fields (MRF), minimising Gibbs energy, solving binary ValuedConstraint Satisfaction Problems (VCSPs).study computational complexity classes discrete optimisation problemsgiven allowing certain types costs every triangle variable-value assignmentsthree distinct variables. show several computational problems, nontrivial tractable classes well known maximum matching problem recentlydiscovered joint-winner property. results, apart giving complete classificationsstudied cases, provide guidance search hybrid tractable classes; is, classesproblems captured restrictions functions (such submodularity)structure problem graph (such bounded treewidth).Furthermore, introduce class problems convex cardinality functionscross-free sets assignments. prove imposing one two conditionsrenders problem NP-hard, conjunction two gives rise novel tractable classsatisfying cross-free convexity property, generalises joint-winner propertyproblems unbounded arity.1. Introductiontopic paper following optimisation problem: given set discrete variablesset functions, depending subset variables, minimise sumfunctions variables. fundamental research problem studiedwithin several different contexts computer science artificial intelligence differentnames: Min-Sum Problems (Werner, 2007), MAP inference Markov Random Fields(MRF) Conditional Random Fields (CRF) (Lauritzen, 1996; Wainwright & Jordan,2008), Gibbs energy minimisation (Geman & Geman, 1984), Valued Constraint SatisfactionProblems (Dechter, 2003), (for two-state variables) pseudo-Boolean optimisation (Boros& Hammer, 2002).use terminology Valued Constraint Satisfaction Problems (VCSPs) (Schiex,Fargier, & Verfaillie, 1995; Dechter, 2003). start special case VCSPs dealsfeasibility (rather optimisation) problem.c2012AI Access Foundation. rights reserved.fiCooper & ZivnyConstraint Satisfaction Problem (CSP) instance consists collection variablesmust assigned values subject specified constraints (Montanari, 1974).CSP instance underlying undirected graph, known constraint graph (or structure), whose vertices variables instance, two vertices adjacentcorresponding variables related constraint.important line research CSPs identify tractable cases recognisable polynomial time. work focused one two generalapproaches: either identifying forms constraint sufficiently restrictive ensuretractability matter combined (Bulatov, Krokhin, & Jeavons, 2005; Feder &Vardi, 1998), else identifying structural properties constraint networks ensuretractability matter forms constraint imposed (Dechter & Pearl, 1988).first approach led identifying certain algebraic closure operations knownpolymorphisms (Jeavons, 1998) necessary set constraint types ensuretractability. set constraint types property called tractable constraintlanguage. second approach used characterise tractable cases boundedarity CSPs (such binary CSPs) (Dalmau, Kolaitis, & Vardi, 2002; Grohe, 2007)unbounded-arity CSPs (Marx, 2010).practice, constraint satisfaction problems usually possess sufficiently restricted structure use sufficiently restricted constraint language falltractable classes. Nevertheless, may still properties ensuresolved efficiently, properties concern structure formconstraints. properties sometimes called hybrid reasons tractability (Dechter, 2003; Cohen, 2003; Cohen & Jeavons, 2006; Cooper, Jeavons, & Salamon,2010; Cohen, Cooper, Green, & Marx, 2011).CSPs capture feasibility aspects given problem. Since many computationalproblems involve seeking solution optimises certain criteria, well satisfying certain restrictions, various general frameworks optimisation problems studiedlinear programming, mixed integer programming others (Hooker, 2007). Onepossibility extend CSPs so-called soft constraint satisfaction problems, allowmeasures desirability associated different assignments variables (Dechter,2003; Meseguer, Rossi, & Schiex, 2006). instance soft CSP, every constraintassociated function (rather relation standard CSPs) represents preferences among different partial assignments, goal find bestassignment. Several general soft CSP frameworks proposed literature (Schiex, Fargier, & Verfaillie, 1995; Bistarelli, Montanari, & Rossi, 1997). paperfocus one general frameworks, valued constraint satisfaction problem (VCSP) (Schiex, Fargier, & Verfaillie, 1995). VCSPs powerful enough includemany interesting optimisation problems (Rossi, van Beek, & Walsh, 2006; Cohen, Cooper,Jeavons, & Krokhin, 2006) and, pointed beginning introduction,equivalent well studied optimisation problems studied computer visionfields computer science artificial intelligence.important line research VCSPs identify tractable cases recognisable polynomial time. well known structural reasons tractability generaliseVCSP (Bertele & Brioshi, 1972; Dechter, 2003). case language restrictions, conditions known guarantee tractability given set valued456fiTractable Triangles Cross-Free Convexity Discrete Optimisationconstraints (Cohen, Cooper, Jeavons, & Krokhin, 2006; Cohen, Cooper, & Jeavons, 2008;Jonsson, Kuivinen, & Thapper, 2011; Kolmogorov, 2011; Kolmogorov & Zivny, 2012).1.1 Contributionspaper full version results described two conference papers (Cooper & Zivny,2011a, 2011c).1.1.1 Binary VCSPsfirst part paper, study hybrid tractability binary VCSPs (i.e. optimisationproblems involving functions two arguments) various sets possible costscorrespond CSPs, CSPs soft unary constraints, Max-CSPs, finite-valued VCSPsgeneral-valued VCSPs.focus classes instances defined allowed combinations binary costs everyassignment 3 different variables (called triangle). motivation investigationone restriction, so-called joint-winner property recently showndefine tractable class (Cooper & Zivny, 2011b). finite sets possible costs (corresponding CSPs Max-CSPs), finitely many possibilities. example,Max-CSPs four possible multi-sets costs, namely {0, 0, 0}, {0, 0, 1},{0, 1, 1} {1, 1, 1}. However, infinite sets possible costs (corresponding finitevalued CSPs general-valued VCSPs) infinitely many combinations. Obviously,cannot consider all, hence consider equivalence relation basedtotal order valuation structure. example, consider four equivalence classesmulti-sets {, , } given = = , = < , = > , < < .sets possible costs consider, prove dichotomy theorem, thus identifying tractable cases respect equivalence relation combinationscosts. turns two non-trivial tractable cases: well-knownmaximum matching problem (Edmonds, 1965b), recently discovered joint-winnerproperty (Cooper & Zivny, 2011b).1.1.2 Non-binary VCSPssecond part paper, introduce cross-free convexity property (CFC),show gives rise novel tractable class VCSPs. Informally speaking, CFCproperty conjunction convex cost functions applied structured set setsvariable-value assignments. CFC property generalises recent results VCSPssatisfying non-overlapping convexity property (Cooper & Zivny, 2011b) droppingassumption input functions non-decreasing allowing assignmentsets hierarchically nested (laminar) also cross-free. (All termsdefined formally Section 4.) generalise tractable class workCooper & Zivny (2011b), algorithm also better running time comparedalgorithm Cooper & Zivny (2011b). Moreover, show relaxing either onecross-free convexity assumptions leads NP-hard class.VCSP instance may subset constraints cross-free convex.Since network projection-safe (Lee & Leung, 2009), use establish softglobal arc consistency subset constraints viewed single global constraint.457fiCooper & Zivnyalso show that, Boolean domains, possible determine polynomial timewhether exists subset constraints VCSP instance satisfiescross-free convexity property renaming variables constraints. explorearea even further, study restrictions overlaps constraint scopes, identifyanother tractable class incomparable cross-free convexity property.1.2 Organisation Paperrest paper organised follows. start, Section 2, defining valuationstructures, valued constraint satisfaction problems, basics flow networks. Section 3devoted classification binary VCSPs defined triangles: Section 3.1,present results CSPs, followed results CSPs soft unary constraintsSection 3.2. Section 3.3, present results Max-CSPs, followed resultsfinite-valued general-valued VCSPs Section 3.4 Section 3.5 respectively.Section 4 devoted results non-binary VCSPs: Section 4.1, presentalgorithm VCSPs satisfying cross-free convexity property analyze runningtime. Section 4.4 shows neither cross-freeness convexity enoughguarantee tractability. Section 4.5, extend class cross-free convex VCSPsBoolean domains using notion renamability. Section 4.6 explores related notionsets variables rather sets variable-value assignments. Finally, concludeSection 5.2. Preliminariessection, define valuation structures, valued constraint satisfaction problems,present basics flow networks.2.1 Valuation Structuresvaluation structure, , totally ordered set, minimum maximum element(denoted 0 ), together commutative, associative binary aggregation operator(denoted ), , , , 0 = , whenever .Members called costs.shall denote Q+ set non-negative rational numbers. define Q+ =Q+ {}. consider following subsets valuation structure Q+ : {0, }, {0, 1},Q+ Q+ , cases aggregation operation standard addition operationrationals +. Moreover, Q+ , define + = + = .2.2 Valued Constraint Satisfaction Problemsinstance Valued Constraint Satisfaction Problem (VCSP) (Schiex, Fargier, &Verfaillie, 1995) given n variables v1 , . . . , vn finite domains D1 , . . . , Dn valuesset constraints C. constraint C pair hs, gi, listvariables = hvi1 , . . . , vim called constraint scope, g m-ary cost functiong : Di1 . . . Dim . assignment values domains variablescalled solution. goal find optimal solution; is, solution minimisestotal cost given aggregation costs restrictions onto constraint458fiTractable Triangles Cross-Free Convexity Discrete Optimisationscope:minv1 D1 ,...,vn Dng(vi1 , . . . , vim ) .hhvi1 ,...,vim i,giCDepending set costs may occur instances, get special casesVCSP: = {0, } corresponds Constraint Satisfaction Problem (CSP), {0, 1}corresponds Maximum Constraint Satisfaction Problem (Max-CSP), Q+ correspondsfinite-valued VCSP, finally Q+ corresponds general-valued VCSP.domains variables same, denote common domainD. CSP instance called satisfiable cost optimal solution zero (i.e.constraints satisfied).Cost functions range {0, } called crisp. Cost functions crispcalled soft.2.3 Binary Valued Constraint Satisfaction ProblemsSection 3 interested special case VCSP boundarity constraints 2; known binary VCSPs. Without loss generality,assume binary VCSPinstance contains constraints possible scopes;nis, n unary constraints 2 binary constraints. denote cost function associatedunary constraint scope hvi ci cost function associatedbinary constraint scope hvi , vj cij . absence constraint variablevi (or variables vi , vj ) modelled cost function ci (or cij , respectively)uniformly zero. Using notation, goal find solution minimisestotal cost given by:nci (vi )cij (vi , vj ) .i=11i<jnRemark 2.1. remark terminological differences. VCSPs studied differentnames Min-Sum, Gibbs energy minimisation, Markov Random Fields; domainvalues sometimes called labels, whereas binary instances called pairwise instances,m-ary cost functions called m-cliques, solutions called labellings.2.4 Network Flowsreview basics flows graphs. refer reader standard textbooks (Ahuja, Magnanti, & Orlin, 2005; Schrijver, 2003) details. presentnotions results needed purposes. particular, deal integral flowsonly. denote N set positive integers zero. Let G = (V, A) directedgraph vertex set V arc set A. arc demand/capacity function [d(a), c(a)] weight (or cost) function w(a), d(a), c(a) N w(a) Q.Let s, V . function f : N called flow (or flow)v V \ {s, t},Xa=(u,v)Af (a) =Xf (a)a=(v,u)A459(flow conservation).fiCooper & Zivnysay flow Pfeasible d(a) fP(a) c(a) A. define valueflowfval(f)=f(a)a=(s,v)Aa=(v,s)A f (a). define cost flow fPaA w(a)f (a). minimum-cost flow feasible flow minimum cost.Algorithms finding minimum-cost flow given value well known (Ahuja,Magnanti, & Orlin, 2005; Schrijver, 2003). consider generalisation minimumcost flow problem. arc convex weight function wa associatescost wa (f (a)) flow f (a) along arc a. particular, consider modelweight functions wa (a A) convex piecewise linear given breakpoints(which coversP case convex functions integers). cost flow fdefined aA wa (f (a)). corresponding problem finding minimum-cost integralflow known minimum convex cost flow problem. network n verticesedges capacities U , minimum convex cost flow problem solvedtime O((m log U )SP (n, m)), SP (n, m) time compute shortest directedpath network n vertices edges (Minoux, 1984, 1986; Ahuja, Magnanti, &Orlin, 2005).3. Complexity Classification Binary VCSPs Defined TrianglesVCSP instance, use word triangle set assignments {hvi , ai, hvj , bi, hvk , ci},vi , vj , vk distinct variables Di , b Dj , c Dk domain values.multi-set costs triangle {cij (a, b), cik (a, c), cjk (b, c)}. triple costsalways refer multi-set binary costs triangle.triangle {hvi , ai, hvj , bi, hvk , ci}, Di , b Dj , c Dk , satisfies jointwinner property (JWP) either three cij (a, b), cik (a, c), cjk (b, c) same, twoequal third one bigger. VCSP instance satisfies joint-winnerproperty every triangle satisfies joint-winner property.Theorem 3.1. (Cooper & Zivny, 2011b) class VCSP instances satisfying JWPtractable.previous work (Cooper & Zivny, 2011b), also showed class definedjoint-winner property maximal allowing single extra triple costs violatesjoint-winner property renders class NP-hard.Theorem 3.2. (Cooper & Zivny, 2011b) Let < , Q+ , Q+ ,multi-set costs satisfy joint-winner property. class instancescosts triangle either satisfy joint-winner property {, , }NP-hard, even Boolean Max-CSPs, CSPs size-3 domains Boolean finite-valuedVCSPs.section consider much broader question, whether allowing fixed settriples costs triangles, necessarily include triples allowedJWP, defines tractable class VCSP instances.case CSP, four possible multi-sets costs ({0, 0, 0}, {0, 0, },{0, , }, {, , }) possible study 16 subsets set. But, giveninfinite set possible costs, Q+ Q+ , infinite number setstriples costs. Obviously, cannot consider sets. Therefore, consider460fiTractable Triangles Cross-Free Convexity Discrete Optimisationcases defined total order < , corresponding partition set possibletriples costs small number types triples.Let denote set possible cost types consideration. Let fixedset allowed costs. D, denote (S) (A allowed) set binaryVCSP instances whose costs lie triples costs triangles belongS.goal classify complexity (S) every D. problem (S)considered tractable polynomial-time algorithm solve intractableNP-hard.Proposition 3.3. Let arbitrary set costs set cost types.1. (S) tractable 0 S, (S 0 ) tractable.2. (S) intractable 0 S, (S 0 ) intractable.Remark 3.4. implicitly allow unary cost functions. fact, tractabilityresults work unary cost functions, NP-hardness results requireunary cost functions.Remark 3.5. consider problems unbounded domains; is, domain sizespart input. However, NP-hardness results obtained problemsfixed domain size.1 case CSPs, need domains size 3 prove NP-hardness,cases domains size 2 sufficient prove NP-hardness. Since binaryCSPs known tractable Boolean domains, VCSP trivially tractabledomains size 1, NP-hardness results tight.3.1 CSPsection, focus set possible costs = {0, }; is, ConstraintSatisfaction Problems (CSPs). consider four following types triples costs:Symbol<>0Costs{0, 0, }{0, , }{0, 0, 0}{, , }set possible cost types thus = {<, >, 0, }. Indeed, four cost typescorrespond precisely four possible multi-sets costs: {0, 0, 0}, {0, 0, }, {0, , }{, , }. dichotomy presented section therefore represents completecharacterisation complexity CSPs defined placing restrictions triples coststriangles.A{0,} (D) allows binary CSPs, A{0,} (D) intractable (Papadimitriou, 1994)unless domain size 2, case equivalent 2-SAT,well-known tractable class (Schaefer, 1978).1. words, considered problems fixed-parameter tractable (Downey & Fellows, 1999)domain size.461fiCooper & Zivny<, >, 0,<, ><, >, 0<, >,<, 0,>, 0,<, 0<,>, 0>,<>00,Figure 1: Complexity CSPs A{0,} (S), {<, >, 0, }.Proposition 3.6. A{0,} (D) intractable unless |D| 2.joint-winner property CSPs givesCorollary 3.7 (of Theorem 3.1). A{0,} ({<, 0, }) tractable.Proposition 3.8. A{0,} ({>, 0, }) tractable.Proof. Since < forbidden, two binary costs triangle zero third binarycost must also zero. words, assignment hv1 , a1 consistent hvi , ai{2, . . . , n}, i, j {1, . . . , n} 6= j, hvi , ai consistenthvj , aj i. Thus Singleton Arc Consistency, procedure enforcing Arc Consistencyevery variable-value pair (Rossi, van Beek, & Walsh, 2006), solves A{0,} ({>, 0, }).Proposition 3.9. A{0,} ({<, >, }) tractable.Proof. class trivial: instances least three variables solution finitecost, since triple costs {0, 0, 0} allowed.Proposition 3.10. A{0,} ({<, >, 0}) intractable unless |D| 2.Proof. straightforward encode 3-Colouring problem binary CSP. resultfollows fact 3-Colouring NP-hard triangle-free graphs (i.e. graphscontain K3 , complete graph 3 vertices, subgraph),derived two results work Lovasz (1973). (Indeed, 3-Colouring NP-hardeven triangle-free graphs degree 4; see Maffray & Preissmann, 1996.)triple costs {, , } cannot occur CSP encoding colouring trianglefree graph.462fiTractable Triangles Cross-Free Convexity Discrete OptimisationResults section, together Proposition 3.3, complete complexity classification, depicted Figure 1: white nodes represent tractable cases shaded nodesrepresent intractable cases.Theorem 3.11. |D| 3, class binary CSP instances defined A{0,} (S),{<, >, 0, }, intractable {<, >, 0} S.3.2 CSP Soft Unary Constraintssimple way convert classical CSPs optimisation problem allow soft unaryconstraints. framework includes well-studied problems Max-Ones Booleandomains (Creignou, Khanna, & Sudan, 2001; Khanna, Sudan, Trevisan, & Williamson,2001) non-Boolean domains (Jonsson, Kuivinen, & Nordh, 2008), Max-Solution (Jonsson & Nordh, 2008), Min-Cost-Hom (Takhanov, 2010).turns dichotomy given Theorem 3.11 remains valid even soft unaryconstraints allowed. case, intractable cases intractable evendomains size 2.Q+use notation A{0,}(S) represent set VCSP instances binarycosts {0, }, unary costs Q+ whose triples costs triangles belongS. words, consider VCSPs crisp binary constraints soft unaryconstraints.Q+Theorem 3.12. |D| 2, class binary CSP instances defined A{0,}(S),{<, >, 0, }, intractable {<, >, 0} S.Proof. tractability part theorem, suffices show tractability{<, >, }, {<, 0, } {>, 0, }, three maximal tractable sets case CSPshown Figure 1.Q+tractability A{0,}({<, 0, }) corollary Theorem 3.1 since jointwinner property allows unary soft constraints.Q+solve A{0,}({>, 0, }) polynomial time, establish Singleton Arc ConsistencyCSP instance corresponding binary constraints loop assignments first variable. assignment a1 variable v1 , determineoptimal global assignment extension hv1 , a1 simply choosing assignment ai variable vi least unary cost ci (ai ) among assignments hvi , aiconsistent hv1 , a1 i.Q+proof Proposition 3.9, instance A{0,}({<, >, }) tractable, sinceinstances least three variables solution finite cost.Sets intractable CSPs clearly remain intractable soft unary constraints allowed. However, want prove intractability even Boolean case;is, |D| = 2.QQ++({<, >, 0}) (and hence, Proposition 3.3, A{0,}({<, >intractability A{0,}, 0, })) follows fact Independent Set problem (Garey & Johnson, 1979)intractable even triangle free graphs. follows standard trick (Poljak,1974) replacing every edge P4 , path 4 vertices (this operation also known463fiCooper & Zivny2-subdivision). particular, graph G edges independent set size k2-subdivision G, denoted G0 , independent set size k + m.Note G0 triangle-free. instance G0 Independent Set problem triangleQ+free graphs encoded instance A{0,}({<, >, 0}) {0, 1} domainstraightforward way: variables correspond vertices; edge {i, j} yields cost functioncij (1, 1) = cij (x, y) = 0 (x, y) 6= (1, 1); ci (0) = 1 ci (1) = 0 every i. SinceQ+G0 triangle-free, constructed instance belongs A{0,}({<, >, 0}).3.3 Max-CSPsection, focus set possible costs = {0, 1}. well knownVCSP costs {0, 1} polynomial-time equivalent unweighted Max-CSP (norepetition constraints allowed) (Rossi, van Beek, & Walsh, 2006). four typestriples costs consider are:Symbol<>01Costs{0, 0, 1}{0, 1, 1}{0, 0, 0}{1, 1, 1}set possible cost types = {<, >, 0, 1}. Again, four costs typescorrespond precisely four possible multi-sets costs: {0, 0, 0}, {0, 0, 1}, {0, 1, 1},{1, 1, 1}. CSP, dichotomy result Max-CSP represents completecharacterisation complexity classes instances defined placing restrictionstriples costs triangles.<, >, 0, 1<, ><, >, 0<, >, 1<, 0, 1>, 0, 1<, 0<, 1>, 0>, 1<>010, 1Figure 2: Complexity Max-CSPs A{0,1} (S), {<, >, 0, 1}.A{0,1} (D) allows binary Max-CSPs, A{0,1} (D) intractable (Garey & Johnson,1979; Papadimitriou, 1994) unless domain size 1.464fiTractable Triangles Cross-Free Convexity Discrete OptimisationProposition 3.13. A{0,1} (D) intractable unless |D| 1.joint-winner property (Cooper & Zivny, 2011b) Max-CSPs givesCorollary 3.14 (of Theorem 3.1). A{0,1} ({<, 0, 1}) tractable.Proposition 3.15. A{0,1} ({<, >}) tractable.Proof. show A{0,1} ({<, >}) contains instances 5 variables, thus showingA{0,1} ({<, >}) trivially tractable. Consider instance A{0,1} ({<, >}) 6variables. Choose 6 arbitrary variables v1 , . . . , v6 6 domain values di Dvi ,1 6. Every cost either 0 1. well known (Goodman, 1959) difficultshow2 every 2-colouring edges K6 (the complete graph 6 vertices)monochromatic triangle. Therefore, triangle costs either {0, 0, 0} {1, 1, 1}.contradiction fact cost types < (i.e. {0, 0, 1}) > (i.e.{1, 1, 0}) allowed.Remark 3.16. ({>}) ({<, >}) tractable finite set costsdue similar Ramsey type argument: given = {0, 1, . . . , K 1}, nK Nevery complete graph G n vertices, n nK , every colouringedges G K colours, monochromatic triangle G. Hencefinitely many instances, stored look-up table. However, setcosts infinite (e.g. Q+ ), classes become intractable, shown next section.Proposition 3.17. A{0,1} ({>, 0, 1}) intractable unless |D| 1.Proof. Given instance Max-2SAT problem, show reduce {0, 1}valued VCSP instance A{0,1} ({>, 0, 1}). result follows well-knownfact Max-2SAT NP-hard (Garey & Johnson, 1979; Papadimitriou, 1994). Recallinstance Max-2SAT given set clauses length 2 n variablesx1 , . . . , xn goal find assignment maximises number clausesleast one true literal.order simplify notation, rather constructing VCSP instance A{0,1} ({>, 0, 1}) goal minimise total cost, construct instance A{0,1} ({<, 0, 1}) goal maximise total cost. implies allowed sets coststriangles {0, 0, 1}, {0, 0, 0}, {1, 1, 1}. Clearly, two problems polynomialtime equivalent.variable xi , create large number copies xji xi domain {0, 1},1 n 1 j . variable xi , new copies xi pairwise joinedequality-encouraging cost function h, h(x, y) = 1 x = h(x, y) = 0 otherwise.choosing large, assume copies xi assignedvalue optimal solutions. effectively ignore contribution2. Take arbitrary vertex v K6 every edge coloured either blue red. pigeonholeprinciple, v incident least 3 blue least 3 red edges. Without loss generality, considerformer case. Let v1 , v2 v3 three vertices incident three blue edges incident v.edges {v1 , v2 }, {v1 , v3 }, {v2 , v3 } blue, blue triangle. three edges red,red triangle.465fiCooper & Zivnycost functions, K = n2 , total cost. straightforward checktriangles involving new copies variables allowed costs.clause (l1 l2 ), l1 l2 literals, create variable zi domain{l1 , l2 }, 1 m. literal l domain zk : l positive literal l = xi ,introduce cost function g zk copy xji xi , g(l, 1) = 1 g(., .) = 0otherwise; l negative literal l = xi , introduce cost function g 0 zkcopy xji xi , g 0 (l, 0) = 1 g 0 (., .) = 0 otherwise.make sure sets costs triangles {0, 0, 1}, {0, 0, 0},{1, 1, 1}, also add cost functions f different clause variables zk zk0involving literal l, f (l, l) = 1 f (., .) = 0 otherwise. contributioncost functions zk zk0 , 1 k 6= k 0 m, less henceimportance large.Answering question whether resulting VCSP instance solutioncost K + pM equivalent determining whether original Max-2SAT instancesolution satisfying least p clauses. clause variable zk addscore assign value l zk literal l assigned true.Proposition 3.18. A{0,1} ({<, >, 0}) A{0,1} ({<, >, 1}) intractable unless |D|1.Proof. present reduction Max-Cut, well-known NP-hard problem (Garey &Johnson, 1979), NP-hard even triangle-free graphs (Lewis & Yannakakis, 1980).instance Max-Cut easily modelled Boolean {0, 1}-valued VCSP instance:every vertex graph represented variable Boolean domain {0, 1},every edge yields cost function f , f (x, y) = 1 x = f (x, y) = 0 x 6= y.Observe since original graph triangle-free, cannot triangle costs{1, 1, 1}. Therefore, constructed instance belongs A{0,1} ({<, >, 0}).A{0,1} ({<, >, 1}) case, instead minimising total cost, maximise totalcost instances A{0,1} ({<, >, 0}). Again, model instance Max-Cutproblem using Boolean variables, every edge yields cost function g, g(x, y) = 0x = g(x, y) = 1 x 6= (where case aim maximise total cost).constructed instance belongs A{0,1} ({<, >, 0}). (In fact, case needoriginal graph triangle-free.)Proposition 3.19. A{0,1} ({>, 0}) tractable.Proof. Let instance A{0,1} ({>, 0}). algorithm loops possibleassignments {hv1 , a1 i, hv2 , a2 i} first two variables. Suppose c12 (a1 , a2 ) = 1 (thecase c12 (a1 , a2 ) = 0 similar). Observe possible variable-value assignmentsvariables {hvi , bi | 3 n, b Di } uniquely split two sets L R that: (1)every hvi , bi L, c1i (a1 , b) = 1 c2i (a2 , b) = 0; every hvi , bi, hvj , ci L, cij (b, c) =0; (2) every hvi , bi R, c1i (a1 , b) = 0 c2i (a2 , b) = 1; every hvi , bi, hvj , ci R,cij (b, c) = 0; (3) every hvi , bi L hvj , ci R, cij (b, c) = 1. Ignoring unary costfunctions moment, find optimal assignment remaining n 2 variables, onedecide many variables vi , 3 n, assigned value b Dihvi , bi L. cost global assignment involving k variable-value assignments L1 + k + (n 2 k) + k(n 2 k) = n 1 + k(n 2 k). variables vi could466fiTractable Triangles Cross-Free Convexity Discrete Optimisationhappen hvi , bi L b Di hvi , ci R c Di . case,choose arbitrary value b xi minimum unary cost ci (b). optimalchoice whatever assignments variables xj (j {3, . . . , 1, + 1, . . . , n}).Assuming variables eliminated taking accountunary cost functions, function minimise given objective function (indrop constant term n 1):XXXX(xi )(n 2xi ) +wiL xi +wiR (1 xi )(each sum {3, . . . , n}), xi {0, 1} indicates whether vi assignedRvalue R L, wiL = min{ci (b) : b Di hvi , bi L}, similarlyPwiL = min{cP Ri (c) : cDi hvi , ci R}. objectivefunctionthusequalk(n2k)+wx+wi (1xi ),Pwhere, above, k =xi number assignments L. objective functionminimised either k = 0 k = n 2. followsPfrom thePfactcontribution unary cost functions objective functionwiL xi + wiR (1 xi )n 2 (since Max-CSP unary costs belong {0, 1}).greater value quadratic term k(n 2 k) values k {1, . . . , n 3},i.e. equal 0 n 2.optimal assignment involves k = 0 (respectively k = n 2) assignmentsL obtained simply choosing value ai (for > 2) minimum unary cost amongassignments hvi , ai R (respectively L).case c12 (a1 , a2 ) = 0, similar argument shows quadratic termobjective function 2(n 2 k) + k(n 2 k) = (k + 2)(n 2 k). alwaysminimised setting k = n 2 sum unary costs greatervalue quadratic term values k 6= n 2. optimal assignmentinvolves k = n 2 assignments L obtained simply choosing value ai (for> 2) minimum unary cost among assignments hvi , ai L.Proposition 3.20. A{0,1} ({>, 1}) tractable.Proof. Let instance A{0,1} ({>, 1}) without unary constraints; i.e.constraints binary. Observe every variable-value assignment hvi , ai, Di ,included zero-cost assignment-pairs involving one variable; i.e.one variable vj , cij (a, b) = 0 b Dj . order minimisetotal cost, maximise number zero-cost assignment-pairs. globalassignment, two zero-cost assignment-pairs involve variable, meansachieved reduction maximum matching problem, problemsolvable polynomial time (Edmonds, 1965b). build graph vertices givenvariables I, edge {vi , vj } Di b Djcij (a, b) = 0.complete proof, show unary constraints make problemdifficult solve; suffices perform preprocessing step reduction maximummatching. Let vi arbitrary variable I. ci (a) = 1 Di ,effectively ignore unary cost function ci since simply adds cost 1 solution.Otherwise, show Di ci (a) = 1 ignored. Take arbitraryassignment variables s(vi ) = a, ci (a) = 1. take b Di467fiCooper & Zivnyci (b) = 0. claim assignment s0 defined s0 (vi ) = b s0 (vj ) = s(vj )every j 6= increase total cost compared s. Since assignmenthvi , ai occur one zero-cost assignment-pair, two cases consider:(1) hvj , ci s(vj ) = c cij (a, c) = 0, claim holds sinceci (a) = 1 ci (b) = 0, overall cost decrease replace b; (2)exactly one j 6= cij (a, c) = 0 s(vj ) = c, cost s0 cannotincrease possible increase cost 1 assigning b vi compensatedunary cost function ci . Therefore, using reduction maximum matching,remove Di ci (a) = 1 keep Di ci (a) = 0.Remark 3.21. proof Proposition 3.20, shown instanceA{0,1} ({>, 1}) reduced instance maximum matching graphs (Edmonds,1965b). remark conversely, given graph G, maximum matching problem0G modelled VCSP instance IG{0,1} ({>, 1}).order vertices G arbitrarily call 1, 2, . . . , n. instance IGn variables v1 , . . . , vn , one vertex G. Let {n1 , . . . , nm } neighbours vertexG, degree vertex G; is, {j | {i, j} E(G)} = {n1 , . . . , nm }.define Di = {0, n1 , . . . , nm }.edge {i, j} E(G), < j, yields cij (j, i) = 1, remaining costs 0.follows definition IG (i) solutions IG maximum cost correspondmaximum matchings G; (ii) IG A{0,1} ({<, 0}). swapping costs 0 1,0get instance IG{0,1} ({>, 1}), whose solutions correspond matchings Gsolutions minimum cost correspond maximum matchings G.Results section, together Proposition 3.3, complete complexity classification, depicted Figure 2: white nodes represent tractable cases shaded nodesrepresent intractable cases.Theorem 3.22. |D| 2, class binary unweighted Max-CSP instances definedA{0,1} (S), {<, >, 0, 1}, intractable either {<, >, 0} S,{<, >, 1} S, {>, 0, 1} S.3.4 Finite-Valued VCSPsection, focus finite-valued VCSPs. words, consider setpossible costs = Q+ . Since infinite number triples costs, considertypes triples defined total order . study three different ways partitioningset triples costs distinct types.3.4.1 Classification respect Orderset possible cost types = {4, <, >, =}, four types definedfollowing table:Symbol4<>=Costs{, , }{, , }{, , }{, , }Remark, , , 6= 6= 6=, , <, , >468fiTractable Triangles Cross-Free Convexity Discrete Optimisation4, <, >, =4, <4, <, >4, <, =4, >, =<, >, =4, >4, =<, ><, =4<>=>, =Figure 3: Complexity finite-valued VCSPs AQ+ (S), {4, <, >, =}.AQ+ (D) allows finite-valued VCSPs, intractable even Boolean domain (Cohen, Cooper, Jeavons, & Krokhin, 2006) includes Max-SAT problemexclusive predicate (Papadimitriou & Yannakakis, 1991; Creignou, Khanna, &Sudan, 2001).Proposition 3.23. AQ+ (D) intractable unless |D| 1.joint-winner property (Cooper & Zivny, 2011b) finite-valued VCSPs givesCorollary 3.24 (of Theorem 3.1). AQ+ ({<, =}) tractable.Proposition 3.25. AQ+ ({4}) intractable unless |D| 1.Proof. show reduction Max-Cut, well-known NP-hard problem (Garey &Johnson, 1979). instance Max-Cut easily modelled Boolean finite-valuedVCSP instance: every vertex graph represented variable Booleandomain {0, 1}, every edge yields cost function f , f (x, y) = 1 x = f (x, y) =0 x 6= y. However, constructed instance belong AQ+ ({4}). Nevertheless,amend VCSP instance infinitesimal perturbations: occurrences cost0 replaced different numbers close 0, occurrences cost1 replaced different numbers close 1. since costs different,clearly instance belongs AQ+ ({4}).Proposition 3.26. AQ+ ({>}) intractable unless |D| 1.Proof. prove perturbation construction proof Proposition 3.17,shows intractability AQ+ ({>, =}). order simplify proof, similarlyproof Proposition 3.17, prove maximising total cost class AQ+ ({<})NP-hard.construction proof Proposition 3.17 add binary cost cij (a, b),< j, cij (a, b) equal 1. assume small (n < 1). simply469fiCooper & Zivnyensures triple costs {1, 1, 1} triangle assignments perturbedbecome {1 + i, 1 + i, 1 + j}.reduction Max-2SAT, literal l, let Cl set variablevalue assignments corresponding l (in xji zk variables). Recallbinary costs pairs assignments within Cl 1 binary costs pairsassignments distinct Cl , Cl0 0 VCSP encoding Max-2SATinstance. place arbitrary ordering literals l1 < l2 < < lr . addbinary cost two variable-value assignments whenever assignmentscorrespond literals li , lj < j. simply ensures triple costs {0, 0, 0}triangle assignments perturbed become {0 + i, 0 + i, 0 + j}.resulting VCSP instance AQ+ ({>}) correctly codes original Max-2SATinstance sufficiently small .Results section, together Proposition 3.3, complete complexity classification, depicted Figure 3: white nodes represent tractable cases shaded nodesrepresent intractable cases.Theorem 3.27. |D| 2, class binary finite-valued VCSP instances definedAQ+ (S), {4, <, >, =}, tractable {<, =}.3.4.2 Classification respect Minimum Costtractable classes A{0,1} ({>, 1}), A{0,1} ({>, 0}) A{0,1} ({<, >}) appear Figure 2,appear subclasses tractable classes AQ+ (S) identified Figure 3.due fact infinite set possible costs = Q+ , Figure 3 coverssubset infinite number possible restrictions triples costs triangles.consider triples costs allow us find generalisations three tractableclasses finite-valued VCSPs, considering restrictions depending relationshipcosts minimum maximum binary cost instance.start minimum cost. Without loss generality assumeminimum binary cost instance 0. consider following types triples costs:Symbol40<0>00Costs{, , 0}{0, 0, }{, , 0}{0, 0, 0}Remark, , > > 0, > 0, > 0simplicity presentation, consider remaining type triples costs,namely {, , } , , > 0. Since possible transform VCSP instanceequivalent instance non-zero costs adding constant > 0 binarycosts, clear allowing triples costs would render VCSP intractable.complexity combinations costs {40 , <0 , >0 , 0} shown Figure 4:white nodes represent tractable cases shaded nodes represent intractable cases.Proposition 3.28. AQ+ ({>0 , 0}) tractable.470fiTractable Triangles Cross-Free Convexity Discrete Optimisation40 , <0 , >0 , 040 , <040 , <0 , >040 , <0 , 040 , >0 , 0<0 , >0 , 040 , >040 , 0<0 , >0<0 , 040<0>00>0 , 0Figure 4: Complexity finite-valued VCSPs AQ+ (S), {40 , <0 , >0 , 0}.Proof. Observe either non-zero binary costs involve variable vk (i.e. cij = 0i, j 6= k) one distinct cost > 0 instance. (Otherwise,two distinct 6= non-zero costs , > 0 instance cij (a, b) =ckl (c, d) = distinct i, j, k, l, easy verify possible assigncosts cik (a, c), cil (a, d), cjk (b, c), cjl (b, d) triangles cost types >0 0.)implies AQ+ ({>0 , 0}) equivalent A{0,1} ({>, 0}) instantiationone variable.Corollary 3.29 (of Theorem 3.1). AQ+ ({<0 , 0}) tractable.Proposition 3.30. AQ+ ({40 , <0 , >0 }) tractable.Proof. Analogously Ramsey type argument proof Proposition 3.15,instance 5 variables must contain either triangle zero costs trianglethree non-zero costs hence cannot belong AQ+ ({40 , <0 , >0 }).Proposition 3.31. AQ+ ({<0 , >0 , 0}) intractable unless |D| 1.Proof. reduction Max-Cut triangle-free graphs proof Proposition 3.18Proposition 3.32. AQ+ ({40 , 0}) intractable unless |D| 1.Proof. shown VCSP remains intractable bipartite graphsBoolean domains (Cooper & Zivny, 2011b). Let instance partitionV1 ,V2 variables. Insignificantly small distinct costs added binarycosts variables V1 j V2 ensure triangles type 400.471fiCooper & ZivnyTheorem 3.33. |D| 2, class binary finite-valued VCSP instances definedAQ+ (S), {40 , <0 , >0 , 0}, tractable {<0 , 0}, {>0 , 0}{40 , <0 , >0 }.3.4.3 Classification respect Maximum CostLet Q+ cost consider following types triples costs:Symbol4M<M>MCosts{, , }{, , }{, M, }{M, M, }Remark, , < <, <, <Again, consider remaining type triples costs, namely {, , }, , < , since allowing triples costs renders VCSP intractable.{4M , <M , >M , } allowed combinations triples costs, clearlymaximum binary cost instance.4M , <M , >M ,4M , <M , >M4M , <M4M , <M ,4M , >M ,<M , >M ,4M , >M4M ,<M , >M<M ,4M<M>M>M ,Figure 5: Complexity finite-valued VCSPs AQ+ (S), {4M , <M , >M , }.complexity combinations costs {4M , <M , >M , } depicted Figure 5: white nodes represent tractable cases shaded nodes represent intractable cases.interesting case AQ+ ({>M , }), turns tractablereduction maximum weighted matching hence proper generalization classA{0,1} ({>, 1}).Proposition 3.34. AQ+ ({>M , }) tractable.Proof. proof similar proof Proposition 3.20. Consider instanceAQ+ ({>M , }), letij = min{ci (u) + cij (u, v) + cj (v) | u Di , v Dj }472fiTractable Triangles Cross-Free Convexity Discrete Optimisationminimum attained u = aji v = aij . assume, withoutloss generality, unary cost functions satisfy i, di Di ci (di ) = 0(by subtracting, necessary, min ci (u) unary cost function ci ). impliesij cij (di , dj ) .Suppose (bi , bj ) 6= (aji , aij ) cij (bi , bj ) < . replace (bi , bj )(aji , aij ) solution produce solution greater cost:binary costs involving bi bj necessarily maximal (i.e. equal ). Therefore, settingcij (bi , bj ) = change cost optimal solution instance I. followsassume one non-maximal binary cost cij (aji , aij )binary cost function cij .Consider weighted complete graph G vertices 1, . . . , n edge weights ij .Let MG maximum weighted matching G. Define solution x = hx1 , . . . , xnjai {i, j} MGxi =.di otherwisesolution well-defined since MG matching. weight MGXn(M ij ) =cost(x).2{i,j}MGhand, consider solution I. LetE(y) = {{i, j} | yi = aji yj = aij ij < }.E(y) matching G weightXn(M ij )cost(y).2{i,j}E(y)Since MG maximum weighted matching, deduce cost(y) cost(x). Hencex optimal solution.Tractability follows tractability maximum weighted matching problem (Edmonds, 1965a).Remark 3.35. seen proof Proposition 3.34 AQ+ ({>M , })tractable via reduction maximum weighted matching problem (Edmonds, 1965a).Similarly Remark 3.21, easy show that, conversely, instance maximum weighted matching problem modelled VCSP instance AQ+ ({>M , }).Corollary 3.36 (of Theorem 3.1). AQ+ ({<M , }) tractable.Proposition 3.37. AQ+ ({4M , <M , >M }) tractable.Proof. Analogously proof Proposition 3.30, instances contain 5 variables.Proposition 3.38. AQ+ ({<M , >M , }) intractable unless |D| 1.473fiCooper & ZivnyProof. reduction Max-Cut proof Proposition 3.18Proposition 3.39. AQ+ ({4M , }) intractable unless |D| 1.Proof. show intractability reduction VCSP bipartite graphsBoolean domains known NP-hard (Cooper & Zivny, 2011b). sufficesreplace zero costs reduction VCSP bipartite graphs givenproof Proposition 3.32 produce equivalent instance AQ+ ({4M , }).Theorem 3.40. |D| 2, class binary finite-valued VCSP instances definedAQ+ (S), {4M , <M , >M , }, tractable {<M , }{>M , } {4M , <M , >M }.3.5 General-Valued VCSPsection, focus general-valued VCSPs. words, consider completevaluation structure Q+ set possible costs . fact, complexity classificationscoincide classifications finite-valued VCSPs obtained Section 3.4.Theorem 3.27 applies = Q+ well. Indeed, hard cases remain intractableallow triangles (involving infinite costs), tractable case, AQ+ ({<, =}),remains tractable: AQ+ ({<, =}) tractable Theorem 3.1.Theorem 3.41. |D| 2, class binary general-valued VCSP instances definedAQ+ (S), {4, <, >, =}, tractable {<, =}.Similarly Theorem 3.33. Indeed, intractable cases remain intractable, tractablecases remain tractable.Theorem 3.42. |D| 2, class binary general-valued VCSP instances definedAQ+ (S), {40 , <0 , >0 , 0}, tractable {<0 , 0}, {>0 , 0}{40 , <0 , >0 }.Similarly Theorem 3.40. Indeed, intractable cases remain intractable, tractablecases remain tractable. (The class AQ+ ({>M , }) becomes trivially tractable =solution finite cost instances two variables.)Theorem 3.43. |D| 2, class binary general-valued VCSP instances definedAQ+ (S), {4M , <M , >M , }, tractable {<M , }{>M , } {4M , <M , >M }.4. Cross-Free Convex VCSPsSection 3, studied computational complexity several classes binary VCSPs.considered cases, joint-winner property (JWP) either one onetractable cases.section, generalise JWP cross-free convexity property (CFC).property defines novel tractable class describe efficient algorithm.Section 4.4, show neither two conditions definition CFC474fiTractable Triangles Cross-Free Convexity Discrete Optimisationproperty dropped without rendering problem NP-hard. Moreover, Section 4.5,present extension CFC Boolean domains. Section 4.6 devoted relatedidea overlaps studied previously SAT Max-SAT.4.1 Definition Examples Cross-Free Convex VCSPsfunction g : {0, . . . , s} Q+ called convex interval [l, u] g finite-valuedinterval [l, u] derivative g non-decreasing [l, u], i.e. g(m+2)g(m+1)g(m + 1) g(m) = l, . . . , u 2. brevity, often say g convexconvex interval [l, u] [0, s] infinite elsewhere (i.e. [0, l 1] [u + 1, s]).Two sets A1 , A2 said nested either disjoint one subset(i.e. A1 A2 = , A1 A2 A2 A1 ). A1 A2 nested,say overlap. say A1 A2 incompletely overlap A1 A2 overlapA1 A2 6= A.Sets A1 , . . . , Ar called laminar (Schrijver, 2003) (or hierarchically nested ; see Cooper& Zivny, 2011a) 1 i, j r, Ai Aj nested. Sets A1 , . . . , Arcalled cross-free every 1 i, j r, either Ai Aj , Ai Aj , Ai Aj = ,Ai Aj = (Schrijver, 2003). clear sets A1 , . . . , Ar laminar, A1 , . . . , Aralso cross-free.notational convenience, interpret solution x (i.e. assignment variablesv1 , . . . , vn ) VCSP instance set hvariable,valuei assignments {hvi , xi | xiDi = 1, . . . , n}.Ai set hvariable,valuei assignments VCSP instance P x solutionP, use notation |x Ai | represent number hvariable,valuei assignmentssolution x lie Ai .Definition 4.1 (Laminar/Cross-free convexity). Let P VCSP instance. Let A1 , . . . , Arlaminar (cross-free) sets hvariable,valuei assignments P. Let si numberdistinct variables occurring set hvariable,valuei assignments Ai . Instance Psatisfies laminar-free (cross-free) convexity property objective function Pg(x) = g1 (|x A1 |) + . . . + gr (|x Ar |) gi : [0, si ] Q+ (i = 1, . . . , r) convexinterval [li , ui ] [0, si ] gi (z) = z [0, li 1] [ui + 1, si ].remark functions gi Definition 4.1 cost functions associatedconstraints.follows definition laminar convexity property implies cross-freeconvexity property.Remark 4.2. Observe addition unary cost function cannot destroy laminar cross-free convexity property. hvariable,valuei assignmenthvj , ai add singleton Ai = {hvj , ai} necessarily either disjointsubset set Ak (and furthermore corresponding function gi : {0, 1} Q+trivially convex).give special case cross-free convexity property, setsdisjoint thus trivially cross-free.475fiCooper & ZivnyExample 4.3 (Value-based soft GCC). Global Cardinality Constraint (GCC),introduced Regin (1996), generalisation AllDifferent constraint (Regin,1994). Given set n variables, GCC specifies domain value lowerbound ld upper bound ud number variables assigned value d.AllDifferent constraint special case GCC ld = 0 ud = 1 every d.Soft versions GCC considered van Hoeve, Pesant, & Rousseau (2006).value-based soft GCC minimises number values givenbound. show value-based soft GCC satisfies cross-free convexity property.every domain value D, let Ad = {hvi , di : = 1, . . . , n}. Clearly, A1 , . . . ,disjoint, = |D|. every d, letld < ldgd (m) =0ld udud > udfollows readily definition gd sequence gd (m + 1) gd (m), =0, . . . , n 1, sequence 1, . . . , 1, 0, . . . , 0, 1, . . . , 1. Therefore, every d, gdnon-decreasing derivative hence convex.Example 4.4 (Nested value-based soft GCC). able nest GCC constraints usefulmany staff assignment problems hierarchy (e.g. senior manager-managerpersonnel, foreman-worker, senior nurse-nurse) (Zanarini & Pesant, 2007). mightwant impose soft global cardinality constraints day prefer10 15 people work, least 5 managers amongexactly 1 senior manger, convex penalties described Example 4.3constraints hold.Suppose constraints VCSP instance consist soft GCC constraintspairwise nested sets variables S1 , . . . , St . Let Aid = {hx, di : x Si }. Clearly,sets assignments Aid cross-free and, shown Example 4.3, cost functionscorresponding soft GCC constraint convex.main result section following theorem:Theorem 4.5. VCSP instance P satisfying cross-free convexity propertysolved polynomial time.Firstly, present algorithm solve VCSPs satisfying laminar convexity property, followed reduction cross-free case laminar case. Secondly, giveproof polynomial-time complexity algorithm.4.2 Algorithm Laminar Convex VCSPscall sets Ai (i = 1, . . . , r) assignment-sets. assume assignment-setsAi distinct, since Ai = Aj two sets merged replacing twofunctions gi ,gj sum (which necessarily also convex). Without loss generality,assume assignment-set consisting variable-value assignments present,corresponding function constant zero function. (If corresponding function476fiTractable Triangles Cross-Free Convexity Discrete Optimisationgi constant zero function, add constant term gi (n) objectivefunction.) useful construction described below. say assignmentset Ak father assignment-set Ai minimal assignment-set properlycontains Ai , i.e. Ai Ak @Aj Ai Aj Ak . follows definitionlaminarity Ak unique hence father relation defines tree. Moreover,definition laminarity, every variable vi P every Di ,unique minimal assignment-set containing hvi , ai.construct directed graph GP whose minimum-cost integral flows value none-to-one correspondence solutions P. GP following nodes:1. source node s;2. variable node vi (i = 1, . . . , n) variable P;3. assignment node hvi , di (d Di , = 1, . . . , n) possible variable-valueassignment P;4. assignment-set node Ai (i = 1, . . . , r) assignment-set P;5. sink node t, identify assignment-set consisting variablevalue assignments.GP following arcs:1. = (s, vi ) variable vi P; demand capacity given d(a) =c(a) = 1 (this forces flow exactly 1 variable node vi ); weightfunction given w(a) = 0;2. = (vi , hvi , di) variables vi Di ; d(a) = 0; c(a) = 1; w(a) = 0;3. = (hvi , di, Aj ) variables vi Di , Aj minimalassignment-set containing hvi , di; d(a) = 0; c(a) = 1; w(a) = 0;4. assignment-set Ai father Aj , arc Ai Aj weightfunction gi , demand d(a) = li capacity c(a) = ui .Clearly, GP constructed P polynomial time. proveminimum-cost flows f value n GP one-to-one correspondence solutionsP and, furthermore, cost f equal cost P correspondingsolution.feasible flows value n since n arcs (s, vi ) leaving sourcedemand capacity equal 1. Flows GP necessarily correspond assignmentunique value xi variable vi since flow 1 node vi must traversenode hvi , xi unique xi Di . remains show every assignmentx = {hv1 , x1 i, . . . , hvn , xn i} feasible (i.e. whose cost P finite),corresponding minimum-cost feasible flow f GP cost g(x) = g1 (|x A1 |) + . . . + gr (|xAr |).arc incoming outgoing hvi , di GP , let f (a) = 1= xi 0 otherwise. construction, assignment-set node Ai GP exactly477fiCooper & Zivnyone outgoing arc father assignment-set. flow fa arc Ai fatherassignment-set Aj uniquely determined assignment values variablessolution x. Trivially, thereforeP minimum-cost flow corresponding assignmentx. cost flow f clearly gi (|x Ai |) corresponds precisely costassignment x.proved correspondence cost solutions P costminimum-cost flows, follows algorithm, given P constructs GPfinds minimum-cost flow, correct.Example 4.6. Let P VCSP instance 4 variables v1 , v2 , v3 , v4 , D1 = D2 = D3 =D4 = {0, 1}, assignment-sets Ai , 1 8 given Figure 6. cost functionsgi , 1 8 arbitrary convex functions.network GP corresponding instance P shown Figure 7: demands capacities square brackets corresponding layer graph, weightsarcs without numbers 0. non-zero weight functions arcsassignment-sets; arcs corresponding cost functions gi , 1 7. SetA8 identified sink t. Minimum-cost feasible flows GP correspond assignments P modulo addition constant g8 (4) (since 4 variablesA8 consists variable-value assignments). bold red edges represent flowf corresponding assignment v1 = v2 = 1 v3 = v4 = 0 total costg1 (1) + g2 (0) + g3 (2) + g4 (1) + g5 (0) + g6 (1) + g7 (3). Finding minimum-cost flow GPequivalent finding optimal solution P.4.3 Laminar VCSPs Cross-Free VCSPsalternative way expressing definition cross-freeness every 1 i, j r,one Ai (A \ Aj ), (A \ Ai ) Aj , Ai Aj , (A \ Ai ) (A \ Aj ) empty. follows directlyA1 , . . . , Ar cross-free A1 , . . . , Ar , (A \ Ai ) 1 r.show reduce VCSP instance cross-free convexity propertyinstance satisfying laminar convexity property.First show without loss generality, assume every Ai satisfies|Ai | b|A|/2c, 1 r. Let Ai arbitrary |Ai | > b|A|/2c. pointedabove, without loss generality Aj , 1 j r, Aj = \ Ai . (IfAj among A1 , . . . , Ar , add Aj corresponding convex costfunction constant zero cost function. would double numberassignment-sets.)Let hi defined hi (y) = gi (n y) let gj0 = gj + hi . Clearly gj0 convex,furthermoregj0 (|Aj x|) = gj (|Aj x|) + hi (|Aj x|)= gj (|Aj x|) + gi (n |Aj x|)= gj (|Aj x|) + gi (|A x| |Aj x|)= gj (|Aj x|) + gi (|Ai x|).eliminate set Ai cost function gi replacing gi , gj single costfunction gj0 .478fiTractable Triangles Cross-Free Convexity Discrete OptimisationA8A1A6A2hv1 , 0i hv3 , 0ihv3 , 1i hv2 , 0iA5hv4 , 1iA7A3A4hv2 , 1i hv4 , 0ihv1 , 1iFigure 6: Laminar sets assignments instance P Example 4.6.[1, 1][0, 1][0, 1][li , ui ][li , ui ]hv1 , 0ihv1 , 1iA1v1hv2 , 0iv2hv2 , 1ig1A2g2A3v3hv3 , 0iv4hv3 , 1iA4A5A6g6A7g7g3g4g5hv4 , 0ihv4 , 1iFigure 7: Network GP corresponding VCSP P Example 4.6.479fiCooper & ZivnySince sets half size A, Ai Aj = 1 i, j r,necessarily Aj = \ Ai . However, case, using argument above,pair complementary sets Ai Aj , eliminate Ai cost function gireplacing gi , gj single cost function gj0 . Consequently, resulting sets A1 , . . . , Arlaminar.Complexity Let P VCSP instance n variables, domain sized, r laminar assignment-sets Ai . maximum number distinct nonoverlapping sets Ai 2nd 1 since sets assignments Ai form tree ndleaves (corresponding single hvariable,valuei assignments) non-leaf nodesleast two sons. Thus r = O(nd). network GP n0 = O(n + nd + r) = O(nd)vertices arcs. GP built O((nd)2 ) time top-down manner, addingassignment-sets inverse order size (which ensures assignment-set alwaysinserted father) using table [hv, ai]=smallest assignment-set (in treebuilt) containing hv, ai.network n0 vertices m0 arcs capacities U , minimum convexcost flow problem solved time O((m0 log U )SP (n0 , m0 )), SP (n0 , m0 )time compute shortest directed path network n0 vertices m0 edges (Ahuja,Magnanti, & Orlin, 2005). Using Fibonacci heaps (Fredman & Tarjan, 1987), SP (n0 , m0 ) =O(m0 + n0 log n0 ) = O(nd log(nd)), since number vertices n0 arcs m0O(nd). maximum capacity U network GP n. Hence optimalsolution cross-free convex VCSP determined O((nd log n)(nd log(nd))) =O((nd)2 (log n)(log n + log d)) time.Remark 4.7. previous work (Cooper & Zivny, 2011b), proved special caseTheorem 4.5 functions gi , 1 r, non-decreasing assignment setslaminar. (Previously, laminar convexity property non-decreasing functions gi ,1 r, called non-overlapping convexity property; also, assignment-setscalled assignment-cliques; see Cooper & Zivny, 2011b.)presented algorithm similar algorithm Cooper & Zivny (2011b) basedfinding minimum-cost flow network. main difference requiresingle arc pair nodes corresponding cost function giarbitrary convex function (which necessarily non-decreasing). running timealgorithm thus better running time algorithm previouswork (Cooper & Zivny, 2011b), O(n3 d2 ). improvement mostly duefact new construction involves O(nd) arcs opposed O((nd)2 ) arcsprevious work (Cooper & Zivny, 2011b). Moreover, algorithm solves strictly biggerclass problems compared previous result (Cooper & Zivny, 2011b). Overall,solve faster!Remark 4.8. remark since construction projection-safe (Lee & Leung,2009), used Soft Global Arc Consistency cross-free convex constraints.Remark4.9. VCSP instance P objective function form g(x) =Pri=1 gi (|xAi |), follows definitions test polynomial time whetherP satisfies cross-free convexity property; is, whether gi convex Ai480fiTractable Triangles Cross-Free Convexity Discrete Optimisationcross-free, 1 r. fact, described algorithm requires assignment-setsAi functions gi given explicitly.conference version work (Cooper & Zivny, 2011a), mentionedrecognition problem open problem. fact, problem easily shown intractable.Given arbitrary VCSP instance P, always exists cross-free convex instance P 0whose optimal solution coincides fixed optimal solution P. Therefore, finding P 0impossible polynomial time unless P=NP otherwise arbitrary VCSP instance Pcould solved polynomial time using Theorem 4.5.4.4 Maximality Cross-Free Convexitysection shows relaxing either convexity cross-freeness (in fact, laminarity)Definition 4.1 leads intractability.Theorem 4.10. class VCSP instances whose objective function form g(x) =g1 (|x A1 |) + . . . + gr (|x Ar |) functions gi convex, sets assignmentsAi may overlap, NP-hard, even |Ai | 2 {1, . . . , r} variablesBoolean.Proof. suffices demonstrate polynomial-time reduction well-known NP-hardproblem Max-2SAT (Garey & Johnson, 1979). Max-2SAT clause l1 l2 (where l1 , l2literals) equivalent convex cost function g(|x {l1 , l2 }|) g(0) = 1g(1) = g(2) = 0. therefore possible code instance Max-2SAT using convexcost functions (on possibly overlapping sets assignments).Theorem 4.11. class VCSP instances whose objective function form g(x) =g1 (|x A1 |) + . . . + gr (|x Ar |) sets assignments Ai laminar,functions gi necessarily convex, NP-hard even |Ai | 3 {1, . . . , r}variables Boolean.Proof. give polynomial-time reduction well-known NP-complete problem3SAT (Garey & Johnson, 1979). Let I3SAT instance 3SAT clauses.constraint AllEqual(l1 , l2 , l3 ) (where l1 , l2 , l3 literals) equivalent (nonconvex) cost function g(|x {l1 , l2 , l3 }|) g(0) = g(3) = 0 g(1) = g(2) = .variable v I3SAT , use following gadget Gv based non-overlappingAllEqual constraints produce multiple copies v1 , . . . , vm variable v multiplecopies w1 , . . . , wm negation v: Gv consists constraints AllEqual(ui , vi , yi )(i {1, . . . , m}), AllEqual(yi , wi , ui+1 ) (i {1, . . . , 1}), AllEqual(ym , wm , u1 ),variables ui , yi occur gadget Gv . easy verify Gv imposesv1 = . . . = vm = w1 = . . . = wm . Furthermore, variables vi , wi occur negativelyGv . replace ith clause I3SAT clause positive variable vreplaced ith copy vi negative variable v replaced ith copy wiv. produces laminar VCSP instance equivalent I3SAT (but whose costfunctions convex).Note NP-hardness reduction proof Theorem 4.11 requires assignmentsets size 3. leaves open complexity laminar (and cross-free) non-convexVCSPs assignment-sets size 2.481fiCooper & Zivnyfollowing result shows complexity cross-free non-convex VCSPsassignment-sets size 2 domains size polynomial-time equivalent cross-freenon-convex VCSPs assignment-sets size 2 domains size 3.Proposition 4.12. Cross-free VCSPs assignment-sets size 2 domainssize > 3 polynomial-time equivalent cross-free VCSPs assignment-setssize 2 domains size 3.Proof. First observe VCSPs assignment-sets size 2, laminaritycross-freeness almost identical. extra condition definition cross-freeness(for A1 , A2 A, A1 A2 6= A1 A2 = A) irrelevant instances3 variable-value assignments. Hence need prove equivalence laminarVCSPs.Let v` D` = {a1 , . . . , ak }, k > 3. replace v` k variablesv`,1 , . . . , v`,k respective domains D`,1 = {1, a1 }, D`,i = {0, 1, ai } = 2, . . . , k 1,D`,k = {0, ak }. (Here assume, without loss generality, 0 1 differentai , = 1, . . . , k.) Moreover, introduce k1 new assignment-sets {hv`,i , 1i, hv`,i+1 , 0i}= 1, . . . , k 1 associated convex function g defined g(1) = 0 g(0) =g(2) = . Finally, assignment-set involving variable-value assignment hv` , ai(for {1, . . . , k}), assignment replaced hv`,i , ai i.function g applied assignment-sets {hv`,i , 1i, hv`,i+1 , 0i} ensurespossible finite-cost assignments variables v`,1 , . . . , v`,k form 1, . . . , 1, ai , 0, . . . , 0.Since exactly one variables v`,1 , . . . , v`,k assigned value D` , one-toone correspondence optimal solutions transformed instance originalinstance.tractability cross-free non-convex VCSPs assignment-sets size 2domains size 3 (or larger, Proposition 4.12) left open problem.case cross-free assignment-sets size 2 Boolean domains showntractable Theorem 4.21 Section 4.6.4.5 Renamable Boolean Cross-Free Convex VCSPssection extend class cross-free convex VCSPs allow renaming certainvariables case Boolean domains. section consider BooleanVCSPs.begin illustrating notion renaming means example. First,require notation. Cost function AtMostr (A) returns 0 x contains r assignments set assignments A, AtMostr (A) returns 1 otherwise. Similarly,cost function AtLeastr (A) returns 0 x contains least r assignments setassignments A, AtLeastr (A) returns 1 otherwise. Note cost functions AtLeast1AtMostr , r = |A| 1, convex [0, |A|].Example 4.13. Let P Max-SAT instance given CNF form following clauses:(a b c),(c d),(c e),482(a e).fiTractable Triangles Cross-Free Convexity Discrete OptimisationClearly, clause literals written AtLeast1 (A) VCSP encodinginstance. Notice that, example, first two clauses overlapping. However,replace second clause equivalent constraint AtMost1 ({c, d}).gives us equivalent problem following constraints:(a b c),AtMost1 ({c, d}),(c e),(a e).P expressed instance satisfying cross-free convexity property crossfree sets assignments {a, b, c}, {c, d}, {c, d, e}, {a, e}.Example 4.13 leads following definitions:Definition 4.14. Given valued constraint form cost function g(|x A|),set Boolean assignments (i.e. literals) size m, define renamingvalued constraint, set Boolean assignments = {` | ` A}, valuedconstraint g 0 (|x A|) = g(m |x A|) = g(|x A|).function g 0 (z) = g(m z) clearly convex g convex.Definition 4.15. Boolean VCSP instance P objective function g1 (|x A1 |) +. . . + gr (|x Ar |) renamable cross-free convex subset constraints Pwhose renaming results equivalent VCSP instance P 0 cross-free convex.Theorem 4.16. class renamable cross-free convex VCSPs recognisable solvable polynomial time.Proof. show recognition polynomial-time simple reduction 2-SAT,well-known problem solvable polynomial time (Garey & Johnson, 1979). Let PBoolean VCSP instance r constraints ith constraint (i = 1, . . . , r)gi (|x Ai |) convex function gi . constraint P, Boolean variablereni indicating whether ith constraint renamed. pair distincti, j {1, . . . , r}, add clauses length 2 follows:1. Ai Aj incompletely overlap add constraint reni renj (since mustrename one two constraints);2. Ai Aj incompletely overlap add constraint reni renj (to avoid introducing overlap renaming).easy see solutions constructed 2-SAT instance correspond validrenamings P give rise equivalent VCSP instance satisfying cross-freeconvexity property. Tractability solving resulting renamed instance follows directlyTheorem 4.5.4.6 Knuth-Nested VCSPsorder relate work previous work, section present different classtractable VCSPs considers sets variables (rather sets assignments)allows overlaps size 1. show known tractable class extendedMax-SAT VCSPs. apply result show special caseassumption convexity cross-free convex VCSPs dropped.483fiCooper & ZivnyDefinition 4.17. Given VCSP instance P variables V = {v1 , . . . , vn } constraintsscopes C = {C1 , . . . , Cm }, define incidence graph P IP = (V (IP ), E(IP )),V (IP ) = V C E(IP ) = {{vi , Cj } | vi Cj }.Definition 4.18. VCSP instance P called Knuth-nested variables Plinearly ordered v1 , . . . , vn IP together edges {{vi , vi+1 } | 1n} {vn , v1 } allows planar drawing circle v1 , . . . , vn , v1 bounds outer face.P called Knuth-co-nested constraint scopes P linearly ordered C1 , . . . , CmIP together edges {{Ci , Ci+1 } | 1 m} {Cm , C1 } allows planardrawing circle C1 , . . . , Cm , C1 bounds outer face.Knuth described linear-time algorithm solving Knuth-nested SAT instances (Knuth,1990). Kratochvl Krivanek generalised Knuths result provided linear-time algorithm recognising solving Knuth-nested Knuth-co-nested SAT/Max-SATinstances (Kratochvl & Krivanek, 1993). Henderson Masters thesis showed severaldifferent proofs results, including proof Knuth-nested Knuth-co-nestedSAT/Max-SAT instances treewidth three (Biedl & Henderson, 2004; Henderson, 2005), hence solvable polynomial time via standard dynamic programmingapproach.Theorem 4.19. class Knuth-nested Knuth-co-nested VCSP instances constraints bounded arity recognisable solvable polynomial time.Proof. Recognition reduced, via simple reduction work KratochvlKrivanek (1993), planarity testing problem (Hopcroft & Tarjan, 1974).Following Hendersons argument (2005, p. 21), easy show P Knuthnested Knuth-co-nested, incidence graph IP P treewidth 3.VCSP domains size d, constraints arity k incidence graphIP clearly equivalent binary VCSP constraint graph IP domainssize dk . result follows fact VCSP instanceconstraint graph bounded treewidth solvable polynomial time (Bertele & Brioshi,1972).Note class Knuth-nested (Knuth-co-nested) VCSP instances (in fact, evenSAT instances) cannot generalised follows work Lichtenstein (1982)satisfiability conjunction two Knuth-nested formulas NP-complete.show class Knuth-nested/Knuth-co-nested instances sectionincomparable class cross-free convex instances defined Section 4.1 evenspecial case Boolean formulas. Moreover, also show class Knuthnested/Knuth-co-nested instances incomparable class renamable Booleancross-free convex instances defined Section 4.5.Example 4.20. SAT instance = (x y) (y z) (y w) Knuth-nestedKnuth-co-nested, neither cross-free renamable cross-free.following SAT instance neither Knuth-nested Knuth-co-nested, crossfree (in fact laminar): (x z) (x u v) (y u w) (z v w).484fiTractable Triangles Cross-Free Convexity Discrete Optimisationturn attention cross-free VCSPs possibly non-convex cost functionsassignment-sets size 2. show tractability Boolean VCSPinstances, i.e. instances 2-element domains, follows Theorem 4.19. Recallcase convex cost functions tractable Theorem 4.5, that, Theorem 4.11,case non-convex cost functions intractable assignments sets size 3.Theorem 4.21. cross-free Boolean VCSP instance assignment-sets size2 solvable polynomial time.Proof. mentioned proof Proposition 4.12, first observe VCSPsassignment-sets size 2 (or fixed size matter) laminarity cross-freenessalmost identical. extra condition definition cross-freenes (for A1 , A2 A,A1 A2 6= A1 A2 = A) irrelevant instances 3 variable-valueassignments. Hence need prove tractability laminar Boolean VCSPsassignment-sets size 2.show Boolean VCSP laminar assignment-sets size 2Knuth-nested. Tractability follows Theorem 4.19.Take arbitrary variable, instance v1 . show order <variables satisfying requirements Knuth-nested property. Since |D1 | = 2,two assignment-sets, say Ai Aj , containing (different) assignments v1 .since assignment-sets size two, one assignmentAi , say assignment variable vk . define v1 < vk . Similarly, oneassignment Aj , say assignment variable vl , define vl < v1 . Continuingreasoning variables vl vk , get another variable smaller (in order< building) vl another variable bigger vk . stop eventually:either variables, assignment-set size 1, variabledomain size 1, last considered assignment-set contains assignments smallestbiggest variables (in order <). easy observe casesplanar drawing required Definition 4.18. variables left, continueway.Next show cross-free VCSPs 3-element domains assignments setssize 2 may neither Knuth-nested Knuth-co-nested.Example 4.22. Take four variables x, y, z, w domain {0, 1, 2}, sets A1 ={hx, 0i, hy, 0i}, A2 = {hy, 1i, hz, 0i}, A3 = {hx, 1i, hz, 1i}, A4 = {hy, 2i, hw, 0i}. instancecross-free (in fact laminar), neither Knuth-nested Knuth-co-nested.5. Conclusionsstudied hybrid reasons tractability optimisation problems castValued Constraint Satisfaction Problems (VCSPs), equivalently Markov Random Fields(MRFs) Min-Sum problems. reasons tractability followrestriction functions (such submodularity) restrictionstructure instance (such bounded treewidth).Firstly, studied binary VCSPs (also known pairwise MRFs). CSPMax-CSP case, obtained complete dichotomy concerning tractability485fiCooper & Zivnyproblems defined placing restrictions possible combinations binary coststriangles variable-value assignments. case finite-valued general-valuedVCSP, obtained complete dichotomies respect equivalence classesnaturally follow total order valuation structure. shownjoint-winner property maximum (weighted) matching non-trivial tractableclasses.Secondly, studied non-binary VCSPs. presented novel class optimisation problems solved efficiently using flow techniques. new classdefined problems convex functions cross-free family variable-value assignments. shown neither two conditions sufficienttractability. Moreover, Boolean domains, managed extend new classusing idea renamability.left open one special case, namely tractability cross-free non-convexVCSPs assignment-sets size 2 domains size 3. (Assignmentsets size 3 make problem intractable even Boolean domains, assignment-setssize 2 Boolean domains shown tractable.)AcknowledgmentsMartin Cooper supported ANR Projects ANR-10-BLAN 0210 0214. StanislavZivny supported Junior Research Fellowship University College, Oxford.ReferencesAhuja, R., Magnanti, T., & Orlin, J. (2005). Network Flows: Theory, Algorithms,Applications. Prentice Hall/Pearson.Bertele, U., & Brioshi, F. (1972). Nonserial dynamic programming. Academic Press.Biedl, T., & Henderson, P. (2004). Nested SAT Graphs Treewidth Three. Tech. rep.CS-2004-70, University Waterloo.Bistarelli, S., Montanari, U., & Rossi, F. (1997). Semiring-based Constraint SatisfactionOptimisation. Journal ACM, 44 (2), 201236.Boros, E., & Hammer, P. L. (2002). Pseudo-Boolean optimization. Discrete Applied Mathematics, 123 (1-3), 155225.Bulatov, A., Krokhin, A., & Jeavons, P. (2005). Classifying Complexity Constraintsusing Finite Algebras. SIAM Journal Computing, 34 (3), 720742.Cohen, D., & Jeavons, P. (2006). complexity constraint languages. Rossi, F., vanBeek, P., & Walsh, T. (Eds.), Handbook Constraint Programming. Elsevier.Cohen, D. A. (2003). New Class Binary CSPs Arc-Constistency DecisionProcedure. Proceedings 9th International Conference PrinciplesPractice Constraint Programming (CP03), Vol. 2833 Lecture Notes ComputerScience, pp. 807811. Springer.Cohen, D. A., Cooper, M. C., Green, M., & Marx, D. (2011). guaranteeing polynomiallybounded search tree size. Proceedings 17th International Conference486fiTractable Triangles Cross-Free Convexity Discrete OptimisationPrinciples Practice Constraint Programming (CP11), Vol. 6876 LectureNotes Computer Science, pp. 160171. Springer.Cohen, D. A., Cooper, M. C., & Jeavons, P. G. (2008). Generalising submodularity Hornclauses: Tractable optimization problems defined tournament pair multimorphisms.Theoretical Computer Science, 401 (1-3), 3651.Cohen, D. A., Cooper, M. C., Jeavons, P. G., & Krokhin, A. A. (2006). ComplexitySoft Constraint Satisfaction. Artificial Intelligence, 170 (11), 9831016.Cooper, M. C., Jeavons, P. G., & Salamon, A. Z. (2010). Generalizing constraint satisfactiontrees: Hybrid tractability variable elimination. Artificial Intelligence, 174 (910), 570584.Cooper, M. C., & Zivny, S. (2011a). Hierarchically nested convex VCSP. Proceedings17th International Conference Principles Practice Constraint Programming (CP11), Vol. 6876 Lecture Notes Computer Science, pp. 187194.Springer.Cooper, M. C., & Zivny, S. (2011b). Hybrid tractability valued constraint problems.Artificial Intelligence, 175 (9-10), 15551569.Cooper, M. C., & Zivny, S. (2011c). Tractable triangles. Proceedings 17th International Conference Principles Practice Constraint Programming (CP11),Vol. 6876 Lecture Notes Computer Science, pp. 195209. Springer.Creignou, N., Khanna, S., & Sudan, M. (2001). Complexity Classification Boolean Constraint Satisfaction Problems, Vol. 7 SIAM Monographs Discrete MathematicsApplications. SIAM.Dalmau, V., Kolaitis, P. G., & Vardi, M. Y. (2002). Constraint Satisfaction, BoundedTreewidth, Finite-Variable Logics. Proceedings 8th International Conference Principles Practice Constraint Programming (CP02), Vol. 2470Lecture Notes Computer Science, pp. 310326. Springer.Dechter, R., & Pearl, J. (1988). Network-based Heuristics Constraint Satisfaction Problems. Artificial Intelligence, 34 (1), 138.Dechter, R. (2003). Constraint Processing. Morgan Kaufmann.Downey, R., & Fellows, M. (1999). Parametrized Complexity. Springer.Edmonds, J. (1965a). Maximum Matching Polyhedron 0, 1 Vertices. JournalResearch National Bureau Standards, 69 B, 125130.Edmonds, J. (1965b). Paths, trees, flowers. Canadian Journal Mathematics, 17,449467.Feder, T., & Vardi, M. Y. (1998). Computational Structure Monotone MonadicSNP Constraint Satisfaction: Study Datalog Group Theory. SIAMJournal Computing, 28 (1), 57104.Fredman, M. L., & Tarjan, R. E. (1987). Fibonacci heaps uses improved networkoptimization algorithms. Journal ACM, 34 (3), 596615.487fiCooper & ZivnyGarey, M. R., & Johnson, D. S. (1979). Computers Intractability: Guide TheoryNP-Completeness. W.H. Freeman.Geman, S., & Geman, D. (1984). Stochastic relaxation, Gibbs distributions,bayesian restoration images. IEEE Transactions Pattern Analysis MachineIntelligence, 6 (6), 7210741.Goodman, A. W. (1959). Sets Acquaintances Strangers Party.American Mathematical Monthly, 66 (9), 778783.Grohe, M. (2007). complexity homomorphism constraint satisfaction problemsseen side. Journal ACM, 54 (1), 124.Henderson, P. (2005). Planar Graphs Partial k-Trees. Masters thesis, UniversityWaterloo.Hooker, J. (2007). Integrated Method Optimization. Springer.Hopcroft, J. E., & Tarjan, R. E. (1974). Efficient planarity testing. Journal ACM,21 (4), 549568.Jeavons, P. G. (1998). Algebraic Structure Combinatorial Problems. TheoreticalComputer Science, 200 (1-2), 185204.Jonsson, P., Kuivinen, F., & Nordh, G. (2008). MAX ONES Generalized Larger Domains.SIAM Journal Computing, 38 (1), 329365.Jonsson, P., Kuivinen, F., & Thapper, J. (2011). Min CSP Four Elements: MovingBeyond Submodularity. Proceedings 17th International Conference Principles Practice Constraint Programming (CP11), Vol. 6876 Lecture NotesComputer Science, pp. 438453. Springer.Jonsson, P., & Nordh, G. (2008). Introduction maximum solution Problem.Complexity Constraints, Vol. 5250 Lecture Notes Computer Science, pp. 255282. Springer.Khanna, S., Sudan, M., Trevisan, L., & Williamson, D. (2001). approximabilityconstraint satisfaction problems. SIAM Journal Computing, 30 (6), 18631920.Knuth, D. E. (1990). Nested satisfiability. Acta Informatica, 28 (1), 16.Kolmogorov, V. (2011). Submodularity tree: Unifying l] -convex bisubmodularfunctions. Proceedings 36th International Symposium Mathematical Foundations Computer Science (MFCS11), Vol. 6907 Lecture Notes ComputerScience, pp. 400411. Springer.Kolmogorov, V., & Zivny, S. (2012). complexity conservative valued CSPs.Proceedings 23rd Annual ACM-SIAM Symposium Discrete Algorithms(SODA12), pp. 750759. SIAM. Full version available arXiv:1110.2809.Kratochvl, J., & Krivanek, M. (1993). Satisfiability co-nsted formulas. Acta Informatica,30 (4), 397403.Lauritzen, S. L. (1996). Graphical Models. Oxford University Press.488fiTractable Triangles Cross-Free Convexity Discrete OptimisationLee, J. H.-M., & Leung, K. L. (2009). Towards efficient consistency enforcement globalconstraints weighted constraint satisfaction. Proceedings 21st InternationalJoint Conference Artificial Intelligence (IJCAI09), pp. 559565.Lewis, J. M., & Yannakakis, M. (1980). node-deletion problem hereditary propertiesNP-complete. Journal Computer System Sciences, 20 (2), 219230.Lichtenstein, D. (1982). Planar formulae uses. SIAM Journal Computing,11 (2), 329343.Lovasz, L. (1973). Coverings colorings hypergraphs. Proceedings 4thSoutheastern Conference Combinatorics, Graph Theory Computing, pp. 312.Maffray, F., & Preissmann, M. (1996). NP-completeness k-colorability problemtriangle-free graphs. Discrete Mathematics, 162 (1-3), 313317.Marx, D. (2010). Tractable hypergraph properties constraint satisfaction conjunctive queries. Proceedings 42nd ACM Symposium Theory Computing(STOC10), pp. 735744.Meseguer, P., Rossi, F., & Schiex, T. (2006). Soft constraints. Rossi, F., van Beek, P.,& Walsh, T. (Eds.), Handbook Constraint Programming. Elsevier.Minoux, M. (1984). polynomial algorithm minimum quadratic cost flow problems.European Journal Operational Research, 18, 377387.Minoux, M. (1986). Solving integer minimum cost flows separable convex cost objectivepolynomially. Mathematic Programming Studies, 26, 237239.Montanari, U. (1974). Networks Constraints: Fundamental properties applicationspicture processing. Information Sciences, 7, 95132.Papadimitriou, C. (1994). Computational Complexity. Addison-Wesley.Papadimitriou, C. H., & Yannakakis, M. (1991). Optimization, Approximation, Complexity Classes. Journal Computer System Sciences, 43 (3), 425440.Poljak, S. (1974). note stable sets colorings graphs. Commentationes Mathematicae Universitatis Carolinae, 15 (2), 307309.Regin, J.-C. (1994). filtering algorithm constraints difference CSPs. Proceedings12th National Conference AI (AAAI94), Vol. 1, pp. 362367.Regin, J.-C. (1996). Generalized Arc Consistency Global Cardinality Constraint.Proceedings 13th National Conference AI (AAAI96), Vol. 1, pp. 209215.Rossi, F., van Beek, P., & Walsh, T. (Eds.). (2006). Handbook Constraint Programming. Elsevier.Schaefer, T. J. (1978). Complexity Satisfiability Problems. Proceedings 10thAnnual ACM Symposium Theory Computing (STOC78), pp. 216226. ACM.Schiex, T., Fargier, H., & Verfaillie, G. (1995). Valued Constraint Satisfaction Problems:Hard Easy Problems. Proceedings 14th International Joint ConferenceArtificial Intelligence (IJCAI95), pp. 631637.Schrijver, A. (2003). Combinatorial Optimization: Polyhedra Efficiency, Vol. 24Algorithms Combinatorics. Springer.489fiCooper & ZivnyTakhanov, R. (2010). Dichotomy Theorem General Minimum Cost Homomorphism Problem. Proceedings 27th International Symposium TheoreticalAspects Computer Science (STACS10), pp. 657668.van Hoeve, W. J., Pesant, G., & Rousseau, L.-M. (2006). global warming: Flow-basedsoft global constraints. Journal Heuristics, 12 (4-5), 347373.Wainwright, M. J., & Jordan, M. I. (2008). Graphical models, exponential families,variational inference. Foundations Trends Machine Learning, 1 (1-2), 1305.Werner, T. (2007). Linear Programming Approach Max-Sum Problem: Review.IEEE Transactions Pattern Analysis Machine Intelligence, 29 (7), 11651179.Zanarini, A., & Pesant, G. (2007). Generalizations global cardinality constraint hierarchical resources. Proceedings 4th International Conference IntegrationAI Techniques Constraint Programming Combinatorial Optimization Problems (CPAIOR07), Vol. 4510 Lecture Notes Computer Science, pp.361375. Springer.490fiJournal Artificial Intelligence Research 44 (2012) 633-708Submitted 11/11; published 08/12Logical DifferenceLightweight Description Logic ELBoris KonevMichel Ludwigkonev@liverpool.ac.ukmichel.ludwig@liverpool.ac.ukDepartment Computer ScienceUniversity Liverpool, UKDirk Waltherdirk.walther@upm.esDepartamento Inteligencia Artificial, Facultad de InformaticaUniversidad Politecnica de Madrid, SpainFrank Wolterwolter@liverpool.ac.ukDepartment Computer ScienceUniversity Liverpool, UKAbstractstudy logic-based approach versioning ontologies. view, ontologiesprovide answers queries vocabulary interest. differencetwo versions ontology given set queries receive different answers.investigate approach terminologies given description logic EL extendedrole inclusions domain range restrictions three distinct types queries:subsumption, instance, conjunctive queries. three cases, present polynomialtime algorithms decide whether two terminologies give answers queriesgiven vocabulary compute succinct representation difference nonempty. present implementation, CEX2, developed algorithms subsumptioninstance queries apply distinct versions Snomed CT NCI ontology.1. IntroductionTerminologies lightweight ontologies used provide common vocabularydomain interest together descriptions meaning terms builtvocabulary relationships them. used areas medicalinformatics, bio-informatics, semantic web capture domain semantics promote interoperability. Terminologies often large complex. example, widelyused medical terminology Snomed CT (Systematized Nomenclature Medicine ClinicalTerms) contains 300 000 term definitions (IHTSDO, 2008). Another exampleNational Cancer Institute ontology (NCI) consisting 60 000 axioms (Golbeck, Fragaso, Hartel, Hendler, Oberhaler, & Parsia, 2003). Engineering, maintaining,using terminologies complex laborious task, practically unfeasiblewithout appropriate tool support. article, focus principled logic-basedapproach support terminology versioning.Dealing multiple versions information unit nothing new computing, version control well established computer technology. Although modern versioncontrol systems provide range operations including support collaborative development, branching, merging, etc., operations extend rely basic operationsc2012AI Access Foundation. rights reserved.fiKonev, Ludwig, Walther, & Wolterdetecting representing differences versions. paper, focusbasic problem versioning.need versioning support recognised ontology research communityontology users, large number approaches tools developed.review currently existing support ontology versioning, distinguish three approachesdescribe according difference ontologies compute:1. versioning based syntactic difference (syntactic diff);2. versioning based structural difference (structural diff);3. versioning based logical difference (logical diff).syntactic diff underlies existing version control systems used software development (Conradi & Westfechtel, 1998) (such as, example, RCS, CVS, SCCS). workstext files represents difference versions blocks text present oneversion another, ignoring meta-information document. observedalready work Noy Musen (2002), ontology versioning cannot rely purelysyntactic diff operation since many syntactic differences (e.g., order ontology axioms)affect semantics ontologies. Therefore, ontology versioning based syntacticdifference essentially limited comparing rather informal change logs (Oliver, Shahar,Shortliffe, & Musen, 1999).structural diff extends syntactic diff taking account informationstructure ontologies. suggested dealing structured hierarchical documents UML diagrams, database schemas, XML documents (see, e.g.,Ohst, Welle, & Kelter, 2003, references within). ontologies, main characteristicstructural diff regards structured objects, is-a taxonomy (Noy & Musen, 2002), set RDF triplets (Klein, Fensel, Kiryakov, & Ognyanov,2002) set class defining axioms (Redmond, Smith, Drummond, & Tudorache, 2008;Jimenez-Ruiz, Cuenca Grau, Horrocks, & Llavori, 2011). Changes ontologies mostlydescribed terms structural operations, example, adding deleting class, extending class, renaming slots, moving class one place hierarchy another,adding deleting axiom, class renaming, etc.; sometimes basic logical propertiesontologies, e.g., equivalence different structural forms concepts, also takenaccount (Palma, Haase, Corcho, & Gomez-Perez, 2009; Jimenez-Ruiz et al., 2011). Ontology versioning based structural diff form available current ontologyeditors ontology management systems either natively plugins (Noy & Musen,2002; Klein et al., 2002; Jimenez-Ruiz et al., 2011).Though helpful, structural diff still deficiency unambiguous semantic foundation syntax dependent. Moreover, tailored towardsapplications ontologies based induced concept hierarchy (or mildextension it), capture modern applications ontology based data access (OBDA) (Poggi, Lembo, Calvanese, Giacomo, Lenzerini, & Rosati, 2008; Lutz, Toman,& Wolter, 2009) ontologies used provide user-oriented view data634fiThe Logical Difference Lightweight Description Logic ELmake accessible via queries formulated solely language ontology withoutknowledge actual structure data.1logical diff recently introduced (Konev, Walther, & Wolter, 2008;Kontchakov, Wolter, & Zakharyaschev, 2010) completely abstracts representation ontology. Here, ontology regarded set axioms formulated logicallanguage formal unambiguous semantics. view, ontologies provideanswers queries vocabulary interest. Typical queries include subsumptionqueries concepts and, ontology used access instance data, instanceconjunctive queries. logical diff motivated view. two versions ontologygive answers class queries relevant application domain, maydeemed difference regardless syntactic structural form; queriesproducing different answers versions may considered characterisationdifference itself. way one can, example, define exactly differences visiblequerying instance data exactly differences expressed subsumptionsconcepts.make approach work practice, least two problems addressed:ontology languages classes queries computational complexityeven detecting two ontology versions differ certain vocabulary leastone exponential harder ontology classification sometimes undecidable;even computational complexity increase, searching differencesontologies within certain vocabulary requires techniques different used standard reasoning (Lutz, Walther, & Wolter, 2007; Lutz& Wolter, 2010; Cuenca Grau, Horrocks, Kazakov, & Sattler, 2008).set queries producing different answers two versions empty,typically infinite and, therefore, cannot presented user such. Thus,techniques succinctly characterise elements present userrequired.aim paper provide first steps toward solutions problemsterminologies (aka classical TBoxes) given description logic ELHr extendsdescription logic EL underlying OWL 2 EL profile role inclusions domainrange restrictions (Baader, Brandt, & Lutz, 2008). main contributions follows:1. argued syntax-dependence regarded advantage rather deficiencycontext versioning (Goncalves, Parsia, & Sattler, 2011; Jimenez-Ruiz et al., 2011). example, Jimenez-Ruiz et al. argue logical equivalence ontologies permissive: evenO0 strongest assumption semantic point view conflicts may still exist. mightresult presence incompatible annotations (statements act comments carrylogical meaning), mismatch modelling styles; example, may written simple languageOWL 2 EL profile contain = (A v B u C), O0 may contain = (B C v A).Even though , explicit use negation disjunction means O0 outside EL profile.agree Jimenez-Ruiz et al. Goncalves et al. various applicationsstructural rather logical difference appropriate. Even syntactic diff applications ontologyversioning. practice, see logic-based approaches complementary structural approaches.interesting analysis NCI versions taking account structural logical differences givenwork Goncalves et al.635fiKonev, Ludwig, Walther, & Woltersubsumption, instance, conjunctive queries, present polynomial-time algorithmsdecide whether two ELHr -terminologies give different answers queryrespective class queries given signature concept role names (noteuse terms signature vocabulary synonymously).Besides polynomial-time decision procedure detecting differences, also developsuccinct presentation (typically infinite) difference. presentation computed polynomial time well.present two different types polynomial-time algorithms deciding existencelogical differences terminologies computing succinct representationit: first type algorithms conceptually transparent keeps twoinput terminologies separate reduces (a substantial part of) difference probleminstance checking problem ABox. algorithms are, however, sufficientlyefficient large inputs. example, substantial performance problems occurcomputing differences versions Snomed CT joint signature sinceconstructed ABox typically quadratic size input terminologies. secondvariant algorithms, based dynamic programming, efficient practice.developed detail acyclic ELHr -terminologies.present implementation, CEX2, based second type algorithmscomputes succinct representation difference acyclic ELHr -terminologiesconcept instance query case. addition, prototype implementationABox-based algorithm used estimate efficiency.important tool investigation, present description logics, ELranELran,u,u , capture subsumption differences instance query difference ELHr -terminologies. result presented general ELHr -TBoxes can,therefore, exploited future work versioning general ELHr -TBoxes.present experiments using CEX2 illustrate efficiency algorithmspotential applications terminologies Snomed CT NCI. plugin Protegediscussed. CEX2 extends functionality first version CEX (Konev, Walther, &Wolter, 2008) OwlDiff plugin (Kremen, Smd, & Kouba, 2011), implementsalgorithms developed Konev, Walther, Wolter. Based Snomed CT, alsoinvestigate performance ABox-based algorithms practice.paper based on, extends work Konev, Walther, Wolter (2008).improve readability, number proofs deferred appendix.2. PreliminariesLet NC , NR , NI countably infinite mutually disjoint sets concept names, rolenames, individual names. EL-concepts C built according ruleC :=|>| C uD|r.C,NC , r NR , C, range EL-concepts. set ELHr -inclusionsconsistsconcept inclusions C v D, ran(r) v ran(r) u C v D,636fiThe Logical Difference Lightweight Description Logic ELconcept equations C D,role inclusions r v s,C EL-concepts r, NR . ELHr -TBox finite set ELHr inclusions. Inclusions form ran(r) v ran(r)uC v also referred rangerestrictions, inclusions form r.> v referred domain restrictions.ELHr -TBox called ELHr -terminology concept inclusions equationsformv C C,ran(r) v C,r.> v C,NC r NR , C EL-concept C 6= >, C 6= > u >, etc.,concept name occurs left-hand side. Note that, concept inclusionsform r.> v C, concept r.> often denoted dom(r). terminology acyclic(or unfoldable) process exhaustively substituting definitions place definedconcept names terminates. example, terminology contains concept inclusionMother v hasMother.Motheracyclic. Formally, consider relation concept names settingB exists ELHr -inclusion form C v C Boccurs C. terminology acyclic transitive closure +irreflexive.description logic, instance data represented ABox assertions form >(a),A(a) r(a, b), a, b NI , NC , r NR . ABox non-empty finiteset ABox-assertions. said singleton ABox contains exactly one ABoxassertion. obj(A) denote set individual names A. knowledge base K (KB)pair (T , A) consisting TBox ABox A. Assertions form C(a)r(a, b), a, b NI , C EL-concept, r NR , called instance assertions. Noteinstance assertions form C(a) C concept name C = >occur ABoxes.semantics ELHr given interpretations = (I , ), domainnon-empty set, function mapping concept name subset AI, role name r binary relation rI , individual nameelement aI . extension C concept C defined induction follows:>I(C u D)I(r.C)Iran(r)I:=:=:=:=C DI{d | e C : (d, e) rI }{d | e : (e, d) rI }satisfiesconcept inclusion C v D, symbols |= C v D, C DI ;637fiKonev, Ludwig, Walther, & Wolterconcept equation C D, symbols |= C D, C = DI ;role inclusion r v s, symbols |= r v s, rI sI ;assertion C(a), symbols |= C(a), aI C ,assertion r(a, b), symbols |= r(a, b), (aI , bI ) rI .say interpretation model TBox (ABox A) |=( A). ELHr -inclusion follows TBox every model model, symbols |= . |= used denote follows empty TBoxsometimes write r vT |= r v s. instance assertion follows KB (T , A)every individual name occurs also occurs obj(A) every model (T , A)model , symbols (T , A) |= . important ways querying ELHr -TBoxesKBssubsumption: check whether |= , ELHr -inclusion TBox ,instance checking: check whether (T , A) |= , instance assertion KB(T , A),conjunctive query answering.define latter, call first-order formula q(~x) conjunctive query form~y (~x, ~y ), conjunction expressions A(t), NC , r(t1 , t2 ), r NR ,t, t1 , t2 drawn NI sequences variables ~x ~y . Let ~x = x1 , . . . , xk . Letinterpretation mapping ~x ~y . Set (a) = aI obj(A).say vector ~a = a1 , . . . , ak -match q(~x) satisfies followingconditions:(t) AI every conjunct A(t) ;((t1 ), (t2 )) rI every conjunct r(t1 , t2 ) ;(xi ) = aIi 1 k.set |= q[~a] if, if, exists ~a -match q(~x) I. Let(T , A) KB. sequence ~a members obj(A) certain answer q(~x)KB (T , A), symbols (T , A) |= q(~a), |= q[~a], every model (T , A).three types querying ELHr -TBoxes studied extensively. complexitysubsumption instance checking PTime (Baader et al., 2008). combinedcomplexity answering Boolean conjunctive queries (i.e., deciding whether (T , A) |= qconjunctive query q without free variables) coNP-complete (Rosati, 2007) datacomplexity PTime (Rosati, 2007). Information reasoners subsumption checkingELHr found work Delaitre Kazakov (2009), Kazakov, Krotzsch,Simancik (2011), Mendez Suntisrivaraporn (2009). Lutz et al. (2009) presentapproach efficient conjunctive query answering ELHr .638fiThe Logical Difference Lightweight Description Logic EL2.1 Normal Formoften convenient consider normalised ELHr -terminologies. Let ELHr terminology concept name. Callprimitive NC \ ({A NC | C } {A NC | v C });pseudo-primitive NC \ {A NC | C }.Note concept names occur primitive pseudo-primitive .Call concept name non-conjunctive pseudo-primitive existsconcept form r.C r.C . Otherwise, called conjunctive. Thus, conjunctive if, if, exists concept name BB exist C1 , . . . , Cn , n 2, C1 u u Cn . Let Xfinite setconcepts. say concept F conjunction concepts X Fform DX D. X called conjunct F and, concept name,called atomic conjunct F . sometimes write F instead X.ELHr -terminology normalised consists ELHr -inclusions followingform:r.B, F , A, B concept names F non-empty conjunctionconcept names every conjunct B 0 F non-conjunctive ;E v r.B, E v r.>, E v F , B concept name, E either conceptname, form s.>, ran(s), F non-empty conjunction conceptnames every conjunct B 0 F non-conjunctive .following lemma shows, ELHr -terminology normalised yieldingmodel conservative extension original terminology.Lemma 1. every ELHr -terminology , one construct polynomial time normalised terminology 0 polynomial size |T | sig(T ) sig(T 0 ), 0 |= ,every model exists model J 0 = J X = X Jevery X sig(T ). Moreover, 0 acyclic acyclic.Normalised terminologies sense defined minor modification normalised terminologies defined Baader (2003). straightforward extensionproof given Baader provided appendix.2.2 Canonical Modeldefine canonical model, IK , ELHr -knowledge bases K. IK constructedpolynomial time gives answers instance queries K; i.e., IK |= if,if, K |= , instance assertion . construction similar canonicalmodel introduced Lutz et al. (2009).Let sub(T ) denote set subconcepts concepts used , rol(T ) setrole names occurring . Take fresh individual names xran(r),D every r rol(T )sub(T ) setNIaux := {xran(r),D | r rol(T ) sub(T )}.639fiKonev, Ludwig, Walther, & Wolterdefine generating interpretation WK KB K = (T , A) follows:W KAW Kr WKaWK:= obj(A) NIaux ;:= {a obj(A) | K |= A(a)} {xran(r),D NIaux | |= ran(r) u v A};:= {(a, b) obj(A) obj(A) | s(a, b) |= v r}{(a, xran(s),D ) obj(A) NIaux | K |= s.D(a) |= v r}{(xran(s),D , xran(s0 ),D0 ) NIaux NIaux | |= ran(s) u v s0 .D0 , |= s0 v r};:= a, obj(A).path WK finite sequence d0 r1 d1 rn dn , n 0, d0 obj(A) and, < n,WK(di , di+1 ) ri+1. use paths(WK ) denote set paths WK . p paths(WK ),tail(p) denotes last element dn p.canonical model IK knowledge base K restriction WK domainelements path WK tail d. following result summarisesmain properties IK .Theorem 2. Let K = (T , A) ELHr -KB.1. IK model K;2. IK computed polynomial time size K;3. xran(s),D IK obj(A), C EL-concept C = ran(r),K |= C(a) if, if, aIK C IK .|= ran(s) u v C if, if, xran(s),D C IK .proof Theorem 2 given appendix. follows Point 3 IK givesanswers instance queries K itself.3. Logical Differencesection, introduce three notions logical difference TBoxesderived notion -inseparability. Intuitively, logical difference two TBoxes T1T2 set relevant formulas T1 |= T2 6|= viceversa. course, formulas relevant depends application domain. manyapplications subsumptions concepts relevant, TBoxes employedaccess instance data, answers instance even conjunctive queries relevantwell. addition, applications large-scale terminologies Snomed CTNCI typically small subset vocabulary terminology relevant.Thus, meaningful notion logical difference take account formulasgiven certain signature interest, signature subset NC NR .Given concept, role, concept inclusion, TBox, ABox, query E, denote sig(E)signature E, is, set concept role names occurring it. call E-concept, -concept inclusion, -TBox, -ABox, -query, respectively, sig(E) .Similarly, EL -concept C EL-concept sig(C) ELHr -inclusionELHr -inclusion sig() .first notion logical difference introduce corresponds applicationssubsumptions relevant.640fiThe Logical Difference Lightweight Description Logic ELDefinition 3 (-concept difference). -concept difference ELHr -TBoxes T1T2 set cDiff (T1 , T2 ) ELHr -inclusions T1 |= T2 6|= .say T1 T2 -concept inseparable, symbols T1 CT2 , cDiff (T1 , T2 ) =cDiff (T2 , T1 ) = .-concept inseparability T1 T2 means T1 replaced T2application concerned ELHr -inclusions.2 following exampleshows, however, -concept inseparable terminologies give different answersinstance query data.Example 4. Let T1 = {ran(r) v A1 , ran(s) v A2 , B A1 u A2 }, T2 = , = {r, s, B}.One show T1 T2 -concept inseparable. However, -ABox ={r(a, c), s(b, c)} (T1 , A) |= B(c) (T2 , A) 6|= B(c).take account differences TBoxes relevant TBoxesused access instance data, consider -instance difference.Definition 5 (-instance difference). -instance difference TBoxes T1 T2set iDiff (T1 , T2 ) pairs form (A, ), -ABox -instanceassertion (T1 , A) |= (T2 , A) 6|= . say T1 T2 -instanceinseparable, symbols T1 T2 , iDiff (T1 , T2 ) = iDiff (T2 , T1 ) = .contrast ELHr , shown Lutz Wolter (2010) EL-TBoxesdifference -concept inseparability -instance inseparability.paper extend result ELHr -TBoxes without range restrictions (the proofgiven Corollary 37):Theorem 6. Let T1 T2 ELHr -TBoxes without range restrictions signature.T1 CT2 if, if, T1 T2 .Sometimes, instance queries sufficiently expressive, conjunctive queriesemployed. case, following notion difference appropriate.Definition 7 (-query-difference). -query difference TBoxes T1 T2set qDiff (T1 , T2 ) pairs form (A, q(~a)), -ABox, q(~x) conjunctive query, ~a tuple individual names (T1 , A) |= q(~a)(T2 , A) 6|= q(~a). say T1 T2 -query inseparable, symbols T1 q ,qDiff (T1 , T2 ) = qDiff (T2 , T1 ) = .observed Lutz Wolter (2010) already, even EL -instance inseparabilityimply -query inseparability. following simple example.Example 8. Let T1 = {A v r.B}, T2 = , = {A, B}. T1 T2 -instanceinseparable, -query inseparable. Consider -ABox = {A(a)}-query q = x.B(x). (T1 , A) |= q (T2 , A) 6|= q.2. refer reader conclusion paper brief discussion claim.641fiKonev, Ludwig, Walther, & Woltershown Lutz Wolter (2010) Example 8 essentially situationdifference -instance inseparability -query inseparabilityEL: two notions become equivalent EL universal role admitted instancequeries (e.g., Example 8, conjunctive query x.B(x) corresponds instance queryu.B(a) universal role u). contrast, ELHr subtle differencesinstance query case.Example 9. Let T1 = {A v s.>, v r1 , v r2 }, T2 = {A v r1 .> u r2 .>}, ={A, r1 , r2 }. T1 T2 -concept -instance inseparable,-query inseparable. show latter, let = {A(a)} let q = x(r1 (a, x) r2 (a, x)).(T1 , A) |= q (T2 , A) 6|= q.seen -concept inseparability imply -instance inseparability-instance inseparability imply -query inseparability. converseimplications, however, hold:Lemma 10. ELHr -TBoxes T1 T2 signatures :T1 q T2T1 T2T1 CT2 .Proof. first implication follows observation every instance queryregarded conjunctive query. second implication, note first v rcDiff (T1 , T2 ), ({s(a, b)}, r(a, b)) iDiff (T1 , T2 ). let C v cDiff (T1 , T2 ).One construct -ABox AC individual EL-concepts D0 :(T , AC ) |= D0 (a) if, if, |= C v D0 (cf. Lemma 36). Thus (AC , D(a))iDiff (T1 , T2 ).introduced three notions difference ELHr -TBoxes, investigate two problems: (i) detect whether difference two ELHr terminologies and, so, (ii) represent differences.follows assume fresh symbols used normalised form terminologies occur signature compute differenceterminologies. obtain following lemma direct corollary Lemma 1.Lemma 11. ELHr -terminologies T1 , T2 normalised forms T10 , T20defined Lemma 1, following hold:cDiff (T1 , T2 ) = cDiff (T10 , T20 );iDiff (T1 , T2 ) = iDiff (T10 , T20 );qDiff (T1 , T2 ) = qDiff (T10 , T20 ).on, unless stated otherwise, consider normalised terminologies only.642fiThe Logical Difference Lightweight Description Logic EL4. Case EL-Terminologiesinvestigating logical difference ELHr -terminologies, illustrate mainideas behind proofs considering -concept difference EL-terminologies.EL-terminology ELHr -terminology consisting EL-inclusions only, is, conceptinclusions form v C concept equations form C. startobservation even acyclic EL-terminologies T1 T2 cDiff (T1 , T2 )contains inclusions least exponential size only. Thus, searching witnessinclusions cDiff (T1 , T2 ), one deal case witness inclusionsleast exponential size.Example 12. ConsiderT1 = {A0 v B0 , A1 Bn } {Bi+1 r.Bi u s.Bi | 0 < n}T2 = {A1 v F0 } {Fi v r.Fi+1 u s.Fi+1 | 0 < n}= {A0 , A1 , r, s}. concept inclusion cDiff (T1 , T2 ) minimal size givenCn v A1 ,C0 = A0 Ci+1 = r.Ci u s.Ci , 0.Clearly, Cn exponential size. Note, however, use structure sharing definesize Cn number subconcepts, Cn polynomial size.derive basic properties EL-terminologies using sequent calculus.4.1 Proof System ELderive basic properties EL Gentzen-style sequent calculus presented Hofmann (2005); see Figure 1. calculus operates sequents form C v D,C, EL-concepts; symbol v treated syntactic separator. derivation(or, equivalently, proof ) sequent C v finite rooted tree whose nodes labelledsequents, whose root labelled C v D, whose leaves labelled axioms(instances Ax AxTop) whose internal nodes labelled resultapplication one inference rules labels children. lengthderivation number rule applications derivation.Example 13. Let = {A B1 uB2 , F v B1 }. derivation sequent r.(F uB2 ) vr.A shown below. root derivation labelled r.(F u B2 ) v r.Atwo leaves B1 v B1 B2 v B2 , respectively.(Ax)B1 v B1(PDefL)(Ax)F v B1B2 v B2(AndL1)(AndL2)F u B2 v B1F u B2 v B2(AndR)F u B2 v B1 u B2(DefR)F u B2 v(Ex)r.(F u B2 ) v r.A643fiKonev, Ludwig, Walther, & WolterCvC(Ax)Cv>(AxTop)CvE(AndL1)C uDCvE CvD(AndR)C vDuECA v(DefL)AvDCvD(Ex)r.C v r.Dv CA(DefR)DvACA v(PDefL)AvDDvE(AndL2)C uDCAv CAFigure 1: Gentzen-style proof system EL-terminologies.Notice basic calculus Hofmann (2005) considers EL without constant >terminologies without concept inclusions. take care >, addedrule (AxTop), (PDefL) rule representing inclusions form v C. Cutelimination, completeness, correctness shown straightforward extensionproof given Hofmann.terminology concepts C, D, write ` C v if, if, existsproof C v calculus Figure 1.Theorem 14 (Hofmann). EL-terminologies concepts C, D, holds|= C v if, if, ` C v D.apply calculus derive description syntactic form concepts C|= C v D, non-conjunctive .Lemma 15. Let normalised EL-terminology, r role name, concept nameEL-concept.1. Assume|=lAi u1inlrj .Cj v A,1jmpseudo-primitive , Ai concept names 1 n, Cj ELconcepts 1 j m, m, n 0. exists Ai , 1 n,|= Ai v A.2. Assume|=l1inlAi urj .Cj v r.D,1jmAi concept names 1 n, Cj EL-concepts 1 j m,m, n 0.exists Ai , 1 n, |= Ai v r.Dexists rj , 1 j m, rj = r |= Cj v D.644fiThe Logical Difference Lightweight Description Logic ELProof. use Theorem 14. First, prove Point 1. Let C = 1in Ai u 1jm rj .Cjassume |= C v A, pseudo-primitive . Let proof C v A.Note that, since pseudo-primitive (and concept name), inspecting formconclusions inference rules, one see root derivationderived either Ax, AndL1, AndL2, DefL, PDefL. showexists Ai , 1 n, |= Ai v induction n + m, i.e.number conjuncts C. easy see n + 1 6|= > v definitionterminologies .base case n + = 1 trivial: root derived oneAx, DefL, PDefL; so, conclude C = A1 ; i.e. n = 1, = 0, setAi = A1 .Assume n + > 1. root derived either AndL1AndL2. cases, premise used application either inference rulesequent C 0 v either C = C 0 u C = u C 0 EL-concept D.Thus, C 0 contains less conjuncts C (but still least one). also conclude|= C 0 v holds Theorem 14. applying induction hypothesis, henceexists concept name Ai conjunct C 0 |= Ai v A. Finally, stillnote Ai also conjunct C.prove Point 2. Let C = 1in Ai u 1jm rj .Cj assume |= C v r.D.Let proof C v r.D. Note due form right-hand side sequentC v r.D, rule used derive root one Ax, AndL1,AndL2, DefL, PDefL, Ex. prove either exists Ai , 1 n,|= Ai v r.D, exists rj , 1 j m, rj = r |= Cj vinduction n + again. Similarly above, n + 1.n + = 1, rule used derive root one Ax, DefL,PDefL, Ex. two subcases:root derived DefL PDefL: n = 1, = 0 C = A1 ; i.e.|= Ai v r.D Ai = A1 .root derived Ax Ex: n = 0, = 1, C = r1 .C1 ,r1 = r. C1 = D, obviously |= C1 v holds. Otherwise, rule Exused derive root ` C1 v holds, implies |= C1 v D.Thus, case, rj = r |= Cj v holds j = 1.case n+m > 1 proved induction analogously proof Point 1 above.apply Lemma 15 elements cDiff (T1 , T2 ).Theorem 16 (Primitive witness EL). Let T1 T2 EL-terminologiessignature. cDiff (T1 , T2 ), either C v v member cDiff (T1 , T2 ),sig() concept name C, EL-concepts occurring .Proof. Let = C v cDiff (T1 , T2 ). proof induction constructionD. 6= > T2 |= C v >. = D1 u D2 , one C v Di , = 1, 2,cDiff (T1 , T2 ) apply induction hypothesis. = r.D1 then, Lemma 15,645fiKonev, Ludwig, Walther, & Woltereither (i) exists conjunct C, concept name, T1 |= v D, (ii)exists conjunct r.C1 C T1 |= C1 v D1 .case (i) follows T2 6|= v otherwise T2 |= C v C v 6cDiff (T1 , T2 ) due |= C v A. Hence, v cDiff (T1 , T2 ).Finally, case (ii) obtain T2 6|= C1 v D1 otherwise |= C v r.C1 , T2 |= r.C1 vC v 6 cDiff (T1 , T2 ) again. Thus, C1 v D1 cDiff (T1 , T2 ) applyinduction hypothesis.Theorem 16, every inclusion C v -concept difference T1 T2 contains basic witness inclusion concept name either right-hand sideleft-hand side. defineset left-hand -concept difference witnesses, cWtnlhs(T1 , T2 ), setNC exists concept v cDiff (T1 , T2 )set right-hand -concept difference witnesses, cWtnrhs(T1 , T2 ), setNC exists concept C C v cDiff (T1 , T2 ).rhsregard concept names cWtnlhs(T1 , T2 ) cWtn (T1 , T2 ) succinct and,certain sense, complete representation -concept difference T1 T2define set -concept difference witnessesrhscWtn (T1 , T2 ) = (cWtnlhs(T1 , T2 ), cWtn (T1 , T2 )).follows, first present polytime algorithm computing cWtnrhs(T1 , T2 ). polytime algorithm computing cWtnlhs(T,)alreadygivenLutzWolter (2010)1 2(for EL-TBoxes). briefly present since extension developed consider ELHr -terminologies. algorithms together decide -concept inseparability since,Theorem 16, T1 T2 -concept inseparable if, if, cWtn (T1 , T2 ) =cWtn (T2 , T1 ) = (, ).4.2 Computing cWtnrhs(T1 , T2 )Let assume want decide whether cWtnrhs(T1 , T2 ). Thus, wantdecide whether exists -concept C T1 |= C v T2 6|= C v A.general strategy follows. LetnoimplyT2 , (A) = {C | T2 6|= C v A, C EL -concept}.aim algorithm checks whether noimplyT2 , (A) contains C T1 |=C v A. two sets C concepts call C cover Cexists C C |= C v D. Thus, C noimplyT2 , (A) covernoimplyT2 , (A) noimplyT2 , (A) exists C C |= C v D.Note C cover noimplyT2 , (A), exists -concept CC v cDiff (T1 , T2 ) if, if, exists C C T1 |= C v A.Thus reduced original problem construction appropriate cover Cdeciding subsumption problem T1 |= C v A, C C. Unfortunately, general,finite cover exists. following example illustrates situation.646fiThe Logical Difference Lightweight Description Logic ELExample 17. (1) Let = {A, B, r} T2 = . noimplyT2 , (A) contains concepts atomic conjunct. Clearly, noimplyT2 , (A) containsfinite cover.(2) Let 0 = {A, B, r} T20 = {A r.A}. noimplyT20 ,0 (A) contains0 \ {A}-concepts contains finite cover.(3) Let 00 = {A, B1 , B2 } T200 = {A B1 u B2 }. {B1 , B2 } covernoimplyT200 ,00 (A).consequence, instead directly constructing cover noimplyT2 , (A), firstconstruct transparent small coversnoimplyT2 , (A) {C | depth(C) n},n 0, depth(C) role-depth C; i.e., number nestings existentialrestrictions C.3 covers denoted noimplynT2 , (A), n 0, singletonsets non-conjunctive T2 finite sets containing k conceptsB1 u uBk T2 . Based sequence, present two distinct algorithms computingcWtnrhs(T1 , T2 ):1. encode infinite sequence noimplynT2 , (A), n 0, polynomial-size ABoxAT2 , . way obtain reduction original problem instancechecking problem knowledge base (T1 , AT2 , ). certain sense, ABoxAT2 , encodes (in general infinite) cover noimplyT2 , (A).2. employ terminology T1 dynamic programming approach decideconcepts noimplynT2 , (A) relevant deciding whether cWtnrhs(T1 , T2 ).Although less transparent, large terminologies latter approach considerablyefficient. develop acyclic terminologies.EL-terminology , concept name signature , setpre(A) = {B | |= B v A}.finite covers noimplynT2 , (A), n 0, defined Figure 2. n = 0,set noimplynT2 , (A) consists concepts without role names. distinguish conjunctive non-conjunctive A. Note non-conjunctive, noimplynT2 , (A)singleton set. Example 17 (3) shows always case conjunctive A. n + 1, distinguish pseudo-primitive concept names, conjunctiveconcept names, definition form r.C. Again, nonnconjunctive A, noimplyn+1T2 , (A) singleton set. Note concepts covers{C | depth(C) n, C EL -concept}, n 0. illustrate definitions usingEL-terminologies Example 17.Example 18. (1) Let = {A, B, r} T2 = . B non-conjunctive T2noimply0T2 , (A) = {B} noimply0T2 , (B) = {A}. B also pseudo-primitiveT2 , noimply1T2 , (A) = {B u r.(A u B)} noimply1T2 , (B) = {A u r.(A u B)}.3. precisely depth(A) = 0, depth(C1 u C2 ) = max{depth(C1 ), depth(C2 )}, depth(r.D) =depth(D) + 1.647fiKonev, Ludwig, Walther, & WolterSet, inductively,all0 =lA0lalln+1=A0A0 uA0ls.alln .Define noimply0T2 , (A) follows:non-conjunctive T2 ,lnoimply0T2 , (A) = {A0 };A0 \pre(A)2conjunctive F T2 ,[noimply0T2 , (A) =noimply0T2 , (B);BFdefine, inductively, noimplyn+1T2 , (A)pseudo-primitive T2 ,lnoimplyn+1T2 , (A) = {A0 uA0 (\pre(A))ls.alln }.2conjunctive F T2 ,noimplyn+1T2 , (A) =[noimplyn+1T2 , (B).BFr.B T2 ,n+1noimplyn+1T2 , (A) = {C,T2 },n+1=(C,T2lA0 uA0 (\preT2 (A))lls.alln ur.E).rEnoimplynr6=s2 ,(B)Figure 2: Definition noimplynT2 , (A)(2) Let 0 = {A, B, r} T20 = {A r.A}. B non-conjunctive T20noimply0T 0 ,0 (A) = {B} noimply0T 0 ,0 (B) = {A}. B pseudo-primitive T2022noimply1T2 , (B) = {A u r.(A u B)}. r.A T20 noimplyT20 ,0 (A) = {B u r.B}.T200(3) Let 00 = {A, B1 , B2 } T200 = {A B1 u B2 }. B1 B2 non-conjunctivenoimply0T 00 ,00 (B1 ) = {B2 } noimply0T 00 ,00 (B2 ) = {B1 }. conjunctive T20022648fiThe Logical Difference Lightweight Description Logic ELand, definition, noimply0T 00 ,00 (A) = {B1 , B2 }. Since contain role names,2noimply0T 00 ,00 (X) = noimplynT 00 ,00 (X), X {A, B1 , B2 } n > 0.22following lemma shows correctness definition noimplynT2 , (A).Lemma 19. Let T2 normalised EL-terminology, signature, NC .noimplynT2 , (A) cover noimplyT2 , (A) {C | depth(C) n}. Namely, n 0,C1. T2 6|= C v A, C noimplynT2 , (A).C2. EL -concepts n = depth(D), T2 6|= v A, |= C vC noimplynT2 , (A).particular, n0 noimplynT2 , (A) cover noimplyT2 , (A).Proof.d C1. Assume first pseudo-primitive T2 . noimplynT2 , (A) consistsC = A0 (\pre (A)) A0 u F , F (possibly empty) conjunction conceptsT2form s.Fi . Lemma 15, T2 6|= C v atomic conjuncts C\ preT2 (A).prove C1 concept names pseudo-primitiveT2 . proofinduction n. n = 0 r.B T2 , assume T2 |= A0 (\pre (A)) A0 v A.T2r.B T2 , Lemma 15 must exist A0 \ preT2 (A)0T2 |= v A. contradicts definitionset preT2 (A)). n = 0conjunctive F T2 , let C noimplynT2 , (A) = BF noimplynT2 , (B). henceexists atomic conjunct B F C noimplynT2 , (B). T2 normalised, Bnon-conjunctive, i.e. property C1 already proved B. Thus, T2 6|= C v B,implies T2 6|= C v otherwise T2 |= C v B would hold.induction step, assume C1 proved n 0.element noimplyn+1Let r.B T2 let CTn+1T2 , (A). Assume2 ,n+1T2 |= CT2 , v A. Lemma 15 two possibilities:T2 |= A0 (\pre (A)) A0 v r.B. Similarly above, claim follows Lemma 15T2fact r.B T2 .r exists E noimplynT2 , (B) T2 |= E v B. excludedinduction hypothesis.derived contradiction. case F T2 , conjunctive T2 , consideredanalogously case n = 0.C2. Let n = 0 assume first non-conjunctive. Let -conceptdepth(D)d = 0 T2 6|= v A. conjuncts \ preT2 (A)obtain |= A0 \pre (A) A0 v D. assume conjunctive T2 F T2 .T2Let -concept depth(D) = 0 T2 6|= v A. T2 6|= v B,conjunct B F . induction, |= C v (unique B must non-conjunctive)C noimply0T2 , (B), therefore |= C v C noimply0T2 , (A).induction step, assume C2 shown n. Let -conceptT2 6|= v depth(D) = n + 1.649fiKonev, Ludwig, Walther, & Wolter(a) Let pseudo-primitive T2 . atomicdconjuncts aredincludedn0\ preA0 \pre (A) u s.all .T2 (A). |= C v follows immediately C =T2(b) Let r.B T2 . Let CTn+1element noimplyn+1T2 , (A) assume2 ,D=llEuEQ0n+1Q0 \ preT2 (A). Hence, |= CT2 , vdistinguish two cases:s.D0 .(s,D0 )Q1EQ0E. consider conjunct s.D0 D.6= r, |= CTn+1v s.D0 , required.2 ,= r, sufficient show exists E noimplynT2 , (B)|= E v D0 . Suppose exist E. Then, (thecontraposition of) induction hypothesis, T2 |= D0 v B. contradictsT2 6|= v (as r.B T2 ).(c) conjunctive T2 F T2 . case analogous caseconjunctive T2 n = 0.Corollary 20. normalised EL-terminologies T1 T2 NC followingconditions equivalent:exists EL -concept C T1 |= C v T2 6|= C v A;exists n 0 C noimplynT2 , (A) T1 |= C v A.Observe direct application Corollary 20 yield procedure comnputing cWtnrhs(T1 , T2 ) gives bound n set noimplyT2 , (A). pointpresent two ways avoiding problem (as well problem conceptsnoimplynT2 , (A) exponential size). Firstly, instead working covers construct ABox encoding covers. contrast concepts, ABoxes admit encodingstructure sharingcycles so, intuitively, admit polynomial reconstructioninfinite concept n0,Cnoimplyn (A) C.T2 ,ABox AT2 , constructed Figure 3, normalised EL-terminologyconcept name sig(T ), set{A}, non-conjunctivenon-conjT (A) ={B1 , . . . , Bn }, B1 u u BnNote construction AT2 , similar construction noimplynT2 , (A).assertions individual play role concepts alln , n 0,assertions individuals play role sets noimplynT2 , (A), n 0. fact, onereadily show AT2 , |= C(A ) C noimplynT2 , (A) non-conjunctiveT2 and, conversely, (a involved proof) shows whenever AT2 , |= D(A )EL-concept D, exist n 0 C noimplynT2 , (A) |= C v D.illustrate construction AT2 , using EL-terminologies Example 17.650fiThe Logical Difference Lightweight Description Logic ELLet{A | sig(T2 ) non-conjunctive T2 } { } NI .set individual names. non-conjunctive T2 , define sets AT2 , (A) assertionsfollowspseudo-primitive T2 ,AT2 , (A) = {A0 (A ) | A0 \ preT2 (A)} {r(A , ) | r },r.B T2 ,AT2 , (A) ={A0 (A ) | A0 \ preT2 (A)}{s(A , ) | r 6= }{r(A , B 0 ) | B 0 non-conjT2 (B), r }Let[AT2 , = { A0 ( ) | A0 } { r( , ) | r }AT2 , (A)Asig(T2 )non-conjunctive T2Figure 3: Construction AT2 , .Example 21. (1) Let = {A, B, r} T2 = .AT2 , = {A(B ), B(A ), r(A , ), r(B , )} ,= {A( ), B( ), r( , )}.(2) Let 0 = {A, B, r} T20 = {A r.A}.AT20 ,0 = {A(B ), B(A ), r(A , ), r(B , 0 )} A0 ,A0 = {A(0 ), B(0 ), r(0 , 0 )}.(3) Let 00 = {A, B1 , B2 } T200 = {A B1 u B2 }.AT200 ,00 = {B1 (B2 ), B2 (B1 )} A00 ,A00 = {A(00 ), B1 (00 ), B2 (00 )}.obtain following characterisation cWtnrhs(T1 , T2 ).Theorem 22. Let T1 T2 normalised EL-terminologies signature.following conditions equivalent :cWtnrhs(T1 , T2 );exist n 0 C noimplynT2 , (A) T1 |= C v A;651fiKonev, Ludwig, Walther, & Wolter(T1 , AT2 , ) |= A(B ) B non-conjT2 (A).equivalence Points 1 2 follows Corollary 20. give detailedproof equivalence Points 2 3 follows general resultsELHr -terminologies present below.Example 23. normalised form terminologies Example 12,000T1 = {A0 v B0 , A1 Bn } {Bi+1 Bi+1u Bi+1| 0 < n}000{Bi+1 r.Bi | 0 < n} {Bi+1 s.Bi | 0 < n}T2 = {A1 v F0 } {Fi Fi0 u Fi00 | 0 < n}{Fi0 v r.Fi+1 | 0 < n} {Fi00 v s.Fi+1 | 0 < n},= {A0 , A1 , r, s}, ABox AT2 , graphically representedr,r,r,r,F 00A0 ,A1r,r,A0 ,A1F 00F 0A0 ,A1A0 ,A11r,nF 00r,A0A1n1r,A1A0F 0A0 ,A10A0F 00A0clear (T1 , AT2 , ) |= A1 (A1 ). fact, (T1 , A) |= A1 (A1 ) holds alreadyrestriction AT2 , individuals {A1 , }.Theorem 24. EL-terminologies T1 T2 signature , set cWtnrhs(T1 , T2 )computed polynomial time.Proof. suffices give polynomial time algorithm decides every whethercWtnrhs(T1 , T2 ). First, ABox AT2 , computed polynomial timequadratic size T2 . Theorem 22, cWtnrhs(T1 , T2 ) iff (T1 , AT2 , ) |= A(B )B non-conjT2 (A), latter condition checked polynomial time sinceinstance checking polynomial time EL-TBoxes.Regarding efficiency approach, observe typical terminologies large, ABox AT2 , indeed quadratic size T2 since \ preT2 (A) typically containconcept names . Thus, large terminologies straightforwardimplementation rather elegant algorithm work efficiently one wouldstore ABox quadratic size instance checking it. refer readerTable 3 discussion prototype implementation approach appliedmodules Snomed CT.describe second approach computing cWtnrhs(T1 , T2 ), worksacyclic EL-terminologies. Recall cWtnrhs(T,1 2 ) if, if, exists EL -concept C T2 6|= C v T1 |= C v A. Thus,6 cWtnrhs(T1 , T2 ) if, if, every EL -concept C C noimplyT2 , (A)652fiThe Logical Difference Lightweight Description Logic ELprocedure NotWitness(E)E pseudo-primitiveT1NotWitness(E) := | preT1 (E) preT2 (A)end(E E1 u u EkT1 )NotWitness(E) := ki=1 NotWitness(Ei )endE r.E 0 T10 )r/ NotWitness(ENotWitness(E) := | preT1 (E) preT2 (A)elsefifi r.A0 T2fiNotWitness(E) := fifi non-conjT2 (A0 ) NotWitness(E 0 )fi pre (E) pre (A)T1T2endendend procedureFigure 4: Computation NotWitness(E).holds C noimplyT1 , (A). approach based computing witnessrelation NW ((sig(T1 ) ) NC ) ((sig(T2 ) ) NC ), defined follows:(E, A) NWif, if,() noimplyT2 , (A) noimplyT1 , (E)Observe cWtnrhs(T1 , T2 ) if, if, (A, A) 6 NW; hence, computeset cWtnrhs(T,)sufficientcompute relation NW. practice, crucial1 2compute relation NW rather complement: typical terminologies conceptnames unrelated sense subsume other. Thus, relationNW much smaller complement (which contains, among others, pairs (E, A)subsume T1 T2 ).determine pairs (E, A) NW, aim computing every concept nameE sig(T1 ) set concept names sig(T2 ) property () holds.set called NotWitness(E) computed Figure 4, followingmodifications: (1) consider sig(T2 ) non-conjunctive T2take conjunctive concept names account later. (2) consider fresh conceptname occurring sig(T1 ) sig(T2 ) informally standing possible concepts.Thus, procedure, NotWitness(E) given Figure 4 recursively associates everyE sig(T1 ) subset set= {All} { | (sig(T2 ) ), non-conjunctive T2 }NW relation((sig(T1 ) ) NC ) (((sig(T2 ) ) NC ) {All}).653fiKonev, Ludwig, Walther, & WolterNote unlike approach computing cWtnrhs(T1 , T2 ) presented previously, approach described handle two terminologies separately.previous approach ABox AT2 , could precomputed T2 re-usedcompare T2 terminology T1 , whereas terminologies analysedsimultaneously. prove correctness procedure NotWitness(E).Lemma 25. normalised acyclic EL-terminologies T1 T2 , signature ,E sig(T1 ) following holds: NotWitness(E) if, if,(E, A) NW.Proof. prove E sig(T1 ) following two conditionsequivalent:NotWitness(E);n 0 C noimplynT2 , (A): T1 6|= C v E.sufficient since n0 noimplynT2 , (A) cover noimplyT2 , (A) (Lemma 19).E 6 sig(T1 ) claim trivial. E sig(T1 ) proof induction relativerelation T1 sig(T1 ) sig(T1 ) (whose definition found page 637). Notesince considered terminologies acyclic sig(T1 ) finite, relation T1well-founded.distinguish possible definitions E T1 . Suppose E pseudoprimitive T1 . , follows definition noimplynT2 , (A)Lemma 15 exist n 0 C noimplynT2 , (A) T1 |= C v E if,if, T1 |= B v E B ( \ preT2 (A)). Note B ( \ preT2 (A)),T1 6|= B v E holds if, if, every B , T1 |= B v E implies B preT2 (A).nThus, every n C noimplyT2 , (A), T1 6|= C v E if, if, preT1 (E) preT2 (A)if, if, NotWitness(E).Assume E E1 u u Ek T1 . Then, concept C, T1 6|= C v E if,if, T1 6|= C v Ei 1 k. Hence, applying induction hypothesisobtain every n C noimplynT2 , (A), T1 6|= C v E if, if, NotWitness(Ei )1 k, if, if, NotWitness(E).Finally, assume E r.E 0 T1 . Notice that, since/ ( sig(T1 ) sig(T2 )) (inparticular, pseudo-primitive T2 ), preT2 (All) = . Thus, definitionevery n 0, noimplynT2 , (All) = {alln }. applying induction hypothesis assumelemma holds E 0 , implies/ NotWitness(E 0 ) if, if,n 0, T1 |= alln v E 0 . distinguish following cases, analogouslycase distinction procedure NotWitness(E) (see Figure 4).r/ , -concept form s.G, NR , r 6=T1 6|= s.G v r.E 0 . Similarly, NotWitness(E 0 ), holds every n 0T1 6|= alln v E 0 . Hence, -concept form s.G, obtain T1 6|= s.G v r.E 0otherwise T1 |= alln v E 0 would hold n = depth(s.G) (where depth(s.G)role-depth s.G). So, Lemma 15, two cases analogous case Epseudo-primitive considered above.Assume r/ NotWitness(E 0 ), is, n0 0n00T1 |= v E .654fiThe Logical Difference Lightweight Description Logic ELFirst, observe definition form r.A0 T2 ,+1unique C noimplynT20,(A) T1 |= C v E r.alln0 conjunct C (andnon-conjunctive T2 definition set ). definition form r.A0T2 , n 0 C noimplynT2 , (A), Lemma 15 T1 6|= C v En100if, if, preT1 (E) preT2 (A), and, n > 0, every C noimplyT2 , (A )00T1 6|= C v .conclude case r/ NotWitness(E 0 ), ,nn 0, C noimplyT2 , (A), T1 6|= C v E, if, if, r.A000T2 , preT1 (E) preT2 (A) 0 C noimplyT2 , (A )00(A0 ) =S1 6|= C v E . Noticem that, definition 0,0 noimplyT2 ,0Bnon-conjT (A0 ) noimplyT2 , (B). Thus, 0 C noimplyT2 , (A ),2T1 6|= C 0 v E 0 holds if, if, 0, B non-conjT2 (A0 )000C 0 noimplymT2 , (B), T1 6|= C v E , if, if, B non-conjT2 (A ), B0NotWitness(E ) holds applying induction hypothesis.Thus, T1 6|= C v E, n 0 C noimplynT2 , (A), if, if,NotWitness(E).Corollary 26. Let T1 T2 normalised acyclic EL-terminologies signature.cWtnrhs(T1 , T2 ) = { sig(T1 ) | B non-conjT2 (A) B 6 NotWitness(A) }.Proof. First, observe cWtnrhs(T1 , T2 ), sig(T1 ) must hold otherwise-concept C T1 |= C v if, if, |= C v A, thus 6cWtnrhs(T1 , T2 ). Now, NC have:cWtnrhs(T1 , T2 )iffiffiffiffsig(T1 ) (by observation) and, definition,exists -concept C T2 6|= C v T1 |=CvAsig(T1 ) exists B non-conjT2 (A)-concept C T2 6|= C v B T1 |= C v (asotherwise T2 |= C v would hold)sig(T1 ) exists B non-conjT2 (A)(A, B) 6 NW (by definition relation NW)sig(T1 ) exists B non-conjT2 (A)B 6 NotWitness(A), Lemma 25.acyclic terminologies, obtain alternative proof Theorem 24.Theorem 27. acyclic EL-terminologies T1 T2 signature , cWtnrhs(T1 , T2 )computed polynomial time using procedure NotWitness(E).Proof. compute set cWtnrhs(T1 , T2 ), sufficient Corollary 26 computesets NotWitness(E) every E sig(T1 ). Assuming T1 T2 classifiedresult classification cached, NotWitness(E) computed E sig(T1 ),worst case, time O((|T1 | + |T2 |)3 ).Example 28. acyclic terminologies T1 , T2 signature Example 23,NotWitness(A0 ) = {A0 },655NotWitness(B0 ) = {A0 }fiKonev, Ludwig, Walther, & Wolterconcept names X sig(T1 ), NotWitness(X) = . particular A1/NotWitness(A1 ), conclude A1 concept difference witness.4.3 Computing cWtnlhs(T1 , T2 )Recall set left-hand -concept difference witnesses, cWtnlhs(T1 , T2 ), setNC exists concept C v C cDiff (T1 , T2 ).tractability computing cWtnlhs(T1 , T2 ) EL proved Lutz Wolter (2010)arbitrary EL-TBoxes reduction simulation checking. formulate mainsteps employ technique dealing logical differenceELHr -terminologies.two interpretations I1 I2 say relation I1 I2-simulation if, if, following conditions hold:(d, e) AI1 , e AI2 ;(d, e) (d, d0 ) rI1 r , exists e0 (d0 , e0 )(e, e0 ) rI2 .I1 e I2 write (I1 , d) (I2 , e) exists -simulation relation I1 I2 (d, e) S. checked polynomial timewhether (I1 , d) (I2 , e) various polynomial-time algorithms checking existencesimulations developed (Clarke & Schlingloff, 2001; Crafa, Ranzato, & Tapparo,2011; van Glabbeek & Ploeger, 2008). Simulations characterise expressive powerEL-concepts following sense.Lemma 29 (Lutz & Wolter, 2010). Let I1 I2 interpretations, signature, I1 ,e I2 .(I1 , d) (I2 , e)EL -concepts C: C I1 e C I2 .follows ,cWtnlhs(T1 , T2 )(IK1 , a) 6 (IK2 , a)Ki = (Ti , A) = {A(a)} IKi canonical model Ki , = 1, 2.see this, recall Theorem 2 every EL-concept C, C IKi if, if,(Ti , A) |= C(a). latter condition equivalent Ti |= v C. have, therefore,proved:Theorem 30 (Lutz & Wolter, 2010). EL-TBoxes T1 T2 signatures , setcWtnlhs(T1 , T2 ) computed polynomial time.following example illustrates use simulations canonical modelsdetermine cWtnlhs(T1 , T2 ).Example 31. Let = {A, r, B1 , B2 }T1 = {A v r.F0 , F0 v F1 u F2 , F1 v r.B1 , F2 v r.B2 },T2 = {A v G1 u G2 , G1 v r.G01 , G2 v r.G02 , G01 v r.B1 , G02 v r.B2 }656fiThe Logical Difference Lightweight Description Logic ELcheck whether cWtnlhs(T1 , T2 ) consider KBs K1 = (T1 , {A(a)}) K2 =(T2 , {A(a)}). cWtnlhs(T1 , T2 ) iff (IK1 , a) 6 (IK2 , a), canonical modelsIK1 IK2 K1 K2 , respectively. Illustrations canonical models IK1 IK2shown below.xran(r),B1B1xran(r),B2B2rxran(r),B1B1rrrrxran(r),G01xran(r),F0IK 1xran(r),B2B2xran(r),G02rrIK2(IK1 , a) 6 (IK2 , a) point xran(r),F0 neither -simulated xran(r),G01-simulated xran(r),G02 . concept inclusion cDiff (T1 , T2 ) left-hand sidegiven v r.((r.B1 ) u (r.B2 )).5. ELHr -Instance Differencepolynomial-time algorithms inseparability logical difference ELHr basedextensions ideas used Section 4 EL. is, however, one importantdifference: introduce new logics, ELran ELran,u,u , concept differencecaptures exactly instance and, respectively, query difference ELHr . prove analogue Theorem 16 languages and, thereby, instance query differenceELHr , introduce sequent calculus characterises ELran -consequencesELHr -terminologies. start investigation instance difference case sincetransparent concept difference case (recall EL differenceinstance concept difference).5.1 ELran -Concept DifferenceRecall Example 4 showing ELHr -concept inseparability imply -instanceinseparability:T1 = {ran(r) v A1 , ran(s) v A2 , B A1 u A2 },T2 = ,= {r, s, B}.Notice ABox = {r(a, c), s(b, c)}, exhibiting instance differenceT1 T2 , c range r s. example suggests ran(r) ran(s)could used complex concepts, kind difference made visible conceptlanguage.Definition 32 (ELran ). C ran -concepts constructed using following syntax ruleC :=|ran(r)|C uD|r.C,NC , C, range C ran -concepts r NR . set ELran -inclusionsconsists concept inclusions C v role inclusions r v s, C C ran concept, EL-concept, r, NR .657fiKonev, Ludwig, Walther, & WolterClearly, every ELHr -inclusion ELran -inclusion. Additionally, ELran -inclusionsconcept ran(r) occur everywhere concepts left-hand side inclusions.gives us additional concept inclusions -concept difference.Example 33. T1 T2 Example 4, T1 |= ran(r) u ran(s) v B,T2 6|= ran(r) u ran(s) v B. Thus, using C ran -concept ran(r) u ran(s) simulateABox {r(a, c), s(b, c)} Example 4 make -difference couldobserved ELHr visible ELran .show Example 33 generalised arbitrary TBoxes. end,consider following straightforward generalisation -concept difference differences ELran .ranrDefinition 34 (ELran-difference). EL -difference ELH -TBoxes T1 T2ranset cDiff ran(T1 , T2 ) EL -inclusions T1 |= T2 6|= .prove equivalence -instance difference ELHr -concept differranence ELran , first associate every ABox individual set CA,aranC -concepts. Assume given. Let, inductively, obj(A):ll0,ran=(CA,aA) u (ran(r));A(a)An+1,ran=(CA,alA(a)AA) u (r(b,a)Alran(r)) u (r(b,a)Aln,ran),r.CA,br(a,b)Asetn,ranran| n 0}CA,a= {CA,an,ran(a) fordall n > 0. Moreover, lemma showsthat, intuObserve |= CA,aran specific concept |=ran (a).itively, infinite conjunction CA,aCA,aConversely, associate ABox C ran -concept. construction straightforward; however, care taken since introduce structure sharingassociate distinct individual names distinct occurrences subconcepts. GivenC ran -concept C, first define path C finite sequence C0 r1 C1 rn Cn ,C0 = C, n 0, ri+1 .Ci+1 conjunct Ci , 0 < n. use paths(C)denote set paths C. p paths(C), tail(p) denotes last elementCn p.Now, let aran ap p paths(C) individual names set inductively:AC = { s(ap , aq ) | p, q paths(C); q = p C 0 , C 0 }{ A(ap ) | conjunct tail(p), p paths(C) }{ >(ap ) | > conjunct tail(p), p paths(C) }{ r(aran , ap ) | ran(r) conjunct tail(p), p paths(C) }Example 35. Let C = (r.(A u ran(v))) u (s.((t.(A u ran(v))) u (t.(B u ran(s)))))C ran -concept. AC represented graphically follows.658fiThe Logical Difference Lightweight Description Logic ELvBrvaranaCindicate aC aran ; individuals identified paths C. Notedifferent occurrences u ran(v) C correspond different individuals AC .Lemma 36. Let ELHr -TBox, ABox, C0 D0 C ran -concepts, leta0 obj(A).n,ran(T , A) |= D0 (a0 ) if, if, exists n 0 |= CA,av D0 ;0|= C0 v D0 if, if, (T , AC0 ) |= D0 (aC0 ).Below, employ lemma transfer analogue Theorem 16 ELranELHr -instance differences. now, note following consequence:Corollary 37. two ELHr -TBoxes T1 T2 , cDiff ran(T1 , T2 ) = if, if,iDiff (T1 , T2 ) = .n,ranProof. (A, D0 (a0 )) iDiff (T1 , T2 ), exists n 0 CA,av0ranD0 cDiff ran(T,).Conversely,CvcDiff(T,),(A,(a))1200120CC00iDiff (T1 , T2 ).Note Theorem 6 follows Corollary 37 since ELHr -TBox withoutrange restrictions |= C v if, if, |= C 0 v D, C 0 obtained Creplacing concept form ran(r) C >.5.2 Proof System ELHrGentzen-style proof system ELHr consists rules given Figures 1 5.Cut elimination, correctness, completeness proof system shown similarlycorresponding proofs given Hofmann (2005).Theorem 38. ELHr -terminologies C ran -concepts C D, holds|= C v if, if, ` C v D.generalise Lemma 15 ELHr -terminologies.Lemma 39. Let ELHr -terminology, concept name r.D EL-concept.Assumelll|=ran(si ) uAj urk .Ck v r.D,1il1jn1kmCk , 1 k m, C ran -concepts l, m, n 0. least one followingconditions holds:659fiKonev, Ludwig, Walther, & Wolterr.(C u ran(r)) v(ExRan)r.C vBvD(Dom)r.C vr.> v BAvD(Ran)ran(r) vs.C v(Sub)r.C vran(r) vran(s) v(RanSub)ran(r) vr vFigure 5: Additional rules ELHr -terminologies.(e1) exists rk , 1 k m, |= rk v r |= Ck u ran(rk ) v D;(e2) exists Aj , 1 j n, |= Aj v r.D;(e3) exists rk , 1 k m, |= rk .> v r.D;(e4) exists si , 1 l, |= ran(si ) v r.D.assume pseudo-primitivell|=ran(si ) uAj u1il1jnlrk .Ck v A,1kmCk , 1 k m, C ran -concepts l, m, n 0. least one followingconditions holds:(a1) exists Aj , 1 j n |= Aj v A;(a2) exists rk , 1 k |= rk .> v A;(a3) exists si , 1 l |= ran(si ) v A.Proof. prove first part lemma, second part proved analogously.Let C = 1il ran(si ) u 1jn Aj u 1km rk .Ck assume |= C v r.Dholds. Then, ` C v r.D Theorem 38, implies existsderivation sequent C v r.D. proof proceeds induction depthD, i.e. maximal length path root one leaves D.Notice l + n + 2, root derived AndL1AndL2. lemma follows induction hypothesis.Otherwise, l + n + = 1. Note l + + n = 0 possible since6|= > v r.D definition terminology . C = A1 C = ran(s1 ), (e2)(e4), respectively, hold already. remains consider case C = r1 .C1 . Then,rule used derive root one Ax, Ex, ExRan, DomSub. consider cases one one:660fiThe Logical Difference Lightweight Description Logic ELroot derived Ax: considering form inference rule,r1 = r C1 = D. Hence |= r1 v r |= C1 u ran(r1 ) v D, implies(e1) holds.root derived Ex: r1 = r ` C1 v D. Hence, |= r1 v r|= C1 v holds Theorem 38. Thus, |= C1 u ran(r1 ) vinfer (e1) holds again.root derived Dom: ` B v r.D r1 .> v B .Theorem 38, |= B v r.D hence, |= r1 .> v r.D, is, (e3)holds.root derived ExRan: obtain ` r1 .(C1 u ran(r1 )) v r.D.Since sequent r1 .(C1 u ran(r1 )) v r.D derivation shorter lengthD, apply induction hypothesis. Hence, either |= r1 .> v r.D,is, (e3) holds, |= r1 v r |= (C1 u ran(r1 )) u ran(r1 ) v D. Hence (e1)holds |= C1 u ran(r1 ) v (C1 u ran(r1 )) u ran(r1 ).root derived Sub: obtain ` s.C1 v r.D r1 v .induction hypothesis, either |= s.> v r.D, |= v r |=C1 u ran(s) v D. seen |= r1 .> v r.D, |= r1 v r|= C1 u ran(r1 ) v D, respectively. Hence (e3) (e1) holds.prove extension Theorem 16 ELran -consequences ELHr -terminologies.give rather detailed description simple witness inclusions contained memberscDiff ran(T1 , T2 ) since going use result analysing conceptdifference ELHr .Theorem 40 (Primitive witness ELran -differences). Let T1 T2 ELHr -terminologiessignature. cDiff ran(T1 , T2 ), either exist {r, s} sig()ranr v cDiff (T1 , T2 ) form C v D, one1. C 0 v ran(r) u C 0 v A,2. v D0 , r.> v D0 ran(r) v D00member cDiff ran(T1 , T2 ), r sig(), sig() concept name, Csubconcept C D0 subconcept D.ran -concept EL-concept.Proof. Let C v cDiff ran(T1 , T2 ), C Cprove theorem induction structure D.Notice 6= > T2 |= C v >. concept name, inclusionPoint 1 exists. = D1 u D2 , one C v Di , = 1, 2, cDiff ran(T1 , T2 ).apply induction hypothesis anddwe inferinclusionPoint 1Point 2 exists. = r.D1 , let C = 1il ran(si ) u 1jn Aj u 1km rk .Ck .Then, Lemma 39, one (e1)(e4) holds. Cases (e2)(e4) directly entailinclusion Point 1 Point 2 exists. case (e1), either rk v r cDiff ran(T1 , T2 )661fiKonev, Ludwig, Walther, & WolterT1 |= Ck u ran(rk ) v D1 T2 6|= Ck u ran(rk ) v D1 (as otherwise T2 |= C v wouldhold). apply induction hypothesis D1 conclude inclusionPoint 1 Point 2 exists.5.3 Instance Difference WitnessesSimilarly Theorem 16 concept difference EL-terminologies derived extension, Theorem 40, ELran , show every member (A, )iDiff (T1 , T2 ) gives rise basic witness either ABox instance queryatomic. keep formulation succinct give abstract description relationship (A, ) iDiff (T1 , T2 ) witness using signature (A, ).interested reader problem derive stronger relationship (A, )witness proof.Theorem 41 (Primitive witness ELHr -instance differences). Let T1 T2 ELHr terminologies signature. iDiff (T1 , T2 ), least one followingconditions holds:1. ({r(a, b)}, s(a, b)) iDiff (T1 , T2 ), r, sig();2. (A, A(b)) iDiff (T1 , T2 ), concept name sig(), individual b,ABox sig(A) sig().3. (A, D(b)) iDiff (T1 , T2 ), singleton ABox A, individual b A, ELconcept sig(A), sig(D) sig();Proof. Let (A, ) iDiff (T1 , T2 ). distinguish following cases.(a) = s(a, b), (T1 , A) |= s(a, b) if, if, r(a, b) T1 |=r v s. (T2 , A) 6|= s(a, b) obtain T2 6|= r v s. Thus, ({r(a, b)}, s(a, b)) iDiff (T1 , T2 )Point 1 holds.(b) Assume = D(b) EL-concept D. Lemma 36, n 0n,rann,ranv D. Theorem 40, one (i) r v s, (ii) v D0 , (iii)v T2 6|= CA,bT1 |= CA,br.> v D0 , (iv) ran(r) v D0 , (v) C v A, (vi) ran(r)uC v member cDiff ran(T1 , T2 ),n,ranD0r sig(), sig() concept name, C subconcept CA,bsubconcept D. (i) r v cDiff ran(T1 , T2 ), ({r(a, b)}, s(a, b)) iDiff (T1 , T2 )Point 1 holds.let F v G denote member cDiff ran(T1 , T2 ) cases (ii)-(vi) above. Conransider ABox AF associated C -concept F Point 2 Lemma 36.sig(AF ) sig() (AF , G(aF )) iDiff (T1 , T2 ).case (ii), obtain F = concept name. Hence AF = {A(aF )} Point 3holds. case (iii), obtain AF = {r(aF , a> ), >(a> )} Point 3 lemma applies(after removing >(a> ) AF ). Similarly, (iv), AF = {r(aran , aF )},Point 3 lemma holds. Finally, cases (v) (vi), G sig() conceptname. Hence Point 2 lemma applies.Theorem 41 justifies following finite representation -instance difference ELHr -terminologies. corresponds exactly three distinct points theorem. Assume T1 T2 given. Let662fiThe Logical Difference Lightweight Description Logic ELset role -instance difference witnesses, iWtnR(T1 , T2 ), consist r vT1 |= r v T2 6|= r v s;set right-hand -instance difference witnesses, iWtnrhs(T1 , T2 ), consistexists (A, A(a)) iDiff (T1 , T2 );set left-hand -instance difference witnesses, iWtnlhs(T1 , T2 ), consistexists C(a) ({A(a)}, C(a)) iDiff (T1 , T2 ) rexists C(c) c = c = b ({r(a, b)}, C(c)) iDiff (T1 , T2 ).set -instance difference witnesses definedrhslhsiWtn (T1 , T2 ) = (iWtnR(T1 , T2 ), iWtn (T1 , T2 ), iWtn (T1 , T2 )).Theorem 41, observe iWtn (T1 , T2 ) = (, , ) if, if, iDiff (T1 , T2 ) = .set iWtnR(T1 , T2 ) easily computed polynomial time analysedpaper. Thus, aim present polynomial-time algorithms computinglhsiWtnrhs(T1 , T2 ) iWtn (T1 , T2 ).5.4 Computing iWtnrhs(T1 , T2 )compute iWtnrhs(T1 , T2 ) two different ways: first, present transparentABox approach works arbitrary ELHr -terminologies, second presentefficient dynamic programming approach works acyclic ELHr -terminologiesonly. approaches introduced Section 4.2 EL-terminologies. startABox approach exhibit -ABox AT2 , depending T2non-conjunctive exists ABox (A, A(d)) iDiff (T1 , T2 ) if,if, (T1 , AT2 , ) |= A(A ) certain individual name . case conjunctivereduced condition defining concept names.deal ELHr -terminologies rather EL-terminologies extendstructure AT2 , significantly. describe model-theoretic properties AT2 , ,require notion -range simulation. capture model-theoretically expressivepower C ran -concepts (the concepts used describe -instance difference terms subsumption, cf. Lemma 36). two ABoxes A1 , A2 designatedindividual names a1 a2 , say relation obj(A1 ) obj(A2 )-simulation if, if,(S1) (a1 , a2 ) S;(S2) : (a, b) A(a) A1 , A(b) A2 ;(S3) r : (a, b) r(a, a0 ) A1 , exists b0 (a0 , b0 )r(b, b0 ) A2 .say -range simulation if, addition,(RS) r : (a, b) exists c r(c, a) A1 , existsc0 r(c0 , b) A2 .follows write663fiKonev, Ludwig, Walther, & Wolter(A1 , a1 ) (A2 , a2 ) exists -simulation (A1 , a1 ) (A2 , a2 );(A1 , a1 ) ran(A2 , a2 ) exists -range simulation (A1 , a1 )(A2 , a2 ).following lemma shows range simulations characterise C ran -concepts.Lemma 42. Let A1 A2 -ABoxes designated individual names a1 a2 .ran -concepts(A1 , a1 ) ran(A2 , a2 ), (T , A1 ) |= C(a1 ) implies (T , A2 ) |= C(a2 ) CC.Proof. apply Lemma 36. Let -range simulation A1 A2(a1 , a2 ) S. One prove induction n n 0, obj(A1 )b obj(A2 ),n,ran(b).() (a, b) S, A2 |= CA1 ,aran assume (A1 , a1 ) ran(A2 , a2 ) (T , A1 ) |= C(a1 ) holds Cn,ranconcept C. Then, Lemma 36, exists n 0 |= CA1 ,a1 v C. Moreover,n,ran(A1 , a1 ) ran(A2 , a2 ), () A2 |= CA1 ,a1 (a2 ), implies(T , A2 ) |= C(a2 ), required.construction , given Figure 6, normalised ELHr terminology signature. advise reader recall definition , givenFigure 3 EL-terminologies consider additional ingredients requiredELHr -terminologies. remind reader definition non-conjT (A)Section 4.2:{A}, non-conjunctivenon-conjT (A) ={B1 , . . . , Bn }, B1 u u BnFigure 6, also use following sets, NC r NR :preC(A) = { B NC | |= B v },preDom(A) = { r NR | |= r.> v },preRan(A) = { r NR | |= ran(r) v },preRole(r) = { NR | |= v r }.following example illustrates definition , .Example 43. T1 = {ran(r) v A1 , ran(s) v A2 , B A1 u A2 }, T2 = , = {r, s, B}defined Example 4,AT2 , = {B( ), r( , ), s( , ), r(B , ), s(B , ), r( , B ), s( , B )}.holds (T1 , AT2 , ) |= B(B ) (T2 , AT2 , ) 6|= B(B ).664fiThe Logical Difference Lightweight Description Logic ELLet{A | sig(T ) non-conjunctive } { } NI .set individual names. non-conjunctive , define sets , (A) assertionsfollowspseudo-primitive ,, (A) = { A0 (A ) | A0 \ preC(A) }{ r(A , ) | r \ preDom(A) }{ r( , ) | r \ preRan(A) },r.B , NR let0= { (s, B 0 ) | B 0 non-conjT (B), preRole(r) \ (preDomT (A) preRanT (B )) }set, (A) = { A0 (A ) | A0 \ preC(A) }{ s( , ) | \ preRan(A) }{ s(A , ) | \ (preRole(r) preDomT (A)) }{ s(A , ) | (s, ) }.Let, = { A0 ( ) | A0 } { r( , ) | r }[, (A)Asig(T )non-conjunctiveFigure 6: Construction , ELHr -terminologies.Lemma 44. every normalised ELHr -terminology signature followingconditions equivalent -ABoxes A, sig(T ) non-conjunctive ,obj(A):1. (T , A) 6|= A(a);2. obj(AT , ) (A, a) ran(AT , , ).Lemma 44 proved appendix.Lemma 45. Let T1 T2 normalised ELHr -terminologies, signature .Let AT2 , ABox constructed Figure 6. following conditions equivalent:exists -ABox (T1 , A) |= A(a) (T2 , A) 6|= A(a);(T1 , AT2 , ) |= A(B ) B non-conjT2 (A).665fiKonev, Ludwig, Walther, & WolterProof. Assume exists -ABox obj(A) (T1 , A) |= A(a) (T2 , A) 6|=A(a). Then, (T2 , A) 6|= A(a), B non-conjT2 (A), (T2 , A) 6|= B(a). Hence,Lemma 44, (A, a) ran(AT2 , , B ). then, Lemma 42, (T1 , AT2 , ) |= A(B ),required.Conversely, suppose (T1 , AT2 , ) |= A(B ) B non-conjT2 (A) Bobj(AT2 , ). Notice that, Lemma 44, (T2 , AT2 , ) 6|= B(B ). Hence (T2 , AT2 , ) 6|= A(B )AT2 , B witness Point 1.Theorem 46. Let T1 T2 normalised ELHr -terminologies signature.iWtnrhs(T1 , T2 ) computed polynomial time.Proof. Lemma 45, iWtnrhs(T1 , T2 ) if, if, B non-conjT2 (A)(T1 , AT2 , ) |= A(B ). remains observe AT2 , constructed polynomialtime checking whether (T1 , AT2 , ) |= A(B ) polynomial time.briefly describe dynamic programming approach computing setriWtnrhs(T1 , T2 ) acyclic terminologies extended EL ELH . extensionNotWitness(E) algorithm Figure 4 ELHr given Figure 7. Figure 4,procedure NotWitness(E) recursively associates every E sig(T1 ) subset= {All} { | (sig(T2 ) ), non-conjunctive T2 }.conditions NotWitness(E) become complex since one takeaccount sets preRan(E) preDomT (E). prove correctness NotWitnessalgorithm, observe following consequence Lemma 36.Corollary 47. Let T1 T2 normalised acyclic ELHr -terminologies signature.ran -concept C C v cDiff ran (T , ) }.iWtnrhs1 2(T1 , T2 ) = { | CProof. First, let iWtnrhs(T1 , T2 ). exists -ABox (T1 , A) |=A(a) (T2 , A) 6|= A(a). Hence, Point 1 Lemma 36 exists n 0n,rann,ranran -concept. Conversely, assumeCA,av cDiff ran(T1 , T2 ). Note CA,a Cexists C ran -concept C C v cDiff ran(T1 , T2 ). Point 2Lemma 36, (AC , A(aC )) iDiff (T1 , T2 ), i.e. iWtnrhs(T,T2 ).1formulate correctness NotWitness algorithm wayCorollary 26.Theorem 48. Let T1 T2 normalised acyclic ELHr -terminologies signature.iWtnrhs(T1 , T2 ) = { sig(T1 ) | B non-conjT2 (A) B 6 NotWitness(A) }.proof extension proofs Lemma 25 Corollary 26. Namely, oneshow sig(T1 ) B sig(T2 ) B non-conjunctiveT2 following conditions equivalent:B NotWitness(A);ran -concepts C: 6|= C v B implies 6|= C v A.C21Using Corollary 47, thus obtain every : iWtnrhs(T1 , T2 ) if, if,ranexists C -concept C T2 6|= C v T1 |= C v if, if, existsB non-conjT2 (A) B 6 NotWitness(A).666fiThe Logical Difference Lightweight Description Logic ELprocedure AuxPP (E)preCT1 (E) = preRanT1 (E) = preDomT1 (E) =return {All}elseAuxconcept := { | preCT1 (E) preCT2 (A) }Auxran := { | preRanT1 (E) preRanT2 (A) }Auxdom := { | preDomT1 (E) preDomT2 (A) }return Auxconcept Auxran Auxdomendend procedureprocedure NotWitness(E)E pseudo-primitive T1NotWitness(E) := AuxPP (E)else E E1 u uSEk T1kNotWitness(E) := i=1 NotWitness(Ei )else E r.E 0 T10preRoleT1 (r) = NotWitness(E )NotWitness(E) := AuxPP (E)elsefifi pseudo-primitive ,fiAuxrole,prim := fipreRoleT1 (r) preDomT2 (A)fifi t.B T2 ,fififi preRoleT1 (r) preRoleT2 (t) preDomT2 (A),fifi preRoleT (r) preRoleT (t)12Auxrole,exist := fifi0/preDom(A)Bnon-conjfiT2 (B)T2fi000fi6preRan(B),existsEnon-conjT1 (E 0 )T2fi00000fiB NotWitness(E ) 6 preRanT1 (E )NotWitness(E) := (Auxrole,prim Auxrole,exist ) AuxPP (E)endendend procedureFigure 7: Computation NotWitness(E) ELHr .5.5 Tractability iWtnlhs(T1 , T2 )prove tractability iWtnlhs(T1 , T2 ) reduction simulation checkingcase EL-terminologies (Theorem 30).Theorem 49. Let T1 T2 ELHr -TBoxes let signature. setiWtnlhs(T1 , T2 ) computed polynomial time.Proof. concept nameiWtnlhs(T1 , T2 )667(IK1 , a) 6 (IK2 , a)fiKonev, Ludwig, Walther, & WolterKi = (Ti , A) = {A(a)} IKi canonical model Ki , = 1, 2. Indeed,({A(a)}, C(a)) iDiff (T1 , T2 ), EL -concept C, if, if, Theorem 2,C IK1/ C IK2 . condition is, Lemma 29, equivalent (IK1 , a) 6 (IK2 , a).latter condition checked polynomial time.Similarly, role name rr iWtnlhs(T1 , T2 )(IK1 , a) 6 (IK2 , a) (IK1 , b) 6 (IK2 , b)Ki = (Ti , A), = {r(a, b)}, IKi canonical model Ki , = 1, 2. Again,latter condition checked polynomial time.6. ELHr -Concept Differencesection present polynomial-time algorithms deciding -concept inseparabilitycomputing succinct representation concept difference ELHr -terminologies.algorithms essentially reduction instance difference case.start introducing succinct representation -concept difference. LetT1 T2 ELHr -terminologies. Since cDiff (T1 , T2 ) cDiff ran(T1 , T2 ), followsTheorem 40 C v cDiff (T1 , T2 ) exists inclusion least onefollowing forms(i) C 0 v A,(ii) ran(r) u C 0 v A,(iii) v D0 ,(iv) r.> v D0 ,(v) ran(r) v D0cDiff (T1 , T2 ), r sig(), sig() concept name, C 0 subconcept CD0 subconcept D. Notice particular case (ii) C 0 EL-concept.Hence, case -instance difference, obtain following representation-concept difference. Assume T1 T2 given. Letset role inclusion -concept difference witnesses, cWtnR(T1 , T2 ), consistr v T1 |= r v T2 6|= r v s;set right-hand -concept difference witnesses, cWtnrhs(T1 , T2 ), consistexists EL-concept C either C v cDiff (T1 , T2 )additionally exists role name r ran(r) u C v cDiff (T1 , T2 ).set left-hand -concept difference witnesses, cWtnlhs(T1 , T2 ), consistexists EL-concept C v C cDiff (T1 , T2 ),role names r exists EL-concept C either r.> v CcDiff (T1 , T2 ) ran(r) v C cDiff (T1 , T2 ).set -concept difference witnesses definedrhslhscWtn (T1 , T2 ) = (cWtnR(T1 , T2 ), cWtn (T1 , T2 ), cWtn (T1 , T2 )).668fiThe Logical Difference Lightweight Description Logic ELObserve cWtn (T1 , T2 ) = (, , ) if, if, cDiff (T1 , T2 ) = . also obtainlhssets cWtnR(T1 , T2 ) cWtn (T1 , T2 ) coincide corresponding witness setsinstance difference, allows us re-use results developeddetecting instance differences.Lemma 50. Let T1 T2 normalised ELHr -terminologies signature.following holds:R1. cWtnR(T1 , T2 ) = iWtn (T1 , T2 ),lhs2. cWtnlhs(T1 , T2 ) = iWtn (T1 , T2 )rhs3. cWtnrhs(T1 , T2 ) iWtn (T1 , T2 )lhsProof. Point 1 follows directly definition. Proving cWtnlhs(T1 , T2 ) iWtn (T1 , T2 )rhscWtnrhs(T1 , T2 ) iWtn (T1 , T2 ) similar Lemma 10. Finally, provelhslhsiWtn (T1 , T2 ) cWtn (T1 , T2 ), assume iWtnlhs(T1 , T2 ). exists ELconcept D(a) ({A(a)}, D(a)) iDiff (T1 , T2 ). T1 |= v T2 6|= vlhsand, therefore, cWtnlhs(T1 , T2 ). argument r iWtn (T1 , T2 ) similar.presented polynomial-time algorithms compute iWtnlhs(T1 , T2 )rhsiWtnR(T,).Thus,remainsanalysecWtn(T,).12126.1 Tractability cWtnrhs(T1 , T2 )prove tractability cWtnrhs(T1 , T2 ) modifying ABox AT2 , introrhsduced prove tractability iWtnrhs(T1 , T2 ). Recall iWtn (T1 , T2 ) iff (T1 , AT2 , ) |=A(B ) B non-conjT2 (A) (cf. Lemma 45). satisfying conditioncWtnrhs(T1 , T2 ) since ABox AT2 , cannot always captured set EL-concepts(cf. Example 4). modification AT2 , motivated observation ABoxcontain individual range two distinct role names, EL-conceptsrather C ran -concepts sufficient capture consequences ABox. Thus,going modify AT2 , minimal way resulting ABox containindividual name range two distinct role names.Definition 51. ABox role-splitting pair assertions r(a, c), s(b, c)A, individual names a, b, c distinct role names r, s.following lemma states main property role-splitting ABoxes.Lemma 52. Let T1 T2 normalised ELHr -terminologies, signaturelet role-splitting -ABox (T1 , A) |= A(a) (T2 , A) 6|= A(a).cWtnrhs(T1 , T2 ).n,rann,ranProof. Lemma 36, exists n 0 T1 |= CA,av T2 6|= CA,av A.Assume first exist b obj(A) r r(b, a) A. Then,n,randefinition since role-splitting, ran(r) occurs CA,adirect scopen,ranexistential restriction r. Hence CA,a equivalent EL -concept,done. assume exists r(b, a) A. Then, since role-splitting,n,ranCA,aequivalent concept ran(r) u C, C EL -concept. caseT1 |= ran(r) u C v T2 6|= ran(r) u C v A, required.669fiKonev, Ludwig, Walther, & Wolter-ABox sig(A) NR 6= , define role-splitting unfoldingindividuals { ar | obj(A), r sig(A) NR } setting= { A(ar ) | A(a) A, r sig(A) NR } { r(as , br ) | r(a, b) A, sig(A) NR }.Example 53. Consider T1 = {ran(r) v A1 , ran(s) v A2 , B A1 u A2 }, T2 = , ={r, s, B} = {r(a, c), s(b, c)} Example 4. (T1 , A) |= B(c) (T2 , A) 6|=B(c). Notice role-splitting unfolding = {r(ar , cr ), r(as , cr ), s(br , cs ), s(bs , cs )}contain individual range one roleccrcsrrrbarbrbs(T1 , ) 6|= B(cr ), (T1 , ) 6|= B(cs ).apply role-splitting unfolding ABox , Figure 6. followingresult concept version Lemma 44 proved appendix reductionLemma 44. ABox AC corresponding EL-concept C introducedLemma 36. simplicity, consider signatures containing least one role name.Lemma 54. every normalised ELHr -terminology , signature NR 6= ,concept name non-conjunctive , role name r , EL -concepts Cfollowing conditions equivalent = C = ran(r) u C:6|= v A;r NR , (A )r obj(AT , ) (AD , aD ) ran(AT , , (A )r ).following lemma proved similarly Lemma 45, using Lemma 54 insteadLemma 44.Lemma 55. Let T1 T2 normalised ELHr -terminologies, signatureNR 6= . following conditions equivalent:cWtnrhs(T1 , T2 );exists r (T1 , AT2 , ) |= A((B )r ) B non-conjT2 (A).Proof. Assume cWtnrhs(T1 , T2 ). Then, either exists EL -concept CT1 |= C v T2 6|= C v A, additionally exists r T1 |=ran(r) u C v T2 6|= ran(r) u C v A. Hence, = C = ran(r) u C,respectively, T2 6|= v B, B non-conjT2 (A), Lemma 54, existsr (A )r obj(AT , ) (AD , aD ) ran(AT2 , , (B )r ). then, Lemma 42(T1 , AT2 , ) |= A((B )r ) (T1 , AD ) |= A(aD ) holds Lemma 36.converse direction, easy see (B )r obj(AT2 , ), B obj(AT2 , ),(AT2 , , (B )r ) ran(AT2 , , B ), implies (T2 , AT2 , ) 6|= A((B )r ) Lemma 44.Consequently, obtain cWtnrhs(T1 , T2 ) applying Lemma 52 using factABox AT2 , role-splitting.670fiThe Logical Difference Lightweight Description Logic ELFinally, obtain tractability result.Theorem 56. Let T1 T2 ELHr -terminologies signature. setcWtnrhs(T1 , T2 ) computed polynomial time.rhsProof. NR = , cWtnrhs(T1 , T2 ) = iWtn (T1 , T2 ), computedpolynomial time Theorem 46.Otherwise NR 6= result follows Lemma 55 fact AT2 ,constructed polynomial time size T2 .7. ELHr -Query Differenceinvestigate query difference ELHr -terminologies, introduce languageELran,u,u extends ELran universal role intersections roles. showconcept differences ELran,u,u correspond query differences ELHr . ELran,u,uprove analogue Theorem 40, states inclusion conceptdifference contains inclusion either left-hand side right-hand sideatomic. Using correspondence concept difference ELran,u,u querydifference ELHr obtain meaningful definition succinct representationquery difference qDiff (T1 , T2 ). Finally, provide polynomial-time algorithms deciding-query inseparability computing succinct representation query difference.7.1 ELran,u,u -Concept Differencestart section defining language ELran,u,u .Definition 57 (ELran,u,u ). Let u (the universal role) fresh logical symbol. C u,u concepts constructed using following syntax ruleC :=|C uD|R.C|u.C,NC , C, range C u,u -concepts R = r1 u u rn r1 , . . . , rn NRn 1. set ELran,u,u -inclusions consists concept inclusions C v roleinclusions r v s, C C ran -concept, C u,u -concept, r, NR .semantics additional constructors straightforward setting, interpretation I,(r1 u u rn )I = rI1 rnI ;uI = .Note regard universal role u logical symbol; i.e., u 6 NR sig(u.C) =sig(C) concept C. Assuming u logical symbol reflects fact firstorder translation uses non-logical symbols. example, signature first-ordertranslation x.A(x) u.A contain non-logical symbols exceptionitself.convenient decompose C u,u -concepts. set C u -concepts definedset C u,u -concepts without universal role. Every C u,u -concept C equivalent671fiKonev, Ludwig, Walther, & Wolterconcept form D0 u u.D1 u u u.Dk , D0 , . . . , Dk C u -concepts.see this, observe concept C subconcept u.D equivalent u.D u C 0 ,C 0 obtained C replacing occurrences u.D >. example,u r.(B u u.E) equivalent concept u.E u u r.(B u >).u,uu ) set C u,u (C u ) concepts whose signaturefollowing denote C(Ccontained .Clearly, every ELran -inclusion ELran,u,u -inclusion. addition, role conjunctionsuniversal role ELran,u,u -inclusions used capture differencesELHr -TBoxes cannot captured ELHr -inclusions.Example 58. first reconsider Example 8. RecallT1 = ,T2 = {A v r.B},= {A, B}.T2 |= v u.B T1 6|= v u.B and, universal role regardedlogical symbol, sig(A v u.B) . Thus, employing universal role ELran,u,usimulate query difference ({A(a)}, x.B(x)) using subsumption v u.B.Second, reconsider Example 9. RecallT1 = {A v s.>, v r1 , v r2 },T2 = {A v r1 .> u r2 .>},= {A, r1 , r2 }.T1 |= v (r1 u r2 ).> T2 6|= v (r1 u r2 ).>. Thus, simulate querydifference ({A(a)}, x.(r1 (a, x) r2 (a, x))) using subsumption v (r1 u r2 ).>.introduce appropriate notion -concept difference ELran,u,u .ran,u,uDefinition 59 (EL-difference). ELran,u,u-difference ELHr -TBoxes T1ran,u,uran,u,u-inclusions T1 |=(T1 , T2 ) ELT2 set cDiffT2 6|= .extend Lemma 39 concepts use universal role conjunctionsroles.Lemma 60. Let ELHr -terminology R.D C u -concept R = t1 u u tqconjunction role names. Assumelll|=ran(si ) uAj urk .Ck v R.D,1il1jn1kmCk , 1 k m, C ran -concepts l, m, n 0. least one followingconditions holds:(e1u ) exists rk , 1 k m, rk vT t1 ,. . . , rk vT tq , |= Ck uran(rk ) vD;(e2u ) exists Aj , 1 j n, |= Aj v R.D;(e3u ) exists rk , 1 k m, |= rk .> v R.D;(e4u ) exists si , 1 l, |= ran(si ) v R.D.672fiThe Logical Difference Lightweight Description Logic ELu universal role |= C v u.D, C C ran -conceptC u -concept, least one following holds:(e1u ) exists subconcept r.C 0 C |= C 0 u ran(r)v D;(e2u ) exists concept name C |= v u.D;(e3u ) exists role name r C |= r.> v u.D;(e4u ) exists role name r C |= ran(r) v u.D;(e5u ) |= C v D;(e6u ) exists subconcept (ran(r) u C 0 ) C |= r.C 0 v D.Theorem 61 (Primitive witnesses ELran,u,u ). Let T1 T2 ELHr -terminologiesran,u,usignature. cDiff(T1 , T2 ), either exist {r, s} sig()ran,u,ur v cDiff(T1 , T2 ) form C v D, one1. C 0 v2. v D0 , r.> v D0 ran(r) v D0member cDiff ran,u,u(T1 , T2 ), sig() concept name, r sig() role0ranname, C C -concept, D0 C u,u -concept, sig(C 0 ), sig(D0 ) sig().Proof. Let C v cDiff ran,u,u(T1 , T2 ), C C ran -concept C u,u -concept.prove result induction structure D. proof verydsimilarproof Theorem40 consider case = u.D1 only. Let C = 1il ran(si ) u1jn Aj u 1km rk .Ck . Then, Lemma 60, one (e1u )(e6u ) holds.Cases (e2u )(e4u ) directly entail existence inclusion Point 2theorem. case (e1u ) exists subconcept r.C 0 C T1 |= C 0 uran(r) v D1 .T2 6|= C 0 u ran(r) v D1 otherwise T2 |= r.C 0 v r.D1 , i.e.(T1 , T2 ). applyT2 |= C v would hold. Thus, C 0 u ran(r) v D1 cDiff ran,u,uinduction hypothesis D1 infer inclusion Point 1 Point 2 exists.(T1 , T2 ) otherwise T2 |= C vSimilarly, case (e5u ), C v D1 cDiff ran,u,uD1 , i.e. T2 |= C v due = u.D1 . applying induction hypothesis D1 ,obtain inclusion Point 1 Point 2 exists.Finally, case (e6u ) exists subconcept ran(r) u C 0 C T1 |= r.C 0 vD1 . Observe first every model T2 every C , exists d0(ran(r) u C 0 )I , implies exists d00 (r.C 0 )I . assumeT2 |= r.C 0 v D1 , would follow every model T2 every C ,exists d00 D1I , i.e. T2 |= C v u.D1 would hold. infer r.C 0 v D1ran,u,ucDiff(T1 , T2 ) applying induction hypothesis D1 , concludeinclusion Point 1 Point 2 exists.7.2 Query Difference Witnessesstart connecting concept differences ELran,u,u query differencesELHr -terminologies. direction query differences ELHr concept differencesELran,u,u straightforward: observe every assertion C(a) C C u,u -concept673fiKonev, Ludwig, Walther, & Wolterregarded Boolean conjunctive query qC,a . example, assertion (u.Aur.B)(a)equivalent conjunctive query xy.(A(x)r(a, y)B(y)) (details translationprovided appendix). obtain (where AC ABox defined Lemma 36):Lemma 62. two ELHr -TBoxes T1 T2 signature , C vcDiff ran,u,u(T1 , T2 ) if, if, (AC , qD,aC ) qDiff (T1 , T2 ).follows distinguish assertion C(a) C C u,u -conceptconjunctive query qC,a . follows Lemma 62 qDiff (T1 , T2 ) = ,cDiff ran,u,u(T1 , T2 ) = .come (considerably involved) direction query differences conceptdifferences ELran,u,u . following lemma provides rather abstract descriptioninclusions qDiff (T1 , T2 ) reflected members cDiff ran,u,u(T1 , T2 ) statinggiven signature.Lemma 63. two ELHr -TBoxes T1 T2 signature , qDiff (T1 , T2 ),(T1 , T2 ) sig(0 ) sig().exists 0 cDiff ran,u,uinterested reader extract detailed description proof givenappendix. proof Lemma 63 given appendix model-theoretic employsclose relationship conjunctive query entailment homomorphisms (Chandra& Merlin, 1977). intuition behind result, however, rather straightforward:(T , A) |= q[~a] conjunctive query q(~x) = ~y (~x, ~y ) ELHr -TBox , everymodel (T , A) mapping variables ~x ~y ~a-match q(~x) I. (T , A) models essentially forest-shaped: consisttree-shaped models attached ABox individuals (cf. Lutz et al., 2009).forest-shaped models, individuals ~y mapped individualsmapped trees attached ABox individuals. mapping, however,exists already conjunctive query q 0 q homomorphic image q 0 q 0essentially forest-shaped: individuals mapped ABox individuals form treesattached core q 0 mapped ABox individuals. words,obtain q 0 partitioning q core subsets correspond C u,u -concepts!Now, exists -ABox conjunctive -query q(~a) (T2 , A) |= q[~a](T1 , A) 6|= q[~a], find conjunctive -query q 0 behaviourq essentially forest-shaped. (A, q 0 ) one obtain required ELran,u,u inclusion C v D, captures subtree query q 0 (a C u,u -concept) C(a C ran -concept) ABox A. intuition last step exactlyLemma 36.note result holds general TBoxes terminologies.Lemma 63 Theorem 61, directly obtain following description primitive witnesses query differences.Theorem 64 (Primitive witness ELHr -query differences). Let T1 T2 ELHr terminologies signature. qDiff (T1 , T2 ), least one followingconditions holds (for individual names a, b):1. ({r(a, b)}, s(a, b)) qDiff (T1 , T2 ), r, sig();674fiThe Logical Difference Lightweight Description Logic EL2. (A, A(b)) qDiff (T1 , T2 ), concept name sig() ABoxsig(A) sig();3. (A, D(b)) qDiff (T1 , T2 ), singleton ABox C u,u -conceptsig(A), sig(D) sig().Observe Theorem 64 coincides Theorem 41 exception Point 3concept C u,u -concept. can, therefore, define following finiterepresentation -query difference. Assume T1 T2 given. Define setqWtnR(T1 , T2 ) role -query difference witnesses set role -instance differRence witnesses; i.e., qWtnR(T1 , T2 ) = iWtn (T1 , T2 );qWtnrhs(T1 , T2 ) right-hand -query difference witnesses set right-handrhs-instance difference witnesses; i.e., qWtnrhs(T1 , T2 ) = iWtn (T1 , T2 );qWtnlhs(T1 , T2 ) left-hand -instance difference witnesses setexists C u,u -concept C ({A(a)}, C(a)) qDiff lhs(T1 , T2 ) ru,uexists C -concept C ({r(a, b)}, C(c)) qDiff (T1 , T2 )c = c = b.set -query difference witnesses definedrhslhsqWtn (T1 , T2 ) = (qWtnR(T1 , T2 ), qWtn (T1 , T2 ), qWtn (T1 , T2 )).Theorem 64, qWtn (T1 , T2 ) = (, , ) if, if, qDiff (T1 , T2 ) = . Algorithmsrhscomputing qWtnR(T1 , T2 ) qWtn (T1 , T2 ) presented section instancedifference. thus remains consider qWtnlhs(T1 , T2 ).7.3 Tractability qWtnlhs(T1 , T2 )u,u -conceptsprove tractability qWtnlhs(T1 , T2 ) first capture expressive power Cusing stronger form simulation interpretations. Let I1 I2 interpretations. -simulation I1 I2 called global intersection preserving-simulation if, addition,every I1 exists d0 I2 (d, d0 ) S;(d, e) S, d0 I1 , R = {r | (d, d0 ) rI1 } =6 , exists e0(e, e0 ) (d0 , e0 ) rI2 r R.write (I1 , d)(I2 , e) exists global intersection preserving -simulationI1 I2 (d, e) S.Lemma 65. Let I1 I2 finite interpretations, signature, I1 , e I2 .u,u(I1 , d)C C: C I1 e C I2 .(I2 , e)checked polynomial time whether (I1 , d)(I2 , e).675fiKonev, Ludwig, Walther, & Wolterproof straightforward extension proof Lemma 29 polynomialtime algorithm deciding existence -simulations.observe Theorem 2 properties canonical model IK KB Kextended C u,u -concepts (in appendix, proof given C u,u -conceptswell). Namely, C u,u -concepts C0 :K |= C0 (a) if, if, aIK C0IK .|= C u v C0 if, if, xC,D C0IK .follows concept name ,qWtnlhs(T1 , T2 )(IK1 , a) 6(IK2 , a),Ki = (Ti , A) = {A(a)}, = 1, 2. also every role name rr qWtnlhs(IK1 , a) 6(T1 , T2 )(IK2 , a) (IK1 , b) 6 (IK2 , b)Ki = (Ti , A) = {r(a, b)}, = 1, 2. Thus, obtain following tractabilityresult:Theorem 66. Let T1 T2 ELHr -terminologies signature. setqWtnlhs(T1 , T2 ) computed polynomial time.8. Implementation Experimentssection, describe experimental evaluation theoretical work developed above. experiments employ CEX2 tool.4 CEX2, implementedpolynomial-time algorithms which, given acyclic ELHr -terminologies T1 T2 signature input, compute witnesses concept difference cDiff (T1 , T2 )instance difference iDiff (T1 , T2 ).5CEX2 written OCaml reasoner CB (Kazakov, 2009) internally usedclassification engine. implementation CEX2, employed algorithmsdeveloped paper. detail, instance difference case acyclic ELHr terminologies T1 T2 ,compute iWtnR(T1 , T2 ), CEX2 performs straightforward comparison roleinclusion chains entailed terminologies T1 T2 ;compute iWtnrhs(T1 , T2 ), CEX2 uses NotWitness algorithm Figure 7employs Theorem 48;compute iWtnlhs(T1 , T2 ), CEX2 checks existence -simulationcanonical models (Theorem 49).4. Available open-source license http://www.csc.liv.ac.uk/~michel/software/cex2/5. extended version CEX2 computing witnesses query difference qDiff (T1 , T2 ) wellpresented (Konev, Ludwig, & Wolter, 2012). addition, Konev et al. describe experiments comparingquery difference witnesses concept instance difference witnesses presentedpaper.676fiThe Logical Difference Lightweight Description Logic ELoutput iWtnlhs(T1 , T2 ) partitioned three sets:set left-hand atomic -instance difference witnesses, iWtnlhs,A(T1 , T2 ),defined set concept names exists EL-concept C({A(a)}, C(a)}) iDiff (T1 , T2 ) (equivalently v C cDiff (T1 , T2 ));set left-hand domain -instance difference witnesses, iWtnlhs,dom(T1 , T2 ),defined set role names r exists EL-concept C({r(a, b)}, C(a)) iDiff (T1 , T2 ) (equivalently, r.> v C cDiff (T1 , T2 ));set left-hand range -instance difference witnesses, iWtnlhs,ran(T1 , T2 ),defined set role names r exists EL-concept C({r(a, b)}, C(b)) iDiff (T1 , T2 ) (equivalently, ran(r) v C cDiff (T1 , T2 )).Obviously, holds that:lhs,AiWtnlhs(T1 , T2 ) iWtnlhs,dom(T1 , T2 ) iWtnlhs,ran(T1 , T2 ).(T1 , T2 ) = iWtnconcept difference case, recallRcWtnR(T1 , T2 ) = iWtn (T1 , T2 ),lhscWtnlhs(T1 , T2 ) = iWtn (T1 , T2 ),use algorithms instance case. also set(T1 , T2 ) = iWtnlhs,X (T1 , T2 )cWtnlhs,XrhsX {A, dom, ran}. compute iWtnrhs(T1 , T2 ), CEX2 exploits cWtn (T1 , T2 )rhsrhsiWtn (T1 , T2 ) (Lemma 50) first computes iWtn (T1 , T2 ) checks usingstraightforward variant NotWitness algorithm concept differences whethercWtnrhs(T1 , T2 ).following three subsections describe experiments conducted.experimental settings follows. programs run PCs equippedIntel Core 2 Duo E6400 CPU 3 GiB main memory. Version 2.0.1 CEX2used.8.1 Comparing Different Versions Snomed CTapplied CEX2 compare January 2009 (SM09a) July 2009 (SM09b) version Snomed CT. SM09a SM09b contain 310013 307693 concept names, respectively. versions use 62 role names, contain role inclusionsdomain range restrictions present. Consequently, one infer Corollary 47 iWtn (SM09b, SM09a) = cWtn (SM09b, SM09a). follows considercWtn (SM09b, SM09a) only.experiments used signatures ranging called Snomed CT subsets,employed UK deployment Snomed CT specific areas. compared SM09a SM09b 159 signatures computing cWtn (SM09b, SM09a)sets . considered signatures always contain 62 SnomedCT role names. comparisons resulted non-empty difference reproduced677fiKonev, Ludwig, Walther, & WolterTable 2. none cases, differences regarding role inclusions detected.Table 2, second column gives number concept names respective subset ,third fifth column number concept witness differences. Observenumber differences correlate size considered signatures , i.e.exist signatures somewhat comparable size, induce greatly varyingnumber difference witnesses (see e.g. subsets Diagnosis Manumat).order determine many difference witnesses computed CEX2 obtainedstraightforward comparison class hierarchies already, also computedsetsclsWtnlhs(SM09b, SM09a) = { | B : v B cDiff (SM09b, SM09a) }clsWtnrhs(SM09b, SM09a) = { B | : v B cDiff (SM09b, SM09a) }considered comparison signatures . results obtainedalso depicted Table 2. One see often great number differences cannotdetected considering classification difference only.last three columns Table 2, give CPU times required computingconcept witnesses:first, times given CEX2 directly applied full terminologiesSM09a SM09b;second, times given one first extracts -modules using module extraction tool MEX (Konev, Lutz, Walther, & Wolter, 2008) SM09a and, respectively,SM09b applies CEX2 extracted -modules. Observe -moduleextracted MEX -query (and, therefore, -concept -instance) inseparablewhole terminology. Thus, computed concept witnesses same.finally, times given if, addition computing concept witnesses fullterminologies SM09a SM09b, CEX2 also computes examples concept inclusionslogical difference explain witnesses. discuss feature CEX2below.One observe extracting MEX modules leads significant improvementperformance CEX2. course, signature large (e.g., DiagnosisFinding), resulting modules almost large Snomed CT effectless significant. Secondly, one observe additional computation example concept inclusions logical difference roughly doubles times neededcomparison.Finally, evaluate practical feasibility using ABox approach computesets iWtnrhs(SM09b, SM09a), implemented computation ABoxes ,together ABox reasoning algorithm checking second condition Lemma 45.tested implementation subsets Snomed CT used evaluating performance CEX2. limit size ABoxes , speedcomputations, first computed modules using MEX. results obtained678fiThe Logical Difference Lightweight Description Logic ELshown Table 3. size -modules computed MEX, i.e. T1 SM09b T2SM09a, shown columns two three, respectively. expected definition , , one observe number concept role membership assertionspresent ABoxes AT2 , grow large, even modules signaturesthousand concept names.8 41 considered subsets implementation ran available physicalmemory (indicated time value -) possible concept membership consequencesABox computed. Overall, observed longest execution time5 hours set Specmatyp. conclusion, one see straightforwardimplementation ABox approach practically useful terminologiessignatures thousand concept names.8.2 Comparing Different Versions NCI Thesaurusalso used CEX2 tool compare distinct versions NCI Thesaurus.distributed releases NCI Thesaurus contain language constructs partELHr (such disjunction value restriction). obtain ELHr -terminologies,removed inclusions contain non-ELHr constructor original terminologies.Typically, affected 5%-8% inclusions present distributed NCIversions. ELHr -versions generated way contain role inclusions welldomain range restrictions.Similarly work Goncalves et al. (2011), compared 71 consecutiveELHr -versions NCI Thesaurus ranging versions 03.10J 10.02d,exception 05.03F 05.04d, could parsed correctly. Version 10.03hlater versions NCI Thesaurus acyclic, hence, couldhandled CEX2 tool.two consecutive versions NCIn NCIn+1 within considered range,computed sets cWtn (NCIn+1 , NCIn ) iWtn (NCIn+1 , NCIn ) signatures =sig(NCIn ) sig(NCIn+1 ). overview set sizes cWtnrhs(NCIn+1 , NCIn )lhs,AcWtn (NCIn+1 , NCIn ) obtained found Figure 8. comparisonssorted chronologically along x-axis according release dates NCI ontologyversions, whereas corresponding number left-hand atomic difference witnessesright-hand difference witnesses found y-axis. One see number righthand difference witnesses remained fairly low throughout different versions. However,occasional spikes occurred number left-hand atomic difference witnessesmaximum value 33487 comparing versions 05.01d 05.03d. Moreover, nonecomparisons except shown Figure 9 left-hand role domain left-hand rolerange difference witnesses identified. Overall, witnesses regarding role inclusionsdetected found every two considered consecutive versions NCInNCIn+1 = sig(NCIn ) sig(NCIn+1 ),cWtn (NCIn+1 , NCIn ) = iWtn (NCIn+1 , NCIn ).running time 140 seconds 228 MiB memory required averagecomputing witnesses example inclusions iDiff (NCIn+1 , NCIn ). Computing witnesses example inclusions cDiff (NCIn+1 , NCIn ) average took 157 secondsused 228 MiB memory.679fiKonev, Ludwig, Walther, & WolterSubset NameAdminAdminprocCdacarestCrcareneurCrcarerespDevicetypDiagimgDiagnosisDrgadrconEndosfindEndosprocEpcream.6aEpenema.7aEpenema.7bEpeye.4Epiuds16FamhistFindingFoodadrconFfoodallerInvestLabinvestLabinvmethLabisolateLabmorphLabspecLabtopogLifestyleManumatNofoodallNonhumanPbclPbhllngPfProvadvSfSocpercirSpecmatypTreatmentVmpVtm| NC |7684319835516401082653941627587980091787340325622314161683832378468148393904379416313485412212727713090905036861839586611137910526136786883043660136672117|cWtnrhs|70128722627741013101000008118241111396611031503238667721246331020810241920|clsWtnrhs|50181826138811310100000524971115344581150323220410111116001088125500|cWtnlhs,A|296119726222131240947135332613131228159554925203380661451816982622134691342274158324692512213|clsWtnlhs|7011364228540647030000042006314954411333374661453169148013131402001080210874000Time (s)cWtn -full ontologies358.51344.60337.91399.57377.36369.20444.66844.261419.52363.23352.84337.41337.42337.50337.53337.26339.361559.23481.20379.42511.12382.32367.20671.36858.11360.801947.19445.75349.73421.23678.53395.27454.39337.68343.78338.13366.14380.19793.12342.70339.14Time (s)cWtn -module extraction9.898.246.7615.5812.248.0138.56486.5310.177.867.307.396.866.767.206.698.841366.087.747.0376.9012.4710.7014.148.2813.3838.0532.4915.367.1112.5012.187.997.128.197.448.9916.10330.4512.959.45Time (s)cWtnexamples654.12642.23556.41704.21680.51589.81775.372699.891708.49662.67573.66631.51556.31629.571236.841233.89633.945017.021516.47677.97769.93680.941290.831005.951113.701272.514463.05765.101224.92721.741907.701358.88761.00634.26569.56629.501300.47685.351315.231247.18633.50Table 2: Subset Comparisons T1 = SM09b T2 = SM09a Resulting Non-EmptyDifference680fiThe Logical Difference Lightweight Description Logic ELSubset NameAdminAdminprocCdacarestCrcareneurCrcarerespDevicetypDiagimgDiagnosisDrgadrconEndosfindEndosprocEpcream.6aEpenema.7aEpenema.7bEpeye.4Epiuds16FamhistFindingFoodadrconFoodallerInvestLabinvestLabinvmethLabisolateLabmorphLabspecLabtopogLifestyleManumatNofoodallNonhumanPbclPbhllngPfProvadvSfSocpercirSpecmatypTreatmentVmpVtm| NC ||sig(T1 ) NC ||sig(T2 ) NC ||{ A(a) | A(a) AT2 , }|(in thousands)|{ r(a, b) | r(a, b) AT2 , }|(in thousands)Time (s)768431983551640108265394162758798009178734032562231416168383237846814839390437941631348541221272771309090503686183958661113791052613678688304366013667211767463071322648452733617110071565888323148780914258513851531263238092716636420719308101321628145757106271182623311605990872884972488386310418566757129281111781197276556750312032363755206361911074156441836115348261446861585973136324400272364442559930210147163134558706427142264731164999188488793248738930141860675412871111612120187711669421235214885684361437434081786368017064321048641402140100341381927674532650461836048367382672682453855667235942501408851332727136986417430833732021332486271122523716810289683165401081480526515038301220141341095148821981910120830130521353874224114712032033628570329423741212713292613573445937827088915801057826299709291.751642.413.863110.073689.222381.0012503.632097.23143.8717.88315.580.320.0660.800.05137.34277.8011.578632.094131.027785.591275.48646.5926.9910110.2516410.12933.003.86518.47249.265819.2318306.562861.37Table 3: Performance ABox Approach Computing iWtnrhs(SM09b, SM09a)681fiKonev, Ludwig, Walther, & Wolter35000Nr Right-Hand WitnessesNr Left-Hand Atomic Witnesses300002500020000150001000050000lhs,AFigure 8: Sizes cWtnrhs(NCIn+1 , NCIn ) Consec (NCIn+1 , NCIn ) cWtnutive ELHr -versions NCIn NCIn+1 NCI ThesaurusT104.04j04.11a05.03d06.02d08.10e08.12d09.06eT204.03n04.09a05.01d06.01c08.09d08.11d09.05d| NC |34245359763802045582660526822970493| NR |769192113123123123|cWtnrhs|25210613841917749681305|cWtnlhs,A|49264023334871438190554726575|cWtnlhs,dom|129211131141|cWtnlhs,ran|129211131131lhsFigure 9: Detailed Results cWtnrhs(T1 , T2 ) cWtn (T1 , T2 ) Selected VersionsNCI Thesaurus using Shared Signatures = sig(T1 ) sig(T2 )682fiThe Logical Difference Lightweight Description Logic ELpeaks atomic left-hand difference witnesses mostly resulted changesgeneral concepts. mentioned already, Goncalves et al. (2011) provide indepth analysis NCI versions. systematic comparison methods used Goncalveset al. logical diff introduced paper would interesting, beyondscope paper. One interesting observation made is, however,peak atomic left-hand witnesses observed versions 05.01d 05.03dcorrelates fact according Goncalves et al. large number non-redundantaxioms added version 05.03d. However, comparable number non-redundantaxioms also added version 04.12g, peak atomic left-hand right-handwitnesses observed analysis.8.3 Scalability Analysisdemonstrated previous sections CEX2 capable finding logical difference two unmodified versions Snomed CT distinct versionsNCI thesaurus restricted ELHr . order see CEX2s performance scales,also tested randomly generated acyclic terminologies various sizes. randomlygenerated terminology contains certain number defined- primitive concept namesrole names. ratio concept equations concept inclusions fixed,ratio existential restrictions conjunctions. random terminologiesgenerated varying number defined concept names using parameters SM09a:62 role names; equality-inclusion ratio 0.525; exists-conjunction ratio 0.304.every chosen size, generated 10 samples consisting two random terminologiesdescribed above. applied CEX2 find logical difference two terminologies joint signature. Figure 10 shows average memory consumption CEX210 randomly generated terminologies various sizes. 10(a) maximum lengthconjunctions fixed two (M=2), 10(b) number conjuncts conjunction randomly selected two M. seen performanceCEX2 crucially depends length conjunctions. 10(b), curves breakpoint CEX2 runs physical memory6 . instance, case M=22,happens terminologies 7 500 defined concept names. Finally, notetime required CEX2 compare two random terminologies highly variedacross different samples. maximum time required CEX2 11 333 seconds.8.4 Additional User Support Analysing Differencesfar discussed experiments CEX2 one computes set conceptinstance difference witnesses two terminologies. Clearly, witnessesprovide sufficient information detailed analysis logical difference twoterminologies. thorough analysis, required consider examplescDiff (T1 , T2 ) iDiff (T1 , T2 ) show certain concept names concept/instancedifference witnesses. Thus, whenever searches concept namesexists C C v cDiff (T1 , T2 ), CEX2 output example concept inclusionsC v cDiff (T1 , T2 ). Similarly, requested, CEX2 also compute example inclusions6. cases classification terminologies CB already requires 3 GiBmemory.683fiKonev, Ludwig, Walther, & Wolter120035003000Memory Consumption MiBMemory Consumption MiB10008006004002500200015001000200500M=10M=2M=220Number Concept Names95008500750065005500450035002500150050010000300005000070000900001100001300001500001700019 000002100002300002500002700002900031 000003300003500000Number Concept Names(a) Short Conjunctions(b) Long ConjunctionsFigure 10: Memory Consumption CEX2 Randomly Generated Terminologiesillustrating left-hand concept differences v C, r.> v C, ran(r) v C, examplesinstance difference case. know Example 12 even minimal examplesexponential size input terminologies. practice, however, Snomed CTNCI additional computation example inclusion every concept/instancedifference witness doubles times required computation. describedalready, observed Table 2, computation times examplesshown last column computation times without examples shown7th column. examples computed CEX2 often reasonable size. instance,consider subset Specimen Material Type (Specmatyp) Table 2, holds(i) exist 10 right-hand -concept witnesses, i.e. |cWtnrhs(SM09b, SM09a)| = 10;(SM09b, SM09a),(ii) set left-hand atomic -concept difference witnesses, cWtnlhs,Acontains 46 concept names.Point (i) (ii), longest concepts C, C v cDiff (SM09b, SM09a)v cDiff (SM09b, SM09a) computed CEX2 twelve concept rolename occurrences (thus far smaller exponential worst case suggests).computed difference witnesses also example concept inclusionswitnesses, interest explain example concept inclusion entailed oneterminology other. Computing minimal subsets terminology entailexample concept inclusion promising approach explaining logical differencesalso known axiom pinpointing justification. supported CEX2,investigated extensively various description logics including EL (Schlobach & Cornet,2003; Baader, Penaloza, & Suntisrivaraporn, 2007; Kalyanpur, Parsia, Horridge, & Sirin,2007; Horridge, Parsia, & Sattler, 2010; Penaloza & Sertkaya, 2010). illustrateapproach, consider subset Specimen Material Type (Specmatyp) Table 2.CEX2 outputsVenipunctureForBloodTest cWtnlhs,A(SM09b, SM09a).684fiThe Logical Difference Lightweight Description Logic EL(1)LaboratoryTest v LaboratoryProcedure u EvaluationProcedureBloodTest LaboratoryTest u roleGroup. hasSpecimen. BloodSpecimen(2)(3) VenipunctureForBloodTest (roleGroup.hasFocus .BloodTest)u Venipunctureu (roleGroup.((procedureSiteDirect.VenousStructure)u (method.PunctureAction)))Figure 11: Minimal Axiom Setalso computes following concept inclusion (slightly simplified hand) member cDiff (SM09b, SM09a):()VenipunctureForBloodTestv roleGroup.hasFocus.EvaluationProcedureUsing axiom pinpointing one compute minimal set inclusions SM09bentails concept inclusion above; set shown Figure 11. Axioms 2 3terminologies, SM09a containsLaboratoryTest v LaboratoryProcedureinstead Axiom 1, explains difference two terminologies. Noteconcept role names shaded grey. seen interaction-concepts heavily depends inclusions built mainly non-concepts; actually none inclusions required derive () -inclusion.finally note CEX2 text-based tool. order make accessibleontology users, Protege plugin, LogDiffViz7 , created, calls CEX2 visualisesontology versions differences hierarchical structure. LogDiffViz alsoprovides basic axiom pinpointing. plugin distributed self-contained Java archivefile (JAR) CEX2 bundled.9. Related Workdescribe relationship work presented paper existing worklogical difference inseparability ontologies. Related work versioning distinction syntactical, structural, logic-based approaches versioningdiscussed introduction already presented here. problemdeciding whether two ontologies -inseparable signature investigated many ontology languages different notions inseparability conceptinseparability, instance inseparability, conjunctive query inseparability, model-theoreticinseparability (i.e., -reducts models first ontology coincide -reductsmodels second ontology). Inseparability also closely related notion conservative extensions since one ontology conservative extension another ontologycontains ontology subset inseparable w.r.t. signature7. Available http://protegewiki.stanford.edu/wiki/Logical_Difference_Vizualiser_(LogDiffViz)685fiKonev, Ludwig, Walther, & Woltersmaller ontology. Thus, algorithmic results deciding conservativity directly relevant inseparability well. tractability results presented paper sharpcontrast known results. start general EL-TBoxes: general ELTBoxes deciding inseparability conservative extensions ExpTime complete problemsconcept, instance conjunctive queries. problems undecidable modeltheoretic inseparability model-theoretic conservative extensions (Lutz & Wolter, 2010).(We note, however, model-theoretic case unexpected positive algorithmic resultsobtained Konev, Lutz, et al., 2008, acyclic EL ALC extensionsinverse roles.) ALC standard extensions without nominals deciding conceptinseparability conservative extensions 2ExpTime-complete (Ghilardi, Lutz, & Wolter,2006; Lutz et al., 2007; Lutz & Wolter, 2011) ALCQIO deciding concept inseparability conservative extensions becomes undecidable (Lutz et al., 2007; Cuenca Grauet al., 2008). Nothing known ALC complexity inseparability instanceconjunctive queries. DL-Lite dialects (Calvanese, Giacomo, Lembo, Lenzerini, &Rosati, 2006), complexity concept, instance, query inseparability rangesPSpace-hard (and ExpTime) description logic underlying OWL 2 QL standard, NP-complete DL-Litehorn , p2 -complete DL-Litebool (Konev, Kontchakov,Ludwig, Schneider, Wolter, & Zakharyaschev, 2011; Kontchakov et al., 2010). DLLitebool model-theoretic inseparability decidable (Kontchakov et al., 2010) DLLitecore concept, instance, query inseparability PTime (Konev et al., 2011).contrast work presented paper, however, attempt made presentlogical difference user two ontology inseparable. mentioned above,work Konev et al. (2012), CEX2 extended conjunctive query difference caseacyclic ELHr -terminologies various experiments based NCI thesaurusdiscussed.work discussed far concerned logical difference inseparability description logic TBoxes. difference description logic conceptsinvestigated, example, work Teege (1994), Brandt, Kusters, Turhan(2002) besides interest kind difference problems considered welltechniques employed rather different. Inseparability conservativityontologies given ontology languages expressive description logics (including first-order logic) considered work Kutz Mossakowski (2008,2011). Similar relationships theories also investigated answer setprogramming (Pearce & Valverde, 2004; Eiter, Fink, & Woltran, 2007; Pearce & Valverde,2012).Finally, note Lemma 15 ABox constructed Figure 3 appear capture describe fundamental properties EL ELHr -terminologies.applied investigate seemingly unrelated problems query containment ontologybased data access using EL-terminologies (Bienvenu, Lutz, & Wolter, 2012b) first-orderrewritability instance queries (Bienvenu, Lutz, & Wolter, 2012a).10. Conclusionpaper, presented polytime algorithms decide concept, instance,query-inseparability w.r.t. signature ELHr -terminologies compute represen686fiThe Logical Difference Lightweight Description Logic ELtation difference non-empty. Experiments using CEX2 based SNOMEDCT NCI show outputs given algorithm mostly reasonable sizeanalysed users. Many extensions, applications, open problems remainexplored. mention them:(1) motivated study -inseparability terminologies problem comparing different versions terminology regarding saycertain signature. potential promising applications found areadecomposing composing ontologies. example, importing ontologyontology 0 (i.e., forming 0 ) often important ensure 0interfere signature . words, 0 conservative extensionsense consequences 0 signature coincideconsequences (Cuenca Grau et al., 2008; Ghilardi et al., 2006; Vescovo,Parsia, Sattler, & Schneider, 2011). observed already, -inseparability generalises conservative extensions and, therefore, algorithms used check whetherone terminology conservative extension another terminology. Algorithms checkingconservative extensions also used extract modules ontologies (Cuenca Grauet al., 2008; Kontchakov, Pulina, Sattler, Schneider, Selmer, Wolter, & Zakharyaschev, 2009;Konev et al., 2011). would interest explore applications inseparabilitytesting algorithms extract modules terminologies check conservativity.(2) Inseparability defined paper mean one terminologyreplaced another terminology every context. various applications inseparabilitymodularity important ensure T1 T2 -inseparable, T1T2 -inseparable well, ontology . called replacementproperty Konev, Lutz, Walther, Wolter (2009) exploited discussed,example, work Cuenca Grau et al. (2008) Kontchakov et al. (2010).notions inseparability introduced paper replacement property.see this, let = {A, A0 , B, B 0 }T1 =v r.BA0 r.B 0T2 =v r.BA0 v r.B 0.T1 T2 -query inseparable (and, therefore, -concept -instance inseparable),T1 even -concept inseparable T2 , = {B v B 0 }. Indeed,observe (T1 ) |= v A0 , (T2 ) 6|= v A0 .important open research problem determine complexity of, developalgorithms strong versions inseparability replacement property ELELHr -terminologies.(3) ELHr rather weak description logic. would great interest explorefar techniques developed ELHr applied ontologies contain additional constructors, still consist mainly ELHr -inclusions. unlikely tractablesound complete algorithms interesting extensions exist, seems worth exploring algorithms sound incomplete extensions algorithms presentedpaper. results direction presented Goncalves, Parsia, Sattler(2012).687fiKonev, Ludwig, Walther, & WolterAcknowledgmentsresearch supported EPSRC grant EP/H043594/1. would like thankWilliam Gatens development LogDiffViz Protege plugin three anonymousreviewers helpful comments.Appendix A. Proofs Section 2Lemma 1 every terminology , one construct polynomial time normalisedterminology 0 polynomial size |T | sig(T ) sig(T 0 ), 0 |= , everymodel exists model J 0 = J X = X J everyX sig(T ). Moreover, 0 acyclic acyclic.Proof. Given terminology , construct normalised terminology 0 five steps follows:First, remove occurrences > conjunctions, replace C occurrence r.C,C concept name >, fresh concept name add conceptdefinition C terminology. Repeat last step exhaustively.Second, replace every ri .Bi inclusion right-hand side form F ur1 .B1 u u rm .Bm (m 1), Bi either concept name Bi = >, Fconjunction concept names F 6= > 2, fresh concept nameBi0 add concept definition Bi0 ri .Bi terminology.Third, replace every inclusion form r.> two inclusions v r.>r.> v terminology.Fourth, consider concept name sequences B0 , . . . , Bn1F0 , . . . , Fn , Fi conjunctions concept names, terminologycontains concept definitions F0 Bi Fi+1 , < n, Bi conjunctFi conjunct Fn . Let Fn0 conjunction concept names Fn except A. Let,0recursively, Fi1result replacing conjunct Bi1 Fi1 conjunction0Fi , 1 n. Replace concept definition F0 terminologyprimitive concept definition v F00 .Fifth, inclusion F , v F , r.> v F , ran(r) v F , Fconjunction concept names, replace every conjunct B F B F 0terminology, F 0 conjunction non-conjunctive concept names, F 0 .see construction indeed yields normalised terminology 0 , observesteps 1, 2, 3 ensure inclusion one following forms: r.B,F , E v r.B, E v r.>, E v F , B concept name, E either conceptname, form s.>, ran(s), F conjunction (possibly conjunctive)concept names. Step 4 breaks cycles concept definitions Step 5 takes careconjuncts conjunction concept names F right-hand side inclusionform F , v F , r.> v F , ran(r) v F non-conjunctive concept names.readily verified 0 acyclic acyclic none steps introduces cyclesconcept definitions.show 0 obtained polynomial time 0 polynomialsize |T |. Let n number inclusions c maximal length inclusionsright-hand side . Clearly, steps 1, 2 3 increase numberinclusions c n, raising total number inclusions 4nc. Steps 4688fiThe Logical Difference Lightweight Description Logic EL5 increase number inclusions, length right-hand sides.length right-hand side inclusion increase sum lengthsright-hand sides inclusions, i.e., 4nc2 upper bound right-handside. upper bound running time steps constructiontherefore 16n2 c3 . Hence, size 0 running time constructionO(n2 c3 ).Notice every new concept name occurs left-hand side unique conceptdefinition C 0 . Thus, every model expanded model J 0interpreting fresh concept names sig(T 0 ) \ sig(T ) setting AJ = C .Moreover, readily checked 0 |= .prove extended version Theorem 2 according EL-conceptsconcepts form ran(r) evaluated correctly canonical model IK ,also C u,u -concepts (which introduced Definition 57).Theorem 2[Extended Version] Let K = (T , A) ELHr -KB.1. IK model K;2. IK computed polynomial time size K;3. xC,D IK obj(A), C0 C u,u -concept form ran(r),K |= C0 (a) if, if, aIK C0IK .|= C u v C0 if, if, xC,D C0IK .Proof. Point 2 follows fact instance checking ELHr done polynomial time.first prove Point 3 EL-concepts C0 sub(T ). proof simultaneousinduction construction C0 . interesting step C0 = r.D0 .start proof direction left right. Assume first K |= C0 (a).(a, xran(r),D0 ) rIK . |= (ran(r) u D0 ) v D0 . Thus, inductionhypothesis, xran(r),D0 D0IK . C0IK , required. assume |= C uD v C0 .(xC,D , xran(r),D0 ) rIK . |= (ran(r) u D0 ) v D0 . inductionhypothesis, xran(r),D0 D0IK . xC,D C0IK , required.Conversely, assume aIK C0IK . exists IK (aIK , d) rIKD0IK . Assume first = b obj(A). induction hypothesis, K |= D0 (b).exists s(a, b) vT r. Thus, K |= C0 (a), required. Assume= xran(s),F . K |= s.F (a), vT r xran(s),F D0IK . inductionhypothesis, |= ran(s) u F v D0 . Thus, K |= C0 (a), required.assume xC,D C0IK . exists xran(s),F |= C u v s.F , vT rxran(s),F D0IK . induction hypothesis, |= ran(s) u F v D0 . Thus |= C u vr.D0 , required.prove Point 3 concepts form C0 = ran(r). Assume K |= (ran(r))(a).exist b s(b, a) vT r. ran(r)IK . Conversely,689fiKonev, Ludwig, Walther, & Wolterassume ran(r)IK . Then, definition IK , exist b s(b, a)vT r. Hence K |= (ran(r))(a), required.Assume |= C u v ran(r). have, C = ran(s), vT r. xC,Dran(r)IK since path WK tail xC,D . converse direction similar.follows proved far IK model (T , A). Thusproved Point 1, remains prove Point 3.prove Point 3 arbitrary C u,u -concepts C0 . interesting step C0 = S.D0 ,= r1 u u rn .Assume first K |= C0 (a). C0IK since IK model K. Similarly,|= C u v C0 , xC,D C0IK since xC,D (C u D)IK IK model .Conversely, assume C0IK . exists IK (aIK , d) IKD0IK . Assume first = b obj(A). induction hypothesis, K |= D0 (b).every ri , 1 n, exists si si (a, b) si vT ri . Thus, K |= C0 (a),required.Assume = xran(s),F . K |= s.F (a), vT ri 1 nxran(s),F D0IK . induction hypothesis, |= ran(s) u F v D0 . Thus, K |= C0 (a),required.assume xC,D C0IK . exists xran(s),F |= C u v s.F , vT ri ,1 n, xran(s),F D0IK . induction hypothesis, |= ran(s) u F v D0 . Thus|= C u v S.D0 , required.Appendix B. Proofs Section 5proofs, require models infinite sets concepts. introduce notationwell known result existence minimal models. Let (possiblyinfinite) set C ran -concepts (which introduced Definition 32), ELHr -TBox,either C u,u -concept (which introduced Definition 57) C ran -concept. write|= say included w.r.t. if, every model ,DI follows C C . following observation follows factC u,u C ran -concepts equivalent Horn formulas (in sense ChangKeisler, 1990):Lemma 67. ELHr -TBoxes sets C ran -concepts exists modelfollowing equivalent, C u,u C ran -concepts D:|= D;DI .come proof Lemma 36. convenience reader formulateresult again.Lemma 36. every ELHr -TBox , ABox A, C ran -concepts C0 D0 ,a0 obj(A):n,ran(T , A) |= D0 (a0 ) if, if, exists n 0 |= CA,av D0 ;0690fiThe Logical Difference Lightweight Description Logic EL|= C0 v D0 if, if, (T , AC0 ) |= D0 (aC0 ).n,ran(a0 )Proof. prove Point 1. direction right left observe |= CA,a0n,rann 0. Thus, |= CA,a0 v D0 implies (T , A) |= D0 (a0 ).ran |= . Then, using compactness,assume (T , A) |= D0 (a0 ). show CA,a00n,ranfind n 0 |= CA,a0 v D0 , required.ran 6|= . Take, every obj(A), model pointAssume CA,a00ran |= C.da C ran -concepts C: da C Ia if, if, CA,amodels exist Lemma 67. may assume mutually disjoint. Takefollowing union models Ia := aobj(A) Ia ;AI = aobj(A) AIa , NC ;rI = aobj(A) rIa {(da , db ) | r0 (a, b) A, r0 vT r}, r NR ;aI = da , obj(A).Claim 1. C ran -concepts C obj(A) following holds Ia :C Ia iff C .proof induction construction C. interesting cases C = ran(r)C = r.D direction right left.LetC assume first C = ran(r). Let C Ia (d0 , d) rI .(d0 , d) aobj(A) rIa , claim follows definition. Otherwise, = da , d0 = dbn,ranevery n 0. Hence,b r0 (b, a) r0 vT r. Thus, ran(r0 ) CA,aranCA,a |= ran(r) obtain C .Assumer.D C Ia . Take d0 (d, d0 ) rI d0 DI .C =00(d, ) a0 obj(A) r , C Ia follows immediately induction hypothesis.Otherwise, = da d0 = db b r0 (a, b) r0 vT r. inductionran |= D. compactness, exists concept E C ranhypothesis, d0 DIb . Hence, CA,bA,bn,ranevery n > 0.|= E v D. r0 (a, b) A, obtain r0 .E CA,aran |= r 0 .D obtain C Ia using r 0 v r. finishes proofthen, CA,aclaim.Now, C v , let C , i.e. Ia obj(A).Claim 1 C Ia , implies DIa C Ia DIa . concludeDI applying Claim 1 again. Similarly, one show C = DI everyC rI sI every r v . follows model .construction I, (aI , bI ) rI every r(a, b) A. Moreover, A(a)ran |= A, implies AIa aI AIobj(A), holds CA,aran 6|= ,claim. thus infer model (T , A) 6|= D0 (a0 ) CA,a00Iaimplies da0 6 D0 0 aI0 6 D0I , Claim 1. Hence, (T , A) 6|= D0 (a0 )derived contradiction.proof Point 2 simple application definition.691fiKonev, Ludwig, Walther, & Wolterprove cut elimination, correctness, completeness calculus ELHrgiven Figures 1 5. start basic observations, easily provedinduction length derivations.Lemma 68. ELHr -terminology , C ran -concepts C, role names r,1. ` > v D, ` C v D;2. ` C v v CA CA , ` C v CA ;3. ` C v r.D ` C v r.(D u ran(r));4. ` C v r.D, r.> v B , ` C v B;5. ` C v ran(r) ran(r) v , ` C v A;6. ` C v r.D, r v , ` C v s.D;7. ` C v ran(r) r v , ` C v ran(s).Lemma 69 (Cut elimination). ELHr -terminology , C ran -concepts C, D, E,` C v ` v E ` C v E.Proof. Let D1 derivation C v D2 derivation v E. Let Lilength Di , = 1, 2. proof lemma induction lexicographicalordering pairs (L2 , L1 ).case L2 = 0 L1 = 0, well cases L2 ends oneAndL1, AndL2, AndR, Ex, DefL, DefR PDefL virtuallyproof Hofmann (2005). Assume D2 ends Dom, last sequent formr.D0 v E, sequent B v E. Lemma 68, Item 4, ` C v r.D0implies ` C v B, induction hypothesis, ` C v E.cases D2 ends ExRan, Ran, Sub, RanSub dealtsimilar way using Lemma 68, Items 3, 57.Theorem 38. Let ELHr -terminology; C0 D0 C ran -concepts. |=C0 v D0 if, if, ` C0 v D0 .Proof. easily checked proof system rules sound ` C0 v D0 ,|= C0 v D0 .Conversely, assume |= C0 v D0 . prove ` C0 v D0 constructinterpretation based derivability sequents . show model. consequence obtain C0I D0I conclude ` C0 v D0 basedproperties I.domain set well-formed pairs x = hC, RC i, C C ran -conceptRC finite set role nameslNR : ` (C uran(r)) v ran(s), RC .rRC692fiThe Logical Difference Lightweight Description Logic ELintroduce following abbreviation. LetlRan(RC ) =ran(r).rRCC ran -concepts C interpretedI(C) = {hD, RD | ` (D u Ran(RD )) v C},r NR interpretedI(r) = {(hC, RC , hD, RD i) | r RD` (C u Ran(RC )) v r.(D u Ran(RD ))}.Note I(C) nonempty every C: consider R0C = {s NR | ` C v ran(s)}.0finite, R0C finite.Notice that, Ax AndR, ` C v C u Ran(RC ) so,Lemma 69, ` (C u rR0 ran(r)) v ran(s), s, ` C v ran(s), R0C .Cffffis, C, R0C well-formed pair and, obviously, C, R0C I(C).show I(C) = C C ran -concepts C. proof inductionconstruction C.1. I(>) = .well-formed pair hC, RC i, ` C u Ran(RC ) v > axiom.2. I(C u D) = I(C) I(D).Let hC, RC I(D1 u D2 ), ` (C u Ran(RC )) v (D1 u D2 ). Since ` (D1 u D2 ) vD1 , Lemma 69, ` (C u Ran(RC )) v D1 , is, hC, RC I(D1 ). Similarly,hC, RC I(D2 ).Conversely, suppose hC, RC I(D1 ) hC, RC I(D2 ) holds, is, ` (C uRan(RC )) v D1 ` (C uRan(RC )) v D2 . AndR, ` (C uRan(RC )) v (D1 uD2 ),is, hC, RC I(D1 u D2 ).3. I(r.C) = {x | I(C) : (x, y) I(r)}.Suppose well-formed pair hD, RD hD, RD I(r.C), `(D u Ran(RD )) v r.C. Then, Lemma 68, Item 3, ` (D u Ran(RD )) v r.(C u ran(r)).Consider RrC = {s NR | ` (C u ran(r)) v ran(s)}. Clearly, r RrC and, similarlyargument R0C above, hC, RrC well-formed pair. Ax AndR, `Curan(r) v CuRan(RrC ), Ex, ` r.(Curan(r)) v r.(CuRan(RrC )) Lemma 69,` (D u Ran(RD )) v r.(C u Ran(RrC )). Then, definition, (hD, RD , hC, RrC i) I(r)and, since ` (C u Ran(RrC )) v C, hC, RrC I(C).Conversely, let (hD1 , RD1 , hD2 , RD2 i) I(r) hD2 , RD2 I(C), is, ` (D1 uRan(RD1 )) v r.(D2 uRan(RD2 )), r RD2 , ` (D2 uRan(RD2 )) v C. Ex` r.(D2 u Ran(RD2 )) v r.C, and, Lemma 69, ` (D1 u Ran(RD1 )) v r.C,is, hD1 , RD1 I(r.C).4. I(ran(r)) = {y | x : (x, y) I(r)}.First show I(ran(r)) = {hC, RC | r RC }. r RC , `C u Ran(RC ) v ran(r), is, I(ran(r)) {hC, RC | r RC }. Suppose hC, RCI(ran(r)), is, ` (C u Ran(RC )) v ran(r). Then, since hC, RC well-formed pair,r RC , is, I(ran(r)) {hC, RC | r RC }.693fiKonev, Ludwig, Walther, & WolterSuppose hC, RC I(ran(r)), is, hC, RC r RC . Letdenote (C u Ran(RC )). induction length derivations one see sequentform r.D v ran(s) derivable NR . Therefore, hr.D, wellformed pair (hr.D, , hC, RC i) I(r). Conversely, let (hD1 , RD1 , hD2 , RD2 i) I(r)then, particular, r RD2 . is, hD2 , RD2 I(ran(r)).show model . need show axioms trueI.1. I(X) I(CX ), whenever X CX X v CX .Let hC, RC I(X), is, ` (C u Ran(RC )) v X. Lemma 68, Item 2, `(C u Ran(RC )) v CX , is, hC, RC I(CX ).2. I(CX ) I(X), whenever X CX .Let hC, RC I(CX ), is, ` (C u Ran(RC )) v CX . Since Ax DefR` CX v X, Lemma 69, ` (C u Ran(RC )) v X, hC, RC I(X).3. (x, y) I(r) I(A), whenever ran(r) v .Let (hC, RC , hD, RD i) I(r), is, ` (C u Ran(RC )) v r.(D u Ran(RD ))r RD . Since r RD and, as, Ax Ran, ` ran(r) v A, AndL1, AndL2` (D u Ran(RD )) v A, is, hD, RD I(A).4. (x, y) I(r) x I(B), whenever r.> v B .Let (hC, RC , hD, RD i) I(r), is, ` (C u Ran(RC )) v r.(D u Ran(RD ))r RD . Notice that, Lemma 68, Item 4, ` (C u Ran(RC )) v B, is,hC, RC I(B).5. I(s) I(r), whenever v r .Let (hC, RC , hD, RD I(r)), ` (C uRan(RC )) v r.(DuRan(RD )) r RD .Lemma 68, Item 6, ` (C u Ran(RC )) v s.(D u Ran(RD )). Since r v ,Ax RanSub, ` ran(r) v ran(s) ` (D u Ran(RD )) v ran(s) AndL1AndL2. Since hD, RD well-formed, RD . Thus, (hC, RC , hD, RD i) I(s)ff|= C0 v D0 , I(C0 ) I(D0 ). Since C0 , R0C0 I(C0 ),ffC0 , R0C0 I(D0 ), ` (C0 u Ran(R0C0 )) v D0 . ` C0 v C0 u Ran(R0C0 ),` C0 v D0 Lemma 69.Proof Lemma 44. Let normalised ELHr -terminology signature. Additionally, let -ABox, sig(T ) non-conjunctive obj(A).direction (1.) (2.), direct consequence construction ,b obj(A) B sig(T ) non-conjunctive (T , A) 6|= B(b)B obj(AT , ).Assume (T , A) 6|= A(a). obj(AT , ). define -range simulation setting,b obj(A) B sig(T ) non-conjunctive B obj(AT , ) :(b, B ) if, if, (T , A) 6|= B(b),(b, ) b obj(A).show indeed -range simulation (a, ) verifyingconditions (S1)(S3) (RS) introduced page 663 hold.694fiThe Logical Difference Lightweight Description Logic EL(S1)(T , A) 6|= A(a) obj(AT , ), immediately follows (a, ) S.(S2) Let (b, ) B(b) B . prove B() , .= B B sig(T ) non-conjunctive , obtain definition(T , A) 6|= B(b). Moreover, holds B 6 preC(B) otherwise (T , A) |= B(b).Thus, definition , (B) B(B ) , . = , immediatelyfollows B( ) , definition , .(S3) Now, let (b, ) r(b, b0 ) r . prove exists0 obj(AT , ) (b0 , 0 ) r(, 0 ) , . = , immediately followsdefinition , r( , ) , (b0 , ) holds definition S.= B B sig(T ) non-conjunctive follows definition(T , A) 6|= B(b). Additionally, infer r 6 preDom(B) otherwise(T , A) |= (r.>)(b) would imply (T , A) |= B(b).Consider cases B defined . B pseudo-primitive , obtaindefinition , (B) r(B , ) , holds (b0 , ) definitionS.B r0 .B 0 , distinguish following two cases. r 600preRole(r ), obtain r \ (preRoleT (r ) preDomT (B)) thus r(B , ) ,definition , holds (b0 , ) definition S. case00r preRole(r ), r preRoleT (r ) \ preDomT (B). Furthermore, (T , A) 6|=00B(b) (T , A) 6|= (r .B )(b), easy see must exist Bi00 non-conjT (B 0 )0000 0r 6 preRan(Bi ) (T , A) 6|= Bi (b ). r(B , Bi00 ) ,0definition , (B) (b , Bi00 ) definition S.(RS) Let (b, ) r(c, b) r . show exists0 r( 0 , ) , . = B B sig(T ) non-conjunctive , obtaindefinition (T , A) 6|= B(b). Furthermore, r 6 preRan(B)otherwise (T , A) |= B(b). Thus, definition , (B) r( , B ) , .= , follows definition , r( , ) , .converse direction (2.) (1.), assume obj(AT , ) (A, a) ran(AT , , ). sufficient show nn,ran6|= CAvA, ,Aimplies (T , , ) 6|= A(A ) Lemma 36. obtain Lemma 42(T , A) 6|= A(a) holds.Thus, prove induction n every concept name B sig(T )n,rannon-conjunctive B obj(AT , ), 6|= CAv B., ,BLet n = 0 B sig(T ) non-conjunctive B obj(AT , ). followslll0,ran0CA=Buran(s)uran(s), ,BB 0 \preC(B)s\preRan(B)695Ar.BTBnon-conjT (B)spreRole(r)\(preDomT (A)preRanT (B))fiKonev, Ludwig, Walther, & Wolter0,ranHence, one see every subconcept form ran(s) occurs CA,, ,Bobtain 6 preRan(B). B non-conjunctive , holds either Bpseudo-primitive B r0 .B 0 . Hence, Lemma 39 conclude0,ran6|= CAv B., ,Bn > 0, let B sig(T ) non-conjunctive B obj(AT , ).distinguish following two cases. B pseudo-primitive , obtainn,ranCA=, ,BllB0 us\preRan(B)B 0 \preC(B)lran(s) uran(s)Ar.BTBnon-conjT (B)spreRole(r)\(preDomT (A)preRanT (B))uls.Css\preDom(B)n,ranv B.C ran -concepts Cs . follows Lemma 39 6|= CA, ,B00B r .B , obtainn,ran=CA, ,BlB 0 \preC(B)ulB0 us\preRan(B)lran(s)Ar.BTBnon-conjT (B)spreRole(r)\(preDomT (A)preRanT (B))ls.Cs u0s\(preRole(r )preDomT (B))lran(s) un1,rans.CA, , 00B 00 non-conjT (B 0 )0 )\(preDom (B)preRan (B 00 ))spreRole(rBC ran -concepts Cs . easy see conditions (e2), (e3) (e4) Lemma 39n,ranv B hold, condition (e1) would fulfilled.hold. Thus, |= CA, ,Bn,rann1,ranB 00 non-conjT (B 0 )CAobserve every subconcept s.CA, ,B, , 00Bn1,ran00000preRole(r ) \ (preDomT (B) preRanT (B )), obtain 6|= CAT , , 00 v BBn1,ranu ran(s) v B 0 Lemma 39induction hypothesis. Thus, 6|= CA, ,B 00every B 00 s. infer condition (e1) hold and, therefore,n,ranv B.6|= CA, ,BAppendix C. Proofs Section 6Proof Lemma 54. Let normalised ELHr -terminology signatureNR 6= . Additionally, let NC concept name non-conjunctive, let r role name, let C EL -concept. Finally, let = C= ran(r) u C.First observe obtain Lemma 36 6|= v holds if, if,(T , AD ) 6|= A(aD ). Additionally, Lemma 44, (T , AD ) 6|= A(aD ) if, if,obj(AT , ) (AD , aD ) ran(AT , , ). Thus, sufficient show followingequivalence:ran(AD , aD ) ran(AT , , ) r : (A )r obj(AT , ) (AD , aD ) (AT , , (A )r )696fiThe Logical Difference Lightweight Description Logic ELNext note ABox AD role-splitting C EL-concept = ran(r) u C,{ s(b, aD ) AD | b obj(AD ), sig(AD ) } = {r(aran , aD )}.Assume first obj(AT , ), (AD , aD ) ran(AT , , ) let obj(AD )obj(AT , )corresponding -range simulation. define relation obj(AD ) obj(AT , )setting every obj(AD ), every obj(AT , ) every role name rr obj(AT , ):(a, r )(a, ) s(c, a) AD sig(AD ) c obj(AD ),= rNote well-defined AD role-splitting.show -range simulation exists r sig(A,T ) (A )robj(AT , ) (aD , (A )r ) , prove conditions (S1)(S3) condition (RS)page 663 hold.(S1) exists s(c, aD ) AD sig(AD ) c obj(AD ),exists 0 obj(AT , ) s( 0 , ) , (aD , ) -range simulation,i.e. s(( 0 )s , (A )s ) , (A )s obj(AT , ). Hence, (aD , (A )s ) .Otherwise, easy see exists r (A )r obj(AT , )obj(AT , ) sig(AT , ) . Thus, (aD , ) S, (aD , (A )r ) .(S2) Let (a, r ) A(a) AD obj(AD ), obj(AT , ),r sig(AT , ). follows definition (a, ) S. Hence, -rangesimulation, A() , , implies A(r ) , definition, .(S3) Let (a, r ) s(a, a0 ) AD a, a0 obj(AD ), obj(AT , ), r sig(AT , ). definition obtain (a, ) S. Additionally, -rangesimulation, exists 0 obj(AT , ) (a0 , 0 ) s(, 0 ) , . Thus,s(r , s0 ) , definition , (a0 , s0 ) definitionAD role-splitting.(RS) Let (a, r ) s(c, a) AD a, c obj(AD ), obj(AT , ), r sig(AT , ). definition , (a, ) holds r = s. -range simulation,exists 0 obj(AT , ) s( 0 , ) = r( 0 , ) , . Hence, r(r0 , r ) , holdsdefinition , .converse direction, assume exists r (A )r obj(AT , )(AD , aD ) ran(AT , , (A )r ) holds. Let obj(AD ) obj(AT , ) corresponding-range simulation. define relation obj(AD ) obj(AT , ) setting everyobj(AD ) every obj(AT , ):(a, )r sig(AT , ) : (a, r ) .straightforward verify obj(AT , ) -range simulation(aD , ) S.697fiKonev, Ludwig, Walther, & WolterAppendix D. Proofs Section 7Proof Lemma 60. require preliminary observations. Let AC ABoxassociated C ran -concept C (Lemma 36). Then, ELHr -terminology , C ran concept C C u,u concept D, |= C v if, K |= D(aC ),K = (T , AC ). Theorem 2 (extended version),|= C v if, if, IK |= D(aC ), IK canonical model K.Note |= C v u.D if, if, DIK 6= d, d0 IKR = t1 u u tn , (d, d0 ) RIK if, if, exists role name(d, d0 ) sIK vT ti , = 1, . . . , n. summarise consequences requireproof below:(i) C u -concept occurrences Si = ri,1 u . . . u ri,mi intersections roles,1 k, |= C v if, if, exist role names si , 1 k,si vT ri,j 1 k, 1 j mi |= C v D0 , D0 obtainedreplacing Si si .(ii) C u -concept, |= C v u.D if, if, exists sequencer10 , . . . , rn0 IK |= (r10 . rn0 .D)(aran ) IK |= (r10 . rn0 .D)(aC ).first case, exists subconcept (ran(r) u C 0 ) C (up commutativityassociativity u) |= r.C 0 v r10 . rn0 .D. second case|= C v r10 . rn0 .D.assume C = 1il ran(si )u 1jn Aj u 1km rk .Ck |= C v R1 .D.Let R1 , . . . , Rk occurrences role intersections R1 .D, Ri = ri,1 u . . . uri,mi , 1 k. (i), find role names si , 1 k, si vT ri,j1 k, 1 j mi |= C v D0 , D0 obtained replacing Risi . applying Lemma 39 |= C v s1 .D0 using t1 vT r1,j , 1 j m1|= D0 v D, obtain one conditions (e1u ), (e2u ), (e3u ), (e4u ) musthold.second part lemma, first prove induction n 1 every C ran concept C every C u -concept |= C v r1 . rn .D least onefollowing conditions holds(e1n ) exists subconcept r.C 0 C |= C 0 u ran(r)v D;(e2n ) exists concept name C |= v u.D;(e3n ) exists role name r C |= r.> v u.D;(e4n ) exists role name r C |= ran(r) v u.D.n = 1, let C C ran concept C u -concept |= C v r1 .D.obtain least one conditions (e1u ), (e2u ), (e3u ), (e4u ) must holdfirst part lemma, hence, one (e1n ), (e2n ), (e3n ), (e4n ) satisfied.n > 1, let C C ran concept C u -concept |= C v r1 . rn .D.apply first part lemma again, conditions (e2u ), (e3u ), (e4u )fulfilled, conclude conditions (e2n ), (e3n ), (e4n ) also satisfied.698fiThe Logical Difference Lightweight Description Logic ELcase (e1u ) holds, exists subconcept r.C 0 C |= C 0 u ran(r) vr2 . rn .D. induction hypothesis obtain least one conditions(e1n ), (e2n ), (e3n ), (e4n ) fulfilled |= C 0 u ran(r) v r2 . rn .D, thus also|= C v r1 . rn .D r sig(C) every subconcept C 0 also subconceptC.Now, |= C v u.D C ran -concept C C u -concept D, (ii)distinguish following two cases:exists subconcept ran(r) u C 0 C sequence r10 , . . . , rn0 0 |=r.C 0 v r10 . rn0 0 .D. n0 = 0, |= r.C 0 v condition (e6u )holds. n0 1 obtain least one conditions (e1n ), (e2n ), (e3n ),(e4n ) satisfied. (e1n ) holds, exists subconcept r0 .C 00 r.C 0|= C 00 u ran(r0 ) v D. r.C 0 = r0 .C 00 , |= C 0 u ran(r) v D.(C 0 u ran(r)) occurs top-level concept C, |= C v holds,thus, condition (e5u ). Otherwise, exists subconcept s.((C 0 u ran(r)) u E)C (e1u ) satisfied |= C 0 u ran(r) u E u ran(s) v D. r.C 0 6= r0 .C 00 , r0 .C 00subconcept C 0 (thus, C) condition (e1u ) holds. Finally, oneconditions (e2n ), (e3n ), (e4n ) satisfied, one (e2u ), (e3u ), (e4u ) holds(ii).exists sequence r10 , . . . , rn0 0 |= C v r10 . rn0 0 .D. n0 = 0 condition(e5u ) holds. n0 1, least one conditions (e1n ), (e2n ), (e3n ), (e4n )holds. Then, (ii), conclude one conditions (e1u ), (e2u ), (e3u ),(e4u ) satisfied well.give translation C u,u -assertions conjunctive queries. similarconstruction ABox C ran -concept given Section 5.1. First, given C u -conceptC, define path C finite sequence C0 R1 C1 . . . Rn Cn , C0 = C, n 0,Ri+1 .Ci+1 conjunct Ci , 1 < n (Ri conjunctions role names). Letxp p paths(C) pairwise distinct variable names setXC = { s(xp , xq ) | p, q paths(C); q = p R C 0 , conjunct R }{ A(xp ) | conjunct tail(p), p paths(C) }Let ~x sequenceV variables XC except xC . conjunctive query qC,aobtained ~x. XC replacing xCVa. Finally,V = D0 uu.D1 u uu.Dkobtain conjunctive query qD,a ~x.( 0ik XD ), (we assume distinctvariables used every XDi , 0 k, ~x sequence variables exceptxD0 ) replacing xD0 a.prove Lemma 63 require preparation. Query answering closely relatedexistence certain homomorphisms interpretations. Let signature,set individual names, I1 , I2 interpretations. function f : I1 I2 called(O, )-homomorphismf (aI1 ) = f (aI2 ) O;699fiKonev, Ludwig, Walther, & WolterAI1 implies f (d) AI2 ;(d1 , d2 ) rI1 implies (f (d1 ), f (d2 )) rI2 r .known (Chandra & Merlin, 1977) exists (O, )-homomorphism I1I2 I1 |= q[~a] conjunctive -query q using individual names~a = a1 , . . . , ak O, I2 |= q[~a].proof slightly refine notion (O, )-homomorphism considering partial (O, )-homomorphisms domains satisfy certain conditions. Namely,every n 0, call partial (O, )-homomorphism level n homomorphismdomain contains elements reachable -role chain length n eithernamed individual element without -predecessor. proveevery ELran,u,u -inclusion C v depth(C), depth(D) n, T1 |= C v impliesT2 |= C v D, exists partial level n homomorphism certain model(T1 , A) certain model (T2 , A).consider partial homomorphisms certain interpretations only,introduce first. Let finite set individual names interpretation.called O-named exists = aI . model called O-forest(F1) everySd O-named, exists one d0(d0 , d) rNR rI ;(F2) infinite sequences d0 , d1 , . . . (di+1 , di ) rNR rI 0di O-named.(F3) (d, d0 ) rNR rI d0 O-named, O-named.Let finite set individual names, n 0, signature. partial function fO-forest model 0 called (O, n, )-homomorphism0(H1) O: aI domain f f (aI ) = aI ;0(H2) d, d0 domain f r : (d, d0 ) rI implies (f (d), f (d0 )) rI ;0(H3) domain f : AI implies f (d) AI ;(H4) exist chain d1 , . . . , dm = (di , di+1 )length > n O-named di , domain f .r rone prove followingLemma 70. Suppose O-forest, 0 interpretation every > 0exists (O, m, )-homomorphism 0 . Assume well |= q[~a] qconjunctive -query using individual names ~a = a1 , . . . , ak O.0 |= q[~a].Proof. Assume ~a -match q(~x) = ~y .q 0 (~x, ~y ) ~a consistselements O. (F2) (F3) definition O-forests (H1) (H4)definition partial homomorphisms, exists > 0 (v), v ~x ~y ,domain (O, m, )-homomorphism f . Take (O, m, )-homomorphism f .~a 0 -match q(~x) 0 , 0 (v) = f ((v)), v ~x ~y .700fiThe Logical Difference Lightweight Description Logic ELFinally, also need technique constructing (O, m, )-homomorphisms. Letinterpretation. > 0, letutIm,,u (d) = {C C| depth(C) m, C },where, above, depth(C) role-depth C; i.e., number nestings existentialrestrictions C.Lemma 71. Let finite signature let > 0 Suppose O-forest 0interpretation000(in0) (aI , bI ) rI implies (aI , bI ) rI , a, b r ;0(in1) tIm,,u (aI ) tIm,,u(aI ), O;00(d0 );(d) tm,,u(in2) exists d0 tm,,uI0exists (O, m, )-homomorphism g 0 .Proof. construct g constructing sequence functions f0 , . . . , fm , fi : 0 ,follows: domain dom(f0 ) f0 consistsaI000exist (d , d) r rI . aI set f0 (aI ) = aI .every remaining dom(f0 ) choose d0 according (in2) set f0 (d) = d0 . Observetm,,u(d) tIm,,u(f0 (d)) dom(f0 ).0suppose fn constructed(fn (d)) dom(fn );(d) tmn,,u(in3) tmn,,uI0(in4) n > 0: dom(fn ) if, if, O-named exists sequenced0 r1I d1 r2I rnI dn = d0 O-named ri d0dom(f0 ).construct fn+1 consider dom(fn ) O-named d0 (d, d0 ) r rI .00u0domain fn+1 consists . Let Rd,d = {r | (d, ) r } Rd,d0 =( rR 0 r).d,dluRd,dtmn,,u(d)0.mn1,,u 0(d )DtI(in3),uRd,d0.ltImn,,u(fn (d))0mn1,,u 0DtI(d )0Thus, choose e (fn (d), e) rI r Rd,d0 tImn1,,u (d0 )tmn1,,u(e) set fn+1 (d0 ) = e. defines fn+1 . Observe fn+1 wellI0defined (F1). Observe fn+1 properties (in3) (in4), (F3).set g = 0nm fm . readily checked g required.701fiKonev, Ludwig, Walther, & Wolterposition prove Lemma 63.Lemma 63 qDiff (T1 , T2 ), exists 0 cDiff ran,u,u(T1 , T2 ) sig(0 )sig().Proof. Assume T1 T2 given let (A, q(~a)) qDiff (T1 , T2 ). Let 0 = sig(A)sig(q). Assume that, contrast shown,T1 |=()T2 |=ELran,u,u -inclusions sig() 0 .Consider model 0 (T2 , A) 0 6|= q[~a]. Lemma 70, obtain contradictionexists obj(A)-forest model (T1 , A) every n > 0exists (obj(A), n, 0 )-homomorphism fn 0 .0Take, every obj(A) model Ia0 T1 da Ia C ran C u,u concepts C:0da C Ia T1 tI 0 (a) |= C00ranC }.tI 0 (a) = {C C0 |interpretations Ia0 exist Lemma 67. define unfoldingIa Ia0 . pathIa0 finite sequence d0 R1 d1 . . . Rn dn , n 0, Ri+1 = Ri+1 set Ri+10role names r Ri+1 iff (di , di+1 ) rIa , < n. path p, tail(p) denoteslast element p. let Ia consist paths Ia0 set0AIa = {p Ia | tail(p) AIa };rIa = {(d, dRd0 ) Ia Ia | r R}.Ia O-forest = . Moreover, C u,u -concepts C p Ia :()p C Ia0tail(p) C Ia .particular, Ia still model T1 .Take following (disjoint) union interpretations Ia := aobj(A) Ia ;AI = aobj(A) AIa , NC ;rI = aobj(A) rIa {(da , db ) | r0 (a, b) A, r0 vT1 r}, r NR ;aI = da , obj(A).show obj(A)-forest, model (T1 , A) exist (obj(A), n, )homomorphisms 0 n > 0. First observe following:Claim 1. EL concepts C Ia :C C Ia702fiThe Logical Difference Lightweight Description Logic ELproof induction construction C. interesting case C = r.Ddirection left right.Assume C Ia . Take d0 (d, d0 ) rId0 DI . (d, d0 ) a0 obj(A) rIa0 , C Ia follows immediately inductionhypothesis. Otherwise, = da , d0 = db b r0 (a, b) r0 vT1 r.induction hypothesis, d0 DIb . Hence, (), T1 tI 0 (b) |= D. compactness,exists concept E tI 0 (b) T1 |= E v D. obtain r0 .E tI 0 (a).T1 |= r0 .E v r0 .D obtain da C Ia using r0 vT1 r ().Claim 2. obj(A)-forest model (T1 , A).obj(A)-forest model follows construction. remainsshow model T1 . role inclusions r v T1 follows constructionrI sI . Suppose C1 v C2 T1 . C1 EL-concept, |= C1 v C2 followsClaim 1 condition Ia models T1 . assume C1 = ran(r)let ran(r)I . 6= da a, C2I since Ia models T1 . = da ,exists r0 (b, a) r0 vT1 r. ran(r0 ) tI 0 (a), T1 tI 0 (a) |= C2 .Hence, (), da C2Ia , i.e. da C2I Claim 1.Claim 3. every n > 0 exists (obj(A), n, 0 )-homomorphism 0 .Lemma 71, sufficient show conditions (in0), (in1), (in2). Condition (in0)follows directly (). Condition (in1) proved induction construction C.interesting step C = S.D = r1 u u rm . Let obj(A) C0tIn, ,u (aI ). Take d0 (aI , d0 ) d0 DI . d0 Ia , then, (), T1 tI 0 (a) |=0 ,u0(aI ). assumeS.D. () compactness, T2 tI 0 (a) |= S.D. Hence C tIn,0d0 6 Ia . r10 , . . . , rk0 b d0 = bI ri0 (a, b) 1 k0every 1 exists 1 j rj0 vT1 ri . tIn, ,u (bI ).00,u(b ). (), every 1 j existsinduction hypothesis tn,I00 ,u001 j k rj vT2 ri . C tIn,(aI ), required.0(in2), let C = Dtn,0 ,u (d) D. 6= aI obj(A),() exists b obj(A) T1 tI 0 (b) |= u.C. compactness (),0 ,u0 ,u0(d) tn,(d0 ), required.T2 tI 0 (b) |= u.C. Hence, exists d0 tn,I00= aI obj(A), then, (in1) shown above, d0 = aI required.finishes proof Lemma 63.ReferencesBaader, F., Brandt, S., & Lutz, C. (2008). Pushing EL envelope further. Proceedings6th International Workshop OWL: Experiences Directions (OWLED2009), Vol. 529 CEUR Workshop Proceedings. CEUR-WS.org.Baader, F., Penaloza, R., & Suntisrivaraporn, B. (2007). Pinpointing descriptionlogic EL+ . Proceedings 30th Annual German Conference Artificial Intelligence (KI 2007), Vol. 4667 Lecture Notes Computer Science, pp. 5267,Heidelberg/Berlin, Germany. Springer Verlag.703fiKonev, Ludwig, Walther, & WolterBaader, F. (2003). Terminological cycles description logic existential restrictions.Proceedings 18th International Joint Conference Artificial Intelligence(IJCAI 2003), pp. 325330, San Francisco, CA, USA. Morgan Kaufmann.Bienvenu, M., Lutz, C., & Wolter, F. (2012a). Deciding FO-rewritability EL. Proceedings 25th International Workshop Description Logics (DL 2012).Bienvenu, M., Lutz, C., & Wolter, F. (2012b). Query containment description logics revisited. Proceedings 13th International Conference Principles KnowledgeRepresentation Reasoning (KR 2012).Brandt, S., Kusters, R., & Turhan, A.-Y. (2002). Approximation difference description logics. Proceedings 8th International Conference PrinciplesKnowledge Representation Reasoning (KR-02), pp. 203214, San Francisco, CA,USA. Morgan Kaufmann.Calvanese, D., Giacomo, G. D., Lembo, D., Lenzerini, M., & Rosati, R. (2006). Datacomplexity query answering description logics. Proceedings TenthInternational Conference Principles Knowledge Representation Reasoning(KR 2006), pp. 260270.Chandra, A. K., & Merlin, P. M. (1977). Optimal implementation conjunctive queriesrelational data bases. Proceedings 9th Annual ACM Symposium TheoryComputing (STOC 77), pp. 7790, New York, NY, USA. ACM.Chang, C. C., & Keisler, H. J. (1990). Model Theory, Vol. 73 Studies LogicFoundations Mathematics. Elsevier, Amsterdam, Netherlands.Clarke, E., & Schlingloff, H. (2001). Model checking. Handbook Automated Reasoning,Vol. II, chap. 24, pp. 16351790. Elsevier, Amsterdam, Netherlands.Conradi, R., & Westfechtel, B. (1998). Version models software configuration management. ACM Computing Surveys (CSUR), 30 (2), 232282.Crafa, S., Ranzato, F., & Tapparo, F. (2011). Saving space time efficient simulationalgorithm. Fundamenta Informaticae, 108 (1-2), 2342.Cuenca Grau, B., Horrocks, I., Kazakov, Y., & Sattler, U. (2008). Modular reuse ontologies: theory practice. Journal Artificial Intelligence Research (JAIR), 31,273318.Delaitre, V., & Kazakov, Y. (2009). Classifying ELH ontologies SQL databases.Proceedings 6th International Workshop OWL: Experiences Directions(OWLED 2009), Vol. 529 CEUR Workshop Proceedings. CEUR-WS.org.Eiter, T., Fink, M., & Woltran, S. (2007). Semantical characterizations complexityequivalences answer set programming. ACM Transactions Computational Logic,8 (3).Ghilardi, S., Lutz, C., & Wolter, F. (2006). damage ontology? case conservative extensions description logic. Proceedings Tenth InternationalConference Principles Knowledge Representation Reasoning (KR 2006),pp. 187197, Menlo Park, CA, USA. AAAI Press.704fiThe Logical Difference Lightweight Description Logic ELGolbeck, J., Fragaso, G., Hartel, F., Hendler, J., Oberhaler, J., & Parsia, B. (2003).National Cancer Institutes thesaurus ontology. Journal Web Semantics, 1 (1),7580.Goncalves, R. S., Parsia, B., & Sattler, U. (2011). Analysing multiple versions ontology:study NCI thesaurus. Proceedings 24th International WorkshopDescription Logics (DL 2011), Vol. 745 CEUR Workshop Proceedings. CEURWS.org.Goncalves, R. S., Parsia, B., & Sattler, U. (2012). Concept-based semantic differenceexpressive description logics. Proceedings 25th International WorkshopDescription Logics (DL 2012).Hofmann, M. (2005). Proof-theoretic approach description-logic. Proceedings20th Annual IEEE Symposium Logic Computer Science (LICS 2005), pp. 229237, Washington, DC, USA. IEEE Computer Society.Horridge, M., Parsia, B., & Sattler, U. (2010). Justification oriented proofs OWL.Proceedings 9th International Semantic Web Conference (ISWC 2010), Vol.6496 Lecture Notes Computer Science, pp. 354369, Berlin/Heidelberg, Germany.Springer-Verlag.IHTSDO (2008). SNOMED Clinical Terms User Guide. International Health Terminology Standards Development Organisation (IHTSDO). Availablehttp://www.ihtsdo.org/publications/introducing-snomed-ct/.Jimenez-Ruiz, E., Cuenca Grau, B., Horrocks, I., & Llavori, R. B. (2011). Supportingconcurrent ontology development: Framework, algorithms tool. Data & KnowledgeEngineering, 70 (1), 146164.Kalyanpur, A., Parsia, B., Horridge, M., & Sirin, E. (2007). Finding justificationsOWL DL entailments. Proceedings 6th International 2nd Asian SemanticWeb Conference (ISWC07+ASWC07), pp. 267280, Berlin/Heidelberg, Germany.Springer Verlag.Kazakov, Y. (2009). Consequence-driven reasoning Horn SHIQ ontologies. Proceedings21st International Conference Artificial Intelligence (IJCAI 2009), pp. 20402045.Kazakov, Y., Krotzsch, M., & Simancik, F. (2011). Unchain EL reasoner. Proceedings 24th International Workshop Description Logics (DL 2011), CEURWorkshop Proceedings. CEUR-WS.org.Klein, M. C. A., Fensel, D., Kiryakov, A., & Ognyanov, D. (2002). Ontology versioningchange detection web. Knowledge Engineering Knowledge Management:Ontologies Semantic Web, Vol. 2473 Lecture Notes Computer Science,pp. 247259. Springer Verlag, Berlin/Heidelberg, Germany.Konev, B., Lutz, C., Walther, D., & Wolter, F. (2008). Semantic modularity moduleextraction description logic. Proceedings 18th European ConferenceArtificial Intelligence (ECAI 2008), Vol. 178 Frontiers Artificial IntelligenceApplications, pp. 5559, Amsterdam, Netherlands. IOS Press.705fiKonev, Ludwig, Walther, & WolterKonev, B., Walther, D., & Wolter, F. (2008). logical difference problem descriptionlogic terminologies. Proceedings 4th International Joint Conference Automated Reasoning (IJCAR 2008), Vol. 5195 Lecture Notes Computer Science,pp. 259274, Berlin/Heidelberg, Germany. Springer Verlag.Konev, B., Kontchakov, R., Ludwig, M., Schneider, T., Wolter, F., & Zakharyaschev, M.(2011). Conjunctive query inseparability OWL 2 QL TBoxes. Proceedings25th Conference Artificial Intelligence (AAAI 2011), Menlo Park, CA, USA.AAAI Press.Konev, B., Ludwig, M., & Wolter, F. (2012). Logical difference computation CEX2.5.Proceedings 6th International Joint Conference Automated Reasoning(IJCAR 2012), Lecture Notes Computer Science, Berlin/Heidelberg, Germany.Springer.Konev, B., Lutz, C., Walther, D., & Wolter, F. (2009). Formal properties modularisation.Modular Ontologies, pp. 2566. Springer Verlag, Berlin/Heidelberg, Germany.Kontchakov, R., Wolter, F., & Zakharyaschev, M. (2010). Logic-based ontology comparisonmodule extraction, application DL-Lite. Artificial Intelligence, 174 (15),10931141.Kontchakov, R., Pulina, L., Sattler, U., Schneider, T., Selmer, P., Wolter, F., & Zakharyaschev, M. (2009). Minimal module extraction DL-Lite ontologies usingQBF solvers. Proceedings 21st International Joint Conference ArtificialIntelligence (IJCAI 2009), pp. 836841, San Francisco, CA, USA. Morgan Kaufmann.Kutz, O., & Mossakowski, T. (2008). Conservativity structured ontologies. Proceedings18th European Conference Artificial Intelligence (ECAI 2008), Vol. 178Frontiers Artificial Intelligence Applications, pp. 8993, Amsterdam,Netherlands. IOS Press.Kutz, O., & Mossakowski, T. (2011). modular consistency proof DOLCE. Proceedings 25th Conference Artificial Intelligence (AAAI 2011), Menlo Park, CA,USA. AAAI Press.Kremen, P., Smd, M., & Kouba, Z. (2011). OWLDiff: practical tool comparisonmerge OWL ontologies. Proceedings 10th International Workshop WebSemantics, pp. 229233, Los Alamitos, CA, USA. IEEE Computer Society Press.Lutz, C., Toman, D., & Wolter, F. (2009). Conjunctive query answering descriptionlogic EL using relational database system. Proceedings 21st InternationalJoint Conference Artificial Intelligence (IJCAI 2009), pp. 20702075, Menlo Park,CA, USA. AAAI Press.Lutz, C., Walther, D., & Wolter, F. (2007). Conservative extensions expressive description logics. Proceedings 20th International Joint Conference ArtificialIntelligence (IJCAI 2007), pp. 453458, Menlo Park, CA, USA. AAAI Press.Lutz, C., & Wolter, F. (2010). Deciding inseparability conservative extensionsdescription logic EL. Journal Symbolic Computing, 45 (2), 194228.Lutz, C., & Wolter, F. (2011). Foundations uniform interpolation forgetting expressive description logics. Proceedings 22nd International Joint Conference706fiThe Logical Difference Lightweight Description Logic ELArtificial Intelligence (IJCAI 2011), pp. 989995, Menlo Park, CA, USA. AAAIPress.Mendez, J., & Suntisrivaraporn, B. (2009). Reintroducing CEL OWL 2 EL reasoner.Proceedings 22nd International Workshop Description Logics (DL 2009),Vol. 477 CEUR Workshop Proceedings. CEUR-WS.org.Noy, N. F., & Musen, M. A. (2002). PromptDiff: fixed-point algorithm comparingontology versions. Proceedings 18th national conference Artificial intelligence, pp. 744750, Menlo Park, CA, USA. AAAI Press.Ohst, D., Welle, M., & Kelter, U. (2003). Differences versions UML diagrams.Proceedings 9th European software engineering conference held jointly11th ACM SIGSOFT international symposium Foundations software engineering(ESEC03/SIGSOFT FSE03), pp. 227236, New York, NY, USA. ACM.Oliver, D. E., Shahar, Y., Shortliffe, E. H., & Musen, M. A. (1999). Representationchange controlled medical terminologies. Artificial Intelligence Medicine, 15 (1),5376.Palma, R., Haase, P., Corcho, O., & Gomez-Perez, A. (2009). Change representationOWL 2 ontologies. Proceedings 6th International Workshop OWL: Experiences Directions (OWLED 2009), Vol. 529 CEUR Workshop Proceedings.CEUR-WS.org.Pearce, D., & Valverde, A. (2004). Uniform equivalence equilibrium logic logic programs. Proceedings 7th International Conference Logic ProgrammingNonmonotonic Reasoning (LPNMR 2004), Vol. 2923 Lecture Notes ComputerScience, pp. 194206, Berlin/Heidelberg, Germany. Springer.Pearce, D., & Valverde, A. (2012). Synonymous theories knowledge representationsanswer set programming. Journal Computer System Sciences, 78 (1), 86104.Penaloza, R., & Sertkaya, B. (2010). complexity axiom pinpointing ELfamily description logics. Proceedings 12th International ConferencePrinciples Knowledge Representation Reasoning (KR 2010), Menlo Park, CA,USA. AAAI Press.Poggi, A., Lembo, D., Calvanese, D., Giacomo, G. D., Lenzerini, M., & Rosati, R. (2008).Linking data ontologies. Journal Data Semantics, 10, 133173.Redmond, T., Smith, M., Drummond, N., & Tudorache, T. (2008). Managing change:ontology version control system. Proceedings 5th International WorkshopOWL: Experiences Directions (OWLED 2008), Vol. 432 CEUR WorkshopProceedings. CEUR-WS.org.Rosati, R. (2007). conjunctive query answering EL. Proceedings 2007International Workshop Description Logic (DL 2007), Vol. 250 CEUR WorkshopProceedings. CEUR-WS.org.Schlobach, S., & Cornet, R. (2003). Non-standard reasoning services debuggingdescription logic terminologies. Proceedings 18th International Joint Conference Artificial Intelligence (IJCAI 2003), pp. 355362, San Francisco, CA, USA.Morgan Kaufmann.707fiKonev, Ludwig, Walther, & WolterTeege, G. (1994). Making difference: subtraction operation description logics.Proceedings 4th International Conference Principles Knowledge Representation Reasoning (KR94), pp. 540550, San Francisco, CA, USA. MorganKaufmann.van Glabbeek, R. J., & Ploeger, B. (2008). Correcting space-efficient simulation algorithm.Proceedings 20th International Conference Computer Aided Verification(CAV 2008), Vol. 5123 Lecture Notes Computer Science, pp. 517529, Heidelberg/Berlin, Germany. Springer Verlag.Vescovo, C. D., Parsia, B., Sattler, U., & Schneider, T. (2011). modular structureontology: Atomic decomposition. Proceedings 22nd International JointConference Artificial Intelligence (IJCAI 2011), pp. 22322237, Menlo Park, CA,USA. AAAI Press.708fiJournal Artificial Intelligence Research 44 (2012) 423-453Submitted 10/11; published 07/12Modelling Observation Correlations Active ExplorationRobust Object DetectionJavier VelezGarrett HemannAlbert S. HuangVELEZJ MIT.EDUGHEMANN ALUM.MIT.EDUASHUANG MIT.EDUMIT Computer Science Artificial Intelligence LaboratoryCambridge, MA, USAIngmar PosnerINGMAR ROBOTS.OX.AC.UKMobile Robotics GroupDept. Engineering Science, Oxford UniversityOxford, UKNicholas RoyNICKROY CSAIL.MIT.EDUMIT Computer Science Artificial Intelligence LaboratoryCambridge, MA, USAAbstractToday, mobile robots expected carry increasingly complex tasks multifarious, realworld environments. Often, tasks require certain semantic understanding workspace.Consider, example, spoken instructions human collaborator referring objects interest; robot must able accurately detect objects correctly understand instructions.However, existing object detection, competent, perfect. particular, performancedetection algorithms commonly sensitive position sensor relative objectsscene.paper presents online planning algorithm learns explicit model spatialdependence object detection generates plans maximize expected performancedetection, extension overall plan performance. Crucially, learned sensor modelincorporates spatial correlations measurements, capturing fact successive measurements taken nearby locations independent. show sensormodel incorporated efficient forward search algorithm information spacedetected objects, allowing robot generate motion plans efficiently. investigate performance approach addressing tasks door text detection indoor environmentsdemonstrate significant improvement detection performance task execution alternative methods simulated real robot experiments.1. IntroductionYears steady progress mapping navigation techniques mobile robots madepossible autonomous agents construct accurate geometric topological maps relativelycomplex environments robustly navigate within (e.g., Newman, Sibley, Smith, Cummins, Harrison, Mei, Posner, Shade, Schroeter, Murphy, Churchill, Cole, & Reid, 2009). Lately,mobile robots also begun perform high-level tasks following natural languageinstructions interaction particular object, requiring relatively sophisticated interpretation agent workspace. recent literature therefore focuses augmentingc2012AI Access Foundation. rights reserved.fiV ELEZ , H EMANN , H UANG , P OSNER & ROY(a)(b)Figure 1: traditional geometric environment map (a) represented simple two dimensionaloccupancy grid regions free-space (cyan) (red) useful navigationlocalization, (b) geometric map augmented semantic informationidentity, structure location objects world allowing richer interactionsagent workspace.metric maps higher-order semantic information location identity objectsworkspace (see Fig. 1).end, advances vision- laser-based object detection recognitionleveraged extract semantic information raw sensor data (e.g., Posner, Cummins, &Newman, 2009; Douillard, Fox, & Ramos, 2008; Martinez-Mozos, Stachniss, & Burgard, 2005;Anguelov, Koller, Parker, & Thrun, 2004). Commonly, output detection systemaccepted prima facie, possibly threshold estimated sensor error. consequencedirectly using results object detector quality resulting map stronglydepends shortcomings object detector. Vision-based object detection, example,oftentimes plagued significant performance degradation caused variety factors includingchange aspect compared encountered training data, changes illumination and,course, occlusion (e.g., Coates & Ng, 2010; Mittal & Davis, 2008). aspect occlusionsaddressed naturally mobile robot: robot choose location sensors carefullyacquiring data performing object detection, thereby improving robustnessdetection process specifically counteracting known detector issues. Rather placing burden providing perfect detections detector itself, robot act improve perception.Rarely, however, ability mobile robot actually exploited building semantic map.paper, present online planning algorithm robot motion explicitly incorporates model performance object detector. primarily address problemcontext robot exploring unknown environment goal building map accuratelylabeled location semantic objects interest here, particular, consider doorstextual signs. However, approach applied problem robot must plantrajectories depend location objects landmarks interest environment.show planning approach weighs benefit increasing confidence potentialsemantic entity cost taking detour succession suitable vantage point.Fig. 2 gives cartoon illustration problem, robot encounters possible new object424fiM ODELLING BSERVATION C ORRELATIONS F ROBUST BJECT ETECTION(a)(b)(c)Figure 2: conceptual illustration (a) robot viewpoint x following originaltrajectory (bold line) towards goal (red star), (b) perception field particularobject detector centered around object hypothesis, (c) alternative path (bolddash-dotted line) along informative route. Cell shadings indicate relative valueobservations taken cell terms mutual information. Lighter valuesindicate lower mutual information therefore desirable vantage points. challengelearning mutual information varies spatially, also capturingmutual information cell changes new measurement.executing path goal. Based expected information available possible vantagepoints, robot may decide original path provided accurate model object,may choose modify path reduce possibility errors object model.make two primary contributions paper. Firstly, describe new sensor modeluses mixture Gaussian Processes model performance object detection system function robots relative position detected features also learnonline model sensor measurements spatially correlated. Typical estimation planning algorithms assume sensor measurements conditionally independent givenknowledge robots position, assumption clearly incorrect properties environment introduce strong correlation sensor measurements. Rather estimatepossible hidden variables capture full sensor model preserve conditional independence,explicitly model spatial correlation measurements use correlation model estimate mutual information measurements taken different locations. usemutual information bias random sampling strategy trajectory generationevaluate expected cost sampled trajectory. Secondly, show incorporatelearned sensor model forward search process using Posterior Belief Distribution (PBD)algorithm (He, Brunskill, & Roy, 2010, 2011) perform computationally efficient deep trajectoryplanning. PBD approximation allows us compute expected costs sensing trajectorieswithout explicitly integrating possible sensor measurements.work first result actively controlling sensor improve accuracy,previous work largely ignored motion cost typically assumed observations conditionally independent given sensor position. Inspired recent progress forward searchplanning uncertainty, demonstrate system allows us efficiently find robust observation plans. paper builds previous work presented ICAPS 2011 (Velez, Hemann,Huang, Posner, & Roy, 2011) provides several substantial extensions. Specifically, describesignificantly richer sensor model, extend approach improved planning algorithm ad425fiV ELEZ , H EMANN , H UANG , P OSNER & ROYdress additional object interest human-readable text. demonstrate overall approachusing real robot well simulation studies.exposition begins problem formulation planning trajectories improve objectdetection Section 2. Section 3 describe specific sensor model characterizesensor models using mutual information. Section 4 gives two different approaches learningsensor models vary spatially, observations correlated spatially. describeplanning algorithm sensor model incorporated system Section 5.follow description implementation efficient planning using sensor modelsSection 6. Section 7 describes object detectors used results. Section 8 shows simulationresults approach improves object detection compared approaches Section 9shows performance system real world trials. Sections 10 11 concludediscussion related work future directions.2. Problem FormulationConsider robot following particular trajectory towards goal environment objectsinterest unknown locations, example, rescue robot looking people first-responderscenario. Traditionally, object detector used waypoints along trajectorydetection either accepted map rejected based simple detector thresholds. However,lack introspection approach regarding confidence object detectorquality data gathered lead unnecessary acceptance spurious detections.systems simply discard lower confidence detections way improve estimatefurther, targeted measurements. contrast, would like robot modify motionminimize total travel cost cost errors deciding whether add newly observedobjects map.Let us represent robot point x R2 SO(2), SO(2) denotes special orthogonal group representing orientation R2 represents location 2D euclidean space. Withoutloss generality, express robot trajectory set waypoints x0:K , associatedmotion cost cmot (x0:K ) sum total travel waypoints x0 xk . robotprior map environment planning path pre-specified goal, computingminimum cost path x0:K well-understood motion planning problem.robot moves, receives output object detector gives rise beliefwhether detected object truly exists location indicated1 . model presenceith object location (ui , vi ) random variable yi {object, no-object}. systemruns, object detector fire give rise objects Yi given locations systemmust reason qualify either genuine objects false firings objectdetector.Let us define decision action ai {accept, reject}, detected object either acceptedmap (the detection determined correspond real object) rejected (the detectiondetermined spurious). Let us also define cost dec : {{accept, reject}{object, no-object}} 7R correct incorrect accept reject decision. cannot know true cost decisions{ai } ultimately know true state objects environment. therefore1. assume robot knows location, sufficiently well-calibrated camera determine locationobject map. work, uncertainty whether object specific type presentgiven location (u, v).426fiM ODELLING BSERVATION C ORRELATIONS F ROBUST BJECT ETECTIONinfer distribution state object p(y) generate plan minimize expectedcost E[dec ] individual decision actions given distribution objects.formulate planning problem choosing plan , comprised sequence waypointsdecision actions, 7 {x0:K a0:Q } path length k Q hypothesized objectsminimize total travel cost along trajectory expected costs decision actionsend trajectory, optimal plan given= arg min cmot (x0:K ) + cdet (x0:K , a) ,(1)x0:K ,acdet (x0:K , a) = Ey|x0:K[dec (a, y)],(2)Ey|x0:K [] denotes expectation respect robots knowledge regarding object,y, executed path x0:K . number hypothesized objects, Q, number possible objects detector fired traversing entire trajectory known beforehand.Note planning problem computing often formulated partially observableMarkov decision process POMDP (Sondik, 1971; Kaelbling, Littman, & Cassandra, 1998),POMDP representation grow combinatorial complexity presence multipledetections. Furthermore, POMDP solutions assume stationary Markov model parameters;sensor model non-stationary explicitly non-Markov want representenvironmental features needed support non-Markov sensor model. Since approachuses sensor model adapts successive observation, new POMDP model wouldneed constructed solved observation. Lastly, explicit POMDP model wouldrequire plan take account possible observations robot might encounter carriesmotion trajectory. precisely, expected cost plan must computed respect possible observations objects, rather object distributions. avoidresulting computational complexity using forward search algorithm similar forward searchapproximation techniques solving POMDPs (Ross, Pineau, Paquet, & Chaib-draa, 2008),known scale well presence complex representations. also avoid explicitly computing observation distribution planning use approximation techniqueknown Posterior Belief Distribution (PBD) algorithm, adapted sensor model.3. Sensor Model Object Detectionorder compute expected cost decision actions, must estimate probability objects existing world given observations might see executing motion plan.therefore require probabilistic model object detector allows us infer distribution object given measurements, p(y|z). know sensor characteristics varyrobot moves around object interactions environment, hence makerelationship explicit writing posterior p(y|z, x) include viewpoint x.Furthermore, measurement, z, taken particular viewpoint x consists outputobject detector, assumed real number indicating confidence detectorobject exists. distribution range confidence measurements dependentparticular object detector captured random variable Z defined continuousrange [zmin , zmax ]. every waypoint x posterior distribution expressedp(y|z, x) = Rp(z|y, x)p(y),yY p(z|y, x)p(y)427(3)fiV ELEZ , H EMANN , H UANG , P OSNER & ROY(a)(b)(c)Figure 3: Different graphical models representing observation function. (a) naive Bayesapproximation assumes every observation z conditionally independent givenknowledge object y. (b) true model assumes observations independent given knowledge environment object y. (c) modelemployed here, correlations approximated way mixture modelinput space waypoints {x R2 SO(2)} (Equ. 9).p(z|y, x) denotes likelihood, every possible state , observing particulardetector confidence x. (The expression would seem require p(y|x), independentwaypoint measurement z received.)3.1 Observation ModelObservations z directly produced physical device, camera, often treatedconditionally independent given state robot (see Fig. 3a). However, observationsindependent given knowledge current state, fact independent givenstate environment shown Fig. 3(b). one (or both) variablesunknown, measurements longer first-order Markov fact correlated.seen intuitively noting robot stationary, aimed static scene,would expect response object detector successive images independent.anticipate observations object detector extremely correlated, expectationnew information would gained handful images.correct observation model maintain history observations. waypointsvisited, knowledge regarding object integrated recursively. Let K denote trajectoryK waypoint-observation pairs obtained sequence K = {(x1 , z 1 ), (x2 , z 2 ), . . . , (xK , z K )}.Knowledge gained step along trajectory integrated posterior distributionKK = (xK , z K ) K1 ,(4)K(5)KKp(y|z , x , ) p(y|z , x ,=K1),p(z K |y, xK , K1 )p(y|T K1 ),p(z K |xK , K1 )(6)z K K th observation, depends current waypoint alsohistory measurements waypoints K1 . denominator Equ. 6 serves moderate428fiM ODELLING BSERVATION C ORRELATIONS F ROBUST BJECT ETECTIONinfluence measurement likelihood posterior based correlations existingobservations taken along trajectory.difficulty model Equ. 6 sensor model p(z K |y, xK , K1 ) difficultarrive at, depending history measurements. Furthermore, K arbitrarilylarge, need model predicts observations given infinite history observations.describe new sensor model Section 4.3.2 Perception Fieldsdeveloping new sensor model, first need way examine sensor modelcaptures effect measurements posterior belief object y, use reductionuncertainty relative current belief next observation. Given waypoint xKtrajectory K1 visited thus far, reduction uncertainty captured mutual informationobject state observation Z K received xKI(Y, Z K ; xK , K1 ) =H(Y ; K1 ) H(Y |Z K ; xK , K1 ),(7)H(Y ; K1 ) H(Y |Z K ; xK , K1 ) denote entropy conditional entropy, respectively (we drop xK entropy since distribution independent robotxK without corresponding observation Z K ). Thus, H(Y ; K1 ) expresses certainty current belief whether object exists given trajectory thus far, unswayednew measurements. every time step, term constant every waypoint consideredtherefore disregarded. conditional entropy Equ. 7 expanded terms posterior state hidden variable given previous trajectory K1 additionalmeasurement taken xK , p(y|z K , xK , K1 ) (c.f. Equs. 6 9), likelihood z K takingparticular value conditioned trajectory thus far whether object viewed xKpresent not, p(z K |xK , K1 ),H(Y |Z K ; xZK , K1 ) =p(z|xK , K1 )H(Y |z, xK , K1 ) ,(8)z|z, xK , K1 ) computed using sensor model p(y|z K , xK , K1 ) given Equ. 6,H(Yfunction belief traversing waypoint-observation trajectory K .expected reduction uncertainty given conditional entropy values waypointsrobots workspace form perception field2 particular object hypothesis (see Fig. 2(b)).use perception field induces sensor model two ways: firstly bias searchinformative path, secondly part evaluation expected cost path.4. Correlation Modelsdescribed previously, conventional first-order Markov sensor models correctly representeffect successive observations implicitly correlated unmodelled environmental2. reduction position uncertainty robot observations across environment sometimes knownsensor uncertainty field (Takeda & Latombe, 1992) active localization. Since application object detection,use term perception field avoid confusion localization problem, concepts otherwiseidentical.429fiV ELEZ , H EMANN , H UANG , P OSNER & ROYvariables. images used object detector conditionally independent correlatedenvironment . robot position scene stationary, probabilityindividual pixel values successive images strongly correlated shared environmentalrepresentation robot position, varying sensor noise. Subsequently, object detectorresponses also strongly correlated. However, correctly representing observations wayrequires environmental model sufficient capture image generation process, intractablecomputational modeling burden. Image-based object detectors detectorsexhibit dependence environment. object detector utilizes propertiesenvironment (geometric otherwise) generate detections cannot priori treated producingconditionally independent observations given state robot. Correctly representingfull generative model object detection takes account environmental properties useddetector frequently intractable task.overcome difficulty, approximate real process object detection simplisticmodel images correlated. replace influence environment correlations observations convex combination fully independent modeldepend history observations, correlated observation model dependhistory observations. treat whether particular observation correlated previousobservation random variable. new posterior belief state world computedKp(z K |y, xK , K1 ) = p(z K K1 )p(zind|y, xK )K+ (1 p(z K K1 ))p(zcorr|y, xK , K1 ),(9)marginalized whether observation z K actually independentprevious observation not. use notation B represent event independentB. Factorizing likelihood way (Equ. 9) allow us capture intuitionrepeated observations similar waypoints add little robots knowledge stateworld treated correlated. Observations afield, however, becomeincreasingly independent; less correlating effect.order complete sensor model uses factorization Equ. 9, need construct model independent correlated likelihoods well model probabilityparticular detection independent previous detections. following sections describetwo different approaches modeling likelihood functions probability independentdetections.4.1 Static Disc Modelfirst sensor model called static disc sensor model, coarse, assumingmeasurements drawn according either learned first-order Markov model accordingnearest previous observation.K |y, xK approximated using histogramThe distribution independent detections zindbased detector performance labeled training data. is, training data collected placingrobot waypoint grid around training object facing object. robot collectsseries images waypoint, generates histogram object detection confidenceswaypoint collected images, histogram gives probability measurement z K specific (relative) waypoint xK . contrast, correlated detection model assumes430fiM ODELLING BSERVATION C ORRELATIONS F ROBUST BJECT ETECTION(a) Perception Field Observations(b) Perception Field One Observation, Disc ModelFigure 4: Perception field possible door using static disc sensor model. unknownobject center (blue) looking towards right. Brighter regions correspondwaypoints likely result higher confidence posterior beliefs. Observationstaken robot denoted location (magenta) oriented point sensordirectly object.measurements fully correlated always equal closest (in x) previously seen observation. described Equ. 9 treat probability observation independence mixingparameter, disc express truncated linear function Euclidean distance, d,two viewpoints. distribution normalized respect maximum distance dmax , beyondobservations treated fully independent. Thus,KK1dmax < dmax(10)p(z) = disc =1dmaxwords, information gained taking additional measurements waypointinformation content observations increases linearly distance previous ones.reference Equ. 6, model results belief update,K |y, xK )p(zindKp(y|T ) = disc+ (1disc ) p(y|T K1 ).(11)K |xK )p(zind431fiV ELEZ , H EMANN , H UANG , P OSNER & ROYFig. 4 shows two example perception fields object detector trained doors (see Section 7training process). Fig. 4(a), see highly informative measurements directlyfront door, 8m 10m. Fig. 4(b), see change perception fieldobservation. mixture parameter static disc model, disc , dmax valueempirically chosen 3 meters.4.2 Dynamic Time-Varying Correlation Modelstatic disc model shown previous section allow sensor model changeaccording data actively seen trajectory. purpose introducing correlationmodel capture effect environment object detector. object detectorsresponse individual object appearances captured dependence (for example, doordetector may different behavior detecting highly reflective glass doors versus solid oakdoors). However, static disc model assumes fixed correlation model sensor modelobjects particular class, regardless changes detectors response across individualinstances object class. previous model also assumes strong (truncated)linear relationship probability two observations correlated distancetwo observations. would like relax assumption order better model broadrange object detectors. second sensor model solves aforementioned issuesstatic disc model, also allows time-varying correlations observations takenobject. sensor models make use factorization Equ. 9, differ modelsK |y, xK ) p(z K |y, xK , K1 ) well structureused detection likelihoods p(zindcorrp(z K K1 ).would like mechanism learning correlation measurementsdepend potentially infinite number previous measurements, use GaussianProcess (GP) model independent correlated sensor models. Gaussian processcollection random variables, finite number joint Gaussian distribution,completely specified mean function covariance function (Rasmussen & Williams, 2006).use GP regression likelihood models always use zero mean function 0Squared Exponential (SE) variance function following structure:0 (scaleI)1 (xx0 )/2SE(x, x0 ) = sigma e(xx ).(12)use notation SEi (X; ) mean kernel SEi function X parameterized.4.2.1 NDEPENDENT C ORRELATED L IKELIHOOD ODELSorder model independent observations use Gaussian Process, GP ind , zero mean function squared-exponential covariance function described above. kernel parameters, ind ,learned training data pairs waypoints x observations z described section 4.2.3.GP takes input particular waypoint x predicts detector output z waypoint.Letting train set labeled waypoint-observation pairs used GP ind , observation modelindependent observation becomesKKzind|y, xK , K1 = zind|y, xKGP ind (0, SEind (T train , xK ; ind )).432(13)fiM ODELLING BSERVATION C ORRELATIONS F ROBUST BJECT ETECTIONsee model depends solely training data provide prediction.Similar independent model use Gaussian Process, GP corr , zero mean function learned SE kernel correlated observation model (Equ. 14) trained model nonindependent observations object detector. kernel parameters, corr , learnedtraining data described Section 4.2.3. Let corr-train set waypoint-observations pairsused train GP corr . But, GP corr uses training data learn kernel parameters makespredictions data acquired current trajectory K1 far, resultsfollowing correlated observation model:Kzcorr|y, xK , K1 GP corr (0, SEcorr (T K1 , xK ; corr )).(14)Unlike independent model GP predicts using training data (Equ. 13), correlatedmodel GPs predictions based solely data observations taken current object ratherK |y, xK , K1 usingobservation histories objects. Predicting likelihood zcorrGP regression marginalizing previous trajectory observations resultsnormal distribution,K2zcorr|y, xK , K1 N (corr,K , corr,K).(15)choice model independent correlated observations using GPs resultsoverall observation model simplifying mixture two Gaussian distributions,2).z K |y, xK , K1 N (obs , obs(16)4.2.2 IXTURE PARAMETER P ROXY NDEPENDENCEreason factor likelihood independent model correlated model capture intuition nearby observations correlated therefore less informative,require baseline model observations remaining robot waypoints. modelprobability observation independent (p(z K K1 ) Equ. 9) treatingtime-varying spatial mixture parameter . mixing parameter chosen functionvariance correlation model estimate,2p(z KK1 ) = p(z KK1 |xK , K1 ) = (xK , K1 ) = 1 ecorr,K .(17)using SE kernel function GP corr , know variance predictioncorr,K function input space distance independent actual prediction value(Rasmussen & Williams, 2006). Note GP corr function current trajectoryworld, K1 , current waypoint xK function training data corr-train .variance estimate GP corr function distance waypointsobservations taken far particular object, encodes intuition observationssimilar waypoints correlated. fact, current waypoint approaches previousK |y, xK , K1 approaches 0, 1, meansobservation waypoints, variance zcorrtrust correlated observation model independent model. Similarly, distancecurrent waypoint previous observation waypoint becomes large, 0trust independent observation model almost exclusively. words, little informationgained taking additional measurements waypoint information contentobservations increases distance previous ones.433fiV ELEZ , H EMANN , H UANG , P OSNER & ROYshown Fig. 3(c), remove add dependency previous waypointscurrent observation z K . use pair GPs model spatial time-varying propertiescorrelations observation sequence object detector.(a) Learned perception field firstobservation.(b) Learned perception field thirdobservation.Figure 5: Learned perception field door detector (a) first observation (b) third observation. (b), previous observations (shown magenta) shift expectinformative vantage points be. panels, unknown object centeredorigin facing right. Brighter regions correspond waypoints likely resulthigher confidence posterior beliefs. Observations taken robot denotedlocation (magenta) oriented point sensor directly object.4.2.3 RAINING ENSOR ODELSdynamic time-varying observation model consist mixture two Gaussians (see Equ. 16),modeled using two Gaussian Processes, GP ind GP corr , every object hypothesis. Gaussian Process maps locations, x, resulting object detection scorez. Every object detector system observation model. independent observation likelihood GPs trained using available training data. labeled tuple(z, x, = {object, no-object}) used independent sample fed independent GP corresponding labeled object state ({object, no-object}). trainingsamples used learn SE kernel independent GP models. way learnmodel detector output likelihood cases object truly existed not, assuming independent observations. two GPs shared across objects constantmeasurements.correlated observation model GPs learned SE kernel usedifferent data. SE kernel trained data object since tryinglearn model correlated detections. split training data set subsets cor434fiM ODELLING BSERVATION C ORRELATIONS F ROBUST BJECT ETECTION(a) Learned perception field firstobservation.(b) Learned perception field thirdobservation.Figure 6: Learned perception field text detector (a) first observation (b) third observation. (b), previous observations (shown magenta) shift expectinformative vantage points be. panels, unknown object centeredorigin facing right. Brighter regions correspond waypoints likely resulthigher confidence posterior beliefs. Observations taken robot denotedlocation (magenta) oriented point sensor directly object.respond objects. SE kernel parameters chosen maximal likelihoodparameters set subsets. However, kernel parameters learned, correlated model GPs initially devoid data. two correlated model GPs instantiatedper-object basis shared across objects. Samples added runtimerobot actively observes detector outputs world. such, correlated model GPs trackcurrent set waypoints observed particular object, whereas independent model GPstrack training samples since treated independent.Using learned dynamic time-varying sensor model derived initial perception fielddoor shown Fig. 5(a). Fig. 5(b) shows perception field several observationstaken around door. Notice expected amount information significantlydecreased around observed points farther waypoints may still yield useful observations.initial perception field shows areas high expected information gain observationaccording training samples particular object detector. Since previousobservations, initial perception field shows use learned independent Gaussian Processobject detector.derived perception field text sign shown Fig. 6(a). Experimentally, truncatedtext perception field waypoints aspect 45 degrees object435fiV ELEZ , H EMANN , H UANG , P OSNER & ROYcomputational efficiency, given detector fire viewing signs obtuseangles training data. Fig. 6(b) shows perception field several observationstaken around object. Notice text detector significantly different perceptionfield door detector, initial shape well response observations. seedoor detector peaks within perception field, signifying regions relatively highinformation gain. text detector, hand, smooth perception fielddrops mainly function depth.5. Planning PerceiveGiven sensor model described previous section, describe planning algorithmtrades necessity gaining additional information object hypothesisoperational cost obtaining information. particular, object first detected, newpath original goal planned based total cost function includes motioncost cmot along path value measurements waypoints along path expressedreduction expected cost decision actions. Recall cost function consists twoterms: motion cost cmot (x0:K ) decision cost cdet (x0:K , a), optimal plangiven Equ. 1, reproduce here:= arg min cmot (x0:K ) + cdet (x0:K , a) ,x0:K ,acdet (x0:K , a) = Ey|x0:K[dec (a, y)],Ey|x0:K [] denotes expectation respect robots knowledge regarding object,executed path x0:K .5.1 Motion costpath cost, cmot (x0:K ), encompasses operational considerations power expendedtime taken moving along particular trajectory typically proportional lengthtrajectory.5.2 Decision Costdecision cost, cdet (x0:K , a), captures expected cost accepting (or rejecting)potential object detection, also captures expected yield information observationsalong path x0:K . trajectory affects cost decision actions terms changingexpectation, rather decision actions themselves, effect allowing algorithm decideobservations needed.Note decision actions treated independently also independentlyrobot motion, allows us compute expected decision costs efficiently.take advantage efficiency move minimization decision actions directly insidecost function. Abusing notation cdec ,cdet (x0:K ) = arg min cdet (x0:K , a)(18)= arg min Ey|x0:K[dec (a, y)].436(19)fiM ODELLING BSERVATION C ORRELATIONS F ROBUST BJECT ETECTIONNext, write plan terms x0:K .= arg min cmot (x0:K ) + cdet (x0:K ) .(20)x0:Kdec (accept, ) dec (reject, ) costs associated declaring object existsnot, respectively, measuring z xK following traversal waypoint-observation trajectoryK1 . costs include penalties imposed accepting true positive detectionaccepting false positive detection, respectively, chosen user systemreflect value/penalty decision particular domain.expectation inside Equ. 19 relies model conditioned trajectory x0:K ;seen Fig. 3(c), x0:K correlated z K . planning, actual z Kreceived cannot known ahead time, evaluate expectation exactly, musttaken respect object state received observations,Ey|x0:K [(a, y)] =Z decKK1p(z|x ,)Ey|z,x0:K1 [dec (a, y)] ,(21)zp(z|xK , K1 ) denotes probability obtaining particular detector confidence valueobserving object x given previous trajectory K1 , computed akinposterior Equ. 6. Section 6.2 show efficiently approximate expectationobservation sequence treating belief normally distributed.planning process proceeds searching sequences x0:K , evaluating paths approximating expectations respect observation sequences object state.paths lowest decision cost tend leading lowest posterior entropy,avoiding large penalty false positives negatives.5.3 Multiple Objectsformally define vantage point relative object y, vy RM , vector dimensional feature space describing configuration robot relative potential object.also define mapping F : R2 SO(2) 7 RM robot waypoint x corresponding vantage point vy = F (x, y). principle, vantage point need restricted spatialcoordinates may incorporate additional information as, example, degree occlusion experienced image contrast (for appearance based detector). work, however,range, r, aspect, , relative object robot oriented directly face objectconsidered vy R SO(2) (see Fig. 2a). important note system mustable accurately compute vantage point; paper stereo camera used estimatedistance orientation potential object. planning approach described farextended planning environment Q object hypotheses considering modified costfunction simply adds cost object. also augment dec (a, y) dec (a, y, i)able provide different decision costs different object types (or even different object instances). augmentation allows us specify relative importance different objects typesalgorithm. work consider objects existence independent objectshence individual object perception fields additive particular waypoint x. also restrictwaypoints correspond robot facing particular hypothesized object.437fiV ELEZ , H EMANN , H UANG , P OSNER & ROYGiven prior information object locations, hypothesize many objectsworld. initially let Q = 0 run object detector robot motion.image processed object detector, system judges whether detection belongs object hypothesis already considered (e.g., using distancehypothesized object detection). detector determines probability objectnew location threshold belong hypothesis objects,number object hypotheses Q increased robot replans. detection determinedcorrespond particular object hypothesis, system updates belief replans.5.4 Multi-Step Planningsimple approach planning considers every possible trajectory goal weightscost taking trajectory, choosing minimum cost trajectory plan. simplealgorithm scales approximately exponentially length planning horizon thusrapidly becomes intractable observations considered. adopt roadmap schemefixed number waypoints sampled every time new waypoint addedcurrent trajectory. graph built sampled poses, straight-line edgessamples.sampling scheme biased towards waypoints likely lead useful observationsusing perception field (see Section 3.2). Due correlations individual observationsmade trajectory waypoints, perception field changes new observations added.particular, correlation model imposed work (Equs. 17 9 dynamic time-varyingmodel Equ. 11 static disc model) forceslim# obs. xKI(Y, Z K ; xK , K1 ) 0,considering measurements waypoints already visited. words, robotprefer observe putative object different waypoints taking repeated measurementsplace.Algorithm R EPLAN N N EW ETECTION (Fig. 7) summarizes planned-waypoints approachsampling evaluating trajectories balance increased confidence motion costs.algorithm uses Posterior Belief Distribution framework able quickly sample trajectoriesmany observations, selects best current plan according costmetric.Figure 8 details stages algorithm example run single door detectedgoing towards goal.6. Efficient Perception Field Computationplanning algorithm needs calculate perception field deep planning horizons (T 1).variant algorithm uses static disc sensor model must evaluate expectedchange belief every potential future waypoint, must carry belief thoughtlevel search tree future trajectories xK+1:K+T . However, using dynamictime-varying sensor model treat belief normally distributed. normaldistribution approximation, limit infinite number observations, mean normaldistribution converge either 0 1 (depending whether object present not),438fiM ODELLING BSERVATION C ORRELATIONS F ROBUST BJECT ETECTIONAlgorithm R EPLAN N N EW ETECTIONInput: object detection z vantage point x// Step 1: Update Belief2: using static disc sensor model3:dmin = arg min |x xi |1:4:5:6:7:8:9:10:11:12:13:14:15:16:17:18:19:20:21:22:23:24:25:26:27:28:29:30:31:32:33:34:disc =xi K1dmindM AXK |y,T K )disc p(zp(z K |T K )Equ 10+ (1disc ) p(y|T K1 )p(y)elseN (corr,K , corr,K ) PREDICT(GP corr , x)21 ecorr,Kn n1 + Kn (zn W (n1 ))n (n1 + Gn bn GTn )1// Step 2: Sample Trajectories{}sampling time remainstraj {}using static disc sensor modely0= 1Pi COMPUTE - PERCEPTION - FIELD(yi1 )xi Pi // sample vantage pointp(yi ) Ez 0 [p(z 0 |xi , K1 , yi1 )p(yi1 )]traj traj xielse00 n00 nGP 0corr GP corr= 1i1Pi COMPUTE - PERCEPTION - FIELD(0i1 , 0i1 , GP corr)xi Pi // sample vantage pointz 0 PREDICT(GP i1corr , GP ind , xi )0GP icorr UPDATE - SENSOR - MODEL(GP i1corr , xi , z )0i 0i1 + Kn (zn 0 W (0i1 ))0i (0i1 + Gn bn GTn )1traj traj xitraj35: EXECUTE - TRAJECTORYEqu 11Equ 17Equs 25, 27 28Equ 8Equs 8 32Equs 9, 13, 14 17Equs 25, 27 28arg min COST(t0 )t0Figure 7: waypoint planning algorithm samples trajectories using perception field,chooses trajectories balance increasing robots confidence objectminimizing trajectory costs (Equ. 1).439fiV ELEZ , H EMANN , H UANG , P OSNER & ROY(a) Initially, robot (bluetriangle) goal (red star)detections firedyet, potential objectsreasoned about.(b) detector fires,robot starts reasoningpotential object.system creates initialperception field trainingdata potential objectplans path takedetections.(c) two detections (magenta) belief highenough system confident door truly existsworld continuestowards goal. Shownresulting perception fieldtwo taken observations.Figure 8: sample run system, initially empty set objects reasoned(a), door detector firing causing new door object hypothesis perception fieldcreated (b). system plans executes path goal allowstake advantageous observations hypothesized door. two observations,system continues towards goal since belief whether door existsincrease expected reward improving confidenceobject model justified additional cost (c). Brighter regions perceptionfields correspond waypoints likely result higher confidence posterior beliefs.Observations taken robot denoted location (magenta) orientedpoint sensor directly object. belief whether door truly existsdenoted green bar.small variance. Additionally, expected cost decision depend variancedistribution: smaller covariance normal posterior, less likely probabilitydecision error. Finally, posterior covariance normal depend sensormodel, observation itself. result, know sensor model information gainmeasurement, predict posterior covariance, hence expected costdecision action, without knowing exact observation sequence itself. approximationbinomial measurement function known Posterior Belief Distribution (PBD) algorithm (Heet al., 2011), used efficiently compute resulting belief time steps. sketchgeneral idea behind PBD below, use compute expected entropy reductionbelief future observations.440fiM ODELLING BSERVATION C ORRELATIONS F ROBUST BJECT ETECTION6.1 Beliefreality, object either exists exists world (denoted ). Labeled trainingdata output object detector form (z, x, = {object, no-object}), pairdetector output particular waypoint knowledge whether object exists not.order use labeled samples train sensor model (a model object detector),keep track belief whether object exists not. Equs. 22 23 show independentcorrelated observation model likelihood given using likelihood given . marginalizebelief independent correlated observations models (Equs. 13 14)getKKp(zind|, xK , K1 ) = p(zind|object, xK , K1 )K+ (1 ) p(zind|no-object, xK , K1 )(22)KKp(zcorr|, xK , K1 ) = p(zcorr|object, xK , K1 )K+ (1 ) p(zcorr|no-object, xK , K1 ),(23)likelihood modeled GP similar Equ. 13 14 independentcorrelated models respectively.Noting write likelihood z terms using (Equs. 22 23),similarly rewrite Equ. 9 terms likelihoods basedKp(z K |, xK , K1 ) = p(z K K1 )p(zind|, xK , K1 )K+ (1 p(z K K1 ))p(zcorr|, xK , K1 ).(24)6.2 Posterior Belief DistributionPBD algorithm allows us estimate expected information gain particular waypointwithout integrating potential observations z. begin framing problem Exponential Family Kalman Filter (efKF) formulation (He et al., 2011) treat statetrying estimate, exponential family observation model,n = n1 N (n , n )(25)zn = exp(zn n bn (n ) + n (zn )).(26)Given single observation canonical link function W mapping state observationparameter , posterior mean variance belief computed as,n = n1 + Kn (zn W (n1 ))n = (n1 + Gn bn GT )1(27)(28)nKn = n1 Gn (Gn n1 GTn + bnzn = n bnfin fifiGn =n fi1(bn zn ).n =n14411 1)(29)(30)(31)fiV ELEZ , H EMANN , H UANG , P OSNER & ROYGPobjectText GP indno-objectText GP indobjectText GP corrno-objectText GP corrobjectDoor GP indno-objectDoor GP indobjectDoor GP corrno-objectDoor GP corrSE kernel5.516.24.24.60.5254.30.0029303SE kernel l0.150.230.140.090.580.490.170.31Table 1: learned GP parameters. Note kernel scale door text differcorrelated observation GPs.particular importance us fact posterior covariance closed form solution,independent posterior mean (He et al., 2011), require integratingpossible observations Z. compute posterior covariance observationsfutureXn+T = (n1 +Gi bi GTi )1 .(32)i=1Rather marginalize potential future observations every future waypoint,compute variance belief observations simply multiplyingvariance observations future waypoints. Given perception fieldfunction variance belief (since entropy normal distribution functionvariance), quickly compute field deep observation trajectories. efficientcomputation allows planning algorithm sample potential observation trajectories manyobservations (T 1), thereby increasing effective search depth algorithm improvingplans.7. Objects: Doors Signssystem general agnostic type detector employed even sensing modalityused. constraint formed need able define vantage points (see Section5.3) compute perception field (see Section 3.2). work, chose test approachtwo different vision-based object detectors: first leverages parts-based object detectorFelzenszwalb, Mcallester, Ramanan (2008) trained find doors; second detector aimsspot human-readable text world commonly found signs. use text-spottinginspired work Posner, Corke, Newman (2010) authors kindly provided usC++ software library latest incarnation text-spotting engine, providesdetection parsing facilities individual words natural scene images.door detector trained approximately 1400 positive 2000 negative examplesmanually labeled images collected large range indoor areas excluding testingenvironment. Performance images testing environment low due false positivestriggered visual structures present training images. detector could re-trained442fiM ODELLING BSERVATION C ORRELATIONS F ROBUST BJECT ETECTIONAveragePrecisionRecallPath Length (m)Total TrialsG REEDY=0.80.31 0.060.44 0.0767.08 2.2350G REEDY=0.60.60 0.070.62 0.0741.95 0.8850P LANNEDdisc0.75 0.060.80 0.0654.98 3.0450RTBSS0.45 0.060.58 0.0747.57 0.1950Table 2: Simulation performance single door scenario, standard error values.improve performance, problem recurs new environments encountered.examples also used train sensor models door detector.text detector trained exactly described Posner et al. (2010). dynamic timevarying sensor model determined using approximately 1800 positive 2000 negative examples manually labeled images collected indoor office environment excludingtesting environment. used text detector localize text environment,actually use contents text itself.mixture parameter dynamic time-varying sensor model, scale factorchosen maximum likelihood estimator using training data detector system.learned scaling values door = 6.5 text = 5.4. Table 1 shows learned GP parametersdoor text detectors.8. Simulation Resultsfirst assessed planning approach using learned models simulated environment.simulation environment consisted robot navigating occupancy map, objectdetections triggered according learned observation model. also simulated false positivesplacing non-object perceptual features probabilistically triggered object detections usinglearned model false-alarms. processing delay incurred actual object detector alsosimulated (the door detector requires approximately 4.5 seconds process spatially decimated512x384 pixel image text detector requires 8 seconds process full 1024x768 pixelimage).8.1 Comparison Algorithmssimulation trials compared algorithm two algorithms. G REEDYalgorithm selected best waypoint according perception field potential object belief object exceeded threshold . Second, compared algorithmRTBSS online POMDP algorithm (Paquet, Tobin, & Chaib-draa, 2005). RTBSS algorithmcould use full sensor model Markov assumption utilized independent part model. One could augment state space include entire historydetections therefore use full sensor model, however large state space would renderPOMDP intractable practice. chose maximum depth equal algorithmmodeled world using resolution 2.5 meters RTBSS algorithm. denotealgorithm using static disc sensor model P LANNEDdisc , dynamic time-varying sensormodel P LANNED.443fiV ELEZ , H EMANN , H UANG , P OSNER & ROYAveragePrecisionRecallPath Length (m)Total TrialsG REEDY=0.80.64 0.030.63 0.02153.32 4.3750G REEDY=0.60.54 0.030.57 0.03121.35 1.3250P LANNEDdisc0.53 0.050.76 0.03138.21 7.1250RTBSS0.70 0.030.66 0.03160.74 6.0850Table 3: Simulation performance multiple door scenario, standard error values.(a) small simulation environment used doorscontaining single object (blue) two non-object(black).(b) multiple object simulation environment useddoors containing 4 objects (blue) 6 nonobjects (black).Figure 9: simulation environments static disc sensor model door detector.8.2 Static Disc Sensor Model SimulationsFirst, tested P LANNEDdisc algorithm small simulation environment one doorobject shown Fig. 9(a). Table 2 shows simulation results static disc model doordetector. Overall, explicitly planning waypoints resulted significantly higher performance.P LANNEDdisc algorithm performed better RTBSS terms precision recall, likelyalgorithm sampled continuous-space waypoints RTBSS algorithm fixeddiscrete representation, RTBSS paths shorter.evaluated P LANNEDdisc algorithm larger, complex scenario containing fourdoors six non-door objects. Fig. 9(b) shows multiple door simulation environment. Table 3shows simulation results multi-door scenario. P LANNEDdisc algorithm resultedsecond shortest paths G REEDY=0.6 superior detection performance. P LANNEDdiscalso resulted significantly shorter paths RTBSS given operating point ROCcurve.8.3 Dynamic Time-Varying Sensor Model Simulationstested P LANNED algorithm small simulation single text signcomplex simulation environment two signs shown Figs. 10 11. Table 4 shows results20 trials using text detector sensor model single object simulation. text signs,444fiM ODELLING BSERVATION C ORRELATIONS F ROBUST BJECT ETECTIONAveragePrecisionRecallPath Length (m)Total TrialsG REEDY=0.70.37 0.100.47 0.1235.32 1.0820P LANNED0.24 0.060.47 0.1220.40 0.7420RTBSS0.20 0.060.40 0.1118.43 0.4320Table 4: Simulation performance single sign scenario, standard error values.see deep trajectory planning help much (compare G REEDY strategyPlanned strategy planning horizon 5). information text detectorspread smoothly (see perception field Fig. 6(a)) hence greedy strategy bestthing do. However, planner took account cost resulted lower precision-recallperformance much shorter path length. also saw correlation sensor model allowedplanned algorithm perform better RTBSS. belief updates predicted RTBSSoverconfident hence RTBSS algorithm resulted shorter path lengths worse precision-recallperformance planned-waypoints algorithm.Figure 10: small simulation environment used text signs containing single object (blue)single non-object (black).Next, evaluated P LANNED algorithm complex scenario containing two objectstwo non-objects shown Fig. 11. Table 5 shows simulation results multiple-objectscenario. P LANNED algorithm resulted best precision-recall performance short pathlength. RTBSS also resulted short path length, lack correlation modelbecame overconfident belief, performing significantly worse planned-waypoints algorithm terms precision-recall.Fig. 11 also shows density trajectories traversed algorithm simulationsrun. Brighter spots denote places simulated robot frequented simulation runs.see P LANNED algorithm kept robot close shortest path costfunction, RTBSS. However, P LANNED algorithm decided spread detections apartcorrelation model employed whereas RTBSS over-valued information gainednearby observations. G REEDY algorithm take account motion costtaking observation saw widespread set trajectories waypoints visitedsimulations.445fiV ELEZ , H EMANN , H UANG , P OSNER & ROYAveragePrecisionRecallPath Length (m)Total TrialsG REEDY=0.70.23 0.060.28 0.0866.72 1.3920P LANNED0.93 0.050.72 0.0733.80 1.0020RTBSS0.54 0.110.43 0.0923.32 0.6320Table 5: Simulation performance multiple signs scenario, standard error values.(a) G REEDY Trajectories(b) P LANNED Trajectories(c) RTBSS TrajectoriesFigure 11: multiple object simulation environment used text containing 2 objects (blue)2 non-objects (red). Shown density paths taken different algorithmssimulation trials. planned approach results narrower space pathsG REEDY avoiding nearby (correlated) observations.8.4 Time Improvements PBDran comparison updating perception field using PBD algorithm (see Section6.2) update requires computing expectation possible detector outputs.created histogram potential detector values either 100 10 bins used sampledcompute expected mutual information gain (the perception field) detectoroutput bins. Table 6 shows results computing perception field 100 times. PBD algorithmallowed us efficiently calculate perception field since explicitly iteratepossible detector values could use Equ. 32.Updating perception field time-consuming part algorithm since mustupdated reasoning future observations planning. total run-timedetermined many trajectories sampled using perception field depthfuture trajectories, could tuned particular scenario application.paper let planning algorithm sample evaluate trajectories amount timerunning object detector single image passed.446fiM ODELLING BSERVATION C ORRELATIONS F ROBUST BJECT ETECTIONminavgmax100 bins z4.17s4.58s4.97s10 bins z1.17s1.20s1.22sPBD0.85s0.85s0.87sTable 6: Timing results computing perception field using either PBD algorithm, explicitly enumerating potential detector values z computing expectationvalues.AveragePrecisionRecallPath Length (m)Total TrialsG REEDY=0.80.53 0.140.60 0.14153.86 33.3410P LANNEDdisc0.7 0.150.7 0.1591.68 15.5610Table 7: Results door real-world trials using robot wheelchair, standard error values.(a) Trajectory executed actual robot wheelchair using planned-waypointsG robot discovers one true door (cyan). Near goal,detects two possible doors (red dots), detours inspect them, (correctly) decides doors.(b) Robotic wheelchairplatformFigure 12: Real world trial door detector using robotic wheelchair platform9. Results Real World TrialsFinally, validated results P LANNEDdisc P LANNED algorithms robot wheelchairplatform (Fig. 12(b)). autonomous wheelchair equipped onboard laser range scanners,primarily used obstacle sensing navigation, Point Grey Bumblebee2 color stereo camera,quad-core laptop main processing unit. stereo camera used accuratelydetermine vantage point particular detection. door textual signs planar,447fiV ELEZ , H EMANN , H UANG , P OSNER & ROYfit plane detection bounding box 3D points stereo camera determineorientation possible object given detection.door text real world trials, robot started particular location orientation.robot given goal position nominal trajectory would bring pastone true object (a door text sign), near several fixtures trigger object detections.Initially, system object hypothesis detector run continuously movedtowards goal shortest path. object detector fired, system started reasoningobject hypothesis corresponding detections. robot deviated shortestpath take observations certain object hypothesis determined cost function. Finally,robot reached goal trial ended. object hypothesis accepted beliefgreater 0.5. cost incorrect decision set 16 times cost meterpath length, cost correct decision set negative incorrect decision.trials capped 20 minutes done real office environment without specialaccommodations realistic possible.Fig. 12(a) shows location door trials. robot always started start location(marked S) given goal location (G). single door couldseen path start goal. Near goal also set windows lightfixtures often caused door detector fire. Fig. 12(a) illustrates trajectory executedsingle trial P LANNEDdisc algorithm, Table 7 summarizes results trialsdoors. G REEDY=0.8 chosen baseline comparison since best performingexisting algorithms according Table 3. P LANNEDdisc algorithm resulted significantlyshorter trajectories maintaining comparable precision recall. doors detected substantial uncertainty, algorithm planned advantageous waypoints increase confidenceignored far away detections high motion cost. interesting see Fig. 12(a)algorithm deviated take observations false detections near goal location, ultimatelycorrectly deciding object hypothesis fact doors.similarly conducted experiment using P LANNED algorithm G REEDY=0.7robotic wheelchair platform text detection algorithm. robot given nominaltrajectory brought past single textual sign (a poster office number placedcommon location poster notifications). trials run daytime hours allowartificial well natural lighting common environmental changes peoplewalking robot. Table 8 summarizes results 5 real-world trials algorithms.see G REEDY algorithm outperformed P LANNED terms precision (consistentsimulation results) much longer path lengths. P LANNED algorithm balancedcost gaining new observations travel time resulted much shorter trajectories.large path-length associated greedy algorithm came two sources: first, greedyalgorithm take path cost account deciding next observation take, secondgreedy algorithm kept taking pictures object hypothesis belief certainthreshold included sporadic object detections caused lights temporary environmentnoise.Lastly, ran small set 3 trials using P LANNED algorithm looking text signscompletely different environment previous trial. ran trials nightpeople walking by. Table 9 shows results. Even different environment, algorithmsbehaved similarly, G REEDY algorithm outperforming P LANNED algorithm costmuch longer paths.448fiM ODELLING BSERVATION C ORRELATIONS F ROBUST BJECT ETECTIONAveragePrecisionRecallPath Length (m)Total TrialsG REEDY=0.71 01 0102.77 7.215P LANNED0.70 0.090.80 0.0934.86 5.295Table 8: Results text real-world trials using robot wheelchair.AveragePrecisionRecallPath Length (m)Total TrialsG REEDY=0.71 01 039.2047 2.763P LANNED0.33 0.331 017.5351 4.353Table 9: Results small text real-world trials different location using robot wheelchair.10. Related Workproblem planning motion trajectories mobile sensor explored numberdomains including planning, sensor placement, active vision robot exploration.general formulation partially observable Markov decision process (Sondik, 1971). Exactsolutions POMDPs computationally intractable, recent progress led approximatesolvers find good policies many large, real-world problems (Pineau, Gordon, & Thrun,2006; Smith & Simmons, 2005; Kurniawati, Hsu, & Lee, 2008; Kurniawati, Du, Hsu, & Lee, 2010).However, complexity representing even approximate POMDP solution led forwardsearch strategies solving POMDPs (Ross et al., 2008; Prentice & Roy, 2009; et al., 2010).Eidenberger Scharinger (2010) formulate problem choosing sensor locations activeperception POMDP similar spirit formulation. However, explicitly modelunderlying physics object generation, model uncertainty object location ratherobject type, also unable plan one step future, therefore worksimilar G REEDY strategies described previous sections. approach inspiredforward search POMDP algorithms, incorporates complex model approximatescorrelations observations.contrast POMDP models active sensing, controls community sensor placement community developed information-theoretic models, goal minimizenorm posterior belief, entropy. objective function depend motion costs vehicle, sub-modular (Krause & Guestrin, 2007). consequence, greedystrategies choose next-most valuable measurement shown boundedly closeoptimal, challenge generate model predicts next-best measurement(Guestrin, Krause, & Singh, 2005; Krause, Leskovec, Guestrin, VanBriesen, & Faloutsos, 2008).terms image processing object recognition, Denzler Brown (2002) SommerladeReid (2010) showed information-theoretic planning could used tune camera parameters improve object recognition performance applied multi-camera systems, althoughuse exhaustive search camera parameters rapidly becomes unwieldy. Lastly, Sridharan, Wyatt, Dearden (2008) showed formulating information-theoretic problemdecision-theoretic POMDP, true multi-step policies improve performance computer449fiV ELEZ , H EMANN , H UANG , P OSNER & ROYvision system terms processing time. However, previous algorithms use modelssequential decision making costs actions independent (or negligible), leadingsubmodular objective function limited improvement greedy strategies.considerable work view point selection active vision brieflyreview here. relevant pieces work include Arbel Ferrie (1999)recently Laporte Arbel (2006) use Bayesian approach model detections relatedours, searches next-best viewpoint, rather computing full plan. workDeinzer, Denzler, Niemann (2003) perhaps similar viewpointselection problem framed using reinforcement learning, authors neglect costscamera movement identify absence costs limitation work. Similarly,system Mittal Davis (2008) learns model object occlusion uses simulated annealingsolve optimal plan; contribution learn predictive model good viewpoints.work Borotschnig, Paletta, Prantl, Pinz (2000) uses appearance-based object detectionsystem plan viewpoints minimize number observations required achieve certainrecognition rate, account correlations different observations.field object localization search seen recent advancements. useobject object relations seems like promising direction shown works Aydemir, Sjoo,Folkesson, Pronobis, Jensfelt (2011) Joho, Senk, Burgard (2011). approach differssystem uses spatial relations single object multiple observations ratherdifferent objects. works Joho et al. (2011) Aydemir, Gobelbecker, Pronobis,Sjoo, Jensfelt (2011) model environment achieve good results, whereas systemmodels correlation observations lieu modeling full environment. ideaattention seems powerful tool visual search (Tsotsos, 1992) systemsdue Meger, Forssen, Lai, Helmer, McCann, Southey, Baumann, Little, Lowe (2008)Andreopoulos, H., Janssen, Hasler, Tsotsos, Korner (2011) exhibiting excellent results. Ratherusing attention, system utilizes mutual information minimizes cost takingobservations. useful note system minimizes single cost functionencodes information path costs, Ye Tsotsos (1999) formalized approachmaximizes probability localizing object minimizes cost.robot exploration, goal generate robot trajectories learn accuratecomplete map minimum travel cost, costs motion must incorporated. Bourgault,Makarenko, Williams, Grocholsky, Whyte (2002) developed full exploration plannerincorporated explicit trade-off motion plans map entropy. Stachniss, Grisetti,Burgard (2005) described planner minimized total expected cost, performed searchnext-best action. address computational challenge, Kollar Roy (2008) usedreinforcement learning learn model expected cost next viewpointexploration, minimize total expected cost complete trajectory.contribution work existing work primarily describe planning modelincorporates action costs detection errors, specifically give approximateobservation model captures dynamic correlations successive measurementsstill allows forward-search planning operate, leading efficient multi-step search improveobject detection.450fiM ODELLING BSERVATION C ORRELATIONS F ROBUST BJECT ETECTION11. Conclusion Future WorkPrevious work planned sensing largely ignored motion costs planned trajectories usedsimplified sensor models strong independence assumptions. paper, presented sensor model approximates correlation observations made similar vantage points,efficient planning algorithm balances moving highly informative vantage pointsmotion cost taking detours. fully model effects entire environmentsensor intractable endeavor. sensor model simplifies environment interactions treatingcorrelations entire history sensor readings. placed emphasis spatialrelations model correlations new sensor readings history previous sensorreadings. properties Gaussian Processes, sensor model allows efficientdeep trajectory sampling utilizing Posterior Belief Distribution framework. tested algorithm two different object detectors (doors signs) found better detector dependentobservation trajectories comparable strategies.system presented planned deviations particular shortest-path trajectorygoal order detect localize objects spotted once. future aimincorporate large scale spatial model object likely encounteredthem. Next generation systems also deal novel objects existsprior object detector detector must created fly. goal createend-to-end online adaptive semantic mapping solution works arbitrary objectsenvironments.ReferencesAndreopoulos, A., H., W., Janssen, H., Hasler, S., Tsotsos, J., & Korner, E. (2011). Active 3d objectlocalization using asimo. IEEE Transactions Robotics, 27(1), 4764.Anguelov, D., Koller, D., Parker, E., & Thrun, S. (2004). Detecting modeling doors mobilerobots. Proc. ICRA.Arbel, T., & Ferrie, F. P. (1999). Viewpoint selection navigation entropy maps. Proc.ICCV, Kerkyra, Greece.Aydemir, A., Sjoo, K., Folkesson, J., Pronobis, A., & Jensfelt, P. (2011). Search real world:Active visual object search based spatial relations. Proc. ICRA.Aydemir, A., Gobelbecker, M., Pronobis, A., Sjoo, K., & Jensfelt, P. (2011). Plan-based objectsearch exploration using semantic spatial knowledge real world. Proc. ECMR,Orebro, Sweden.Borotschnig, H., Paletta, L., Prantl, M., & Pinz, A. (2000). Appearance-based active object recognition. Image Vision Computing, 18(9), 715727.Bourgault, F., Makarenko, A. A., Williams, S. B., Grocholsky, B., & Whyte, D. H. F. (2002). Information based adaptive robotic exploration. Proc. IROS, EPFL, Lausanne.Coates, A., & Ng, A. Y. (2010). Multi-camera object detection robotics. Proc. ICRA.Deinzer, F., Denzler, J., & Niemann, H. (2003). Viewpoint selection - planning optimal sequencesviews object recognition. Proc. ICCV. Springer.451fiV ELEZ , H EMANN , H UANG , P OSNER & ROYDenzler, J., & Brown, C. M. (2002). Information theoretic sensor data selection active objectrecognition state estimation. IEEE Trans. Pattern Analysis Machine Intelligence,24(2), 145157.Douillard, B., Fox, D., & Ramos, F. (2008). Laser vision based outdoor object mapping.Proc. RSS.Eidenberger, R., & Scharinger, J. (2010). Active perception scene modeling planningprobabilistic 6d object poses. Proc. IROS.Felzenszwalb, P., Mcallester, D., & Ramanan, D. (2008). discriminatively trained, multiscale,deformable part model. Proc. CVPR.Guestrin, C., Krause, A., & Singh, A. (2005). Near-optimal sensor placements Gaussian Processes. Proc. ICML.He, R., Brunskill, E., & Roy, N. (2010). PUMA: Planning uncertainty macro-actions.Proc. AAAI, Atlanta, GA.He, R., Brunskill, E., & Roy, N. (2011). Efficient planning uncertainty macro-actions.Journal Artificial Intelligence Research, 40, 523570.Joho, D., Senk, M., & Burgard, W. (2011). Learning search heuristics finding objects structured environments. Robotics Autonomous Systems, 59(5), 319328.Kaelbling, L., Littman, M., & Cassandra, A. (1998). Planning acting partially observablestochastic domains. Artificial Intelligence, 101, 99134.Kollar, T., & Roy, N. (2008). Trajectory optimization using reinforcement learning map exploration. International Journal Robotics Research, 27(2), 175197.Krause, A., & Guestrin, C. (2007). Near-optimal observation selection using submodular functions.Proc. AAAI.Krause, A., Leskovec, J., Guestrin, C., VanBriesen, J., & Faloutsos, C. (2008). Efficient sensorplacement optimization securing large water distribution networks. Journal Water Resources Planning Management, 134, 516.Kurniawati, H., Du, Y., Hsu, D., & Lee, W. (2010). Motion planning uncertainty robotictasks long time horizons. International Journal Robotics Research, 30(3).Kurniawati, H., Hsu, D., & Lee, W. (2008). SARSOP: Efficient point-based POMDP planningapproximating optimally reachable belief spaces. Proc. RSS.Laporte, C., & Arbel, T. (2006). Efficient discriminant viewpoint selection active bayesianrecognition. International Journal Computer Vision, 68(3), 267287.Martinez-Mozos, O., Stachniss, C., & Burgard, W. (2005). Supervised Learning PlacesRange Data using Adaboost. Proc. ICRA.Meger, D., Forssen, P., Lai, K., Helmer, S., McCann, S., Southey, T., Baumann, M., Little, J., &Lowe, D. (2008). Curious george: attentive semantic robot. Robotics AutonomousSystems, 56(6), 503511.Mittal, A., & Davis, L. (2008). general method sensor planning multi-sensor systems:Extens ion random occlusion. International Journal Computer Vision, 76, 3152.452fiM ODELLING BSERVATION C ORRELATIONS F ROBUST BJECT ETECTIONNewman, P., Sibley, G., Smith, M., Cummins, M., Harrison, A., Mei, C., Posner, I., Shade, R.,Schroeter, D., Murphy, L., Churchill, W., Cole, D., & Reid, I. (2009). Navigating, recognising describing urban spaces vision laser. International Journal RoboticsResearch, 28(11-12).Paquet, S., Tobin, L., & Chaib-draa, B. (2005). Real-time decision making large POMDPs.18th Canadian Conference Artificial Intelligence.Pineau, J., Gordon, G., & Thrun, S. (2006). Anytime point-based approximations largePOMDPs. Journal Artificial Intelligence Research, 27, 335380.Posner, I., Corke, P., & Newman, P. (2010). Using text-spotting query world. Proc. IROS.Posner, I., Cummins, M., & Newman, P. (2009). generative framework fast urban labelingusing spatial temporal context. Autonomous Robots, 26(2), 153170.Prentice, S., & Roy, N. (2009). belief roadmap: Efficient planning belief space factoringcovariance. International Journal Robotics Research, 8(11-12), 14481465.Rasmussen, C. E., & Williams, C. K. I. (2006). Gaussian Processes Machine Learning. MITPress.Ross, S., Pineau, J., Paquet, S., & Chaib-draa, B. (2008). Online planning algorithms POMDPs.Journal Artificial Intelligence Research, 32(1), 663704.Smith, T., & Simmons, R. (2005). Point-based POMDP algorithms: Improved analysis implementation. Proc. UAI.Sommerlade, E., & Reid, I. (2010). Probabilistic surveillance multiple active cameras. Proc.ICRA.Sondik, E. J. (1971). Optimal Control Partially Observable Markov Processes. Ph.D. thesis,Stanford University.Sridharan, M., Wyatt, J., & Dearden, R. (2008). HiPPo: Hierarchical POMDPs planning information processing sensing actions robot. Proc. ICAPS.Stachniss, C., Grisetti, G., & Burgard, W. (2005). Information gain-based exploration using RaoBlackwellized particle filters. Proc. RSS, Cambridge, MA, USA.Takeda, H., & Latombe, J. (1992). Sensory uncertainty field mobile robot navigation. Proc.ICRA.Tsotsos, J. K. (1992). relative complexity active vs. passive visual search. InternationalJournal Computer Vision, 7(2), 127141.Velez, J., Hemann, G., Huang, A., Posner, I., & Roy, N. (2011). Planning perceive: Exploitingmobility robust object detection. Proc. ICAPS, Freiburg, Germany.Ye, Y., & Tsotsos, J. (1999). Sensor planning 3d object search. Computer Vision ImageUnderstanding, 73(2), 145168.453fiJournal Artificial Intelligence Research 44 (2012) 335-382Submitted 03/12; published 06/12Plan-based Policies Efficient Multiple Battery Load ManagementMaria FoxDerek LongDaniele MagazzeniMARIA . FOX @ KCL . AC . UKDEREK . LONG @ KCL . AC . UKDANIELE . MAGAZZENI @ KCL . AC . UKDepartment InformaticsKings College LondonStrand, London WC2R 2LS, UKAbstractEfficient use multiple batteries practical problem wide growing application.problem cast planning problem uncertainty. describe approachadopted modelling solving problem, seen Markov Decision Problem, buildingeffective policies battery switching face stochastic load profiles.solution exploits adapts several existing techniques: planning deterministic mixeddiscrete-continuous problems Monte Carlo sampling policy learning. paper describesdevelopment planning techniques allow solution non-linear continuous dynamicmodels capturing battery behaviours. approach depends carefully handled discretisation temporal dimension. construction policies performed using classificationapproach idea offers opportunities wider exploitation problems. approachgenerality described paper.Application approach leads construction policies that, simulation, significantlyoutperform currently use best published solutions battery management problem. achieve solutions achieve 99% efficiency simulation comparedtheoretical limit far fewer battery switches existing policies. Behaviourphysical batteries exactly match simulated models many reasons, confirmtheoretical results lead real measured improvements performance also conductreport experiments using physical test system. results demonstrate obtain5%-15% improvement lifetimes case two battery system.1. Introductionpaper describe application planning important problem multiple batterymanagement. paper extended developed version work originally presentedInternational Conference Automated Planning Scheduling (Fox, Long, & Magazzeni, 2011)and, particular, adds physical results work described paper.increasing number systems depend batteries power supply, ranging small mobile devices large high-powered devices batteries used local storage electricalsubstations. many systems significant user-benefits, engineering reasons,base supply multiple batteries, load switched batteries control system. order power systems longest time possible, necessary devise switchingstrategies extract maximum possible lifetime batteries. show planningused basis highly efficient switching strategy.Due physical chemical properties batteries, possible extract greater proportion energy stored single battery capacity C stored n batteriesc2012AI Access Foundation. rights reserved.fiF OX , L ONG & AGAZZENIcapacity C/n, n > 1. Throughout paper, refer efficiency switchingstrategy use multiple batteries, talking proportion charge extractbatteries service load, compared servicing load single batterycapacity equal combined collection batteries equivalent physical properties.proportion high, example: 90%, switching strategy consideredhighly efficient.key efficient use multiple batteries lies design effective policiesmanagement switching load them. concerned situationload serviced entirely one suite batteries time, chargebattery drains batteries charge levels remain static. problem distinctproblem managing cells within single battery, objective usually keepcharge cells level. Batteries exhibit phenomenon recovery, consequencechemical properties battery: charge drawn battery, stored charge releasedchemical reaction, takes time replenish charge. general, charge drawnbattery faster reaction replenish lead battery appearingbecome dead when, fact, still contains stored charge. Therefore, efficient use multiplebatteries achieved exploiting recovery. allowing battery rest, reactionreplenish charge battery become functional again. Thus, efficient use multiplebatteries involves carefully timing use rest periods. Determining timing seenplanning problem.paper organised follows. begin presenting multiple battery usage problemdetail, describing battery model use.Section 4 describe approach adopted solving deterministic versionproblem, assume know load profile service. provide PDDL +encoding problem describe planning technique dealing continuityinvolved domain. complete section comparing performance plan-basedsolutions best policies currently considered multiple battery management.Section 5 show high quality plans obtained deterministic problemsused learn efficient policy general case load profiles knownadvance. describe classification process used evaluate performancepolicy servicing stochastic load profiles. Related work discussed Section 6.Section 7 present details physical experiment, using 6 Volt lead acid batteries,conducted order confirm simulation results. describe experimental setupand, interests reproducibility, parameter estimation process followed.report experimental results discuss significance.Section 8 outlines plans future work Section 9 concludes paper.2. MotivationsMany electrically powered systems rely large, heavy batteries supply adequate levels powercurrent. power requirements devices supplied multiple lightweightbatteries, coordinated supply load would typically supplied much largerbattery, could significantly change way devices used range applicationsmight suited.336fiP LAN - BASED P OLICIES E FFICIENT ULTIPLE BATTERY L OAD ANAGEMENTExamples powered systems could benefit distribution battery power includeexternally powered electric prosthetics. Prostheses powered electric motors functional attractive body-powered prosthetics, heavy expensive.power requirements capable prosthetic arm, combining elbow dexterous hand, necessitate large, hence heavy, battery. high torque motors required drive prosthetic elbowrequire high voltages current, modern dexterous hands require significantly currenttraditional single-motor electric hands.primitive prosthetic arm could run elbow hand 1 Amp Hour battery,dexterous hands require batteries much 2 Amp Hour capacities, handelbow run battery, even current larger capacities neededconsequent increase weight heat. high power demand requires either multiplebatteries carried batteries frequently recharged replaced. weight externallypowered prostheses common source dissatisfaction amongst users placementbatteries minimise weight effects important part prosthetic design. batterypower distributed around body, power requirements met carefullycoordinated multiple independent batteries power much smaller capacity,weight issue made less significant user, heat generated batteries alsoreduced making comfortable wear.benefits potentially obtained situation batteries carried order power portable electrical devices. Military personnel currently carry 20kgbatteries field power communication equipment, vision sensing systemselectronic devices. Robotic devices often battery powered rely carrying large numbers batteries maximise operational lifetime. Electric cars typically carry multiple batteries,although must sometimes used series maximise power availability. creates different constraints way used consider paper. However,technology develops, opportunities arise exploiting partitioned batteries electric vehicles.One advantages able distribute battery power across multiple independentbatteries ability swap batteries die, requiring small battery sparescarried instead one large one. hot-swapping capability could important roleplay mobile computing devices where, instead recharge battery every 6 hoursso, continuous power longer period could achieved selectively replacing spent cells.major motivation work done therefore obtain close-to-optimal batteryperformance high-powered devices, benefitting ability distribute weightheat production.3. Multiple Battery Usage Problemmultiple battery usage planning problem explored several authors, electrical engineering perspective, example work Benini et al. (2003) Rao et al. (2003),also scheduling perspective (Jongerden, Haverkort, Bohnenkamp, & Katoen, 2009)optimisation perspective (Wang & Cassandras, 2011) (in latter, simplifying assumptionload shared arbitrarily batteries made). Benini et al. construct accuratebattery model, parameterising capture lithium-ion, cadmium-nickel lead-acid battery types,show hand constructed policies achieve efficiency, relative single battery,70% 97.5%. achieve this, policy constructed select new battery whenever337fiF OX , L ONG & AGAZZENIvoltage battery currently servicing load drops certain threshold. next batteryselected according one four alternative policies (Benini et al., 2003):Vmax : select battery pack highest state charge.Vmin : select battery pack lowest state charge.Tmax : select battery pack unused longest time.Tmin : select battery unused shortest time.authors show Vmax best policies, tested four batteries. generalcase n batteries, Vmax referred best-of-n.Jongerden et al. (2009) uses model checking strategy, based U PPAAL, schedule batteryuse given known load profile. approach based use different battery model,Kinetic Battery Model, discussed detail below. non-linear continuous modelauthors treat discretisation scheduling horizon. approach allowsfind highly effective schedules, scale well need use finegrained discretisation temporal dimension. worth emphasising, since contrastsapproach, Jongerden et al. work fixed size discretisation time, allowingfocus scheduling resources (batteries) load periods.deployed systems, standard policies typically static, based rapid switching available batteries. fact, optimal use multiple batteries achieved theoreticallyswitching extremely high frequency, behaviour convergessingle battery (Rao et al., 2003). Unfortunately, theoretical solution achievable practicelosses physical process switching batteries, frequency increases. fact, switching losses MOSFETs approximately linearly dependent switchingfrequency also current switched (Eberle, 2008). Tmax Vmax policies appliedfixed frequencies commonly fielded solutions, often achieve less 80%efficiency (Benini et al., 2003).3.1 Objectivespaper objective construct policies multiple battery problems, loadmodelled probabilistically using known distributions load size, load duration load frequency(or equivalently, gaps successive loads). primary purpose, constructingpolicies, achieve longest possible battery lifetime. best deployed solutions typicallydeliver less 80% efficiency, best published solutions deliver less 95%efficiency (our reading suggests high values simulation rather physicalexperiments). show approach, based construction optimising solutions MonteCarlo sampled problem instances use construction appropriate policies, producesrobust solutions deliver better 99% efficiency simulation. Furthermore, side-effectway solutions constructed, achieve efficiency lifetime usingsmaller numbers battery switches published policies. beneficial side-effect reducespotential switching losses implementing policy. use Kinetic Battery Model (Manwell& McGowan, 1993) (KiBaM) basis construction optimising solutions raiseschallenges treatment non-linear mixed discrete-continuous optimisation problem,discuss below.338fiP LAN - BASED P OLICIES E FFICIENT ULTIPLE BATTERY L OAD ANAGEMENT3.2 Kinetic Battery ModelKinetic Battery Model (Manwell & McGowan, 1993; Jongerden et al., 2009) batterycharge distributed two wells: available-charge well bound-charge well (see Figure 1).BoundchargeAvailablechargeCharge flowLoad drawschargeTotal chargeFigure 1: Kinetic Battery Modelfraction c total charge stored available-charge well, fraction 1 cbound-charge well. available-charge well supplies electrons directly load (i(t)),denotes time, whereas bound-charge well supplies electrons available-chargewell. charge flows bound-charge well available-charge well valvefixed conductance, k. Moreover, rate charge flows wells dependsheight difference two wells. heights two wells given by:h1 =y1ch2 =y21cy1 available charge y2 bound charge. load appliedbattery, available charge reduces, height difference two wells grows.load removed, charge flows bound-charge well available-charge wellheights equal again. change charge wells given following systemdifferential equations:(dy1dt = i(t) + k(h2 h1 )dy2dt = k(h2 h1 )initial conditions y1 (0) = c C y2 (0) = (1 c) C, C total battery capacity.describe discharge process battery, Jongerden et al. (2009), adopt coordinates representing height difference two wells, = h2 h1 , total chargebattery, = y1 + y2 . new setting y1 = c( (1 c)).change wells given system differential equations(i(t)0dt = c kdt = i(t)solutions339fiF OX , L ONG & AGAZZENI(k0(t) = ci 1ek0(t) = Ck 0 = k/(1 c)c, (0) = 0 (0) = C. condition battery empty(t) = (1 c)(t).model less sophisticated used Benini et al. (2001), comparison battery models Jongerden Haverkort (2009) concludes Kinetic Battery Model (KiBaM)best performance modelling.3.3 Battery Usage PlanningAlthough battery load management seen scheduling problem, setting considermakes planning problem. given load profile service, knew numberswitching actions batteries would required, times actionsperformed, problem could managed scheduling problem. case,however, number switching actions cannot identified advance, period loadshared arbitrarily different batteries. Thus, battery load management becomesplanning problem. discretising time shortest time battery mustuse, possible construct scheduling problem maximum possible numberbattery switches considered, switches might used. difficultyapproach shortest period use short compared battery lifetime:physical experiments (Section 7), example, maximum number switches would700, larger capacity batteries smaller loads number switches could easilyseveral thousand. scheduling approach used Jongergen et al. (2009) cannot scale managetens intervals.Furthermore, KiBaM, deterministic non-linear continuous model battery performance, lends itself, principle, use optimisation problem solver find bestbattery usage plan, given load profile. multiple battery usage problem, deterministicform, clearly optimisation problem Wang Cassandras (2011) shown that,certain assumptions, tackled analytically (despite non-linear), using KiBaM.order assume load split arbitrarily batteries (which easilyachievable practice). also assume load serviced arbitrary schedulewithin given timespan, provided total charge drawn batteries meets requiredworkload. second assumption consistent situation, load mustserviced according demands placed user specific times, without flexibility. Unfortunately,analysis cannot modified deal situation consider.interest speculate whether standard Operations Research approach, usingform Mixed-Integer Linear Program (MILP) model, might used solve deterministicmultiple battery usage problem. first glance answer trivial: since model non-linear,clear MILP cannot used. sophisticated approach might considered, usingapproximation exponential recovery curves using piece-wise linear components. However,precise shape recovery curves depends state charge batterystart period recovery (both available bound parts), approximations musteither built dynamically, else model must anticipate possible states chargetimes points, effectively building entire search space states charge battery340fiP LAN - BASED P OLICIES E FFICIENT ULTIPLE BATTERY L OAD ANAGEMENTmodel. former approach cannot achieved standard MILP awaresolving technology could manage approach; latter approach obviously impracticalanything trivial situations.real battery usage problems load profile generated external processes, typicallycontrolled directly indirectly user demands. demands often modelled probabilistically, reflecting typical patterns use. work assume profiles drawnknown distribution. consequence planning problem ceases deterministicoptimisation problem, probabilistic problem plan must policy, discussedSection 5.3.4 Approachadopt approach based combination two ideas. Firstly, sample distributionloads arrive deterministic problem, solve using continuous KiBaMbattery model. leads interesting continuous non-linear optimisation problem,solve using discretise-and-validate approach. Currently using UPMurphi (Della Penna,Intrigila, Magazzeni, & Mercorio, 2009) solve deterministic instances but, discretisation,metric temporal planner could used principle. Secondly, use decision tree classifiercombine solutions sample problem instances learn policy MDPproblems drawn. classification process maps states actions produces policyform decision tree.approach domain-specific respects:discretisation scheme, based general principles, selected problemdomain load distribution.use search heuristic that, restricted battery problem alone, suitedproblems.aggregation solutions policy makes use entirely general approach,extent approach yields good policies depend nature problemspace applied.make use existing tools far possible, simplify construction solution.4. Solving Deterministic Multiple Battery Problemssection consider multiple battery management problem optimisation problem,faced known deterministic load profile.4.1 PDDL + Battery ModelP DDL + (Fox & Long, 2006) extension standard planning domain modelling language,PDDL , capture continuous processes events. dynamics KiBaM capturedeasily PDDL +. Figure 2 show two processes, consume recover, governbehaviour batteries event triggered attempting load battery availablecharge exhausted. addition, durative action variable duration allowsplanner use battery interval (see Figure 3). two processes active whenever341fiF OX , L ONG & AGAZZENIpreconditions satisfied, meaning usually execute concurrently. Together, modeldraining charge recovery described differential equation d/dt.event triggered ever positive load active service.(:process consume:parameters (?b - battery):precondition (switchedOn ?b):effect (and (decrease (gamma ?b) (* #t (load)))(increase (delta ?b) (* #t (/ (load) (cParam ?b))))))(:process recover:parameters (?b - battery):precondition (>= (delta ?b) 0):effect (and (decrease (delta ?b) (* #t (* (kprime ?b) (delta ?b))))))(:event batteryDead:parameters (?b - battery):precondition (and (switchedOn ?b)(<= (gamma ?b) (* (-1 (cParam ?b)) (delta ?b)))):effect (and (not (switchedOn ?b)) (dead ?b)))Figure 2: Part PDDL + encoding KiBaM dynamics(:durative-action use:parameters (?b - battery):duration (>= ?duration 0):condition (and (at start (switchedOff ?b))(over (switchedOn ?b))):effect (and (at start (and (switchedOn ?b) (not (switchedOff ?b))(increase (services) 1)))(at end (and (switchedOff ?b) (not (switchedOn ?b))(decrease (services) 1)))))Figure 3: P DDL + durative action battery useload profile serviced encoded PDDL + problem use timed initialliterals, allow expression exogenous events corresponding, case, changesload value. fragment problem (which also contains battery specification) shownFigure 4.use PDDL + modelling language grants several benefits. Firstly, allows us useVAL (Howey, Long, & Fox, 2004) validate solutions analytically continuous model,allowing us confirm discretisation use construction solutions compromise correctness plan. Secondly, provides us semantics model terms342fiP LAN - BASED P OLICIES E FFICIENT ULTIPLE BATTERY L OAD ANAGEMENT(define (problem 2B) (:domain kibam)(:objects b1 b2 - battery)(:init(= (cParam b1) 0.166)(= (kprime b1) 0.122)(= (gamma b1) 5.5)(= (delta b1) 0)...(at 0 (= (load) 0.25))(at 1.00 (= (load) 0.50))(at 2.00 (= (load) 0.25))(at 3.00 (= (load) 0.50))(at 4.00 (= (load) 0.25))...Figure 4: Fragment PDDL + problemtimed hybrid automaton described Fox Long (2006). Finally, make use existing tools construct search spaces defined PDDL + models, UPMurphi (DellaPenna et al., 2009).paper PDDL +, Fox Long (2006) propose semantics based mappingtimed hybrid automata (Alur & Dill, 1994). semantics domain instantiated twobatteries given three hybrid automata shown Figure 5, variables d, g, Lrefer PDDL + functions delta, gamma, load services, respectively. semanticsone route model-checking systems designed manage timed hybrid automataadapted operate directly battery problem. batteries reveal non-linear behaviourdefinitions expressions governing rates change d1 d2 pairstates switchedOnB1 switchedOffB1 equivalent pair B2. Unfortunately,equations beyond reach current model-checking systems, discretisingranges variables functions managed UPMurphi.variable time-slip variable introduced Fox Long (2006) allowscorrect modelling PDDL + domains events standard hybrid automata. particular,time-slip variable increases rate 1 whenever preconditions events disaster (positiveload battery used) notOptimal (a battery used without load service)satisfied. state three hybrid automata invariant condition statingtime-slip variable must 0, guarantees events applied soonpreconditions become true, without action transitions occurring between.4.2 Discretise-and-Validate Approachtechnique based discretise-and-validate approach (see Figure 6), continuousdynamics problem relaxed discretised model, discrete time steps corresponding step functions resource values used place original continuous dynamics.relaxed problem solved using forward reachability analysis solutions validatedcontinuous model using validator, VAL (Howey et al., 2004), provides analyticsolutions differential equations involved models.343fiF OX , L ONG & AGAZZENIbatteryDeadB1Inv: T=0Flow:d1 = 0 g1 = 0Jump: g1 (1-c)d1d1 = d1g1 = g1= - 1T= 0 V = 1deadB1useB1stopInv: T=0Flow:d1 = L/c - kd1g1 = -LT= 0 V = 1switchedOnB1Jump: d1 = d1g1 = g1= - 1useB1startT= 0 V = 1Inv: T=0Flow:L > 0 /\ = 0T=1L = 0 /\ > 0T=1T= 0 V = 1switchedOffB1loadProfileInv: T=0Flow:d1 = -kd1Jump: d1 = d1g1 = g1= + 1batteryDeadB2Inv: T=0Flow:d2 = 0g2 = 0T= 0 V = 1deadB2Jump: g2 (1-c)d2d2 = d2g2 = g2= - 1disasternotOptimalJump: L > 0s=0Jump: L = 0s>0useB2stopInv: T=0Flow:d2 = L/c - kd2g2 = -LT= 0 V = 1switchedOnB2Jump: d2 = d2g2 = g2= - 1Inv: T=0Flow:Inv: T=0Flow:d2 = -kd2T= 0 V = 1useB2startT= 0 V = 1notSatisfactoryserviceswitchedOffB2Jump:Figure 5: Hybrid automata modelling two kinetic batteries schedulingContinuous ModelDiscretiseSolveValidateFigure 6: Discretise Validate Approachvalidation process used identify whether finer discretisation required guideremodelling relaxed problem. example, simulation, first considered timediscretisation = 0.1, obtained plan shown Figure 7 (left). However, validateddiscrete solution generated planner continuous model, foundsolution indeed valid, highlighted following fragment VAL report:CheckingUpdatingUpdatingUpdatingnext happening (time 5.08986)(gamma b1) (0.502404) 0.337447 assignment(delta b1) (0.328362) 0.550475 assignment(delta b2) (0.405504) 0.257052 assignmentEVENT triggered (time 5.08986)Triggered event (batterydead b1)Deleting (switchedon b1)Adding (dead b1)Invariant (use b1) condition unsatisfiedtime 5.08986 5.1.344fiP LAN - BASED P OLICIES E FFICIENT ULTIPLE BATTERY L OAD ANAGEMENT0.0:3.40:3.90:4.00:4.50:5.10:5.30:5.60:5.80:6.10:6.80:7.00:8.30:8.50:8.70:(use b1)[3.40](use b2)[0.50](use b1)[0.10](use b2)[0.50](use b1)[0.60](use b2)[0.20](use b1)[0.30](use b2)[0.20](use b1)[0.30](use b2)[0.70](use b1)[0.20](use b2)[1.30](use b1)[0.20](use b2)[0.20](satisfied)0.0:3.40:3.90:4.00:4.50:5.08:5.35:5.43:6.10:6.15:6.55:6.60:7.10:7.15:8.15:8.45:8.65:8.70:(use b1) [3.40](use b2) [0.50](use b1) [0.10](use b2) [0.50](use b1) [0.58](use b2) [0.27](use b1) [0.08](use b2) [0.57](use b1) [0.05](use b2) [0.40](use b1) [0.05](use b2) [0.50](use b1) [0.05](use b2) [1.00](use b1) [0.30](use b2) [0.20](use b1) [0.05](satisfied)Figure 7: Plans generated using different time discretisations: = 0.1 (left) = 0.01 (right)precise analysis provided VAL allows us know exact value charge(simulated) batteries execution plan. example, charge battery 1terminates 0.01014 time units time expected discretised model. suggestsrefinement discretisation, setting = 0.01, eventually produced valid plan, shownFigure 7 (right). seen, finer discretisation handles sensitive interactionssystem switches battery 2 charge battery 1 almost fully drained (at time point 5.08).Although Jongerden et al. (2009) also use discretisation approach, fix granularitytime-step advance. contrast, use variable sized discretisation, allowing rangealternative step sizes considered search.introduce formal statement deterministic version probleminterested in. hybrid system system whose state description involves continuous welldiscrete variables. approximate system discretising continuous componentsstate (which assume bounded) dynamic behaviours obtaining finite numberstates.Definition 1 (Finite State Temporal System) Finite State Temporal System (FSTS) 5tuple (S,s0 ,A,D,F ), where: finite set states, s0 initial state, finite setactions, finite set durations F : transition function, i.e.F (s, a, d) = s0 iff system reach state s0 state via action duration d.state S, also define set EnAct(s)= {a A|d : F (s, a, d) S}, setactions enabled state s.FSTS, state assumed contain special temporal variable denoting timeelapsed current path initial state s. following use notation t(s)value variable state s. si , sj F (si , a, d) = sj , t(sj ) = t(si ) + d.345fiF OX , L ONG & AGAZZENIDefinition 2 (Trajectory) trajectory FSTS = (S, s0 , A, D, F ) sequence =s0 a0 d0 s1 a1 d1 s2 a2 d2 . . . sn where, 0, si state, ai action, diduration F (si , ai , di ) = si+1 . trajectory, write (i), (i) (i) denotestate si , action ai duration di , respectively. Finally, denote || length ,P||1given number actions trajectory, duration , i.e. = i=0 (i).order define planning problem system, assume set goal statesG specified. Moreover, finite state system, fix finite temporal horizon,T, require plan reach goal within time . case battery usage planningproblem, horizon important represents target duration serviceprovided battery. fact, good upper bound found battery problem,discussed section 4.3.Definition 3 (Planning Problem FSTS) Let = (S, s0 , A, D, F ) FSTS. Then, planningproblem (PP) triple P = (S, G, ) G set goal states finitetemporal horizon. solution P trajectory s.t.: | | = n, , (0) = s0(n) G.constraints add temporal planning problem parameterised iteratively relaxed order explore successively larger spaces plans. use finite collectionpossible durations segments processes (Definition 2). set refined addition smaller durations successive searches fail find solution. Allowing different durationswithin search enables planner construct states interact executing processesdifferent time points, stepping quickly along timeline interestingfeatures.4.3 Monotonicity Property Planningbattery domain important property supports simple heuristic evaluation functionstates: charge battery monotonically decreases time optimal solutionone gives longest possible plan. upper bound duration solutionfound using observation optimal duration cannot exceed single batterycombined capacity equal sum capacities multiple batteries (assumingdischarging flow behaviours). horizon, construct search discretisedsearch space. make approach practical, essential informed heuristicsearch space. domain, duration plan current state plus total remainingcharge admissible, completely uninformative, duration plus total available chargehighly informative. also equivalent minimising total bound charge.heuristic suitable class domains: domain monotonicallydecreasing resource, longest plan required (such satellite domain finiteamount resources), heuristic sums plan duration available resource informative.use variant best-first search (Algorithm 1) efficiently explore reachablespace. use variable discretisation efficiently, break symmetry structure searchspace arises possible orderings different length action instances. Redundancyeliminated disallowing use long duration actions immediately following shorter durationversions actions. Long duration actions used event action346fiP LAN - BASED P OLICIES E FFICIENT ULTIPLE BATTERY L OAD ANAGEMENTintervened since last short action family. also disallow repeated consecutive useshort duration actions beyond accumulated duration next longer duration action.longest duration action repeated arbitrarily often.Algorithm 1 Dynamic State Space Search (P)Input: planning problem P = ((S, s0 , A, D, F ), G, )Output: valid plan1: Q (s0 , null, 0);2: H s0 ;3: s0 G return ;4: Q 6=5:(sh , ai , dk ) argmax(s,a,d)Q h(s);6:aj EnAct(sh )7:aj 6= ai {dl D|t(sh ) + dl };8:else {dl D|dl dk t(sh ) + dl };9:dl10:s0 F (sh , aj , dl );11:s0 G return ;12:s0/ H13:Q Q (s0 , aj , dl );14:H H s0 ;4.4 Plan Search Variable Discretisationillustrate way range differently sized duration intervals leadsignificant benefits size set visited nodes search space, compared usingfixed duration increment.Consider load profile shown top Figure 8. planning problem two batteriesdefined according definitions 1 3, G = {s S|t(s) = 2.42}, i.e. goal servicewhole load profile. temporal horizon set duration profile well.definition FSTS straightforward: set actions = {useB1, useB2, wait}former actions refer battery used latter one applicableactive service. set durations use example = {0.01, 0.4, 0.5, 1.0} (measuredminutes). practice, define set durations start minimum valueadd exponentially increasing values maximum duration given longest intervaldifferent events (i.e., load variations). particular, smallest duration includedorder handle sensitive interactions.initial state s0 load active service batteries limitedinitial capacity. setting, plan search variable discretisation proceeds follows:1. battery used period 1 minute (when load idle). corresponding transition shown Figure 8.2. one minute load applied battery 1 used. corresponds transition< s1 , useB1, 1.0, s2 >. However, sake simplicity, let us assume that, due347fiF OX , L ONG & AGAZZENIFigure 8: Example search using variable discretisationlimited capacity, batteries cannot used continuously 1 minute. transition thusvalid shorter duration considered.3. Battery 1 used 0.5 minute. Then, since load still applied, second battery used.before, transition < s2 , useB2, 1.0, s3 > considered, casewould active service load.4. Battery 2 used 0.5 minute. next period load applied, battery used.transition < s3 , wait, 0.5, s4 > considered, would lead positive loadactive service, duration action wait reduced 0.4.5. service last load period 0.02 minute, battery 1 could used. However,sample instance let us assume remaining charge battery 1 allows service0.01 minute. So, finally, battery 2 used end load profile.validity transition dynamically checked search since invalid transitionstrigger specific events (e.g. event batteryDead triggered step 2 event disastertriggered step 4) which, turn, violates invariant conditions corresponding actions (abattery must die use). Moreover, variable discretisation 6 statesvisited order reach goal, using uniform discretisation necessary exploreleast 242 states since finest discretisation 0.01 must used order correctly handleinteractions steps 5 6.benefit use differently sized durations discretisation favouringlonger durations reduces number switches solutions generate, leading solutionsbetter practical terms based high frequency switching batteries,shown subsequent results.348fiP LAN - BASED P OLICIES E FFICIENT ULTIPLE BATTERY L OAD ANAGEMENT4.5 Performance Deterministic Load Problemspresent first set experimental results show, simulation, performancesolver deterministic battery usage optimisation problem. use case study proposed Jongerden et al. (2009), two types jobs considered, low current job (250mA) high current job (500 mA), according following load profiles:continuous loads: one load low current jobs (CL 250), one high currentjobs (CL 500) one alternating low current job high current job (CL alt);intermittent loads short idle periods one minute jobs: onelow current jobs (ILs 250), one high current jobs (ILs 500), one alternatinglow current job high current job (ILs alt);intermittent loads long idle periods two minutes jobs: one lowcurrent jobs (ILl 250) one high current jobs (ILl 500).first step, used load profiles validate variable-range discretisation KiBaMmodel (planning-KiBaM), find appropriate discretisation continuous variablesinvolved system dynamics (i.e. variables process durations). usedVAL validate solutions discretised model continuous model. workJongerden et al. (2009), considered two battery types, one capacity 5.5 Amin (B1 ) onecapacity 11 Amin (B2 ). small batteries, typical capacities smallportable devices PDAs mobile phones. battery types parameters:c = 0.166 k 0 = 0.122min1 . discretised , rounding 0.00001, and,load profiles battery types, obtained lifetimes computedoriginal KiBaM validated Jongerden Haverkort (2008).generate scheduling plans multiple batteries, used approach described sections 4.2 4.3 set durations = {0.01, 0.02, 0.05, 0.1, 0.25, 0.5, 1.0}.example PDDL + plan shown Figure 9, row < ti , ai , di > containstime point ti action ai (whose duration di ) applied.0.0:1.20:1.30:1.80:2.40:2.50:3.10:4.60:4.70:6.20:(use(use(use(use(use(use(use(use(use(useb1)b1)b2)b1)b1)b2)b1)b1)b2)b1)[1.00][0.10][0.10][0.20][0.10][0.10][1.00][0.10][0.10][0.30]Figure 9: Fragment PDDL+ planFigure 10 shows fragment corresponding VAL report. Note VAL provides analyticsolutions differential equations involved KiBaM dynamics.evaluate efficiency approach, compared solutions obtained usingU PPAAL-based approach. resulting lifetimes shown Table 1 upper bound349fiF OX , L ONG & AGAZZENIFigure 10: Fragment VAL reportcolumn shows theoretical upper bound given best-of-two policy extremely highfrequency switching. seen, first two rows table, powerextracted battery nominal capacity 5.5 Amin 12.16 min 250 mA,3.04 Amin, loading continuously 250 mA, 4.59 500 2.3 Amindrawing continuous load 500 mA. gives indication extent limitconversion bound charge available charge affects performance batteries.loadprofileUpper boundlifetimeB1B2U PPAAL-KiBaMlifetimeB1B2CL 250CL 500CL altILs 250ILs 500ILs altILl 250ILl 50012.164.597.0344.7910.8216.9584.9121.8612.044.586.4840.8010.4816.9178.9618.6846.9212.1621.26132.844.7972.75216.984.91N/AN/AN/AN/AN/AN/AN/AN/APlanning-KiBaMlifetime (visited states)B1B212.14 (194)4.59 (116)7.03 (136 )44.76 (552)10.8 (131)16.92 (159)84.88 (488)21.85 (173)46.91 (691)12.14 (194)21.2 (350)132.7 (1068)44.76 (552)72.55 (599)216.8 (1123)84.88 (488)Table 1: System lifetime (in minutes) load profiles according different battery usages350fiP LAN - BASED P OLICIES E FFICIENT ULTIPLE BATTERY L OAD ANAGEMENTload profiles considered observe approach outperforms U PPAAL-basedone significantly, providing solutions achieve 99% efficiency comparedtheoretical limit. key points described preceding parts section allow resultingsearch efficiently prune state space quickly find solutions. particular, usingvariable discretisation possible consider much finer discretisation variablesused work Jongerden et al. (2009) handle sensitive interactions.crucial, particularly available charge batteries almost exhausted. Jongerden etal. (2009) describe plans optimal, important note respectdiscretisation use; finer-grained discretisation offers opportunity higherquality solution found cost much larger state space. Despite large statespace model creates, solver visits small collection states (as shown table).problems solved less second.dealing larger batteries type B2 , state space becomes large exhaustive approach infeasible. Indeed, works Jongerden et al. (2009, 2008), authorsable handle second case. also found high quality solutions batteries type B2 :example shown Figure 11 compared standard best-of-two solution, showinghuge improvement obtain policy. Note slicing load periods occurstowards end plan, phenomenon observed plans.also considered 8 battery system (an example behaviour shown Figure 14).Benini et al. (2003) indicate designers SMBus (SBS Implementers Forum, 2000)architecture, communication control architecture protocol useddevelopment Smart Batteries, suggest might good reasons partitioncharge among four batteries. fact, examples systems usingfour batteries, HP 6-cell lithium-ion Smart Battery packs. practice, partitioning chargebatteries offers multiple benefits, including opportunities use industry standard cellsexploit different distributions weight possible cooling requirements. tradeoffsbenefits potential loss efficiency arising partitioning complex.batteries used, larger state space planning policylearning; constructing solution 8 battery problem significantly harder 4 batteryproblem, present results evidence scale larger systems, subsumingsmaller cases.results reported Table 2, show scale effectively much larger problems. Notice number switches use produce results significantly smallerbest-of-8 policy giving theoretical upper bound, however resulting solutions achieve99% efficiency. final column, labelled Plan-based Policy, shows performancepolicies discuss next section, applied load profiles. generate slightlyworse performance switches, maintain lifetime performance.One final observation worth noting structure usage profile across batteriesleads, two-battery case, one battery discharged sooner other. 8-batterycase effect pronounced, several batteries discharged others stillsignificant charge remaining. interesting consequence: using policy becomespossible hot-swap batteries, replacing used batteries new ones, system active.fact one batteries still hold charge allows loads serviced usedbatteries exchanged charged ones policy adapt new states charge351fiF OX , L ONG & AGAZZENI12total charge battery 1total charge battery 2available charge battery 1available charge battery 2battery schedule10charge (Ahr)8642001000200030004000time (0.01 min)500060007000(a) Vmax (based feasible frequency switching used (Jongerden etal. 2009))12total charge battery 1total charge battery 2available charge battery 1available charge battery 2battery schedule10charge (Ahr)8642001000200030004000500060007000time (0.01 min)(b) PlanFigure 11: ILs alt load test two batteries type B2batteries used ones replaced. marked contrast high-frequencyswitching policies, batteries discharge approximately time.5. Plans Policiesshown generate high quality plans deterministic multiple battery managementproblems, turn attention stochastic problem really interested solving.general, cannot know advance load profile applied batteries,352fiP LAN - BASED P OLICIES E FFICIENT ULTIPLE BATTERY L OAD ANAGEMENTloadprofileCL 250CL 500CL altILs 250ILs 500ILs altILl 250ILl 5008 batteries B2lifetime (number switches)Upper boundPlanPlan-based Policy310.6 (31072)134.7 (13472)192.8 (19280)660.7 (33076)308.7 (15476)424.8 (21280)1008.9 (33692)480.9 (16090)307.6 (485)133.4 (266)190.8 (355)654.1 (495)305.7 (293)420.6 (357)998.8 (471)476.1 (295)307.6 (992)133.4 (571)190.8 (806)654.1 (904)305.7 (513)420.6 (614)998.8 (822)476.1 (597)Table 2: System lifetime (in minutes) load profiles serviced 8 batteriesassume probability distribution characterising typical use batteries available.probabilistic problem cast hybrid temporal Markov Decision Process (MDP).Formally, MDP defined follows:Definition 4 Markov Decision Process 4-tuple, (S, A, P, R), set states,finite set actions, P probability function Pa (s, s0 ) = P r(st+1 = s0 |st = s, = a)probability action cause transition state s0 appliedtime t, R reward function, Ra (s, s0 ) reward earned making transitionstate s0 action a.Markov property probability distribution transition stateaffected path state reached. general, MDPs defined finitestate spaces, continuous MDP also considered, states embeddedmultidimensional real space. battery usage problem seen continuous MDP,states tuples define (continuous) state parameters batteries alsocurrent state load battery servicing load (if load non-zero). Actionsproblem indicate battery service load, also correspond eventschange current load. battery problem actions switching batteriesdeterministic, events cause load changes probabilistic, representing uncertaintydemands user powered system. time events also governedstochastic process, timing switching actions controllable.formally, problem n batteries, state characterised tuple(sb1 , sa1 , sb2 , sa2 , ..., sbn , san , B, t, L), sbi bound charge battery i, sai available charge battery i, B number battery currently servicing load (1 B n),time state L current load. state deterministic action, UseB 0 , causes transition state (sb1 , sa1 , sb2 , sa2 , ..., sbn , san , B 0 , t, L), batteryB 0 battery servicing load. also non-deterministic action, wait(T), timeinterval, causes transition state time advanced time t0 + , statecharge battery B updated according battery model load might different(according probability distribution governing loads). interpretation action353fiF OX , L ONG & AGAZZENIadvances time next event, battery depleted available charge,load changes, time passed, whichever first.reward function battery problem gives positive reward transition, proportional advance variable t. system enters state currently activebattery available charge, terminates (or, equivalently, enters special final statetransitions loop without incrementing t). reward system means optimalsolution one greatest duration.solution MDP policy:Definition 5 policy, , MDP (S, A, P, R), mapping : A, specifying actionexecute state.battery problem, policy function determines battery useload must serviced, using current states charge available batteries basismaking decision.Considerable research effort invested problem finding policies MDPs,discussed Section 6.way approach problem see mapping classification, statebatteries mapped class corresponding correct choice battery. usesolutions determinised problems basis classifier construction problem useexisting machine learning approach build good classifier. overall approach sketchedFigure 12.Several important observations made. Firstly, successful construction classifierdepends exploitable structure space defined solutions determinisedproblems. Secondly, states described continuous variables: discretisepurpose building classifier. Thirdly, solution set generally cover whole spacereachable states, important complete policy sensible default actiondeal states policy fails handle. case, default action best-of-n rule,best published hand-constructed policies problem. policy suggestsswitch battery whose available charge critical threshold, policy actionignored, default action used. discuss impact physical experimentsSection 7.Finally, note deployment constructed policies require efficiently implemented cheap hardware. Simple classifier rule systems effectivelyimplemented look-up tables, ideal implementation Field Programmable GateArrays (FPGAs) purpose-built hardware.5.1 Policy Learning Classificationlearn policy classification, first necessary generate appropriate training dataset. problem, data set must associate states batteries current loadappropriate decision (which battery use service load). construct trainingset building sample profiles stochastic description expected loads.distributions used describe amplitude, duration frequency loads shown Figure 13.deterministic solutions problems constructed described Section 4. Trainingdata generated plans simulating execution recording battery354fiP LAN - BASED P OLICIES E FFICIENT ULTIPLE BATTERY L OAD ANAGEMENTFigure 12: Plan-based policy learning: figure illustrates approach policy learningschematically. off-line training phase involves construction set planningproblem instances sampling initial state distribution, followed construction plans instance. plans classified obtain state-to-actionmapping form decision tree used policy.states, load battery choice fixed time increment throughout plan. example,increment 0.01 minutes training data generated plan record battery statescharge (available bound), load currently selected battery (which might mightchanged previous time increment) every 0.01 minute interval throughout plan.experiments selected time increment smallest increment usedvariable discretisation described Section 4.4, requirement approach.choice time increment determines frequency decision-cycle learned policy.time increment also determines much training data generated single plan, accordingmakespan plan. order reduce volume training data fine-grained timeincrements used long makespan batteries, possible randomly sample setstate-battery-selection pairs across multiple plans. experiments need this.training data generated, classifier learned using standard machine learningapproach. W EKA (Hall, Frank, Holmes, Pfahringer, Reutemann, & Witten, 2009) machinelearning framework, developed University Waikato, provides set classificationclustering algorithms data-mining tasks. W EKA takes input training set, comprising listinstances sharing set attributes. order perform classification battery usageproblem data, consider instances following form:= (1 , 1 , . . . , N , N , B, L)355fiF OX , L ONG & AGAZZENIdenote available charge total charge ith battery, respectively, Bcurrently active battery L current load (this essentially state MDPwithout time label, since want policy operate independently time). setting,attribute used class battery B.stochastic load profiles defined distribution of:load amplitude l [100 . . . 750] mA;load/idle period duration [0.1 . . . 5] min;load frequency f [0.3 . . . 0.7].probability distributions shown Figure 13.P(l)P(d)P(f)0.400.400.352500.55000.50.200.151007500.150.100.050.20.1load amplitude l (mA)0.251.00.100.60.42.50.35.0load/idle period duration (min)0.7load frequency fFigure 13: Probability distributions stochastic load profiles10battery scheduleloadload amplitude / battery use86420050001000015000time (0.01 min)2000025000Figure 14: Plan-based policy 8 batteries stochastic loadleads load profiles irregular (see bottom Figure 14) thereforeharder handle regular profiles considered Jongerden et al. generatedset stochastic load profiles produced near-optimal plan usingdeterministic solving described Section 4. set plans used training setclassification process.356fiP LAN - BASED P OLICIES E FFICIENT ULTIPLE BATTERY L OAD ANAGEMENTAlgorithmDMNBtextNaiveBayesNaiveBayesSimpleNaiveBayesUpdateableLogisticMultilayerPerceptronRBFNetworkSimpleLogisticSMOIB1IBkAdaBoostM1AttributeSelectedClassifierBaggingClusteringRegressionCVParameterSelectionDaggingDecorateENDEnsembleSelectionGradingLogitBooostRandomCommitteeRandomSubSpaceRotationForestStackingVoteVFIDecisionTableDTNBDecisionStumpJ48J48graftOneRLADTreeNBTreeSimpleCartcross-validation success18%37%36%36%44%51%43%44%44%99%99%27%98%98%26%98%19%44%99%99%99%19%47%99%99%99%19%19%23%90%90%27%99%99%56%45%99%99%model size26 Mb26 Mb29 Mb18 Mb9 Mb31 Mb15 Mb70 Mb12 Mb21 Mb22 Mb6 Mb6 Mb2 Mb13 Mb114 Mb86 MbTable 3: Performance classification algorithms tested 10,000 training examples357fiF OX , L ONG & AGAZZENIorder select suitable classification algorithm, applied classifiers provided WEKA data set 10,000 training examples. first evaluated performancenumber correctly classified instances cross-validation. discarded classifiersproviding less 70% correctness. considered memory time required useclassifier. output classification process model encoding resulting decisiontree. cases, generated model requires significant memory store (more 500MbRAM memory), slow used. parameters also used determinenumber training examples classify, bigger training set, better performancehigher memory time requirements. classifiers performancereported Table 3....if(b2gamma<=0.297404){if(b2gamma<=0.296404){if(b2gamma<=0.288404){if(b2gamma<=0.286404){if(b2gamma<=0.277404){return 1;}if(b2gamma>0.277404){return 2;}}if(b2gamma>0.286404){return 1;}}if(b2gamma>0.288404){return 2;}}if(b2gamma>0.296404){if(b2y1<=-0.043615){return 1;}if(b2y1>-0.043615){if(b1gamma<=0.164404){return 1;}if(b1gamma>0.164404){return 2;}}...Figure 15: Fragment decision treeAccording criteria, selected J48 classifier, implements machine learning algorithm C4.5 (Quinlan, 1993). output decision tree whose leaves represent, case358fiP LAN - BASED P OLICIES E FFICIENT ULTIPLE BATTERY L OAD ANAGEMENTloadprofileR100R250R500R750Upper boundtime()sw()792.6(15.5)369.8(1.91)226.7(2.13)188.3(0.8)71383(1379)28952(853)14671(512)11519(463)Plan-based Policytime()sw()786.2(15.4)366.7(2.02)224.6(2.27)186.4(0.7)1667(161)1518(143)987(122)302(33)Table 4: Average system lifetime number switches stochastic load profiles 8 batterysystemsstudy, battery used (a fragment tree shown Figure 15). cardinalitytraining set, empirical evaluation showed best result obtained using 250,000 trainingexamples (note involves considering 4 106 real values characterising statesbattery selections training examples) since extending training set makesignificant improvement performance increases memory time requirements.5.2 Results Policiesorder use decision tree embedded WEKA classes loading classification modelbattery simulation framework. model 8 battery case represented tree61 levels consists 7645 nodes, one containing comparison one statevariables threshold. Applying decision tree determine battery loaddecision point takes negligible time.evaluate performance policy considered four probability distributionsdifferent average value load amplitude, namely 100, 250, 500, 750 mA. distributiongenerated 100 stochastic load profiles used policy service them. Note loadprofiles used evaluating policy independent ones used training, althoughdrawn probability distributions.Table 4 shows average value standard deviation system lifetime numberswitches obtained using best-of-8 policy high frequency switching policy.Also case, observe policy achieves 99% efficiency comparedtheoretical upper bound given best-of-8 policy executed high frequency (recallinfeasible practice). Moreover, number switches used policy slightlygreater corresponding deterministic solving, one order magnitude lowercorresponding value best-of-n policy.6. Related Workvariety approaches proposed solving continuous Markov Decision Processes (Sanner & Boutilier, 2009). Meuleau et al. (2009) propose hybrid AO* search, using dynamic programming approach guide heuristic search problems involving continuous resourcesused stochastic actions. approach handle time-dependent resource consumption,appears MDP could modelled solution approach. authors giveempirical data solution problems 25,000 states. model, appropriate359fiF OX , L ONG & AGAZZENIdiscretisation, contains 1086 states 8 batteries. Mausam Weld (2008) describeplanner concurrent MDPs, MDPs temporal uncertainty. Again, problemssimilar ours, although planner manage continuous time-dependent resources,directly applicable problem. Furthermore, largest problems consider contain4,000,000 states take hour solve.solving large MDPs, researchers identified variety techniques helpovercome prohibitive cost policy iteration value iteration, classical techniquessolving MDPs. general, techniques approximate solution, often focussing partspolicy apply states likely visited along trajectory. Relevant techniquesdiscussed work Bertsekas Tsitsiklis (1996).approach branch work devoted development plan-based reasoninguncertainty. fact, explicit modelling uncertainty impractical, sampling provideeffective alternative.Hindsight Optimisation (HO) (Chang, Givan, & Chong, 2000; Fern, Yoon, & Givan, 2006)become well-researched technique learning policies based plans. policy alwaysproposes best action next state, therefore less robust uncertaintyencountered reality. HO technique works follows: given MDP state, s, firststep sample, MDP, large number deterministic instances processinitial state s. next step solve instances using deterministic planner fixedhorizon. Finally, estimated value state computed average value obtaineddeterministic plans. possible choose, state, move led bestperformance average samples.Although approach similar Hindsight Optimisation, significant differences.First, previous works direction addressed propositional domains (see, e.g.work Fern, Yoon Givan (2004, 2006, 2007), Konigsbuch, Kuter Infantes, 2010)interested hybrid discrete-continuous problem, deal non-linearcontinuous deterministic planning models drain recovery behaviour batteries, usingsampling provide noise encountered reality. approach sample deterministicinstances problem using simple assumptions underlying distributions governingphysical reality. many natural situations, Gaussian distributions work well approximationuncertainty problem. work, example, show sampling manydeterministic discretised cases, planning solutions exactly, possibleclassify states solution plans policy robustly manage load distributionsimulated real battery configurations. weaknesses assumptions madeunderlying distributions overcome introducing default actions (described Section 5),applied policy finds state outside range applicabilitypolicy. Integrating policy default action leads competent policies performwell across wide range physical situations, including situations dissimilarencountered learning phase.Another important difference rather averaging plan states obtain policy,approach use decision tree classifier arrange states according informationcontent (reflected well support partitioning planned actions). resultsclassification actions states, policy proposes best action use statedetermined online comparing policy state variables real values encountered policyexecuted. Although training policy-learning expensive terms time computational360fiP LAN - BASED P OLICIES E FFICIENT ULTIPLE BATTERY L OAD ANAGEMENTresources, planning learning done offline, offline process strongly resourcebounded. classification phase produces policy form decision tree, compactexecution takes negligible time key feature application. fact,due continuity involved battery model, need planning long horizon(up 60,000 time steps), resulting state space huge. makes approach basedexplicit mapping state action impractical. particular, possible computeHO-based policy offline map state best action according policy values.hand, using HO online (which viable many cases) infeasible application,nature battery scheduling problem requires fast interaction policybattery system. approach meets scalability fast-response requirements.Finally, idea looking ahead scenarios, benefiting experience gained, powerful. HO assumed that, general, experience deterministicplanner sufficient give insights best moves possible real state encounteredexecution. However, another important aspect makes approach different, investigated deeply different context (Fox, Long, & Magazzeni, 2012), that, many cases,necessary distinguish plan state policy state. example, planstate might contain variable representing whether unreliable valve open closed, observable experience records effects unreliability example, effect flow-ratepipe given time period. policy-state variable therefore constructed recordobserved flow rate, proxy whether valve open closed. approach,call observable-correlate policy learning, different averaging plan statesencountered planning, policy states capture actual situation experienced,plan states remain abstracted distanced reality. work (Fox et al., 2012),apply exactly policy-learning technique described problem learning robustobservable-correlate policies following boundary surface algal bloom. contextdefine collection policy state variables correlate plan state variables observableexperience.7. Physical Experimentssection report results obtained kitchen table experiment comprising simplecircuit constructed breadboard components Arduino Mega board usedsensing control.1 Using apparatus able demonstrate simulationresults translate reality. part future work, experiments undertakenprofessional laboratory continue explore benefits limitations approach.goal experiment demonstrate plan-based policy method achieves similarlifetime achieved best-of-two policy, significantly reduced switching.clear simulation results plan-based policy achieve close optimal lifetimefraction switching best-of-two requires, although simulation also suggestsbest-of-two policy achieve within less 1% theoretical optimal even switchingfrequency every 5 minutes. therefore expected little opportunity learnedpolicy improve lifetime therefore hoping achieve similar lifetime1. results figures presented throughout section presented colour order clarify relationshipsmultiple plots. Unfortunately, several figures difficult interpret monochrome readerrecommended view figures using appropriate medium.361fiF OX , L ONG & AGAZZENIFigure 16: photograph battery apparatus constructed manage two batteries.much lower switching frequency. results show plan-based policy exhibit muchlower frequency switching. fact found plan-based policy achieves significantly longerlifetimes well.begin describing built circuit used experiment. recallKiBaM model, explain parameters estimated. plan-based best-of-twopolicies rely able read state available charge batteries. difficultestimate, performance policies depends absolutely estimating quantityaccurately, explain read state available charge set-up. Finally presentresults experiments describe plans future work.7.1 Electronic Apparatusconstructed experimental apparatus suite two batteries, shown Figure 16.used Ritar 6 volt lead acid batteries nominal capacity 1 Amp hour 20 hours discharge(1Ah@20h). connected batteries circuit Arduino Mega board.Part circuit constructed allow Arduino read voltage connectedbattery. want ensure current drawn measure voltage negligible, highexternal resistance, 3.6k 7.2k, used bridge Arduino input. Using voltmeterread 6.5-6.7V fresh battery, consider VEM F = 6.5V. high voltageArduino inputs maximum input voltage 5V. Since, considering batteryvoltage sensing element circuit resistance R, VEM F = iR VEM F = 6.5V ,use R = 7.2 + 3.6 = 10.8k order divide voltage achieve negligible current0.0006A. higher resistance might seem preferable still reduce current losses,362fiP LAN - BASED P OLICIES E FFICIENT ULTIPLE BATTERY L OAD ANAGEMENT3.611883.6+ 6VB1Volts (L2 Lo)Volts (L2 Hi)PWM S2Volts (B2)Volts (L1 Lo)Volts (L1 Hi)PWM S1Volts (B1)GndArduino Mega+ 6VB27.27.2Figure 17: battery apparatus two batteries.Arduino uses analog-to-digital converter based measuring charge capacitor time.approach relies sufficient current flow capacitor get accurate measurementsshort time periods high resistance prevents this. practice, resistance 10klimit Arduino respond changes inputs within timing constraintssampling. resistances voltage reading Arduino VEM F 0.00063600 = 4.34V , within operating range.current diverted load consisting switch two resistors 8 1. roleswitch, MOSFET IRF630 controlled using pulse width modulated outputArduino, ensure smooth delivery power resistors. load 6.5/(9+r +Rs )r internal resistance battery Rs effective variable switch resistance pulsewidth modulated control. data sheet Ritar 6V battery lists internal resistance, r,50m, measured 0.34, value almost 7 times greater. believe discrepancycomes systematic distortion sensed values reported Arduino. consistentlyuse readings experiments regard discrepancy systematic error.experiments use currents varying 0.2A 0.3A, so, VEM F = 6.5V = 0.3A,Rs 12, lower battery less charged (and voltage drops) higherlower current load required.circuit diagram shown Figure 17. noted load duplicateddesign, completely separates parts circuit responsible interacting363fiF OX , L ONG & AGAZZENIbattery. fielded systems load would common diodes used prevent flow electricitybatteries different charge states.7.2 Estimating Parameterswork used Kinetic Battery Model (Manwell & McGowan, 1993) followedparameter estimation process described Manwell McGowan (1994). Followingdescription, extended KiBaM three parts: capacity model, voltage model lifetimemodel. use simple lifetime model (we assume change battery behaviourdue recharging).7.2.1 C APACITY ODELcapacity model, describes capacity varies battery drained allowedrest, described first order differential system. quantityqmax (I)maximum amount charge, Amp hours, could hope extract batterydischarged continuously, nominal current I, drained. time takes drainbattery nominal current . linked following equation:qmax (I) =1 ek0Ck 0 cT+ c(k 0 1 + ek0 )derived model described Section 3.2. model relies three constants: C,maximum capacity battery Amp hours, k, rate per hour conductancebound well available well model, c, ratio availablekcharge maximum capacity. Section 3.2, k 0 defined c(1c). seen qmax (I) =.constants found fitting curve data. obtained data draining batteriesone time, fully charged state, using different currents circuit describedSection 7.1. example data collected shown Figure 18, top curvemeasured voltage battery time, line 5.25V point batteryconsidered dead, point cloud comprising thick curve 208mA measured load,thin straight line running point cloud rolling average load. vertical lineshows treated battery dead. shown Figure 19, uncertaintyexactly battery dies.values C, k, c calculated are:C = 1.372Ahk = 0.1967h1c = 0.3870k 0 = 0.8290h1364fiP LAN - BASED P OLICIES E FFICIENT ULTIPLE BATTERY L OAD ANAGEMENTFigure 18: Battery discharge curve: terminal voltage (top curve) load (bottom curve).millivolts (top curves) 0.1 milliamps (bottom curves)560055005400530052005100500049004800330003350034000345003500035500half-seconds36000365003700037500Figure 19: close point cloud voltage curve point batteryconsidered dead.365fiF OX , L ONG & AGAZZENI0.7Observed dataFitted curveData Sheet values0.6- Amps0.50.40.30.20.100246810- hours1214161820Figure 20: Data current time drain batteries. Data Sheet values shown comparison.fitted curve I, fitted C, k, c values, shown Figure 20. squarepoints observed data, stars data points reported Ritar 6V batterydata sheet. found data sheet appears consistently under-estimate performancebattery. seen observed data points clustered 0.17A 0.3Aregion curve. unable report points lower currents, pulse widthmodulation could set appropriately low value without dropping control voltageMOSFET switch point switch opens. could report points highcurrents without melting resistors comprising load circuit.used C, k, c values construct initial state battery load managementplanning problem, learned policy plans produced model. Therefore,accurate estimation parameters important. policy far less effectivewrong capacity model used. learned policy using time granularity 0.01h,36 seconds. timing loops collecting data Arduino sensors use averagescomputed 0.5 seconds: data points Figure 19 shown resolution. Thus,collect 72 data points sensor decision points granularity planningmodel and, consequently, learned policy. seen, considerable noisevalues reduce noise construct rolling average preceding window 65points. selected 65 avoid particularly noisy data values generated switchbatteries.366fiP LAN - BASED P OLICIES E FFICIENT ULTIPLE BATTERY L OAD ANAGEMENT7.2.2 VOLTAGE ODELorder able exploit plan-based policies necessary able evaluate statecharge batteries every decision point. known difficult accurately evaluatestate charge behaviour batteries noisy, variable highly non-linear. However,terminal voltage recognised reasonable proxy state charge. therefore observeoutput voltage battery calculate state charge reading.measured terminal voltage, Eobs , falls battery drained, producing typicalknee-shaped curve representing decrease voltage time current drawn,illustrating collapse voltage battery dead. Manwell McGowan modelvoltage curve using equation:Vobs = VEM F + AX + BX/(D X)QX defined qmax(I) Q total charge consumed date battery.parameters A, B found non-linear curve fitting data, using voltagetime constant current discharges. used 4 sets data obtained draining batteriesfully charged, one time battery apparatus, estimate curve Ritar 6V batteries.Figure 21 shows example discharge curve. batteries effectively dead soonvoltage drops knee. occurs 5.25V . Figure 21 also shows voltage model curve (thesolid black line), type described above, fitted discharge data battery. casedischarged battery past critical point considered dead, showvoltage drops dramatically (and load cannot maintained reliably). vertical line showspoint battery judged dead curve fitted data point.seen, curve fits well knee, behaviour longer governedsimple quadratic voltage model.parameter values computed batteries are:= 0.194mV s1B = 2.22 103 mV s1= 1.05h.governs almost linear decay voltage first part discharge curveeasiest parameter estimate accurately. B together determine shape initiationdip voltage battery gets close dying threshold. fit values Bmuch sensitive noise value A.7.2.3 E VALUATING TATE C HARGE BATTERYUsing Arduino Mega board, collect voltage current values batteries frequency every half second. battery use, compute rolling average last65 voltage readings reported since battery first loaded (before this, reported voltagereadings inaccurate). computed first rolling average fix VEM F ,value take fully charged open circuit voltage battery (ie: voltageavailable load serviced). calculate Eobs Q every 36 seconds everybattery.367fiF OX , L ONG & AGAZZENIFigure 21: Voltage time.observed voltage affected load battery time observe it,adjust observed voltage reading, Eobs , take account internal resistance loadbattery. results unloaded observed voltage Vobs :Vobs = Eobs + 0.34Iobscalculate difference Vobs VEM F be:Vadj = Vobs VEM F .Then, calculate X first obtain value F :F =B + AD + Vadj2AThen:rDVadjuse root quadratic equation X X 1.given battery, b, calculate charge consumed b time t, sum currentreadings taken far (measured milliamps, taken every half second) divided large constant,7.2 106 , gives result Amp hours. value Q, total charge consumed dateb.value X, proportion available charge current drawn,obtained two parameters Eobs Q, using voltage model given above.Q.X Q, compute qmax (I) XX=FF2368fiP LAN - BASED P OLICIES E FFICIENT ULTIPLE BATTERY L OAD ANAGEMENTevaluate state charge battery. variable total capacity,C, minus total charge consumed, Q, Amp hours. gives us estimate totalremaining charge, accessible bound chemicalproperties battery. variable difference bound available chargewells, enabling us estimate long would need drain battery. Since available chargealways less equal bound charge, always pair values (Inom , Tnom ),that, battery run Inom time Tnom , would reached current statecharge. GivenQX=qmax (Inom )using equation qmax (I) given Section 7.2.1,Ck 0 cXTnom00= 1 ek Tnom + c(k 0 Tnom 1 + ek Tnom )QTherefore, Tnom solution01 + (c 1)ek + ck 0 (1CX)T = 0Qtime, Tnom , nominally required continuously drain battery fully charged,current I, calculated numerically plugging equations Newton-Raphsonmethod, appropriate initial value (we use 4, since expected lifetime batterydischarge rates using 2-4 hours). Given that:qmax (I) = Inom Tnomthat:Inom =qmax (I)Tnomcomputed as:0Inom (1 ek Tnom )ck 0available charge calculated as:c( (1 c))discussed Section 3.2.best-of-two policy discussed Section 3 implemented always choosebattery highest available charge. Executing policy requires state chargeread reasonable accuracy fixed frequency. example, one might fix frequencyevery 6 minutes, select next 6-minute interval battery highest availablecharge (which equal c( (1 c)) explained Section 3.2).369fiF OX , L ONG & AGAZZENI7.2.4 R ECHARGING E FFECTSclear perform multiple experiments lead-acid batteries necessaryrecharge discharges. Recharging lead-acid batteries known impactperformance: deteriorate repeated cycling. However, gel-type batteriesused deep cycle batteries cycled hundreds times reach enddesign life.Manwell McGowan (1994) proposed lifetime model based rainflow cyclecounting algorithm takes account fact recharging damages batteriesaffects ability deliver charge. Given batteries brand new, usedone 30 times, hypothesise effects repeated discharging rechargingsignificant lifetime experiment2 . extended, larger scale experiment,rainflow model would interest, adopting it, exploring changes behaviourmodel, left future work.additional important effect battery behaviour temperature. experimentsconducted office environment normal working temperatures. One factorsgoverned choice discharge currents fact high discharge currents batterieswarm noticeably, model using likely cease valid without changesparameters. ignored temperature effects treat batteries though usedconstant standard operating temperature, reasonable approximation.7.3 Experimentscarried three sets experiments apparatus consisting two Ritar 6V batteries connected circuit shown Figures 16 17. simulation tests demonstratedperformance approach suites 8 batteries, performing experimentsphysical apparatus would time-consuming. 2-battery experiments took11 hours drain batteries and, anything went wrong experiment, losscommunications PC, experiment restarted resulting loss daymore.performing experiments noticed Arduino distorts measured values:time voltages, therefore amps internal resistance. distortions appear consistentacross experiments, resulting systematic error. particular, times measuredsuggest Arduino measures 1 hour every 1.4 hours real time, 7 8 hour lifetimemeasured Arduino actually approximately 10 11 hours real time. report datavalues directly Arduino measurements, unadjusted systematic errors,borne mind lifetime values considerably longer measured real time.consistency, times reported relative measures (in practice, timing loadcontrol discharge curves values performed using Arduino clock,measurements entirely consistent one another).randomly generated 10 different load profiles, drawn distribution usedtrain policy, alternating 0.2 0.3 Amps intervals constantload durations distributed around 30 minutes distribution shown Figure 22.2. experiments report load profiles 16 run batteries cycled 15 times.later profiles observe batteries showed behaviour suggested slight deteriorationperformance possible lifetimes lower experiments would case new batteries.370fiP LAN - BASED P OLICIES E FFICIENT ULTIPLE BATTERY L OAD ANAGEMENTFigure 22: Distribution load durations used experiments.load profile ran best-of-two plan-based policy could performdirect comparison lifetime achieved number switches performed. resulted 16load-execution experiments. first two load profiles restricted best-of-two policyswitch every 5 minutes, best-of-two policy plan-based policy switchedsimilar number times entire run. simulation results suggest plan-based policyswitch 20 times, experiments reveal noise sensordata leads errors estimation state charge cause policy switchfrequently would anticipate. Frequent switching indicates policy respondingspurious artifacts sensed data variability real behaviour batteries.discuss Section 8.plan-based policy applied every 36 seconds (0.01 hours), reflecting granularityplans learned policy. also ran experiment best-of-two policy allowedswitch every 36 seconds, ensure results obtained biased offeringplan-based policy faster reaction time, changes battery state charge, best-of-two.wanted establish whether plan-based policy achieve similar lifetimes bestof-two policy lower numbers switches. also wished confirm betternaive simple policy sequencing, first battery used dead,second battery used. obvious (the sequencing policy much worse simulation),observed behaviour plan-based policy superficially similar sequencing, sincefavours mostly using one battery heavily discharged switching second batterysignificant intervals, thought useful perform physical comparison. case2-battery setup sequencing involves 1 switch (the minimum number switches possibletwo battery case).ran 21 complete experiments total. plots showing battery voltagesexperiments, last lowest point battery voltage curves (the red green curves)points corresponding battery died.Figure 23 shows best-of-two policy running second load profile. curves showcharacteristic discharge/recovery pattern, separated step separation caused internalresistance battery (when battery recovering voltage open circuit, loadedreduced internal resistance).371fiF OX , L ONG & AGAZZENIFigure 23: run showing behaviour best-of-two policy.Figure 24: run showing behaviour plan-based policy.load voltage curves red curve (battery B1 ) fuzzy noisereadings sensors battery. phenomenon consistentlyproblem B1 dependent battery, appears feature circuit itself.372fiP LAN - BASED P OLICIES E FFICIENT ULTIPLE BATTERY L OAD ANAGEMENTFigure 25: Best-of-two plan-based policy, running load profile.seen lifetime achieved plan-based policy longer, numberswitches also reduced. y-axis removed, load measured tenthsmilliamps voltage tenths millivolts, before.strange striations green (B2 ) curve start graph due failureArduino correctly capture battery voltage period, affectperformance policy (we simple fail safes ensure spurious data sortaffect performance).Figure 24 shows behaviour plan-based policy running second load profile.top two curves represent usage two batteries, B1 B2 . Battery B1 (the red curve)used first 10,000 half-seconds, B2 briefly used policy switches backB1 half way run. second half graph, two batteriesinterleaved, rising curves B1 correspond periods B2 use B1resting.alternating load represented bottom two curves. seen loadchanges, measured voltage changes (the top curve registers slight blip).internal resistance means lower voltage loss battery currentchanges. would expect 34mV (if internal resistance 0.34)difference current 0.1A. actually higher that, appearsslight over-reaction changes load, causing battery voltage drop sharplybattery first loaded, pull back, battery tends recover sharply, fallback line, load reduced.Figure 25 shows best-of-two policy plan-based policy run secondload profile side-by-side. red plots B1 green B2 . blue purple points shows373fiF OX , L ONG & AGAZZENIFigure 26: Two executions plan-based policy different load profiles. y-axisremoved, load measured tenths milliamps voltage tenths millivolts,before.B1 /B2 serviced load (and value load) best-of-two, black points,slightly displaced these, show B2 serviced load plan-based policy (B1serviced load rest time). voltage curves plan-based policy offsetcurves best-of-two displayed plot. labellingy-axis removed avoid confusion. see three interesting features:1. plan-based policy tends use B1 first B2 second, although sequentially.2. plan-based policy runs longer, demonstrating increased lifetime achieved.3. Best-of-two essentially alternates batteries (minor variations due slightdiscrepancies batteries factors).Figure 26 shows comparison plan-based policy working first second loadprofiles. performance policy first load profile shown upper voltage curvesupper load curves, curves second load profile displaceddifferentiate them. plot highlights similarity way policy manages batteriescase: general strategy run B1 knee, resting briefly period,oscillate B1 B2 low frequency while, entering periodB1 rapidly switched B2 B1 converges empty. policy finishes B2 .374fiP LAN - BASED P OLICIES E FFICIENT ULTIPLE BATTERY L OAD ANAGEMENTFigure 27: plan-based policy plotted estimated available charge. y-axisremoved, load measured tenths milliamps voltage tenths millivolts,before. Charge measured tenths milliamp hours.interesting difference consequence (random) loads: B1 faced heavierloads first part second profile, dies faster first profile. However, B2faces slightly less arduous time second half second profile manages lastconsiderably longer. particular, load interval 30,00033,000 high load servicedB2 first profile, period happens lower load second profile.key reason B2 dies faster first profile: available charge depletedperiod real opportunity rest point. final period load firstprofile high load kills B2 quickly, final period load second profilelower one. allows B2 recover bound charge period, depletingavailable charge slowly sustaining little longer critical period.Figure 26 upper policy execution switches frequently window 41,00043,000 half seconds, B1 dies. plan-based policy includes defaultaction switch battery avoid currently loaded battery dying prematurely.reason protect batteries policy effects errors sensor datapropagate state charge model. effect default action casecause policy switch B2 B1 almost charge, back B1 soonrecovered enough able loaded (according state charge model).Figure 27 shows policy first load profile again, time plotted estimatedavailable charge (based voltage readings voltage model). graph shows severalimportant features. black crosshairs mark estimated available charge (measured 0.1mAhunits) B1 grey crosshairs show B2. discontinuities due changing375fiF OX , L ONG & AGAZZENIFigure 28: sequencing policy showing shorter lifetime load profile 2.load values. discontinuity, model adjusts load (usingestimated internal resistance), clear additional effect cannotcapture way. already mentioned, also case discrepancybattery terminal voltage readings different loads 0.1A 0.34 = 34mV ,0.1A difference load 0.34 internal resistance, graph shows differencesmuch greater. effect appears worsen battery discharges (see wideninggaps loaded unloaded voltages recorded batteries red/green curvesparticularly red curve). However, interestingly, voltage-capacity model seemsmarginally less unstable lower states charge (the steps get slightly smaller casesblack curve).also seen, available charge model breaks situations (whenobservations cannot fitted consistently initial state assumed battery). leadsavailable charge values negative (particularly 4200045000 period).causes policy revert default action, somewhat simplistic implementationdefault leads oscillation batteries period.Figure 28 shows results obtained draining batteries sequence, using secondload profile. performance optimal terms switching, lifetime achieved muchshorter achieved plan-based policy similar lifetime best-of-twocase. fact best-of-two worse sequential scheduling profileprobably due variation battery behaviour: seems likely best-of-two performsimilarly results load profiles.clearly seen plan-based policy achieves consistently longer lifetimebest-of-two policy, significantly reduced switching. results summarised Table 5.376fiP LAN - BASED P OLICIES E FFICIENT ULTIPLE BATTERY L OAD ANAGEMENTLoadProfilePlan-based PolicyLifetime SwitchesBest-of-twoLifetime SwitchesSequentialLifetime SwitchesMax.127.8878.03371477.5347.00073817.07918.778.913456789107.9747.8317.0307.1207.6697.6778.3416.972911581736218833137.5636.9986.2267.0857.6456.5155.9016.8907057016097066495845676909.049.239.118.819.118.878.918.92Mean7.65357.56.936651.47.07918.97Table 5: Table summarising results physical experiments. Lifetimes given hours,reported using Arduino clock measurements revealed hourmeasured Arduino approximately 1.4 hours real time. first two experiments used lower switching frequency Best-of-two policy: seen,increased frequency later experiments offer apparent advantage.two results included calculating mean number switches Best-of-twopolicy.paired t-test results shows significant (p = 0.013). expectimprovements even marked case n > 2 batteries, performing experiments topic future work. final column table, labelled max showstheoretical maximum lifetime batteries given load profile. values probablyrather higher maximum value could achieved practice, since pointbatteries considered dead based observed terminal voltages loaded. internalresistance batteries means point earlier idealised battery modelused simulation. average efficiency batteries 85% policy 77%best-of-two compared theoretical maximum, consistent expectation theoretical value rather high previously reported performance batterymanagement systems typically achieve around 80% efficiency.8. Future Workpaper brings together three distinct directions research. Firstly, work concernedspecific problem solution: management multiple batteries. Secondly, developexploit techniques planning PDDL + continuous non-linear dynamics. Thirdly,devise implement approach policy construction based planning deterministicsamples. directions offers scope work.377fiF OX , L ONG & AGAZZENIFigure 29: battery voltage, load estimated charge curves plan-based policy runningload profile 4. axis shows millivolts, 0.1 milliamps 0.1 milliamp hourscurve respectively.research battery management potential real application physical experiments reveal theoretical results translate measurable benefits. physical experimentsshow higher switching rates plan-based policy control simulation results lead oneexpect noted key reason errors attempt diagnose statecharge batteries noisy sensed voltage data. anticipate robust sensingcould resolve problem extent, modification consider carefulimplementation default action tracking state charge. Figure 29 showsplan-based policy run fourth load profile, estimated available charge often judgednegative! triggers application default action many cases switchescontrary policy choices either side spurious data point. fact, 158 switchesexecution run, least 90 generated spurious data triggering default actions. Similarly,load profiles 13 identify least 50, 8 54 cases respectively, defaultaction causes switch batteries advice policy sensible state chargeestimates either side switches. strongly suggests careful implementationestimation state charge, respecting expected continuity behaviour, couldlead much better switching rates better stability behaviour policy.experiments would obviously benefit performed robustly constructedexperimental apparatus additional runs accumulate additional data. hope continue pursue direction collaboration commercial partners might interestedexploiting ideas achieve fielded systems.378fiP LAN - BASED P OLICIES E FFICIENT ULTIPLE BATTERY L OAD ANAGEMENTwork continuous planning, particularly problems include complex processesevents, remains focus research interest us. considering problems arisingdifferent domains, including control autonomous underwater vehicles control powersystems (Bell, Coles, Coles, Fox, & Long, 2009). also exploring ways hybridplanning might interface effectively lower control levels shared model systemdynamics. role dynamic discretisation managing complex process dynamics, particularlynon-linear behaviours, one continuing explore.work construction policies via classification trajectory samples builtplanner applied sampled initial states also direction continuing pursue. recentwork algal bloom mapping (Fox et al., 2012) indicates directions considering.particular, states used planning model allow planner solve sampled problem instancesneed states used learning policy. important,planner exploit knowledge available determinised instances problem find highquality solutions hope careful selection observable elementsvisited states presented classifier, classification process discover correlationsobservable states actions selected planner states, orderidentify effective policy structures. potentially powerful way approach planninguncertainty intend investigate much further.9. Conclusionspaper presented interesting potentially important problem, managing systems powered multiple independent batteries, constructed novel solution it.brought together research planning policy learning arrive new powerful approach.experimentally evaluated plans learned policies simulation results reveal solution achieve better 99% efficiency compared theoretical optimal(which unachievable practice). achieve high efficiencies,low cost terms battery switching. beneficial switching wasteful energytends reduce quality service without additional smoothing circuitry adds energylosses.confirmed results simulation gone explore behaviourideas physical tests results confirm real batteries far less well-behavedsimulated counterparts. Nevertheless, policies learn continue behave successfullyindeed get results showing 5% 15% lifetime improvements best-of-twopolicy equal load profiles, still achieving lower switching rates.approach solving battery usage problem adapts several existing technologiesautomated planning, solve problem seen MDP. use Monte Carlo samplinggenerate instances determinised load profiles solving problems using optimaldeterministic solver, combining solutions form policy. Adopting sampling approachtackling problem-solving uncertainty become increasingly common onereasons usually offers better scaling opportunities attempting explicitlyreason distributions. policy construction approach adapts use machine learningconstruct classifier. construction high quality solutions deterministic problems, usespecial variable-range discretisation solve non-linear continuous optimisation problemhigh accuracy, exploring small proportion state space.379fiF OX , L ONG & AGAZZENIapproach scalable effective. Although solution implement paperdomain-specific several respects, components general already begunillustrate point adapting approach problems. elements tailoredproblem selection discretisation range search heuristic. However,believe characteristics multiple battery usage problem shared, outline,domains expect approach adapted domains relative ease.Acknowledgmentswould like thank Marijn Jongerden Boudewijn Haverkort introducing us multiple battery usage problem, drawing attention scheduling problem related policybased approaches. would also like extend thanks anonymous reviewershandling editor, Carmel Domshlak, help improving text paper.work partially funded EPSRC Project Automated Modelling Reformulation Planning (EP/G0233650).ReferencesAlur, R., & Dill, D. L. (1994). Theory Timed Automata. Theoretical Computer Science,126(2), 183235.Bell, K. R. W., Coles, A. J., Coles, A. I., Fox, M., & Long, D. (2009). Role AI PlanningDecision Support Tool Power Substation Management. AI Communications, 22(1), 3757.Benini, L., Castelli, G., Macii, A., Macii, E., Poncino, M., & Scarsi, R. (2001). Discrete-TimeBattery Models System-Level Low-Power Design. Large Scale Integration (VLSI)Systems, IEEE Transactions on, 9(5), 630 640.Benini, L., Macii, A., Macii, E., Poncino, M., & Scarsi, R. (2003). Scheduling Battery UsageMobile Systems. Large Scale Integration (VLSI) Systems, IEEE Transactions on, 11(6),1136 1143.Bertsekas, D. P., & Tsitsiklis, J. N. (1996). Neuro-Dynamic Programming. Athena Scientific.Chang, H. S., Givan, R., & Chong, E. K. P. (2000). On-line Scheduling via Sampling. Proceedings Int. Conf. Automated Planning Scheduling (ICAPS), pp. 6271.Della Penna, G., Intrigila, B., Magazzeni, D., & Mercorio, F. (2009). UPMurphi: Tool Universal Planning PDDL+ Problems. Proceedings Int. Conf. Automated PlanningScheduling (ICAPS), pp. 106113.Eberle, W. A. T. (2008). MOSFET Current Source Gate Drivers, Switching Loss ModelingFrequency Dithering Control MHz Switching Frequency DC-DC Converters. Ph.D. thesis,Queens University, Kingston, Ontario, Canada.Fern, A., Yoon, S. W., & Givan, R. (2004). Learning Domain-Specific Control KnowledgeRandom Walks. Proceedings Int. Conf. Automated Planning Scheduling (ICAPS),pp. 191199.Fern, A., Yoon, S. W., & Givan, R. (2006). Approximate Policy Iteration Policy Language Bias: solving Relational Markov Decision Processes. J. Artificial Intelligence Research(JAIR), 25, 75118.380fiP LAN - BASED P OLICIES E FFICIENT ULTIPLE BATTERY L OAD ANAGEMENTFox, M., & Long, D. (2006). Modelling Mixed Discrete-Continuous Domains Planning. J.Artificial Intelligence Research (JAIR), 27, 235297.Fox, M., Long, D., & Magazzeni, D. (2011). Automatic Construction Efficient Multiple Battery Usage Policies. Proceedings Int. Conf. Automated Planning Scheduling,(ICAPS), pp. 7481.Fox, M., Long, D., & Magazzeni, D. (2012). Plan-based Policy-Learning Autonomous FeatureTracking. Proceedings Int. Conf. Automated Planning Scheduling (ICAPS).Hall, M., Frank, E., Holmes, G., Pfahringer, B., Reutemann, P., & Witten, I. H. (2009). WEKAData Mining Software: Update. SIGKDD Explorations, 11(1), 1018.Howey, R., Long, D., & Fox, M. (2004). VAL: Automatic Plan Validation, Continuous EffectsMixed Initiative Planning Using PDDL. Proceedings Int. Conf. Tools AI(ICTAI), pp. 294301.Jongerden, M., Haverkort, B., Bohnenkamp, H., & Katoen, J.-P. (2009). Maximizing System Lifetime Battery Scheduling. Proceedings 39th Annual IEEE/IFIP Int. Conf. Dependable Systems Networks (DSN 2009), pp. 6372.Jongerden, M., & Haverkort, B. (2008). Battery Modeling. Tech. rep. TR-CTIT-08-01, CentreTelematics Information Technology, University Twente.Jongerden, M., & Haverkort, B. (2009). Battery Model Use?. IET Software (Special IssuePerformance Engineering), 3(6), 445457.Manwell, J., & McGowan, J. (1993). Lead Acid Battery Storage Model Hybrid Energy Systems.Solar Energy, 50, 399405.Manwell, J., & McGowan, J. (1994). Extension Kinetic Battery Model Wind/HybridPower Systems. Proceedings 5th European Wind Energy Association Conference(EWEC), pp. 284289.Mausam, & Weld, D. S. (2008). Planning Durative Actions Stochastic Domains. J. ArtificialIntelligence Research (JAIR), 31, 3382.Meuleau, N., Benazera, E., Brafman, R. I., Hansen, E. A., & Mausam (2009). Heuristic SearchApproach Planning Continuous Resources Stochastic Domains. J. Artificial Intelligence Research (JAIR), 34, 2759.Quinlan, J. R. (1993). C4.5: Programs Machine Learning. Morgan Kaufmann Publishers Inc.,San Francisco, CA, USA.Rao, R., Vrudhula, S., & Rakhmatov, D. (2003). Analysis Discharge Techniques MultipleBattery Systems. Proceedings 2003 Int. Symposium Low Power ElectronicsDesign (ISLPED 03), pp. 4447.Sanner, S., & Boutilier, C. (2009). Practical Solution Techniques First-Order MDPs. ArtificialIntelligence, 173(5-6), 748788.SBS Implementers Forum (2000). System Management Bus (SMBus) Specification, Version 2.0.Tech. rep., System Management Interface Forum, Inc.Teichteil-Konigsbuch, F., Kuter, U., & Infantes, G. (2010). Incremental Plan Aggregation Generating Policies MDPs. Proceedings 9th Int. Conf. Autonomous Agents MultiAgent Systems (AAMAS), pp. 12311238.381fiF OX , L ONG & AGAZZENIWang, T., & Cassandras, C. G. (2011). Optimal Control Multi-Battery Energy-Aware Systems.Proceedings 50th IEEE Conference Decision Control European ControlConference (CDC-ECC), pp. 14971502.Yoon, S. W., Fern, A., & Givan, R. (2007). Using Learned Policies Heuristic-Search Planning.Proceedings Int. Joint Conf. Artificial Intelligence (IJCAI), pp. 20472053.382fiJournal Artificial Intelligence Research 44 (2012) 533-585Submitted 03/12; published 07/12Domain Function: Dual-Space ModelSemantic Relations CompositionsPeter D. Turneypeter.turney@nrc-cnrc.gc.caNational Research Council CanadaOttawa, Ontario, Canada, K1A 0R6AbstractGiven appropriate representations semantic relations carpenter woodmason stone (for example, vectors vector space model), suitablealgorithm able recognize relations highly similar (carpenterwood mason stone; relations analogous). Likewise, representationsdog, house, kennel, algorithm able recognize semanticcomposition dog house, dog house, highly similar kennel (dog house kennelsynonymous). seems two tasks, recognizing relations compositions,closely connected. However, now, best models relations significantlydifferent best models compositions. paper, introduce dual-spacemodel unifies two tasks. model matches performance bestprevious models relations compositions. dual-space model consists spacemeasuring domain similarity space measuring function similarity. Carpenterwood share domain, domain carpentry. Mason stone sharedomain, domain masonry. Carpenter mason share function,function artisans. Wood stone share function, function materials.composition dog house, kennel domain overlap dog house(the domains pets buildings). function kennel similar functionhouse (the function shelters). combining domain function similarities variousways, model relations, compositions, aspects semantics.1. Introductiondistributional hypothesis words occur similar contexts tend similarmeanings (Harris, 1954; Firth, 1957). Many vector space models (VSMs) semantics usewordcontext matrix represent distribution words contexts, capturingintuition behind distributional hypothesis (Turney & Pantel, 2010). VSMs achievedimpressive results level individual words (Rapp, 2003), clearextend level phrases, sentences, beyond. example, knowrepresent dog house vectors, represent dog house ?One approach representing dog house treat unit, way handleindividual words. call holistic noncompositional approach representingphrases. holistic approach may suitable phrases, scale up.vocabulary N individual words, N 2 two-word phrases, N 3 threeword phrases, on. Even large corpus text, possiblephrases never appear corpus. People continually inventing new phrases,able understand new phrases although never heard before;able infer meaning new phrase composition meaningsc2012National Research Council Canada. Reprinted permission.fiTurneycomponent words. scaling problem could viewed issue data sparsity,better think problem linguistic creativity (Chomsky, 1975; Fodor &Lepore, 2002). master natural language, algorithms must able represent phrasescomposing representations individual words. cannot treat n-grams (n > 1)way treat unigrams (individual words). hand, holistic approach idealidiomatic expressions (e.g., kick bucket) meaning cannot inferredcomponent words.creativity novelty natural language require us take compositional approach majority n-grams encounter. Suppose vector representations dog house. compose representations represent doghouse ? One strategy represent dog house average vectors doghouse (Landauer & Dumais, 1997). simple proposal actually works, limited degree(Mitchell & Lapata, 2008, 2010). However boat house house boat would representedaverage vector, yet different meanings. Composition averagingdeal order sensitivity phrase meaning. Landauer (2002) estimates80% meaning English text comes word choice remaining 20% comesword order.Similar issues arise representation semantic relations. Given vectorscarpenter wood, represent semantic relations carpenterwood ? treat carpenter :wood unit search paraphrases relationscarpenter wood (Turney, 2006b). large corpus, could find phrasescarpenter cut wood, carpenter used wood, wood carpenter.variation holistic approach enable us recognize semantic relationscarpenter wood highly similar relations mason stone.However, holistic approach semantic relations suffers data sparsitylinguistic creativity problems holistic approach semantic composition.could represent relation carpenter wood averaging vectors.might enable us recognize carpenter wood mason stone,would incorrectly suggest carpenter wood stone mason. problemorder sensitivity arises semantic relations arose semantic composition.Many ideas proposed composing vectors (Landauer & Dumais, 1997;Kintsch, 2001; Mitchell & Lapata, 2010). Erk Pado (2008) point two problemscommon several proposals. First, often adaptivecapacity represent variety possible syntactic relations phrase. example,phrase horse draws, horse subject verb draws, whereas objectverb phrase draws horse. composition vectors horse drawsmust able adapt variety syntactic contexts order properly modelgiven phrases. Second, single vector weak handle long phrase, sentence,document. single vector encode fixed amount structural informationdimensionality fixed, upper limit sentence length, henceamount structure encoded (Erk & Pado, 2008, p. 898). fixed dimensionalityallow information scalability.Simple (unweighted) averaging vectors lacks adaptive capacity, treatskinds composition way; flexibility represent differentmodes composition. good model must capacity adapt different situations.534fiDomain Function: Dual-Space Modelexample, weighted averaging, weights tuned different syntacticcontexts (Mitchell & Lapata, 2008, 2010).Information scalability means size semantic representations growproportion amount information representing. sizerepresentation fixed, eventually information loss. hand,size representations grow exponentially.One case problem information scalability arises approachesmap multiple vectors single vector. example, represent dog house addingvectors dog house (mapping two vectors one), may informationloss. increase number vectors mapped single vector,eventually reach point single vector longer contain informationmultiple vectors. problem avoided try map multiple vectorssingle vector.Suppose k-dimensional vector floating point elements b bits each.vector hold kb bits information. Even allow b grow, k fixed,eventually information loss. vector space model semantics, vectorsresistance noise. perturb vector noise threshold ,significant change meaning represents. Therefore thinkvector hypersphere radius , rather point. may also putbounds [r, +r] range values elements vector.1 finitenumber N hyperspheres radius packed bounded k-dimensionalspace (Conway & Sloane, 1998). According information theory, finite setN messages, need log2 (N ) bits encode message. Likewise,finite set N vectors, vector represents log2 (N ) bits information.Therefore information capacity single vector bounded k-dimensional spacelimited log2 (N ) bits.Past work suggests recognizing relations compositions closely connectedtasks (Kintsch, 2000, 2001; Mangalath, Quesada, & Kintsch, 2004). goal researchunified model handle compositions relations, also resolvingissues linguistic creativity, order sensitivity, adaptive capacity, information scalability.considerations led us dual-space model, consisting domain spacemeasuring domain similarity (i.e., topic, subject, field similarity) function spacemeasuring function similarity (i.e., role, relationship, usage similarity).analogy : b :: c : (a b c d; example, traffic street waterriverbed), b relatively high domain similarity (traffic street comedomain transportation) c relatively high domain similarity (waterriverbed come domain hydrology). hand, c relativelyhigh function similarity (traffic water similar roles respective domains;things flow) b relatively high function similarity (streetriverbed similar roles respective domains; things carrythings flow). combining domain function similarity appropriate ways,1. models vectors normalized unit length (e.g., models use cosine measuresimilarity), elements must lie within range [1, +1]. element outside range,length vector greater one. general, floating point representations minimummaximum values.535fiTurneyrecognize semantic relations traffic street analogousrelations water riverbed.semantic composition, appropriate way combine similarities may dependsyntax composition. Lets focus noun-modifier composition example.noun-modifier phrase ab (for instance, brain doctor), head noun b (doctor)modified adjective noun (brain). Suppose word c (neurologist)synonymous ab. functional role noun-modifier phrase ab determinedhead noun b (a brain doctor kind doctor) b relatively high degreefunction similarity c (doctor neurologist function doctors).b high degree domain similarity c (brain, doctor, neurologist comedomain clinical neurology). combining domain function similarity,recognize brain doctor synonymous neurologist.Briefly, proposal compose similarity measures instead composing vectors.is, apply various mathematical functions combine cosine similarity measures,instead applying functions directly vectors. addresses informationloss problem, preserve vectors individual component words. (Wemap multiple vectors single vector.) Since two different spaces, alsoflexibility address problem adaptive capacity.2 model compositional,resolves linguistic creativity problem. deal order sensitivity combiningsimilarity measures ways recognize effects word order.might argued present model semantic composition,way compare words form two phrases order derive measure similarityphrases. example, Section 4.3 derive measure similarity phrasesenvironment secretary defence minister, actually provide representationphrase environment secretary. hand, past work problemsemantic composition (reviewed Section 2.1) yields representation compositephrase environment secretary different union representationscomponent words, environment secretary.argument based assumption goal semantic compositioncreate single, general-purpose, stand-alone representation phrase, composite,distinct union representations component words. assumptionnecessary approach use assumption. believeassumption held back progress problem semantic composition.argue present model semantic composition, composition similarities, composition vectors. Vectors represent individual words,similarities inherently represent relations two (or more) things. Composing vectorsyield stand-alone representation phrase, composing similarities necessarilyyields linking structure connects phrase phrases. Similarity compositionresult stand-alone representation phrase, practical applicationsrequire stand-alone representations. Whatever practical tasks performedstand-alone representations phrases, believe performed equally well (or better)similarity composition. discuss issue depth Section 6.2. Two similarity spaces give us options similarity composition one space, two typescharacters (0 1) give us options generating strings one type character (0 alone).536fiDomain Function: Dual-Space Modelnext section surveys related work modeling semantic compositionsemantic relations. Section 3 describes build domain function space. testhypothesis value two separate spaces, also create mono space,merger domain function spaces. present four sets experiments dual-space model Section 4. evaluate dual-space approachmultiple-choice analogy questions SAT (Turney, 2006b), multiple-choice nounmodifier composition questions derived WordNet (Fellbaum, 1998), phrase similarity rating problems (Mitchell & Lapata, 2010), similarity versus association problems(Chiarello, Burgess, Richards, & Pollock, 1990). discuss experimental resultsSection 5. Section 6 considers theoretical questions dual-space model. Limitations model examined Section 7. Section 8 concludes.paper assumes familiarity vector space models semantics.overview semantic VSMs, see papers Handbook Latent Semantic Analysis(Landauer, McNamara, Dennis, & Kintsch, 2007), review Mitchell Lapatas(2010) paper, survey Turney Pantel (2010).2. Related Workexamine related work semantic composition relations. introduction, mentioned four problems semantic models, yield four desideratasemantic model:1. Linguistic creativity: model able handle phrases (in casesemantic composition) word pairs (in case semantic relations)never seen before, familiar component words.2. Order sensitivity: model sensitive order wordsphrase (for composition) word pair (for relations), order affectsmeaning.3. Adaptive capacity: phrases, model flexibility representdifferent kinds syntactic relations. word pairs, modelflexibility handle variety tasks, measuring degree relationalsimilarity two pairs (see Section 4.1) versus measuring degree phrasalsimilarity two pairs (see Section 4.3).4. Information scalability: phrases, model scale neither lossinformation exponential growth representation size number componentwords phrases increases. n-ary semantic relations (Turney, 2008a),model scale neither loss information exponential growthrepresentation size n, number terms relations, increases.review past work light four considerations.2.1 Semantic CompositionLet ab phrase, noun-modifier phrase, assume vectorsb represent component words b. One earliest proposals semanticcomposition represent ab vector c average b (Landauer &537fiTurneyDumais, 1997). using cosine measure vector similarity, taking averageset vectors (or centroid) adding vectors, c = a+b. Vector additionworks relatively well practice (Mitchell & Lapata, 2008, 2010), although lacks ordersensitivity, adaptive capacity, information scalability. Regarding order sensitivityadaptive capacity, Mitchell Lapata (2008, 2010) suggest using weights, c = a+b,tuning weights different values different syntactic relations. experiments(Mitchell & Lapata, 2010), weighted addition performed better unweighted addition.Kintsch (2001) proposes variation additive compositionc sum a,Pb, selected neighbours ni b, c = + b + ni . neighbours vectorswords given vocabulary (i.e., rows given wordcontext matrix).neighbours chosen manner attempts address order sensitivity adaptivecapacity, still problem information scalability due fixed dimensionality.Utsumi (2009) presents similar model, different way selecting neighbours.Mitchell Lapata (2010) found simple additive model peformed betteradditive model included neighbours.Mitchell Lapata (2008, 2010) suggest element-wise multiplication compositionoperation, c = fi b, ci = ai bi . Like vector addition, element-wise multiplication suffers lack order sensitivity, adaptive capacity, information scalability.Nonetheless, experimental evaluation seven compositional models two noncompositional models, element-wise multiplication best performance (Mitchell &Lapata, 2010).Another approach use tensor product composition (Smolensky, 1990; Aerts& Czachor, 2004; Clark & Pulman, 2007; Widdows, 2008), outer product,C = b. outer product two vectors (a b), n elements, n nmatrix (C). outer product three vectors n n n third-order tensor.results information scalability problem: representations grow exponentially largephrases grow longer.3 Furthermore, outer product perform wellelement-wise multiplication Mitchell Lapatas (2010) experiments. Recent worktensor products (Clark, Coecke, & Sadrzadeh, 2008; Grefenstette & Sadrzadeh, 2011)attempted address issue information scalability.Circular convolution similar outer product, outer product matrixcompressed back vector, c = ~ b (Plate, 1995; Jones & Mewhort, 2007).avoids information explosion, results information loss. Circular convolutionperformed poorly Mitchell Lapatas (2010) experiments.Baroni Zamparelli (2010) Guevara (2010) suggest another model compositionadjective-noun phrases. core strategy share use holistic vectorstrain compositional model. partial least squares regression (PLSR), learnlinear model maps vectors component nouns adjectives linearapproximations holistic vectors phrases. linguistic creativity problemavoided linear model needs holistic vectors training;need holistic vectors plausible adjective-noun phrases. Given phrasetraining data, linear model predicts holistic vector phrase, given3. ways avoid exponential growth; example, third-order tensor rank 1three modes may compactly encoded three component vectors. Kolda Bader (2009)discuss compact tensor representations.538fiDomain Function: Dual-Space Modelcomponent vectors adjective noun. works well adjective-nounphrases, clear generalize parts speech longer phrases.One application semantic composition measuring similarity phrases (Erk &Pado, 2008; Mitchell & Lapata, 2010). Kernel methods applied closelyrelated task identifying paraphrases (Moschitti & Quarteroni, 2008), emphasiskernel methods syntactic similarity, rather semantic similarity.Neural network models combined vector space models tasklanguage modeling (Bengio, Ducharme, Vincent, & Jauvin, 2003; Socher, Manning, & Ng,2010; Socher, Huang, Pennington, Ng, & Manning, 2011), impressive results. goallanguage model estimate probability phrase decide severalphrases likely. VSMs improve probability estimates language modelmeasuring similarity words phrases smoothing probabilitiesgroups similar words. However, language model, words considered similardegree exchanged without altering probability given phrase,without regard whether exchange alters meaning phrase. likefunction similarity, measures degree words similar functional roles,language models missing anything like domain similarity.Erk Pado (2008) present model similar two parts,vector space measuring similarity model selectional preferences. vectorspace similar domain space model selectional preferences plays rolesimilar function space. individual word represented triple, = ha, R, R1 i,consisting words vector, a, selectional preferences, R, inverse selectionalpreferences, R1 . phrase ab represented pair triples, hA0 , B 0 i. triple A0modified form triple represents individual word a. modificationsadjust representation model meaning altered relation bphrase ab. Likewise, triple B 0 modified form triple B represents b,B 0 takes account affects b.transformed A0 represent influence b meaning a,vector transformed new vector a0 A0 . Let rb vector representstypical words consistent selectional preferences b. vector a0composition rb . Erk Pado (2008) use element-wise multiplicationcomposition, a0 = fi rb . intention make like typical vector x wouldexpected phrase xb. Likewise, b0 B 0 , b0 = b fi raErk Pados (2008) model related models (Thater, Furstenau, & Pinkal, 2010)address linguistic creativity, order sensitivity, adaptive capacity, information scalability,suitable measuring similarity semantic relations. Consideranalogy traffic street water riverbed. Let hA0 , B 0 represent traffic :streetlet hC 0 , D0 represent water :riverbed. transformation A, B, C, A0 , B 0 ,C 0 , D0 reinforces connection traffic street waterriverbed, help us recognize relational similarity traffic :streetwater :riverbed. course, models designed relational similarity,surprising. However, goal find unified model handlecompositions relations.539fiTurney2.2 Semantic Relationssemantic relations, make general observations order sensitivity. Let: b c : two word pairs let simr (a : b, c : d) < measure degreesimilarity relations : b c : d. : b :: c : good analogy,simr (a : b, c : d) relatively high value. general, good model relationalsimilarity respect following equalities inequalities:simr (a : b, c : d) = simr (b : a, : c)(1)simr (a : b, c : d) = simr (c : d, : b)(2)simr (a : b, c : d) 6= simr (a : b, : c)(3)simr (a : b, c : d) 6= simr (a : d, c : b)(4)example, given carpenter :wood mason :stone make good analogy, followsEquation 1 wood :carpenter stone :mason make equally good analogy. Also,according Equation 2, mason :stone carpenter :wood make good analogy.hand, suggested Equation 3, carpenter :wood analogous stone :mason.Likewise, indicated Equation 4, poor analogy assert carpenter stonemason wood.Rosario Hearst (2001) present algorithm classifying word pairs accordingsemantic relations. use lexical hierarchy map word pairs featurevectors. classification scheme implicitly tell us something similarity. Two wordpairs semantic relation class implicitly relationally similartwo word pairs different classes. consider relational similarityimplied Rosario Hearsts (2001) algorithm, see problem ordersensitivity: Equation 4 violated.Let simh (x, y) < measure degree hierarchical similaritywords x y. simh (x, y) relatively high, x share common hypernymrelatively close given lexical hierarchy. essence, intuition behindRosario Hearsts (2001) algorithm is, simh (a, c) simh (b, d) high,simr (a : b, c : d) also high. is, simh (a, c) simh (b, d) high enough,: b c : assigned relation class.example, consider analogy mason stone carpenter wood. common hypernym mason carpenter artisan; see simh (mason, carpenter)high. common hypernym stone wood material; hence simh (stone, wood)high. seems good analogy indeed characterized high values simh (a, c)simh (b, d). However, symmetry simh (x, y) leads problem. simh (b, d) high,simh (d, b) must also high, implies simr (a : d, c : b) high. is,incorrectly conclude mason wood carpenter stone (see Equation 4).later work classifying semantic relations used different algorithms,underlying intuition hierarchical similarity (Rosario, Hearst, & Fillmore,2002; Nastase & Szpakowicz, 2003; Nastase, Sayyad-Shirabad, Sokolova, & Szpakowicz,2006). use similar intuition here, since similarity function space closely related540fiDomain Function: Dual-Space Modelhierarchical similarity, simh (x, y), see later (Section 4.4). However, includingdomain space relational similarity measure saves us violating Equation 4.Let simf (x, y) < function similarity measured cosine vectors xfunction space. Let simd (x, y) < domain similarity measured cosinevectors x domain space. Like past researchers (Rosario & Hearst, 2001; Rosarioet al., 2002; Nastase & Szpakowicz, 2003; Veale, 2004; Nastase et al., 2006), lookhigh values simf (a, c) simf (b, d) indicators simr (a : b, c : d) high,also look high values simd (a, b) simd (c, d). Continuing previous example,conclude mason wood carpenter stone, woodbelong domain masonry stone belong domain carpentry.Let determiner (e.g., the, a, an). Hearst (1992) showed patterns formX (a bird crow) kind X (the crow kindbird) used infer X hypernym (bird hypernym crow).pairpattern matrix VSM rows word pairs columnsvarious X . . . patterns. Turney, Littman, Bigham, Shnayder (2003) demonstratedpairpattern VSM used measure relational similarity. Supposepair-pattern matrix X word pair : b corresponds row vector xi c :corresponds xj . approach measure relational similarity simr (a : b, c : d)cosine xi xj .first patterns pairpattern matrices generated hand (Turneyet al., 2003; Turney & Littman, 2005), later work (Turney, 2006b) used automaticallygenerated patterns. authors used variations technique (Nakov & Hearst,2006, 2007; Davidov & Rappoport, 2008; Bollegala, Matsuo, & Ishizuka, 2009; Seaghdha& Copestake, 2009). models suffer linguistic creativity problem.models noncompositional (holistic), cannot scale handlehuge number possible pairs. Even largest corpus cannot contain pairshuman speaker might use daily conversation.Turney (2006b) attempted handle linguistic creativity problem within holisticmodel using synonyms. example, corpus contain traffic street withincertain window text, perhaps might contain traffic road. containwater riverbed, perhaps water channel. However, best partialsolution. Turneys (2006b) algorithm required nine days process 374 multiple choice SATanalogy questions. Using dual-space model, without specifying advance wordpairs might face, answer 374 questions seconds (see Section 4.1).Compositional models scale better holistic models.Mangalath et al. (2004) presented model semantic relations represents wordpairs vectors ten abstract relational categories, hyponymy, meronymy, taxonomy, degree. approach construct kind second-order vector spaceelements vectors degrees similarity, calculated cosinesfirst-order wordcontext matrix.instance, carpenter :wood represented second-order vector composedten cosines calculated first-order vectors. second-order vector, valueelement corresponding to, say, meronymy would cosine two first-order vectors, xy. vector x would sum first-order vectors carpenter wood.vector would sum several vectors words related meronymy,541fiTurneypart, whole, component, portion, contains, constituent, segment. cosinex would indicate degree carpenter wood related meronymy.Mangalath et al.s (2004) model suffers information scalability order sensitivityproblems. Information loss takes place first-order vectors summed alsohigh-dimensional first-order space reduced ten-dimensional second-orderspace. order sensitivity problem second-order vectors violate Equation 3,pairs c : : c represented second-order vector.natural proposal represent word pair : b way would representphrase ab. is, whatever compositional model phrases could alsoapplied word pairs. However problems compositional model ordersensitivity information scalability carry word pairs. example, represent: b c = + b c = fi b, violate Equation 3, + b = b +fi b = b fi a.3. Three Vector Spacessection, describe three vector space models. three spaces consist wordcontext matrices, rows correspond words columns correspondcontexts words occur. differences among three spaces kindscontexts. Domain space uses nouns context, function space uses verb-based patternscontext, mono space merger domain function contexts. Mono spacecreated order test hypothesis useful separate domainfunction spaces; mono space serves baseline.3.1 Constructing WordContext MatricesBuilding three spaces involves series steps. three main steps,substeps. first last steps three spaces;differences spaces result differences second step.1. Find terms contexts: input: corpus lexicon, output: terms contexts.1.1. Extract terms lexicon find frequencies corpus.1.2. Select terms given frequency candidate rows frequencymatrix.1.3. selected term, find phrases corpus contain term withingiven window size.1.4. Use tokenizer split phrases tokens.1.5. Use part-of-speech tagger tag tokens phrases.2. Build termcontext frequency matrix: input: terms contexts, output:sparse frequency matrix.2.1. Convert tagged phrases contextual patterns (candidate columns).2.2. contextual pattern, count number terms (candidate rows)generated pattern rank patterns descending order counts.2.3. Select top nc contextual patterns columns matrix.542fiDomain Function: Dual-Space Model2.4. initial set rows (from Step 1.2), drop row matchtop nc contextual patterns, yielding final set nr rows.2.5. row (term) column (contextual pattern), count numberphrases (from Step 1.5) containing given term matching givenpattern, output resulting numbers sparse frequency matrix.3. Weight elements smooth matrix: input: sparse frequency matrix,output: singular value decomposition (SVD) weighted matrix.3.1. Convert raw frequencies positive pointwise mutual information (PPMI)values.3.2. Apply SVD PPMI matrix output SVD component matrices.input corpus Step 1 collection web pages gathered university websiteswebcrawler.4 corpus contains approximately 51010 words, comes280 gigabytes plain text. facilitate finding term frequencies sample phrases,indexed corpus Wumpus search engine (Buttcher & Clarke, 2005).5 rowsmatrices selected terms (words phrases) WordNet lexicon.6found selecting terms WordNet resulted subjectively higher qualitysimply selecting terms high corpus frequencies.Step 1.1, extract unique words phrases (n-grams) index.sense fileWordNet 3.0, skipping n-grams contain numbers (only letters, hyphens, spacesallowed n-grams). find n-gram corpus frequencies querying Wumpusn-gram. n-grams frequency least 100 least 2 characterscandidate rows Step 1.2. selected n-gram, query Wumpus findmaximum 10,000 phrases Step 1.3.7 phrases limited window 7 wordsleft n-gram 7 words right, total window size 14 + n words.use OpenNLP 1.3.0 tokenize part-of-speech tag phrases (Steps 1.4 1.5).8tagged phrases come 46 gigabytes.9Step 2.1, generate contextual patterns part-of-speech tagged phrases.Different kinds patterns created three different kinds spaces. detailsstep given following subsections. phrase may yield several patterns.three spaces 100,000 rows, maximum 10,000 phrasesper row several patterns per phrase. result millions distinct patterns,filter patterns Steps 2.2 2.3. select top nc patterns sharedlargest number rows. Given large number patterns, may fitRAM. work limited RAM, use Linux sort command, designedefficiently sort files large fit RAM. row, make filedistinct patterns generated row. concatenate files4.5.6.7.corpus collected Charles Clarke University Waterloo.Wumpus available http://www.wumpus-search.org/.WordNet available http://wordnet.princeton.edu/.limit 10,000 phrases per n-gram required make Wumpus run tolerable amount time.Finding phrases time-consuming step construction spaces. use solid-statedrive (SSD) speed step.8. OpenNLP available http://incubator.apache.org/opennlp/.9. tagged phrases available author request.543fiTurneyrows alphabetically sort patterns concatenated file. sorted file,identical patterns adjacent, makes easy count number occurrencespattern. counting, second sort operation yields ranked list patterns,select top nc .possible candidate rows Step 1.2 might matchpatterns Step 2.3. rows would zeros matrix, removeStep 2.4. Finally, output sparse frequency matrix F nr rows nccolumns. i-th row corresponds n-gram wi j-th column correspondscontextual pattern cj , value element fij F number phrasescontaining wi (from Step 1.5) generate pattern cj (in Step 2.1). Step 3.2, useSVDLIBC 1.34 calculate singular value decomposition, format outputsparse matrix Step 2.5 chosen meet requirements SVDLIBC.10Step 3.1, apply positive pointwise mutual information (PPMI) sparse frequency matrix F. variation pointwise mutual information (PMI) (Church &Hanks, 1989; Turney, 2001) PMI values less zero replacedzero (Niwa & Nitta, 1994; Bullinaria & Levy, 2007). Let X matrix resultsPPMI applied F. new matrix X number rows columnsraw frequency matrix F. value element xij X defined follows:fijpij = Pnr Pncj=1 fiji=1(5)Pncj=1 fijpi = Pnr Pnc(6)PnrfPncij= Pnr i=1(7)i=1pji=1j=1 fijj=1 fijpijpi pjpmiij = logpmiij pmiij > 0xij =0 otherwise(8)(9)definition, pij estimated probability word wi occurs contextcj , pi estimated probability word wi , pj estimated probabilitycontext cj . wi cj statistically independent, pij = pi pj (by definitionindependence), thus pmiij zero (since log(1) = 0). product pi pjwould expect pij wi occurs cj pure random chance. hand,interesting semantic relation wi cj , expect pij largerwould wi cj indepedent; hence find pij > pi pj ,thus pmiij positive. word wi unrelated (or incompatible with) context cj ,may find pmiij negative. PPMI designed give high value xijinteresting semantic relation wi cj ; otherwise, xij valuezero, indicating occurrence wi cj uninformative.10. SVDLIBC available http://tedlab.mit.edu/dr/svdlibc/.544fiDomain Function: Dual-Space ModelFinally, Step 3.2, apply SVDLIBC X. SVD decomposes X productthree matrices UVT , U V column orthonormal form (i.e., columnsorthogonal unit length, UT U = VT V = I) diagonal matrixsingular values (Golub & Van Loan, 1996). X rank r, also rank r. Letk , k < r, diagonal matrix formed top k singular values, let UkVk matrices produced selecting corresponding columns U V.matrix Uk k VkT matrix rank k best approximates original matrix X,sense minimizes approximation errors. is, X = Uk k VkT minimizeskX XkF matrices X rank k, k . . . kF denotes Frobenius norm (Golub& Van Loan, 1996). final output three matrices, Uk , k , Vk , formtruncated SVD, X = Uk k VkT .3.2 Domain Spaceintuition behind domain space domain topic word characterizednouns occur near it. use relatively wide window ignore syntacticcontext nouns appear.domain space, Step 2.1, tagged phrase generates two contextualpatterns. contextual patterns simply first noun left given n-gram(if one) first noun right (if one). Since window size 7words side n-gram, usually nouns sides n-gram.nouns may either common nouns proper nouns. OpenNLP uses Penn Treebanktags (Santorini, 1990), include several different categories noun tags.noun tags begin capital N, simply extract first words left rightn-gram tags begin N. extracted nouns converted lowercase. noun appears sides n-gram, one contextual patterngenerated. extracted patterns always unigrams; noun compound,component noun closest n-gram extracted.Table 1 shows examples n-gram boat. Note window 7 wordscount punctuation, number tokens window may greaternumber words window. see Table 1 row vectorn-gram boat frequency matrix F nonzero values (for example)columns lake summer (assuming contextual patterns makefiltering Step 2.3).Step 2.3, set nc 50,000. Step 2.4, drop rows zero,left nr equal 114,297. PPMI (which sets negative elements zero)149,673,340 nonzero values, matrix density 2.62%. Table 2 showscontextual patterns first five columns last five columns (the columnsorder ranks Step 2.2). Count column table gives number rows(n-grams) generate pattern (that is, counts mentioned Step 2.2).last patterns begin c counts ties brokenalphabetical order.545fiTurneyTagged phraseswould/MD visit/VB Big/NNP Lake/NNP and/CC take/VB our/PRP$boat/NN on/IN this/DT huge/JJ beautiful/JJ lake/NN ./. There/EXwas/VBDPatternslake2the/DT large/JJ paved/JJ parking/NN lot/NN in/IN the/DT boat/NNramp/NN area/NN and/CC walk/VB south/RB along/IN the/DTlotramp3building/VBG permit/NN ./. / Anyway/RB ,/, we/PRP should/MDhave/VB a/DT boat/NN next/JJ summer/NN with/IN skiing/NNand/CC tubing/NN paraphernalia/NNS ./.permitsummer1Table 1: Examples Step 2.1 domain space n-gram boat. three taggedphrases generate five contextual patterns.Column12345PatterntimepartyearswaynameCount91,48384,44584,41784,17281,960Column49,99649,99749.99849,99950,000Patterncluco-conspiratorconcisenesscondyleconocerCount443443443443443Table 2: Contextual patterns first last columns domain space. CLUabbreviation Chartered Life Underwriter terms, condyle roundbump bone forms joint another bone, conocerSpanish verb know, sense acquainted person.3.3 Function Spaceconcept function space function role word characterizedsyntactic context relates verbs occur near it. use narrowwindow function space domain space, based intuition proximityverb important determining functional role given word. distant verbless likely characterize function word. generate relatively complex patternsfunction space, try capture syntactic patterns connect given wordnearby verbs.Step 2.1, tagged phrase generates six contextual patterns. giventagged phrase, first step cut window 3 tokens given n-gram3 tokens it. remaining tokens left n-gram punctuation, punctuation everything left punctuation removed.remaining tokens right n-gram punctuation, punctuation everything right punctuation removed. Lets call remaining tagged phrasetruncated tagged phrase.Next replace given n-gram truncated tagged phrase generic marker,546fiDomain Function: Dual-Space ModelX. simplify part-of-speech tags reducing first character(Santorini, 1990). example, various verb tags (VB, VBD, VBG, VBN, VBP,VBZ) reduced V. truncated tagged phrase contains V tag, generateszero contextual patterns. phrase contains V tag, generate two typescontextual patterns, general patterns specific patterns.general patterns, verbs (every token V tag) tags removed(naked verbs) tokens reduced naked tags (tags without words).specific patterns, verbs, modals (tokens tags), prepositions (tokens tags),(tokens tags) tags removed tokens reduced nakedtags. (See Table 3 examples.)general specific patterns, left X, trim leading naked tags.right X, trim trailing naked tags. tag to, replaceremaining naked tags to. sequence N tags (N N N N N) likelycompound noun, reduce sequence single N.given truncated tagged phrase, two patterns, one general patternone specific pattern. either patterns tokens left rightsides X, make two patterns duplicating X splitting patternpoint two Xs. one new patterns verb, dropit. Thus may three specific patterns three general patternsgiven truncated tagged phrase. specific general patterns same, onegenerated.Table 3 shows examples n-gram boat. Note every pattern must containgeneric marker, X, least one verb.Truncated tagged phrasesthe/DT canals/NNS by/IN boat/NNand/CC wandering/VBG the/DTPatternsX C wanderingX C wanderingTypesgeneralspecific2a/DT charter/NN fishing/VBG boat/NNcaptain/NN named/VBN Jim/NNPfishing X N namedfishing XX N namedgeneralgeneralgeneral3used/VBN from/IN a/DTand/CC lowered/VBD to/TOused X C loweredused XX C loweredused X C loweredused XX C loweredgeneralgeneralgeneralspecificspecificspecific1boat/NNTable 3: Examples Step 2.1 function space n-gram boat. three truncatedtagged phrases generate eleven contextual patterns.Step 2.3, set nc 50,000. Step 2.4, rows zero dropped,nr 114,101. PPMI, 68,876,310 nonzero values, yielding matrix density1.21%. Table 4 shows contextual patterns first last five columns.547fiTurneylast patterns begin counts ties brokenalphabetical order.Column12345PatternXX NXXXCount94,31282,17179,13172,63772,497Column49,99649,99749,99849,99950,000Patternsince X Nsinking Xsupplied Xsupports X Nsuppressed XCount381381381381381Table 4: Contextual patterns first last columns function space.contextual patterns function space complex patternsdomain space. motivation greater complexity observation mereproximity enough determine functional roles, although seems sufficient determining domains. example, consider verb gives. word X occurs neargives, X could subject, direct object, indirect object verb. determinefunctional role X, need know case applies. syntactic context connects X gives provides information. contextual pattern X gives impliesX subject, gives X implies X object, likely direct object, gives Xsuggests X indirect object. Modals prepositions supply informationfunctional role X context given verb. verb gives appears 43different contextual patterns (i.e., 43 50,000 columns function space correspondsyntactic patterns contain gives).Many row vectors function space matrix correspond verbs. mightseem surprising characterize function verb syntactic relationverbs, consider example, verb run. row vector runPPMI matrix function space 1,296 nonzero values; is, run characterized1,296 different contextual patterns.Note appearing contextual pattern different nonzero valuecontextual pattern. character string word run appears 62 differentcontextual patterns, run X. row vector word run nonzerovalues 1,296 contextual patterns (columns), X.3.4 Mono SpaceMono space simply merger domain space function space. Step 2.3,take union 50,000 domain space columns 50,000 function space columns,resulting total nc 100,000 columns. Step 2.4, total nr 114,297rows. mono matrix PPMI 218,222,254 nonzero values, yielding density1.91%. values mono frequency matrix F equal corresponding valuesdomain function matrices. rows mono space matrixcorresponding rows function space matrix. rows, corresponding valueszeros (but nonzero elements rows, correspond valuesdomain matrix).548fiDomain Function: Dual-Space Model3.5 Summary SpacesTable 5 summarizes three matrices. following four sets experiments, usethree matrices (the domain, function, mono matrices) cases;generate different matrices set experiments. Three four sets experimentsinvolve datasets used past researchers. made specialeffort ensure words three datasets corresponding rows threematrices. intention three matrices adequate handleapplications without special customization.SpacedomainfunctionmonoRows (nr )114,297114,101114,297Columns (nc )50,00050,000100,000Nonzeros (after PPMI)149,673,34068,876,310218,222,254Density (after PPMI)2.62%1.21%1.91%Table 5: Summary three spaces.3.6 Using Spaces Measure Similarityfollowing experiments, measure similarity two terms, b, cosineangle corresponding row vectors, b:sim(a, b) = cos(a, b) =bkak kbk(10)cosine angle two vectors inner product vectors,normalized unit length. cosine ranges 1 vectors pointopposite directions ( 180 degrees) +1 point direction ( 0degrees). vectors orthogonal ( 90 degrees), cosine zero. rawfrequency vectors, necessarily cannot negative elements, cosine cannotnegative, weighting smoothing often introduce negative elements. PPMI weightingyield negative elements, truncated SVD generate negative elements, eveninput matrix negative values.semantic similarity two terms given cosine two corresponding rowsUk pk (see Section 3.1). two parameters Uk pk need set.parameter k controls number latent factors parameter p adjusts weightsfactors, raising corresponding singular values pk power p.parameter k well-known literature (Landauer et al., 2007), p less familiar.use p suggested Caron (2001). following experiments (Section 4),explore range values p k.Suppose take word w list words descending ordercosines w, using Uk pk calculate cosines. p high, go list,cosines nearest neighbours w decrease slowly. p low, decreasequickly. is, high p results broad, fuzzy neighbourhood low p yields sharp,crisp neighbourhood. parameter p controls sharpness similarity measure.549fiTurneyreduce running time SVDLIBC, limit number singular values1500, usually results less 1500 singular values. example, SVDdomain space 1477 singular values. long k greater 1477,experiment range k values without rerunning SVDLIBC. generate Uk pkU1477 p1477 simply deleting 1477 k columns smallest singular values.experiments, vary k 100 1400 increments 100 (14 values k)vary p 1 +1 increments 0.1 (21 values p). p 1,give weight factors smaller singular values; p +1, factorslarger singular values weight. Caron (2001) observes researchers useeither p = 0 p = 1; is, use either Uk Uk k .Let simf (a, b) < function similarity measured cosine vectors bfunction space. Let simd (a, b) < domain similarity measured cosinevectors b domain space. similarity measure combines simd (a, b)simf (a, b), four parameters tune, kd pd domain space kf pffunction space.one space, feasible us explore 14 21 = 294 combinations parametervalues, two spaces 294 294 = 86, 436 combinations values. make searchtractable, initialize parameters middle ranges (kf = kd = 700pf = pd = 0) alternate tuning simd (a, b) (i.e., kd pd ) holdingsimf (a, b) (i.e., kf pf ) fixed tuning simf (a, b) holding simd (a, b) fixed. stopsearch improvement performance training data. almostcases, local optimum found one pass; is, tuned parametersonce, improvement try tune second time. Thus typicallyevaluate 294 3 = 882 parameter values (3 tune one similarity, tune other,try first see improvement possible).11could use standard numerical optimization algorithm tune four parameters, algorithm use takes advantage background knowledgeoptimization task. know small variations parameters make small changesperformance, need make fine-grained search, knowsimd (a, b) simf (a, b) relatively independent, optimize separately.rows matrices based terms WordNet index.sense file.file, nouns singular forms verbs stem forms. calculatesim(a, b), first look exact matches b terms correspondrows given matrix (domain, function, mono). exact match found,use corresponding row vector matrix. Otherwise, look alternate formsterms, using validForms function WordNet::QueryData Perl interfaceWordNet.12 automatically converts plural nouns singular forms verbsstem forms. none alternate forms exact match row matrix,map term zero vector length k.11. use Perl Data Language (PDL) searching parameters, calculating cosines, operationsvectors matrices. See http://pdl.perl.org/.12. WordNet::QueryData available http://search.cpan.org/dist/WordNet-QueryData/.550fiDomain Function: Dual-Space Model3.7 Composing Similaritiesapproach semantic relations compositions combine two similarities,simd (a, b) simf (a, b), various ways, depending task hand syntaxphrase hand. general, want combined similarity highcomponent similarities high, want values component similaritiesbalanced. achieve balance, use geometric mean combine similarities, insteadarithmetic mean. geometric mean suitable negative numbers,cosine negative cases; hence define geometric mean zerocomponent similarities negative:geo(x1 , x2 , . . . , xn ) =(x1 x2 . . . xn )1/n xi > 0 = 1, . . . , n0 otherwise(11)3.8 Element-wise MultiplicationOne successful approaches composition, far, element-wise multiplication, c = fi b, ci = ai bi (Mitchell & Lapata, 2008, 2010). approachmakes sense elements vectors negative. elementsb positive, relatively large values ai bi reinforce other, resultinglarge value ci . makes intuitive sense. ai bi highly negative,ci highly positive, although intuition says ci highly negative. MitchellLapata (2008, 2010) designed wordcontext matrices ensure vectorsnegative elements.values matrix Uk pk typically half positive half negative.use element-wise multiplication baseline following experiments.fair baseline, cannot simply apply element-wise multiplication row vectors Uk pk .One solution would use PPMI matrix, X, negative elements,would allow element-wise multiplication take advantage smoothing effectSVD. solution use row vectors X = Uk k VkT . Although PPMI matrix,X, sparse (see Table 5), X Uk pk density 100%.Let a0 b0 vectors X correspond terms b. rowvectors benefit smoothing due truncated SVD, elements almostpositive. negative elements, set zero. Let c0 = a0 fi b0 .apply element-wise multiplication vectors, multiply Vk kp1 ,resulting vector c = c0 Vk p1compared row vectors matrixkpUk k :p1X(Vk p1k ) = (Uk k Vk )(Vk k )===Uk k VkT Vk kp1Uk k p1kUk pk(12)(13)(14)(15)Note that, since Vk column orthonormal, VkT Vk equals Ik , k k identity matrix.551fiTurneySimilarly, row vector Uk pk , find counterpart a0 X multiplying1pk Vk :(Uk pk )(k1p VkT ) = Uk pk 1pk Vk(16)= Uk k VkT(17)= X(18)Let nn(x) (nn nonnegative) function converts negative elements vectorx zero:nn(hx1 , . . . , xn i) = hy1 , . . . , ynxi xi > 0yi =0 otherwise(19)(20)version element-wise multiplication may expressed follows:p11pc = (nn(a1pk Vk ) fi nn(bk Vk )) Vk k(21)Another way deal element-wise multiplication would use nonnegativematrix factorization (NMF) (Lee & Seung, 1999) instead SVD. yet foundimplementation NMF scales matrix sizes (Table 5).past experiments smaller matrices, SVD NMF similar performance.4. Experiments Varieties Similaritiessection presents four sets experiments. first set experiments presents dualspace model semantic relations evaluates model multiple choice analogyquestions SAT. second set presents model semantic compositionevaluates multiple choice questions constructed WordNet. thirdset applies dual-space model phrase similarity dataset Mitchell Lapata(2010). final set uses three classes word pairs Chiarello et al. (1990) testhypothesis dual-space model, domain space function space captureintuitive concepts association similarity.4.1 Similarity Relationsevaluate dual-space model applied task measuring similaritysemantic relations. use set 374 multiple-choice analogy questions SATcollege entrance exam (Turney, 2006b). Table 6 gives example one questions.task select choice word pair analogous (most relationally similar)stem word pair.Let : b represent stem pair (e.g., lull :trust). answer SAT questionsselecting choice pair c : maximizes relational similarity, simr (a : b, c : d), definedfollows:552fiDomain Function: Dual-Space ModelStem:Choices:Solution:(1)(2)(3)(4)(5)(3)lull:trustbalk:fortitudebetray:loyaltycajole:compliancehinder:destinationsoothe:passioncajole:complianceTable 6: example question 374 SAT analogy questions. Lulling persontrust analogous cajoling person compliance.sim1 (a : b, c : d) = geo(simf (a, c), simf (b, d))(22)sim2 (a : b, c : d) = geo(simd (a, b), simd (c, d))(23)sim3 (a : b, c : d) = geo(simd (a, d), simd (c, b))sim1 (a : b, c : d) sim2 (a : b, c : d) sim3 (a : b, c : d)simr (a : b, c : d) =0 otherwise(24)(25)intent sim1 measure function similarity across two pairs. domainsimilarity inside two pairs measured sim2 , whereas domain similarity acrosstwo pairs given sim3 . relational similarity, simr , simply function similarity,sim1 , subject constraint domain similarity inside pairs, sim2 , mustless domain similarity across pairs, sim3 .Figure 1 conveys main ideas behind Equations 22 25. want high functionsimilarities (indicated F) : c b : d, measured sim1 . also preferrelatively high domain similarities (marked D) : b c : (measured sim2 ),contrast relatively low domain similarities ( D) : c : b (as given sim3 ).13Using example Table 6, see lulling person trust analogouscajoling person compliance, since functional role lull similar functionalrole cajole (both involve manipulating person) functional role trust similarfunctional role compliance (both states person in).captured sim1 . constraint sim2 (a : b, c : d) sim3 (a : b, c : d) impliesdomain similarities lull :trust (the domain confidence loyalty) cajole :compliance(the domain obedience conformity) greater equal domainsimilarities lull :compliance cajole :trust.Analogy way mapping knowledge source domain target domain(Gentner, 1983). source domain mapped c target domain,play role source domain c plays target domain.theory behind sim1 . b source domain c target13. recently came across rectangular structure Lepage Shin-ichis (1996) papermorphological analogy (see Figure 1). Although algorithm task differ considerablyalgorithm task Lepage Shin-ichi (1996), independently discoveredunderlying structure analogical reasoning.553fiTurneybFcFsimr (a : b, c : d)relational similarityFigure 1: diagram reasoning behind Equations 22 25. F represents highfunction similarity, means high domain similarity, indicates lowdomain similarity.domain, internal domain similarity b internal domain similarityc less cross-domain similarities. motivates constraintsim2 sim3 . definition natural expression Gentners (1983) theory analogy.Recall four equations introduced Section 2.2. repeat equationsconvenience:simr (a : b, c : d) = simr (b : a, : c)(26)simr (a : b, c : d) = simr (c : d, : b)(27)simr (a : b, c : d) 6= simr (a : b, : c)(28)simr (a : b, c : d) 6= simr (a : d, c : b)(29)Inspection show definition relational similarity Equation 25 satisfiesrequirements Equations 26, 27, 28, 29. understood consideringFigure 1. Equation 26 tells us rotate Figure 1 vertical axis withoutaltering network similarities, due symmetry figure. Equation 27 tellsus rotate Figure 1 horizontal axis without altering networksimilarities.hand, cannot swap c holding b fixed,would change F links (although would change links).words, sim1 sim3 would changed, although sim2 would affected.Therefore Equation 28 satisfied.Also, cannot swap b holding c fixed, would changelinks (although would change F links). words, sim2sim3 would changed, although sim1 would affected. Therefore Equation 29554fiDomain Function: Dual-Space Modelsatisfied. see sim1 would violate Equation 29, due symmetrycosines, simf (b, d) = simf (d, b). constraint sim2 (a : b, c : d) sim3 (a : b, c : d) breakssymmetry.Another way break symmetry, Equation 29 satisfied, would usesimilarity measure inherently asymmetric, skew divergence. Equation 25,symmetry broken natural way considering domain function similarityapply analogies, need introduce inherently asymmetric measure. Also,note symmetries Equations 26 27 desirable; wish breaksymmetries.would reasonable include simd (a, c) simd (b, d) sim3 , decidedleave out. seems us function similarities simf (a, c) simf (b, d),high values good analogy, might cause simd (a, c) simd (b, d)relatively high, even though cross domains. people observe certain kindabstract function similarity frequently, function similarity might become populartopic discussion, could result high domain similarity.example, carpenter :wood analogous mason :stone. domain carpenter :woodcarpentry domain mason :stone masonry. functional role carpentersimilar functional role mason, artisans. Although carpentermason belong different domains, high degree abstract function similaritymay result discussions mention together, discussions specialized trades, skilled manual labour, construction industry, workplace injuries.words, high function similarity two words may cause rise domainsimilarity. Therefore include simd (a, c) simd (b, d) sim3 .five choices SAT question relational similarity zero, skipquestion. use ten-fold cross-validation set parameters SAT questions.parameter values selected nine ten folds, kd = 800, pd = 0.1, kf = 300,pf = 0.5. parameters determined, 374 SAT questions answeredseconds. Equation 25 correctly answers 191 questions, skips 2 questions,incorrectly answers 181 questions, achieving accuracy 51.1%.4.1.1 Comparison Past Workcomparison, average score senior highschool students applying US universities57.0%. ACL Wiki lists many past results 374 SAT questions.14 Table 7shows top ten results time writing. table, dual-space refers dualspace model using Equation 25. Four past results achieved accuracy 51.1%higher. four used holistic approaches hence able address issuelinguistic creativity. best previous algorithm attains accuracy 56.1% (210 correct,4 skipped, 160 incorrect) (Turney, 2006b). difference 51.1% 56.1%statistically significant 95% confidence level, according Fishers Exact Test.majority algorithms Table 7 unsupervised, Dual-Space, PairClass(Turney, 2008b), BagPack (Herdagdelen & Baroni, 2009) use limited supervision. PairClass BagPack answer given SAT question learning binary classification modelspecific given question. training set given question consists one14. See http://aclweb.org/aclwiki/index.php?title=SAT Analogy Questions.555fiTurneyAlgorithmLSA+PredicationKNOW-BESTk-meansBagPackVSMDual-SpaceBMIPairClassPERTLRAHumanReferenceMangalath et al. (2004)Veale (2004)Bicici Yuret (2006)Herdagdelen Baroni (2009)Turney Littman (2005)Bollegala et al. (2009)Turney (2008b)Turney (2006a)Turney (2006b)Average US college applicantAccuracy42.043.044.044.147.151.151.152.153.556.157.095% confidence37.247.438.048.239.049.339.049.342.252.546.156.546.156.546.957.348.558.951.061.252.062.3Table 7: top ten results 374 SAT questions, ACL Wiki. 95%confidence intervals calculated using Binomial Exact Test.positive training example, stem pair question, ten randomly selected pairs(assumed) negative training examples. induced binary classifier used assignprobabilities five choices probable choice guess. Dual-Space usestraining set tune four numerical parameters. three algorithms bestdescribed weakly supervised.4.1.2 Sensitivity Parameterssee sensitive dual-space model values parameters, performtwo exhaustive grid searches, one coarse, wide grid another fine, narrowgrid. point grids, evaluate dual-space model using whole set374 SAT questions. narrow grid search centred parameter valuesselected nine ten folds previous experiment, kd = 800, pd = 0.1,kf = 300, pf = 0.5. searches evaluate 5 values parameter, yielding total54 = 625 parameter settings. Table 8 shows values explored two gridsearches Table 9 presents minimum, maximum, average, standard deviationaccuracy two searches.GridCoarseFineParameterkdpdkfpfkdpdkfpf100-1.0100-1.0600-0.31000.3425-0.5425-0.5700-0.22000.4Values750 10750.00.5750 10750.00.5800900-0.10.03004000.50.614001.014001.010000.15000.7Table 8: range parameter values two grid searches.556fiDomain Function: Dual-Space ModelGridCoarseFineMinimum31.042.5AccuracyMaximum Average48.740.751.647.3Standard deviation4.12.0Table 9: sensitivity dual-space model parameter settings.accuracy attained heuristic search (described Section 3.6) ten-foldcross-validation, 51.1% (Table 7), near best accuracy fine grid search usingwhole set 374 SAT questions, 51.6% (Table 9). evidence heuristic searcheffective. Accuracy coarse search varies 31.0% 48.7%, demonstratesimportance tuning parameters. hand, accuracy fine searchspans narrower range lower standard deviation, suggests dualspace model overly sensitive relatively small variations parameter values;is, parameters reasonably stable. (That nine ten folds cross-validationselect parameters evidence stability.)4.1.3 Parts SpeechSince domain space based nouns function space based verbs, interestingknow performance dual-space model varies different parts speech.answer this, manually labeled 374 SAT questions part-of-speech labels.labels single pair ambiguous, labels become unambiguous contextwhole question. example, lull :trust could noun :verb, contextTable 6, must verb :noun.Table 10 splits results various parts speech. None differencestable statistically significant 95% confidence level, according FishersExact Test. larger varied set questions needed determine partspeech affects dual-space model.Parts speechnoun:nounnoun:adjective adjective:nounnoun:verb verb:nounadjective:adjectiveverb:adjective adjective:verbverb:verbverb:adverb adverb:verbRight973527912110191Accuracy50.853.049.137.560.064.70.051.1Wrong93312815761181Skipped10001002Total19166552420171374Table 10: Performance dual-space model various parts speech.4.1.4 Order Sensitivityseems function space work Equation 25. use sim1 alone,dropping constraint sim2 sim3 , accuracy drops 51.1% 50.8%.557fiTurneydrop statistically significant. hypothesize small drop due designSAT test, primarily intended test students understanding functionalroles, domains.verify hypothesis, reformulated SAT questions would testfunction domain comprehension. method first expand choice pairc : including stem pair : b, resulting full explicit analogy : b :: c : d.expanded choice, : b :: c : d, generate another choice, : :: c : b. Table 11 showsreformulation Table 6. Due symmetry, sim1 must assign similarity: b :: c : : :: c : b. new ten-choice test evaluates function domainsimilarities.Choices:Solution:(1)(2)(3)(4)(5)(6)(7)(8)(9)(10)(6)lull:trust::balk:fortitudelull:fortitude::balk:trustlull:loyalty::betray:trustlull:trust::betray:loyaltylull:compliance::cajole:trustlull:trust::cajole:compliancelull:destination::hinder:trustlull:trust::hinder:destinationlull:trust::soothe:passionlull:passion::soothe:trustlull:trust::cajole:complianceTable 11: expanded SAT question, designed test function domain comprehension. Choices (5) (6) similarity according sim1 .task expanded ten-choice SAT questions originalfive-choice questions, select best analogy. solution Table 11solution Table 6, except stem pair explicit Table 11. signficantchange five new distractors added choices. answer ten-choicequestions selecting choice : b :: c : maximizes simr (a : b, c : d).ten-choice reformulated SAT test, simr (Equation 25) attains accuracy47.9%, whereas sim1 alone (Equation 22) achieves 27.5%. difference statisticallysignificant 95% confidence level, according Fishers Exact Test. stringenttest supports claim function similarity insufficient itself.test value two separate spaces, use single spacesimd simf Equation 25. model still four parameters tune, kd , pd , kf ,pf , matrix used similarities. best result accuracy40.4% ten-question reformulated SAT test, using function space simdsimf . significantly 47.9% accuracy dual-space model simdbased domain space simf based function space (95% confidence level, FishersExact Test).Table 12 summarizes results. cases matrix simd used,model based sim1 alone (Equation 22). cases, model basedsimr (Equation 25). five-choice ten-choice SAT questions, original558fiDomain Function: Dual-Space Modeldual-space model accurate modified models. Significant columnindicates whether accuracy modified model significantly less originaldual-space model (95% confidence level, Fishers Exact Test). difficult ten-choicequestions clearly show value two distinct spaces.Algorithmdual-spacemodified dual-spacemodified dual-spacemodified dual-spacemodified dual-spacemodified dual-spacemodified dual-spacedual-spacemodified dual-spacemodified dual-spacemodified dual-spacemodified dual-spacemodified dual-spacemodified dual-spaceAccuracy51.147.343.637.750.841.735.847.940.438.234.827.525.114.4SignificantyesyesyesyesyesyesyesyesyesyesQuestionsfive-choicefive-choicefive-choicefive-choicefive-choicefive-choicefive-choiceten-choiceten-choiceten-choiceten-choiceten-choiceten-choiceten-choiceMatrix simddomain spacefunction spacemono spacedomain spaceuseduseduseddomain spacefunction spacemono spacedomain spaceusedusedusedMatrix simffunction spacefunction spacemono spacedomain spacefunction spacemono spacedomain spacefunction spacefunction spacemono spacedomain spacefunction spacemono spacedomain spaceTable 12: Accuracy original five-choice questions reformulated ten-choicequestions. modified models, intentionally use wrong matrix (ormatrix) simd simf . modified models show accuracy decreasesone space used.4.1.5 Summarydual-space model performs well current state-of-the-art holistic modeladdresses issue linguistic creativity. results reformulated SAT questionssupport claim value two separate spaces.mentioned Section 2.2, task classifying word pairs accordingsemantic relations (Rosario & Hearst, 2001; Rosario et al., 2002; Nastase & Szpakowicz,2003) closely connected problem measuring relational similarity. Turney (2006b)applied measure relational similarity relation classification using cosine similaritymeasure nearness nearest neighbour supervised learning algorithm. dualspace model (Equation 25) also suitable relation classification nearest neighbouralgorithm.4.2 Similarity Compositionssecond set experiments, apply dual-space model noun-modifier compositions. Given vectors dog, house, kennel, would like able recognizedog house kennel synonymous. compare dual-space model holisticapproach, vector addition, element-wise multiplication. approaches evaluated559fiTurneyusing multiple-choice questions automatically generated WordNet, usingWordNet::QueryData Perl interface WordNet. Table 13 gives example onenoun-modifier questions.Stem:Choices:Solution:(1)(2)(3)(4)(5)(6)(7)(1)dog housekenneldoghousecaninedwellingeffectlargenesskennelTable 13: example multiple-choice noun-modifier composition question.questions, stem bigram choices unigrams. Choice (1)correct answer, (2) modifier, (3) head noun. Choice (4) synonymhypernym modifier (5) synonym hypernym head noun.synonyms hypernyms found, noun randomly chosen. last two choices, (6)(7), randomly selected nouns. Choices (2) (4) either nouns adjectives,choices must nouns.stem bigram choice unigrams must corresponding rows functionspace (the space least number rows). stem bigram must noun senseWordNet (it may also senses parts speech). solution unigram, (1),must member synset (synonym set) first noun sense stem bigram(the frequent dominant sense bigram, bigram used noun),cannot simply hyphenation (dog-house) concatenation (doghouse)stem bigram.requirements result total 2180 seven-choice questions, randomlysplit 680 training (parameter tuning) 1500 testing.15 questions deliberately designed difficult. particular, approaches strongly attractedchoices (2) (3). Furthermore, attempt ensure stem bigramscompositional; may idiomatic expressions compositional approachcould possibly get right. want bias questions imposing theoriesdistinguishing compositions idioms construction.Let ab represent noun-modifier bigram (dog house) let c represent unigram(kennel). answer multiple-choice questions selecting unigram maximizescompositional similarity, simc (ab, c), defined follows:sim1 (ab, c) = geo(simd (a, c), simd (b, c), simf (b, c))sim1 (ab, c) 6= c b 6= csimc (ab, c) =0 otherwise15. questions available online appendix http://jair.org/.560(30)(31)fiDomain Function: Dual-Space ModelEquations 30 31 illustrated Figure 2.6=bF6=csimc (ab, c)noun-modifier compositional similarityFigure 2: diagram Equations 30 31.thinking behind sim1 c (kennel ) high domain similaritymodifier (dog) head noun b (house); furthermore, functionbigram ab (dog house) determined head noun b (house), head nounhigh function similarity c (kennel ). add constraints 6= c b 6= csim1 tends high values sim1 (ab, a) sim1 (ab, b).16 seemsplausible humans use constraints like this: reason dog house cannot meanthing house, extra word dog dog house would serve purpose;would meaningless noise.17constraints 6= c b 6= c could expressed terms similarities,simd (a, c) < simd (b, c) < t, high threshold (e.g., = 0.9), wouldadd another parameter model. decided keep model relatively simple.seven choices noun-modifier question compositional similarityzero, skip question. training set, best parameter settings kd = 800,pd = 0.3, kf = 100, pf = 0.6. testing set, Equation 31 correctly answers 874questions, skips 22 questions, incorrectly answers 604, yielding accuracy 58.3%.4.2.1 Comparison ApproachesMitchell Lapata (2010) compared many different approaches semantic compositionexperiments, considered one task (the task examine Section 4.3).paper, chosen compare smaller number approaches largernumber tasks. include element-wise multiplication experiments,approach best performance Mitchell Lapatas (2010) experiments. Vector16. spite constraints, still worthwhile include head noun modifier distractorsmultiple-choice questions, enables us experimentally evaluate impactdistractors various algorithms constraints removed (see Table 15). Also, future usersdataset may find way avoid distractors without explicit constraints.17. philosophy language, Grice (1989) argued proper interpretation language requires uscharitably assume speakers generally insert random words speech.561fiTurneyaddition included due historical importance simplicity. Although MitchellLapata (2010) found weighted addition better unweighted addition,include weighted addition experiments, perform wellelement-wise multiplication Mitchell Lapatas (2010) experiments. includeholistic model noncompositional baseline.Table 14 compares dual-space model holistic model, element-wise multiplication, vector addition. latter three models, try three spaces.Algorithmdual-spaceholisticholisticholisticmultiplicationmultiplicationmultiplicationadditionadditionadditionSpacedomain functionmonodomainfunctionmonodomainfunctionmonodomainfunctionAccuracy58.381.679.167.555.757.546.348.350.139.8Table 14: Results noun-modifier questions.table, dual-space refers dual-space model using Equation 31. holisticmodel, ab represented corresponding row vector given space. RecallSection 3.1 that, Step 1.1, rows matrices correspond n-grams WordNet,n may greater one. Thus, example, dog house correspondingrow vector three spaces. holistic model simply uses row vectorrepresentation dog house. element-wise multiplication, ab represented usingEquation 21. vector addition model, ab represented + b, vectorsnormalized unit length added. four models use constraints6= c b 6= c. four models use training data parameter tuning.difference dual-space model (58.3%) best variation elementwise multiplication (57.5%) statistically significant 95% confidence level, according Fishers Exact Test. However, difference dual-space model(58.3%) best variation vector addition (50.1%) significant.4.2.2 Limitations Holistic Approachthree spaces, holistic model significantly better models,inability address issue linguistic creativity major limitation. 2180 multiplechoice questions used experiments intentionally constructedrequirement stem bigram must corresponding row function space (seeabove). done could use holistic model baseline; however,gives misleading impression holistic model serious competitorcompositional approaches. design, Table 14 shows holistic model achieveideal (but unrealistic) conditions.562fiDomain Function: Dual-Space ModelMitchell Lapatas (2010) dataset, used experiments Section 4.3, illustrateslimitations holistic model. dataset consists 324 distinct pairs bigrams,composed 216 distinct bigrams. 216 bigrams, 28 (13%) occur WordNet.324 pairs bigrams, 13 (4%) contain bigrams occur WordNet. Givenmatrices use (with rows based WordNet), holistic approach wouldreduced random guessing 96% pairs Mitchell Lapatas (2010) dataset.might argued failure holistic approach Mitchell Lapatas(2010) dataset due decision base rows matrices terms WordNet.However, suppose attempt build holistic model frequent bigrams. Web1T 5-gram corpus (Brants & Franz, 2006) includes list bigrams appeared 40times terabyte text, total 314,843,401 bigrams. Using compositionalapproach, matrices use represent majority bigrams.hand, holistic approach would require matrix 314,843,401 rows,considerably beyond current state art.One possibility build matrix holistic approach needed, giveninput set n-grams, instead building large, static, multipurpose matrix.two problems idea. First, slow. Turney (2006b) used approachSAT analogy questions, required nine days run, whereas dual-space modelprocess SAT questions seconds, given static, multipurpose matrix. Second,requires large corpus, corpus size must grow exponentially n, lengthphrases. Longer phrases rare, larger corpora needed gather sufficientdata model phrases. Larger corpora also result longer processing times.given application, may wise predefined list bigrams holisticrepresentations, would wise expect list sufficient coverbigrams would seen practice. creativity human language use requirescompositional models (Chomsky, 1975; Fodor & Lepore, 2002). Although holistic modelincluded baseline experiments, competitor models;supplement models.4.2.3 Impact Constraintsuse sim1 alone (Equation 30), dropping constraints 6= c b 6= c, accuracydrops signficantly, 58.3% 13.7%. However, models benefit greatlyconstraints. Table 15, take best variation model Table 14look happens constraints dropped.Algorithmdual-spaceholisticmultiplicationadditionSpacedomain functionmonodomaindomainconstraints58.381.657.550.1Accuracyconstraints13.749.68.22.5difference-44.6-32.0-49.3-47.6Table 15: impact constraints, 6= c b 6= c, accuracy.563fiTurney4.2.4 Element-wise MultiplicationSection 3.8, argued c = fi b suitable row vectors matrix Uk pksuggested Equation 21 alternative. use c = fi b domainspace, instead Equation 21, performance drops significantly, 57.5% 21.5%.4.2.5 Impact Idiomsgap holistic model models may due idiomaticbigrams testing questions. One successful approaches determiningwhether multiword expression (MWE) compositional noncompositional (idiomatic)compare holistic vector representation compositional vector representation(for example, high cosine two vectors suggests MWE compositional,idiomatic) (Biemann & Giesbrecht, 2011; Johannsen, Alonso, Rishj, & Sgaard, 2011).However, approach suitable here, want assumegap entirely due idiomatic bigrams; instead, would like estimate muchgap due idiomatic bigrams.WordNet contains clues use indicators bigram might lesscompositional bigrams (allowing compositionality matter degree).One clue whether WordNet gloss bigram contains either head nounmodifier. example, gloss dog house outbuilding serves shelterdog, contains modifier, dog. suggests dog house may compositional.classified 1500 testing set questions head (the first five charactershead noun bigram match first five characters word bigrams gloss),modifier (the first five characters modifier bigram match first five charactersword bigrams gloss), (both head modifier match), neither(neither head modifier match). four classes approximately equallydistributed testing questions (424 head, 302 modifier, 330 both, 444 neither).match first five characters allow cases like brain surgeon, glosssomeone surgery nervous system (especially brain). bigramclassified both, first five characters surgeon match first five characterssurgery.Table 16 shows accuracy models varies four classes questions. three compositional models (dual-space, multiplication, addition), neither class significantly less accurate three classes (Fishers Exact Test,95% confidence), difference significant holistic model. threecompositional models, neither class 17% 20% less accurate classes.supports view significant fraction wrong answers compositionalmodels due noncompositional bigrams.Another clue compositionality WordNet whether head noun hypernymbigram. example, surgeon hypernym brain surgeon. classified1500 testing set questions hyper (the head noun member synsetimmediate hypernym first noun sense bigram; lookhypernym hierarchy look senses bigram) (not hyper).testing set, 621 questions hyper 879 not.564fiDomain Function: Dual-Space ModelAlgorithmdual-spaceholisticmultiplicationadditionSpacedomain functionmonodomaindomain63.082.761.853.6head63.083.763.756.8Accuracymodifier neither64.645.982.178.462.944.856.336.758.381.657.550.1Table 16: variation accuracy different classes bigram glosses.Table 17 gives accuracy models classes. tablegeneral pattern Table 16. three compositional models significantly loweraccuracy class, decreases 6% 8%. significant differenceholistic model.Algorithmdual-spaceholisticmultiplicationadditionSpacedomain functionmonodomaindomainAccuracyhyper62.0 55.681.0 82.061.8 54.554.8 46.858.381.657.550.1Table 17: variation accuracy different classes bigram hypernyms.4.2.6 Order SensitivityNote vector addition element-wise multiplication lack order sensitivity, Equation 31 sensitive order, simc (ab, c) 6= simc (ba, c). see impactreformulating noun-modifier questions test order-sensitivity. Firstexpand choice unigram c including stem bigram ab, resulting explicitcomparison ab c. expanded choice, ab c, generate another choice,ba c. increases number choices seven fourteen. Due symmetry,vector addition element-wise multiplication must assign similarityab c ba c.Table 18 compares dual-space model element-wise multiplication vector addition, using reformulated fourteen-choice noun-modifier questions. holistic modelincluded table rows matrices reversed babigrams (which may seen another illustration limits holistic model).stricter test, dual-space model significantly accurate element-wisemultiplication vector addition (Fishers Exact Test, 95% confidence).dual-space model perform well fourteen-choice questions, needsimd simf . drop simd Equation 31 (function alone Table 18),ignoring modifier paying attention head noun. Accuracy drops41.5% 25.7%. drop simf Equation 31 (domain alone Table 18),equation becomes symmetrical, similarity assigned ab c565fiTurneyAlgorithmdual-spacemultiplicationmodified dual-spacemodified dual-spaceadditionSpacedomain functiondomainfunction alonedomain alonedomainAccuracy41.527.425.725.722.5Table 18: Results reformulated fourteen-choice noun-modifier questions.ba c. Accuracy drops 41.5% 25.7%.18 dual-space model significantlyaccurate either modified dual-space models (Fishers Exact Test, 95%confidence).4.2.7 Summaryreformulated fourteen-choice noun-modifier questions (Table 18), dual-spacesignificantly better element-wise multiplication vector addition. originalseven-choice questions (Table 14), difference large, questionstest order. Unlike element-wise multiplication vector addition, dual-spacemodel addresses issue order sensitivity. Unlike holistic model, dual-spaceaddresses issue linguistic creativity.4.3 Similarity Phrasessubsection, apply dual-space model measuring similarity phrases,using Mitchell Lapatas (2010) dataset human similarity ratings pairs phrases.dataset includes three types phrases, adjective-noun, noun-noun, verb-object.108 pairs type (108 3 = 324 pairs phrases). pair phrasesrated 18 human subjects. ratings use 7 point scale, 1 signifies lowestdegree similarity 7 signifies highest degree. Table 19 gives examples.Let ab represent first phrase pair phrases (environment secretary) let cdrepresent second phrase (defence minister). rate similarity phrase pairssimp (ab, cd), defined follows:simp (ab, cd) = geo(simd (a, c), simd (b, d), simf (a, c), simf (b, d))(32)equation based instructions human participants (Mitchell & Lapata,2010, Appendix B), imply function domain similarity must highphrase pair get high similarity rating. Figure 3 illustrates reasoning behindequation. want high domain function similarities correspondingcomponents phrases ab cd.18. coincidence modified dual-space models accuracy 25.7% fourteenchoice questions. Although aggregate accuracy same, individual questions, two modelstypically select different choices.566fiDomain Function: Dual-Space ModelParticipant114114114109109109111111111Phrase typeadjective-nounadjective-nounadjective-nounnoun-nounnoun-nounnoun-nounverb-objectverb-objectverb-objectGroup222000222Phrase paircertain circumstance particular caselarge number great majorityevidence low costenvironment secretary defence ministeraction programme development plancity centre research worklift hand raise headsatisfy demand emphasise needlike people increase numberSimilarity642641741Table 19: Examples phrase pair similarity ratings Mitchell Lapatas (2010)dataset. Similarity ratings vary 1 (lowest) 7 (highest).bFFcsimp (ab, cd)phrasal similarityFigure 3: diagram Equation 32.4.3.1 Experimental SetupMitchell Lapata (2010) divided dataset development set (for tuning parameters) evaluation set (for testing tuned models). development set6 ratings phrase pair evaluation set 12 ratings phrase pair.development evaluation sets contain phrase pairs, judgmentsdifferent participants. Thus 6324 = 1, 944 rated phrase pairs developmentset 12 324 = 3, 888 ratings evaluation set.19challenging evaluation, divide dataset phrase pairs ratherparticipants. development set 108 phrase pairs 18 ratingsevaluation set 216 phrase pairs 18 ratings each. three phrase types,randomly select 36 phrase pairs development set (3 36 = 108 phrase pairs)19. information paragraph based Section 4.3 paper Mitchell Lapata (2010)personal communication Jeff Mitchell June, 2010.567fiTurney72 evaluation set (3 72 = 216 phrase pairs). Thus 18 108 = 1, 944ratings development set 18 216 = 3, 888 evaluation set.Mitchell Lapata (2010) use Spearmans rank correlation coefficient (Spearmansrho) evaluate performance various vector composition algorithms taskemulating human similarity ratings. given phrase type, 108 phrase pairsdivided 3 groups 36 pairs each. group evaluation set, 12 peoplegave similarity ratings pairs given group. group 36 pairs givendifferent group 12 people. score algorithm given phrase typeaverage three rho values, one rho three groups. 12 people rating 36pairs group, 12 36 = 432 ratings. human ratings representedvector 432 numbers. algorithm generates one rating pair group,yielding 36 numbers. make algorithms ratings comparable human ratings,algorithms ratings duplicated 12 times, yielding vector 432 numbers. Spearmansrho calculated two vectors 432 ratings. 3 phrase types 3 rhovalues 432 ratings per rho value, 3,888 ratings.20believe evaluation method underestimates performance algorithms. Combining ratings different people one vector 432 numbersallow correlation adapt different biases. one person gives consistently low ratingsanother person gives consistently high ratings, people ranking,ranking matches algorithms ranking, algorithm get highscore. fair evaluation, score algorithm calculating one rho valuehuman participant given phrase type, calculate averagerho values participants.given phrase type, 108 phrase pairs divided 3 groups 36 pairs each.development set, randomly select 12 phrase pairs 3 groups(3 12 = 36 phrase pairs per phrase type). leaves 24 phrase pairs 3groups evaluation set (3 24 = 72 phrase pairs per phrase type). humanparticipants ratings represented vector 24 numbers. algorithms ratingsalso represented vector 24 numbers. rho value calculated two vectors24 numbers input. given phrase type, algorithms score average 54rho values (18 participants per group 3 groups = 54 rho values). 3 phrase types54 rho values 24 ratings per rho value, 3,888 ratings.4.3.2 Comparison ApproachesTable 20 compares dual-space model vector addition element-wise multiplication.use development set tune parameters three approaches. vectoraddition, ab represented + b cd represented c + d. similarity abcd given cosine two vectors. Element-wise multiplication uses Equation 21represent ab cd. dual-space model uses Equation 32.average correlation dual-space model (0.48) significantly averagecorrelation vector addition using function space (0.51). Element-wise multiplicationmono space (0.47) also significantly vector addition using function space (0.51).20. information paragraph based personal communication Jeff Mitchell June, 2010.Mitchell Lapatas (2010) paper describe Spearmans rho applied.568fiDomain Function: Dual-Space ModelAlgorithmhumandual-spaceadditionadditionadditionmultiplicationmultiplicationmultiplicationCorrelationad-nn nn-nn0.560.540.480.540.470.610.320.550.490.550.430.570.350.580.390.45phrase typevb-ob avg0.570.560.430.480.420.500.410.420.480.510.410.470.390.440.270.37Commentleave-one-out correlation subjectsdomain function spacemono spacedomain spacefunction spacemono spacedomain spacefunction spaceTable 20: Performance models evaluation dataset.difference dual-space model (0.48) element-wise multiplication monospace (0.47) signficant. average correlation algorithm based 162 rhovalues (3 phrase types 3 groups 18 participants = 162 rho values = 162 participants).calculate statistical significance using paired t-test 95% significance level,based 162 pairs rho values.4.3.3 Order SensitivityMitchell Lapatas (2010) dataset test order sensitivity. Given phrase pairab cd, test order sensitivity adding new pair ab dc. assumenew pairs would given rating 1 human participants. Table 21,show happens transformation applied examples Table 19.save space, give examples participant number 114.Participant114114114114114114Phrase typeadjective-nounadjective-nounadjective-nounadjective-nounadjective-nounadjective-nounGroup222222Phrase paircertain circumstance particular casecertain circumstance case particularlarge number great majoritylarge number majority greatevidence low costevidence cost lowSimilarity614121Table 21: Testing order sensitivity adding new phrase pairs.Table 22 gives results new, expanded dataset. stringentdataset, dual-space model performs significantly better vector additionvector multiplication. Unlike element-wise multiplication vector addition, dualspace model addresses issue order sensitivity.manually inspected new pairs automatically rated 1 foundrating 1 reasonable cases, although cases could disputed. example,original noun-noun pair tax charge interest rate generates new pair tax chargerate interest original verb-object pair produce effect achieve result generatesnew pair produce effect result achieve. seems natural tendency correct569fiTurneyAlgorithmhumandual-spaceadditionadditionadditionmultiplicationmultiplicationmultiplicationCorrelationad-nn nn-nn0.710.810.660.370.220.250.150.220.230.230.200.240.180.220.180.19phrase typevb-ob avg0.730.750.620.550.190.220.180.180.190.220.180.210.180.190.120.17Commentleave-one-out correlation subjectsdomain function spacemono spacedomain spacefunction spacemono spacedomain spacefunction spaceTable 22: Performance dataset expanded test order sensitivity.incorrectly ordered pairs minds assign higher ratingsdeserve. predict human ratings new pairs would vary greatly, dependinginstructions given human raters. instructions emphasizedimportance word order, new pairs would get low ratings. prediction supportedresults SemEval 2012 Task 2 (Jurgens, Mohammad, Turney, & Holyoak, 2012),instructions raters emphasized importance word order wronglyordered pairs received low ratings.4.3.4 Summarydataset test order sensitivity, vector addition performs slightly betterdual-space model. dataset tests order sensitivity, dual-spacemodel surpasses vector addition element-wise multiplication large margin.4.4 Domain versus Function Associated versus SimilarChiarello et al. (1990) created dataset 144 word pairs labeled similar-only,associated-only, similar+associated (48 pairs three classes). Table 23shows examples dataset. labeled pairs created cognitivepsychology experiments human subjects. experiments, found evidenceprocessing associated words engages left right hemispheres brainways different processing similar words. is, seemsfundamental neurological difference two types semantic relatedness.21hypothesize similarity domain space, simd (a, b), measure degreetwo words associated similarity function space, simf (a, b), measuredegree two words similar. test hypothesis, define similar-only,simso (a, b), associated-only, simao (a, b), similar+associated, simsa (a, b), follows:ratio(x, y) =x/y x > 0 > 00 otherwise(33)21. controversy among cognitive scientists distinction semantic similarityassociation (McRae, Khalkhali, & Hare, 2011).570fiDomain Function: Dual-Space ModelWord pairtable:bedmusic:arthair:furhouse:cabincradle:babymug:beercamel:humpcheese:mouseale:beeruncle:auntpepper:saltfrown:smileClass labelsimilar-onlysimilar-onlysimilar-onlysimilar-onlyassociated-onlyassociated-onlyassociated-onlyassociated-onlysimilar+associatedsimilar+associatedsimilar+associatedsimilar+associatedTable 23: Examples word pairs Chiarello et al. (1990), labeled similar-only,associated-only, similar+associated. full dataset Appendix.simso (a, b) = ratio(simf (a, b), simd (a, b))(34)simao (a, b) = ratio(simd (a, b), simf (a, b))(35)simsa (a, b) = geo(simd (a, b), simf (a, b))(36)intention simso high simf high simd low, simao highsimd high simf low, simsa high simd simf high.illustrated Figure 4.FFFbbbsimso (a, b)simao (a, b)simsa (a, b)similar-onlyassociated-onlysimilar+associatedFigure 4: Diagrams Equations 34, 35, 36.571fiTurney4.4.1 Evaluationexperiments three preceding subsections, three sets parametersettings dual-space model. Table 24 shows parameter values. effect,three sets parameter setttings give us three variations similarity measures, simso ,simao , simsa . evaluate three variations see well correspondlabels Chiarello et al.s (1990) dataset.Similaritysimr (a : b, c : d)simc (ab, c)simp (ab, cd)Descriptionsimilarity relationssimilarity noun-modifier compositionssimilarity phrasesSection4.14.24.3kd800800200pd-0.10.30.3kf300100600pf0.50.60.6Table 24: Parameter settings dual-space model.given similarity measure, simso , sort 144 word pairs descending order similarities look top N pairs see manydesired label; case simso , would like see majoritytop N label similar-only. Table 25 shows percentage pairsdesired labels three variations three similarity measures. Noterandom guessing would yield 33%, since three classes pairs size.Source parameterssimr (a : b, c : d)simc (ab, c)simp (ab, cd)N102030102030102030Percentage top N desired labelsimilar-only associated-only similar+associated709090808580637773909080807070706773509080658080477773Table 25: Percentage top N word pairs desired labels.three sets parameter settings, Table 25 displays high density desiredlabels tops sorted lists. density slowly decreases movelists. evidence three similarity measures capturing three classesChiarello et al. (1990).another test hypothesis, use three similarity measures create featurevectors three elements word pair. is, word pair : b representedfeature vector hsimso (a, b), simao (a, b), simsa (a, b)i. use supervised learningten-fold cross-validation classify feature vectors three classes Chiarelloet al. (1990). learning algorithm, use logistic regression, implemented572fiDomain Function: Dual-Space ModelWeka.22 results summarized Table 26. results lend supporthypothesis similarity domain space, simd (a, b), measure degreetwo words associated similarity function space, simf (a, b), measuredegree two words similar.Source parameterssimr (a : b, c : d)simc (ab, c)simp (ab, cd)Accuracy61.159.058.3similar-only0.5470.5830.472F-measureassociated-only similar+associated0.6600.6250.7020.4900.6990.563average0.6110.5920.578Table 26: Performance logistic regression three similarity measures features.Table 25, similar-only seems sensitive parameter settings associatedonly similar+associated. hypothesize function similaritydifficult measure domain similarity. Note construction functionspace (Section 3.3) complex construction domain space (Section 3.2).Intuitively, seems easier identify domain thing identify functionalrole. Gentners (1991) work suggests children master domain similaritybecome competent function similarity.5. Discussion Experimentssection discusses results previous section.5.1 Summary ResultsSection 4.1, used 374 multiple-choice analogy questions evaluate dual-spacemodel relational similarity, simr (a : b, c : d). difference performancedual-space model (51.1% accuracy) best past result (56.1% accuracy), usingholistic model, statistically significant. Experiments reformulated versionquestions, designed test order sensitivity, supported hypothesisdomain function space required. Function space sensitive ordermerging two spaces (mono space) causes significant drop performance.Section 4.2, automatically generated 2,180 multiple-choice noun-modifier composition questions WordNet, evaluate dual-space model noun-modifier compositional similarity, simc (ab, c). difference performance dual-spacemodel (58.3% accuracy) state-of-the-art element-wise multiplication model (57.5%accuracy) statistically significant. best performance obtained holistic model (81.6%), model address issue linguistic creativity.experiments suggest significant fraction gap holistic modelmodels due noncompositional phrases. limitation element-wise multiplication model lack sensitivity order. Experiments reformulated version22. Weka available http://www.cs.waikato.ac.nz/ml/weka/.573fiTurneyquestions, designed test order sensitivitiy, demonstrated statistically significantadvantage dual-space model element-wise multiplication vector additionmodels.Section 4.3, used Mitchell Lapatas (2010) dataset 324 pairs phrasesevaluate dual-space model phrasal similarity, simp (ab, cd). reformulated versiondataset, modified test order sensitivitiy, showed statistically significant advantagedual-space model element-wise multiplication vector addition models.Section 4.4, used Chiarello et al.s (1990) dataset 144 word pairs, labeledsimilar-only, associated-only, similar+associated, test hypothesis similaritydomain space, simd (a, b), measure degree two words associatedsimilarity function space, simf (a, b), measure degree two wordssimilar. experimental results support hypothesis. interestingChiarello et al. (1990) argue fundamental neurological difference waypeople process two kinds semantic relatedness.experiments support claim dual-space model address issueslinguistic creativity, order sensitivity, adaptive capacity. Furthermore, dual-spacemodel provides unified approach semantic relations semantic composition.5.2 Corpus-based Similarity versus Lexicon-based Similarityresults Section 4.4 suggest function similarity may correspond kindtaxonomical similarity often associated lexicons, WordNet (Resnik,1995; Jiang & Conrath, 1997; Leacock & Chodrow, 1998; Hirst & St-Onge, 1998).word pairs Table 23 labeled similar-only kinds words typicallyshare common hypernym taxonomy. example, table:bed share hypernymfurniture. believe correct, necessarily imply lexiconbased similarity measures would better corpus-based approach,used here.various similarities Section 4, arguably relational similarity, simr (a : b, c : d),makes use function similarity. itself, function similarity achieves 50.8%SAT questions (original five-choice version; see Table 12). However, best performanceachieved SAT questions using WordNet 43.0% (Veale, 2004). differencestatistically significant 95% confidence level, based Fishers Exact Test.Consider analogy traffic street water riverbed. One SAT questionsinvolves analogy, traffic :street stem pair water :riverbed correctchoice. simr (a : b, c : d) (Equation 25) function similarity (Equation 22)make correct choice. recognize traffic water high degreefunction similarity; fact, similarity used hydrodynamic models traffic flow(Daganzo, 1994). However, must climb WordNet hierachy way entityfind shared hypernym traffic water. believe manuallygenerated lexicon capture functional similarity discoveredlarge corpus.6. Theoretical Considerationssection examines theoretical questions dual-space model.574fiDomain Function: Dual-Space Model6.1 Vector Composition versus Similarity Compositiondual-space model, phrase stand-alone, general-purpose representation,composite phrase, apart representations component words. compositemeaning constructed context given task. example, task measuresimilarity relation dog :house relation bird :nest, composemeanings dog house one way (see Section 4.1); task measure similarityphrase dog house word kennel, compose meanings doghouse another way (see Section 4.2); task measure similarity phrasedog house phrase canine shelter, compose meanings dog housethird way (see Section 4.3). composition construction explicitly ties togethertwo things compared, depends nature comparisondesired, task performed. hypothesize single stand-alone,task-independent representation constructed suitable purposes.noted introduction, composition vectors result stand-alonerepresentation phrase, composing similarities necessarily yields linking structureconnects phrase phrases. linking structures seen Figures1 4. Intuitively, seems important part understand phraseconnecting phrases. Part understanding dog house connectionkennel. Dictionaries make kinds connections explicit. perspective,idea explicit linking structure seems natural, given making connnections amongwords phrases essential aspect meaning understanding.6.2 General Form Similarities Dual-Space Modelsubsection, present general scheme ties together various similaritiesdefined Section 4. scheme includes similarities chunks textarbitrary size. scheme encompasses phrasal similarity, relational similarity,compositional similarity.Let chunk text (an ordered set words), ht1 , t2 , . . . , tn i, tiword. represent semantics = hD, Fi, F matrices.row vector di D, = 1, 2, . . . , n, row vector domain space representsdomain semantics word ti . row vector fi F, = 1, 2, . . . , n, row vectorfunction space represents function semantics word ti . keep notationsimple, parameters, kd pd domain space kf pf function space,implicit. Assume row vectors F normalized unit length. Notesize representation scales linearly n, number words t, henceinformation scalability. large values n, inevitably duplicatewords t, representation could easily compressed sublinear size without lossinformation.Let t1 t2 two chunks text representations T1 = hD1 , F1 T2 =hD2 , F2 i, t1 contains n1 words t2 n2 words. Let D1 D2parameters, kd pd , let F1 F2 parameters, kf pf . D1n1 kd , D2 n2 kd , F1 n1 kf , F2 n2 kf . Note D1 DT1 n1 n1matrix cosines two row vectors D1 . is, element i-th575fiTurneyrow j-th column D1 DT1 cos(di , dj ). Likewise, D1 D2 n1 n2 matrixcosines row vector D1 row vector D2 .Suppose wish measure similarity, sim(t1 , t2 ), two chunkstext, t1 t2 . paper, restricted similarity measures followinggeneral form:sim(t1 , t2 ) = f (D1 DT1 , 1 2 , 2 2 , F1 F1 , F1 F2 , F2 F2 )(37)words, input composition function f cosines (and implicitparameters, kd , pd , kf , pf ); f operate directly row vectors D1 ,D2 , F1 , F2 . contrast much work discussed Section 2.1, compositionoperation shifted representations, T1 T2 , similarity measure,f . exact specification f depends task hand. T1 T2 sentences,envision structure f determined syntactic structurestwo sentences.23Consider relational similarity (Section 4.1):sim1 (a : b, c : d) = geo(simf (a, c), simf (b, d))(38)sim2 (a : b, c : d) = geo(simd (a, b), simd (c, d))(39)sim3 (a : b, c : d) = geo(simd (a, d), simd (c, b))sim1 (a : b, c : d) sim2 (a : b, c : d) sim3 (a : b, c : d)simr (a : b, c : d) =0 otherwise(40)(41)fits form Equation 37 t1 = ha, bi t2 = hc, di. seesim1 based cosines F1 FT2 , sim2 based cosines D1 D1 D2 D2 ,sim3 based cosines D1 D2 .Consider compositional similarity (Section 4.2):sim1 (ab, c) = geo(simd (a, c), simd (b, c), simf (b, c))sim1 (ab, c) 6= c b 6= csimc (ab, c) =0 otherwise(42)(43)seen instance Equation 37 t1 = ha, bi t2 = hci.case, sim1 based cosines D1 DT2 F1 F2 . constraints, 6= c b 6= c,expressed terms cosines D1 D2 , simd (a, c) 6= 1 simd (b, c) 6= 1.(Equivalently, could use cosines F1 FT2 .) Similar analyses apply similaritiesSections 4.3 4.4; similarities also instances Equation 37.Although representations T1 T2 sizes linear functions numbers phrases t1 t2 , size composition Equation 37 quadraticfunction numbers phrases t1 t2 . However, specific instances generalequation may less quadratic size, may possible limit growth23. Note requirement two chunks text, t1 t2 , numberwords. is, n1 necessarily equal n2 . Section 4.2, n1 6= n2 .576fiDomain Function: Dual-Space Modellinear function. Also, general, quadratic growth often acceptable practicalapplications (Garey & Johnson, 1979).function words (e.g., prepositions, conjunctions), one option would treatwords. would represented vectors similarities would calculated function domain spaces. Another possibility woulduse function words hints guide construction composition function f .function words would correspond vectors; instead would contribute determining linking structure connects two given chunks text. first optionappears elegant, choice options made empirically.6.3 Automatic Composition SimilaritiesSection 4, manually constructed functions combined similarity measures,using intuition background knowledge. Manual construction scaletask comparing two arbitrarily chosen sentences. However, good reasonsbelieving construction composition functions automated.Turney (2008a) presents algorithm solving analogical mapping problems,analogy solar system Rutherford-Bohr model atom. Givenlist terms solar system domain, {planet, attracts, revolves, sun, gravity, solar system, mass}, list terms atomic domain, {revolves, atom, attracts,electromagnetism, nucleus, charge, electron}, automatically generate one-to-onemapping one domain other, {solar system atom, sun nucleus, planetelectron, mass charge, attracts attracts, revolves revolves, gravity electromagnetism}. twenty analogical mapping problems, attains accuracy 91.5%,compared average human accuracy 87.6%.algorithm scores quality candidate analogical mapping composingsimilarities mapped terms. composition function addition individualcomponent similarities holistic relational similarities. algorithm searchesspace possible mappings mapping maximizes composite similaritymeasure. is, analogical mapping treated argmax problem, argumentmaximized mapping function. effect, output algorithm (an analogicalmapping) automically generated composition similarities. mapping structuresfound algorithm essentially linking structures seeFigures 1 4.believe variation Turneys (2008a) algorithm could used automatically compose similarities dual-space model; example, possibleidentify paraphrases using automatic similarity composition. proposal searchcomposition maximizes composite similarity, subject various constraints (suchconstraints based syntax sentences). Turney (2008a) points analogicalmapping could used align words two sentences, experimentallyevaluate suggestion.Recent work (Lin & Bilmes, 2011) shown argmax problems solved efficiently effectively framed monotone submodular function maximizationproblems. believe automatic composition similarities fit naturallyframework, would result highly scalable algorithms semantic composition.577fiTurneyRegarding information scalability, dual-space model suffer informationloss (unlike approaches represent compositions vectors fixed dimensionality),sizes representations grow lengths phrases grow. growthmight quadratic, exponential. questions automatecomposition similarities, may impact computational complexityscaling longer phrases, evidence questions tractable.7. Limitations Future WorkOne area future work experiment longer phrases (more two words)sentences, discussed Section 6.3. interesting topic research parsing mightused constrain automatic search similarity composition functions.focused two spaces, domain function, seems likely usmodel spaces would yield better performance. currently experimenting quad-space model includes domain (noun-based contextual patterns),function (verb-based), quality (adjective-based), manner (adverb-based) spaces.preliminary results quad-space promising. Quad-space seems relatedPustejovskys (1991) four-part qualia structure.Another issue avoided morphology. discussed Section 3.6, usedvalidForms function WordNet::QueryData Perl interface WordNet mapmorphological variations words base forms. implies that, example,singular noun plural form semantic representation.certainly simplification sophisticated model would use different representationsdifferent morphological forms word.also avoided issue polysemy. possible extend past workpolysemy VSMs dual-space model (Schutze, 1998; Pantel & Lin, 2002; Erk& Pado, 2008).paper, treated holistic model dual-space modelcompetitors, certain cases, idiomatic expressions, holisticapproach required. Likewise, holistic approach limited inability handlelinguistic creativity. considerations suggest holistic dual-space modelsmust integrated. another topic future work.Arguably limitation dual-space model four parameterstune (kd , pd , kf , pf ). hand, perhaps model adaptive capacitymust parameters tune. research needed.number design decisions made construction domain functionspace, especially conversion phrases contextual patterns (Sections 3.2 3.3).decisions guided intuitions. expect exploration experimental evaluation design space fruitful area future research.construction function space (Section 3.3) specific English. may generalize readily Indo-European languages, languages may presentchallenge. another topic future research.composite similarities use geometric mean combine domainfunction similarities, see reason restrict possible composition functions.578fiDomain Function: Dual-Space ModelEquation 37 allows composition function f . Exploring space possible compositionfunctions another topic future work.Another question formal logic textual entailment integratedapproach. dual-space model seems suitable recognizing paraphrases,obvious way handle entailment. generally, focused variouskinds similarity, scale phrases (red ball) sentences (The ballred), encounter truth falsity. Gardenfors (2004) argues spatial modelsbridge low-level connectionist models high-level symbolic models. claimsspatial models best questions similarity symbolic models bestquestions truth. yet know join two kinds models.8. Conclusionsgoal research develop model unifies semantic relationscompositions, also addressing linguistic creativity, order sensitivity, adaptive capacity, information scalability. believe dual-space model achieves goal,although certainly room improvement research.many kinds wordcontext matrices, based various notions context;Sahlgren (2006) gives good overview types context exploredpast work. novelty dual-space model includes two distinctcomplementary wordcontext matrices work together synergistically.two distinct spaces, two distinct similarity measures,combined many different ways. multiple similarity measures, similarity composition becomes viable alternative vector composition. example, instead multiplying vectors, c = fi b, multiply similarities, simsa (a, b) =geo(simd (a, b), simf (a, b)). results suggest fruitful new way lookproblems semantics.AcknowledgmentsThanks George Foster, Yair Neuman, David Jurgens, reviewers JAIRhelpful comments earlier version paper. Thanks Charles Clarkecorpus used build three spaces, Stefan Buttcher Wumpus,creators WordNet making lexicon available, developers OpenNLP,Doug Rohde SVDLIBC, Jeff Mitchell Mirella Lapata sharing dataanswering questions evaluation methodology, Christine Chiarello, CurtBurgess, Lorie Richards, Alma Pollock making data available, Jason RennieWordNet::QueryData Perl interface WordNet, developers Perl DataLanguage.ReferencesAerts, D., & Czachor, M. (2004). Quantum aspects semantic analysis symbolicartificial intelligence. Journal Physics A: Mathematical General, 37, L123L132.579fiTurneyBaroni, M., & Zamparelli, R. (2010). Nouns vectors, adjectives matrices: Representing adjective-noun constructions semantic space. Proceedings 2010Conference Empirical Methods Natural Language Processing (EMNLP 2010),pp. 11831193.Bengio, Y., Ducharme, R., Vincent, P., & Jauvin, C. (2003). neural probabilistic languagemodel. Journal Machine Learning Research, 3, 11371155.Bicici, E., & Yuret, D. (2006). Clustering word pairs answer analogy questions.Proceedings Fifteenth Turkish Symposium Artificial Intelligence NeuralNetworks (TAINN 2006), Akyaka, Mugla, Turkey.Biemann, C., & Giesbrecht, E. (2011). Distributional semantics compositionality 2011:Shared task description results. Proceedings Workshop DistributionalSemantics Compositionality (DiSCo 2011), pp. 2128, Portland, Oregon.Bollegala, D., Matsuo, Y., & Ishizuka, M. (2009). Measuring similarity implicitsemantic relations Web. Proceedings 18th International ConferenceWorld Wide Web (WWW 2009), pp. 651660.Brants, T., & Franz, A. (2006). Web 1T 5-gram Version 1. Linguistic Data Consortium,Philadelphia.Bullinaria, J., & Levy, J. (2007). Extracting semantic representations word cooccurrence statistics: computational study. Behavior Research Methods, 39 (3),510526.Buttcher, S., & Clarke, C. (2005). Efficiency vs. effectiveness terabyte-scale information retrieval. Proceedings 14th Text REtrieval Conference (TREC 2005),Gaithersburg, MD.Caron, J. (2001). Experiments LSA scoring: Optimal rank basis.. ProceedingsSIAM Computational Information Retrieval Workshop, pp. 157169, Raleigh,NC.Chiarello, C., Burgess, C., Richards, L., & Pollock, A. (1990). Semantic associativepriming cerebral hemispheres: words do, words dont . . . sometimes,places. Brain Language, 38, 75104.Chomsky, N. (1975). Logical Structure Linguistic Theory. Plenum Press.Church, K., & Hanks, P. (1989). Word association norms, mutual information, lexicography. Proceedings 27th Annual Conference Association Computational Linguistics, pp. 7683, Vancouver, British Columbia.Clark, S., Coecke, B., & Sadrzadeh, M. (2008). compositional distributional modelmeaning. Proceedings 2nd Symposium Quantum Interaction, pp. 133140,Oxford, UK.Clark, S., & Pulman, S. (2007). Combining symbolic distributional models meaning.Proceedings AAAI Spring Symposium Quantum Interaction, pp. 5255,Stanford, CA.Conway, J. H., & Sloane, N. J. A. (1998). Sphere Packings, Lattices Groups. Springer.580fiDomain Function: Dual-Space ModelDaganzo, C. F. (1994). cell transmission model: dynamic representation highwaytraffic consistent hydrodynamic theory. Transportation Research Part B:Methodological, 28 (4), 269287.Davidov, D., & Rappoport, A. (2008). Unsupervised discovery generic relationships usingpattern clusters evaluation automatically generated SAT analogy questions.Proceedings 46th Annual Meeting ACL HLT (ACL-HLT-08), pp.692700, Columbus, Ohio.Erk, K., & Pado, S. (2008). structured vector space model word meaning context.Proceedings 2008 Conference Empirical Methods Natural LanguageProcessing (EMNLP-08), pp. 897906, Honolulu, HI.Fellbaum, C. (Ed.). (1998). WordNet: Electronic Lexical Database. MIT Press.Firth, J. R. (1957). synopsis linguistic theory 19301955. Studies LinguisticAnalysis, pp. 132. Blackwell, Oxford.Fodor, J., & Lepore, E. (2002). Compositionality Papers. Oxford University Press.Gardenfors, P. (2004). Conceptual Spaces: Geometry Thought. MIT Press.Garey, M. R., & Johnson, D. S. (1979). Computers Intractability: Guide TheoryNP-Completeness. Freeman.Gentner, D. (1983). Structure-mapping: theoretical framework analogy. CognitiveScience, 7 (2), 155170.Gentner, D. (1991). Language career similarity. Gelman, S., & Byrnes, J.(Eds.), Perspectives Thought Language: Interrelations Development, pp.225277. Cambridge University Press.Golub, G. H., & Van Loan, C. F. (1996). Matrix Computations (Third edition). JohnsHopkins University Press, Baltimore, MD.Grefenstette, E., & Sadrzadeh, M. (2011). Experimenting transitive verbs DisCoCat. Proceedings GEMS 2011 Workshop GEometrical Models NaturalLanguage Semantics.Grice, H. P. (1989). Studies Way Words. Harvard University Press, Cambridge,MA.Guevara, E. (2010). regression model adjective-noun compositionality distributionalsemantics. Proceedings 2010 Workshop GEometrical Models NaturalLanguage Semantics (GEMS 2010), pp. 3337.Harris, Z. (1954). Distributional structure. Word, 10 (23), 146162.Hearst, M. (1992). Automatic acquisition hyponyms large text corpora. Proceedings 14th Conference Computational Linguistics (COLING-92), pp. 539545.Herdagdelen, A., & Baroni, M. (2009). Bagpack: general framework represent semanticrelations. Proceedings EACL 2009 Geometrical Models Natural LanguageSemantics (GEMS) Workshop, pp. 3340.581fiTurneyHirst, G., & St-Onge, D. (1998). Lexical chains representations context detectioncorrection malapropisms. Fellbaum, C. (Ed.), WordNet: ElectronicLexical Database, pp. 305332. MIT Press.Jiang, J. J., & Conrath, D. W. (1997). Semantic similarity based corpus statisticslexical taxonomy. Proceedings International Conference ResearchComputational Linguistics (ROCLING X), pp. 1933, Tapei, Taiwan.Johannsen, A., Alonso, H. M., Rishj, C., & Sgaard, A. (2011). Shared task systemdescription: Frustratingly hard compositionality prediction. ProceedingsWorkshop Distributional Semantics Compositionality (DiSCo 2011), pp. 2932, Portland, Oregon.Jones, M. N., & Mewhort, D. J. K. (2007). Representing word meaning order information composite holographic lexicon. Psychological review, 114, 137.Jurgens, D. A., Mohammad, S. M., Turney, P. D., & Holyoak, K. J. (2012). SemEval-2012Task 2: Measuring degrees relational similarity. Proceedings First JointConference Lexical Computational Semantics (*SEM), pp. 356364, Montreal,Canada.Kintsch, W. (2000). Metaphor comprehension: computational theory. Psychonomic Bulletin & Review, 7 (2), 257266.Kintsch, W. (2001). Predication. Cognitive Science, 25 (2), 173202.Kolda, T., & Bader, B. (2009). Tensor decompositions applications. SIAM Review,51 (3), 455500.Landauer, T. K. (2002). computational basis learning cognition: ArgumentsLSA. Ross, B. H. (Ed.), Psychology Learning Motivation: AdvancesResearch Theory, Vol. 41, pp. 4384. Academic Press.Landauer, T. K., & Dumais, S. T. (1997). solution Platos problem: latent semantic analysis theory acquisition, induction, representation knowledge.Psychological Review, 104 (2), 211240.Landauer, T. K., McNamara, D. S., Dennis, S., & Kintsch, W. (2007). Handbook LatentSemantic Analysis. Lawrence Erlbaum, Mahwah, NJ.Leacock, C., & Chodrow, M. (1998). Combining local context WordNet similarityword sense identification. Fellbaum, C. (Ed.), WordNet: Electronic LexicalDatabase. MIT Press.Lee, D. D., & Seung, H. S. (1999). Learning parts objects nonnegative matrixfactorization. Nature, 401, 788791.Lepage, Y., & Shin-ichi, A. (1996). Saussurian analogy: theoretical accountapplication. Proceedings 16th International Conference ComputationalLinguistics (COLING 1996), pp. 717722.Lin, H., & Bilmes, J. (2011). class submodular functions document summarization.49th Annual Meeting Association Computational Linguistics: HumanLanguage Technologies (ACL-HLT), pp. 510520.582fiDomain Function: Dual-Space ModelMangalath, P., Quesada, J., & Kintsch, W. (2004). Analogy-making predication usingrelational information LSA vectors. Proceedings 26th Annual MeetingCognitive Science Society, p. 1623, Austin, TX.McRae, K., Khalkhali, S., & Hare, M. (2011). Semantic associative relations adolescents young adults: Examining tenuous dichotomy. Reyna, V., Chapman,S., Dougherty, M., & Confrey, J. (Eds.), Adolescent Brain: Learning, Reasoning,Decision Making, pp. 3966. APA, Washington, DC.Mitchell, J., & Lapata, M. (2008). Vector-based models semantic composition. Proceedings ACL-08: HLT, pp. 236244, Columbus, Ohio. Association ComputationalLinguistics.Mitchell, J., & Lapata, M. (2010). Composition distributional models semantics.Cognitive Science, 34 (8), 13881429.Moschitti, A., & Quarteroni, S. (2008). Kernels linguistic structures answer extraction. Proceedings 46th Annual Meeting Association ComputationalLinguistics Human Language Technologies: Short Papers, p. 113116, Columbus,OH.Nakov, P., & Hearst, M. (2006). Using verbs characterize noun-noun relations. Proceedings 12th International Conference Artificial Intelligence: Methodology,Systems, Applications (AIMSA 2006), pp. 233244, Varna, Bulgaria.Nakov, P., & Hearst, M. (2007). UCB: System description SemEval Task 4. Proceedings Fourth International Workshop Semantic Evaluations (SemEval 2007),pp. 366369, Prague, Czech Republic.Nastase, V., Sayyad-Shirabad, J., Sokolova, M., & Szpakowicz, S. (2006). Learning nounmodifier semantic relations corpus-based WordNet-based features. Proceedings 21st National Conference Artificial Intelligence (AAAI-06), pp.781786.Nastase, V., & Szpakowicz, S. (2003). Exploring noun-modifier semantic relations.Proceedings Fifth International Workshop Computational Semantics (IWCS5), pp. 285301, Tilburg, Netherlands.Niwa, Y., & Nitta, Y. (1994). Co-occurrence vectors corpora vs. distance vectorsdictionaries. Proceedings 15th International Conference ComputationalLinguistics, pp. 304309, Kyoto, Japan.Seaghdha, D., & Copestake, A. (2009). Using lexical relational similarity classifysemantic relations. Proceedings 12th Conference European ChapterAssociation Computational Linguistics (EACL-09), Athens, Greece.Pantel, P., & Lin, D. (2002). Discovering word senses text. Proceedings EighthACM SIGKDD International Conference Knowledge Discovery Data Mining,pp. 613619, Edmonton, Canada.Plate, T. (1995). Holographic reduced representations. IEEE Transactions Neural Networks, 6 (3), 623641.Pustejovsky, J. (1991). generative lexicon. Computational Linguistics, 17 (4), 409441.583fiTurneyRapp, R. (2003). Word sense discovery based sense descriptor dissimilarity. Proceedings Ninth Machine Translation Summit, pp. 315322.Resnik, P. (1995). Using information content evaluate semantic similarity taxonomy.Proceedings 14th International Joint Conference Artificial Intelligence(IJCAI-95), pp. 448453, San Mateo, CA. Morgan Kaufmann.Rosario, B., & Hearst, M. (2001). Classifying semantic relations noun-compoundsvia domain-specific lexical hierarchy. Proceedings 2001 ConferenceEmpirical Methods Natural Language Processing (EMNLP-01), pp. 8290.Rosario, B., Hearst, M., & Fillmore, C. (2002). descent hierarchy, selectionrelational semantics. Proceedings 40th Annual Meeting AssociationComputational Linguistics (ACL-02), pp. 247254.Sahlgren, M. (2006). Word-Space Model: Using distributional analysis represent syntagmatic paradigmatic relations words high-dimensional vector spaces.Ph.D. thesis, Department Linguistics, Stockholm University.Santorini, B. (1990). Part-of-speech tagging guidelines Penn Treebank Project. Tech.rep., Department Computer Information Science, University Pennsylvania.(3rd revision, 2nd printing).Schutze, H. (1998). Automatic word sense discrimination. Computational Linguistics, 24 (1),97124.Smolensky, P. (1990). Tensor product variable binding representation symbolicstructures connectionist systems. Artificial Intelligence, 159216.Socher, R., Huang, E. H., Pennington, J., Ng, A. Y., & Manning, C. D. (2011). Dynamicpooling unfolding recursive autoencoders paraphrase detection. AdvancesNeural Information Processing Systems (NIPS 2011), pp. 801809.Socher, R., Manning, C. D., & Ng, A. Y. (2010). Learning continuous phrase representationssyntactic parsing recursive neural networks. Proceedings NIPS-2010Deep Learning Unsupervised Feature Learning Workshop.Thater, S., Furstenau, H., & Pinkal, M. (2010). Contextualizing semantic representationsusing syntactically enriched vector models. Proceedings 48th Annual MeetingAssociation Computational Linguistics, pp. 948957.Turney, P. D. (2001). Mining Web synonyms: PMI-IR versus LSA TOEFL.Proceedings Twelfth European Conference Machine Learning (ECML-01),pp. 491502, Freiburg, Germany.Turney, P. D. (2006a). Expressing implicit semantic relations without supervision.Proceedings 21st International Conference Computational Linguistics44th Annual Meeting Association Computational Linguistics (Coling/ACL06), pp. 313320, Sydney, Australia.Turney, P. D. (2006b). Similarity semantic relations. Computational Linguistics, 32 (3),379416.Turney, P. D. (2008a). latent relation mapping engine: Algorithm experiments.Journal Artificial Intelligence Research, 33, 615655.584fiDomain Function: Dual-Space ModelTurney, P. D. (2008b). uniform approach analogies, synonyms, antonyms, associations. Proceedings 22nd International Conference ComputationalLinguistics (Coling 2008), pp. 905912, Manchester, UK.Turney, P. D., & Littman, M. L. (2005). Corpus-based learning analogies semanticrelations. Machine Learning, 60 (13), 251278.Turney, P. D., Littman, M. L., Bigham, J., & Shnayder, V. (2003). Combining independentmodules solve multiple-choice synonym analogy problems. ProceedingsInternational Conference Recent Advances Natural Language Processing(RANLP-03), pp. 482489, Borovets, Bulgaria.Turney, P. D., & Pantel, P. (2010). frequency meaning: Vector space modelssemantics. Journal Artificial Intelligence Research, 37, 141188.Utsumi, A. (2009). Computational semantics noun compounds semantic space model.Proceedings 21st International Joint Conference Artificial Intelligence(IJCAI-09), pp. 15681573.Veale, T. (2004). WordNet sits SAT: knowledge-based approach lexical analogy.Proceedings 16th European Conference Artificial Intelligence (ECAI 2004),pp. 606612, Valencia, Spain.Widdows, D. (2008). Semantic vector products: initial investigations. Proceedings2nd Symposium Quantum Interaction, Oxford, UK.585fiJournal Artificial Intelligence Research 44 (2012) 196Submitted 01/12; published 05/12C OLIN: Planning Continuous Linear Numeric ChangeAmanda ColesAndrew ColesMaria FoxDerek LongAMANDA . COLES @ KCL . AC . UKANDREW. COLES @ KCL . AC . UKMARIA . FOX @ KCL . AC . UKDEREK . LONG @ KCL . AC . UKDepartment Informatics, Kings College London,Strand, London WC2R 2LS, UKAbstractpaper describe COLIN, forward-chaining heuristic search planner, capable reasoning COntinuous LINear numeric change, addition full temporal semanticsPDDL 2.1. work make two advances state-of-the-art terms expressive reasoning capabilities planners: handling continuous linear change, handling duration-dependent effects combination duration inequalities, require tightly coupled temporal numeric reasoning planning. COLIN combines FF-styleforward chaining search, use Linear Program (LP) check consistencyinteracting temporal numeric constraints state. LP used compute boundsvalues variables state, reducing range actions need consideredapplication. addition, develop extension Temporal Relaxed Planning Graph heuristic CRIKEY 3, support reasoning directly continuous change. extend range taskvariables considered suitable candidates specifying gradient continuous numericchange effected action. Finally, explore potential employing mixed integer programming tool optimising timestamps actions plan, solutionfound. support this, contribute selection extended benchmark domainsinclude continuous numeric effects. present results COLIN demonstrate scalabilityrange benchmarks, compare existing state-of-the-art planners.1. Introductionconsiderable progress development automated planning techniquesdomains involving independent temporal metric conditions effects (Eyerich, Mattmuller,& Roger, 2009; Coles, Fox, Long, & Smith, 2008a; Gerevini, Saetti, & Serina, 2006; Edelkamp,2003; Coles, Fox, Long, & Smith, 2008b). development powerful heuristics propositionalplanning shown offer benefits solution extended planning problems, includingplanning uncertainty (Palacios & Geffner, 2009), planning numbers planningtime. However, combination integration metric temporal features, metricquantities change time-dependent ways, remains challenge received relatively littleattention.Interaction time numbers planning problems occur many ways.simplest case, using PDDL 2.1 (Fox & Long, 2003), numeric effects actions updatedinstantaneously, start end points actions known (and fixed)point action execution. corpus domains past International Planning Competitionsadhere restrictions. Time numbers interact least two complex ways. First,actions variable, possibly constrained, durations (instantaneous) effectsc2012AI Access Foundation. rights reserved.fiC OLES , C OLES , F OX & L ONGactions depend values durations. allows domain models capture effectsprocesses discretised step effects, adjusted according demands specific probleminstances. Second, effects actions considered continuous across execution,values metric variables time point depend long continuous effectsacting them.example, problem sand loaded lorry modelled amountsand loaded depends time spent loading. first approach capture increasequantity loaded sand step function applied end loading action. secondapproach, process loading sand modelled continuous linear function timespent loading, amount sand lorry observed point throughoutloading process. safety device must engaged lorry three-quartersfull, second models allow planner necessary accessunderlying process behaviour make good planning choices integrate actionsolutions. alternative models exploiting duration-dependent effects split loadingaction two parts around time point safety device must engaged,alternatives become complicated relatively modest changes domain.Continuous change forms common many important problems. include: energy management, consumption replenishment restricted continuous resourcesfuel, tracking progress chemicals storage tanks chemical plants, choreographing robot motion execution tasks, managing efficient use time.cases, model using discrete time-independent change adequate planning. However, discretisation always practical: find reasonable solution (or, indeed, find one all) identifyingappropriate granularity discretisation non-trivial, perhaps requiring range choicesfine-grained make discrete model infeasibly large. cases, numericchange cannot appropriately discretised, unavoidably necessary accessvalues numeric variables execution actions, order manage interactionsnumeric values.paper present planner, COLIN, capable reasoning variable, durationdependent, linear change linear continuous numeric effects. key advance COLIN makesable reason time-dependent change use linear programs combine metric temporal conditions effects representation. COLIN satisficingplanner attempts build good quality solutions complex class problems. Since COLINforward-searching planner requires representation states, means compute progression states heuristic function guide search path initial goalstate. COLIN built planner CRIKEY 3 (Coles, Fox, Long et al., 2008a). However, CRIKEY 3requires numeric change discrete cannot reason continuous numeric change, duration dependent change (where duration actions fixed state actionbegins). able reason successfully problems characterised continuous change, coping efficiently wide range practical problems inspired real applications,major contribution made COLIN.organisation paper follows. Section 2 explain features PDDL 2.1COLIN handle, contrast repertoire CRIKEY 3. Section 4 defineproblem addressed COLIN. Section 5 outline background temporalmetric planning supports COLIN, before, Section 6, describing details foundations COLIN lie CRIKEY 3. COLIN inherits representation states CRIKEY 3,2fiC OLIN : P LANNING C ONTINUOUS C HANGEwell machinery confirming temporal consistency plans basisheuristic function. Section 7 describe systems literature addressed similarhybrid discrete-continuous planning problems COLIN designed handle. Section 8explains state progression extended COLIN handle linear continuous change, Section 9 describes heuristic guides search solutions. Section 10 consider severalelements COLIN improve efficiency plan quality, without affecting fundamentalbehaviour planner. Since time-dependent numeric change little explored,benchmarks existence allow full quantitative evaluation. therefore presentcollection continuous domains used analysis, show COLINfares these. appendix containing explanations technical detail detailedsummaries background work COLIN depends, ensures paper completeself-contained.2. Language Features C RIKEY 3 COLINCOLIN builds CRIKEY 3 handling continuous features PDDL 2.1. C RIKEY 3 restricted management discrete change, COLIN handle full range linear continuous numeric effects. metric functions PDDL 2.1 repertoire COLINscale-up scale-down, non-linear updates, general form plan metrics. Managing plan metrics defined terms domain variables remains challenge planningyet fully confronted contemporary planner. COLIN handle restrictedform quality metric, exploits instrumented variable called total-cost. allowsCOLIN minimise overall cost shortest plan find using total-time (the defaultmetric used temporal planners).common CRIKEY 3, COLIN cope Timed Initial Literals, important featureintroduced PDDL 2.2 (Hoffmann & Edelkamp, 2005). PDDL 2.1 backward compatibleMcDermotts PDDL (McDermott, 2000) therefore supports ADL (Pednault, 1989). COLINhandle full ADL, deal restricted form conditional effect seenairplane-landing problem described section 11. restricted form allows cost actiondependent state applied. general forms conditional effect cannothandled.collection features, COLIN able fully manage discrete continuousnumeric change occur directly result actions. PDDL + (Fox & Long, 2006)supports modelling continuous change brought exogenous processes events.triggered actions, model independent continuous behaviour broughtworld rather planners direct action. key additional features PDDL +support processes events. COLIN handle features restrictedmanagement continuous change expressed durative action device.detailed explanations syntaxes semantics PDDL 2.1 PDDL +, includingsemantics implementations state representation state progression must constructed, readers refer work Fox Long (2003, 2006).3fiC OLES , C OLES , F OX & L ONGLanguagePDDL 2.1PDDL 2.1Language FeatureNumeric conditions effectsContinuous numeric effectsC RIKEY 3yesCOLINyesyesPDDL 2.1PDDL 2.1General plan metricsUse total-costAssign (to discrete variables)Scale-up/down#tDurative actionsyesyesyesyesyesyesPDDL 2.1Duration inequalitieslimitedyesPDDL 2.2TILsConditional EffectsADLyesyespartialPDDL 2.1PDDL 2.1PDDL 2.1PDDL 2.1PDDLPDDLCommentBasic treatment follows Metric-FFModification state representationModification heuristicSectionAppendix BSection 8Section 9Limited formTreatment follows Metric-FFSection 10continuous effectsIncludes required concurrencyCOLIN handlesduration-dependent effectslimited effectsSection 6Appendix CSections 8 9Section 6Section 10Table 1: Language features handled CRIKEY 3 COLIN.3. Motivationnumber accounts planning successfully applied real problems,frequency applications reported increasing. following examples involve domains hybrid discrete-continuous dynamics. dynamics typically dealtdiscretising time, packaging continuous numeric effects step functions, integratingpropositional planning techniques specialised solvers. examples hybriddiscrete-continuous reasoning could exploited improve plan quality solution time.Operations refineries (Boddy & Johnson, 2002; Lamba, Dietz, Johnson, & Boddy, 2003)chemical plants (Penna, Intrigila, Magazzeni, & Mercorio, 2010), continuousprocesses reflect flows materials, mixing chemical reactions, heating cooling.Management power thermal energy aerospace applications power management critical, management solar panel arrays International SpaceStation (Knight, Schaffer, & B.Clement, 2009; Reddy, Frank, Iatauro, Boyce, Kurklu, AiChang, & Jonsson, 2011). example, Knight et al. (2009) rely high-fidelity powermodel (TurboSpeed) provide support reasoning continuous power supplydifferent configurations solar panels. Power management critical problemspace applications (including planetary rovers landers, inspiring temporal-metriccontinuous Rovers domain used one benchmark evaluation domains Section 11).Chien et al. (2010) describe planner used support operations Earth Observing 1 (EO1), management thermal energy generated instruments sufficiently important on-board planner uses (highly constrained) CPU cycles modeltrack value. EO-1 inspires temporal-metric-continuous Satellite benchmark describedSection 11.Management non-renewable power contexts, battery powered devices.battery management problem described Fox et al. (2011) relies non-linear model,4fiC OLIN : P LANNING C ONTINUOUS C HANGECOLIN must currently reduce discrete linear approximation, coupled iterated validation solution refinement, order optimise power use. Battery managementexample continuous problem cannot solved continuous dynamicsremoved.Assignment time-dependent costs Aircraft Landing domain (Dierks, 2005),continuous processes govern changing costs use runway landingtime deviates optimal landing time aircraft. problem inspiresAircraft-Landing benchmark domain described Section 11.Choreography mobile robotic systems: many cases, operations robotic platformsinvolve careful management motion alongside tasks, continuous motionrobot constrains accessibility specific tasks, inspection observation.Existing examples hybrid discrete-continuous planning models reasoning problems kind include work using flow tubes capture constraints continuousprocesses (Leaute & Williams, 2005; Li & Williams, 2008). Problems involving autonomousunderwater vehicles (AUVs) inspired temporal-metric-continuous AUV benchmark presented Section 11.4. Problem DefinitionCOLIN designed solve class problems temporal metric, feature linearcontinuous metric change. refer class temporal-metric-continuous problems,contains substantial subset problems expressed PDDL 2.1.step towards class temporal-metric-continuous problems, recall definitionsimple temporal-metric planning problem one time-dependent metricchange. Simple temporal-metric problems represented tuple hI, A, G, i, where:initial state: set propositions assignment values set numericvariables. Either sets may empty. notational convenience, refervector numeric values given state v.A, set actions, hdur , pre ` , eff ` , pre , pre , eff i, where:pre ` (pre ) start (end) conditions a: state starts (ends),conditions must hold (for detailed account subtleties semanticsaction application, see Fox & Long, 2003).eff ` (eff ) start (end) effects a: starting (ending) updates world stateaccording effects. given collection effects eff x , x {`, a}, consists of:effx , propositions deleted world state;eff +x , propositions added world state;eff nx , effects acting upon numeric variables.pre invariant conditions a: must hold every point open intervalstart end a.dur duration constraints a, calculated basis world statestarted, constraining length time pass start enda. refer special parameter ?duration, denoting duration a.5fiC OLES , C OLES , F OX & L ONGG, goal: set propositions conditions numeric variables.optionally , metric optimisation function, defined function values numericvariables end plan, special variable total-time, denoting makespanplan.solution problem time-stamped sequence actions, associated durations,transforms initial state state satisfying goal, respecting conditions imposed.durations actions must specified explicitly, since possible action specificationssatisfied different duration values.PDDL 2.1 numeric conditions used pre ` , pre , pre , dur G expressedform:hf (v), op, ci, op {, <, =, >, }, c <v vector metric fluents planning problem, f (v) function appliedvector numeric fluents c arbitrary constant. Numeric effects used eff ` effexpressed as:hv, op, f (v)i, op {=, +=, =, -=, =}restricted form numeric expressions set expressions Linear Normal Form (LNF).expressions f (v) weighted sum variables plus constant, expressibleform w v + c, vector constants, w. notable consequence permitting dur takeform set LNF constraints ?duration ?duration need evaluate singlefixed value. instance, may constrain value ?duration lie within range values,e.g. (?duration v1 ) (?duration v2 ), numeric variables v1 v2 . Restrictingconditions effects use LNFs allows metric expressions captured linearprogram model, fact exploit COLIN.class temporal-metric problems extended temporal-metric-continuous problemstwo additions:1. action described additional component: set linear continuousnumeric effects, cont, form hv, ki, k <, denoting increases v rate kper unit time. corresponds PDDL 2.1 effect (increase (v) (* #t k)).2. start end effects actions (eff n` eff na may, additionally, include parameter?duration, denoting duration action, hence written:hv, op, w v + k.(?duration) + ci s.t. op {+=, =, -=}, c, k <temporal-metric-continuous problems relationship time numbers complex temporal-metric problems. first extension allows value variable v dependlength time elapsed since continuous effect acting upon began. second extension implies that, ?duration fixed, value variables depend durationassigned action. fact , planners allow literal ?duration appear effects,even actions value parameter constrained take single fixed valueduration constraint (e.g. (= ?duration 10)). typical idiom name intended valueduration metric fluent initial state (e.g. (= (durationOfAction) 10)) usefluent effects.6fiC OLIN : P LANNING C ONTINUOUS C HANGE(:durative-action saveHard:parameters ():duration (= ?duration 10):condition(and (at start (canSave))(over (>= (money) 0))):effect(and (at start (not (canSave)))(at end (canSave))(at start (saving))(at end (not (saving)))(increase (money) (* #t 1))))(:durative-action lifeAudit:parameters ():duration (= ?duration (patience)):condition(and (at start (saving))(at end (boughtHouse))(at end (>= (money) 0))):effect (and (at end (happy)))))(:durative-action takeMortgage:parameters (?m - mortgage):duration (= ?duration (durationFor ?m)):condition(and (at start (saving))(at start (>= (money) (depositFor ?m)))(over (<= (money) (maxSavings ?m)))):effect(and (at start (decrease (money) (depositFor ?m)))(decrease (money) (* #t (interestRateFor ?m)))(at end (boughtHouse))))Figure 1: Actions Borrower Domain.Temporal-metric-continuous problems form significant subset problems expressiblePDDL + language (Fox & Long, 2006), including linear continuous change within durativeactions. problems include non-linear continuous change, explicitly representevents processes, although use certain modelling tricks capture similar behaviours.4.1 Example Problemrunning example temporal-metric-continuous domain use problem shown Figure 1. this, Borrower Domain, borrower use mortgage buy house. domainsimplified order focus attention key aspects continuous reasoning proposed realistic application. Furthermore, domain exploit variable duration actions,even though ability handle key feature COLIN. example illustrates requiredconcurrency, means interesting interactions multiple actions affecting single continuous variable, allows us demonstrate differences alternative heuristics describedSection 9. Management required concurrency also key feature COLIN, domainsvariable durations discussed later paper.domain, obtain mortgage necessary appropriate active savings planable lay deposit. conditions achieved saving hard, actioncannot applied parallel itself, preventing borrower building capitalarbitrarily high rate multiple parallel applications saveHard. sake examplerestrict saving periods durations 10 years produce interesting interactions7fiC OLES , C OLES , F OX & L ONG(:objects shortMortgage longMortgage - mortgage)(:init (= (money) 0)(canSave)(= (patience) 4)(= (depositFor shortMortgage) 5)(= (durationFor shortMortgage) 10)(= (interestRateFor shortMortgage) 0.5)(= (maxSavings shortMortgage) 6)(= (depositFor longMortgage) 1)(= (durationFor longMortgage) 12)(= (interestRateFor longMortgage) 0.75)(= (maxSavings longMortgage) 6))(:goal (and (happy)))(:metric minimize (total-time))Figure 2: example problem Borrower Domain.durations mortgages sample problem. person starts saving tied10-year savings plan.constraint able start mortgage leads required concurrency savingtaking mortgage. effects saving repaying interest therefore combine yielddifferent linear effects value money variable, saving action requiresvariable remain non-negative throughout duration saveHard action. Furthermore,order qualify tax relief, mortgage carries maximum allowed level savings throughoutmortgage (which prevents mortgage taken late savings plan). Finally,lifeAudit action places constraint gap end saving actionpoint mortgage completed (and also ensures borrower enddebt). action acknowledges borrowers happy manage completemortgages within short periods (limited patience) save hard.simple problem instance consider shown Figure 2. Two possible solutionsshown Figure 3. first solution borrower takes longer mortgage,advantage start earlier requires lower deposit. Money rises rate 1first part saving action, decreases 1 mortgage starts. rises rate0.25 (the difference saving mortgage rates) saving action concludes,continues decrease rate 0.75 mortgage ends. life audit action must startsaving action cannot end end mortgage action. second solutionborrower takes shorter mortgage, cannot start early requires much largerdeposit. consequence, life audit cannot start first saving action: mortgagefinishes late included inside life audit beginning within first saving action. meetinitial condition life audit, borrower must therefore perform second saving actionfollow first. Clearly first solution preferable since interested minimisingmakespan.8fiC OLIN : P LANNING C ONTINUOUS C HANGEmoney12 units1 unittakeMortgage longMortgage10 unitssaveHardlifeAudit4 unitsmoney10 unitstakeMortgage shortMortgage5 units10 units10 unitssaveHardsaveHardlifeAudit4 unitsFigure 3: Possible solutions Borrower problem.5. Background Metric Temporal Planningrecent work discrete numeric planning built ideas introduced planner MetricFF (Hoffmann, 2003). discrete numeric planning problem introduces numeric variablesplanning domain hold real numeric value (or undefined, yetgiven value). Actions conditions expressed terms variables, effectsact upon them. provide heuristic guidance, Metric-FF introduced extension relaxedplanning graph (RPG) heuristic (Hoffmann & Nebel, 2001), Metric RPG heuristic, supportingcomputation relaxed plan problems involving discrete numeric change.propositional RPG heuristic, performs forwards-reachability analysis delete effectsactions relaxed (ignored). numeric effects, ignoring decrease effects alwaysrelax problem, conditions require variable hold value less given constant.Thus, reachability analysis extends forwards, upper- lower- bounds valuesnumeric variables computed: decrease effects effect upon upper bound increaseeffects effect upon lower bound, assignment effects replace value upper(lower) bound incumbent lower (greater) value (respectively) wouldassigned. Deciding whether precondition satisfied given layer performed (optimistically)9fiC OLES , C OLES , F OX & L ONGbasis these: condition w v c1 , optimistically high value w vcomputed using upper bound fluent v assigned value v correspondingweight w positive, or, otherwise, using lower bound.alternative use Metric RPG proposed LPRPG (Coles, Fox, Long et al.,2008b), linear program constructed incrementally capture interactionsactions. approach restricted actions linear effects, general Metric-FF,provides accurate heuristic guidance handling metric problems performsignificantly better problems metric resources must exchanged one another ordercomplete solution.Numeric planning also gives opportunity define metric optimisation functions termsmetric variables within problem description. example, objective minimise fuel consumption defined domains quantity fuel available metric variable.optimisation function also include special variable total-time, representing makespan(execution duration) plan. planners restricted weighted sum across variables(although PDDL 2.1 syntax allows unrestricted expression across variables). general,planners yet capable optimising metric functions effectively: task finding planremains difficult. However, planners attempt optimise functions,notable LPG (Gerevini & Serina, 2000) (and, domains numeric effectscount action cost, LAMA, due Richter & Westphal, 2010).Although introduction PDDL 2.1 led increased interest temporal planning, earlierwork planning time influential. IxTeT (Ghallab & Laruelle, 1994) introducedchronicles, consisting temporal assertions constraints set state variables, timelines chronicles single state variables. Timelines since widely usedplanners followed different trajectory development led PDDL family languages (Pell, Gat, Keesing, Muscettola, & Smith, 1997; Frank & Jonsson, 2003; Cesta,Cortellessa, Fratini, & Oddi, 2009). IxTeT also pioneered use many important techniques,including simple temporal networks linear constraints.language introduced planner Temporal Graph Plan (TGP) (Smith & Weld, 1999)allowed (constant) durations attached actions. semantics actions requiredpreconditions, pre, true entire duration action, effects actions,eff, become available instantaneously ends. values affected variables treatedundefined inaccessible execution, although intended semantics (at least TGP)values considered unobservable intervals and, therefore, plansconformant respect possible values variables intervals. GPsolves problems using temporally extended version Graphplan planning graph (Blum& Furst, 1995) reason temporal constraints. temporal heuristic effective formtemporal planning developed Haslum Geffner (2001) Vidal Geffner (2006)explored constraint propagation approach handling problems.Even using expressive temporal model defined PDDL 2.1, many temporal planners make use restricted TGP semantics, exploiting simplification PDDL 2.1 encodingknown action compression. compression performed setting pre weakestpreconditions actions, eff + (eff ) strongest add (delete) effects. propo1. Conditions w v c rewritten form negating sides. Further, stating w v = crewritten pair conditions, w v c (w v) c10fiC OLIN : P LANNING C ONTINUOUS C HANGEActionQ, PPPAction BRPRQ,SAction CActionT, GOrdering achiever preconditionOrdering deleter preconditionFigure 4: problem SAPA.sitional case, terms action representation introduced earlier, are:pre = pre ` ((pre pre ) \ eff +`)+eff + = (eff +` \ eff ) eff++eff = ((eff` \ eff ` ) eff ) \ effMany modern temporal planners, MIPS - XXL (Edelkamp & Jabbar, 2006) earlier versions LPG (Gerevini & Serina, 2000), make use action compression technique. However,applying compression lead incompleteness (Coles, Fox, Halsey, Long, & Smith, 2008)(in particular, failure solve certain temporal problems). issues surrounding incompletenessfirst discussed reference planner CRIKEY (Fox, Long, & Halsey, 2004) and, later,problem structures causing said introduce required concurrency (Cushing, Kambhampati, Mausam, & Weld, 2007). Borrower domain one example problemcompression prevents solution. lifeAudit takeMortgage actions initialpreconditions satisfied inside interval saveHard action, since actionadds saving start, deletes end.Required concurrency critical ingredient planning continuous effects,change occurs change occurs important throughout execution actions. order avoid producing poor quality plans or, indeed, excluding possible solutions, must allowconcurrency actions wherever problem description permits it. nave extensioncompression approach would discretise continuous numeric change step function effectsoccurring ends relevant actions, precluding possibility managing interactionnumeric variables execution actions continuous effects. therefore buildapproach planner capable reasoning required concurrency. Borrower domain, mortgage action must overlap saving action, cannot early (to meetdeposit requirement) late (to meet maximum savings constraint ensurelife audit performed early possible). example illustrates, problems includereasoning continuous linear change typically also require concurrency.Several planners are, currently, capable reasoning PDDL 2.1 startend semantics,opposed relying compression approach. earliest PDDL 2.1 planner reasons successfully semantics VHPOP (Younes & Simmons, 2003), partial-order planner.11fiC OLES , C OLES , F OX & L ONGplanner depends heuristic guidance based relaxed planning graph usedFF, guidance fail problems required concurrency. Nevertheless, search spaceexplored VHPOP includes interleavings action start end points allow solutionproblems required concurrency. V HPOP suffers problems encounteredearlier partial-order planners performance scales poorly many domains. PSYS (Garrido,Fox, & Long, 2002; Garrido, Onainda, & Barber, 2001) Graphplan-inspired plannerproduce plans domains required concurrency. Time represented successive layersgraph, using uniform time increment successive layers. approach similar wayTGP uses plan graph represent temporal structure, TPSYS supports model actionsseparates start end effects actions dictated PDDL 2.1 semantics.Another planner adopts Graphplan-based approach temporal planning LPGP (Long& Fox, 2003a), case time successive layers variable. Instead using layersgraph represent passage fixed-duration increments time, used representsuccessive happenings time points state changes occur. time successive state changes allowed vary within constraints imposed action durations whose endpoints fixed particular happenings. linear program constructed, incrementally, modelconstraints solution program interleaved selection action choices.approach suffers weaknesses Graphplan planner: exhaustive iterativedeepening search impractical large problems, computation storage mutex relations becomes expensive larger problems. Nevertheless, LPGP provides useful approachtreatment PDDL 2.1 durative actions, splitting end points treatedinstantaneous snap actions. solution (original) planning problem expressedterms these, subject four conditions:1. start snap-action paired end snap-action (and end applied withoutcorresponding start applied earlier);2. start end action, invariants action pre respected;3. actions must currently executing state considered goal state;4. step plan occurs preceding step, time start endaction respect duration constraints.APA (Do & Kambhampati, 2003) one earliest forward-search planners solve temporal PDDL 2.1 problems. works priority queue events. durative action startedend point queued time future executed. choice pointsplanner include starting new action, also special wait action, advances timenext entry queue, corresponding action end point executed. allows SAPAreason concurrency solve problems required concurrency. Unfortunately,search space include necessary interleavings achieve complete search. example,consider problem illustrated Figure 4. solve problem, action must start, actionB must start early enough allow C complete ends (and deletes P ) late enoughaction start B ends end ends. actions required orderallow applied, achieving goal G. SAPA starts action A, queue containend A. choices open start B immediately, end earlyallow execute successfully, else complete A, advances time far allow B12fiC OLIN : P LANNING C ONTINUOUS C HANGELight MatchLightUnused MatchLightUnused MatchLightMend FuseNeeds Fixing FuseLightNeeds Fixing FuseFixed FuseFigure 5: Required Concurrencyexploit effect P A, preventing C executed. fact, simpler problem defeats SAPA:B end condition Q instead end effect G C dispensedwith. However, additional complexity existing example impossible inferstart B examination B alone, timing constraints start Bdepends actions C immediately obvious temporal constraintsaffect placement B. difficulty adopting waiting approach hardanticipate long wait next interesting time point depends interaction actionsyet even selected.different approach forward-search temporal planning explored CRIKEY familyplanners (Coles, Fox, Halsey et al., 2008; Coles, Fox, Long et al., 2008a). planners useaction splitting approach used LPGP, work heuristically guided forward search.heuristics planners use relaxed planning graph starting point (Hoffmann & Nebel,2001), extend adding guidance temporal structure plan, pruningchoices easily demonstrated violate temporal constraints inferring choicestemporal constraints imply them. planners use Simple Temporal Network model solvetemporal constraints action end points accumulated successiveaction choices. Split actions also used extend LPG temporal version respectssemantics PDDL 2.1 (Gerevini, Saetti, & Serina, 2010) (earlier versions LPG use compressed action models described above). Recent work Haslum (2009) explored waysheuristics temporal planning constructed, remaining admissible.Temporal Fast Downward (Eyerich et al., 2009), based Helmerts Fast Downward planner (Helmert, 2006), uses approach slight refinement compressed action model,allowing required concurrency managed. authors demonstrate plannersolve Match problem shown Figure 5. mistakenly claim SAPA cannot solveproblem cannot consider applying action starting ending lightingmatch: fact, SAPA apply mend fuse action match lit, muchway done Temporal Fast Downward. problem planners face situationsaction must started time last happening, next queuedevent: neither planner includes choice search space.Huang et al. (2009) developed temporal planner exploiting planning-as-SATisfiabilityparadigm. uses Graphplan-to-SAT encoding, starting LPGP action-splitting compilation, using fixed time increment successive layers graph. approach13fiC OLES , C OLES , F OX & L ONGadequate problems appropriate time increment identified, possible, general, time-dependent effects domain. Furthermore, approachineffective significant difference durations actions, time increment becomes short relative actions. planner produce optimal (makespan)plans using iterative deepening search. planner combines existing ideas achieve objectivesmainly interest relationship SAT-based approaches temporalplanning, TM - LPSAT discussed below.C RIKEY 3, planners mentioned, capable solving simple temporalplanning problems described above. restricted management discrete change.Duration-dependent change cannot handled planners. fact, plannersmanage kind reasoning numbers outside durations actions. COLIN thereforesignificantly extends competence PDDL-compliant temporal planners.6. C RIKEY3: Forward-Chaining Temporal PlannerTemporal forward-chaining planners two kinds choices make constructionplans. Firstly, non-temporal case, choice must made actions apply (thesechoices considered planning element problem). Secondly, choices mustmade apply actions (these seen scheduling choices construction solutions). CRIKEY 3 (Coles, Fox, Long et al., 2008a), temporal forward-chaining planner,exploits distinction choices, using separate procedures make planning decisions (which actions start end) scheduling decisions (when place actionstimeline). decisions must checked consistency respect existingtemporal constraints confirm actions completely scheduled. section,briefly describe CRIKEY 3 performs planning scheduling, since architecture formsbasis COLIN work subsequently described paper. Full details temporalmanagement CRIKEY 3 provided Coles et al.CRIKEY 3 uses forward-chaining heuristic state-space search drive planning decisions.makes use Enforced Hill-Climbing (EHC) algorithm introduced FF (Hoffmann & Nebel,2001) repeated, convenience, Algorithm 1. EHC incomplete, solution cannotfound CRIKEY 3 plans again, using weighted A* search. discuss search described within basic enforced hill-climbing algorithm FF extended perform temporalplanning. order this, number modifications required. particular:1. get applicable actions(S): planner must reason two actions per durative action,start action end action, rather applying action immediately consideringfinished (as non-temporal case).2. get applicable actions(S), apply(a, S): invariant conditions durative actions mustmaintained throughout execution, requires active invariants recordedstate order prevent application actions conflict them.3. goal state(S): state goal state (i.e. path solution plan)actions must completed.14fiC OLIN : P LANNING C ONTINUOUS C HANGEAlgorithm 1: Enforced Hill-Climbing AlgorithmData: P = hA, I, Gi - planning problemResult: P , solution plan1 best heuristic evaluate heuristic(I);2 best heuristic = 03return [];4 closed {I};5 open list [hI, []i];6 open list 6= []7hS, P element removed front open list;8applic(S) get applicable actions(S);apply helpful filter (applic(S));910foreach applic(S)110 apply(a, S);120 6 closed13add 0 closed ;14P 0 P followed a;valid plan(P 0 )1516goal state(S 0 )17return P 0 ;181920212223242526h evaluate heuristic(S 0 );h < best heuristicopen list [hS 0 , P 0 i];best heuristic h;break;elseh <append hS 0 , P 0 onto open list;return f ailure;4. valid plan(P ): temporal (scheduling) constraints candidate plans must respected.particular, duration constraints durative actions must satisfied. discussedSection 6.1.consider modifications turn. First, durative actions compiledtwo non-temporal actions. modified version LPGP action compilation (Long & Fox,2003a) used this, described Coles et al. (2008). durative action a, formhdur , pre ` , eff ` , pre , pre , eff i, split two non-temporal (in fact, instantaneous) snapactions form hpre, eff i:a` = hpre ` , eff `aa = hpre , eff15fiC OLES , C OLES , F OX & L ONGperforming search snap actions, taking appropriate care ensureconstraints satisfied, restrictions expressivity imposed use action compression avoided. becomes possible search plan start end pointsdifferent actions coordinated, solving problems required concurrency. pricesearch space much larger: original action replaced two snap-actions,length solution plans doubled. circumstances blow-up avoidedidentifying actions compression safe (Coles, Coles, Fox, & Long, 2009a), i.e.use action compression compromise soundness completeness.approach described Coles et al., actions still split start end snap-actions,end points compression-safe actions inserted either effects neededinvariants would otherwise violated another action chosen application. consequence,one search decision point needed per compression-safe action (choose apply start),rather two. Recent versions CRIKEY 3 COLIN make use restricted actioncompression technique search.split actions start end points, modifications basic search algorithmneeded handle constraints arise consequence. CRIKEY 3 makes use extendedstate representation, adding two elements state tuple. resulting state defined= hF, P, E, i, where:F represents facts hold current world state: set propositionscurrently true, W , vector, v, recording values numeric variables.P ordered list snap actions, representing plan reach initial state.E ordered list start events, recording actions started yet finished;collection temporal constraints actions plan reach F .purpose start event list E record information currently executingactions, assist formation sound plans. entry e E tuple hop, , dmin, dmaxwhere:op identifier action, start snap-action op` added plan;index snap-action added plan reach S;dmin, dmax minimum maximum duration op, determined stateop started.minimum maximum duration action depend state applied(e.g. duration recharge action may depend level charge time execution),durations must computed based state preceding step i. However, given actionstarted, bounds duration remain fixed. PDDL 2.1 also allows actions durationsconstrained conditions hold end action, actions supportedplanners.extended state definition leads corresponding extensions get applicable actions(S).before, snap-action deemed logically applicable state preconditions presatisfied S. However, additional condition must satisfied: effects must violate16fiC OLIN : P LANNING C ONTINUOUS C HANGEactive invariants. invariants active given state determined E denoteinvariants state event list E as:inv(S) = e.op.preeEapply end snap-action, aa , required entry e E whose operator entryop equal a. prevents planner attempting apply ends actionsyet started.Assuming action, a, found applicable chosen step plan, functionapply(a, S), applied temporally-extended state, S, yields successor 0 = hF 0 , P 0 , E 0 , 0 i.first two elements updated non-temporal case: F 0 = apply(a, F ), P 0 = P +[a].obtain 0 , begin setting 0 = . Furthermore, > 0:0 = 0 { t(i) t(i 1)}t(i) variable representing time step scheduled executed.is, new step must come least (a small unit time) preceding step. separationrespects requirement interfering actions must separated least (Fox & Long, 2003),strictly stronger required actions actually mutually exclusive.accurate realisation PDDL 2.1 semantics could implemented, would incur costoffering little apparent benefit. Finally, resulting value E 0 (and whether 0 changedfurther) depends whether start end snap-action:start action a` applied, E 0 = E + [ha, i, dmin, dmax i], dmin dmax correspond lower- upper-bounds duration a, evaluated contextvaluation F .end action aa applied, start entry {e E | e.op = a} chosen, E 0assigned value E 0 = E \ e. often case one instanceaction open, one choice pairing, case multiple instancesaction executing concurrently, search branches choicee. e chosen, final modification made 0 encode duration constraintsaction finished:0 = 0 {e.dmin t(i) t(e.i) e.dmax }information encoded state currently executing actions, extensionneeded goal state(S) minor: state goal state satisfies non-temporal versiongoal state(S), event list state, E, empty.search strategy leads natural way handle PDDL 2.2 Timed Initial Literals (TILs)directly. Dummy TIL actions introduced, comprising effects TILs timepoint, added plan earlier TIL actions already added,delete invariants open action. special case, TIL actions createentry E: facts F amended execution. do, however, produceupdated set temporal constraints. snap actions, TIL added step plan,TIL must fall earlier preceding step. Then, 0 = 0 {ts t(i) t() ts},17fiC OLES , C OLES , F OX & L ONGts time-stamp TIL prescribed happen, name denotingstart plan t() = 0. seen, constraints ensure TIL occurappropriate time, step prior TIL must occur it, stepTIL must occur it.changes described subsection ensure plans produced CRIKEY 3 logically sound: check logical applicability, coupled maintenance E throughoutsearch, ensures preconditions, either propositional numeric, broken. Useget applicable actions(S) guarantees actions logically applicable: guarantee adding snap-action plan, judged applicable way, violatetemporal constraints. example, possible preconditions satisfied planP = [a` , b` , ba , aa ], P logically sound. However, duration b greaterduration P temporally sound. next section discuss functionvalid plan(P ) modified identify reject temporally inconsistent plans.6.1 Temporal Plan Consistencystate temporally consistent steps [0...n 1] plan, P , reachesassigned values [t(0)...t(n 1)], representing times execution correspondingsteps, respecting temporal constraints, . checked use valid plan(P 0 ),called line 15 Algorithm 1 function call trivial non-temporal case,temporal case serves check temporal consistency plan. state temporalconstraints cannot satisfied immediately pruned search, since extension actionsequence lead solution plan valid.temporal constraints built CRIKEY 3 state expressed form:lb t(b) t(a) ublb, ub < 0 lb ubconstraints conveniently expressible Simple Temporal Problem (STP) (Dechter,Meiri, & Pearl, 1989). variables within STP consist timestamps actions,inequality constraints specified form. Crucially, purposes,validity STP (and assignment timestamps events therein) determinedpolynomial time solving shortest-path problem within Simple Temporal Network (STN),directed-graph representation STP. event STP represented vertexSTN. additional node t() represent time 0 time first actionplan, t(0), constrained fall within t(). constraint form adds two edgesgraph: one b weight ub, one b weight lb. Attemptingsolve shortest-path problem t() event yields one two outcomes: eitherterminates successfully, providing time-stamp step, terminates unsuccessfully duepresence negative-cost cycle within STN indicating temporal inconsistency (anyschedule would require least one step scheduled itself).CRIKEY 3, STP used check temporal consistency choices made reachstep S, based temporal constraints must hold plan P reach S,additional constraints determined E: list actions started, yetfinished. variables vars STP partitioned two sets: variables, t(i)step P f variables, one f (i) entry hop, i, dmin, dmax E. variablescorrespond times steps already added plan, might times18fiC OLIN : P LANNING C ONTINUOUS C HANGEstart end points actions. time points might correspond starts actionsyet finished subset actions (only) associated f variablesassociated pending end times actions. consistency terminologyintroduced CRIKEY 3 (Coles, Fox, Long et al., 2008a), use refer timenext event plan occur (which could execution last actions applied).time point next choice made, either start new actioncompletion existing one, therefore seen time associated final state,S, generated current plan head. ever one timepoint called valuemoves forward plan head extends. constraints follows:, constraining variables ensure temporal consistency stepsplan reach (and include constraints introduced timed initial literals);{dmin f (i) t(i) dmax | hop, i, dmin, dmax E} is, futureaction end point committed (but yet applied), recorded durationconstraint must respected;{ f (i) t(n 1) | hop, i, dmin, dmax E} is, future action end pointmust come last step current plan, ensure future.t(now ) t(n 1) is, current time (the time next eventplan occur) least last event plan.Solving STP confirms temporal consistency decisions made far. STPcannot solved, state pruned: plan induced startend action representation temporally invalid. last two categories constraints particularly important:without them, pruning could undertaken basis plan P reach S. Includingthem, however, allows STP identify cases end point action never addedplan, would lead temporal inconsistency. goal states cannot containexecuting actions (i.e. E must empty), allows CRIKEY 3 prune states earlierdefinitely path state end points added plan.Timed initial literals easily managed STP using dummy TIL actions describedearlier. constraints dummy TIL action already applied included .dummy TIL action yet occur automatically treated end action yetapplied. Thus, f variable added each, so, last step plan farconstrained come TIL event yet happen.7. Planning Continuous Numeric Changechallenging variants temporal numeric problems combine two arrive problems time-dependent metric fluents. Although problems exhibiting hybrid discrete-continuousdynamics studied research communities time, example, verification (Yi, Larsen, & Pettersson, 1997; Henzinger, Ho, & Wong-Toi, 1995; Henzinger, 1996),timed automata capture exactly kind behaviour, relatively little workcontinuous dynamics planning community.PDDL 2.1 model mixed discrete-continuous change extends propositional state transition model include continuous change state variables. state transition system19fiC OLES , C OLES , F OX & L ONGdiscrete changes transition instantaneously states. system particular state, continuous change occur state variables time passes. soon discretechange occurs system changes state. PDDL + (Fox & Long, 2006) extended allowexogenous events processes (controlled nature) well durative actions. leadsformal semantics based theory Hybrid Automata (Henzinger, 1996). actioncauses discrete state change might trigger continuous process. continues timeevent triggered leading new state. time later another action might taken.Early work exploring planning continuous processes includes Zeno system Penberthy Weld (1994), processes described using differential equations. Zeno sufferslimitations partial order planners time, unable solve largeplanning problems without significant aid carefully crafted heuristic function. importantly, fundamental constraint behaviour allow concurrent actions applycontinuous effects variable. imposes significant restriction kindsproblems solved, making Zeno much less expressive COLIN. constraintfollows, part, way model requires effects specified differential equations, rather continuous update effects, simultaneous equations must consistentone another rather accumulating additive effects. authors say must specifyentire continuous behaviour interval [of durative action] semantics insistcontinuous behaviours result direct, explicit action.Another early planner handle continuous processes McDermotts PTOP system (McDermott, 2003), heuristic search planner, using regression-based heuristic. plausibleprogression technique used within PTOP guide search sufficiently powerful recogniseinteractions could prevent future application actions, thereby restricting scalabilityproblems form consider here. PTOP competed International Planning Competition 2004, solved small subset problems (although, interestingly,solved involved expressive combination ADL temporal windows plannercould manage). PTOP interesting variant heuristic forward search approach, sinceavoids grounding representation, using approach similar means-ends linear planning approach generate relaxed plan estimates number actions required achievegoal given state.7.1 TM-LPSATrecently, Shin Davis developed TM - LPSAT (Shin & Davis, 2005), based earlierLPSAT system (Wolfman & Weld, 1999). - LPSAT first planner implement PDDL +semantics. implemented compilation scheme horizon-bounded continuousplanning problem compiled collection SAT formulas enforce PDDL + semantics,together associated set linear metric constraints numeric variables. compiledformulation passed SAT-based arithmetic constraint solver, LPSAT. L PSAT consistsDPLL solver LP solver. SAT-solver passes triggered constraints LP-solver,hands back conflict sets form nogoods constraints cannot resolved.solution horizon increased process repeats, otherwise solution decodedplan. order support concurrency compilation exploits LPGP separation actionstart end points. different versions TM - LPSAT exploiting different solvers: LPSATMathSAT-04 (Audemard, Bertoli, Cimatti, Kornilowicz, & Sebastiani, 2002)20fiC OLIN : P LANNING C ONTINUOUS C HANGEexploited. novelty TM - LPSAT lies compilation decoding phases, since solverswell-established systems.compilation scheme TM - LPSAT implements full PDDL + semantics. Althoughincludes events processes, specific PDDL +, TM - LPSAT also handle variable duration durative actions, durative actions continuous effects duration-dependent end-effects.continuous effects concurrent actions quantity two time-points summedactions active quantity period. Therefore, TM - LPSAT supports concurrentupdates continuous variables.- LPSAT interesting approach, theory capable solving large class problemsvaried continuous dynamics. However, reported empirical data suggests plannerslow unable solve problems requiring plans steps. possibleexperiment publicly available implementation system.7.2 KongmingHui Li Brian Williams explored planning hybrid systems (Li & Williams, 2008, 2011).work focussed model-based control, using techniques based constraint reasoning.continuous dynamics system modelled flow tubes capture envelopescontinuous behaviours (Leaute & Williams, 2005). dimensions tubes functiontime (typically expanding allowed extend), requirement madesuccessive continuous behaviours must connected connecting start one tube (the precondition surface) cross-section preceding tube; i.e. intersection two spaces mustnon-empty. relevant work area development planner Kongming,described Li Williams.Kongming solves class control planning problems continuous dynamics. basedconstruction fact action layers flow tubes, within iterative plan graph structureintroduced Graphplan (Blum & Furst, 1995). graph developed, every action producesflow tube contains valid trajectories develop time. Starting feasibleregion, actions whose preconditions intersect feasible region applied reachable states time point computed using state equations system. initialstate system variables single known values. valid trajectory must passsequence flow tubes, must also meet constraints specified dynamics actionsselected. mutex relation used Graphplan extended continuous dynamics wellpropositional fragment language. graph iteratively extended Graphplan,search plan conducted successive extension.plan-graph encoding problem continuous dynamics translated MixedLogical-Quadratic Program (MLQP). metric objective functions used planner optimise behaviour defined terms quadratic functions state variables. exampleproblem considered Li Williams (2008) 2-d representation simple autonomous underwater vehicle (AUV) problem AUV glide, ascend descend avoidingobstacles. language used version PDDL 2.1 extended enable dynamics encoded.continuous nature problem lies fact that, continuous action, AUVone continuous range positions determined control system. Kongmingdepends translation planning problems MLQPs constraints describing dynamics problem must linear. Since effects continuous actions involve product rate21fiC OLES , C OLES , F OX & L ONGchange time, one values treated variable. Kongmingrate change variable, time discretised, contrasts COLIN rateschange remain constant continuously variable length intervals. discretisation timeKongming exploited support state updates within plan graph: successive layers graphseparated constant uniform time increment. approach suffers disadvantageduration plan limited number happenings plan, since solver cannotrealistically solve problems tens layers plan graph.Kongming support concurrent continuous updates state variable, so,respect, PDDL 2.1 expressive extended language used Kongming. partdue difficulty resolving precisely semantics dynamics describedactions used Kongming. dynamic constraint specifies limits rate changespecific variable: unclear whether concurrent actions combined taking unionintersection bounds constraint specifies rate change given fluent.7.3 UPMurphiOne recently developed planner uses PDDL 2.1 reasons continuous processesUPMurphi (Penna, Intrigila, Magazzeni, & Mercorio, 2009). UPMurphi takes completely different approach considered far. Instead reasoning continuous change directly,UPMurphi works guessing discretisation iteratively refining solution discretised problem validate original problem specification. iterative drivercoarseness discretisation, well planning horizon, making interestingly differentbasic architecture TM - LPSAT.UPMurphi begins continuous representation problem starts discretising it.First actions discretised taking specific values feasible ranges. resultsseveral versions action. UPMurphi explores state space, explicitly constructingcurrent discretisation. Plans constructed using planning-as-model-checkingparadigm (Cimatti, Giunchiglia, Giunchiglia, & Traverso, 1997): heuristic guidesearch. plan found validated original continuous model, usingplan validator (Fox, Howey, & Long, 2005). invalid, discretisation refinedsearch resumes. UPMurphi fails find plan one discretisation starts finer graineddiscretisation. Subsequent refinements lead ever denser feasible regions, increasinglycomplex construct.UPMurphi used build partial policies handle uncertainty likely arisepractice execution hybrid control plans. controller table initially synthesised,consisting (state,action) pairs plan first constructs. However, table might lackstates could visited controller, robust. subsequent steprobustify controller randomly perturbing states finding new pathsnew states. perturbed states reachable, probability distributionused identify likely ones. called safe states. controller tableextended safe (state, action) pairs. controller table, policy, referredUniversal Plan.22fiC OLIN : P LANNING C ONTINUOUS C HANGE7.4 Approaches Continuous Reasoningcompletely different way manage continuous quantities model continuous resource consumption production terms uncertainty amount consumed produced.approach taken HAO* algorithm (Meuleau, Benazera, Brafman, Hansen, & Mausam,2009) Markov Decision Process (MDP) constructed consisting hybrid states.state contains set propositional variables also collection distributions resourceconsumption production values. states hybrid, standard value iteration approaches cannot used find policies. hybrid AO* approach described usedfind best feasible policy. feasible region constructed HAO* continuous distributionresource values resource considered uncontrollable (unlike Kongming,assumed executive maintains control values region eventuallychosen).Planning continuous processes important applications and, many application areas planning, led development systems combine generic planningtechnology carefully tuned domain-specific performance achieve necessary combination problem coverage performance. good example work BoddyJohnson (2002) colleagues (Lamba et al., 2003) planning oil refinery operations. workuses quadratic program solver, coupled heuristically guided assignment discrete decisionvariables (corresponding actions), solve real problems.8. COLIN: Forward Chaining Planning Continuous Linear Changesection describe CRIKEY 3 extended reason duration-dependentcontinuous numeric change, building planner COLIN ( COntinuous LINear dynamics).decided give planner specific name highlight capabilities. demonstrated Section 4.1, key difference introduced continuous numeric change logical numericconstraints longer neatly separated temporal constraints: values numericvariables state depend timestamps durations actions, vice versa. relativebenefits handling temporal numeric constraints together, rather separating out,apparent motivating domains outlined Section 3 amply rehearsedpaper describing PDDL + (Fox & Long, 2006).need cope integrated numeric temporal constraints raises number importantissues planning domains. First, checking whether action choice consistentlonger achieved using STP, numeric constraints interact temporalconstraints, STP sufficiently expressive capture this. Second, changing valuesnumeric variables time brings new challenges determining action applicability:precondition satisfied immediately following application action, might becomesatisfied allowing certain amount time elapse. Finally, need provideheuristic guidance. cover first two issues section, defer discussionheuristic guidance next.8.1 Temporal-Numeric Plan Consistency Linear Programmingbegin problem temporal-numeric plan consistency, techniques used dealingissue also amended use solving issues encountered determining23fiC OLES , C OLES , F OX & L ONGaction applicability. Considering definition STP given Section 6.1, make observation STP could equally well written linear program (LP). CRIKEY 3,STP efficiently solved using shortest-path algorithm. However, observation becomesimportant wish reason continuous change numeric resources alongside temporal constraints. case, use LP capture temporal constraints numericconstraints, including interaction two. describe LP built,serving replacement valid plan(S) function called search, invokesSTP solver CRIKEY 3. diagram structure LP create shown Figure 6,plan P = [a0 , ..., an2 , an1 ] reach state S, an1 action recently addedplan. (For simplicity, shows case event queue E empty.)construction LP begins variables (a subset of) constraintsSTP. STP variable ti (the time-stamp (snap) action ai ) corresponding LP variablestepi (shown across top Figure 6), STP variable ei (for future end actionstep i) corresponding LP variable estep . also construct constraints correspondingtotal-ordering action steps, STP: step P still sequenced (i.e.stepi stepi1 n > > 0), future end snap-action later stepn1(i.e. estepi stepn1 estep variables).extend LP numeric constraints problem, beginning effectsactions. Since numeric effects discrete continuous, create two additionalvectors variables per step plan. first these, vi , represents values statevariables v immediately prior ai executed (in case step 0, vi equal valuesv initial state, I). second, vi0 , contains values v immediately ai executed.Figure 6, variables v0 enumerated v0 ...vm1 and, similarly, v0 0 shown0v00 ...vm1. avoid proliferation indices index valuestime stamp Figure 6, vi ith value v time step corresponding layervariable appears. use two vectors layer required order representdiscrete changes caused actions: snap-action cause value variable differentimmediately execution. represent within LP, action step effectvariable v vi0 = vi 2 . Otherwise, discrete effect hv 0 +=w v + k.(?duration) + ci,constraint introduced define value vi0 :3vi0 = vi + w v + k.(ce(i) cs(i)) + cfunctions cs(i) ce(i) denote time-stamp variables corresponding startend action step i. step end action, ce(i) = step , cs(i)step variable start action finished step i. Similarly, step initiates action,cs(i) = step , ce(i) either estep action yet finished or, otherwise,step variable end action started step i. Therefore, substituting ce(i) cs(i)?duration captures relationship effect action duration.2. Note identities implemented efficiently simply introducing unnecessary additionalvariable. Similarly, variable subject effects conditions added LP,introduced becomes relevant.3. effects using operator -=, i.e. decrease effects, first term right-hand side negated.assignment effects, operator =, first term right-hand side (i.e. vi ) omitted entirely (the valuev assignment depend value v beforehand).24fiC OLIN : P LANNING C ONTINUOUS C HANGEMetric fluents v 0 v m1Snapactions 0 n1 corresponding timepoint variablesv0v0v0v0v0v0v0v0v1v1v1v1v1v1v1v1vvvvvva0v3v3v3step 2v3v m1v m1State 2State 1vn1step n12v3...v m12...v m12a2...v m12...v m1step 12......step 0a1v3......State 02v3v3v m12v m1State n...2...vActive continuous change affecting (some) variablesActions cause instantaneous step changes fluent valuesTemporal ConstraintsActions sequenced separated:step i+1 stepi >=action starts durative action, a, ended action j:dmin(a) <= step j step <= dmax(a)Metric Variable Constraints: Step EffectsMetric Variable Constraints: Continuous EffectsVariables updated action effects:Variables updated active continuous effects:v j = vj + vj (stepi+1 stepi )v j state i+1 v j state updated effectincluding timedependent stepeffectsvalues state i+1, v j determinedaccumulated effects active continuous effectsFigure 6: Diagrammatic Representation LP Used COLIN. Note subscripts attachedv v 0 fluents diagram indices vector fluents state,indices step represent different time steps plan. metric fluentsalso notionally indexed time step, shown diagram orderavoid clutter.Continuous numeric change occurs steps plan, rather instantexecution step itself. capture continuous effects, building LP considerstep turn, start plan, recording gradient total (linear) continuouschange acting upon variable v v, v denotes gradient active ai1execution action ai . restrictions language handled COLIN, describedSection 4, total-order constraints snap-actions, value variable viknown constant within interval successive actions: continuous changelinear. gradient variable v changed either starting action (initiating25fiC OLES , C OLES , F OX & L ONGadjustment prevailing continuous effect v given dvdt += k, k <) endingaction (terminating effect initiated start). values constants computedfollows4 :variables, v0 = 0; is, continuous numeric change activevariable start plan.ai continuous numeric effect v vi+1 = vi ;ai initiates continuous numeric effect,dvdtai terminates continuous numeric effect,+= k, vi+1 = vi + k;dvdt+= k, vi+1 = vi k;basis values, add constraints LP:vi+1 = vi0 + vi+1 (step i+1 step )Again, distinction vi vi0 important: vi determined basis continuouschange interval steps 1, immediately prior discrete effectmay occur step.created variables represent values fluents step introducedconstraints capture effects actions them, consider constraints arisepreconditions snap-action, invariants must respected startsends actions, constraints durations actions plan.numeric precondition form hv, {, =, }, w v + ci, must hold order apply step i,add constraint LP:vi {, =, }w vi + caction starting stepi ending stepj , invariants added LP0form, vectors variables [vi0 , vj1] [vi+1 , vj ] (vi v0 j excludedPDDL 2.1 semantics require invariants action hold end points).case end action (starting i) yet appeared plan, invariantsimposed vectors variables vi0 onwards: must end future, invariantsmust violated step current plan point started.Finally, add duration constraints. action starting stepi , denote variablecorresponding time finishes ce(i), ce(i) = step j end actioninserted plan step j, ce(i) = estep otherwise (as defined above). Then,duration constraint a, form h?duration, {, =, }, w v + ci, add constraint:ce(i) step {, =, }w vi + cprocess constructs LP captures numeric temporal constraints governplan, interactions them. STP CRIKEY 3, solution LPcontains values variables [step 0 ...step n ], i.e. assignment time-stamps actionsplan. prevent LP assigning variables arbitrarily large (but valid) values, set4. Variables trivially shown constant (i.e. action effect referring variable)removed LP replaced throughout values initial state.26fiC OLIN : P LANNING C ONTINUOUS C HANGEPlan ActionDelta value LP Variablem0 = 0saveHard startLP Constraintsstep0=0m0=0m00m1 = 1takeMortgage startm2 =14= m00step1step0 +step0 + 10m1= m00 + 1.(step1 step0 )0m01= m1 10step2m2lifeAudit startm3 =14saveHard end= step1 +=m01+14 .(step2step0 + 10 step1 + 12step1 )m02= m2step3step2 +m3= m02 + 41 .(step3 step2 )m5 = 066step3 +=m0334 .(step3= step1 + 12 step2 + 4step2 )m04lifeAudit end60= m3step4m4takeMortgage end0= step0 + 10 step1 + 12step2 + 4m03m4 = 346= m4step5step4 += step2 + 4m5= m04 0.(step3 step2 )0m05= m5Table 2: Variables constraints Borrower problemLP objective function minimise step n , last step plan far.purposes valid plan(S) function, LP built plan P reach state cannotsolved, prune state search space need consider further:path legal goal state. way, LP scheduler used replacementSTP order determine plan validity.8.2 Example: LP Borrower Problemorder illustrate LP construction plan consider example Borrower problem introduced Section 4.1. Recall one solution plan problem following structure:0:1:2:3:4:5:saveHard starttakeMortgage start longMortgagelifeAudit startsaveHard endtakeMortgage end longMortgagelifeAudit end.LP six-step Borrower solution plan contains variables constraints shownTable 2. six step variables represent time-stamps six snap-actions plan,variable represents money saved Borrower. initial state, = 0,27fiC OLES , C OLES , F OX & L ONGhence m0 = 0. Starting saveHard action instantaneous numeric effects, introducingconstraint m00 = m0 (if effect m, instance instantaneous increasesavings k, constraint would m00 = m0 + k). Due invariant conditionsaveHard action, savings remain zero, constraint m00 0 added: seenconstraint duplicated mi m0i execution saveHard action,ensure invariant continues hold. Notice, also, action takeMortgage started,invariant action (the savings level remains less equal maxSavings cap)also appears, applies values execution. Additional constraints capturediscrete change connecting value m0i mi . cases example valuesequal, one constraint shows discrete effect: m01 = m1 1 captures deductiondeposit caused initiating takeMortgage action.previously described, temporal constraints LP take two forms. First,constraints form step i+1 step + , forcing step i+1 follow step , enforcing sequencing snap-actions. Second, duration constraints restrict duration actions, e.g.step3 = step0 + 10 forces step3 (the end point saveHard) occurs precisely 10 units (theduration saveHard) step0 , start snap-action.final constraints consider modelling continuous numeric change. firstconstraint type gives value m1 execution saveHard startexecution takeMortgage start. constraint, m1 = m00 + 1.(step1 step0 ), basedvalue m1 , 1: action currently executing continuous changesaveHard, increases 1 per unit time. second constraint, m2 =m01 + 41 .(step2 step1 ), based value m2 (1 34 ) = 14 , found addingactive gradients actions started yet finished. illustratestwo actions active linear continuous effects variable simultaneously. NotesaveHard end applied (at step3 ) gradient continuous change (m4 ) becomes43 active continuous effect takeMortgage action.Solving temporal constraints problem without considering metric fluents yieldssolution step0 = 0, step1 = , step2 = 8 + 2, step3 = 10, step4 = 12 +step5 = 12 + 2. Unfortunately, proposal violates constraint m01 0, since:m01 = m1 1 = m00 + 1.(step1 step0 ) 1 = m0 + 1 = 0 + 1 = 11. constraint start time takeMortgage action cannot identifieddependent discrete initial effect action, active continuous effectsaveHard action invariant saveHard. simple example illustrates strengthusing LP perform scheduling alongside resolution numeric constraints:timestamps satisfy temporal numeric constraints.8.3 TemporalNumeric Searchperforming state-space search, state, S, snapshot world along plan trajectory, coming one action step another. absence continuous numeric change,valuations define known precisely: propositions hold, valuesnumeric variables v. presence continuous numeric change, however,hold: variable v undergoing continuous numeric change (or subject active durationdependent change) valuations state depend snap-actions applied far,28fiC OLIN : P LANNING C ONTINUOUS C HANGEtimes snap-actions applied much time passed sincelast action applied. Within representation state time-stamps snap-actionsplan fixed (during plan-construction, LP used confirm planscheduled subject current constraints), valuation numeric fluents constrainedwithin ranges determined constraints temporal variables interactionsthem.consequence flexibility commitment values temporal continuouslychanging variables, COLIN requires different state representation one used CRIKEY 3.Rather representing values numeric variables single vector v, use twovectors: vmax vmin . hold maximum minimum values, respectively,numeric variable S. computation bounds variables achieved using smallextension LP described Section 8.1. state S, reached plan P (wherelast step P ), add another vector variables LP, denoted vnow , another time-stampvariable, step . variables vnow represent values state variable point(at time step ) along state trajectory following . numeric variables time-stampconstrained additional action appended plan:must follow previous step, i.e. stepnow stepnmust precede coincide ends actions started yetfinished, i.e. estep(i), estep(i) stepvariable vnow vnow , compute value based continuous numericchange:vnow = vn0 + vnow (stepnow stepn )Finally, every invariant condition hv, {, =, }, w v + ci action startedyet finished:vnow {, =, }w vnow + cLP used find upper lower bounds variables. variables vnow vnow , two calls made LP solver: one objective set maximise vnow ,one minimise vnow . taken values vmax vmin S. simplest case, variable v subject (direct indirect) continuous duration-dependentchange, value v time-independent, vmax = vmin , value determinedsuccessive application effects actions P , i.e. mechanism usedCRIKEY 3, indeed classical (non-temporal) planning.Since upper lower bounds value variable, rather fixed assignment, action applicability function, get applicable actions(S), must modified. CRIKEY 3,action said applicable state preconditions satisfied. COLIN, definitionmeans numeric precondition satisfied different. preserve completeness,employ mechanism used metric relaxed planning graphs, discussed detailSection B. Specifically, numeric precondition w x c, calculate optimistic valuew x using upper bound v x corresponding weight w positive, or, otherwise,using lower bound. Then, resulting value greater equal c, preconditionconsidered satisfied. (As before, numeric conditions w x c, equivalent precondition appropriate form obtained multiplying sides inequality 129fiC OLES , C OLES , F OX & L ONGPlan ActionDelta value LP Variablem0 = 0saveHard startLP Constraintsstep0=0m0=0m00m1 = 1takeMortgage startmnow =14= m00step1step0 +step0 + 10m1= m00 + 1.(step1 step0 )0m01= m1 10step1 +stepnowmnow=m0nowm01+14 .(stepnow= mnowstep1 )6step0 + 10 step1 + 120606Table 3: Variables constraints first stages Borrower Problemconstraints form w x = c replaced equivalent pair conditions w x c,w x c.)test applicability action relaxed, serves filter, eliminating actionscertainly inapplicable. instance, precondition + b 3 could satisfied upperbounds b 2, even assignment timestamps actions within LP attain= 2 conflicts needed attain b 1. rely subsequent LP consistency checkdetermine whether actions truly applicable. Nonetheless, filtering applicable actionsbasis variable bounds state useful tool reducing number candidatesmust individually verified LP.8.3.1 E XAMPLE U SE B ORROWER P ROBLEMbriefly illustrate way variable constructed used contextBorrower problem. Consider situation selection first two actions (saveHard starttakeMortgage start). LP construction yields constraints shown Table 3. SolvingLP minimum maximum values stepnow gives values 1 + 10 respectively,meaning earliest time third action applied 1 + latest10.5 Similarly, solving LP minimum maximum values mnow gives bounds4 6. information could, principle, constrain actions applied currentstate.8.4 Comments LP EfficiencyLP solved every node search space, important process madeefficient possible. adding variable vectors LP step i, necessaryconsider state variable, v, become unstable prior step i, onefollowing effects acting it:1. direct continuous numeric change, i.e. changing v according gradient;5. practice, efficiency, COLIN actually solve LP minimum maximum values stepnow ,uses variable communicate constraints metric variables state.30fiC OLIN : P LANNING C ONTINUOUS C HANGE2. direct duration-dependent change, i.e. change v dependent duration action(whose duration non-fixed);3. discrete change, magnitude change based one variablesfalling either previous two categories.variables meet one conditions omitted LP, valuescalculated based successive effects actions applied step i, substitutedconstant within LP constraints referring them. reduces number state variablesconstraints must added LP also reduces number times LP mustsolved state find variable bounds: irrelevant variables eliminatedvector vnow . similar simplification that, applying plan a0 ...an1 reaches statevmin = vmax , continuous numeric change acting v, v become stable, i.e.value independent times assigned preceding plan steps. case, firststep k v becomes unstable, value v determined simple applicationdiscrete effects, hence v omitted vj , vj0 , n 1 < j.opportunity exploit LP solved state similar solvedparent state: represents plan, extra snap-action appended end.lower bounds time-stamp variables LP therefore based values computedparent states. Suppose state expanded reach state 0 applying snap action, a,step plan. point, LP corresponding plan built solvedobjective minimise step . Assuming plan indeed scheduled (if cannot,0 pruned successors generated it), value objective functionstored 0 lower bound time-stamp a. states subsequently reached 0 ,stored value used LP lower bound step appending actions planconstrain hence increase value step , never remove constraintsorder allow decrease.well storing lower bounds time-stamp variables, make use boundsvmin ,vmax state 0 generating successors it. state reached via planlength i, applying action leads state 0 new action step i+1 inheritsconstraints imposed previously step calculating variable bounds 0 . Therefore,values vmax vmin serve upper lower bounds (respectively) vi+1 LPbuilt determine feasibility 0 . Similarly, combine discrete numeric effectsvalues vmax vmin give bounds v0 i+1 . variable v subjecteffect, optimistically large (small) outcome effect computed basis vmax0 . Otherwise, variables uponvmin , taken upper (lower) bound vi+10discrete effect, vi+1 = vi .Finally, presence timed initial literals (TILs) allows us impose stricter boundstime-stamp variables. step j plan dummy action corresponding TIL time t,upper bound step , < j, lower bound step k , j < k (or estepvariable) + . Similarly, plan yet contain step corresponding TIL timet, upper bound step variables . Furthermore, TIL time correspondsdeadline deletes fact p present initial state, never added action,never reinstated TIL. case:plan step requires p precondition, step ;31fiC OLES , C OLES , F OX & L ONGestep end action end condition p, estep ;estep end action invariant condition p, estep t.9. Heuristic Computationsearch algorithms described far paper make use heuristic guide plannerefficiently search space towards goal. introduced necessary machinerysupport linear continuous numeric duration-dependent effects turn attentionconstruction informed heuristic face time-dependent change.Appendices B C revisit standard Metric-FF Relaxed-Planning Graph (RPG)heuristic Temporal RPG (TRPG) used CRIKEY 3, provide details approaches reference. depend initial construction reachability graph,based plan graph introduced Graphplan (Blum & Furst, 1995). graph consists alternating layers facts (f l) actions (al). TRPG, convenience, index layersearliest time could represent, although still enumerated consecutive integersfinitely many times relevant process construction. sectionexplain heuristic computation techniques introduced planners modifiedreason interacting temporalnumeric behaviour. describe two variants heuristic:basic version, active continuous change relaxed discrete step changes, refinedvariant relaxation replaced careful approximation continuousvalues. show, using Borrower example, benefits refined approach.heuristics based underlying use relaxed plan step-count. use relaxedplan makespan tie-breaker ordering plans step-count. Step-count dominatesheuristic first priority find feasible solution planning problem meansattempting minimise number choices must made resolved search.course, emphasis rapidly finding feasible plan compromise quality plan,particularly problems step-count poorly correlated makespan. Subsequentattempts improve quality initial feasible solution, either iteratively improvingsolution search using bound derived feasible solution prunesearch space, possible, consider work.9.1 Basic Integrated Heuristic Computation Continuous Numeric Effectsfirst version COLIN (Coles, Coles, Fox, & Long, 2009b) introduced three significant modifications TRPG used CRIKEY 3, order generate heuristic values presencecontinuous duration-dependent effects. first modification simply equips heuristicmeans approximate effects continuous change.action continuous effect equivalent dvdt += k relaxed instantaneousstart effect hv, +=, k dmax (a)i. is, effect changing variable treatedintegral effect upper bound duration action appliedstart action. ensures behaviour relaxed, contrast to, say,applying effect end action. dmax (a) calculated pointaction added TRPG, based maximum duration constraints refervariables cannot change time (that is, state-independent).32fiC OLIN : P LANNING C ONTINUOUS C HANGEconstraints exist, duration allowed infinite (and variables affected continuouseffects action similarly uninformed bounds).action discrete duration-dependent effect variable v then, calculatingmaximum (minimum) effect upon v (as discussed, non-temporal case, Appendix B), ?duration variable relaxed whichever dmin(a) dmax (a) giveslargest (smallest) effect. Relaxation effect achieved without changing timing,associated start end action indicated action specification.second modification affects action continuous numeric effect variable either end precondition invariant refers numeric variable.invariant end precondition places constraint way process governedaction affect value variable, constraint reflected corresponding upperlower bounds value variable. Specifically, action decreases v rate kinvariant end precondition v c, upper bound v end action mustleast k.(dmin(a) elapsed (a)) + c, elapsed (a) maximum amount timecould executing state evaluated (0 currently executing, otherwise,maximum entries E). condition ensures variable could achievenecessary value support application action. might appear strange boundset higher c, reason relaxation accumulates increase effects ignoresdecrease effects assessing upper bound, necessary, end action,accumulated increases value variable allow outstanding consumptionorder still meet c bound end action. corresponding conditionrequired action increases v rate k, invariant end precondition v c,lower bound v cannot k.(dmin(a) elapsed (a)) + c. conditionsadded explicit additional preconditions aa purposes constructing TRPG.third modification deals problem constructing appropriate initialisationbounds numeric variables first layer TRPG. CRIKEY 3 valuesinitialised actual values metric variables, since values current statechange time passes without actions applied. true COLIN, sinceactions started, yet finished, govern process, cause variableschange simply consequence time passing. basic heuristic proposed reliesable integrate continuous numeric change, determine variable bounds fl (0.0)two stages. First, bounds variable v set according obtained LPSection 8.3. Then, entry e E, corresponding start action, a,continuous effect v positive gradient k, upper bound v f l(0.0) increasedk.remaining(e). Here, remaining(e) maximum amount time could elapsestate evaluated future end snap-action paired start event e. maximumremaining execution time calculated subtracting lower bound amount timeelapsed since start action maximum duration. casegradient negative, lower bound decreased.9.2 Refined Integrated HeuristicTime-dependent change arises two sources: continuous numeric effects, initiated start snapactions, discrete duration-dependent effects apply either end durative actions.33fiC OLES , C OLES , F OX & L ONGpurposes refined heuristic described section, treat continuous effectsdiscrete duration-dependent effects ends actions way, attachingcontinuous linear effect acting relevant variable effects appropriate snap-action,a, denoting set continuous effects g(a). continuous effects, cont(a), initiateda` , cont(a) g(a` ). is, gradient effects start include continuouseffects a. duration-dependent effects end snap-action aa split effect twoparts:discrete effect aa , hv, {+=, -=, =}, w v + k.dmin(a) + cigradient effect v, added g(aa ). effect defined hv, ki original effect usedoperator += = otherwise, hv, ki.Thus, instantaneously, end aa , effect available assuming smallest possibleduration used. executes greater duration, continuous effect appliedgradient change taken coefficient k ?duration variablecorresponding effect a.Unfortunately, treatment proposed cannot applied duration-dependent start effects, since effects always available start action, regardless duration. Thus,employ approach taken basic heuristic used COLIN: calculating maximum (minimum) effect a` affected variable, v, ?duration variable substitutedwhichever dmin(a) dmax (a) gives largest (smallest) effect.collection linear continuous effects, g(a), associated snap-action,a, adjust construction TRPG. First, identify, variable, v, associatedmaximum rate change, vmax (t), following layer al(t). set sumpositive rates change, affecting v, snap-actions al(t):v max (t) =XXaal(t)hv,kig(a)kdefinition relies restriction one instance action executetime. restriction hold, clear finite bound p(a) number instancesaction execute concurrently, incorporate calculation v max (t)follows:XXv max (t) =p(a)kaal(t)hv,kig(a)finite bound exists, action could, principle, applied arbitrarily many timesparallel hence set v max (t) = .6 Following layer al(t) v max (t) =longer need reason upper bound continuous change v since upper boundv become immediately layer. noted degradationbehaviour will, worst case, lead heuristic behaviour basic heuristicwhere, again, arbitrarily many copies action execute concurrently, magnitudeincrease decrease effects becomes unbounded. extension heuristic consider6. note that, experience, presence infinitely self-overlapping actions continuous numeric changeoften bug domain encoding: difficult envisage real situation parallel productionunbounded.34fiC OLIN : P LANNING C ONTINUOUS C HANGEcontinuous effects refined way worsen guidance situation.remainder section, consider variables whose values modified actionsfinite bounds number concurrently executing copies allowed.Armed upper bound value rate change variable following layer al(t),deduce maximum value variable time t0 > t, simply applyingappropriate change maximum value variable time t. remaining challengedecide far advance t0 construction TRPG. construction TRPGCRIKEY 3 time constrained advance next action end point, dependingwhether new facts available following recent action layer (lines 2934 Algorithm 2). order manage effects active continuous processes, add third possibility:time advance earliest value accumulated effect active continuous changevariable satisfy previously unsatisfied precondition. set preconditions interestalways finite, so, assuming variable subject non-zero effect, boundrelevant advance always defined (or, set preconditions empty, advance required).compute value time follows. numeric precondition may writtenconstraint vector numeric variables, v, form w v c, vectors constants wc. define function ub follows:X w[i] y[i] w[i] 0ub(w, x, y) =w[i] x[i] otherwisew[i]wupper bound w v t0 then: ub(w, vmin (t0 ), vmax (t0 )).earliest point numeric precondition w v c become satisfiedsmallest value t0 ub(w, vmin (t0 ), vmax (t0 )) c.example, suppose action precondition x + 2y z c, w =h1, 2, 1i (assuming x, z numeric fluents case). Substitutingprevious equation yields:ub(h1, 2, 1i, hx, y, zimin (t0 ), hx, y, zimax (t0 )) = 1.xmax (t0 ) + 2.ymax (t0 ) 1.zmin (t0 )= 1.(xmax (t) (t0 ) + xmax (t + ))+2.(y max (t) (t0 ) + ymax (t + ))1.(z min (t) (t0 ) + zmin (t + ))(The values x, z based starting points + accountsinstantaneous changes triggered actions al(t).) value t0 produced computationinfinite, maximum possible rate increase expression x + 2y z must zero.7Otherwise, t0 time new numeric precondition first become satisfied dueactive continuous effects and, earlier earliest point action end pointapplied, next fact layer TRGP f l(t0 ).9.2.1 MPROVING B OUNDS VARIABLES FACT-L AYER Z EROPreviously, setting bounds fact-layer zero could thought consisting two stages:finding initial bounds using LP then, passage time could cause boundsdiverge due active continuous numeric change, integrating change prior setting7. find t0 requires simple rearrangement formula extract t0 directly.35fiC OLES , C OLES , F OX & L ONGbounds layer zero TRPG. explicit model numeric gradients planninggraph, reconsider approach. intuition behind new approachfollows:1. variable v, create associated variable tnow (v) LP, solve LPminimise value variable.2. Fixing value tnow (v) lower-bound, maximise minimise value v findbounds point used bounds v fl (0.0).3. v > 0 current state, vmax (t) values TRPG offset v or,similarly, v < 0, vmin (t) values offset.first steps based ideas described Section 8.3, process subtlydifferent trying determine bounds v given point time, ratherappear reachable. before, tnow (v) must still come recent plan stepused determine value v. reflected pair constraints:tnow (v) stepvnow = vi0 + vnow (tnow (v) step )Additionally, since variable associated single v, ratherappropriate v, constrain if, necessarily, v cannot referred (eitherprecondition, duration within effect) least certain steps plan, ratherweaker requirement recent step. purposes, observeactions referring v require, delete add fact p, possible interaction prequire-delete-add form, tnow (v) must come plan step adds p.formally, require-delete-add idiom holds p p true initial state, actionpreconditions/effects p, interaction action p characterisedone following patterns:+1. p pre ` (a), p eff` (a), p eff ` (a)+2. p pre (a), p eff(a), p eff (a)+3. p pre ` (a), p eff` (a), p eff (a)(An action may exhibit either first two interactions, third.)LP variable corresponding point p added, denote step p ,determined one two ways. First, p present state evaluated, step p LPvariable corresponding plan step recently added p. Otherwise, case 4 above,know p eff +(a) action currently executing. case, step p LPvariable estep corresponding end a. defined variable, add constraintLP:tnow (v) step p +Solving LP objective minimise tnow (v) finds earliest possible timev referred to. Then, fixing tnow (v) minimised value, minimise maximise36fiC OLIN : P LANNING C ONTINUOUS C HANGEbounds vnow . gives us bounds v appropriate early possibleactions plan far.obtained variable bounds LP must, before, account factpassage time causes bounds change active continuous numeric change. Whereasintegrated change prior TRPG, mechanism handling gradients directly TRPG expansion. Thus, start-event-queue entry e E correspondingstart action, A, continuous effect v positive (negative) gradient k,add gradient effect upper (lower) bound v TRPG. previously restricted integrated effect e remaining(e), maximum remaining time actionmust end, limit long gradient effect active: starts al(0.0) finishesal(remaining(e)). Then, given fact layer value vmax (t) updated accordingly:vmax (t)+=XX{k | hv, ki g(op(e)) k > 0 remaining(e)}eESimilarly, vmin(t) amended account effects hv, ki, k < 0.9.3 Using Two Variants Integrated Heuristic Borrower Problemillustrate computation two heuristic functions choice point Borrowerproblem. example shows refined heuristic guides planner shorter makespanplan basic heuristic, improved heuristic information leads selectionbetter choices helpful actions. Consider situation following execution first action,saveHard start. Figure 7 (top) shows TRPG relaxed plan constructed using basicheuristic.heuristic generates cost state 5: four actions shown relaxed plan,together extra one end saveHard action already started. relaxed plangenerates two helpful actions, start lifeAudit start takeMortgage short. attempt start lifeAudit action quickly dismissed temporally inconsistent, dependingboughtHouse becoming true ends, helpful action chosen. Unfortunately, action selected interaction saving process depositrequirement (at least five savings must acquired) forces action start earliertime 5. constraint invisible TRPG, continuous effect saveHardabstracted start effect, full ten savings therefore appear available immediately.plan constructed using short mortgage, introducing second saving actionshown lower plan Figure 3. start short mortgage pushedlate life audit cannot overlap end first saveHard action finishmortgage action.lower part Figure 7 shows happens refined heuristic used solveproblem. saveHard action starts before, time heuristic relaxbehaviour continuous savings process long mortgage, requires smaller depositinitiate it, becomes available short mortgage. consequence this, relaxedplan selects long mortgage, action starts early enough life audit overlapend end saveHard action. planner correctly guided optimal plan,shown top Figure 3. crucial difference two heuristics, refinedheuristic able access accurate information value savings timepoints37fiC OLES , C OLES , F OX & L ONG0: saveHard_startsaving10lifeAudit_starttakeMortgage_start shortmoney : [20,20]money : [ ,10]saveHard_endcanSavetakeMortgage_start long10+10+2lifeAudit_endsaveHard_startboughtHousehappytakeMortgage_end short0: saveHard_startsaving15lifeAudit_starttakeMortgage_start longmoney : [ ,t]money : [0.75t,10]takeMortgage_start short1210money : [t,10]12+lifeAudit_endsaveHard_endcanSavetakeMortgage_end longboughtHousehappyFigure 7: TRPG relaxed plan Borrower problem, following initial executionsaveHard start time 0, constructed using original version COLIN (topdescribed Section 9.1) revised version (bottom described Section 9.2).Action layers depicted rounded rectangles fact layers ovals. actionlayers labelled times constructed reachability analysis.start savehard action. leads finer-grained structure TRPG,seen fact six action layers arrival goal, rather fourcase basic heuristic used. estimated makespan final plan 12 + ,makespan according basic heuristic 10 + 2. basic heuristic leads non-optimalsolution requires extra saveHard action, giving solution makespan 20 + 2,contrast makespan 12 + optimal plan.benefit refined heuristic, extra work involved constructing modifiedTRPG, better helpful actions chosen makespan estimate therefore accurate. choice similar length plans made based makespan. TRPG, constructedrefined heuristic Borrower problem, even contain short mortgage actionearly enough layer considered relaxed plan.38fiC OLIN : P LANNING C ONTINUOUS C HANGE10. Improving Performancesection present two techniques use improve performance COLIN. firsttechnique, described Section 10.1, generalisation earlier exploitation one-shot actions (Coles et al., 2009a) situation encapsulate continuous processes, leadingfaster plan construction problems action types. second technique, describedSection 10.2, exploits LP defines constraints within final plan optimise planmetric. leads better quality plans many cases.10.1 Reasoning One-Shot Actionsearlier work (Coles et al., 2009a) observed common modelling deviceplanning domains leads use actions applied once. call actionsone-shot actions. arise, particular, collection resourcesused once. key difference one-shot actions imply TRPG continuouseffects generated one-shot actions lapse certain point reached:one-shot action continuous numeric effect v, a` first appears action layeral(t), gradient v due effect finishes, latest, al(t + dmax (a)).end aa one-shot action duration-dependent effect v, (implicit)continuous effect acting v finishes, latest, layer al(t + dmax (a))termination point implied, cases, fact action one-shot.modify TRPG construction reflect restrictions extending data recordedaction layer include, snap-action action a, maximum remaining executiontime a, denoted rem(t, a). one-shot actions, layer al(t) a` first appears,rem(t, a` ) = dmax (a), aa first appears, rem(t, aa ) = dmax (a) dmin(a). actionsone-shot rem(t, a` ) rem(t, aa ) initialised . make three minorchanges layer update rules accommodate rem values. First, calculating activegradient variable v following action layer al(t):XXv max (t) =p(a)kaal(t)|rem(a,t)>0hv,kig(a)seen, subset actions execution time remaining considered. Second,next action layer al(t + t) following al(t), value positive rem decrementedt, amount time elapsed since previous layer. Third, consequence this,additional criterion must considered calculating time-stamp next fact-layer, t0 ,described Section 9.2. Since time remaining complete action may expire, mayneed insert additional fact layer denote point rem value reaches 0continuous effects acting one variables need recalculated. time-stampearliest layer is:t0 = + min{rem(t, a) > 0 | al(t)}One-shot actions exploited still improving upper bound durationaction a. case actions state-dependent duration constraints (i.e. upperbound calculated based variables subjected effects actions), dmax (a) may39fiC OLES , C OLES , F OX & L ONGgross over-estimate duration a. Suppose maximum duration boundedformula w v + c. layer al(t) a` appears, compute maximum durationa, started layer, based variable bounds recorded f l(t). coulduse value determine bound remaining execution time a. However, futurelayer f l(t0 ), variable bounds might changed, beginning al(t0 ), calculatingmaximum duration based f l(t0 ), would allowed execute possibly longer periodtime, allowing continuous effects persist longer.remain faithful relaxation, possibility exploiting increased duration(by starting t0 ) must included TRPG, well allowing possibility startt, thereby obtaining effects sooner. Therefore, one-shot action allowed startearliest layer al(t) preconditions satisfied, giving initial maximum durationdmax (a, t) based fact later f l(t). But, later fact layer f l(t0 ) admits greater duration(dmax (a, t0 ), value dmax action layer t0 ), remaining execution timereconsidered. First, simple case, variables duration constraint changed f l(t0 ),subject active continuous effects. case, apply pair dummy effectsfact layer t00 = t0 + dmax (a, t):hrem(a` , t00 ) += (dmax (a, t0 ) dmax (a, t))ihrem(aa , t00 ) += (dmax (a, t0 ) dmax (a, t))i.Note increase rem values delayed layer t00 because, order benefitlonger duration a, must started layer t0 .complex case, variables duration constraint changed f l(t0 )duration also affected continuous effects variables depends on.situation, subsequent fact layer might admit marginally bigger duration last.avoid recalculate new duration repeatedly, schedule pair dummy effectsbased global, layer-independent, maximum value duration a:hrem(a` , t00 ) += (dmax (a) dmax (a, t))ihrem(aa , t00 ) += (dmax (a) dmax (a, t))i.relaxation weaker might be, efficient compute.10.2 Plan Optimisationplan metric specified PDDL 2.1 problem files indicate measure qualityuse evaluating plans. metric expressed terms task numeric variablestotal execution time plan (by referring variable total-time). use LPCOLIN offers opportunity optimisation plan respect metric: plan0consisting n steps, numeric variables vn1end plan, stepn1time-stamp final step (i.e. action dictating makespan plan) LP objectiveset minimise function these. LP must solved minimise time-stamplast action (the makespan plan) order arrive lower bound time nextaction. However, also solved optimise plan metric.40fiC OLIN : P LANNING C ONTINUOUS C HANGEAlthough possible consider ways use metric-optimising LP value planconstruction, guide search, focussed much limited, less costly, use:attempt post hoc optimisation, attempting exploit flexibility temporal structurefinal plan optimise plan quality last stage plan construction.order post hoc optimisation useful, planning problems must propertypossible vary quality metric plan scheduling actions occurdifferent times. possible wide range interesting situations, scheduling aircraftland close given target time possible, taking images satellites certain times dayview clearer, minimising wasted fuel penalising time elapsing startingengine plane take off. last represents general class problemsmay desirable minimise amount time two activities: different metrictotal time taken plan execution. capture interesting cases, first extendlanguage supported COLIN allow limited subset ADL conditional effects, allowconditions action executed vary effects action metric valueplan. Second, discuss MILP built, based LP described Section 8.1,support post hoc plan optimisation.planner handles conditional effects standard compilation. However, conditionaleffects metric variables appear plan quality metric preconditionsactions dealt differently. call variables metric tracking variablesexploit fact rescheduling plan affect values variables without changingvalidity plan. example shown Figure 10.2 action land airplane,conditional effects metric tracking variable total-cost. domain structuredland actions must start beginning plan, end points represent actuallanding times aircraft problem. duration actions set correspondearliest latest possible points plane could land. seen, propositionaleffects action whether plane lands early late: plane landed,longer flying. However, numeric effects, effects metric tracking variabletotal-cost, depend duration action and, particular, whether plane landedearly late. plane lands early, penalty paid certain rate per unit time planelands desired target value. plane lands late, fixed cost paid, additionpenalty (at different rate) per unit time plane lands desired target value.considering action single action, pair conditional effects, planner decideupon actions needed construct sound plan (in planes landed) whilstleaving subsequent optimisation phase decision whether plane landedearly, late, time.general, straightforward exploit LP described Section 8.1 attempt reschedule actions plan optimise value plan metric (provided metric functionlinear). However, plan contains actions conditional effects metric tracking variables,becomes possible exploit representation effects extended LP, using integervariables, order offer powerful optimisation step. conditional effect eitheractivated not: introduce 0-1 variable represent case effect.variable connected corresponding constraints determine whether conditionassociated effect true not.deal two kinds constraints 0-1 variables MILP encoding plan optimisation problem: one special case actions scheduled fixed time-windows41fiC OLES , C OLES , F OX & L ONG(:durative-action land:parameters (?p - plane ?r - runway):duration (and (>= ?duration (earliest ?p)) (<= ?duration (latest ?p))):condition(and(at start (takeOff))(over (flying ?p))(at end (scheduled ?p ?r))):effect(and(at start (flying ?p))(at end (landed ?p))(at end (not (flying ?p)))(when (at end (< (?duration) (target ?p)))(at end(increase (total-cost)(* (earlyPenaltyRate ?p) (- (target ?p) ?duration)))))(when (at end (> ?duration (target ?p)))(at end(increase (total-cost)(+ (latePenalty ?p)(* (latePenaltyRate ?p) (- ?duration (target ?p)))))))))Figure 8: PDDL Domain Conditional Effects Airplane Landing Problem. literaltakeOff special proposition manipulated dummy action force landingactions anchored point time.governed timed initial literals affect whether conditions satisfiedcase satisfaction conditions determined status continuous effectscontrolled actions plan (so, example, cost action might depend whethercontinuously changing value passed threshold time action executed).cases handled straightforward encoding linkage value0-1 condition variable corresponding conditions (the details given Appendix D).also extended conditional effects allow affect ?duration variableaction, similar devices encoding MILP.MILP solved single final step construction plan, optimisingplan metric quality rescheduling actions best exploit precise timing actionsinteraction, limited conditional effects, plan quality.11. Continuous Linear Benchmark DomainsCOLIN one first planners support PDDL 2.1 models featuring continuous linear changeduration-dependent effects8 currently benchmarks available exploit fea8. Specifically, duration-dependent effects depend non-fixed durations.42fiC OLIN : P LANNING C ONTINUOUS C HANGEtures. support evaluation foster future comparisons planners designedsolve problems, produced number domains features9 .first domains extension Metric Time variant Rovers domain,2002 International Planning Competition (IPC 2002) (Long & Fox, 2003b). focusaction navigate, responsible moving rover one location another. originalmodel, discrete effect, start action, decrease energy level rover8 units, coupled precondition must least 8 units energy available.replace continuous numeric effect energy, condition energymust least zero action. duration original action specified 5,use effect gradient 8/5. Written thus, action net effect conditions:energy decreased 8 units, must become negative. continuous change modelsaccurately use power navigate action: whilst power use may actually linear,closer linear instantaneous. make model still realistic,introduce new action domain: journey-recharge, shown Figure 9. exploitinginteraction continuous numeric effects variable, use action captureoption rover tilting solar panels face sun whilst navigating two points.account power use reorienting solar panels, start end action, 0.2units energy used. benefit consumption that, whilst action executing,energy rover increased according constant positive gradient. final modificationdomain, alter duration constraint existing recharge action. originalencoding, constraint is:(= ?duration (/ (- 80 (energy ?x)) (recharge-rate ?x))).forces duration action sufficient restore level charge 80 (full capacity). new formulation, replace = <= duration constraint specifiesmaximum duration battery charged: need restored full capacityevery time action applied. Following three modifications, domain usedstandard IPC 2002 benchmark problems. addition this, also createdproblems considering single rover, issue battery power management muchgreater importance.next domains extension Time variant Satellite domain,taken IPC 2002. Here, continuous variant domain, make three key changesdomain model. First, original formulation, proposition used indicate whetherpower available operate instrumentation given satellite. Switching instrumentrequired deleted fact, switching added again. Thus,scope parallel power usage, instrumentation effectively used unit power. Now, usenumeric variable represent power, preconditions effects variable replacingpreconditions effects proposition previously used. Second, exploiting potentialdiffering power requirements, instruments operated one two modes:cooled, uncooled. cooled mode, active sensor cooling used reduce sensor noise, enablingimages taken less time. cooling, however, requires additional energy. Third,finally, compulsory sunrise phase start plan, satellites9. P DDL domain problem descriptions evaluation tasks available online appendix maintainedJAIR paper.43fiC OLES , C OLES , F OX & L ONG(:durative-action journey-recharge:parameters (?x - rover ?y - waypoint ?z - waypoint):duration (>= ?duration 0.2):condition (and (over (moving ?x ?y ?z))(over (<= (energy ?x) 80))(at start (>= (energy ?x) 0.2))(at end(>= (energy ?x) 0.2))):effect (and (at start (decrease (energy ?x) 0.2))(increase (energy ?x) (* #t (recharge-rate ?x)))(at end(decrease (energy ?x) 0.2))))Figure 9: journey-recharge action continuous-numeric Rovers domainmove shaded planet, direct sunlight. leads increasepower availability, modelled linear continuous numeric effect attached action, sunrise,must applied. Interaction effect preconditions powering instrumentsensures operated sooner power available. problem files usedomain slightly modified versions IPC competition problems, updated define poweravailability numeric variable encode power requirements cooled uncooledsensor operation. problems domain characteristics similarBorrower problem used running example.exploring use continuous numeric effects, next domain models operationscooperating Autonomous Underwater Vehicles (AUVs). AUVs move waypointsunderwater perform two sorts science gathering operations. first taking watersample given waypoint, performed AUV appropriate location,whose water sample chamber empty. second taking image target interest.requires two AUVs cooperate: one illuminate target torch, one takeimage it. AUV domain inspired problem described Maria Foxinvited lecture 2009 International Conference Automated Planning Scheduling.data acquired, must communicated ship surface. SatelliteRovers domains, AUVs energy-constrained finite battery powerpower usage actions continuous throughout execution. interesting continuousnumeric aspects domain arise use model drift. introduce variablerecord far AUV drifted nominal position, update two ways. First,activity plan contained within action drift small, positive continuous numericeffect drifted distance. Second, add localise action sets drifted distancezero, duration (and hence energy requirements) depending drifted distance priorapplication. drifting affects domain actions. simplest case, samplewater take image given location, AUV cannot drifted two metres, henceintroducing need first localise case. interestingly, AUV shiningtorch, drifting affects much light falling target. Thus, shine-torch actionAUV ?v three effects amount light falling given target ?t:44fiC OLIN : P LANNING C ONTINUOUS C HANGEstart: (increase (light-level ?t) (- 1000 (distance-from-waypoint ?v)))throughout: (decrease (light-level ?t) (* #t (fall-off)))end: decrease (light-level ?t) remaining contribution ?v makingillumination.constant (fall-off) pessimistically derived formul involving inverse-squarelaw, giving linear approximation decay illumination levels due drift. Then,take-image action itself, duration function (light-level ?t): less light available, longer requires take image.final domain use Airplane Landing domain (Dierks, 2005), first posed challenge Kim Larsen invited lecture 2009 International Conference AutomatedPlanning Scheduling. problem models scheduling landing aircraft airportrunway. plane, three landing times specified: earliest possible landing time,latest possible landing time, target (desired) landing time. Since time must allowedairplanes clear runway landed, use runway heavily subscribed resource, possible planes land ideal time. Planes can, therefore,land early late, incurs penalty. penalty modelled duration-dependenteffect, shown earlier paper (Figure 10.2 Section 10). able constructset airplane landing problems using real data Edinburgh Airport arrivals board. Resultsrunning COLIN problems reported Section 12.12. EvaluationCOLIN temporal planner, able solve problems required concurrency, handlediscrete continuous metric variables. first question address costlyextension underlying CRIKEY 3 system allow COLIN manage continuous effects? COLINparticularly powerful planner general PDDL 2.1 planners similarexpressive power available comparison continuous problems. However, extensionsnecessary support continuous reasoning add overhead cost solving problemscontinuous effects. compare performance COLIN temporalplanners selection temporal problems without continuous effects (Section 12.1) orderevaluate much overhead paid COLIN setting managing (redundant) structures,comparison state-of-the-art planners pay price.move considering performance COLIN problems continuous dynamics. second question is: much improvement obtain using refinedheuristic instead basic heuristic, dealing problems continuous change?planners discussed Section 7 able scale large complex problems, comparetwo versions COLIN. present performances new benchmark problems continuous processes, setting foundation future comparative evaluation alternative approachesproblems.third question considered concerns quality solutions produced COLIN,comparison optimal solutions found. COLIN satisficing plannerperform efficiently wide range continuous planning problems, interestedunderstanding much solution quality must sacrificed order obtain efficiencyachieved COLIN.45fiC OLES , C OLES , F OX & L ONGFinally, consider question: expensive move solving STP (sufficient purely discrete temporal planning) solving LP (necessary handling continuouseffects)? particular, practical solve multiple LPs performing heuristic state evaluations?Since LP construction solution central architecture COLIN importantrelied upon scale appropriately range complexity problems COLINexpected solve.following experiments consider large number domains domain variants.temporal comparisons use Simple Time Time variants Depots, Driverlog, Rovers,Satellite Zeno, IPC 2002, Airport Pipes-No-Tankage IPC 2004.Airport variant used Strips Temporal variant.comparisons basic refined heuristics continuous domains, usenew continuous benchmark domains introduced Section 11: Airplane Landing, Rovers, SatelliteCooled (the Satellite variant sensor cooling) AUV domain.post-hoc optimisation experiments use Airplane Landing problem, Cafe domain introduced empirical analysis CRIKEY 2 (Coles, Fox, Halsey et al., 2008), variantAirport amount fuel burned minimised, version Satellite timewindows, rewards obtained scheduling observations tighter windows.cases use competition benchmark sets instances available. continuous Rovers Satellite domains used IPC 2002 Complex Time problem sets.instances work continuous domain variants possible get better makespan plansthem, respecting continuous dynamics, possible instancessolved using discrete domain variants. generated increasing sized instances Airplane Landing domain number planes landed increased (in nth instanceproblem, n planes must landed). wrote problem generator AUV domainincreases number AUVs, waypoints goals instances (they range 2 AUVs, 4waypoints 1 goal, 6 AUVs, 16 waypoints 6 goals). experiments run 3.4GHzPentium machine, limited 30 minutes 1GB memory.12.1 Comparison Existing Temporal Plannerstemporal planners actually solve full range temporal problems. alreadyobserved, many temporal planners cannot solve problems required concurrency. Even withinclass problems required concurrency, easier problems, solvedleft packing actions within plan harder ones possible. leftpacking mean actions must executed concurrently actions planstarted time other. property means approach adopted Sapa,extending forward search include choice either start new action else advance timeearliest point currently executing action terminates, sufficient solve problem.contrast, problem cannot left packed require possibility advancing timeintermediate point execution action order coordinate correct interleavingactions it. describe problems requiring temporal coordination. Oneplanners also handle problems requiring temporal coordination LPG-s (Gereviniet al., 2010).therefore compare COLIN LPG-td, LPG-s, Sapa temporal baseline planner developed temporal satisficing track 2008 International Planning Competition. Neither46fiC OLIN : P LANNING C ONTINUOUS C HANGEDepots Simple TimeDriverlog Simple Time100Colin Solution Time (s)Colin Solution Time (s)1001010.11010.1LPG-TD usedTemp-Baseline used0.010.010.1110Best Solution Time (s)LPG-TD usedLPG.s usedTemp-Baseline used0.010.011000.1Rovers Simple Time100Satellite Simple Time100Colin Solution Time (s)100Colin Solution Time (s)110Best Solution Time (s)1010.11010.1LPG-TD usedTemp-Baseline used0.010.010.1110Best Solution Time (s)LPG-TD usedTemp-Baseline used0.010.011000.1110Best Solution Time (s)100Zeno Simple Time1000Colin Solution Time (s)1001010.10.010.01LPG-TD usedTemp-Baseline used0.1110Best Solution Time (s)1001000Figure 10: Comparison time taken solve problems simple temporal planning benchmarks.COLIN compared best LPG -td, LPG .s, Sapa temporal baseline planner,problem file shape colour points indicate plannerbest therefore used plot. Planners appearing particular datasetbest problems collection.temporal baseline planner, Sapa LPG-td solve problems requiring kind temporalcoordination. temporal baseline planner compiles away temporal information, using action47fiC OLES , C OLES , F OX & L ONGDepots TimeDriverlog Time100Colin Solution Time (s)Colin Solution Time (s)1001010.11010.1LPG-TD usedTemp-Baseline used0.010.010.1110Best Solution Time (s)LPG-TD usedTemp-Baseline used0.010.011000.1Rovers Time100Colin Solution Time (s)Colin Solution Time (s)100Satellite Time1001010.11010.1LPG-TD usedLPG.s used0.010.01110Best Solution Time (s)0.1110Best Solution Time (s)LPG-TD usedLPG.s usedTemp-Baseline used0.010.011000.1110Best Solution Time (s)100Figure 11: Comparison time taken solve problems complex temporal planning benchmarks (first set). COLIN compared best LPG-td, LPG.s, Sapa temporalbaseline planner, problem file shape colour points indicatebest. Planners appearing particular dataset bestproblems collection.compression, solves problems non-temporal metric propositional problems.solutions found, using Metric-FF core planning system, temporal informationreintroduced annotating plan suitable timestamps based critical path analysis.details published planner, source code brief information availableIPC 2008 web site. approach cannot therefore solve problems required concurrency, fast effective simpler problems temporal actions sequenced.straightforward identify many cases action compression applied safelyanalysis implemented COLIN reduce overhead reasoning action end pointsunnecessary. Therefore, behaviour temporal baseline planner similarCOLIN actions safely compressed. Figures 10, 11 12 show CPU timecomparisons COLIN best performances Sapa, LPG-td, LPG-s temporal baseline planner, across wide representative collection temporal benchmark domains.Figure 10 shows performance simple temporal problems, action durations fixed,Figures 11 12 show results complex temporal problems, including48fiC OLIN : P LANNING C ONTINUOUS C HANGEAirport Strips Temporal1000100100Colin Solution Time (s)Colin Solution Time (s)Zeno Time10001010.10.010.01110Best Solution Time (s)10010.1LPG-TD usedLPG.s used0.1100.010.011000LPG-TD usedTemp-Baseline used0.1Pipes No-Tankage Temporal110Best Solution Time (s)1001000Pipes Tankage Temporal1000100Colin Solution Time (s)Colin Solution Time (s)1001011010.10.1LPG-TD usedTemp-Baseline used0.010.010.1110Best Solution Time (s)0.010.01100LPG-TD usedTemp-Baseline used0.1110Best Solution Time (s)1001000Figure 12: Comparison time taken solve problems complex temporal planning benchmarks (second set). COLIN compared best LPG-td, LPG.s, Sapa temporal baseline planner, problem file shape colour points indicatebest. Planners appearing particular dataset bestproblems collection.duration actions determined context executed (although noneaction effects depend this), problems metric variables. None problemsfeature required concurrency forms temporal coordination. figures, plannersappearing dataset best problems domain.Analysis Figures 1012 shows COLIN indeed pay overhead computation timesolution temporal problems feature continuous dynamics. overhead particularly significant simple temporal problems interesting temporal structuretemporal baseline planner tends perform well. overhead paid COLIN lowercomplex temporal problems, temporal reasoning required sometimes challenging. makespan results Figures 13, 14 15 show COLIN produces good qualityplans, especially complex temporal problems, although temporal baseline planner stillcompetitive terms CPU time makespan. suggests temporal structure,even complex temporal benchmarks, quite simple planner well ignoringtemporal structure present, rather trying reason generating plans.49fiC OLES , C OLES , F OX & L ONGDepots Simple TimeDriverlog Simple Time100300250Colin Solution QualityColin Solution Quality80604020LPG-TD usedLPG.s usedSapa usedTemp-Baseline used00204060Best Solution Quality80200150100LPG.s usedSapa usedTemp-Baseline used500100050100150200Best Solution Quality300Satellite Simple Time400400350350300300Colin Solution QualityColin Solution QualityRovers Simple Time250250200150100250200150100LPG-TD usedLPG.s usedSapa usedTemp-Baseline used500050100150200250Best Solution Quality300350LPG-TD usedLPG.s usedTemp-Baseline used500400050100150200250Best Solution Quality300350400Zeno Simple Time6000Colin Solution Quality5000400030002000LPG-TD usedLPG.s usedSapa usedTemp-Baseline used1000001000200030004000Best Solution Quality50006000Figure 13: Comparison plan quality simple temporal planning benchmarks. COLIN compared best LPG-td, LPG.s, Sapa temporal baseline planner, problem file shape colour points indicate best. Plannersappearing particular dataset best problemscollection.detailed results experiments, showing raw runtime quality comparisons planners used experiment, presented Appendix E.50fiC OLIN : P LANNING C ONTINUOUS C HANGEDepots TimeDriverlog Time4001000350800Colin Solution QualityColin Solution Quality300250200150600400100200LPG-TD usedLPG.s usedSapa used500LPG-TD usedLPG.s usedSapa usedTemp-Baseline used0050100150200250Best Solution Quality3003504000200Rovers Time400600Best Solution Quality8001000Satellite Time400600350500Colin Solution QualityColin Solution Quality300250200150400300200100LPG-TD usedLPG.s usedSapa used50LPG-TD usedLPG.s usedSapa usedTemp-Baseline used10000050100150200250Best Solution Quality3003504000100200300400Best Solution Quality500600Figure 14: Comparison plan quality complex temporal planning benchmarks (first set).COLIN compared best LPG -td, LPG .s, Sapa temporal baseline planner,problem file shape colour points indicate best.Planners appearing particular dataset best problemscollection.12.2 Solving Problems Continuous Linear Change Duration-Dependent Effectsfocus section examining scalability COLIN continuous benchmarkdomains developed and, specifically, comparing two variants TRPG discussed Section 9. are: basic heuristic, discretises time, refined heuristic,capable handling continuous numeric change directly. continuous benchmarks,described Section 11, characterised sophisticated temporal structure (including requiredconcurrency) giving rise interesting opportunities concurrent behaviour. problems time-dependent effects continuous effects, reach temporalplanners used last experiment. problems used experiment designed relyexploitation features, baseline planner ignored continuous dynamicswould unable solve problems.Results comparing basic refined heuristics shown Figure 16. BeginningAirplane Landing domain Rovers domain variant, performance either51fiC OLES , C OLES , F OX & L ONGZeno TimeAirport Strips Temporal4001000350800Colin Solution QualityColin Solution Quality300250200150600400100200LPG-TD usedLPG.s usedSapa used500LPG-TD usedLPG.s usedTemp-Baseline used0050100150200250Best Solution Quality30035040002005040403020108001000Pipes No-Tankage Temporal50Colin Solution QualityColin Solution QualityPipes No-Tankage Temporal400600Best Solution Quality302010LPG-TD usedLPG.s usedTemp-Baseline used0LPG-TD usedLPG.s usedTemp-Baseline used00102030Best Solution Quality40500102030Best Solution Quality4050Figure 15: Comparison plan quality complex temporal planning benchmarks (secondset). COLIN compared best LPG-td, LPG.s, Sapa temporal baselineplanner, problem file shape colour points indicatebest. Planners appearing particular dataset bestproblems collection.heuristic used: relaxed plans found same. expected, twodomains interaction time numbers relatively limited. Airplane Landingproblem, action durations affect variable used measure plan cost usedpreconditions. Thus, selection actions TRPG unaffected. Rovers domain, continuous change arises consuming power navigate actions, producing powerrecharging. Capturing time-dependent nature precisely effect relaxed plans, nature relaxation leads rarely require recharge actions,conditions needed affected whether effects integratednot. Nevertheless, two domains illustrate guaranteed like-for-like situations,heuristic guidance same, refined heuristic negligibly expensivecompute, despite additional overheads tracking gradient effects TRPG expanded.also seen COLIN scales well across Airplane Landing instances, althoughmanages solve 9 14 Rovers problems (these well within two minutes).52fiC OLIN : P LANNING C ONTINUOUS C HANGERovers Continuous TimeAirplane Landing Edinburgh Time1000100Basic HeuristicRefined HeuristicRefined HeuristicBasic Heuristic10010TimeTime (s)10110.10.10.010.0151015202530Problem Number35404525046Satellite Cooled Time10001214Satellite Cooled Makespan1000Basic HeuristicRefined HeuristicBasic HeuristicRefined Heuristic900800Makespan100Time (s)810Problem Number1070060050014003000.120024681012Problem Number14161820246AUV Time1012Problem Number14161820AUV Makespan10000600Basic HeuristicRefined Heuristic1000500100400MakespanTime81030012000.11000.01Basic HeuristicRefined Heuristic05101520Problem Number25305101520Problem Number2530Figure 16: Comparison Basic Refined TRPG variants continuous domains. boxedgraphs makespan comparisons Satellite Cooled AUV domains, placedright corresponding runtime graphs.Satellite Cooled domain, runtime taken find plans using refinedheuristic comparable using basic heuristic: problems (e.g. 13, 18)slower; others (e.g. 12, 15) faster. interesting comparison makemakespan data (shown right). seen, refined heuristic generally produces53fiC OLES , C OLES , F OX & L ONGbetter quality plans. difference quality due refined heuristic better capturingrelationship time numbers, leading better actions chosen relaxed plan.way example, consider state reached beginning sunrise action:basic heuristic, LP used obtain bounds power availability state,free reign much time allow elapse. lower-bound found slightlyzero (corresponding allowing time elapse), upper-bound foundpeak power availability (corresponding applying entirety sunrise action).building TRPG bounds, cooled sensor operation immediately available,hence goals always achieved first actions using sensor cooling: durationactions lower, making attractive. resulting relaxed plan, hencehelpful actions, therefore lead search use sensor cooling.refined heuristic, LP used obtain bounds power availabilitystate, bounds must obtained soonest possible point. Thus, lowerbound still slightly zero, upper bound also slightlyzero. positive gradients effect power availability variables includedTRPG, influencing layers different actions become applicable. Specifically,actions without sensor cooling lower power requirements, hence appear earlierlayers. Then, goals first achieved actions using sensor cooling (where increasedduration acquiring image without cooling compensated sufficiently ablestart taking image sooner) relaxed plan, hence helpful actions, usesensor cooling goals. seen situation closely analogousdifferences alternative mortgages Borrower domain.extent trade-off influences plan quality varies problems, dependinginitial orientation satellites, images required. least benefit arisessatellite requires substantial reorientation point towards first target case,time taken allows energy level rise sufficiently support sensor cooling. greatest benefitarises opposite situation, satellite requires minimal reorientation then, switchingsensor cooled mode require substantial amount time elapse supportenergy requirement precondition.aid understanding scalability implications results, Satellite problemsbased used 2002 IPC, similar fundamental size. However, continuousreasoning added makes underlying problems fundamentally muchdifficult solve.AUV domain, use refined heuristic increases problem coverage, 30problems solved rather 27. Applying Wilcoxon Matched-Pairs Signed-Ranks Testpaired time-taken data mutually solved problems, find reject null hypothesisrefined heuristic better basic heuristic, p 0.05. Observing performance planner, difference performance arises due way driftingprocess handled two approaches. Specifically, accounted differencebounds fact layer zero TRPG calculated. Consider state actionAUV communicate image data started. domain encoding ensurescommunication completed, AUV cannot perform activities. point, priorevaluating state using TRPG heuristic, LP used give bounds values state54fiC OLIN : P LANNING C ONTINUOUS C HANGEvariable. Considering variable recording far communicating AUV driftedvariable (distance-from-waypoint auv0), abbreviated dfw0:basic heuristic employs approach set Section 8.3. single timestampvariable introduced, must come action started, along additionalvariable constraint dfw0. Maximising minimising value additionalvariable yields bounds dfw0. lower bound infinitesimally largerprior starting action, due time elapsed. upper bound correspondsallowing large amount time elapse.refined heuristic employs approach set Section 9.2.1. Here, timestampvariable introduced task variable, case concerned tnow (dfw0).prior case, constrained action applied. Additionally,however, domain model enforces action refer valuevariable communicate action finished, specific tnow must also come(future) end action applied. bounds dfw0 found followingremaining steps Section 9.2.1: LP solved minimise value tnow variable,value variable fixed minimum LP solved maximiseminimise value dfw0. Critically, tnow variable must come endaction applied, rather start, lower bound dfw0 larger.increase lower bound dfw0 affects whether, TRPG, preconditionsform (<= (dfw0) c) considered satisfied initial fact layer. satisfied,delayed earliest layer localise action reduces value dfw0.difference affect relaxed plan found: solution extraction, action requiring(<= (dfw0) c) chosen, localise action necessary achieve TRPG,action added relaxed plan. cannot come earlier endcommunicate action applied, is, point bounds dfw0 calculated,sort localisation necessary ultimately applied. Thus, boundsrefined heuristic lead better relaxed plans found, containing localise actionswould otherwise omitted.give indication difficulty problems, AUV problems range problems2 AUVs, 5 waypoints, 2 objectives 2 goals harder end 6 AUVs, 15waypoints, 6 objectives 7 goals. major hurdle preventing COLIN scaling evenlarger problems inability see implicit deadline created shine-torchaction started. AUV shining torch finite energy, planner starts shinetorch action one AUV, preparation another AUV take image, addsplan actions involving second AUV unrelated taking image, delaylead insufficient energy shine torch long enough gain requiredexposure photograph taking action eventually started. leads planner deadend forced resort best-first search, much less effective EHCdomain. implicit deadlines occur many planning problems temporal coordinationissues COLIN faces could avoided using branch-ordering heuristic promotesactions whose applicability time-limited due ends currently-executing actions, perhapsrelaxing unnecessary ordering constraints imposed COLIN due total order search.scope paper, interesting avenues future work.55fiC OLES , C OLES , F OX & L ONG(:durative-action burning-fuel:parameters (?a - airplane):duration(>= ?duration (* 60 (engines ?a))):condition (and (at start (not-burning-fuel ?a))(at end (taking-off ?a))):effect (and (at start (can-start-engines ?a))(at start (not (not-burning-fuel ?a)))(increase (wasted-fuel) (* #t (engines ?a)))))Figure 17: burning-fuel action added Airport domain12.3 Post Hoc Plan Optimisationsection evaluate effectiveness post hoc plan optimisation strategy. describedSection 10, plan optimisation phase occurs planning complete never changeactions plan. lifting Partial Order prior scheduling (Veloso, Perez, &Carbonell, 1990), provide scheduler little flexibility order actions.long ordering constraints remaining (greedy) partial-order lifting respected,scheduler reduce plan cost altering time-points actions occur and,possible, durations. Minimising objective plan makespan effectplan quality domains metric sensitive times actions applied,since, default, COLIN minimises makespan solution final LP completed plan.benchmark domains literature, make use one existing suitabledomain introduce new variations existing benchmarks, order test feature.first domain, existing domain property, Airplane Landingdomain, used earlier section, described Section 11. Here, penalties incurredlanding depend whether, extent, early late. Therefore, givensequence landings, times assigned impact quality plan.next two benchmark problems variants problems introduced InternationalPlanning Competitions 2002 (Long & Fox, 2003b) 2004 (Hoffmann & Edelkamp, 2005).First, consider modified version Satellite domain. modify domain addingtime windows (modelled using TILs) clear view given objective.photograph objective taken time window, quality plan improves,better quality picture preferable. problem introduce three time windowsobjective, bounded random duration, taking photograph objectivepreferred. second adapted benchmark taken IPC2004 Airport domain.Airplane Landing problem described previously concerned scheduling landing timesaircraft, Airport domain concerned coordinating ground traffic: moving planesgates runways, eventually take-off, whilst respecting physical separation mustmaintained aircraft safety reasons. add domain metric minimisetotal amount fuel burnt aircrafts engines starting eventually takesoff. capture PDDL 2.1, add action shown Figure 17. action must occurplanes engines started cannot finish plane started take-off(hence duration least startup action). two points increases56fiC OLIN : P LANNING C ONTINUOUS C HANGEamount fuel wasted rate proportional number engines fitted aircraft:larger planes (for number engines greater) waste fuel per unit time.Satellite Airport domains use standard problem sets competitions,adding minor changes needed support modifications made, whilst leaving underlyingproblems unaltered.final domain consider cafe domain, first used evaluate CRIKEY (Coles, Fox,Halsey et al., 2008). domain, tea toast must made delivered tablecafe. kitchen, however, one plug socket, preventing two items madeconcurrently. restriction allows problem number interesting metric functions:minimise total time serve customers (the plan makespan), minimise timedelivery tea toast given table, minimise amount items cooleddelivered table. consider latter two variants here.results experiments presented Figure 18. Starting top-left,Airplane Landing domain, post hoc optimisation gives modest improvement plan quality.due limited scope optimisation: even partial-order lifting, orderplanes going land fixed plan, adjusted precise timesplanes going land within ordering.Moving Airport domain variant burning-fuel action Figure 18 top-rightpost hoc scheduling able give large improvements plan quality. original plans,optimisation, burning-fuel action given plane started point priorrelevant can-start-engines fact needed ended point relevanttaking-off fact true, necessarily timely manner. Following post hoc optimisation,due objective function used, burning-fuel action starts late possible finishesearly possible.cafe domain, results two metrics used shown central graphsFigure 18. two diagonal lines correspond original plans. given problem, twoplans identical: evaluation metric differs. two lower lines show qualityplan scheduling respect relevant metric. Observing post-scheduled plans,actions scheduled one would intuitively expect. minimising total deliverywindow times, items given table delivered succession, even first item loses heatwaiting second item prepared. contrast, minimising heat loss itemsdelivered tables soon prepared, even delay twoitems delivered.Finally, results variant Satellite domain observation windows shownbottom-left Figure 18. Whilst marked improvements previous twodomains, scheduler able make headway better scheduling observations.original plan given problem will, satellite, fix observations make,order made. remains enough flexibility able improve planquality, reducing plan cost around factor 2.12.4 Comparison Optimal Solutionsinvestigated difference quality optimal solutions solutions producedCOLIN order form impression close optimal COLIN get. this, ranCOLIN admissible heuristic uses makespan estimate produced TRPG, using57fiC OLES , C OLES , F OX & L ONGAirplane Landing Edinburgh20000Airport Fuel Loss4500colin-standardcolin-optimise180004000160003500140003000Solution QualitySolution Qualitycolin-standardcolin-optimise1200010000800025002000150060001000400050020000051015202530Problem Number35404550246Cafe (Delivery Window)120081012Problem Number141618Cafe (Heat Loss)colin-standardcolin-optimise80100colin-standardcolin-optimised70801000Delivery Window Metric20Heat Loss Metric6080060060504040304002020020100024681012Problem Number14161820024681012Problem Number14161820Satellite Reward1200colin-standardcolin-optimiseSolution Quality1000800600400200024681012Problem Number14161820Figure 18: Quality plans produced Colin without post hoc optimisation. fourgraphs, lower better.value used COLIN results presented Figures 16 18. callvariant optimalCOLIN.AUV Rover domains, variable-duration actions domaindurations chosen small actions used plan. -length actionsmight chosen, example, relocalise slightly drifted, recharge usednegligible amount power. domains, optimal search consider plans comprisingalmost entirely actions duration optimal makespan. example scale this,58fiC OLIN : P LANNING C ONTINUOUS C HANGEProblem[2-5] instance01020304050607optimalCOLINmakespan time (secs)20.0010.0030.0010.0030.0010.0240.0010.2940.0033.9450.00369.93-COLINmakespan20.00134.00438.00744.00648.00962.0166.014time (secs)0.010.010.010.010.020.030.03Table 4: Comparison makespans solution time airplane landing problems solved optimalCOLIN COLIN using refined heuristic. Problem 7 could solvedoptimalCOLIN within 1 hour bound.AUV problem 1, solved COLIN, find plan makespan 34.031. Careful analysishand suggests plan cannot improved, optimal. OptimalCOLIN must consider plans34,031 steps order prove plan optimal. means problemcompletely reach optimal planning.similar problem arises Rovers domain, recharge action littlelong, series -long recharge actions applied, reaching ostensibly different states,without making progress. Clearly, potential -duration actions arisecontinuous temporal domain. problem search-space explosion also arisetemporal domain orders magnitude differences longest shortestpossible actions.However, Airplane-Landing Satellite Cooling domains, variable-durationactions domains made arbitrarily short search. Therefore, optimalCOLINprinciple able solve problems domains. fact, given 4 Gb memory 1 hourruntime instance, able solve 6 airplane landing instances, shown Table 4.table shows, time required solve problems increases fast: problem 5 couldsolved 3.94 seconds, problem 6 69.93 seconds, problem 7 could solved withinhour available. basis decided unnecessary extend time availableoptimalCOLIN would unlikely cope large instances.Table 4 shows COLIN sacrifices optimality speed. sacrifice important,pay terms time required solve problems. COLIN able solve 62 airplanelanding problems, instance taking 33.02 seconds solve.found optimalCOLIN could report candidate solution first Satellite domain instance, within 368 seconds. However, could prove within time available solutionoptimal, include it.12.5 Costs Associated LP Schedulingtransition CRIKEY 3 COLIN switch solving STP state solvingLP. important issue consider impact time taken evaluate59fiC OLES , C OLES , F OX & L ONGfeasibility plan constructed reach every state considered search. default modeoperation, COLIN uses STP evaluate state unless temporalnumeric constructsnecessitate use LP. evaluate whether appropriate (or whether always usingLP would faster), compare overheads STP solving LP solving equivalentproblems, created variant COLIN that, every state S, schedules plan reach independently using three different schedulers: original STP solver used standard versionCOLIN, equivalent LP solved using CPLEX (IBM ILOG CPLEX Optimization Studio)equivalent LP solved using CLP (Lougee-Heimer, 2003). STP solver used incremental STP algorithm due Cesta Oddi (1996), previously used CRIKEY 3. LPsolvers used tighter variable bounds described Section 8.4. order evaluatecost associated use LP instead STP, modified COLIN collect data revealingcosts technique applied node evaluated search plan.possible compare performance straightforwardly, simply running COLIN using STPversus COLIN LP, minor variations caused numerical accuracy leaddifferent trajectories followed, masking intended comparison. aside, interesting observe minor (and essentially uncontrollable) differences computed makespansrelaxed plans lead significant variations performance (relaxed plans equal h-valuessorted makespan estimates search).wish compare STP LP approaches, necessary consider domainsreason: is, without continuous-numeric duration-dependent effects.order consider problems scheduling interesting necessary (in contrasttemporally simple problems Section 12.1) consider domains required concurrency.Currently benchmarks exist, planners attempt solve problems.use representatives competition domains features: compiled timed initialliteral domains IPC2004, use Airport (with Time Windows) PipesNoTankage (with deadlines). also use Match-Lift Driverlog Shift domains (Halsey, 2005).completeness, include results domain scheduler strictly necessary:PipesNoTankage Temporal domain IPC2004.Figure 19 shows mean time spent scheduling per state, using approach, problemsdomains. exclude graph data problems solvedplanner less second, accuracy profiling data sufficiently reliablemeasure time spent scheduler overall time taken small. Sinceinteresting variation results domains present data together across threegraphs, sorted scheduling time per node using CPLEX. intended nominalanalogue hard scheduling problems given planning problem are. increasescheduling time CPLEX generally corresponds increase scheduling time CLPSTP solver, except easier problems noise sufficient tip balancefigures small. Note differing y-axis scales three graphs, sorting problemsaccording difficulty allows us display data appropriate range distinguishresults. sake maintaining reasonable y-axis ranges final problem, problem 60,omitted graphs; problem figures CPLEX 239ms, CLP 139msSTP 38ms.results Figure 19 are, course, indicative scalability COLIN, runningthree schedulers state, significantly slower usual configuration. practice,domains continuous duration dependent effects, COLIN automatically60fiC OLIN : P LANNING C ONTINUOUS C HANGELow Difficulty0.4STPCLP0.3 CPLEXMST/State (ms)0.350.250.20.150.10.05123456789 10 11 12Problem Number131415161718192033343536373839405859Medium DifficultyMST/State (ms)1.4STPCLP1.2 CPLEX10.80.60.40.2212223242526272829 30 31 32Problem NumberHigh Difficulty40MST/State (ms)3530STPCLPCPLEX2520151050414243444546474849 50 51 52Problem Number5354555657Figure 19: Mean Scheduling Time (MST) per State Temporal Planning Problems. problemnumber appears leftmost three corresponding columns case.disable LP scheduler use efficient STP solver. Further, plannerrun profiling enabled, subject significant overheads.61fiC OLES , C OLES , F OX & L ONG&&&&&&!"#"$# !" '(' )#)*')' +(,#(,!"#"$(' )(' )!"#"$%#!"2#"$%#!"& # !-.# **&')&( ' )/ ' - / (0-. 0 '# 11Figure 20: Time spent various activities solvers, CPLEX CLP, viewed proportion total time spent CPLEX. slice labelled MILPSolverCPX/CLPtime spent destructor MILP solver CPLEX CLP: housekeeping operation implementations (which written C++).Considering relative performance STP LP solvers, clear overheads incurred necessary (for domains continuous effects) move using LP ratherSTP. mean ratio time spent scheduling problems CPLEXspent using STP solver 5.81, figure CLP 3.71. Analysis data suggestsratios change problem difficulty increases, rather overhead constant factorharder problems.Despite increased scheduling overheads still worth noting solving scheduling problem relatively small fraction cost reasoning done state. plangiven state scheduled check feasibility state evaluated using temporal RPG heuristic described Section 9. well known, analysis performanceFF forward search planners, majority search time spent evaluatingheuristic. give indication relative cost scheduling versus heuristic computationgive admissible estimate mean fraction time spent, per-state, running schedulerversus computing heuristic. estimate guaranteed overestimate true meanstates scheduler demonstrate temporal problem solution:states RPG heuristic never evaluated, heuristic evaluation actually appliedfewer states scheduler. Nonetheless, data shows that, across problems, usingSTP solver scheduling accounts average less 5% state evaluation time. CLPCPLEX figures 13% 18% respectively. suggests that, although schedulingadd overhead solving problems, relatively small compared cost heuristiccomputation.perhaps surprising observation made Figure 19 CLP generally solvesscheduling problems much efficiently CPLEX. Given reputation CPLEXhighly efficient commercial LP solver wanted investigate case problems.62fiC OLIN : P LANNING C ONTINUOUS C HANGEperformed analysis profiling data, breaking results function call,observe time spent various aspects constructing solving LP thorough CLPCPLEX library calls. data, presented Figure 20 shows time spent functionfraction total time taken CPLEX schedule plans (each summed across problems).used section CLP data represents time saved using CLP versus CPLEX.presentation means equally sized slices pies represent length timetaken either solvers respective methods.important insight gain data timeLP solvers spent solve function, indeed observed search portionnegligible: barely visible. majority time is, fact, spent adding rows LPmatrix, i.e. adding constraints LP actually solved. Comparing CPLEX CLP,takes 6 times longer, average, add row matrix. LPs createdidentical, hence involve adding number rows matrix. portionchart corresponds methods, many also take longer search,pre-processing steps adding new columns (variables) setting upper bounds. Sinceadding rows matrix significant portion time taken constructing-then-solvingLPs COLIN, results large overhead. LPs created COLIN small simplesolve, compared difficult industrial-sized problems CPLEX designed.results suggest that, fact, best type LP solver use task relativelylight-weight LP solver, overheads, create models efficiently, even perhapswould scale large-scale problems. notable, although less marked, differencetwo LP solvers time spent destructor, called free memory usedLP solver state evaluated. Here, takes 23 times longer, average,call destructor CPLEX destructor CLP. less impact rowadding overheads, since LP deleted per state, rather per LP constraint.general, would normally noticeable issue solving single difficult LP. However,COLIN, number LPs solved equal number states evaluated, overheadbecome noticeable.One interesting outcome study if, future, COLIN extendednon-linear continuous change, requiring use mathematical programming solver state(along research developments), overheads may well prohibitive. searchwithin solver, greater overhead would occur due change, factmajor contributor time overheads using LP.13. Conclusionsrange problems solved effectively planners grows, rangeopportunities technology applied real problems. recent years, planning extended solve problems real temporal structure, requiring temporal coordination, problemsinclude metric resources interactions use causal structure plans.shown range extended still further, include linear continuous processeffects. extension power planners demands several steps. first modelextension form allows relationship constraints imposed plansnew expressiveness, actions used solve problem, properly expressed.second step develop means represent world state consistently, order63fiC OLES , C OLES , F OX & L ONGcharacterise space search plan conducted. third step developway compute progression states using action models extended representation.step complete, is, principle, possible plan: search space constructedsearched using classic simple search techniques. practice, process unlikely leadsolutions many interesting problems fourth step, order make search possiblelarge spaces, construct informed heuristic guide search.paper built earlier work completed first steps, adding thirdfourth steps allow us solve planning problems continuous effects. toolsused achieve well-established Operations Research tools: LP solvers extensionsMILP solvers. contributions made show tools harnessedcheck consistency states, model state progression compute heuristics successfully guide search large spaces develop planning problems.additional contribution established collection benchmark problemsdirection research planning. planning community witnessed creationbenchmarks propagation powerful aid development technology, supportingclear empirical evaluation challenging researchers improve results others.shown COLIN solve interesting complex problems, remains much roomimprovement. Apart extending capability planner improving informednessheuristic improving early pruning dead end states, also opportunityextend still range problems expressed solved. particular,interested problems non-linear continuous effects, power thermal curves.seems possible non-linear effects might approached similar approach usedCOLIN, adapting NLP solver role LP solver COLIN. Alternatively, mightpossible approximate non-linear effects piecewise linear effects, much wayAUV domain described paper, performing process automatically.Planning becoming increasingly key technology robotic systems become powerfulcomplex begin see limits low level control strategies managingcontrol systems. Autonomy demands powerful predictive control planningoffers possible solutions problem. Planning continuous effects importanttool collection offer tackling new demands.Acknowledgmentsauthors wish thank handling editor, Malte Helmert, anonymous reviewersconsiderable contributions paper. authors also wish thank members Planning Group helpful discussions long gestation work.authors also wish acknowledge EPSRC support work, specificallygrants EP/G023360/2 EP/H029001/2.64fiC OLIN : P LANNING C ONTINUOUS C HANGEAppendix A. GlossaryName(i, v)(i, v)Action compressionalce(i)cs(i)dec(i, v)dmin (dmax)Eeff +xeffxeff nxestepielapsed(a)f (i)Descriptionlower bound assignment effects variable v due actions layer reachability graph.upper bound assignment effects variable v due actions layer reachability graph.technique simplifying structure durative actionstreating simple non-durative action unioneffects ends durative action unionpreconditions.Action layer reachability graph constructed heuristic purposes.First Use62Function returning variable corresponding end timesnap-action position current plan.Function returning variable corresponding start timesnap-action position current plan.21Set (discrete) decreasing effects variable v layerreachability graph.rate change variable v (associated stateachieved execution plan.minimum (maximum) duration action. use dmin(a)(dmax(a)) relevant action required explicitdmin(a, t) (dmax(a, t)) value anchored actionlayer al(t).62event list recording action start times durative actionswhose end points yet included plan.Propositional add effects action, x, present, indicates whether start end action.Propositional delete effects action, x, present,indicates whether start end action.Numeric effects action, x, present, indicateswhether start end action.name LP variable corresponding timedurative action finish, started ith step plan,finished within plan constructed far.maximum time action could executingstate heuristically evaluated.13variable STN CRIKEY 3 corresponds timecurrently incomplete action eventually finish.6562926212114444202715fiC OLES , C OLES , F OX & L ONGNameflDescriptionFact layer reachability graph constructed heuristic purposes.First Use26inc(i, v)Set (discrete) increasing effects variable v layerreachability graph.invariants active state S.62Left packingstructure plans concurrency concurrent actions start simultaneously.39name variable created represent time endcurrent plan STP LP used check temporal consistency state.15hop, , dmin, dmaxEvent record CRIKEY state, containing durative action, op,started step i, minimum maximum durationaction.14preprepre `p(a)Conditions required complete action.Invariant conditions durative action.Conditions required initiate action.bound number instances durative actionmay execute concurrently.45428remaining(e)maximum amount remaining time actionevent record e could continue executing following stateheuristically evaluated.Information associated durative action al(t) reachability analysis constructed COLIN, indicating much timecould continue execute layer.28stepiname LP variable corresponding timeaction ai applied plan.20t(i)variable STP CRIKEY 3 represents timestep plan executed.property planning problems require concurrency order manage interactions actionsdeadlines.Action effects refer ?duration, causing numeric fluentschange different amounts according length actioncausing effect.14inv(S)rem(t, a)Temporal coordinationTime-dependent change661433392fiC OLIN : P LANNING C ONTINUOUS C HANGENameDescriptionUsed describe continuous change: complete accountuse semantics, see original discussion usePDDL 2.1 (Fox & Long, 2003).First Use3ub(w, x, y)Function used calculate bounds effects continuous numeric change.29vUsed represent vector metric fluents associatedplanning domain, values state. vector treatedindexable: v[i] ith entry v.vector values metric fluents start state, immediately following step effects application action.vectors lower upper bounds values numeric variables state (during plan construction).5Symbol used represent vector constants equal dimensionsize vector metric fluents relevant planningproblem.5#tv0vmin , vmaxw672124fiC OLES , C OLES , F OX & L ONGAppendix B. Metric Relaxed Planning Graph HeuristicRelaxed Planning Graph (RPG) heuristic Metric-FF (Hoffmann, 2003)popular numeric planning heuristic last decade, widely used many planners.intuition behind heuristic generalise delete-relaxation include numeric variables.case propositions, relaxation simply ignore propositional delete effects so,(relaxed) actions applied, set true propositions non-decreasing. case numbers,relaxation replaces exact assignments numeric variables bound constraints upperlower bounds. Applying relaxed actions extends bounds reducing lower boundsdecrease effects increasing upper bounds increase effects. Checking whether numericprecondition satisfied simply matter testing whether constraint satisfiedvalue within bounds. delete-relaxed problem solved (non-optimally) polynomialtime, number actions resulting relaxed plan taken heuristic estimatedistance evaluated state goal.purpose RPG support heuristic computation. Relaxed planning undertakentwo phases: graph expansion, solution extraction. graph expansion phase purposebuild RPG, identifying facts actions become reachable. RPG consistsalternate fact layers, consisting propositions hold optimistic bounds v, actionlayers, containing actions whose preconditions satisfied preceding fact layer. casepropositional preconditions, precondition satisfied relevant fact containedprevious layer. case numeric preconditions, satisfied assignmentvariables appearing precondition, consistent upper lower bounds, leadsatisfied. define function ub(w, x, y) as:X w[j] y[j] w[j] 0ub(w, x, y) =w[j] x[j] otherwisew[j]w(this function defined Section 9.2).Then, denoting fact layer set propositions, f l(i), upper lower variable bounds(vmin (i), vmax (i)), precondition w v c action layer considered true iff:ub(w, vmin (i), vmax (i)) cseed graph construction, fact layer 0 contains facts true S. Thus, action layer0 consists actions whose preconditions satisfied fact layer 0. Fact layer 1 setoptimistic outcome taking fact layer 0, applying actions action layer 0.formally, considering propositions, applying actions action layer i, i.e. actions al(i)leads fact layer + 1 where:f l(i + 1) = f l(i) {eff + (a) | al(i)}Considering numbers, action layer set optimistic increase decrease effectsvariable v across actions are, respectively:inc(i, v) = {(ub(w, vmin (i), vmax (i)) + c) > 0 | al(i) s.t. hv, +=, w v + ci eff n (a)}dec(i, v) = {(ub(w, vmax (i), vmin (i)) + c) < 0 | al(i) s.t. hv, +=, w v + ci eff n (a)}68fiC OLIN : P LANNING C ONTINUOUS C HANGEexchange minimum maximum bounds v two expressions important:causes expression extreme possible appropriate direction. Similarly,optimistic upper lower bounds v, following available assignment effects, are:(i, v) = max{(ub(w, vmin (i), vmax (i)) + c) | al(i) s.t.hv, =, w v + ci eff n (a)}(i, v) = min{(ub(w, vmax (i), vmin (i)) + c) | al(i) s.t.hv, =, w v + ci eff n (a)}new bounds become:vmax (i + 1)[j] = max{a (i, v[j]), vmax (i)[j] +vmin (i + 1)[j] = min{a (i, v[j]), vmin (i)[j] +XXinc(i, v[j])}dec(i, v[j])}is, find upper (lower) bounds v[j] next layer, choiceapplying largest (smallest) single assignment effect, sum increase (decrease) effects. computed bounds variables layer + 1, graph expansion continuesiteratively, finding actions applicable action layer + 1, hence facts layer + 2,on. Graph expansion terminates one two cases: either fact layer satisfies propositionalnumeric goals, addition layers would never lead preconditionssatisfied condition signalled new propositions appearing accumulationlarger smaller bounds variables would lead numeric preconditions becomingsatisfied. case, relaxed problem cannot solved hence, original problem,plan starting reach G. heuristic value state set .Assuming graph expansion terminates goals reached, second phase extractsolution planning graph. recursive procedure, regressing goals backinitial fact layer. fact layer augmented set goals (facts numeric preconditions)achieved layer. Beginning inserting top-level goals G planninggraph first layers appeared, solution extraction repeatedly picks latestoutstanding goal planning graph selects way achieve it. propositional goals,single action (with effect adding goal) chosen, preconditions inserted goalsachieved (again, earliest possible layers). satisfy numeric goal w v c layeri, actions effects acting upon variables (with non-zero coefficients) v chosen,net increase w v, k, sufficient allow residual precondition w v c ksatisfied fact layer 1. point, residual precondition added goal achievedlayer 1 (or earlier possible), preconditions actions chosen supportprecondition added goals achieved previous layers.Solution extraction terminates outstanding goals achieved fact layer 0, sincetrue state evaluated need supporting actions. actions selectedsolution extraction form relaxed plan goal. length (number actions)relaxed plan forms heuristic estimate, h(S). Additionally, actions relaxed planchosen action layer 0 form basis helpful actions S, used restrictstates explored enforced hill-climbing search: action effect commonactions chosen action layer 0 considered helpful.69fiC OLES , C OLES , F OX & L ONGAppendix C. Temporal Reasoning Relaxed Planning GraphsSeveral approaches proposed building temporal relaxed planning graphs (TRPGs).three additional features TRPGs attempt manage, compared RPGs:1. temporal structure durative actions: aa applied a` appliedit.2. Action durations: end effects actions available appropriate delaystarted.3. PDDL 2.1 startend semantics, allowing effects preconditions attachedstarts ends actions.TRPG employed Sapa (Do & Kambhampati, 2003) satisfies first two these,third. Sapa, action compressed temporally-extended action obeyingTGP semantics, discarding delete effects, relaxation, building TGP -style planninggraph (Smith & Weld, 1999). use compression time-stamped TGP representationcaptures durations start-before-end relationships, use compression causesheuristic find false dead-ends cases required concurrency.TRGP used CRIKEY (Coles, Fox, Halsey et al., 2008) avoids action compression,ignores durations actions. non-temporal RPG built terms snap-actions usedsearch, additional precondition end snap-action particular dummy fact,added corresponding start, appeared preceding fact layer. use snap-actionsmeans preconditions effects lost (ensuring heuristic longer identifies falsedead-ends created approach used Sapa), limitation heuristicforced separation start end action, ordering constraint.CRIKEY 3 (Coles, Fox, Long et al., 2008a), heuristic constructed combinestrengths earlier heuristics, accounting durations actions, whilst alsorespecting startend semantics. briefly describe construction TRPG, sincebasis heuristic used COLIN. structure TRPG similar constructedMetric-FF, instead fact layer assigned index, assigned time-stamp(indicating minimum amount time must pass initial layer factslayer question appear). capture durations actions, record, end action aa ,earliest layer tmin (aa ) appear. value set 0 actions alreadyexecuting state evaluated (as need first insert start actionRPG). actions, value initialised , commencing TRPG construction.build TRPG follow Algorithm 2. First, number initialisation steps performed.time-zero fact layer fl (0) initialised (at line 1) contain facts true 10 . setea initialised contain end snap-actions must appear TRPG actionexecuting, end reachable (i.e. appear TRPG), else state dead end.ea empty, satisfies goals G (line 14), TRPG need built, since plancomplete.Following initialisation, TRPG expanded, beginning = 0 using fact layerf l(t) determine action layer al(t). preconditions action satisfied fact layer10. simplicity omit handling numeric fluents explanation performed exactlyearlier description RPG heuristic implemented Metric-FF.70fiC OLIN : P LANNING C ONTINUOUS C HANGEAlgorithm 2: Building Temporal RPG CRIKEY 3.Data: = hF, E, - state evaluatedResult: R = hfls, alsi, relaxed planning graph1 fl (0) F ;2 fls hfl (0)i;3 als h i;4 0;5 ea ;6 prev al ;7 prev fl fl (0);8 foreach aa9{e E | e.op = a} =10tmin (aa ) ;11else12tmin (aa ) 0;13ea ea {aa };141516171819202122232425262728293031323334353637G fl (0) ea = return goal state;<fl (t + ) prev fl ;al (t) {aa | pre(aa ) fl (t) tmin (aa ) t};foreach aa al (t) prev alfl (t + ) fl (t + ) eff + (aa );al (t) al (t) {a` | pre(a` ) fl (t)};foreach a` al (t) prev alfl (t + ) fl (t + ) eff + (a` );tmin (aa ) = min[tmin (aa ), + dmin(a)];als als + al (t);fls fls + fls(t + );prev al al (t);G fl (t + ) ea al (t)return R = hfls, alsi;prev fl 6= fl (t + )prev fl fl (t + );+ ;elseprev fl fl (t + );ep = {tmin (aa ) | pre(aa ) fl (t) tmin (aa ) > t};ep 6= min[ep];else ;return dead end;71fiC OLES , C OLES , F OX & L ONGfl (t) whether appear al (t) depends whether start end snap-action.first simpler case (line 20) that, start snap-action a` applicable, added al (t)tmin (aa ) set + dmin(a), dmin(a) priori lower bound durationa. state-independent measure minimum duration a, i.e. minimum durationconstraint referring constants, taken value dmin(a). Otherwise,minimum duration constraints depends state action applied,dmin(a) = : certain time must elapse start endaction. state-dependent terms cannot evaluated since TRPG determines relaxed state,real state.second case, covering end snap-actions, preconditions end action aabecome satisfied fact layer fl (t), addition aa al (t) depends whether startaction occurred sufficiently far past (line 17). tmin (aa ) aa addedal(t); < tmin (aa ) < , aa postponed al(tmin (aa )); otherwise, startaction yet appear, aa postponed relevant start appears.determined actions newly appear al (t), fact layer fl (t + ) updatednon-temporal RPG case, taking fl (t) (optimistically) applying effectsactions al (t). f l(t + ) al(t) contain necessary goals end snap-actions(line 27) must decided fact layer consider next. Clearly, infeasible createnew fact layers spacing fl (0) fact layer goals appear. Fortunately,also unnecessary, many fact action layers graph would identical.Instead, determine next fact layer consider follows:new facts fl (t+) true fl (t) (line 29), next layer expandfl (t + ) appearance new potentially useful facts makes necessary considerwhether actions become applicable layer.fl (t + ) = fl (t), know visiting fl (t + ) futile. case (line 34),time-stamp next fact layer visit earliest future point postponedend action becomes applicable:min{tmin (aa ) | pre(aa ) f l(t) tmin (aa ) > t}minimum values (or undefined) state prunedprocedure exits early, signalling result search procedure.TRPG successfully constructed (that is, starting state dead end) graphreturned contains finite set fact action layers, associated real time value.Assuming graph expansion terminates goals reached, relaxed solution extracted.solution extraction procedure used Metric-FF needs one minor modification suitableuse TRPG: end action aa chosen support goal given fact layer,action already executing state evaluated, corresponding start a` mustscheduled selection (at layer first appeared). purpose correspondsdummy facts CRIKEY: end action chosen, start must also executed.final remark TRPG, timed initial literals (TILs) included employingmachinery introduced delay ends actions appropriate layer. dummy TILactions {TILj ...TILm } yet applied tmin (TILj ) = 0.0, since TILj could appliedfirst action layer. intuition state evaluated snapshot world,72fiC OLIN : P LANNING C ONTINUOUS C HANGEtaken earlier end previous action, later pointnext TIL event occurs (due constraints discussed Section 6.1). minimum timestampslater TILs, TILk {TILj+1 ...TILm }, set relative time point:tmin (TILk ) = ts(TILk ) ts(TILj ).Appendix D. Post-Hoc Plan Optimisationappendix contains details MILP construction briefly described Section 10.2.D.1 Optimising Time WindowsFirst let us consider simple case action conditional effect metric-trackingvariable reward (where objective problem maximise reward), effectoccurs depends truth value single proposition p time specifier ts relativeaction (either start, all, end):(when (ts (p)) (at end (increase (reward) k))).case p manipulated actions, without allowing MILP introduce newactions completely change order plan steps (with complexity modificationswould entail), little scope optimisation. case truth value p dictatedtimed initial literals (TILs), interesting case: changing time-stampsstart end (LP variables step step j ), condition satisfied,direct effect metric function. relationship encoded within LP. wayexample, consider case p becomes true time false b; then, again, becomestrue c false d. case, two time-windows could potentially satisfycondition effect. Whether action wholly partially within onewindows depends time-specifier attached p:ts =at start, a` (step ) lie within one time windows;ts =at end, aa (step j ) lie within one time windows;otherwise, ts =over all, a` aa lie within one time windows.three cases, question must answered value variable lie withinknown range? case requires conjunction two conditions hold and,two cases, one hold. given step variable step , time window (a, b),introduce (MI)LP binary variable switch ab corresponding observation,constraints take logical form:switch ab (step > a) (step < b)Thus, switch variable takes value 1, time-stamp point p needed mustfall within time-window [a, b] vice versa. introducing two additional binary variables,denoted ga lb, logical constraint represented series inequalities (using N73fiC OLES , C OLES , F OX & L ONGdenote large number):step (a + ) switch abstep + (b ) switch abstep + N gastep N lbswitch ab ga lb00b1first two constraints encode forwards implication: switch ab set 1, steplie range [a + , b ] (a non-zero amount separation, epsilon, neededPDDL semantics avoid inspecting value p time changedTIL). latter three constraints encode reverse implication: step strictly greaterstrictly less b, ga lb hold value 1 thus, switch ab .Returning example, time specifier all, windows (a, b)(c, d), constraints added are:switch ab1switch ab2switch abswitch cd1switch cd2switch cdswitch p(step > a) (step < b)(step j > a) (step j < b)(switch ab1 switch ab2 )(step > c) (step < d)(step j > c) (step j < d)(switch cd1 switch cd2 )(switch ab switch cd )is, switch ab 1 entirety action falls within (a, b), switch cd 1 fallswithin (c, d) switch p 1 either hold. final switch variable used capturebenefit effect itself: holds value 1, increase value reward endplan k, is, apply conditional effect. variable reward already appearobjective function form LP variable reward 0n , n last step plan.Thus, modify constraints define reward 0n k switch p added value.change ensure variable providing value reward objective functioninclude reward k condition time executed holds.Generalising, extend case conditional effect depends truthformula f consisting conjunction time-specified propositional facts [(ts1 p1 )...(tsj pj )].(tsi pi ) f , create constraints, indicated above, switch variable switch pitake value 1 pi holds time-specifier tsi . gives us list switch variables= [switch p1 ...switch pj ]. Then, encode fact conjunction f must hold, createvariable switch f add constraints:j switch f + 1.switch p1 + ... + 1.switch pj 0switch f + 1.switch p1 + ... + 1.switch pj 1 jDefined thus, switch f takes value 1 iff switch variables takes value 1,precisely case conjunct satisfied. Then, much before, updatingconstraint dictating value LP variable reward 0n , add k switch f value.D.2 Optimising Numeric-Dependent ConditionsPerhaps complex case time windows conditions conditional effectdepend values numeric variables domain. (The PDDL 2.2 definition (Hoffmann74fiC OLIN : P LANNING C ONTINUOUS C HANGE& Edelkamp, 2005) include case TILs change values numeric variables11 ,consider case here.) simple case, time-specifier numericconditions either start end. complicated case onetime-specifiers all. case, potentially, snap-actions plan,start end action condition belongs, could affect whether conditionassociated effect met. must therefore check status conditionpoint execution action. Suppose action O, stepk steplvariables denoting start end time-stamps action, conditional effectnumeric precondition LNF:(over (>= (w v) c)) (at end (increase (reward) k)).encode this, need add constraints ensure conditional outcome occurs iff wv c times within O. Since change linear, (as conditionsO) need check values numeric variables immediately immediatelyaction time step within O, also immediately following start immediatelyend itself. Thus, variables corresponding values v must examinelist:00, vl ]., ..., vl1 , vl1e = [vk0 , vk+1 , vk+1stated earlier, case start/at end conditions somewhat easier: start,e = [vk ], end, e = [vl ]. Irrespective time specifier, basis list e,capture whether condition met, adding switch variable switch indicate whethercondition met vectors, switch variables [switch t1 ...switch tn ] element [1..s]list e, indicating whether met single vector. constraints (wheresmall number) then:w.e[x] N + (N + c) switchx[1...s]w.e[x] (c ) + N switch txx[1...s]switch + 1.switch t0 + ... + 1.switch ts 1 s.first quantification ensures switch = 1, lower bound c imposed w velement e. second quantification ensures vector v index x e satisfiesw v c, corresponding switch variable switch tx take value 1. thirdconstraint ensures switch variables switch tx take value 1, switch must, too, set1. appropriately constrained switch variable update constraint governingvalue LP variable reward 0n (the value reward end plan) increasevalue k switch.D.3 Optimising Time-Dependent Conditionsfinal extension allow conditional effects refer truth values timedpropositions, values numeric variables, also value duration action.11. Timed Initial Fluents used domain models, unofficial extension language.semantics extension straightforward Timed Initial Literals.75fiC OLES , C OLES , F OX & L ONGsituation appears example airplane landing problem (Section 10.2) value(total-cost) updated conditional effect, condition effect depend?duration. consider example order show MILP extendedhandle updates. First, previous cases, need add constraints ensureMILP solver chooses obtain conditioned outcome conditioned effect, conditionmust met. So, example, introduce new variable binary switch variablecondition, new constraints. land action plane ?p, starting finishingtime-stamps action step n step respectively, add pair constrained switch variables.sake example give meaningful names early late. constraintsadded LP then:target p step + step nstep step ntarget p + step step nstep step nN earlyN (N + target p) earlyN late(target p + ) latenew constraints ensure plane lands early, variable early take value1, vice versa. Similarly, lands late, late must take value 1 vice versa. caseexample, conditional effects action mutually exclusive, though truegeneral case.defined early late switch variables, objective function MILP mustaugmented reflect conditional outcomes action. Two terms must added oneswitch variable effect obtained switch variable 1. Abbreviating termsearlyPenaltyRate, latePenaltyRate latePenalty epr, lpr lp, respectively,objective terms plane p are:early (epr p) (target p (step step n ))late (lpr p) ((step step n ) target p) + late (lp p)Note unlike previous cases, objective function quadratic: objectivecontains terms switch variable multiplied constant step variable.arises as, unlike previous cases, conditional effect duration dependent fixed, constant value k. Whilst raises computational cost optimising MILP, cost acceptable: incurred once, solution plan found.Appendix E. Details Empirical Evaluation Colingraphs presented show detailed runtime quality comparisons analysed Section 12. comparative data graphed. Since graphs sometimes superimpose curves oneanother, making difficult see COLIN performing, Tables 613 show raw timequality results COLIN compared average best times qualities problems. Besttimes qualities also reported corresponding quality time (respectively)solution, planner(s) generated best result.76fiC OLIN : P LANNING C ONTINUOUS C HANGEDepots Simple Time100010010010Time (s)Time (s)Driverlog Simple Time1000ColinLPG-TDLPG.sSapaTemp-Baseline10110.10.10.01ColinLPG-TDLPG.sSapaTemp-Baseline0.01510Problem Number1520246Rovers Simple Time10010Time (s)Time (s)1012Problem Number1416182014161820Satellite Simple Time100ColinLPG-TDLPG.sSapaTemp-Baseline10810.1ColinLPG-TDLPG.sSapaTemp-Baseline10.10.010.0124681012Problem Number141618201416182024681012Problem NumberZeno Simple Time100001000ColinLPG-TDLPG.sSapaTemp-BaselineTime (s)1001010.10.0124681012Problem NumberFigure 21: Comparison time taken solve problems various simple temporal planning benchmarks planners COLIN, LPG-td, LPG.s, Sapa temporal baseline planner.Planners appearing particular dataset solve problemscollection.77fiC OLES , C OLES , F OX & L ONGDepots Simple Time600Driverlog Simple Time3000ColinLPG-TDLPG.sSapaTemp-Baseline50025002000Makespan400MakespanColinLPG-TDLPG.sSapaTemp-Baseline3001500200100010050000510Problem Number1520246Rovers Simple Time400400250MakespanMakespan3001012Problem Number1416182014161820Satellite Simple Time500ColinLPG-TDLPG.sSapaTemp-Baseline3508200150ColinLPG-TDLPG.sSapaTemp-Baseline300200100100500024681012Problem Number141618201416182024681012Problem NumberZeno Simple Time70006000ColinLPG-TDLPG.sSapaTemp-BaselineMakespan50004000300020001000024681012Problem NumberFigure 22: Comparison plan quality problems various simple temporal planning benchmarks planners COLIN, LPG-td, LPG.s, Sapa temporal baseline planner.Planners appearing particular dataset solve problemscollection.78fiC OLIN : P LANNING C ONTINUOUS C HANGEDepots Time100Driverlog Time10000ColinLPG-TDLPG.sSapaTemp-Baseline10ColinLPG-TDLPG.sSapaTemp-Baseline1000Time (s)Time (s)10011010.10.10.010.01510Problem Number15202468Rovers Time1001416182014161820Satellite Time1000ColinLPG-TDLPG.sSapa100Time (s)10Time (s)1012Problem Number1ColinLPG-TDLPG.sSapaTemp-Baseline1010.10.10.010.0124681012Problem Number1416182024681012Problem NumberFigure 23: Comparison time taken solve problems complex temporal planning benchmarks (first set) planners COLIN, LPG-td, LPG.s, Sapa temporal baselineplanner. Planners appearing particular dataset solve problemscollection.79fiC OLES , C OLES , F OX & L ONGZeno Time1000Airport Strips Temporal1000ColinLPG-TDLPG.sSapa100100Time (s)10Time (s)ColinLPG-TDLPG.sTemp-Baseline10110.10.10.010.0124681012Problem Number1416182051015Pipes No-Tankage Temporal1000010003540455035404550Pipes Tankage Temporal10000ColinLPG-TDLPG.sTemp-Baseline1000ColinLPG-TDLPG.sTemp-Baseline100Time (s)100Time (s)202530Problem Number1010110.10.10.010.0151015202530Problem Number3540455051015202530Problem NumberFigure 24: Comparison time taken solve problems complex temporal planning benchmarks (second set) planners COLIN, LPG-td, LPG.s, Sapa temporal baselineplanner. Planners appearing particular dataset solve problemscollection.80fiC OLIN : P LANNING C ONTINUOUS C HANGEDepots Time2000Driverlog TimeColinLPG-TDLPG.sSapaTemp-Baseline150070006000ColinLPG-TDLPG.sSapaTemp-BaselineMakespanMakespan5000100040003000200050010000024681012Problem Number141618202468Rovers Time800140060012005001000MakespanMakespan1416182014161820Satellite Time1600ColinLPG-TDLPG.sSapa7001012Problem Number4008003006002004001002000ColinLPG-TDLPG.sSapaTemp-Baseline024681012Problem Number1416182024681012Problem NumberFigure 25: Comparison plan quality complex temporal planning benchmarks (first set)planners COLIN, LPG-td, LPG.s, Sapa temporal baseline planner. Plannersappearing particular dataset solve problems collection.81fiC OLES , C OLES , F OX & L ONGZeno Time400Airport Strips Temporal1000ColinLPG-TDLPG.sSapa350800ColinLPG-TDLPG.sTemp-Baseline250MakespanMakespan300200150600400100200500024681012Problem Number1416182051015Pipes No-Tankage Temporal1401203540455035404550Pipes Tankage TemporalColinLPG-TDLPG.sTemp-Baseline140120ColinLPG-TDLPG.sTemp-Baseline100Makespan100Makespan202530Problem Number80608060404020200051015202530Problem Number3540455051015202530Problem NumberFigure 26: Comparison plan quality complex temporal planning benchmarks (secondset) planners COLIN, LPG-td, LPG.s, Sapa temporal baseline planner. Planners appearing particular dataset solve problems collection.82fiC OLIN : P LANNING C ONTINUOUS C HANGECOLINTimeQualitydepotssimpletime10.0336.00820.0354.013313.69 170.037411.6782.0315672.5651.02489101.3990.0251112130.1785.0261415160.2599.025170.4861.016181920211.6996.02922driverlogsimpletime10.0292.00620.16169.02130.0467.01240.25129.01850.11106.01760.09114.01170.0558.01180.19151.02390.42213.026100.0484.021110.04108.019123.22380.041131.09283.039141.45240.036150.25283.0431617181920AverageTimeQualityBestTimePlannerQualityPlannerLPG -td,TBL28.000 (0.0)46.090 (0.374)80.002 (0.11)56.24 (0.42)115.000 (0.11)156.004 (205.29)45.001 (0.11)87.003 (0.48)190.000 (0.29)52.000 (0.05)152.000 (0.26)133.000 (0.62)64.001 (0.33)64.001 (0.96)186.000 (0.45)46.000 (0.08)28.050 (10.728)105.000 (1.04)94.000 (0.19)101.002 (27.84)73.000 (0.56)329.007 (111.46)LPG -s+td91.001 (0.04)104.001 (0.03)40.02 (0.116)99.001 (0.02)75.08 (0.957)64.070 (0.963)49.090 (0.3)77.090 (0.869)150.002 (0.13)49.090 (0.404)85.090 (0.474)274.3 (0.05)240.003 (0.76)163.22 (4.388)157.210 (13.457)1510.000 (50.78)653.008 (16.28)361.67 (79.06)1478.000 (47.08)478.000 (6.19)LPG -s0.0400.0973.4623.0971.620103.4000.6770.2230.6254.1007.9335.8800.4750.4002.6550.9282.7163.2170.51710.44317.44854.66734.83464.243110.81774.818131.250171.00259.04995.784213.30081.465212.845188.25179.26583.767212.50285.27355.261109.144118.787238.29185.872406.6090.0 (28.000)0.01 (54.11)0.02 (82.000)0.04 (88.000)0.11 (115.000)1.51 (186.000)0.01 (82.17)0.09 (105.000)0.29 (190.000)0.03 (88.2)0.26 (152.000)0.62 (133.000)0.02 (82.17)0.1 (107.3)0.45 (186.000)0.04 (98.18)0.23 (61.000)1.03 (117.43)0.14 (164.36)0.75 (209.000)0.19 (90.18)8.95 (536.000)0.0240.1660.0390.1500.2230.2410.0860.2380.3010.1310.1531.0120.6501.7903.299112.9807.13364.42047.0806.19096.025127.46059.215112.46092.05493.03163.838153.071204.69293.645104.058339.586274.338291.536278.9201849.014790.049644.5601478.000478.0000.0 (91.05)0.01 (110.19)0.01 (40.04)0.01 (110.15)0.01 (83.17)0.02 (74.000)0.01 (51.09)0.01 (167.24)0.04 (232.26)0.01 (71.11)0.01 (119.18)0.05 (274.3)0.31 (299.000)0.24 (391.000)0.14 (278.34)50.78 (1510.000)1.86 (1052.000)21.03 (869.000)47.08 (1478.000)6.19 (478.000)TBLLPG -tdLPG -tdLPG -tdLPG -tdTBLLPG -tdLPG -tdTBLLPG -tdLPG -tdTBLTBLLPG -tdTBLLPG -tdTBLTBLLPG -tdTBLLPG -tdTBLLPG -td,TBLLPG -s,TBLTBLLPG -td,TBLLPG -tdTBLTBLLPG -td,TBLTBLTBLTBLLPG -tdLPG -tdTBLLPG -tdLPG -tdLPG -tdLPG -tdLPG -tdSapaLPG -sTBLLPG -tdLPG -sLPG -sLPG -sLPG -tdLPG -tdLPG -tdLPG -tdLPG -sLPG -sLPG -tdLPG -tdSapaLPG -tdLPG -tdLPG -sLPG -tdLPG -sLPG -sSapaLPG -sSapaSapaSapaSapaLPG -sSapaSapaTBLLPG -sSapaSapaLPG -tdLPG -sTBLLPG -tdLPG -tdTable 6: Results Simple Domains: Best results show best time (corresponding quality)planner(s) achieved time best quality (corresponding time) planner(s)achieving quality. TBL Temporal Baseline planner following tables.83fiC OLES , C OLES , F OX & L ONGCOLINTimeQualityroverssimpletime10.0267.00720.0248.00630.0273.0140.0250.00550.04126.01460.09186.02870.04107.01380.06119.01890.09176.028100.11155.019110.11161.025120.05103.014130.22198.029140.12170.021150.21208.038160.26203.032170.23267.036180.49217.039190.72339.047209.51392.063satellitesimpletime10.0141.00820.0165.01230.0250.0140.0487.01950.0574.01660.0772.01970.0972.02280.1484.02490.2195.028100.26101.029110.37113.031122.49137.0411313.38 214.061145.11166.039156.47183.056165.78170.045174.52133.041180.82107.0311927.98 349.07520AverageTimeQualityBestTimePlannerQualityPlannerTBLTBLLPG -tdTBLLPG -td,TBLLPG -tdTBLTBLLPG -td,TBLTBLTBLTBLTBLTBLTBLLPG -tdLPG -tdLPG -tdLPG -tdLPG -td67.007 (0.02)45.000 (0.02)67.089993 (0.15)45.039997 (0.081)107.15 (0.01)183.29 (0.02)91.001 (0.04)113.19 (0.02)156.18999 (0.508)139.14 (25.24)161.025 (0.11)88.08 (1.035)173.17998 (6.716)139.000 (0.04)175.21999 (2.776)186.20998 (6.67)218.22993 (9.01)140.28 (0.15)297.5 (0.38)351.000 (0.49)COLINTBL41.000 (0.01)65.000 (0.02)29.000 (0.02)58.000 (0.01)61.001 (0.09)58.09 (0.01)46.001 (0.13)41.000 (0.03)51.001 (0.33)63.000 (0.05)72.000 (0.07)98.000 (0.11)99.002 (2.75)68.000 (0.13)58.001 (2.54)73.002 (4.12)92.002 (4.29)75.002 (1.42)114.003 (2.57)137.24 (31.225)0.0320.0300.0400.0300.2200.0580.1460.2470.1485.1100.0850.2531.5650.2390.6711.4641.9922.1581.0584.31772.83448.22376.23851.621131.461229.58097.851131.066166.296158.878179.322112.041227.708157.483207.310208.513255.936170.494319.138380.9320.0 (67.08)0.01 (47.06)0.0 (77.000)0.01 (50.06)0.01 (107.15)0.01 (277.000)0.01 (98.13)0.02 (113.19)0.02 (159.000)0.02 (141.23)0.02 (185.26)0.01 (90.11)0.06 (190.33)0.02 (145.22)0.04 (215.29)0.04 (242.000)0.11 (259.000)0.1 (160.000)0.23 (319.000)0.49 (351.000)0.0220.0420.0490.0800.1030.1280.1790.2660.5170.5630.8062.1838.6044.85910.84513.91616.0702.0849.7409.73943.03266.31045.04078.26875.25972.84667.65573.26770.86679.67890.667119.533165.517111.078152.335143.923124.49593.269170.718231.7930.0 (46.07)0.01 (65.012)0.01 (58.09)0.01 (58.000)0.01 (82.13)0.01 (58.09)0.02 (75.12)0.03 (41.000)0.04 (58.000)0.05 (63.000)0.07 (72.000)0.11 (98.000)0.2 (208.000)0.13 (68.000)0.17 (183.000)0.23 (137.000)0.21 (142.000)0.1 (101.000)0.17 (130.000)0.26 (196.000)COLIN ,TBLTBLLPG -td,TBLTBLTBLTBLLPG -td,TBLLPG -td,TBLLPG -tdLPG -td,TBLLPG -tdLPG -tdLPG -tdLPG -tdLPG -tdLPG -tdLPG -tdLPG -tdLPG -tdLPG -sSapaSapaTBLTBLLPG -sTBLSapaSapaCOLINSapaSapaLPG -tdSapaSapaSapaTBLTBLLPG -tdLPG -tdLPG -tdLPG -tdLPG -tdLPG -sTBLLPG -sLPG -tdLPG -sLPG -tdLPG -tdLPG -tdLPG -sLPG -tdLPG -sLPG -sLPG -sLPG -sLPG -sSapaTable 7: Results Simple Domains: Best results show best time (corresponding quality)planner(s) achieved time best quality (corresponding time) planner(s)achieving quality.84fiC OLIN : P LANNING C ONTINUOUS C HANGECOLINTimezenosimpletime10.0120.0230.0240.0450.0360.0570.0880.1590.31100.12110.26120.3136.19141519.4816477.4717759.19181948.9520163.2Quality173.001592.008350.007885.013656.011995.016931.014895.0131583.0271191.022796.0151208.0272062.042836.0513173.0454947.0714309.0795364.096AverageTimeQuality0.0160.0330.0470.0620.1210.1910.2180.4401.0210.6961.7851.9005.261360.25410.105127.658246.09843.73722.74059.687178.602666.022356.6191226.041868.8321456.4341006.035869.2911268.4561461.053823.4331617.8561303.2541577.8212561.8082394.0725232.6032983.1015043.1465315.165BestTimePlannerQualityPlanner0.0 (180)0.01 (866.05)0.01 (280.04)0.01 (936.000)0.0 (400.06)0.01 (603.06)0.01 (706.08)0.02 (836.07)0.03 (789.12)0.03 (743.13)0.03 (763.1)0.03 (1199.13)0.03 (923.14)0.3 (2068.18)0.46 (2254.18)0.86 (1702.24)2.36 (3436.34)2.61 (3453.3)8.91 (3769.36)6.43 (4578.4)TBLTBLTBLLPG -td,TBLTBLTBLTBLTBLTBLTBLTBLTBLTBLTBLTBLTBLTBLTBLTBLTBL173.001 (0.01)592.008 (0.02)280.04 (0.01)885.013 (0.04)400.06 (0.0)603.06 (0.01)706.08 (0.01)836.07 (0.02)789.12 (0.03)743.13 (0.03)510.050 (8.037)1166.120 (8.568)923.14 (0.03)1169.100 (1433.534)2254.18 (0.46)1702.24 (0.86)3436.34 (2.36)2383.003 (121.90)3769.36 (8.91)4578.4 (6.43)COLINCOLINTBLCOLINTBLTBLTBLTBLTBLTBLSapaSapaTBLSapaTBLTBLTBLLPG -sTBLTBLTable 8: Results Simple Domains: Best results show best time (corresponding quality)planner(s) achieved time best quality (corresponding time) planner(s)achieving quality.85fiC OLES , C OLES , F OX & L ONGCOLINTimeQualityairportstripstemporal10.0664.00720.06185.00830.09202.01140.30127.01950.26227.0260.30301.0470.30301.0480.52538.05991.40516.058100.33126.017110.35228.02120.36265.035130.44311.034140.71528.057150.58365.049161.68625.0761711.47 603.0841858.89 777.0951938.65 574.076202121.65366.12222.52 487.1472323.68 510.1662431.00 826.1562590.81 934.204262728293031323334353646.83 408.1083778.05 672.1363849.89 847.21739404152.64 895.24142434446AverageTimeQuality0.0300.0300.0400.1200.1030.1300.1300.2371.6720.1300.1350.1550.1700.4650.2572.7386.09325.983226.60766.56380.2406.1786.4258.30734.57753.4805.19094.76094.9555.4206.820443.07514.95015.71017.11016.06336.82017.833476.65091.39524.803841.96035.450701.750289.27064.026185.022200.533127.070227.055251.363251.363432.165433.918126.062228.055239.352254.852426.657357.633536.440542.947652.658453.467643.625305.519428.104369.363493.835650.231480.785511.111623.910596.410641.000834.000724.570859.000879.000887.000350.705620.047580.741962.000510.004596.749579.000639.506331.000737.000BestTime0.01 (64.07)0.01 (185.000)0.01 (200.12)0.02 (127.19)0.02 (227.2)0.02 (240.41)0.02 (240.41)0.06 (402.6)0.10 (402.000)0.01 (126.17)0.02 (228.2)0.02 (228.37)0.03 (237.37)0.07 (390.57)0.06 (273.48)0.14 (558.000)0.61 (569.000)0.25 (733.000)0.50 (413.000)0.27 (740.000)0.33 (285.97)0.78 (286.26)0.49 (265.28)0.61 (377.18)2.47 (539.000)3.88 (584.000)7.50 (505.33)7.53 (706.000)4.47 (687.000)5.42 (641.000)6.82 (834.000)7.78 (815.000)14.95 (859.000)15.71 (879.000)17.11 (887.000)1.36 (322.000)32.41 (831.000)3.61 (573.000)476.65 (962.000)182.79 (584.000)21.77 (573.000)841.96 (579.000)70.90 (573.000)701.75 (331.000)289.27 (737.000)PlannerQualityPlannerTBL64.000 (0.02)185.000 (0.01)200.000 (0.03)127.000 (0.04)227.000 (0.05)232.000 (0.07)232.000 (0.07)394.000 (0.09)402.000 (0.10)126.000 (0.05)228.000 (0.07)228.37 (0.02)230.002 (0.14)390.57 (0.07)273.48 (0.06)404.68 (0.24)417.7 (2.16)447.88 (18.81)413.000 (0.50)450.87 (55.03)285.000 (0.77)286.26 (0.78)264.004 (510.166)376.003 (826.156)477.49 (10.45)377.57 (103.08)504.004 ()541.82 (181.99)505.82 (185.44)641.000 (5.42)834.000 (6.82)634.14 (878.37)859.000 (14.95)879.000 (15.71)887.000 (17.11)322.000 (1.36)357.006 (672.136)322.006 (847.217)962.000 (476.65)436.007 ()322.006 (895.241)579.000 (841.96)573.000 (70.90)331.000 (701.75)737.000 (289.27)LGP -tdLGP -td,TBLTBLTBLTBLTBLTBLTBLLGP -tdTBLTBLTBLTBLTBLTBLLGP -tdLGP -tdLGP -tdLGP -tdLGP -tdTBLTBLTBLTBLLGP -tdLGP -tdTBLLGP -tdLGP -tdLGP -tdLGP -tdLGP -tdLGP -tdLGP -tdLGP -tdLGP -tdLGP -tdLGP -tdLGP -tdLGP -tdLGP -tdLGP -tdLGP -tdLGP -tdLGP -tdLGP -tdLGP -tdLGP -tdLGP -tdLGP -tdLGP -tdLGP -tdLGP -tdLGP -tdLGP -tdTBLLGP -sTBLTBLTBLTBLTBLLGP -tdTBLLGP -tdTBLLGP -sLGP -sTBLTBLLGP -sTBLTBLLGP -tdLGP -tdTBLLGP -tdLGP -tdLGP -tdLGP -tdLGP -sLGP -sLGP -tdLGP -sLGP -sLGP -tdLGP -tdLGP -tdLGP -tdTable 9: Results Complex Domains: Best results show best time (corresponding quality) planner(s) achieved time best quality (corresponding time)planner(s) achieving quality.86fiC OLIN : P LANNING C ONTINUOUS C HANGECOLINTimeQualitydepotstime10.0359.74620.1466.793467.41258.12556735.56129.741891043.34296.3541112130.27108.4931415160.6050.58817181920212.0876.75322driverlogtime10.02303.00620.17462.01930.03202.00940.14474.01950.20343.01960.03275.00770.12316.01280.43699.02490.57730.028100.21294.031110.16522.026125.901066.038132.841052.031146.51787.034150.27212.0421617181920AverageTimeQualityBestTimePlannerQualityPlannerTBLTBLTBLLGP -tdLGP -tdLGP -tdTBLLGP -td,TBLLGP -tdTBLLGP -tdLGP -tdTBLTBLLGP -tdTBLTBLLGP -tdTBLLGP -tdTBLLGP -td55.181 (0.01)60.556 (0.02)127.592 (0.03)160.139 (0.04)777.544 (3.12)287.543 (2.35)107.340 (0.12)108.399 (0.10)1655.27 (0.97)151.173 (11.364)599.323 (1.42)132.893 (4.33)82.834 (1.373)157.701 (1.02)382.973 (4.16)26.390 (0.56)46.2 (0.20)395.768 (0.89)366.474 (1.22)592.561 (7.98)54.731 (36.794)343.110 (3.86)LGP -tdTBL302 (0.01)294.090 (0.678)173.020 (0.131)402.001 (0.04)161.090 (0.973)260 (0.02)268.1 (0.01)388.110 (1.244)591 (0.02)220.110 (0.415)306.13 (0.486)627.260 (1151.634)597.180 (4.193)625.150 (7.34)212.042 (0.27)3652.008 (28.03)2238 (3.31)1476.68 (78.94)3993 (63.87)1691.007 (66.56)LGP -td0.0360.1030.08717.0432.9702.3508.9330.2400.65011.0271.0205.3870.4170.4272.2904.7541.9773.0770.5203.7439.69347.47057.69871.719141.362186.587781.855287.543141.337119.1611783.468226.698726.121163.03299.751216.623399.69776.90087.495535.942428.210880.63971.388445.2320.00 (58.9311)0.01 (73.2211)0.03 (127.592)0.04 (160.139)2.82 (786.1666)2.35 (287.543)0.02 (142.158)0.10 (108.3988)0.33 (1911.6665)0.03 (294.293)0.32 (608.9167)0.46 (165.5889)0.02 (104.436)0.10 (329.133)0.42 (416.4197)0.03 (166.236)0.19 (88.26)0.75 (709.8509)0.14 (486.722)0.93 (1050.2797)0.16 (101.373)3.86 (343.1095)0.0170.1780.0440.1280.2490.2800.0720.3610.1930.1590.181233.3691.7433.24859.46966.3859.58048.780110.92541.060302.625442.260279.018453.060250.058266.432350.041695.275748.078328.450515.0681230.3251218.3051125.322766.1555120.5042787.0622319.2294005.0053828.0030.00 (302.05)0.01 (438.19)0.01 (173.06)0.01 (441.15)0.01 (195.18)0.02 (260)0.01 (268.1)0.01 (894.24)0.02 (591)0.01 (394.11)0.01 (512.18)0.05 (829.32)0.32 (903)0.41 (1161)0.10 (939.43)28.03 (3652.0081)3.31 (2238)27.48 (2885)63.87 (3993)15.56 (5965)LGP -td,TBLTBLTBLTBLLGP -tdLGP -td,TBLTBLLGP -tdTBLTBLTBLLGP -tdLGP -tdTBLLGP -sLGP -tdLGP -tdLGP -tdLGP -tdLGP -tdTBLLGP -tdTBLLGP -tdLGP -sLGP -tdTBLSapaLGP -sLGP -sSapaLGP -sLGP -sLGP -sLGP -tdTBLLGP -sLGP -sSapaLGP -tdSapaSapaLGP -sSapaLGP -tdTBLSapaLGP -tdSapaSapaSapaSapaSapaCOLINLGP -sLGP -tdTBLLGP -tdLGP -sTable 10: Results Complex Domains: Best results show best time (corresponding quality) planner(s) achieved time best quality (corresponding time)planner(s) achieving quality.87fiC OLES , C OLES , F OX & L ONGCOLINTimeQualitypipesnotankagetemporal10.036.00320.0420.01130.0512.00840.0722.01250.0514.00660.0516.00970.0612.00780.0616.00990.0824.013100.0828.015110.1611.026121.7722.554130.2716.535140.3814.532150.1411.526166.7830.07174.2511.023181.0812.529190.199.525200.4617.039210.099.01722230.1914.0182440.4629.03225261.8932.053270.2911.527280.7722.551292.4621.54630319.2118.3573213.0135.0383310.5321.8743420.5230.70935363738391.1913.8574041103.464.434950AverageTimeQuality0.0150.0580.0400.0580.0430.0600.0750.1000.1130.120300.430302.1100.7450.3800.7852.6671.1430.9000.2477.9770.0802.9750.24027.0154.8504.1401.61316.6975.5604.6653.2404.48510.53034.07024.20078.24531.76064.8550.96710.41034.820346.23515.4007.50651.52617.52083.02113.01724.02316.01516.52029.02636.0379.54416.97115.68714.44616.70031.40110.54418.2219.89826.6819.06236.70021.340101.01632.16528.14813.57242.97718.27232.20516.09851.50421.87439.58217.33418.07836.00012.82218.35634.6654.17316.49318.380Time0.00 (6.02)0.01 (20.09)0.01 (16.07)0.01 (16.07)0.01 (12.000)0.01 (18.08)0.01 (12.05)0.03 (18.000)0.02 (20.09)0.02 (28.13)0.05 (8.15)0.28 (17.33)0.05 (11.21)0.15 (14.000)0.12 (14.27)0.48 (40.000)0.09 (8.15)0.20 (18.35)0.11 (9.17)0.46 (0.46)0.02 (9.17)2.05 (23.4)0.19 (14.018)13.57 (173.000)1.12 (42.000)1.46 (27.39)0.29 (11.527)0.77 (22.551)0.34 (14.27)1.39 (27.41)0.17 (16.9367)0.89 (17.31)10.53 (21.874)1.40 (23.370033)48.40 (24.000)6.49 (21.156633)31.76 (36.000)0.32 (13.6433)0.17 (12.21)10.41 (34.665)0.10 (4.09)2.15 (16.32)15.40 (18.38)BestPlannerTBLTBLTBLTBLLGP -td,TBLTBLTBLLGP -tdTBLTBLTBLTBLTBLLGP -tdTBLLGP -tdTBLTBLTBLCOLINTBLTBLCOLINLGP -tdLGP -tdTBLCOLINCOLINTBLTBLTBLTBLCOLINTBLLGP -tdTBLLGP -tdTBLTBLLGP -tdTBLTBLTBLQualityPlanner6.000 (0.01)20.011 (0.04)12.008 (0.05)16.07 (0.01)12.000 (0.01)16.009 (0.05)12.007 (0.06)16.001 (0.19)20.09 (0.02)28.015 (0.08)8.15 (0.05)11.002 (1200.92)11.21 (0.05)13.25 (0.99)11.526 (0.14)27.53 (3.41)8.002 (11.023)12.529 (1.08)9.17 (0.11)14.003 (17.039)9.000 (0.13)23.4 (2.05)14.018 (0.19)29.032 (40.46)22.33 (8.58)25.000 (9.07)10.19 (0.48)22.551 (0.77)14.27 (0.34)27.41 (1.39)13.000 (0.34)17.31 (0.89)21.874 (10.53)23.370033 (1.40)10.669 ()15.000 (150.00)36.000 (31.76)12.000 (129.39)12.21 (0.17)34.665 (10.41)4.000 (0.90)16.32 (2.15)18.38 (15.40)LGP -tdCOLINCOLINTBLLGP -tdCOLINCOLINLGP -sTBLCOLINTBLLGP -sTBLTBLCOLINTBLLGP -sCOLINTBLLGP -sLGP -tdTBLCOLINCOLINTBLLGP -tdTBLCOLINTBLTBLLGP -tdTBLCOLINTBLLGP -sLGP -tdLGP -tdLGP -tdTBLLGP -tdLGP -tdTBLTBLTable 11: Results Complex Domains: Best results show best time (corresponding quality) planner(s) achieved time best quality (corresponding time)planner(s) achieving quality.88fiC OLIN : P LANNING C ONTINUOUS C HANGECOLINTimeQualitypipestankagetemporal10.046.00320.1224.01330.2312.00840.5218.00950.0914.00760.0916.0170.2616.00880.3018.0129104.4036.02110.4013.03112131.5113.531453.0922.0521559.6117.541171833.0611.5271920.2916.537202.5412.0292122232423.2531.56425983.7828.5582632.5533.053272.3010.02429191.4923.5483058.9828.55531187.2232.713323334160.4225.03837399.1415.1914051.8221.87741269.574.93495082.8222.378AverageTimeQuality0.0200.1470.1800.2750.1530.1601.1751.275238.05726.135316.58512.4806.420334.62564.6901500.780105.157252.310118.3231.83077.57041.080112.000710.420389.2557.000364.450354.695249.670696.495304.470432.870598.180298.62051.820138.05791.77082.8208.00663.37115.52026.02020.01717.01818.52223.03166.74157.57127.60127.10513.26519.35714.88631.00028.62617.58224.61014.59525.69030.00037.78256.48331.99116.01228.27430.77731.04134.15821.13340.06520.66522.92521.8776.68717.34022.378Time0.00 (6.02)0.02 (22.1)0.06 (16.07)0.04 (16.07)0.02 (14.06)0.02 (14.06)0.07 (18.08)0.30 (18.012)0.95 (104.000)0.32 (54.26)0.40 (13.031)5.08 (43.000)1.51 (13.53)9.17 (19.37)59.61 (17.541)1500.78 (31.000)33.06 (11.527)17.59 (11.21)2.54 (12.029)0.11 (10.19)53.50 (30.000)41.08 (30.000)23.25 (31.564)527.52 (116.500)32.55 (33.053)2.30 (10.024)191.49 (23.548)58.98 (28.555)0.84 (31.7833)179.24 (35.3167)304.47 (21.133333)31.22 (33.99)598.18 (20.665)9.14 (15.191)51.82 (21.877)32.63 (6.13)91.77 (17.34)82.82 (22.378)BestPlannerTBLTBLTBLTBLTBLTBLTBLCOLINLGP -tdTBLCOLINLGP -tdCOLINTBLCOLINLGP -tdCOLINTBLCOLINTBLLGP -tdLGP -tdCOLINLGP -tdCOLINCOLINCOLINCOLINTBLTBLTBLTBLLGP -tdCOLINCOLINTBLTBLCOLINQualityPlanner6.000 (0.03)22.1 (0.02)12.008 (0.23)16.07 (0.04)14.007 (0.09)14.06 (0.02)16.000 (0.31)18.012 (0.30)46.22 (6.34)36.02 (4.40)13.003 (1235.31)11.21 (19.88)13.000 (11.33)18.000 (1276.24)12.23 (69.77)31.000 (1500.78)11.527 (33.06)11.21 (17.59)12.029 (2.54)10.19 (0.11)21.38 (101.64)30.000 (41.08)31.564 (23.25)24.39 (619.96)30.93 (745.96)10.024 (2.30)23.548 (191.49)28.555 (58.98)29.833 (810.62)33.000 (1213.75)21.133333 (304.47)25.038 (160.42)20.665 (598.18)15.191 (9.14)21.877 (51.82)4.93 (269.57)17.34 (91.77)22.378 (82.82)LGP -sTBLCOLINTBLCOLINTBLLGP -tdCOLINTBLCOLINLGP -sTBLLGP -tdLGP -tdTBLLGP -tdCOLINTBLcolinTBLTBLLGP -tdCOLINTBLTBLCOLINCOLINCOLINLGP -tdLGP -tdTBLCOLINLGP -tdCOLINCOLINCOLINTBLCOLINTable 12: Results Complex Domains: Best results show best time (corresponding quality) planner(s) achieved time best quality (corresponding time)planner(s) achieving quality.89fiC OLES , C OLES , F OX & L ONGCOLINTimeroverstime10.0320.0130.0340.0350.06640.9370.0780.179100.29110.15120.8613141516170.70181.35192092.59satellitetime10.0220.0130.0140.0350.1160.1070.1280.3090.38100.64110.94123.571321.89146.19158.761622.181715.29183.851957.6620Quality67.00748.00663.0152.006125.014273.118105.017149.944177.022170.881122.022230.036245.864390.804129.596182.91678.616140.42290.12114.38126.544139.608175.74295.549283.351336.599433.308267.73292.436336.356232.66169.256520.602AverageTimeQualityBestTimePlannerQualityPlannerLGP -td67.007 (67.007)46.0007 (0.02)63.01 (0.03)52 (0.02)107.12 (0.702)273.118 (40.93)90.538574 (0.411)134 (0.04)128.6895 (0.24)154.48767 (1.448)170.881 (0.15)114.130005 (0.682)237.8161 (0.86)137.7917 (0.62)205.3755 (0.10)210 (0.46)230.036 (0.70)155.0909 (0.33)394.5915 (0.61)390.804 (92.59)COLIN0.0480.0270.0620.0400.25814.0100.1380.4940.1500.5340.1700.4760.5600.8650.5000.4602.2231.8170.61049.10773.52456.26970.52552.733129.069299.794107.121160.906137.345192.855195.395134.818292.200206.147239.333210.000341.230220.290394.591505.6670.02 (80)0.01 (48.006)0.02 (80)0.02 (52)0.02 (113.2)0.08 (284.965)0.03 (138.9286)0.04 (134)0.06 (146)0.06 (190.3077)0.07 (200.5)0.04 (145.5294)0.26 (346.5833)0.35 (268.7368)0.10 (205.3755)0.46 (210)0.34 (392.353)0.33 (155.0909)0.61 (394.5915)3.38 (502.7423)0.0260.0380.0680.0990.1540.1380.2650.4690.8130.9491.7039.14729.90010.61834.22534.88294.3905.30649.42430.264193.553225.716168.343319.688262.900255.894222.818205.241307.457262.206370.878423.836504.815387.271333.289510.100380.053290.099527.707854.9580.01 (205.28)0.00 (235.12)0.01 (78.616)0.01 (359.28)0.01 (254.563)0.01 (264.51)0.02 (296.16)0.03 (226.503)0.05 (351.8529)0.04 (231.485)0.07 (283.916)0.13 (389.4588)0.23 (464.408)0.15 (461.855)0.18 (267.4431)0.23 (602.7849)0.24 (378.459)0.10 (324.406)0.19 (352.355)0.24 (584.663)COLINLGP -tdLGP -s, LGP -tdLGP -tdLGP -tdLGP -tdLGP -tdLGP -tdLGP -tdLGP -tdLGP -tdLGP -tdLGP -tdLGP -tdLGP -tdLGP -tdLGP -tdLGP -tdLGP -tdLGP -s, LGP -td,TBLLGP -tdCOLIN ,TBLTBLTBLTBLTBLTBLLGP -td,TBLLGP -tdTBLLGP -tdLGP -tdLGP -tdLGP -tdLGP -tdLGP -tdTBLLGP -tdLGP -td129.596 (0.02)182.916 (0.01)78.616 (0.01)140.42 (0.03)162.39699 (0.528)114.38 (0.10)126.544 (0.12)139.608 (0.30)175.74 (0.38)231.485 (0.04)283.351 (0.94)336.599 (3.57)433.308 (21.89)267.73 (6.19)264.6641 (2.48)336.356 (22.18)232.66 (15.29)169.256 (3.85)352.355 (0.19)498.90106 (113.277)LGP -sCOLINLGP -tdSapaCOLINSapaLGP -tdLGP -sSapaCOLINSapaLGP -sLGP -sLGP -tdLGP -tdCOLINLGP -tdLGP -tdCOLINCOLINCOLINCOLINCOLINSapaCOLINCOLINCOLINCOLINLGP -tdCOLINCOLINCOLINCOLINLGP -sCOLINCOLINCOLINLGP -tdTable 13: Results Complex Domains: Best results show best time (corresponding quality) planner(s) achieved time best quality (corresponding time)planner(s) achieving quality.90SapafiC OLIN : P LANNING C ONTINUOUS C HANGECOLINTimezenotime10.0120.0130.0340.0850.0460.0870.0688.5390.22100.41110.411210.34131.2814371.61156.9516130.3317443.7918188.851920Quality3.67223.43510.08921.2878.19621.96633.3143.72239.94936.45822.26472.13986.473117.625381.626117.05277.33289.082AverageTimeQuality0.0180.0190.0330.0840.0750.0750.1102.2500.1940.3100.2712.8741.592256.8135.52443.183161.24377.35773.04080.6603.48923.67213.33622.34022.01620.92124.86330.68948.52833.48725.57651.10558.23572.619241.31886.693118.17970.542137.90991.146BestTimePlannerQualityPlanner0.01 (0.01)0.01 (0.01)0.01 (14.4211)0.01 (21.708)0.02 (27.0419)0.02 (20.8864)0.01 (25.6744)0.02 (24.2375)0.03 (72.1579)0.06 (46.1848)0.05 (46.1576)0.09 (42.1671)0.07 (42.0593)0.62 (58.8983)0.87 (274.8496)3.07 (67.554)3.97 (117.1512)4.48 (56.8345)9.83 (168.0886)28.69 (78.4703)COLIN , LPG -td3.424 (0.01)23.431 (0.01)10.089 (0.03)21.287 (0.08)8.196 (0.04)16.578 (0.17)18.283 (0.05)24.238 (0.02)23.238 (0.395)20.864 (0.14)13.666 (0.443)38.992 (0.846)42.059 (0.07)39.064 (651.493)117.171 (8.644)54.717 (23.883)77.332 (443.79)56.835 (4.48)107.729 (136.25)78.470 (28.69)LPG -tdCOLIN , LPG -tdLPG -tdLPG -sLPG -tdLPG -tdLPG -tdLPG -tdLPG -tdLPG -tdLPG -tdLPG -tdLPG -tdLPG -tdLPG -tdLPG -tdLPG -tdLPG -tdLPG -tdLPG -tdLPG -tdCOLINCOLINCOLINSapaLPG -sLPG -tdSapaLPG -sSapaSapaLPG -tdSapaSapaSapaCOLINLPG -tdLPG -sLPG -tdTable 14: Results Complex Domains: Best results show best time (corresponding quality) planner(s) achieved time best quality (corresponding time)planner(s) achieving quality.91fiC OLES , C OLES , F OX & L ONGReferencesAudemard, G., Bertoli, P., Cimatti, A., Kornilowicz, A., & Sebastiani, R. (2002). SAT-based approach solving formulas boolean linear mathematical propositions. Proceedings 18th International Conference Automated Deduction, Vol. 2392, pp. 193208.Springer-Verlag, LNAI Series.Blum, A., & Furst, M. (1995). Fast Planning Planning Graph Analysis. ProceedingsInternational Joint Conference Artificial Inteligence (IJCAI).Boddy, M. S., & Johnson, D. P. (2002). New Method Global Solution Large Systems Continuous Constraints. Proceedings 1st International Workshop GlobalConstraint Optimization Constraint Satisfaction (COCOS), Vol. 2861 Lecture NotesComputer Science, pp. 142156. Springer.Cesta, A., & Oddi, A. (1996). Gaining Efficiency Flexibility Simple Temporal Problem.Proceedings 3rd International Workshop Temporal Representation Reasoning(TIME).Cesta, A., Cortellessa, G., Fratini, S., & Oddi, A. (2009). Developing End-to-End PlanningApplication Timeline Representation Framework. Proceedings 21st ConferenceInnovative Applications Artificial Intelligence (IA*AI).Chien, S. A., Tran, D., Rabideau, G., Schaffer, S. R., Mandl, D., & Frye, S. (2010). Timeline-BasedSpace Operations Scheduling External Constraints. Proceedings InternationalConference AI Planning Scheduling (ICAPS), pp. 3441.Cimatti, A., Giunchiglia, F., Giunchiglia, E., & Traverso, P. (1997). Planning via Model Checking:Decision Procedure R. Recent Advances AI Planning, 4th European ConferencePlanning, ECP, pp. 130142.Coles, A. I., Fox, M., Halsey, K., Long, D., & Smith, A. J. (2008). Managing concurrencytemporal planning using planner-scheduler interaction. Artificial Intelligence, 173, 144.Coles, A. I., Fox, M., Long, D., & Smith, A. J. (2008a). Planning Problems Requiring TemporalCoordination. Proceedings 23rd AAAI Conference Artificial Intelligence (AAAI08).Coles, A. I., Fox, M., Long, D., & Smith, A. J. (2008b). Hybrid Relaxed Planning GraphLPHeuristic Numeric Planning Domains. Proceedings 18th International Conference Automated Planning Scheduling (ICAPS), pp. 5259.Coles, A. J., Coles, A. I., Fox, M., & Long, D. (2009a). Extending Use Inference Temporal Planning Forwards Search. Proceedings 19th International ConferenceAutomated Planning Scheduling (ICAPS 09).Coles, A. J., Coles, A. I., Fox, M., & Long, D. (2009b). Temporal Planning Domains LinearProcesses. Proceedings 21st International Joint Conference Artificial Intelligence(IJCAI). AAAI Press.Cushing, W., Kambhampati, S., Mausam, & Weld, D. (2007). temporal planning reallytemporal planning?. Proceedings International Joint Conference AI (IJCAI), pp.18521859.92fiC OLIN : P LANNING C ONTINUOUS C HANGEDechter, R., Meiri, I., & Pearl, J. (1989). Temporal Constraint Networks. Proceedings Principles Knowledge Representation Reasoning (KR), pp. 8393. Toronto, Canada.Dierks, H. (2005). Finding Optimal Plans Domains Restricted Continuous EffectsUPPAAL-Cora. ICAPS Workshop Verification Validation Model-Based PlanningScheduling Systems.Do, M. B., & Kambhampati, S. (2003). Sapa: Multi-objective Metric Temporal Planner. JournalArtificial Intelligence Research (JAIR), 20, 155194.Edelkamp, S. (2003). Taming numbers durations model-checking integrated planningsystem. Journal Artificial Intelligence Research (JAIR), 20, 195238.Edelkamp, S., & Jabbar, S. (2006). Cost-Optimal External Planning. Proceedings 21stNational (American) Conference Artificial Intelligence (AAAI). AAAI Press.Eyerich, P., Mattmuller, R., & Roger, G. (2009). Using Context-enhanced Additive HeuristicTemporal Numeric Planning. Proceedings 19th International ConferenceAutomated Planning Scheduling (ICAPS 2009). AAAI Press.Fox, M., & Long, D. (2003). PDDL2.1: extension PDDL expressing temporal planningdomains. Journal Artificial Intelligence Research (JAIR), 20, 61124.Fox, M., & Long, D. (2006). Modelling Mixed Discrete-Continuous Domains Planning. JournalArtificial Intelligence Research (JAIR), 27, 235297.Fox, M., Howey, R., & Long, D. (2005). Validating Plans Context Processes ExogenousEvents. Proceedings 20th National Conference Artificial Intelligence 17thInnovative Applications Artificial Intelligence Conference (AAAI), pp. 11511156.Fox, M., Long, D., & Halsey, K. (2004). Investigation Expressive Power PDDL2.1.Proceedings 16th European Conference Artificial Intelligence (ECAI).Fox, M., Long, D., & Magazzeni, D. (2011). Automatic Construction Efficient Multiple BatteryUsage Policies. Proceedings 21st International Conference Automated PlanningScheduling (ICAPS).Frank, J., & Jonsson, A. K. (2003). Constraint-Based Attribute Interval Planning. Constraints,8(4), 339364.Garrido, A., Fox, M., & Long, D. (2002). Temporal Planning System Durative ActionsPDDL2.1. Proceedings 15th Eureopean Conference Artificial Intelligence(ECAI), pp. 586590.Garrido, A., Onainda, E., & Barber, F. (2001). Temporal Planning System Time-OptimalPlanning. Proceedings 10th Portuguese Conference Artificial Intelligence, pp.379392. Springer.Gerevini, A., Saetti, A., & Serina, I. (2006). Approach Temporal Planning SchedulingDomains Predictable Exogenous Events. Journal Artificial Intelligence Research(JAIR), 25, 187231.Gerevini, A., Saetti, A., & Serina, I. (2010). Temporal Planning Problems Requiring Concurrency Action Graphs Local Search. Proceedings 20th InternationalConference Automated Planning Scheduling (ICAPS).93fiC OLES , C OLES , F OX & L ONGGerevini, A., & Serina, I. (2000). Fast Plan Adaptation Planning Graphs: Local Systematic Search Techniques. Proceedings 5th International Conference ArtificialIntelligence Planning Systems (AIPS), pp. 112121.Ghallab, M., & Laruelle, H. (1994). Representation Control IxTeT, Temporal Planner.Proceedings 2nd International Conference Artificial Intelligence Planning Systems(AIPS), pp. 6167.Halsey, K. (2005). CRIKEY!: co-ordination temporal planning. Ph.D. thesis, UniversityDurham.Haslum, P., & Geffner, H. (2001). Heuristic planning time resources. Proceedings6th European Conference Planning (ECP01), pp. 121132.Haslum, P. (2009). Admissible Makespan Estimates PDDL2.1 Temporal Planning. Proceedings ICAPS Workshop Heuristics Domain-Independent Planning.Helmert, M. (2006). Fast Downward Planning System. Journal Artificial Intelligence (JAIR),26, 191246.Henzinger, T. (1996). Theory Hybrid Automata. Proceedings 11th Annual Symposium Logic Computer Science. Invited tutorial., pp. 278292. IEEE Computer SocietyPress.Henzinger, T., Ho, P.-H., & Wong-Toi, H. (1995). user guide HYTECH. E. Brinksma,W.R. Cleaveland, K.G. Larsen, T. Margaria, B. Steffen, editors, Tool AlgorithmsConstruction Analysis Systems: (TACAS 95), volume 1019 Lecture NotesComputer Science, pp. 4171.Hoffmann, J. (2003). Metric-FF Planning System: Translating Ignoring Delete Lists Numeric State Variables. Journal Artificial Intelligence Research (JAIR), 20, 291341.Hoffmann, J., & Edelkamp, S. (2005). Deterministic Part IPC-4: Overview. JournalArtificial Intelligence Research (JAIR), 24, 519579.Hoffmann, J., & Nebel, B. (2001). FF planning system: Fast plan generation heuristicsearch. Journal Artificial Intelligence Research (JAIR), 14, 253302.Huang, R., Chen, Y., & Zhang, W. (2009). Optimal Temporally Expressive Planner: InitialResults Application P2P Network Optimization. Proceedings InternationalConference Automated Planning Scheduling (ICAPS).Knight, R., Schaffer, S., & B.Clement (2009). Power planning international space stationdomain. Proceedings 6th International Workshop Planning SchedulingSpace (IWPSS).Lamba, N., Dietz, M., Johnson, D. P., & Boddy, M. S. (2003). Method Global OptimizationLarge Systems Quadratic Constraints. Proceedings 2nd International WorkshopGlobal Optimization Constraint Satisfaction (COCOS), Vol. 3478 Lecture NotesComputer Science, pp. 6170. Springer.Leaute, T., & Williams, B. (2005). Coordinating Agile Systems Model-based ExecutionTemporal Plans. Proceedings 20th National Conference AI (AAAI).Li, H., & Williams, B. (2008). Generative systems hybrid planning based flow tubes. Proc.18th Int. Conf. Aut. Planning Scheduling (ICAPS).94fiC OLIN : P LANNING C ONTINUOUS C HANGELi, H., & Williams, B. (2011). Hybrid Planning Temporally Extended Goals SustainableOcean Observing. Proceedings International Conference AssociationAdvancement AI (AAAI): Special Track Sustainability AI.Long, D., & Fox, M. (2003a). Exploiting Graphplan Framework Temporal Planning.Proceedings 13th International Conference Automated Planning Scheduling(ICAPS), pp. 5261.Long, D., & Fox, M. (2003b). 3rd International Planning Competition: Results Analysis.Journal Artificial Intelligence Research (JAIR), 20, 159.Lougee-Heimer, R. (2003). Common Optimization INterface Operations Research. IBMJournal Research Development, 47(1), 5766.McDermott, D. (2003). Reasoning Autonomous Processes Estimated RegressionPlanner. Proceedings 13th International Conference Automated PlanningScheduling (ICAPS).McDermott, D. V. (2000). 1998 AI Planning Systems Competition. AI Magazine, 21(2), 3555.Meuleau, N., Benazera, E., Brafman, R. I., Hansen, E. A., & Mausam (2009). Heuristic SearchApproach Planning Continuous Resources Stochastic Domains. Journal ArtificialIntelligence Research (JAIR), 34, 2759.Palacios, H., & Geffner, H. (2009). Compiling Uncertainty Away Conformant Planning ProblemsBounded Width. Journal Artificial Intelligence Research (JAIR), 35, 623675.Pednault, E. P. D. (1989). ADL: Exploring Middle Ground STRIPS SituationCalculus. Proceedings International Conference Knowledge Representation (KR),pp. 324332.Pell, B., Gat, E., Keesing, R., Muscettola, N., & Smith, B. D. (1997). Robust Periodic PlanningExecution Autonomous Spacecraft. Proceedings International Joint ConferenceAI (IJCAI), pp. 12341239.Penberthy, S., & Weld, D. (1994). Temporal Planning Continuous Change. Proceedings12th National Conference AI (AAAI), pp. 10101015. AAAI/MIT Press.Penna, G. D., Intrigila, B., Magazzeni, D., & Mercorio, F. (2009). UPMurphi: Tool Universal Planning PDDL+ Problems. Proceedings 19th International ConferenceAutomated Planning Scheduling (ICAPS 2009), pp. 1923. AAAI Press.Penna, G. D., Intrigila, B., Magazzeni, D., & Mercorio, F. (2010). PDDL+ Benchmark Problem:Batch Chemical Plant. Proceedings International Conference AI PlanningScheduling (ICAPS), pp. 222225.Reddy, S. Y., Frank, J. D., Iatauro, M. J., Boyce, M. E., Kurklu, E., Ai-Chang, M., & Jonsson,A. K. (2011). Planning Solar Array Operations International Space Station. ACMTransactions Intelligent Systems Technology, 2, 124.Richter, S., & Westphal, M. (2010). LAMA Planner: Guiding Cost-Based Anytime PlanningLandmarks. Journal Artificial Intelligence Research (JAIR), 39, 127177.Shin, J., & Davis, E. (2005). Processes Continuous Change SAT-based Planner. ArtificialIntelligence, 166, 194253.95fiC OLES , C OLES , F OX & L ONGSmith, D., & Weld, D. S. (1999). Temporal Planning Mutual Exclusion Reasoning. Proceedings 16th International Joint Conference AI (IJCAI), pp. 326337.Veloso, M., Perez, M., & Carbonell, J. (1990). Nonlinear planning parallel resource allocation.Proceedings DARPA Workshop Innovative Approaches Planning, SchedulingControl, pp. 207212.Vidal, V., & Geffner, H. (2006). Branching pruning: optimal temporal POCL planner basedconstraint programming. Artificial Intelligence, 170(3), 298335.Wolfman, S., & Weld, D. (1999). LPSAT System Application Resource Planning.Proceedings 16th International Joint Conference Artificial Intelligence (IJCAI).Yi, W., Larsen, K., & Pettersson, P. (1997). UPPAAL Nutshell. International JournalSoftware Tools Technology Transfer, 1(1).Younes, H. L. S., & Simmons, R. G. (2003). VHPOP: Versatile heuristic partial order planner..Journal Artificial Intelligence Research (JAIR), 20, 405430.96fiJournal Artificial Intelligence Research 44 (2012) 709-755Submitted 04/12; published 08/12Online Speedup Learning Optimal PlanningCarmel DomshlakErez KarpasDCARMEL @ IE . TECHNION . AC . ILKARPASE @ TECHNION . AC . ILFaculty Industrial Engineering ManagementTechnion - Israel Institute TechnologyHaifa, 32000, IsraelShaul MarkovitchSHAULM @ CS . TECHNION . AC . ILFaculty Computer ScienceTechnion - Israel Institute TechnologyHaifa, 32000, IsraelAbstractDomain-independent planning one foundational areas field Artificial Intelligence. description planning task consists initial world state, goal, set actionsmodifying world state. objective find sequence actions, is, plan,transforms initial world state goal state. optimal planning, interested finding plan, one cheapest plans. prominent approach optimal planningdays heuristic state-space search, guided admissible heuristic functions. Numerous admissibleheuristics developed, strengths weaknesses, well knownsingle best heuristic optimal planning general. Thus, heuristicchoose given planning task difficult question. difficulty avoided combiningseveral heuristics, requires computing numerous heuristic estimates state,tradeoff time spent time saved combined advantagesdifferent heuristics might high. present novel method reduces cost combining admissible heuristics optimal planning, maintaining benefits. Using idealizedsearch space model, formulate decision rule choosing best heuristic computestate. present active online learning approach learning classifier decisionrule target concept, employ learned classifier decide heuristic computestate. evaluate technique empirically, show substantially outperformsstandard method combining several heuristics via pointwise maximum.1. Introductioncenter problem intelligent autonomous behavior task selecting actionstake next. Planning AI best conceived model-based approach automated actionselection (Geffner, 2010). models represent current situation, goals, possible actions.Planning-specific languages used describe models concisely. main challengeplanning computational, planning languages lead intractable problems worstcase. However, using rigorous search-guidance tools often allows efficient solving interestingproblem instances.classical planning, concerned synthesis plans constituting goal-achievingsequences deterministic actions, significant algorithmic progress achieved lasttwo decades. turn, progress classical planning translated advances involvedplanning languages, allowing uncertainty feedback (Yoon, Fern, & Givan, 2007; Palaciosc2012AI Access Foundation. rights reserved.fiD OMSHLAK , K ARPAS , & ARKOVITCH& Geffner, 2009; Keyder & Geffner, 2009; Brafman & Shani, 2012). optimal planning,objective find plan, find one cheapest plans.prominent approach domain-independent planning, optimal planning particular,state-space heuristic search. natural view planning task search problem,use heuristic search algorithm solve it. Recent advances automatic construction heuristicsdomain-independent planning established many heuristics choose from,strengths weaknesses. However, wealth heuristics leads new question: givenspecific planning task, heuristic choose?paper, propose selective max online learning approach combinesstrengths several heuristic functions, leading speedup optimal heuristic-search planning.high level, selective max seen hyper-heuristic (Burke, Kendall, Newall, Hart, Ross,& Schulenburg, 2003) heuristic choosing among heuristics. based seemingly trivial observation that, state, one heuristic best state.principle, possible compute several heuristics state, choose one according values provide. However, heuristic computation domain-independent planningtypically expensive, thus computing several heuristic estimates state takes long time.Selective max works predicting state heuristic yield best heuristicestimate, computes heuristic.always clear decide best heuristic state is, firstanalyze idealized model search space describe choose best heuristicstate order minimize overall search time. describe online active learningprocedure uses decision rule formulated idealized model. procedure constitutesessence selective max.experimental evaluation, conducted using three state-of-the-art heuristicsdomain-independent planning, shows selective max effective combining severalheuristics optimal search. Furthermore, results show using selective max resultsspeedup baseline heuristic combination method, selective max robust different parameter settings. claims supported selective max runnerup ex-aequo last International Planning Competition, IPC-2011 (Garca-Olaya, Jimenez, &Linares Lopez, 2011).paper expands conference version (Domshlak, Karpas, & Markovitch, 2010)several ways. First, improve expand presentation selective max decision rule.Second, explain handle non-uniform action costs principled way. Third, empiricalevaluation greatly extended, includes results IPC-2011, well controlledexperiments three different heuristics, exploration parameters selectivemax affect performance.2. Previous WorkSelective max speedup learning system. general, speedup learning concerned improving performance problem solving system experience. computational difficultydomain-independent planning led many researchers use speedup learning techniques orderimprove performance planning systems; survey many these, see workMinton (1994), Zimmerman Kambhampati (2003), Fern, Khardon, Tadepalli (2011).710fiO NLINE PEEDUP L EARNING PTIMAL P LANNINGSpeedup learning systems divided along several dimensions (Zimmerman & Kambhampati, 2003; Fern, 2010). Arguably important dimension phase learning takesplace. offline, inter-problem, speedup learner analyzes problem solvers performancedifferent problem instances attempt formulate rule would improveperformance would also generalize well future problem instances. Offline learningapplied extensively domain-independent planning, varying degrees success (Fern et al.,2011). However, one major drawback offline learning need training examplescase, planning tasks domains interest.Learning also take place online, problem solving. online, intra-problem,speedup learner invoked problem solver concrete problem instance solverworking on, attempts learn online, objective improving solvers performancespecific problem instance solved. general, online learners assumed pretrained other, previously seen problem instances; information relycollected process solving concrete problem instance called for. Onlinelearning shown extremely helpful propositional satisfiability (SAT) generalconstraint satisfaction (CSP) solving, nogood learning clause learning amongessential components state-of-the-art solver (Schiex & Verfaillie, 1993; Marques-Silva& Sakallah, 1996; Bayardo Jr. & Schrag, 1997). Thus, indirectly, SAT- CSP-based domainindependent planners already benefit online learning techniques (Kautz & Selman, 1992;Rintanen, Heljanko, & Niemela, 2006). However, best knowledge, work firstapplication online learning optimal heuristic-search planning.3. Backgrounddomain-independent planning task (or planning task, short) consists descriptioninitial state, goal, set available operators. Several formalisms describing planning tasksuse, including STRIPS (Fikes & Nilsson, 1971), ADL (Pednault, 1989), SAS+ (Backstrom& Klein, 1991; Backstrom & Nebel, 1995). describe SAS+ formalism, one usedFast Downward planner (Helmert, 2006), top implemented evaluatedselective max. Nothing, however, precludes using selective max context formalisms.SAS+ planning task given 4-tuple = hV, A, s0 , Gi. V = {v1 , . . . , vn } set statevariables, associated finite domain dom(vi ). complete assignment V calledstate. s0 specified state called initial state, goal G partial assignment V .finite set actions. action given pair hpre(a), eff(a)i partial assignments Vcalled preconditions effects, respectively. action also associated cost C(a) R0+ .action applicable state iff |= pre(a). Applying changes value statevariable v eff(a)[v] eff(a)[v] specified. resulting state denoted sJaK. denotestate obtained sequential application (respectively applicable) actions a1 , . . . , akstarting state sJha1 , . . . , ak iK. action sequence plan s0 Jha1 , . . . , ak iK |= G.optimal planning, interested finding onePthe cheapest plans, cost planha1 , . . . , ak sum constituent action costs ki=1 C(ai ).SAS+ planning task = hV, A, s0 , Gi easily seen state-space search problemwhose states simply complete assignments variables V , transitions uniquely determined actions A. initial goal states also defined initial state goal .optimal solution state-space search problem found using search algorithm711fiD OMSHLAK , K ARPAS , & ARKOVITCHadmissible heuristic h. heuristic evaluation function h assigns estimate distanceclosest goal state state evaluates. length cheapest path stategoal denoted h (s), h called admissible never overestimates true goal distanceis, h(s) h (s) state s. works expanding states order increasingf (s) := g(s) + h(s), g(s) cost cheapest path initial state knownfar.4. Selective Max Decision RuleMany admissible heuristics proposed domain-independent planning; varycheap compute yet accurate, accurate yet expensive compute. general,accurate heuristic is, fewer states would expanded using it.accuracy heuristic functions varies different planning tasks, even different statestask, may able produce robust optimal planner combining several admissible heuristics. Presumably, heuristic accurate, is, provides higher estimates,different regions search space. simplest best-known way using point-wise maximum heuristics use state. Given n admissible heuristics,h1 , . . . , hn , new heuristic, maxh , defined maxh (s) := max1in hi (s). easy seemaxh (s) hi (s) state heuristic hi . Thus search using maxh expectedexpand fewer states using individual heuristic.PHowever, denote time neededcompute hi ti , time needed compute maxh ni=1 ti .mentioned previously, selective max form hyper-heuristic (Burke et al., 2003)chooses heuristic compute state. view selective max decision rule dr,given set heuristics h1 , . . . , hn state s, chooses heuristic computestate. One natural candidate decision rule heuristic yields highest,is, accurate, estimate:drmax ({h1 , . . . , hn }, s) := hargmax1in hi (s) .Using decision rule yields heuristic accurate maxh , still computingone heuristic per state time targmax1in hi (s) .analysis, however, take account different computation times different heuristics. instance, let h1 h2 pair admissible heuristics h2 h1 .priori, seems using h2 always preferred using h1 formercause expand fewer states. However, suppose given planning task, expands 1000states guided h1 100 states guided h2 . computing h1 statetakes 10 ms, computing h2 state takes 1000 ms, switching h1 h2 increasesoverall search time. Using maxh h1 h2 makes things worse, h2 h1 ,thus computing maximum simply wastes time spent computing h1 . possible,however, computing h2 carefully chosen states, computing h1 states,would result expanding 100 states, reducing overall search time comparedrunning h2 .example shows, even given knowledge heuristics estimates advance,clear heuristic computed state objective minimize overallsearch time. Therefore, begin formulating decision rule choosing one twoheuristics, respect idealized state-space model. Selective max operates online712fiO NLINE PEEDUP L EARNING PTIMAL P LANNINGs0f2 = cf1 = csgFigure 1: illustration idealized search space model f -contours two admissibleheuristicsactive learning procedure, attempting predict outcome decision rule chooseheuristic compute state.4.1 Decision Rule Perfect Knowledgeformulate decision rule choosing two given admissible heuristics, h1 h2 ,compute state idealized search space model. order formulate decisionrule, make following assumptions:search space tree single goal, constant branching factor b, uniform costactions. idealized search space model used past analyze behavior(Pearl, 1984).time ti required computing heuristic hi independent state evaluated;w.l.o.g. assume t2 t1 .heuristics consistent. heuristic h said consistent obeys triangleinequality: two states s, s0 , h(s) h(s0 ) + k(s, s0 ), k(s, s0 ) optimal costreaching s0 s.have: (i) perfect knowledge structure search tree, particularcost optimal solution c , (ii) perfect knowledge heuristic estimatesstate, (iii) perfect tie-breaking mechanism.Obviously, none assumptions holds typical search problems, later examineindividual influence framework.Adopting standard notation, let g(s) cost cheapest path s0 s. Definingmaxh (s) = max(h1 (s), h2 (s)), use notation f1 (s) = g(s) + h1 (s), f2 (s) = g(s) +h2 (s), maxf (s) = g(s) + maxh (s). algorithm consistent heuristic h expandsstates increasing order f = g + h (Pearl, 1984). particular, every state f (s) <h (I) = c surely expanded , every state f (s) > c surely713fiD OMSHLAK , K ARPAS , & ARKOVITCHexpanded . states f (s) = c might might expanded , dependingtie-breaking rule used. perfect tie-breaking assumption, statesf (s) = c expanded lie along optimal plan.Let us consider states satisfying f1 (s) = c (the dotted line Fig. 1) satisfyingf2 (s) = c (the solid line Fig. 1). states f1 = c f2 = c contourssurely expanded h1 h2 , respectively. states contours(the grid-marked region Fig. 1), is, states SE = {s | maxf (s) < c },surely expanded using maxh (Pearl, 1984, Thm. 4, p. 79).objective minimizing search time, note optimal decision stateSE compute heuristic all, since states surely expanded anyway.Assuming still must choose one heuristics, would choose compute cheaperheuristic h1 . Another easy case f1 (s) c . states, computing h1 (s) sufficesensure surely expanded, using perfect tie-breaking rule, expandedunless must be. h1 also cheaper compute h2 , h1 preferred, regardlessheuristic estimate h2 state s.Let us consider optimal decision states, is, f1 (s) < cf2 (s) c . fact, enough consider shallowest states; Figure 1,states part f2 = c contour separates grid-marked line-markedareas. Since f1 (s) f2 (s) based g(s), h2 (s) > h1 (s), is, h2accurate state h1 . interested solely reducing state expansions, h2would obviously right heuristic compute s. However, objective reducingactual search time, h2 may actually wrong choice might much expensivecompute h1 .Let us consider effects two alternatives. compute h2 (s),surely expanded, f2 (s) = c , thus whether expands depends tiebreaking. before, assuming perfect tie-breaking, thus expanded unlessmust be. Computing h2 would cost us t2 time.contrast, compute h1 (s), surely expanded f1 (s) < c . Notecomputing h2 computing h2 one descendants s0 clearly sub-optimalstrategy pay cost computing h2 , yet pruning limited searchsub-tree rooted s0 . Therefore, choices really either computing h2 s, computing h1states sub-tree rooted lie f1 = c contour. Suppose needexpand l complete levels state space reach f1 = c contour. Thus, needgenerate order bl states, invest bl t1 time calculating h1 states lief1 = c contour.Considering two options, optimal decision state thus compute h2 iff t2 < bl t1 ,express differently, l > logb ( tt12 ). special case, heuristics take timecompute, decision rule reduces l > 0, is, optimal choice simply accurateheuristic state s.Putting cases together yields decision rule dropt , below, lsdepth go f1 (s) = c :714fiO NLINE PEEDUP L EARNING PTIMAL P LANNINGh1 , f1 (s) < c , f2 (s) < ch , f (s) c11.dropt ({h1 , h2 }, s) :=h1 , f1 (s) < c , f2 (s) c , ls logb ( tt12 )h , f (s) < c , f (s) c , l > log ( t2 )212b t14.2 Decision Rule without Perfect Knowledgeidealized model makes several assumptions, appear problematicmeet practice. examine assumptions closely, needed, suggestpragmatic compromises.First, model assumes search space forms tree single goal state,heuristics question consistent, perfect tie-breaking rule. Althoughfirst assumption hold planning tasks, second assumption satisfiedmany state-of-the-art heuristics (Karpas & Domshlak, 2009; Helmert & Domshlak, 2009; Bonet& Helmert, 2010), third assumption realistic, prevent us usingdecision rule suggested model.idealized model also assumes branching factor heuristic computationtimes constant across search states. application decision rule planningpractice, deal assumption adopting average branching factor heuristiccomputation times, estimated random sample search states.Finally, decision rule dropt requires unrealistic knowledge heuristic estimates,well optimal plan cost c depth ls go state f1 (s) = c .obviously knowledge practice, must use approximation decisionrule.first approximation make ignore trivial cases require knowledge c ;cases either surely expanded, h1 enough prune s. Instead, applyreasoning complicated case states, resulting following decision rule:(h1 , ls logb ( tt12 )drapp1 ({h1 , h2 }, s) :=.h2 , ls > logb ( tt21 )next step somehow estimate depth go ls number layers needexpand tree f1 reaches c . order derive useful decision rule, assume lspositive correlation h (s) = h2 (s) h1 (s); is, h1 h2 close, ls low,h1 yields much lower estimate h2 , implying h1 accurate s,depth go f1 (s) = c large. approximation uses simplest correlationlinear one h (s) ls , hyper-parameter controlling slope.Recall idealized model, actions unit cost, thus cost-to-go depthto-go same. However, planning tasks, notably, planning tasks 2008International Planning Competition, feature non-uniform action costs. Therefore, decision ruleconverts heuristic estimates cost-to-go heuristic estimates depth-to-go dividingcost-to-go estimate average action cost. modifying estimate depthto-go, ls , average action cost, denote c. Plugging715fiD OMSHLAK , K ARPAS , & ARKOVITCHdecision rule yields:(h1 ,drapp2 ({h1 , h2 }, s) :=h2 ,h (s) c logb ( tt12 ).h (s) > c logb ( tt21 )Given b, t1 , t2 , c, quantity c logb (t2 /t1 ) becomes fixed, follows denotesimply threshold .Note linear correlation h (s) ls occurs simple cases. firstcase h1 value remains constant subtree rooted s, is, additive errorh1 increases 1 level s. case, f1 increases 1 expanded levelsub-tree (because h1 remains same, g increases 1), take expanding exactlyh (s) = h2 (s) h1 (s) levels reach f1 = c contour. second caseabsolute error h1 remains constant, is, h1 increases 1 level expanded, f1increases 2. case, need expand h (s)/2 levels. generalizedcase estimate h1 increases constant additive factor c, results h (s)/(c+1)levels expanded.Furthermore, empirical evidence support conclusion exponentialgrowth search effort function heuristic error, even assumptions mademodel hold. particular, experiments Helmert Roger (2008) IPC benchmarksheuristics small constant additive errors show number expanded nodestypically grows exponentially (still small additive) error increases.Finally, remark decision rule always chooses admissible heuristic,resulting heuristic estimate always admissible. Thus, even chosen heuristiccorrect one according dropt , result loss optimality solution,possible increase search time.5. Online Learning Decision Ruledecision rule drapp2 still requires knowledge h1 h2 , use binarylabel state. compute value decision rule paying computationtime heuristics, t1 + t2 , and, importantly, use binary classifier predictvalue decision rule unknown state. Note use classifier online,problem solving process, time spent learning classification counted time spentproblem solving. Furthermore, active learning, choose pay labelstate, payment also computation time. Therefore refer setting activeonline learning.follows, provide general overview selective max procedure, describeseveral alternatives components. decision rule states expensiveheuristic h2 computed search state h2 (s) h1 (s) > . decision ruleserves binary target concept, corresponds set states expensiveheuristic h2 significantly accurate cheaper heuristic h1 states where, according model, reduction expanded states computing h2 outweighs extra timeneeded compute it. Selective max uses binary classifier predict value decisionrule. several steps building classifier:716fiO NLINE PEEDUP L EARNING PTIMAL P LANNINGevaluate(s)hh, conf idencei := CLASSIFY(s, model)(conf idence > )return h(s)elselabel := h1h2 (s) h1 (s) > c logb (t2 /t1 ) label := h2update model hs, labelireturn max(h1 (s), h2 (s))Figure 2: selective max state evaluation procedure1. Training Example Collection: first need collect training examples,representative entire search space. Several state-space sampling methods discussedSection 5.1.2. Labeling Training Examples: training examples collected, first usedestimate average branching factor b, average heuristic computation times t1 t2 ,average action cost c. b, t1 , t2 , c estimated, use estimatethreshold = c logb (t2 /t1 ) decision rule.generate label training example calculating h (s) = h2 (s) h1 (s),comparing decision threshold: h (s) > , label h2 , otherwiseh1 . t1 > t2 simply switch heuristics decision always whethercompute expensive heuristic; default compute cheaper heuristic,unless classifier says otherwise.3. Feature Extraction: obtained set training examples, must decidefeatures characterize example. Since target concept based heuristic values,features represent information heuristics derived typicallyproblem description current state.several feature-construction techniques characterizing states planning tasksproposed previous literature (Yoon, Fern, & Givan, 2008; de la Rosa, Jimenez, &Borrajo, 2008), designed inter-problem learning, is, learningdifferent planning tasks already solved offline. However, approach,concerned one problem, online setting, thus techniquesapplicable. implementation, use simplest features possible, takingstate variable feature. empirical evaluation demonstrates, even elementaryfeatures suffice selective max perform well.4. Learning: set labeled training examples, represented vectorfeatures, train binary classifier. Several different choices classifier discussedSection 5.2.completing steps described above, binary classifier usedpredict value decision rule. However, classifier likely perfect accuracy,717fiD OMSHLAK , K ARPAS , & ARKOVITCHconsult confidence classifier associates classification. resulting stateevaluation procedure selective max depicted Figure 2. every state evaluatedsearch algorithm, use classifier decide heuristic compute. classificationconfidence exceeds confidence threshold , parameter selective max, indicatedheuristic computed s. Otherwise, conclude enough information makeselective decision s, compute regular maximum h1 (s) h2 (s). However,use opportunity improve quality prediction states similar s, updateclassifier generating label based h2 (s)h1 (s) learning newly labeled example.decisions dedicate computation time obtain label new example constituteactive part learning procedure. also possible update estimates b, t1 , t2 , c,change threshold accordingly. However, would result concept tryinglearn constantly changing phenomenon known concept drift usually affectslearning adversely. Therefore, update threshold .5.1 State-Space Samplinginitial state-space sample serves two purposes. First, used estimate branching factorb, heuristic computation times t1 t2 , average action cost c, computethreshold = c logb (t2 /t1 ), used specify concept. concept specified,state-space sample also provides us set examples classifier initiallytrained. Therefore, important initial state-space sample representativestates evaluated search. number states initial sample controlledparameter N .One option use first N states search. However, method biased towardsstates closer initial state, therefore likely represent search space well. Thus,discuss three sophisticated state-space sampling procedures, basedperforming random walks, probes, initial state. details samplingprocedures vary, probe terminates pre-set depth limit.first sampling procedure, refer biased probes, uses inverse heuristicselection bias choosing next state go probe. Specifically, probabilitychoosing state successor random walk continue proportional1/ maxh (s). biases sample towards states lower heuristic estimates,likely expanded search.second sampling procedure similar first one, except chooses successoruniformly, thus refer unbiased probes. sampling procedures addgenerated states (that is, states along probe well siblings) statespace sample, terminate collecting N training examples. depth limitrandom walks sampling schemes, set estimate goal depth;discuss goal depth estimate later.third state-space sampling procedure, referred PDB sampling, proposedHaslum, Botea, Helmert, Bonet, Koenig (2007). procedure also uses unbiased probes,adds last state reached probe state-space sample. depthprobe determined individually, drawing random depth binomial distribution aroundestimated goal depth.718fiO NLINE PEEDUP L EARNING PTIMAL P LANNINGNote three sampling procedures rely estimate minimum goal depth.actions unit cost, minimum goal depth h (s0 ), thus useheuristic estimate it. evaluation, used twice heuristic estimate initial state,2 maxh (s0 ), goal depth estimate. However, non-uniform action costs, goal depthcost longer measured units. seems could divide heuristicbased estimate average action cost c, recall use state-space sample orderobtain estimate estimate c, thus creating circular dependency. Although possibleestimate c taking average cost actions problem description, reasonassume actions equally likely used. Another option modifystate-space sampling procedures, place cost limit, rather depth limit, probe.However, would pose problem presence 0-cost actions. case, probereaches cost limit yet possible 0-cost action apply, clear whether probeterminate. Therefore, keep using depth-limited probes attempt estimate depthcheapest goal. compute heuristic estimate initial state, use numberactions heuristic estimate based goal depth estimate.possible every heuristic, use empirical evaluation monotonically-relaxed planheuristic. heuristic, also known FF heuristic (Hoffmann & Nebel, 2001), provideinformation: first use heuristic find relaxed plan initial state, usenumber actions relaxed plan goal depth estimate.5.2 Classifierlast decision made choice classifier. Although many classifiers used here,several requirements must met due particular setup. First, training classification must fast, performed time-constrained problem solving. Second,classifier must incremental support active learning. achieved allowing onlineupdates learned model. Finally, classifier provide us meaningful measureconfidence predictions.several classifiers meet requirements, found Naive Bayes classifier providegood balance speed accuracy. One note Naive Bayes classifierassumes strong conditional independence features. Although fullyrealistic assumption planning tasks, using SAS+ task formulation contrast classicalSTRIPS formulations helps lot: instead many highly dependent binary variables,much smaller set less dependent ones.Although, empirical evaluation demonstrate, Naive Bayes appearssuitable classifier use selective max, classifiers also used. obviouschoice replacement classifier would different Bayesian classifier. One classifierAODE (Webb, Boughton, & Wang, 2005), extension Naive Bayes, somewhat relaxesassumption independence features, typically accurate NaiveBayes. However, added accuracy comes cost increased training classification time.Decision trees another popular type classifier allows even faster classification.decision tree induction algorithms incremental, Incremental Tree Inducer(ITI) algorithm (Utgoff, Berkman, & Clouse, 1997) supports incremental updating decision treestree restructuring, also freely available implementation C. evaluation, usedITI incremental mode, incorporated every example tree immediately,719fiD OMSHLAK , K ARPAS , & ARKOVITCHtree likely used many classifications pairs consecutive updates trainingexamples active learning. classification confidence ITI classifier obtainedfrequency examples leaf node classification came.different family possible classifiers k-Nearest Neighbors (kNN) (Cover & Hart, 1967).order use kNN, need distance metric examples, which, features,simply states. choice features, opt simplicity use Euclidean distancemetric. kNN enjoys fast learning time suffers slow classification time.classification confidence obtained simple (unweighted) vote k nearest neighbors.Another question related choice classifier feature selection. planning tasks,number variables, accordingly, features, 2000 (for example, task 35AIRPORT domain 2558 variables). performance Naive Bayes kNN likelyimproved using feature selection, poses problem initial sample considered.Since feature selection done right initial sample obtained,based initial sample. could cause problem since features might appearirrelevant according initial sample, yet turn relevant active learningused low-confidence states encountered. Therefore, use feature selectionempirical evaluation selective max.5.3 Extension Multiple Heuristicspoint, discussed choose heuristic compute statetwo heuristics choose from. given two heuristics, decisionrule presented Section 4 inapplicable, extending handle two heuristicsstraightforward. However, extending selective max use two heuristics straightforward simply compare heuristics pair-wise manner, use voting rule chooseheuristic compute.many possible voting rules, go simplest one, comparesevery pair heuristics, chooses winner vote, weighted confidence pairwise decision. overall winner simply heuristic highest total confidencepairwise comparisons, ties broken favor cheaper-to-compute heuristic. Althoughrequires quadratic number classifiers, training classification time (at least NaiveBayes) appear much lower overall time spent heuristic computations, thusoverhead induced learning classification likely remain relatively low reasonableheuristic ensembles.6. Experimental Evaluationevaluate selective max empirically, implemented top open-source Fast Downwardplanner (Helmert, 2006). empirical evaluation divided three parts. First, examine performance selective max using last International Planning Competition, IPC-2011,benchmark. Selective max runner-up ex-aequo IPC-2011, tying 2nd placeversion Fast Downward using abstraction merge-and-shrink heuristic (Nissim, Hoffmann, & Helmert, 2011), losing sequential portfolio combining heuristics usedrunners-up (Helmert, Roger, & Karpas, 2011). Second, present series controlled parametricexperiments, examine behavior selective max different settings. Finally,720fiO NLINE PEEDUP L EARNING PTIMAL P LANNINGParameterNSampling methodClassifierDefault value10.61000Biased probesNaive BayesMeaningheuristic difference biasconfidence thresholdinitial sample sizestate-space sampling methodclassifier typeTable 1: Parameters selmax entry IPC-2011.compare selective max simulated sequential portfolio, using heuristics selectivemax.6.1 Performance Evaluation: Results IPC-2011IPC-2011 experiments (Garca-Olaya et al., 2011) run IPC organizers,machines, time limit 30 minutes memory limit 6 GB per planning task.competition included new domains, none participants seen before, thusprecluding participants using offline learning approaches.Although many planners participated sequential optimal track IPC-2011, reportresults relevant selective max. selective max entry IPC-2011 called selmax,consisted selective max uniform action cost partitioning version hLA (Karpas &Domshlak, 2009) hLM-CUT (Helmert & Domshlak, 2009) heuristics. parameters usedselective max IPC-2011 reported Table 1. Additionally, heuristics selmax usedentered individually BJOLP (hLA ) lmcut (hLM-CUT ), report results threeplanners. comparison selective max regular maximum hLA hLM-CUTwould interesting, entry IPC-2011, thus report it.controlled experiments, compare selective max regular maximum, wellbaseline combination methods.Figure 3 shows anytime profile three planners IPC-2011 tasks, plotting number tasks solved different timeouts, time limit 30 minutes. Additionally, Table2 shows number tasks solved domain IPC-2011, 30 minutes, includesnumber problems solved winner, Fast Downward Stone Soup 1 (FDSS-1), reference.results show, selective max solves problems individual heuristicsuses. Furthermore, anytime profile selective max dominates heuristics,range 214 seconds full 30 minute timeout. behavior anytime plotshorter timeouts due overhead selective max, consists obtaining initial statespace sample, well learning classification. However, appears selective max quicklycompensates relatively slow start.6.2 Controlled Experimentsseries controlled experiments, attempted evaluate impact different parametersselective max. controlled following independent variables:Heuristics: used three state-of-the-art admissible heuristics: hLA (Karpas & Domshlak,2009), hLM-CUT (Helmert & Domshlak, 2009), hLM-CUT+ (Bonet & Helmert, 2010). None721fiD OMSHLAK , K ARPAS , & ARKOVITCH160Solved Instances14012010080BJOLPlmcutselmax6002004006008001000Timeout (seconds)1200140016001800Figure 3: IPC-2011 anytime performance. line shows number problems IPC-2011solved BJOLP, lmcut, selmax planners, respectively, different timeouts.DomainbarmanelevatorsfloortilenomysteryopenstacksparcprinterparkingpegsolscanalyzersokobantidybottransportvisitallwoodworkingTOTALBJOLP4142201411317620147109151lmcut418715161321812201461012167selmax418720141341710201461012169FDSS-1418720161471914201471312185Table 2: Number planning tasks solved IPC 2011 domain BJOLP, lmcut,selmax planners. best result 3 planners bold. number problemssolved Fast Downward Stone Soup 1 (FDSS-1) domain also includedreference.722fiO NLINE PEEDUP L EARNING PTIMAL P LANNINGbase heuristics yields better search performance others across planningdomains. heuristics, hLA typically fastest compute least accurate,hLM-CUT expensive compute accurate, hLM-CUT+ expensive compute accurate.1 data gathered experiments,hLM-CUT takes average 4.5 time per state hLA , hLM-CUT+ takes 53 timeper state hLA . evaluate selective max possible subsets twothree heuristics.admissible heuristics SAS+ planning competitivethree (for example, Helmert, Haslum, & Hoffmann, 2007; Nissim et al., 2011; Katz &Domshlak, 2010), based expensive offline preprocessing, followed fastonline per-state computation. contrast, hLA , hLM-CUT hLM-CUT+ performcomputation online, thus better exploited selective max.Additionally, empirically examine effectiveness selective max deciding whethercompute heuristic value all. done combining accurate heuristic,hLM-CUT+ , blind heuristic.Heuristic difference bias : hyper-parameter controls tradeoff computation time heuristic accuracy. Setting = 0 sets threshold 0, forcing decisionrule always choose accurate heuristic. Increasing increases threshold, forcing decision rule choose accurate heuristic h2 value much higherh1 . evaluate selective max values 0.1, 0.5, 1, 1.5, 2, 3, 4, 5.Confidence threshold : confidence threshold controls active learning part selective max. Setting = 0.5 turns active learning completely, chosen heuristicalways comes confidence least 0.5. Setting = 1 would mean using active learning almost always, essentially reducing selective max regular point-wise maximization.evaluate selective max values 0.51, 0.6, 0.7, 0.8, 0.9, 0.99.Initial sample size N : initial sample size N important parameter,used train initial classifier active learning done, alsosource estimates branching factor, average action cost, heuristic computationtimes. thus affects threshold : Increasing N increases accuracy initialclassifier various aforementioned estimates, also increases preprocessingtime. evaluate selective max values N 10, 100, 1000.Sampling method: sampling method used obtain initial state-space sample important affects initial sample, thus accuracy thresholdinitial classifier. evaluate selective max three different sampling methods,Pdescribed Section 5.1: biased probes (selPh ), unbiased probes (selUh ), samplingmethod Haslum et al. (2007) (selPDBh ).Classifier: choice classifier also important. Naive Bayes classifier comBbines fast learning classification (selNh ). sophisticated variant NaiveBayes called AODE (Webb et al., 2005) also considered (selAODE). AODEh1. course, three heuristics computable polynomial time SAS+ description planning task.723fiD OMSHLAK , K ARPAS , & ARKOVITCHParameterHeuristicsNSampling methodClassifierDefault valuehLA / hLM-CUT10.6100PDB (Haslum et al., 2007)Naive BayesMeaningheuristics usedheuristic difference biasconfidence thresholdinitial sample sizestate-space sampling methodclassifier typeTable 3: Default parameters selh .accurate Naive Bayes, higher classification learning times, well increased memory overhead. Another possible choice using incremental decision trees (Utgoff et al., 1997), offer even faster classification, expensive learningtree structure needs changed (selITh ). also consider kNN classifiers (Cover & Hart,1967), offer faster learning Naive Bayes, usually expensive classificaNtion, especially k grows larger (selkN, k = 3, 5).hTable 3 describes default values independent variables.subsequent experiments, vary one independent variables, keeping rest defaultvalues. experiments, search planning task instance limited 30minutes2 3 GB memory. search times include time needed translatingplanning task PDDL SAS+ building Fast Downward data structures,common planners, tangential issues considered study. searchtimes include learning classification time selective max.Heuristicsbegin varying set heuristics use. every possible choice twoheuristics uniform action cost partitioning version hLA (which simply referhLA ), hLM-CUT hLM-CUT+ , compare selective max methods heuristiccombination, well individual heuristics. compare selective max (selh )regular maximum (maxh ), well planner chooses heuristic computestate randomly (rndh ). clear whether random choice favorexpensive accurate heuristic cheaper less accurate one, simply useuniform random choice.experiment conducted 31 domains conditional effects axioms(which none heuristics used support) International Planning Competitions19982008. domains vary difficulty number tasks, normalizescore planner domain 0 1. Normalizing numberproblems domain good idea, always possible generate numbereffectively unsolvable problems domain, fraction solved problemsapproach zero. Therefore, normalize number problems solved domainnumber problems domain solved least one planners.measure normalized coverage undesirable property introducing2. search given single core 3GHz Intel E8400 CPU machine.724fiO NLINE PEEDUP L EARNING PTIMAL P LANNINGHeuristichLAhLM-CUThLM-CUT+High variance unit costLow variance unit costNon-uniform cost0.89 (175)0.98 (345)0.80 (136)0.83 (136)0.96 (343)0.94 (160)0.81 (132)0.94 (336)0.86 (146)TOTAL0.91 (656)0.92 (639)0.89 (614)(a) Individual HeuristicsDomainsHigh variance unit costLow variance unit costNon-uniform costTOTALmaxh0.90 (164)0.97 (345)0.92 (156)0.94 (665)rndh0.74 (123)0.95 (342)0.79 (138)0.85 (603)selh0.93 (174)0.97 (346)0.93 (157)0.95 (677)hLA / hLM-CUT+High variance unit costLow variance unit costNon-uniform costTOTAL0.84 (149)0.93 (335)0.85 (144)0.89 (628)0.68 (115)0.88 (327)0.71 (122)0.78 (564)0.90 (164)0.96 (342)0.86 (145)0.92 (651)hLM-CUT / hLM-CUT+High variance unit costLow variance unit costNon-uniform costTOTAL0.80 (131)0.94 (336)0.87 (147)0.89 (614)0.75 (122)0.93 (335)0.86 (145)0.87 (602)0.80 (130)0.97 (344)0.93 (156)0.91 (630)hLA / hLM-CUT / hLM-CUT+High variance unit costLow variance unit costNon-uniform costTOTAL0.84 (149)0.93 (335)0.85 (144)0.89 (628)0.69 (116)0.90 (332)0.75 (130)0.81 (578)0.87 (154)0.97 (345)0.89 (150)0.92 (649)HeuristicshLA / hLM-CUT(b) Combinations two heuristicsTable 4: Average normalized coverage, total coverage parentheses, broken groupsdomains unit cost actions high variance coverage, domains unit costactions low variance coverage, domains non-uniform action costs. Table(a) shows results individual heuristics, table (b) shows resultsmaximum (maxh ), random choice (rndh ), selective max (selh ) combinationsset heuristics listed major row.new planner could change normalized coverage planners, believebest reflects performance nonetheless. overall performance measure, listaverage normalized coverage score across domains. Using normalized coverage meansdomains equal weight aggregate score. Additionally, list domainnumber problems solved planner (in parentheses next domainname), planner list number problems solved parentheses.Tables 4 5 summarize results experiment. divided domainsexperiment 3 sets: domains non-uniform action costs, domains unit actioncosts exhibited high variance number problems solved different725fiD OMSHLAK , K ARPAS , & ARKOVITCHHeuristicsDomainshLAhLM-CUThLA / hLM-CUTHigh variance unit costLow variance unit costNon-uniform costTOTAL3.233.4813.234.822.81.141.011.4hLA / hLM-CUT+High variance unit costLow variance unit costNon-uniform costTOTAL4.014.5513.665.85hLM-CUT / hLM-CUT+High variance unit costLow variance unit costNon-uniform costTOTALhLA / hLM-CUT / hLM-CUT+hLM-CUT+maxhrndhselh1.01.01.01.03.882.143.992.931.461.21.171.251.771.011.01.161.01.01.01.03.172.383.852.92.161.851.721.892.291.581.321.661.011.011.031.011.01.01.01.01.71.291.181.351.241.191.161.2High variance unit costLow variance unit costNon-uniform cost4.064.6515.23.811.591.371.781.021.031.01.01.03.612.052.742.11.571.49TOTAL6.11.911.181.02.561.67Table 5: Geometric mean ratio expansions relative maxh , broken groups domains unit cost actions high variance coverage, domains unit cost actionslow variance coverage, domains non-uniform action costs.planners, domains unit action costs exhibited low variance numberproblems solved different planners. make distinction conductedfollowing experiments, examine effects parameters selectivemax, unit cost action domains exhibited high variance. Tables 4 5summarize results three sets domains, well domains combined.Detailed, per-domain results relegated Appendix A.Table 4 lists normalized coverage score, averaged across domains, total numberproblems solved parentheses. Table 4a lists individual heuristic,Table 4b every combination method every set two heuristics. Table 5 showsaccurate heuristic combination methods is. Since, given set baseheuristics, maxh accurate heuristic possible, accuracy evaluated relativemaxh . evaluate heuristics accuracy task number states expandedusing heuristic, divided number states expanded using maxh .compute geometric mean domain tasks solved plannersaccuracy ratio, list geometric mean numbers. row listsresults combination two three heuristics; combinations two heuristics,leave cell representing heuristic combination empty.Looking results individual heuristics first, see accurate heuristic(hLM-CUT+ ) well overall, least accurate heuristic (hLA ) solvedtasks total, hLM-CUT wins terms normalized coverage. However, lookingresults individual domains, see best heuristic use varies, indicatingcombining different heuristics could indeed practical value.turn attention empirical results combinations possible subsetstwo heuristics. results clearly demonstrate one heuristicused, selective max always better regular maximum random choice, termsnormalized coverage absolute number problems solved. Furthermore, poorperformance rndh , coverage accuracy, demonstrates decision rule726fiO NLINE PEEDUP L EARNING PTIMAL P LANNING700650Solved Instances600550500450400maxh350rndhselh3002004006008001000Timeout (seconds)1200140016001800Figure 4: hLA / hLM-CUT / hLM-CUT+ anytime profile. line shows number problemsIPC 1998 2006 solved maximum (maxh ), random choice (rndh ), selectivemax (selh ) combination methods hLA , hLM-CUT , hLM-CUT+ heuristics,different timeouts.classifier used selective max important success, computing oneheuristic state randomly insufficient, say least.compared individual heuristics, selective max least wellindividual heuristics uses, combinations except hLM-CUT hLM-CUT+ .likely hLM-CUT hLM-CUT+ based similar procedure,thus heuristic estimates highly correlated. see hinders selective max,consider extreme case two heuristics correlation 1.0 (that is, yieldheuristic values), selective max offer benefit. Finally, remarkbest planner experiment selective max combination hLA hLM-CUT .results based 30 minute time limit, which, commonly usedIPC, arbitrary, number tasks solved 30 minutes tell completetale. Here, examine anytime profile different heuristic combination methods,plotting number tasks solved different timeouts, timeout 30 minutes.Figure 4 shows plot three combination methods three heuristics used.figure shows, advantage selh baseline combination methods evengreater shorter timeouts. indicates advantage selh maxh even727fiD OMSHLAK , K ARPAS , & ARKOVITCHHeuristicshLA / hLM-CUTOverhead12%hLA / hLM-CUT+15%hLM-CUT / hLM-CUT+9%hLA / hLM-CUT / hLM-CUT+10%Table 6: Selective max overhead. row lists average percentage time spent learningclassification, total time taken selective max, set heuristics.greater evident results 30 minutes, selh indeed effectiveminimizing search time. Since anytime plots combinations pairs heuristicssimilar, omit sake brevity.Finally, present overhead statistics using selective max proportion time spentlearning classification, including time spent obtaining initial state-space sample, total solution time. Table 6 presents average overhead selective maxcombinations two heuristics. Detailed, per-domain resultspresented Table 18 Appendix A. results show, selective max incur noticeable overhead, still relatively low. also worth mentioning overheadvaries significantly different domains.also performed empirical evaluation using selective max accurate heuristicalongside blind heuristic. blind heuristic returns 0 goal states, costcheapest action non-goal states. experiment, chose accurateheuristic, hLM-CUT+ . compare performance using hLM-CUT+ alone,using selective max hLM-CUT+ blind heuristic. blind heuristic returnsconstant value non-goal states, decision rule selective max uses combineheuristic h blind heuristic hb simply h(s) + hb , is, compute hpredicted value h greater constant threshold. Recall that,h(s) + g(s) < c , computing h simply waste time, pruned.Therefore, makes sense compute h(s) h(s) c g(s). Notethreshold computing h depends g(s), thus constant. showsconstant threshold computing h(s) best possible decision rule. Unfortunately,selective max decision rule based approximation fails capture subtletiescase.Table 7 shows normalized coverage using hLM-CUT+ , using selective maxhLM-CUT+ blind heuristic. results show, selective max little effectdomains, though harm performance some, one domain OPENSTACKSactually performs better single heuristic. Table 8 shows average expansions ratio,using number states expanded hLM-CUT+ baseline; note using blindheuristic never increases heuristic accuracy. results show, selective max choosesuse blind heuristic quite often, expanding average twice many stateshLM-CUT+ alone.728fiO NLINE PEEDUP L EARNING PTIMAL P LANNINGcoveragehLM-CUT+selhairport (31)freecell (13)logistics00 (17)mprime (24)mystery (17)pipesworld-tankage (9)satellite (9)zenotravel (12)1.00 (31)1.00 (13)1.00 (17)1.00 (24)1.00 (17)1.00 (9)1.00 (9)1.00 (12)1.00 (31)1.00 (13)1.00 (17)1.00 (24)1.00 (17)1.00 (9)1.00 (9)1.00 (12)blocks (27)depot (7)driverlog (14)grid (2)gripper (6)logistics98 (6)miconic (140)pathways (5)pipesworld-notankage (17)psr-small (48)rovers (7)schedule (27)storage (15)tpp (6)trucks-strips (9)1.00 (27)1.00 (7)1.00 (14)1.00 (2)1.00 (6)1.00 (6)1.00 (140)1.00 (5)1.00 (17)1.00 (48)1.00 (7)1.00 (27)1.00 (15)1.00 (6)1.00 (9)1.00 (27)1.00 (7)1.00 (14)1.00 (2)1.00 (6)1.00 (6)0.86 (121)1.00 (5)1.00 (17)1.00 (48)1.00 (7)1.00 (27)0.93 (14)1.00 (6)1.00 (9)elevators-opt08-strips (18)openstacks-opt08-strips (19)parcprinter-08-strips (21)pegsol-08-strips (27)scanalyzer-08-strips (13)sokoban-opt08-strips (25)transport-opt08-strips (11)woodworking-opt08-strips (14)1.00 (18)0.89 (17)1.00 (21)1.00 (27)1.00 (13)1.00 (25)1.00 (11)1.00 (14)0.83 (15)1.00 (19)1.00 (21)1.00 (27)0.77 (10)1.00 (25)1.00 (11)0.93 (13)TOTAL1.00 (614)0.98 (589)Table 7: Normalized coverage hLM-CUT+ selective max combining hLM-CUT+ blindheuristic. Domains grouped domains unit cost actions high variancecoverage, domains unit cost actions low variance coverage, domainsnon-uniform action costs, respectively.729fiD OMSHLAK , K ARPAS , & ARKOVITCHexpansionshLM-CUT+selhairport (31)freecell (13)logistics00 (17)mprime (24)mystery (18)pipesworld-tankage (9)satellite (9)zenotravel (12)1.01.01.01.01.01.01.01.01.03.131.021.223.24.233.112.37blocks (27)depot (7)driverlog (14)grid (2)gripper (6)logistics98 (6)miconic (121)pathways (5)pipesworld-notankage (17)psr-small (48)rovers (7)schedule (27)storage (14)tpp (6)trucks-strips (9)1.01.01.01.01.01.01.01.01.01.01.01.01.01.01.01.921.361.157.671.01.1814.241.01.272.121.561.215.111.61.01elevators-opt08-strips (15)openstacks-opt08-strips (17)parcprinter-08-strips (21)pegsol-08-strips (27)scanalyzer-08-strips (10)sokoban-opt08-strips (25)transport-opt08-strips (11)woodworking-opt08-strips (13)1.01.01.01.01.01.01.01.013.411.081.241.014.871.05.8646.97GEOMETRIC MEAN1.02.3Table 8: Average ratio expanded states baseline hLM-CUT+ selective maxcombining hLM-CUT+ blind heuristic. Domains grouped domainsunit cost actions high variance coverage, domains unit cost actions lowvariance coverage, domains non-uniform action costs, respectively.730fiO NLINE PEEDUP L EARNING PTIMAL P LANNINGexperiments varied heuristics selective max uses. followingexperiments, fix set heuristics, examine impact parameters selective max performance. still need evaluate 20 different configurations selectivemax, focus eight selected domains: AIRPORT, FREECELL, LOGISTICS 00, MPRIME, MYS TERY , PIPESWORLD - TANKAGE, SATELLITE , ZENOTRAVEL. eight domainshighest observed variance number tasks solved across different planners, unitaction cost domains used. domains chosen order reduce computation timerequired experiments manageable quantity. excluded domains non-uniformaction costs, use different method estimating goal depth state-spacesampling method, one parameters examine. Below, focus one parameterselective max time, present total number tasks solved eight chosen domains,different values parameter. Detailed, per-domain results parameter appearAppendix A.hyper-parameterFigure 5a plots total number problems solved, different values .results show, selective max fairly robust respect value , unless largevalue chosen, making difficult selective max choose accurateheuristic.Detailed, per-domain results appear Table 19 Appendix A, well Figure 6.results show complex picture, seems cutoff valuedomain, increasing past value impairs performance. one exceptionPIPESWORLD - TANKAGE domain, setting = 5 helps.confidence thresholdFigure 5b plots total number problems solved, different values , Detailed,per-domain results appear Table 20 Appendix A. results indicate selectivemax also robust values , unless set low value, causing selective maxbehave like regular point-wise maximum.initial sample size NFigure 5c plots total number problems solved different values N .x-axis logscale. Detailed, per-domain results appear Table 21 Appendix A.results show, default value N = 100 best (of three values tried), althoughselective max still fairly robust respect choice parameter.sampling methodFigure 7 shows total number problems solved using different methods initialstate-space sampling. Detailed, per-domain results appear Table 22 Appendix A.results demonstrate, choice sampling method notably affect performanceselective max. However, detailed results show, effect evident FREE CELL domain. also remark default sampling method, PDB, performs worseothers. Indeed using probe based sampling methods, selective max outperformsusing hLA alone. However, difference due FREECELL domain,state certainty would generalize across domains.731fiSolved InstancesOMSHLAK , K ARPAS , & ARKOVITCH17417217016816600.511.522.533.544.550.80.850.90.951Solved Instances(a) Hyper-parameter1741721701681660.50.550.60.650.70.75Solved Instances(b) Confidence threshold174172170168166101001000(c) Initial Sample Size NFigure 5: Number problems solved selective max different values (a) hyperparameter (b) confidence threshold , (c) initial sample size N .732fiSolved InstancesNLINE PEEDUP L EARNING PTIMAL P LANNING5045403530252015105airportfreecelllogistics00mprimemysterypw-tankagesatellitezenotravel012345Figure 6: Number problems solved selective domain different values .180160Solved Instances140120100806040200PDB174Probe178Sampling MethodUnbiasedProbe180Figure 7: Number problems solved selective max different sampling methods.733fiD OMSHLAK , K ARPAS , & ARKOVITCH180160Solved Instances140120100806040200NB174AODE168ITI156Classifier3NN1585NN161Figure 8: Number problems solved selective max different classifiers.classifierFigure 8 shows total number problems solved using different classifiers. Detailed,per-domain results appear Table 23 Appendix A. Naive Bayes appears bestclassifier use selective max, although AODE also performs quite well. Even thoughkNN enjoys fast learning, classifier used mostly classification, expected,kNN well. However, increased accuracy k = 5 seems payfaster classification k = 3.6.3 Comparison Sequential PortfoliosSequential portfolio solvers optimal planning another approach exploiting meritsdifferent heuristic functions, successful practice, Fast DownwardStone Soup sequential portfolio (Helmert et al., 2011) winning sequential optimal track IPC2011. sequential portfolio utilizes different solvers running sequentially, prespecified time limit. one solver fails find solution allotted time limit, sequentialportfolio terminates it, moves next solver. However, sequential portfolio solverneeds know time allowance problem trying solve beforehand, setting knowncontract anytime (Russell & Zilberstein, 1991). contrast, selective max usedinterruptible anytime manner, time limit need known advance.Here, compare selective max sequential portfolios heuristics.exact time took search using heuristic alone solve problem,determine whether sequential portfolio assigns heuristic time limit ablesolve problem. Using data, simulate results two types sequential portfolioplanners. first setting, assume time limit known advance, simulateresults contract portfolio giving equal share time heuristics. second setting,simulate interruptible anytime portfolio using binary exponential backoff time limits: starting734fi700700650650Solved InstancesSolved InstancesNLINE PEEDUP L EARNING PTIMAL P LANNING600550500selhportctr450600550500selhportctr450portintportint400400200400600 800 1000 1200 1400 1600 1800Timeout (seconds)200700700650650600550500selhportctr450600 800 1000 1200 1400 1600 1800Timeout (seconds)hLA / hLM-CUT+(b)Solved InstancesSolved InstanceshLA / hLM-CUT(a)400600550500selhportctr450portintportint400400200400600 800 1000 1200 1400 1600 1800Timeout (seconds)200hLM-CUT / hLM-CUT+(c)400600 800 1000 1200 1400 1600 1800Timeout (seconds)hLA / hLM-CUT / hLM-CUT+(d)Figure 9: Anytime profiles sequential portfolios selective max. plot shows number problems solved selective max (selh ), simulated contract anytime portfolio(portctr ), simulated interruptible portfolio (portint ) using (a) hLA hLM-CUT (b)hLA hLM-CUT+ (c) hLM-CUT hLM-CUT+ , (d) hLA , hLM-CUT , hLM-CUT+ .time limit 1 second heuristic, increase time limit factor 2 noneheuristics able guide solve planning problem. several possibleorderings heuristics here, use de facto best ordering problem. denotecontract anytime portfolio portctr , interruptible anytime portfolio portint .Figure 9 shows number problems solved different time limits selective max,contract anytime sequential portfolio, interruptible anytime sequential portfolio.results show, contract anytime sequential portfolio almost always outperforms selectivemax. hand, sequential portfolio know time limit advance,performance deteriorates significantly. best heuristic combination selective max, hLAhLM-CUT , outperforms interruptible anytime portfolio using heuristics,735fiD OMSHLAK , K ARPAS , & ARKOVITCHselective max combination hLM-CUT hLM-CUT+ . combinations heuristics,interruptible anytime portfolio performs better selective max.7. DiscussionLearning planning active field since early days planning (Fikes, Hart,& Nilsson, 1972), recently receiving growing attention community. However, despiteearly work (Rendell, 1983), relatively little work dealt learning state-space searchguided distance-estimating heuristics, one prominent approaches planningdays. works direction devoted learning macro-actions (see, example,Finkelstein & Markovitch, 1998; Botea, Enzenberger, Muller, & Schaeffer, 2005; Coles & Smith,2007). Recently, learning heuristic search planning received attention: Yoon et al.(2008) suggested learning (inadmissible) heuristic functions based upon features extractedrelaxed plans. Arfaee, Zilles, Holte (2010) attempted learn almost admissible heuristicestimate using neural network. Perhaps closely related work Thayer,Dionne, Ruml (2011), learn correct errors heuristic estimates online. Thayer et al. attempt improve accuracy single given heuristic, selective max attempts choose oneseveral given heuristics state. two works differ technically point. importantly, however, none aforementioned approaches guarantee resulting heuristicadmissible, thus optimal solution found. contrast, focus optimal planning, aware previous work deals learning optimalheuristic search.experimental evaluation demonstrates selective max effective methodcombining arbitrary admissible heuristics baseline point-wise maximization. Also advantageous selective maxs ability exploit pairs heuristics, one guaranteed alwaysleast accurate other. example, hLA heuristic used two actioncost partitioning schemes: uniform optimal (Karpas & Domshlak, 2009). heuristic inducedoptimal action cost partitioning least accurate one induced uniform actioncost partitioning, takes much longer compute. Selective max might used learnworth spending extra time compute optimal cost partitioning, not.contrast, max-based combination two heuristics would simply waste time spentcomputing uniform action cost partitioning.controlled parametric experiments demonstrate right choice classifiersampling method initial state-space sample important. parametersselective max appear affect performance much, long set reasonablevalues. implies selective max could improved using faster, accurate, classifiers,developing sampling methods represent state-space well.Finally, remark Fast Downward Autotune entry sequential optimal track2011 edition International Planning Competition, used ParamILS (Hutter, Hoos,Leyton-Brown, & Stutzle, 2009) choose best configuration Fast Downward planner,chose use selective-max combine hLM-CUT hmax (Bonet, Loerincs, & Geffner, 1997).provides evidence selective max practically valuable method combiningheuristics optimal planning.736fiO NLINE PEEDUP L EARNING PTIMAL P LANNINGcoveragehLAhLM-CUThLM-CUT+airport (33)freecell (58)logistics00 (21)mprime (24)mystery (17)pipesworld-tankage (13)satellite (10)zenotravel (13)0.91 (30)1.00 (58)1.00 (21)0.88 (21)0.88 (15)1.00 (13)0.70 (7)0.77 (10)0.85 (28)0.26 (15)0.95 (20)1.00 (24)1.00 (17)0.92 (12)0.70 (7)1.00 (13)0.94 (31)0.22 (13)0.81 (17)1.00 (24)1.00 (17)0.69 (9)0.90 (9)0.92 (12)blocks (28)depot (7)driverlog (14)grid (3)gripper (7)logistics98 (6)miconic (142)pathways (5)pipesworld-notankage (18)psr-small (49)rovers (8)schedule (30)storage (15)tpp (6)trucks-strips (10)0.96 (27)1.00 (7)1.00 (14)1.00 (3)1.00 (7)1.00 (6)1.00 (142)0.80 (4)1.00 (18)1.00 (49)1.00 (8)1.00 (30)1.00 (15)1.00 (6)0.90 (9)1.00 (28)1.00 (7)0.93 (13)0.67 (2)1.00 (7)1.00 (6)0.99 (141)1.00 (5)0.94 (17)1.00 (49)0.88 (7)1.00 (30)1.00 (15)1.00 (6)1.00 (10)0.96 (27)1.00 (7)1.00 (14)0.67 (2)0.86 (6)1.00 (6)0.99 (140)1.00 (5)0.94 (17)0.98 (48)0.88 (7)0.90 (27)1.00 (15)1.00 (6)0.90 (9)elevators-opt08-strips (22)openstacks-opt08-strips (20)parcprinter-08-strips (22)pegsol-08-strips (28)scanalyzer-08-strips (16)sokoban-opt08-strips (30)transport-opt08-strips (12)woodworking-opt08-strips (19)0.77 (17)0.90 (18)0.68 (15)0.96 (27)0.56 (9)0.83 (25)1.00 (12)0.68 (13)1.00 (22)1.00 (20)0.82 (18)1.00 (28)0.94 (15)1.00 (30)0.92 (11)0.84 (16)0.82 (18)0.85 (17)0.95 (21)0.96 (27)0.81 (13)0.83 (25)0.92 (11)0.74 (14)TOTAL0.91 (656)0.92 (639)0.89 (614)Table 9: Detailed per-domain results individual heuristic. Normalized coverageshown, number problems solved shown parentheses. Domains groupeddomains unit cost actions high variance coverage, domains unitcost actions low variance coverage, domains non-uniform action costs,respectively.Acknowledgmentswork partly supported Israel Science Foundation (ISF) grant 1045/12.Appendix A. Detailed Results Empirical Evaluationappendix, present detailed per-domain, results experiments described Section 6.Table 9 shows normalized coverage number problems solved domain,individual heuristics. normalized coverage score planner X domain numberproblems domain solved planner X, divided number problems domainsolved least one planner. Tables 10 17 give results combinations twoheuristics. Tables 10, 12, 14, 16 list normalized coverage individual heuristics used,combination using selective max (selh ), regular maximum (maxh ), random choiceheuristic state (rndh ) 30 minutes. Tables 11, 13, 15, 17 give geometricmean ratio expanded states relative maxh domain, problems solvedconfigurations. number tasks solved planners listed parentheses nextdomain. final row gives geometric mean geometric means domain.737fiD OMSHLAK , K ARPAS , & ARKOVITCHcoveragehLAhLM-CUTmaxhrndhselhairport (33)freecell (58)logistics00 (21)mprime (24)mystery (17)pipesworld-tankage (13)satellite (10)zenotravel (13)0.91 (30)1.00 (58)1.00 (21)0.88 (21)0.88 (15)1.00 (13)0.70 (7)0.77 (10)0.85 (28)0.26 (15)0.95 (20)1.00 (24)1.00 (17)0.92 (12)0.70 (7)1.00 (13)0.91 (30)0.71 (41)0.95 (20)1.00 (24)1.00 (17)0.92 (12)0.70 (7)1.00 (13)0.85 (28)0.28 (16)0.95 (20)0.75 (18)0.76 (13)0.85 (11)0.70 (7)0.77 (10)0.91 (30)0.84 (49)1.00 (21)1.00 (24)1.00 (17)0.92 (12)0.80 (8)1.00 (13)blocks (28)depot (7)driverlog (14)grid (3)gripper (7)logistics98 (6)miconic (142)pathways (5)pipesworld-notankage (18)psr-small (49)rovers (8)schedule (30)storage (15)tpp (6)trucks-strips (10)0.96 (27)1.00 (7)1.00 (14)1.00 (3)1.00 (7)1.00 (6)1.00 (142)0.80 (4)1.00 (18)1.00 (49)1.00 (8)1.00 (30)1.00 (15)1.00 (6)0.90 (9)1.00 (28)1.00 (7)0.93 (13)0.67 (2)1.00 (7)1.00 (6)0.99 (141)1.00 (5)0.94 (17)1.00 (49)0.88 (7)1.00 (30)1.00 (15)1.00 (6)1.00 (10)1.00 (28)1.00 (7)1.00 (14)0.67 (2)1.00 (7)1.00 (6)0.99 (141)1.00 (5)0.94 (17)1.00 (49)1.00 (8)1.00 (30)1.00 (15)1.00 (6)1.00 (10)1.00 (28)1.00 (7)0.93 (13)0.67 (2)1.00 (7)1.00 (6)0.99 (141)0.80 (4)0.94 (17)1.00 (49)1.00 (8)1.00 (30)1.00 (15)1.00 (6)0.90 (9)1.00 (28)1.00 (7)1.00 (14)0.67 (2)1.00 (7)1.00 (6)1.00 (142)1.00 (5)0.94 (17)1.00 (49)1.00 (8)1.00 (30)1.00 (15)1.00 (6)1.00 (10)elevators-opt08-strips (22)openstacks-opt08-strips (20)parcprinter-08-strips (22)pegsol-08-strips (28)scanalyzer-08-strips (16)sokoban-opt08-strips (30)transport-opt08-strips (12)woodworking-opt08-strips (19)0.77 (17)0.90 (18)0.68 (15)0.96 (27)0.56 (9)0.83 (25)1.00 (12)0.68 (13)1.00 (22)1.00 (20)0.82 (18)1.00 (28)0.94 (15)1.00 (30)0.92 (11)0.84 (16)1.00 (22)0.90 (18)0.82 (18)0.96 (27)0.94 (15)0.97 (29)0.92 (11)0.84 (16)0.77 (17)0.90 (18)0.68 (15)0.96 (27)0.44 (7)1.00 (30)0.92 (11)0.68 (13)1.00 (22)0.90 (18)0.82 (18)0.96 (27)0.94 (15)0.97 (29)0.92 (11)0.89 (17)TOTAL0.91 (656)0.92 (639)0.94 (665)0.85 (603)0.95 (677)Table 10: Detailed per-domain normalized coverage using hLA hLM-CUT . line showsnormalized coverage domain, number problems solved shownparentheses. Domains grouped domains unit cost actions high variancecoverage, domains unit cost actions low variance coverage, domainsnon-uniform action costs, respectively.738fiO NLINE PEEDUP L EARNING PTIMAL P LANNINGexpansionshLAhLM-CUTmaxhrndhselhairport (28)freecell (15)logistics00 (20)mprime (18)mystery (14)pipesworld-tankage (11)satellite (7)zenotravel (10)2.881.011.06.347.91.616.277.981.12529.611.01.891.152.351.261.01.01.01.01.01.01.01.01.01.61116.961.04.25.191.622.323.32.22.141.01.521.171.121.092.02blocks (27)depot (7)driverlog (13)grid (2)gripper (7)logistics98 (6)miconic (141)pathways (4)pipesworld-notankage (17)psr-small (49)rovers (7)schedule (30)storage (15)tpp (6)trucks-strips (9)7.43.457.22.151.07.741.039.652.011.272.181.152.161.7446.111.01.321.091.731.041.01.01.02.161.01.311.01.01.01.021.01.01.01.01.01.01.01.01.01.01.01.01.01.01.02.21.912.891.571.022.691.017.911.971.111.771.031.451.4212.121.611.31.241.831.01.081.01.01.361.151.091.151.561.01.01elevators-opt08-strips (17)openstacks-opt08-strips (18)parcprinter-08-strips (15)pegsol-08-strips (27)scanalyzer-08-strips (7)sokoban-opt08-strips (25)transport-opt08-strips (11)woodworking-opt08-strips (12)21.511.1724.133.7269.215.7412.0931.61.031.01.01.011.01.071.011.01.01.01.01.01.01.01.01.05.991.039.341.821.471.333.795.681.371.151.01.011.141.041.441.28GEOMETRIC MEAN4.821.41.02.931.25Table 11: Detailed per-domain expansions relative maxh using hLA hLM-CUT . rowshows geometric mean ratio expanded nodes relative maxh . Domainsgrouped domains unit cost actions high variance coverage, domainsunit cost actions low variance coverage, domains non-uniform actioncosts, respectively.739fiD OMSHLAK , K ARPAS , & ARKOVITCHcoveragehLAhLM-CUT+maxhrndhselhairport (33)freecell (58)logistics00 (21)mprime (24)mystery (17)pipesworld-tankage (13)satellite (10)zenotravel (13)0.91 (30)1.00 (58)1.00 (21)0.88 (21)0.88 (15)1.00 (13)0.70 (7)0.77 (10)0.94 (31)0.22 (13)0.81 (17)1.00 (24)1.00 (17)0.69 (9)0.90 (9)0.92 (12)0.94 (31)0.53 (31)0.76 (16)1.00 (24)1.00 (17)0.69 (9)0.90 (9)0.92 (12)0.85 (28)0.26 (15)0.95 (20)0.67 (16)0.71 (12)0.62 (8)0.70 (7)0.69 (9)0.91 (30)0.71 (41)1.00 (21)1.00 (24)1.00 (17)0.69 (9)1.00 (10)0.92 (12)blocks (28)depot (7)driverlog (14)grid (3)gripper (7)logistics98 (6)miconic (142)pathways (5)pipesworld-notankage (18)psr-small (49)rovers (8)schedule (30)storage (15)tpp (6)trucks-strips (10)0.96 (27)1.00 (7)1.00 (14)1.00 (3)1.00 (7)1.00 (6)1.00 (142)0.80 (4)1.00 (18)1.00 (49)1.00 (8)1.00 (30)1.00 (15)1.00 (6)0.90 (9)0.96 (27)1.00 (7)1.00 (14)0.67 (2)0.86 (6)1.00 (6)0.99 (140)1.00 (5)0.94 (17)0.98 (48)0.88 (7)0.90 (27)1.00 (15)1.00 (6)0.90 (9)0.96 (27)1.00 (7)1.00 (14)0.67 (2)0.71 (5)1.00 (6)0.99 (140)1.00 (5)0.94 (17)0.98 (48)0.88 (7)0.90 (27)1.00 (15)1.00 (6)0.90 (9)0.93 (26)0.86 (6)0.93 (13)0.67 (2)0.86 (6)0.83 (5)0.99 (140)0.80 (4)0.83 (15)0.98 (48)0.88 (7)0.90 (27)1.00 (15)1.00 (6)0.70 (7)0.93 (26)1.00 (7)0.93 (13)0.67 (2)1.00 (7)1.00 (6)1.00 (142)1.00 (5)0.94 (17)1.00 (49)1.00 (8)1.00 (30)1.00 (15)1.00 (6)0.90 (9)elevators-opt08-strips (22)openstacks-opt08-strips (20)parcprinter-08-strips (22)pegsol-08-strips (28)scanalyzer-08-strips (16)sokoban-opt08-strips (30)transport-opt08-strips (12)woodworking-opt08-strips (19)0.77 (17)0.90 (18)0.68 (15)0.96 (27)0.56 (9)0.83 (25)1.00 (12)0.68 (13)0.82 (18)0.85 (17)0.95 (21)0.96 (27)0.81 (13)0.83 (25)0.92 (11)0.74 (14)0.82 (18)0.80 (16)0.95 (21)0.96 (27)0.81 (13)0.77 (23)0.92 (11)0.79 (15)0.59 (13)0.85 (17)0.55 (12)0.96 (27)0.38 (6)0.83 (25)0.92 (11)0.58 (11)0.73 (16)0.85 (17)1.00 (22)0.96 (27)0.81 (13)0.80 (24)0.92 (11)0.79 (15)TOTAL0.91 (656)0.89 (614)0.89 (628)0.78 (564)0.92 (651)Table 12: Detailed per-domain normalized coverage using hLA hLM-CUT+ . line showsnormalized coverage domain, number problems solved shownparentheses. Domains grouped domains unit cost actions high variancecoverage, domains unit cost actions low variance coverage, domainsnon-uniform action costs, respectively.740fiO NLINE PEEDUP L EARNING PTIMAL P LANNINGexpansionshLAhLM-CUT+maxhrndhselhairport (28)freecell (13)logistics00 (16)mprime (16)mystery (13)pipesworld-tankage (8)satellite (7)zenotravel (9)3.051.221.08.457.762.1719.266.621.047.571.01.231.111.421.031.01.01.01.01.01.01.01.01.01.4310.541.05.24.771.485.943.092.812.051.01.571.71.864.124.04blocks (26)depot (6)driverlog (13)grid (2)gripper (5)logistics98 (5)miconic (140)pathways (4)pipesworld-notankage (15)psr-small (48)rovers (7)schedule (27)storage (15)tpp (6)trucks-strips (7)6.9721.811.115.041.06.11.040.563.081.312.751.092.292.7246.091.01.01.011.011.01.01.01.01.121.01.011.01.01.01.011.01.01.01.01.01.01.01.01.01.01.01.01.01.01.02.155.463.712.141.02.141.018.031.751.141.811.01.531.8812.024.283.962.564.741.03.791.01.02.461.271.451.092.161.171.01elevators-opt08-strips (13)openstacks-opt08-strips (16)parcprinter-08-strips (12)pegsol-08-strips (27)scanalyzer-08-strips (6)sokoban-opt08-strips (21)transport-opt08-strips (11)woodworking-opt08-strips (11)28.61.1724.874.9223.0715.6615.3453.271.011.01.01.01.01.01.01.01.01.01.01.01.01.01.01.07.11.039.232.156.881.334.268.537.461.091.191.01.431.012.841.91GEOMETRIC MEAN5.851.161.02.91.89Table 13: Detailed per-domain expansions relative maxh using hLA hLM-CUT+ . rowshows geometric mean ratio expanded nodes relative maxh . Domainsgrouped domains unit cost actions high variance coverage, domainsunit cost actions low variance coverage, domains non-uniform actioncosts, respectively.741fiD OMSHLAK , K ARPAS , & ARKOVITCHcoveragehLM-CUThLM-CUT+maxhrndhselhairport (33)freecell (58)logistics00 (21)mprime (24)mystery (17)pipesworld-tankage (13)satellite (10)zenotravel (13)0.85 (28)0.26 (15)0.95 (20)1.00 (24)1.00 (17)0.92 (12)0.70 (7)1.00 (13)0.94 (31)0.22 (13)0.81 (17)1.00 (24)1.00 (17)0.69 (9)0.90 (9)0.92 (12)0.94 (31)0.22 (13)0.76 (16)1.00 (24)1.00 (17)0.69 (9)0.90 (9)0.92 (12)0.82 (27)0.21 (12)0.95 (20)0.88 (21)0.88 (15)0.62 (8)0.70 (7)0.92 (12)0.85 (28)0.22 (13)0.95 (20)1.00 (24)0.94 (16)0.69 (9)0.80 (8)0.92 (12)blocks (28)depot (7)driverlog (14)grid (3)gripper (7)logistics98 (6)miconic (142)pathways (5)pipesworld-notankage (18)psr-small (49)rovers (8)schedule (30)storage (15)tpp (6)trucks-strips (10)1.00 (28)1.00 (7)0.93 (13)0.67 (2)1.00 (7)1.00 (6)0.99 (141)1.00 (5)0.94 (17)1.00 (49)0.88 (7)1.00 (30)1.00 (15)1.00 (6)1.00 (10)0.96 (27)1.00 (7)1.00 (14)0.67 (2)0.86 (6)1.00 (6)0.99 (140)1.00 (5)0.94 (17)0.98 (48)0.88 (7)0.90 (27)1.00 (15)1.00 (6)0.90 (9)0.96 (27)1.00 (7)1.00 (14)0.67 (2)0.86 (6)1.00 (6)0.99 (140)1.00 (5)0.94 (17)0.98 (48)0.88 (7)0.90 (27)1.00 (15)1.00 (6)0.90 (9)1.00 (28)1.00 (7)0.93 (13)0.67 (2)0.86 (6)1.00 (6)0.99 (140)1.00 (5)0.89 (16)0.98 (48)0.88 (7)0.90 (27)1.00 (15)1.00 (6)0.90 (9)1.00 (28)1.00 (7)1.00 (14)0.67 (2)1.00 (7)1.00 (6)0.99 (141)1.00 (5)0.94 (17)1.00 (49)0.88 (7)1.00 (30)1.00 (15)1.00 (6)1.00 (10)elevators-opt08-strips (22)openstacks-opt08-strips (20)parcprinter-08-strips (22)pegsol-08-strips (28)scanalyzer-08-strips (16)sokoban-opt08-strips (30)transport-opt08-strips (12)woodworking-opt08-strips (19)1.00 (22)1.00 (20)0.82 (18)1.00 (28)0.94 (15)1.00 (30)0.92 (11)0.84 (16)0.82 (18)0.85 (17)0.95 (21)0.96 (27)0.81 (13)0.83 (25)0.92 (11)0.74 (14)0.82 (18)0.85 (17)0.95 (21)0.96 (27)0.81 (13)0.83 (25)0.92 (11)0.79 (15)0.82 (18)0.95 (19)0.82 (18)0.96 (27)0.81 (13)0.83 (25)0.92 (11)0.74 (14)0.95 (21)0.95 (19)0.91 (20)0.96 (27)0.94 (15)0.83 (25)0.92 (11)0.95 (18)TOTAL0.92 (639)0.89 (614)0.89 (614)0.87 (602)0.91 (630)Table 14: Detailed per-domain normalized coverage using hLM-CUT hLM-CUT+ . line showsnormalized coverage domain, number problems solved shownparentheses. Domains grouped domains unit cost actions high variancecoverage, domains unit cost actions low variance coverage, domainsnon-uniform action costs, respectively.742fiO NLINE PEEDUP L EARNING PTIMAL P LANNINGexpansionshLM-CUThLM-CUT+maxhrndhselhairport (26)freecell (12)logistics00 (16)mprime (21)mystery (16)pipesworld-tankage (8)satellite (7)zenotravel (12)1.169.551.02.21.693.093.661.611.01.01.01.011.011.010.981.091.01.01.01.01.01.01.01.01.044.371.01.841.521.752.391.31.161.261.01.01.321.611.511.22blocks (27)depot (7)driverlog (13)grid (2)gripper (6)logistics98 (6)miconic (140)pathways (5)pipesworld-notankage (16)psr-small (48)rovers (7)schedule (27)storage (15)tpp (6)trucks-strips (9)1.027.531.714.031.051.081.01.223.491.031.661.01.071.561.321.01.01.021.01.01.051.01.021.011.01.011.01.01.01.01.01.01.01.01.01.01.01.01.01.01.01.01.01.01.01.014.071.361.91.031.061.01.131.91.021.281.01.031.161.141.021.251.491.281.051.061.01.221.41.031.31.01.071.561.26elevators-opt08-strips (18)openstacks-opt08-strips (17)parcprinter-08-strips (17)pegsol-08-strips (27)scanalyzer-08-strips (13)sokoban-opt08-strips (25)transport-opt08-strips (11)woodworking-opt08-strips (13)1.751.01.711.331.221.041.291.451.091.01.01.011.021.041.011.061.01.01.01.01.01.01.01.01.41.01.371.151.141.011.151.261.721.01.01.21.131.031.261.12GEOMETRIC MEAN1.661.011.01.351.2Table 15: Detailed per-domain expansions relative maxh using hLM-CUT hLM-CUT+ .row shows geometric mean ratio expanded nodes relative maxh . Domainsgrouped domains unit cost actions high variance coverage, domainsunit cost actions low variance coverage, domains non-uniform actioncosts, respectively.743fiD OMSHLAK , K ARPAS , & ARKOVITCHcoveragehLAhLM-CUThLM-CUT+maxhrndhselhairport (33)freecell (58)logistics00 (21)mprime (24)mystery (17)pipesworld-tankage (13)satellite (10)zenotravel (13)0.91 (30)1.00 (58)1.00 (21)0.88 (21)0.88 (15)1.00 (13)0.70 (7)0.77 (10)0.85 (28)0.26 (15)0.95 (20)1.00 (24)1.00 (17)0.92 (12)0.70 (7)1.00 (13)0.94 (31)0.22 (13)0.81 (17)1.00 (24)1.00 (17)0.69 (9)0.90 (9)0.92 (12)0.94 (31)0.53 (31)0.76 (16)1.00 (24)1.00 (17)0.69 (9)0.90 (9)0.92 (12)0.79 (26)0.26 (15)0.95 (20)0.75 (18)0.71 (12)0.69 (9)0.70 (7)0.69 (9)0.91 (30)0.57 (33)0.95 (20)0.96 (23)1.00 (17)0.85 (11)0.80 (8)0.92 (12)blocks (28)depot (7)driverlog (14)grid (3)gripper (7)logistics98 (6)miconic (142)pathways (5)pipesworld-notankage (18)psr-small (49)rovers (8)schedule (30)storage (15)tpp (6)trucks-strips (10)0.96 (27)1.00 (7)1.00 (14)1.00 (3)1.00 (7)1.00 (6)1.00 (142)0.80 (4)1.00 (18)1.00 (49)1.00 (8)1.00 (30)1.00 (15)1.00 (6)0.90 (9)1.00 (28)1.00 (7)0.93 (13)0.67 (2)1.00 (7)1.00 (6)0.99 (141)1.00 (5)0.94 (17)1.00 (49)0.88 (7)1.00 (30)1.00 (15)1.00 (6)1.00 (10)0.96 (27)1.00 (7)1.00 (14)0.67 (2)0.86 (6)1.00 (6)0.99 (140)1.00 (5)0.94 (17)0.98 (48)0.88 (7)0.90 (27)1.00 (15)1.00 (6)0.90 (9)0.96 (27)1.00 (7)1.00 (14)0.67 (2)0.71 (5)1.00 (6)0.99 (140)1.00 (5)0.94 (17)0.98 (48)0.88 (7)0.90 (27)1.00 (15)1.00 (6)0.90 (9)0.96 (27)1.00 (7)0.93 (13)0.67 (2)0.86 (6)0.83 (5)0.99 (140)0.80 (4)0.83 (15)0.98 (48)0.88 (7)0.93 (28)1.00 (15)1.00 (6)0.90 (9)1.00 (28)1.00 (7)0.93 (13)0.67 (2)1.00 (7)1.00 (6)1.00 (142)1.00 (5)0.94 (17)1.00 (49)1.00 (8)1.00 (30)1.00 (15)1.00 (6)1.00 (10)elevators-opt08-strips (22)openstacks-opt08-strips (20)parcprinter-08-strips (22)pegsol-08-strips (28)scanalyzer-08-strips (16)sokoban-opt08-strips (30)transport-opt08-strips (12)woodworking-opt08-strips (19)0.77 (17)0.90 (18)0.68 (15)0.96 (27)0.56 (9)0.83 (25)1.00 (12)0.68 (13)1.00 (22)1.00 (20)0.82 (18)1.00 (28)0.94 (15)1.00 (30)0.92 (11)0.84 (16)0.82 (18)0.85 (17)0.95 (21)0.96 (27)0.81 (13)0.83 (25)0.92 (11)0.74 (14)0.82 (18)0.80 (16)0.95 (21)0.96 (27)0.81 (13)0.77 (23)0.92 (11)0.79 (15)0.64 (14)0.90 (18)0.59 (13)0.96 (27)0.38 (6)0.90 (27)0.92 (11)0.74 (14)0.95 (21)0.80 (16)0.86 (19)0.96 (27)0.94 (15)0.87 (26)0.92 (11)0.79 (15)TOTAL0.91 (656)0.92 (639)0.89 (614)0.89 (628)0.81 (578)0.92 (649)Table 16: Detailed per-domain normalized coverage using hLA , hLM-CUT hLM-CUT+ . lineshows normalized coverage domain, number problems solvedshown parentheses. Domains grouped domains unit cost actions highvariance coverage, domains unit cost actions low variance coverage,domains non-uniform action costs, respectively.744fiO NLINE PEEDUP L EARNING PTIMAL P LANNINGexpansionshLAhLM-CUThLM-CUT+maxhrndhselhairport (26)freecell (13)logistics00 (16)mprime (18)mystery (13)pipesworld-tankage (9)satellite (7)zenotravel (9)2.291.221.09.217.852.6818.817.261.16417.81.02.741.415.083.781.231.047.651.01.211.131.381.011.11.01.01.01.01.01.01.01.01.0445.831.04.264.482.274.533.071.716.731.01.991.431.932.452.45blocks (27)depot (7)driverlog (13)grid (2)gripper (5)logistics98 (5)miconic (140)pathways (4)pipesworld-notankage (15)psr-small (48)rovers (7)schedule (27)storage (15)tpp (6)trucks-strips (9)7.5919.6311.365.041.06.431.040.633.091.312.771.092.32.7360.391.027.531.734.061.061.081.01.024.291.031.671.01.071.561.331.01.011.031.011.01.051.01.01.131.01.011.01.011.01.011.01.01.01.01.01.01.01.01.01.01.01.01.01.01.01.585.222.792.151.021.791.07.762.351.11.780.991.331.916.051.672.462.034.911.01.581.01.02.531.241.381.091.581.411.33elevators-opt08-strips (14)openstacks-opt08-strips (16)parcprinter-08-strips (13)pegsol-08-strips (27)scanalyzer-08-strips (6)sokoban-opt08-strips (21)transport-opt08-strips (11)woodworking-opt08-strips (11)33.161.1745.314.9424.1316.4315.553.331.651.02.021.341.51.031.291.371.11.01.01.011.051.051.011.01.01.01.01.01.01.01.01.04.651.035.911.695.51.142.664.022.91.071.01.261.871.131.821.63GEOMETRIC MEAN6.11.911.181.02.561.67Table 17: Detailed per-domain expansions relative maxh using hLA , hLM-CUT hLM-CUT+ .row shows geometric mean ratio expanded nodes relative maxh .Domains grouped domains unit cost actions high variance coverage,domains unit cost actions low variance coverage, domains nonuniform action costs, respectively.745fiD OMSHLAK , K ARPAS , & ARKOVITCHoverheadhLA /hLM-CUThLA /hLM-CUT+hLM-CUT /hLM-CUT+Threeairport (28)freecell (13)logistics00 (20)mprime (23)mystery (17)pipesworld-tankage (9)satellite (7)zenotravel (12)blocks (26)depot (7)driverlog (13)grid (2)gripper (7)logistics98 (6)miconic (141)pathways (5)pipesworld-notankage (17)psr-small (49)rovers (7)schedule (30)storage (15)tpp (6)trucks-strips (9)elevators-opt08-strips (16)openstacks-opt08-strips (16)parcprinter-08-strips (18)pegsol-08-strips (27)scanalyzer-08-strips (13)sokoban-opt08-strips (24)transport-opt08-strips (11)woodworking-opt08-strips (14)4%4%8%7%3%11%14%15%21%45%29%26%13%15%1%5%22%8%15%13%18%2%3%32%15%2%9%2%5%12%5%7%8%7%7%3%11%18%35%35%29%45%17%13%31%4%1%17%11%24%13%12%1%2%75%9%6%2%4%2%23%5%1%13%2%6%8%10%10%26%2%14%26%1%5%6%3%4%20%3%26%5%2%2%12%8%10%1%28%10%14%7%2%9%1%6%3%2%5%8%21%5%10%21%6%22%5%4%7%22%12%19%24%10%3%7%9%23%5%15%1%7%3%4%AVERAGE12%15%9%10%Table 18: Selective max overhead. row lists average percentage time spent learningclassification, total time taken selective max, domain,set heuristics. Domains grouped domains unit cost actions highvariance coverage, domains unit cost actions low variance coverage,domains non-uniform action costs, respectively.746fiO NLINE PEEDUP L EARNING PTIMAL P LANNINGcoverageairport (50)freecell (80)logistics00 (28)mprime (35)mystery (30)pipesworld-tankage (50)satellite (36)zenotravel (20)SUMsel=0.1h304921241712813174sel=0.5h304921241712813174sel=1h304921241712813174sel=1.5h304921241712813174sel=2h304921221712712170sel=3h304921231612711169sel=4h304921211513710166sel=5h304921211513710166Table 19: Number problems solved selective max domain varying valueshyper-parametercoverageairport (50)freecell (80)logistics00 (28)mprime (35)mystery (30)pipesworld-tankage (50)satellite (36)zenotravel (20)SUMsel=0.51h304821241712813173sel=0.6h304921241712813174sel=0.7h304921241712813174sel=0.8h304921241712813174sel=0.9h304921241712813174sel=0.99h304921241712813174Table 20: Number problems solved selective max domain varying valuesconfidence thresholdTable 18 lists average overhead selective max domain, combinationtwo heuristics.Tables 19, 20, 21, 22 23 list number problems solved domain, variousvalues , , N , sampling method classifier, respectively.coverageairport (50)freecell (80)logistics00 (28)mprime (35)mystery (30)pipesworld-tankage (50)satellite (36)zenotravel (20)SUM=10selNh304721241712813172=100selNh304921241712813174=1000selNh304621241712813171Table 21: Number problems solved selective max domain varying valuesinitial Sample Size N747fiD OMSHLAK , K ARPAS , & ARKOVITCHcoverageairport (50)freecell (80)logistics00 (28)mprime (35)mystery (30)pipesworld-tankage (50)satellite (36)zenotravel (20)SUMselPDBh304921241712813174selPh305321241712813178PselUh305521241712813180Table 22: Number problems solved selective max domain different samplingmethods. PDB sampling method Haslum et al. (2007), P biased probessampling method, U P unbiased probes sampling method.coverageairport (50)freecell (80)logistics00 (28)mprime (35)mystery (30)pipesworld-tankage (50)satellite (36)zenotravel (20)SUMBselNh304921241712813174selAODEh254920241712813168selITh303420241712712156Nsel3Nh303520241712713158Nsel5Nh284620231710611161Table 23: Number problems solved selective max domain different classifiers748fiO NLINE PEEDUP L EARNING PTIMAL P LANNINGcoverageselhportintportctrairport (33)freecell (58)logistics00 (21)mprime (24)mystery (17)pipesworld-tankage (13)satellite (10)zenotravel (13)0.91 (30)0.84 (49)1.00 (21)1.00 (24)1.00 (17)0.92 (12)0.80 (8)1.00 (13)0.91 (30)0.91 (53)0.95 (20)0.96 (23)1.12 (19)0.92 (12)0.70 (7)0.92 (12)0.91 (30)0.93 (54)1.00 (21)0.96 (23)1.24 (21)1.00 (13)0.70 (7)0.92 (12)blocks (28)depot (7)driverlog (14)grid (3)gripper (7)logistics98 (6)miconic (142)pathways (5)pipesworld-notankage (18)psr-small (49)rovers (8)schedule (30)storage (15)tpp (6)trucks-strips (10)1.00 (28)1.00 (7)1.00 (14)0.67 (2)1.00 (7)1.00 (6)1.00 (142)1.00 (5)0.94 (17)1.00 (49)1.00 (8)1.00 (30)1.00 (15)1.00 (6)1.00 (10)1.00 (28)1.00 (7)1.00 (14)0.67 (2)1.00 (7)1.00 (6)1.00 (142)1.00 (5)1.00 (18)1.00 (49)1.00 (8)1.00 (30)1.00 (15)1.00 (6)0.90 (9)1.00 (28)1.00 (7)1.00 (14)0.67 (2)1.00 (7)1.00 (6)1.00 (142)1.00 (5)1.00 (18)1.00 (49)1.00 (8)1.00 (30)1.00 (15)1.00 (6)1.00 (10)elevators-opt08-strips (22)openstacks-opt08-strips (20)parcprinter-08-strips (22)pegsol-08-strips (28)scanalyzer-08-strips (16)sokoban-opt08-strips (30)transport-opt08-strips (12)woodworking-opt08-strips (19)1.00 (22)0.90 (18)0.82 (18)0.96 (27)0.94 (15)0.97 (29)0.92 (11)0.89 (17)0.82 (18)0.90 (18)0.82 (18)0.96 (27)0.81 (13)0.97 (29)0.92 (11)0.84 (16)0.86 (19)0.95 (19)0.82 (18)0.96 (27)1.00 (16)0.97 (29)1.00 (12)0.89 (17)TOTAL0.95 (677)0.94 (672)0.96 (685)Table 24: Detailed coverage portfolio using hLA / hLM-CUT . Number problems solved selective max (selh ), simulated interruptible portfolio (portint ), simulated contractanytime portfolio (portctr ) domain using heuristics hLA / hLM-CUT . Domainsgrouped domains unit cost actions high variance coverage, domainsunit cost actions low variance coverage, domains non-uniform actioncosts, respectively.749fiD OMSHLAK , K ARPAS , & ARKOVITCHcoverageselhportintportctrairport (33)freecell (58)logistics00 (21)mprime (24)mystery (17)pipesworld-tankage (13)satellite (10)zenotravel (13)0.91 (30)0.71 (41)1.00 (21)1.00 (24)1.00 (17)0.69 (9)1.00 (10)0.92 (12)0.91 (30)0.91 (53)0.95 (20)1.00 (24)1.12 (19)0.92 (12)0.80 (8)0.85 (11)0.91 (30)0.93 (54)1.00 (21)1.00 (24)1.18 (20)1.00 (13)0.80 (8)0.85 (11)blocks (28)depot (7)driverlog (14)grid (3)gripper (7)logistics98 (6)miconic (142)pathways (5)pipesworld-notankage (18)psr-small (49)rovers (8)schedule (30)storage (15)tpp (6)trucks-strips (10)0.93 (26)1.00 (7)0.93 (13)0.67 (2)1.00 (7)1.00 (6)1.00 (142)1.00 (5)0.94 (17)1.00 (49)1.00 (8)1.00 (30)1.00 (15)1.00 (6)0.90 (9)0.93 (26)1.00 (7)1.00 (14)0.67 (2)1.00 (7)1.00 (6)1.00 (142)1.00 (5)1.00 (18)1.00 (49)1.00 (8)1.00 (30)1.00 (15)1.00 (6)0.70 (7)0.96 (27)1.00 (7)1.00 (14)0.67 (2)1.00 (7)1.00 (6)1.00 (142)1.00 (5)1.00 (18)1.00 (49)1.00 (8)1.00 (30)1.00 (15)1.00 (6)0.80 (8)elevators-opt08-strips (22)openstacks-opt08-strips (20)parcprinter-08-strips (22)pegsol-08-strips (28)scanalyzer-08-strips (16)sokoban-opt08-strips (30)transport-opt08-strips (12)woodworking-opt08-strips (19)0.73 (16)0.85 (17)1.00 (22)0.96 (27)0.81 (13)0.80 (24)0.92 (11)0.79 (15)0.82 (18)0.85 (17)1.00 (22)0.96 (27)0.75 (12)0.83 (25)0.92 (11)0.79 (15)0.82 (18)0.85 (17)1.00 (22)0.96 (27)0.94 (15)0.83 (25)1.00 (12)0.79 (15)TOTAL0.92 (651)0.93 (666)0.94 (676)Table 25: Detailed coverage portfolio using hLA / hLM-CUT+ . Number problems solved selective max (selh ), simulated interruptible portfolio (portint ), simulated contractanytime portfolio (portctr ) domain using heuristics hLA / hLM-CUT+ . Domainsgrouped domains unit cost actions high variance coverage, domainsunit cost actions low variance coverage, domains non-uniform actioncosts, respectively.750fiO NLINE PEEDUP L EARNING PTIMAL P LANNINGcoverageselhportintportctrairport (33)freecell (58)logistics00 (21)mprime (24)mystery (17)pipesworld-tankage (13)satellite (10)zenotravel (13)0.85 (28)0.22 (13)0.95 (20)1.00 (24)0.94 (16)0.69 (9)0.80 (8)0.92 (12)0.88 (29)0.24 (14)0.95 (20)1.00 (24)1.18 (20)0.69 (9)0.80 (8)0.92 (12)0.88 (29)0.26 (15)0.95 (20)1.00 (24)1.24 (21)0.85 (11)0.80 (8)0.92 (12)blocks (28)depot (7)driverlog (14)grid (3)gripper (7)logistics98 (6)miconic (142)pathways (5)pipesworld-notankage (18)psr-small (49)rovers (8)schedule (30)storage (15)tpp (6)trucks-strips (10)1.00 (28)1.00 (7)1.00 (14)0.67 (2)1.00 (7)1.00 (6)0.99 (141)1.00 (5)0.94 (17)1.00 (49)0.88 (7)1.00 (30)1.00 (15)1.00 (6)1.00 (10)1.00 (28)1.00 (7)0.93 (13)0.67 (2)0.86 (6)1.00 (6)0.99 (140)1.00 (5)0.89 (16)1.00 (49)0.88 (7)0.93 (28)1.00 (15)1.00 (6)0.90 (9)1.00 (28)1.00 (7)0.93 (13)0.67 (2)1.00 (7)1.00 (6)0.99 (141)1.00 (5)0.94 (17)1.00 (49)0.88 (7)1.00 (30)1.00 (15)1.00 (6)1.00 (10)elevators-opt08-strips (22)openstacks-opt08-strips (20)parcprinter-08-strips (22)pegsol-08-strips (28)scanalyzer-08-strips (16)sokoban-opt08-strips (30)transport-opt08-strips (12)woodworking-opt08-strips (19)0.95 (21)0.95 (19)0.91 (20)0.96 (27)0.94 (15)0.83 (25)0.92 (11)0.95 (18)0.82 (18)0.90 (18)1.00 (22)0.96 (27)0.81 (13)0.93 (28)0.92 (11)0.79 (15)0.86 (19)0.95 (19)1.00 (22)0.96 (27)0.94 (15)0.93 (28)0.92 (11)0.84 (16)TOTAL0.91 (630)0.90 (625)0.93 (640)Table 26: Detailed coverage portfolio using hLM-CUT / hLM-CUT+ . Number problems solvedselective max (selh ), simulated interruptible portfolio (portint ), simulatedcontract anytime portfolio (portctr ) domain using heuristics hLM-CUT / hLM-CUT+ .Domains grouped domains unit cost actions high variance coverage,domains unit cost actions low variance coverage, domains nonuniform action costs, respectively.751fiD OMSHLAK , K ARPAS , & ARKOVITCHcoverageselhportintportctrairport (33)freecell (58)logistics00 (21)mprime (24)mystery (17)pipesworld-tankage (13)satellite (10)zenotravel (13)0.91 (30)0.57 (33)0.95 (20)0.96 (23)1.00 (17)0.85 (11)0.80 (8)0.92 (12)0.91 (30)0.91 (53)0.95 (20)1.00 (24)1.18 (20)0.92 (12)0.80 (8)0.92 (12)0.91 (30)0.93 (54)1.00 (21)1.00 (24)1.18 (20)0.92 (12)0.80 (8)0.92 (12)blocks (28)depot (7)driverlog (14)grid (3)gripper (7)logistics98 (6)miconic (142)pathways (5)pipesworld-notankage (18)psr-small (49)rovers (8)schedule (30)storage (15)tpp (6)trucks-strips (10)1.00 (28)1.00 (7)0.93 (13)0.67 (2)1.00 (7)1.00 (6)1.00 (142)1.00 (5)0.94 (17)1.00 (49)1.00 (8)1.00 (30)1.00 (15)1.00 (6)1.00 (10)1.00 (28)1.00 (7)1.00 (14)0.67 (2)1.00 (7)1.00 (6)1.00 (142)1.00 (5)1.00 (18)1.00 (49)1.00 (8)1.00 (30)1.00 (15)1.00 (6)0.90 (9)1.00 (28)1.00 (7)1.00 (14)0.67 (2)1.00 (7)1.00 (6)1.00 (142)1.00 (5)1.00 (18)1.00 (49)1.00 (8)1.00 (30)1.00 (15)1.00 (6)0.90 (9)elevators-opt08-strips (22)openstacks-opt08-strips (20)parcprinter-08-strips (22)pegsol-08-strips (28)scanalyzer-08-strips (16)sokoban-opt08-strips (30)transport-opt08-strips (12)woodworking-opt08-strips (19)0.95 (21)0.80 (16)0.86 (19)0.96 (27)0.94 (15)0.87 (26)0.92 (11)0.79 (15)0.82 (18)0.90 (18)1.00 (22)0.96 (27)0.81 (13)0.97 (29)0.92 (11)0.84 (16)0.86 (19)0.95 (19)1.00 (22)0.96 (27)0.81 (13)0.97 (29)0.92 (11)0.89 (17)TOTAL0.92 (649)0.95 (679)0.95 (684)Table 27: Detailed coverage portfolio using hLA / hLM-CUT / hLM-CUT+ . Number problemssolved selective max (selh ), simulated interruptible portfolio (portint ), simulated contract anytime portfolio (portctr ) domain using heuristics hLA / hLM-CUT/ hLM-CUT+ . Domains grouped domains unit cost actions high variancecoverage, domains unit cost actions low variance coverage, domainsnon-uniform action costs, respectively.Tables 24, 25, 26 27 list normalized coverage domain selective max,simulated contract interruptible sequential portfolios.ReferencesArfaee, S. J., Zilles, S., & Holte, R. C. (2010). Bootstrap learning heuristic functions. Felner,A., & Sturtevant, N. (Eds.), Proceedings Third Annual Symposium CombinatorialSearch (SoCS 2010), pp. 5260. AAAI Press.Backstrom, C., & Klein, I. (1991). Planning polynomial time: SAS-PUBS class. Computational Intelligence, 7(3), 181197.Backstrom, C., & Nebel, B. (1995). Complexity results SAS+ planning. Computational Intelligence, 11(4), 625655.Bayardo Jr., R. J., & Schrag, R. (1997). Using CSP look-back techniques solve real-world SATinstances. Kuipers, B., & Webber, B. L. (Eds.), Proceedings Fourteenth NationalConference Artificial Intelligence (AAAI 1997), pp. 203208. AAAI Press.752fiO NLINE PEEDUP L EARNING PTIMAL P LANNINGBonet, B., & Helmert, M. (2010). Strengthening landmark heuristics via hitting sets. Coelho,H., Studer, R., & Wooldridge, M. (Eds.), Proceedings 19th European ConferenceArtificial Intelligence (ECAI 2010), pp. 329334. IOS Press.Bonet, B., Loerincs, G., & Geffner, H. (1997). robust fast action selection mechanismplanning. Kuipers, B., & Webber, B. L. (Eds.), Proceedings Fourteenth NationalConference Artificial Intelligence (AAAI 1997), pp. 714719. AAAI Press.Botea, A., Enzenberger, M., Muller, M., & Schaeffer, J. (2005). Macro-FF: Improving AI planningautomatically learned macro-operators. Journal Artificial Intelligence Research, 24,581621.Brafman, R., & Shani, G. (2012). multi-path compilation approach contingent planning.Hoffmann, J., & Selman, B. (Eds.), Proceedings Twenty-Sixth AAAI ConferenceArtificial Intelligence (AAAI 2012), pp. 915. AAAI Press.Burke, E., Kendall, G., Newall, J., Hart, E., Ross, P., & Schulenburg, S. (2003). Hyper-Heuristics:Emerging Direction Modern Search Technology. Handbook Metaheuristics, International Series Operations Research & Management Science, chap. 16, pp. 457474.Coles, A., & Smith, A. (2007). Marvin: heuristic search planner online macro-action learning. Journal Artificial Intelligence Research, 28, 119156.Cover, T. M., & Hart, P. E. (1967). Nearest neighbor pattern classification. IEEE TransactionsInformation Theory, 13(1), 21 27.de la Rosa, T., Jimenez, S., & Borrajo, D. (2008). Learning relational decision trees guidingheuristic planning. Rintanen, J., Nebel, B., Beck, J. C., & Hansen, E. (Eds.), ProceedingsEighteenth International Conference Automated Planning Scheduling (ICAPS2008), pp. 6067. AAAI Press.Domshlak, C., Karpas, E., & Markovitch, S. (2010). max max: Online learningspeeding optimal planning. Fox, M., & Poole, D. (Eds.), Proceedings TwentyFourth AAAI Conference Artificial Intelligence (AAAI 2010), pp. 10711076. AAAI Press.Fern, A. (2010). Speedup learning. Sammut, C., & Webb, G. I. (Eds.), Encyclopedia MachineLearning, pp. 907911. Springer.Fern, A., Khardon, R., & Tadepalli, P. (2011). first learning track international planningcompetition. Machine Learning, 84(1-2), 81107.Fikes, R. E., Hart, P. E., & Nilsson, N. J. (1972). Learning executing generalized robot plans.Artificial Intelligence, 3, 251288.Fikes, R. E., & Nilsson, N. J. (1971). STRIPS: new approach application theoremproving problem solving. Artificial Intelligence, 2, 189208.Finkelstein, L., & Markovitch, S. (1998). selective macro-learning algorithm applicationNxN sliding-tile puzzle. Journal Artificial Intelligence Research, 8, 223263.Garca-Olaya, A., Jimenez, S., & Linares Lopez, C. (2011). 2011 international planning competition. Tech. rep., Universidad Carlos III de Madrid. http://hdl.handle.net/10016/11710.Geffner, H. (2010). model-based approach autonomous behavior: personal view. Fox,M., & Poole, D. (Eds.), Proceedings Twenty-Fourth AAAI Conference ArtificialIntelligence (AAAI 2010), pp. 17091712. AAAI Press.753fiD OMSHLAK , K ARPAS , & ARKOVITCHHaslum, P., Botea, A., Helmert, M., Bonet, B., & Koenig, S. (2007). Domain-independent construction pattern database heuristics cost-optimal planning. Holte, R. C., & Howe, A. E.(Eds.), Proceedings Twenty-Second AAAI Conference Artificial Intelligence (AAAI2007), pp. 10071012. AAAI Press.Helmert, M. (2006). Fast Downward planning system. Journal Artificial Intelligence Research, 26, 191246.Helmert, M., & Domshlak, C. (2009). Landmarks, critical paths abstractions: Whats difference anyway?. Gerevini, A., Howe, A., Cesta, A., & Refanidis, I. (Eds.), ProceedingsNineteenth International Conference Automated Planning Scheduling (ICAPS2009), pp. 162169. AAAI Press.Helmert, M., Haslum, P., & Hoffmann, J. (2007). Flexible abstraction heuristics optimal sequential planning. Boddy, M., Fox, M., & Thiebaux, S. (Eds.), Proceedings SeventeenthInternational Conference Automated Planning Scheduling (ICAPS 2007), pp. 176183. AAAI Press.Helmert, M., & Roger, G. (2008). good almost perfect?. Fox, D., & Gomes, C. P. (Eds.),Proceedings Twenty-Third AAAI Conference Artificial Intelligence (AAAI 2008), pp.944949. AAAI Press.Helmert, M., Roger, G., & Karpas, E. (2011). Fast Downward Stone Soup: baseline buildingplanner portfolios. ICAPS 2011 Workshop Planning Learning, pp. 2835.Hoffmann, J., & Nebel, B. (2001). FF planning system: Fast plan generation heuristicsearch. Journal Artificial Intelligence Research, 14, 253302.Hutter, F., Hoos, H. H., Leyton-Brown, K., & Stutzle, T. (2009). ParamILS: automatic algorithmconfiguration framework. Journal Artificial Intelligence Research, 36, 267306.Karpas, E., & Domshlak, C. (2009). Cost-optimal planning landmarks. Boutilier, C.(Ed.), Proceedings 21st International Joint Conference Artificial Intelligence (IJCAI 2009), pp. 17281733.Katz, M., & Domshlak, C. (2010). Implicit abstraction heuristics. Journal Artificial IntelligenceResearch, 39, 51126.Kautz, H., & Selman, B. (1992). Planning satisfiability. Neumann, B. (Ed.), Proceedings10th European Conference Artificial Intelligence (ECAI 1992), pp. 359363. John WileySons.Keyder, E., & Geffner, H. (2009). Soft goals compiled away. Journal Artificial IntelligenceResearch, 36, 547556.Marques-Silva, J. P., & Sakallah, K. A. (1996). GRASP - new search algorithm satisfiability.Proceedings 1996 IEEE/ACM International Conference Computer-Aided Design(ICCAD 1996), pp. 220227.Minton, S. (1994). Machine Learning Methods Planning. Morgan Kaufmann Publishers Inc.Nissim, R., Hoffmann, J., & Helmert, M. (2011). Computing perfect heuristics polynomial time:bisimulation merge-and-shrink abstraction optimal planning. Walsh, T. (Ed.),Proceedings 22nd International Joint Conference Artificial Intelligence (IJCAI11),pp. 19831990. AAAI Press/IJCAI.754fiO NLINE PEEDUP L EARNING PTIMAL P LANNINGPalacios, H., & Geffner, H. (2009). Compiling uncertainty away conformant planning problemsbounded width. Journal Artificial Intelligence Research, 35, 623675.Pearl, J. (1984). Heuristics: Intelligent Search Strategies Computer Problem Solving. AddisonWesley.Pednault, E. P. D. (1989). ADL: Exploring middle ground STRIPS situationcalculus. Brachman, R. J., Levesque, H. J., & Reiter, R. (Eds.), Proceedings FirstInternational Conference Principles Knowledge Representation Reasoning (KR1989), pp. 324332. Morgan Kaufmann.Rendell, L. A. (1983). new basis state-space learning systems successful implementation. Artificial Intelligence, 20(4), 369392.Rintanen, J., Heljanko, K., & Niemela, I. (2006). Planning satisfiability: Parallel plans algorithms plan search. Artificial Intelligence, 170(1213), 10311080.Russell, S. J., & Zilberstein, S. (1991). Composing real-time systems. Mylopoulos, J., & Reiter,R. (Eds.), Proceedings 12th International Joint Conference Artificial Intelligence(IJCAI 1991), pp. 212217. Morgan Kaufmann.Schiex, T., & Verfaillie, G. (1993). Nogood recording static dynamic constraint satisfactionproblems. Journal Artificial Intelligence Research, 3, 4855.Thayer, J. T., Dionne, A. J., & Ruml, W. (2011). Learning inadmissible heuristics search.Bacchus, F., Domshlak, C., Edelkamp, S., & Helmert, M. (Eds.), Proceedings TwentyFirst International Conference Automated Planning Scheduling (ICAPS 2011), pp.250257. AAAI Press.Utgoff, P. E., Berkman, N. C., & Clouse, J. A. (1997). Decision tree induction based efficienttree restructuring. Machine Learning, 29(1), 544.Webb, G. I., Boughton, J. R., & Wang, Z. (2005). naive Bayes: Aggregating one-dependenceestimators. Machine Learning, 58(1), 524.Yoon, S., Fern, A., & Givan, R. (2007). FF-Replan: baseline probabilistic planning. Boddy,M., Fox, M., & Thiebaux, S. (Eds.), Proceedings Seventeenth International ConferenceAutomated Planning Scheduling (ICAPS 2007), pp. 352359. AAAI Press.Yoon, S., Fern, A., & Givan, R. (2008). Learning control knowledge forward search planning.Journal Machine Learning Research, 9, 683718.Zimmerman, T., & Kambhampati, S. (2003). Learning-assisted automated planning: looking back,taking stock, going forward. AI Magazine, 24, 7396.755fiJournal Artificial Intelligence Research 44 (2012) 397-421Submitted 01/12; published 06/12Semantic Similarity Measures Applied OntologyHuman-Like InteractionEsperanza AlbaceteJavier CalleElena CastroDolores CuadraEALBACET@INF.UC3M.ESFCALLE@INF.UC3M.ESECASTRO@INF.UC3M.ESDCUADRA@INF.UC3M.ESComputer Science Department, Carlos III University,Madrid 28911, SpainAbstractfocus paper calculation similarity two concepts ontologyHuman-Like Interaction system. order facilitate calculation, similarity functionproposed based five dimensions (sort, compositional, essential, restrictive descriptive)constituting structure ontological knowledge. paper includes proposal computingsimilarity function dimension knowledge. Later on, similarity values obtainedweighted aggregated obtain global similarity measure. order calculate weightsassociated dimension, four training methods proposed. training methodsdiffer element fit: user, concepts pairs concepts, hybrid approach.evaluating proposal, knowledge base fed WordNet extended usingknowledge editing toolkit (Cognos). evaluation proposal carriedcomparison system responses given human test subjects, providingmeasure soundness procedure revealing ways proposal mayimproved.1. Introductionmain purpose ontology human-like interaction system unify representationconcept, relating appropriate terms, well conceptsshares semantic relation. Furthermore, ontological component also ableperform certain inferential processes, calculation semantic similarityconcepts. subject similarity continues widely studied fieldsliterature computer science, artificial intelligence, psychology linguistics. Good similaritymeasures necessary several techniques fields including information retrieval,clustering, data-mining, sense disambiguation, ontology translation automatic schemamatching. present paper focuses study semantic similarity conceptsontology framework natural interaction.principal benefit gained procedure ability substitute one conceptanother based calculation similarity two, given specific circumstances.users perspective, procedure allows use synonyms (terms related singleconcept) concept case user familiar original concept itself.Moreover, semantic similarity offers possibility build explanations clarifying conceptuser based similar concepts, thereby enhancing communicative effectiveness.2012 AI Access Foundation. rights reserved.fiALBACETE, CALLE, CASTRO & CUADRAhand, system may also able understand previously-unknown concept,long user able relate similar concepts previously known system.way, system learn new concepts automatically enrich ontology improvefuture interactions.first task study develop semantic similarity measure takes accountparticular ontological dimensions described earlier study (Calle, Castro & Cuadra, 2008).approach, conceptualization comprises seven ontological dimensions: semiotic, sort,compositional, essential, restrictive, descriptive, comparative. first three dimensionspreviously applied related works, stated Section 2. Essential, restrictivedescriptive dimensions part nature concept, influence human judgmentsimilarity detailed Section 3. seventh one, comparative dimension, derivedprevious dimensions charge calculating degree similarityontological concepts.second goal present article evaluate quality mechanism developedcalculation similarities two concepts ontology speciallydesigned human-like interaction system (Calle F., 2004). achieve this, severalexperiments designed performed here. experimentsconsequent evaluation semantic similarity measure carried out, however,necessary implement similarity dimensions defined conceptual model feeddatabase large number concepts.briefly outline content follows paper, Section 2 reviews literaturesimilarity measures ontologies methods available evaluation. Section 3,approach similarity measures applied ontological model based several dimensionsproposed. Section 4, detailed explanation provided experiments designed testproposal, well results obtained execution. Section 5 discusses limitationsencountered study. Finally, Section 6 presents conclusions future research.2. Related Workpresent section paper two main objectives. First, aims provide overviewdifferent types approaches available comparison concepts ontologies and,doing, identify foundations desired similarity measure may modeled,taking account seven dimensions described previous study (Calle et al., 2008).Secondly, aims select best way evaluate results yielded desired similaritymeasure according studies regarding similarity metrics assessment.Basically two types methods exist comparison terms graph-based ontology:edge-based methods using graph edges types data source node-based methodsusing graph nodes properties main data source. simplest intuitivesimilarity measure, former method based mainly counting number edgespath two terms graph (Rada, Mili, Bicknell & Blettner, 1989). Within edgebased method, two general approaches exist: firstly, distance approach selects eithershortest path average paths (when one path exists) secondly common398fiSEMANTIC SIMILARITY MEASURES APPLIED ONTOLOGYpath approach calculates similarity directly length path lowest commonancestor two terms root node (Wu & Palmer, 1994). past years,variety edge-based methods defined (Resnik, 1995; Leacock & Chodorow, 1998).edge-based methods grounded two basic assumptions: firstly, nodes linksuniformly distributed ontology, is, terms depthspecificity (Budanitsky, 1999) and, secondly, edges level ontology indicatesemantic distance terms. However, suppositions rarely truemajority ontologies. reason, several strategies proposed responsefact. One example strategy weighting edges according hierarchicaldepth use node density link type (Richardson, Smeaton & Murphy, 1994).Nevertheless, strategies solve aforementioned problems due fact termsdepth necessarily specificity edges levelnecessarily represent semantic distance.second, node-based, method relies comparison properties termsinvolved related terms themselves, ancestors descendants.commonly used concept methods information content (IC), providing measurespecific informative term is. IC term c quantified negativelog-likelihood, IC = -log p(c), p(c) probability occurrence c specificcorpus, generally estimated annotation frequency. Another approach employedobtain IC based number children term ontological structure (Seco,Veale & Hayes, 2004). concept IC applied common ancestors two termsorder quantify information share and, thereby, measure semantic similarity.way, two main approaches exist. first informative common ancestor (MICA)technique common ancestor highest IC considered (Resnik, 1995).second disjoint common ancestor (DCA) technique disjoint commonancestors considered (the common ancestors subsume commonancestor). one definition (Lin, 1998), similarity two concepts using node-basedmethod expressed ratio amount information needed statecommonality two concepts information needed fully describe them.Moreover, similarity measure hierarchical ontologies called ontology structure-basedsimilarity (OSS) also defined (Schickel-Zuber, 2007) whose major ingredientcomputation a-priori score concept c, (APS(c)), shares similarities IC(i.e., calculated topology structure ontology reflectinginformation contained within concepts).Additionally, several hybrid methods also defined attempt improveresults techniques defined above. work Jiang Conrath, (1997), example,combined model defined derived edge-based notion adding informationcontent decision factor. link strength two concepts defined differenceinformation content them.aim collecting different methods approaches, SimPack, generic Javalibrary similarity measures use ontologies, created (Bernstein, Kaufmann,399fiALBACETE, CALLE, CASTRO & CUADRAKiefer & Brki, 2005) includes implementation ontology-based similarity methods(including edge-based node-based measures). important note majoritytechniques described define semantic similarity concepts appliedhierarchical ontologies whose structure takes account one two dimensionsgraph. example, WordNet (Fellbaum, 1998) consists ontological graph100,000 concepts whose edges model is_a part_of relationships. Perl module(Pedersen, Patwardhan & Michelizzi, 2004) implemented lexical databasevariety semantic similarity measures. Another example application Gene Ontology(Department Genetics, Stanford University School Medicine, California, USA., 2000), oneimportant ontologies within bioinformatics community, 20,000 conceptsmodeling is_a part_of relationships graph. Thus, nonetechniques described section supposed appropriate dealingtwo dimensions similarity, nevertheless useful attempt definedimensions present studys ontological model.second aim present section review assessment techniques ontologicalsimilarity functions used earlier studies. gold standard established majorityexperimental evaluations similarity (Resnik, 1999; Jiang & Conrath, 1997; Altintas, Karsligil,& Coskun, 2005; Schickel-Zuber, 2007; Bernstein et al., 2005) based experimentdescribed Miller Charles study (1991) become benchmark determiningsimilarity words natural language processing research. experiment reliessimilarity assessments made 38 university students provided 30 name pairs chosenpriori cover high, intermediate low levels similarity asked assesssimilarity meaning scale 0 (no similarity) 4 (perfect synonymy). averagescored values represents good estimation degree similarity two terms.certain evaluations based human judgment (Inkpen, 2007; Bernstein et al., 2005),variations number participants way administer questionnaireintroduced. one studies (Bernstein et al., 2005), website containing survey tooldesigned perform evaluation. Web experiment, subjects asked assesssimilarity 73 pairs concepts scale 1 (no similarity) 5 (identical). Finally,subjects also given possibility adding comments assessment. evaluatequality similarity measures, results compared test subjects assessmentsusing corrected Spearman rank correlation coefficient.concluded human reasoning one widely-used methodscomparison performing validation similarity measure. reason,methodology also used experimentation section present study. Sincedifficult run user-based evaluation complicated ontologies, example, GeneOntology (Lord, Stevens, Brass & Goble, 2003), deemed necessary findmodel ontology elements test subjects could understand. Therefore,ontological module implemented, must populated sufficiently good coveragedomain knowledge, is, enough knowledge meet system requirements.400fiSEMANTIC SIMILARITY MEASURES APPLIED ONTOLOGY3. Theoretical Approachconceptual model grounding present study (Calle et al., 2008) distributes ontologicalknowledge seven different dimensions. semiotic dimension represents relationshipconcepts, terms language. example, shown Figure 1, conceptWordNets synset 3082979 corresponds machine able perform calculationsautomatically, one terms associated concept computer. termsrelated concept computing machine, computing device, data processor,electronic computer information processing system, also linked conceptcorresponds English language (synset 6947032).Figure 1: Example semiotic dimension representationsort dimension represents is_a relationship concepts, relates conceptconcepts models polytree structure. instance, shown Figure 2, termsnode, server web site related concepts instances computer.Figure 2: Sort dimension exampleessential dimension represents general taxonomy concepts. taxonomylocated nodes top polytree represented sort dimension. Therefore,relations included design already observed sort dimension. sinceorganize knowledge higher abstraction level (they discriminative)taken account separately, adding extra value similarity measure.design crucial attaining good similarity measures, determines usefulnessdimension. essential dimension WordNet (Princeton Univ., 2011), example,classifies concepts four main linguistic categories (verb, noun, adjective, adverb).approach adequate linguistic interaction domain, may weaker generalinteraction domain. proposal includes essential design inspired previous (Calle et al.,2008) related works (Gee, 1999; Miller, 1995) refined preliminaryexperimentation. design departs three main categories (abstract, actions entities)develops main classes concepts, shown Figure 3. Finally, addedproposal aimed general interaction domains, could improved suited specificdomains particular interaction systems.401fiALBACETE, CALLE, CASTRO & CUADRAconcept[05835747]..abstractactionentity[05854150][06320569][00001740]attributecircumstancesui generis[00024264][14512817][90000001]placetimerolelanguage[08513718][00028270][00722061][06282651]activityenvironment[00407535][08567235].domain.interactivestaticactive[01946439][01564315][00524481][05999266]unidirectionalcomm. agentcommunicativeagent[90000002][02956371]humanmechanical[02743391][02891236]userInteractionsystem[10741590]reactivecyclic[02105176][00675701][05661996]Figure 3: Essential dimension taxonomycompositional dimension represents part-whole relationship concepts.way, concept relationships collection concepts part it.Figure 4 shows concepts part computer, example hard disk,RAM ALU.computer[03082979]hard diskRAMALU[03492542][04052757][02995345]Figure 4: Compositional dimension examplerestrictive dimension shown Figure 5 describes compatibility conceptsrelated action rest. example, action compute relatedconcepts computer, calculator laptop, among others.402fiSEMANTIC SIMILARITY MEASURES APPLIED ONTOLOGYcomputer[03082979]Action concept:computecalculator[02938886][00637259]laptop[03642806]Figure 5: Restrictive dimension exampledescriptive dimension shown Figure 6 charge relationships threekinds concepts: generic concept (entity, abstract entity action), attribute likelycharacterize concept, domain (of values) attribute defined. Noticecould several available domains given attribute, domain couldnumeric (magnitudes regarding unit) enumerated (a concept composed setnamed values also concepts). example, instance generic concept harddisk value numeric domain information bytes attribute conceptstorage capacity.Generic concept:hard diskAttribute concept:storage capacityDomain concept:Information bytes[03492542][13562133][13626013]Figure 6: Descriptive dimension exampleFinally, comparative dimension derived previous dimensions responsiblecalculating real time degree similarity ontological concepts. paper,fact, focuses precisely similarity calculation. Finally, reasons efficiency,frequently requested similarities buffered, is, stored calculated, periodicallyupdated retrieved necessary.4. Proposalpaper proposes evaluates similarity measure based combination individualsimilarity measures according dimensions explained (see Section 3).combination produced training across numerous observations affect weightdimension contributes final decision. Training performed accordingdifferent criteria. one hand, different human subjects support judgments differentcombinations dimensions. hand, nature concept determinesrelevant dimension comparison. example, comparing concept scannerconcept printer, sort dimension could influential, since typescomputer peripherals; however restrictive dimension could influentialrelated different actions. opposite may happen concepts teacher tutorial403fiALBACETE, CALLE, CASTRO & CUADRArelated similar actions according restrictive dimension,teaching, sort dimension little influence case.following step describe similarity measure adapted described ontologicaldimensions except semiotic dimension. Yet approach, similaritysemiotic dimension, similarity terms frequently described edit distanceLevenshtein distance (1966), is, number changes necessary turn one stringanother string. decision leave dimension apart supported preliminary studiesmeasure yields average error rate 50% cases 80%.Furthermore, every concept study, accuracy provided dimension lowerdimensions (the semiotic dimension never produced bestprediction), dimension never ranked first tested separately.reason, estimated cannot contribute positively results (at least, cannotproperly adapted). Last least, preliminary experimentation training includingdimension, observed weight tended zero, drawbackslowing convergence weights rest dimensions. However, work,evolution similarity measure (supported knowledge dimension)incorporated global measure similarity.4.1 Inference Mechanismssub-section describes method used calculate degree similarity twogiven concepts ontology. Since ontological knowledge structured differentdimensions, similarity measure also based dimensions. Therefore, partialsimilarity calculations made sort, essential, compositional, restrictivedescription dimensions described previously. resulting overall similarity twoconcepts obtained calculation weighted average five partial similaritiesSs, Sc, Se, Sr Sd similarity measures according sort, compositional,essential, restrictive description dimensions, respectively. values w1, w2, w3, w4 w5represent weights assigned dimension resulting total similaritytwo concepts value 0 (completely different concepts) 1 (the twoconcepts same).following sections describe detail procedures developed calculationpartial similarities.4.1.1 SIMILARITY ACCORDING SORT DIMENSIONsort dimension represents is_a relationship concepts. dimensionpolytree structure, allowing concept descendant one concept. Similaritydimension often calculated proportional intersection list predecessors404fiSEMANTIC SIMILARITY MEASURES APPLIED ONTOLOGYcompared concepts regarding total size lists. define measure, variationedge-counting technique concretely, conceptual similarity measure definedwork Wu Palmer (1994) employed. Given two concepts, C1 C2,measure definedN1 N2 number ancestors C1 C2, N3 number commonancestors C1 C2 (in advantageous tree several found polytree).4.1.2 SIMILARITY ACCORDING COMPOSITIONAL DIMENSIONcompositional dimension represents part-whole relationship concepts.reason, appropriate way calculate similarity two concepts baseddimension comparison parts (or ingredients) concepts. Furthermore,calculation must also take account fact concept may consist requiredoptional concepts. detail important calculating similarity since greater weight mustgiven required ingredients appearing concepts, lower weight givenoptional ingredients. resulting similarity two concepts, C1 C2, termscompositional dimension obtained applying formula:N1 number common components arising intersectioncomponents concept C1 components concept C2 type required; N2number common components arising intersection components C2required components C1; N3 number required components C1 C2common; N4 total number common components (both required optional)two concepts; M1 M2 represent number required components concepts C1C2, respectively. Finally, M3 M4 indicate total number components C1 C2 have.4.1.3 SIMILARITY ACCORDING ESSENTIAL DIMENSIONessential dimension contains set abstract concepts define generic typesconcepts (such action, entity, abstract, circumstance attribute). generic classificationfrequently influences human speakers estimating similarity. works similaritycalculation posed concepts comparable included categoryWordNets taxonomy (RiTa.WordNet, 2008). approach endows critical valuedimension, omitting rest classification. proposeddimension contribute similarity estimation (albeit certain weightcould different rest), concepts observed design essentialdimension may influence similarity estimation.405fiALBACETE, CALLE, CASTRO & CUADRAmethod calculating similarity two concepts C1 C2 essentialdimension based intersection essential ancestors (ancestors within subsetessential concepts). formalized follows:Card(E1) Card(E2) are, respectively, total number essential ancestorsconcepts C1 C2, Card(E1 E2) indicates number common essential ancestors.4.1.4 SIMILARITY ACCORDING RESTRICTIVE DIMENSIONrestrictive dimension defined concept representing action anotherconcept representing entity. Similarity dimension calculated different waydepending type concepts compared. reason, two different similaritymeasures exist dimension: comparing two actions comparing two entities. Similaritytwo concepts representing entity based action conceptsentities common. formula used calculation similarity comparingtwo entities, C1 C2, definedM1 M2 number common actions positive negativerestrictive relationship entities C1 C2, respectively. values N1, N2, N3 N4represent, respectively, total number actions positive relationship entityC1, negative relationship C1, positive relationship entity C2, negativerelationship C2.regards similarity two concepts representing action, calculated basedset concepts defined actions, similar higher numberrestricted concepts common. formula calculate similarity two actionconcepts (C1, C2) particular sign (positive negative) definedN3 number common entities shared two actions, N1 N2total number entities restrictive relationship C1 C2, respectively.4.1.5 SIMILARITY ACCORDING DESCRIPTIVE DIMENSIONdescription dimension represents relationship concept, attribute valueconcrete domain. Similarity dimension calculated differently depending typeconcepts compared, is, entities, attributes domains. pairs concepts (C1, C2)representing entity, applicable formula defined406fiSEMANTIC SIMILARITY MEASURES APPLIED ONTOLOGYN1 number common attributes without default value assigned, N2number common attributes whose value entities assigneddefault, N3 number common attributes value oneassigned default. terms M1 M2 correspond total number attributesrelated concepts C1 C2, respectively.concepts (C1, C2) attributes, formula apply definedN3 number common values attributes, N1, N2 total numberpossible values attributes C1 C2, respectively.Finally, concepts compared (C1, C2) represent domains, similarity accordingdimension calculated based amount common attributes (for domainsapply) number values shared domains.N3 number common attributes shared domains (C1, C2), N1, N2total number attributes associated them. Finally, M3 number common valuesdefined domains, M1, M2 total number values two domains.Finally, concepts compared (C1, C2) may values belonging domain, eitherenumerated numeric type. operating domains, necessary define previouslycorrespondence them. Numeric domains related function (typically,lineal proportion). Relating enumerated domain numeric domain achievedassigning enumerated value fuzzy label numeric domain. Finally,correspondence two enumerated domains always involves intermediate numericdomain (with correspondence defined two domains). valuescomparable, formula measure similarity defined follows:Cinf Csup are, respectively, lower limit upper limit within rangevalues, C1 C2 correspondent numeric comparable values.4.2 Preliminary Experimentationtesting proposal, preliminary experiments performed refineobtain first perspective validity. experiments instructed setsimilarity measures obtained total 20 pairs concepts evaluated 17 human subjects.dataset described Section 5.1.407fiALBACETE, CALLE, CASTRO & CUADRASpecifically, individual influence dimension similarity tested thorough setexperiments involving separately. Since combination them,need training either. Figure 7 shows box plot represents error measures producedindividually dimension.100Error (%)806040200SortCompositionalEssentialRestrictiveDescriptiveFigure 7: Performance isolated dimensions OntologyFigure 8 shows series twenty pairs, every dimension produced better predictionothers least once. fact, essential dimension provided best response almost halfcases, descriptive dimension best one case.Restrictive10%Descriptive5%Sort30%Compositional15%Essential40%Figure 8: Cases dimension ranked firstfact lead conclusion essential design appropriate,descriptive dimension weak. analysis found latter lacked sufficientknowledge, improved line evaluation (more knowledge added).Despite improvement, since analysis introduction knowledge performedmanually (in contrast dimensions, knowledge obtained WordNet),could still enhanced would improve individual results dimension. Besides,result definitive, since weights may different interaction domains,volume knowledge base important too. useful consequence one408fiSEMANTIC SIMILARITY MEASURES APPLIED ONTOLOGYfive ontological dimensions contribute similarity function, supportinghypothesis adequate combination may yield better resultsindividual approaches.4.3 Weights Training MethodsAssigning proper weight dimension crucial achieving good results. Sincehuman test subject usually give relevance five dimensions similarity,basic training program regarding weights associated dimension developed.program based reinforcement learning technique (specifically variant Q learningalgorithm) implemented order determine, several iterations,appropriate value weights applied dimension (previously defined Section 4.1)minimize error formula result human judgment. Therefore, inputtraining algorithm set similarity judgments made human test subjects.algorithm follows next steps:a) initial step, five weights w1, w2, w3, w4 w5 applied dimension(see formula Section 4.1) initialized 1.b) iteration training algorithm, results dimension similaritycalculated according formulas described Sections 4.1.1 4.1.5.Subsequently, five new weights calculated according next criteria:1.2.3. Failure meet conditions 1) 2),parameter ranged 1 5 (one dimension),represents individual score represents similarity value 0 10 onepair concepts scored one participant.stands increaseweight (for dimension i) current iteration,represents increaseprevious iteration. max(Simi) min(Simi) represent maximumminimum similarity individual values, respectively. Finally, stands learning rate.training focused different points view, tested evaluated.Firstly, pair-oriented training implemented order individually adjust weights20 concept pairs, independently specific user. weights adjustedindividually pairs concepts, taking one user per iteration. way,iteration, new array refined weights obtained used evaluating similarity.test consists calculating similarity (with array weights) comparinghuman assessment.Since degree significance assigned dimension may depend subjectivitytesters, particular interest make adjustment weights based user.409fiALBACETE, CALLE, CASTRO & CUADRAexperiment, training weights performed user consisted20 iterations (one pair concepts). iteration training algorithm, absoluteerror committed relation corresponding pair calculated. running training17 users, average absolute errors iterations calculated.third method designed order address shortcomings pair-orientedtraining. indicated storing array weights possible pairconcepts medium sized ontology requires unusually extensive physical resources. Besides,significant coverage thus defined knowledge would require far much training. short,realistic develop method high number combinations concepts.However, preliminary experimentation checked weights applied pairalso likely applied combinations two concepts. Therefore,new training method (feature-oriented) proposed slightly modifying pair-oriented one.feature-oriented method, array weights stored concept insteadpair concepts (which solve problems storage extent training). timeone concept compared other, array weights reviewed refined.similarity calculation given pair based aggregation arrays concepts.Finally, observed method showed different behavior depending pairconcepts compared: method achieving worst results average also bestspecific pairs. Subsequently, hybrid method proposed developed,combining feature-oriented user-oriented trainings, aiming profit advantagesmethod. training similar focused user, iterationarray weights refined different degree, taking account array storedparticular concept. Therefore, particular dimension usually relevant concept,adaptation user dimension strengthened.5. Evaluationconceptual model ontology defined, weights training methodsproposed, next step study evaluate proposal. present section describesexperiments run evaluating proposal, design results obtaineddiscussion. knowledge base supported relational database management systemOracle 11g, logic ontology component (including inference mechanisms)implemented Java. knowledge bases designed satisfy specific purposes withinresearch project. initial knowledge load obtained large lexical databaseWordNet (Fellbaum, 1998) including existing concepts (synsets), terms relationships(corresponding sort compositional dimensions). Since proposed ontological modeldefines relationships concepts (essential, restrictive descriptive), necessaryadd knowledge. Cognos.Onto tool enables knowledge edition managementspecific model. tool belongs larger toolkit, Cognos (Calle et al., 2011) already usedseveral research projects. toolkit seeks ease interaction corpus analysis, annotation,implementation management, diverse yet integrated tools aimed specific typeknowledge (pragmatic, NLP related, ontological etc.).410fiSEMANTIC SIMILARITY MEASURES APPLIED ONTOLOGY5.1 Experimental Design PreparationFirst all, necessary choose Interaction Domain define entireexperiment. concepts involved subset whole knowledge base, restrictedspecific domain. participants chosen order constitute good coveragefocused domain. Finally, additional knowledge fed experts interaction domainrelated projects research framed (as test subjectsparticipant experiments).methodology chosen evaluate proposed similarity measure based Millersbenchmark (Miller & Charles, 1991). Experiments designed determine whetherresult attained application similarity function pair concepts reliableor, words, result falls within acceptable range compared similarityjudgments made human test subjects.begin experimental phase study, initial loading concepts must first madeproposed ontology. reason, WordNets synsets (Princeton Univ., 2011) takenconcepts, together corresponding semiotics, sort compositional relationships.Knowledge domain experts responsible populating remaining dimensionsontological model (i.e., essential, restrictive descriptive) subset 350 concepts,selected relevance interaction domain.chosen domain labeled computer science teaching interaction domain withinSpanish academic socio-cultural environment. area knowledge familiar testsubjects selected heterogeneous domain (different roles, ages,genders). perform evaluation, test designed test subject ratesimilarity pairs concepts. set pairs meet basic criterion: least twopairs included explore proposed dimensions, one clear incidencedimension another one without (or little impact).total number twenty-one test subjects available, four outliers leftapart. discarded checking judgment responsesuniform rest sample. participant scores follow normal distributionremoving outliers. reason, sample size calculated test statisticalsignificance result least ten subjects ensure 99% confidence. Therefore,sample size seventeen participants sufficient ensure data representative.Theseventeen subjects experts interaction domain (technical education), specificallyfive technical students, seven researchers five lecturers. ages ranged 20 50distributed follows: seven subjects 20-30 year-old range, six 30-40year-old range remaining four 40-50 year-old range. regard gender,slightly half female (9) rest male (8). chosen interactiondomain applied research project THUBAN (TIN2008-02711). participantprovided test containing set twenty pairs concepts domain. Sinceobservations follow normal distribution, determined minimum significant samplesize would sixteen 99% confidence. Therefore, set twenty pairs concepts providessignificant results. However, larger domain, size dataset may different attain411fiALBACETE, CALLE, CASTRO & CUADRAstatistically significant results. coherence components systemproposal integrated, similarity measures ranged zero (no similarity) ten(absolutely identical, concept). addition, pairs, subjects askedjustify score, indicating specific parameters similarity took accountmaking decision.After obtaining individual survey results, average total humanassessments pair concepts calculated.Table 1 shows 20 pairs conceptsincluded test right pair, range (difference maximumminimum scores), standard deviation average rating assigned users.Pair IDPair conceptsRangeStandarddeviationAveragesimilarity0Reading lamp Personal computer61.762.7112345678910111213141516171819Laptop Server computerTeacher TutorialMeeting room LaboratoryServer computer MicrowaveOffice LaboratoryScreen BlackboardStapler FolderPlug Power stripOffice Meeting roomPencil CD markerAssociate professor Teaching AssistantAssociate professor Bachelorwrite papers programgive lecture teachKeyboard MouseFridge MicrowaveHard disk drive PendriveScanner PrinterPoster Blackboard67889774635876573861.621.922.152.022.251.832.191.211.690.991.342.532.151.601.411.770.941.891.826.475.064.352.245.766.123.948.296.297.298.065.184.537.767.355.358.475.944.24Table 1: Pairs concepts average similaritymethods subject iteration order (either analyzed pair human judge),alter result training. order avoid effect endow significanceresults, preliminary experiments minimum number repetitions (with differentorder) determined reduce stochastic gain significance (close 275), consequentlydecided program 300 repetitions different order method. graphstables, error rates pairs (identified pair_id) numbered 0 19, iterationsnumbered 1 20.5.2 Experimentssection presents results obtained execution experiments correspondingfour weight adjustment algorithms described Section 4.3. experiments412fiSEMANTIC SIMILARITY MEASURES APPLIED ONTOLOGYperformed subset ontological knowledge stored acquired computer scienceteaching domain. first experiment performed pair-oriented training and, orderevaluate results training, average absolute error calculated (for pair)similarity based human judgment result obtained applyingsimilarity measure proposed according following formula:corresponds index iterate human judge specific pairconcepts n number test subjects. Finally, errorpairId represents absolute errorhuman judgment pair result obtained training algorithmiteration. Table 2 shows absolute errors calculated experiment pairconcepts, well average error which, 18.5% comes slightly closer scoresprovided human subjects.Pair Id012345678910111213141516171819AVGerror (%) 15.2 14.8 38.3 18.6 19.4 18.1 17.6 18.8 20.2 15.4 13.4 18.0 22.5 19.6 15.2 13.0 15.3 20.9 17.1 19.0 18.5Table 2: Pair-oriented training error ratenoted eleven cases, error rate less average, eight caseserror rate around average, one pair (#2) shows excessive error rate requiresanalysis discussion (see subsection 5.3). Figure 9 shows comparison trendlines regarding error rate accumulated pair-oriented training algorithmaccumulated error similarity function without weights training.Figure 9: Accumulated average error pair-oriented trainingsecond place, absolute error obtained pair feature-oriented trainingshown Table 3. results, compared obtained pair-oriented training,show slightly worse performance (with mean error rate 20,2%). However,recalled method advantages (realistic storage training extent).413fiALBACETE, CALLE, CASTRO & CUADRAPair Id012345678910111213141516171819AVGerror (%) 15.0 14.9 38.2 18.4 30.3 22.7 17.5 18.5 20.1 21.6 13.4 24.9 21.2 19.2 15.2 13.0 14.4 20.6 16.8 25.4 20.2Table 3: Feature-oriented training error ratethird experiment executed user-oriented training. order evaluate resultsexperiment, average absolute error calculated (for human judge)similarity based human judgment 20 pairs concepts resultobtained applying similarity measure proposed. way, error averagecalculated follows:corresponds index iterate pair concepts specific user, nnumber pairs concepts errorpairId represents absolute error humanjudgment pair result training algorithm iteration.case, average error rate achieved 23.9%, even worse featureoriented training. absolute error rate obtained iteration shown Table 4.Pair Id012345678910111213141516171819AVGerror (%) 18.6 14.1 40.7 17.9 30.8 16.8 22.9 17.5 37.1 17.1 34.6 35.8 24.3 21.5 31.2 13.6 13.9 27.4 22.3 20.8 23.9Table 4: User-oriented training error rateFigure 10 shows comparison trend lines correspondent error rate accumulateduser-oriented training algorithm accumulated error without weight training.observed, user-oriented training trend line follows downward curve 20iterations reaches error rate 23.9%. Comparing trend lines, concludedtraining decreases accumulated error adapts calculated similarities subjectsjudgments, yet would desirable improve adaptation (since still far featureoriented training).Figure 10: Accumulated average error user-oriented training414fiSEMANTIC SIMILARITY MEASURES APPLIED ONTOLOGYobserved, user-oriented feature-oriented training methods ableimprove similarities calculation, becoming noteworthy approaches. Consequently,found interest explore method combines them. new hybrid methoddeparts user-oriented approach, takes account weights vector obtainedfeature-oriented training described section 4.3. shown Table 5, user error ratesuccessfully reduced 21.2% respect user-oriented training. However,method degrades performance achieved feature alone method.Pair Id012345678910111213141516171819AVGerror (%) 16.0 14.3 39.0 16.9 29.4 17.3 22.2 17.4 25.1 18.5 21.5 29.6 22.0 19.9 22.8 13.2 13.1 23.8 19.2 22.8 21.2Table 5: User-feature hybrid training error rate5.3 Discussion Results ObtainedAmong results, concept pair 2 (teacher-tutorial) scored error rate 38%average similarity assigned users (see Table 1) 5.06. latter value significantly highconsidering fact first concept refers person second static entity.Reviewing participant responses question, however, understood test subjectsgave higher score sole feature concepts common, activity teaching.Analyzing results outlier, appears algorithm tendency graduallyincrease weight restrictive dimension, longer training necessary adaptweight vector relevant dimension restrictive one. Using training algorithmfaster convergence would ensure good result pair, could adversely affectresults. However, convergence guaranteed larger number users.Figure 11 shows comparison absolute error obtained four experimentsperformed work (pair-oriented, user-oriented, feature-oriented hybrid trainings)pair, also average results method. first experiment performed, pairoriented training, achieves best average error rate, 18.5%, although pairmentioned error exceeded 38%. However, experiment major limitation:trained weight vector pair concepts possible cannot stored due large numbercombinations existing concepts ontology. shortcoming mitigateddevelopment feature-oriented training, achieving error rate 20.2%, figureslightly worse pair-oriented training error. Nevertheless, resultfully reflect impact training test pairs include concepts appearexperiment. calculation average error restricted pairsconcepts repeated one pair, error amounts 22.8%.case, experiment important advantage since implementation realisticapplied large ontologies.user-oriented training aimed adapting weights subject orderconfirm assumption every test subject assigns value dimensions.Although error rate achieved (23.9%) satisfactory either pair feature415fiALBACETE, CALLE, CASTRO & CUADRAoriented trainings, figure included sub-section 5.2 training shows decreasingtrend line which, compared trend line without training, allows conclusionuser-oriented experiment able adapt individual judgment. reason,improvement attempted user-training result combinationfeature-oriented experiment.Figure 11: Comparison experiment resultshybrid training detailed Section 5.2 achieved 21.2% error rate, reducesuser-oriented training, balances performance user-oriented method(reduces standard deviation). Taking account feature-oriented training method dependsexperience features knowledge base might lack experience,response obtained could satisfactory cases. fact, calculating errorproduced feature-oriented method dataset (not restricted repeated pairs)result amounted 22.8%. sum, feature-oriented method provides better resultsenough knowledge available. last results presented Figure 11 concern experimentobserving sort dimension (which frequent method calculating similarities).average error rate 24.1%, higher four methods discussed.addition, observed error rate experiment is, several cases, faraverage error. Figure 12 shows boxplot comparing performance four training methodsproposed sort dimension formula.416fiSEMANTIC SIMILARITY MEASURES APPLIED ONTOLOGY100Error (%)806040200PairFeatureUserHybridSortFigure 12: Performance training methodseen, regarding error predictions, sort dimension obtains highermaximum (although also lower minimum), higher median (except user training) higherdeviation rest. graph, concluded error rate achievedsort dimension method (used previous studies similarity) greater error rateachieved feature training method. order check statistically, null hypothesisformulated (the average error methods) also alternative hypothesis, (theaverage error feature-oriented method lower sort dimension method error).measure discrepancy calculated sample twenty measures error (one perpair) result (-1.78) found outside acceptance range (-1.64, +), thereforenull hypothesis rejected alternative accepted significance level 0.05.Consequently, considered true error shown feature-oriented method lowererror produced sort dimension method.Finally, Figure 13 shows average final weights four experiments. showsrelevance taken experiments dimension, yet cannot extrapolatedinteraction domains. dependent set pairs chosen experiment, resultsshow five dimensions taken account, diverse weights.Descriptive14%Sort 26%Restrictive12%Compositional22%Essential 26%Figure 13: Average weights ontological dimensions417fiALBACETE, CALLE, CASTRO & CUADRA6. Conclusions Perspective Future Researchpaper defines similarity measure multi-dimensional knowledge model ontologytype, specifically ontology aimed supporting Human-Like Interaction. proposedmeasure based five dimensions ontological knowledge: sort, compositional, essential,restrictive descriptive. five weighted aggregated order obtainglobal similarity measure. equations applied dimension general usedontologies observe dimensions, yet observingaggregating similarity result proposed enhanced accuracy.solution presents another challenge, form weights calculation. fact,person decides similarity concepts unwittingly makes dimensionsprevail others. criteria may diverse, work focused studyingdependence weights nature concepts, either pairs (pair training method)individually (feature training method), described Section 4.3. work also exploresinfluence past behavior users perform concept pair evaluations (andultimately, user owns device usually interacts it). Following line, userdependent training proposed, finally hybrid one (merging feature user benefits)included too. evaluated compared order ascertain oneperforms better, obtaining best results pair-oriented training.order evaluate performance proposed similarity measure, resultsrecorded compared taken human test subjects. evaluation techniqueapplied several studies similarity measures considered gold standard.experimental phase, four training algorithms developed according differentperspectives. Thus, phase included pair-oriented, feature-oriented, user-orientedhybrid experiment. every case, error rate calculated respect human subjectassessments. best results corresponded pair-oriented method achieved errorrate 18.5%. Since implementation experiment realistic large ontologies,feature-oriented experiment required despite slightly worsening results previousexperiment, concretely, producing error rate 20.2%. However, feature-orientedexperiment big advantage able applied easily large ontologies.Moreover, user-oriented training aimed adapt weights subject orderconfirm assumption every test subject assigns value dimensions.experiment highest error rate algorithms (23.9%), demonstrated,error rate follows decreasing trend line while, training done, error rate followsasymptotic tendency. addition this, experiment shows slightly better resultstaking account sort dimension (which average error rate 24.1%maximum 60.4%). reason, concluded user-oriented experiment ableadapt individual judgment (although adaptation slow). Finally, hybridexperiment combines feature-oriented user-oriented training and, error rate21.2%, nevertheless manages reduce error user-oriented training, wellbalancing error atypical cases common rest experiments.418fiSEMANTIC SIMILARITY MEASURES APPLIED ONTOLOGYSince hybrid experiment manages balance results experiments,currently, improved hybrid algorithm developed. algorithm calculationweights iteration affected depending error produced featureexperiment pair concepts corresponding iteration.performance training methods proposed closely related available extentknowledge. reason, authors also currently working mechanisms increasingquality completeness ontological knowledge. manual acquisition newknowledge expert requires great deal resources would desirable developadvanced mechanism learn new concepts relations. challenge attainknowledge acquisition human-like interaction human subjects. Therefore,lifetime system, knowledge bases would enriched interacting users.Finally, refinement similarities formulation also interesting line work, especiallysemiotic dimension reintroducing influence global similarity calculation.Acknowledgmentsdevelopment approach construction part LaBDA-Interactor HumanLike Interaction System, part research projects SemAnts (TSI-020110-2009-419)THUBAN (TIN2008-02711) CADOOH (TSI-020302-2011-21), supported SpanishMinistry Industry, Tourism Commerce Spanish Ministry Education,respectively. Besides, knowledge bases populated using COGNOS toolkit developedresearch project MA2VICMR (S2009/TIC-1542) supported RegionalGovernment Madrid.ReferencesAltintas, E., Karsligil, E., & Coskun, V. (2005). new semantic similarity measure evaluatedword sense disambiguation. Procs. 15th NODALIDA conference. Joensuu.Bernstein, A., Kaufmann, E., Kiefer, C., & Brki, C. (2005). SimPack: Generic Java LibrarySimilarity Measures Ontologies. Zurich: Technical report.Budanitsky, A. (1999). Lexical semantic relatedness application natural languageprocessing. University Toronto. Technical report.Calle, F. (2004). Interaccin Natural mediante procesamiento intencional: Modelo de Hilos endilogos. Thesis, (PhD). Politecnic University Madrid.Calle, F. J., Albacete, E., Snchez, E., del Valle, D., Rivero, J., & Cuadra, D. (2009). Cognos:Natural Interaction Knowledge Management Toolkit. International ConferenceApplications Natural Language Information Systems (NLDB 2009) (pp. 303-304).Saarbrken, Germany: Lecture Notes Computer Science.Calle, F., Castro, E., & Cuadra, D. (2008). Ontological dimensions applied Natural Interaction.Procs. First International Workshop Ontologies Interactive Systems , 91-96 .419fiALBACETE, CALLE, CASTRO & CUADRADepartment Genetics, Stanford University School Medicine, California, USA. (2000). Geneontology: tool unification biology. Gene Ontology Consortium. Naturegenetics Vol. 25, No. 1. , 25-29.Fellbaum, C. (1998). WordNet: Electronic Lexical Database. Cambridge, UK: MIT Press.Gee, J.P. (1999). Introduction Discourse Analysis. Routledge.Inkpen, D. (2007). Semantic similarity knowledge applications. STUDIA UNIV. BABESBOLYAI, INFORMATICA, Volume LII, , 11-22.Jiang, J. J., & Conrath, D. W. (1997). Semantic Similarity Based Corpus Statistics LexicalTaxonomy. International Conference Research Computational Linguistics. Taiwan.COGNOS Toolkit. (2011). Retrieved July 2011,http://labda.inf.uc3m.es/doku.php?id=es:labda_lineas:cognosRiTa.WordNet: WordNet library Java/Processing. (2008). [Online]. Available:http://www.rednoise.org/rita/wordnet/documentation/Leacock, C., & Chodorow, M. (1998). Combining Local Context WordNet SimilarityWord Sense Identification. Electronic Lexical Database , 265-283.Levenshtein, V. I. (1966). Binary codes capable correcting deletions, insertions reversals.Soviet Physics Doklady vol 10 , 707-710.Lin, D. (1998). Information-Theoretic Definition Similarity. Proceedings 15thInternational Conf. Machine Learning, (pp. 296-304). Madison, Wisconsin USA.Lord, P. W., Stevens, R. D., Brass, A., & Goble, C. A. (2003). Investigating semantic similaritymeasures across Gene Ontology: relationship sequence annotation.Bioinformatics , 1275-1283.Miller, G. A., & Charles, W. G. (1991). Contextual correlates semantic similarity. LanguageCognitive Processes , 1-28.Miller, G. A. (1995). WordNet: Lexical Database English. Communications ACM vol38 ,No. 11: 39-41.Pedersen, T., Patwardhan, S., & Michelizzi, J. (2004). WordNet:: Similarity measuringrelatedness concepts. Demonstration Papers HLT-NAACL 2004 (pp. 38-41). Boston,Massachusetts, USA: Association Computational LinguisticsPrinceton Univ. (February 3, 2011). WordNet: lexical database English. Obtenido deWordNet: lexical database English: http://wordnet.princeton.edu/Rada, R., Mili, H., Bicknell, E., & Blettner, M. (1989). Development application metricsemantic nets. IEEE Trans. Systems, Man, Cybernetics. 19. , 17-30.Resnik, P. (1999). Semantic Similarity Taxonomy: Information-Based MeasureApplication Problems Ambiguity Natural Language. Journal ArtificialIntelligence Research , 95-130.Resnik, P. (1995). Using information content evaluate semantic similarity taxonomy.IJCAI'95 Proceedings 14th international joint conference Artificial intelligence(pp 448-453). San Francisco, USA: Morgan Kaufmann Publishers Inc.420fiSEMANTIC SIMILARITY MEASURES APPLIED ONTOLOGYRichardson, R., Smeaton, A. F., & Murphy, J. (1994). Using WordNet Knowledge BaseMeasuring Semantic Similarity Words. Proceedings AICS Conference.Dublin, Ireland: Technical Report.Schickel-Zuber, V. (2007). OSS: semantic similarity function based hierarchical ontologies.IJCAI'07 Proceedings 20th international joint conference Artifical intelligence(pp. 551-556). San Francisco, CA, USA: Morgan Kaufmann Publishers Inc.Seco, N., Veale, T., & Hayes, J. (2004). Intrinsic Information Content Metric SemanticSimilarity WordNet. ECAI'2004, 16th European Conference ArtificialIntelligence, (pp. 1089-1090). Valencia, Spain .Wu, Z., & Palmer, M. (1994). Verb semantics lexical selection. ACL'94 Proceedings32nd annual meeting Association Computational Linguistics (pp. 133-138).Stroudsburg, USA: Association Computational Linguistics.421fiJournal Artificial Intelligence Research 44 (2012) 141-177Submitted 11/11; published 05/12Algorithms Limits Compact Plan RepresentationsChrister BackstromPeter Jonssonchrister.backstrom@liu.sepeter.jonsson@liu.seDepartment Computer ScienceLinkoping UniversitySE-581 83 Linkoping, SwedenAbstractCompact representations objects common concept computer science. Automated planning viewed case concept: planning instance compactimplicit representation graph problem find path (a plan) graph.graphs represented compactly planning instances, pathsusually represented explicitly sequences actions. cases knownplans always compact representations, example, using macros. showresults extend general case, proving number bounds compactrepresentations plans various criteria, like efficient sequential random accessactions. addition this, show results consequencesgained reformulating planning problem. contrastalso prove number positive results, demonstrating restricted cases plansuseful compact representations, well proving macro plans favourable accessproperties. results finally discussed relation relevant contexts.1. Introductionusage study representations objects much smaller objectscommonplace computer science. us encounter representationsdaily basis form zipped files, mp3 files etc. practical cases, usuallytalk compressed objects, terms compact succinct commontheoretical studies. meaning terms vary common interesting casesize representation polylogarithmic size object.Sometimes sufficient compute compact representation object, instance,archiving file. cases representation must also support various operationsefficiently without first unpacking object explicit representation. Performingoperations compact representation often harder performing operationexplicit object, cases compact representation make easieremphasising inherent structure object.One archetypical case using compact representations automated planning, althoughseldom viewed way. planning instance implicit representation graphtypically exponentially larger representation, instance,solutions, plans, paths graph. Consider, example, Strips instancen variables. variables implicitly define state space 2n states actionpreconditions define 2nm arcs graph. Similarly, define instancespaths exponential length too. Although planning instances alreadyc2012AI Access Foundation. rights reserved.fiBackstrom & Jonssoncompact representations, little attention paid compact representationssolutions, usually represented explicitly. paper introduces analysesnumber compact representations.first turn computer science general find compact representationsarbitrary strings intensively studied field. example, Charikar et al. (2005)Rytter (2003) address problem approximating smallest string representation usingcompressed grammar. Bille et al. (2011) show representations permit efficientaccess matching operations, Jansson, Sadakane, Sung (2012) demonstraterepresentations efficient edit operations. structured objects arbitrary stringspotentially compact representations. following examples,displaying positive well negative results various areas. Galperin Wigderson(1983) Wagner (1986) study complexity common graph operationsgraphs implicitly represented circuits tell whether two vertices connected.Balcazar (1996) uses variant approach study complexity search AI,using circuit generates adjacency list vertex. Bulatov Dalmau (2006)present efficient algorithm certain CSP problems relies using compactrepresentation set solutions. Liberatore Schaerf (2010) study preprocessingmodel checking focus size preprocessed parts. Cadoli et al. (2000) studyvarious formalisms knowledge representation study problems modelled oneformalism transformed another formalism polynomially largerrepresentation.One approach compact representations various areas use macros.concept widely used long time also planning, although seldompurpose providing compact representations. exception following case. 3Sclass (Jonsson & Backstrom, 1998b) planning instances property optimalplans exponential length always possible decide polynomial timeplan not. Gimenez Jonsson (2008) showed plans 3S classalways polynomial-size representation using macros, macro plans evengenerated polynomial time. is, although plan may exponential length,thus necessarily take exponential time generate, possible generate compactrepresentation polynomial time. Jonsson (2009) later demonstrated similar resultsnumber classes. Although particular classes planning instances maystill restricted much practical use, principle compressing solutionusing macros interesting tool planning plan explanation.approaches compact plan representation appear sparingly literature.notable exception Liberatore (2005a) studies two concepts plan representationefficient random access efficient sequential access respectively. like macroplans examples representing one long plan compactly. might also considerrepresenting large set plans compactly. instance, plan recognition maysimultaneously consider exponential number candidate plans (Geib, 2004). Althoughseldom viewed way, also reactive plan representation large set plans,one state goal reached. is, however, known reactiveplans cannot compact, efficient correct general case (Jonsson, Haslum,& Backstrom, 2000), although properties important, instance, spaceshipapplications (Williams & Pandurang Nayak, 1997). Pomdps may similarly thought142fiAlgorithms Limits Compact Plan Representationsprobabilistic variant reactive plans compactness representations importantalso case (Boutilier & Poole, 1996). Yet another case size planbig plan necessarily long, occur various types branchingplans, contingent planning (Bonet & Geffner, 2000). three different conceptsisolated other. instance, Bonet (2010) casts contingent planningproblem conformant planning, is, branching plan represented onelong non-branching plan, branches appearing subplans. cases,interesting know objects question compact representations. Althoughcompact representation save space, may secondary many cases.important aspect object compact representation objectinherent structure may exploit also purposes. instance, representset many plans representation using recursive macros, similar, emphasizedifferences similarities plans. make comparisonsoperations plans efficient. Similarly, case branching plans mightwant exploit structure clearly displays two branches commondiffer.positive results macro representations (Gimenez & Jonsson, 2008; Jonsson, 2009)prompt obvious question whether long plans always compressed using macros(or method). show paper unlikely, matter typecompact representation try use (macro plans, finite automata whatever).remainder paper organized follows. Section 2 introduces basic notationconcepts well planning framework used paper, also containsuseful definitions complexity results. first ask, Section 3, whether(optimal) plans instance compact representations. find answerno; possible, neither macros method. However, resultsexclude plans instance compact solutions. Section 4thus restrict question whether uniform compact representation one plansolvable instance. precisely, ask algorithm correspondsone compact representation solvable instance. show algorithmunlikely exist must also able access actions plan useful way.Section 5 turn non-uniform case, asking solvable instance leastone plan compact representation. primarily consider representationsefficiently access actions plan sequentially randomly. show alsoseems unlikely general case, interesting special casesrepresentations exist. section also investigate macro representationsextend results Gimenez Jonsson two ways. prove planspolynomial-size macro representation random accessed polynomialtime without access full plan. However, also prove cannot alwaysrepresent plans compactly using macros. Section 6 analyse whether get aroundproblem long plans reformulating planning problem. Alsoanswered negatively. actually ask plan original problem, probleminherently intractable also using reformulation. However, even consideringdecision problem still seems possible make planning simpler reformulation.Finally, Section 7 contains discussion results paper relatedrelevant various topics like adding information guide planners, causal graphs143fiBackstrom & Jonssonplan explanation. paper ends summary results together listopen questions.results paper appeared previous conference publication(Backstrom & Jonsson, 2011b).2. Preliminariessection consists three parts. first part introduces general notationterminology used paper. second part defines two planning frameworks usedpaper, Finite Functional Planning propositional Strips, presentsconstructions frequently used. third part briefly recapitulates conceptadvice-taking Turing machines also defines 3SAT problem usedseveral occasions paper.2.1 General Notation Terminologysequence objects x1 , x2 , . . . , xn written hx1 , x2 , . . . , xn i, hi denoting emptysequence. Given set X objects, set sequences X, including hi, denotedX . set, sequence aggregation X objects, write |X| denotecardinality (the number objects) x write ||X|| denote size (the numberbits representation) x. composition two functions f g denotedf g defined (f g)(x) = f (g(x)).negation propositional atom x denoted x. literal either atomnegation set L(X) literals set X atoms defined L(X) ={x, x | x X}. Negation extended literals ` literal `. Negationalso extended sets X set literals X = {` | ` X}. Letsubset L(X) set X atoms. P os(Y ) = {x X | x } setatoms appear positive , N eg(Y ) = {x X | x } set atomsappear negated Atoms(Y ) = P os(Y )N eg(Y ). set consistent P os(Y )N eg(Y ) empty set Z atoms satisfies P os(Y ) Z N eg(Y )Z = .update operator n binary function given set X atoms setliterals, X n set atoms defined X n = (X N eg(Y )) P os(Y ).2.2 Planningpositive results compact representations, want results apply generalpowerful planning languages possible, results hold also languagesrestricted. Hence, use Finite Functional Planning formalism (Backstrom &Jonsson, 2011a), makes minimum assumption language, exceptground language state variables finite domains.Definition 1. Finite Functional Planning (FFP) frame tuple hV, D, Ai Vimplicitly ordered set variables, : V N domain function maps every variablefinite subset natural numbers set actions. frame implicitlydefines state space S(f ) = D(v1 ) . . . D(vn ), v1 , . . . , vn variablesV order. members S(f ) referred states. action twoassociated total functions, precondition pre(a) : S(f ) {0, 1} postcondition144fiAlgorithms Limits Compact Plan Representationspost(a) : S(f ) S(f ). pairs states s, S(f ) actions A,1) pre(a)(s) = 12) = post(a)(s).sequence = ha1 , . . . , a` plan state s0 S(f ) state s` S(f )either1) = hi s0 = s`2) states s1 , . . . , s`1 S(f ) ai si1 si (for 1 `).FFP instance tuple p = hV, D, A, I, Gi f = hV, D, Ai FFP frame,S(f ) state G : S(f ) {0, 1} total function. state S(f ) goalstate p G(s) = 1. goal G reachable state S(f ) plangoal state p. solution p plan goal state S(f ).solution p called plan p.complexity computing pre- postconditions actions goalfunction referred step complexity. paper, consider subclassFFP([P]) consists FFP frames instances polynomial step complexity.occasionally also consider restrictions FFP([P]) use notation FFP(p)class FFP frames f (and instances p) action pre- postconditions(and G) computed p(||f ||) time (and p(||p||) time), p polynomial.furthermore say FFP([P]) instance p = hV, D, A, I, Gi deterministicS(p) p plan s, onepre(a)(s) = 1. is, instance deterministic planner never facedchoice two actions.proving compact representation exist, result gets strongeruse weaker formalism. is, want use restricted formalism possible,since results automatically apply formalisms expressive.Hence, use propositional Strips results. number commonvariants propositional Strips known equivalentSAS+ formalism strong form polynomial reduction (Backstrom, 1995).refer Strips paper variant called propositional Stripsnegative goals (PSN) Backstrom. defined special case FFP([P])uses binary variables, define traditional way, treating variablespropositional atoms.Definition 2. Strips frame tuple f = hV, Ai V set propositional atomsset actions. state space defined S(f ) = 2V states subsetsV . action precondition pre(a) postcondition post(a),consistent sets literals V . pairs states s, S(f ) actions A,1) satisfies pre(a)2) = n post(a).sequence = ha1 , . . . , a` plan state s0 S(f ) state s` S(f )either1) = hi s0 = s`2) states s1 , . . . , s`1 S(f ) ai si1 si (for 1 `).145fiBackstrom & JonssonStrips instance tuple p = hV, A, I, Gi f = hV, Ai Strips frame,state S(f ) G consistent set literals V . state S(f ) goal statep satisfies G. goal G reachable state S(f ) plangoal state p. solution p plan goal state S(f ).solution p called plan p.notation : X frequently used define action precondition Xpostcondition .negative results proven hold Strips. However, cases resultshold even many restricted subclasses Strips. would lead far surveycases paper use restriction unary actions archetypical casethroughout paper.Definition 3. Strips action unary |post(a)| = 1, set Strips actions unaryactions unary Strips frame instance unary action set unary.Unary actions may seem like limiting restriction demonstratedsufficient many cases use on-board controllers spacecrafts (Muscettola et al.,1998; Brafman & Domshlak, 2003). surprising, though, since Strips planningPSPACE-complete remains even restricted unary actions (Bylander, 1994).Given Strips instance always possible construct corresponding Strips instanceunary. following reduction unary instances simplified Strips versionreduction used SAS + (Backstrom, 1992, proof Theorem 6.7).Construction 4. Let p = hV, A, I, Gi Strips instance. Construct correspondinginstance p 0 = hV 0 , A0 , 0 , G0 follows. Define Vlock = {vlock| A}. let V 0 =000V Vlock , = G = G Vlock . Define A, containsfollowing actions:},abegin : pre(a) Vlock {vlock},aend : post(a) {vlock } {vlock} {` }, ` post(a).ai : {vlockleave without proof construction polynomial reduction classStrips instances class unary Strips instances. furthermore worth notingconstruction easily modified use padding redundant variablesmake original actions correspond number actions unary instance.Hence, possible make reduction plans unary instanceconstant factor longer corresponding plans original instance.also make frequent use Strips instances include encodings binarycounters based following construction, uses one action bitincrement non-negative integer encoded binary.Construction 5. n-bit binary counter encoded Strips follows: let V ={x1 , . . . , xn } let contain n actionsai : {xi , xi1 , . . . , x1 } {xi , xi1 , . . . , x1 } (1 n).146fiAlgorithms Limits Compact Plan Representationsfollowing plan counting 0 16 using 5-bit counter according Construction 5:ha1 , a2 , a1 , a3 , a1 , a2 , a1 , a4 , a1 , a2 , a1 , a3 , a1 , a2 , a1 , a5 i.could modify binary counter use unary actions described Construction 4, direct way get unary actions count Gray code.Construction 6. (Backstrom & Klein, 1991) n-bit Gray-code counter encodedStrips follows: let V = {x1 , . . . , xn } let contain 2n actionssi : {xi , xi1 , xi2 , . . . , x1 } {xi } (1 n),ri : {xi , xi1 , xi2 , . . . , x1 } {xi } (1 n).following plan counting 0 16 5-bit Gray-code counter accordingConstruction 6:hs1 , s2 , r1 , s3 , s1 , r2 , r1 , s4 , s1 , s2 , r1 , r3 , s1 , r2 , r1 , s5 i.2.3 Complexity Theoryuse abbreviation DTM deterministic Turing machine NTM nondeterministic Turing machine. addition standard types, also use advicetaking Turing machines deterministic nondeterministic type.advice-taking Turing machine associated sequence a1 , a2 , a3 , . . . advicestrings, special advice tape advice function a, natural numbersadvice sequence, a(n) = . input x advice tape immediately loadeda(||x||). continues normal way, except also accessadvice written advice tape. exists polynomial p ||a(n)|| p(n),n > 0, said use polynomial advice. complexity class P/polyset decision problems solved advice-taking DTM runspolynomial time using polynomial advice. extended that, instance,NP/poly defined NTMs run polynomial time using polynomial advice.Note advice depends size input, content. Furthermore,advice sequence must exist; need computable. following tworesults literature used later paper.Theorem 7. a) NP P/poly, polynomial hierarchy collapses (Karp & Lipton,1980, Theorem 6.1). b) Let k > 0 integer. pk pk /poly, polynomialhierarchy collapses level k + 2 (Yap, 1983, Lemma 7 combined Theorem 2).3SAT problem consists instances form C = {c1 , . . . , cm } ci ,1 m, called clause set exactly three literals universebinary variables. instance C satisfiable exists assignment truthvalues variables used C least one literal true ci C.otherwise unsatisfiable. Deciding satisfiability 3SAT NP-complete, decidingunsatisfiability coNP-complete. precisely, use following definition3SAT paper.147fiBackstrom & JonssonDefinition 8. integers n > 0, let Xn = {x1 , . . . , xn } set variables letm(n)m(n) number possible 3-literal clauses Xn . Let c1n , c2n , . . . , cnm(n)12fixed systematic enumeration clauses let Cn = {cn , cn , . . . , cn }. clausem(n) 1cin defines three literals that1 cin = {`1i , `2i , `3i }. Further, let Cn0 , Cn1 , . . . , Cn2fixed systematic enumeration subsets Cn , let n = hXn , Cn i, 0 <m(n)2m(n) . Also implicitly define set En = {e1n , e2n , . . . , en } atoms subsetsEni = {ejn | cjn Cni }, 0 < 2m(n) .m(n)1 systematic enumeration possible 3SAT instancessequence 0n , 1n , . . . , 2nn variables, hence equivalent usual definition 3SAT. Technically speaking,redundant encoding 3SAT since allows instances specify variablesused clauses. harmless, however; non-redundant instances remain,still hard instances neither redundantly encoded instancesharder non-redundant counterpart. Since m(n) 8n3 , enumerationsCn En chosen polynomial-time computable, assumeenumerations fixed on. also note set Eni uniquelyidentifies clause set Cni .3. Representing Arbitrary Plans Compactlyknown cases planning instances exponential-size plansplans always polynomial-size representation (Gimenez & Jonsson, 2008).obvious question thus whether plans, including exponential length,polynomial representations. one interpretation question answer trivially yes.Observation 9. set plans arbitrary FFP([P]) instance p O(||p||)size representation, since instance together deterministic planning algorithmsuccessively enumerates outputs plans representation.Although trivial useful observation highlights fundamental issues representations. Liberatore (2005a) discusses similar representation, insteadspecifying algorithm defines lexiographic ordering actions. Furthermore,adds plan index able represent single plan rather whole setplans. However, index unproblematic, see soon.interesting interpretation question whether every single planparticular instance polynomial representation. Even precisely,polynomial p every plan every planning instance representationsize O(p(n)), n size planning instance? investigate questionconsider simple compact notation possible, index number planparticular instance. Since instances may infinitely many plans, due cyclesstate-transition graph, consider optimal plans only. is, however, guaranteeeven index small enougha polynomial number bits may sufficientrepresent it.1. sometimes omit index n, assumed obvious context, thus write `ki rather`kn,i .148fiAlgorithms Limits Compact Plan RepresentationsConstruction 10. Given arbitrary integer n > 0, construct Strips instance p n =hVn , , , Gn Vn = {x1 , . . . , xn , y}, = , Gn = {x1 , . . . , xn } containsactionsai : {xi , xi1 , . . . , x1 } {xi , xi1 , . . . , x1 , y} (1 n)bi : {xi , xi1 , . . . , x1 } {xi , xi1 , . . . , x1 , y} (1 n).Lemma 11. every integer n > 0, instance pn according Construction 10 22optimal plans.n 1Proof. Let n > 0 arbitrary integer p n corresponding Strips instance accordingConstruction 10. instance binary counter variables x1 , . . . , xnConstruction 5, except extra variable independently settrue false, depending whether action type ai bi chosen. Since variablesx1 , . . . , xn interpreted binary number, let notation hm, yi representstate x1 , . . . , xn encodes number false, let hm, yi representcorresponding state true. Whenever state hm, yi hm, yi (where < 2n 1)possible go either hm + 1, yi hm + 1, yi using one action,states. state transition graph instance appears Figure 1. initial stateh0, yi goal states h2n 1, yi h2n 1, yi. Hence, plan p n mustlength 2n 1. every state goal state two different actionschoose lead different states. However, goal reachablenstates choice action matter. Hence, 22 1 differentplans p n .m=012n 1y=12y=0goalinitFigure 1: State-transition graph proof Lemma 11.Although atom redundant particular example whole construction couldpart larger instance, purpose. also notedinstances used proof optimal plans; plans length.set prove previous claim.Theorem 12. every integer n > 0, takes 2n 1 bits index optimal plansinstance pn according Construction 10.Proof. Since m-bit number distinguish 2m different objects,follows Lemma 11 least 2n 1 bits necessary index plans p n149fiBackstrom & Jonssonresult immediately implies (optimal) plans Strips instancepolynomial-size representations. holds even restrictions, like unary actions.basing Construction 10 Gray counter instead binary counter every actiontwo postconditions. Rewriting using Construction 4 yields equivalent instanceunary actions using block four actions action original instance.Although plans get 4 times longer number plans remain same. Hence,Theorem 12 still holds.theorem leaves possibility open plans instancepolynomial representations, although can. interesting question thusmany plans instance polynomial representations? answerquestion stray field information theory Kolmogorov complexity.scope paper treat field detail, loosely speaking, Kolmogorovcomplexity string size smallest DTM generate stringinput. Let K(x) denote Kolmogorov complexity binary string x. followinglemma due Buhrman et al. (2000, Lm. 1).Lemma 13. (Incompressibility lemma) Let c positive integer. Every set cardinalityleast m(1 2c ) + 1 elements x K(x) blog mc c.lemma used show fraction plans compactly representedapproaches zero size instances approaches infinity.Theorem 14. Let p arbitrary polynomial. Consider instances pn according Construction 10 arbitrary integers n > 0. Let t(n) total number planspn let s(n) number plans represented p(n) bits.limn s(n)t(n) = 0.nProof. Let p arbitrary polynomial. know Lemma 11 t(n) = 22 1 .every n > 0, let c(n) = 2n p(n) 2. incompressibility lemma saysleast t(n)(1 2c(n) ) + 1 plans K() blog t(n)c c(n).is, 2c(n) t(n) 1 plans K() < blog t(n)c c(n). Usingvalues t(n) c(n) above, simplifies say 2p(n)+1 1plans K() p(n). Hence, s(n) 2p(n)+1 1. theorem follows since2p(n)+1 10 limn s(n)= 0.t(n) limn 22n 1means even case plans every solvable instancecompact representations, probability particular plan compact representationvanishingly low large instances. Although strictly necessary useKolmogorov complexity prove Theorem 14 makes information-theoreticaspect compact representations clearer.4. Uniform Compact Representations Plansknow cannot, general, compress arbitrary exponential plans subexponential size. choose plan use? previousresult still leaves open possibility small fraction solutions planning instance could compact representations. However, planner (or oracle whatever)150fiAlgorithms Limits Compact Plan Representationswould choose us plan present us compact representationof. Suppose planner could actually this, would make use it? still needactual plan cannot avoid exponential size. Hence, interesting caseseems could least access useful information plan efficiently.term representation used loose sense here, need really preciselydefined moment. suffices note representation needs kinddata structure kind access algorithm, extreme cases eithervector data trivial access algorithm algorithm embeds data.could mean access compact representation efficiently? investigatetwo criteria. first one efficiently retrieve actions actualplan sequentially. interpretation efficient actions retrievedpolynomial delay (Johnson, Papadimitriou, & Yannakakis, 1988). second criterionaction actual represented plan random accessed polynomial time,size instance.looking explicit representations plan take look uniformcase, single representation covers instances. precisely,consider case single algorithm works compact representationplan every solvable instance.Theorem 15. algorithm solvable Strips instance p eithergenerate plan p sequentially polynomial delay random access actionplan p polynomial time, P = NP.Note theorem follow fact Strips planning PSPACEcomplete since solvable instances considered. proving theorem needintroduce extra technical machinery. start encoding 3SAT instances accordingDefinition 8 Strips follows.Construction 16. Let n > 0 i, 0 < 2m(n) , arbitrary integers. ConstructStrips instance p = hVn , , Eni , {goal}i Vn = Xn En {cts, ctu, goal, inc}{v0 , . . . , vm(n) } actions specified Table 1.previously noted, subset Eni En uniquely identifies 3SAT instance tellingclauses Cn enabled . is, initial state selects particular nvariable instance interested in. actions partitioned three groups. Groupcontains two actions acs acu, set atoms cts ctu respectively.actions block cts ctu initially false, one atomsset true plan. is, cts ctu mutually exclusive. Group II consistsactions require cts true group III consists actions require ctu true.two groups actions thus also mutually exclusive. Hence, every plan must startexactly one action group rest plan consists actionseither group II group III, depending first action is. intentionfollowing: plan starts action acs commits verifyingsatisfiable plan starts action acu commits verifyingsatisfiable. either case plan ends action satisfies goal planverified commitment made first action. interpreted viewing151fiBackstrom & Jonssonacs : {ctu} {cts}acu : {cts} {ctu}Commit prove satisfiabilityCommit prove unsatisfiabilityIIaseti : {cts, v0 } {xi }avt0 : {cts} {v0 }Set xiStart verificationavt0j : {cts, ejn , vj1 ,} {vj }avtkj : {cts, ejn , vj1 , `kj } {vj }ags : {cts, vm(n) } {goal}Skip disabled clause cjVerify cj true since `kj trueConclude instance satisfiableavf j : {ctu, inc, ejn , `1j , `2j , `3j } {inc}aixi : {ctu, inc, xi , xi1 , . . . , x1 }{inc, xi , xi1 , . . . , x1 }agu : {ctu, inc, x1 , . . . , xn } {goal}Verify clause cj falseIncrement counterIIIConclude instance unsatisfiableIndex ranges: 1 n, 1 j m(n) 1 k 3.Table 1: Actions Construction 16.planner theorem prover first outputs theorem (the first action plan)proof theorem (the rest plan).Lemma 17. every integer n > 0 integer 0 < 2m(n) , Stripsinstance pin according Construction 16 following properties:1. computed polynomial time n.2. corresponds 3SAT instance sin every plan pin starts actionacs sin satisfiable otherwise action acu.3. always least one plan.Proof. Property 1 trivial prove. prove property 2, first note initialstate contains atoms En . previously noted, subset Eni En uniquelyidentifies 3SAT instance telling clauses Cn enabled .two cases: plan starts action acs commits verifyingn satisfiable plan starts action acu commits verifyingsatisfiable. either case plan ends action satisfies goalplan verified commitment made first action. details two casesfollows.plan verifies satisfiability, must formkm(n)hacs, aseti1 , . . . , asetih , avt0 , avtk11 , . . . , avtm(n), agsi.{z}||{z}assignverifyassign block h actions set satisfying assignment x1 , . . . , xn . verifykblock consists one action = avtj j clause cjn . cjn enabled (ejn true),1 kj 3 verifies `kj cjn true assignment. Otherwise, cjn152fiAlgorithms Limits Compact Plan Representationsdisabled kj = 0, = avt0j skips cjn without verifying anything.planner thus1. committed verify satisfiable,2. chosen satisfying assignment x1 , . . . , xn3. chosen one literal enabled clause witness clause trueassignment.last action ags makes goal true three steps successful. Noteworks also case clause enabled, corresponds triviallysatisfiable instance empty set clauses. Obviously, plan formsatisfiable.plan instead verifies unsatisfiability, must formhacu, b0 , a1 , b1 , a2 , b2 , . . . , ah , bh , agui,h = 2n 1. Except first last actions, plan viewed twointerleaved sequences= ha1 , . . . , ah = haix1 , aix2 , aix1 , aix3 , . . . , aix1= hb0 , b1 , . . . , bh i.aixi actions increment actions use x1 , . . . , xn form binary counter. Sincevariables correspond number 0 initial state 2n 1 incrementactions, subplan enumerates possible truth assignments x1 , . . . , xn . Sequenceconsists actions type avf j . avf j action verifies correspondingclause cjn enabled false current assignment x1 , . . . , xn . aixi actionsrequire inc true set false, avf j actions instead require inc falseset true. Hence plan synchronised alternates actionstwo sequences. Since first aixi action preceeded avf j actionavf j action last aixi action, follows must unsatisfiedenabled clause every possible truth assignment, since synchronization otherwiseget stuck counter cannot increment. is, plan typeunsatisfiable. last action agu makes goal true succeeded. Notecase need actions skip disabled clauses since sufficientdemonstrate one enabled clause false assignment.follows plan first form satisfiable second formn unsatisfiable. Furthermore, since first action commitment restplan whether verify satisfiability unsatisfiability, sufficient check actiondecide satisfiable not.Property 3 follows immediately property 2 since plan must eithertwo forms.necessary tools prove theorem.153fiBackstrom & JonssonProof Theorem 15. Suppose algorithm either sequential random access stated precondition theorem. solve 3SAT instancepolynomial time asking algorithm first action plancorresponding instance p tell action whether satisfiable. However,implies P = NP.proof would still hold rewriting Construction 16 described Construction 4,is, Theorem 15 holds even restricted set unary Strips instances only.5. Non-Uniform Compact Representations PlansTheorem 15 uses strong criterion: requires one single algorithm handleinstances. relaxed variant non-uniform case, allow differentrepresentations different instances. is, consider compact representationssingle plans different access criteria. order must first defineprecisely mean representations.5.1 Compact Representations Access Mechanismsdefine concepts Csar Crar representations action sequencescharacterised access properties2 .Definition 18. Let f arbitrary function. Let f = hV, D, Ai FFP([P]) framelet . representation DTM. Furthermore:1. f -compact |||| f (||f ||) runs f (||f ||) space including inputoutput tapes.2. f -compact sequential-access representation (f -Csar) f -compact,takes input generates actions sequentially f (||f ||) timesuccessive action.3. f -compact random-access representation (f -Crar) f -compactarbitrary index (where 1 ||) input, outputs actionf (||f ||) time.Note definition require representations computable.could used two separate functions, one bound access time one boundsize, would allow better precision. However, choose use single functionsince makes theory simpler clearer sufficient precisionpurposes paper. consider output tape cleared actionsoutput single action, sequence . Also note space complexityincludes input output tapes, implies longest sequence f -Crar2. Note definition differs slightly previous one (Backstrom & Jonsson, 2011b). First,generalised definition allow compact representations arbitrary function f ,arbitrary polynomial. Second, order improve precision longer use O() notationexact functions. Finally, representations restriction space time. Nonechanges matter results previous publication, details proofs.154fiAlgorithms Limits Compact Plan Representationsrepresent less 2|||| actions since input limited |||| bits. Csarcorresponding limit since input. Furthermore, time restriction f -Csarviewed generalisation polynomial delay concept restrictedpolynomials. often apply definition instances rather frames. Althoughmakes slight difference technically, important principle ignoringallows simpler theorems proofs. write Crar Csar referringwhole family representations particular type.5.2 Sequential-Access Representationssequential access non-uniform case would like ask solvable Stripsinstances least one plan polynomial Csar. Unfortunately, still remainsopen question. Hence, consider restricted case question alsorequire Csar must verifiable within resource constraint, definefollows.Definition 19. every FFP([P]) plan representation type R, define following decision problem:Plan Representation VerificationInstance: FFP([P]) instance p = hV, D, A, I, Gi string .Question: R-representation plan p?complexity verification measured ||p|| + ||||. state followingtheorem polynomial Csars.Theorem 20. Let C arbitrary complexity class p arbitrary polynomial.p-Csar verification NP C every solvable Strips instance least one plancorresponding p-Csar, PSPACE NP C .Proof. Let p arbitrary polynomial. Suppose p-Csar verification NPC everysolvable Strips instance least one plan corresponding p-Csar. Let parbitrary Strips instance. decide p plan guessing stringlength p(||p||) bits check string p-Csar plan p.done polynomial time (in ||p||) using NTM oracle C since p-Csarverification NPC . However, deciding Strips instance plan PSPACEcomplete (Bylander, 1994, Thm. 3.1) follows PSPACE NPC , since pchosen arbitrarily.is, Csar planning instance limited use must first verifycorrect using it, since verification may difficult solving instance itself.Also note C class polynomial hierarchy, PSPACE NPC impliescollapse hierarchy. preceding theorem holds restriction unary Stripsinstances, since planning still PSPACE-complete restriction (Bylander, 1994,Thm. 3.3). fact, holds restrictions planning still PSPACE-complete,includes several cases Bylanders analysis well many subclassesSAS+ planning (see Backstrom & Nebel, 1995; Jonsson & Backstrom, 1998a, overviewsresults).155fiBackstrom & JonssonAlthough result may seem disapointing, holds conditionmust check whether Csar correct. means, instance, theoremirrelevant correctness Csar guaranteed design. One case following.Theorem 21. Every Strips instance according Construction 16 planpolynomial Csar.Proof. Consider arbitrary instance p . Add n + 1 extra bits b0 , . . . , bn b0tells satisfiable not. satisfiable remaining bits specify satisfyingassignment bi gives value vi , otherwise undefined. claimsimple deterministic algorithm uses p b0 , . . . , bn generatesplan p polynomial delay follows.Suppose b0 says satisfiable h bits b1 , . . . , bn one.plan p formkm(n), agsi.hacs, aseti1 , . . . , asetih , avt0 , avtk11 , . . . , avtm(n){z}|{z}|assignverifyactions assign block easily generated b1 , . . . , bn . avtkj actions,output avt0j cjn enabled otherwise output avtkj smallest k `kjtrue specified assignment. Clearly algorithm works polynomial delay.Instead suppose satisfiable. plan second type must cyclepossible assignments. generating corresponding countingactions trivial. assignment must also output avf j action. Determinesmallest j cjn enabled satisfied current assignment,output avf j . done polynomial time since polynomial numberclauses.Clearly construction polynomial Csar plan p .following theorem demonstrates also general harder classesinstances even optimal plans polynomial Csars design.Theorem 22. subclass X Strips polynomial p decidinginstances X plan PSPACE-complete solvable instances Xoptimal plan p-Csar.Proof. PSPACE characterised class polynomial-space bounded DTMs.Bylander (1994, Thm. 3.1) used fact demonstrate polynomial reductionPSPACE Strips planning. refer Bylander details brief: givenmachine input x constructs deterministic Strips instance plan(x) accepts. Hence, polynomial time problem checkvalid state find action, any, applied state. followspolynomial p every solvable Strips instance planp-Csar. Furthermore, since instance deterministic one plan,must optimal.156fiAlgorithms Limits Compact Plan Representationseven general observation every deterministic FFP([P]) instancesolvable exactly one plan, thus optimal, plan polynomialCsar. contrast next consider class instances solvable instancesalways plans polynomial Csars optimality guarantee.example thus illustrates Csar representation gives guaranteesactual data represents. precisely, example uses class reversibleFFP([P]) instances, state-transition graph symmetric.Definition 23. FFP([P]) frame f = hV, D, Ai reversible pairs statesS(f ), whenever action also action a0s.Note reversible instances easy special case planning; decidingplan still PSPACE-complete (Jonsson et al., 2000, Thm. 18). is, plansstill exponential length.Theorem 24. polynomial q polynomials p, every solvablereversible FFP(p) instance (q p)-Csar plan.Proof. Let p = hV, D, A, I, Gi solvable FFP(p) instance f = hV, D, Aireversible. Consider algorithm Figure 2. Optplan assumed algorithmoptplan(s,G) returns length shortest plan G.ignoring process B, clear algorithm outputs optimal plan p sinceplan assumption. Process B finds two actions a1 a2 executingha1 , a2 state ends state s. choice actions must exist sinceplan goal state f reversible. synchronisation processesB make actions a1 , a2 appear adjacent order outputalgorithm, thus interfere plan produced process A.make plan longer. Choosing actions process B done double looppairs actions checking them.obvious polynomial r process B runs r(||p||) time.choose r also allow extra time run process parallel. Let p togetheralgorithm. |||| ||p|| + c constant c. Choose r also satisfiesn + c r(n) n > 0. Obviously, r-Csar plan p. Choosepolynomial q r(n) q(p(n)) n > 0.Algorithm Optplan must obviously run polynomial space, complexity otherwiseimportant. parallel algorithm used proof theorem extentnonsense algorithm. Process job consulting ordinary planning algorithm(optplan) gives time guarantees. Process B, hand, contributes nothingrelevant plan satisfies access time requirement. is, process B buys timeprocess find plan generating irrelevant actions frequently enough satisfytime requirements. wait statements strictly necessary illustratetune step complexity algorithm slowing process B desired.Although example might, perhaps, considered somewhat pathological, clearlydemonstrates Csar (just like Crar) representation. likedata structures certain access properties guarantee particular157fiBackstrom & Jonsson1234567891011121314151617181920:=G(s) = 0parallelprocess A:` = optplan(s, G)a0 s.t. pre(a0 )(s) = 1:= post(a0 )(s)optplan(t, G) < `:= a0 , ` := optplan(t, G)process B:process runningchoose a1 s.t. pre(a1 )(s) = 1u := post(a1 )(s)choose a2 s.t. pre(a2 )(u) = 1 post(a2 )(u) =output a1waitoutput a2waitoutput:= post(a)(s)Figure 2: Csar algorithm reversible instances.properties actual data stored. uncommon plan representations either.instance, reactive plan could constructed behaviour stillconsidered correct run polynomial time space. alternatively userandom walk algorithm, also output actions polynomial delay.eventually reach goal plan, also output lot redundantactions. algorithm Figure 2 may, sense, viewed derandomized variantrandom walk.5.3 Random-Access Representationscase non-uniform random access clearer case sequential access.answer question existence Crars without qualificationsverifiability.Theorem 25. polynomial p every solvable Strips instanceleast one plan corresponding p-Crar, polynomial hierarchy collapses.Since theorem conditioned verifiability representations strongerresult Theorem 20. proving theorem need introduce additional theory.158fiAlgorithms Limits Compact Plan RepresentationsConstruction 26. Let n > 0 arbitrary integer. Construct Strips instance p n =hVn , , , {goal}iVn = Xn En {v0 , . . . , vm(n) } {svi, sva, sia, sii, sti, t, f, goal}actions specified Table 2.abi : {svi, sva, sia, sii, sti} {svi, t}Begin instance blockaba : {svi, sia} {sva, f , v0 , v1 , . . . , vm(n) }Begin assignment blockavtkj : {sva, vj , vj1 , ejn , `kj } {vj }avf j : {sva, vj , vj1 , ejn , `1j , `2j , `3j } {vj , f }Verify `j true clause cjVerify clause cj falseavsj : {sva, vj , vj1 , ejn } {vj }Skip disabled clause cjaaf : {sva, vm(n) , f } {sva, sia}aat : {sva, vm(n) , f } {sva, sia, t}Verify assignment satisfyingVerify assignment satisfyingaixi : {sia, xi , xi1 , . . . , x1 } {sia, xi , xi1 , . . . , x1 }arx : {sia, xn , . . . , x1 } {sia, svi, sti, xn , . . . , x1 }Increment assignment counterReset assignment counterais : {sti, t} {sti, sii}aiu : {sti, t} {sti, sii}Verify instance satisfiableVerify instance unsatisfiable1aiij : {sii, ejn , enj1 , . . . , e1 } {sii, ejn , ej1n , . . . , en }Increment instance counterari :m(n){sii, en , . . . , e1n }{goal}instances checkedIndex ranges: 1 n, 1 j m(n) 1 k 3.Table 2: Actions Construction 26.previous Construction 16 allows plans two types, either choosing assignmentverifying clauses chaining, enumerating assignments demonstrateone false clause each. Construction 26 mixes methods. check instancesatisfiable plan must enumerate variable assignments assignmentmust walk clauses chaining. enabled clause demonstrateseither true literal none literals true, disabled clauses skippedover. Atoms f keep track whether clauses true assignment,m(n)case instance satisfiable. extra counter uses variables e1n , . . . , enenumerates possible subsets Eni En , thus implicitly enumerating 3SAT instancesm(n) 10n , . . . , 2n. counter constitutes outer loop, Eni , possibleassignments x1 , . . . , xn tested described above. plan thoughtimplementing algorithm Figure 3.Lemma 27. integers n > 0, instance pn according Construction 26following properties:1. computed polynomial time n.159fiBackstrom & Jonsson13SAT instances size n2clear3assignments x1 , . . . , xn4clear f5clauses cj6cj disabled7nothing8elsif `kj cj satisfied9nothing10else (neither `1j , `2j `3j satisfied)11set f12f13set1415report satisfiable16else17report unsatisfiableFigure 3: Algorithmic description Construction 26.2. always least one plan.3. exist constants bn every i, 0 < 2m(n) , actionposition bn + plan pn ais 3SAT instance sin satisfiableaiu sin unsatisfiable.Proof. addition previous explanation construction note instancedesigned deterministic. Setting = 2n (m(n) + 3) + 2 bn = + 1 satisfiesclaim, since action position bn + ais satisfiable aiuunsatisfiable.set prove theorem.Proof Theorem 25. Suppose p polynomial solvable Strips instancesleast one plan corresponding p-Crar. n > 0, let p ncorresponding instance according Construction 26 let n p-Crar plann p n . assumption, n n must exist every n.Construct advice-taking DTM takes input form = hp n , ii,n integers n > 0 0 < 2m(n) . Let represented binaryusing exactly m(n) bits. Then, ||I || strictly increasing n depends n.Let sn = ||I || (which well defined since ||I || depend i). Define advicefunction a(sn ) = n . advice thus p-Crar plan Stripsinstance p n according Construction 26. (Recall need knowadvice exists, find it.) Let bn refer corresponding constantsmust exist according Lemma 27. Since run whatever algorithm used accessn , follows assumptions find action bn + n polynomialtime sn . Let return yes action ais otherwise return no.160fiAlgorithms Limits Compact Plan RepresentationsGiven arbitrary 3SAT instance , compute corresponding inputrun instance. construction, answers yes satisfiable. input computed polynomial time ||s || runs polynomial time using polynomial advice. Since solves satisfiability 3SAT followsNP P/poly, impossible unless polynomial hierarchy collapses level 2according Theorem 7a.worth noting plans instances according Construction 26contains subplan every 3SAT instance particular size. Hence, alternativelyview plan representation set exponentially many plans, showsrepresenting one long plan representing large set plans fundamentallydifferent issues.Also Crars restricted cases prove always exist.One example is, again, Construction 16.Theorem 28. Every Strips instance according Construction 16 planpolynomial Crar.Proof. Add n+1 extra bits b0 , . . . , bn explained proof Theorem 21 constructpolynomial-time random-access algorithm follows.Suppose b0 says satisfiable h bits b1 , . . . , bn one.plan p formkm(n)hacs, aseti1 , . . . , asetih , avt0 , avtk11 , . . . , avtm(n), agsi.|{z}|{z}assignverifySince h n construct whole assign sequence determine specific actionpolynomial time. avtkj actions correspond specific clause cjn . Sinceclauses ordered compute j index action ask for. cjnenabled output avf 0j otherwise output avf kj smallest k `kjtrue specified assignment b1 , . . . , bn .Instead suppose satisfiable. first last actions interleaved aixi avf j actions. aixi actions function countervariables x1 , . . . , xn . Let arbitrary index plan firstlast action. odd, ai aixk action k easily computed i,output aixk . even ai avf j action. value x1 , . . . , xn immediatelyai easily computed i. Use value check enabled clausesorder finding clause cjn satisfied output avf j .Clearly construction polynomial Crar plan p .Another much larger class instances plans Crars considerednext section.5.4 Relationships Compact Representationssection investigate Crar Csar concepts relate other. Sincemacro plan also viewed compact representation also investigate macro161fiBackstrom & Jonssonplans relate concepts. start showing polynomial Crarplan also polynomial Csar plan.Theorem 29. polynomial q polynomials p, FFP([P]) framesf = hV, D, Ai , p-Crar, also (q p)-Csar.Proof. Let f arbitrary FFP([P]) instance let p-Crar plan f .Use action counter initiated index first action plan. Generateactions plan sequentially repeatedly asking action indexed counterincrementing counter. Let 0 denote algorithm together . ||||-bitcounter sufficient since must shorter 2|||| actions. Suppose takes r(m) timeincrement m-bit counter. constant c ||0 || |||| + c, 0 runs2p(||f ||) + c space 0 runs p(||f ||) + r(p(||f ||)) + c time. Define polynomial qq(n) = 2r(n)+c. Obviously, ||0 || ||||+c q(||||) q(p(||f ||)), 2p(||f ||)+c q(p(||f ||))p(||f ||) + r(p(||f ||)) + c q(p(||f ||)) 0 (q p)-Csar .opposite hold, however. particular, instances accordingConstruction 26 plan Crar plan Csar,construction acts separation two concepts.Theorem 30. Unless polynomial hierarchy collapses, polynomial qevery polynomial p, every Strips instance p every plan p, p-Csar(q p)-Crar.Proof. Let X denote class Strips instances used proof Theorem 25. Sinceinstances deterministic polynomial p every solvable instancep-Csar plan. However, follows proofpolynomial r instances X plan r-Crar. Hence,polynomial q instances plan (q p)-Crar.Although previously defined paper, makes sense also lookmacro plans context. macro sequence two actions. Macroscommonly used planning treated single action planner. Macrosuseful planning certain subsequences actions occur frequentlyplans (Korf, 1987). However, macros may also used purpose representingplan compact structured way. especially true macros allowedalso contain macros, since allows hierarchies macros. instance, wellknown shortest solution Towers-of-Hanoi problem arbitrary numberdisks described recursive schema (Gill, 1976, Ex. 319) although planexponential number disks. 3S class planning instances (Jonsson &Backstrom, 1998b) property always find polynomial timeinstance plan, plan may exponential length thus cannotgenerated subexponential time. Gimenez Jonsson (2008) showed plans 3Sinstances always polynomial-size representation using macros. fact, macroplan even generated polynomial time although actual non-macro plan wouldtake exponential time generate. result later generalised classes162fiAlgorithms Limits Compact Plan Representationsplanning instances Jonsson (2009). show polynomial-size macro plansimmediate connection compact plan representations. However, contrastGimenez Jonsson discuss generate macro plans analyseproperties.Macro plans powerful tools representing plans compactly. Hence,interesting identify criteria compact macro-plan representations exist not.problem scope paper, give partial answerquestion following way. straightforward see macro plan viewedcontext free grammar (CFG): let actions terminals, let macrosvariables, let macro expansions production rules let root macrostart symbol. note use macros represent single plan, rather represent various possibilities planning, macro expansions must acyclic orderproduce unique well-defined plan. Hence, macro plan defined acyclic CFG.CFGs used represent single string compactly often referredcompressed grammars. Furthermore, compressed grammar permits efficientrandom access string represents; access necessary preprocessingpolynomial time size grammar (Bille et al., 2011). precisely, considergrammar size n represents string length N derivation tree maximumheight h. polynomial time preprocessing, size grammar, possiblerandom access symbol string index O(log N ) time or, alternatively,O(h) time. algorithms typically work first computing length substringsgenerated rule, preprocessing step, use information findsymbol certain index top-down search. Since grammar acyclic get h n.Hence, following proposition immediate properties compressed grammars.Proposition 31. polynomial r every FFP([P]) frame hV, D, Aievery macro plan sequence , used random access actionr(||||) time.thus get following relationship macro plans Crars.Theorem 32. polynomial p polynomials q, FFP([P]) framesf = hV, D, Ai action sequences , macro plan ||||q(||f ||) (p q)-Crar.Proof. Let r polynomial macro plans 0 random accessed r(||0 ||)time. Let together random access algorithm. |||| |||| + cconstant c. Define p p(n) = r(n) + c. get |||| |||| + c q(||f ||) + cr(q(||f ||)) + c = p(q(||f ||)). Furthermore, runs r(||||) r(q(||f ||)) + c = p(q(||f ||))space time. follows (p q)-Crar .follows Theorem 29 every plan polynomial macro plan alsopolynomial Crar. is, class polynomial macro plans subclass classpolynomial Crars, know proper subclass. way, resultsimply cannot always find polynomial macro plan instance.Corollary 33. polynomial p every solvable Strips instanceleast one plan corresponding macro plan size p(||p||) polynomial hierarchy collapses.163fiBackstrom & JonssonProof. Immediate Theorems 25 32.6. Problem Reformulationconcluded seems little hope plans compactlyrepresented general case, turn idea problem reformulation seehelp. may seem place context is, contrary,quite logical step take. far, analysed planning problems plans,results hold for. obvious that, when, results hold alsoplanning instances solved reformulating instances problem.thus hypothetically possible could get around problems approach.However, say something useful relevant this, sufficient looknaive approaches, polynomial reductions, investigate stronger criterion.basic idea reformulation transform planning instance another equivalent instance, either another planning instance instance problem.reformulation useful, solution new instance must use solveoriginal instance, something must gained. Often, reformulation used intention overall process faster solving original instance directly. Commonvariants reformulate planning SAT, CSP, model checking another planningproblem. Reformulation planning SAT first suggested Kautz Selman(1992) still popular approach planning. Long, Fox, Hamdi (2002) discussreformulation planning general Edelkamp, Leue, Visser (2007) discussconnections model checking planning.reformulation process viewed shown Figure 4. planning instance psolution find directly using ordinary planning. Solving p via reformulationinstead follows indirect path figure. First p reformulated new instanceR(p) (of problem). instance solved produces solution R(p).Finally, transformed back solution p.pDirectR(p)IndirectFigure 4: Reformulation generation problem.Obviously, reformulation cannot help us plans exponential. Even firsttwo steps indirect path took polynomial time polynomial size, wouldstill necessarily take exponential time transform exponential.164fiAlgorithms Limits Compact Plan Representationsis, problem inherently intractable whichever method use solve it. Reformulationcould potentially speed things up, could somehow used directly solutionoriginal problem, would happen rarely, all.situation different, though, consider decision problem rathergeneration problem, is, ask plan whether plan not.case use solution R(p) directly, since decision problems twopossible answers, yes no. may thus escape inherent intractability. variantreformulation shown Figure 5. Since exponential solution generated case,reformulation could potentially efficient. know decision problemStrips PSPACE-complete general case. reformulated problem easiersolve, could beneficial first reformulate p R(p) ask instancesolution not. would possible check solutionembarking generating possibly exponentially long plan. Consider, instance,3S class (Jonsson & Backstrom, 1998b) plans may exponential sizealways possible decide polynomial time plan. thus seems like casereformulating decision problems interesting one look at,give improvement, hardly improvement plan generationvia reformulation either.pDirectR(p)IndirectYes/NoYes/NoFigure 5: Reformulation decision problem.Let PE(Strips) denote decision problem (that is, plan existence) Strips.following two results trivial, illustrative.Theorem 34. a) exists decision problem X function R holdsp PE( Strips) R(p) X p R(p) answer. b)complexity class C, decision problem X C polynomial-time computablefunction R holds p PE( Strips) R(p) X p R(p)answer, PSPACE C.Proof. a) Let X = PE(Strips) R identity function. b) Immediate, since Rpolynomial reduction PE(Strips) X.cases reformulate PSPACE-complete problem PSPACE-complete problem, interesting. prove anything better, must obviouslylook X R useful restrictions.165fiBackstrom & Jonssonimportant note reformulating planning NP-complete problem, instance SAT, magically make planning NP-complete. reasonStrips planning PSPACE-complete allows exponential solutions. soonrestrict solutions bounded fixed polynomial, planning belongsNP. Furthermore, encodings planning instances SAT typically use atoms encodeactions appear position plan, is, exponential number extraatoms required general case. Hence, either original problem alreadyNP blow instance exponentially reformulating SAT.latter case, complexity results longer comparable. Also note, deliberately restrict ask plan certain length shorter,actually solving restricted version optimization problem, also caseplanning would harder. fact, seems unlikely planning generalcould reformulated problem NP. order avoid straightforward naiveapproaches reformulation consider analyse reformulations defined follows.Definition 35. Let p = hV, A, I, Gi Strips instance, let f = hV, Ai let = hI, Gi.Let X decision problem. reformulation PE(Strips) X pair hR, rifunctions maps every instance p = hf , PE(Strips) corresponding instancex = R(r(f ), ) X p x answer. hR, ri polynomialreformulation also fixed polynomials p, q1) ||r(f )|| O(p(||f ||))2) R computable O(q(||r(f )|| + ||d ||)) time.thus consider reformulation involves two functions, R r. Function rmain reformulation function, intended reformulate difficult part instance.even require function computable, require exists. FunctionR used transform initial goal descriptions something similarnew instance use, combine result delivered r proper instanceX. noted reformulation concept similar, although identical,compilation concept used Nebel (2000).Theorem 36. polynomial reformulation PE( Strips) X NP,unless polynomial hierarchy collapses.Proof. Suppose hR, ri reformulation. arbitrary integer n > 0, let f un = hVn ,defined Construction 16, without action acs, let = hEni , {goal}i,0 < 2m(n) . follows trivially proof Lemma 17 instancep = hf un , solution unsatisfiable (note SAT partinstance disarmed).Construct advice-taking NTM input = hf un , ii, n > 0 0< 2m(n) , representing binary using m(n) bits. Clearly, ||I || strictly increasingdepends n, let sn = ||I || (for arbitrary i). Define advice functiona(sn ) = r(f un ). (Note need know advice exists, find it).Let first compute , compute x = R(a(sn ), ) = R(r(f un ), ),polynomial time since a(sn ) given free advice. assumption, x Xanswer yes p solution. Also assumption, X NP166fiAlgorithms Limits Compact Plan Representationssolve x guessing solution verifying polynomial time. Hence, decidingp solution NP/poly.arbitrary 3SAT instance , compute polynomial time. answers yesn unsatisfiable. However, unsatisfiability 3SAT coNP-completefollows coNP NP/poly, impossible unless polynomial hierarchycollapses level 3, according Theorem 7b.result pushed arbitrarily high polynomial hierarchy, thus makingunlikely planning could reformulated anything simpler all.Corollary 37. polynomial reformulation hR, ri PE( Strips) decisionproblem X pk , k > 1, unless polynomial hierarchy collapses level k + 2.Proof sketch. Construction 16 demonstrates encode existential quantification(choosing truth assignment sat part) universal quantification (enumeratingtruth assignments unsat part). Hence, straightforward modifyanalogous construction QBF formulae k alternations. Given that, restproof analogous proof Theorem 36, must use oracle pk1 .argument leads pk pk /poly, impossible unless polynomial hierarchycollapses level k + 2, according Theorem 7b.Since proofs build Construction 16 rely exact positionactions follows also Theorem Corollary hold restricted unaryStrips instances only.7. Discussionsection consists five parts. first transfer reformulation theoremgeneral result adding information guide planners, discuss explainvarious results literature. discuss potential relationship causalgraphs compact representations. followed discussion resultspaper could relevant plan explanation. fourth part discusses relatedwork compact representations compilation. section ends summaryresults list open questions.7.1 Reformulation Additional InformationTheorem 36 broader consequences reformulation. fact, impliesway help planner adding information planning frame, matterinformation get it, unless accept amount informationalways polynomially bounded frame size. following theorem function gassumed represent additional information, need even computable.require result polynomially bounded.Theorem 38. Let p arbitrary polynomial. Consider function g algorithm1. g maps Strips frames {0, 1} ||g(f )|| O(p(||f ||)) frames f167fiBackstrom & Jonsson2. Strips instances p = hf, i, algorithm answers yes input hp, g(f )ip plan.runs polynomial time, polynomial hierarchy collapses.Proof. Assume function g algorithm properties describedtheorem. Define function r r(f ) = hf , g(f )i every Strips frame f . Alsodefine function R R(hf , xi, ) = hhf , i, xi every Strips instance p = hf ,every string x. R(r(f ), ) = hhf , i, g(f )i = hp, g(f )i, hR, ri polynomialreformulation Strips planning equivalent problem algorithm solvepolynomial time. However, reformulation exist according Theorem 36,unless polynomial hierarchy collapses.result extended upwards polynomial hierarchy way Corollary 37 (no longer requiring polynomial algorithm). means cannotmake planning simpler adding polynomial amount additional information frameuse clever algorithm use information planning. Planning remainhard without extra information. may sometimes help add information particular instance somehow guide planner, systematic wayadd information frame level required polynomial size.planning literature rich methods intended make planningefficient adding information one way another, although methods perhapsalways thought so. non-exhaustive list methods, similar,abstraction hierarchies, macros, case-based planning, annotated planning landmarks.State space abstraction planning goes back least Abstrips planner (Sacerdoti, 1974). main idea form abstraction hierarchies variables, thusimplicitly actions, planner plan important goalsfirst get abstract plan refined detailed plan. Knoblock(1994) proposed algorithm automatically computing abstraction hierarchies.algorithm successful many examples demonstrated sometimesfail produce exponential plans instances linear optimal plan (Backstrom& Jonsson, 1995). surprising since use abstraction hierarchyviewed adding information planning frame. Automatic generation abstractionhierarchies systematic way add information thus treated special caseTheorem 38.Adding set macros planning frame similar using abstraction hierarchies, Knoblock (1993, pp. 110111) noted. planner uses abstraction searchesplan abstract space tries refine action subplan lowerlever. planner uses macros search abstract space instead alreadyset macros available correspond subplan. Finding macro worksexpanded thus similar refining abstract action. Also usemacros demonstrated speed planning considerably certain cases (Korf,1987). Macros typically added frame level, learning suggestedone method create macros automatically (Korf, 1985). However, macros typically treated action planner expanded findingplan. Hence, addition macros may also backfire make planning less efficient,168fiAlgorithms Limits Compact Plan Representationsadding redundant actions may (Haslum & Jonsson, 2000). again,surprising since addition macros addition information thus also coveredTheorem 38.Case based planning (see Spalazzi, 2001, survey) uses stored plans plan skeletonsplanner tries reuse modifying and/or extending them. one sense,similar macro planning, advanced macros macro expansion methods.One also view similar abstraction, plan must refined orderwork. difference abstraction planner finds plan skeleton planningabstract space case-based planner set plans stored database.plans may handcoded, usually result learning previous planningsituations. well known also case-based planning may fail improve efficiencycases used must similar actual instance hand (Nebel & Koehler, 1995;Liberatore, 2005b). Also explained special case Theorem 38.term annotated planning sometimes used refer number similar techniques adding control information planner. Examples Prodigy planner(Veloso et al., 1995) allows control information like rules goal orderingTlplan (Bacchus & Kabanza, 2000) allows adding temporal-logic axioms controlplanner. techniques good using hand-tailored control rules/axiomsparticular application domain, immediate Theorem 38 cannot helpus general case.Planning landmarks (Hoffmann, Porteous, & Sebastia, 2004) idea addingexplicit subgoals (called landmarks) planning instance. intention tellplanner landmarks must achieved plan order achieve overallgoal. Landmarks may also ordered, guide planner. However,authors point out, deciding variable value (or logic formula) necessarysubgoal PSPACE-complete problem. Hence, one usually considers incompletesets landmarks. interestingly, landmarks differ previous methodsimportant aspect; landmarks added instance level, frame level.Although might quite rigid difference practice, seems fundamentalessence. Hence, adding landmarks non-uniform case adding informationthus immediately covered Theorem 38. meaningfully analyse non-uniformcase remains open question.7.2 Causal GraphsKnoblock (1994) defined ordering variables planning instanceused guidance finding abstraction hierarchies. ordering variablesfundamental also 3S class (Jonsson & Backstrom, 1998b) ordering implicitlydefined abstraction hierarchy. concept ordering variablesintention defining abstraction hierarchy, define tractable subclasses etc. nowadaysusually referred causal graph (see Chen & Gimenez, 2010, survey usingproperties causal graph define tractable subclasses planning). Many papers stilluse Knoblocks definition, follows:every Strips action let Vpre(a) = Atoms(pre(a)) Vpost(a) = Atoms(post(a)). Let169fiBackstrom & Jonssonf = hV, Ai Strips frame. causal graph f directed graph GCG = hV,u, v V , u v u 6= vu Vpre(a) Vpost(a) v Vpost(a) .idea behind causal graphs strongly connected component graphcorrespond abstraction level. applying definition examplespaper, find instance according Construction 16 causal graph containinglarge strongly connected component. is, would possible form goodabstraction hierarchies based causal graphs. However, Theorem 25 saysplans instances seem likely useful compact representations anyway.Plans binary counter Construction 5 polynomial Crars sincepolynomial macro plans. Yet, whole causal graph instance also stronglyconnected. hand, plans Gray counter Construction 6 exponentialpolynomial Crars too, causal graph acyclic case. thus seemscausal graph type used Knoblock many others sufficient,even necessarily useful, tool judging plans compact representations.variants causal graphs, though. One example interaction networks (Chen &Gimenez, 2010). Another Jonssons (2009) refined version Knoblocks causal graph,defined follows:Let f = hV, Ai Strips frame. refined causal graph f directed graphGRCG = hV, u, v V , u v u 6= v either1) u Vpre(a) Vpost(a) v Vpost(a)2) u, v Vpost(a) eithera) a0 u Vpost(a0 ) v 6 Vpost(a0 )b) a0 u 6 Vpost(a0 ) v Vpost(a0 ) .major difference variant Knoblocks two variablesappear postcondition action, necessarily form cyclegraph. Hence, unary actions longer prerequisite acyclic graphs. usingrefined causal graph, Gray counter binary counter acyclicgraphs, Construction 16 still large strongly connected component. is,three examples acyclicity refined causal graph correlates whether planscompact representations not. correlation seems hold general,difference two types causal graphs suggests study variationsconcept could lead insight topic compact representations.turn fruitful, would likely carry also areas causalgraphs used, like model checking (Wehrle & Helmert, 2009).7.3 Plan Explanationresults paper also important plan explanation. Bidot et al. (2010)suggest important planning systems (and AI systems) able170fiAlgorithms Limits Compact Plan Representationsexplain plans decisions user, else user may trust system.Similarly, Southwick (1991) writes:seems general agreement amongst involved KBS researchorder useful, system must able explain reasoning user.Although consider advanced explanation methods, do, resultsimplications possible explain meaningfully. plan explanation, resultsnecessarily bad planning. Consider example plan instanceConstruction 16. case 3SAT instance unsatisfiable, almost whole planconsists alternating sequence form ha, b, a, b, a, b, . . .i, denotes eitheractions aix 1 , . . . , aix n b denotes either actions avf 1 , . . . , avf . firstgroup actions together implement increment function, thus servepurpose. Similarly, second group consists actions serve purposeverifying clause false. abstraction action sequence couldform hinc, vfy, inc, vfy, inc, vfy, . . .i, inc denotes counting actions vfyverification actions. purpose explanation, seems useful replaceactual actions abstract explanations functions. abstract sequenceeasier understand, also allows using macros compress it, mightenhance explaining power. However, particular case, would probably evenuseful abstract whole sequence loop, similar. essentiallyboils partitioning set actions equivalence classesclass consist actions meaningfully seen implementing concept.seems interesting important investigate one partitionset actions equivalence classes useful abstractions.Plan explanation could also mean trace explanation model checking, wouldanalogously make long trace shorter abstract order make easier understand. well known close ties planning model checking,model-checking traces viewed plans vice versa (Edelkamp et al., 2007).number steps (or clock cycles) exponential number state variables;even system divided subsystems, individual subsystems may exponential behaviour blows combined subsystems. exponential-sizeplan/trace much use engineerit almost impossible task analyseunderstand plan. planning/verifying system could autonomously findrepetitive patterns, even recursive repetitive patterns, plan abstract these,would considerably easier understand happens why. fact, mayinteresting execute plan, even simulator, compact understandableexplanation plan may actual goal.Furthermore, Geib (2004) discusses problem combinatorial explosion plan recognition, exponential number plans may share plan prefix recognized far.could clearly useful structured compact representations plan candidatessave space allow intelligent operations plans. Althoughproblem slightly different representing single long plan, seentwo problems related.cases, primary purpose compact representation would thus findexploit inherent structure plan, set plans, rather save space.171fiBackstrom & Jonsson7.4 Additional Related WorkLiberatore (2005a) also studied problem representing plans compactlysimilarities well differences results ours. contrast us,considers also plans represented sequences states, sequences actions.cases, considers random access representation well sequential representation. random-access representation action sequences (TA) essentiallyCrar concept, except specifies must implemented circuit.sequential representation action sequences (SA), hand, differentCsar concept. function takes state input returns next state.Hence, like restricted type reactive plan Csar, resultsthus immediately comparable ours. instance, contrary Theorem 29proves TA representation cannot polynomially converted SA representation,clearly shows SA Csar quite different concepts. proofplanning instances plans SA representation thus obviously carryalso Csars. Furthermore, uses planning language actions modelledpolynomial-size circuits. coincides class FFP([P]). Hence, hardnessproofs weaker since use restricted Strips language cases.finally noted Liberatores Theorem 17 case TA representationsresult similar Theorem 25, use different methods different conditions.Nebel (2000) defines concept compilation planning languages. Althoughways similar reformulation concept, also differences. compilationfunction planning frame another frame different planning language.compilation need resource bounded resulting frame must polynomiallybounded original frame. initial state goal must possible translatepolynomial time. is, first step corresponds function r secondstep essentially corresponds function R. However, Nebel considers compilationplanning languages also requires concept modularity presentapproach. Furthermore, focus complexity decision problemquestion whether size solutions preserved compilations.7.5 Conclusions Open Questionscurrent status knowledge non-uniform compact representationsvisualized Figure 6. outer box represents set solvable Strips instancesinner boxes represent subsets least one plan instanceCsar, Crar polynomial macro plan. know classes least one planinstance guaranteed polynomial macro plan, like Towers Hanoi 3S.also know classes least one plan instance Crar knowplans also polynomial macro plan. Construction 16 example.open question plan Crar polynomial macro plan.know classes instance plan Csar knowalso Crar, example, class reversible systems. However, Construction 26class instances plan Csar plan Crar,case provides strict separation Csar Crar concepts. Whetherclasses Strips instances plan Csar remains open question, though.172fiAlgorithms Limits Compact Plan RepresentationsplansPlans w. CsarPlans w. Crar3STowersHanoiPlans w. poly.macro plans??anythinghere?reversiblesystems?Construction 16Construction 26Figure 6: Current status Csars Crars.thus following chain inclusionspolynomial macro plans Crar Csar Strips,know first last inclusions strict.Theorem 15 may seem weak since conditioned P 6= NP. Using similartechniques proofs Lemma 27 Corollary 37 could encode QBFarbitrary number alternating quantifiers and, hence, push result polynomialhierarchy. However, remains open question condition could strengthenedway P 6= PSPACE.argued number results hold also restricted unaryinstances, cases also restrictions, otherwise largely unexploredarea. Little currently known various structural restrictions affectresults paper. applies whether plans Csars Crarswhether polynomial-size macro plans.consider non-uniform case compact representations single planssingle instances, might also interesting consider non-uniform case reformulation adding information. However, seems straightforward since couldalways reformulate instance single bit telling whether instance solvable not.reformulation clearly interesting additional criteria necessary.previously (Backstrom & Jonsson, 2011a) defined complexity measure basedpadding intended insensitive plan length. concept seems relatedNebels compilations, although two concepts identical directly comparable.thus reasonable believe also compact representations padded complexitysomehow related, especially since padded complexity motivated instanceslong plans. However, yet know relationship is. Furthermore, would173fiBackstrom & Jonssoninteresting consider compilations look size compact representationsplan rather size explicit plans.AcknowledgmentsMalte Helmert, Anders Jonsson anonymous reviewers paper earlierconference version provided valuable comments suggestions.ReferencesBacchus, F., & Kabanza, F. (2000). Using temporal logics express search control knowledge planning. Artificial Intelligence, 116 (1-2), 123191.Backstrom, C., & Jonsson, P. (2011a). PSPACE-complete planning problems equalequal others. Proceedings 4th International Symposium Combinatorial Search (SoCS11) Castell de Cardona, Barcelona, Spain, pp.1017.Backstrom, C., & Jonsson, P. (2011b). Limits compact representations plans. Proceedings 21st International Conference Automated Planning Scheduling,(ICAPS11), Freiburg, Germany, pp. 1825.Backstrom, C. (1992). Computational Complexity Reasoning Plans. PhD dissertation, Linkoping University, Linkoping, Sweden.Backstrom, C. (1995). Expressive equivalence planning formalisms. Artificial Intelligence,76 (1-2), 1734.Backstrom, C., & Jonsson, P. (1995). Planning abstraction hierarchies exponentially less efficient. Proceedings 14th International Joint ConferenceArtificial Intelligence (IJCAI95), Montreal, QC, Canada, pp. 15991605.Backstrom, C., & Klein, I. (1991). Planning polynomial time: SAS-PUBS class.Computational Intelligence, 7, 181197.Backstrom, C., & Nebel, B. (1995). Complexity results SAS+ planning. ComputationalIntelligence, 11, 625656.Balcazar, J. (1996). complexity searching implicit graphs. Artificial Intelligence,86 (1), 171188.Bidot, J., Biundo, S., Heinroth, T., Minker, W., Nothdurft, F., & Schattenberg, B. (2010).Verbal plan explanations hybrid planning. Proceedings 24th MKWIrelated PuK-workshop: Planung/Scheduling und Konfigurieren/Entwurfen (PuK10),pp. 23092320.Bille, P., Landau, G., Raman, R., Sadakane, K., Satti, S., & Weimann, O. (2011). Randomaccess grammar-compressed strings. Proceedings 22nd ACM-SIAM Symposium Discrete Algorithms (SODA11), San Fransisco, CA, USA, pp. 373389.Bonet, B. (2010). Conformant plans beyond: Principles complexity. ArtificialIntelligence, 174 (3-4), 245269.174fiAlgorithms Limits Compact Plan RepresentationsBonet, B., & Geffner, H. (2000). Planning incomplete information heuristic searchbelief space. Proceedings 5th International Conference Artificial Intelligence Planning Systems (AIPS00), Breckenridge, CO, USA, pp. 5261.Boutilier, C., & Poole, D. (1996). Computing optimal policies partially observabledecision processes using compact representations. Proceedings 13th NationalConference Artificial Intelligence (AAAI96), Portland, OR, USA, Vol. 2, pp. 11681175.Brafman, R. I., & Domshlak, C. (2003). Structure complexity planning unaryoperators. Journal Artificial Intelligence Research, 18, 315349.Buhrman, H., Jiang, T., Li, M., & Vitanyi, P. M. B. (2000). New applicationsincompressibility method: Part II. Theoretical Computer Science, 235 (1), 5970.Bulatov, A., & Dalmau, V. (2006). simple algorithm Maltsev constraints. SIAMJournal Computing, 36 (1), 1627.Bylander, T. (1994). computational complexity propositional STRIPS planning.Artificial Intelligence, 69 (1-2), 165204.Cadoli, M., Donini, F., Liberatore, P., & Schaerf, M. (2000). Space efficiency propositionalknowledge representation formalisms. Journal Artificial Intelligence Research, 13,131.Charikar, M., Lehman, E., Liu, D., Panigrahy, R., Prabhakaran, M., Sahai, A., & Shelat, A.(2005). smallest grammar problem. IEEE Transactions Information Theory,51 (7), 25542576.Chen, H., & Gimenez, O. (2010). Causal graphs structurally restricted planning. Journal Computer System Sciences, 76 (7), 579592.Edelkamp, S., Leue, S., & Visser, W. (2007). Summary Dagstuhl seminar 06172directed model checking. Directed Model Checking, No. 06172 Dagstuhl SeminarProceedings. Dagstuhl, Germany.Galperin, H., & Wigderson, A. (1983). Succinct representations graphs. InformationControl, 56 (3), 183198.Geib, C. (2004). Assessing complexity plan recognition. Proceedings 19thNational Conference Artificial Intelligence (AAAI04), San Jose, CA, USA, pp.507512.Gill, A. (1976). Applied Algebra Computer Sciences. Prentice Hall. EnglewoodCliffs, NJ.Gimenez, O., & Jonsson, A. (2008). complexity planning problems simplecausal graphs. Journal Artificial Intelligence Research, 31, 319351.Haslum, P., & Jonsson, P. (2000). Planning reduced operator sets. Proceedings5th International Conference Artificial Intelligence Planning Systems (AIPS00),Breckenridge, CO, USA, pp. 150158.Hoffmann, J., Porteous, J., & Sebastia, L. (2004). Ordered landmarks planning. JournalArtificial Intelligence Research, 22, 215278.175fiBackstrom & JonssonJansson, J., Sadakane, K., & Sung, W.-K. (2012). Compressed random access memory.ArXiv, abs/1011.1708v2.Johnson, D. S., Papadimitriou, C. H., & Yannakakis, M. (1988). generating maximalindependent sets. Information Processing Letters, 27 (3), 119123.Jonsson, A. (2009). role macros tractable planning. Journal Artificial Intelligence Research, 36, 471511.Jonsson, P., & Backstrom, C. (1998a). State-variable planning structural restrictions:Algorithms complexity. Artificial Intelligence, 100 (1-2), 125176.Jonsson, P., & Backstrom, C. (1998b). Tractable plan existence imply tractableplan generation. Annals Mathematics Artificial Intelligence, 22 (3-4), 281296.Jonsson, P., Haslum, P., & Backstrom, C. (2000). Towards efficient universal planning:randomized approach. Artificial Intelligence, 117 (1), 129.Karp, R. M., & Lipton, R. J. (1980). connections nonuniform uniform complexity classes. Proceedings 12th ACM Symposium TheoryComputing (STOC80), Los Angeles, CA, USA, pp. 302309.Kautz, H. A., & Selman, B. (1992). Planning satisfiability. Proceedings 10thEuropean Conference Artificial Intelligence (ECAI92), Vienna, Austria, pp. 359363.Knoblock, C. A. (1993). Generating Abstraction Hierarchies: Automated ApproachReducing Search Planning. Kluwer Academic Publishers. Norwell, MA.Knoblock, C. A. (1994). Automatically generating abstractions planning. ArtificialIntelligence, 68 (2), 243302.Korf, R. E. (1985). Macro-operators: weak method learning. Artificial Intelligence,26 (1), 3577.Korf, R. E. (1987). Planning search: quantitative approach. Artificial Intelligence,33 (1), 6588.Liberatore, P., & Schaerf, M. (2010). size data structures used symbolic modelchecking. ArXiv, abs/1012.3018.Liberatore, P. (2005a). Complexity issues finding succinct solutions PSPACE-completeproblems. ArXiv, abs/cs/0503043.Liberatore, P. (2005b). complexity case-based planning. Journal ExperimentalTheoretical Artificial Intelligence, 17 (3), 283295.Long, D., Fox, M., & Hamdi, M. (2002). Reformulation planning. Proceedings5th International Symposium Abstraction, Reformulation Approximation(SARA02), Kananaskis, AB, Canada, Vol. 2371 Lecture Notes Computer Science, pp. 1832. Springer.Muscettola, N., Pandurang Nayak, P., Pell, B., & Williams, B. C. (1998). Remote agent:boldly go AI system gone before. Artificial Intelligence, 103 (1-2),547.176fiAlgorithms Limits Compact Plan RepresentationsNebel, B. (2000). compilability expressive power propositional planningformalisms. Journal Artificial Intelligence Research, 12, 271315.Nebel, B., & Koehler, J. (1995). Plan reuse versus plan generation: theoreticalempirical analysis. Artificial Intelligence, 76 (1-2), 427454.Rytter, W. (2003). Application Lempel-Ziv factorization approximationgrammar-based compression. Theoretical Computer Science, 302 (1-3), 211222.Sacerdoti, E. D. (1974). Planning hierarchy abstraction spaces. Artificial Intelligence,5 (2), 115135.Southwick, R. W. (1991). Explaining reasoning: overview explanation knowledgebased systems. Knowledge Engineering Review, 6, 119.Spalazzi, L. (2001). survey case-based planning. Artificial Intelligence Review, 16,336.Veloso, M. M., Carbonell, J. G., Perez, A., Borrajo, D., Fink, E., & Blythe, J. (1995). Integrating planning learning: PRODIGY architecture. Journal ExperimentalTheoretical Artificial Intelligence Research, 7 (1), 81120.Wagner, K. (1986). complexity combinatorial problems succinct input representation. Acta Informatica, 23 (3), 325356.Wehrle, M., & Helmert, M. (2009). causal graph revisited directed model checking.Proceedings 16th International Symposium Static Analysis (SAS09), LosAngeles, CA, USA, Vol. 5673 Lecture Notes Computer Science, pp. 86101.Springer.Williams, B., & Pandurang Nayak, P. (1997). reactive planner model-based executive. Proceedings 15th International Joint Conference Artificial Intelligence (IJCAI97), Nagoya, Japan, pp. 11781185.Yap, C.-K. (1983). consequences non-uniform conditions uniform classes. Theoretical Computer Science, 26, 287300.177fiJournal Artificial Intelligence Research 44 (2012) 275-333Submitted 12/11; published 6/12Algorithms Generating Ordered Solutions ExplicitAND/OR StructuresPriyankar GhoshAmit SharmaP. P. ChakrabartiPallab Dasguptapriyankar@cse.iitkgp.ernet.inamit.ontop@gmail.comppchak@cse.iitkgp.ernet.inpallab@cse.iitkgp.ernet.inDepartment Computer Science EngineeringIndian Institute Technology KharagpurKharagpur-721302, IndiaAbstractpresent algorithms generating alternative solutions explicit acyclic AND/ORstructures non-decreasing order cost. proposed algorithms use best first searchtechnique report solutions using implicit representation ordered cost.paper, present two versions search algorithm (a) initial version best firstsearch algorithm, ASG, may present one solution generatingordered solutions, (b) another version, LASG, avoids constructionduplicate solutions. actual solutions reconstructed quickly implicitcompact representation used. applied methods test domains,synthetic others based well known problems including searchspace 5-peg Tower Hanoi problem, matrix-chain multiplication problemproblem finding secondary structure RNA. Experimental results show efficacyproposed algorithms existing approach. proposed algorithmspotential use various domains ranging knowledge based frameworks servicecomposition, AND/OR structure widely used representing problems.1. Introductionuse AND/OR structures modeling solving complex problems efficientlyattracted significant amount research effort last decades. Initially,AND/OR search spaces mostly used problem reduction search solving complexproblems, logical reasoning theorem proving, etc., overall problemhierarchically decomposed conjunction disjunction subproblems (Pearl, 1984;Nilsson, 1980). Subsequently, AND/OR structures also applied variety domains, e.g., representing assembly plans (Homem de Mello & Sanderson, 1990), generating VLSI floor-plans (Dasgupta, Sur-Kolay, & Bhattacharya, 1995), puzzle solving (Fuxi,Ming, & Yanxiang, 2003), etc. Traditionally algorithm AO* (Pearl, 1984; Nilsson, 1980;Martelli & Montanari, 1978, 1973; Chang & Slagle, 1971) used searching implicitly defined AND/OR structures. empirical study AO* found BonetGeffners (2005) work.recent past renewed research interest towards applicationAND/OR structures. various planning problems, including conditional planninghandle uncertainty, AND/OR structure (Russell & Norvig, 2003) natural formc2012AI Access Foundation. rights reserved.fiGhosh, Sharma, Chakrabarti, & Dasguptarepresentation. problem generating solutions representationsstudied extensively (Hansen & Zilberstein, 2001; Jimenez & Torras, 2000; Chakrabarti,1994). Dechter Mateescu (2007) presented explicit AND/OR search spaceperspective graphical models. Different search strategies (best first, branch bound,etc.) AND/OR search spaces graphical models discussed MarinescuDechter (2007b, 2006). AND/OR search spaces also used solving mixed integerlinear programming (Marinescu & Dechter, 2005), 0/1 integer Programming (Marinescu& Dechter, 2007a), combinatorial optimization graphical models (Marinescu & Dechter,2009a, 2009b). AND/OR Multivalued Decision Diagrams (AOMDD), combineidea Multi-Valued Decision Diagrams(MDD) AND/OR structures, presentedMateescu, Dechter, Marinescu (2008) research along directionfound work Mateescu Dechter (2008). AND/OR search spaces alsoapplied solution sampling counting (Gogate & Dechter, 2008). Smooth Deterministic Decomposable Negative Normal Forms (sd-DNNF) (Darwiche, 2001) exhibit explicitAND/OR DAG structure used various applications including compilingknowledge (Darwiche, 1999), estimating belief states (Elliott & Williams, 2006), etc.Apart domains planning, constraint satisfaction, knowledge based reasoning,etc., AND/OR structure based techniques also widely used various application baseddomains, e.g., web service composition (Gu, Xu, & Li, 2010; Shin, Jeon, & Lee, 2010; Gu,Li, & Xu, 2008; Ma, Dong, & He, 2008; Yan, Xu, & Gu, 2008; Lang & Su, 2005), visiongraphics tasks (Chen, Xu, Liu, & Zhu, 2006), etc. Lang Su (2005) describedAND/OR graph search algorithm composing web services user requirements.et al. (2008) advocated use AND/OR trees capture dependenciesinputs outputs component web services propose top-down search algorithmgenerate solutions AND/OR tree. research uses AND/OR structurescontext web service composition found works Gu et al. (2010,2008), Shin et al. (2010) Yan et al. (2008). Chen et al. (2006) applied explicitAND/OR structures cloth modeling recognition important problemvision graphics tasks.recent adoption AND/OR search spaces wide variety AI problemswarrants research towards developing suitable algorithms searching AND/ORstructures different perspectives. general setting, fundamental problemremains find minimum cost solution AND/OR structures. given explicitAND/OR graph structure, minimum cost solution computed using either topdown bottom-up approach. approaches based principle dynamicprogramming complexity linear respect size searchspace. Finding minimum cost solution explicit AND/OR structure fundamentalstep approaches use implicit representation systematically exploresearch space. particularly case AO* (Nilsson, 1980) potentialsolution graph (psg) recomputed every time current explicit graph nodeexpanded. view recent research AND/OR structures used leveragedwide variety problems ranging planning domain web service composition,need generating ordered set solutions given AND/OR structure becomesimminent. briefly mention areas ordered solutions useful.276fiGenerating Ordered Solutions Explicit AND/OR StructuresOrdered set solutions explicit AND/OR DAG used develop usefulvariants AO* algorithm. Currently AO*, minimum cost solution computed whereas several variants A* algorithm exist, solutions often soughtwithin factor cost optimal solution. approaches (Ebendt & Drechsler, 2009;Pearl, 1984) developed adapt A* algorithm using inadmissible heuristics,leveraging multiple heuristics (Chakrabarti, Ghose, Pandey, & DeSarkar, 1989), generatingsolutions quickly within bounded sub-optimality, etc. Typically techniques orderOpen list using one evaluation function, next element expansion selectedordered subset Open using criterion. Similar techniques developedAO* search ordered set potential solutions made available. setused node selection expansion instead expanding nodes currentbest psg. opens interesting area significant research potentialexisting variations A* algorithm extended AND/OR search spaces.context model based programming, problem finding ordered setsolutions significant importance. Elliott (2007) used valued sd-DNNFs representproblem proposed approach generate k-best solutions. Since valued sd-DNNFsAND/OR structure, proposed approach possibly earliest algorithmgenerating ordered set solutions AND/OR structure. problem findingordered set solutions graphical models studied Flerova Dechter (2011, 2010).However techniques use alternative representations algorithm, AND/ORsearch spaces constructed (Dechter & Mateescu, 2007) graphical models. Recentresearch involving AOMDD based representation weighted structures suggested futureextensions towards generalizing Algebraic Decision Diagrams introduces notioncost AOMDDs. envisage ordered set solutions finds useful applicationscontext research around AND/OR decision diagram based representation.domain service composition, primary motivation behind providing setalternative solutions ordered cost offer choices, trading specifiedcost criterion (to limited extent) favor unspecified criteria (primarilystandpoint quality). Shiaa, Fladmark, Thiell (2008) presented approachgenerating ranked set solutions service composition problem. Typicallyquality criteria subjective nature difficult express terms single scalar costfunction able combine cost/price quality aspects together.aspects quality often encountered context serving custom user requirementsuser prefers minimize cost/price solution preserving his/herpreferences. example, booking holiday package specific destination, travelservice portal typically offers list packages various combinations attractions,hotel options meal plans ordered single cost criterion, namely, costpackage. general product/solution composed number componentscompositional flavor similar service composition becomes important presentuser set alternative solutions ordered cost he/she select bestalternative according his/her preferences.Dynamic programming formulations typically underlying AND/OR DAG structure, formally studied past (Martelli & Montanari, 1973). Besidesclassical problems like matrix chain multiplication, many real world optimization problems offer dynamic programming formulations, alternative solutions ordered cost277fiGhosh, Sharma, Chakrabarti, & Dasguptauseful practice. One example problem finding secondary structureRNA (Mathews & Zuker, 2004) important problem Bioinformatics. RNAsmay viewed sequences bases belonging set {Adenine(A), Cytocine(C), Guanine(G), Uracil(U)}. RNA molecules tend loop back form base pairsresulting shape called secondary structure. primary factor influencessecondary structure RNA number base pairings (higher number base pairings generally implies stable secondary structure). well established rulesbase pairings, problem maximizing number base pairings interesting dynamic programming formulation. However, apart number base pairings,factors influence stability, factors typically evaluatedexperimentally. Therefore, given RNA sequence, useful compute poolcandidate secondary structures (in decreasing order number base pairings)may subjected experimental evaluation order determine stablesecondary structure.problem generating ordered set solutions well studied domains.discrete optimization problems, Lawler (1972) proposed general proceduregenerating k-best solutions. similar problem finding k probable configurationsprobabilistic expert systems addressed Nilsson (1998). Fromer Globerson (2009)addressed problem finding k maximum probability assignments probabilistic modeling using LP relaxation. context ordinary graphs, Eppstein (1990)studied problem finding k-smallest spanning trees. Subsequently, algorithmfinding k-best shortest paths proposed Eppsteins (1998) work. HamacherQueyranne (1985) suggested algorithm k-best solutions combinatorial optimization problems. Algorithms generating k-best perfect matching presentedChegireddy Hamacher (1987). researchers applied k-shortest path problempractical scenarios, as, routing transportation, developed specific solutions(Takkala, Borndorfer, & Lobel, 2000; Subramanian, 1997; Topkis, 1988; Sugimoto & Katoh,1985). However none approaches seems directly applicable AND/OR structures. Recently schemes related ordered solutions graphical models (Flerova &Dechter, 2011, 2010) anytime AND/OR graph search (Otten & Dechter, 2011)proposed. Anytime algorithms traditional search space (Hansen & Zhou, 2007)well addressed research community.paper, address problem generating ordered set solutions explicitAND/OR DAG structure present new algorithms. existing method, proposedElliott (2007), works bottom-up computing k-best solutions current nodek-best solutions children nodes. present best first search algorithm,named Alternative Solution Generation (ASG) generating ordered set solutions.proposed algorithm maintains list candidate solutions, initially containingoptimal solution, iteratively generates next solution non-decreasing order costselecting minimum cost solution list. iteration, minimum costsolution used construct another set candidate solutions, addedcurrent list. present two versions algorithma. Basic ASG (will referred ASG henceforth) : version algorithmmay construct particular candidate solution once;278fiGenerating Ordered Solutions Explicit AND/OR Structuresb. Lazy ASG LASG : Another version ASG algorithm constructs every candidate solution once.algorithms, use compact representation, named signature, storingsolutions. signature solution, actual explicit form solutionconstructed top-down traversal given DAG. representation allowsproposed algorithms work top-down fashion starting initial optimalsolution. Another salient feature proposed algorithms algorithms workincrementally unlike existing approach. proposed algorithms interruptedpoint time execution set ordered solutions obtained farobserved subsequent solutions generated algorithms resumedagain. Moreover, upper limit estimate number solutions required knownpriori, algorithms optimized using estimate.rest paper organised follows. necessary formalisms definitionspresented Section 2. Section 3, address problem generating ordered setsolutions trees. Subsequently Section 4, address problem finding alternativesolutions explicit acyclic AND/OR DAGs non-decreasing order cost. present twodifferent solution semantics AND/OR DAGs discuss existing approach wellproposed approach, along comparative analysis. Detailed experimental results,including comparison performance proposed algorithms existingalgorithm (Elliott, 2007), presented Section 5. used randomly constructedtrees DAGs well well-known problem domains including 5-peg TowerHanoi problem, matrix-chain multiplication problem problem findingsecondary structure RNA test domain. time required memory usedgenerating specific number ordered solutions different domains reporteddetail. Section 6, outline briefly applying proposed algorithms implicitlyspecified AND/OR structures. Finally present concluding remarks Section 7.2. Definitionssection, describe terminology AND/OR trees DAGs followeddefinitions used paper. G = hV, Ei AND/OR directed acyclic graph,V set nodes E set edges. G refernodes nodes DAG respectively. direction edges Gparent node child node. nodes G successors called terminalnodes. non-terminal nodes G two types i) nodes ii) nodes .V V set nodes G respectively, n = |V |, n = |V |,n = |V |. start (or root) node G denoted vR . edges edgesedges emanate nodes nodes respectively.Definition 2.a [Solution Graph] solution graph, S(vq ), rooted node vq V ,finite sub-graph G defined as:a. vq S(vq );b. vq node G vq S(vq ), exactly one immediatesuccessors G S(vq );c. vq node G vq S(vq ), immediate successorsG S(vq );279fiGhosh, Sharma, Chakrabarti, & Dasguptad. Every maximal (directed) path S(vq ) ends terminal node;e. node vq successors G S(vq ).solution graph G mean solution graph root vR .Definition 2.b [Cost Solution Graph] G , every edge eqr E node vqnode vr finite non-negative cost ce (hvq , vr i) ce (eqr ). Similarly every node vqfinite non-negative cost denoted cv (vq ). cost solution defined recursivelyfollows. every node vq S, cost C(S, vq ) is:cv (vq ), vq terminal node;cv (vq ) + C(S, vr ) + ce (hvq , vr i) , vq node,C(S, vq ) =vr successor vq S;PC(S, vj ) + ce (hvq , vj i) , 1 j k, vq nodecv (vq ) +degree k, v1 , . . . , vk immediate successors vq S.Therefore cost solution C(S, vR ) also denoted C(S). denoteoptimal solution every node vq opt(vq ). Therefore, optimal solutionentire AND/OR DAG G , denoted Sopt , opt(vR ). cost optimal solutionrooted every node vq G Copt (vq ), defined recursively (for minimum costobjective functions) follows:cv (vq ), vq terminal node;cv (vq ) + min Copt (vj ) + ce (hvq , vj i) , 1 j k, vq nodeCopt (vq ) =degree k, v1 , . . . , vk immediate successors vq G ;cv (vq ) + P Copt (vj ) + ce (hvq , vj i) , 1 j k, vq nodedegree k, v1 , . . . , vk immediate successors vq G .cost optimal solution Sopt G denoted Copt (vR ) or, alternatively,Copt (Sopt ). objective function needs maximized, instead min function,max function used definition Copt (vq ).may noted possible one solution nodevq qualify optimal one, i.e., cost, costminimum. Ties optimal solution node vq resolved arbitrarilyone among qualifying solutions (determined tie-breaking) markedopt(vq ).AND/OR tree, = hV, Ei, AND/OR DAG additionally satisfiesrestrictions tree structure i.e., one parent node node vq. context AND/OR trees, use eq denote edge pointsvertex vq . alternating AND/OR tree, = hV, Ei, AND/OR treerestriction alternation nodes nodes. Everychild node either node terminal node, every childrennode either node terminal node. use term solution tree denotesolutions AND/OR trees.also discuss different solution semantics, namely tree based semantics, AND/ORDAGs. Every AND/OR DAG converted equivalent AND/OR tree traversing280fiGenerating Ordered Solutions Explicit AND/OR Structuresintermediate nodes reverse topological order replicating subtree rootedevery node whenever in-degree traversed node 1. detailsshown Procedure ConvertDAG. Suppose AND/OR DAG G convertedequivalent AND/OR tree . define solutions solutions Gtree based semantics.Procedure ConvertDAG(G )input : AND/OR DAG Goutput: equivalent AND/OR tree1 Construct list , non-terminal nodes G , sorted reverse topologicalorder;2 empty3vq Remove first element ;/* Suppose Ein (vq ) list incoming edges vq*/4InDegree(vq ) > 152 InDegree(vq )6et Ein (vq )[i];Replicatesub-tree rooted vq vq root;7Modify target node et vq vq ;89end10end11 endpaper use solution semantics defined Definition 2.a defaultsemantics solutions AND/OR DAGs. tree based semantics used,explicitly mentioned.2.1 Example2, 34h3i3, 29v1v1v2h1ih1ih2iv32, 37v2v42, 8h1iv5h3ih4iv63, 11v7h1ih1ih2ih2iv9v10v11v12v13v14v15v9v10576912152057Figure 1: Alternating AND/OR Tree281v62, 35v83, 9h1ih1ih1ih3ih4iv7h3i2, 41h4iv54012h2iv33, 43v4v84, 17h2ih5ih1i35h5i2, 8917h2iFigure 2: AND/OR DAG52fiGhosh, Sharma, Chakrabarti, & Dasguptapresent example alternating AND/OR tree Figure 1. figure,terminal nodes represented circle thick outline. nodes shownfigures outgoing edges connected semi-circular curve examples.edge costs shown side edge within angled bracket. costterminal nodes shown inside box. every non-terminal node vq , pair costs,cv (vq ) Copt (vq ), shown inside rectangle.Figure 1 optimal solution every node shown using thick dashed edgesarrow head. optimal solution AND/OR tree traced followingthick dashed edges node v1 . cost optimal solution tree 34. Also,Figure 2 shows example DAG; cost optimal solution DAG 89.3. Generating Ordered Solutions AND/OR Treessection address problem generating ordered solutions trees. usenotion alternating AND/OR trees, defined Section 2, present algorithms.alternating AND/OR tree presents succinct representation correctnessproofs much simpler alternating AND/OR trees. Appendix C show everyAND/OR tree converted equivalent alternating AND/OR tree respectsolution space.worth noting search space problems (e.g. search space multipeg Tower Hanoi problem) exhibit alternating AND/OR tree structure. Moreover,algorithms presented alternating AND/OR trees work without modificationgeneral AND/OR trees. section, first present existing algorithm (Elliott,2007) briefly, present proposed algorithms detail.3.1 Existing Bottom-Up Evaluation Based Method Computing AlternativeSolutionsillustrate working existing method proposed Elliott (2007)computing alternative solutions trees using example alternating AND/OR tree.method (will referred BU henceforth) computes k-best solutions bottomup fashion. every node, vq , k-best solutions computed k-best solutionschildren vq . overall idea follows.a. node vq , solution rooted vq obtained selecting solutionchild. Therefore k-best solutions vq computed selecting top k solutionsentire pool consisting solutions children.b. case nodes, every child node vq k solutions.solution rooted node vq obtained combining one solution everychild vq . Different combinations solutions children nodes vq generatedifferent solutions rooted vq . Among combinations, top k combinationsstored vq .Figure 3 show working existing algorithm. every intermediate node2-best solutions shown within rounded rectangle. every node vq , ith -bestcost.solution rooted vq shown triplet form |{z}: < child, solidx >, |{z}{z}|example, node v1 second best solution shown 2 : hv2 , 2i, 37; means282fiGenerating Ordered Solutions Explicit AND/OR Structures2nd best solution rooted v1 obtained selecting 2nd best solution v2 .Similarly, every node vq , ith solution rooted vq shown tripletform : |sol vec|, cost triplets. sol vec comma separated list solution indicesevery element sol vec corresponds child vq . j th element sol vecshows index solution j th child. example, 2nd best solution rooted v2shown 2 : |2, 1|, 32. means 2nd best solution rooted v2 computed using2nd best solution 1st child (which v5 ) best solution (1st ) 2ndchild (which v6 ). index sol vec corresponds child shown placingchild node name every index position.2, 34h3i3, 29v2h5i2, 8v51 : hv2 , 1i, 342 : hv2 , 2i, 37v1h1ih2iv5 v61 : |1, 1|, 292 : |2, 1|, 32v32, 3735h1i1 : hv9 , 1i, 82 : hv10 , 1i, 11v7 v81 : |1, 1|, 371 : |1, 2|, 40v4h3ih4iv63, 11v71 : hv11 , 1i, 112 : hv12 , 1i, 1512h3ih2iv84, 17h1ih1i1 : hv13 , 1i, 172 : hv14 , 1i, 20h2ih1ih2iv9v10v11v12v13v14v155769121520Figure 3: Example working existing algorithmexisting method works input parameter k, i.e., number solutionsgenerated known priori. Also method inherently incrementalnature, thus perform efficiently solutions needed demand, e.g.,first, top 20 solutions needed, next 10 solutions needed. casetop 20 solutions recomputed computing next 10 solutions, i.e.,21st solution 30th solution. Next present proposed top-down approachsuffer limitation.3.2 Top-Down Evaluation Algorithms Generating Ordered Solutionsfar discussed existing approaches primarily use bottom-up approachcomputing ordered solutions. propose top-down approach generating alternative solutions non-decreasing order cost. may noted top-down283fiGhosh, Sharma, Chakrabarti, & Dasguptaapproach incremental nature. use edge marking based algorithm, AlternativeSolution Generation (ASG), generate next best solutions previously generated solutions. initial phase ASG algorithm, compute optimal solutiongiven alternating AND/OR tree perform initial marking edges.following terminology notions used describe ASG algorithm.context AND/OR trees, use eq denote edge points vertex vq .use following definitions describing proposed top-down approaches.Definition 3.c [Aggregated Cost] AND/OR DAG G , aggregated cost, ca ,edge eij node vi node vj , defined : ca (eij ) = ce (eij ) + Copt (vj ).v12, 34[e2 : 5]2,3 : 5h3i3, 293,4 : 1v2h1ih2i[e3 : 1]v32, 37v435h5i2, 8h1iv5h4iv63, 11h3iv7v84, 1712[e11 : 4][e9 : 3]h1i9,10 : 3h2ih2i[e13 : 3]h3i11,12 : 4h1i[e14 : 6] h1ih2i13,14 : 3 14,15 : 6v9v10v11v12v13v14v155769121520Figure 4: Example OR-edge marking swap optionMarking edge : notion marking edge follows.node vq , L(vq ) list edges vq sorted non-decreasing order aggregatedcost edges. define (i,i+1) difference cost edges, eiei+1 , ei ei+1 emanate node vq , ei+1 edge nextei L(vq ). Procedure MarkOR describes marking process edgesnode. Intuitively, mark represents cost increment incurred correspondingedge replaced solution next best sibling. edge maximumaggregated cost marked.Consider solution, Scur , containing edge ei = (vq , vi ), ei Eopt (Scur ).mark ei cost increment incurred construct next best solutionScur choosing another child vq . Figure 4 marks corresponding edgese2 , e3 , e9 , e11 , e13 , e14 [e2 : 5], [e3 : 1], [e9 : 3], [e11 : 4], [e13 : 3], [e14 : 6].284fiGenerating Ordered Solutions Explicit AND/OR StructuresProcedure MarkOR(vq )12345678Construct L(vq ) ; /* List edges vq sorted non-decreasing order cavalues */count number elements L(vq ) ;1 = count 1ec L(vq )[i] ;en L(vq )[i + 1] ;tmp = (ca (en ) ca (ec )) ;Mark ec pair [en : tmp ] ;endDefinition 3.d [Swap Option] swap option ij defined three-tuple hei , ej , ijei ej emanate node vq , ej edge next ei L(vq ),ij = ca (ej ) ca (ei ). Also, say swap option ij belongs node vq .Consider node vq sorted list L(vq ). may observed L(vq )every consecutive pair edges forms swap option. Therefore, k edges L(vq ),k1 swap options formed. node vq , swap options ranked accordingrank original edges L(vq ). Figure 4 swap options : (2,3) = he2 , e3 , 5i,(3,4) = he3 , e4 , 1i, (9,10) = he9 , e10 , 3i, (11,12) = he11 , e12 , 4i, (13,14) = he13 , e14 , 3i,(14,15) = he14 , e15 , 6i. Consider node v1 L(v1 ) = he2 , e3 , e4 i. Therefore, swapoptions, (2,3) (3,4) , belong v1 . node v1 , rank (2,3) (3,4) 1 2respectively.Definition 3.e [Swap Operation] Swap operation defined application swapoption ij = hei , ej , ij solution Sm contains edge ei following way:. Edge ea. Remove subtree rooted vi Sm . Let modified tree Smoriginal edge ij ., constructed previous step. Letb. Add subtree opt(vj ) Sm. Edge e swapped edge .newly constructed solution SmjijIntuitively, swap operation ij = hei , ej , ij constructs new solution SmSm contains edge ei . Moreover, cost Sm increased ij compared costSm C(Sm , vi ) = Copt (vi ).proposed algorithms use swap option based compact representation, named signature, storing solutions. Intuitively, alternative solution describedset swap operations performed optimal solution Sopt . interesting observeapplying ordered sequence swap options, h1 , , k i, applicationswap operation creates intermediate alternative solution. example,first swap option sequence, 1 , applied optimal solution, Sopt , new solution, say S1 , constructed. Then, 2nd swap option, 2 , applied S1 , yetanother solution S2 constructed. Let Si denote solution obtained applyingswap options, 1 , , , Sopt sequence. Although, ordered sequence swapoptions, like h1 , , k i, used compact representation alternativesolution, following key points important observe.A. Among possible sequences generate particular solution, need precludesequences contain redundant swap options (those swap options whose orig285fiGhosh, Sharma, Chakrabarti, & Dasguptainal edge present solution applied). formally definedlater superfluous swap options. Also order applying swap options another important aspect. two swap options, j 1 < j ksource edge j belongs sub-tree included solutionSi applying Si1 . case, apply j place , i.e.,apply j directly Si1 , effect source edge j presentSi1 , i.e., swapping location j sequence, j becomesredundant swap option solution constructed would different swappedsequence original sequence. formally define order relation pairswap options based observation later part section formalizecompact representation solutions based order relation.B. Suppose swap option j belongs node vpj . important observeapplication j Sj1 construct Sj , invalidates applicationswap options belong edge path root node vpjsolution Sj . Sj application swap optionbelongs edge path root node vpj would make swapvpj redundant. fact, swap option belonging node vpi , 1 j,application swap options belong edge pathroot node vpi invalidated solution Sj reason. conditionrestricts set swap options applied particular solution.C. Finally, two swap options j 1 < j kj independent other, is, (a) applying Si1 subsequentlyapplication j Sj1 , (b) applying j Si1 subsequently applicationSj1 , ultimately construct solution. happensoriginal edges j present Si1 , thus application one swap optioninfluence application other. However, desirable use oneway generate solution Sj . Section 3.3, propose variation top-downapproach (called LASG) resolves issue.Definition 3.f [Order Relation R] define order relation, namely R, pairswap options follows.a. path vi vr , ei er edges, qi rjswap options, (qi , rj ) R. example, Figure 4 ((3,4) , (13,14) ) R.b. pq = hep , eq , pq rt = , et , rt two swap options vq = vr ,(pq , rt ) R. Figure 4 ((2,3) , (3,4) ) R.Implicit Representation Solutions : use implicit representationstoring every solution optimal one. solutions constructedoptimal solution applying set swap options optimal solutionfollowing way. (i , j ) R, applied j . Therefore, every solutionrepresented sequence swap options, appears j (i , j ) R.Intuitively application every swap option specifies swapped edgepart solution. Since swap options applied specific order R, mayhappen edge become part solution due applicationearlier swap option may get swapped due application later swap option.286fiGenerating Ordered Solutions Explicit AND/OR StructuresDefinition 3.g [Superfluous Swap Option] Consider sequence swap options =h1 , , corresponding solution Sm . Clearly possible swap option, ,1 m, present sequence original edgepresent solution Si1 constructed successive applications swapoptions 1 , , i1 solution Sopt . application effect Si1 , i.e.,solution Si identical solution Si1 . swap option superfluous swapoption respect sequence swap options corresponding solution Sm .Property 3.1 sequence swap options corresponding solution minimal,superfluous swap option.property follows definition superfluous swap options notionimplicit representation solution.Definition 3.h [Signature Solution] minimal sequence swap options corresponding solution, Sm , defined signature, Sig(Sm ), solution.may noted optimal solution Sopt alternating AND/OR tree ,Sig(Sopt ) = {}, i.e., empty sequence. possible construct one signaturesolution, R partial order. important observe different signaturesparticular solution equal length sets swap options correspondingdifferent signatures also equal. Therefore set swap options correspondingsignature canonical representation signature. Henceforth use setnotation describing signature solution.v12, 392,3 : 5h3i3, 293,4 : 1h2iv3v2h1i2, 37v435h5i2, 8h1iv5h3ih4iv63, 11v7v84, 1712h2ih2ih1i9,10 : 3h3i11,12 : 4h1ih1ih2i13,14 : 3 14,15 : 6v9v10v11v12v13v14v155769121520Figure 5: solution, S2 , AND/OR tree shown Figure 4Figure 5 show solution, say S2 , AND/OR tree shown Figure 4.solution highlighted using thick dashed lines arrow head. pair, cv (vq ), C(S2 , vq ),287fiGhosh, Sharma, Chakrabarti, & Dasguptashown within rectangles beside node vq solution S2 , used rectangles rounded corner whenever C(S2 , vq ) 6= Copt (vq ). Since S2 generated applyingswap option (2,3) solution Sopt , signature S2 , Sig(S2 ) = h(2,3) i. Consideranother sequence, 2 = h(2,3) , (9,10) i, swap options. worth noting 2 alsorepresents solution S2 . second swap option 2 , namely 9,10 ,applied solution constructed applying (2,3) Sopt source edge (9,10) ,e9 , present solution. Hence (9,10) superfluous swap option 2 .Definition 3.i [Vopt Eopt ] solution graph Sm AND/OR DAG G ,define set nodes,fiVopt (Sm ), set edges, Eopt (Sm ), as:a. Vopt (Sm ) = vq fi vq Sm solution graph Sm (vq ) identical solution graphopt(vq )fib. Eopt (Sm ) = epr fi edge epr Sm , vr Vopt (Sm )Clearly, node vq Vopt (Sm ), vq present Sopt , (a) solution graphSm (vq ) identical solution graph Sopt (vq ), (b) C(Sm , vq ) = Copt (vq )Definition 3.j [Swap List] swap list corresponding solution Sm , L(Sm ), listswap options applicable Sm . Let Sig(Sm ) = {1 , , } i, 1 m,swap option belongs node vpi . application swap optionsbelong edges path root node vpi invalidated solutionSm . Hence, remaining swap options invalidated Sm appliedSm constructing successor solutions Sm .important observe swap option , source edge belongsEopt (Sm ), application invalidated Sm . Hence, solution Sm , construct L(Sm ) restricting swap operations edges belonging Eopt (Sm ).Moreover, condition also ensures cost newly constructed solutioncomputed directly form cost parent solution value applied swapconstructed formoption. elaborate, suppose solution Smapplying jk .) = C(S ) +cost Sm computed directly form C(Sm ) jk : C(Smjkej Eopt (Sm ). Procedure ComputeSwapList(Sm ) describes details computing swapoptions given solution Sm .Procedure ComputeSwapList(Sm)123456L(Sm ) ; Compute Eopt (Sm );foreach edge ec Eopt (Sm )exists swap option edge ec/* Suppose ec emanates node vq ec = L(vq )[i]. Also ecmarked pair htmp , en i, en = L(vq )[i + 1]*/cn hec , en , tmp i; Add cn L(Sm );endendswap list optimal solution, L(Sopt ), Figure 4, {(2,3) , (9,10) }.solution S1 , shown Figure 6, Vopt = {v6 , v10 }, except node v6 v10 ,nodes vi S1 , opt(vi ) 6= S1 (vi ). also rectangles rounded corner usedC(S1 , vq ) 6= Copt (vq ). Therefore, Eopt = {e6 , e10 }. Since exists swap option288fiGenerating Ordered Solutions Explicit AND/OR Structuresv12, 372,3 : 5h3i3, 323,4 : 1h2iv2v3h1i2, 37v435h5i2, 11h1iv5h4iv63, 11h3iv7v84, 1712h2ih1i9,10 : 3h2ih3i11,12 : 4h1ih1ih2i13,14 : 3 14,15 : 6v9v10v11v12v13v14v155769121520Figure 6: solution, S1 , AND/OR tree shown Figure 4edges, e6 e10 , swap list solution S1 , L(S1 ) = . Hence, solutionSm , L(Sm ) may empty, though Vopt (Sm ) never empty.Although use notation ij denote swap option edge ei originaledge edge ej swapped edge, succinct representation, also use singlesubscript, 3 , k , ij etc., represent swap option. alternative representationswap options relate edge.Definition 3.k [Successors Predecessors Solution] set successorspredecessors solutionfi Sm defined as:fi constructeda. Succ(Sm ) = {Smapplying swap optionbelongs swapfi list Sm }fi Succ(S )}b. P red(Sm ) = {SmProperty 3.2 solution Sm alternating AND/OR tree following state P red(S ), C(S ) C(S )ment holds: Smproperty follows definitions. One special case requires attention. Consider) = C(S ) P red(S ). case arise swapcase C(Smoption cost 0 applied Sm . occurs case tie.3.2.1 ASG Algorithmpresent ASG, best first search algorithm, generating solutions alternatingAND/OR tree non-decreasing order costs. overall idea algorithmfollows. maintain list, Open, initially contains optimal solution Sopt .point time Open contains set candidate solutions next best289fiGhosh, Sharma, Chakrabarti, & Dasguptasolution non-decreasing order cost selected. iteration minimum costsolution (Smin ) Open removed Open added another list, named, Closed.Closed list contains set ordered solutions generated far. successorset Smin constructed successor solution currently presentOpen well already added Closed inserted Open. Howeveroptimization, use sublist Closed, named TList, store relevant portion Closedchecking respect solutions TList sufficient figure whethersuccessor solution already added Closed. interesting observealgorithm interrupted time set ordered solutions computed farobtained. Also, algorithm resumed solutions needed.details ASG algorithm presented Algorithm 4.Algorithm 4: Alternative Solution Generation (ASG) Algorithm1234567891011121314151617181920input : alternating AND/OR treeoutput: Alternative solutions non-decreasing order costCompute optimal solution Sopt , perform edge marking populateswap options;Create three lists, Open, Closed, TList, initially empty;Put Sopt Open;lastSolCost C(Sopt );Open emptySmin Remove minimum cost solution Open ;lastSolCost < C(Smin )Remove elements TList;lastSolCost C(Smin );endAdd Smin Closed TList;Compute swap list, L(Smin ), Smin ;/* Construct Succ(Smin ) using L(Smin ) add new solutions Open*/foreach ij L(Smin )Construct Sm applying ij Smin ;Construct signature Sm , Sig(Sm ), concatenating ij Sig(Smin );/* Check whether Sm already present Open TList*/(Sm Open) (Sm TList)Add Sm Open;endendReport solutions Closed;pseudo-code Line-1 Line-4 computes optimal solution Sopt , performsmarking edges, populates swap options, initializes Open, Closed TList.loop Line-10 responsible generating new solution every time executedlong Open empty. Line-6 ASG algorithm, solutioncurrent minimum cost solution Open (Smin ) selected removed Open.TList populated maintained Line-7 Line-10. loop Line-13 generates290fiGenerating Ordered Solutions Explicit AND/OR Structuressuccessor solutions Smin one one adds newly constructed solutionsOpen newly constructed solution already present Open well addedTList (Line-16 checking). proof correctness Algorithm 4 presentedAppendix A. discuss following issues related Algorithm 4.Checking Duplication : order check whether particular solution Si alreadypresent Open TList, signature Si matched signatures solutionsalready present Open TList. sufficient check equalityset swap options respective signatures set unique particularsolution. may noted TList used optimization, avoids searchingentire Closed list.Resolving Ties : removing minimum cost solution Open list, tiemay encountered among set solutions. Suppose tie among set Stie ={S1 , , Sk }. ties resolved favor predecessor solutions, is,Si , Sj Stie , (If Si predecessor Sj ) (Si removed Sj )cases ties resolved arbitrarily favor solutionadded Open first.3.2.2 Working ASG Algorithmillustrate working ASG algorithm example AND/OR tree shownFigure 4. contents different lists obtained first iterations outermostloop shown Table 1. use signature solution representationpurpose. solutions already present Open also constructed expandingcurrent Smin , highlighted under-braces.It.1234Smin{}{(9,10) }{(2,3) }{(2,3) , (3,4) }L(Smin )(2,3) , (9,10)(3,4)(11,12) , (13,14)5{(2,3) , (3,4) ,(13,14) }(11,12) , (14,15)6{(2,3) , (3,4) ,(13,14)(11,12) }7{(2,3) , (3,4) ,(13,14) , (11,12) }(14,15)Open{(2,3) }, {(9,10) }{(2,3) }{(2,3) , (3,4) }{(2,3) , (3,4) , (11,12) },{(2,3) , (3,4) , (13,14) }{(2,3) , (3,4) , (11,12) },{(2,3) , (3,4) , (13,14) , (11,12) }{(2,3) , (3,4) , (13,14) , (14,15) }{(2,3) , (3,4) , (13,14) , (11,12) },|{z}{(2,3) , (3,4) , (13,14) , (14,15) }Closed{}{}, {(9,10) }{}, {(9,10) }, {(2,3) }{}, {(9,10) }, {(2,3) },{(2,3) , (3,4) }{}, {(9,10) }, {(2,3) },{(2,3) , (3,4) }{(2,3) , (3,4) , (13,14) }{}, {(9,10) }, {(2,3) },TList{}{(9,10) }{(2,3) }{(2,3) , (3,4) }{(2,3) , (3,4) ,(13,14) }{(2,3) , (3,4),{(2,3) , (3,4) }(11,12) }{(2,3) , (3,4) , (13,14) }{(2,3) , (3,4) , (11,12) }{(2,3) , (3,4) , (13,14) , (14,15) }, {}, {(9,10) }, {(2,3) },{(2,3) , (3,4) ,{(2,3) , (3,4) , (13,14) ,{(2,3) , (3,4) }(13,14) , (11,12) }(11,12) , (14,15) }{(2,3) , (3,4) , (13,14) }{(2,3) , (3,4) , (11,12) }{(2,3) , (3,4) , (13,14) ,(11,12) }Table 1: Working ASG Algorithm291fiGhosh, Sharma, Chakrabarti, & Dasguptaentering outermost loop (Line 5), ASG computes optimal solutionSopt , populates swap options, inserts Sopt Open. Thus, point time, Opencontains optimal solution Sopt ; Closed TList empty. first iterationSopt (the signature Sopt {}) selected removed Open. swap listSopt , L(Sopt ), computed. L(Sopt ), consists two swap options, namely (2,3) (9,10) .ASG adds two new solutions {(2,3) } {(9,10) } Open. solution Sopt addedClosed TList.next iteration, solution {(9,10) } minimum cost among solutionscurrently Open, selected removed Open, swap list {(9,10) } computedsubsequently {(9,10) } added Open TList. happens, L({(9,10) }) = (owingfact Eopt = {e6 , e10 } exists swap option edges, e6 e10 ),thus nothing else happens iteration. next iteration, solution {(2,3) } removedOpen ultimately solution {(2,3) , (3,4) } added Open adding {(2,3) }Closed well TList. Next two iterations proceed similar fashion. Now, consider6th iteration. iteration, solution {(2,3) , (3,4) , (11,12) } removed Open,successor set one solution, {(2,3) , (3,4) , (11,12) , (13,14) }, already presentOpen (inserted Open Iteration-5). Therefore, solution {(2,3) , (3,4) , (11,12) , (13,14) }inserted Open again. shown Iteration-7 Table 1.3.3 Technique Avoiding Checking Duplicates Opensection, present technique avoid checking done adding newlyconstructed solution Sm Open determine whether Sm already present Open.first explain scenario example, portion previous exampleshown Figure 4. Figure 7-10, solutions shown using thick dashed linearrow head. Also rectangles rounded corner used highlight factcorresponding node marked solution belong Vopt set solution.v42, 37h4i3, 11h2i4, 17h3i11,12 : 4h4ih3iv7h1iv42, 44v83, 15h1i13,14 : 3h3iv74, 20h3ih2iv8h1ih1iv11v12v13v14v11v12v13v14691215691215Figure 8: Solution S3Figure 7: Running ExampleConsider solutions S1 , S2 S3 (shown Figure 9, Figure 10 Figure 8).(a) L(Sopt ) = {(11,12) , (13,14) }, (b) Succ(Sopt ) = {S1 , S2 },(c) Sig(S1 ) = {(13,14) }, (d) Sig(S2 ) = {(11,12) }, (e) Sig(S3 ) = {(13,14) , (11,12) }.Algorithm 4 constructs solution S3 (shown Figure 8) adding Open twice(i) part adding Succ(S1 ) Open, (ii) adding Succ(S2 ) Open.292fiGenerating Ordered Solutions Explicit AND/OR Structuresv42, 40h4i3, 11h2i4, 20h3i11,12 : 4h4ih3iv7v42, 41v8h1i3, 15h1ih3iv74, 17h3ih2ih1iv8h1i13,14 : 3v11v12v13v14v11v12v13v14691215691215Figure 9: Solution S1Figure 10: Solution S2use following definitions describe another version ASG algorithm,constructs solutions way check find whether solution alreadyadded Open avoided.Definition 3.l [Solution Space DAG(SSDAG)] solution space DAG alternatingAND/OR tree directed acyclic graph (DAG), G = hV, Ei, V setpossible solutions AND/OR tree , E set edges defined as:fifi Sp , Sm V,fiE = espm fifi espm directed edge node Sp Sm ,fi Sm Succ(Sp )Clearly Sopt root node G .Definition 3.m [Solution Space Tree Completeness] solution space treealternating AND/OR tree tree = hV , E V V, V setpossible solutions AND/OR tree , E set edges definedas:fifi Sp , Sm V ,fifiedirectededgenode,pfi pmE = epm fifi Sp P red(Sm ),fi P red(Sm ), (Sp 6= ) edge Sm .pppsibling set solution Sm , denoted using Sib(T , Sm ). solution space treeAND/OR tree complete V = V.may noted complete solution space tree alternating AND/OR treenecessarily unique. possible alternating AND/OR treeone complete solution space tree. However solution space DAG AND/OR treeunique.Definition 3.n [Native Swap Options Solution] Consider solution Sm alternating AND/OR tree . Suppose Sm constructed applying swap option ijsolution Sp . Since swap option ij = hei , ej , ij used construct Sm , node vjpresent Sm . native swap options solution Sm respect swap option ij ,N (Sm , ij ), subset L(Sm ), comprises following swap options :293fiGhosh, Sharma, Chakrabarti, & Dasguptav12, 492,3 : 5h3i3, 323,4 : 1h2iv2v3h1i2, 43v435h5i2, 11h1iv5h3ih4iv63, 11v7v84, 2312h2ih1i9,10 : 3h2ih3i11,12 : 4h1ih1ih2i13,14 : 3 14,15 : 6v9v10v11v12v13v14v155769121520Figure 11: solution, S4 , AND/OR tree shown Figure 4a. jk , jk swap option edge ejb. , belongs node vq vq node Sm (vj )use term N (Sm ) denote native swap options ij understoodcontext. Intuitively native swap options solution Sm swap optionsbecome available immediately applying ij , available predecessorsolution Sm .Consider solution S4 shown Figure 11 Sig(S4 ) = {(2,3) , (3,4) , (13,14) }.solution highlighted using thick dashed lines arrow head. used rectangles rounded corner beside node vq solution S4 , C(S4 , vq ) 6= Copt (vq ).Suppose S4 constructed form solution S3 (where Sig(S3 ) = {(2,3) , (3,4) }) using swapoption (13,14) . N (S4 , (13,14) ) = {(14,15) } whereas L(S4 ) = {(11,12) , (14,15) }.consider solution S6 Sig(S6 ) = {(2,3) , (3,4) , (11,12) , (13,14) ). worth observing applying native swap options S4 instead swap options L(S4 )prevents construction solution S6 solution S4 . S6 also constructedapplying (13,14) solution S5 , Sig(S5 ) = {(2,3) , (3,4) , (11,12) }. However, maynoted (13,14) native swap option solution S5 .3.3.1 Lazy ASG Algorithmintuition behind version ASG algorithm follows. newlyconstructed solution Sm , need check whether Sm already present OpenSm constructed part computing successor set multiple solutions.Instead using entire swap list solution construct successorsadd solutions Open, using native swap options constructing subsetsuccessor set ensures following. subset constructed using native swap options294fiGenerating Ordered Solutions Explicit AND/OR Structuresconsists solutions currently present Open thusadded Open without comparing existing entries Open. constructionremaining successor solution Sminsertion Open delayedadded Closed.every predecessor solution SmAlgorithm 5: Lazy ASG (LASG) Algorithm12345678910111213141516171819202122232425262728input : alternating AND/OR treeoutput: Alternative solutions non-decreasing order costCompute optimal solution Sopt , perform edge marking populateswap options;Create two lists, Open Closed, initially empty;Put Sopt Closed list;Create solution space tree Sopt root;Compute swap list, L(Sopt ), Sopt ;Construct Succ(Sopt ) using L(Sopt );forall Sm Succ(Sopt )Add Sm Open;endOpen emptySmin Remove minimum cost solution Open ;/* Suppose Smin constructed Sm applying swap option ij*/Add node corresponding Smin connect node using edgeSm ;Compute swap list L(Smin ) list native swap options N (Smin , ij );/* Expansion using native swap options*/foreach tmp N (Smin , ij )Construct Stmp Smin applying tmp ;Construct signature Stmp , Sig(Stmp ), concatenating tmpSig(Smin );Add Stmp Open;end/* Lazy Expansion*/forall Sp Sib(T , Smin )ij L(Sp )Construct Sp Sp using ij ;Construct signature Sp , Sig(Sp ), concatenating ij Sig(Sp );Add Sp Open;endendAdd Smin Closed;endReport solutions Closed;solution space tree maintained throughout course algorithmadded Closed. Based ideadetermine every predecessor Sm295fiGhosh, Sharma, Chakrabarti, & Dasguptapresent lazy version ASG algorithm, named LASG. selecting minimum costsolution Open, algorithm explores successor set current minimum costsolution lazy fashion. solution Sm , first subset Succ(Sm ) constructedusing native swap options Sm . solutions belong Succ(Sm )explored late possible described above. resolving ties, LASG algorithmuses strategy used ASG algorithm. details LASG algorithmpresented Algorithm 5. proof correctness algorithm presentedAppendix B.Consider example tree shown Figure 7 solutions S1 S2 (shown Figure 9Figure 10). Initially Open contain Sopt N (Sopt ) = {(11,12) , (13,14) }.Sopt selected Open, S1 S2 added Open. Next S1 selectedfollowed S2 . Since, N (S1 ) = N (S2 ) = , selecting S1 S2 successorsolutions constructed using native swap list. Among predecessors S3 , S2added last Closed. selecting removing S2 Open, solution S3 constructedpreviously selected predecessor S1 using swap option (11,12) usedconstruct solution S2 Sopt .3.3.2 Working LASG Algorithm (on AND/OR tree Figure 4)entering outermost loop (Algorithm 5, Line 10), LASG computesoptimal solution Sopt constructs Succ(Sopt ).solutions Succ(Sopt )added Open contents Open becomes {(2,3) }, {(9,10) } . contentsdifferent lists solution added Closed shown Table 2. solutionsrepresented using signatures. solutions added Open resultlazy expansion, highlighted using under-brace.Iteration123Smin{}{(9,10) }{(2,3) }{(2,3) , (3,4) }N (Smin )(2,3) , (9,10)(3,4)(11,12) , (13,14)4{(2,3) , (3,4) , (13,14) }(14,15)5{(2,3) , (3,4) , (11,12) }6{(2,3) , (3,4) , (13,14) ,(11,12) }Open{(2,3) }, {(9,10) }{(2,3) }{(2,3) , (3,4) }{(2,3) , (3,4) , (11,12) },{(2,3) , (3,4) , (13,14) }{(2,3) , (3,4) , (11,12) },{(2,3) , (3,4) , (13,14) , (14,15) }Closed{}{}, {(9,10) }{}, {(9,10) }, {(2,3) }{}, {(9,10) }, {(2,3) },{(2,3) , (3,4) }{}, {(9,10) }, {(2,3) },{(2,3) , (3,4) }{(2,3) , (3,4) , (13,14) }{(2,3) , (3,4) , (13,14) , (14,15) }, {}, {(9,10) }, {(2,3) },{(2,3) , (3,4) , (13,14) , (11,12) }{(2,3) , (3,4) }|{z}{(2,3) , (3,4) , (13,14) }{(2,3) , (3,4) , (11,12) }{(2,3) , (3,4) , (13,14) , (14,15) }, {}, {(9,10) }, {(2,3) },{(2,3) , (3,4) }{(2,3) , (3,4) , (13,14) }{(2,3) , (3,4) , (11,12) }{(2,3) , (3,4) , (13,14) ,(11,12) }Table 2: Working LASG Algorithmgenerating first four solutions, contents different lists LASGidentical contents corresponding lists ASG (shown Table 1).296fiGenerating Ordered Solutions Explicit AND/OR Structuressoltuions, native swap list equal actual swap list solution. worth noting that, unlike ASG, LASG outermost loop startsgenerating optimal solution Sopt , thus generating solutioniteration number LASG less ASG 1. 4th iteration, solution S4 = {(2,3) , (3,4) , (13,14) } native swap list equal swap listdescribed previously. holds true solution S5 = {(2,3) , (3,4) , (11,12) }solution S6 = {(2,3) , (3,4) , (13,14) , (11,12) }. important observe LASG addssolution S6 = {(2,3) , (3,4) , (13,14) , (11,12) } Open generation solutionS5 = {(2,3) , (3,4) , (11,12) } part lazy expansion (highlighted using under-braceTable 2). Whereas, ASG algorithm adds S6 Open generating solutionS4 = {(2,3) , (3,4) , (13,14) }.3.4 Complexity Analysis Comparison among ASG, LASG BUsection present complexity analysis ASG LASG compareBU. use following parameters analysis.a. n n denote total number nodes number nodesalternating AND/OR tree.b. denotes degree node maximum number children.c. denotes maximum number edges solution.d. denotes maximum size Open. present complexity analysisgenerating c solutions. Therefore size Closed O(c).3.4.1 Complexity ASGTime Complexity : time complexity major steps Algorithm 4follows.a. Computing first solution done bottom-up fashion, thus requiring O(n )steps. edges emanating node sorted non-decreasing orderaggregated cost compute marks edges, marking process takesn .d. log . Since valuelarge general (can upper boundedconstant), n .d. log = O(n ).b. number swap options available solution equal numberedges solution. Thus, swap list every solution builtO(m) time. c solutions, generating swap options take O(c.m).c. Since size successor set solution most, size Open,c.m. Also size TList equal c (the sizeClosed).d. Open list implemented using Fibonacci heap. Individual insert deleteoperation Open take O(1)(amortized) O(lg o) time respectively. Hence,inserting Open deleting Open altogether takes O(o. lg o) timeO(c.m. log(c.m)).e. checking duplicates requires scanning entire Open TList. Sincelength TList c, newly constructed solution checking takesO(c + o) time O(c + o) solutions generated. Since O(c + o) actuallyO(o), generating c solutions, step takes O(o)2 time. Also, maximum value297fiGhosh, Sharma, Chakrabarti, & DasguptaO(c.m). Thus, time complexity step O(c.m)2 . Clearlystep dominates O(o. lg o) total time taken insertions Opendeletions Open.However, time bound improved maintain hash mapsolutions Open TList, case checking duplicatesdone O(o) time. case O(o. lg o) (total time taken insertionsOpen deletions Open) becomes dominant time required checkingduplicates.f. upper limit estimate could made estimating size solution treen regular complete alternating AND/OR trees. importantobserve value independent average degree node.Combiningtogether get time complexityASG algorithm :factors2222n + = n + (c.m) = n + c .n = O(c .n )Howeverif additionalhashreduced :map used time complexityn + o. lg = n + c. n . lg(c.n ) = n + n .(c. lg c + c. lg n )Space Complexity: following data-structures primarily contribute space complexity ASG algorithm.a. Three lists, namely, Open, Closed, TList maintained throughout courserunning ASG. contributes O(o + c) factor, O(o).b. Since number swap options upper bounded total number edges,constructing swap list contributes factor, O(n .d) space complexity.Also marking solution requires putting mark every node AND/ORtree, thus adding another O(n ) space clearly dominated previousO(n .d) factor.c. Since signature solution essentially set swap options, sizesignature upper bounded total number swap options available. CombiningOpen Closed list, altogether (c + o) solutions need stored.Since (c + o)O(o), total space required storing solutions o.n .d .Combiningfactorstogether get space complexity ASG algorithm :+ n .d + o.n .d = O(o.n .d)additionalhash map used improve time complexity, another additional o.n .d space required maintaining hash map. Although exact spacerequirement doubled, asymptotically space complexity remains same.3.4.2 Complexity LASGTime Complexity : Compared Algorithm 4, Algorithm 5 checkduplicates adds solution Open required. Thereforeterms complexity remain except term corresponding checkingduplicates. However, created maintained course Algorithm 5.Creating maintaining tree require O(c) time. Also lazy expansionswap list previously generated sibling solutions searched (Line 19 Line 20Algorithm 5). size swap list solution O(m), maximumnumber edges solution. Also O(m) sibling solutions298fiGenerating Ordered Solutions Explicit AND/OR Structuressolution. Therefore complexity lazy expansion O(c.m2 ). Since O(c.m2 )dominant factor, time complexity LASG O(c.m2 ) = O(c.n ).Space Complexity : Compared ASG algorithm, LASG algorithm maintainTList. However LASG maintains solution space tree whose size equalClosed list, thus adding another O(c) factor space complexity incurred ASGalgorithm. interesting observe worst case space complexity remains O(o +n .d + o.n .d) = O(o.n .d) equal space complexity ASG algorithm.3.4.3 Comparison BUtime complexity generating c best solutions AND/OR tree O(n .c. log c)space complexity O(n .c). detailed analysis found workElliott (2007). Since, n .d = O(n ), space complexity ASG LASGalgorithm reduces O(n .c) time complexity LASG log c factor betterBU whereas time complexity ASG quadratic respect c compared(c. log c) factor BU. additional hash-map used reduce time overheadduplicate checking, ASG beats LASGBU terms time complexity,O(n ) n .(c. lg c + c. lg n ) asymptotically lower O(n .c. log c).However worst case complexity possible AND/OR trees duplicate solution generated. Empirical results show length Open, hardly reachesO(c.m).4. Ordered Solution Generation AND/OR DAGssection, present problem generating solutions non-decreasing ordercost given AND/OR DAG. present working existing algorithmgenerating solution tree based semantics default semantics. Next presentmodifications ASG LASG handling DAG.4.1 Existing Bottom-Up AlgorithmFigure 12 shows example working existing bottom-up approach, BU,AND/OR DAG Figure 2. use notations used Figure 3 describedifferent solutions Figure 12 generation top 2 solutions tree-basedsemantics shown.important notice although BU correctly generates alternative solutionsAND/OR DAGs tree based semantics, BU may generate solutionsinvalid default semantics. Figure 13 present solution AND/OR DAGFigure 2. solution example solution correct tree-basedsemantics invalid default semantics. solution DAG (highlighted usingthick dashed lines arrow heads) Figure 13 generated 3rd solutionAND/OR DAG Figure 2 running BU. every non-terminal node, entry(within rectangle) corresponding 3rd solution highlighted using bold face. maynoted terminal nodes, v9 v10 , included solution DAG thoughemanate parent node. Therefore, solution valid onedefault semantics.299fiGhosh, Sharma, Chakrabarti, & Dasgupta2, 89v1v2 v31 : |1, 1|, 892 : |2, 1|, 90h1i3, 43v21 : hv5 , 1i, 432 : hv4 , 1i, 44v440h1iv71 : hv5 , 1i, 412 : hv5 , 2i, 443, 43h1iv7 v81 : |1, 1|, 352 : |2, 1|, 38v21 : hv9 , 1i, 92 : hv10 , 1i, 12v6v452401 : hv5 , 1i, 432 : hv4 , 1i, 44v52, 35v83, 9h1iv72, 41v3h4i1 : hv5 , 1i, 412 : hv5 , 2i, 44h1iv7 v81 : |1, 1|, 352 : |2, 1|, 38v652h3ih4i17h2ih2ih5ih1ih3ih4i3, 9v3h4iv52, 352, 41v1h1ih2ih5ih1i2, 89v2 v31 : |1, 1|, 892 : |2, 1|, 903 : |1, 2|, 921 : hv2 , 1i, 342 : hv2 , 2i, 37v817h2iv9v10v9v105757Figure 13: solution (tree based semantics)Figure 12: BU approach AND/OR DAGProposed Extension BU Generate Alternative Solutions DefaultSemantics : propose simple top-down traversal pruning based extensionBU generate alternative solutions default semantics. generating orderedsolutions node vq combining solutions children, following.newly constructed solution rooted vq , top-down traversal solutionstarting vq done check whether two edges node presentparticular solution (a violation default semantics). violationdefault semantics detected, solution pruned list alternative solutionsrooted vq . Therefore, every node, new solution constructed,additional top-down traversal used detect semantics violation.4.2 Top-Down Method DAGsproposed top-down approaches (ASG LASG) also applicable AND/ORDAGs generate alternative solution DAGs default semantics. methodcomputing cost increment application swap option needs modifiedincorporate fact node may included solution DAG multiplepaths root node. use notion participation count computing costincrement.Participation Count : notion participation count applicable intermediatenodes solution DAG follows. solution DAG, participation countintermediate node, vq , total number distinct paths connecting root node, vR ,vq . example, Figure 14, optimal solution DAG shown using thick dashedlines arrow heads, participation count every intermediate nodesshown within circle beside node.300fiGenerating Ordered Solutions Explicit AND/OR Structuresv1h2ih1i1h1iv23, 431h5i2,5,4 : 1v4v12, 892h4iv52h1iv7v32, 413,5,6 : 141h1i2, 35v23, 44v6v452401h5ih1ih4iv51117h2ih1i2, 413,5,6 : 14h1iv62, 35h3ih4iv8v352h3i3, 97,9,10 : 3h2ih1i40h4i2, 90v7v83, 97,9,10 : 317h2iv9v10v9v105757Figure 15: Solution DAG S1Figure 14: AND/OR DAGuse notation ijk denote swap option context AND/OR DAGs,swap option ijk belongs node vi , source edge swap option eijnode vi node vj , destination edge eik node vi node vk .4.2.1 Modification Proposed Top-Down ApproachASG algorithm modified handling AND/OR DAGs following way.computation successor solution Line 14 Algorithm 4 modified incorporateparticipation count node applied swap option belongs.overall method shown Algorithm 6(in next page).order apply LASG AND/OR DAGs, apart using mentionedmodification computing cost newly generated solution, another modificationneeded computing native swap options given solution. modificationexplained example. Consider solution, S1 , shown Figure 15. S1 highlightedusing thick dashed lines arrow heads. pair, cv (vq ), C(S1 , vq ), shown withinrectangles beside node vq ; rectangles rounded corner used C(S1 , vq ) 6=Copt (vq ). Swap option (2,5,4) applied Sopt generate S1 . applicationswap option (2,5,4) , participation count node v5 decremented 1. ThereforeS1 path root node node v5 node v5 still present S1 .result, swap option (7,9,10) available S1 participation count equal1 node v7 , whereas (7,9,10) available parent solution Sopt participationcount 2 node v7 . words, (7,9,10) available S1 parent solutionSopt value participation count node v7 . Therefore (7,9,10) becomesnative swap option S1 . generalized definition native swap options solutionpresented below.Definition 4.o [Native Swap Options Solution] Consider solution SmAND/OR DAG G , Sm constructed applying swap option hij solutionSp . Since swap option hij = hehi , ehj , hij used construct Sm , node vj belongs301fiGhosh, Sharma, Chakrabarti, & DasguptaSm . Similarly, participation count node vi remains greater zero applying hij Sm , node vi belongs Sm . native swap options solution Smrespect swap option hij , N (Sm , hij ), subset L(Sm ), comprises followingswap options :a. hjk , hjk swap option edge ehjb. , belongs node vq vq node Sm (vj )c. , node vi present Sm belongs node vq vqnode Sm (vi ).use term N (Sm ) denote native swap options hij understoodcontext. Intuitively native swap options solution Sm swap optionsbecome available immediately applying hij , available predecessorsolution Sm .Algorithm 6: ASG Algorithm AND/OR DAGsinput : AND/OR DAG Goutput: Alternative solutions G non-decreasing order cost1 Compute optimal solution Sopt , perform edge marking populateswap options;2 Create three lists, Open, Closed, TList, initially empty;3 Put Sopt Open;4 lastSolCost C(Sopt );5 Open empty6Smin Remove minimum cost solution Open;7lastSolCost < C(Smin )8Remove elements TList;9lastSolCost C(Smin );10end11Add Smin Closed TList;12Compute swap list, L(Smin ), Smin ;/* Construct Succ(Smin ) using L(Smin ) add new solutions Open*/13foreach ij L(Smin )14Construct Sm applying ij Smin ;15Construct signature Sm , Sig(Sm ), concatenating ij Sig(Smin );16Let ij belongs node vq , p participation count vq ,cost increment ij ;17C(Sm ) = C(Sm ) + p ;/* Check whether Sm already present Open TList*/18(Sm Open) (Sm TList)19Add Sm Open;20end21 end22 Report solutions Closed;worth noting Definition 4.o native swap option generalizationearlier definition native swap option (Definition 3.n), defined context trees.302fiGenerating Ordered Solutions Explicit AND/OR Structurescase trees, participation count node maximum 1. Therefore,application swap option solution, participation count node,original edge swap option points to, becomes 0. Therefore thirdcondition never applicable trees.LASG (Algo. 5) applied AND/OR DAGs, mentioned modificationcomputing cost newly generated solution general definition nativeswap option generate ordered solutions default semantics.4.2.2 Working ASG LASG Algorithm AND/OR DAGdescribe working ASG algorithm example DAG shown Figure 2.entering outermost loop, TList Closed empty, Open containsoptimal solution Sopt . contents different lists obtained first cyclesoutermost loop shown Table 3. solution represented signature.solutions already present Open also constructed expandingcurrent Smin , highlighted under-braces. example, solution {(2,5,4) , (3,5,6) }added Open Iteration 2 (while constructing successor solutions {(2,5,4) })constructed Iteration 5 expanding solution {(3,5,6) }.L(Smin )Open(2,5,4) , (3,5,6) , (7,9,10){(2,5,4) }, {(3,5,6) }, {(7,9,10) }(3,5,6) , (7,9,10){(3,5,6) }, {(7,9,10) }, {(2,5,4) , (3,5,6) },{(2,5,4) , (7,9,10) }3 {(2,5,4) , (7,9,10) }{(3,5,6) }, {(7,9,10) }, {(2,5,4) , (3,5,6) },It.12Smin{}{(2,5,4) }4{(7,9,10) }{(3,5,6) }, {(2,5,4) , (3,5,6) },5{(3,5,6) }(2,5,4) , (7,9,10){(2,5,4) , (3,5,6) }, {(3,5,6) , (7,9,10) }|{z}Closed{}{}, {(2,5,4) }{}, {(2,5,4) }{(2,5,4) , (7,9,10) }{}, {(2,5,4) },{(2,5,4) , (7,9,10) },{(7,9,10) }{}, {(2,5,4) },{(2,5,4) , (7,9,10) },{(7,9,10) }, {(3,5,6) }Table 3: Example Working ASG Algorithm DAG shown Figure 2illustrate working LASG algorithm example DAG shown Figure 2. contents different lists solution added Closed shownTable 4. worth noting solution S1 = {2,5,4 }, swap list L(S1 ) ={(3,5,6) , (7,9,10) } whereas native swap list N (S1 ) = {(7,9,10) }. solutionsadded Open result lazy expansion, highlighted using under-brace. example,Iteration 7 LASG adds solution S5 = {(2,5,4) , (3,5,6) } Open generationsolution S4 = {3,5,6 } part lazy expansion, whereas ASG algorithm adds S5Open generating solution S1 = {2,5,4 }.4.2.3 Generating Solutions Tree Based SemanticsUnlike default semantics, ASG LASG straight forward extensiongenerating solutions tree based semantics. Figure 13 show examplesolution valid tree based semantics, invalid default semantics,edges emanating form node v7 , namely e(7,9) e(7,10) ,303fiGhosh, Sharma, Chakrabarti, & DasguptaN (Smin )Open(2,5,4) , (3,5,6) , (7,9,10) {(2,5,4) }, {(3,5,6) }, {(7,9,10) }(7,9,10){(3,5,6) }, {(7,9,10) },{(3,5,4) , (7,9,10) }2 {(2,5,4) , (7,9,10) }{(3,5,6) }, {(7,9,10) },It.1Smin{}{(2,5,4) }3{(7,9,10) }{(3,5,6) }4{(3,5,6) }(7,9,10){(3,5,6) , (7,9,10) },{(2,5,4) , (3,5,6) }|{z}Closed{}{}, {(2,5,4) }{}, {(2,5,4) }{(2,5,4) , (7,9,10) }{}, {(2,5,4) },{(2,5,4) , (7,9,10) },{(7,9,10) }{}, {(2,5,4) },{(2,5,4) , (7,9,10) },{(7,9,10) }, {(3,5,6) }Table 4: Example Working LASG Algorithm DAG shown Fugure 2present solution. two edges included solution twodifferent paths emanating form root node, v1 . existing bottom-up approachstores alternative solutions node terms solutions childrennode, representation allows different paths stored explicitly, thus makingBU amenable generating alternative solutions tree-based semantics.contrary, approach works top-down using compact representation (signature) storing solutions. signature based representation, currentlypossible store fact particular node included solution twodifferent paths may select different child node. use equivalenttree constructed form given graph, compact representation work correctly, case, node would reachable root node onepath. AND/OR DAG converted equivalent AND/OR tree representationusing procedure ConvertDAG (described Section 2) ASG LASG applied equivalent tree representation order generate alternative solutionscorrectly tree-based semantics. However, worst case, procedure ConvertDAGincurs space explosion blow worst case complexity ASGLASG algorithms. Using compact representations generate ordered solutionstree-based semantics given AND/OR DAG containing space explosionworst case complexity algorithms remain comparable BU turnsinteresting open problem.5. Experimental Results Observationsobtain idea performance proposed algorithms compareexisting approach, implemented ASG, LASG BU (existing bottom-upapproach) tested following test domains.a. set synthetically generated AND/OR trees;b. Tower Hanoi (TOH) problem;c. set synthetically generated AND/OR DAGs;d. Matrix-chain multiplication problem;e. problem determining secondary structure RNA sequences.304fiGenerating Ordered Solutions Explicit AND/OR Structuresmay noted implementation ASG algorithm, implementedspace efficient version ASG algorithm (without separate hash-map storingsolutions Open Closed, thereby incurring extra overhead time duplicationchecking). Another important point every test case reported running timeASG LASG generating particular number solutions includes time requiredconstructing optimal solution graph. details different test domainsfollows.5.1 Complete Treesgenerated set complete d-ary alternating AND/OR trees varying (a)degree non-terminal nodes (denoted d), (b) height (denoted h).(d, h)(2, 7)(2, 9)(2, 11)(2, 13)(2, 15)(2, 17)(3, 5)(3, 7)(3, 9)(3, 11)(3, 13)(4, 5)(4, 7)(4, 9)(5, 5)(5, 7)(6, 5)(6, 7)(7, 5)(7, 7)100 solutionsASGLASGBU0.0270.0050.0040.2160.0100.0151.1700.0310.0686.0720.1240.25730.4340.5171.180130.746 2.2654.9520.0460.0060.0050.5280.0170.0375.8120.1060.34366.3131.5523.973636.822 12.363 31.0430.1440.0110.0332.9160.0560.57358.7561.2667.6980.3340.0120.08112.2270.1772.0660.6990.0220.16132.6200.6547.4641.3060.0300.28781.1971.786 15.892300 solutionsASGLASGBU0.0860.0140.0091.4480.0350.04610.0980.0940.18457.7570.3480.777278.453 1.4333.9176.44313.2770.1960.0150.0184.7640.0600.15355.1700.2901.733620.996 3.71214.32334.150 128.3141.0410.0250.09225.3410.1811.561544.989 3.32727.0632.7920.0360.400102.577 0.44311.7175.3840.0711.418288.257 1.56637.75812.0060.0921.833785.160 4.284 102.431500 solutionsASGLASGBU0.1860.0230.0204.1370.0600.09727.3540.2160.407158.520 0.5241.641766.201 2.8067.25710.306 29.7030.4590.0260.04210.3450.0880.457156.158 0.4944.9136.60733.92355.510 303.7852.6100.0420.12369.5960.2642.1075.17238.6067.3740.0620.930283.689 0.82726.99415.1330.1342.235832.235 2.59490.46529.8700.1794.3226.890 241.064Table 5: Comparison running time (in seconds) generating 100, 300, 500 solutionscomplete alternating AND/OR trees (T denotes timeout 15 minutes)trees viewed search space gift packing problem,(a) terminal nodes represent cost elementary items,(b) nodes model choice among items (elementary composite nature)represented children,(c) nodes model repackaging items returned children.Every packaging incurs cost modeled cost intermediate nodes.objective find alternative gifts order non-decreasing cost.Table 5 shows time required generating 100, 300, 500 solutions variouscomplete alternating AND/OR trees. implemented ASG, LASGexisting bottom-up algorithm corresponding running time shown columnheading ASG, LASG BU, respectively. used time limit 15 minutes305fiGhosh, Sharma, Chakrabarti, & Dasgupta(d, h)(2, 7)(2, 9)(2, 11)(2, 13)(2, 15)(2, 17)(3, 5)(3, 7)(3, 9)(3, 11)(3, 13)(4, 5)(4, 7)(4, 9)(5, 5)(5, 7)(6, 5)(6, 7)(7, 5)(7, 7)ASG12.63352.770116.582287.898664.7891785.15617.27082.609335.3011474.4779139.31240.285213.8161563.77064.879529.73897.7031264.828137.5272628.461100 solutionsLASGBU13.16811.04726.15248.48463.254198.234173.730797.234413.8553193.2341257.387 12777.23417.25811.68848.086111.438184.3751009.1881071.352 9088.9387872.055 81806.68824.46947.453128.629767.4531158.582 12287.45340.35588.281343.2542217.18858.191151.047862.3325449.79790.703242.2191995.195 11882.781ASG28.105144.730341.227832.5621767.86747.531235.855926.0043234.523121.336559.7343209.145182.2701254.715270.0272747.238369.0864869.551300 solutionsLASGBU32.29314.26675.35569.953165.824292.703399.4451183.703804.8014747.7032047.859 19003.70349.23014.812134.102152.062376.7661387.3121656.844 12504.5629565.598 112559.81267.102112.609284.9221826.3591699.191 29246.359110.480225.781596.9575675.000148.453372.1411273.641 13433.391205.914576.5942627.211 28295.281ASG41.676230.168566.7661396.7582942.62976.270393.1131507.973199.254917.824305.8912008.344443.6564203.957606.133500 solutionsLASGBU49.83216.609128.93487.922269.766373.172612.1841514.1721197.2666078.1722849.61724334.17280.98017.938219.555192.688577.7661765.4382238.15215920.18811251.035 143312.938116.535129.016451.2232105.2662240.01233725.266179.801363.281858.8529132.812245.227593.2341695.68421416.984317.492910.9693273.70344707.781Table 6: Comparison space required (in KB) generating 100, 300, 500 solutionscomplete alternating AND/OR treesentries marked denotes time-out occurred test cases.space required generating 100, 300, 500 solutions reported Table 6.observed terms time space required, LASG outperforms ASGBU. ASG BU, test cases BU performs better ASGrespect time required generating specific number solutions. spacerequirement ASG BU generating specific number solutions interestingcorrelation degree(d) height(h) parameter tree. low numerical valuesh parameter, e.g., (d, h) combinations like (2, 7), (3, 5) etc., BU performsbetter ASG. contrary, combinations, least oneh parameters high value, e.g., (d, h) combinations like (2, 17), (7, 5), (4, 9) etc.,ASG outperforms BU.5.1.1 Experimentation Queue Bounded LengthSince Open grow rapidly, ASG LASG incur significant overheadterms time well space maintain Open list number solutionsgenerated known priori. fact, ASG checking duplicates Openactually primary source time complexity storing solutions Open majorcontributing factor space complexity. number solutions generatedknown priori, proposed top-down approach leverage fact using boundedlength queue implementing Open. bounded length queue used, timerequirement along space requirement decreases significantly.306fiGenerating Ordered Solutions Explicit AND/OR Structures(d, h)(2, 7)(2, 9)(2, 11)(2, 13)(2, 15)(2, 17)(3, 5)(3, 7)(3, 9)(3, 11)(3, 13)(4, 5)(4, 7)(4, 9)(5, 5)(5, 7)(6, 5)(6, 7)(7, 5)(7, 7)100 solutionsASGLASGBU0.0110.0080.0040.0300.0110.0150.0510.0310.0680.1250.1030.2570.4730.4211.1802.1292.1994.9520.0120.0090.0050.0310.0180.0370.1330.1020.3431.2461.1433.97310.713 10.313 31.0430.0190.0080.0330.0710.0550.5731.0990.9987.6980.0250.0130.0810.2010.1612.0660.0360.0180.1610.5430.4607.4640.0420.0290.2871.9401.705 15.892300 solutionsASG LASGBU0.003 0.0020.0090.008 0.0060.0460.020 0.0110.1840.043 0.0590.7770.168 0.1643.9170.766 1.005 13.2770.003 0.0020.0180.012 0.0060.1530.048 0.0431.7330.477 0.636 14.3234.160 5.555 128.3140.006 0.0040.0920.026 0.0231.5610.443 0.552 27.0630.009 0.0310.4000.083 0.078 11.7170.014 0.0111.4180.240 0.325 37.7580.020 0.0131.8330.807 0.843 102.431500 solutionsASG LASGBU0.005 0.0040.0200.014 0.0080.0970.023 0.0170.4070.065 0.0581.6410.254 0.3467.2571.146 1.492 29.7030.005 0.0040.0420.019 0.0100.4570.071 0.0614.9130.693 0.905 33.9236.013 7.890 303.7850.010 0.0060.1230.038 0.0332.1070.641 0.808 38.6060.015 0.0080.9300.116 0.153 26.9940.021 0.0102.2350.326 0.431 90.4650.025 0.0224.3220.870 1.125 241.064Table 7: Comparison running time (in seconds) generating 100, 300, 500 solutionscomplete alternating AND/OR trees bounded length Open queue ASGLASG(d, h)(2, 7)(2, 9)(2, 11)(2, 13)(2, 15)(2, 17)(3, 5)(3, 7)(3, 9)(3, 11)(3, 13)(4, 5)(4, 7)(4, 9)(5, 5)(5, 7)(6, 5)(6, 7)(7, 5)(7, 7)ASG10.10923.87554.609135.477361.8591071.25812.00839.469169.469971.9307075.10920.664116.6091082.63333.344324.25851.742825.85978.1411919.805100 solutionsLASGBU2.38311.0474.88348.48414.883198.23454.883797.234214.8833193.234854.883 12777.2342.61711.68811.160111.43888.0471009.188780.0279088.9387007.852 81806.6885.01647.45357.016767.453889.016 12287.45310.19588.281217.7152217.18819.773151.047657.6485449.79735.742242.2191677.051 11882.781ASG27.78164.430141.203317.445738.9921845.56234.609101.320353.4771529.0318763.02356.703247.3201607.85984.422565.531121.0311227.742169.2972542.047300 solutionsLASGBU5.50814.2668.00869.95318.008292.70358.0081183.703218.0084747.703858.00819003.7035.74214.81214.285152.06291.1721387.312783.15212504.5627010.977 112559.8128.141112.60960.1411826.359892.14129246.35913.320225.781220.8405675.00022.898372.141660.77313433.39138.867576.5941680.176 28295.281ASG45.789104.117225.969497.5081114.4222615.65657.617163.102537.3282085.36710457.79793.031377.9222132.516135.812806.797190.7581628.797260.4063163.438500 solutionsLASGBU8.63316.60911.13387.92221.133373.17261.1331514.172221.1336078.172861.13324334.1728.86717.93817.410192.68894.2971765.438786.27715920.1887014.102 143312.93811.266129.01663.2662105.266895.26633725.26616.445363.281223.9659132.81226.023593.234663.89821416.98441.992910.9691683.301 44707.781Table 8: Comparison space required (in KB) generating 100, 300, 500 solutionscomplete alternating AND/OR trees bounded length Open queue ASGLASG307fiGhosh, Sharma, Chakrabarti, & Dasguptashow effect using bounded length queue implement Open Table 7 (reporting time requirement) Table 8 (reporting memory usage) generating100, 300, 500 solutions, number solutions generated known beforehand. Table 7 Table 8 show case ASG LASG outperformsBU terms time well space requirements. Particularly, ASG performs wellsetting, outperforming LASG cases.5.1.2 Experimentation Compare Incremental Natureproposed top-down algorithms incremental nature whereas existing bottomup approach incremental. generating specified number ordered solutions,methods generate next solution incrementally without needing restart itself,whereas existing approach needs restarted. example, generatingfirst 10 ordered solutions, ASG LASG generate 11th solution directly datastructures maintained far algorithms perform necessary updatesdata structures. Whereas, BU needs restarted input parameter 11 generating11th solution. Table 9 compare time needed generate subsequent 11thsolution 12th solution incrementally generating first 10 solutions. orderclarity comparison among running times respective algorithms,used higher precision (upto 6th decimal place) reporting running timeTable 9. Clearly, ASG LASG outperform BU generating 11th 12thsolution terms time requirement.(d, h)(2, 7)(2, 9)(2, 11)(2, 13)(2, 15)(2, 17)(3, 5)(3, 7)(3, 9)(3, 11)(3, 13)(4, 5)(4, 7)(4, 9)(5, 5)(5, 7)(6, 5)(6, 7)(7, 5)(7, 7)first 100.0024030.0091110.0285190.0972810.3964601.5610200.0016920.0120970.0973560.9343897.8985300.0058330.0515980.8130280.0515300.1724750.0534220.5029390.0338311.198354ASG11th0.0002010.0019570.0033110.0147760.0636410.2518390.0001580.0015420.0130460.1279431.0823190.0006500.0069560.1102050.0013270.0242620.0027010.0614170.0037060.15614512th0.0002010.0013020.0035330.0159290.0592290.2777630.0001510.0015720.0144050.1565791.1940900.0006710.0071960.1247500.0016410.0244380.0030920.0697270.0038460.166501first 100.0010030.0030230.0067000.0258770.1024930.4468990.0006830.0040840.0311590.3111282.8115390.0021430.0170460.2945610.0046380.0597510.0052820.1845840.0128620.466560LASG11th0.0002400.0007140.0012500.0041130.0144900.0614220.0001760.0005830.0039480.0331690.2828360.0003030.0022090.0276120.0007530.0061160.0006360.0171160.0012660.03879212th0.0001230.0006290.0013460.0049180.0200310.0823660.0001120.0009590.0046040.0475940.3877150.0005820.0031150.0372810.0006520.0071970.0010870.0240420.0012820.061305first 100.0013440.0035960.0146280.0593260.2384180.9626350.0010550.0095070.0856100.7782987.0370500.0041810.0449130.7277660.0059630.1522850.0108950.4069470.0181850.929941BU11th0.0013590.0036960.0150460.0613930.2460420.9897770.0011010.0099310.0893790.8111767.3137150.0044340.0478670.7759500.0063580.1625270.0116040.4353980.0195670.98932612th0.0013970.0038950.0155210.0627170.2517461.0158480.0011330.0103360.0934190.8465787.6086190.0047250.0509400.8234420.0067820.1731910.0125560.4653010.0208961.052566Table 9: Comparison running time (in seconds) generating first 10 solutions11th solution 12th solution incrementally complete alternatingAND/OR trees308fiGenerating Ordered Solutions Explicit AND/OR Structures5.2 Multipeg Tower Hanoi ProblemConsider problem Multipeg Tower Hanoi (Majumdar, 1996; Gupta, Chakrabarti,& Ghose, 1992). problem, pegs fastened stand. Initially disks restsource peg small disk large disk ordering. objective transferdisks destination peg B minimum legal moves. legal move,topmost disk tower transferred peg larger disktopmost disk. problem multi-peg tower Hanoi solved recursively follows.a. Move recursively topmost k (k varies 1 1) disksintermediate peg, I, using pegs.b. Transfer remaining k disks B recursively, using ( 1) pegsavailable.c. Recursively move k disks transferred previously, intermediatepeg B, using pegs.may noted choice value k, may take value 11. Solutions different values k may take different number moves,solution incurs minimum number moves optimal solution. choicevalue k modeled node, every choice, problem dividedthree sub-problems. decomposition sub-problems modelednode. Therefore, search spaces multi-peg Tower Hanoi problem correspondalternating AND/OR trees.#disks8910111213100 solutionsASGLASGBU0.0340.0300.0690.1190.1160.2640.4790.6351.3102.4212.1783.1717.4537.448 11.43725.379 25.115 38.458300 solutionsASGLASGBU0.1040.0840.2520.3140.2890.9421.7061.6583.3056.5736.16112.99821.232 21.081 43.35868.574 67.170 140.392500 solutionsASGLASGBU0.2000.1380.5770.5900.4582.1832.3032.8297.59210.6519.67829.24235.82535.66399.593112.411 112.470 332.113#Opt. No.Moves232731394755Table 10: Comparison running time (in seconds) alternating AND/OR trees corresponding search spaces 5-peg Tower Hanoi problem differentnumber disks#disks8910111213100 solutionsASGLASGBUASG36.66443.008416.31264.51696.211111.3201471.656131.266295.672341.0005074.219326.352957.3361113.508 17197.312999.6023155.086 3664.117 57512.8123198.15610339.078 12022.883 190297.969 10412.242300 solutionsLASGBUASG80.734660.062105.039154.2662359.156166.789383.4538161.719373.4531158.797 27728.5621039.3673719.352 92906.5623247.54712078.914 307872.969 10483.570500 solutionsLASGBU117.008903.812197.8593246.656427.76611249.2191204.719 38259.8123767.617 128300.31212137.242 425447.969Table 11: Comparison space required (in KB) alternating AND/OR trees corresponding search spaces 5-peg Tower Hanoi problem different numberdisksused search space 5 peg Tower Hanoi problem different numberdisks, , generated alternative solutions non-decreasing order cost using ASG309fiGhosh, Sharma, Chakrabarti, & DasguptaLASG algorithms. cost function expresses number legal moves. valuevaried 8 13, Table 10 Table 11, report time requiredspace required, respectively, generating 100, 300, 500 solutions every test cases.Experimental results show performance ASG similar performanceLASG respect space time. However ASG well LASG outperformsBU respect time space requirements.5.3 Randomly Constructed AND/OR DAGsconstructed set randomly generated AND/OR DAGs evaluated ASG,LASG, BU algorithm generating solutions default semantics. usedproposed extension BU algorithm generating solutions default semantics.n60220920334042124962414474488444088422233334444100 solutionsASG LASGBU0.027 0.006 0.0390.060 0.009 0.0960.363 0.020 0.1060.020 0.006 0.0190.203 0.018 0.0673.550 0.045 0.73026.659 0.201 14.6200.065 0.008 0.0340.877 0.025 0.4007.422 0.160 26.6831.972300 solutionsASGLASGBU0.0890.021 0.1580.2810.030 1.1002.4850.059 0.2660.1230.021 0.0981.4830.048 0.25730.302 0.126 1.681257.605 0.612 33.3820.3480.027 0.2176.9100.069 0.99469.097 0.449 66.5585.819500 solutionsASGLASGBU0.1720.0330.2820.5940.0513.6656.1630.1000.5280.2800.0320.2454.0430.0830.54185.863 0.2152.766710.708 1.194 52.4060.8170.0492.25118.823 0.1181.365194.452 0.927 109.0769.426Table 12: Comparison running time (in seconds) generating 100, 300, 500 solutions AND/OR DAGs (T denotes timeout 15 minutes)n60220920334042124962414474488444088422233334444ASG11.60923.14174.08213.91448.867229.820772.44130.648121.535471.6252722.938100 solutionsLASGBU8.8758.12516.21931.31239.000106.87510.4928.17235.44566.938118.707389.844339.676 1996.87517.33229.60965.578287.109266.078 2729.2971256.535ASG32.85262.516220.64846.117151.363705.8092245.93885.781381.1331183.379300 solutionsLASGBU30.79710.90646.71149.562105.852172.56232.53911.297101.16898.188312.246621.094825.984 3321.87553.96173.359168.305737.891550.477 6945.7032353.562ASG54.094100.555371.34477.445262.8161200.3363732.523140.312659.4341927.961500 solutionsLASGBU50.03513.25074.37965.188168.375230.37554.60214.422163.273129.438507.762852.3441327.406 4646.87593.53986.641275.594883.984843.484 8419.9223447.809Table 13: Comparison space required (in KB) generating 100, 300, 500 solutionsAND/OR DAGsTable 12 Table 13 compare time required space required running ASG,LASG BU generating 100, 300, 500 solutions every test cases. firstsecond columns every row provide size (n ) average out-degree (d)DAG. results obtained test domain similar results randomly310fiGenerating Ordered Solutions Explicit AND/OR Structuresconstructed AND/OR trees. may noted terms time space required,LASG outperforms ASG BU. ASG BU, test casesBU performs better ASG respect time required generating specificnumber solutions. Whereas, space requirement ASG BU generatingspecific number solutions interesting co-relation average degree(d)size (n ) parameter DAG. low numerical values nparameter, e.g., (n , d) combinations like (60, 2), (33, 3) etc., BU performs betterASG. contrary, combinations, least one nparameter high value, e.g., (n , d) combinations like (920, 2), (9624, 3), (40884, 4)etc., ASG outperforms BU.5.4 Matrix-Chain Multiplication Problemalso used well-known matrix-chain multiplication (Cormen, Stein, Rivest, &Leiserson, 2001) problem experimentation. search space popular dynamicprogramming formulation problem correspond AND/OR DAG.DAGCnstr.#matricesTime(Sec)200.033300.200400.898503.033608.3357019.5918041.9609082.578100151.814SoptCnstr.Time(Sec)0.0010.0030.0080.0160.0290.0460.0710.1010.14310 solutions15 solutions20 solutionsASGLASGBUASGLASGBUASGLASGBU0.0030.0090.0190.0470.0880.1400.2090.2960.4090.0020.0080.0180.0480.0900.1420.2120.3000.4120.2062.78515.58093.267342.212862.3870.0040.0120.0240.0620.1180.1870.2800.3960.5460.0030.0100.0240.0650.1200.1900.2820.3980.5480.2884.08723.414140.513509.9060.0050.0150.0300.0790.1480.2350.3510.4960.6880.0040.0120.0300.0810.1510.2380.3540.4990.6830.3735.40631.112187.227678.718Table 14: Comparison time required (in seconds) AND/OR DAGs correspondingsearch spaces matrix-chain multiplication different number matrices, (T denotes timeout 15 minutes)#matrices2030405060708090100ASG19.64166.367156.559308.984537.383859.8441290.1171843.8282537.58210 solutionsLASG20.20369.273160.227315.012545.117869.1601301.4061857.4802556.883BU160.918555.6841317.6372563.9654411.8556978.496ASG20.54367.809157.738310.277538.930862.1331293.1481847.6022542.74615 solutionsLASG21.22770.695161.785316.543546.512870.8671303.4261859.8122560.043BU234.305821.9021960.2813825.2236592.508ASG21.91469.516158.758311.551539.914863.9771295.8521851.1642549.35220 solutionsLASG22.77372.523162.852318.145547.551872.2191305.0901861.7892566.992BU303.9731081.9022594.2075075.2628759.441Table 15: Comparison space required (in KB) AND/OR DAGs correspondingsearch spaces matrix-chain multiplication different number matricesGiven sequence matrices, A1 , A2 , , , n matrices matrix Ai dimension pi1 pi , problem objective find efficient way multiply311fiGhosh, Sharma, Chakrabarti, & Dasguptamatrices. classical dynamic programming approach works follows. SupposeA[i,j] denotes matrix results evaluating product, Ai Ai+1 Aj , m[i, j]minimum number scalar multiplications required computing matrix A[i,j] .Therefore, cost optimal solution denoted m[i, j] recursively defined:m[i, j] =0,minik<j= j;m[i, k] + m[k + 1, j] + pi1 pk pj , < j.choice value k modeled node every choice, problemdivided three sub-problems. decomposition sub-problems modelednode. worth noting unlike search space 5-peg ToH problem,search space matrix-chain multiplication problem corresponds AND/OR DAG.used search space different matrix sequences varying lengthgenerated alternative solutions order non-decreasing cost. Table 14, reporttime required Table 15, report memory used generating 10, 15,20 solutions every test cases.Table 14, test case, also report time required constructingexplicit AND/OR DAG recursive formulation 2nd column, optimalsolution construction time 3rd column. interesting observe relativeperformance ASG LASG search space similar obtained 5peg ToH search space though search space domain AND/OR DAG. ASGLASG perform approximately respect time space requirement.However, advantage ASG well LASG BU respect timespace requirement significant domain.5.5 Generating Secondary Structure RNAAnother relevant problem alternative solutions play important rolecomputation secondary structure RNA. RNA molecules viewed stringsbases, base belongs set {Adenine, Cytocine, Guanine, U racil} (alsodenoted {A, C, G, U }). RNA molecules tend loop back form base pairsresulting shape called secondary structure (Mathews & Zuker, 2004). stabilitysecondary structure largely depends number base pairings (in general, largernumber base pairings implies stable secondary structure). Althoughfactors influence secondary structure, often possible expressfactors using cost function typically evaluated empirically. Therefore,useful generate set possible alternative secondary structures ordered decreasingnumbering base pairings given RNA subjected experimentalevaluation.computation optimal secondary structure considering underlying principle maximizing number base-pairings nice dynamic programming formulation (Kleinberg & Tardos, 2005). Given RNA molecule B = hb1 b2 bnbi {A, C, G, U }, secondary structure B set base pairings, = {(i, j)},i, j {1, 2, n}, satisfies following conditions:312fiGenerating Ordered Solutions Explicit AND/OR StructuresTest CaseTC1TC2TC3TC4TC5TC6TC7TC8TC9TC10TC11TC12TC13TC14Organism NameAnaerorhabdus FurcosaArchaeoglobus FulgidusChlorobium LimicolaDesulfurococcus MobilisHaloarcula JaponicaHalobacterium Sp.Mycoplasma GenitaliumMycoplasma HyopneumoniaeMycoplasma PenetransPyrobaculum AerophilumPyrococcus AbyssiSpiroplasma MelliferumSulfolobus AcidocaldariusSymbiobacterium Thermophilum# Bases114124111129122120104105103131118107126110Table 16: Details RNA sequences used Experimentationa. (i, j) D, + 4 < j : condition states ends pairseparated least four intermediate bases.b. elements pair consists either {A, U } {C, G} (in either order).c. base appears one pairings, i.e., matching.d. (i, j) (k, l) two pairs D, possible < k < l < j, i.e.,two pairings cross other.TestCaseTC1TC2TC3TC4TC5TC6TC7TC8TC9TC10TC11TC12TC13TC14DAG Cnstr.Time (Sec)34.46457.99926.42383.94351.29046.50816.76622.77518.83191.41947.66022.64967.91328.911Sopt Cnstr.Time (Sec)0.0420.0570.0380.0650.0510.0470.0290.0330.0310.0730.0470.0340.0610.038ASG0.0940.1260.0840.1440.1140.1070.0680.0770.0680.1670.1110.0780.1400.0875 solutionsLASGBU0.095 449.9160.128 823.4930.089 363.4210.152 1089.4620.116 681.4290.108 598.4190.069 210.8060.078 284.4550.072 233.9990.1700.109 627.7440.079 288.5200.141 962.6410.085 366.693ASG0.1450.1930.1350.2300.1760.1660.1010.1200.1090.2490.1730.1160.2060.13410 solutionsLASGBU0.148 893.6820.1980.133 718.3260.2270.180 1349.1810.1750.103 410.8170.122 559.3180.111 458.2900.2630.171 1253.0340.123 573.6020.2180.137 724.113ASG0.1970.2710.1830.3140.2390.2260.1360.1530.1440.3470.2200.1650.2900.18215 solutionsLASGBU0.202 1359.7590.2770.186 1077.0940.3170.2450.2380.144 621.7920.165 836.3590.148 683.4110.3550.2400.167 849.1340.2880.186 1072.552Table 17: Comparison time required (in seconds) AND/OR DAGs correspondingsearch spaces RNA secondary structure different number bases (Tdenotes timeout 30 minutes)mentioned conditions dynamic programming formulation follows.Suppose P (i, j) denotes maximum number base pairings secondary structurebi bj . P (i, j) recursively defined :P [i, j] =0,nmax P [i, j 1], max 1 + P [i, k 1] + P [k + 1, j 1] ,ik<j313+ 4 j,+ 4 < j.fiGhosh, Sharma, Chakrabarti, & DasguptaHere, choice value k modeled node every choice,problem divided three sub-problems. decomposition sub-problemsmodeled node. experimented search space problemset RNA molecule sequences obtained test-cases developed Szymanski,Barciszewska, Barciszewski, Erdmann (2005). details test cases shownTable 16.every test cases, report time required Table 17 generating 5, 10, 15solutions. setting, space required reported Table 18. Table 17,test case, also report time required constructing explicit AND/OR DAGrecursive formulation 2nd column, time required constructingoptimal solution time 3rd column. use high value time-out (1800 seconds)order gather running time required BU. limit maximum solutions generated15 generating higher number solutions, BU timedtest cases. worth noting result obtained domain similarresult obtained matrix-chain multiplication problem domain. space timewise ASG LASG perform similarly outperform BU significantly respecttime well space requirement.TestCaseTC1TC2TC3TC4TC5TC6TC7TC8TC9TC10TC11TC12TC13TC14ASG1647.5552254.5311473.8522606.2422045.9301912.2271101.1251293.8121170.0942984.7731974.6951295.1412438.8981475.4775 solutionsLASG1694.6882310.0081516.9222665.8202097.4141963.3671138.6331333.3361207.6333047.5392022.9061335.8832496.4691517.828BU7409.3369902.9536629.89111358.9459021.2738499.5705087.6805855.5475352.4778641.4225924.66410657.9456627.844ASG1651.2732258.7731477.4922610.8752049.8441916.4221104.4221297.7501173.0232990.2111979.3441297.2732442.9611478.55510 solutionsLASG1697.7972315.2581521.7502671.7112101.8361968.3051142.0231338.0701211.5233053.9772030.9221339.5162502.6251521.352BU14656.46913103.49217875.43010036.93811560.20310562.76617119.82011701.69513099.055ASG1654.3672262.4921480.5552615.7192052.8671921.1171108.0471302.2421176.3522994.7731983.6641299.8052447.1721482.23415 solutionsLASG1700.4922318.0081526.7972675.6332106.0001972.1721144.1091342.4841213.9063059.7812038.4611341.9142506.7031525.344BU21846.15619518.62514924.82017211.40615718.61717420.71919519.742Table 18: Comparison space required (in KB) AND/OR DAGs correspondingsearch spaces RNA secondary structure different number bases5.6 Observationsexperimental data shows LASG algorithm generally outperforms ASGalgorithm existing bottom-up approach terms running time completealternating AND/OR trees AND/OR DAGs. Whereas, problem domains,i.e., 5-peg Tower Hanoi problem, matrix-chain multiplication problem,problem determining secondary structure RNA sequences, overall performanceASG algorithm similar performance LASG algorithm. behaviorexplained average maximum length statistics Open list, reportedTable 19 - Table 23, mentioned test domains.314fiGenerating Ordered Solutions Explicit AND/OR Structurescase complete trees random DAGs, ASG algorithm, average wellmaximum size Open grows much faster LASG algorithm (Table 19Table 20), increase size tree/DAG.(d, h)(2, 7)(2, 9)(2, 11)(2, 13)(2, 15)(2, 17)(3, 5)(3, 7)(3, 9)(3, 11)(3, 13)(4, 5)(4, 7)(4, 9)(5, 5)(5, 7)(6, 5)(6, 7)(7, 5)(7, 7)100 solutionsASGLASGavg.max.avg. max.235383751599941894731202427470915630655461094752411491174423291384523242644833365584130454912024215613015172346549610899191289173363454248669153139 1061551138 1216734142710317637487383194381162823245142248812162352146307726114446249335178134891412761236224651297342243347652615081931138435450529300 solutionsASGLASGavg.max.avg. max.43562917928926574931220528693513537483100516266320761550 2726348366916067711211087 16117401323341652435984005791260162723224438766151954 10354995617541267 156920624006256503109282148967814674878697196687113134076555496105321652429724708885089991150711263686873323461789707213910747148357754 115116687961500 solutionsASGLASGavg.max.avg. max.5457922363724103756944910691125121843851177126748527242261 384457673 11436798318241527 28191107197253910077026135881012 2084269045327162213681460 26721432 177633226375452106517932352221265 28371025 1807550810694852174235850710548321781825016035971216461221 121958749157311595228091204 22739841922Table 19: Average maximum length Open generating 100, 300, 500 solutions complete alternating AND/OR treesn60220920334042124962414474488444088422233334444100 solutionsASGLASGavg.max.avg. max.1813383963479854771331530295711622720240958102100119692364475008991137462614422 286663944915109905610124074760253485752214931258437749804300 solutionsASGLASGavg.max.avg. max.4287681312821144205821041742898278332639604119315428129585799675 125614803 29314851 156943087 85825746 133913742563187458716614204590 101822254 44062847 1831852 1004500 solutionsASGLASGavg.max.avg. max.6431138219411172131393296126902133055129469781875234422487497811013 181024442483571337 252771547 1423271254 2756214039963768681187423558885165536743727401565 34939611215Table 20: Average maximum length Open generating 100, 300, 500 solutions randomly constructed AND/OR DAGsSince ASG algorithm checks presence duplicates expanding solution,time required duplication checking grows rapidly test domains. Hence,overall time required generating specific number solutions also increases rapidly(faster BU LASG) increase size tree/DAG. result,BU outperforms ASG respect time requirement trees DAGs. However315fiGhosh, Sharma, Chakrabarti, & Dasguptamemory used generating specific number solutions increases moderately (slowerBU) increase size tree/DAG. Therefore respect spacerequirement, ASG outperforms BU larger trees DAGs.LASG BU, time well memory requirement BU increasesfaster LASG degree AND/OR tree DAG increases.happens because, BU, time taken merging sub-solutions nodesmemory required storing alternative solutions rooted different nodesincreases rapidly increase degree node.contrary, test domains, 5-peg Tower Hanoi problem, matrix-chainmultiplication problem, probelm finding secondary structure RNA sequences,average maximum size Open ASG LASG comparable (Table 21, Table 22 Table 23). Therefore, LASG algorithm, time savedavoiding duplication checking compensated extra overhead maintainingsolution space tree checks required lazy expansion. Hence running timewell space requirement almost algorithms threementioned problem domains.Moreover, due low values average maximum size Open, ASGoutperforms BU respect time requirement memory used threetest domains. three domains also, LASG BU, time wellmemory requirement BU increases faster LASG size searchspace (AND/OR tree DAG) increases.6. Ramifications Implicitly Specified AND/OR Structuressection, briefly discuss use proposed algorithms generation alternativesolutions non-decreasing order cost implicit AND/OR search spaces. Onepossible way extend standard AO generating given number solutions,say k, follows. Instead keeping one potential solution graph(psg), stage kpsgs computed explicitly constructed search space instead expandingone node, k nodes, (that is, one node psg), expanded once.expanding nodes, k psgs recomputed again. Since cost nodesoften recomputed expanding nodes, swap options associated nodeupdated every recomputation.Another possible approach could run AO generates optimal solution.point time swap options computed explicit portiongraph swap option minimum cost applied optimal solution.resulting psg expanded resulting expansion explicit graph.swap options re-evaluated incorporate cost update. next best psgcomputed. process continues till second best solution derived. amongremaining successor psgs first solution successor psgs second solution,promising psg selected expanded. process continues till third solutionfound. successor psgs also added already existing pool candidatepsgs. two broad steps, (a) selecting next best psg pool candidatepsgs, (b) keeping expanding explicit graph till next best solutionfound, continued till k solutions found.316fiGenerating Ordered Solutions Explicit AND/OR Structures# disks8910111213100 solutionsASGLASGavg. max. avg. max.5592416866122427110918353791322187614021938585147259482118200300 solutionsASGLASGavg. max. avg. max.11118691174163331119252216367142283296611177373473776234492675 1240252437500 solutionsASGLASGavg. max. avg. max.17437513523526548419838234569323444748688229155866812004047241016 1828377697Table 21: Average maximum length Open generating 100, 300, 500 solutions 5-peg Tower Hanoi problem different number disks# matrices203040506070809010010 solutionsASGLASGavg. max. avg. max.46872539841627112673123589086151751269114476112136234851221813249413222641410314230757616725915 solutionsASGLASGavg. max. avg. max.6812134591232309415798182731291202111001691181899413718832910314725846911215732860912216744582321633720 solutionsASGLASGavg. max. avg. max.90176469516029311619212522690152151266123205151267108160243437117170335607127180427777136190583 1145262477Table 22: Average maximum length Open generating 10, 15, 20 solutionsmatrix-chain multiplication problemsTest caseTC1TC2TC3TC4TC5TC6TC7TC8TC9TC10TC11TC12TC13TC145 solutionsASGLASGavg. max. avg. max.4584417450955095479046895093499047864574499347844281428046894484407739735911659113551065410533643151519851974178407310 solutionsASGLASGavg. max. avg. max.931767512510019294170901688214210119487155981868714910520095168831577311997188861598014770119128251116212115225110211671165598103193100185821546911215 solutionsASGLASGavg. max. avg. max.135249951431462661251971322441152101522921191971402461141841552941272061212319213814427712021411521493146189350161280171317166321951727813514927614023912023197176Table 23: Average maximum length Open generating 5, 10, 15 solutionsgenerating secondary structure RNA sequences317fiGhosh, Sharma, Chakrabarti, & Dasguptaimportant observe methods heavily depend incorporating updates explicit DAG like adding nodes, increase cost, etc., recomputingassociated swap options along signatures use swap options. Handlingdynamic updates DAG efficiently use implicit AND/OR search spacesremains interesting future direction.7. Conclusionwork presented top-down algorithm generating solutions givenweighted AND/OR structure (DAG) non-decreasing order cost. Ordered solutionsAND/OR DAGs useful number areas including model based programming,developing new variants AO*, service composition based user preferences, real lifeproblems dynamic programming formulation, etc. proposed algorithm twoadvantages (a) works incrementally, i.e., generating specific number solutions,next solution generated quickly, (b) number solutions generatedknown priori, algorithm leverage generate solutions faster. Experimentalresults show efficacy algorithm state-of-the-art. also opensseveral interesting research problems development applications.8. Acknowledgmentsthank anonymous reviewers editor, Prof. Hector Geffner, valuablecomments enriched presentation paper significantly. also thankProf. Abhijit Mitra, International Institute Information Technology, Hyderabad, India,valuable inputs regarding test domain involving secondary structure RNA.thank Aritra Hazra Srobona Mitra, Research Scholar, Department Comp. Sc. &Engg., Indian Institute Technology Kharagpur, India, proof reading paper.Appendix A. Proof Correctness Algorithm 4Lemma A.1 Every solution optimal solution Sopt constructedSopt applying sequence swap options according order R.Proof: [Lemma A.1] Every solution Sopt alternating AND/ treeconstructed choosing non optimal edges nodes. Considersolution Sm , corresponding set non-optimal edges suppose|S | = m. apply relation R obtain ordered sequence edgese1 , e2 , e1 appears e2 (e1 , e2 ) R. show existssequence swap options constructed . every edge eij(here eij ith edge 1 m), append subsequence edgesei1 , . . . , eij 1 eij , ei1 , . . . , eij edges emanateparent vq , ei1 , . . . , eij 1 first ij 1 edges L(vq ).get sequence edges aug mentioned augmentation.aug basically concatenation subsequences 1 , . . . , , sequence edgesei1 , . . . , eij ei1 , . . . , eij edges emanate parent vq ,ei1 , . . . , eij first ij edges L(vq ). construct aug follows.318fiGenerating Ordered Solutions Explicit AND/OR Structuresevery , construct = hi1 ,i2 , . . . , ij 1,ij i, ik ,ik +1 = heik , eik +1 , ik ,ik +1i1 ik (ij 1). constructed concatenating every individual . Hence existssequence swap options corresponding every solution Sm .Definition A.p [Default Path] Lemma A.1, every non-optimal solution Smconstructed initial optimal solution applying sequence swap options,(Sm ), according order R. sequence solutions formed following (Sm )corresponds path Sopt Sm SSDAG G . path defined defaultpath, Pd (Sm ), Sm .Lemma A.2 SSDAG alternating AND/OR tree contains every alternativesolution .Proof: [Lemma A.2] prove induction length default path Pdsolutions.[Basis (n = 1) :] Consider swap list Sopt . solutions whose default path lengthequal 1 form Succ(Sopt ). Therefore solutions present G.[Inductive Step :] Suppose solutions whose default path length less equaln present G. prove solutions default path length equaln + 1 also present G. Consider solution Sm Pd (Sm ) = n + 1. Let (Sm ) =(S ) = h , , i. Since P (S ) =h1 , , n , n+1 i. Consider solution Sm1nV, swap optionn, Smn+1 L(Sm ), directed edge Sm Sm G .Hence every solution default path length equal n + 1 also present G.Lemma A.3 alternating AND/OR tree , Algorithm 4 adds solutions Closed(at Line 11) non-decreasing order cost.Proof: [Lemma A.3] Consider following invariants Algorithm 4 followdescription Algorithm 4.a. minimum cost solution Open always removed Line 6 Algorithm 4.b. cost solutions added Open, exploring successor setsolution Sm (at Line 13 Algorithm 4), greater equal C(Sm ).two invariants follows Algorithm 4 adds solutions Closed (at Line 11)non-decreasing order cost.Lemma A.4 alternating AND/OR tree , every node SSDAG ,Agorithm 4 generates solution corresponding node.Proof: [Lemma A.4] Lemma A.3 follows Algorithm 4 generates solutionsnon-decreasing order cost. generating solution Sm , mean adding SmClosed (at line 11 Algorithm 4). purpose proof contradiction, let us assumeAlgorithm 4 generate solution Sm . Also let Sm first occurrence319fiGhosh, Sharma, Chakrabarti, & Dasguptascenario generating solutions mentioned order. According Lemma A.1,exists sequence swap options = 1 , . . . , k corresponding Sm . Also considerwhose sequence swap options = , . . . ,solution Sm1k1 . According Property 3.2,C(Sm ) C(Sm ). Consider following two cases:) < C(S ): Sincea. C(Smfirst instance incorrect scenario, Algo generatedrithm 4 generates solutions non-decreasing order cost, Smprior Sm .) = C(S ): Since Algorithm 4 resolves tie favor parent solution,b. C(SmSm first instance incorrect scenario case also Smgenerated prior Sm .. generated Algorithm 4,swap option k belongs swap list Smis, Sm added Closed, Sm also expanded solutionsapplying one swap option, added Open list. Sinceconstructed Smapplying one swap option ,constructed SmalsoaddedOpenk. Thereforeexploring successors Smalso eventually generatedAlgorithm 4 - contradiction.Lemma A.5 alternating AND/OR tree , Algorithm 4 add solutionClosed (at Line 11 Algorithm 4) once.Proof: [Lemma A.5] purpose contradiction, let us assume Sm firstsolution added Closed twice. Therefore Sm must added Open twice.Consider following facts.a. Sm added Closed first time, value lastSolCost C(Sm ),Sm added TList.b. description Algorithm 4 follows contents TList deletedvalue lastSolCost increases.c. Lemma A.3 follows Algorithm 4 generates solutions non-decreasingorder cost. Hence, Sm generated second time, valuelastSolCost change C(Sm ).facts follow Sm present TList Sm added Opensecond time. Since, adding solution Open, Algorithm 4 checks whetherpresent TList (at Line 16 Algorithm 4); Algorithm 4 must doneadding Sm Open second time. Therefore Sm could added Opensecond time contradiction.Theorem A.1 Sj V, Sj generated (at Line 11) Algorithm 4non-decreasing order costs ties among solutions costs resolvedmentioned before.Proof: [Theorem A.1] Follows Lemma A.2, Lemma A.3, Lemma A.4 Lemma A.5.320fiGenerating Ordered Solutions Explicit AND/OR StructuresAppendix B. Proof Correctness Algorithm 5Definition B.q [Reconvergent Paths Solution Space DAG] Two paths, (i) p1 =Si11 Si1n (ii) p2 = Si21 Si2m , SSDAG G alternatingAND/OR tree reconvergent following holds:a. Si11 = Si21 , i.e. paths start node;b. Si1n = Si1m , i.e. paths ends node;c. (j [2, n 1])(k [2, 1]), Si1j 6= Si2k ; i.e. paths commonintermediate node.Definition B.r [Order Generation Time] context Algorithm 5, defineorder relation, V V, (Sp , Sq ) Sp generated Algorithm 5 Sq .V set vertices SSDAG G alternating AND/OR tree .Lemma B.1 Algorithm 5 adds solutions Closed list non-decreasing ordercosts.Proof: [Lemma B.1] Consider following invariants Algorithm 5 followdescription Algorithm 5.a. minimum cost solution Open always removed line 11 Algorithm 4.b. Algorithm 5 expands solution, say Sp , two phases. first phase Spexpanded using native swap options Sp . solutions added Openresult application native swap options, cost greaterequal C(Sp ). second phase, i.e., lazy expansion, Sp expandedusing non native swap option. solution Sp may undergo second phase times0 (|L(Sp )| |N (Sp , k )|) k used construct Sp . every lazyexpansion Sp , new solution added Open. Consider solution Smusing Algorithm 5 P red(S ). Suppose swapconstructed Smjoption L(Sm ),/ N (Sm , j ), i.e., native swap option Sm .). Suppose successors respectively,Clearly L(Smccconstructed application , i.e., SmSc , SmSc . Also let Scadded Closed Sm .Consider fact Algorithm 5 apply swap option Sm , is, Sc) C(S ), C(S ) C(S ).added Open Sc added Closed. Since C(SmccAccording Algorithm 5, applied Sm (during lazy expansion), Scadded Open right Sc added Closed. Consider time periodadding Sm adding Sc Closed. period, every solution addedClosed cost C(Sm ) C(Sc ), i.e., cost less equal C(Sc ).general, application swap option add solution Open delayedamount time, say , solutions, added Closedtime interval, cost less equal solution consideration.321fiGhosh, Sharma, Chakrabarti, & Dasguptafacts follow Algorithm 5 adds solutions Closed listnon-decreasing order costs.Lemma B.2 two reconvergent paths SSDAG G alternating AND/ORtree equal length.Proof: [Lemma B.2] Consider paths:12n12(i) p1 = S1SpSn , (ii) p2 = S1SpSn .edges paths represent application swap option solution. p1p2 start solution also end solution. Therefore setsswap options used paths also same. Hence lengths pathsequal, is, context p1 p2 , n = m.Lemma B.3 set reconvergent paths length n, Algorithm 5 generatesone path.Proof: [Lemma B.3] following cases possible.[Case 1 (n = 2) :] Consider following two paths:1212(i) p1 = S1S2S3 , (ii) p2 = S1S2S3 .obvious 1 = 2 2 = 1 . Suppose S2 S2 . Algorithm 5apply swap option 1 S2 . Therefore p2 generated Algorithm 5.[Case 2 (Any values n) :] case, path belonging set reconvergent paths, consists n different swap options, suppose 1 , , n . Also startnode end node paths consideration Sp Sm . Consider nodespaths length 1 Sp . Clearly n nodes.Among nodes, suppose Algorithm 5 adds Sp1 Closed first, Sp1 constructedSp applying swap option 1 . According Algorithm 5, 1 appliednode constructed Sp added Closed Sp1 . Therefore,paths starting Sp , whose second node Sp1 , generatedAlgorithm 5. use similar argument paths Sp1 Sm length n 1determine paths generated Algorithm 5. stage, setpaths grown further, one path towards Sm continue grow.applying previous argument n times, one path Sp Smconstructed. Therefore Algorithm 5 generate one path Sp Sm .Definition B.s [Connection Relation Rc Rc ] define connection relation, Rc ,symmetric order relation pair nodes, vq vr , belonging alternatingAND/OR tree as:(vq , vr ) Rc | exists node vp ,exist two paths, (i) p1 = vp . . . vq ,(ii) p2 = vp . . . vr322fiGenerating Ordered Solutions Explicit AND/OR StructuresSimilarly connection relation, Rc , defined two swap options follows. Consider two swap options iq jr , iq = hei , eq , iq jr = hej , er , jr i. Supposeedges ei eq emanate vp , edges ej er emanate vt .(iq , jr ) Rc (vp , vt ) Rc .Definition B.t [Mutually Connected Set] solution Sm , set Vm nodesmutually connected,v1 , v2 Vm , (v1 6= v2 ) {(v1 , v2 ) Rc }Consider set nodes, Vm = {v1 , , vk }, swap option j belongs vj1 j k. set swap options Vm = {1 , , k } mutually connected.Lemma B.4 Suppose Sm solution alternating AND/OR tree , P red(Sm ) ={S1 , , Sk }, swap option j used construct Sm Sj 1 j k.swap options 1 , , k mutually connected.Proof: [Lemma B.4] Since Sm constructed S1 , , Sk applying 1 , , k respectively, 1 , , k present signature Sm . Suppose set = {1 , , k }.show, b , (a , b ) Rcpurpose proof contradiction, let us assume (i1 , i2 )/ Rc . Also Sm constructed applying i1 i2 Si1 Si2 respectively. Consider path p1 SSDAGstarts Sopt ends Sm , along p1 , Si1 parent Sm .along path, i2 applied application swap option i1 . Similarly consider path p2 SSDAG starts Sopt ends Sm , along p2 ,Si2 parent Sm . Along path, i1 applied application swapoption i2 .Suppose i1 i2 belongs node v1 v2 respectively. Since along path p1 , i1swap option applied last, Sm contains node v1 . Similarly along path p2 , i2swap option applied last. Hence Sm contains node v2 . Therefore, mustnode vr , exist paths node v1 v2 implies(i1 , i2 ) Rc . arrive contradiction proves 1 , , k mutually connected.Definition B.u [Subgraph SSDAG] Consider solution Sp alternating AND/ORtree Tand mutually connected set Vm nodes Sp , vq Vm , C(Sp , vq ) =(S , V ) = hVCopt (vq ) . subgraph Gsubpsub , Esub SSDAG respect SpVm defined follows. Vsub consists solutions constructedSp applying sequence swap options belonging Vm , Esub set edgescorresponding swap options belong Vm .(S , V )Lemma B.5 number total possible distinct solutions level Gsubp,|V|=n.n+d2n1323fiGhosh, Sharma, Chakrabarti, & DasguptaProof: [Lemma B.5] Consider swap options belong nodes Vm .(S , V ) represented sequencerespect swap options, every solution Sr Gsubpnumbers length n, Seq(Sr ), every number corresponds distinct node Vm .numerical value number represent rank swap option chosennode vq Vm . According representation, level:i. sum numbers Seq(Sr ) solution, Sr , equal sum numbersSeq(Sr ) solution, Sr , level;ii. sum numbers Seq(Sr ) solution, Sr , increased 1 sumnumbers Seq(Sr ) solution, Sp , previous level.Hence, dth level, n slots 1 increments need madeSeq(Sr ). instance well known combinatorial problem packing n + 1objects n slotsrestriction keeping least one object per slot.n+d2done n1 ways.Theorem B.1 solution space tree constructed Algorithm 5 complete.Proof: [Theorem B.1] purpose contradiction, suppose Sm first solutiongenerated Algorithm 5. Also P red(Sm ) = {Spi } Sm constructedSpi applying qi , 1 k. Lemma B.4 follows setswap options {qi | 1 k} mutually connected. Therefore set nodes Vmswap options belong also mutually connected. Suppose |Vm | = n.Consider solution Sq , Vm mutually connected, 1 k, every qibelongs set native swap options Sq respect swap option usedconstruct Sq . Clearlyvt Vm , C(Sq , vt ) = Copt (vt )argue Sq generated Algorithm 5 Sm first solutionrooted ,generated Algorithm 5. Consider subtree Tsubqedges corresponding swap options belong Vm considered. proveequalnumber solutions generated Algorithm 5 every level Tsubnumber solutions level Gsub (Sq , Vm ).Consider solution Sq set Succ(Sq ). Suppose Succ(Sq , Vm ) setsuccessor solutions constructed Sq applying swap options belongingminimum cost solution Succ(Sq , Vm ). Accordingnodes Vm , SminAlgorithm 5 initially Succ(Smin ) partially explored using set native swap optionsSmin. non native swap option, b , belongs nodes Vm , usedexplore Succ(Smin), right sibling solution Smin, constructed applying b Sq,added Closed. Consider fact solution Sq , vt Vm , C(Sq , vt ) = Copt (vt )holds. Therefore swap options belonging Vm also eventually used exploresuccessors Smin. Similarly second best successor Sq able use.one swap option, c , used construct Sminimmediate children Smin Tsub consist solutions, obtainedapplication one swap option Vm Smin. native swap list Smincontainsswap option ranking next c . swap options, used construct324fiGenerating Ordered Solutions Explicit AND/OR Structuresn 1 sibling solutions Smin, used lazy expansion, accountsanother n 1 children Smin. Hence would n children Smin.Similarly, second best successor Sq Tsub n 1 immediate children.n 2 children on. childrenthird best successor Sq Tsubsolutions children solutions own, increasing numbersolutions level tree. way, increasing level, numbersolutions present level keeps increasing. prove following proposition partproving Theorem B.1.) givenProposition B.1 level d, number solutions N (d, n, TsubnXn+d2N (d, n, Tsub ) =N (d 1, k, Tsub ) =n1k=1Proof: [Proposition B.1] second level, n solutions. give risek=1k+n1Xk+k=1n2Xk=1kk=1solutions third level. Similarly fourth levelnXnX) + ... + 1) + N (3, n 1, Tsubk.... + 1 = N (3, n, Tsubextend level result follows.) = 1N (1, n, Tsub) = nN (2, n, TsubnXn+1N (3, n, Tsub ) =k=2k=1N (4, n, Tsub)=nXk=1N (3, k, Tsub)=n+23induction depth d.determine number solutions level Tsub[Basis (d = 1) :]) = n.Clearly, N (1, n, Tsub[Inductive Step :] Suppose, dth level number solutions n+d2= n+d2n1d1 .Therefore + 1th level,nXn+d2n+d3n+d1N (d + 1, n, Tsub ) =+N (d, k, Tsub ) =+ + 1 =d1d1n1k=1Since Algorithm 5 generate duplicate node, Proposition B.1(S , V ) level equal number solutionsnumber solutions Gsubq(S , V ) also generatedlevel Tsub , level set solutions Gsubq(S , V ),Algorithm 5 Tsub . Therefore, level, Sm belongs Gsubqalso generated Algorithm 5. Therefore Sm also generated Algorithm 5contradiction establishes truth statement Theorem B.1.325fiGhosh, Sharma, Chakrabarti, & DasguptaAppendix C. Conversion AND/OR Tree AlternatingAND/OR TreeAND/OR tree generalization alternating AND/OR tree restrictionstrict alternation nodes relaxed. words intermediatenode child another intermediate node similar parent childrelation also allowed node. present algorithm convert AND/ORequivalent alternating AND/OR tree.use two operations namely, folding unfolding conversions. Correspondingevery edge, stack, update-list, used conversions. AND/OR tree, considertwo nodes, vq vr , similar type (AND/OR) connected edge er .Edges, e1 , , ek emanate er .[Folding Node :] Suppose vq vr nodes. folding vr performedfollows.source edges e1 , , ek changed vr vq costs updatedce (ei ) ce (ei ) + ce (er ) + cv (vr ) 1 k, new cost sumold cost cost edge points source ei . triplethvr , cv (vr ), ce (er )i pushed update-list ei , 1 k.edge er along node vr removed vq .[Folding Node :] Suppose vq vr nodes. folding vr performed follows.source edges e1 , , ek changed vr vq . One edges amonge1 , , ek , suppose ei , selected arbitrarily cost updated ce (ei )ce (ei ) + ce (er ) + cv (vr ) 1 k. triplet hvr , cv (vr ), ce (er )i pushedupdate-list ei , whereas triplet hvr , 0, 0i pushed update-list ej ,1 j k j 6= i.edge er along node vr removed vq .unfolding operation reverse folding operationnodes. works node vq follows.Procedure Unfold(node vq )1234567891011forall edge ei emanate vqupdate list ei emptyhvt , c1 , c2 pop(update list ei );exists edge et vq points node vtCreate node vt , connect vt using edge et vq ;cv (vt ) c1 ;ce (et ) c2 ;else c2 6= 0ce (et ) c2 ;endend326fiGenerating Ordered Solutions Explicit AND/OR StructuresFunction Convert takes root node AND/OR tree transforms equivalentalternating AND/OR tree recursively.Function Convert(vq )1234567every child vq terminal nodevq parent vp typeApply f old operation vq ;endelseforeach child vr vq , vr intermediate AND/OR nodeConvert(vr );endFunction Revert takes root node alternating AND/OR tree convertsoriginal AND/OR tree recursively.Function Revert(vq )123456every child vq terminal nodereturn;Perform unf old operation vq ;foreach child vr vqRevert(vr );endoverall process generating alternative solutions AND/OR tree follows.AND/OR tree converted alternating AND/OR tree using Convert function,solutions generated using ASG algorithm. solutions transformed backusing Revert function. proof correctness presented below.C.1 Proof CorrectnessSuppose AND/OR tree two nodes, vq vr , similar type (AND/OR)connected edge er . Edges e1 , , ek emanate er . fold operation1 AND/OR tree generated applicationapplied vq vr . Letf old operation.Lemma C.1 context mentioned above, present claim following twopropositions.Proposition C.1 set solutions node vq generated set1 node v applying unfold operation v solutionssolutionsqq.1 1 contains node v , exists soluProposition C.2 every solution Smq1tion Sm generated Sm applying unfold vq .Proof: [Proposition C.1] present proof following cases. Considersolution Sm contains node vq .327fiGhosh, Sharma, Chakrabarti, & Dasguptaa. vq vr nodes: two cases possible.1. vr absent Sm : Since fold operation modifies edge er only,1 . Thereforeedges vq also presentalso1present solution set remain unchangedapplication unfold operation.2. vr present Sm : Since k distinct edges emanating vr ,let one edges, say ei , present Sm . prove1 1 , application unfold operation 1solution Smgenerate Sm . application fold operation node vr modifies source1 .cost edge ei vr vq ce (ei ) ce (ei ) + ce (er ) + cv (vr )1 solution 1 , edge e present 1 . AlsoSuppose Sm1subtree rooted vq , remaining parts Smidentical11other. Clearly Sm exists solution application1 generates .unfold operation vq Smb. vq vr nodes: Since vq node Sm containedges emanate vq . Therefore edge er vr present1 1 , following holds.Sm . Consider solution Sm1.1. vq present Sm2. subtrees rooted children vq vr Sm identical1 .subtrees rooted children vq Sm1 identical3. subtree rooted vq , remaining parts Smother.1 exists solution 1 application unfold operation vClearly Smq1Sm generates Sm .1solution Smcontain node vq , valid solutionwell.Proof: [Proposition C.2] present proof following cases. Consider1 1 contains node v .solution Smqa. vq vr nodes: Since vq node, exactly one edge ei vq1 . two cases possible.belong Sm1 : Since fold operation modifies1. ei modified folding vredge er edges vr only, edges vq1 . Since e modified folding, solutionalso present1Sm also valid solution .1 : Suppose e connects v v2. ei modified folding vrq1 generate solution .1Sm . Apply unfold operation node vq Smedge ei replaced edge er connects vq vr eiconnect vr vi . argue Sm valid solution since328fiGenerating Ordered Solutions Explicit AND/OR Structuressubtree rooted vi modified sequence (a) folding vr1 , (b) unfolding v construct 1 .constructq1 containb. vq vr nodes: Since vq node, Smedges emanate vq . two types edges emanating1 (a) Type-1 : edges v also presentvqqvq , (b) Type-2 : edges added vq folding edges1 generate solutionvr . Apply unfold operation node vq SmSm . Sm contain Type-1 edges, another edge er vq . Sm , vq vrconnected er Type-2 edges originated vr . argue Smvalid solution since subtree rooted nodes pointed Type-2 edges1 ,modified sequence (a) folding vr construct1.(b) unfolding vq construct Sm Sm1 1 contain node v valid solutionClearly solution Smqwell.Lemma C.2 function Convert applied root node AND/OR tree ,alternating AND/OR tree generated.Proof: [Lemma C.2] Function Convert traverses every intermediate node depth firstmanner. Consider sequence nodes, vq1 , vq2 , , vqn type, vqiparent vqi+1 1 < n. Obviously, fold operation applied vqi+1vqi , 1 < n. words, fold operation applied sequencenodes reverse order folding vqi+1 , edges vqi+1 modifiedmoved vqi , 1 < n. function call Convert(vq2 ) returns, edgesvq2 , , vqn already moved vq1 sequence nodes, vq1 , vq2 , , vqnflattened. Therefore, every sequence nodes type flattened, functioncall Convert(vR ) returns, vR root alternating AND/OR treegenerated.Lemma C.3 function Revert applied alternating AND/OR tree , updatelist every edge becomes empty.Proof: [Lemma C.3] Follows description Revert.Theorem C.1 AND/OR tree , possible construct alternating AND/ORtree using function Convert, set possible solutions generatedorder increasing cost applying Algorithm 4 , convertingindividual solutions using function Revert.Proof: [Theorem C.1] According Lemma C.2, application function Convertalternating AND/OR tree generated. Consider intermediate AND/OR0 , 1 , , ntrees generated folding every node . Letn . Since generated i+10 = ,=sequence AND/OR trees329fiGhosh, Sharma, Chakrabarti, & Dasgupta, 0 < n, accordingfolding exactly one nodegenerated i+1 unfoldingsolutionsLemma C.3, solution , Revert unfolds every node vqvq folded Convert transforming . Thereforegenerated solutions .Lemma C.1,node. Accordingsolution,solutionsReferencesBonet, B., & Geffner, H. (2005). algorithm better AO ?. Proceedings20th national conference Artificial intelligence - Volume 3, pp. 13431347. AAAIPress.Chakrabarti, P. P. (1994). Algorithms searching explicit AND/OR graphsapplications problem reduction search. Artif. Intell., 65 (2), 329345.Chakrabarti, P. P., Ghose, S., Pandey, A., & DeSarkar, S. C. (1989). Increasing searchefficiency using multiple heuristics. Inf. Process. Lett., 32 (5), 275275.Chang, C. L., & Slagle, J. R. (1971). admissible optimal algorithm searchingAND/OR graphs. Artif. Intell., 2 (2), 117128.Chegireddy, C. R., & Hamacher, H. W. (1987). Algorithms finding k-best perfect matchings. Discrete Applied Mathematics, 18 (2), 155165.Chen, H., Xu, Z. J., Liu, Z. Q., & Zhu, S. C. (2006). Composite templates cloth modelingsketching. Proceedings 2006 IEEE Computer Society ConferenceComputer Vision Pattern Recognition - Volume 1, pp. 943950. IEEE ComputerSociety.Cormen, T. H., Stein, C., Rivest, R. L., & Leiserson, C. E. (2001). Introduction Algorithms(2nd edition). McGraw-Hill Higher Education.Darwiche, A. (1999). Compiling knowledge decomposable negation normal form.Proceedings 16th international joint conference Artifical intelligence - Volume1, pp. 284289. Morgan Kaufmann Publishers Inc.Darwiche, A. (2001). Decomposable negation normal form. J. ACM, 48, 608647.Dasgupta, P., Sur-Kolay, S., & Bhattacharya, B. (1995). VLSI floorplan generationarea optimization using and-or graph search. VLSI Design, 1995., Proceedings8th International Conference on, pp. 370 375.Dechter, R., & Mateescu, R. (2007). AND/OR search spaces graphical models. Artif.Intell., 171 (2-3), 73106.Ebendt, R., & Drechsler, R. (2009). Weighted search - unifying view application.Artificial Intelligence, 173 (14), 1310 1342.Elliott, P. (2007). Extracting k best solutions valued And-Or acyclic graph.Masters thesis, Massachusetts Institute Technology.Elliott, P., & Williams, B. (2006). DNNF-based belief state estimation. Proceedings21st national conference Artificial intelligence - Volume 1, pp. 3641. AAAIPress.330fiGenerating Ordered Solutions Explicit AND/OR StructuresEppstein, D. (1990). Finding k smallest spanning trees. Proc. 2nd ScandinavianWorksh. Algorithm Theory, No. 447 Lecture Notes Computer Science, pp. 3847. Springer Verlag.Eppstein, D. (1998). Finding k shortest paths. SIAM J. Comput., 28 (2), 652673.Flerova, N., & Dechter, R. (2010). best solutions graphical models. 1st WorkshopConstraint Reasoning Graphical Structures.Flerova, N., & Dechter, R. (2011). Bucket mini-bucket schemes best solutionsgraphical models. GKR 2011(a workshop IJCAI 2011).Fromer, M., & Globerson, A. (2009). LP view m-best MAP problem. AdvancesNeural Information Processing Systems (NIPS) 22, pp. 567575.Fuxi, Z., Ming, T., & Yanxiang, H. (2003). solution billiard balls puzzle using aoalgorithm application product development. Palade, V., Howlett, R., &Jain, L. (Eds.), Knowledge-Based Intelligent Information Engineering Systems,Vol. 2774 Lecture Notes Computer Science, pp. 10151022. Springer Berlin /Heidelberg.Gogate, V., & Dechter, R. (2008). Approximate solution sampling (and counting)AND/OR spaces. CP, pp. 534538.Gu, Z., Li, J., & Xu, B. (2008). Automatic service composition based enhanced servicedependency graph. Web Services, 2008. ICWS 08. IEEE International Conferenceon, pp. 246 253.Gu, Z., Xu, B., & Li, J. (2010). Service data correlation modeling applicationdata-driven service composition. Services Computing, IEEE Transactions on, 3 (4),279291.Gupta, P., Chakrabarti, P. P., & Ghose, S. (1992). Towers Hanoi: generalizations,specializations algorithms. International Journal Computer Mathematics, 46,149161.Hamacher, H. W., & Queyranne, M. (1985). K best solutions combinatorial optimizationproblems. Annals Operations Research, 4, 123143.Hansen, E. A., & Zhou, R. (2007). Anytime heuristic search. J. Artif. Intell. Res. (JAIR),28, 267297.Hansen, E. A., & Zilberstein, S. (2001). LAO : heuristic search algorithm findssolutions loops. Artificial Intelligence, 129 (1-2), 35 62.Homem de Mello, L., & Sanderson, A. (1990). AND/OR graph representation assemblyplans. Robotics Automation, IEEE Transactions on, 6 (2), 188 199.Jimenez, P., & Torras, C. (2000). efficient algorithm searching implicit AND/ORgraphs cycles. Artif. Intell., 124, 130.Kleinberg, J., & Tardos, E. (2005). Algorithm Design. Addison-Wesley Longman PublishingCo., Inc., Boston, MA, USA.Lang, Q. A., & Su, Y. (2005). AND/OR graph search algorithm discovering composite web services. International Journal Web Services Research, 2 (4), 4664.331fiGhosh, Sharma, Chakrabarti, & DasguptaLawler, E. L. (1972). procedure computing k best solutions discrete optimizationproblems application shortest path problem. Management Science,18 (7), pp. 401405.Ma, X., Dong, B., & He, M. (2008). AND/OR tree search algorithm web service composition. PACIIA 08: Proceedings 2008 IEEE Pacific-Asia WorkshopComputational Intelligence Industrial Application, pp. 2327, Washington, DC,USA. IEEE Computer Society.Majumdar, A. A. K. (1996). Generalized multi-peg Tower Hanoi problem. JournalAustralian Mathematical Society. Series B. Applied Mathematics, 38, 201208.Marinescu, R., & Dechter, R. (2005). AND/OR branch-and-bound solving mixed integerlinear programming problems. CP, p. 857.Marinescu, R., & Dechter, R. (2006). Memory intensive branch-and-bound search graphical models. AAAI.Marinescu, R., & Dechter, R. (2007a). Best-first AND/OR search 0/1 integer programming. CPAIOR, pp. 171185.Marinescu, R., & Dechter, R. (2007b). Best-first AND/OR search graphical models.AAAI, pp. 11711176.Marinescu, R., & Dechter, R. (2009a). AND/OR branch-and-bound search combinatorialoptimization graphical models. Artif. Intell., 173 (16-17), 14571491.Marinescu, R., & Dechter, R. (2009b). Memory intensive AND/OR search combinatorialoptimization graphical models. Artif. Intell., 173 (16-17), 14921524.Martelli, A., & Montanari, U. (1973). Additive AND/OR graphs. Proceedings3rd international joint conference Artificial intelligence, San Francisco, CA, USA.Morgan Kaufmann Publishers Inc.Martelli, A., & Montanari, U. (1978). Optimizing decision trees heuristically guidedsearch. Commun. ACM, 21, 10251039.Mateescu, R., & Dechter, R. (2008). AND/OR multi-valued decision diagrams constraintnetworks. Concurrency, Graphs Models, pp. 238257.Mateescu, R., Dechter, R., & Marinescu, R. (2008). AND/OR multi-valued decision diagrams (AOMDDs) graphical models. J. Artif. Intell. Res. (JAIR), 33, 465519.Mathews, D. H., & Zuker, M. (2004). RNA secondary structure prediction. EncyclopediaGenetics, Genomics, Proteomics Bioinformatics. John Wiley & Sons, Ltd.Nilsson, D. (1998). efficient algorithm finding probable configurationsprobabilistic expert systems. Statistics Computing, 8, 159173.Nilsson, N. J. (1980). Principles artificial intelligence. Tioga Publishing Co.Otten, L., & Dechter, R. (2011). Anytime AND/OR depth-first search combinatorialoptimization. SoCS.Pearl, J. (1984). Heuristics: intelligent search strategies computer problem solving.Addison-Wesley Longman Publishing Co., Inc., Boston, MA, USA.332fiGenerating Ordered Solutions Explicit AND/OR StructuresRussell, S., & Norvig, P. (2003). Artificial Intelligence: Modern Approach (2nd editionedition)., chap. Planning, pp. 375461. Prentice-Hall, Englewood Cliffs, NJ.Shiaa, M. M., Fladmark, J. O., & Thiell, B. (2008). incremental graph-based approachautomatic service composition. IEEE International Conference Services Computing, 4 (2), 4664.Shin, D. H., Jeon, H. B., & Lee, K. H. (2010). sophisticated approach composingservices based action dominance relation. Services Computing Conference (APSCC), 2010 IEEE Asia-Pacific, pp. 164 170.Subramanian, S. (1997). Routing algorithms dynamic, intelligent transportation networks. Masters thesis, Virginia Technical Univ., Dept. Civil Engineering.Sugimoto, K., & Katoh, N. (1985). algorithm finding k shortest loopless pathsdirected network. Trans. Information Processing Soc. Japan, 26, 356364.Japanese.Szymanski, M., Barciszewska, M. Z., Barciszewski, J., & Erdmann, V. A. (2005). 5S Ribosomal RNA Database. http://biobases.ibch.poznan.pl/5SData/. Online Database.Takkala, T., Borndorfer, R., & Lobel, A. (2000). Dealing additional constraintsk-shortest path problem. Proc. WM 2000.Topkis, D. M. (1988). k-shortest path algorithm adaptive routing communicationsnetworks. Trans. Communications, 36 (7), 855859.Yan, Y., Xu, B., & Gu, Z. (2008). Automatic service composition using AND/OR graph.E-Commerce Technology Fifth IEEE Conference Enterprise Computing,E-Commerce E-Services, 2008 10th IEEE Conference on, pp. 335338.333fiJournal Artificial Intelligence Research 44 (2012) 491-532Submitted 11/11; published 07/12Riffled Independence Efficient InferencePartial RankingsJonathan Huangjhuang11@stanford.eduJames H. Clark CenterStanford University, Stanford CA 94305, USAAshish Kapoorakapoor@microsoft.comMicrosoft ResearchOne Microsoft WayRedmond WA 98052-6399, USACarlos Guestringuestrin@cs.cmu.eduGates Hillman Complex, Carnegie Mellon University,5000 Forbes Avenue, Pittsburgh, PA 15213, USAAbstractDistributions rankings used model data multitude real world settingspreference analysis political elections. Modeling distributions presentsseveral computational challenges, however, due factorial size set rankingsitem set. challenges quite familiar artificial intelligencecommunity, compactly represent distribution combinatorially largespace, efficiently perform probabilistic inference representations.respect ranking, however, additional challenge referhuman task complexity users rarely willing provide full ranking long listcandidates, instead often preferring provide partial ranking information.Simultaneously addressing challenges i.e., designing compactly representable model amenable efficient inference learned using partialranking data difficult task, necessary would like scale problemsnontrivial size. paper, show recently proposed riffled independenceassumptions cleanly efficiently address challenges. particular,establish tight mathematical connection concepts riffled independencepartial rankings. correspondence allows us develop efficientexact algorithms performing inference tasks using riffled independence based representations partial rankings, somewhat surprisingly, also shows efficient inferencepossible riffle independent models (in certain sense) observationstake form partial rankings. Finally, using inference algorithm, introducefirst method learning riffled independence based models partially ranked data.1. Probabilistic Modeling Ranking Data: Three ChallengesRankings arise number machine learning application settings preference analysis movies books (Lebanon & Mao, 2008) political election analysis (Gormley& Murphy, 2007; Huang & Guestrin, 2010). many problems, great interestbuild statistical models ranking data order make predictions, form recommendations, discover latent trends structure construct human-comprehensible datasummaries.c2012AI Access Foundation. rights reserved.fiHuang, Kapoor & GuestrinModeling distributions rankings difficult problem, however, due factnumber items ranked increases, number possible rankings increasesfactorially. combinatorial explosion forces us confront three central challengesdealing rankings. First, need deal storage complexity compactly represent distribution space rankings?1 algorithmic complexity efficiently answer probabilistic inference queries given distribution?Finally, must contend refer human task complexity,challenge stemming fact difficult accurately elicit full rankinglarge list candidates human user; choosing list n! options easy taskusers typically prefer provide partial information. Take American PsychologicalAssociation (APA) elections, example, allow voters rank order candidatesfavorite least favorite. 1980 election, five candidates, therefore5! = 120 ways rank five candidates. Despite small candidate list, voterselection preferred specify top-k favorite candidates rather writingfull rankings ballots (see Figure 1). example, roughly third voterssimply wrote single favorite candidate 1980 election.three intertwined challenges storage, algorithmic, human task complexitycentral issues probabilistic modeling rankings, models efficientlyhandle three sources complexity limited applicability. paper, examineflexible intuitive class models rankings based generalization probabilisticindependence called riffled independence, proposed recent work (Huang & Guestrin,2009, 2010). previous papers focused primarily representational (storagecomplexity) issues, concentrate inference incomplete observations (i.e., partialrankings), showing addition storage complexity, riffle independence based modelsefficiently address issues algorithmic human task complexity.fact two issues algorithmic human task complexity intricately linkedriffle independent models. considering partial rankings, give users flexibilityprovide much little information care give. context partialranking data, relevant inference queries also take form partial rankings.example, might want predict voters second choice candidate given informationfirst choice. One main contributions paper show inferencepartial ranking queries performed particularly efficiently riffle independentmodels.main contributions work follows:2reveal natural fundamental connection riffle independent modelspartial rankings. particular, show collection partial rankingsitem set form complete characterization space observations upon1. Note common wonder one would care represent distribution rankingsnumber sample rankings never nearly large. problem number samples alwaysmuch smaller n! however, means rankings never observed, limiting abilityestimate probability arbitrary ranking. way overcome paucity samplesexploit representational structure, much alignment solving storage complexityissue.2. paper extended presentation paper (Huang, Kapoor, & Guestrin, 2011) appeared2011 Conference Uncertainty Artificial Intelligence (UAI) well results firstauthors dissertation (Huang, 2011).492fiEfficient Inference Partial RankingsFirstChoiceSecondChoiceThirdChoiceFourthChoiceFifthChoice#votes53421373451230123------273------------1198413------1513---------30231254186Figure 1: Example partial ranking data (taken American Psychological Associationelection dataset, 1980)one efficiently condition riffle independent model. result, showranked items satisfy riffled independence relationship, conditioningpartial rankings done efficiently, running time O(n|H|), |H| denotesnumber model parameters.prove that, sense (which formalize), impossible efficiently conditionriffle independent models observations take form partial rankings.propose first algorithm capable efficiently estimating structureparameters riffle independent models heterogeneous collections partiallyranked data.show results real voting preference data evidencing effectivenessmethods.2. Riffled Independence Rankingsranking, , items item set one-to-one mapping rankset R = {1, . . . , n} denoted using vertical bar notation 1 (1)| 1 (2)| . . . | 1 (n).say ranks item i1 (or over) item i2 rank i1 lessrank i2 . example, might {Corn, P eas, Apples, Oranges} rankingCorn|P eas|Apples|Oranges encodes preference Corn Peas turn preferred Apples on. collection possible rankings item set denoted(or Sn implicit).Since n! rankings n items, intractable estimate even explicitlyrepresent arbitrary distributions Sn without making structural assumptionsunderlying distribution. many possible simplifying assumptions onemake, focus approach proposed recent papers (Huang & Guestrin,2009, 2010) ranks items assumed satisfy intuitive generalized notionprobabilistic independence known riffled independence. paper, argueriffled independence assumptions particularly effective settings one would likemake queries taking form partial rankings. remainder section,review riffled independence.493fiHuang, Kapoor & Guestrinriffled independence assumption posits rankings item set generated independently generating rankings smaller disjoint item subsets (say,B) partition , piecing together full ranking interleaving (or riffle shuffling)smaller rankings together. example, rank item set foods, one might firstrank vegetables fruits separately, interleave two subset rankings formfull ranking. formally define riffled independence, use notions relative rankingsinterleavings.Definition 1 (Relative ranking map). Given ranking subset ,relative ranking items A, (), ranking, SA , (i) < (j)(i) < (j).Definition 2 (Interleaving map). Given ranking partition disjointsets B, interleaving B (denoted, AB ()) (binary) mappingrank set R = {1, . . . , n} {A, B} indicating whether rank occupiedB. rankings, denote interleaving ranking vertical bar notation:[AB ()](1)|[AB ()](2)| . . . |[AB ()](n).Example 3. Consider partitioning item set vegetables = {Corn, P eas}fruits B = {Apples, Oranges}, well full ranking four items: =Corn|Oranges|P eas|Apples. case, relative ranking vegetables () =Corn|P eas relative ranking fruits B () = Oranges|Apples. interleaving vegetables fruits AB () = A|B|A|B.Definition 4 (Riffled Independence). Let h distribution consider subsetitems complement B. sets B said riffle independenth decomposes (or factors) as:h() = mAB (AB ()) fA (A ()) gB (B ()),distributions mAB , fA gB , defined interleavings relative rankingsB respectively. words, B riffle independent relative rankingsB, well interleaving mutually independent. refer mABinterleaving distribution fA gB relative ranking distributions.Riffled independence found approximately hold number real datasets(Huang & Guestrin, 2012). relationships identified data, insteadexhaustively representing n! ranking probabilities, one represent factorsmAB , fA gB , distributions smaller sets.2.1 Hierarchical Riffle Independent Modelsrelative ranking factors fA gB distributions rankings.reduce parameter space, natural consider hierarchical decompositions item setsnested collections partitions (like hierarchical clustering). example, Figure 2.1shows hierarchical decomposition vegetables riffle independent fruits amonghealthy foods, healthy foods are, turn, riffle independent subsetdesserts: {Doughnuts, &M s}.494fiEfficient Inference Partial Rankings{C,P,A,O,D,M}{D,M}{C,P,A,O}Doughnuts, M&Ms{C,P}{A,O}Corn, PeasApples, OrangesFigure 2: example hierarchy six food items.simplicity, restrict consideration binary hierarchies, defined tuplesform H = (HA , HB ), HA HB either (1) null, case H called leaf,(2) hierarchies item sets B respectively. second case, Bassumed form nontrivial partitioning item set.Definition 5. say distribution h factors riffle independently respecthierarchy H = (HA , HB ) item sets B riffle independent respect h,fA gB factor riffle independently respect subhierarchies HA HB ,respectively.Like Bayesian networks, hierarchies represent families distributions obeyingcertain set (riffled) independence constraints parameterized locally. drawmodel, one generates full rankings recursively starting drawing rankingsleaf sets, working tree, sequentially interleaving rankings reachingroot. parameters hierarchical models simply interleaving relativeranking distributions internal nodes leaves hierarchy, respectively.general, number total parameters required represent hierarchical riffleindependent model (as Bayesian networks) still scale exponentially numberitems. example, number interleavings p items n p items np .often case however, much fewer parameters necessary. example, thinmodels (Huang & Guestrin, 2012), number items factored modelstage hierarchy never small constant k, always represented(degree k) polynomial number parameters. use |H| refer numberparameters necessary representing distribution factors according hierarchyH.decomposing distributions rankings small pieces (like Bayesian networksdone distributions), hierarchical models allow better interpretability,efficient probabilistic representation, low sample complexity, efficient MAP optimization,and, show paper, efficient inference.Example 6. Figure 3(a), reproduce hierarchical structure learned usingfully ranked subset APA data consisting 5000 training examples HuangGuestrin (2012). five candidates election: (1) William Bevan, (2) IraIscoe, (3) Charles Kiesler, (4) Max Siegle, (5) Logan Wright (Marden, 1995). Strikingly,structure learned using algorithm (maximum likelihood) knows nothingunderlying politics APA, leaf nodes correspond exactlypolitical coalitions dominated APA 1980 election research psychologists495fiHuang, Kapoor & GuestrinA={12345}B={1345}C={2}CommunitypsychologistsmB,C().14B|C|B|B|B.19B|B|C|B|B.25B|B|B|C|B.25B|B|B|B|C.18mD,E()fC()D|D|E|E.2821.00D|E|D|E.12D|E|E|D.12D={13}E={45}E|D|D|E.14ResearchpsychologistsClinicalpsychologistsE|D|E|D.12E|E|D|D.22(a) Hierarchical structure learnedvia MLE using 5000 full rankingsAPA dataset.C|B|B|B|BfD()fE()1|3.504|5.483|1.505|4.52(b) Riffle independent model parameters learned via MLE using5000 full rankings APA dataset.Figure 3: Example hierarchical model APA election. Candidates enumeratedas: (1) William Bevan, (2) Ira Iscoe, (3) Charles Kiesler, (4) Max Siegle, (5)Logan Wright (Marden, 1995).(candidates 1 3), clinical psychologists (candidates 4 5), communitypsychologists (candidate 2).Figure 3(b), plot corresponding parameter distributions learned viamaximum likelihood. three relative ranking distributions, correspondingpolitical party, well two interleaving distributions (one interleaving researchclinical psychologists, one interleaving community psychologistremaining candidates). Since parameter distribution constrained sum 1,total 11 free parameters.2.2 Model Estimationpaper estimate riffle independent models based methods introducedearlier work. Given hierarchial structure model, maximum likelihood parameterestimates hierarchical riffle independent model straightforward compute via frequency estimates. estimate correct structure model challengingproblem. key insight lies noticing two subsets B riffle independent,j, k B, independence relation (i) ((j) < (k)) must hold.structure learning algorithms operate hunting tripletwise independencerelations within data. defer interested readers details (Huang & Guestrin,2012).496fiEfficient Inference Partial RankingsNote earlier work, assumed algorithms access datasetconsisting i.i.d. full rankings provided users. current work, relaxassumptions allowing users provide partially ranked data. One assumption throughout, however, user full ranking mind items. particular,current work address incomplete ranking problem, users mightseen items (we discuss possible extensions incomplete ranking settingSection 9.3. Decomposable ObservationsGiven prior distribution, h, rankings observation O, Bayes rule tells usposterior distribution, h(|O), proportional L(O|) h(), L(O|)likelihood function. operation conditioning h observation typically computationally intractable since requires multiplying two n! dimensional functions, unlessone exploit structural decompositions problem. section, describe decomposition certain class likelihood functions space rankingsobservations factored simpler parts. observation decomposableway, show one efficiently condition riffle independent prior distributionO. simplicity paper, focus primarily subset observations whose likelihoodfunctions encode membership subset rankings Sn .Definition 7 (Subset observations). subset observation binary observation whoselikelihood proportional indicator function subset Sn i.e.,1.L(O|) =0 otherwiserunning example, consider class first place observations throughoutchapter (we consider far general observation models later sections). firstplace observation =Corn ranked first, example, associated collectionrankings placing item Corn first place (O = { : (Corn) = 1}). interestedcomputing posterior h(| O). Thus first place scenario, given voterstop choice would like infer preferences remaining candidates.Given partitioning item set two subsets B, sometimes possibledecompose (or factor ) subset observation involving items smaller subset observations involving A, B interleavings B independently. decompositionsoften exploited efficient inference.Example 8.Consider first place observation= Corn ranked first,decomposed two independent observations observationrelative ranking Vegetables, observation interleaving VegetablesFruits:497fiHuang, Kapoor & GuestrinOA = Corn ranked first among Vegetables,OA,B = First place occupied Vegetable.condition case, one updates relative ranking distributionVegetables (A) zeroing rankings vegetables place Corn firstplace, updates interleaving distribution zeroing interleavingsplace Vegetable first place, normalizes resulting distributions.example nondecomposable observation observation= Corn third place.see decompose (with respect Vegetables Fruits), enoughnotice interleaving Vegetables Fruits independent relativeranking Vegetables. If, example, element interleaves (Vegetables)B (Fruits) AB () = A|B|A|B, since (Corn) = 3, relative rankingVegetables constrained () = P eas|Corn. Since interleavingsrelative rankings independent, see cannot decomposable.Formally, use riffle independent factorizations define decomposability respecthierarchy H item set.Definition 9 (Decomposability). Given hierarchy H item set, subset observation decomposes respect H likelihood function L(O|) factors riffleindependently respect H.subset observations prior decompose according hierarchy,show (as Example 8) posterior also decomposes.Proposition 10. Let H hierarchy item set. Given prior distribution hsubset observation decompose respect H, posterior distributionh(|O) also factors riffle independently respect H.Proof. Denote likelihood function corresponding L (in proof,matter assumed subset observation result holds arbitrarylikelihoods).use induction size item set n = ||. base case n = 1 triviallytrue. Next consider general case n > 1. posterior distribution, Bayes rule,written h(|O) L() h(). two cases. H leaf node,posterior h0 trivially factors according H, done. Otherwise, L hfactor, assumption, according H = (HA , HB ) following way:L() = mL (AB ())fL (A ())gL (B ()), h() = mh (AB ())fh (A ())gh (B ()).Multiplying grouping terms, see posterior factors as:h(|O) = [mL mh ](AB ()) [fL fh ](A ()) [gL gh ](B ()).show h(|O) factors respect H, need demonstrate (by Definition 5)distributions [fL fh ] [gL gh ] (after normalizing) factor respect HA498fiEfficient Inference Partial RankingsHB , respectively. Since fL fh factor according hierarchy HA assumption|A| < n since H leaf, invoke inductive hypothesis showposterior distribution, proportional fL fh must also factor according HA .Similarly, distribution proportional gL gh must factor according HB .4. Complete Decomposabilitycondition Proposition 10, prior observation must decompose respect exactly hierarchy, sufficient one efficient inference, mightfirst glance seem restrictive render proposition useless practice. overcomelimitation hierarchy specific decomposability, explore special family observations (which call completely decomposable) property decomposabilitydepend specifically particular hierarchy, implying particularobservations, efficient inference always possible (provided efficient representationprior distribution also possible).illustrate observation decompose respect multiple hierarchiesitem set, consider first place observation =Corn ranked first. arguedExample 8 decomposable observation. Notice however decomposabilityparticular observation depend items partitionedhierarchy. Specifically, instead Vegetables Fruits, sets = {Corn, Apples}B = {P eas, Oranges} riffle independent, similar decomposition would continuehold, decomposing observation relative ranking items (Cornfirst among items A), observation interleaving B (First placeoccupied element A).formally capture notion observation decompose respectarbitrary underlying hierarchies, define complete decomposability:Definition 11 (Complete decomposability). say subset observation completely decomposable decomposes respect every possible hierarchy itemset . denote collection possible completely decomposable (subset) observations C. See Figure 4 illustration set C.Conceptually, completely decomposable observations correspond indicator functionsriffle independent possible. Complete decomposability guaranteeobservation one always exploit available factorized structure priordistribution order efficiently condition O.Proposition 12. Let H binary hierarchy item set. Given prior hfactorizes respect H, completely decomposable observation O, posteriorh(|O) also decomposes respect H.Proof. Proposition 12 follows simple corollary Proposition 10.Example 13. simplest example completely decomposable observation uniform observation Ounif = , includes possible rankings correspondsuniform indicator function unif rankings. Given hierarchy H, unif showndecompose riffle independently respect H, factor also uniform,hence Ounif completely decomposable.499fiHuang, Kapoor & GuestrinH1H6H2CompletelyDecomposableObservationsH5H3H4Figure 4: diagram illustrating collection completely decomposable observations, C.shaded region (labeled Hi ) represents family subset observationsSn decompose respect hierarchy Hi . collection Cseen intersection shaded regions, subset observationslie inside intersection ones conditioning performedlinear time (in number model parameters).uniform observation course particularly interesting context Bayesianinference, hand, given stringent conditions Definition 11,obvious nontrivial completely decomposable observations even exist. Nonetheless,exist nontrivial examples (such first place observations), nextsection, exhibit rich general class completely decomposable observations.5. Complete Decomposability Partial Ranking Observationssection discuss mathematical problem fully characterizing classcompletely decomposable observations. main contribution section showcompletely decomposable observations correspond precisely partial rankingsitem set.Partial rankings. begin discussion introducing partial rankings, allowitems tied respect ranking dropping verticals vertical barrepresentation .Definition 14 (Partial ranking observation). Let 1 , 2 ,. . . , r ordered collectionsubsets partition (i.e., = j = 6= j). partial rankingobservation 3 corresponding partition collection rankings rank items3. remarked Ailon (2007), note term partial ranking used confusedtwo standard objects: (1) Partial order, namely, reflexive, transitive anti-symmetric binary500fiEfficient Inference Partial Rankingsitems j < j. denote partial ranking 1 |2 | . . . |r saytype = (|1 |, |2 |, . . . , |r |). denote collection partial rankings(over n items) P.partial ranking defined viewed coset subgroup =S1 S2 Sr . Given type full ranking , onepartial ranking type containing , thus therefore equivalently denote partialranking 1 |2 | . . . |r , element 1 |2 | . . . |r . Note cosetnotation allows multiple rankings refer partial ranking .space partial rankings defined captures rich natural classobservations. particular, partial rankings encompass number commonly occurringspecial cases, traditionally modeled isolation, work (as wellrecent works Lebanon & Lafferty, 2003; Lebanon & Mao, 2008) usedunified setting.Example 15. Partial ranking observations include:(First place, Top-1 observations): First place observations correspond partialrankings type = (1, n 1). observation Corn ranked firstwritten Corn|Peas,Apples,Oranges.(Top-k observations): Top-k observations partial rankings type = (1, . . . , 1, nk). generalize first place observations specifying items mappingfirst k ranks, leaving n k remaining items implicitly ranked behind. example,observation Corn ranked first Peas ranked second writtenCorn|Peas|Apples,Oranges.(Desired/less desired dichotomy): Partial rankings type = (k, n k) correspondsubset k items preferred desired remaining subset n k items.example, partial rankings type (k, n k) might arise approval votingvoters mark subset approved candidates, implicitly indicating disapprovalremaining n k candidates.(Ratings): Finally, partial rankings come form rating data where,example, restaurants rated as, ?, ??, ? ? ?. corresponding partial rankingwould thus tie restaurants rated number stars, rankingrestaurants stars restaurants fewer stars.(Trivial observations): Partial rankings type = (n) refer trivial observationswhose likelihood functions uniform entire space rankings, . trivialobservation rankings item set = {Corn, P eas, Apples}, example,simply written simply Corn, P eas, Apples.show partial ranking observations decompose, exhibit explicit factorization respect hierarchy H items. simplicity, begin consideringsingle layer case, items partitioned two leaf sets B. factorization depends following notions consistency relative rankings interleavingspartial ranking.relation; (2) ranking subset [which discuss Section 9 incomplete rankings].search engines, example, although top-k elements returned, remaining n kimplicitly assumed ranked behind [and therefore, search engines return partial rankings].501fiHuang, Kapoor & GuestrinDefinition 16 (Restriction consistency). Given partial ranking = 1 |2 | . . . |rsubset , define restriction partial ranking itemsobtained intersecting A. Hence restriction is:[S ]A = 1 A|2 A| . . . |r A.Given ranking, items A, say consistent partial rankingmember restriction A, [S ]A .Definition 17 (Interleaving consistency). Given interleaving AB two sets A, Bpartition , say AB consistent partial ranking = 1 | . . . |r (withtype ) first 1 entries AB contain number Bs 1 ,second 2 entries AB contain number Bs 2 , on. Givenpartial ranking , denote collection consistent interleavings [S ]AB .example, consider partial ranking= Corn, Apples|P eas, Oranges,places single vegetable single fruit first two ranks, single vegetablesingle fruit last two ranks. Alternatively, partially specifies interleavingAB|AB. full interleavings A|B|B|A B|A|B|A consistent (by droppingvertical lines) A|A|B|B consistent (since places two vegetables first tworanks).Using notions consistency partial ranking, show partial rankingobservations decomposable respect binary partitioning (i.e., single layerhierarchy) item set.Proposition 18 (Single layer hierarchy). partial ranking observationbinary partitioning item set (A, B), indicator function , , factors riffleindependently as:() = mAB (AB ()) fA (A ()) gB (B ()),(5.1)factors mAB , fA gB indicator functions consistent interleavingsrelative rankings, [S ]AB , [S ]A [S ]B , respectively.single layer decomposition Proposition 18 turned recursive decomposition partial ranking observations arbitrary binary hierarchies, establishesmain result. particular, given partial ranking prior distributionfactorizes according hierarchy H, first condition topmost interleaving distribution zeroing parameters corresponding interleavings consistent, normalizing distribution. need condition subhierarchiesHA HB relative rankings B consistent , respectively.Since consistent sets, [S ]A [S ]B , partial rankings themselves,algorithm conditioning partial ranking applied recursivelysubhierarchies HA HB . precise, show that:Theorem 19. Every partial ranking completely decomposable (P C).502fiEfficient Inference Partial Rankingsprcondition (Prior hprior , Hierarchy H, Observation = 1 |2 | . . . |r )isLeaf(H)forallhprior ()hpost ();0otherwiseNormalize (hpost ) ;return (hpost );elseforallmprior ( ) [S ]ABmpost ( );0otherwiseNormalize (mpost ) ;f (A ) prcondition (fprior , HA , [S ]A ) ;g(B ) prcondition (gprior , HB , [S ]B ) ;return (mpost , fpost , gpost );Algorithm 1: Pseudocode prcondition, algorithm recursively conditioning hierarchical riffle independent prior distribution partial ranking observations. See Definitions 1617 [S ]A , [S ]B , [S ]AB . runtime prcondition O(n |H|), |H|number model parameters. Input: parameter distributions prior hprior represented explicit tabular form, observation form partial ranking. Output:parameter distributions posterior hpost represented explicit tabular form.Since proof Theorem 19 fairly straight forward given form factorization (Equation 5.1), deferred Appendix. consequence Theorem 19Proposition 12, conditioning partial ranking observations performed efficiently.See Algorithm 1 details recursive conditioning algorithm.running time complexity conditioning partial ranking? recursionAlgorithm 1 operates parameter distribution once, setting probabilitiesinterleavings relative rankings distribution either zero not,normalizing. decide whether zero probability not, one must check partialranking consistency either interleaving relative ranking, requiresO(n) time. Therefore, total, Algorithm 1 requires O(n |H|) time, |H|total number model parameters. Notice complexity conditioning dependslinearly complexity prior whenever prior distribution compactlyrepresented, efficient inference partial ranking observations also possible.stated Section 2, |H| general scale exponentially n, thin chain models,number items factored model stage neversmall constant k, verifying interleaving relative ranking consistency performedconstant time, implying conditioning operation linear number modelparameters, guaranteed polynomial n.Example 20. example, consider conditioning APA distribution Example 6 observation Candidate 3 ranked first place, alsorepresented partial ranking = 3|1, 2, 4, 5. Recall candidate 3 CharlesKiesler, research psychologist.Figure 5(a) show structure parameters prior distributionAPA election data, highlighting particular interleavings relative rankings503fiHuang, Kapoor & GuestrinmB,C()C|B|B|B|B.14C|B|B|B|BmB,C()0B|C|B|B|B.19B|C|B|B|B.22B|B|C|B|B.25B|B|C|B|B.29B|B|B|C|B.25B|B|B|C|B.29B|B|B|B|C.18B|B|B|B|C.21mD,E()fC()mD,E()fC()D|D|E|E.2821.00D|D|E|E.5421.00D|E|D|E.12D|E|D|E.23D|E|E|D.12D|E|E|D.23E|D|D|E.14E|D|D|E0E|D|E|D.12E|D|E|D0E|E|D|D.22E|E|D|D0fD()fE()fD()fE()1|3.504|5.481|304|5.483|1.505|4.523|11.005|4.52(a) Structure parameters prior distribution (with consistent relative rankings interleavings highlighted).(b) Structure parameters posterior distribution conditioning.Figure 5: Example conditioning APA hierarchy (from Example 6) first placeobservation Candidate 3 ranked first place.consistent O. example, possible interleavings research psychologists(D) clinical psychologists (E), interleavings consistentrank research psychologist first among research clinical psychologists.therefore three consistent interleavings: D|D|E|E, D|E|D|E, D|E|E|D.Conditioning sets relative rankings interleavings consistentzero normalizes resulting parameter distribution. resulting riffleindependent representation posterior distribution shown Figure 5(b).5.1 Impossibility Resultinteresting consider completely decomposable observations exist beyond partialrankings. One main contributions show observations.Theorem 21 (Converse Theorem 19). Every completely decomposable observation takesform partial ranking (C P).Together, Theorems 19 21 form significant insight nature rankings,showing notions partial rankings riffled independence deeply connected.fact, result shows even possible define partial rankings via completedecomposability!practical matter, Theorem 21 shows algorithm based simplemultiplicative updates parameters exactly condition observationstake form partial rankings. computational complexity conditioningobservations partial rankings remains open. conjecture approximateinference approaches may necessary efficiently handling complex observations.504fiEfficient Inference Partial Rankings5.2 Proof Impossiblity Result (Theorem 21)turn proving Theorem 21. Since proof significantly longer less obviousproof converse (Theorem 19), sketch main ideas drive proofrefer interested readers details Appendix.Recall definition linear span set vectors vector spaceintersection linear subspaces containing set vectors. prove Theorem 21,introduce analogous concepts span set rankings.Definition 22 (rspan pspan). Let X Sn collection rankings. definepspan(X) intersection partial rankings containing X. Similarly, definerspan(X) intersection completely decomposable observations containing X.formally,\\pspan(X) =, rspan(X) =O.O:XO, OC:XSexample, X = {Corn|P eas|Apples, Apples|P eas|Corn}, checkedpartial ranking three items containing items X entire set itself.Thus pspan(X) = Corn, P eas, Apples.proof strategy establish two claims: (1) pspan set alwayspartial ranking, (2) fact, rspan pspan set X exactlysets. Since claim (1) fact partial rankings involve riffledindependence, defer related proofs Appendix. Thus have:Lemma 23. X Sn , pspan(X) partial ranking.Proof. See Appendix.following discussion instead sketch proof claim (2). first show, however,Theorem 21 must hold indeed true claims (1) (2) hold.Proof. (of Theorem 21): Given C, want show P. claim(2), rspan(O) = pspan(O). Since element C, however, also= rspan(O), thus = pspan(O). Finally Lemma 23 (claim (2)) guaranteespspan(O) partial ranking, conclude P.proceed establish claim rspan(X) = pspan(X). followingproposition lists several basic properties rspan use severalproofs. follow directly definition write proofs.Proposition 24.I. (Monotonicity) X, X rspan(X).II. (Subset preservation) X, X 0 X X 0 , rspan(X) rspan(X 0 ).III. (Idempotence) X, rspan(rspan(X)) = rspan(X).One inclusion proof rspan(X) = pspan(X) follows directly factP C (Theorem 19):505fiHuang, Kapoor & GuestrinformPspan(X)X0 X; 0;, 0 0 Xt disagree relative ordering items a1 , a2Xt ;foreach XtAdd partial ranking obtained deleting vertical bar itemsa1 a2 Xt ;+ 1;return (any element Xt ) ;Algorithm 2: Pseudocode computing pspan(X). formPspan(X) takes set partialrankings (or full rankings) X input outputs partial ranking. algorithm iterativelydeletes vertical bars elements X agreement. Note necessarykeep track t, ease notation proofs. algorithmdirect way computing pspan(X), again, simplifies proof main theorem.Lemma 25. subset orderings, X, rspan(X) pspan(X).Proof. Fix subset X Sn let element rspan(X). would like showelement pspan(X). Consider partial ranking P covers X(i.e., 0 0 X). want see . Theorem 19, P C,therefore, C. Since rspan(X), 0 0 X, conclude,definition rspan, . Since holds partial ranking covering X,pspan(X).remains task establishing reverse inclusion:Proposition 26. subset orderings, X, rspan(X) pspan(X).prove Proposition 26, consider problem computing partial ranking span(pspan) given set rankings X. Algorithm 2, show simple procedure basediteratively finding rankings X disagree pairwise ranking two items,replacing rankings partial ranking vertical bar twoelements removed. show algorithm provably outputs correctresult.Proposition 27. Given set rankings X input, Algorithm 2 outputs pspan(X).Proof. See Appendix.final step able prove Proposition 26, prove following twotechnical lemmas relate computation pspan Algorithm 2 riffled independence, really form heart argument. particular, completelydecomposable observation C, Lemma 28 shows ranking containedforce rankings also contained O.Lemma 28. Let C suppose exist 1 , 2 disagree relativeranking items i, j . ranking obtained swapping relative rankingitems i, j within 3 must also contained O.506fiEfficient Inference Partial RankingsProof. Let h indicator distribution corresponding observation O.show swapping relative ranking items i, j 3 result rankingassigned nonzero probability h, thus showing new ranking contained O.Let = {i, j} B = \A. Since C, h must factor riffle independently accordingpartition (A, B). Thus,h(1 ) = m(AB (1 )) f (A (1 )) g(B (1 )) > 0,h(2 ) = m(AB (2 )) f (A (2 )) g(B (2 )) > 0.Since 1 2 disagree relative ranking items A, factorization impliesparticular f (A = i|j) > 0 f (A = j|i) > 0. Since h(3 ) > 0, must alsom(AB (3 )), f (A (3 )), g(B (3 )) positive probability.therefore swap relative ranking A, , obtain new ranking positiveprobability since terms decomposition new ranking positiveprobability.Lemma 29 provides conditions removing vertical bar onerankings X change support completely riffle independent distribution. illustrate example, consider completely decomposable observationcontains partial ranking = Corn, P eas|Apples, Oranges subset.Lemma 29 guarantees that, if, addition, exists element disagreesrelative ordering of, say, P eas Oranges, fact partial ranking0 0 = Corn, P eas, Apples, Oranges (with bar removed ) must alsosubset O. Formally,Lemma 29. Let = 1 | . . . |i |i+1 | . . . |k partial ranking item set ,0 0 = 1 | . . . |i i+1 | . . . |k , partial ranking sets i+1merged. Let a1 ij=1 j a2 kj=i+1 j . element Cadditionally exists ranking disagrees relativeordering a1 , a2 , 0 0 O.Proof. key strategy proof Lemma 29 argue large subsets rankingsmust contained completely decomposable observation decomposing rankingstranspositions invoking technical lemma (Lemma 28) repeatedly.See Appendix details.use Lemma 29 show reverse inclusion Proposition 26 alsoholds, establishing two sets rspan(X) pspan(X) fact equal therebyproving desired result, C P.Proof. (of Proposition 26) iteration t, Algorithm 2 producesSset partial rankings,Xt . denote union partial rankings time Xt Xt . NoteX0 = X XT = pspan(X). idea proof show iterationt, following set inclusion holds: rspan(Xt ) rspan(Xt1 ). indeed holds,507fiHuang, Kapoor & Guestrinfinal iteration , shown that:pspan(X) = XT ,(Proposition 27)rspan(XT ),(Monotonicity, Proposition 24)rspan(X0 ),(since rspan(Xt ) rspan(Xt1 ), shown below),rspan(X)(X0 = X, see Algorithm 2)would prove Proposition.remains show rspan(Xt ) rspan(Xt1 ). claim Xt rspan(Xt1 ).Let Xt . Xt1 , since Xt1 rspan(Xt1 ), rspan(Xt1 )proof done. Otherwise, Xt \Xt1 . second case, use factiteration t, vertical bar i+1 deleted partial ranking = 1 | . . . |i |i+1 | . . . |k (which subset Xt1 ) form partial ranking0 0 = 1 | . . . |i i+1 | . . . |k . (which subset Xt ). Furthermore, ordervertical bar deleted algorithm, must existed partialranking (and therefore full ranking 0 ) disagreed relative ordering items a1 , a2 opposite sides bar. Since Xt \Xt1 assume0 0 .would like apply Lemma 29. Note C Xt1 O,also O, since Xt1 . application Lemma 29 shows0 0 therefore O.shown fact holds observation C Xt1O, therefore taking intersection supports C, see Xtrspan(Xt1 ). Taking rspan sides yields:rspan(Xt ) rspan(rspan(Xt1 )),rspan(Xt1 ).(Subset preservation, Proposition 24)(Idempotence, Proposition 24)5.3 Going Beyond Subset ObservationsThough stated results far subset observations, commenttheory would look like considered general likelihood functions.order avoid confusion, refer general class functions call completely decomposable functions, instead completely decomposable subset observationsDefinition 11.Definition 30. function h : Sn R called completely decomposable functionfactors riffle independently respect every hierarchy item set . denoteecollection possible completely decomposable functions C.e nearly same. quite simple restate Theorem 19discuss, C Crespect general case completely decomposable functions:Theorem. Every partial ranking indicator function completely decomposable function.508fiEfficient Inference Partial RankingsUnfortunately, proof converse (Theorem 21) easily generalize,instead used show support ({ Sn : h() > 0}) every completelydecomposable function partial ranking. natural, however, suspect fullconverse indeed exist every completely decomposable function proportionalindicator function partial ranking. fact, suspected converse almostholds. have:Theorem. h completely decomposable function supported partial ranking= 1 | . . . |r |i | =6 2 = 1, . . . , r, h proportional indicatorfunction .Proof. See Appendix.Example 31. completely decomposable functions, possible awayassumption |i | =6 2 i. example, function defined as:2/3 = Corn|P eas|Apples1/3 = P eas|Corn|Apples ,h() =0otherwisesupported partial ranking = Corn, P eas|Apples (where |1 | = 2),proportional indicator function (i.e., uniform rankingsassigned positive probability).However, still possible show h completely decomposable function.prove so, necessary establish three things: {Corn, P eas} {Apples}riffle independent, {Corn, Apples} {P eas} riffle independent,{P eas, Apples} {Corn} riffle independent. example, respect partitioning sets = {Corn, Apples} B = {P eas}, seeh() = m(AB ()) f (A ()) g(B ()),where:2/31/3m(AB ) =0AB = A|B|AAB = B|A|A ,AB = A|A|Bf (A ) =10{AC} = A|C,otherwiseg(B ) = 1.Therefore, |i | = 2, possible completely decomposable functionsuniform supports.5.4 Conditioning Noisy Observationsconclude section remark handling noise observations.assumed paper observed partial rankings always consistent usersunderlying full ranking, situations one may wish model noisiersetting, partial rankings may misreported small probability.natural model accounts noise, example, might be:1L(O|) =.(5.2)|O|1 otherwise509fiHuang, Kapoor & Guestrinprior distribution factorizes respect hierarchy H, conditioningnoisy likelihood Equation 5.2 results posterior distribution writtenweighted mixture prior distribution posterior would resultedconditioning noise-free observation. component posteriordistribution factorizes respect H, mixture factor general (andfactor according theory). result, iteratively conditioning multiplepartial rankings according noisy likelihood function would quickly leadunmanageable number mixture components. therefore believe approximateinference methods conditioning multiple noisy partial ranking observations fruitfularea research.6. Model Estimation Partially Ranked Datamany ranking based applications, datasets predominantly composed partial rankings rather full rankings due fact humans, partial rankings typicallyeasier faster specify. addition, many datasets heterogeneous, containing partialranking different types. example, American Psychological Assoication wellIrish House Parliament elections, voters allowed specify top-k candidatechoices value k (see Figures 7(a) 7(b)). section use efficientinference algorithm proposed Section 5 estimating riffle independent modelpartially ranked data. estimating model using partially ranked data typicallyconsidered difficult estimating one using full rankings, common practice (e.g., see Huang & Guestrin, 2010) simply ignore partial rankingsdataset. ability method incorporate available data however, leadsignificantly improved model accuracy well wider applicability method.section, propose first efficient method estimating structure parametershierarchical riffle independent model heterogeneous datasets consisting arbitrarypartial ranking types. Central approach idea given someones partial preferences, use efficient algorithms developed previous section infer fullpreferences consequently apply previously proposed algorithms designedwork full rankings.6.1 Censoring Interpretations Partial Rankingsmodel estimation problem full rankings stated follows. Given i.i.d. trainingexamples (1) , . . . , (m) (consisting full rankings) drawn hierarchical riffle independent distribution h, recover structure parameters h.partial ranking setting, assume i.i.d. draws, trainingexample (i) undergoes censoring process producing partial ranking consistent (i) .example, censoring might allow ranking top-k items (i)observed. allow arbitrary types partial rankings arise via censoring,make common assumption partial ranking type resulting censoring (i)depend (i) itself.510fiEfficient Inference Partial Rankings6.2 Algorithmtreat model estimation partial rankings problem missing data problem.many problems, could determine full ranking corresponding observation data, could apply algorithms work completely observeddata setting. Since full rankings given, utilize Expectation-Maximization (EM)approach use inference compute posterior distribution full rankingsgiven observed partial ranking. case, apply algorithms HuangGuestrin (2010, 2012) designed estimate hierarchical structuremodel parameters dataset full rankings.Given initial model h collection training examples {O(1) , O(2) , . . . , O(m) }consisting partial rankings, EM-based approach alternates following twosteps convergence achieved.(E-step): observation, O(i) = (i) (i) , training examples, useinference compute posterior distribution full ranking couldgenerated O(i) via censoring, h(|O(i) = (i) (i) ). Since observations takeform partial rankings hence completely decomposable, use efficientalgorithms Section 5 perform E-step.(M-step): M-step, one maximizes expected log-likelihood trainingdata respect model. hierarchical structure modelprovided, known beforehand, M-step performed using standard methods optimizing parameters. structure unknown, use structuralEM approach, analogous methods graphical models literaturestructure learning incomplete data (Friedman, 1997, 1998).Unfortunately, (riffled independence) structure learning algorithm HuangGuestrin (2010) unable directly use posterior distributions computedE-step. Instead, observing sampling riffle independent modelsdone efficiently exactly (as opposed to, example, MCMC methods), simplysample full rankings posterior distributions computed E-steppass full rankings structure learning algorithm Huang Guestrin(2010). number samples necessary, instead scaling factorially, scalesaccording number samples required detect riffled independence (whichmild assumptions polynomial n, Huang & Guestrin, 2010).7. Related WorkRankings permutations recently become active area research machinelearning due part hinge role play information retrieval preferenceelicitation. Algorithms RankSVM (Joachims, 2002) RankBoost (Freund,Iyer, Schapire, & Singer, 2003), example, successful large scale rankingproblems appear web search. main aims work differ webscale settings however instead seeking single optimal ranking respectobjective function, seek understanding large collection rankings via densityestimation. following, outline two major lines research influencedwork.511fiHuang, Kapoor & Guestrin7.1 Additive Multiplicative Decompositionspaper builds particular upon thread recent work tractable models permutation data based function decompositions. Kondor, Howard, Jebara (2007)Huang, Guestrin, Guibas (2008, 2009) considered additive decompositions distribution weighted sum Fourier basis functions. papers show low-frequencyFourier assumptions often effective coping representational complexityworking distributions permutations. show particular conditioningprior distributions low frequency likelihood functions often arise multiobjecttracking problems performed especially efficiently.Unfortunately, low frequency assumptions applicable distributions definedrankings, address ranking problems specifically, Huang Guestrin (2009,2010) introduced concept riffled independence useful generalization probabilistic independence rankings. Using multiplicative decompositions based riffledindependence, showed possible learn hierarchical structure modelgiven fully ranked dataset. previous papers topic riffled independencefocused problems related efficiently representing distributions, main focuscurrent paper lies efficient reasoning/inference tackling human task complexityconsidering partial rankings.interesting note natural efficient condition Fourier basedrepresentation low-frequency observations (involving small number items)=Alice third place, multiplicative decomposition based riffled independencewould able efficiently condition observation. hand,multiplicative decompositions allow us condition top-k observations efficiently (independently size k), whereas top-k observations would difficult handleFourier theoretic setting (except small k).7.2 Mallows Modelswork also fits larger body research well known Mallows distributionrankings, parameterized by:h(; , 0 ) (,0 ) ,(7.1)function refers Kendalls tau distance metric rankings. Mallowsdistribution (Equation 7.1) always shown special case hierarchical riffle independent model items sequentially factored model oneone (Huang, 2011) (see Figure 6).Mallows models (as well similar distance based models) advantagecompactly represent distributions large n, admit conjugate priordistributions (Meila, Phadnis, Patterson, & Bilmes, 2007). Estimating parameterspopular problem statisticians recovering optimal 0 data knownconsensus ranking rank aggregation problem known N P -hard (Bartholdi,Tovey, & Trick, 1989). Many authors focused approximation algorithms instead.Like Gaussian distributions, Mallows models tend lack flexibility, LebanonMao (2008) propose nonparametric model ranked (and partially ranked) data basedplacing weighted Mallows kernels top training examples, which, show,512fiEfficient Inference Partial Rankings{Corn,Peas,Apples,Oranges,Doughnuts}{Corn}{Peas,Apples,Oranges,Doughnuts}{Peas}{Apples,Oranges,Doughnuts}{Apples}{Oranges,Doughnuts}{Oranges}{Doughnuts}Figure 6: Mallows model always factors according refer chainstructure items factored one one. Mallows distribution five items food item set mode (or central ranking)0 = Corn|P eas|Apples|Oranges|Doughnuts, example, must factor according hierarchical structure.realize far richer class distributions, learned efficiently. However,address inference problem, immediately clear many Mallows modelspapers whether one efficiently perform inference operations like marginalizationconditioning models. Riffle independent models, hand, encompassclass distributions rich well interpretable, additionally,identified precise conditions efficient conditioning possible (the conditionsobservations take form partial rankings).several recent works model partial rankings using Mallows based models.Busse, Orbanz, Buhmann (2007) learned finite mixtures Mallows models topk data (also using EM approach). Lebanon Mao (2008), mentioned,developed nonparametric model based Mallows models handle arbitrarytypes partial rankings. settings, central problem marginalize Mallowsmodel full rankings consistent particular partial ranking.efficiently, papers rely fact (first shown Fligner & Verducci, 1986)marginalization step performed closed form. closed form equation FlignerVerducci (1986), however, seen special case setting since Mallowsmodels always shown factor riffle independently according chain structure.Specifically, compute sum rankings consistent partial ranking, necessary condition , compute normalization constantresulting function. conditioning step performed using methodsdescribed paper, normalization constant computed multiplyingnormalization constant factor hierarchical decomposition. Thus, insteadresorting complicated mathematics inversion combinatorics, theorycomplete decomposability offers simple conceptual way understand Mallows modelsconditioned efficiently partial ranking observations.513fiHuang, Kapoor & GuestrinFinally recent related work, Lu Boutilier (2011) considered even generalclass observations based DAG (directed acyclic graph) based observationsprobabilities rankings consistent DAG relative ranking relationsset zero. Lu Boutilier show particular conditioning problemDAG-based class observations #P -hard. additionally propose efficientrejection sampling method performing probabilistic inference within general classDAG observations prove sampling method exact class partialrankings discussed paper.8. Experimentssection, demonstrate method learning hierarchical riffle independent models partial rankings simulated data well real datasets taken differentdomains. experiments, initialize distributions uniform, use random restarts.8.1 Datasetsaddition roughly 5000 full rankings, APA dataset 10,000 top-k rankings5 candidates. previous work, used full rankings APA data (Huang& Guestrin, 2010, 2012), able use entire dataset. Figure 7(a) plots,k {1, . . . , 5}, number ballots APA data length k.Likewise, Meath dataset (Gormley & Murphy, 2007) taken 2002Irish Parliament election 60,000 top-k rankings 14 candidates. APAdata, used full rankings Meath data previous work, useentire dataset. Figure 7(b) plots, k {1, . . . , 14}, number ballotsMeath data length k. particular, note vast majority ballots datasetconsist partial rather full rankings, half electorate preferring listfavorite three four candidates. run inference (Algorithm 1)5000 top-k examples Meath data 10 seconds dual 3.0 GHz Pentium machineunoptimized Python implementation. Using brute force inference, estimatejob would require roughly one hundred years.extracted third dataset database searchtrails collected WhiteDrucker (2007), browsing sessions roughly 2000 users logged 20082009. many cases, users unlikely read articles news story twice,often possible think order user reads collectionarticles top-k ranking articles concerning particular story/topic. abilitymodel visit orderings would allow us make long term predictions user browsingbehavior, even recommend curriculums articles users. ran algorithmsroughly 300 visit orderings eight popular posts www.huffingtonpost.comconcerning Sarah Palin, popular subject 2008 U.S. presidential election. Sinceuser visited every article, full rankings data thuseven exist option learning using subset full rankings.514fiEfficient Inference Partial Rankingsnumber votesnumber votes60005000400030002000100020,00010,00000122345number candidates468 10 12 14k(a) APA election data(b) Irish election dataFigure 7: Histograms top-k ballot lengths APA Irish election datasets. Whereasmajority electorate provided full rankings APA election data(probably due fact five candidates), vast majorityvoters Irish election data provided top-3 top-4 choices.{12345}{12345}{12345}{2}{2345}{2}{2345}{1345}{345}{1}{3}{45}(a) Structure learned usingsubset full rankings(out 300 given trainingexamples){345}{1}{5}{34}(b) Structure learned usingtraining examples 1 iteration EM{13}Research{2}Communitypsychologists{45}Clinical(c) Structure learned usingtraining examples structural convergence (3 iterations)Figure 8: Structure learning subset APA dataset (300 rankings, randomlysampled, including full partial rankings).8.2 APA Structure Learning ResultsDue unordinarily large number full rankings APA data, gains madeadditionally using partially ranked data insignificant. better illustrate benefitspartial rankings, subsampled dataset 300 rankings (including full partialrankings) present results smaller dataset. Performing structure learning usingfull rankings 300 training examples (consisting roughly 100 examples),one obtains structure Figure 8(a), seen match correctstructure Figure 3(a) learned using 5000 full rankings. Figures 8(b) 8(c)515fiHuang, Kapoor & Guestrintraining time (seconds)test log-likelihoodx 10 4-2-3-4-5-6EMFlat-EM4training time (seconds)test log-likelihood-5-5.2-5.4-5.6-5.8-6k>21050EMFlat-EM UniformFill-In(b) Training time comparisonEM approach FlatEMUniform Fill-In methods.x 10k>115UniformFill-In(a) Test set log-likelihood comparisonEM approach FlatEMUniform Fill-In methods.k>0202.521.51k>3(c) Test set log-likelihoods, trainingtop-t rankings largerfixed k.k>0k>1k>2k>3(d) Training times, trainingtop-t rankings larger fixedk.Figure 9: APA experimental results experiment repeated 200 bootstrappedresamplings dataplot results EM algorithm former displaying resulting structuresingle EM iteration latter result structural convergence, occursthird iteration, showing method learn correct structure given300 training examples.compared EM algorithm two alternative baseline approachesrefer plots FlatEM Uniform Fill-in. FlatEM algorithmEM algorithm except two details: (1) performs conditioning exhaustivelyinstead exploiting factorized model structure, (2) performs M-step withoutsampling. Uniform Fill-in approach treats every top-k ranking training setuniform collection votes full rankings consistent top-k ranking,accomplished using one iteration EM algorithm.Figure 9(a) plot test set loglikelihoods corresponding approach, EMFlatEM almost identical results performing much betterUniform Fill-in approach. hand, Figure 9(b), compares running timesthree approaches, shows FlatEM far costly (for datasets,cannot even run reasonable amount time).516fiEfficient Inference Partial Rankings1st iteration2nd iteration{0,1,2,3,4,5,6,7}3rd iteration{0,1,2,3,4,5,6,7}{0,1,2,3,4,5,6,7}{5}{0,1,2,3,4,6,7}{5}{0,1,2,3,4,6,7}{5}{0,1,2,3,4,7}{6}{0,1,2,3,4,7}{0,1,2,3,4,6,7}{7}{0,1,2,3,4,7}{0,1,2,3,4}{7}{0,1,2,3,4}{6}{0,2,3}{0,2,3}{1,4}Log likelihood: -818.6579(a){0,2,3}{7}{1,4,6}{1,4}Log likelihood: -769.2369(b)Log likelihood: -767.2760(c)Figure 10: Iterations Structure EM Sarah Palin data structural changesiteration highlighted red. Structural convergence occurs threeiterations. Note structure discovered using visit orders,text information pages incorporated learning process.figure best viewed color.verify partial rankings indeed make difference APA data, plotresults estimating model subsets APA training data consisting top-krankings length larger fixed k. Figures 9(c) 9(d) show log-likelihoodrunning times k = 0, 1, 2, 3 k = 0 entire training set k = 3subset training data consisting full rankings. results show,including partial rankings indeed help average improving test log-likelihood(with diminishing returns).8.3 Structure Discovery EM Larger n.experiments led several observations using EM learning partialrankings. First, observe typical runs converge fixed structure quickly,three EM iterations. Figure 10 shows progress EM Sarah Palindata, whose structure converges third iteration. expected, log-likelihoodincreases iteration, remark structure becomes interpretableexample, leaf set {0, 2, 3} corresponds three posts Palins wardrobeelection, posts leaf set {1, 4, 6} related verbal gaffesmade Palin campaign. Notice structure discovered purely usingdata visit orders text information used experiments.517fiHuang, Kapoor & Guestrin-2.72x 104test log-likelihood# EM iterationsconvergence3025201510EMdecomposableconditioning-2.8[Lebanon & Mao, 08]-2.84-2.8850-2.7600 1 2 3 4 5 6 7 8 9 10 11 12k250 1000 4000 16000 64000# partial rankings training set(in addition full rankings)(a)(b)Figure 11: (a): Number EM iterations required convergence training setcontains rankings length longer k. (b): Density estimation syntheticdata. plot test loglikelihood learning 343 full rankings0 64,000 additional partial rankings.5000 trainingexamples4Test log-likelihoodx 10-4.5-4.625000 trainingexamples5x 10-1.26[LM08][LM08]-1.28-1.3-4.7-1.32[LM08]-4.8[LM08]-1.34-4.9FullMixed (Full+Partial)FullMixed (Full+Partial)Figure 12: Density estimation small (5000 examples) large subsets (25000 examples) Meath data. compare method work LebanonMao (2008) two settings: (1) training available data (2) trainingsubset full rankings.Secondly, number EM iterations required reach convergence log-likelihooddepends types partial rankings observed. ran algorithm subsetsMeath dataset, time training = 2000 rankings length larger518fiEfficient Inference Partial Rankingsfixed k. Figure 11(a) shows number iterations required convergencefunction k (with 20 bootstrap trials k). observe fastest convergencedatasets consisting almost-full rankings slowest convergence consistingalmost-empty rankings, almost 25 iterations necessary one trains using rankingstypes. Finally remark model obtained first iteration EMinteresting thought result pretending voter completelyambivalent regarding n k unspecified candidates.8.4 Value Partial Rankingsverify larger n using partial rankings addition full rankingsallows us achieve better density estimates. first learned models synthetic datadrawn hierarchy, training using 343 full rankings plus varying numbers partialranking examples (ranging 0-64,000). repeat setting 20 bootstraptrials, evaluation, compute log-likelihood testset 5000 examples.speed, learn structure H fix H learn parameters trial.Figure 11(b), plots test log-likelihood function number partialrankings made available training set, shows indeed able learnaccurate distributions data form partial rankings madeavailable.8.5 Comparing Nonparametric ModelComparing performance riffle independent models approaches possible previous work since able handle partial rankings. Usingmethods developed current paper, however, compare riffle independent modelsstate-of-the-art nonparametric estimator Lebanon Mao (2008) (tohereby refer LM08 estimator) data (setting regularization parameter C =1,2,5, 10 via validation set). Figure 11(b) shows (naturally)data drawn synthetically riffle independent model, EM method significantly outperforms LM08 estimator. remark theory, LM08 guaranteedcatch performance (under appropriate conditions) given enough training examples.Meath data, approximately riffle independent, trained subsetssize 5,000 25,000 (testing remaining data). subset, evaluated EMalgorithm learning riffle independent model LM08 estimator (1)using full ranking data, (2) using data. before, methods betterpartial rankings made available.smaller training set, riffle independent model performs well betterLM08 estimator. larger training set 25,000, see nonparametricmethod starts perform slightly better average, advantage nonparametricmodel guaranteed consistent, converging correct model givenenough data. advantage riffle independent models, however, simple,interpretable, highlight global structures hidden within data.519fiHuang, Kapoor & Guestrin9. Future Directionsremain several possible extensions current work. list openquestions extensions following.9.1 Inference Incomplete Rankingsshown paper one exploit riffled independence structure conditionobservation takes form partial ranking. spacepartial rankings rich useful many settings, cover important classobservations: incomplete rankings, defined ranking (or partialranking) subset itemset . example, Theorem 21 shows conditioning problem pairwise observations form Apples preferred Bananasnondecomposable. Note top-k rankings considered complete rankings sinceimplicitly rank items last n k positions.then, tractably condition incomplete rankings? One possible approachconvert Fourier representation using methods (Huang & Guestrin, 2012),conditioning pairwise ranking observation using Fourier domain conditioningalgorithm proposed (Huang et al., 2008). Fourier domain approach would useful one particularly interested low-order marginal probabilities posteriordistributions.Fourier approach viable, another option may assumeposterior distribution takes particular riffle independent structure (in waymean field methods graphical models literature would assume factorizedposterior). research question interest is: hierarchical structure usedpurposes approximating posterior?9.2 Reexamining Data Independence Assumptionspaper, assumed throughout training examples independentidentically distributed. However practice always safe assumptionsnumber factors impact validity both. example, internet surveyuser must perform series preference ranking tasks sequence, concernusers prior ranking tasks may bias results future rankings.Another source bias lies reference ranking may displayed,user asked rearrange items dragging dropping. one hand, showingeveryone reference ranking may bias resulting data. hand,showing every user different reference ranking may mean training examplesexactly identically distributed.Yet another form bias lies partial ranking types reported data.formulate EM algorithm, assumed users preferences influencewhether chooses to, say, report full ranking instead top-3 ranking. practice,however, partial ranking types user preferences often correlated. Irish elections, example, typically one Sinn Fein candidate, rankSinn Fein first typically likely reported top-1 choice.520fiEfficient Inference Partial RankingsUnderstanding, identifying, finally, learning spite different types biasesmay occur eliciting preference data remains fundamental problem ranking.9.3 Probabilistic Modeling Strategic Votinginteresting consider differences actual vote distributions consideredpaper approximate riffle independent distributions. Take APA dataset,example, optimal approximation riffle independent hierarchy reflectsunderlying political coalitions within organization. Upon comparisonapproximation empirical distribution, however, marked differences arise.example, riffle independent approximation underestimates number votes obtainedcandidate 3 (a research psychologist) ultimately election.One possible explanation discrepancy may lie idea voters tend votestrategically APA elections, placing stronger candidates opposing political coalitionslower ranking, rather revealing true preferences. interesting linefuture work lies detecting studying presence strategic voting electiondatasets. Open questions include (1) verifying mathematically whether strategic votingindeed exist in, say, APA election data, (2) so, strategic voting effectstrong enough overwhelm riffled independence structure learning algorithms,(3) strategic voting manifest partial ranking votes.10. Conclusionprobabilistic reasoning problems, often case certain data types suggestcertain distribution representations. example, sparse dependency structure dataoften suggests Markov random field (or graphical model) representation (Friedman,1997, 1998). low-order permutation observations (depending itemstime), recent work (Huang et al., 2009; Kondor, 2008) shown Fourier domainrepresentation appropriate. preference ranking scenarios, one must contendhuman task complexity difficulty involved human rank long list itemsoften leads partially, instead fully ranked data. paper, showndata takes form partial rankings, hierarchical riffle independent modelsnatural representation.conjugate priors, showed riffle independent model guaranteedretain factorization structure conditioning partial ranking (which performed efficiently). surprisingly, work shows observationstake form partial rankings amenable simple multiplicative update basedconditioning algorithms. Finally, showed possible learn hierarchical riffleindependent models partially ranked data, significantly extending applicabilityprevious work.Acknowledgmentsproject formulated largely conducted internship Jonathan HuangMicrosoft Research. Additional work supported part ONR MURIN000140710747, ARO MURI W911NF0810242. Carlos Guestrin funded521fiHuang, Kapoor & Guestrinpart NSF Career IIS-064422. thank Eric Horvitz, Ryen White, Dan Liebling,Yi Mao discussions.Appendix A. Proofsappendix, provide supplementary proofs theoretical resultspaper.A.1 Proof Theorem 19prove Theorem 19 (as well later results), refer rank sets.Definition 32. Given partial ranking type , denote rank set occupiedRi . Note Ri dependswritten R1 = {1, . . . , 1 },PR2 = {1 + 1, . . . , 1 + 2 }, . . . , Rr = { r1i=1 + 1, . . . , n}.refer following basic fact regarding rank sets:Proposition 33. = 1 | . . . |r i, (i ) = Ri .Proof. (of Theorem 19) use induction size itemset. cases n = 1, 2trivial since every distribution S1 S2 factors riffle independently. considergeneral case n > 2.Fix partial ranking = 1 |2 | . . . |r type binary partition itemset subsets B. show indicator function factors as:() = m(AB ()) f (A ()) g(B ()),(A.1)factors m, f g indicator functions set consistent interleavings,[S ]AB , sets consistent relative rankings, [S ]A [S ]B , respectively.Equation A.1 true, shown must decompose respecttop layer H. show decomposes hierarchically, must also showrelative ranking factors fA gB decompose respect HA HB ,subhierarchies item sets B. establish second step (assumingEquation A.1 holds), note fA gB indicator functions restricted partialrankings, [S ]A [S ]B , partial rankings smaller item setsB. inductive hypothesis (and fact B assumed strictlysmaller sets ) shows functions fA gB factor accordingrespective subhierarchies.turn establishing Equation A.1. suffices prove following twostatements equivalent:I. ranking consistent partial ranking (i.e., ).II. following three conditions hold:(a) interleaving AB () consistent (i.e., AB () [S ]AB ),(b) relative ranking () consistent (i.e., () [S ]A ),(c) relative ranking B () consistent (i.e., B () [S ]B ).522fiEfficient Inference Partial Rankings(I II): first show implies conditions (a), (b) (c).(a) , i,|j Ri : AB (j) = A| = |j Ri : 1 (j) A|,= |k : k A|,(by Definition 2)(by Proposition 33)= |i A|.argument (replacing B) shows i, |jRi : AB (j) = B| = |i B|. two conditions (by Definition 17) showAB consistent .(b) , (by Definition 14) ranks items items j< j. Intersecting A, also see ranks itemitem j i, j. Definition 2, () also ranks itemitem j i, j. finally Definition 16 again,see () consistent partial ranking .(c) (Same argument (b)).(II I): assume conditions (a), (b), (c) hold, show .Proposition 33 sufficient show item k , (k) Ri .prove claim, show induction item k A, (k) Ri(and similarly k B, (k) Ri ).Base case. base case (i = 1), assume k 1 A, goal show(k) R1 . condition (a), AB () [S ]AB . Definition 17,means that: |1 A| = {j R1 : [AB ()](j) = A} = {j R1 : 1 (j) A}.words, = |1 A| items lie rank set R1 = {1, . . . , 1 }.show item k maps rank R1 , must show relativeranking elements A, k among first m. condition (b), () [S ]A ,implying item subset 1 occupies first positions relativeranking A. Since k 1 A, item k among first items ranked ()therefore (k) R1 . similar argument shows k 1 B implise(k) R1 .Inductive case. show k A, (k) Ri . condition (b),() [S ]A , implying item subset (and hence, item k) occupiesfirst = |i A| positions relative ranking beyond items i1j=1 (jA). inductive hypothesis mutual exclusivity, items, togetheri1i1j=1 (j B) occupy ranks j=1 Rj , therefore (k) R` ` i.hand, condition (a) assures us |i A| = {j Ri : 1 (j) A}words, ranks Ri occupied exactly items A. Therefore,(k) Ri . Again, similar argument shows k B implies (k) Ri .A.2 pspan Set Always Partial Rankingreason pspan set rankings, first introduce basic conceptsregarding combinatorics partial rankings. collection partial rankings523fiHuang, Kapoor & Guestrinforms partially ordered set (poset) 0 0 obtained 0 0dropping vertical lines. example, S3 , 1|2|3 12|3. Hasse diagramgraph node corresponds partial ranking node x connectednode via edge x exists partial ranking z x z(see Lebanon & Mao, 2008). top Hasse diagram partial ranking 1, 2, . . . , n(i.e., ) bottom Hasse diagram lie full rankings. See Figure 13example partial ranking lattice S3 .Lemma 34. [Lebanon & Mao, 2008] Given two partial rankings , 0 0 ,exists unique supremum 0 0 (a node Ssup sup Ssup sup0 0 Ssup sup , node greater Ssup sup ). Similarly,exists unique infimum 0 0 .Lemma 35. Given two partial rankings , 0 0 , relation 0 0 holdslies 0 0 Hasse diagram.Proof. lies 0 0 Hasse diagram, 0 0 trivial sinceobtained dropping vertical bars 0 0 . given lie0 0 , would like show 0 0 6 . Let Sinf inf unique infimum0 0 guaranteed Lemma 34. definition Hasse diagram,obtained dropping verticals vertical bar representationSinf inf . Since lie 0 0 , must vertical bardropped 0 0 dropped (if exist bar,0 0 ), hence must exist pair items i, j separated single verticalbar unseparated 0 0 . Therefore exists 0 0 (j) < (i)even though exists . conclude 0 0 6 .Lemma 36 (Lemma 23 main body). X Sn , pspan(X) partial ranking.Proof. Consider subset X Sn . partial ranking containing every element Xmust upper bound every element X Hasse diagram Lemma 35.Lemma 34, must exist unique least upper bound (supremum) X, Ssup sup ,common upper bound X, must also ancestor Ssup suphence Ssup sup . therefore see partial ranking containing X mustsuperset Ssup sup . hand, Ssup sup partial ranking containing X.Since pspan(X) intersection partial rankings containing X, pspan(X) =Ssup sup therefore pspan(X) must partial ranking.A.3 Proofs Claim rspan(X) = pspan(X)simplify notation remaining proofs, introduce following definition.Definition 37 (Ties). Given partial ranking = 1 | . . . |r , say items a1a2 tied (written a1 a2 ) respect a1 , a2 i.following basic properties tie relation straightforward.Proposition 38.524fiEfficient Inference Partial Rankings1231|2312|313|22|133|1223|11|2|31|3|22|1|33|1|22|3|13|2|1Figure 13: Hasse diagram lattice partial rankings S3 .I. respect fixed partial ranking , tie relation, , equivalence relationitem set (i.e., reflexive, symmetric transitive).II. exist , 0 disagree relative ranking items a1 a2 ,a1 a2 respect .III. 0 0 , a1 a2 respect , a1 a2 respect 0 0 .IV. a1 a2 respect , (a1 ) < (a3 ) < (a2 ) item a3, a1 a2 a3 .Proposition 39. Given set rankings X input, Algorithm 2 outputs pspan(X).Proof. prove three things, together prove proposition: (1) algorithmterminates, (2) stage elements X contained pspan(X), (3)upon termination, pspan(X) contained element X.1. First note algorithm must terminate finitely many iterationsloop since stage least one vertical bar removed partial ranking,vertical bars removed elements X,disagreements relative ordering.2. show stage algorithm, every element Xt subsetpspan(X). initialization, course, X0 , simply singletonset consisting element X, therefore pspan(X).Suppose pspan(X) every Xt . replacedXt+1 , want show pspan(X) well. Algorithm 2,j, = 1 | . . . |j |j+1 | . . . |r , written 1 | . . . |jj+1 | . . . |r , vertical bar j j+1 deleted due existencepartial ranking Xt , 0 0 Xt disagrees relativeordering items a1 , a2 opposite sides bar. Since 0 0subsets pspan(X) assumption, know a1 a2 respect pspan(X)(Proposition 38, II). Suppose a1 a2 i0 . xi0 , x a1 a2 respect pspan(X) (III)Proposition 38. Moreover, (I, transitivity), see x respectpspan(X). two elements i0 . (IV) Proposition 38,items lying , i+1 , . . . , i0 thus tied respect pspan(X) thereforeremoving bar items a1 a2 (producing, example, ) resultspartial ranking subset pspan(X).525fiHuang, Kapoor & Guestrin3. Finally, upon termination, ranking X contained elementXt , would exist two items a1 , a2 whose relative rankingdisagree upon, contradiction. Therefore, every element Xt containsevery element X thus pspan(X) every Xt .Lemma 40. Let = 1 | . . . |i |i+1 | . . . |k partial ranking item set ,0 0 = 1 | . . . |i i+1 | . . . |k , partial ranking sets i+1merged. Let a1 ij=1 j a2 kj=i+1 j . element Cadditionally exists ranking disagrees relativeordering a1 , a2 , 0 0 O.Proof. fix completely decomposable work h, indicatordistribution corresponding O. Let 0 0 . prove lemma, need establishh() > 0. Let 0 element 0 (k) = (k) k \(i i+1 ).Since supp(h) assumption, h( 0 ) > 0.Since 0 match items except i+1 , exists sequencerankings 0 , 1 , 2 , . . . , = adjacent rankings sequence differpairwise exchange items b1 , b2 i+1 . show stepalong sequence, h( ) > 0 implies h( t+1 ) > 0, prove h() > 0.Suppose h( ) > 0 t+1 differ relative rankingitems b1 , b2 i+1 (without loss generality, assume (b2 ) < (b1 )t+1 (b1 ) < t+1 (b2 )).idea following paragraph use previous lemma (Lemma 28) provet+1 positive probability so, necessary argueexists ranking 0 h( 0 ) > 0 0 (b1 ) < 0 (b2 ) (i.e., 0 disagreesrelative ranking b1 , b2 ). Let element . a1 , rearrangea1 ranked first among elements . a2 i+1 , rearrangea2 ranked last among elements i+1 . Note still elementpossible rearrangements therefore h() > 0. assume (b2 ) < (b1 )since otherwise shown wanted show. Thus relative orderinga1 , a2 , b1 , b2 within a1 |b2 |b1 |a2 . Note treat case items a1 , a2 , b1 , b2distinct, argument follows cases a1 = b2 a2 = b1 .since disagrees relative ordering a1 , a2 assumption (andhence disagrees ), apply Lemma 28 conclude swapping relative orderinga1 , a2 within (obtaining a2 |b2 |b1 |a1 ) results ranking, 0 , h( 0 ) > 0.Finally, observe 0 must disagree relative ranking a2 , b2 ,invoking Lemma 28 shows swap relative ordering a2 , b2 within(obtaining a1 |a2 |b1 |b2 ) result ranking 0 h( 0 ) > 0. element 0 ranksb1 b2 , wanted show.shown exist rankings disagree relative ordering b1b2 positive probability h. applying Lemma 28 shows swaprelative ordering items b1 , b2 within obtain t+1 h( t+1 ) > 0,concludes proof.526fiEfficient Inference Partial RankingsA.4 Uniformity C Functions Partial Rankingthus far shown element C must supported partial ranking.following, show (up certain class exceptions), element mustassign uniform probability members partial ranking.Theorem 41. h completely decomposable function supported partial ranking= 1 | . . . |r |i | =6 2 = 1, . . . , r, h uniform (i.e.,1Qh() =|i | ).establish Theorem 41, must establish two supporting results: (1) Lemma 42factors h r smaller completely decomposable functions, nonzero everywhere domain, (2) Theorem 43 establishes uniformity completelydecomposable function nonzero everywhere domain.Lemma 42. completely decomposable Qfunction, h, supported partial ranking= 1 | . . . |r , must factor as: h() = ri=1 h((i )), factor distributionh((i )) completely decomposable function Si .Proof. Since h completely decomposable, (i ) riffle independent(\i ) i. Since h supported partial ranking = )1 | . . . |r , however,interleaving complement deterministic therefore conclude fact(i ) fully independent(\i ). Since (i ) (\i ) i,Qrfactorization: h() = i=1 h((i )).turn establishing factor h((i )) completely decomposableobservation. Fix = 1 (without loss generality) consider partition set 1subsets B. would like see sets B riffle independentrespect h((1 )). Since h assumed completely decomposable, knowriffle independent complement, B(\1 ). words, B = B(\1 ),variables (), AB , B (the relative ranking A, interleavingremaining items, relative ranking remaining items, respectively) mutuallyindependent. observe (1) interleaving B, AB , deterministicfunction interleaving AB (2) relative ranking B, B , deterministicfunction B , thus proving , AB B mutually independent henceB riffle independent.Theorem 43. Let h completely decomposable function h() > 0 Snn > 2. two rankings 1 , 2 differ single transposition,h(1 ) = h(2 ).proof strategy Theorem 43 involve examining ratio twoprobabilities h(1 ) h(2 ). define operation transforming 1 2 newrankings 10 20 ratio rankings preserved (i.e., h(1 )/h(2 ) =h(10 )/h(20 )). performing sequence ratio-preserving operations, show that:h(1 )h(2 )=,h(2 )h(1 )Theorem 43 easily follows.527fiHuang, Kapoor & Guestrinuse two types operations transform ranking new ranking: (1)changing interleaving two sets B within ranking , (2), changingrelative ranking set within ranking . precisely, given rankingpartitioning item set subsets B, uniquely index triplet(, , B ), = A,B (), = (), B = B (). two operationsdefined follows:1. Changing interleaving A, B within 0 : yields new ranking 0indexed ( 0 , , B ).0 (or 0 ): yields new2. Changing relative ranking (or B) withinB000ranking indexed (, , B ) [or (, , B )].use operations obtain 10 20 , interested conditionstransformation ratio-preserving (i.e., h(1 )/h(2 ) = h(10 )/h(20 )).following lemma provides sufficient conditions ratio-preservation.Lemma 44. Let h completely decomposable function consider 1 , 2 Snh(2 ) > 0. partitioning item set subsets B, have:1. 1 2 match interleaving B (i.e., A,B (1 ) = AB (2 )),h(10 )h(1 )00h(2 ) = h(20 ) , 1 2 formed changing interleaving setsB within 1 2 new interleaving 0 .2. 1 2 match relative ranking (or B) (i.e., (1 ) = (2 ) (orh( 0 )h(1 )= h(10 ) , 10 20 formed changingB (1 ) = B (2 ))), h(2)20relative ranking set (or B) within 1 2 new relative ranking0(or B ).Proof. Since proofs parts 1 2 nearly identical, prove part 1 here.Since h C, sets B riffle independent assumption, hencefactorizations:h(1 )m(1 ) f (1A ) g(2B )=.h(2 )m(2 ) f (2A ) g(2B )1 2 match interleaving sets B, = 1 = 2 ,thus interleaving terms, m(1 ) m(2 ) numeratordenominator.hand, examine ratio h(10 ) h(20 ), also seeinterleaving terms must cancel:h(1 )m(10 ) f (1A ) g(2B )=.h(2 )m(20 ) f (2A ) g(2B )therefore that:f (1A ) g(piBh(10 )h(1 )1 )==.Bh(2 )h(20 )f (2 ) g(2 )528fiEfficient Inference Partial Rankingsestablished Lemma 44, turn establishing three short claims (usinglemma) allow us prove finally prove Theorem 43. interesting noterequire n > 2 (strictly) claim III swap order jnumerator denominator. third item k proof thoughtplaying role dummy variable analogous temporary storage variables onemight use implementing swap function. necessity third item preciselyresult hold special case n = 2.Proposition 45. Let h : Sn R completely decomposable function n > 2h() > 0 Sn . following equivalences (whereratios, entries explicitly written assumed match identicallynumerator denominator).I.II.h(i|j| . . . |k| . . . )h(i|j|k| . . . )=.h(j|i| . . . |k| . . . )h(j|i|k| . . . )h(. . . |i| . . . |j| . . . )h(i|j| . . . )=.h(. . . |j| . . . |i| . . . )h(j|i| . . . )III.h(j|i|k| . . . )h(i|j|k| . . . )=.h(j|i|k| . . . )h(i|j|k| . . . )Proof.I. Equality holds since 1 2 match interleaving sets = {k}B = \{k}. Thus change interleaving B 1 2item k inserted rank 3 preserving ratio.II. Equality holds II since 1 2 match interleaving sets = {i, j}B = \{i, j}. Thus change interleaving B 12 items j occupy first two ranks preserving ratioh(1 ) h(2 ).III. following use 1 2 refer arguments numeratordenominator, respectively, preceding line.h(i|j|k| . . . )h(i|k|j| . . . )=,h(j|i|k| . . . )h(k|i|j| . . . )h(j|i|k| . . . )=,h(j|k|i| . . . )h(i|j|k| . . . )=,h(i|k|j| . . . )h(k|j|i| . . . )=,h(k|i|j| . . . )h(j|i|k| . . . )=,h(i|j|k| . . . )(since 1 , 2 match relative ranking {j, k})(since 1 , 2 match interleaving {j} \{j})(since 1 , 2 match relative ranking {i, j})(since 1 , 2 match relative ranking {i, k})(since 1 , 2 match interleaving {k} \{k}).529fiHuang, Kapoor & GuestrinProof. (of Theorem 43) want show two rankings differ single transposition,assigned equal probability h. Suppose 2 obtained1 swapping ranks items j. Additionally, let k item besides j(such item must exist since n > 2). following, use Proposition 45 showh(1 )/h(2 ) = h(2 )/h(1 ). before, entries explicitly writtenassumed match identically numerator denominator.h(. . . |i| . . . |j| . . . )h(i|j| . . . )h(1 )==, (by Prop. 45, Part II)h(2 )h(. . . |j| . . . |i| . . . )h(j|i| . . . )h(i|j| . . . |k| . . . )h(i|j|k| . . . )==, (by Prop. 45, Part I)h(j|i| . . . |k| . . . )h(j|i|k| . . . )h(j|i|k| . . . ), (by Prop. 45, Part III)=h(i|j|k| . . . )h(j|i| . . . |k| . . . )=, (by Prop. 45, Part I)h(i|j| . . . |k| . . . )h(j|i| . . . )h(. . . |j| . . . |i| . . . )==, (by Prop. 45, Part II)h(i|j| . . . )h(. . . |i| . . . |j| . . . )h(2 )=.h(1 )Since assumed h(1 ) h(2 ) > 0, must conclude h(1 ) = h(2 ).Finally, assemble supporting results prove Theorem 41.Proof. (of Theorem 41) Lemma 42, completely decomposable function h must factoras:rh() =h((i )),(A.2)i=1factor distribution h((i )) completely decomposable function Si .assumption, |i | 6= 2. |i | = 1, corresponding factor h((i )) must triviallyuniform. Otherwise, |i | > 2. latter case, apply Theorem 43h((i )) show must assign equal probability two rankings differsingle transposition. However, given rankings 1 , 2 Si , obtain sequencetranspositions transforms 1 2 , therefore, Theorem 43 fact impliesfactor h((i )) constant inputs. proved factor Equation A.2constant, conclude h must constant support.ReferencesAilon, N. (2007). Aggregation partial rankings, p-ratings top-m lists. Proceedingseighteenth annual ACM-SIAM symposium Discrete algorithms, SODA 07,New Orleans, Louisiana.Bartholdi, J. J., Tovey, C. A., & Trick, M. (1989). Voting schemesdifficult tell won. Social Choice Welfare, 6(2).530fiEfficient Inference Partial RankingsBusse, L. M., Orbanz, P., & Buhmann, J. (2007). Cluster analysis heterogeneous rankdata. 24th Annual International Conference Machine Learning, Corvallis,Oregon.Fligner, M. A., & Verducci, J. S. (1986). Distance based ranking models. JournalRoyal Statistical Society, 48.Freund, Y., Iyer, R., Schapire, R. E., & Singer, Y. (2003). efficient boosting algorithmcombining preferences. Journal Machine Learning Research (JMLR), 4, 933969.Friedman, N. (1997). Learning belief networks presence missing values hidden variables. Proceedings Fourteenth International Conference MachineLearning, ICML 97, pp. 125133, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.Friedman, N. (1998). bayesian structural em algorithm. 14th ConferenceUncertainty Artificial Intelligence, UAI 98, Madison, Wisconsin.Gormley, C., & Murphy, B. (2007). latent space model rank data. Proceedings2006 conference Statistical network analysis, ICML06, pp. 90102, Berlin,Heidelberg. Springer-Verlag.Huang, J., Kapoor, A., & Guestrin, C. (2011). Efficient probabilistic inference partialranking queries. 27th Conference Uncertainty Artificial Intelligence, UAI11, Barcelona, Spain.Huang, J. (2011). Probabilistic Reasoning Learning Permutations: Exploiting Structural Decompositions Symmetric Group. Ph.D. thesis, Carnegie Mellon University.Huang, J., & Guestrin, C. (2009). Riffled independence ranked data. Bengio, Y.,Schuurmans, D., Lafferty, J., Williams, C. K. I., & Culotta, A. (Eds.), AdvancesNeural Information Processing Systems 22, NIPS 08, pp. 799807. MIT Press.Huang, J., & Guestrin, C. (2010). Learning hierarchical riffle independent groupingsrankings. Proceedings 27th Annual International Conference MachineLearning, ICML 10, pp. 455462, Haifa, Israel.Huang, J., & Guestrin, C. (2012). Uncovering riffled independence structure rankeddata. Electronic Journal Statistics, 6, 199230.Huang, J., Guestrin, C., & Guibas, L. (2008). Efficient inference distributions permutations. Platt, J., Koller, D., Singer, Y., & Roweis, S. (Eds.), Advances NeuralInformation Processing Systems 20, NIPS 07, pp. 697704. MIT Press, Cambridge,MA.Huang, J., Guestrin, C., & Guibas, L. J. (2009). Fourier theoretic probabilistic inferencepermutations. Journal Machine Learning Research (JMLR), 10, 9971070.Joachims, T. (2002). Optimizing search engines using clickthrough data. Proceedingseighth ACM SIGKDD international conference Knowledge discovery datamining, KDD 02, pp. 133142, New York, NY, USA. ACM.Kondor, R., Howard, A., & Jebara, T. (2007). Multi-object tracking representationssymmetric group. Meila, M., & Shen, X. (Eds.), Proceedings Eleventh531fiHuang, Kapoor & GuestrinInternational Conference Artificial Intelligence Statistics March 21-24, 2007,San Juan, Puerto Rico, Vol. Volume 2 JMLR: W&CP.Kondor, R. (2008). Group theoretical methods machine learning. Ph.D. thesis, ColumbiaUniversity.Lebanon, G., & Lafferty, J. (2003). Conditional models ranking poset. S. Becker,S. T., & Obermayer, K. (Eds.), Advances Neural Information Processing Systems15, NIPS 02, pp. 415422, Cambridge, MA. MIT Press.Lebanon, G., & Mao, Y. (2008). Non-parametric modeling partially ranked data. Platt,J. C., Koller, D., Singer, Y., & Roweis, S. (Eds.), Advances Neural InformationProcessing Systems 20, NIPS 07, pp. 857864, Cambridge, MA. MIT Press.Lu, T., & Boutilier, C. (2011). Learning mallows models pairwise preferences.28th Annual International Conference Machine Learning, ICML 11, Bellevue,Washington.Marden, J. I. (1995). Analyzing Modeling Rank Data. Chapman & Hall.Meila, M., Phadnis, K., Patterson, A., & Bilmes, J. (2007). Consensus rankingexponential model. Tech. rep. 515, University Washington, Statistics Department.White, R., & Drucker, S. (2007). Investigating behavioral variability web search.Proceedings 16th international conference World Wide Web, WWW 07,Banff, Alberta, Canada. ACM.532fiJournal Artificial Intelligence Research 44 (2012) 383-395Submitted 01/12; published 06/12Research NoteNarrative Planning: Compilations Classical PlanningPatrik HaslumPATRIK . HASLUM @ ANU . EDU . AUAustralian National University, CanberraOptimisation Research Group, NICTAAbstractmodel story generation recently proposed Riedl Young casts planning,additional condition story characters behave intentionally. means charactersperceivable motivation actions take. show condition compiled away (inways one) produce classical planning problem solved off-the-shelfclassical planner, efficiently Riedl Youngs specialised planner.1. Introductionclassical AI planning model, assumes actions deterministic plannercomplete knowledge control world, often thought restricted,many potential applications problems appear requirements fit model. Recently, however, shown problems thought go beyond classical modelnevertheless solved classical planners means compilation, i.e., systematic remodellingproblem classical plan reformulated problem meets also non-classicalrequirements. striking example work Palacios Geffner (2006), showed conformant planning (generating plans robust certain forms uncertainty) compiledclassical planning problem. Another example, closer topic paper, workPorteous, Teutenberg, Pizzi Cavazza (2011), use planner generate variationsdrama encoding constraints sequencing events within it.paper another problem kind: planning fabula, meaning event structure,plot, story.1 fabula planning problem considered formulated Riedl Young(2010). main difference classical planning notion intentionality: actions storytaken different characters, story considered believable, charactersbehave intentionally, i.e., (perceivable) motivations actions take. RiedlYoung argue [the fact classical planners take account character intentions]limits applicability off-the-shelf planners techniques generating stories, developinstead narrative planner, IPOCL, extends traditional partial-order causal link plannermechanism enforce plans respect character intentionality. show fabulaplanning, defined Riedl Young, compiled classical planning problem,hence fact solved off-the-shelf classical planner. preclude usingextended formalism, like introduced Riedl Young, better suited purposemodelling fabula planning problems, since compilation easily automated. advantageobvious: allows bring bear narrative planning problem entirety existing,1. telling story, discourse, distinct fabula (e.g., Gervas 2009). aforementioned workPorteous et al. seen application planning generating different discourses given fabula.c2012AI Access Foundation. rights reserved.fiH ASLUMfuture, work algorithms classical planning, dramatically reduced cost developmenttime effort. surprising find classical planners run compiled problemfar efficient IPOCL algorithm, well capable more, like finding setdiverse plans.different theories distinguishes story arbitrary sequenceevents, i.e., gives storiness (e.g., Gervas 2009; Mateas & Sengers, 1999). aimcriticise particular model narrative planning proposed Riedl Young merelyshow criterion adopted character intentionality achieved classicalplanner without modification, simply restating problem given planner solve.Whether done also models narrative generation open question.2. Narrative Planning Intentional Plansstory King Jafar becomes married Jasmine. magic genie.also story genie dies.(From textual representation story generated IPOCL; Riedl & Young 2010.)Riedl Young observe many parallels plans narrative levelfabula. sequences events change state (story) world. storyperceived coherent plausible, event sequence must logically possible (i.e., preconditions achieved event takes place) connected causes effects (i.e., eventcontributes something story, setting stage later events). goal story,view, end state storys author mind; Riedl Young call storyoutcome, distinguish character goals. story believable, charactersappear intentional agents: characters actions possible, contributeoutcome story, perceivable contributing goals character(which necessarily authors goal). seen non-redundancyrequirement subsets plan: action done character story directlyindirectly contribute achieving goal character. course, goals characterchange throughout course story, influenced characters, eventsworld around them. change characters goals must also cause.illustrate narrative planning, evaluate believability plans generated IPOCL,Riedl Young (appendix A.1, p. 254256) use following small example scenario. dramatis personae are:King Jafar, lives Castle;Aladdin, Knight loyal Jafar;Jasmine, beautiful woman, also lives Castle;Genie, imprisoned Magic Lamp;Dragon, lives Mountain possesses Lamp start story.Characters travel two locations. knight slay monster (only DragonGenie monsters). character take things dead character (pillage), givethings another character (the Lamp item interest). character Lampsummon Genie, thereby gaining control it. Genie, magic, cause characterfall love another character. Two characters love, otherwise engaged,marry. goals story (married Jafar Jasmine) (dead Genie). Notegoals represent story outcome; (initially) intended character.384fiNARRATIVE P LANNING : C OMPILATIONS C LASSICAL P LANNINGRiedl Young distinguish two types planning actions: intentional actions, correspond actions taken one story characters (actors action), happenings,actors correspond accidental events, forces nature, etc. classifications actions intentional happenings, assignment role actor(s) parametersintentional actions, part domain theory. Examples happenings scenariocharacter fall love another beautiful, scary monster frightenanother character.Character intentions modelled modal literals form (intends f ), character f fact, i.e., normal literal. Intentions arise effect actions, either happeningscharacter actions. example, happening (fall-in-love ?man ?woman) effect (loves?man ?woman) establishes intention (intends ?man (married ?man ?woman)). Similarly,action (deliver-witty-insult ?speaker ?hearer ?victim), ?speaker actor, couldeffect (amused ?hearer), also unintended (by speaker) effect (intends ?victim (dead?speaker)). special category actions cause intentions delegating actions, onecharacter commands (or persuades, bribes, otherwise influences) another achieve something.example, (order ?king ?knight ?goal) effect (intends ?knight ?goal).Riedl Young define notion intentionality context partially ordered causallink (POCL) plans.2 following definition summarises definitions 3, 5 6 (pp. 232234)article:Definition 1 intentional plan one every occurrence intentional action partframe commitment. frame commitment subset steps (i.e., action occurrences)plan, associated modal literal (intends g), satisfying four requirements: (1) Character actor every step . (2) final step sfin makes g true. (3)motivating step sm plan, adds (intends g) precedes steps . Wellsay motivational link sm every step frame commitment, . Note smpart . (4) step sfin path causal motivationallinks sfin . complete (fabula) plan one intentional valid classical sense.Condition (4) departs slightly definitions stated Riedl Young: requirestep temporally precedes sfin . would appear promiscuous, sinceallows unrelated action incorporated frame commitment adding spurioustemporal constraints. IPOCL algorithm, however, incorporate step existingframe commitment step causal link step already frame, servemotivating step frame commitment whose final step causal link step alreadyframe (cf. items 1 2 page 235 article). Hence, frames algorithmgenerates always satisfy condition (4).3. Compilation 1: Explicit Justification Trackingfirst compilation based explicit tracking justifications, form causalmotivating links, actions plan. inspired work Karpas Domshlak (2011)pruning redundant action sequences search space, also relies notionjustification actions.2. detailed account POCL planning found paper McAllester Rosenblitt (1991) AItextbooks.385fiH ASLUMuse three kinds modal literals: (intends f ) (delegated f ),character f fact, (justified f I), f fact intention, i.e., modal literalfirst form. intends modality part narrative planning problem specification,appear action effects initial state. modalities used describecompilation. course, modal conditions cannot expressed directly classical planningformalism like PDDL. PDDL model, replaced separate modal predicatepredicate (resp. combination two predicates) appear non-modal fact, whose arguments concatenation arguments modal literal. example, (intends Aladdin (hasJafar Lamp)) replaced (intends-has Aladdin Jafar Lamp), (justified (at Aladdin Mountain)(intends Aladdin (has Jafar Lamp))) replaced (justified-at-has Aladdin Mountain Aladdin JafarLamp).compiled problem, intentional action associated intention actionsactor(s). intention precondition action. action achieveintention, creates outstanding obligation make use least one effects achieveprecondition action, done actor, contributes, directly indirectly,achieving intention. modelled justified modality. explained earlier, actionsmodal effects form (intends B f ), i.e., make character (different actor)intend goal. modelled delegated modality.intentional action, (a ~x), narrative planning problem, compiled problemone distinct action combination intention (intends xA (p ~y )), xAparameter represents actor (a ~x), effect (e ~z) (a ~x). name compiledaction (a-e-because-intends-p ~x ~y ), call (e ~z) chosen effect. Note parameters ~zchosen effect composed subset parameters ~x action, possibly explicitconstants. Action (a-e-because-intends-p ~x ~y ) read character xA performs action (a ~x)achieve effect (e ~z) step towards achieving characters intended goal (p ~y ). (e ~z)unify (p ~y ), action must broken two cases: one forcedequal one forced distinct. intentional action oneactor, compiled problem must distinct action (possible relevant) choiceeffect intention actor. happening (i.e., action without actor) onecorresponding action compiled problem.justified delegated modalities combine track causal motivational linkscompiled problem. (possible relevant) justified literals true initial state,required true goal state. Action (a-e-because-intends-p ~x ~y ) makes chosen effectunjustified, deleting (justified (e ~z) (intends xA (p ~y ))). Since goal requires justified literalshold, plan must include action, actor intention, whoseprecondition requires (e ~z); action make (justified (e ~z) (intends xA (p ~y ))) true again.chosen effect modal literal (intends zA (q z~ )) subgoal (q z~ ) becomes unjustified,also becomes delegated second character, zA . provides motivational linkaction (a-e-because-intends-p ~x ~y ) action zA takes achieve (q z~ ). Delegationends character achieves goal. goal delegated, character may achievegoal. ensures step created delegation eventually justified, characterperformed making use achieved fact (q z~ ).Let (a ~x) intentional action, xA parameter represents actor, (e ~z) choseneffect (intends xA (p ~y )) intention actor. preconditions compiled action(a-e-because-intends-p ~x ~y ) are:386fiNARRATIVE P LANNING : C OMPILATIONS C LASSICAL P LANNING(1)(2)(3a)(3b)(3c)(4)preconditions (a ~x);(intends xA (p ~y ));w (delegated w (q z~ )), effect (a ~x) form (intends zA (q z~ ));w 6= xA (delegated w (p ~y )), (p ~y ) effect (a ~x);w (delegated w (q z~ )), effect (q z~ ) (a ~x) intends modal literal.(intends zA (q z~ )), (e ~z) modal literal form (intends zA (q z~ )).plan compiled problem, sets actions associated intention form framecommitment. Precondition (2) ensures steps frame preceded motivating step.Precondition (4) ensures one (intentional) motivating step. Preconditions (3ac)ensure action taken delegates (a) achieves (bc) goal already delegatedanother character.effects compiled action are:(1) effects (a ~x);(2) (justified (q ~v ) (intends xA (p ~y ))), (non-static) precondition (q ~v ) (a ~x).(3a) (justified (q z~ ) (intends xA (p ~y ))) (delegated zA (q z~ )), (e ~z) modal literalform (intends zA (q z~ ));(3b) (justified (e ~z) (intends xA (p ~y ))), (e ~z) intends literal (e ~z) equal(p ~y );(4) (delegated xA (e ~z)), (e ~z) equals (p ~y );Effects (2) (3) make preconditions action justified, chosen effect unjustified,explained above. chosen effect modal intends literal (case 3a), intended subgoalbecomes unjustified, also delegated character. Effect (4) ends delegationgoal action final step frame commitment. (Note, however, actioneffect even goal delegated; matter.)chosen effect (e ~z) unified (p ~y ), compiled action must split two:one additional precondition ~z = ~y , ensuring equal, one additionalprecondition ~z 6= ~y , ensuring not. necessary since effects compiledaction depend whether (e ~z) equals (p ~y ) not. Furthermore, mentioned above, originalaction (a ~x) one actor, compiled problem one action every combinationintention chosen effect actor. case, conditions (p ~y ) (e ~z)schema interpreted actor separately. is, (pi ~y ) (ei ~zi )intention chosen effect actor xiA , compiled action effect (justified (ei ~zi ) (intendsxiA (pi ~y ))) (ei ~zi ) intends literal (ei ~zi ) equal (pi ~y ) (item 3b), regardlesswhether (ei ~zi ) equals (pj ~y j ) j 6= i, vice versa.illustrate compilation, consider following action example scenario RiedlYoung (appendix A.1, p. 255), written PDDL-like syntax:(:action slay:parameters (?knight - knight ?monster - monster ?where - place):actors (?knight):precondition (and (alive ?knight) (at ?knight ?where) (alive ?monster) (at ?monster ?where)):effect (and (not (alive ?monster)) (dead ?monster)))actor action knight. Consider intention (intends ?knight (dead ?who)).action one relevant choice effect, (dead ?monster) (the negative literal appearaction precondition goal). However, since intention unifies chosen effect,387fiH ASLUMcompiled problem must still include two actions, one ?who = ?monster one ?who 6=?monster:(:action slay-1-because-intends-dead:parameters (?knight - knight ?monster - monster ?where - place):precondition (and (alive ?knight) (at ?knight ?where) (alive ?monster) (at ?monster ?where)(intends ?knight (dead ?monster))(not (exists (?c) (and (not (= ?c ?knight)) (delegated ?c (dead ?monster)))))):effect (and (not (alive ?monster)) (dead ?monster)(justified (at ?knight ?where) (intends ?knight (dead ?monster)))(justified (at ?monster ?where) (intends ?knight (dead ?monster)))(not (delegated ?knight (dead ?monster)))))(:action slay-2-because-intends-dead:parameters (?knight - knight ?monster - monster ?where - place ?who - monster):precondition (and (alive ?knight) (at ?knight ?where) (alive ?monster) (at ?monster ?where)(intends ?knight (dead ?who))(not (exists (?c) (delegated ?c (dead ?monster))))(not (= ?who ?monster))):effect (and (not (alive ?monster)) (dead ?monster)(justified (at ?knight ?where) (intends ?knight (dead ?who)))(justified (at ?monster ?where) (intends ?knight (dead ?who)))(not (justified (dead ?monster) (intends ?knight (dead ?who))))))(The alive literals dont need justification, way make true unless trueinitially.) Corresponding intention (intends ?knight (has ?who ?what)) compiled problemaction:(:action slay-because-intends-has:parameters (?knight ?monster ?where ?who ?what):precondition (and (alive ?knight) (at ?knight ?where) (alive ?monster) (at ?monster ?where)(intends ?knight (has ?who ?what))(not (exists (?c) (delegated ?c (dead ?monster))))):effect (and (not (alive ?monster)) (dead ?monster)(justified (at ?knight ?where) (intends ?knight (has ?who ?what)))(justified (at ?monster ?where) (intends ?knight (has ?who ?what)))(not (justified (dead ?monster) (intends ?knight (has ?who ?what))))))prove correctness compilation general, need concept togglingaction (Hickmott & Sardina, 2009). action toggling w.r.t. effect action iffactions precondition implies negation effect. is, action makes true fact f ,precondition must include f , fact f mutex f , action makes f false,must require f true. action toggling transformed equivalent setactions are, though size set exponential number non-toggling effectsoriginal action.Theorem 2 Let P narrative planning problem, action toggling w.r.t. effects.Let P compiled problem described above. Every plan P intentional.Proof: Consider step S, instance action (a-e-because-intends-p ~x ~y ). Letactor (i.e., constant bound actor parameter xA a) (e ~z) chosen effect(a ~x). action part frame commitment goal (p ~y ). construction,388fiNARRATIVE P LANNING : C OMPILATIONS C LASSICAL P LANNINGprecondition (a-e-because-intends-p ~x ~y ) includes (intends (p ~y )). must motivatingstep establishes precondition, otherwise would classically valid.(e ~z) equals (p ~y ), final step frame commitment.(e ~z) equal (p ~y ) modal literal, (a-e-because-intends-p ~x ~y ) destroys(justified (e ~z) (intends (p ~y )). Since literals goals P , must later step, ,re-establishes it. construction, action actor, (intends (p ~y ))associated intention, (e ~z) precondition. step adds(e ~z), must causal link labelled (e ~z) (since actions toggling, (e ~z)true s). cannot step sadd adds (e ~z), so,action associated sadd would applied state one effects, (e ~z), alreadytrue, thus toggling. Suppose steps sdel sadd taking place ,sdel destroys (e ~z) sadd makes true again; several steps, let sdel firstsadd last, causal link sadd . Since actions toggling sdel requires(e ~z), causal link sdel . chain causal links sdel sadd ,subplan consisting steps including causal predecessors sadd (whichinclude sdel ) must executable, results execution action associated saddapplied state one effects, (e ~z), already true, hence toggling.Thus, must chain causal links sdel sadd , therefore . Sincepart frame commitment (it motivating intention),finite number steps frame commitment causally follow s, repeated applicationreasoning leads conclusion must chain causal links finalstep frame.(e ~z) modal literal form (intends zA (q z~ )), (a-e-because-intends-p ~x ~y ) destroys(justified (q z~ ) (intends (p ~y )), adds (delegated zA (q z~ )). above, must step, frame commitment s, re-establishes (justified (q z~ ) (intends (p ~y )).construction, (delegated w (q z~ )) true one character w time (any actionadds delegation requires character it), character currently holdingdelegation (q z~ ) make true. Thus, (by argument above) causalchain final step delegates frame commitment goal (q z~ ) .actions compiled problem toggling w.r.t. intends literals, step must causal chainprecondition (intends zA (q z~ )) action frame commitment delegate,thus serves motivating step frame. Thus, chain motivating causallinks , following argument above, therefore final stepframe commitment belongs to.2may noted apparently reasonable story plans disallowed. example, charactercannot delegate goal intends another character. This, however, consequenceRiedl Youngs definition intentional plans, compilation (and hence appliesalso IPOCL planner): final step frame commitment must achieve intendedgoal must action actor holds intention (conditions 1 & 2 Definition1). rules delegating ones goals. desired, would difficult modifycompilation allow kind secondary delegation: requries adding exception w 6=xA precondition (3a) effect like (4) case. plan also cannot charactertrying failing multiple means achieve goals. Again, consequence RiedlYoungs definition, compilation: every action taken character must chain389fiH ASLUMcausal motivational links final step (condition 4 Definition 1). rules characterstaking actions prove ultimately futile.Theorem 2 shows compilation sound. question whether also complete, i.e.,whether existence intentional plan narrative planning problem P always implies existenceplan compiled problem P , somewhat complicated. first glance, given intentionalplan P , appears plan compiled problem P could constructed selectingaction suitable representative a-. . .-because-. . ., intentions chosen effectsmatch frames commitment belongs S. Since intentional, framecommitment preceded motivating step, ensuring intends preconditions compiledaction satisified, final step, causal link least one effectsanother step, ensuring deleted justified literals restored. is, however, one pointcorrespondence fail, due restriction compiled problem delegatedgoal achieved character delegated to: Suppose character delegatesgoal g character B, i.e., character performs intentional action whose (relevant) effect(intends B g). plan intentional, must frame commitment belongingcharacter B, associated intention (intends B g); frame must final stepachieves g, step must source causal link step performed character A,belonging frame commitment step established motivation. Yet,nothing prevents another character, C, achieving g purposes, long characterB also achieves g. compiled problem, however, allow character C achieve g longdelegated B, i.e., motivating step final step B. couldremedied elaborate justification tracking mechanism, distinguishesfact achieved different characters.practical perspective, combinations actions intentions, modal literals,present compiled problem restricted possible relevant.example, initial state goal needs include justified literals actuallynegated possibly applicable action (which found standard relaxed reachabilityanalysis). example scenario, fact character Lamp never causally contribute to, e.g., goal character another item (there items have)goal murdering another character. Thus, actions like pillage-because-intends-dead order-hasbecause-intends-has order-has-because-intends-dead never part valid plan.information could found simple techniques like back-chaining relevance analysis.Applying compilation Riedl Youngs example scenario, applying classical planner, using forward-chaining A* search LM-Cut heuristic (Helmert & Domshlak, 2009),compiled problem, produces plan shown figure 1. planner outputs sequenceactions, transformed partially ordered plan polynomial time post-processing step(Backstrom, 1998). Enumerating shortest plans reveals two variations: one Jafar travelsback Castle marry Jasmine, one Jafar orders Aladdin bring Lamp,climactic events (the wedding Aladding slaying Genie) take place Castle.(The latter one Riedl Young report found IPOCL, shown Figure 15, p. 259,article.) Note possible Jafar command Aladdin make (loves JasmineJafar) true, Aladdin means achieve goal delegatingGenie, which, explained above, permitted definition frame commitment.Finding shortest plans end itself: rather, side effect fact plannerusually seek achieve story outcome simplest way. somewhat odds390fiNARRATIVE P LANNING : C OMPILATIONS C LASSICAL P LANNINGFigure 1: story plan generated example problem. Motivational links drawn gray.dashed edge ordering constraint. outlines group actions form framecommitment character. avoid clutter, causal links justified predicatesshown; instead, causal links chosen effect action drawn bold.seen chosen effect, except final steps, links (directly indirectly)precondition action frame commitment.391fiH ASLUMmaking story interesting. Porteous Cavazza (2009) argue complexification, i.e.,making story convoluted order make interesting, achieved postingadditional author goals form PDDL3 trajectory constraints, specifying fact mustachieved point plan; fact must never true point;fact must achieved another. PDDL3 trajectory constraints also compiled away(Gerevini, Haslum, Long, Saetti & Dimopoulos, 2008). Methods generating diverse setplans (Srivastava, Kambhampati, Nguyen, Do, Gerevini & Serina, 2007) could also usedautomate complexification.total time generate plan around 45 seconds (and that, half actual search;rest grounding preprocessing.) time required compilation lesssecond. stark contrast running time IPOCL planner problem, reported12 hours even problem-specific search heuristic (Riedl & Young, 2010). However,example represents small problem. contains actions objects necessaryform intended story plan, more. realistic scenario problem specificationcontains many possible actions objects relevant story outcome,allow construction materially different plans goal. size compiled problemgrow quite quickly size original narrative planning problem increases.example, larger version problem, including three actions items,none directly relevant achieving outcome, takes nearly 30 minutes solve.4. Compilation 2: Meta-PlanningMagic Lamp, thought Jafar. could summon Genie gain controlit. controlled Genie, could command make Jasmine love me.second compilation based simulating characters process forming intentionsmaking plans, using explicit character planning actions. similarity Wolfe Russells (2011) use explicit establishment intentions means guide plan search efficiently. Compared justification-tracking compilation, less complex also less stringent:plans meta-planning compiled problem guaranteed intentional, accordingdefinition Riedl Young, although time be.meta-planning action allows character adopt intention achieving preconditionaction achieves goal character already intends. avoid characters making plansnever act on, counter tracks number intentions character has, requiredzero end plan.3 counter represented standard propositionalencoding (though limits depth intentions character hold), numeric fluent.Let (a ~x) intentional action, xA parameter represents actor, (e ~z) choseneffect. corresponding action compiled problem additional precondition (intendsxA (e ~z)) effects (intends xA (e ~z)) decreases xA intention count 1. words,take action, actor must one effects current set intentions, performingaction releases actor intention. (e ~z) modal literal form (intends zA(q z~ )), precondition effect refer instead (q z~ ), action also increases intentioncount zA , i.e., effect move (q z~ ) intention set xA character zA .Happenings add character intentions must also increase intention count.3. exceptions must made: example, character dies, obviously cannot act outstanding intentions, invalidate plan.392fiNARRATIVE P LANNING : C OMPILATIONS C LASSICAL P LANNINGprecondition (p ~y ) (a ~x), compiled problem also action (plan-to-a ~x),precondition (intends xA (e ~z)) effects (intends xA (p ~y )) increasing xA intentioncount. (a ~x) several preconditions, order achievement among enforcedadding subsets preconditions meta-planning actions. example, action (give?who ?what ?to-who ?where) preconditions: (has ?who ?what); (at ?who ?where); (at?to-who ?where). Adding (has ?who ?what) precondition meta-planning actionestablishes (intends ?who (at ?who ?where)) forces character intends give somethingplan acquire item, actually so, planning travel (if necessary)place recipient gift is. necessary reasonable constraints orderachievement action preconditions may found landmark ordering analysis (Hoffmann,Porteous, & Sebastia, 2004). Manually adding constraints meta-planning actions givesflavour (a simulation of) methods HTN planning (Erol, Nau, & Hendler, 1994).reason meta-planning compilation guarantee intentional plansforces characters motivate action plan, force monitorplans still valid action takes place. example, Aladdin plans slay Dragonorder pillage Lamp, thief steals Lamp Dragon Aladdinway Mountain, Aladdin still license slay Dragon, even though longercontributes getting Lamp (in fact, must slay Dragon avoid leftunfulfilled intention). part, could rectified encoding elaborate structurecharacter plans set outstanding goals. example, directed graph encodingcould track dependency relations intentions, dependence story world facts.may also provide basis allowing characters revise plans face changedcircumstances.Limited computational experiments meta-planning compilation suggestproduces much smaller (ground) problems justification-tracking compilation,still harder current heuristic search-based planners solve.5. ConclusionResearch classical planning problem developed wide array of, sometimes highlyeffective, methods solving problems. compilations, capabilities existingclassical planners leveraged solve many problems surfaceappear classical planning problems. Like loyal knight story, classical plannercommittedly try solve whatever task set it, expressed planning domainspecification. trick setting right task.noted, narrative planning model defined Riedl Young limitations.example, allow create story character tries fails achieve goal.Brenner (2010) describes approach story generation interleaves classical planningindividual characters goals, based characters state knowledge, plan execution, i.e.,adding events story. permits system generate stories characters forcedabandon plans learning new facts, postpone planning crucial facts become known.Brenner claims would quite difficult describe [such plot] single plan, let alonegenerate single planner run. indeed appear quite difficult, whetherimpossible remains open question.393fiH ASLUMAcknowledgmentswish thank Alban Grastien, Malte Helmert, Robert Mattuller reviewers usefulcomments drafts paper. work supported Australian Research Council discovery project DP0985532 Exploiting Structure AI Planning. NICTA fundedAustralian Government represented Department Broadband, CommunicationsDigital Economy Australian Research Council ICT Centre Excellence program.ReferencesBackstrom, C. (1998). Computational aspects reordering plans. Journal AI Research, 9, 99137.Brenner, M. (2010). Creating dynamic story plots continual multiagent planning. Proc. 24thAAAI Conference Artificial Intelligence, pp. 15171522.Erol, K., Nau, D., & Hendler, J. (1994). HTN planning: Complexity expressivity. Proc.National Conference Artificial Intelligence (AAAI94), pp. 11231128.Gerevini, A., Haslum, P., Long, D., Saetti, A., & Dimopoulos, Y. (2008). Deterministic planningfifth international planning competition: PDDL3 experimental evaluationplanners. Artificial Intelligence, 173(5-6), 619668.Gervas, P. (2009). Computational approaches storytelling creativity. AI Magazine, 30(3),4962.Helmert, M., & Domshlak, C. (2009). Landmarks, critical paths abstractions: Whats difference anyway?. Proc. 19th International Conference Automated Planning Scheduling (ICAPS09).Hickmott, S., & Sardina, S. (2009). Optimality properties planning via Petri net unfolding: formal analysis. Proc. 19th International Conference Automated Planning Scheduling(ICAPS09), pp. 170177.Hoffmann, J., Porteous, J., & Sebastia, L. (2004). Ordered landmarks planning. Journal AIResearch, 22, 215278.Karpas, E., & Domshlak, C. (2011). Living edge: Safe search unsafe heuristics. Proc.ICAPS11 Workshop Heuristics Domain-Independent Planning, pp. 5358.Mateas, M., & Sengers, P. (1999). Narrative intelligence. Narrative Intelligence: PapersAAAI Fall Symposium. AAAI Press.McAllester, D., & Rosenblitt, D. (1991). Systematic nonlinear planning. Proc. 9th NationalConference Artificial Intelligence.Palacios, H., & Geffner, H. (2006). Compiling uncertainty away: Solving conformant planningproblems using classical planner (sometimes). Proc. 21st National Conference Artificial Intelligence (AAAI06).Porteous, J., & Cavazza, M. (2009). Controlling narrative generation planning trajectories:role constraints. Proc. 2nd International Conference Interactive Digital Storytelling,pp. 234245.394fiNARRATIVE P LANNING : C OMPILATIONS C LASSICAL P LANNINGPorteous, J., Teutenberg, J., Pizzi, D., & Cavazza, M. (2011). Visual programming plan dynamics using constraints landmarks. Proc. 21st International Conference AutomatedPlanning Scheduling (ICAPS11), pp. 186193.Riedl, M., & Young, R. (2010). Narrative planning: Balancing plot character. Journal AIResearch, 39, 217268.Srivastava, B., Kambhampati, S., Nguyen, T., Do, M., Gerevini, A., & Serina, I. (2007). Domainindependent approaches finding diverse plans. Proc. 20th International ConferenceArtificial Intelligence (IJCAI07), pp. 20162022.Wolfe, J., & Russell, S. (2011). Bounded intention planning. Proc. 22nd InternationalJoint Conference AI (IJCAI11), pp. 20392045.395fiJournal Artificial Intelligence Research 44 (2012) 179-222Submitted 10/10; published 05/12Improving Statistical Machine TranslationResource-Poor LanguageUsing Related Resource-Rich LanguagesPreslav Nakovpnakov@qf.org.qaQatar Computing Research InstituteQatar FoundationTornado Tower, Floor 10, P.O. Box 5825Doha, QatarHwee Tou Ngnght@comp.nus.edu.sgDepartment Computer ScienceNational University Singapore13 Computing DriveSingapore 117417Abstractpropose novel language-independent approach improving machine translationresource-poor languages exploiting similarity resource-rich ones. precisely, improve translation resource-poor source language X1 resourcerich language given bi-text containing limited number parallel sentences X1 -Ylarger bi-text X2 -Y resource-rich language X2 closely relatedX1 . achieved taking advantage opportunities vocabulary overlapsimilarities languages X1 X2 spelling, word order, syntax offer:(1) improve word alignments resource-poor language, (2) augmentadditional translation options, (3) take care potential spelling differencesappropriate transliteration. evaluation IndonesianEnglish using MalaySpanishEnglish using Portuguese pretending Spanish resource-poor showsabsolute gain 1.35 3.37 BLEU points, respectively, improvement best rivaling approaches, using much less additional data. Overall,method cuts amount necessary real training data factor 25.1. IntroductionRecent developments statistical machine translation (SMT), e.g., availability efficient implementations integrated open-source toolkits like Moses (Koehn, Hoang, Birch,Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, Dyer, Bojar, Constantin, &Herbst, 2007), made possible build prototype system decent translationquality language pair days even hours. theory. practice,requires large set parallel sentence-aligned texts two languages (bitexts) language pair. large high-quality bi-texts rare; except Arabic,Chinese, official languages European Union (EU), 6,500+ worldlanguages remain resource-poor SMT viewpoint.c2012AI Access Foundation. rights reserved.fiNakov & Ngnumber resource poor languages becomes even striking consider language pairs instead individual languages. Moreover, even resource-rich language pairscould poor bi-texts specific domain, e.g., biomedical.manually creating small bi-text could relatively easy, building large onehard time-consuming. Thus, publicly available bi-texts SMT come parliament debates legislation multi-lingual countries (e.g., French-English Canada,Chinese-English Hong Kong), international organizations like UnitedNations European Union. example, Europarl corpus parliament proceedings consists 1.3M parallel sentences (up 44M words) per language 11languages (Koehn, 2005), JRC-Acquis corpus provides comparable amount European legislation 22 languages (Steinberger, Pouliquen, Widiger, Ignat, Erjavec, Tufis,& Varga, 2006).Due increasing volume EU parliament debates ever-growing Europeanlegislation, official languages EU especially privileged SMT perspective. includes classic SMT languages English French (whichalready resource-rich), important international ones like Spanish Portuguese,many rest limited number speakers resource-poor yearsago. Thus, becoming official language EU turned easy recipegetting resource-rich bi-texts quickly.aim tap potential EU resources used nonEU languages closely related one official languages EU. ExamplesEUnon-EU language pairs include SwedishNorwegian, BulgarianMacedonian1 ,Romanian-Moldovan2 other. Croatia joins EU, Serbian, Bosnian,Montenegrin3 also able benefit Croatian gradually turning resource-rich (allfour languages split Serbo-Croatian breakup Yugoslavia 90sremain mutually intelligible). newly-made EU-official (and thus resourcerich) Czech Slovak languages another possible pair candidates. SpanishCatalan,Irish-Gaelic Scottish, Standard GermanSwiss German, ItalianMaltese4 goodexamples. see below, even resource-rich languages like Spanish Portuguese benefit proposed approach. course, many pairs closely relatedlanguages could make use others bi-texts also found outside Europe:one example MalayIndonesian, experimenting below.non-EU language pairs could potentially benefit include Modern Standard ArabicDialectical Arabic (e.g., Egyptian, Levantine, Gulf, Iraqi Arabic), MandarinCantonese,RussianUkrainian, TurkishAzerbaijani, HindiUrdu, many other.1. heated linguistic debate whether Macedonian represents separate languageregional literary form Bulgarian. Since clear criteria distinguishing dialectlanguage, linguists divided issue. Politically, Macedonian language recognizedBulgaria (which refers official language Republic Macedonia accordanceconstitution) Greece (mostly dispute use name Macedonia).2. Macedonian, debate existence Moldovan language. linguistsgenerally agree Moldovan one dialects Romanian, politically, national languageMoldova called Moldovan Romanian.3. serious internal political division Montenegro whether national languagecalled Montenegrin Serbian.4. Though, Maltese might benefit Arabic Italian.180fiImproving SMT Resource-Poor Languagepropose using bi-texts resource-rich language pairs build better SMTsystems resource-poor pairs exploiting similarity resource-poor language resource-rich one. precisely, build phrase-based SMT systemstranslate resource-poor language X1 resource-rich language given smallbi-text X1 -Y much larger bi-text X2 -Y , X1 X2 closely related.motivated observation related languages tend (1) similar wordorder syntax, and, importantly, (2) overlapping vocabulary, e.g., casa (house)used Spanish Portuguese; also (3) similar spelling. vocabulary overlap means resource-rich auxiliary language used sourcetranslation options words cannot translated resources availableresource-poor language. actual text, vocabulary overlap might extend individual words short phrases (especially resource-rich languages transliteratedlook like resource-poor one), means translations whole phrases couldpotentially reused related languages. Moreover, vocabulary overlapsimilarity word order used improve word alignments resourcepoor language biasing word alignment process additional sentence pairsresource-rich language. take advantage opportunities: (1) improveword alignments resource-poor language, (2) augment additional translation options, (3) take care potential spelling differencesappropriate transliteration.apply approach IndonesianEnglish using Malay SpanishEnglishusing Portuguese Italian (and pretending Spanish resource-poor), achievingsizable performance gains (up 3.37 BLEU points) using additional bi-textsrelated resource-rich language. show approach outperformsbest rivaling approaches, using less additional data. Overall, cut amountnecessary real training data factor 25.approach based phrase-based SMT model (Koehn, Och, & Marcu, 2003),commonly used state-of-the-art model today. However, general ideaseasily extended SMT models, e.g., hierarchical (Chiang, 2005), treelet(Quirk, Menezes, & Cherry, 2005), syntactic (Galley, Hopkins, Knight, & Marcu, 2004).remainder article organized follows: Section 2 provides overviewrelated work, Section 3 presents motivating example several languages, Section 4introduces proposed approach discusses various alternatives, Section 5 describesdatasets use, Section 6 explains transliterate Portuguese Italian looklike Spanish automatically, Section 7 presents experiments discusses results,Section 8 analyses results detail, and, finally, Section 9 concludes suggestspossible directions future work.2. Related Workgeneral problem formulation special case domain adaptation. Moreover,three basic concepts central work: (1) cognates related languages,(2) machine translation closely related languages, (3) pivoting statisticalmachine translation. review previous work topics below, alsomentioning related work whenever appropriate.181fiNakov & Ng2.1 Domain AdaptationDomain adaptation (or transfer learning) problem arises situations training test data come different distributions, thus violating fundamentalassumption statistical learning theory. problem instance special casedomain adaptation, in-domain data scarce, plenty out-of-domaindata. Many efficient techniques developed domain adaptation natural language processing; see work Daume Marcu (2006), Jiang Zhai (2007a, 2007b),Chan Ng (2005, 2006, 2007), Dahlmeier Ng (2010) examples.Unfortunately, techniques directly applicable machine translation,much complicated, leaves lot space variety proposed solutions.despite limited previous work domain adaptation SMT,focused almost exclusively adapting European parliament debates news domainpart annual competition machine translation evaluation WMT workshop.mention proposed approaches, Hildebrand, Eck, Vogel, Waibel(2005) use information retrieval techniques choose training samples similartest set way adapt translation model, Ueffing, Haffari, Sarkar(2007) adapt translation model semi-supervised manner using monolingual datasource language. Snover, Dorr, Schwartz (2008) adapt translationlanguage model, using comparable monolingual data target language. NakovNg (2009b) adapt translation model phrase-based SMT combining phrasetables using extra features indicating source phrase; use combinationtechnique part proposed approach below. Finally, Daume Jagarlamudi (2011)address domain shift problem mining appropriate translations unseen words.2.2 CognatesCognates defined pairs source-target words similar spelling (and thus likelysimilar meaning), example, developpement French vs. development English. Manyresearchers used likely cognates co-occurring parallel sentences training bi-textimprove word alignments ultimately build better SMT systems.Al-Onaizan, Curin, Jahr, Knight, Lafferty, Melamed, Och, Purdy, Smith, Yarowsky(1999) extracted likely cognates Czech-English, using one variationslongest common subsequence ratio LCSR (Melamed, 1995) described Tiedemann(1999) similarity measure. used cognates improve word alignmentsIBM models 14 three different ways: (1) seeding parameters IBM model1, (2) constraining word co-occurrences training IBM models 14, (3)adding cognate pairs bi-text additional sentence pairs. last approachperformed best later used Kondrak, Marcu, Knight (2003) demonstrated improved SMT nine European languages. extended Nakov,Nakov, Paskaleva (2007), combined LCSR sentence-level co-occurrencesbi-text competitive linking (Melamed, 2000), language-specific weights, Webn-gram frequencies.Unlike approaches, extract cognates source targetlanguage, use cognates source related languagedifferent target. Moreover, implicitly rely existence cognates;182fiImproving SMT Resource-Poor Languagetry extract all, leave original sentence contexts.5Note approach orthogonal kind cognate extraction originaltraining bi-text, thus two combined (which Section 7.7).Another relevant line research using cognates adapt resources one languageanother one. example, Hana, Feldman, Brew, Amaral (2006) adapt Spanishresources Brazilian Portuguese train part-of-speech tagger.Cognates cognate extraction techniques used many applications,e.g., automatic translation lexicon induction. example, Mann Yarowsky (2001)induce translation lexicons resource-rich language (e.g., English) resourcepoor language (e.g., Portuguese) using resource-rich bridge language closely relatedlatter (e.g., Spanish). use pre-existing translation lexicons sourceto-bridge mapping step (e.g., English-Spanish), string distance measures findingcognates bridge-to-target step (e.g., Spanish-Portuguese). work extendedSchafer Yarowsky (2002), later Scherrer (2007), relies graphemicsimilarity inducing bilingual lexicons Swiss German Standard German.Koehn Knight (2002) describe several techniques inducing translation lexiconsmonolingual corpora. Starting unrelated German English corpora, look(1) identical words, (2) cognates, (3) words similar frequencies, (4) wordssimilar meanings, (5) words similar contexts. bootstrapping process,new translation pairs added lexicon iteration.recent work automatic lexicon induction includes Haghighi, Liang,Berg-Kirkpatrick, Klein (2008), Garera, Callison-Burch, Yarowsky (2009).Finally, lot research string similarity applied cognateidentification: Ristad Yianilos (1998) Mann Yarowsky (2001) use minimum edit distance ratio MEDR weights learned automatically usingstochastic transducer. Tiedemann (1999) Mulloni Pekar (2006) learn automatically regular spelling changes two related languages, incorporatesimilarity measures based LCSR MEDR, respectively. Kondrak (2005) proposesformula measuring string similarity based LCSR correction addressesgeneral preference short words. Klementiev Roth (2006) Bergsma Kondrak (2007) propose discriminative frameworks measuring string similarity. RappoportLevent-Levi (2006) learn substring correspondences cognates, using string-levelsubstitutions framework Brill Moore (2000). Finally, Inkpen, Frunza, Kondrak(2005) compare several orthographic similarity measures cognate extraction.cognates typically extracted related languages, wordssimilar spelling unrelated languages well, e.g., Arabic, Chinese, Japanese,Korean proper names transliterated English, uses different alphabet. Seework Oh, Choi, Isahara (2006) overview comparison differenttransliteration models, well proceedings annual NEWS named entities workshop, features shared tasks transliteration mining generation (Li & Kumaran,2010). Transliteration modeled using character-based machine translation techniques(Matthews, 2007; Nakov & Ng, 2009a; Tiedemann & Nabende, 2009), relatedcharacter-based SMT model Vilar, Peter, Ney (2007), Tiedemann (2009).5. However, experiments, extract cognates training transliteration systemresource-rich source language X2 resource-poor one X1 .183fiNakov & Ng2.3 Machine Translation Closely Related LanguagesYet another relevant line research machine translation closely related languages, arguably simpler general SMT, thus handled using wordfor-word translation manual language-specific rules take care necessary morphological syntactic transformations. tried number language pairsincluding CzechSlovak (Hajic, Hric, & Kubon, 2000), TurkishCrimean Tatar (Altintas &Cicekli, 2002), IrishScottish Gaelic (Scannell, 2006), among others. recently,Apertium open-source machine translation platform http://www.apertium.org/developed, uses bilingual dictionaries manual rules translatenumber related languages, including SpanishCatalan, SpanishGalician, OccitanCatalan, Macedonian-Bulgarian. contrast, language-independent, statistical approach, different objective: translate third language X.special case line research translation dialectslanguage, e.g., Cantonese Mandarin (Zhang, 1998), dialectlanguage standard version language, e.g., Arabic dialect(e.g., Egyptian) Modern Standard Arabic (Bakr, Shaalan, & Ziedan, 2008; Sawaf,2010; Salloum & Habash, 2011). again, manual rules and/or language-specific toolstypically used. case Arabic dialects, complication arisesinformal status dialects, standardized used formal contextsrather informal online communities6 social networks, chats, TwitterSMS messages. causes mismatch domain genre.Thus, translating Arabic dialects Modern Standard Arabic requires, amongthings, normalizing informal text formal form. fact, generalproblem, arises informal sources like SMS messages Tweets language(Han & Baldwin, 2011). main focus coping spelling errors, abbreviations, slang, typically addressed using string edit distance, also takingpronunciation account. different task, try reuse good,formal text one language help improve SMT another language.closely related relevant line research language adaptation normalization,done specifically improving SMT another language. example, Marujo,Grazina, Lus, Ling, Coheur, Trancoso (2011) described rule-based system adapting Brazilian Portuguese (BP) European Portuguese (EP), used adapt BPEnglish bi-texts EPEnglish. Unlike work, heavily relied language-specificrules, approach statistical largely language-independent; importantly,different objective: translate third language X.2.4 PivotingAnother relevant line research improving SMT using additional languages pivots.Callison-Burch, Koehn, Osborne (2006) improved phrase-based SMT SpanishFrench English using source-language phrase-level paraphrases extracted usingpivoting technique Bannard Callison-Burch (2005) eight additional languagesEuroparl corpus (Koehn, 2005).6. Egyptian Wikipedia one notable exception.184fiImproving SMT Resource-Poor Languageexample, using German pivot, extracted English paraphrases parallel English-German bi-text looking English phrases alignedGerman phrase: e.g., control check aligned unter controlle,hypothesized paraphrases probability. Spanish/French paraphrasesadded additional entries phrase table SpanishEnglish/FrenchEnglishphrase-based SMT system paired English translation original Spanish/French phrase. system tuned minimum error rate training (MERT)(Och, 2003), adding extra feature penalizing low-probability paraphrases; yieldedhuge increase coverage (from 48% 90% test word types 10K trainingsentence pairs used), 1.8 BLEU points absolute improvement.Unlike kind pivoting, improve source-language lexical coverage,augment source- target-language sides. Second, pivoting ignorescontext extracting paraphrases, take account. Third, usingadditional language one related source, able get increase BLEUcomparable even better pivoting achieves eight pivot languages.negative side, approach limited requires auxiliary languageX2 related source language X1 , pivoting language Zrelated X1 target language . However, need one additional parallelcorpus (for X2 -Y ), pivoting needs two: one X1 -Z one Z-Y . Finally, noteapproach orthogonal pivoting, thus two combined (whichSection 7.8).note pivoting general technique, widely usedstatistical machine translation, e.g., triangulation, one wants build FrenchGerman machine translation system French-English English-German bi-text,without access French-German bi-text. case, pivoting donesentence-level, e.g., cascading translation systems, first translating FrenchEnglish, translating English German (de Gispert & Mario, 2006; Utiyama& Isahara, 2007) phrase-level, e.g., using phrase table composition,done off-line (Cohn & Lapata, 2007; Wu & Wang, 2007), integrateddecoder (Bertoldi, Barbaiani, Federico, & Cattoni, 2008). also shownpivoting outperform direct translation, e.g., translating Arabic Chinese couldwork better using English pivot done directly (Habash & Hu, 2009). Moreover,argued English might always optimal choice pivot language(Paul, Yamamoto, Sumita, & Nakamura, 2009). Finally, pivoting techniques alsoused word-level, e.g., translation lexicon induction Japanese Germanusing English (Tanaka, Murakami, & Ishida, 2009), improving word alignments (Filali& Bilmes, 2005; Kumar, Och, & Macherey, 2007). Pivot languages also usedlexical adaptation (Crego, Max, & Yvon, 2010).Overall, general pivoting techniques aim build machine translationsystem new (resource-poor) language pair X-Y , assuming existence bi-texts X-ZZ-Y auxiliary pivoting language Z, e.g., would useful translatingMalay Indonesian, pivoting English. contrast, interestedbuilding better system translating X X Z, e.g.,Indonesian English. assume bi-text X-Z small, oneZ-Y large, require X closely related languages.185fiNakov & NgAnother related line research statistical multi-source translation, focusestranslating text given multiple source languages single target language (Och& Ney, 2001; Schroeder, Cohn, & Koehn, 2009). situation arises small numberresource-rich languages context United Nations European Union,could hardly expected resource-poor languages.3. Motivating ExampleConsider Article 1 Universal Declaration Human Rights:human beings born free equal dignity rights. endowedreason conscience act towards one another spiritbrotherhood.let us see translated closely related Malay Indonesiandissimilar Spanish Portuguese.3.1 Malay IndonesianMalay (aka Bahasa Malaysia) Indonesian (aka Bahasa Indonesia) closely relatedAstronesian languages, 180 million speakers combined. Malay officialMalaysia, Singapore Brunei, Indonesian national language Indonesia.two languages mutually intelligible great extent, differ orthography/pronunciation vocabulary.Malay Indonesian use unified spelling system based Latin alphabet,exhibit occasional differences orthography due diverging pronunciation, e.g.,kerana vs. karena (because) Inggeris vs. Inggris (English) Malay Indonesian,respectively. rarely, differences historical, e.g., wang vs. uang (money).two languages differ substantially vocabulary, mostly loan words,Malay typically follows English pronunciation, Indonesian tends followDutch, e.g., televisyen vs. televisi, Julai vs. Juli, Jordan vs. Yordania. wordsLatin origin end -y English, Malay uses -i, Indonesian uses -as, e.g.,universiti vs. universitas, kualiti vs. kualitas.many cognates two languages, also falsefriends, words identically spelled different meanings two languages.example, polisi means policy Malay police Indonesian. also manypartial cognates, e.g., nanti means (future tense marker) later Malaylater Indonesian. result, fluent Malay fluent Indonesian differsubstantially. Consider, example, Malay Indonesian versions Article 1Universal Declaration Human Rights (from official website United Nations):Malay: Semua manusia dilahirkan bebas dan samarata dari segi kemuliaandan hak-hak. Mereka mempunyai pemikiran dan perasaan hati dan hendaklah bertindak di antara satu sama lain dengan semangat persaudaraan.Indonesian: Semua orang dilahirkan merdeka dan mempunyai martabat dan hak-hak yang sama. Mereka dikaruniai akal dan hati nurani danhendaknya bergaul satu sama lain dalam semangat persaudaraan.186fiImproving SMT Resource-Poor LanguageSemantically, overlap substantial, native speaker Indonesian understand Malay version says, would find parts quite fluent.example, 50% overlap individual word level (overlappingwords underlined). fact, actual vocabulary overlap much higher, e.g.,one word Malay text exist Indonesian: samarata.differences due use different morphological forms, e.g., hendaklah vs. hendaknya(conscience), derivational variants hendak (want).course, word choice translation often matter taste, thus differences necessarily required. test this, asked native speaker Indonesianadapt Malay version Indonesian preserving many words possible.yielded following, arguably somewhat less fluent, Indonesian version, sixwords Malay version:Indonesian (closer Malay): Semua manusia dilahirkan bebas dan mempunyai martabat dan hak-hak yang sama. Mereka mempunyai pemikiran danperasaan dan hendaklah bergaul satu sama lain dalam semangat persaudaraan.Note increase average length matching phrases adapted version.3.2 Spanish PortugueseSpanish Portuguese also exhibit noticeable degree mutual intelligibility, differpronunciation, spelling, vocabulary. Unlike Malay Indonesian, however,also differ syntactically exhibit high level spelling differences; seentranslation Article 1 Universal Declaration Human Rights:Spanish: Todos los seres humanos nacen libres e iguales en dignidadderechos y, dotados como estan de razon conciencia, deben comportarse fraternalmente los unos con los otros.Portuguese: Todos os seres humanos nascem livres e iguais em dignidadee em direitos. Dotados de razao e de consciencia, devem agir uns para com osoutros em esprito de fraternidade.see exact word-level overlap Spanish Portuguesequite low: 17% only. Still, see overlap level short phrases,word level.Spanish Portuguese share 90% vocabulary thus observed leveloverlap may appear surprisingly low. reason many cognates twolanguages exhibit minor spelling variations. variations stem different rulesorthography, e.g., senhor vs. senor Portuguese Spanish, also duegenuine phonological differences. example, Portuguese suffix -cao correspondsSpanish suffix -cion, e.g., evolucao vs. evolucion. Similar systematic differences existverb endings like -ou vs. -o (for 3rd person singular, simple past tense), e.g., visitou vs.visito, -ei vs. -e (for 1st person singular, simple past tense), e.g., visitei vs. visite.also occasional differences apply particular word only, e.g., dizer vs. decir,Mario vs. Mario, Maria vs. Mara.187fiNakov & NgGoing back example, ignore spelling variations cognatestwo languages, overlap jumps significantly:Portuguese (cognates transliterated Spanish):Todos los seres humanos nacen libres e iguales en dignidad en derechos.Dotados de razon de conciencia, deben agir unos para con los otros enesprito de fraternidad.words sentence Spanish, differences officialSpanish version due different word choice translator; fact, sentencebecome fluent Spanish agir unos par changed comportarse los unos con.4. Methodexamples suggest may feasible use bi-texts one language improve SMT related language, possibly suitable transliteration cognatesadditional language match target spelling.Thus, describe two general strategies improving phrase-based SMTresource-poor language X1 target language , using bi-text X2 -Yrelated resource-rich language X2 : (a) bi-text concatenation, possible repetitionsoriginal bi-text balance, (b) phrase table combination, bi-textused build separate phrase table, two phrase tables combined.discuss advantages disadvantages general strategies, proposehybrid approach combines strengths trying avoid limitations.4.1 Concatenating Bi-textssimply concatenate bi-texts X1 -Y X2 -Y one large bi-text usetrain SMT system. offers several potential benefits.First, yield improved word alignments sentences came X1 -Ybi-text, e.g., since additional sentences provide new contexts rare wordsbi-text, thus potentially improving alignments, turn could yield betterphrase pairs. Rare words known serve garbage collectors (Brown, Della Pietra,Della Pietra, Goldsmith, Hajic, Mercer, & Mohanty, 1993) IBM word alignmentmodels. Namely, rare source word tends align many target language words ratherallowing stay unaligned align source words. problemlimited IBM word alignment models (Brown, Della Pietra, Della Pietra, & Mercer, 1993);also exists HMM model Vogel, Ney, Tillmann (1996). See Graca, Ganchev,Taskar (2010) detailed discussion examples garbage collector effect.Moreover, concatenation provide new source-language side translation options, thusincreasing lexical coverage reducing number unknown words; also providenew useful non-compositional phrases source-language side, thus yielding fluenttranslation output. also offers new target-language side phrases known source phrases,could improve fluency providing translation options language modelchoose from. Finally, inappropriate phrases including words X2 existX1 match test-time input, inappropriate new target-language translationsstill chance filtered language model.188fiImproving SMT Resource-Poor LanguageHowever, simple concatenation problematic. First, concatenating smallbi-text X1 -Y much larger one X2 -Y , latter dominate wordalignment phrase extraction, thus hugely influencing lexical phrase translationprobabilities, yield poor performance. counter-acted repeatingsmall bi-text several times large one dominate. Second, since bitexts merged mechanically, way distinguish phrases extractedbi-text X1 -Y coming bi-text X2 -Y . formertarget language pair thus probably preferred, using latteravoided since might contain inappropriate translations words X1 .example, phrase pair Indonesian-English bi-text could (correctly) translatepolisi police, one Malay-English bi-text could (correctly Malay,inappropriately Indonesian) translate policy. Malay word polisiIndonesian word polisi false friends.experiment combining original additional training bi-textfollowing three ways:cat1: simply concatenate original additional training bi-text formnew training bi-text, use train phrase-based SMT system.catk: concatenate k copies original one copy additional trainingbi-text form new training bi-text. value k selected originalbi-text approximately matches size additional bi-text.catk:align: concatenate k copies original one copy additionaltraining bi-text form new training bi-text. generate word alignmentsconcatenated bi-text. throw away sentence pairs alignments,except one copy original bi-text. Thus, effectively induce word alignmentsoriginal bi-text only, using concatenated bi-text estimatestatistics them. use alignments build phrase tableoriginal bi-text.first second method represent simple balanced bi-text concatenation,respectively. third method version second one, additional bi-textused improve word alignments original bi-text, usedphrase extraction. Thus, isolates effect improved word alignments effectimproved vocabulary coverage additional training bi-text provide. cat1catk:align basic building blocks sophisticated approach below.4.2 Combining Phrase Tablesalternative way making use additional training bi-text resource-richlanguage pair X2 -Y order train improved phrase-based SMT system X1build separate phrase tables X1 -Y X2 -Y , (a) used together,e.g., alternative decoding paths, (b) merged, e.g., using one extra featuresindicate bi-text phrase pair came from, (c) interpolated, e.g., using simple linearinterpolation.189fiNakov & NgBuilding two separate phrase tables offers several advantages. First, preferablephrase pairs extracted bi-text X1 -Y clearly distinguished (or givenhigher weight linear interpolation compared to) potentially riskier onesX2 -Y bi-text. Second, lexical phrase translation probabilities combinedprincipled manner. Third, using X2 -Y bi-text, much largerX1 -Y problematic more: dominate case simpleconcatenation above. Finally, bi-text merging, many additional sourceand target-language phrases, offer new translation options. negative side,opportunity lost improve word alignments sentences X1 -Y bi-text.experiment following three phrase table combination strategies:Two-tables: build two separate phrase tables, one two bi-texts,use alternative decoding paths (Birch, Osborne, & Koehn, 2007).Interpolation: build two phrase tables, Torig Textra , originaladditional bi-text, respectively, use linear interpolation combinecorresponding conditional probabilities: Pr(e|s) = Prorig (e|s) + (1 ) Prextra (e|s).optimize value development dataset, i.e., run MERT mergedphrase tables generated using different values , choose value givesrise phrase table achieves highest tuning BLEU score. orderreduce search space, try five values (.5, .6, .7, .8 .9), i.e.,reduce tuning discrete set, use four conditionalprobabilities phrase table.Merge: build two separate phrase tables, Torig Textra , originaladditional training bi-text, respectively. concatenate them, giving priority Torig follows: keep source-target phrase pairs Torig , addingsource-target phrase pairs Textra present Torig .source-target phrase pair added, retain associated conditional probabilities (forward/reverse phrase translation probability, forward/reverse lexicalizedphrase translation probability) phrase penalty.7 add threeadditional features entry new table: F1 , F2 , F3 . value F11 source-target phrase pair originated Torig , 0.5 otherwise. Similarly,F2 =1 source-target phrase pair came Textra , F2 =0.5 otherwise.value F3 1 source-target phrase pair Torig Textra ,0.5 otherwise. Thus, three possible feature value combinations: (1;0.5;0.5),(0.5;1;0.5) (1;1;1); last one used phrase pair TorigTextra . experiment using (1) F1 only, (2) F1 F2 , (3) F1 , F2 , F3 .set weights phrase table features, including standard fiveadditional three, using MERT. optimize number additional features(one, two, three) development set, i.e., run MERT phrase tablesone, two, three extra features choose phrase table achievedhighest BLEU score tuning, suggested work Nakov (2008).7. theory, also re-normalize probabilities since may sum one. practice,important since log-linear phrase-based SMT model require featuresprobabilities all, e.g., F1 , F2 , F3 , phrase penalty probabilities.190fiImproving SMT Resource-Poor Language4.3 Proposed ApproachTaking account potential advantages disadvantages two generalstrategies, propose approach tries get best them, namely: (i )improved word alignments X1 -Y , biasing word alignment process additionalsentence pairs X2 -Y , (ii ) increased lexical coverage, using additional phrasepairs X2 -Y bi-text provide. achieved using Merge combinephrase tables catk:align cat1. process described detailfollows:1. Build balanced bi-text Brep , consists X1 -Y bi-text repeated k timesfollowed one copy X2 -Y bi-text. Generate word alignments Brep ,truncate them, keeping word alignments one copy X1 -Y bi-text. Useword alignments extract phrases, build phrase table Trep trunc .2. Build bi-text Bcat simple concatenation bi-texts X1 -Y X2 -Y .Generate word alignments Bcat , extract phrases, build phrase table Tcat .3. Generate merged phrase table combining Trep trunc Tcat . merging givespriority Trep trunc uses extra features indicating origin entrycombined phrase table.5. Datasetsexperiment following bi-texts monolingual English data:Indonesian-English (in-en):train: 28,383 sentence pairs (0.8M, 0.9M words);dev: 2,000 sentence pairs (56.6K, 63.3K words);test: 2,000 sentence pairs (58.2K, 65.0K words);monolingual English en : 5.1M words.Malay-English (ml-en):train: 190,503 sentence pairs (5.4M, 5.8M words);dev: 2,000 sentence pairs (59.7K, 64.5K words);test: 2,000 sentence pairs (57.9K, 62.4K words);monolingual English en ml : 27.9M words.Spanish-English (es-en):train: 1,240,518 sentence pairs (35.7M, 34.6M words);dev: 2,000 sentence pairs (58.9K, 58.1K words);test: 2,000 sentence pairs (56.2K, 55.5K words);monolingual English en es:pt : 45.3M words (the pt-en it-en).191fiNakov & NgPortuguese-English (pt-en):train: 1,230,038 sentence pairs (35.9M, 34.6M words).dev: 2,000 sentence pairs (59.3K, 58.5K words);test: 2,000 sentence pairs (56.5K, 55.7K words);monolingual English en es:pt : 45.3M words (the es-en it-en).Italian-English (it-en):train: 1,565,885 sentence pairs (43.5M, 44.1M words);dev: 2,000 sentence pairs (56.8K, 57.7K words);test: 2,000 sentence pairs (57.4K, 60.3K words);monolingual English en es:it : 45.3M words (the es-en pt-en).lengths sentences bi-texts limited 100 tokens.language pairs, development testing bi-text, 2,000 parallelsentence pairs. made sure development testing bi-texts shared sentences training bi-texts; excluded monolingual English datasentences English sides development testing bi-texts.training bi-text datasets es-en, pt-en, it-en built v.3Europarl corpus, excluding Q4/2000 portion data (2000-10 2000-12),created testing development datasets.built in-en bi-texts comparable texts downloaded Web.translated Indonesian texts English using Google Translate, matched8English texts using cosine similarity measure heuristic constraintsbased document length words sentences, overlap numbers, words uppercase, words title. Next, extracted pairs sentences matcheddocument pairs using competitive linking (Melamed, 2000), retained ones whosesimilarity pre-specified threshold. ml-en bi-text built similarly.pairs languages, monolingual English text training language modelconsists English side corresponding bi-text plus additional English textsource.Note monolingual data training English language modelSpanish, Portuguese, Italian since es-en, pt-en, it-enorigin: fact, exceptions, sentences bi-texts alignedEnglish make es-en-pt-it four-text, since translations (from Englishlanguages) original parliamentary debates. Thus, English sidees-en, pt-en, it-en, unaligned English sentences distribution.case, however, Malay Indonesian, come differentsources different topics discuss issues Malaysia Indonesia, respectively. particular, differ lot use named entities: names persons,locations, organizations talk about. separate monolingual texts train English language models ml-en in-en; see below,indeed yield different performance SMT.8. Note automatic translations used matching only; final bi-text contained automatic translations.192fiImproving SMT Resource-Poor Language6. Transliterationmentioned above, approach relies existence large number cognates related languages. linguists define cognates words derivedcommon root9 (Bickford & Tuggy, 2002), computational linguists typically ignore origin,defining words different languages mutual translations similar orthography (Melamed, 1999; Mann & Yarowsky, 2001; Bergsma & Kondrak, 2007).adopt latter definition.seen Section 3, transliteration helpful languages likeSpanish Portuguese, many regular spelling differences. Thus, buildsystem automatic transliteration Portuguese Spanish, train listautomatically extracted pairs likely cognates. apply system Portugueseside pt-en training bi-text.Classic approaches automatic cognate extraction look non-stopwords similarspelling appear parallel sentences bi-text (Kondrak et al., 2003). case,however, need extract cognates Spanish Portuguese given pt-enes-en bi-texts only, i.e., without pt-es bi-text. Although easy constructpt-es bi-text Europarl corpus, chose since, general, synthesizingbi-text X1 -X2 would impossible: e.g., cannot done ml-in given trainingdatasets in-en ml-en since English sides sentences common.Thus, extracted list likely cognates Portuguese Spanishtraining pt-en es-en bi-texts using English pivot follows: startedIBM model 4 word alignments, extracted four conditional lexical translationprobabilities: Pr(pj |ei ) Pr(ei |pj ) Portuguese-English, Pr(sk |ei ) Pr(ei |sk )Spanish-English, pj , ei , sk stand Portuguese, English Spanishword, respectively. Following Wu Wang (2007), induced conditional lexicaltranslation probabilities Pr(pj |sk ) Pr(sk |pj ) Portuguese-Spanish follows:Pr(pj |sk ) =Pr(pj |ei , sk ) Pr(ei |sk )PAssuming pj conditionally independent sk given ei , simplify this:Pr(pj |sk ) =Pr(pj |ei ) Pr(ei |sk )PSimilarly, Pr(sk |pj ), obtainPr(sk |pj ) =Pr(sk |ei ) Pr(ei |pj )Pexcluded stopwords, words length less three, containing digits.calculated Prod(pj , sk ) = Pr(pj |sk ) Pr(sk |pj ), excluded PortugueseSpanish word pairs (pj , sk ) Prod(pj , sk ) < 0.01. value 0.01previously suggested filtering phrase pairs obtained using pivoting (Callison-Burch, 2008,2012; Denkowski & Lavie, 2010; Denkowski, 2012). remaining pairs, extractedlikely cognates based Prod(pj , sk ) orthographic similarity pj sk .Following Melamed (1995), measured orthographic similarity using longestcommon subsequence ratio (lcsr), defined follows:9. E.g., Latin tu, Old English thou, Greek su, German du cognates meaning 2nd person singular.193fiNakov & Nglcsr(s1 , s2 ) =|LCS(s1 ,s2 )|max(|s1 |,|s2 |)lcs(s1 , s2 ) longest common subsequence s1 s2 , |s| length s.retained likely cognates pairs lcsr 0.58 higher; valuefound Kondrak et al. (2003) optimal number language pairsEuroparl corpus.Finally, performed competitive linking (Melamed, 2000), assuming Portuguese wordform one Spanish best cognate match. Thus, using valuesProd(pj , sk ), induced fully-connected weighted bipartite graph. Then, performedgreedy approximation maximum weighted bipartite matching graph, i.e.,competitive linking, follows: First, accepted cognates cross-lingual pair (pj , sk )highest Prod(pj , sk ) graph, discarded words pj sk consideration. Then, accepted next highest-scored pair, discardedinvolved wordforms forth. process repeated matchableword pairs left.Note cognate extraction algorithm three components: (1) orthographic,based lcsr, (2) semantic, based pivoting English, (3) competitive linking.semantic component important makes extraction false friendsunlikely. Consider example Spanish-Portuguese word pairs largo largolargo longo. latter pair true cognates, former pair falsefriends since largo means long Spanish wide Portuguese. word largo appears8,489 times es-en bi-text 432 times pt-en bi-text. However,different meanings, get aligned English word high probability,results low scores conditional probabilities: Pr(pj |sk ) = 0.000464Pr(sk |pj ) = 0.009148; thus, Prod(pj , sk ) = 0.000004, 0.01 threshold.result, false friend pair largo largo get extracted. contrast,true cognate pair largo longo get extracted corresponding conditionalprobabilities 0.151354 0.122656, respectively, product 0.018564,0.01 (moreover, lcsr = 0.6, 0.58 threshold).competitive linking component helps prevent issues related word inflectioncannot handled using pivoting alone. example, word green SpanishPortuguese two forms: verde singular, verdes plural. Without competitive linking, would extract verde verde (Prod(pj , sk ) = 0.353662)verdes verdes (Prod(pj , sk ) = 0.337979), also incorrect word pairs verde verdes(Prod(pj , sk ) = 0.109792) verdes verde (Prod(pj , sk ) = 0.106088). Competitive linking, however, prevents asserting Portuguese Spanish wordone true cognate, effectively eliminates wrong pairs.Thus, taken together, semantic component competitive linking make extraction false friends unlikely. Still, occasionally, get wrong alignmentsintrusa intrusas, singular form matched plural form,occurs mostly case rare words like intrusa (intruder, feminine) whose alignmentstend unreliable, inflected forms available competitivelinking choose from.Note described transliteration system focusing precision lessrecall. extracted likely cognate pairs going used train194fiImproving SMT Resource-Poor LanguageSMT-based transliteration system. system translation component,able generate many options, target language model component,would help filter options. translation component tend generategood options, thus needs trained primarily instances systematic, regulardifferences, evolucao evolucion, suffix change -cao -cionlearned. Occasional differences dizer decir cannot generalized thusless useful (they also less frequent, thus missing arguablyimportant), simply memorized model whole words still used.also note focus precision cognate pair extraction meangoing extract primarily cognate pairs spelling differences.explained above, spelling one component cognate pair extraction approach;also semantic competitive linking component, could eliminate manycandidates close spelling prefer others dissimilarities (recall correctchoice largo longo wrong largo largo).Note generality transliteration approach necessarily compromisedfact LCSR requires languages use writing system. example, Cyrillic-written Serbian Roman-written Croatian still compared usingLCSR, initial letter-by-letter mapping Cyrillic Roman alphabets, generally straightforward. course, even using alphabet,languages different orthographical conventions, might make lookdivergent actual phonetics would suggest, e.g., compare qui/chi, gui/ghi,glio/llo Spanish Italian. Even though LCSR Italian-Spanish cognateschi qui lower threshold 0.58, correspondence stringsstill learned longer cognates, e.g., macchina maquina. wouldallow transliteration system convert chi qui word.Going back actual experiments, result cognate extraction procedure,ended 28,725 Portuguese-Spanish cognate pairs, 9,201 (or 32.03%)spelling differences. pair list cognate pairs, added spacestwo adjacent letters wordforms, appended start endcharacters ^ $. example, cognate pair evolucao evolucion became^ e v l u c $ ^ e v l u c n $randomly split resulting list training (26,725 pairs) developmentdataset (2,000 pairs), trained tuned character-level phrase-based monotoneSMT system similar Finch Sumita (2008) transliterate Portuguese wordformSpanish wordform. used Spanish language model trained 14M word tokens(obtained above-mentioned 45.3M-token monolingual English corpus excluding punctuation, stopwords, words length less three, containing digits):one per line character-separated added start end charactersexample. set maximum phrase length language model order ten;found values tuning development dataset. tuned system usingMERT, saved feature weights. tuning BLEU 95.22%, baselineBLEU, leaving Portuguese words intact, 87.63%.195fiNakov & NgFinally, merged training tuning datasets retrained. usedresulting system saved feature weights transliterate Portuguese sidetraining pt-en bi-text, yielded new ptes -en training bi-text.repeated procedure Italian-English. extracted 25,107 ItalianSpanish cognate pairs, 14,651 (or 58.35%) spelling differences. Then,split list training (23,107 pairs) development dataset (2,000 pairs),trained character-level phrase-based monotone SMT system Spanish-English;tuning BLEU 94.92%. used resulting system transliterate Italianside training it-en bi-text, thus obtaining new ites -en training bi-text.also applied transliteration Malay Indonesian, even though knewspelling differences two languages rare. extracted 5,847 likelycognate pairs, 844 (or 14.43%) spelling differences, used traintransliteration system. highest tuning BLEU 95.18% (for maximum phrase sizeLM order 10), baseline 93.15%. re-trained systemcombination training development datasets, transliterated Malayside training ml-en bi-text, yielded new mlin -en training bi-text.7. Experiments Evaluationdescribe baseline system, perform various experiments assesssimilarity original (Indonesian Spanish) auxiliary languages(Malay Portuguese). improve IndonesianEnglish SpanishEnglish SMTusing Malay Portuguese, respectively, auxiliary languages.also take closer look improving SpanishEnglish SMT, performing numberadditional experiments. First, try using additional language dissimilarSpanish, substituting Portuguese Italian. Second, experiment two auxiliarylanguages simultaneously: Portuguese Italian. Finally, combine methodtwo orthogonal rivaling approaches: (1) using cognates source targetlanguage (Kondrak et al., 2003), (2) source-language side paraphrasing pivotlanguage (Callison-Burch et al., 2006).7.1 Baseline SMT Systembaseline, used following setup: first tokenized lowercased sidestraining bi-text. built separate directed word alignments EnglishXXEnglish (X{Indonesian, Spanish}) using IBM model 4 (Brown, Della Pietra, DellaPietra, & Mercer, 1993), combined using intersect+grow heuristic (Koehnet al., 2007), extracted phrase pairs maximum length seven. thus obtainedphrase table phrase pair associated five standard parameters:forward reverse phrase translation probabilities, forward reverse lexical translation probabilities, phrase penalty. trained log-linear model using standardSMT feature functions: trigram language model probability, word penalty, distance-based10distortion cost, parameters phrase table.10. also tried lexicalized reordering (Koehn, Axelrod, Mayne, Callison-Burch, Osborne, & Talbot, 2005).yielded higher absolute BLEU scores, relative improvement sample experimentssimilar achieved distance-based re-ordering.196fiImproving SMT Resource-Poor Languageset weights optimizing BLEU (Papineni, Roukos, Ward, & Zhu, 2002) usingMERT separate development set 2,000 sentences (Indonesian Spanish),used beam search decoder (Koehn et al., 2007) translate 2,000 test sentences(Indonesian Spanish) English. Finally, detokenized output, evaluatedlowercased gold standard using BLEU.7.2 Cross-lingual Translation Experiments#123456Trainml-enmlin -enml-enml-enml-enmlin -enDevml-enml-enml-enin-enin-enin-enTestml-enml-enin-enin-enin-enin-enLMenmlenmlenmlenmleninenin10K44.9338.9913.6913.9815.5616.4420K46.9840.9614.5814.7516.3817.3640K47.1541.0214.7614.9116.5217.6280K48.0441.8815.1215.5117.0418.14160K49.0142.8115.8416.2717.9019.15Table 1: Malay-Indonesian cross-lingual SMT experiments: training Malaytesting Indonesian different number training ml-en sentence pairs. Columns 2-5 present bi-texts used training, development,testing, monolingual data used train English language model.following columns show resulting BLEU (in %) different numbers mlen training sentence pairs. Lines 1-2 show results training, tuning,testing Malay, followed lines 3-6 results training Malay testingIndonesian. mlin stands Malay transliterated Indonesian, enmlenin refer English side ml-en in-en bi-text, respectively.Here, study similarity original auxiliary languages.First, measured vocabulary overlap original auxiliary languages. Spanish Portuguese, feasible since training pt-en es-enbi-texts time span Europarl corpus English sides largelyoverlap. found 110,053 Portuguese 121,444 Spanish word types pt-en esen bi-texts, respectively, 44,461 identical, means 40.40%Spanish word types present Portuguese side pt-en bi-text. Unfortunately,could directly measure vocabulary overlap Malay Indonesianway since English sides in-en ml-en bi-texts overlap content.Second, following general experimental setup baseline system, performedcross-lingual experiments, training one language pair testing another one,order assess cross-lingual similarity Indonesian-Malay Spanish-Portuguese,potential combining corresponding training bi-texts. results shownTables 1 2. see, cross-lingual evaluation training ml-en (pt-en)instead in-en (es-en), testing (es) text yielded huge decrease BLEUcompared baseline: three times (for Malay) five times (for Spanish) evenlarge training datasets, even proper English LM development datasetused: compare line 1 lines 3-5 Table 1, line 1 lines 3-4 Table 2.197fiNakov & Ng#1234567Trainpt-enptes -enpt-enpt-enptes -enes-enes-enDevpt-enpt-enpt-enes-enes-enes-enes-enTestpt-enpt-enes-enes-enes-enes-enpt-enLMenes:ptenes:ptenes:ptenes:ptenes:ptenes:ptenes:pt10K21.2810.914.404.918.1822.872.9920K23.1111.564.775.129.0324.713.1440K 80K24.43 25.7212.16 12.504.57 5.025.64 5.829.97 10.6625.80 27.083.33 3.54160K26.4312.834.996.3511.3527.903.37320K27.1013.275.326.8712.2628.463.94640K 1.23M27.78 27.9613.48 13.715.085.346.447.1012.69 13.7929.51 29.904.183.99Table 2: Portuguese-Spanish cross-lingual SMT experiments: training Portuguese testing Spanish different number training pt-ensentence pairs. Lines 1-2 show results training, tuning, testingPortuguese, lines 3-5 training Portuguese testing Spanish,lines 6-7 training Spanish testing Spanish Portuguese.Columns 2-5 present bi-texts used training, development, testing,monolingual data used train English language model. followingcolumns show resulting BLEU (in %) different numbers training sentence pairs. ptes stands Portuguese transliterated Spanish. EnglishLMs pt-en es-en (marked enes:pt ).Portuguese-Spanish, show results direction, trainingSpanish testing Portuguese: compare line 6 line 7 Table 2. results showcomparable, slightly larger, drop BLEU direction. carry reversedirection experiments Malay-Indonesian since enough parallel in-en data.Third, experimented transliteration changing Malay look like IndonesianPortuguese look like Spanish. caused BLEU score double Spanish(compare line 5 lines 3-4 Table 2, improved far less Indonesian (compare line 6lines 3-5 Table 1). Training transliterated data testing Malay/Portugueseyielded 10% relative decrease Malay 50% Portuguese11 : compare line 1line 2 Tables 1 2. Thus, unlike Spanish Portuguese, found far less systematicspelling variations Malay Indonesian. closer inspection confirmed this: manyextracted likely Malay-Indonesian cognate pairs spelling differences fact formsword existing languages, e.g., kata berkata (to say).One interesting result Table 1 switching language model trained enmlone trained enin yields significant improvements (compare lines 4 5 Table 1).may appear striking since former monolingual English text five timesbigger latter one, yet, smaller language model yields better results.due partial domain shift, especially, respect named entities: even thoughtexts English domain, discuss events different countries,involve country-specific cities, companies, political parties leaders; goodlanguage model able prefer good English translations named entities.11. Interestingly, lines 2 5 Table 2 show, system trained 1.23M transliterated ptes -en sentencepairs performs equally well translating Portuguese Spanish input text: 13.71% vs. 13.79%.198fiImproving SMT Resource-Poor Language7.3 Improving IndonesianEnglish SMT using MalayFigure 1: Impact k BLEU catk different number extra ml-ensentence pairs IndonesianEnglish SMT. Shown BLEU scoresdifferent numbers k = 1,2,. . .,16 repetitions in-en concatenated10000n pairs ml-en, n {1,2,4,8,16}.First, study impact k catk. IndonesianEnglish SMT using Malayadditional language. tried values k 1k16 10000n extraml-en sentence pairs, n{1,2,4,8,16}. see Figure 1, highest BLEU scoresachieved (n; k){(1;2),(2;2),(4;4),(8;7),(16;16)}, i.e., k n. Thus, orderlimit search space, used relationship k n experiments (alsoPortuguese Spanish). note lot fluctuation resultsFigure 1, probably due small sizes training corpora. Givenfluctuation, results over-interpreted, e.g., may chancepeaks different curves right places. Still, overall tendencyvisible: need keep balance original auxiliary bi-texts.Tables 3 4 show results experiments improving IndonesianEnglish SMTusing 10K, 20K, . . ., 160K additional pairs ml-en parallel sentences. Table 3 comparesperformance approach baseline three concatenation methods described Section 4.1: cat1, catk, catk:align, Table 4 comparesperformance approach various alternative ways combining two phrase tables, namely, using alternative decoding paths, phrase table interpolation, phrase tablemerging, introduced Section 4.2.199fiNakov & Ngin-en28.4K28.4K28.4K28.4K28.4Kml-en10K20K40K80K160KBaseline23.80<23.80<23.80<23.80<23.80<cat124.29<24.37<24.3824.17<24.43<catk24.29<(1)24.48(2)24.54(4)24.65<(8)<25.00(16)catk:align24.01<(1)<24.35<(2)<24.39<(4)24.18<(8)24.27<(16)approach24.51(2;1) (+0.72)<24.70(2;2) (+0.90)<24.73(4;2) (+0.93)<24.97(8;3) (+1.17)<25.15(16;3) (+1.35)<Table 3: Improving IndonesianEnglish SMT using different numbers additional Malay-English sentence pairs (varying amount additionaldata): concatenations, repetitions, truncations, approach.baseline 28,383 in-en sentence pairs only. Shown BLEU scores (in %)different approaches. subscript shows best parameter value(s) founddevelopment set used test set produce given result: firstvalue number repetitions original bi-text second value,any, number extra features added phrase table. BLEUscores statistically significantly better baseline/our approachmarked left/right side < (for p < 0.01) (for p < 0.05).in-en28.4K28.4K28.4K28.4K28.4Kml-en10K20K40K80K160KBaseline23.80<23.80<23.80<23.80<23.80<Two Tables23.79<24.24<24.27<24.11<<24.58<Interpolation23.89<(.9)24.22<(.8)24.27<(.8)24.46<(.8)<24.58<(.8)Merge23.97<(3)24.46<(3)24.43(3)<24.67(3)<24.79(3)approach24.51(2;1) (+0.72)<24.70(2;2) (+0.90)<24.73(4;2) (+0.93)<24.97(8;3) (+1.17)<25.15(16;3) (+1.35)<Table 4: Improving IndonesianEnglish SMT using different numbers additional Malay-English sentence pairs (varying amount additionaldata): comparing approach various alternatives. baseline28,383 in-en sentence pairs only. Shown BLEU scores (in %) different approaches. subscript shows best parameter value(s) founddevelopment set used test set produce given result: mergingmethods, first value number repetitions original bi-textsecond value, any, number extra features added phrase table;interpolation, show weight phrase pairs in-en. BLEUscores statistically significantly better baseline/our approachmarked left/right side < (for p < 0.01) (for p < 0.05).Several interesting general observations Tables 3 4 made. First,using additional Indonesian-English sentences yields better results. Second, oneexception, experiments yield improvements baseline. Third, improvementsalways statistically significant approach, according Collins, Koehn,Kucerovas (2005) sign test.200fiImproving SMT Resource-Poor LanguageOverall, among different bi-text combination strategies, approach performsbest, followed catk, merge, interpolation, close performance;three strategies ones consistently yield higher BLEU numberadditional ml-en sentence pairs grows. Methods like cat1, catk:align, two-tablessomewhat inconsistent respect. latter method performs worstone go baseline (for 10K ml-en sentence pairs).One possible reason relatively bad performance two-tables couldtune weights compared models: phrase tablefeature weights, means five additional features. well known MERT cannothandle many features (Chiang, Knight, & Wang, 2009; Hopkins & May, 2011),believe case takes 3035 iterations finish, methodsnormally need 78 iterations. closer look MERT revealed two issues: (1)n-best list many identical translations, i.e., spurious ambiguity became evenbigger problem. (2) MERT, identical translations different feature values,could confused optimization. believe problems caused factoften two identical translations would found use phrasesdifferent tables thus different scores.Note also high values interpolation parameter Table 4: 0.80.9.indicate original bi-text needs weighted higher auxiliary one, thussupporting need balanced concatenations repetitions original bi-text,indirectly explaining catk performs better cat1 Table 3.7.4 Improving SpanishEnglish SMT using PortugueseNext, experiment using Portuguese improve SpanishEnglish SMT.results shown Tables 5 6. Overall, consistentIndonesianEnglish SMT using additional Malay-English bi-text (shown Tables 34 above). observe that, size original bi-text increases,gain BLEU decreases, expected. Note also transliterationimportant: doubles absolute gain BLEU achieved method.Table 7 compares performance technique 160K vs. 1.23M additionalpt-en parallel sentence pairs, without transliteration training bi-textsdifferent numbers parallel es-en sentence pairs (10K, 20K, . . ., 320K). table showsimportance transliteration, responsible half improvementbaseline brought method. fact, small original es-en bi-texts (10K,20K, 40K), using 160K transliterated additional pt-en sentence pairs works betterusing 1.23M additional non-transliterated pt-en sentence pairs (which eight times bigger).example, given 10K original training es-en sentence pairs, going 160K 1.23Madditional pt-en sentence pairs improves BLEU 0.25% (from 23.98% 24.23%),using 160K transliterated pt-en data yields improvement 1.75% (from 23.98%25.73%). impact transliteration surprising: already seenTable 2, where, comparing lines 4 5, see transliterating Portugueselook like Spanish effectively doubles BLEU score: 4.91% 8.18% 10K,7.10% 13.79% 1.23M parallel training sentence pairs.201fiNakov & Nges-en pt-en Translit.10K 160Kyes20K 160Kyes40K 160Kyes80K 160Kyes160K 160KyesBaseline22.87<22.87<24.71<24.71<25.80<25.80<27.0827.08<27.9027.90cat123.54<<25.26<25.19<<26.1626.24<<26.7827.2327.26<27.83<28.14<catk23.83<(16)<25.42(16)<25.29<(8)<26.18(8)25.92<(4)<26.93(4)27.09<(2)27.53(2)27.83<(1)28.14(1)<catk:align22.93<(16)<23.31<(16)24.91<(8)24.88<(8)25.99<(4)25.88<(4)27.01<(2)27.09<(2)27.94(1)28.06(1)method23.98(16;3) (+1.11)<25.73(16;3) (+2.86)<25.65(8;2) (+0.94)<26.36(8;3) (+1.65)<26.49(4;2) (+0.69)<26.95(4;3) (+1.15)27.30(2;2) (+0.22)<27.49(2;3) (+0.41)28.05(1;3) (+0.15)28.16(1;2) (+0.26)<Table 5: Improving SpanishEnglish SMT using 160K additional PortugueseEnglish sentence pairs (varying amount original data): concatenations, repetitions, truncations, method. first column containsnumber original (es-en) sentence pairs. Column 3 shows whether transliteration used; following columns list BLEU scores (in %) differentmethods. subscript shows best parameter value(s) found development set used test set produce given result: first valuenumber repetitions original bi-text second value, any,number extra features added phrase table. BLEU scoresstatistically significantly better baseline/our method markedleft/right side < (for p < 0.01) (for p < 0.05).Note impact transliteration diminishes size es-en bi-text grows.surprising: size good original es-en bi-text grows,less less learned additional pt-en bi-text, regardless whetherwithout transliteration.7.5 Improving SpanishEnglish SMT Using ItalianHere, experiment Italian auxiliary language improving SpanishEnglishphrase-based SMT. Figure 2 shows results using Italian Portuguese auxiliary languages method transliteration. see major consistent dropBLEU score using Italian instead Portuguese. example, 10K es-en sentencepairs 160K additional pt-en/it-en sentence pairs, absolute drop BLEU0.9%: 25.73% vs. 24.82%, respectively. Moreover, 160K original es-ensentence pairs, method goes slightly baseline (by -0.05) using it-ensmall improvement (by +0.26) pt-en.Still, Figure 2 shows Italian, dissimilar Spanish Portuguese,useful auxiliary language smaller sizes original es-en training bi-text. Thus,conclude degree similarity auxiliary sourcelanguage matter, dissimilar languages still potentially useful auxiliarylanguages.202fiImproving SMT Resource-Poor Languagees-en pt-en Translit.10K 160Kyes20K 160K40K 160K80K 160K160K 160KyesyesyesyesBaseline22.87<22.87<24.71<24.71<25.80<25.80<27.0827.08<27.9027.90Two tables<23.81<25.29<25.22<26.0725.96<<26.6826.89<27.20<27.9928.11Interpol.<23.73(.5)<25.22<(.5)25.02<(.5)<26.07(.7)26.15<(.6)<26.43(.7)27.04<(.8)27.42(.5)27.72(.5)28.13(.6)Merge23.60(2)<25.16<(2)method23.98(16;3) (+1.11)<25.73(16;3) (+2.86)<<25.32(3)<26.04<(3)25.99<(3)<26.64(3)27.02<(3)27.29(3)27.95(2)28.17(2)<<25.65(8;2) (+0.94)26.36(8;3) (+1.65)<26.49(4;2) (+0.69)<26.95(4;3) (+1.15)27.30(2;2) (+0.22)<27.49(2;3) (+0.41)28.05(1;3) (+0.15)28.16(1;2) (+0.26)<Table 6: Improving SpanishEnglish SMT using 160K additional PortugueseEnglish sentence pairs (varying amount original data): comparingmethod various alternatives. first column contains numberoriginal (es-en) sentence pairs. Column 3 shows whether transliterationused; following columns list BLEU scores (in %) different methods.subscript shows best parameter value(s) found development setused test set produce given result: merging methods, firstvalue number repetitions original bi-text second value,any, number extra features added phrase table; interpolation,show weight phrase pairs in-en. BLEU scoresstatistically significantly better baseline/our method markedleft/right side < (for p < 0.01) (for p < 0.05).7.6 Improving SpanishEnglish SMT Using Portuguese Italianseen Portuguese Italian useful auxiliary languages,tried use together. experiments carried wayused single auxiliary language, except double usual numberrepetitions k original bi-text auxiliary bi-texts dominatecatk:align. example, 10K original es-en training sentence pairs 160Kauxiliary pt-en 160K it-en sentence pairs, need include 32 copies originalbi-text instead 16, before.results combination shown Figure 2. Comparing resultsusing pt-en data only, see small consistent improvement.example, 10K original es-en sentence pairs, 160K additional pt-en 160K additionalit-en sentence pairs, absolute increase BLEU scores 0.18%: 25.73%25.91%. size absolute improvement using 20K, 40K, 80K, 160Kadditional pt-en it-en sentence pairs comparable: 0.10-0.20% average.Thus, potential gains using multiple auxiliary languages simultaneously.203fiNakov & NgSystembaselinemethod: 160K pt-en pairsimprovementmethod: 1.23M pt-en pairsimprovementmethod: 160K pt-en, translit.improvementmethod: 1.23M pt-en, translit.improvement10K22.8723.98+1.1124.23+1.3625.73+2.8626.24+3.3720K24.7125.65+0.9425.70+0.9926.36+1.6526.82+2.1140K25.8026.49+0.6926.78+0.9826.95+1.1527.47+1.6780K27.0827.30+0.2227.49+0.4127.49+0.4127.85+0.77160K27.9028.05+0.1528.22+0.3228.16+0.2628.50+0.60320K28.4628.52+0.0628.58+0.1228.43-0.0328.70+0.24Table 7: SpanishEnglish: testing method using 160K vs. 1.23M additionalpt-en sentence pairs, without transliteration. Shown BLEUscores (in %) absolute improvement baseline training bi-textsdifferent numbers parallel es-en sentence pairs (10K, 20K, . . ., 320K)fixed number additional pt-en sentence pairs: 160K 1.23M. statisticallysignificant improvements baseline marked (for p < 0.01)(for p < 0.05).7.7 Combining Method Cognate Extraction TechniqueKondrak et al. (2003)Next, combined method cognate extraction technique Kondrak et al.(2003), pairs likely cognates extracted original training bi-textadded bi-text additional 1-word-to-1-word sentence pairs.Systembaselinecognatesimprovement (baseline)(1.23M pt-en pt-en) + cognatesimprovement (baseline)improvement (our: 1.23M pt-en)(1.23M pt-en, transl.) + cognatesimprovement (baseline)improvement (our: 1.23M, transl.)10K22.8723.50+0.6324.55+1.68+0.3226.35+3.48+0.1120K24.7125.22+0.5125.98+1.27+0.2826.78+2.07-0.0440K25.8026.31+0.5126.73+0.93-0.0527.34+1.54-0.1380K27.0827.38+0.3027.67+0.59+0.1827.79+0.71-0.06160K27.9028.10+0.2028.33+0.43+0.1128.50+0.60+0.00320K28.4628.74+0.2828.90+0.44+0.3228.68+0.22-0.02Table 8: SpanishEnglish: combining method cognate extractiontechnique Kondrak et al. (2003). Shown BLEU scores (in %)absolute improvements (over baseline method) training bitexts different numbers parallel es-en sentence pairs (10K, 20K, . . ., 320K)fixed number additional pt-en sentence pairs (1.23M), withouttransliteration. statistically significant improvements marked (forp < 0.01) (for p < 0.05).204fiImproving SMT Resource-Poor Language28.527.526.525.5baseline+it-en+pt-en+it-en +pt-en24.523.522.510K20K40K80K160KFigure 2: Improving SpanishEnglish SMT using 160K Italian-English160K Portuguese-English additional sentence pairs (varyingamount original data) transliteration.results adding cognates training es-en bi-text, i.e., reimplementation algorithm, shown top lines Table 8. seeabsolute improvement 0.5% BLEU es-en size 40K, improvementstatistically significant.Next, combined method cognate extraction method follows: first,augmented original es-en bi-text cognate pairs, used augmentedbi-text instead es-en method. Table 8 shows results combinationmethod cognate extraction (BLEU scores % absolute improvementsbaseline method) training bi-texts different numbers paralleles-en sentence pairs (10K, 20K, . . ., 320K) fixed number additional pt-en sentencepairs (1.23M), without transliteration. see, worth combiningmethod cognate extraction technique Kondrak et al. (2003) small originales-en datasets, e.g., 10K 20K (in cases statistically significant improvements occurusing method only), method use transliteration.205fiNakov & Ngfound interesting combining method cognate extraction technique Kondrak et al. (2003) help much used transliteration compareduse it. Thus, analyzed case 10K es-en sentence pairs1.23M pt-en pairs. cognate extraction technique yielded 25,362 Spanish-Englishlikely cognate pairs, including 10,611 unique Spanish words. Portuguese side1.23M pt-en data contained 14 0.13% 10,611 unique Spanish words. Thus,information Spanish-English likely cognates provide word alignmentsphrase pairs clearly complementary pt-en bi-text gives. However,transliteration, source side 1.23M ptes -en bi-text contained 8,867 83.56%10,611 unique Spanish words Spanish-English likely cognate pairs. drasticjump means Spanish-English likely cognate pairs little add topptes -en already provides, explains lack improvement combinedmethod transliteration used.7.8 Combining Method Phrase Table Pivoting TechniqueCallison-Burch et al. (2006)Finally, combined method phrase table pivoting technique Callison-Burchet al. (2006) since orthogonal.First, tried reproduce phrase table pivoting experiments Callison-Burchet al. (2006), turned complicated (even though used originalcode pivoting) various differences experimental setups: (1)used Moses instead Pharaoh translation; (2) used IRSTLM instead SRILMlanguage modeling; (3) used different tokenization; (4) used maximum phraselength seven instead ten; (5) created training/dev/test datasetEuroparl v.3, different version Europarl corpus available2006 (which also implies different baseline, etc.).results shown Table 9. bottom three lines show results reportedCallison-Burch et al. (2006), top three lines report BLEU scoresreproduction experiments, 1.3M pairs used eightadditional pivot languages: Danish, Dutch, Finnish, French, German, Italian, Portuguese,Swedish. BLEU scores lower, good enough studyingpotential combining two methods.combination carried following way: built final mergedphrase table method, paraphrased source side pivoting usingmethod Callison-Burch et al. (2006). middle lines table show BLEUscores (in %) combined method absolute improvements (over baselinemethod) training bi-texts different numbers parallel es-en sentence pairs(10K, 20K, . . ., 320K) fixed amount additional pt-en pairs (160K 1.23M pairs),without transliteration.results show worth combining method phrase table pivotingsmall es-en datasets, e.g., 10K 20K (in cases, statistically significant improvementsoccur using method only), method use transliteration,case cognate extraction technique Kondrak et al. (2003).206fiImproving SMT Resource-Poor LanguageExperimentsbaselinePivoting (+8 pairs 1.3M)improvement (baseline)(160K pt-en) + pivotingimprovement (over baseline)improvement (over method)(1.23M pt-en) + pivotingimprovement (over baseline)improvement (over method)(160K pt-en, transl.) + pivotingimprovement (over baseline)improvement (over method)(1.23M pt-en, transl.) + pivotingimprovement (over baseline)improvement (over method)Callison-Burch et al. (2006)baselinePivoting (+8 pairs 1.3M)improvement (over baseline)10K22.8723.33+0.4624.32+1.45+0.3424.64+1.77+0.4125.82+2.95+0.0926.39+3.52+0.1520K24.7124.88+0.1725.95+1.24+0.3026.18+1.47+0.4826.49+1.78+0.1327.01+2.30+0.1940K25.8026.10+0.3026.70+0.90+0.2126.87+1.07+0.0927.06+1.26+0.1127.53+1.73+0.0680K27.0827.06-0.0227.36+0.28+0.0627.60+0.52+0.1127.51+0.43+0.0227.77+0.69-0.08160K27.9028.09+0.1928.02+0.12-0.0328.35+0.45+0.1328.35+0.45+0.1928.58+0.68+0.08320K28.4628.49+0.0328.56+0.10+0.0428.69+0.23+0.1128.58+0.12+0.1528.66+0.20-0.0422.623.3+0.725.026.0+1.026.527.2+0.726.528.0+1.528.729.0+0.330.030.0+0.0Table 9: SpanishEnglish: combining method phrase table pivotingtechnique Callison-Burch et al. (2006). Shown BLEU scores (in %)absolute improvements (over baseline method) trainingbi-texts different numbers parallel es-en sentence pairs (10K, 20K, . . .,320K) fixed amount additional pt-en pairs: (1) 1.3M pairseight additional languages pivoting, (2) 160K 1.23M pairs onelanguage (Portuguese) method (with without transliteration).last three lines show results phrase table pivoting experiments reportedCallison-Burch et al. (2006) first three lines show reproductionexperiments. statistically significant improvements marked(for p < 0.01) (for p < 0.05).Again, found interesting pivoting help much transliterationwithout it. Thus, closer look interaction pivoting transliteration10K es-en sentence pairs 160K pt-en pairs. particular, looked numberusable phrase pairs respect test data, i.e., phrase pairs whose source sidematches test data, found without pivoting, using transliteration increasesnumber 657,541 1,863,950, i.e., 183.47%, using pivoting transliteration increases number 819,324 2,214,580, i.e., 170.29%. lower relativeincrease number usable phrases one possible explanation correspondinglower increase BLEU: +0.34 +0.09 absolute, respectively, withouttransliteration.207fiNakov & Ng8. Analysis DiscussionBelow, perform deeper analysis method obtained results.8.1 Merging Phrase Tablescompare merging catk:align cat1 (with 1-3 extra features, describedabove), two simpler alternatives: (a) substituting cat1 ml-en, (b) mergingphrase tables derived original bi-texts, in-en ml-en.implement evaluate following alternative method: (c) trainalignment models combined bi-text consists k copies in-en onecopy ml-en, truncate alignments appropriately, build two separate phrasetables. first table catk:align method (built one copy in-en),second one similar phrase table corresponds ml-en. Unlike (a) above, wordalignments second phrase table influenced k copies in-en. refersecond phrase table ml :catk:align. motivation trying alternative(1) bit simpler implement, (2) somewhat symmetric phrasetables. Yet, like method, alignment models benefit data,phrase tables remain language-specific thus combined using extra features.Table 10 compares method (line 3) above-described three alternatives, (a),(b) (c), different numbers training ml-en sentence pairs. see, overall,method (line 3) performs best, newly described alternative (c), shown line4, ranked second. Merging phrase tables derived original bi-texts in-enml-en worst (line 1), explained fact cannot benefitimproved word alignments small in-en bi-text (unlike combinations,notably one line 2). However, easy explain table alonemethod better two alternatives.Thus, looked phrase tables unknown words, case 160Kml-en additional sentence pairs. results shown Table 11, offers goodinsight: two good performing combinations lines 3-4 simply larger phrase tablescompared lines 1-2. importantly, translates higher numberphrase pairs potentially usable translating test sentences, i.e., matchinput test time. Naturally, translation options mean larger search spacethus opportunities find better translations, explains better performancecombinations lines 3-4. table also compares number unknown wordtypes word tokens translating test data. see methodlowest number unknowns, explain good performance. hand,numbers unknown words comparable three methods.summary, Table 11 shows two important factors influencing BLEU: (i ) total usednumber phrase pairs, (ii ) number unknown word types tokens translationtime. method ranks best criteria, second best method line 4 rankssecond first factor last second one (but close methods).Thus, could conclude impact unknown words BLEU limiteddifferences small. phrase table size seems correlate somewhat better BLEU,least two best performing methods. Finally, comparing line 2 lines 3-4,conclude using in-en bi-text help align ml-en bi-text beneficial.208fiImproving SMT Resource-Poor Languageget even better insight, looked characteristics tablescombined: (1) phrase table sizes overlap, (2) number distinct source phrasesoverlap, (3) average differences four standard scores merged tablesshared phrase pairs: inverse phrase translation probability (f |e), inverse lexical weightingpw (f |e), direct phrase translation probability (e|f ), direct lexical weighting pw (e|f ).results shown Table 12. table shows method combines phrasetables much higher overlap (10-50 times higher!), terms numberphrase pairs number distinct source phrases. Moreover, absolute differencesscores shared phrase pairs halved (i.e., similar)two phrase tables combined method, cat1 catk:align, comparedphrase tables combined three alternative approaches, last fourcolumns Table 12 show. high similarity scores cat1 catk:aligncould one possible explanation similar performance shown Table 3.Thus, method wins combining two tables already madesimilar, thus appropriate combination. key element buildsecond table concatenation ml-en in-en bi-texts, in-en bitext minor influence word alignment (it simply much smaller), muchinfluence phrase extraction scoring. makes resulting phrase table muchsimilar first phrase table (which also made similar second table,via word alignments only), also much bigger trained ml-en data(14M vs. 11M phrase pairs); turn yields larger merged phrase table despitehigher overlap tables merged.1234Merged Phrase Tablesin-enml-encatk:align ml-encatk:align cat1catk:align ml :catk:align10K23.9724.1124.5124.1420K24.4624.5624.7024.5440K24.4324.5424.7324.6580K24.6724.6224.9724.72160K24.7925.0225.1525.08Table 10: Merging phrase tables Indonesian-English SMT: BLEU scores.BLEU shown % different numbers training ml-en sentence pairs.8.2 Transliterationcloser look transliteration: studying many words affectsimpact number unknown words (also known OOVs, out-of-vocabulary words).First, look number word types word tokens changed processtransliteration source side additional training bi-text. results shownTable 13. see transliterating Malay Indonesian affects smallnumber words: 7.61% word types 5.78% word tokens.surprising since spelling differences Malay Indonesian limited,explained Section 3.1. contrast, transliterating Portuguese Spanish changes44.71% word types 23.17% word tokens, agrees observationsSections 3.2 6. Italian even affected transliteration Portuguese:70.45% word types 34.37% word tokens changed.209fiNakov & Ng1234Merged Phrase Tablesin-enml-encatk:align ml-encatk:align cat1catk:align ml :catk:alignPhrase PairsTotal# Used % Used14.23M1.28M9.02%14.07M1.25M8.88%15.83M1.70M10.71%14.92M1.37M9.17%UnknownTypes Tokens14111917141319061300174314451933BLEU24.7925.0225.1525.08Table 11: Merging phrase tables derived in-en ml-en (160K): numberphrase pairs unknown words. Shown total number phrasepairs merged phrase table number phrase pairs used decodetest data, followed number unknown word types tokens,BLEU score (in %).Merged Phrase Tables1 in-enml-en2 catk:align3 catk:align4 catk:alignml-encat1ml:catk:alignPhrase PairsPT1 PT23.2M 11.1M 72.1KSource PhrasesPT1 PT21.1M 7.7M 10.3K2.23%0.91%0.65%1.2M 7.7M 11.1K2.51%0.95%3.1M 14.0M 1.2M1.2M 8.8M 0.6M49.40%0.21 0.05 0.13 0.056.54%3.1M 11.9M 87.6K1.2M 7.6M 11.6K2.85%0.99%0.73%0.40 0.18 0.19 0.100.14%39.39%8.67%0.40 0.18 0.19 0.100.13%3.1M 11.1M 77.1K0.70%Avg. Score Diff.(f |e) pw (f |e) (e|f ) pw (e|f )0.40 0.18 0.18 0.090.15%Table 12: Comparison phrase tables merged Tables 10 11. Shownnumber phrase pairs / source phrases phrase tablenumber/percent appear tables. last four columnsshow average absolute differences four standard phrase table scoresphrase pairs appear tables; scores inverse phrase translationprobability (f |e), inverse lexical weighting pw (f |e), direct phrase translationprobability (e|f ), direct lexical weighting pw (e|f ).One important reason higher number changes would that, unlike SpanishPortuguese, Italian form plural nouns adjectives adding -svowel change. example, singular form adjective meaning greenverde three languages: Spanish, Portuguese, Italian. However, plural formdiffers: verdes regardless gender Spanish Portuguese, verdi(plural masculine) verde (plural feminine) Italian. Thus, transliterating PortugueseSpanish would leave verdes intact, Italian, changes would needed. Givenfrequency use plural nouns adjectives, expect manydifferences ItalianSpanish PortugueseSpanish. Overall, small numberword types/tokens changed explains transliteration limited use Malayimportant Spanish Italian.210fiImproving SMT Resource-Poor Language1TransliterationMalayIndonesianWord TypesChangedTotal8,259108,5952PortugueseSpanish52,3037.61%5.78%116,98944.71%3ItalianSpanishWord TokensChangedTotal316,4445,472,3728,315,83535,889,87723.17%88,767126,00570.45%14,962,68043,530,24634.37%Table 13: Transliteration: number words training data changed.Shown number word types word tokens changed, comparedtotal number word types tokens source side differenttraining bi-texts.12345Bi-text(s)ml-enmlin -enin-enin-en+ml-enin-en+mlin -enSentences160K160K28.4K28.4K+160K28.4K+160KUnknownTypes Tokens3,1157,1012,9127,2881,5472,1011,1701,5321,1821,544BLEU17.9019.1523.8024.4324.72Table 14: Unknown words Indonesian test dataset. Shown numberunknown word types word tokens, Bleu score % different trainingbi-texts simple bi-text concatenations (cat1). counts respecttraining bi-text; actual number unknown words translation timediffer. Indonesian bi-texts tuning testing enin monolingualdata used language modeling.Next, studied impact transliteration number unknown wordstest data. Table 14 shows results Indonesian, using Indonesian bi-textstuning testing enin monolingual data language modeling. Comparing lines 12, see transliteration limited impact reducing numberunknown word types training Malay data: number unknown word typesdrops slightly 3,115 2,912, number unknown word tokens actuallygrows, 7,101 7,288. Yet, improvement BLEU, 17.90 19.15.improvement consistent n-gram scores included BLEU: 1-gram (48.46 vs.50.12), 2-gram (22.09 vs. 23.46), 3-gram (12.49 vs. 13.54), 4-gram (7.67 vs. 8.44).Thus, apparently, number unknown word types important numberunknown word tokens. Moving line 3, see number unknownword types halved training Indonesian instead Malay, confirmssimilarity Indonesian Malay. Comparing lines 4 5,see concatenate Malay Indonesian training bi-texts, impacttransliteration minimal: terms word types/tokens BLEU.211fiNakov & Ng1234567891011Bi-text(s)pt-enptes -enit-enites -enes-enes-en+it-enes-en+ites -enes-en+pt-enes-en+ptes -enes-en+pt-en+it-enes-en+ptes -en+ites -enSentences160K160K160K160K160K160K+160K160K+160K160K+160K160K+160K160K+160K+160K160K+160K+160KUnknownTypes Tokens3,97317,5801,57410,3375,52923,0882,41313,492362440347406316374273295240257264280221232Bleu6.3511.354.069.3827.9027.6527.6927.8328.1427.8928.02Table 15: Unknown words Spanish test dataset. Shown numberunknown word types word tokens, Bleu score % different trainingbi-texts simple bi-text concatenations (cat1). counts respecttraining bi-text; actual number unknown words translation timediffer. Spanish bi-texts used tuning testing.relative differences number unknown words much sizeabletransliterating Portuguese/Italian Spanish, Table 15 shows. Comparing lines 1-23-4, see number unknown word types/tokens halved, BLEUdoubles. confirms importance transliteration languages.Going line 5, see 10-15 times drop number unknown word typestraining Spanish bi-text. drop looks drastic compared MalayTable 14, partly explained larger size training es-en bi-text,contains 160K sentence pairs compared 28.4K pairs in-en. Since es-en bi-textreduced number unknown word types 362, becomes hard reducenumber further. Still, lines 6-11 show, concatenating es-en pt-en it-en yieldssizable improvements using es-en only. Moreover, transliteration helps consistentlyreducing number unknown words reductions biggerMalay relative, also absolute terms. Still, reductionsnumber unknown words great absolute terms, thus correspondingdifferences BLEU small. Yet, transliteration yields consistent improvementconcatenations: Spanish+Italian, Spanish+Portuguese, Spanish+Portuguese+Italian.Overall, conclude large relative drops number unknown wordscorrespond sizable improvements BLEU; however, results small relative differences less conclusive: correspond small fluctuation BLEU PortugueseItalian somewhat larger differences Malay. could feature muchlower token/type ratio Malay, agglutinative language, thus quite richwordforms: Table 13 shows, token/type ratio 50 Malay,307 345 Portuguese Italian, respectively.212fiImproving SMT Resource-Poor Language8.3 Relative ImprovementFinally, address important question much real data method saves.Figure 3 compares graphically improvements baseline using method160K vs. 1.23M pt-en sentence pairs transliteration different number originaltraining es-en sentence pairs. see figure that, 10K real traininges-en sentence pairs, using 160K additional pt-en method yields BLEU scorecomparable achieved 40K real es-en sentence pairs, i.e., cutnecessary real data factor four. see using 1.23M pt-ensentence pairs improves factor five. Similarly, 20K real es-en training sentencepairs, method achieves BLEU score would require 33.5 times much realtraining es-en data baseline system match.Figure 4 summarizes statistics, showing many times real data wouldneeded baseline match performance method. see cutfactor 1.54 25, using 160K 1.23M additional pt-en sentences.2928272625baseline24method: 160Kmethod: 1.23M232210K20K40K80K160K320KFigure 3: SpanishEnglish: improvements baseline using method160K vs. 1.23M pt-en sentence pairs transliteration differentnumber original training es-en sentence pairs.213fiNakov & Ng6method: 160K5method: 1.23M4321010K20K40K80K160KFigure 4: Trade-off SpanishEnglish PortugueseEnglish data.Shown number times need grow original es-en training dataorder achieve BLEU score using method 160K/1.23Madditional pt-en sentence pairs transliteration.9. Conclusionproposed novel language-independent method improving statistical machinetranslation resource-poor languages exploiting similarity related resource-richones. achieved significant gains BLEU, improve best rivalingapproaches, using much less additional data.studied impact using less closely related language auxiliarylanguage (Italian instead Portuguese improving SpanishEnglish SMT), triedusing Portuguese Italian together auxiliary languages, combinedmethod two orthogonal rivaling approaches: (1) using cognates sourcetarget language, (2) source-language side paraphrasing pivot language.experiments yielded statistically significant improvements small datasets.214fiImproving SMT Resource-Poor LanguageBased experimental results, make several interesting conclusions:1. shown using related languages help improve SMT: achieved1.35 3.37 improvement BLEU in-en (+ml-en) es-en (+pt-en).2. simple concatenation help, problematic additional sentencesout-number ones original bi-text.3. Concatenation work well original bi-text repeated enough timesadditional bi-text dominate.4. Merging phrase tables giving priority original bi-text using additionalfeatures good strategy.5. Part improvement combining bi-texts due increased vocabularycoverage another part comes improved word alignments. best resultsachieved two sources first isolated combined (our method).6. Transliteration help lot case systematic spelling variationsoriginal additional source languages.7. Overall, reduce amount necessary real training data factor 25.future work, would like extend approach several interesting directions.First, want make better use multi-lingual parallel corpora, e.g., accessSpanish-Portuguese-English corpus, used two separate bi-texts Spanish-EnglishPortuguese-English. Second, would like try using auxiliary languagesrelated target language. Finally, would like experiment sophisticatedways get auxiliary language closer source go beyond simple transliteration.Acknowledgmentswould like thank anonymous reviewers constructive commentssuggestions, helped us improve quality manuscript. researchsupported research grant POD0713875, Singapore National ResearchFoundation International Research Centre @ Singapore Funding Initiativeadministered IDM Programme Office.ReferencesAl-Onaizan, Y., Curin, J., Jahr, M., Knight, K., Lafferty, J., Melamed, D., Och, F. J.,Purdy, D., Smith, N., & Yarowsky, D. (1999). Statistical machine translation. Tech.rep., CLSP, Johns Hopkins University, Baltimore, MD.Altintas, K., & Cicekli, I. (2002). machine translation system pair closelyrelated languages. Proceedings 17th International Symposium ComputerInformation Sciences, ISCIS 02, pp. 192196, Orlando, FL.Bakr, H. A., Shaalan, K., & Ziedan, I. (2008). hybrid approach converting writtenEgyptian colloquial dialect diacritized Arabic. Proceedings 6th International Conference Informatics Systems, INFOS 08, Cairo, Egypt.215fiNakov & NgBannard, C., & Callison-Burch, C. (2005). Paraphrasing bilingual parallel corpora.Proceedings 43rd Annual Meeting Association Computational Linguistics,ACL 05, pp. 597604, Ann Arbor, MI.Bergsma, S., & Kondrak, G. (2007). Alignment-based discriminative string similarity.Proceedings 45th Annual Meeting Association Computational Linguistics, ACL 07, pp. 656663, Prague, Czech Republic.Bertoldi, N., Barbaiani, M., Federico, M., & Cattoni, R. (2008). Phrase-based statistical machine translation pivot languages. Proceedings International WorkshopSpoken Language Translation, IWSLT 08, pp. 143149, Honolulu, HI.Bickford, A., & Tuggy, D. (2002).Electronic glossary linguistic terms.http://www.sil.org/mexico/ling/glosario/E005ai-Glossary.htm.Birch, A., Osborne, M., & Koehn, P. (2007). CCG supertags factored statistical machinetranslation. Proceedings Second Workshop Statistical Machine Translation, WMT 07, pp. 916, Prague, Czech Republic.Brill, E., & Moore, R. C. (2000). improved error model noisy channel spelling correction. Proceedings 38th Annual Meeting Association ComputationalLinguistics, ACL 00, pp. 286293, Hong Kong.Brown, P. F., Della Pietra, S. A., Della Pietra, V. J., Goldsmith, M. J., Hajic, J., Mercer,R. L., & Mohanty, S. (1993). dictionaries data too. ProceedingsWorkshop Human Language Technology, HLT 93, pp. 202205, Princeton, NJ.Brown, P. F., Della Pietra, S. A., Della Pietra, V. J., & Mercer, R. L. (1993). mathematics statistical machine translation: parameter estimation. Computational Linguistics, 19 (2), 263311.Callison-Burch, C. (2008). Syntactic constraints paraphrases extracted parallelcorpora. Proceedings 2008 Conference Empirical Methods NaturalLanguage Processing, pp. 196205.Callison-Burch, C. (2012). How-to guide extracting syntactically constrained paraphrases.. http://www.cs.jhu.edu/ccb/howto-extract-paraphrases.html. Retrieved2012-05-14.Callison-Burch, C., Koehn, P., & Osborne, M. (2006). Improved statistical machine translation using paraphrases. Proceedings main conference Human LanguageTechnology Conference North American Chapter Association Computational Linguistics, HLT-NAACL 06, pp. 1724, New York, NY.Chan, Y. S., & Ng, H. T. (2005). Word sense disambiguation distribution estimation.Proceedings 19th International Joint Conference Artificial Intelligence,IJCAI 05, pp. 10101015, Edinburgh, UK.Chan, Y. S., & Ng, H. T. (2006). Estimating class priors domain adaptation wordsense disambiguation. Proceedings 21st International Conference Computational Linguistics 44th annual meeting Association ComputationalLinguistics, COLING-ACL 06, pp. 8996, Sydney, Australia.216fiImproving SMT Resource-Poor LanguageChan, Y. S., & Ng, H. T. (2007). Domain adaptation active learning word sensedisambiguation. Proceedings 45th Annual Meeting AssociationComputational Linguistics, ACL 07, pp. 4956, Prague, Czech Republic.Chiang, D. (2005). hierarchical phrase-based model statistical machine translation.Proceedings 43rd Annual Meeting Association Computational Linguistics, ACL 05, pp. 263270, Ann Arbor, MI.Chiang, D., Knight, K., & Wang, W. (2009). 11,001 new features statistical machinetranslation. Proceedings Human Language Technologies: Annual ConferenceNorth American Chapter Association Computational Linguistics,NAACL-HLT 09, pp. 218226, Boulder, CO.Cohn, T., & Lapata, M. (2007). Machine translation triangulation: Making effective usemulti-parallel corpora. Proceedings 45th Annual Meeting AssociationComputational Linguistics, ACL 07, pp. 728735, Prague, Czech Republic.Collins, M., Koehn, P., & Kucerova, I. (2005). Clause restructuring statistical machinetranslation. Proceedings 43rd Annual Meeting Association Computational Linguistics, ACL 05, pp. 531540, Ann Arbor, MI.Crego, J. M., Max, A., & Yvon, F. (2010). Local lexical adaptation machine translationtriangulation: SMT helping SMT. Proceedings 23rd InternationalConference Computational Linguistics, COLING 10, pp. 232240, Beijing, China.Dahlmeier, D., & Ng, H. T. (2010). Domain adaptation semantic role labelingbiomedical domain. Bioinformatics, 26 (8), 10981104.Daume, III, H., & Jagarlamudi, J. (2011). Domain adaptation machine translationmining unseen words. Proceedings 49th Annual Meeting AssociationComputational Linguistics: Human Language Technologies, ACL-HLT 11, pp. 407412, Portland, OR.Daume, III, H., & Marcu, D. (2006). Domain adaptation statistical classifiers. J. Artif.Int. Res., 26, 101126.de Gispert, A., & Mario, J. (2006). Catalan-English statistical machine translation without parallel corpus: Bridging Spanish. Proceedings 5th WorkshopStrategies developing Machine Translation Minority Languages LREC,SALTMIL 06, pp. 6568, Genoa, Italy.Denkowski,M.(2012).READMEfileParexparaphrase extractor.. https://github.com/mjdenkowski/parex/blob/master/README.Retrieved 2012-05-14.Denkowski, M., & Lavie, A. (2010). METEOR-NEXT METEOR paraphrase tables:Improved evaluation support five target languages. Proceedings Joint 5thWorkshop Statistical Machine Translation MetricsMATR, pp. 339342.Filali, K., & Bilmes, J. (2005). Leveraging multiple languages improve statistical MTword alignments. Proceedings IEEE Automatic Speech Recognition Understanding Workshop, ASRU 05, Cancun, Mexico.217fiNakov & NgFinch, A., & Sumita, E. (2008). Phrase-based machine transliteration. ProceedingsWorkshop Technologies Corpora Asia-Pacific Speech Translation, TCAST08, pp. 1318, Hyderabad, India.Galley, M., Hopkins, M., Knight, K., & Marcu, D. (2004). Whats translation rule?.Proceedings Human Language Technology Conference North AmericanChapter Association Computational Linguistics, HLT-NAACL 04, pp. 273280, Boston, MA.Garera, N., Callison-Burch, C., & Yarowsky, D. (2009). Improving translation lexicon induction monolingual corpora via dependency contexts part-of-speech equivalences. Proceedings Thirteenth Conference Computational Natural Language Learning, CoNLL 09, pp. 129137, Boulder, CO.Graca, J., Ganchev, K., & Taskar, B. (2010). Learning tractable word alignment modelscomplex constraints. Comput. Linguist., 36, 481504.Habash, N., & Hu, J. (2009). Improving Arabic-Chinese statistical machine translationusing English pivot language. Proceedings Fourth Workshop StatisticalMachine Translation, WMT 09, pp. 173181, Athens, Greece.Haghighi, A., Liang, P., Berg-Kirkpatrick, T., & Klein, D. (2008). Learning bilingual lexicons monolingual corpora. Proceedings 46th Annual MeetingAssociation Computational Linguistics, ACL 08, pp. 771779, Columbus, OH.Hajic, J., Hric, J., & Kubon, V. (2000). Machine translation close languages.Proceedings Sixth Conference Applied Natural Language Processing, ANLP00, pp. 712, Seattle, WA.Han, B., & Baldwin, T. (2011). Lexical normalisation short text messages: Makn sens#twitter. Proceedings 49th Annual Meeting Association Computational Linguistics: Human Language Technologies, ACL-HLT 11, pp. 368378,Portland, Oregon.Hana, J., Feldman, A., Brew, C., & Amaral, L. (2006). Tagging Portuguese Spanish tagger using cognates. Proceedings International Workshop CrossLanguage Knowledge Induction, CrossLangInduction 06, pp. 3340, Trento, Italy.Hildebrand, A. S., Eck, M., Vogel, S., & Waibel, A. (2005). Adaptation translationmodel statistical machine translation based information retrieval. Proceedings10th Annual Conference European Association Machine Translation,EAMT 05, pp. 133142, Budapest, Hungary.Hopkins, M., & May, J. (2011). Tuning ranking. Proceedings 2011 ConferenceEmpirical Methods Natural Language Processing, EMNLP 11, pp. 13521362,Edinburgh, Scotland, UK.Inkpen, D., Frunza, O., & Kondrak, G. (2005). Automatic identification cognatesfalse friends French English. Proceedings International ConferenceRecent Advances Natural Language Processing, RANLP 05, pp. 251257, Borovets,Bulgaria.218fiImproving SMT Resource-Poor LanguageJiang, J., & Zhai, C. (2007a). Instance weighting domain adaptation NLP. Proceedings 45th Annual Meeting Association Computational Linguistics,ACL 07, pp. 264271, Prague, Czech Republic.Jiang, J., & Zhai, C. (2007b). two-stage approach domain adaptation statisticalclassifiers. Proceedings sixteenth ACM conference Conference information knowledge management, CIKM 07, pp. 401410, Lisbon, Portugal. ACM.Klementiev, A., & Roth, D. (2006). Named entity transliteration discovery multilingual comparable corpora. Proceedings main conference Human Language Technology Conference North American Chapter AssociationComputational Linguistics, HLT-NAACL 06, pp. 8288, New York, NY.Koehn, P. (2005). Europarl: parallel corpus evaluation machine translation.Proceedings Tenth Machine Translation Summit, MT Summit 05, pp. 7986,Phuket, Thailand.Koehn, P., Axelrod, A., Mayne, A. B., Callison-Burch, C., Osborne, M., & Talbot, D.(2005). Edinburgh system description IWSLT speech translation evaluation.Proceedings International Workshop Spoken Language Translation, IWSLT05, Pittsburgh, PA.Koehn, P., Hoang, H., Birch, A., Callison-Burch, C., Federico, M., Bertoldi, N., Cowan, B.,Shen, W., Moran, C., Zens, R., Dyer, C., Bojar, O., Constantin, A., & Herbst, E.(2007). Moses: Open source toolkit statistical machine translation. Proceedings 45th Annual Meeting Association Computational Linguistics.Demonstration session, ACL 07, pp. 177180, Prague, Czech Republic.Koehn, P., & Knight, K. (2002). Learning translation lexicon monolingual corpora.Proceedings ACL-02 Workshop Unsupervised Lexical Acquisition, pp. 916,Philadelphia, PA.Koehn, P., Och, F. J., & Marcu, D. (2003). Statistical phrase-based translation. Proceedings Conference North American Chapter AssociationComputational Linguistics Human Language Technology, NAACL 03, pp. 4854,Edmonton, Canada.Kondrak, G. (2005). Cognates word alignment bitexts. Proceedings TenthMachine Translation Summit, MT Summit 05, pp. 305312, Phuket, Thailand.Kondrak, G., Marcu, D., & Knight, K. (2003). Cognates improve statistical translationmodels. Proceedings Conference North American ChapterAssociation Computational Linguistics Human Language Technology, NAACL03, pp. 4648, Edmonton, Canada.Kumar, S., Och, F. J., & Macherey, W. (2007). Improving word alignment bridgelanguages. Proceedings Joint Conference Empirical Methods NaturalLanguage Processing Computational Natural Language Learning, EMNLP-CoNLL07, pp. 4250, Prague, Czech Republic.Li, H., & Kumaran, A. (Eds.). (2010). NEWS 10: Proceedings 2010 Named EntitiesWorkshop, Uppsala, Sweden.219fiNakov & NgMann, G. S., & Yarowsky, D. (2001). Multipath translation lexicon induction via bridgelanguages. Proceedings second meeting North American ChapterAssociation Computational Linguistics Language technologies, NAACL 01,pp. 18, Pittsburgh, PA.Marujo, L., Grazina, N., Lus, T., Ling, W., Coheur, L., & Trancoso, I. (2011). BP2EPAdaptation Brazilian Portuguese texts European Portuguese. Proceedings15th Conference European Association Machine Translation, EAMT11, pp. 129136, Leuven, Belgium.Matthews, D. (2007). Machine transliteration proper names. Masters thesis, SchoolInformatics, University Edinburgh.Melamed, D. (1995). Automatic evaluation uniform filter cascades inducing N-besttranslation lexicons. Proceedings Third Workshop Large Corpora,VLC 95, pp. 184198, Cambridge, MA.Melamed, D. (1999). Bitext maps alignment via pattern recognition. ComputationalLinguistics, 25 (1), 107130.Melamed, D. (2000). Models translational equivalence among words. ComputationalLinguistics, 26 (2), 221249.Mulloni, A., & Pekar, V. (2006). Automatic detection orthographic cues cognaterecognition. Proceedings 5th International Conference Language ResourcesEvaluation, LREC 06, pp. 23872390, Genoa, Italy.Nakov, P. (2008). Improved statistical machine translation using monolingual paraphrases.Proceedings 18th European Conference Artificial Intelligence, ECAI 08,pp. 338342, Patras, Greece.Nakov, P., Nakov, S., & Paskaleva, E. (2007). Improved word alignments using Webcorpus. Proceedings International Conference Recent AdvancesNatural Language Processing, RANLP 07, pp. 400405, Borovets, Bulgaria.Nakov, P., & Ng, H. T. (2009a). Improved statistical machine translation resource-poorlanguages using related resource-rich languages. Proceedings ConferenceEmpirical Methods Natural Language Processing, EMNLP 09, pp. 13581367,Singapore.Nakov, P., & Ng, H. T. (2009b). NUS WMT09: Domain adaptation experimentsEnglish-Spanish machine translation news commentary text. ProceedingsFourth Workshop Statistical Machine Translation, WMT 09, pp. 7579, Athens,Greece.Och, F. J. (2003). Minimum error rate training statistical machine translation.Proceedings 41st Annual Meeting Association Computational Linguistics,ACL 03, pp. 160167, Sapporo, Japan.Och, F. J., & Ney, H. (2001). Statistical multi-source translation. Proceedings MTSummit VIII. Machine Translation Information Age, MT Summit 01, pp. 253258, Santiago de Compostela, Spain.220fiImproving SMT Resource-Poor LanguageOh, J.-H., Choi, K.-S., & Isahara, H. (2006). comparison different machine transliteration models. J. Artif. Int. Res., 27, 119151.Papineni, K., Roukos, S., Ward, T., & Zhu, W.-J. (2002). BLEU: method automaticevaluation machine translation. Proceedings 40th Annual MeetingAssociation Computational Linguistics, ACL 02, pp. 311318, Philadelphia, PA.Paul, M., Yamamoto, H., Sumita, E., & Nakamura, S. (2009). importance pivotlanguage selection statistical machine translation. Proceedings Human Language Technologies: Annual Conference North American ChapterAssociation Computational Linguistics, NAACL-HLT 09, pp. 221224, Boulder,CO.Quirk, C., Menezes, A., & Cherry, C. (2005). Dependency treelet translation: Syntacticallyinformed phrasal SMT. Proceedings 43rd Annual Meeting AssociationComputational Linguistics, ACL 05, pp. 271279, Ann Arbor, MI.Rappoport, A., & Levent-Levi, T. (2006). Induction cross-language affix lettersequence correspondence. Proceedings International Workshop CrossLanguage Knowledge Induction, CrossLangInduction 06, pp. 1724, Trento, Italy.Ristad, E., & Yianilos, P. (1998). Learning string-edit distance. IEEE Trans. Pattern Anal.Mach. Intell., 20 (5), 522532.Salloum, W., & Habash, N. (2011). Dialectal standard Arabic paraphrasing improveArabic-English statistical machine translation. Proceedings First WorkshopAlgorithms Resources Modelling Dialects Language Varieties, pp.1021, Edinburgh, Scotland, UK.Sawaf, H. (2010). Arabic dialect handling hybrid machine translation. Proceedings9th Conference Association Machine Translation Americas,AMTA 10.Scannell, K. (2006). Machine translation closely related language pairs. ProceedingsLREC2006 Workshop Strategies Developing Machine TranslationMinority Languages, Genoa, Italy.Schafer, C., & Yarowsky, D. (2002). Inducing translation lexicons via diverse similaritymeasures bridge languages. Proceedings 6th Conference NaturalLanguage Learning, COLING 02, pp. 17, Taipei, Taiwan.Scherrer, Y. (2007). Adaptive string distance measures bilingual dialect lexicon induction. Proceedings 45th Annual Meeting ACL: Student ResearchWorkshop, ACL 07, pp. 5560, Prague, Czech Republic.Schroeder, J., Cohn, T., & Koehn, P. (2009). Word lattices multi-source translation.Proceedings 12th Conference European Chapter AssociationComputational Linguistics, EACL 09, pp. 719727, Athens, Greece.Snover, M., Dorr, B., & Schwartz, R. (2008). Language translation model adaptationusing comparable corpora. Proceedings Conference Empirical MethodsNatural Language Processing, EMNLP 08, pp. 857866, Honolulu, HI.221fiNakov & NgSteinberger, R., Pouliquen, B., Widiger, A., Ignat, C., Erjavec, T., Tufis, D., & Varga, D.(2006). JRC-Acquis: multilingual aligned parallel corpus 20+ languages.Proceedings 5th International Conference Language Resources Evaluation, LREC 06, pp. 21422147, Genoa, Italy.Tanaka, R., Murakami, Y., & Ishida, T. (2009). Context-based approach pivot translation services. Proceedings 21st International Joint Conference Artificalintelligence, IJCAI 09, pp. 15551561, Pasadena, CA.Tiedemann, J. (1999). Automatic construction weighted string similarity measures.Joint SIGDAT Conference Empirical Methods Natural Language ProcessingLarge Corpora, EMNLP-VLC 99, pp. 213219, College Park, MD.Tiedemann, J. (2009). Character-based PSMT closely related languages. Proceedings13th Annual Conference European Association Machine Translation,EAMT 09, pp. 1219, Barcelona, Spain.Tiedemann, J., & Nabende, P. (2009). Translating transliterations. International JournalComputing ICT Research, 3 (1), 3341.Ueffing, N., Haffari, G., & Sarkar, A. (2007). Semi-supervised model adaptation statistical machine translation. Machine Translation, 21, 7794.Utiyama, M., & Isahara, H. (2007). comparison pivot methods phrase-based statistical machine translation. Human Language Technologies 2007: ConferenceNorth American Chapter Association Computational Linguistics;Proceedings Main Conference, NAACL-HLT 07, pp. 484491, Rochester, NY.Vilar, D., Peter, J.-T., & Ney, H. (2007). translate letters?. ProceedingsSecond Workshop Statistical Machine Translation, pp. 3339, Prague, CzechRepublic.Vogel, S., Ney, H., & Tillmann, C. (1996). HMM-based word alignment statistical translation. Proceedings 16th conference Computational linguistics, COLING96, pp. 836841, Copenhagen, Denmark.Wu, H., & Wang, H. (2007). Pivot language approach phrase-based statistical machinetranslation. Machine Translation, 21 (3), 165181.Zhang, X. (1998). Dialect MT: case study Cantonese Mandarin. Proceedings 36th Annual Meeting Association Computational Linguistics17th International Conference Computational Linguistics, COLING-ACL 98, pp.14601464, Montreal, Quebec, Canada.222fiJournal Artificial Intelligence Research 44 (2012) 97-140Submitted 2/2012; published 5/2012Solving Limited Memory Influence DiagramsDenis Deratani MauaCassio Polpo de CamposMarco Zaffalondenis@idsia.chcassio@idsia.chzaffalon@idsia.chIstituto Dalle Molle di Studi sullIntelligenza Artificiale (IDSIA)Galleria 2, Manno, 6928 SwitzerlandAbstractpresent new algorithm exactly solving decision making problems representedinfluence diagrams. require usual assumptions forgetting regularity;allows us solve problems simultaneous decisions limited information.algorithm empirically shown outperform state-of-the-art algorithm randomlygenerated problems 150 variables 1064 solutions. show problemsNP-hard even underlying graph structure problem low treewidthvariables take bounded number states, admit provably goodapproximation variables take arbitrary number states.1. IntroductionInfluence diagrams (Howard & Matheson, 1984) graphical models aimed representation problems decision making uncertainty. Traditionally, designedhandle situations involving single, non-forgetful decision maker. Limited memory influence diagrams (hereafter LIMIDs) generalizations influence diagrams allowdecision making limited information, case simultaneous decisions, boundedmemory controllers non-communicating cooperative agents (Zhang, Qi, & Poole, 1994;Lauritzen & Nilsson, 2001; Poupart & Boutilier, 2003; Detwarasiti & Shachter, 2005).precisely, LIMIDs relax regularity forgetting assumptions influence diagrams,namely, complete temporal ordering decision variables,disclosed information (i.e., decisions observations made) remembered consideredfuture decisions. assumptions might hard meet applications, might lead exponential growth size policies, consequentlyintractability.Solving (limited memory) influence diagram refers finding optimal plan action,is, combination decision rules, policies, associate possible observationaction. Optimality understood maximizing expected utility. taskempirically theoretically shown hard (de Campos & Ji, 2008). fact,show solving LIMID NP-hard even admit singly connected diagramsbounded number states per variable,1 devising algorithm producesprovably good approximate solutions within fixed factor unlike exist evendiagrams low treewidth.1. diagram singly connected underlying (undirected) graph contains cycles.2012 AI Access Foundation. rights reserved.fiMaua, de Campos, & ZaffalonLauritzen Nilsson (2001) shown LIMIDS satisfy certain graphstructural conditions (which forgetting regularity imply) solved exactlydynamic programming procedure complexity exponential treewidth. Hence,solving LIMIDs computationally similar performing probabilistic inferenceBayesian networks (Koller & Friedman, 2009). fact, single policy updating (SPU)algorithm Lauritzen Nilsson (2001) performs local search space policiesstep performs probabilistic inference evaluate candidate solution.However, many problems fail meet conditions necessary SPU achieving optimality, cases SPU might converge local optimum much inferioractual (global) optimum. circumvent problem, de Campos Ji (2008) formulatedcredal reformulation (CR) algorithm maps LIMID mixed integer linearprogramming problem. showed CR algorithm able solve small problems exactly obtain good approximations medium-sized problems relaxingintegrality constraints.show paper LIMIDs solved exactly variable eliminationscheme simultaneously propagates sets (partial) solutions. Although algorithmruns exponential time worst case (which expected, problemNP-hard), show many problem instances possible obtain optimalsolution efficiently pruning solutions Pareto-dominated others. heartalgorithms efficiency property moment variable eliminationlocal Pareto dominance implies global Pareto dominance, is, partial solutionPareto-dominated another partial solution cannot part optimal solution,hence safely discarded. show experimentally pruning Paretodominated local solutions enormously save computational resources, enable uscompute exact solutions much bigger problems previous algorithms. fact,algorithm orders magnitude faster CR algorithm randomly generateddiagrams containing 150 variables 1064 strategies.paper organized follows. Section 2 describes LIMID formalism presentsnew results complexity solving LIMID. variable elimination algorithmcomputing exact solutions presented Section 3, evaluated Section 4. last,Sections 5 6 contain related work final discussion. improve readability,proofs supporting results given appendix.2. Limited Memory Influence Diagramssection, describe LIMID formalism, state complexity solving LIMIDinstance, show LIMID transformed equivalent (in termsmaximum expected utility) diagram whose utilities nonnegative decision variablesparents. LIMIDs input algorithm next section. startexample decision problem limited information, use throughoutrest paper illustrate motivate concepts. Although example (whichessentially team coordination problem) rather simple, easily extendedaccount realistic scenarios.98fiSolving LIMIDs2.1 Fire Dispatching Problemparticular fire station contains group firefighters divided three units. firedispatcher decides units dispatch reported accident. dispatchedunit costs -1 utile, units dispatched cost utiles. case fire, highernumber dispatched teams higher chances minimum damage (which impliessaving lives preventing third-party financial losses). make things simple, consideraccident handled either appropriately, case say success,inappropriately, case say failure. Ideally, dispatcher wantsmaximize chance success minimizing number dispatched teams (andhence cost operation). successful operation rewarded 7/2 utiles,failure gets zero utiles.2.2 Variables Domainsformalism (limited memory) influence diagrams, quantities eventsinterest represented three distinct types variables nodes.2 Chance variablesrepresent events decision maker control, outcomes testsconsequences actions. Decision variables represent options available decisionmaker. Finally, value variables represent additive parcels utility associated stateworld. set variables considered relevant problem denoted U.variable X U associated domain X , finite non-empty setvalues X assume. elements X called states. assume existenceempty domain , {}, contains single elementdomain. Decision chance variables assumed domains differentempty domain, whereas value variables always associated empty domain.fire dispatching problem, represent act dispatchingunit decision variable Ti ; hence three decision variables T1 , T2 , T3domains T1 = T2 = T3 = {a, w}, stands act means unitdispatched, w stands wait means unit dispatched. outcomeincident assignment units represented binary chance variabledomain = {s, f } (representing success failure, respectively), evaluatedvalue variable V (which associated ). also individual costs per unitdispatched, modeled three value variables V1 , V2 V3 . set relevantvariables problem U = {T1 , V1 , T2 , V2 , T3 , V3 , O, V }.domain x set variables x = {X1 , . . . , Xn } U given Cartesianproduct X1 Xn variable domains. Thus, element u U definesstate world, is, realization actions events interest. xsets variables x U, x element domain x , writexy denote projection x onto smaller domain , is, xy containscomponents x compatible variables y. convention,x , . cylindrical extension x set x , {x x : xy = y}.Often, write X1 Xn denote set {X1 , . . . , Xn } and, clear context,X denote singleton {X}. instance, x = {T1 , O} = {T1 }, x =2. make distinction node graphical representation decision problemcorresponding variable.99fiMaua, de Campos, & Zaffalon{(a, s), (w, s), (a, f ), (w, f )}. Also, x = (w, s) x xy = w xO = s.cylindrical extension x given sx = {(a, s), (w, s)}.2.3 Operations Real-Valued Functionsoperations real-valued functions need defined. Let f g functionsdomains x , respectively. product f g defined functiondomain xy (f g)(w) = f (wx )g(wy ) w domain. Sum functionsdefined analogously: (f + g)(w) = f (wx ) + g(wy ). Notice product sumfunctions associative commutative, product distributes sum, is,f g = gf , f + g = gP+ f , f (g + h) = f g + f h. f function x , U,sum-marginalP f returnswP function x\y elementPdomain ( f )(w) = xwx f (x). Notice x = , f = f . Also,sum-marginal operationinheritscommutativityPP PP Pand associativity addition realnumbers, hence xy f = x\y f = y\x x f .{fxy }yy set containing functions fxy domain x , one element ,write fxy denote function w xy satisfies fxy (w) = fxw (wx ).instance, X two binary-valued variables domains X = {x1 , x2 }= {y 1 , 2 }, fX1 fX2 two functions X fX1 (x1 ) = 1/2,y2y2y1fX (x2 ) = 1/2, fX (x1 ) = 0 fX (x2 ) = 1, function fXfX(x2 , 1 ) = fX1 (x2 ) = 1/2 ,fX(x2 , 2 ) = fX2 (x2 ) = 1 .fX(x1 , 1 ) = fX1 (x1 ) , = 1/2fX(x1 , 2 ) = fX2 (x1 ) , = 0clear context, write 1 denote function returns onevalues domain 0 denote function returns always zero. x x ,indicator function Ix returns one x = x zero otherwise.f g functions domain x k real number, expressions f gf = k denote f (x) g(x) f (x) = k, respectively, x x (e.g.,previous example fX1 = 1/2). Finally, function domain containingsingle element (e.g., empty domain) identified real number returns.2.4 DefinitionLIMID L consists direct acyclic graph (DAG) set variables U annotatedvariable types (decision, chance value), together collection (conditional)probability mass functions (one per chance variable) utility functions (one per valuevariable). value nodes graph assumed children. precisemeaning arcs varies according type node point. Arcs enteringchance value nodes denote stochastic functional dependency, respectively; arcsentering decision nodes describe information awareness time decision made.variable X U, denote paX set parents X, is, setnodes arc pointing X. Similarly, let chX denote setchildren X (i.e., nodes arc X), faX , paX {X} denotefamily. let C, V partition U sets chance, decision valuevariables, respectively. chance variable C C associated set {pC : paC }100fiSolving LIMIDsV1V2V3T1T2T3uV1 (a) = uV2 (a) = uV3 (a) = 1uV1 (w) = uV2 (w) = uV3 (w) = 0pTO1 ,T2 ,T3 (s, t1 , t2 , t3 ) = I(a,a,a)uV (s) = 7/2uV (f ) = 0VFigure 1: LIMID representing fire dispatching problem.(conditional) probability mass functions pC quantifying decision makers beliefsstates x C conditional state parents (if C parents, singleprobability mass function assigned). Using notation introduced previous section,equivalently represent set probability mass functions associated variable Cpafunction pC C . assume chance variable X C stochastically independentnon-descendant non-parents given parents. value variable V V associatedreal-valued utility function uV paV , quantifies (additive) contributionstates parents overall utility. Thus, thePoverall utility state x CDgiven sum utility functions, is, u(x) = V V uV (xpaV ).2.5 LIMID Fire Dispatching ProblemFigure 1 depicts LIMID fire dispatching problem. graph, chance, decisionvalue variables represented ovals, rectangles diamonds, respectively.value variables V1 , V2 V3 associated utility functions uV1 , uV2 uV3 , respectively,representing cost per unit dispatched. utility outcome quantifiedfunction uV associated value variable V . chance variable associatedfunction pTO1 ,T2 ,T3 quantifies conditional probabilities P (O = o|T1 = tT1 , T2 =tT2 , T3 = tT3 ) success (o = s) failure (o = f ) given joint decision T1 ,T2 ,T3 .According model figure, dispatching three units results certain success,whereas dispatching less three units leads failure.2.6 Policies Strategiesdecision variable least one parent, policy specifies actionpossible state configuration parents, is, : paD .parents, state . set policies variable denoted. instance, policy T1 first unit running example state T1 .space policies T1 given T1 = {a, w}.Let , DD denote space possible combination policies. element= (D )DD said strategy L. Given policy state paD , letpdenote probability mass function conditional paD = pD = ID () .parents, pD = ID unconditional probability mass function .101fiMaua, de Campos, & Zaffalonpasimplify notation, sometimes write pD irrespective whether parent.paone-to-one correspondence functions pD policiespaDspecifying policy equivalent specifying pD vice-versa. denotepaset functions pD obtained way PD . So, instance, PT1 = {Ia , Iw }.strategy induces joint probability mass function variables Cpa paps ,pC CpD ,(1)CCDDassociated expected utilityEs [L] ,XpsCDXuV .(2)V VNotice two sums Eq. (2) different semantics. outer (leftmost) sumdenotes sum-marginalset variables C D, whereas inner (rightmost) denotesoverall utility function V V paV results sum functions uV .fire dispatching problem, eight possible strategies consisting decisionact wait units, example, = (T1 , T2 , T3 ) = (a, w, a) possiblestrategy. policy T1 = dispatches unit T1 induces probability mass functionpT1 = Ia T1 . Likewise, policy T2 = w induces function pT2 = Iw , policyT3 = induces pT3 = Ia . strategy = (a, w, a) induces joint probabilitymass function x O,T1 ,T2 ,T3T1 ,T2 ,T3ps (x) = pO(x)pT1 (xT1 )pT2 (xT2 )pT3 (xT3 ) ,expected utilityXEs [L] =ps [uV1 + uV2 + uV3 + uV ]O,T1 ,T2 ,T3=Xhps (x) uV1 (xT1 ) + uV2 (xT2 ) + uV3 (xT3 ) + uV (xO ) = 2 .xO,T1 ,T2 ,T3optimal strategy = (a, a, a) dispatches units, hand,expected utility Es [L] = 1/2.2.7 Theoretical Complexitytreewidth graph measures resemblance tree given numbervertices largest clique corresponding triangulated moral graph minus one(Bodlaender, 1996). Bayesian networks, complexity solving LIMID stronglyaffected treewidth. Given LIMID L treewidth , evaluate expectedutility given strategy time space exponential (Koller & Friedman,2009). Hence, bounded constant, computing Es [L] takes (at most) polynomialtime input size.primary task LIMID find strategy maximal expected utility,is, findEs [L] Es [L]102s.(3)fiSolving LIMIDsvalue Es [L] called maximum expected utility L denoted MEU[L].real problems, enumerating strategies prohibitively costly. fact,computing MEU bounded treewidth diagrams NP-hard (de Campos & Ji, 2008),and, following result implies, remains NP-hard even simpler LIMIDs.Theorem 1. Given singly connected LIMID treewidth equal two, variables three states, deciding whether strategy expected utilitygreater given k NP-complete.proof, based reduction partition problem (Garey & Johnson, 1979),given appendix.usual assumptions complexity theory, problem NP-hard solvebest available options (i) trying devise algorithm runs efficientlymany instances exponential worst-case complexity, (ii) trying developapproximation algorithm instances provides polynomial time solutionprovably within certain range optimal solution. Section 3, take option (i),present algorithm efficiently computes optimal solutions many LIMIDs,runs exponential time many others. following state result suggestsalternative (ii) likely unfeasible, even consider diagrams boundedtreewidth.Given > 1, -approximation algorithm (for solving LIMID) obtains strategyMEU[L]Es [L] .(4)set = 1/(1 ), 0 < < 1, -approximation algorithm finds solutionwhose induced relative error , is,MEU[L] Es [L].MEU[L](5)following result indicates provably good approximation algorithms existunless P=NP.Theorem 2. Given singly connected LIMID L bounded treewidth, (unless P=NP)polynomial time -approximation algorithm, 1 < < 2 ,number numerical parameters (i.e., probabilities utilities) required specify L.defer proof appendix. result asserts algorithm findssolutions LIMIDs polynomial time cannot guarantee relative error smaller1 2 , even set inputs restricted LIMIDs bounded treewidth. Hence,polynomial-time algorithm LIMIDs must eventually produce poor solutions,relative error close one large models. exception treewidthnumber states per variable bounded. cases, shown constructivelyearly work (Maua, de Campos, & Zaffalon, 2011) -approximationalgorithm runs polynomial time.103fiMaua, de Campos, & Zaffalon2.8 Constraining LIMIDs Nonnegative Utilitiesprinciple, utilities associated value variables LIMID take realvalue. complicates ordering functions use algorithmdevise here. Fortunately, easily efficiently transform LIMID Lequivalent LIMID L0 utilities nonnegative whose optimal strategiesalso optimal strategies L. Moreover, obtaining Es [L] Es [L0 ] strategystraightforward.Let L LIMID let k denote smallest utility value associatedvalue variables, is, V V follows k uV , VuV (x) = k x paV . following transformation generates new LIMID L0whose value variables associated nonnegative values.Transformation 3. value variable V V, substitute associated utility functionuV new utility function u0V = uV k.transformation shifts utility functions uV 0, makes uV (x) = 0least one V x paV . Since affects value variables, strategy L (theLIMID transformation) also valid strategy L0 (the transformed LIMID).expected utilities strategy L L0 related according followingresult.Proposition 4. strategy s, Es [L] = Es [L0 ] + k|V|.Proof. expected utility respect L0 givenEs [L0 ] =X=XxV=XXps (x)xXu0V (xpaV )VXps (x)[uV (xpaV ) k]ps (x)xuV (xpaV ) k|V|Xps (x)xV= Es [L] k|V| ,last step followsPx ps (x)= 1.optimal strategy L satisfies Es [L] Es [L] s, hence Proposition 4ensures Es [L] = Es [L0 ]+k|V| Es [L0 ]+k|V| = Es [L], implies alsooptimal strategy L0 . Similarly, optimal strategy L0 ,proposition Es [L0 ] = Es [L] k|V| Es [L] k|V| = Es [L0 ] s, thereforealso optimal L. following corollary summarizes results.Corollary 5. strategy L0 optimal strategy also optimalstrategy L.104fiSolving LIMIDsConsider running example more. smallest utility value k = 1.utilities associated value variables transformed LIMID L0 givenu0V (s) = 9/2u0V (f ) = 1u0V1 (a) = 0u0V1 (w) = 1u0V2 (a) = 0u0V2 (w) = 1u0V3 (a) = 0u0V3 (w) = 1 .strategy = (a, w, a) expected utility Es [L0 ] = Es [L] k|V| = 2 (1)4 = 2.optimal strategy = (a, a, a) obtains Es [L0 ] = 9/2.rest paper, consider LIMIDs nonnegative utilities, dueProposition 4 incur loss generality.2.9 Decision Nodes Many Parents Versus Parentless Decision Nodespolicy decision variable parents corresponds choice one states.Hence, space policies nodes contains number policies polynomialinput. hand, cardinality space policies decision nodesmany parents exponential number states parents. see this, considerten-state decision variable D. parents space policies contains 104policies. However, four ternary parent nodes, space contains 103 = 1081policies.One might wonder whether LIMIDs whose decision nodes many parentsdifficult solve LIMIDs parentless decision nodes. show that,least theoretical perspective, case, LIMIDefficiently mapped MEU-equivalent LIMID decision nodes parents.show optimal strategy original diagram producedoptimal strategy transformed diagram. particularly relevant algorithmssearch space policies, case algorithm devise here, allowus, without loss generality, focus LIMIDs whose decision nodes parents.formally describing transformation showing produces diagramequal MEU, let us first give idea works. end, considerLIMID L decision node least one parent (e.g., diagram Figure 2(a)),let 1 , . . . , denote configurations paD . policy maps configurationpadecision . function pD associated policy seenset probability mass functions pT1 , . . . , pTm pTi = p= ID ( ) , is,function pTi represents choice state fixed configuration parents.Recall policy associated parentless variable simply choice state.transformation replaces decision variable decision variables T1 , . . . , Tmchance variables X1 , . . . , Xm policy Ti corresponds decision (i )original variables policy (see diagram Figure 2(b)). chain X1 Xm chancevariables responsible making policy Ti active parents assumeconfiguration , occurs , either blocking allowing informationflow according value parents D. Thus, parents act selector105fiMaua, de Campos, & ZaffalonpaDpaDX1X2chDT1T2(a)XmchDTm(b)Figure 2: piece diagram (a) (b) Transformation 6.decides probability mass functions pTi associated decision nodes T1 , . . . , Tmgoing used, transformed diagram acts original one. probabilityX,Ti ,paDmass functions pXi1set ensure Xi = Ti paD = Xi = Xi1otherwise.Transformation 6. Consider LIMID L decision node least one parent,let 1 , . . . , denote configurations paD . Remove add = |paD |chance nodes X1 , . . . , Xm decision nodes T1 , . . . , Tm domains Xi = Ti =(for = 1, . . . , m). Add arc every parent X1 , . . . , Xm , arcevery Xi Xi+1 , < m, arc every Ti Xi , = 1, . . . , m. Add arcXm child D. Associate X1 functionxpaD = 1 xX1 = xT11,,papX11 (x) = 0,xpaD = 1 xX1 6= xT11/m xpaD 6= 1 .node Xi , = 2, . . . , m, associate function1, (xpaD 6=0, (xpaD 6=X,Ti ,paDpXi1(x)=pa1, (x=0, (xpaD =xXixXixXixXi= xXi1 )6= xXi1 )= xTi )6= xTi ) .paFinally, functions pX X child X substituted Xm domain,without altering numerical values.Figure 2 depicts decision node many parents (on left) new sub-diagramgenerated Transformation 6 (on right). difficult see treewidthtransformed diagram increased three, subgraph containingnew nodes, parents children triangulated contains cliques|paD {Xi , Xi1 , Di }| variables.3 Also, transformation two differentdecision variables affect different parts, hence transforming diagram diagramparentless decisions increase treewidth three. followingresult states also optimality strategies preserved transformation.3. Since treewidth given size largest clique triangulated moral graph minus one, |paD |lower bound treewidth original graph.106fiSolving LIMIDsProposition 7. Let L0 result applying Transformation 6 decision variableLIMID L, s0 denote strategy L0 , T1 , . . . , Tm denote correspondingpolicies T1 , . . . , Tm s0 . Let also policy ( ) = TipaD . Finally, let strategy L obtained substituting T1 , . . . , Tm s0(and keeping remaining policies). optimal strategy Ls0 optimal strategy L0 .proof appendix. decision variable original LIMID,transformed model contains chance variables specifying m|D |3 values, decisionnodes |D | states. treewidth original diagram bounded,bounded transformation takes polynomial time.4 example ten-statedecision variable four ternary parents, transformation replaces decision variable34 = 81 decision variables whose space policies contain 10 elements each, besides81 chance variables. combined space policies, is, T1 T81 containsalso 1081 elements, total search space still (doubly) exponential input.However, algorithms take advantage smaller local policy spaces reach bettersolutions, particularly true algorithm devise later on.rest paper assume without loss generality decision nodesparents utilities nonnegative.3. Solving LIMIDssection, describe new algorithm solving LIMIDs exactly propagatingmultiple non-dominated solutions. start defining basic algebraic structurealgorithm, given framework valuation algebra. showframework alone, similar one used SPU, might lead poor accuracy. thusextend framework sets valuations attempt improve accuracy increasingcomplexity. Efficiency obtained pruning sets cardinality kept smallpossible without affecting accuracy.3.1 Valuation Algebrabasic ingredients algorithmic framework representing handling information LIMIDs called valuations, encode information (probabilities, utilitiespolicies) elements domain. valuation associated subsetvariables U, called scope. concretely, valuation scope x pair (p, u)nonnegative real-valued functions p u domain x ; refer p uprobability utility part, respectively, . Often, write x make explicitscope x valuation . x U, denoted set possiblevaluationsscope x x . set possible valuations thus given , xU x . setclosed two basic operations combination marginalization. Combinationrepresents aggregation information defined follows.Definition 8. = (p, u) = (q, v) valuations scopes x y, respectively,combination valuation (pq, pv + qu) scope x y.4. treewidth bounded output algorithm, is, optimal strategy, mighttake space exponential input.107fiMaua, de Campos, & ZaffalonMarginalization, hand, acts coarsening information:Definition 9. = (p, u) valuationPscopeP x, set variablesx, marginal valuation ( x\y p, x\y u) scope y. case, sayz , x \ eliminated , denote z .Notice definitions combination marginalization slightly differ previous works influence diagrams (e.g., Lauritzen & Nilsson, 2001), usually requiredivision utility part probability part. removal division operationturns important feature discuss maximality valuations later on,otherwise definition equivalent valuations division, sense onecould easily reformulate message-passing algorithms like SPU using definition.terms computational complexity, combining two valuations scopesx y, respectively, requires 3|xy | multiplications |xy | additions numbers;computing , x, costs |xy | operations addition. words, costcombining marginalizing valuation exponential cardinality scope (andlinear cardinality domain). Hence, wish work valuations whosescope small possible. following result shows framework respectsnecessary conditions computing efficiently valuations (in sense keepingscope valuations obtained combinations marginalizations valuationsminimal).Proposition 10. system (, U, , ) satisfies following three axioms (weak)labeled valuation algebra (Shenoy & Shafer, 1990; Kohlas, 2003).(A1) Combination commutative associative, is, 1 , 2 , 31 2 = 2 1 ,1 (2 3 ) = (1 2 ) 3 .(A2) Marginalization transitive, is, z z x z(x=z )z .(A3) Marginalization distributes combination, is, x x ,x z x(x )z = x yyz .Proof. (A1) follows directly commutativity, associativity distributivity productsum real-valued functions, (A2) follows directly commutativity summarginal operation. show (A3), consider two valuations (p, u) (q, v) scopesx y, respectively, set z x z x y. definition ,XX[(p, u) (q, v)]z =pq,(pv + qu) .xy\z108xy\zfiSolving LIMIDsSince x \ z = \ z, p u functions x , followsXXXXXqv+uq, p(pv + qu) = ppq,xy\zy\zxy\zy\zy\zX Xv ,= (p, u)q,y\zy\zequals (p, y) (q, v)yz .following result Kohlas (2003, Section 2.2) direct consequence (A3)shall use prove correctness algorithm.Lemma 11. x x , , z z x = , (x )z = x z.primary goal valuation algebra computation marginal valuationsform = (1 ) . Let {X1 , . . . , Xn } set variables appearingscopes 1 , . . . , . marginal computed efficiently variable eliminationprocedure5 receives set = {1 , . . . , } permutation variables(1)(k )X1 , . . . , Xn , = 1, . . . , n replaces valuations , . . . , whose scope contains(1)(k ) (Xi )variable (Xi ) marginal =. Algorithm 1 describesprocedure. algorithm returns valuation j k , j , . . . , k n ,equals Axioms (A1)(A3) (Kohlas, 2003, Section 4.1).Algorithm 1 VariableElimination(x,,)Input: permutation variables x = {X1 , . . . , Xn } set valuations ={1 , . . . , } subsets xOutput: marginal valuation (1 )1: Let 02: 1 n(1)(k)3:Let , . . . , denote valuations i1 whose scope contains (Xi )(1)(k) (Xi )4:Compute =(1)(k)Let i1 {i } \ {i , . . . , }6: end7: return combination valuations n5:complexity variable elimination procedure given size largestvaluation generated loop. valuation might size exponentialsize valuations 1 , . . . , given input, but, discuss later on, certainconditions size bounded procedure takes time polynomialinput.5. Variable elimination algorithms also known literature fusion algorithms (Shenoy & Shafer,1990) bucket elimination (Dechter, 1999).109fiMaua, de Campos, & Zaffalon3.2 Computing Expected Utilitiesuse valuation algebra framework introduced compute expected utilitygiven strategy using variable elimination. Let = (D )DD strategy LIMIDL whose expected utility want compute, permutation variablesC D. assume decision nodes L parents (otherwise need firstapply Transformation 6), strategy simply configuration .procedure Algorithm 2 computes expected utility induced strategy s.paprocedure calls variable elimination set contains valuation C = (pC C , 0)chance variable, valuation V = (1, uV ) value variable, valuation= (ID , 0) decision variable. following result.Algorithm 2 ExpectedUtility(L, , s)Input: LIMID L whose decision nodes parents, permutation variablesC D, strategy = (D )DDOutput: expected utility1: Let2: C Cpa3:Add C = (pC C , 0)4: end5: V V6:Add V = (1, uV )7: end8:9:Add = (ID , 0)10: end11: Let VariableElimination(C D, , )12: return utility partProposition 12. procedure described Algorithm (2) returns expected utilitystrategy s.Proof. Let output Variable Elimination Algorithm. According Axioms(A1)(A3), = ,"# "# "#pa=pC C , 0(ID , 0)(1, uV ) .CCDDV VLet p u denote probability. definitionP utility part, respectively,Qpacombination, = (ps , ps V V uV ), ps = PXCD pX X (1). Sinceps isPprobabilityP distribution C D, follows p = xCD ps (x) = 1. Finally,u = CD ps V V uV , equals Es [L] (2).Consider LIMID fire dispatching problem (Figure 1) strategy =(a, w, a) whose expected utility want compute using procedure above. assume110fiSolving LIMIDsutilities nonnegative (i.e., already applied Transformation 3). According procedure Algorithm 2, first generate set = {O = (pTO1 ,T2 ,T3 , 0), V1 =(1, u0V1 ), V2 = (1, u0V2 ), V3 = (1, u0V3 ), V = (1, u0v ), T1 = (Ia , 0), T2 = (Iw , 0), T2 =(Ia , 0)}. X1 = O, X2 = T1 , X3 = T2 , X4 = T3 , let permutation variables(Xi ) = Xi = 1, . . . , 4. variable elimination algorithminput produces valuations1 = (V )O2 = (V1 T1 1 )T13 = (V2 T2 2 )T24 = (V3 T3 3 )T3loop, outputs valuation = 4 = (1, 2). Similarly, computeexpected utility optimal strategy = (a, a, a) run variable elimination= {O = (pTO1 ,T2 ,T3 , 0), V1 = (1, u0V1 ), V2 = (1, u0V2 ), V3 = (1, u0V3 ), V = (1, u0v ), T1 =(Ia , 0), T2 = (Ia , 0), T2 = (Ia , 0)}, outputs = (1, 9/2).general, described procedure take time exponential input. However,L bounded treewidth shown exists permutationprocedure takes time polynomial input. (e.g., Koller & Friedman, 2009,Section 23.4.3). Hence, space strategies sufficiently small, find optimalstrategy simply ranking strategies according expected utilities. However,expect feasible realistic diagram space strategies increasesexponentially number decision nodes (assuming parents), evendiagrams bounded treewidth bounded number states per variable.3.3 Local Search Algorithmsfirst attempt design fast algorithm solve LIMIDs, one might suggest localsearch scheme starts random solution repeatedly explores neighborhoodorder find solution higher expected utility. treewidth diagrambounded, expected utility neighbor solution efficiently computed,complexity algorithm given size neighborhood. possible approachdefine neighborhood solution strategies obtained changingsingle policy, gives local search space polynomial input. Algorithm 3describes greedy procedure step looks new policy improvescurrent best solution. algorithm guaranteed find strategy locallyoptimal neighborhood, is, cannot improved changing onepolicies. Lauritzen Nilsson (2001) stated sufficient conditions diagramsatisfy order guarantee solution produced local search procedure(globally) optimal. Unfortunately, following example shows, conditionsviolated even structurally simple chain diagrams, cases local searchprocedure might output local optima poor accuracy.Consider LIMID running example, suppose start strategy s0 =(a, w, a), expected utility 2. first step might try improve policyT1 , producing strategy = (w, w, a) whose expected utility 3. Since higherexpected utility initial solution, set sbest update highestexpected utility found. Next, try search better policy T2 , generatestrategy = (w, a, a). strategy expected utility 2, less111fiMaua, de Campos, & ZaffalonAlgorithm 3 GreedyPolicySearch(L, , s0 )Input: LIMID L, permutation variables C D, initial strategys0 = (D )DDOutput: locally optimum strategy sbest1: Let sbest s0 Esbest [L] ExpectedUtility(L, , s0 )2: repeat3:Generate new candidate strategy replacing single policy sbest4:Compute Es [L] ExpectedUtility(L, , s)5:Es [L] > Esbest [L]6:Set sbest Esbest [L] Es [L]7:end8: current solution cannot improved way9: return sbestX0D1D2X1X2DnXnRFigure 3: chain structure diagram n decision variables.expected utility best solution found far. Finally, look better policy T3 ,leads us strategy = (w, w, w), whose expected utility 4. Since bettercurrent best solution set sbest update associated expected utility.Since change single policy improve strategy, algorithm haltssolution whose expected utility 4 maximum expected utility 9/2 achievedstrategy = (a, a, a) (more 10% relative error), or, terms original diagram(by means Proposition 4), expected utility zero maximum expected utility1/2.procedure described similar SPU algorithm,illustrates pitfalls local search. fact, SPU output local optimum(but would start uniform policy every decision variable). Note solutionobtained greedy local search example degrades ratio utilitysuccess (achieved strategy (a, a, a)) utility failure increases. instance,utility success increased u0V (s) = 10 utility failure remainedsame, is, u0V (f ) = 0, algorithm would reach solution whose expected utility4, error 60% relative maximum expected utility 10. Moreover, casesSPU performs poorly rare. instance, plots Figure 4 show SPUs relativeperformance chain diagrams like one Figure 3. diagram generatedindependently sampling conditional distribution associated chance nodesymmetric Dirichlet distribution parameter 1/m, number variablestates. maximum expected utility diagram computed using algorithmdevise here, took less 3 seconds diagram experiment.(blue) point plots Figure 4 depict relative error SPU givendiagram. (red) line indicates third quartile fixed configuration. di112fiSolving LIMIDs0.60.70.60.50.40.30.20.10Relative error0.50.40.30.20.101020304050Number decision nodes1020304050Number states per variableFigure 4: Relative performance SPU randomly generated chain diagrams. (blue)circle depicts experiment (red) line depicts third quartile.agrams left hand-side plot obtained number states fixed 15,diagrams right number decision variables fixed ten.example, see third quartile line right-hand side plot 25%chain diagrams 20 states ten decision variables, SPU returned strategy(MEU[L] Es [L])/ MEU[L] 0.1. Also, cases SPU obtains 70%relative error. hand, see majority cases solutionreturned SPU achieved relative error less 10%. all, experimentsshow local search effective many cases, may produce poor results.3.4 Ordered Valuationsalso exploit redundancy computation expected utility neighboringstrategies decide whether candidate solution improves current solution withoutrun variable elimination completely? instance, evaluating qualitynew candidate strategy differs current best strategy policyassociated T1 , insight inspecting two valuations 2 producedvariable elimination example Section 3.2 using two different strategies? Fortunately,answer yes, show need concept ordered valuations.Let us define partial order (i.e., reflexive, antisymmetric transitive relation), set possible valuations, follows.Definition 13. two valuations = (p, u) = (q, v) , saydominates (conversely, say dominated ), write ,equal scope, p q, u v.scope x, deciding whether dominates costs 2|x | operationscomparison numbers. following result shows algebra valuationsmonotonic respect dominance.Proposition 14. system (, U, , , ) satisfies following two additional axiomsordered valuation algebra (Haenni, 2004).113fiMaua, de Campos, & Zaffalon(A4) Combination monotonic respect dominance, is,x x (x ) (x ) .(A5) Marginalization monotonic respect dominance, is,x xx x .Proof. (A4). Consider two valuations (px , ux ) (qx , vx ) scope x (px , ux )(qx , vx ), two valuations (py , uy ) (qy , vy ) scope satisfying (py , uy ) (qy , vy ).definition , px qx , ux vx , py qy uy vy . Sincefunctions nonnegative, follows px py qx qy , px uy qx vy py ux qy vx .Hence, (px , ux ) (py , uy ) = (px py , px uy + py ux ) (qx qy , qx vy + qy vx ) = (qx , vx ) (qy , vy ).(A5). Let subset x. follows monotonicity respect additionreal numbersXXXX(px , ux )y =px ,uxqx ,vx = (qx , vx )y .x\yx\yx\yx\yHence, result follows.Axioms (A4) (A5) assert combination marginalization preserve partialordering valuations. allow us detect suboptimal strategies earlyvariable elimination procedure. Consider comparing strategies = (w, w, w) s0 =(w, a, w) LIMID running example. third iteration loop (i.e.,= 3), variable elimination procedure produces valuations s3 = (ps3 , us3 )000s3 = (ps3 , us3 ) strategies s0 , respectively,ps3 (a) = 1 ,ps3 (w) = 1 ,us3 (a) = 3 ,us3 (w) = 3 ,0ps3 (w) = 1 ,00us3 (w) = 2 .ps3 (a) = 1 ,0us3 (a) = 2 ,000Thus, s3 s3 . Since s4 = (sT3 V3 s3 )T3 s4 = (sT3 V3 s3 )T3 , know0Axioms (A4) (A5) s4 s4 , hence Es0 [L] Es [L]. Therefore,need continue execution variable elimination s0 , expected value cannothigher s.Unfortunately, suboptimal solutions always produce valuations dominated optimal one variable elimination. example, consider strategies= (a, w, a) = (a, a, a). third step, variable elimination generates valuationss3 = (ps3 , us3 ) s3 = (ps3 , us3 )ps3 (a) = 1 ,ps3 (w) = 1 ,us3 (a) = 2 ,us3 (w) = 2 ,ps3 (w) = 1 ,us3 (w) = 1 .ps3 (a) = 1 ,us3 (a) = 9/2 ,114fiSolving LIMIDsThus, even though optimal strategy, s3 6 s3 .algorithm devise later exploits fact suboptimal solutionsearly detected eliminated search space. suboptimal solutionsmight eliminated variable elimination, algorithm runs exponential timeworst case, expected, problem NP-hard. Fortunately,experiments random problems suggest situations like frequent.3.5 Sets Valuationsmultiple runs variable elimination different inputs elimination ordering (i.e., permutation variables) represented sets frameworkalgorithm devise later on. instance, might consider set 3 valuations3 produced variable elimination third iteration loop every possiblestrategy. Due monotonicity combination marginalization respect ,immediately halt computation valuations 3 dominatedother, is, remove dominated valuations 3 . formalizedconcept maximal valuations operator max:Definition 15. Given finite set valuations , say maximalholds . operator max returns set max()maximal valuations .x set valuations scope x, set maximal valuations max(x )obtained m2 comparisons , (, ) x x .valuations set x scope x, say x also scopex. extend combination marginalization sets valuations follows.Definition 16. x two sets valuations ,x , {x : x x , }denotes set obtained combinations valuation x valuation .Definition 17. x x set valuations scope x x,x , {x : x x }denote set valuations obtained element-wise marginalization valuations y.checked sets valuations combination marginalization definedelement-wise satisfy axioms (A1)(A3), therefore form valuation algebra. Hence,Lemma 11 applies also sets valuations marginalization combination definedabove.Lemma 18. x x two sets valuations scope x y,respectively, z set variables z z x = , (x )z =x yz .Proof. result follows element-wise application Lemma 11 (x )z(x )z .115fiMaua, de Campos, & Zaffalon3.6 Solving LIMIDs Exactlyready describe MultiplePolicyUpdating (MPU) algorithm,solves arbitrary LIMIDs exactly. algorithm assumes decision nodesvariables, utilities nonnegative, hence Transformations 6 3applied running algorithm case assumptions fail.Consider LIMID L permutation variables C D, let n = |C D|.paalgorithm initialized generating set S0 contains singleton {(pC C , 0)}chance variable C, singleton {(1, uV )} value variable V setvaluations {(pD , 0)}, contains one element (pD , 0) per policy , decisionvariable D. set-valued variable elimination performed sets valuationsS0 , dominated valuations discarded set marginalization. Finally,optimal solution obtained utility part single maximal valuationset combination sets valuations Sn obtained variable elimination.procedure detailed Algorithm 4.Algorithm 4 MultiplePolicyUpdating(L, )Input: LIMID L permutation variables COutput: maximum expected utility1: Let S02: C Cpa3:Add singleton {(pC C , 0)} S04: end5: V V6:Add singleton {(1, uV )} S07: end8:9:Add set {(Id , 0) : } S010: end11: 1 n(1)(k)12:Let , . . . , denotewhoseh sets Si1scope contains (Xi )(1)(k) (Xi )13:Compute = max(1)(k)Let Si Si1 {i } \ {i , . . . , }15: end16: Let denote set combination sets Sn17: return utility part u (p, u) max(S)14:Since variables eliminated end loop, valuations setsempty scope probability utility parts identifiednumbers. Hence, algorithm outputs expected utility (i.e., real number) line 17.Let us illustrate algorithm example. more, consider LIMID Lfire dispatching problem applying Transformation 3 assume eliminationordering variables used example Section 3.2. start empty set116fiSolving LIMIDsS0 . chance variable, add set= {(pTO1 ,T2 ,T3 , 0)}S0 . add setsV1 = {(1, u0V1 )} ,V2 = {(1, u0V2 )} ,V3 = {(1, u0V3 )} ,V = {(1, u0V )}S0 due value variables V1 , V2 , V3 V , respectively. decision variable T1causes setT1 = {(Ia , 0), (Iw , 0)}scope T1 included S0 , similarly, variables T2 T3 , is, addsetsT2 = {(Ia , 0), (Iw , 0)} ,T3 = {(Ia , 0), (Iw , 0)} ,scopes T2 T3 , respectively, S0 , obtaining S0 = {O , V1 , V2 , V3 , V , T1 , T2 , T3 }.first iteration (i = 1) variable elimination loop (lines 1115),1 = max [V ]O = {(p1 , u1 )} ,p1 u1 functions T1 ,T2 ,T3 p1 = 1 u1 (a, a, a) = 9/2,u1 (x) = 1 x 6= (a, a, a). Note 1 singleton since singletonsinvolved computation. second iterationw2 = max [V1 T1 1 ]T1 = {(pa2 , ua2 ), (pw2 , u2 )} ,wwpa2 , ua2 , pw2 , u2 functions T2 ,T3 p2 = p2 = 1, u2 (a, a) = 9/2,wu2 (x) = 1 x 6= (a, a), u2 = 2. Note labeled functionsaccording policies T1 generated them. allows us easily extractoptimal strategy end algorithm.third iteration need compute3 = max [V2 T2 2 ]T2(a,a)= max({(p3(a,a)= {(p3(a,a)(a,a)(a,w)(a,a), u3(a,a)(a,w)), (p3(w,w)(a,w), u3(w,w), u3), (p3, u3(a,w)(w,w)(w,w)(w,w)), (p3(w,w), u3)}))} ,(a,a)p3 , u3 , p3, u3, p3, u3functions T3 p3=(a,w)(w,w)(a,a)(a,a)(a,w)(w,w)p3= p3= 1, u3 (a) = 9/2, u3 (w) = 1, u3= 2, u3= 3. Notevaluation associated policies T2 = w T2 = appear 3(a,w) (a,w)generate valuation equal (p3, u3). implies strategies (a, w, w)(w, a, w) expected utility, also strategies (a, w, a) (w, a, a).117fiMaua, de Campos, & Zaffalonlast iteration, generate set4 = max [V3 T3 3 ]T3(a,a,a)(a,a,a)= max({(p4, u4(a,a,a)(a,a,a)= {(p4, u4(a,a,a)(a,a,a)(w,w,a)), (p4(w,w,a), u4(a,a,w)), (p4(a,a,w), u4(w,w,w)), (p4(w,w,w), u4)}))} ,(w,w,a)(w,w,a)(a,a,w)(a,a,w)(w,w,w)(w,w,w)p4, u4, p4, u4, p4, u4, p4, u4functions(a,a,a)(w,w,a)(a,a,w)(w,w,w)(a,a,a)empty set p4= p4= p4= p4= 1, u4= 9/2,(w,w,a)(a,a,w)(w,w,w)u4= 3, u4= 2, u4= 4.(a,a,a)Finally, S4 = {4 }, algorithm returns u4= 9/2,expected utility optimal strategy (a, a, a). one see, optimal strategyeasily recovered labeling valuations corresponding policies.Differently message-passing algorithms obtain approximate solutionsLIMIDs (repeatedly) propagating single valuation (e.g., SPU algorithm),MPU algorithm computes exact solutions propagating several maximal valuationscorrespond partial combinations local decision rules. efficiency algorithmhandling propagation many valuations derives early removal valuationsperformed max operation propagation step.Consider set L , {s : }, given"#=papC C , 0"CC#DD(Id , 0)#!"(1, uV )V Vfunctions Id consistent policies s. difficult see#"L =papC C , 0CC#{(Id , 0) : }#!"{(1, uV )}V VDD="X.X S0Hence, Proposition 12 L valuation probability partone utility part equal expected utility strategy . Since relationinduces strict (linear) order L , MEU diagram equals utility part(single) valuation max(L ).N variable elimination procedure propagationstep responsibleNfor obtaining max( Sn ) = max(L ) efficiently distributingmax X S0 X , allows significant reduction cardinalitiessets scopes valuations produced.formally prove correctness algorithm. start showing maxdistributes marginalization combination:Lemma 19. (Distributivity maximality). x x two finite setsordered valuations z x, following holds.118fiSolving LIMIDs(i) max(x max(y )) = max(x );(ii) max(max(x )z ) = max(zx ).Proof. Part (i) shown Fargier, Rollon, Wilson (2010, Lemma 1(iv)).use similar proof show part (ii) also holds. First, show max(zx )zzmax(max(x ) ). Assume, show contradiction, element x max(zx ),x x , element max(max(x )z ). definition max(x ),zzzx max(x ) x x . Hence, (A5) implies zx x , x xzzzzfollows z/ max(max(x )z )x = x , therefore x max(x ) . Since xz max(max(x )z ) zx z . contradicts initial assumpztion since z x .Let us show max(xz ) max(max(x )z ). Assume contradictionzzz max(max(x )z ) \ max(zx ). Since z x , z max(x )zz z . shown max(x ) max(max(x )z ), hence z = zz max(zx ), contradiction.iteration propagation step, combination sets current poolsets Si produces set maximal valuations initial factorization marginalizedXi+1 , . . . , Xn :Lemma 20. {0, 1, . . . , n}, follows{X1 ,...,Xi }max,= maxS0Sii, Si collection sets valuations generated i-th iterationpropagation step MPU.Proof. induction i. basis (i = 0) follows trivially.Assume result holds i, is,{X1 ,...,Xi }max.= maxSiS0eliminating Xi+1 sides applying max operation get{X1 ,...,Xi } Xi+1Xi+1max max= max max.S0Si119fiMaua, de Campos, & ZaffalonApplying Lemma 19(ii) sides (A2) left-hand side yields{X1 ,...,Xi+1 }Xi+1max= maxS0Si= maxXi+1Bi+1Si \Bi+1= maxXi+1maxBi+1Si \Bi+1= maxSi \Bi+1= max,Si+1passage first second identity follows element-wise application(A1) Lemma 11, third follows second Lemma 19(i), last twofollow definitions Si+1 , respectively.able show correctness algorithm solving LIMIDs exactly.Theorem 21. Given LIMID L, MPU outputs MEU[L].NProof. algorithm returns utility part valuation (p, u) max, which,nNLemma 20 = n, equals max. definition S0 , valuationS0NS0 satisfies"=#CCpapC C , 0"#DD(Id , 0)"#(1, uV ) ,V Vcombination decisions (d)N , corresponds strategy ,exactly one valuationS0 strategy . Hence, PropoNsition 12, setcontains pair (1, Es [L]) every strategy inducingS0distinct expected utility. Moreover, since functions empty scope correspondNnumbers, relation specifies total ordering valuations,S0strategy associated (p, u). Sinceimplies single maximalelement.LetN(p, u) max, follows maximality Es [L] Es [L] s,S0hence u = MEU[L].120fiSolving LIMIDs3.7 Complexity Analysisvariable elimination, complexity algorithm depends permutation given input. time complexity algorithm given cost creatingsets valuations initialization step plus overall cost combinationmarginalization operations performed propagation step. Regarding initialization step, loops chance value variables generate singletons, thus take timelinear input. Since decision nodes parents, set added due decision variable contains , |D | valuations. Let , maxDD cardinalitylargest domain decision variable. initialization loop decision variablestakes O(|D|) time, polynomial input. Let us analyze propagationstep. running time propagating (sets of) valuations exponential maximumnumber variables scope valuations generated loop step.number depends permutation chosen best case equal treewidthdiagram plus one. Although finding optimal permutation (i.e., one leadsminimum maximum number variables per scope) NP-hard task, generatepermutations using standard heuristics variable elimination Bayesian networks,minimizing number fill-ins cardinality domain neighborset, empirically shown produce good elimination orderings (Jensen &Nielsen, 2007; Koller & Friedman, 2009).Consider permutation induces maximum number variables per scope, diagram bounded number states per variable . costcombination marginalization bounded constant, complexity dependsnumber operations performed. Moreover, case .Let denote cardinality largest set , = 1, . . . , n. Thus, computingrequires |U |1 operations combination (becausemaximum numberNsets might need combine compute Bi propagation step)operations marginalization. worst case, equal |D| O(|D| ), is,sets associated decision variables combined without discarding valuation.Hence, worst-case complexity propagation step exponential numberdecision variables, even width elimination ordering number states pervariable bounded. Note however pessimistic scenario and, average,removal non-maximal elements greatly reduces complexity, experimentsSection 4 show.3.8 Reverse Topological Orderingvaluations used MPU specify twice many numbers cardinality domainassociated scope. possible decrease number numerical parameters pervaluation algorithm needs handle factor two constraining eliminationvariables follow reverse topological ordering according diagram, is,requiring variable processed descendants processed.following result shows, reverse topological ordering produces valuations whoseprobability part equals one coordinates.121fiMaua, de Campos, & ZaffalonBV1CEV2FFigure 5: LIMID reverse topological ordering increases treewidth.Proposition 22. defines reverse topological ordering variables C D,= 1, . . . , n valuations probability part p = 1, 1 functionalways returns unity.Proof. show result induction i. Regarding basis, reversetopological ordering X1 variable containing value nodes children. Hence,paXB1 = {X1 } {{(1, uV )} : V chX1 }, definition X1 equals {(pX1 1 , 0)}paXpaXX1 chance node, {(pX1 1 , 0) : pX1 1 PX1 } decision node. followsPpaX PpaX P1 = max({( X1 pX1 1 , X1 pX1 1 V chX uV )}). Since paX , pX111PpaX1probability mass function X1 , p == 1. AssumeX1 pX1Ninductive hypothesisresultholds1,...,1,let,xBi \S0 .N= max([ Bi S0 ] x ). inductive hypothesis valuations set Bi \ S0probability part p = 1. Hence, definition combination, valuations xcontain also probability part equal one. reverse topological ordering impliestime variable Xi processed propagation step, childrenpaXprocessed. Hence, element Bi S0 set Xi , equals {(pXi , 0)} XipaXpaXchance node, {(pXi , 0) : pXi PXi } Xi decision node, {(1, uXi )}value node. Thus, = max(Xi x ). case Xi value nodeimmediate, since valuation result combination two valuationsprobability part equal one. Xi value nodeXpaX X paXpaX= maxpXi ,pXi ux : (pXi , 0) faXi , (1, ux ) xXiXiXpaXpaX= max 1,pXi ux : (pXi , 0) Xi , (1, ux ) x ,Xisince pXi probability mass function paX .result states assume reverse topological elimination ordering, MPUneeds care utility part valuations. Unfortunately, constrainingelimination order might increase complexity algorithm, following exampleshows.Consider LIMID Figure 5, variables assumed binary (we omitspecification probabilities utilities relevant matter).122fiSolving LIMIDsinitialization, S0 = {A , B , C , , E , F , V1 , V2 }. Using reversetopological elimination ordering implies first eliminate E, generatesset1 = max [E , V1 V2 ]E = {(1, u1 )} ,whose single element (1, u1 ) scope {A, C, D, F } size 24 = 16. Eliminating variablesordering F, C, B, A, D, E, hand, generates following sets.1 = max [F V2 C ]F = {(p1 , u1 )} ,2 = max [C E 1 ]C = {(p2 , u2 )} ,n(d) (d)3 = max [B ]B = (p3 , u3 ) : ,n(d) (d)4 = max [A V1 3 ]A = (p4 , u4 ) : ,n(d) (d)5 = max [2 4 ]D = (p5 , u5 ) : ,n(d)=(1,u):.6 = max E56scopes valuations 1 , 2 , 3 , 4 , 5 6 are, respectively, {E, C}, {D, E},{D, A}, {E, D}, {E} {}. one see, largest valuation generated using orderingF, C, B, A, D, E contains two variables scope therefore size 22 = 4.four-fold decrease size compared size set 1 generated using reversetopological ordering.Notice however even though using reverse topological ordering might increasesize valuations generated variable elimination, necessarily resultshigher complexity MPU. overall complexity algorithmdepends size largest valuation generated also cardinalitygenerated sets, possible reverse topological ordering induces significantlysmaller sets, produces valuations whose probability parts always equal one,might increase number dominated elements.4. Experimentsevaluate performance algorithm random LIMIDs generated followingway. LIMID parameterized number decision nodes , |D|, numberchance nodes c , |C|, maximum cardinality domain family chancevariable C , maxC |faC |, maximum cardinality domain familydecision variable , maxD |faD |. set number value nodes v + 2.variable Xi , = 1, . . . , c + + v, sample Xi contain 2 4 states.repeatedly add arc decision node children value nodeparents (so decision node least one value node children).step guarantees decisions relevant computation MEU. Finally,repeatedly add arc neither makes domain variable greater givenbounds makes treewidth 10, arcs added without exceeding123fiMaua, de Campos, & Zaffalonbounds.6 Note generates diagrams decision chance variableslog2 1 log2 C 1 parents, respectively. DAG obtained,randomly sample probability mass functions utility functions associated chancevalue variables, respectively.compare MPU CR algorithm de Campos Ji (2008) 1620 LIMIDsrandomly generated described procedure parameters 5 50, 8 c 50,8 64 16 C 64. MPU implemented C++ testedcomputer CR.7 Table 1 contrasts running times algorithm (averagesstandard deviation) different configurations randomly generated LIMIDs. rowcontains percentage solved diagrams (SCR SMPU ) time performance (TCRTMPU ) algorithms N diagrams randomly generated using parametersd, c, v, , C . fixed parameter configuration, MPU outperforms CRorders magnitude (line 12 contains case average running timeCR lower MPUs, note case CR solve one instance, whereasMPU solved 86% instances). Also, CR unable solve diagrams50 variables, whereas MPU could solve diagrams containing 150 variables32. algorithms failed solve diagrams = 64. diagramconsider unsolved algorithm algorithm able reach exact solutionwithin limit 12 hours. all, MPU appears scale well number nodes(i.e., d, c v) poorly domain cardinality family decision variables(i.e., ).good succinct measure hardness solving LIMID total numberstrategies ||, represents size search space brute-force approach. ||also loosely interpreted total number alternatives (over decision variables)problem instance. Figure 6 depicts running time number strategieslog-log scale two algorithms test set random diagrams.algorithm, solved instances shown, covers approximately 96% casesMPU, 68% CR. note MPU solved cases CR solved (butopposite). Again, see MPU orders magnitude faster CR. Within limit12 hours, MPU able compute diagrams containing 1064 strategies, whereasCR solved diagrams 1025 strategies.reduction complexity obtained removal non-maximal valuationspropagation step checked Figure 7, shows maximum cardinalityset generated propagation step contrast number strategies.diagram (a point figure) solved MPU, cardinality sets remains bounded106 vary number strategies (which equals largest cardinalitypropagated set worst case valuation discarded). showsworst-case analysis Section 3.7 pessimistic.6. Since current algorithms checking whether treewidth graph exceeds fixed k slowk 5 (Bodlaender, 1996), resort greedy heuristic resulted diagrams whose actualtreewidth ranged 5 10.7. used CR implementation available http://www.idsia.ch/~cassio/id2mip/ CPLEX(http://www.ilog.com) mixed integer programming solver. implementation MPUdownloaded http://www.idsia.ch/~cassio/mpu/.124fiSolving LIMIDsNcvCSCR (%)TCR (s)SMPU (%)TMPU (s)6060606060606060303060303060609030603030606030606060603060303030305551010101010101010101010202020202020201010202020303030303050508888882828282828282828888888878785858583838388888484877712121212121212121212122222222222222212122222223232323232525212168121681216163232326481216163232648163212168121681281281616161616161616641632646416161664326464161616161616161616161616161001001009893100968310930301009338300001006070501196280960600106 459 436 5115 53107 2730.4 0.21175 61263340 89662838 14931070 246173 0132687 75645443 100709660 103037 205944 99203820 81276455 934411895 12662849 40983416 48272261 65723448 58375014 29741001001001001001001001009610093860100100981007876010010010010010010098100100100100961000.006 0.010.02 0.050.002 0.010.02 0.02103 7860.007 0.010.05 0.080.2 0.247 1420.2 0.4905 28472440 76060.01 0.007155 1196270 182229 84938 14171592 34020.02 0.0080.5 0.50.6 1522 40112 110.07 0.0435 2142 100.1 0.03230 10270.2 0.11753 74050.5 0.09Table 1: Performance MPU CR randomly generated LIMIDs (numbersrounded down).125fiMaua, de Campos, & Zaffalon105MPUCRRunning time (s)104103102101100101102101102010401060Number strategies (||)Maximum set cardinality (maxi |i |)Figure 6: Running time MPU CR randomly generated LIMIDs.106105104103102101100101102010401060Number strategies (||)Figure 7: Maximum number valuations set propagation step MPU.5. Related WorkInfluence diagrams introduced Howard Matheson (1984) concise languagespecification utility-based decision problems. substantial literatureformalizes influence diagrams develop algorithms premises forgettingregularity (Cooper, 1988; Qi & Poole, 1995; Shachter & Peot, 1992). pointinterested reader works Jensen Nielsen (2007) Koller Friedman (2009).Zhang et al. (1994) studied families LIMIDs could solved dynamic programming, LIMIDs respecting forgetting regularity. SPU algorithmLauritzen Nilsson (2001) solves cases polynomial time diagram126fiSolving LIMIDsbounded treewidth. best knowledge, attempt (globally) solve arbitrary LIMIDs exactly without recurring exhaustive search space strategiesCR algorithm de Campos Ji (2008) compare algorithm.Shenoy Shafer (1990) introduced framework valuation algebras, statesbasic algebraic requirements efficient computation valuations. recently,Haenni (2004) incorporated partially ordered preferences algebra enable approximate computation. Fargier et al. (2010) extended framework preferencedegree structure order capture common algebraic structure optimization problems based partial order. algebra develop Section 3 partly castedframework.PFU framework Pralet, Verfaillie, Schiex (2007) subsumes many formalismsprobabilistic reasoning, constraint satisfaction decision making. comesdecision problems framework geared towards sequential decision makingequivalent assumptions non-forgetting, although authors mention possibilityextending limited information decision scenarios.variable elimination algorithm develop conceptually close messagepassing algorithm Dubus, Gonzales, Perny (2009). algorithm, however,handle uncertainty target primarily obtention Pareto-efficient solutionsspecific class multi-objective optimization problems.close relation maximum posteriori (MAP) inference Bayesiannetworks LIMIDs whose decision variables parents. sense, algorithm de Campos (2011), solves MAP propagating Pareto efficient probabilitypotentials join tree, relates ours.6. ConclusionSolving limited memory influence diagrams hard task. complexity resultspresented show problem NP-hard even diagrams bounded treewidthnumber states per variable, obtaining provably good approximationspolynomial time unlikely number states small.Despite theoretical hardness problem, developed algorithmspite exponential worst-case complexity performed empirically well large setrandomly generated problems. algorithms efficiency based early removalsuboptimal solutions, helps algorithm drastically reduce search space.Designing good heuristics elimination orderings algorithm seemscomplex task standard variable elimination algorithms (e.g., beliefupdating Bayesian networks), second component, cardinalityset, together domain cardinalities wish minimize. fact, preliminaryexperimentation shown favoring set cardinality expense domain cardinalitymight good approach. Unlike standard variable elimination, given eliminationordering LIMID, seem possible determine true complexityMPU advance (i.e., prior running algorithm). open question whetherMPUs complexity estimated beforehand, heuristics finding eliminationorderings perform better.127fiMaua, de Campos, & ZaffalonAcknowledgmentswork partially supported Swiss National Science Foundation (SNSF)grants no. 200020 134759/1, 200020 137680/1 200020 132252, Hasler Foundation grantno. 10030, Canton Ticino Computational Life Sciences Project. thankreviewers pointing us related work making number comments helpedus improve readability paper. short version paper appeared NIPS11 (Maua & de Campos, 2011).Appendix A. Missing Proofssection contains long proofs supporting results left main parttext improve readability.following two lemmas used proof Theorem 1 later on.Lemma 23. 2 real number nonnegative integer 2 + 2(i+3) <2+2 .Proof. Since 2 22 , 2 + 2(i+3) = 2 + 22 2i1 2 (1 + 2i1 ),sufficient show 1 + 2i1 < 22 . Binomial Theorem(1 + 2i1 2i)=2X2k=0k(2i1 )k .k = 0, . . . , 2i ,22i (2i 1) (2i k + 1)=(2i )k .kk!Hence,(1 + 2i1 2i)22XXXk i1 kk(2 ) (2) =22k = 2 ,k=02itherefore 1 + 2i1 < 2k=0k=0.4Lemma 24. 0 x 1/2 2x1 + 2x1 2x .Proof. obtain result approximating functions left- right-hand sideinequalities truncated Taylor expansions f (x) g(x), respectively,4showing 2x1 + 2x1 f (x) g(x) 2x . n-th order Taylor expansionleft-hand side around zero givenTn (x) = 1 +n/2X[ln(2)]2kk=1(2k)!x2k .Clearly, series converges hence 2x1 + 2x1 = limn Tn (x). Moreover,n, residual Rn (x) = 2x1 + 2x1 Tn (x) positive terms sum128fiSolving LIMIDsnon negative. Thus,f (x) = T2 (x) = 1 +[ln(2)]2 2x 2x1 + 2x1 .2similar fashion, apply variable change = x4 right-hand sideobtain Taylor expansion around zero, givenTn0 (y) = 1 +=1+nX[ln(2)]kk=1nXk=1k!yk[ln(2)]k 4kx ,k!also converges positive residual. Hence,42x = lim Tn0 (x)n= 1 + x4 ln(2) + x2 ln(2)X[ln(2)]k1k=21 + x4 ln(2) + x2 ln(2)Xk=2= 1 + x4 ln(2) +[ln(2)]2321k!!!x4k224k1x2 = g(x) .inequality obtained noticing [ln(2)]k1 /k! < 1/2, x 1/2 ln(2)geometric seriesXk=2124k11 X 1 k1ln(2)1 X 1 k< 7= 6 <.= 742222232k=0k=0Finally, since x2 1/4 < 15 ln(2)/32ln(2)2g(x) = 1 + x ln(2) x +3215ln(2)2< 1 + x ln(2)ln(2) +32322[ln(2)] 2=1+x = f (x) .224Hence, 2x g(x) f (x) 2x1 + 2x1 result holds.following result shows solving LIMIDs NP-hard even assume boundedtreewidth number states per variable.129fiMaua, de Campos, & ZaffalonX0D1D2X1X2DnXnRFigure 8: LIMID used solve partition problem Proof Theorem 1.Proof Theorem 1. Given strategy s, deciding whether Es [L] > k done polynomial time (Koller & Friedman, 2009), problem NP. Hardness shown usingreduction partition problem, NP-complete (Garey & Johnson, 1979)stated follows. PGiven setP n positive integers a1 , . . . , , set= {1, . . . , n} iI ai = iA\I ai ? assume n > 3.PPLet = 21 iA ai . even partition subset achieves iI ai = a.solve partition, consider rescaled problem (dividing every elementa),PvP= ai /a 2 elements look partitioniI vi = 1 (becausev=2).iAConsider following LIMID topology Figure 8. n binary decisionnodes labeled D1 , . . . , Dn . decision Di take states d1 d2 . chainchance nodes n + 1 ternary variables X0 , X1 , . . . , Xn states x, y, z.arc Xn single value node R. notational purposes, specify functionf domain {x, y, z} triple (f (x), f (y), f (z)). value node associatedutility function uR = (0, 0, 1). = 1, . . . , n, chance node Xi associated setconditional probability mass functions givend1 ,x= (ti , 0, 1 ti ),pXd2 ,x= (1, 0, 0),pXpdX1i,y = (0, 1, 0),pdX2i,y = (0, ti , 1 ti ),pdX1i,z = (0, 0, 1),pdX2i,z = (0, 0, 1),DXti [0, 1] (we specify variables later on). Note pXii i1 (w) = 0 everyw faXi wXi 6= wXi1 wXi 6= z. Finally, define pX0 = (1/3, 1/3, 1/3).Given strategy = (D1 , . . . , Dn ), let , {i : Di = d1 } index set policiesDi () = d1 .Es [L] =XpX0CDn!DXpXii i1 pDii=1=XXpX0XnuRnXi1pXiii=1CD\{Xn }Letps , pX0nXi1pXiii=1130pDipDi uR .fiSolving LIMIDsXpXn ,npX0Xi1pXiii=1CD\{Xn }XpDi =ps .CD\{Xn }Xn1w CD wXn = x (i.e., w xCD ) follows pXnn0 wXn1 = x. wXn1(wfaXn ) 6=Xn2= x pXn1(wfaXn1 ) 6= 0n1DXAlso, {1, . . . , n}, pXii i1 (wfaXi )wXn2 = x recursively.equals ti 1 otherwise. Hence,( Q1ti , wXi = x = 1, . . . , n 1ps (w) = 3 iI0,otherwise,n1Yti .ps (w) =3CDXpXn (x) =iIwxLikewise, holds w CD( Qps (w) =13iA\I ti ,0,wXi = = 1, . . . , n 1otherwise,thereforepXn (x) =n1ti .3iA\ISince pXn probability mass function Xn , pXn (z) = 1 pXn (x) pXn (y),XpXn uREs [L] =Xn= 1 pXn (x) pXn (y)1Y1=1titi .33iIiA\ILet us assume initially ti = 2vi . reduction original problemway polynomial, use upper bound outcomereduction obtain later. difficultseePP Es [L] concave functionv1 , . . . , vn achieves maximum iI vi = iA\I vi = 1. Since strategydefines partition vice-versa, even partition MEU[L] =1 1/3(1/2 + 1/2) = 2/3.show reduction encodes numbers ti time space polynomialb, number bits used encode original problem. part close analogylast part proof hardness MAP Bayesian networks de Campos(2011, Theorem 10).setting ti represent 2vi 6b + 3 bits precision (rounding necessary),is, choosing ti 2vi ti < 2vi + , 0 < 2(6b+3) ,131fiMaua, de Campos, & Zaffalon2vi ti < 2vi + 2(6b+3) , implies (by using Lemma 23 = vi 26b= 6b) 2vi ti < 2vi +2 .Assume even partition exists. Then8P6b6b5bti < 22 n iI vi = 21+2 n 21+2 ,iI6b n< 22PiA\Ivi6b n= 21+25b21+2,iA\I5b221 1+25b5b2+ 21+2=1.MEU[L] > 133(6)5bLet r equal 22encoded 5b + 3 bits precision (and rounded up), is,5b5b22(5b+3)2r<2+2, implies (by Lemma 24 = 25b 2 = 5b)5b5b5b15b4b22r < 22 +2= 22< 22 .(7)reduction done verifying whether MEU[L] > 1 r/3. already knoweven partition associated strategy obtains expected utility greater1 r/3, Equality (6) fact r rounded up. Let us considercase even partition exist. want show case MEU[L]4b1 22 /3, Inequality (7) implies MEU[L] < 1 r/3. Since evenpartition, strategy inducespartitionPP that, integer c differentzero, iI ai = ac iA\I ai = a+c, original numbersai positive integers add 2a. followsti +ti = 2c/a1 + 2c/a1 .iIiA\Iright-hand side equality function c {a, . . . , a}\{0}, symmetricrespect y-axis (i.e., f (c) = f (c)) monotonically increasing c > 0.Therefore, obtains minimum c = 1. Hence,ti +ti 21/a1 + 21/a1 .iIiA\ISince n > 3 implies 2 (because numbers ai positive integers),Lemma 24421/a1 + 21/a1 21/a .number ai encoded least log2 ai bits, therefore b log2 (a1 ) + +log2 (an ) = log2 (a1 ). latter greater equal log2 (a1 + + ),hence also greater log2 a. Thus, 2b , implies a4 24b44btherefore 1/a4 24b 21/a 22 . Hence,4b21/a1 + 21/a1 22.8. Since number bits used encode partition problem must greater equal n,n/2b n/b 1, hence 2(j+1)b n < 2jb , j > 0.132fiSolving LIMIDsThus, even partition exist4b221ti 1MEU[L] = 1ti +< 1 r/3 .33iIiA\Isummarize, built LIMID L polynomial time since ti specifiedDXusing O(b) bits n functions pXii i1 , encoding 18 numbers (whicheither 1, 0 ti ), 2n + 2 variables bounded number states. shownone-to-one correspondence partitions original problemstrategies L, given rational r = f (b) encoded O(b) bits existenceeven partition equivalent MEU[L] > 1 r/3.following lemma used proof Theorem 2. similar result shownPark Darwiche (2004, Lemma 9).Lemma 25. x 1 follows x + 1/2 > 1/ ln(1 + 1/x).Proof. Let f (x) = ln(1 + 1/x) 1/(x + 1/2).f 0 (x) =x211+ 2,+ x x + x + 1/4strictly negative x 1 since x2 +x < x2 +x+1/4. Hence, f (x) monotonicallydecreasing function x 1. limx f (x) = 0, f (x) strictly positive [1, ).Thus, result follows ln(1 + 1/x) > 1/(x + 1/2), since x 1.show approximately solving LIMIDs bounded treewidth givenminimum performance NP-hard.Proof Theorem 2. show fixed 0 < < 1 existence polynomialtime 2 -approximation algorithm solving LIMID would imply existencepolynomial time algorithm CNF-SAT problem, known impossibleunless P=NP (Garey & Johnson, 1979). similar reduction used ParkDarwiche (2004, Theorem 8) show analogous inapproximability result maximumposteriori inference Bayesian networks. Notice 1 < < 2 0 < < 1= 2 , hence existence -approximation algorithm implies existence2 -approximation, suffices desired result show latter cannottrue (unless P=NP).clause disjunction literals, literal either boolean variablenegation. say clause satisfied if, given assignment truth valuesvariables, least one literals evaluates 1. Thus, decide truth-valueassignment satisfies clause time linear number variables. CNF-SATproblem defined follows. Given set clauses C1 , . . . , Cm (subsets ) booleanvariables X1 , . . . , Xn , assignment truth values variables satisfiesclauses?positive integer q specify later on, consider LIMID obtained follows(the topology depicted Figure 9). boolean variable Xi add q binary133fiMaua, de Campos, & ZaffalonB1Dn1Sn1Dn2BqDnqSn2...D11B2...S11D12Snq...D1qS12S01US1qS0qS02Figure 9: Graph structure LIMID used proof Theorem 2.decision variables Di1 , . . . , Diq q chance variables Si1 , . . . , Siq domain {0, 1, . . . , m}.Additionally, q clause selector variables S01 , . . . , S0q taking values {1, 2, . . . , m},q binary variables B 1 , . . . , B q , value node U B q parent. illustratedFigure 9, LIMID consists q replicas polytree-shaped diagram variablesD1j , . . . , Dnj , S0j , . . . , Snj , B j , probability mass functions variables B 1 , . . . , B qchosen make expected utility equal product expected utilitiesreplica. replicas (i.e., j {1, . . . , q}), variable Dij (i = 1, . . . , n)represents assignment truth value Xi parents. selector variablesS0j represent choice clause process, is, S0j = k denotes clause Ckprocessed, summing S0j process clauses. variable Sij , =j1, . . . , n j = 1, . . . , q, Dij Si1parents. variables B j Snj and,j > 1, B j1 parents. j, assign uniform probabilities S0j , is, pS j , 1/m.0j = 1, . . . , q, set probabilities associated variables S1j , . . . , Snj Ckclause selected S0j Sij set zero Ck satisfied DijD1 , . . . , Di1 , Sij = Si1otherwise. Formally, x {S j ,Dj ,S j }1,j j1,p ji i1 (x) ,Si1,0,i1jjxSi = xSi1 = 0 ;jjxSi = 0 xSi1 = k 1 Xi = xDi satisfies Ck ;jjxSi = xSi1 = k 1 Xi = xDi satisfy Ck ;otherwise.Notice S1j first case never occurs since S0j takes values {1, . . . , m}.jjoint state configuration x S0j , . . . , Snj , D1j , . . . , Dnj xS0 = k {1, . . . , m} (i.e.,jclause Ck processed) xSn = 0, follows!njDij Si1pS jp jpDj (x)0i=1Siequals 1/m 0 < n clause Ck satisfied Xi = xDijX1 = xD1 , . . . , Xi1 = xDi1 , variables S1j , . . . , Si1assume value k (i.e.,134fiSolving LIMIDsjjjjxS1 = = xSi1 = k), xSi = = xSn = 0. Otherwise, equals 0. Hence,(partial) strategy sj = (Dj , . . . , Dnj ) x = 01jpsS j (x)nX,j ,...,S jpS j0n10ni=1jDij Si1pSijSAT (sj )pD j(x)=,jD1j ,...,DnSAT (sj ) denotes number clauses satisfied truth-value assignmentjSn BX1 , . . . , Xn according sj . variable B j associated function pBjx faBj ,jB j = xB j1 xSn= 0;1, xj j1jjpSBnj B (x) = 1, xB = 0 xSn 6= 0 ;0, otherwise;j10B 1 assume xB = 1. Hence, joint state configuration xB 1 , . . . , B q , Sn1 , . . . , Snqq1B 1 = = xB q = 1 xSn= = xSn = 0;1, xj j11q1pSBnj B (x) = 1, xB = = xB = 0 xSn 6= 0;j=10, otherwise.qFinally, set utility functionu associated U return 1 B q = 1 0j j1Q1q1otherwise. way, u qj=1 pSBnj B(x) equals 1 xB = = xB = 1 xSn =q= xSn = 0 zero otherwise. Thus, strategy = (s1 , . . . , sq ), sj =Dj , . . . , Dnj , follows1Es [L] =XuCD=quB 1 ,...,B q1 ,...,S qSnn==j1pS j0j=1XXjpSBnj BqjpSBnj BuB 1 ,...,B q j=11 ,...,S qSnnqjpsS j (0) =nj=1i=1j1j=1qnpjDij Si1SijXpS jjS0j ,...,Sn1jjD1 ,...,DnjSn BpBjj1pD j0ni=1pjDij Si1SijpD jjpsS jnq1SAT (sj ) .mqj=1instance CNF-SAT problem satisfiable optimum strategySAT (sj ) = j, MEU[L] = 1. hand, instance135fiMaua, de Campos, & Zaffalonsatisfiable, j strategy SAT (sj ) 1, henceMEU[L] (m 1)q /mq . given 0 < < 1, let q positive integer chosen1/2 > mq /(m + 1)q . show later q obtained polynomialinput. CNF-SAT instance satisfiable, 2 -approximation algorithmMEU[L] returns value Es [L]qm1 qMEU[L]>,Es [L]>m+12rightmost strict inequality follows m/(m + 1) > (m 1)/m.hand, CNF-SAT instance satisfiable, approximation returnsm1 qEs [L] MEU[L].Hence, use 2 -approximation algorithm solve CNF-SAT checking whetheroutput E[L] > (m1)q /mq . Since q positive integers, test bound (m1)q /mqobtained polynomnial time.remains show reduction polynomial input. LIMID containsq(2n + 2) + 1 variables, requiring specification 2(m + 1)2 numbers{0, 1/m, 1}. , number numerical parameters L, polynomially boundedq(m + 1)2 (4n + 4) + 2. Therefore, suffices show q polynomial n.definition, q obeys1 q21+> 2[q(m+1) (4n+4)+2] ,equivalent1q ln 1 +> q [(m + 1)2 (4n + 4) + 2] ln 2[(m + 1)2 (4n + 4) + 2]ln 21ln 1 +! 11[(m + 1)2 (4n + 4) + 2]ln 21ln 1 +q 1 >q>Since (by Lemma 25) + 1/2 > 1/ ln(1 + 1/m) 2 > ln(2), suffices choose q1q > (2m + 1)[(m + 1)2 (4n + 4) + 2] 1 .2+1words, q polynomially bounded 1 4n 1 . Therefore, MEU[L]approximated polynomial time ratio greater 2 solveCNF-SAT polynomial time.next result show Transformation 6 preserves expected utility strategies,strategies easily mapped back forward original transformeddiagrams.136fiSolving LIMIDspaD = jX1X2T1T2Xj1XjXj+1Tj1TjTj+1XmchDTmFigure 10: Reasoning proof Transformation 6.,paX,T ,paProof Proposition 7. looking definition functions pX11 pXii1 ,= 2, . . . , m, see paD selects j , is, conditional paD = j ,variable Xj independent Xj1 = 1, . . . , m, 6= j, variable Xiindependent Ti . words,Pr(Xj |Xj1 , Tj , paD = j ) = Pr(Xj |Tj , paD = j ) , pXjj .and, 6= j,XPr(Xi |Xi1 , Ti , paD = j ) = Pr(Xi |Xi1 , paD = j ) , pXii1 .visualize situation removing arc Xj1 Xj arcsTi Xi 6= j diagram Figure 2(b) (the arcs leaving paD also removedconditioning value paD ), results diagram Figure 10. Noteprinciple case j = 1 deserves special attention, X1 dependXi variable, function associated X1 slightly differ others.Nevertheless, similar reasoning applied. omit case j = 1sake simplicity.follows previous reasoningpXjm , Pr(Xm |paD = j )XXX=(pX1 pT1 )(pXjj pTj )pXii1 pTiT1 ,...,Tm X1 ,...,Xm1=XpTjTjXi=2,i6=jpXjjXi=j+1Xj ,...,Xm1XpXi1X1 ,...,Xj1|=XTjpTjXXj ,...,Xm1pXjjXpXii1 .i=j+1137pX1j1XpXi1i=2XpTiT1 ,...,Tm \Tj i=1,i6=j{z=1}fiMaua, de Campos, & ZaffalonXpaD = j , function pXi16= j equals indicator function IXi =Xi1 ,design, pXjj = IXj =Tj . thuspXjm=XXpTjTjIXj =TjIXi =Xi1 .i=j+1Xj ,...,Xm1term outer sum Tj , inner sum Xj , . . . , Xm1 differs zeroTj = Xj = Xj+1 = = Xm , case equals one. Hence,XpXjm =pTj IXm =TjTj= pTj .paconsider strategy s0 = (T1 , . . . , Tm , . . . ) L0 , let pXmD functionequals pXjm every value j paD . Let also policy original decisionvariable L ( j ) = Tj j, strategy L obtainedsubstituting policies T1 , . . . , Tm s0 . Finally, let pT1 , . . . , pTm distributionspainduced policies T1 , . . . , Tm s0 , pD distribution induced .paDSince value j paD pXm ( j ) = pTj , since design Xm = ,papafollows pXmD = pD . Hence, combination policies T1 , . . . , Tm L0derive corresponding policy L. converse also true: policypapagenerate T1 , . . . , Tm pXmD = pD (simply choose Ti = ( j ) i). Thus,one-to-one correspondence policies T1 , . . . , Tm policies ,papaone-to-one correspondence induced functions pXmD pD .remains show combination policies T1 , . . . , Tm corresponding policyinduce expected utility. Let C 0 D0 denote, respectively, set chancedecision variables L0 , C set chance decision variables L.Also, given strategy L, letpap0s ,pX X .CD\{D}paNote function independent choice policy , ps = p0s pD .Given strategy s0 L0XXEs0 [L0 ] =ps0uVC 0 D0=XV Vp0s,papX11C 0 D0X,T ,papXii1i=2!pTi=p0sXuVV VCD\{D}X XC 0 \C D0 \DXXCD\{D} XmpapXmD p0sXXpXii1i=2{z|=,paDpX11uVV Vi=1!XXPpaD= Xm pXmuVV V138,Ti ,paDpTii=1}fiSolving LIMIDs=XXpapD p0sXpsCDXuVV VCD\{D}=XuVV V= Es [L] ,strategy L obtained s0 substituting T1 , . . . , Tm corresponding policy .ReferencesBodlaender, H. L. (1996). linear-time algorithm finding tree-decompositions smalltreewidth. SIAM Journal Computing, 25 (6), 13051317.Cooper, G. F. (1988). method using belief networks influence diagrams. FourthWorkshop Uncertainty Artificial Intelligence.de Campos, C. P. (2011). New results MAP problem Bayesian networks.Proceedings 22nd International Joint Conference Artificial Intelligence, pp.21002106.de Campos, C. P., & Ji, Q. (2008). Strategy selection influence diagrams using imprecise probabilities. Proceedings 24th Conference Uncertainty ArtificialIntelligence, pp. 121128.Dechter, R. (1999). Bucket elimination: unifying framework reasoning. ArtificialIntelligence, 113 (1-2), 4185.Detwarasiti, A., & Shachter, R. D. (2005). Influence diagrams team decision analysis.Decision Analysis, 2, 207228.Dubus, J.-P., Gonzales, C., & Perny, P. (2009). Multiobjective optimization using GAImodels. Proceedings 21st International Joint Conference Artificial Intelligence, pp. 19021907.Fargier, H., Rollon, E., & Wilson, N. (2010). Enabling local computation partiallyordered preferences. Constraints, 15, 516539.Garey, M. R., & Johnson, D. S. (1979). Computers Intractability: Guide TheoryNP-Completeness. W. H. Freeman.Haenni, R. (2004). Ordered valuation algebras: generic framework approximatinginference. International Journal Approximate Reasoning, 37 (1), 141.Howard, R. A., & Matheson, J. E. (1984). Influence diagrams. Readings PrinciplesApplications Decision Analysis, pp. 721762. Strategic Decisions Group.Jensen, F. V., & Nielsen, T. D. (2007). Bayesian Networks Decision Graphs (2ndedition). Information Science Statistics. Springer.Kohlas, J. (2003). Information Algebras: Generic Structures Inference. Springer-Verlag,New York, USA.139fiMaua, de Campos, & ZaffalonKoller, D., & Friedman, N. (2009). Probabilistic Graphical Models: Principles Techniques. MIT Press.Lauritzen, S. L., & Nilsson, D. (2001). Representing solving decision problemslimited information. Management Science, 47, 12351251.Maua, D. D., & de Campos, C. P. (2011). Solving decision problems limited information. Advances Neural Information Processing Systems 24, pp. 603611.Maua, D. D., de Campos, C. P., & Zaffalon, M. (2011). Solving limited memory influencediagrams. ArXiv:1109.1754v2 [cs.AI].Park, J. D., & Darwiche, A. (2004). Complexity results approximation strategiesMAP explanations. Journal Artificial Intelligence Research, 21, 101133.Poupart, P., & Boutilier, C. (2003). Bounded finite state controllers. Advances NeuralInformation Processing Systems 16 (NIPS).Pralet, C., Verfaillie, G., & Schiex, T. (2007). algebraic graphical model decisionuncertainties, feasibilities, utilities. Journal Artificial Intelligence Research, 29,421489.Qi, R., & Poole, D. (1995). new method influence diagram evaluation. ComputationalIntelligence, 11, 498528.Shachter, R. D., & Peot, M. A. (1992). Decision making using probabilistic inference methods. Proceedings 8th Conference Uncertainty Artificial Intelligence,pp. 276283.Shenoy, P., & Shafer, G. (1990). Axioms probability belief-function propagation.Proceedings 4th Conference Uncertainty Artificial Intelligence, pp.169198.Zhang, N. L., Qi, R., & Poole, D. (1994). computational theory decision networks.International Journal Approximate Reasoning, 11 (2), 83158.140fi