Journal Artificial Intelligence Research 46 (2013) 579-605

Submitted 4/12; published 4/13

Predicting Behavior Unstructured Bargaining
Probability Distribution
David H. Wolpert

david.h.wolpert@gmail.com

Santa Fe Institute,
1399 Hyde Park Road, Santa Fe, NM 87501;
Information Sciences Group,
Los Alamos National Laboratory,
MS B256, Los Alamos, NM 87545

James W. Bono

jwbono@gmail.com

Abstract
experimental tests human behavior unstructured bargaining games, typically
many joint utility outcomes found occur, one. suggests predict
outcome game probability distribution. contrast
conventionally done (e.g, Nash bargaining solution), predict single
outcome. show translate Nashs bargaining axioms provide distribution
outcomes rather single outcome. prove subset axioms forces
distribution utility outcomes power-law distribution. Unlike Nashs original
result, result holds even feasible set finite. feasible set convex
comprehensive, mode power law distribution Harsanyi bargaining
solution, require symmetry Nash bargaining solution. However,
general modes joint utility distribution experimentalists Bayesoptimal predictions joint utility. bargains corresponding modes
joint utility distributions modes distribution bargains general, since
one bargain may result joint utility. introducing distributional
bargaining solution concepts, show external regulator use optimally
design unstructured bargaining scenario. Throughout demonstrate analysis
computational experiments involving flight rerouting negotiations National Airspace
System. emphasize results formulated unstructured bargaining,
also used make predictions noncooperative games modeler
knows utility functions players possible outcomes game,
know move spaces players use determine outcomes.

1. Introduction
game theory, bargaining refers scenarios two people must come
joint agreement outcome. structured bargaining, scenario modeled
noncooperative game, players making explicitly delineated alternating moves, e.g.,
proposals counter-proposals (Osborne & Rubinstein, 1994; Aumann & Hart, 1992).
contrast, unstructured bargaining, scenario modeled without explicit delineation
alternating moves. Instead known modeler feasible set
joint-utilities would arise possible bargains humans might reach.
Arguably, real-world bargaining scenarios, interaction bargainers
c
2013
AI Access Foundation. rights reserved.

fiWolpert & Bono

free form, requires unstructured bargaining analysis. Therefore, able
predict potentially emulate behavior interacting humans, need
accurate model human unstructured bargaining.
unstructured bargaining two spaces; space bargains space
joint expected utility vectors players assign bargain. Note may
one bargain results outcome. shorthand, conventional
leave term expected implicit refer utility. Similar shorthand
refer joint (expected) utility vector outcome.
game theoretic modeling date unstructured bargaining starts specification feasible set possible outcomes. knowledge
modeler concerning unstructured bargaining scenario. particular, modeler
ignorant spaces possible moves players may make reach bargain
even ignorant space possible bargains.
Traditional unstructured bargaining concerned specifying map takes
feasible set single outcome x S, i.e., point-valued solution concept (Nash,
1950; Harsanyi & Selten, 1972; Kalai & Smorodinsky, 1975; Kalai, 1977). example,
case Nashs original work, showed specified S,
axioms force unique prediction outcome x S. work regards map
normatively, providing fair reasonable bargaining outcomes. work regards
positively, prediction agreement reached humans bargaining
unstructured manner (Nydegger & Owen, 1974; Roth & Malouf, 1979; Camerer, 2003).
paper concerned positive viewpoint. extensive discussion
interpretation Nashs solution, see work Rubinstein, Safra, Thomson (1982).
contradiction theoretical positive work, experimental literature makes clear
real world one bargain non-zero probability outcome
given unstructured bargaining problem (Camerer, 2003; Roth & Malouf, 1979).1
accommodate this, paper consider maps take unstructured bargaining
problems feasible set probability distribution S, rather single element
S.2 this, derive parameterized set possible maps bargaining problems
distributions problems. (Intuitively, parameters reflect bargaining
power players.) call maps distributional bargaining concepts, call
images utility distributions.
many advantages using distributional bargaining concepts, addition
according experimental data better point-valued solution concepts. major
one arises external regulator modify aspects bargaining
game utility function bargaining outcomes. modifying game,
regulator changes associated distribution, therefore modifies value
expected utility. Accordingly, calculate Bayes-optimal modification
game.
1. fact, given noisy nature human behavior, would stunning certain physically possible
outcomes actually occurred exactly 0 probability, rather small, non-zero probability.
2. approach allows either finite infinite; succinctness generically refer
probability distribution even infinite properly refer probability density
function.

580

fiPredictive Unstructured Bargaining

approach deriving distributional bargaining concept translate Nashs
axioms unstructured bargaining conventionally applied maps produce single
utility outcome rather utility distribution apply distribution-valued maps.
precise, use probabilistic versions Nashs axioms Scale Invariance (SI)
Independence Irrelevant Alternatives (IIA). also use probabilistic version
axiom Translation-Invariance utilities (TI).
Adopting Bayesian perspective, view axioms formalizations ignorance modeler many scenarios, rather assumptions human behavior:
us, SI means modeler know anything relative probabilities outcomes chosen players likely change one simply scales utilities
possible outcomes. Similarly, us IIA means modeler know anything
relative probabilities outcomes chosen players likely change
subset possible outcomes removed. us, TI means modeler
know anything relative probabilities outcomes chosen
players likely change utilities possible bargains simply translated
constant.
axioms add extra one outcomes players strictly
better default outcome Non-Zero probability (NZ). (NZ imposed
holds experiments, due subject inattention nothing else.)
work, which, like ours, also concerned distributions (Peters
& Tijs, 1984). However, work Peters Tijs (1984) differs two important
ways. First, also use probabilistic versions SI IIA, unlike us, also
use axiom Pareto optimality (PAR). (As discussed conclusion, need
impose assumption binding contracts, therefore need assume joint utility
Pareto-optimal.) addition, unlike us, use NZ, (explicitly) use
TI. Second, Peters Tijs translate IIA SI different probabilistic versions
ours.
result difference, arrive different distributional solution concept
Peters Tijs. particular, axioms force distribution power
law set joint utility outcomes Pareto superior default outcome. contrast,
axioms used Peters Tijs result tightly characterized solution, power
law otherwise. Accordingly, approach Peters Tijs used derive
Bayes-optimal modification game might applied external regulator
game.
Perhaps importantly, version IIA means solution concept applies
bargaining game, even finite ones non-convex comprehensive.
quite important positive bargaining solution concept. real-world unstructured
bargaining, common bargainers consider finite number possible
outcomes. (Typically, real human bargainers field consider option
tossing weighted coin choose among possible bargains.) scenarios
feasible set finite.
generally, since dispense PAR, nothing restricts analysis
scenarios traditionally viewed bargaining. discuss conclusion,
results also used predict outcomes noncooperative games whenever
modeler knows feasible set games joint-utility outcomes, cannot
581

fiWolpert & Bono

tractably elaborate move spaces players. situations, best modeler
provide distribution final joint utility outcome. distributional
bargaining concept provides way this.
current paper work Peters Tijs papers extend
game theory replacing solution concepts point-valued (or set-valued) solution concepts probability distributions. particular, work Wolpert
Bono (2011) introduces distribution-valued solution concept noncooperative games
modeler know move spaces. work introduces map
input specification arbitrary non-cooperative game. output map
distribution possible mixed strategy profiles (i.e., player joint choices)
input noncooperative game. contrast, present paper introduces map taking
specification unstructured bargaining game input, producing distribution
player joint utilities output.
also recent papers artificial intelligence literature that, like ours, focus
non-equilibrium solution concepts (Brafman & Tennenholtz, 2003; Rezek, Leslie, Reece,
Roberts, Rogers, Dash, & Jennings, 2008; Aydogan & Yolum, 2012; Duan, Dogru, Ozen, &
Beck, 2012). Rather merely invoking equilibrium, Brafman Tennenholtz (2003)
employ reinforcement learning algorithm efficiently achieve coordination common
interest stochastic games. Rezek et al. (2008) look game theoretic solution concepts
machine learning perspective, i.e. assume players make inferences
opponents Bayesian framework derive novel fictitious play algorithm. Recent
work AI focuses bargaining, Aydogan Yolum (2012)
Duan et al. (2012), particularly closely related current paper. main difference
paper focuses unstructured game, whereas focus previous work
structured games.
1.1 Contribution Paper
first contribution paper derive distributional bargaining concept,
outlined above. focus two major advantages distributional bargaining
concept point-valued set-valued bargaining concepts: 1) ability modeler
apply decision theory predict outcomes, 2) ability external regulator
employ control theory optimally regulate system.
elaborate first advantage, consider modeler outcome
bargaining often loss function, measuring quality predicting
joint expected utility outcome x actual outcome x0 .3 Given loss
function, given distribution outcomes, well-defined Bayes-optimal
prediction single joint utility outcome. Bayes-optimal prediction vary
loss function. Accordingly, whatever distributional bargaining concept used, general
associated Bayes-optimal prediction neither likely joint expected joint utility
outcome (the Harsanyi solution) likely bargain.
Let vector-valued function taking bargains associated joint utility outcomes. general, need invertible. (E.g., feasible set set K possible
3. shorthand, often abbreviate joint expected utility outcome joint utility outcome,
even outcome.

582

fiPredictive Unstructured Bargaining

joint choices among N players, K N + 1.) cases cannot go
prediction joint utility (whether made conventional bargaining concept
distributional one) prediction joint bargain. One would need likelihood function
P (b | x) giving relative probabilities bargains b given joint utility x invert
prediction joint utility prediction bargain reached.
hand, invertible, distribution joint utilities fix
distribution bargains. However, even case, Jacobian non-uniform,
likely joint utility likely joint bargain. Intuitively, may
concentrate probability density regions possible joint utility outcomes (those
corresponding relatively many possible bargains), diminish others.
Note though, feasible set countable invertible, issue nonuniform Jacobian disappears. provides yet another benefit using distribution
bargaining concept like applicable countable (and even finite) feasible
sets, convex comprehensive ones.
illustration foregoing, consider scenario naturally modeled unstructured bargaining. Often path aircraft National Airspace System
renegotiated inflight, e.g., due unforeseen weather. negotiations follow
particular protocol unstructured. Accordingly, use illustrate
Nash distributional bargaining model. emphasize fact even simple scenario, map bargains joint utility outcomes non-invertible. result,
straightforward evaluate probability density function possible joint utility
outcomes, cannot said evaluating relative probabilities various bargains.
address second major advantage distribution-valued solution concept,
i.e. ability external regulator employ control theory optimally regulate
system, consider external regulator real-valued welfare function defined
bargains players, modify parameters bargaining
game. show distributional solution concept allows external regulator
perform Bayes-optimal configuration bargaining game, i.e., regulator
set parameters game control optimize expected welfare
resultant bargaining outcome. context Nash distributional bargaining concept,
call approach setting parameters Nash distributional bargaining management.
example, suppose regulator modify set allowed bargains
within range. case, Nash distributional bargaining concept used
determine regulators optimal modification. Similar external interventions would
changing default bargain, even modifying relative bargaining power players.
complicated type external intervention regulator presents
non-binding suggested bargain players. view suggested bargain
Schelling-like focal point negotiations, introduce model mapping
suggested bargain associated distortion distribution bargaining outcomes.
model provides regulator well-defined algorithm choosing suggested
bargain optimizes expected welfare outcome negotiations. use
application flight rerouting negotiations demonstrate Nash distributional bargaining
management using suggested bargains.
type optimal control strategic agents captured Nash distributional
bargaining management concept recent theme artificial intelligence literature.
583

fiWolpert & Bono

example, growing literature focused optimally managing negotiations
strategic, self-interested agents (Brafman & Tennenholtz, 1996; Chalamish & Kraus, 2012;
Lopez-Carmona, Marsa-Maestre, Klein, & Ito, 2012). primary difference
paper current literature bargaining game unstructured, others
use structured game. management unstructured flight rerouting negotiations
also considered type constrained automated mechanism design, like framework
studied Vorobeychik, Reeves, Wellman (2012). is, mechanism
paper bargaining scenario modified (automated) external regulator
unique flight rerouting game. Similarly, domain air traffic management, work
Agogino Tumer (2012) studies multi-agent approach managing air traffic flows
agents reinforcement learners. again, main difference
paper others difference controlling structured unstructured
games.
Despite recent interest optimal control strategic agents, foundation
ideas found conventional concepts correlated equilibrium. Ashlagi,
Monderer, Tennenholtz (2008) point out, correlated equilibrium often conceptualized
arising external party provides recommended actions game participants
cannot enforce recommendations. Ashlagi et al. look value correlation,
i.e. welfare improvement arising external parties recommendations,
directly analogous welfare improvement arising Nash distributional bargaining
management.
1.2 Roadmap Paper
section 1.3 start overview notation. section 2, provide
general definition distributional bargaining concepts. focus Nash
distributional bargaining concept, defined NZ probabilistic versions TI,
SI IIA. next prove Nash distributional bargaining concepts take form
power law distribution. result holds bargaining games, even non-convex,
non-comprehensive ones, even ones finite set possible bargains.
section 3, discuss Nash distributional bargaining concept. focus
mathematical structure, physical meaning, normative implications external
regulator change feasible set. show mode Nash distributional
bargaining concept Harsanyi bargaining solution. point also
impose Nashs symmetry axiom, instead get Nash bargaining solution mode
distribution outcomes.
section 4 introduce model flight rerouting negotiations. show
setting yields feasible set convex, comprehensive, even connected.
also demonstrate NDB model use prediction. also explore issues
invertibility bargaining set mentioned above.
section 5 develop concept Nash distributional bargaining management,
external regulator make recommendation bargainers modify
aspect unstructured bargaining game. demonstrate concepts using flight
rerouting scenario show external regulator make welfare improvements.
conclude discussion results directions future research.
584

fiPredictive Unstructured Bargaining

1.3 Notation
consider conventional N -dimensional unstructured bargaining problems. problem pair bounded feasible set, RN , special disagreement point, S.
assume contains least two elements, exists x
x (i.e, generalized strict inequality {xi > di i} holds). refer
problem simple = 0 point x (i.e., set N utility values
bargaining problem) component less 0. Given bargaining problem (S, d),
use term outcome refer element S. refer (S, d) standard
contains open set convex comprehensive4 .
define + B two sets A, B RN mean set x written
+ b A, b B, similarly B. (So particular, B
\ B.) also assume usual topology RN , etc. Given RN
k RN k 0, define kA subset RN given replacing
x Hadamard (i.e., component component) product kx , (k1 x1 , k2 x2 , . . .).
N
define AB two sets A, B R
R similarly.
shorthand, throughout use R measure implicit. particular,
feasible sets finite, expressions like dx . . . implicitly use point mass measure, i.e.,
equivalent sums.

2. Nash Distributional Bargaining Concepts
Bayesian perspective, interested posterior probability density function,
P (x | S, d). proceed decomposing distribution prior
likelihood. Instead, analogy Nashs approach, model P (x | S, d) directly, using
Nashs axioms restrict form.
convenient simplify notation, (again following Nash)
talk terms maps (S, d) distributions x rather terms P (x | S, d):
Definition 1. N -dimensional (distributional) bargaining concept map
N -dimensional bargaining problem (S, d) probability density function support
restricted S. (When countable, image probability distribution rather
probability density function, generically refer distribution implicit cardinality making clear whether mean probability distribution probability
density function.)
generically indicate distributional bargaining concept symbol ,
indicate value problem (S, d) S,d . S,d (x) non-negative real number
produced applying (S, d) evaluating resultant density function x.
use unstructured bargaining approach make prediction human bargaining behavior particular physical setting (laboratory field), addition explicitly specifying bargaining problem (S, d), must also specification
physical setting. Often specification implicit, make explicitly.
restrict attention bargaining problems (S, d) associated physical settings
every x physically possible. go also restrict attention
4. Recall comprehensive x S, x d, S.

585

fiWolpert & Bono

physical settings physically possible x x occur
non-zero probability. justify noting laboratory field settings,
even bargaining structured Pareto efficient outcome dominant
Nash equilibrium underlying game, would still non-zero probability
outcomes arise, due bounded rationality players. (A stronger version
reasoning invoke would require even outcomes
player worse disagreement point occur non-zero probability.)
Given foregoing, straightforward translate Nashs unstructured bargaining
axioms context distributional bargaining concepts. particular, SI, TI IIA
get translated follows:
Definition 2. N -dimensional distributional bargaining concept Nash distributional bargaining (NDB) concept RN ,
pSI x, y, x, d, k 1,
S,d (x)
S,d (y)

(kS,kd) (kx)
,
(kS,kd) (ky)

=

pTI x, x d, RN , S,d (x) = S{a},da (x a),
pIIA S, x, y, x, d,
S,d (x)
S,d (y)

=

T,d (x)
T,d (y)

NZ x, x d, S,d (x) 6= 0
pSI means rescale coordinates problem get new problem,
relative probability two points x original problem
relative probability rescaled versions points new (rescaled) problem.
translation Nashs scaling invariance axiom, SI, context distributional bargaining concepts. results paper, could weakened pSI
following:
pSI simple, standard, closed problem (S, 0), x, x, 0, k 1,
S,0 (x)
S,0 (y)

=

(kS,0) (kx)
(kS,0) (ky)

(1)

expository simplicity though, use stronger version given Def. 2.
pTI implements assumption translation-invariance utility functions. (Recall
notation, subtraction operator sets set subtraction.)
pIIA means remove potential solutions x bargaining
problem, relative probabilities x dont change. translation
Nashs IIA axiom context distributional bargaining concepts. Finally,
586

fiPredictive Unstructured Bargaining

mentioned earlier, NZ axiom says outcomes players strictly better
default outcome receive positive probability occurring.
axiom requires solution concept conform physical reality human behavior.
emphasize pIIA pSI assumptions human behavior. Rather
reflections ignorance modeler, prior probabilities Bayesian
modeling reflections ignorance. modeler know preferred
scale predict player behavior, want use distribution
invariant rescalings (Jaynes & Bretthorst, 2003). pIIA formalizes.
Similarly, modeler know removing set alternatives would affect
relative probabilities remaining ones, want use distribution
affect relative probabilities. pSI formalizes.5
remainder section work derivation main mathematical
result, presented Thm. 1. Loosely speaking, result says bargaining concept
must product (over players) power law distributions, obey axioms
presented above.
Given two N -dimensional problems (S, d) (T, d), define W . W .
Therefore pIIA, N -dimensional Nash bargaining concept , x,
x, d, S,d (x)/S,d (y) = W,d (x)/W,d (y). Similarly, W , therefore x,
x, d, T,d (x)/T,d (y) = W,d (x)/W,d (y). establishes following:
Lemma 1. Fix two N -dimensional problems (S, d) (T, d). x,
x, d, N -dimensional NDB concept ,
S,d (x)
S,d (y)

=

T,d (x)
T,d (y)

(2)

kinds geometric distortions underlying existence proofs Nash, KalaiSmorodinsky egalitarian solutions cannot applied (S, d) non-standard,
especially contains finite number elements. inability handle finite
one major obstacles Nash bargaining theory. real-world unstructured
bargaining, quite common people bargain finite number possible outcomes, without ever considering possibility using randomization device decide
final bargain.6
contrast, Lemma 1 means establish form T,d particular
class problems (T, d) infinite , established form pair
(S T, d). true even neither convex comprehensive. Indeed,
even finite.7 following result provides form T,d particular class
problems (T, d).
5. course, modeler knowledge preferred scales / human
players would change distribution alternatives removed, knowledge
replace pSI /or pIIA, respectively.
6. Indeed, one could argue real bargaining scenarios cannot involve uncountably infinite number
bargaining outcomes. all, real human beings cannot specify digits real number given
interval. communicating one another, real human beings able specify
numbers
finite precision, together finite set infinite-precision real numbers, like , 2, etc.
7. authors examined bargaining solutions non-standard domains. example, many papers
extend Nash solution non-convex (Kaneko, 1980; Herrero, 1989; Conley & Wilkie, 1996; Zhou,

587

fiWolpert & Bono

Theorem 1. Let N -dimensional NDB concept closed, simple,
standard problem (T, 0), T,0 (x) differentiable throughout interior .
constants ai , = 1, . . . n, independent
Q d, problems (S, d)
x x d, S,d (x) (xi di )ai .
Proof. Define 00 {d} x0 x d. use pTI write S,d (x) = 00 ,0 (x0 ).
suffices prove theorem x0 0 bargaining problem (S 00 , 0).
Let 0 {S 00 \ {x00 00 : x00i 0} {0}}. 0 00 without x00
least one player better disagreement point, unioned
disagreement point. (S 0 , 0) simple bargaining problem. addition, pIIA,
x0 , 0 0
0 ,0 (x0 )
0 ,0 (y 0 )

=

00 ,0 (x0 )
00 ,0 (y 0 )

Therefore suffices prove theorem problem (S 0 , 0).
Let convex, comprehensive closure 0 . (T, 0) simple, standard
bargaining problem whose interior contains (S 0 , 0). Therefore pIIA, suffices prove
proposition (T, 0).
Let interior . Note hypothesis, T,0 (.) differentiable function
.
N -dimensional z 0, define N -dimensional vector ln[z] , (ln(z1 ), ln(z2 ), . . .).
Make similar definitions ez sets ln[A] eA subset RN .
Note exception points zero-valued components, point
written vector ez z RN , since (T, 0) simple bargaining problem.
particular, every point written ez z RN .
Say z point T,0 (ez ) non-zero (so ez ). Define
associated scalar P(z) , ln[T,0 (ez )]. Note T,0 (.) non-zero Def. 2(i).
addition, continuous derivative throughout hypothesis. Therefore P(.)
continuous derivative throughout N ln[T ].
Choose vector k whose components equal 1 except component i,
interval (0, 1]. Define set k Ki . Note since simple
standard, Ki . Therefore ln[k] + N N .
Combining pSI pIIA, x0 ,
T,0 (kx0 )
T,0 (ky)

=

T,0 (x0 )
T,0 (y)

Taking logarithms get
P(ln[k] + ln[x0 ]) P(ln[k] + ln[y]) = P(ln[x0 ]) P(ln[y])
x0 , , k Ki .
1997; Mariotti, 1998a). generally replacing certain Nashs axioms. recent
work furthers analysis non-standard problems include finite domains (Mariotti, 1998b; Xu &
Yoshihara, 2006; Kbrs & Sertel, 2007; Peters & Vermeulen, 2010). However, general conclusion
work goal reach; work typically fails find convincing solution concept
closely resemble Nashs solution single-valued finite problems.

588

fiPredictive Unstructured Bargaining

Given P must differentiable across N , take partial derivative
sides equation respect ln[ki ]. Evaluate derivatives limit
P(v)
ki 1. allowed x0 , establishes P(u)
ui vi = 0 u, v N , i.e.,
establishes

P(u)
ui

constant across N . Therefore P linear coordinate across N .
0

Since true coordinates i, P(x0 ) = ln[S 0 ,0 (ex )] hyperplane across
Q 0Nai.
0
0
Accordingly, 0 ,o (x ) must product monomials across , i.e., T,0 (x ) (xi )
across . (The coefficients hyperplane give powers {ai } monomials.)
Converting x0 back x gives claimed result.

refer distributional bargaining concept meets conditions Thm. 1
power law distributional bargaining concept.
axioms giving power law distribution always hold real world.
simple example, default bargain may serve focal point, case one
might presume S,d (d) > 0. cases, either differentiability assumption
Theorem 1 must relaxed, one axioms defining NDBs must be.
generally, even differentiability assumed model focal point,
may possible motivate distributional bargaining concepts besides power
law bargaining concept. particular, several axiomatic arguments motivate
predicting behavior single decision maker according logit distribution
utilities (Train, 2003). Logit distributions utilities also motivated modeling
behavior multiple interacting players noncooperative game, e.g., arguments
originally used motivate logit Quantal Response Equilibrium (McKelvey & Palfrey,
1995), arguments based maximum entropy inference (Wolpert, Harre, Olbrich,
Bertschinger, & Jost, 2012). arguments suggest (but formally
derive) idea
Q
predicting behavior product logit distributions, S,d (x) exp(xi ), rather
product monomials. (One advantage distributional bargaining concept
require specification d, since replacing xi xi di exponents
change value probability distribution.)
rest paper, unless specified otherwise, restrict attention power
law distributional bargaining concepts. Note computational complexity evaluating power law distributional concept issue: one simply needs evaluate
product monomials get relative probabilities. normalization constant
also desired, worst, gleaned via standard Monte Carlo methods.
(a power law bargaining concept) S,d normalized standard
problem, ai must exceed 1. generally though, could countably infinite,
utilities player given xi {1/k +di : k = 1, 2, . . .}. S,d normalized
case, ai must exceed 0. Therefore Lemma 1, ai must always exceed 0.

3. Discussion Power Law Distributional Bargaining Concepts
section discuss assorted mathematical characteristics power law distributional
bargaining concepts, together physical implications.
589

fiWolpert & Bono

3.1 Relation Power Law Distributional Bargaining Concepts
Point-Valued Bargaining Concepts
emphasized derive functional form Thm. 1 need
analogs last two Nashs unstructured bargaining axioms, PAR SYM. However
straightforward incorporate desired. One natural way translate PAR
context distributional unstructured bargaining would require improving
every players utility (i.e., changing x way shrinks component xi )
cannot decrease probability density. Imposing NDB concept would rule
ai less 0.8
Similarly SYM translated mean ai must value.
call power law bargaining concept meets additional requirement fully
Nash distributional bargaining concept.
fully NDB concept (S, d), S,D
Q
supportQis distribution (xi di ) scalar . fixed d, level
curves x ofQ (xi di ) independent (assuming 6= 0). particular
maximum (xi di ) independent . Accordingly, find maximum,
Q
take = 1. Accordingly, mode S,d (x) maximum x (xi di ).
words, Nash bargaining solution bargaining problem (S, d) mode
fully NDB concept applied (S, d).
addition, fixed > 0, send , probability event {x
lies away Nash bargaining solution} 0. sense, solutions
Nash solution become impossible limit.
However, general feasible set possible bargains uncountable, function
taking joint bargains joint utility outcomes non-uniform Jacobian.
cases, likely bargain bargain corresponding Nash solution.
Furthermore, finite , general Bayes-optimal guess x Nash
bargaining
R solution. example, quadratic loss functions, Bayes-optimal guess
x dx S,d (x)x. differs Nash solution, argmaxx S,d (x). Furthermore,
consider expanding ith coordinate border S, leaving border along
axis j unchanged. quadratic loss, change general change
Bayes-optimal guess xj , even change Nash bargaining solution point.
final comment, recall impose SYM requirement power
law bagaining concept, players may differ, i.e., players allowed
heterogenous. case mode solution concept Nash bargaining
solution weighted Nash solution (Harsanyi & Selten, 1972), evaluated
constants power law exponents NDB concept.
3.2 Mathematical Structure Power Law Bargaining Concepts
Nowhere definition power law bargaining concept explicitly refer differences
xi di . differences arise Thm. 1? Ultimately, reason
definition bargaining problem requires specifying feasible set special
8. extreme way impose PAR would make predictions using P AR(S),d , P AR(S)
Pareto frontier S. Lemma 1, equivalent masking S,d Pareto frontier S,
i.e., replacing new distribution 0S,d whose support restricted Pareto frontier S,
x, x0 frontier, 0S,d (x)/0S,d (x0 ) = S,d (x)/S,d (x0 ).

590

fiPredictive Unstructured Bargaining

disagreement point within feasible set. Due this, translation invariance condition
translates point along point x gets evaluated. causes
difference two points invariant , reflected Thm. 1.
Note also S,d product distribution support. sense, independence
players automatic. independence explicit part definition
power law bargaining concept. Ultimately, independence arises fact
SI axiom Def. 2 involves scaling players utility independently.9
One must careful interpreting player independence. mean
distribution S,d , utility values players statistically independent.
Unless box, general border serve couple players. example,
= 0,

YZ
S,d (xi ) =
[ dxi S,d (xi , xi )]






YZ


[ dxi (xj )aj ]




j


6
[xi ]


= S,d (x),

(3)

i.e., product marginal distributions player utilities equal joint
distribution player utilities.
example implications this, say border changed
range possible utility values player grow particular range utilities
players, stay elsewhere.10 statistical coupling
players generally change. Intuitively, modification S, infer
likely value xj6=i given particular value xi changed. (This
despite independence irrelevant alternatives axiom.)
3.3 Physical Meaning Power Law Distributional Bargaining Concepts
Thm. 1 specify values S,d (x) x 6 d. However, Q
often case
x x 6 d, x0 x0 x, (x0i di )ai arbitrarily
close zero.11 Moreover, given x0 x, would quite peculiar experiments
case S,d (x) > S,d (x0 ). strongly suggests meets conditions
Thm. 1, stipulate S,d (x) = 0 x 6 d. (Formally though,
need make requirement analysis paper.)
9. example, modified axiom first rotating space RN , applying scaling operators,
rotating back, would longer product distribution individual xi , rather
linear combinations xi .
10. Formally, specify box values xi : {xj [bj , tj ] : j 6= i, tj > bj j 6= i}.
alone, extend associated border along coordinate i, i.e., xi , modify
expanding set xi (xi , xi ) S.
11. example, let sphere centered d, consider x x1 < 0
time x2 , x3 , . . . > 0. distribution point (, x2 , x3 , . . .) arbitrarily close zero
taking small enough.

591

fiWolpert & Bono

components ai Thm. 1 physically interpreted determined
distribution S,d physically interpreted. Ultimately,
physically mean unstructured bargaining scenario.
One possible physical meaning S,d population average humans
unstructured games feasible set disagreement point d.
interpretation, one adopts power law bargaining concept, makes sense
constants ai identical. Another possibility distribution interpreted
population average, rather refer set N particular individuals involved
bargaining problem hand. case, one adopts power law bargaining concept,
constants ai differ general. several reasons. One different people
different bargaining styles, different powers persuasion. generally,
bargaining scenarios certain players weak power affect outcome,
least able affect aspects outcome.
general two interpretations mutually consistent. Thats averaging (a population of) different distributions proportional product monomials
give product monomials, general. interpretation one adopts ultimately
depends interpretation one feels best characterizes bargaining scenario
consideration.
also third possible interpretation, one averages
bargainers, also set structured bargaining scenarios. interpretation,
distribution unstructured bargaining interpreted average distributions
structured bargaining scenarios. Nashs bargaining axioms would interpreted
reflecting ignorance external modeler concerning structure game
players engaged in.
3.4 Knitting Together 2-Player Distributions Get Multi-Player
Distributions
problematic aspects requiring IIA applies full joint distribution
N players utilities n > 2. However seems less objectionable stipulate
IIA applies distribution two players utilities, conditioned utilities
player(s). example, require P (x1 , x2 | x3 , . . . xN , S, d) obeys
IIA.12 also impose conditions defining power law bargaining concepts
conditional distribution. result conditional distribution
product monomials, i.e., i, j 6= i,
P (xi , xj | xi,j , S, d) (xi di )ai (xi,j ) (xj dj )aj (xi,j )

(4)

full generality exponents vary value xi,j .
set n(n 1) equations involving full joint distribution S,d = P (x |
S, d), parameterized matrices {ai (xi,j )}. infinite number joint
distributions P (x | S, d) obey equations simultaneously set
matrices {ai (xi,j )}: inspection, distribution power law bargaining
12. Note condition concern scenario players 3 N somehow fix
utilities values x3 , . . . xN , players 1 2 bargain. Rather concerns full
N -player bargaining scenario N players bargain together.

592

fiPredictive Unstructured Bargaining

Q
concept (i.e., form S,d (x) (xi di )ai ) obeys equations degenerate
case matrices constants.
generally, requiring Eq.s (4) simultaneously hold given set
matrices {ai (xi,j )} provides constraints conditioned two-player bargaining
concepts P (xi , xj | xi,j , S, d) knit together give full N -player bargaining concept.
precise, given matrices {ai (xi,j )}, full joint distribution S,d (x) = P (x | S, d)
must obey following equations:
Z
dx S,d (x) = 1,
Z
ai (xi,j )
aj (xi,j )
dx0i,j S,d (xi , xj , x0i,j )
i, j 6= i, S,d (x) (xi di )
(xj dj )
(5)
Future work involves investigating properties knitting together conditional
bargaining concepts.

4. NDB Flight Path Rerouting
section, demonstrate NDB using example flight rerouting negotiations
National Airspace System. Flight rerouting negotiations take place humans
aircrafts cockpit manning air traffic control (ATC) severe weather
air traffic result need changes scheduled flight path aircraft
already airborne. en route rerouting often referred tactical rerouting,
distinguish strategic rerouting, takes place airlines operation
center ATC flight must rerouted airborne.
Tactical rerouting negotiations initiated either cockpit ATC. Though
generally back-and-forth, offer/counter type feel, negotiations
follow set protocol. Therefore, unstructured bargaining approach appropriate
studying tactical rerouting negotiations. (Strategic rerouting negotiations might also
well suited unstructured bargaining approach.) addition, worth emphasizing
although natural tendency pilot defer ATC, light greater
knowledge latter, legally responsibility ultimately lies pilot.
Tactical rerouting negotiations generally result distance/heading pair determines way-point pilot agrees fly. example, negotiation might
look like this:
Cockpit: Denver Center, United 1492. Request 20 degrees right weather.
Controller: United 1492, long need heading?
Cockpit: Looks like 40 miles so.
point controller might grant permission, might make counter proposal,
accept 20 [degrees] left?. latter case, negotiations often continue
manner above. distance/heading pair summarized (l, ),
593

fiWolpert & Bono

[90, 90] angle degrees, l [0, N ] distance miles.13 constitutes
set bargains B, i.e., B = [0, E] [90, 90].
cockpit ATC preferences bargains make,
summarized utility functions set available bargains, B. example,
cockpit doesnt want fly far course, also doesnt want fly close
storm center. many cases, ATC might want best cockpit. Yet
cases, ATC might concerned air traffic things (e.g., impact
rerouting traffic) cockpit know care about.
evaluate utilities bargain (l, ) cockpit ATC, need evaluate
certain features flight path results it. determine flight path
results (l, ) first translate bargain way-point Cartesian coordinate
system. Cartesian coordinate system, say flights current position
w1 = (0, 0), flight pointing direction positive horizontal axis.
Let r radians equivalent . means agreed way-point (l, ), located
w2 = (l cos r , l sin r ) Cartesian coordinates.
meeting way-point, w2 , flight return fix along original
flight path. rerouting negotiations, fix also part negotiated bargain.
However, bargains relatively uncommon. simplify model, assume that,
whatever bargain, flight return fix located along horizontal axis
point w3 = (E, 0) Cartesian coordinates. Using linear interpolation connect w1 , w2
w3 , create constant-speed, constant-altitude 3D flight path, l, (t) = (el, (t), nl, (t)).
order simplify notation, refer components l, (t) e(t) n(t)
dependence (l, ) implicit.
use l, (t) = (e(t), n(t)) calculate utility-relevant features bargain
(l, ). features include total length l, (t), given

Z
e(t) 2 n(t) 2
Ll, = dt
+
,


whether l, (t) maintains safe distance weather center C radius
R,
(
1 mint ||l, (t) C|| > R
Dl, =
0 otherwise.
assume cockpits utility linear combination total length maintenance
safe distance, given by:
sc (l, ) = L Ll, + Dl,
real numbers L 0 0.
assume ATCs utility linear combination maintenance safe distance
new flight paths impact existing air traffic. express traffic penalty
function H(e, n), defined Cartesian coordinates. total traffic penalty flight
path l, (t)
Z
Hl, =

dt H(e(t), n(t)).

13. limit angle [90, 90] heading changes 90 degrees extremely uncommon.

594

fiPredictive Unstructured Bargaining

Feasible Set Utility Outcomes
800

1000

Cockpit Utility

1200

1400

1600

1800

2000

2200
600

400

200

0

200
400
ATC Utility

600

800

1000

1200

Figure 1: Feasible set joint utility outcomes rerouting unstructured bargaining
problem. feasible set non-convex comprehensive,
even connected. Parameters ac = aatc = 1, L = 4, = 300, H = 0.01
= 300. weather large circle center C = (150, 20) radius
R = 40. traffic penalty given H(e, n) = n.
express ATCs utility satc (l, ) = + H Hl, real numbers 0
H 0.
thing left specify outcome negotiations break down, i.e.,
default bargain, db . might that, event negotiations break down, cockpit
choose path maximizes objectives without getting clearance ATC.
scenario, punishment cockpit receives result changing course without
ATC approval also enter cockpits objective. However, likely many
complicated variable factors consider modeling effect punishment,
pilot attitude, idiosyncratic airline rules, etc. Hence, simplicity, simply take
default bargain original flight path, i.e., straight line (0, 0) (E, 0).
Therefore, bargaining game given
= {(xc , xatc ) R2 : xc = sc (l, ) xatc = satc (l, ) (l, ) B}
= (sc (E, 0), satc (E, 0)).
Figure 1 shows feasible set utility outcomes associated flight rerouting
model. Note set neither convex comprehensive. Furthermore,
breaks, feasible set consists disconnected subsets. Despite irregularities
feasible set, NDB distribution applied trivially, giving
S,d (x) (xc dc )ac (xatc datc )aatc
595

fiWolpert & Bono

Indifference Curves Multiple Intersections
1.5

Heading (radians)

1

0.5

ATC
Cockpit

0

0.5

1

1.5
0

50

100

150
Distance

200

250

300

Figure 2: Indifference curves sc satc plotted space bargains flight
path rerouting problem. s(., .) invertible indifference curves
multiple crossings. Parameters ac = aatc = 1, L = 4, = 300,
H = 0.01 = 300. weather large circle center C = (150, 20)
radius R = 40. traffic penalty given H(e, n) = n.
x x d, S,d (x) = 0 otherwise.
discussed introduction, want use NDB distribution joint
utilities make predictions bargains b B, dont likelihood
function P (b | x), must able invert mapping = (sc , satc ). Unfortunately,
invertible, seen noting figure 2 indifference curves sc
satc multiple crossings.
Recall map invertible, translate distribution
joint utilities distribution bargains would use Jacobian s.
precise, (l, ) point B, NDB distribution bargains, B
S,d , evaluated
(l, ) given
B
S,d (l, ) = S,d (sc (l, ), satc (l, )) |J(l, )|,
J(l, ) determinant Jacobian
fi
sc
c
fi

fi l
fi satc satc
l



s,
fi
fi
fi.
fi

Note though even invertible, Jacobian might well-defined. Indeed,
flight-rerouting contains discontinuities flight paths cross outside inside
596

fiPredictive Unstructured Bargaining

NDB Concept Translated Space Bargains
80
60
40

Heading

20
0
20
40
60
80
0

50

100

150
Distance

200

250

300

Figure 3: NDB concept mapped back space bargains B 0 = [0, 1, ..., N ]
[90, 89, ..., 89, 90], ac = aatc = 1, L = 4, = 300, H = 0.01
= 300. weather large circle center C = (150, 20) radius
R = 40. traffic penalty given H(e, n) = n.
weather radius, variable Dl, jumps 1 zero. Partial derivatives sc
satc exist points B.
Say set bargains countable, e.g., grid imposed technological
constraints aircraft cognitive constraints pilots air traffic controllers.
case would need worry discontinuities calculate Jacobian.
might still non-invertible however, cannot transform distribution
joint utilities distribution bargains. fact happens flight-rerouting
set bargains modified grid integer distances degrees, i.e.,
B 0 = [0, 1, ..., N ] [90, 89, ..., 89, 90].

Regardless cardinality B, one translate distribution joint utilities
distribution bargains even non-invertible, long one knows likelihood
bargains given joint utility outcomes. example, say B countable,
bargains give rise particular joint utility x equally likely. case,
|Bx | bargains give rise x, probability bargain simply
S,d (x)/|Bx |. Using assumption grid B 0 = [0, 1, ..., N ] [90, 89, ..., 89, 90],
figure 3 shows NDB distribution flight-rerouting problem space bargains.
597

fiWolpert & Bono

5. NDB Management
Suppose regulator, external unstructured bargaining game, preferences
joint utility bargain reached, formalized real-valued social welfare function
U (x S). Suppose change characteristics (S, d).
example, might able change joint utility default bargain certain
ways, replace one several 0 S. cases, assuming model game
NDB S,d , regulator choose
= argmax E (U (x))
Z
= argmax dx U (x)S ,d (x)

(6)

generally, may affect s(.), dB , and/or B, image (S, d).
general setting regulator choose
= argmax E (U )
Z
= argmax dx U (x)s (B ),s (d ) (x)
B

(7)

generally still, social welfare function U might replaced function W
defined space bargains B. long invertible , Eq. 7 still holds
variant. (Just define U (x (B )) W (s1
(x)).) However non-invertible
certain , equation must modifed. situation, assuming
associated likelihood function, regulator choose action
= argmax E (W (b))
Z
= argmax dxdb P (b | x)W (b)s (B ),s (d ) (x)
B

(8)

addition types actions regulator, others
considered typical axiomatic analyses unstructured bargaining including analysis
paper point. example regulators intervention simply
suggest bargain players start bargain, i.e., provide focal
bargain bargaining. advise regulator cases, need model
effects types action regulator, use model Eq. 7 .
refer types optimal choice external regulator NDB
management. mean claim always possible implement NDB
management. situations external regulator may able anything
substantial effect distribution possible bargains players.
rest section present preliminary analysis NDB management, concentrating
example regulators possible action provide focal bargain.
5.1 NDB Management Rerouting Negotiations
demonstrate simple form NDB management using example rerouting negotiations Section 4, related focal point concept introduced Schelling (1960).
Specifically, assume regulator make suggestion flight rerouting
598

fiPredictive Unstructured Bargaining

cockpit ATC begin rerouting negotiations. idea
suggestion serve focal bargain negotiations, effect raising relative probability bargains similar suggested one. suggestion external
regulator could implemented automated software operates
cockpit ATC.14
Use B
S,d (l, ) indicate distribution (l, ) B induced NBD game
(S, d). (So needed, B
S,d (l, ) implicitly depends Jacobian likelihood
function P (b | x) = P (l, | x).) assume regulator models effects
suggested focal bargain (l0 , 0 ) distribution B


l0 ,0 (l, ) B
Pl0 ,0 (l, )
S,d (l, )1
Z(B)
bivariate Gaussian distribution mean (l0 , 0 ) covariance matrix ,
[0, 1] measures strength impact managers suggestion players
behavior, Z(B) normalization constant . call NDB management
distribution. clearly ad hoc model focal bargains; use simple
way demonstrate NDB management.
assume minimize computational requirements, regulator requires
space bargains grid integer distances degrees, i.e., B 0 = [0, 1, ..., E]
[90, 89, ..., 89, 90].15 mentioned Section 4, even finite set possible bargains
instances multiple bargains map joint utility outcomes.
usual, address must specify P (b | x); make simple assumption
likelihood uniform b map x, zero others. Accordingly,
B
S,d (l, ) =

S,d (s(l, ))
,
|Bx |

|Bx | number bargains B give rise outcome x.
usual, regulator choose action means suggest heading
distance maximize resultant expected social welfare. Writing social welfare
function W (l, ), optimal action
Z
argmax(l0 ,0 )B El0 ,0 ,S,d (W ) =
dl W (l, )Pl0 ,0 (l, ).
B

general, W might depend impact rerouting traffic flow entire
National Airspace System, addition depending utility cockpit ATC.
simplicity though, take W (l, ) = sc (l, ) + (1 )satc (l, ) [0, 1].
Figure 4 depicts effect regulators recommendation predicted flight path.
left panel NDB distribution bargains, B
S,d . represents prediction
regulator makes suggestion. NDB modal bargain produces detoured
flight path denoted center panel sequence two lines joined open
14. Experimental studies find empirical support theory focal points bargaining (Binmore,
Swierzbinski, Hsu, & Proulx, 1993).
15. practice, may need explicit requirement, since unlikely non-integer values
would considered players.

599

fiWolpert & Bono

NDB Distribution Bargains

Flight Paths

NDB Management Distribution

80

80
50

60

60

40
30

40

40

20

0

20

20
10

Heading

NorthSouth

Heading

20

0

0

10
20
20

40

40

30
40

60

60

50
80

80
0

50

100

150

Distance

200

250

300

0

50

100

150

WestEast

200

250

300

0

50

100

150

200

250

300

Distance

Figure 4: Left Panel: NDB distribution B 0 = [0, 1, ..., E] [90, 89, ..., 89, 90]
ac = 1, aatc = 3, L = 4, = 300, H = 0.01 = 300. Center Panel:
Weather large circle center C = (150, 20) radius R = 40, NDB
modal flight path (circle line), NDB management modal flight path (square line),
NDB management recommended waypoint (triangle line). Right Panel: NDB
management distribution = .5, = .5, = 360 l = 1, 200.
traffic penalty given H(e, n) = n.

circle; flight begins detour point (0, 0) returns original flight path
(300, 0). reason modal flight path pass closer weather
traffic penalty given H(e, n) = n, i.e., traffic denser north. means ATC
rewarded flight paths pass south. since aatc = 3 > 1 = ac , ATC
relatively successful convincing cockpit accept bargains result longer,
southerly flight paths.
NDB management distribution shown right panel. represents prediction regulator made optimal suggestion. case, optimal suggestion
(130, 10). path associated optimal suggestion denoted center panel
pair lines joined triangle. modal bargain NDB management distribution induced suggestion denoted pair lines joined square. Note
regulators suggestion effect skewing distribution toward bargains
shorter distances l smaller heading changes. regulator assigns
equal weight ATC cockpit utility, whereas NDB distribution skewed toward
outcomes better ATC. result, modal bargain shifts significantly
north.
600

fiPredictive Unstructured Bargaining

6. Conclusion Future Work
laboratory field unstructured bargaining experiments, typically found
one bargain arise. accommodate this, paper consider
maps take unstructured bargaining problem probability distribution
S, rather single element S. approach translate Nashs axioms
unstructured bargaining apply distribution-valued map. this, derive
Nash distributional bargaining concept, maps feasible set joint utility
outcomes power law set. power law nature intriguing due
ubiquity empirical power laws real economy, e.g., wage distributions, city sizes,
etc.
Future work involves trying translate variants Nashs axioms distributional
bargaining concept. example, may prove possible translate weak monotonicity
axiom Kalai-Smorodinsky solution concept (Kalai & Smorodinsky, 1975) probabilistic terms. so, combining probabilistic versions remaining
Kalai-Smorodinsky axioms (which shared axioms Nash bargaining concept), may able produce Kalai-Smorodinsky distributional bargaining concept.
many advantages using distributional bargaining concepts, addition
according experimental fact multiple bargains arise given
game. One advantage concepts seamlessly accommodate feasible sets
convex comprehensive, even finite feasible sets. Another advantage arises
external regulator modify aspects bargaining game
utility function bargaining outcomes. modifying game, regulator
changes associated distribution, therefore modifies value expected
utility. Accordingly, calculate Bayes-optimal modification game.
emphasize nothing formal definition bargaining game
translation Nashs axioms restricts analysis scenarios traditionally
viewed bargaining. particular, assume binding contracts. Therefore
reason assume outcome game Pareto optimal. (Without
binding contract players behavior, player typically incentive
ability change behavior way helps hurts opponent,
thereby moves final outcome Pareto frontier.) reason, distributional
bargaining concepts applied model noncooperative game modeler
knows feasible set games joint utility outcomes, know (or
cannot tractably elaborate) full underlying extensive form game. modeling
noncooperative game behavior limited information, best modeler
provide distribution final joint utility outcome. Intuitively speaking,
distribution amounts average extensive form games associated
feasible set possible joint utilities. Nash distributional bargaining concept provides
way form distribution outcomes, using axioms seem applicable many
scenarios traditionally viewed unstructured bargaining.16
16. subtlety using Nash distributional bargaining concept way needs
outcome noncooperative game reasonably expected behave default
point d. However even restriction dispensed using distributional bargaining
concepts, e.g., using logit one mentioned end Sec. 2.

601

fiWolpert & Bono

illustration, work Smith, Suchanek, Williams (1988) artificial
speculative market run rounds. created bubble burst, leaving
test subjects poorer richer. Predicting moves full extensive form game
subjects engaged rounds laborious, best. alternative, one could
imagine predicting experiments outcome based knowing feasible set, i.e.,
knowing ways money could end divided among subjects end
experiment. Future work involves analyzing Nash distributional bargaining concept
situations.
market experiments could prove useful source data testing alternative
distributional bargaining theories, addition experiments explicitly viewed terms
unstructured bargaining. particular, possible produce Kalai-Smorodinsky
distributional bargaining concept, discussed above, may possible compare
predictions alternative theory NDB concept using data market
experiments. comparative study would analogous types meta-studies
conducted compare non-cooperative solution concepts using experimental data (Wright
& Leyton-Brown, 2010).
discussed above, function plays prominent role making predictions
bargains rather joint utilities. Arguably, knowledge even affect
predictions make joint utility outcomes. example, say set possible
bargains large players cannot expected examine completely,
concentrates vast majority bargains onto single joint utility x, e.g., one
Pareto frontier. case, would seem reasonable ascribe higher probability
x would single bargain mapped it. However traditional approaches
unstructured bargaining ignore predicting joint utilities, relying axioms solely
concerned space joint utilities. part, approach
adopted here. (The section optimal recommendations diverged modeling
effect recommendation.) Future work involves incorporating knowledge
directly predictions joint utility outcomes.

Acknowledgments
would like thank Sylvia Thoron helpful discussion.

References
Agogino, A., & Tumer, K. (2012). multiagent approach managing air traffic flow.
Autonomous Agents Multi-Agent Systems, 24, 125. 10.1007/s10458-010-9142-5.
Ashlagi, I., Monderer, D., & Tennenholtz, M. (2008). value correlation. Journal
Artificial Intelligence Research, 33, 575613.
Aumann, R., & Hart, S. (1992). Handbook Game Theory Economic Applications.
North-Holland Press.
602

fiPredictive Unstructured Bargaining

Aydogan, R., & Yolum, P. (2012). Learning opponents preferences effective negotiation:
approach based concept learning. Autonomous Agents Multi-Agent Systems,
24, 104140. 10.1007/s10458-010-9147-0.
Binmore, K., Swierzbinski, J., Hsu, S., & Proulx, C. (1993). Focal points bargaining.
International Journal Game Theory, 22, 381409. 10.1007/BF01240133.
Brafman, R. I., & Tennenholtz, M. (1996). partially controlled multi-agent systems.
Journal Artificial Intelligence Research, 4, 477507.
Brafman, R. I., & Tennenholtz, M. (2003). Learning coordinate efficiently: modelbased approach. Journal Artificial Intelligence Research, 19, 1123.
Camerer, C. (2003). Behavioral Game Theory: Experiments Strategic Interaction. Princeton University Press.
Chalamish, M., & Kraus, S. (2012). Automed: automated mediator multi-issue
bilateral negotiations. Autonomous Agents Multi-Agent Systems, 24, 536564.
10.1007/s10458-010-9165-y.
Conley, J. P., & Wilkie, S. (1996). extension nash bargaining solution nonconvex
problems. Games Economic Behavior, 13, 2638.
Duan, L., Dogru, M., Ozen, U., & Beck, J. (2012). negotiation framework linked
combinatorial optimization problems. Autonomous Agents Multi-Agent Systems,
25, 158182. 10.1007/s10458-011-9172-7.
Harsanyi, J., & Selten, R. (1972). generalized nash solution two-person bargaining
games incomplete information. Management Science, 18, 80106.
Herrero, M. J. (1989). nash program: Non-convex bargaining problems. Journal
Economic Theory, 49 (2), 266 277.
Jaynes, E. T., & Bretthorst, G. L. (2003). Probability Theory : Logic Science.
Cambridge University Press.
Kalai, E. (1977). Proportional solutions bargaining situations: Interpersonal utility
comparisons. Econometrica, 45 (7), 16231630.
Kalai, E., & Smorodinsky, M. (1975). solutions nashs bargaining problem. Econometrica, 43 (3), 513518.
Kaneko, M. (1980). extension nash bargaining problem nash social welfare
function. Theory Decision, 12, 135148. 10.1007/BF00154358.
Kbrs, O., & Sertel, M. (2007). Bargaining finite set alternatives. Social Choice
Welfare, 28, 421437. 10.1007/s00355-006-0178-z.
Lopez-Carmona, M., Marsa-Maestre, I., Klein, M., & Ito, T. (2012). Addressing stability issues mediated complex contract negotiations constraint-based, nonmonotonic utility spaces. Autonomous Agents Multi-Agent Systems, 24, 485535.
10.1007/s10458-010-9159-9.
603

fiWolpert & Bono

Mariotti, M. (1998a). Extending nashs axioms nonconvex problems. Games Economic Behavior, 22 (2), 377 383.
Mariotti, M. (1998b). Nash bargaining theory number alternatives finite.
Social Choice Welfare, 15, 413421. 10.1007/s003550050114.
McKelvey, R. D., & Palfrey, T. R. (1995). Quantal response equilibria normal form
games. Games Economic Behavior, 10, 638.
Nash, J. (1950). bargaining problem. Econometrica, 18 (2), 155162.
Nydegger, R. V., & Owen, G. (1974). Two-person bargaining: experimental
test nash axioms. International Journal Game Theory, 3, 239249.
10.1007/BF01766877.
Osborne, M., & Rubinstein, A. (1994). Course Game Theory. MIT Press, Cambridge,
MA.
Peters, H., & Tijs, S. (1984). Probabilistic bargaining solutions. Operations Research
Proceedings. Springer-Verlag.
Peters, H., & Vermeulen, D. (2010). WPO, COR, IIA bargaining solutions. accepted
International Journal Game Theory.
Rezek, I., Leslie, D. S., Reece, S., Roberts, S. J., Rogers, A., Dash, R. K., & Jennings,
N. R. (2008). similarities inference game theory machine learning.
Journal Artificial Intelligence Research, 33, 259283.
Roth, A. E., & Malouf, M. W. K. (1979). Game-theoretic models role information
bargaining. Psychological Review, 86 (6), 574594.
Rubinstein, A., Safra, Z., & Thomson, W. (1992). interpretation nash bargaining solution extension non-expected utility preferences. Econometrica,
60 (5), 11711186.
Schelling, T. (1960). strategy conflict. Harvard university press.
Smith, V. L., Suchanek, G. L., & Williams, A. W. (1988). Bubbles, crashes, endogenous
expectations experimental spot asset markets. Econometrica, 56 (5), pp. 11191151.
Train, K. E. (2003). Discrete Choice Methods Simulation. Cambridge University Press.
Vorobeychik, Y., Reeves, D. M., & Wellman, M. P. (2012). Constrained automated mechanism design infinite games incomplete information. accepted Journal
Autonomous Agents Multiagent Systems.
Wolpert, D. H., Harre, M., Olbrich, E., Bertschinger, N., & Jost, J. (2012). Hysteresis effects
changing parameters noncooperative games. Physical Review E, 85, 036102. DOI:
10.1103/PhysRevE.85.036102.
604

fiPredictive Unstructured Bargaining

Wolpert, D. H., & Bono, J. W. (2011). Distribution-valued solution concepts. working
paper.
Wright, J. R., & Leyton-Brown, K. (2010). Beyond equilibrium: Predicting human behavior
normal form games. Twenty-Fourth Conference Artificial Intelligence (AAAI10). forthcoming.
Xu, Y., & Yoshihara, N. (2006). Alternative characterizations three bargaining solutions
nonconvex problems. Games Economic Behavior, 57 (1), 86 92.
Zhou, L. (1997). nash bargaining theory non-convex problems. Econometrica,
65 (3), 681685.

605

fiJournal Artificial Intelligence Research 46 (2013) 687-716

Submitted 12/12; published 04/13

NuMVC: Efficient Local Search Algorithm
Minimum Vertex Cover
Shaowei Cai

SHAOWEICAI . CS @ GMAIL . COM

Key Laboratory High Confidence Software Technologies
Peking University, Beijing, China

Kaile Su

K . SU @ GRIFFITH . EDU . AU

Institute Integrated Intelligent Systems
Griffith University, Brisbane, Australia

Chuan Luo

CHUANLUOSABER @ GMAIL . COM

Key Laboratory High Confidence Software Technologies
Peking University, Beijing, China

Abdul Sattar

. SATTAR @ GRIFFITH . EDU . AU

Institute Integrated Intelligent Systems
Griffith University, Brisbane, Australia

Abstract
Minimum Vertex Cover (MVC) problem prominent NP-hard combinatorial
optimization problem great importance theory application. Local search proved
successful problem. However, two main drawbacks state-of-the-art MVC local
search algorithms. First, select pair vertices exchange simultaneously, timeconsuming. Secondly, although using edge weighting techniques diversify search,
algorithms lack mechanisms decreasing weights. address issues, propose two
new strategies: two-stage exchange edge weighting forgetting. two-stage exchange
strategy selects two vertices exchange separately performs exchange two stages.
strategy edge weighting forgetting increases weights uncovered edges, also
decreases weights edge periodically. two strategies used designing
new MVC local search algorithm, referred NuMVC.
conduct extensive experimental studies standard benchmarks, namely DIMACS
BHOSLIB. experiment comparing NuMVC state-of-the-art heuristic algorithms
show NuMVC least competitive nearest competitor namely PLS
DIMACS benchmark, clearly dominates competitors BHOSLIB benchmark. Also,
experimental results indicate NuMVC finds optimal solution much faster current
best exact algorithm Maximum Clique random instances well structured ones.
Moreover, study effectiveness two strategies run-time behaviour
experimental analysis.

1. Introduction
Minimum Vertex Cover (MVC) problem consists of, given undirected graph G = (V, E),
finding minimum sized vertex cover, vertex cover subset V every
edge G least one endpoint S. MVC important combinatorial optimization problem
many real-world applications, network security, scheduling, VLSI design industrial
machine assignment. equivalent two well-known combinatorial optimization problems:
Maximum Independent Set (MIS) problem Maximum Clique (MC) problem,
c
2013
AI Access Foundation. rights reserved.

fiC AI , U , L UO & ATTAR

wide range applications areas information retrieval, experimental design, signal
transmission, computer vision, also bioinformatics problems aligning DNA protein
sequences (Johnson & Trick, 1996). Indeed, three problems seen three different
forms problem, viewpoint practical algorithms. Algorithms MVC
directly used solve MIS MC problems. Due great importance theory
applications, three problems widely investigated last several decades
(Carraghan & Pardalos, 1990; Evans, 1998; Pullan & Hoos, 2006; Richter, Helmert, & Gretton,
2007; Cai, Su, & Chen, 2010; Li & Quan, 2010b; Cai, Su, & Sattar, 2011).
Theoretical analyses indicate three problems MVC, MIS, MC computationally
hard. NP-hard associated decision problems NP-complete (Garey &
Johnson, 1979). Moreover, hard solve approximately. NP-hard approximate
MVC within factor smaller 1.3606 (Dinur & Safra, 2005), although one achieve
approximation ratio 2 o(1) (Halperin, 2002; Karakostas, 2005). Besides inapproximability
MVC, Hastad shows MIS MC approximable within |V |1 > 0,
unless NP=ZPP1 (Hastad, 1999, 2001). Recently, conclusion enhanced MC
approximable within |V |1 > 0 unless NP=P (Zuckerman, 2006), derived
derandomization Hastads result. Moreover, currently best polynomial-time approximation
algorithm MC guaranteed find clique within factor O(n(loglogn)2 /(logn)3 )
optimum (Feige, 2004).
algorithms solve MVC (MIS, MC) fall two types: exact algorithms heuristic
algorithms. Exact methods mainly include branch-and-bound algorithms (Carraghan &
Pardalos, 1990; Fahle, 2002; Ostergard, 2002; Regin, 2003; Tomita & Kameda, 2009; Li &
Quan, 2010b, 2010a), guarantee optimality solutions find, may fail give
solution within reasonable time large instances. Heuristic algorithms, mainly include local
search algorithms, cannot guarantee optimality solutions, find optimal
satisfactory near-optimal solutions large hard instances within reasonable time. Therefore,
appealing use local search algorithms solve large hard MVC (MC, MIS) instances.
Early heuristic methods Maximum Clique designed initial responses
Second DIMACS Implementation Challenge (Johnson & Trick, 1996), Maximum Clique
one three challenge problems. that, huge amount effort devoted designing
local search algorithms MVC, MC MIS problems (Aggarwal, Orlin, & Tai, 1997; Battiti &
Protasi, 2001; Busygin, Butenko, & Pardalos, 2002; Shyu, Yin, & Lin, 2004; Barbosa & Campos,
2004; Pullan, 2006; Richter et al., 2007; Andrade, Resende, & Werneck, 2008; Cai et al., 2010,
2011). review heuristic algorithms three problems found recent paper
MVC local search (Cai et al., 2011).
work devoted efficient local search algorithm MVC. Typically, local search
algorithms MVC solve problem iteratively solving k-vertex cover problem. solve
k-vertex cover problem, maintain current candidate solution size k, exchange
two vertices iteratively becomes vertex cover. However, observe two drawbacks
state-of-the-art MVC local search algorithms. First, select pair vertices exchanging
simultaneously according heuristic (Richter et al., 2007; Cai et al., 2010, 2011),
rather time-consuming, explained Section 3. second drawback edge
weighting techniques. basic concept edge weighting increase weights uncovered
1. ZPP class problems solved expected polynomial time probabilistic algorithm zero
error probability.

688

fiN U MVC: N E FFICIENT L OCAL EARCH LGORITHM INIMUM V ERTEX C

edges diversify search. Previous MVC local search algorithms utilize different edge weighting
schemes. example, COVER (Richter et al., 2007) increases weights uncovered edges
step, EWLS (Cai et al., 2010) EWCC (Cai et al., 2011) increase weights uncovered
edges reaching local optima. However, algorithms mechanism
decrease weights. believe deficient weighting decisions made long
ago may mislead search.
address two issues MVC local search algorithms, paper proposes two new
strategies, namely two-stage exchange edge weighting forgetting. two-stage exchange
strategy decomposes exchanging procedure two stages, i.e., removing stage
adding stage, performs separately. first selects vertex removes current
candidate solution, selects vertex random uncovered edge adds it. twostage exchange strategy yields efficient two-pass move operator MVC local search,
first pass linear-time search vertex-to-remove, second pass lineartime search vertex-to-add. contrast standard quadratic, all-at-once move
operator. Moreover, two-stage exchange strategy renders algorithm flexible
employ different heuristics different stages. Indeed, NuMVC algorithm utilizes
highly greedy heuristic removing stage, adding stage, makes good use
diversifying heuristic within framework similar focused random walk (Papadimitriou, 1991).
second strategy propose edge weighting forgetting. increases weights
uncovered edges one step. Moreover, averaged edge weight achieves
threshold, reduces weights edges multiplying constant factor (0 < < 1) forget
earlier weighting decisions. best knowledge, first time forgetting
mechanism introduced local search algorithms MVC.
two strategies combined design new local search algorithm called NuMVC.
carry detailed experimental study investigate performance NuMVC, compare
PLS (Pullan, 2006), COVER (Richter et al., 2007) EWCC (Cai et al., 2011),
leading heuristic algorithms MVC (MC, MIS). Experimental results show NuMVC
competes well solvers DIMACS benchmark, shows dramatic improvement
existing results whole BHOSLIB benchmark. parts work published
early version paper (Cai, Su, & Sattar, 2012).
paper, additionally carry experimental analyses provides
insights two strategies NuMVC. compare NuMVC exact algorithm
MaxCLQdyn+EFL+SCR (Li & Quan, 2010a), best exact Maximum Clique algorithm
found literature. Experimental results indicate NuMVC finds optimal solution
much faster exact algorithm random instances well structured ones.
importantly, conduct experimental investigations study run-time behaviour NuMVC
effectiveness two new strategies NuMVC.
remainder paper organized follows. next section, introduce
definitions notations used paper. present two strategies: two-stage exchange
edge weighting forgetting. Section 5, describe NuMVC algorithm. Section 6
presents experimental study NuMVC comparative results algorithms, including
heuristic exact algorithms. followed detailed investigations run-time
behaviour NuMVC effectiveness two new strategies Section 7. Finally,
conclude paper summarizing main contributions future directions.
689

fiC AI , U , L UO & ATTAR

2. Preliminaries
undirected graph G = (V, E) consists vertex set V edge set E V V ,
edge 2-element subset V . edge e = {u, v}, say vertices u v
endpoints edge e. Two vertices neighbors belong common
edge. denote N (v) = {u V |{u, v} E}, set neighbors vertex v.
undirected graph G = (V, E), independent set subset V pairwise nonadjacent elements clique subset V pairwise adjacent elements. maximum
independent set maximum clique problems find maximum sized independent set
clique graph, respectively.
note three problems MVC, MIS MC seen three different forms
problem, viewpoint experimental algorithms. vertex set independent
set G V \S vertex cover G; vertex set K clique G V \K
vertex cover complementary graph G. find maximum independent set graph
G, one find minimum vertex cover Cmin G return V \Cmin . Similarly, find

maximum clique graph G, one find minimum vertex cover Cmin
complementary

graph G, return V \Cmin
.
Given undirected graph G = (V, E), candidate solution MVC subset vertices.
edge e E covered candidate solution X least one endpoint e belongs
X. search procedure, NuMVC always maintains current candidate solution.
convenience, rest paper, use C denote current candidate solution. state
vertex v denoted sv {1, 0}, sv = 1 means v C, sv = 0 means v
/ C.
step neighboring candidate solution consists exchanging two vertices: vertex u C
removed C, vertex v
/ C put C. age vertex number steps since
state last changed.
state-of-the-art MVC local search algorithms, NuMVC utilizes edge weighting
scheme. edge weighting local search, follow definitions notations EWCC (Cai
et al., 2011). edge weighted undirected graph undirected graph G = (V, E) combined
weighting function w edge e E associated non-negative integer number
w(e) weight. use w denote mean value edge weights.
Let w weighting function G. candidate solution X, set cost X
X

cost(G, X) =

w(e)

eE e covered X

indicates total weight edges uncovered X. take cost(G, X) evaluation
f unction, NuMVC prefers candidate solutions lower costs.
vertex v V ,
dscore(v) = cost(G, C) cost(G, C )
C = C\{v} v C, C = C {v} otherwise, measuring benefit changing
state vertex v. Obviously, vertex v C, dscore(v) 0, greater dscore
indicates less loss covered edges removing C. vertex v
/ C,
dscore(v) 0, higher dscore indicates greater increment covered edges adding
C.
690

fiN U MVC: N E FFICIENT L OCAL EARCH LGORITHM INIMUM V ERTEX C

3. Two-Stage Exchange
section, introduce two-stage exchange strategy, adopted NuMVC
algorithm exchange pair vertices.
state-of-the-art MVC local search algorithms, NuMVC iterated k-vertex
cover algorithm. finding k-vertex cover, NuMVC removes one vertex current
candidate solution C goes search (k 1)-vertex cover. sense, core
NuMVC k-vertex cover algorithm given positive integer number k, searching k-sized
vertex cover. find k-vertex cover, NuMVC begins candidate solution C size k,
exchanges two vertices iteratively C becomes vertex cover.
local search algorithms MVC select pair vertices exchange simultaneously
according certain heuristic. example, COVER selects pair vertices maximize
gain(u, v) (Richter et al., 2007), EWLS (Cai et al., 2010) EWCC (Cai et al., 2011) select
random pair vertices score(u, v) > 0. strategy selecting two vertices exchange
simultaneously leads quadratic neighborhood candidate solutions. Moreover, evaluation
pair vertices depends evaluations (such dscore) two vertices, also
involves relationship two vertices, like belong edge. Therefore,
rather time-consuming evaluate candidate pairs vertices.
contrast earlier MVC local search algorithms, NuMVC selects two vertices
exchanging separately exchanges two selected vertices two stages. iteration,
NuMVC first selects vertex u C highest dscore removes it. that, NuMVC
selects uniformly random uncovered edge e, chooses one endpoint v e higher
dscore restrictions adds C. Note two-stage exchange strategy
resembles respect min-conflicts hill-climbing heuristic CSP (Minton, Johnston,
Philips, & Laird, 1992), shows surprisingly good performance N-queens problem.
Selecting two vertices exchanging separately may cases miss greedier
vertex pairs consist two neighboring vertices. However, usual local search
algorithms, trade-off accuracy heuristics complexity per step.
Let R denote set candidate vertices removing adding separately. time
complexity per step selecting exchanging vertex pair simultaneously |R| |A|;
complexity per step selecting two vertices separately, NuMVC, |R| + |A|.
worthy note that, heuristics local search algorithm often based intuition
experience rather theoretically empirically derived principles insights, cannot say
certain less greedy good thing (Hoos & Stutzle, 2004). hand,
lower time complexity always desirable.

4. Edge Weighting Forgetting
section, present new edge weighting technique called edge weighting forgetting,
plays important role NuMVC.
proposed strategy edge weighting forgetting works follows. edge
associated positive integer number weight, edge weight initialized one.
iteration, edge weights uncovered edges increased one. Moreover,
average weight achieves threshold, edge weights reduced forget earlier weighting
decisions using formula w(e) := w(e), constant factor 0 1.
691

fiC AI , U , L UO & ATTAR

Note edge weighting techniques MVC local search, including one work,
fall general penalty idea optimization problems, dates back Morris
breakout method (Morris, 1993) widely used local search algorithms constraint
optimization problems SAT (Yugami, Ohta, & Hara, 1994; Wu & Wah, 2000; Schuurmans,
Southey, & Holte, 2001; Hutter, Tompkins, & Hoos, 2002). results therefore provide
evidence effectiveness general applicability algorithmic technique.
Edge weighting techniques successfully used improve MVC local search
algorithms. example, COVER (Richter et al., 2007) updates edge weights step,
EWLS (Cai et al., 2010) EWCC (Cai et al., 2011) update edge weights reaching local
optima. However, previous edge weighting techniques mechanism decrease
weights, limits effectiveness. strategy edge weighting forgetting
work introduces forgetting mechanism reduce edge weights periodically, contributes
considerably NuMVC algorithm.
intuition behind forgetting mechanism weighting decisions made long ago
longer helpful may mislead search, hence considered less important
recent ones. example, consider two edges e1 e2 w(e1 ) = 1000 w(e2 ) =
100 step. use w(e) denote increase w(e). According evaluation
function, next period time, algorithm likely cover e1 frequently e2 ,
may assume period w(e1 ) = 50 w(e2 ) = 500, makes w(e1 ) =
1000 + 50 = 1050 w(e2 ) = 100 + 500 = 600. Without forgetting mechanism, algorithm
would still prefer e1 e2 covered future search. reasonable,
period e2 covered much fewer steps e1 is. Thus, e2 take priority covered
sake diversification. let us consider case forgetting mechanism (assuming
= 0.3 setting experiments). Suppose w(e1 ) = 1000 w(e2 ) = 100
algorithm performs forgetting. forgetting mechanism reduces edge weights
w(e1 ) = 10000.3 = 300 w(e2 ) = 1000.3 = 30. period time, w(e1 ) = 50
w(e2 ) = 500, w(e1 ) = 300 + 50 = 350 w(e2 ) = 30 + 500 = 530. case,
algorithm prefers cover e2 rather cover e1 future search, expect.
Although inspired smoothing techniques clause weighting local search algorithms
SAT, forgetting mechanism NuMVC differs smoothing techniques SAT
local search algorithms. According way clause weights smoothed, three
main smoothing techniques clause weighting local search algorithms SAT best
knowledge: first pull clause weights mean value using formula wi :=
wi + (1 ) w, ESG (Schuurmans et al., 2001), SAPS (Hutter et al., 2002) Swcca
(Cai & Su, 2012); second subtract one clause weights greater
one, DLM (Wu & Wah, 2000) PAWS (Thornton, Pham, Bain, & Jr., 2004); last
employed DDWF (Ishtaiwi, Thornton, Sattar, & Pham, 2005), transfers weights
neighbouring satisfied clauses unsatisfied ones. obvious forgetting mechanism
NuMVC different smoothing techniques.
Recently, forgetting mechanism proposed vertex weighting technique significant
MC local search algorithm DLS-MC (Pullan & Hoos, 2006), important sub-algorithm
PLS (Pullan, 2006) CLS (Pullan, Mascia, & Brunato, 2011). DLS-MC algorithm employs
vertex weighting scheme increases weights vertices (by one) current clique
reaching local optimum, periodically decreases weights (by one) vertices
currently penalty. Specifically, utilizes parameter pd (penalty delay) specify
692

fiN U MVC: N E FFICIENT L OCAL EARCH LGORITHM INIMUM V ERTEX C

number penalty increase iterations must occur algorithm performs forgetting
operation. However, Pullan Hoos also observed DLS-MC sensitive pd
parameter, optimal value pd varies considerably among different instances. Indeed,
performance DLS-MC given optimizing pd parameter. contrast, forgetting
mechanism NuMVC much less sensitive parameters (as shown Section 7.4),
thus robust.
also notice formula used forgetting mechanism NuMVC adopted
long-term frequency-based learning mechanisms tabu search (Taillard, 1994). However,
Taillars algorithm, parameter (using term work) always greater one,
formula used penalizing move rather forgetting penalties.

5. NuMVC Algorithm
section, present NuMVC algorithm, utilizes strategies two-stage
exchange edge weighting forgetting.
Algorithm 1: NuMVC
1

2
3
4
5
6
7
8
9
10
11
12

13
14
15

16
17
18
19
20

NuMVC (G,cutoff)
Input: graph G = (V, E), cutoff time
Output: vertex cover G
begin
initialize edge weights dscores vertices;
initialize confChange array all-1 array;
construct C greedily vertex cover;
C := C;
elapsed time < cutoff
uncovered edge
C := C;
remove vertex highest dscore C;
continue;
choose vertex u C highest dscore, breaking ties favor oldest
one;
C := C\{u}, confChange(u) := 0 confChange(z) := 1 z N (u);
choose uncovered edge e randomly;
choose vertex v e confChange(v) = 1 higher dscore, breaking ties
favor older one;
C := C {v}, confChange(z) := 1 z N (v);
w(e) := w(e) + 1 uncovered edge e;
w w(e) := w(e) edge e;
return C ;
end

better understanding algorithm, first describe strategy called configuration checking
(CC), used NuMVC. CC strategy (Cai et al., 2011) proposed handling
693

fiC AI , U , L UO & ATTAR

cycling problem local search, i.e., revisiting candidate solution visited recently
(Michiels, Aarts, & Korst, 2007). strategy successfully applied local search
algorithms MVC (Cai et al., 2011) well SAT (Cai & Su, 2011, 2012).
CC strategy NuMVC works follows: vertex v
/ C, neighboring vertices
never change states since last time v removed C, v added
back C. CC strategy seen prohibition mechanism, shares spirit
differs well-known prohibition mechanism called tabu (Glover, 1989).
implementation CC strategy maintain Boolean array confChange vertices.
search procedure, vertices confChange value 0 forbidden
add C. confChange array initialized all-1 array. that, vertex v
removed C, confChange(v) reset 0, vertex v changes state,
z N (v), confChange(z) set 1.
outline NuMVC algorithm Algorithm 1, described below. beginning, edge
weights initialized 1, dscores vertices computed accordingly; confChange(v)
initialized 1 vertex v; current candidate solution C constructed iteratively
adding vertex highest dscore (ties broken randomly), becomes vertex
cover. Finally, best solution C initialized C.
initialization, loop (lines 7-18) executed given cutoff time reached.
search procedure, uncovered edge, means C vertex cover,
NuMVC updates best solution C C (line 9). removes one vertex highest
dscore C (line 10), breaking ties randomly, go search vertex cover
size |C| = |C | 1. note that, C, vertex highest dscore minimum
absolute value dscore since dscores negative.
iteration loop, NuMVC swaps two vertices according strategy two-stage
exchange (lines 12-16). Specifically, first selects vertex u C highest dscore
remove, breaking ties favor oldest one. removing u, NuMVC chooses uncovered
edge e uniformly random, selects one es endpoints add C follows:
one endpoint whose confChange 1, vertex selected; confChange values
endpoints 1, NuMVC selects vertex higher dscore, breaking ties
favor older one. exchange finished adding selected vertex C. Along
exchanging two selected vertices, confChange array updated accordingly.
end iteration, NuMVC updates edge weights (lines 17-18). First, weights
uncovered edges increased one. Moreover, NuMVC utilizes forgetting mechanism
decrease weights periodically. detail, averaged weight edges achieves threshold
, edge weights multiplied constant factor (0 < < 1) rounded
integer edge weights defined integers NuMVC. forgetting mechanism forgets
earlier weighting decision extent, past effects generally longer helpful
may mislead search.
conclude section following observation, guarantees executability
line 15.
Proposition 1. uncovered edge e, least one endpoint v edge e
confChange(v) = 1.
Proof: Let us consider arbitrary uncovered edge e = {v1 , v2 }. proof includes two cases.
(a) least one v1 v2 never changes state initialization. Without
694

fiN U MVC: N E FFICIENT L OCAL EARCH LGORITHM INIMUM V ERTEX C

loss generality, assume v1 vertex. initialization, confChange(v1 ) set
1. that, removing v1 C (which corresponds vs state sv changing 0
1) make confChange(v1 ) 0, v1 never changes state initialization,
confChange(v1 )= 1.
(b) v1 v2 change states initialization. e uncovered, v1
/ C
v2
/ C. Without loss generality, assume last removing v1 happens last
removing v2 . last time v1 removed, v2 C holds. Afterwards, v2 removed,
means v2 changes state, confChange(v1 ) set 1 v1 N (v2 ).

6. Empirical Results
section, present detailed experimental study evaluate performance NuMVC
standard benchmarks literature, i.e., DIMACS BHOSLIB benchmarks. first
introduce DIMACS BHOSLIB benchmarks, describe preliminaries
experiments. Then, divide experiments three parts. purpose first part
demonstrate performance NuMVC detail. second compare NuMVC state-ofthe-art heuristic algorithms. Finally, last part compare NuMVC state-of-the-art exact
algorithms.
6.1 Benchmarks
good set benchmarks fundamental demonstrate effectiveness new solvers.
use two standard benchmarks MVC (MIS, MC) research, DIMACS benchmark
BHOSLIB benchmark. DIMACS benchmark includes instances industry
generated various models, BHOSLIB instances random ones high difficulty.
6.1.1 DIMACS B ENCHMARK
DIMACS benchmark taken Second DIMACS Implementation Challenge
Maximum Clique problem (1992-1993)2 . Thirty seven graphs selected organizers
summary indicate effectiveness algorithms, comprising Second DIMACS Challenge
Test Problems. instances generated real world problems coding theory,
fault diagnosis, Kellers conjecture Steiner Triple Problem, etc, random graphs
various models, brock p hat families. instances range size less
50 vertices 1,000 edges greater 4,000 vertices 5,000,000 edges. Although
proposed two decades ago, DIMACS benchmark remains popular benchmark
widely used evaluating heuristic algorithms MVC (Richter et al., 2007; Pullan, 2009;
Cai et al., 2011; Gajurel & Bielefeld, 2012), MIS (Andrade et al., 2008; Pullan, 2009) MC
algorithms (Pullan, 2006; Katayama, Sadamatsu, & Narihisa, 2007; Grosso, Locatelli, & Pullan,
2008; Pullan et al., 2011; Wu, Hao, & Glover, 2012). particular, DIMACS benchmark
used evaluating COVER EWCC. convenient us use benchmark also
conduct experiments comparing NuMVC COVER EWCC. Note DIMACS
graphs originally designed Maximum Clique problem, MVC algorithms tested
complementary graphs.
2. ftp://dimacs.rutgers.edu/pub/challenges

695

fiC AI , U , L UO & ATTAR

6.1.2 BHOSLIB B ENCHMARK
BHOSLIB3 (Benchmarks Hidden Optimum Solutions) instances generated randomly
phase transition area according model RB (Xu, Boussemart, Hemery, & Lecoutre,
2005). Generally, phase-transition instances generated model RB proved
hard theoretically (Xu & Li, 2006) practically (Xu & Li, 2000; Xu, Boussemart,
Hemery, & Lecoutre, 2007). SAT version BHOSLIB benchmark extensively used
SAT competitions4 . Nevertheless, SAT solvers much weaker MVC solvers
problems, remains justifiable referring results SAT Competition 2011
benchmark. BHOSLIB benchmark famous hardness influential enough
strongly recommended MVC (MC, MIS) community (Grosso et al., 2008; Cai et al., 2011).
widely used recent literature reference point new local search solvers
MVC, MC MIS5 . Besides 40 instances, large instance frb100-40 4,000
vertices 572,774 edges, designed challenging MVC (MC, MIS) algorithms.
BHOSLIB benchmark designed MC, MVC MIS, graphs
benchmark expressed two formats, i.e., clq format mis format. BHOSLIB
instance, graph clq format one mis format complementary other. MC
algorithms tested graphs clq format, MVC MIS algorithms tested
mis format.
6.2 Experiment Preliminaries
discuss experimental results, let us introduce preliminary information
experiments.
NuMVC implemented C++. codes NuMVC EWCC publicly available
first authors homepage6 . codes COVER downloaded online7 , PLS
kindly provided authors. four solvers compiled g++ -O2 option.
experiments carried machine 3 GHz Intel Core 2 Duo CPU E8400 4GB
RAM Linux. execute DIMACS machine benchmarks8 , machine requires 0.19
CPU seconds r300.5, 1.12 CPU seconds r400.5 4.24 CPU seconds r500.5.
NuMVC, set = 0.5|V | = 0.3 runs, except challenging instance
frb100-40, = 5000 = 0.3. Note also parameters state-ofthe-art MVC (MC, MIS) algorithms, DLS-MC (Pullan & Hoos, 2006) EWLS (Cai et al.,
2010). Moreover, parameters DLS-MC EWLS vary considerably different instances.
instance, algorithm performed 100 independent runs different random seeds,
run terminated upon reaching given cutoff time. cutoff time set 2000
seconds instances except challenging instance frb100-40, cutoff
time set 4000 seconds due significant hardness.
NuMVC, report following information instance:
optimal (or minimum known) vertex cover size (V C ).
3.
4.
5.
6.
7.
8.

http://www.nlsde.buaa.edu.cn/kexu/benchmarks/graph-benchmarks.htm
http://www.satcompetition.org
http://www.nlsde.buaa.edu.cn/kexu/benchmarks/list-graph-papers.htm
http://www.shaoweicai.net/research.html
http://www.informatik.uni-freiburg.de/srichter/
ftp://dimacs.rutgers.edu/pub/dsj/clique/

696

fiN U MVC: N E FFICIENT L OCAL EARCH LGORITHM INIMUM V ERTEX C

number successful runs (suc). run said successful solution size V C
found.
VC size shows min (average, max) vertex cover size found NuMVC
100 runs.
averaged run time 100 runs (time). run time successful run
time find V C solution, failed run considered cutoff time.
instances NuMVC achieve 100% success rate, also report averaged
run time successful runs (suc time). run time measured CPU seconds.
inter-quartile range (IQR) run time 100 runs. IQR difference
75th percentile 25th percentile sample. IQR one famous robust
measures data analysis (Hoaglin, Mosteller, & Tukey, 2000), recommended
measurement closeness sampling distribution community experimental
algorithms (Bartz-Beielstein, Chiarandini, Paquete, & Preuss, 2010).
number steps averaged 100 runs (steps). steps successful run
needed find V C solution, steps failed run executed
running cut off. instances NuMVC achieve 100% success
rate, also report averaged steps successful runs (suc steps).
successful runs instance, time steps columns marked
n/a. success rate solver instance less 75%, 75th percentile
run time sample cutoff time represent real 75th percentile. case,
report IQR, instead mark n/a corresponding column. Actually,
success rate solver certain instance less 75%, solver considered
robust instance given cutoff time.
6.3 Performance NuMVC
section, report detailed performance NuMVC two benchmarks.
6.3.1 P ERFORMANCE N U MVC



DIMACS B ENCHMARK

performance results NuMVC DIMACS benchmark displayed Table 1. NuMVC
finds optimal (or best known) solutions 35 37 DIMACS instances. Note 2 failed
instances brock graphs. Furthermore, among 35 successful instances, NuMVC
consistently (i.e., 100 runs) 32 instances, 24 solved within 1 second.
Overall, NuMVC algorithm exhibits excellent performance DIMACS benchmark except
brock graphs. Remark brock graphs artificially designed defeat greedy
heuristics explicitly incorporating low-degree vertices optimal vertex cover. Indeed,
algorithms preferring higher-degree vertices GRASP, RLS, k-opt, COVER EWCC
also failed graphs.
6.3.2 P ERFORMANCE N U MVC



BHOSLIB B ENCHMARK

Table 2, illustrate performance NuMVC BHOSLIB benchmark. NuMVC
successfully solves BHOSLIB instances terms finding optimal solution, size
697

fiC AI , U , L UO & ATTAR

Graph
Instance Vertices
brock200 2
brock200 4
brock400 2
brock400 4
brock800 2
brock800 4
C125.9
C250.9
C500.9
C1000.9
C2000.5
C2000.9
C4000.5
DSJC500.5
DSJC1000.5
gen200 p0.9 44
gen200 p0.9 55
gen400 p0.9 55
gen400 p0.9 65
gen400 p0.9 75
hamming8-4
hamming10-4
keller4
keller5
keller6
MANN a27
MANN a45
MANN a81
p hat300-1
p hat300-2
p hat300-3
p hat700-1
p hat700-2
p hat700-3
p hat1500-1
p hat1500-2
p hat1500-3

200
200
400
400
800
800
125
250
500
1000
2000
2000
4000
500
1000
200
200
400
400
400
256
1024
171
776
3361
378
1035
3321
300
300
300
700
700
700
1500
1500
1500

V C

suc

VC size

188

100
100
96
100
0
0
100
100
100
100
100
1
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
27
100
100
100
100
100
100
100
100
100

188
183
371(371.16,375)
367
779
779
91
206
443
932
1984
1920(1921.29,1922)
3982
487
985
156
145
345
335
325
240
984
160
749
3302
252
690
2221(2221.94,2223)
292
275
264
689
656
638
1488
1435
1406

183
371
367
776
774
91
206
443
932
1984
1920
3982
487
985
156
145
345
335
325
240
984
160
749
3302
252
690
2221
292
275
264
689
656
638
1488
1435
1406

NuMVC
time(suc time)
0.126
1.259
572.390(512.906)
4.981
n/a
n/a
< 0.001
< 0.001

0.128
2.020
2.935
1994.561(1393.303)
252.807
0.012
0.615
< 0.001
< 0.001

0.035
< 0.001
< 0.001
< 0.001

0.062
< 0.001

0.038
2.51
< 0.001

86.362
1657.880(732.897)
0.003
< 0.001

0.001
0.011
0.006
0.008
3.751
0.071
0.060

steps(suc steps)
137610
1705766
645631471(585032783)
6322882
n/a
n/a
136
3256
133595
1154155
231778
777848959(564895994)
7802785
3800
134796
1695
69
38398
1522
203
1
23853
42
15269
384026
6651
90642150
571607432(251509010)
100
98
1863
1248
1103
2868
445830
5280
10668

Table 1: NuMVC performance results, averaged 100 independent runs, DIMACS
benchmark instances. VC column marked asterisk means minimum
known vertex cover size proved optimal.

698

fiN U MVC: N E FFICIENT L OCAL EARCH LGORITHM INIMUM V ERTEX C

worst solution finds never exceeds V C + 1. NuMVC finds optimal solutions 100%
success rate 33 40 instances, averaged success rate remaining 7
instances 82.57%. results dramatically better existing results literature
benchmark. Also, NuMVC finds sub-optimal solution size V C + 1 BSHOSLIB
instances quickly, always less 30 seconds. indicates NuMVC used
approximate MVC problem efficiently even limited time.
Besides 40 BHOSLIB instances Table 2, challenging instance frb100-40,
hidden minimum vertex cover size 3900. designer BHOSLIB benchmark
conjectured instance solved PC less day within next two
decades9 . latest record challenging instance 3902-sized vertex cover found
EWLS, also EWCC.
run NuMVC 100 independent trials within 4000 seconds frb100-40, = 5000
= 0.3 (this parameter setting yields best performance among combinations
= 2000, 3000, ..., 6000 = 0.1, 0.2, ..., 0.5). Among 100 runs, 4 runs find 3902-sized
solution averaged time 2955 seconds, 93 runs find 3903-sized solution (including
3902-sized) averaged time 1473 seconds. Also, interesting note NuMVC
locate rather good approximate solution hard instance quickly: size vertex
covers NuMVC finds within 100 seconds 3903 3905.
Generally, finding (k+1)-vertex cover much easier k-vertex cover. Hence,
NuMVC, well MVC local search algorithms also solve MVC problem
solving k-vertex cover problem iteratively, majority running time used finding
best vertex cover C (of run), trying, without success, find vertex cover size
(|C | 1).
6.4 Comparison Heuristic Algorithms
recent literature five leading heuristic algorithms MVC (MC, MIS), including
three MVC algorithms COVER (Richter et al., 2007), EWLS (Cai et al., 2010) EWCC (Cai
et al., 2011), two MC algorithms DLS-MC (Pullan & Hoos, 2006) PLS (Pullan, 2006).
Note EWCC PLS improved versions EWLS DLS-MC respectively, show
better performance original versions DIMACS BHOSLIB benchmarks. Therefore,
compare NuMVC PLS, COVER EWCC.
comparing NuMVC heuristic algorithms, report V C , suc, time well
IQR. averaged run time successful runs (suc time) cannot indicate comparative
performance algorithms correctly unless evaluated algorithms close success rates,
f (100suc)
calculated time100cutof
, report statistics. results
suc
bold indicate best performance instance.
6.4.1 C OMPARATIVE R ESULTS



DIMACS B ENCHMARK

comparative results DIMACS benchmark shown Table 3. DIMACS instances
easy solved solvers 100% success rate within 2 seconds, thus
reported table. Actually, fact DIMACS benchmark reduced 11
useful instances really emphasizes need make new benchmark.
9. http://www.nlsde.buaa.edu.cn/kexu/benchmarks/graph-benchmarks.htm

699

fiC AI , U , L UO & ATTAR

Graph
Instance Vertices
frb30-15-1
frb30-15-2
frb30-15-3
frb30-15-4
frb30-15-5
frb35-17-1
frb35-17-2
frb35-17-3
frb35-17-4
frb35-17-5
frb40-19-1
frb40-19-2
frb40-19-3
frb40-19-4
frb40-19-5
frb45-21-1
frb45-21-2
frb45-21-3
frb45-21-4
frb45-21-5
frb50-23-1
frb50-23-2
frb50-23-3
frb50-23-4
frb50-23-5
frb53-24-1
frb53-24-2
frb53-24-3
frb53-24-4
frb53-24-5
frb56-25-1
frb56-25-2
frb56-25-3
frb56-25-4
frb56-25-5
frb59-26-1
frb59-26-2
frb59-26-3
frb59-26-4
frb59-26-5

450
450
450
450
450
595
595
595
595
595
760
760
760
760
760
945
945
945
945
945
1150
1150
1150
1150
1150
1272
1272
1272
1272
1272
1400
1400
1400
1400
1400
1534
1534
1534
1534
1534

V C
420
420
420
420
420
560
560
560
560
560
720
720
720
720
720
900
900
900
900
900
1100
1100
1100
1100
1100
1219
1219
1219
1219
1219
1344
1344
1344
1344
1344
1475
1475
1475
1475
1475

suc

VC size

NuMVC
time (suc time)

steps (suc steps)

100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
100
95
100
100
86
100
100
100
100
100
97
100
100
100
88
38
96
79
100

420
420
420
420
420
560
560
560
560
560
720
720
720
720
720
900
900
900
900
900
1100
1100
1100(1100.05,1101)
1100
1100
1219(1219.14,1220)
1219
1219
1219
1219
1344
1344(1344.03,1345)
1344
1344
1344
1475(1475.12,1476)
1475(1475.62,1476)
1475(1475.04,1476)
1475(1475.21,1476)
1475

0.045
0.053
0.191
0.049
0.118
0.515
0.447
0.178
0.563
0.298
0.242
4.083
1.076
2.757
10.141
2.708
4.727
13.777
3.973
10.661
38.143
176.589
606.165(532.805)
7.89
19.529
895.006(715.123)
205.352
51.227
266.871
39.893
470.682
658.961(617.485)
121.298
49.446
26.761
843.304(687.845)
1677.801(1160.020)
644.831(580.032)
1004.550(741.208)
61.907

37963
44632
173708
41189
105468
386287
334255
129279
422638
218800
208115
3679770
959874
2473081
9142719
2029588
3605881
10447444
3000680
8059236
24628019
113569606
386342329(343518242)
5092072
12690957
514619149(416394360)
117980833
29376406
152982736
22817023
259903023
350048132(326853745)
67043078
26030031
14109165
440874471(350993718)
875964146(592010913)
325417225(295226277)
517521634(375976753)
31682895

Table 2: NuMVC performance results, averaged 100 independent runs, BHOSLIB
benchmark instances. BHOSLIB instances hidden optimal vertex cover,
whose size shown VC column.

indicated Table 3, NuMVC outperforms COVER EWCC instances,
competitive complementary PLS. eight hard instances least one
solver fails achieve 100% success rate, PLS dominates brock graphs NuMVC
dominates others, including two putatively hardest instances C2000.9 MANN a81
(Richter et al., 2007; Grosso et al., 2008; Cai et al., 2011), well keller6 MANN a45.
700

fiN U MVC: N E FFICIENT L OCAL EARCH LGORITHM INIMUM V ERTEX C

Graph
Instance

V C

suc

PLS
time (IQR)

suc

brock400 2
brock400 4
brock800 2
brock800 4
C2000.9
C4000.5
gen400 p0.9 55
keller6
MANN a45
MANN a81
p hat1500-1

371
367
776
774
1920
3982
345
3302
690
2221
1488

100
100
100
100
0
100
100
92
1
0
100

0.15 (0.16)
0.03 (0.03)
3.89 (3.88)
1.31 (1.52)
n/a
67 (59)
15.17 (17)
559 (515)
1990 (n/a)
n/a
2.36 (3.07)

3
82
0
0
0
100
100
100
94
1
100

COVER
time (IQR)
1947 (n/a)
960 (988)
n/a
n/a
n/a
658 (290)
0.35 (0.1)
68 (6)
714 (774)
1995 (n/a)
18.10 (17.23)

suc

EWCC
time (IQR)

suc

NuMVC
time (IQR)

20
100
0
0
0
100
100
100
88
1
100

1778 (n/a)
25.38 (25.96)
n/a
n/a
n/a
739 (903)
0.05 (0.04)
3.76 (3.57)
763 (766)
1986 (n/a)
9.79 (9.77)

96
100
0
0
1
100
100
100
100
27
100

572 (646)
4.98 (6.14)
n/a
n/a
1994 (n/a)
252 (97)
0.03 (0.01)
2.51 (0.76)
86 (95)
1657 (n/a)
3.75 (3.19)

Table 3: Comparison NuMVC state-of-the-art heuristic algorithms DIMACS
benchmark. VC column marked asterisk means minimum known
vertex cover size proved optimal.

C2000.9, NuMVC finds 1920-sized solution, also finds 1921-sized solution
70 runs, number 31, 6 32 PLS, COVER, EWCC respectively. Note
PLS performs well brock family comprises three sub-algorithms, one
favors lower degree vertices.
Table 3 indicates C2000.9 MANN a81 remain difficult modern algorithms,
none algorithms solve good success rate reasonable time.
hand, instances solved quickly (in less 100 seconds) least one algorithm, PLS
NuMVC, low IQR value (always less 100), indicates quite stable performance.
6.4.2 C OMPARATIVE R ESULTS



BHOSLIB B ENCHMARK

Table 4, present comparative results BHOSLIB benchmark. concentrating
considerable gaps comparisons, report results two groups small instances
(frb30 frb35), solved within several seconds solvers.
results Table 4 illustrate NuMVC significantly outperforms algorithms
BHOSLIB instances, terms success rate averaged run time, also
demonstrated Figure 1. take look comparison NuMVC EWCC,
EWCC performs obviously better PLS COVER benchmark. NuMVC solves 33
instances 40 100% success rate, 4 instances EWCC does. instances
solved algorithms 100% success rate, overall averaged run time 25 seconds
NuMVC 74 seconds EWCC. instances, averaged success rate 90%
NuMVC, compared 50% EWCC.
excellent performance NuMVC underlined large gaps NuMVC
solvers hard instances. example, instances solvers fail
find optimal solution 100% success rate, NuMVC achieves overall averaged success
rate 82.57%, dramatically better PLS, COVER EWCC, 0.85%,
17.43% 35.71% respectively. Obviously, experimental results show NuMVC delivers
701

fiC AI , U , L UO & ATTAR

Graph
Instance

V C

suc

PLS
time (IQR)

suc

frb40-19-1
frb40-19-2
frb40-19-3
frb40-19-4
frb40-19-5
frb45-21-1
frb45-21-2
frb45-21-3
frb45-21-4
frb45-21-5
frb50-23-1
frb50-23-2
frb50-23-3
frb50-23-4
frb50-23-5
frb53-24-1
frb53-24-2
frb53-24-3
frb53-24-4
frb53-24-5
frb56-25-1
frb56-25-2
frb56-25-3
frb56-25-4
frb56-25-5
frb59-26-1
frb59-26-2
frb59-26-3
frb59-26-4
frb59-26-5

720
720
720
720
720
900
900
900
900
900
1100
1100
1100
1100
1100
1219
1219
1219
1219
1219
1344
1344
1344
1344
1344
1475
1475
1475
1475
1475

100
100
100
100
95
100
100
21
100
100
30
3
2
100
79
1
6
20
21
10
1
0
0
11
27
0
0
3
0
30

10.42 (10.38)
85.25 (72.75)
9.06 (10.21)
77.39 (90.56)
496 (529.25)
52.31 (55.5)
170 (202.2)
1737 (n/a)
111 (130)
261 (300)
1658 (640)
1956 (n/a)
1989 (n/a)
93 (80)
967 (1305)
1982 (n/a)
1959 (n/a)
1771 (n/a)
1782 (n/a)
1955 (n/a)
1993 (n/a)
n/a
n/a
1915 (n/a)
1719 (n/a)
n/a
n/a
1978 (n/a)
n/a
1708 (420)

100
100
100
100
100
100
100
100
100
100
100
48
39
100
100
17
50
99
48
95
24
17
97
93
100
16
9
21
3
98

COVER
time (IQR)
1.58 (0.55)
17.18 (16.09)
5.06 (4)
11.79 (8.67)
124 (131)
14.34 (12.8)
38 (35.4)
110 (121)
21 (18)
105 (103 )
268 (305)
1325 (n/a)
1486 (n/a)
33 (25)
168 (246)
1796 (n/a)
1279 (n/a)
273 (223)
1428 (n/a)
423 (315)
1698 (n/a)
1598 (n/a)
537 (692)
476 (460)
168 (128)
1607 (n/a)
1881 (n/a)
1768 (n/a)
1980 (n/a)
431 (476)

suc

EWCC
time (IQR)

suc

100
100
100
100
100
100
100
100
100
100
100
82
56
100
100
30
81
100
81
100
56
52
100
100
100
21
7
64
20
100

0.55 (0.48)
11.30 (14.21)
2.97 (2.35)
13.79 (16.05)
41.71 (39.08)
9.07 (9.3)
15 (14.1)
56 (70.4)
15 (12.5)
42 (40.1)
124 (135)
905 (1379)
1348 (n/a)
24 (27)
85 (97)
1696 (n/a)
1006 (1270)
117 (136)
900 (1480)
125 (115)
1268 (n/a)
1387 (n/a)
285 (250)
183 (188)
80 (81)
1778 (n/a)
1930 (n/a)
1294 (n/a)
1745 (n/a)
174 (182)

100
100
100
100
100
100
100
100
100
100
100
100
95
100
100
86
100
100
100
100
100
97
100
100
100
88
37
96
79
100

NuMVC
time (IQR)
0.24 (0.18)
4.08 (3.77)
1.07 (1.03)
2.76 (2.83)
10.14 (10.54)
2.71 (2.6)
5 (5.1)
14 (11.9)
4 (4.3)
11 (10.9)
38 (46)
177 (149)
606 (788)
8 (7)
19 (19)
895 (1099)
205 (200)
51 (48)
266 (311)
40 (44)
470 (466)
659 (780)
121 (118)
50 (49)
27 (23)
843 (849)
1677 (n/a)
636 (788)
1004 (1391)
62 (70)

Table 4: Comparison NuMVC state-of-the-art local search algorithms
BHOSLIB benchmark. BHOSLIB instances hidden optimal vertex cover,
whose size shown VC column.

702

fiN U MVC: N E FFICIENT L OCAL EARCH LGORITHM INIMUM V ERTEX C

100

2000

90

1800

80

1600

70

1400
average run time (s)

average success rate

best performance hard random benchmark, vastly improving existing performance
results. also observe that, NuMVC always minimum IQR value instances,
indicates apart efficiency, robustness NuMVC also better solvers.

60
50
40
30
20

1200
1000
800

PLS
COVER
EWCC
NuMVC

600
400

10
0
760(frb40)

PLS
COVER
EWCC
NuMVC

200
945(frb45)

0
760(frb40)

1150(frb50)1272(frb53) 1400(frb56) 1534(frb59)
number vertices graph

945(frb45)

1150(frb50)1272(frb53) 1400(frb56) 1534(frb59)
number vertices graph

Figure 1: Comparison NuMVC local search algorithms BHOSLIB benchmark
terms success rate (left) averaged run time (right)
also compare NuMVC COVER EWCC challenging instance frb100-40.
Given failure PLS large BHOSLIB instances, run PLS instance.
comparative results frb100-40 shown Table 5, indicates NuMVC
significantly outperforms COVER EWCC challenging instance.
Finally, would like remark performance NuMVC BHOSLIB benchmark
better four-core version CLS (Pullan et al., 2011), even divide run time
NuMVC 4 (the number cores utilized CLS). consider machine speed ratio
divide run time NuMVC 4, NuMVC would dramatically better CLS
BHOSLIB benchmark.
Size
VC
3902
3903

suc
0
33

COVER
avg suc time
n/a
2768

suc
1
79

EWCC
avg suc time
2586
2025

suc
4
93

NuMVC
avg suc time
2955
1473

Table 5: Comparative results frb100-40 challenging instance. solver executed
100 times instance timeout 4000 seconds.

6.5 Comparison Exact Algorithms
section, compare NuMVC state-of-the-art exact Maximum Clique algorithm.
Generally, exact algorithms heuristic algorithms somewhat complementary
applications. Usually, exact algorithms find solutions structured instances faster heuristic
algorithms faster random ones.
703

fiC AI , U , L UO & ATTAR

Compared MVC MIS, many exact algorithms designed Maximum Clique
problem (Carraghan & Pardalos, 1990; Fahle, 2002; Ostergard, 2002; Regin, 2003; Tomita &
Kameda, 2009; Li & Quan, 2010b, 2010a). recent branch-and-bound MC algorithm MaxCLQ
(Li & Quan, 2010b) utilizes MaxSAT inference technologies (Li, Manya, & Planes, 2007)
improve upper bounds shows considerable progress. Experimental results MaxCLQ (Li &
Quan, 2010b) random graphs DIMACS instances indicate MaxCLQ significantly
outperforms previous exact MC algorithms. MaxCLQ algorithm improved using two
strategies called Extended Failed Literal Detection Soft Clause Relaxation, resulting better
algorithm denoted MaxCLQdyn+EFL+SCR (Li & Quan, 2010a). Due great success
MaxCLQdyn+EFL+SCR, compare algorithm MaxCLQdyn+EFL+SCR.
compare NuMVC MaxCLQdyn+EFL+SCR DIMACS benchmark instances.
results MaxCLQdyn+EFL+SCR taken previous work (Li & Quan, 2010a).
MaxCLQdyn+EFL+SCR evaluated BHOSLIB benchmark much harder
requires effective technologies exact algorithms (Li & Quan, 2010a).
run time results MaxCLQdyn+EFL+SCR obtained 3.33 GHz Intel Core 2 Duo
CPU linux 4 Gb memory, required 0.172 seconds r300.5, 1.016 seconds
r400.5 3.872 seconds r500.5 execute DIMACS machine benchmarks (Li & Quan,
2010a). corresponding run time machine 0.19, 1.12 4.24 seconds. So, multiply
reported run time MaxCLQdyn+EFL+SCR 1.098 (=(4.24/3.872+1.12/1.016)/2=1.098,
average two largest ratios). normalization based methodology established
Second DIMACS Implementation Challenge Cliques, Coloring, Satisfiability,
widely used comparing different MaxClique algorithms (Pullan & Hoos, 2006; Pullan, 2006; Li
& Quan, 2010b, 2010a).
Graph
Instance
brock400 2
brock400 3
brock400 4
brock800 2
brock800 3
brock800 4
keller5
MANN a27
MANN a45

V C

371
369
367
776
775
774
749
252
690

NuMVC
suc
time
96
100
100
0
0
0
100
100
100

572.39
8.25
4.98
n/a
n/a
n/a
0.04
<0.001
86.86

MaxCLQdyn+EFL
+SCR time
125.06
251.44
119.24
5138.10
3298.39
2391.44
6884.46
0.17
21.169

Graph
Instance

V C

NuMVC
suc
time

p hat300-3
p hat700-2
p hat700-3
p hat1000-2
p hat1000-3
p hat1500-1
p hat1500-2
sanr200 0.9
sanr400 0.7

264
656
638
954
932
1488
1435
158
379

100
100
100
100
100
100
100
100
100

0.001
0.006
0.008
0.019
0.032
3.75
0.071
<0.001
0.008

MaxCLQdyn+EFL
+SCR time
1.31
3.27
1141.92
108.94
113860.40
3.10
866.51
5.20
97.72

Table 6: Comparison NuMVC state-of-the-art exact MaxClique algorithm MaxCLQdyn+EFL+SCR DIMACS benchmark.

Table 6, present performance NuMVC MaxCLQdyn+EFL+SCR
DIMACS instances. results indicate NuMVC finds optimal solution much faster
MaxCLQdyn+EFL+SCR random instances p hat sanr instances.
believe similar results would hold hard random benchmarks like BHOSLIB ones,
MaxCLQdyn+EFL+SCR evaluated instances due high hardness (Li & Quan,
2010a), NuMVC performs well them.
structured instances, note MaxCLQdyn+EFL+SCR mainly evaluated
brock instances NuMVC performs worst, open DIMACS instances
704

fiN U MVC: N E FFICIENT L OCAL EARCH LGORITHM INIMUM V ERTEX C

MANN a81, johnson32-2-4 keller6, remain difficult solve exact
algorithms (Li & Quan, 2010a). Although MaxCLQdyn+EFL+SCR overall performs better,
NuMVC also finds optimal solution significantly faster MaxCLQdyn+EFL+SCR
structured instances, two brock instances keller5.
Finally, would like note although heuristic solvers find optimal solutions fast,
unable prove optimality solutions find. hand, run time
exact algorithm spent finding optimal solution also proving optimality.
sense, heuristic exact algorithms cannot compared fair way. Nevertheless,
experiments suggest heuristic approaches appealing solving large instances reasonable
short time.

7. Discussions
section, first explore run-time distribution NuMVC representative
instances, investigate effectiveness two-stage exchange strategy
forgetting mechanism NuMVC. Finally, analyze performance NuMVC different
settings two parameters forgetting mechanism, shows NuMVC
sensitive parameters.
7.1 Run-time Distributions NuMVC
subsection, conduct empirical study gain deeper insights run-time behavior
NuMVC. specifically, study run-time distribution NuMVC several representative
instances. purpose comparison, also report run-time distribution EWCC,
best competing MVC local search solver.
Consider randomized algorithm solving given optimization problem instance, halting
soon optimal solution found. run time algorithm viewed
random variable, fully described distribution, commonly referred run-time
distribution (RTD) literature algorithm performance modeling (Hoos & Stutzle, 2004;
Bartz-Beielstein et al., 2010). methodology studying run-time behavior algorithms
based RTDs widely used empirical analysis heuristic algorithms (Hoos & Stutzle,
1999; Finkelstein, Markovitch, & Rivlin, 2003; Watson, Whitley, & Howe, 2005; Pullan & Hoos,
2006). also follow methodology study here.
studying typical run-time behaviour, choose instances NuMVC reaches optimal
solution 100 runs, appropriate difficulty. DIMACS benchmark, select
brock400 4 MANN a45, reasonable size hardness. Also, two
instances represent two typical instance classes NuMVC, NuMVC poor performance
brock instances, dominates heuristic algorithms MANN instances.
BHOSLIB benchmark, frb56-25-5 frb59-26-5 selected. appropriate
instances studying run-time behavior NuMVC, since neither easy
solved short time difficult reach 100% success rate.
empirical RTD graphs NuMVC EWCC shown Figure 2 (the RTD
instance based 100 independent runs reach respective optimal solution). According
graphs, NuMVC shows large variability run time. investigation indicates
RTDs quite well approximated exponential distributions, labeled ed[m](x) = 1 2x/m ,
median distribution. test goodness approximations, use
705

fiC AI , U , L UO & ATTAR

Empirical RTD NuMVC EWCC MANN_a45

Empirical RTD NuMVC EWCC brock400_4
1
0.9
0.8
0.7

1
0.9

RTD NuMVC
ed[3.6]
RTD EWCC
ed[12]

0.8
0.7
0.6
P(solve)

P(solve)

0.6
0.5

0.5

0.4

0.4

0.3

0.3

0.2

0.2

0.1

0.1

0
2
10

1

0

10

1

2

10
10
runtime [CPU sec]

10

0
1
10

3

10

Empirical RTD NuMVC EWCC frb56255

0.8
0.7

0.9

RTD NuMVC
ed[19]
RTD EWCC
ed[53]

0.8
0.7

2

3

10

4

10

RTD NuMVC
ed[45]
RTD EWCC
ed[116]

0.6
P(solve)

P(solve)

1

10
10
runtime [CPU sec]

1

0.6
0.5

0.5

0.4

0.4

0.3

0.3

0.2

0.2

0.1

0.1

0
1
10

0

10

Empirical RTD NuMVC EWCC frb59265

1
0.9

RTD NuMVC
ed[59]
RTD EWCC
ed[532]

0

10

1

10
runtime [CPU sec]

2

10

0
1
10

3

10

0

10

1

10
runtime [CPU sec]

2

10

3

10

Figure 2: Run-time distributions (RTDs) NuMVC EWCC applied two DIMACS instances
(top) two BHOSLIB instances (bottom); empirical RTDs well approximated
exponential distributions, labeled ed[m](x) = 1 2x/m plots.

Kolmogorov-Smirnov test, fails reject null hypothesis sampled run time
stems exponential distributions shown figures standard confidence level
= 0.05 p-values 0.19 0.88. EWCC, Kolmogorov-Smirnov test
shows RTDs MANN a45 two BHOSLIB instances also exponential distributions,
RTD brock400 4 exponential distribution.
observation exponential RTDs NuMVC consistent similar results
high performance SLS algorithms, e.g., MaxClique (Pullan & Hoos, 2006), SAT (Hoos &
Stutzle, 1999), MAXSAT (Smyth, Hoos, & Stutzle, 2003), scheduling problems (Watson
et al., 2005). arguments (Hoos & Stutzle, 1999; Hoos & Stutzle, 2004) made stochastic
local search algorithms characterized exponential RTD, conclude that, NuMVC,
probability finding optimal solution within fixed amount time (or steps) depend
run time past. Consequently, robust w.r.t. cutoff time thus, restart
706

fiN U MVC: N E FFICIENT L OCAL EARCH LGORITHM INIMUM V ERTEX C

time. Therefore, performing multiple independent runs NuMVC parallel result closeto-optimal parallelization speedup. Similar observations made DIMACS
instances BHOSLIB instances.
practical interest also RTD analysis NuMVC difficult instances
algorithms experiments fail achieve high success rate (i.e., 40%). RTDs
cases would show algorithm stagnates suggest a-posteriori restart time
algorithm. purpose, select MANN a81 frb59-26-2 analysis. RTDs
NuMVC two instances illustrated Figure 3. Interestingly, RTDs
observe obvious stagnation, confirms NuMVC robust w.r.t. cutoff time
thus restart time. Therefore, increasing cutoff time, expect higher success
rate algorithm difficult instances.
Empirical RTD NuMVC MANN_a81 frb59262
1
0.9
RTD NuMVC MANN_a81
0.8
RTD NuMVC frb59262
0.7

P(solve)

0.6
0.5
0.4
0.3
0.2
0.1
0
1
10

2

3

10

10

4

10

runtime [CPU sec]

Figure 3: Run-time distributions (RTDs) NuMVC MANN a81 frb59-26-2 instances,
NuMVC finds optimal (or best known) solution less half runs.

7.2 Effectiveness Two-Stage Exchange
study effectiveness two-stage exchange strategy, compare NuMVC
alternative algorithm NuMVC0 selects two vertices exchanging simultaneously.
step, NuMVC0 first chooses uncovered edge e uniformly random, evaluates pair
vertices u v u current candidate solution v one endpoint e
conf Change(v) = 1. evaluating benefit (i.e., decrement cost function)
exchanging vertex pair u v, NuMVC0 first checks whether neighbors. u
v neighbors, benefit dscore(u) + dscore(v) + w(e{u, v}); otherwise, benefit
dscore(u) + dscore(v). NuMVC0 selects vertex pair greatest benefit exchange.
NuMVC (and also NuMVC0 ) algorithm, two candidate vertices add
current candidate solution C (i.e., endpoints selected uncovered edge). Hence,
worst case, NuMVC performs 2 + |C| evaluations, NuMVC0 evaluate 2 |C|
pairs vertices. Moreover, NuMVC needs check dscore vertex (vertex)
707

fiC AI , U , L UO & ATTAR

evaluation, NuMVC0 performs vertex-pair evaluation involves pair vertices
relationship, thus time-consuming. Based analysis, conjecture
complexity per step NuMVC least 2 times lower NuMVC0 . Also,
mentioned Section 3, two-stage exchange strategy less greedy one selecting
two vertices exchanging simultaneously, NuMVC0 does.
investigation carried 4 DIMACS instances different families well
12 BHOSLIB instances. DIMACS benchmark, select brock400 2, C4000.5,
MANN a45, p hat 1500-1. instances different characteristics, described
(Pullan et al., 2011). Note following conclusions DIMACS instances
complementary DIMACS graphs.
DIMACS brock instances minimum vertex covers consist medium lower
degree vertices, designed defeat greedy heuristics.
DIMACS C p hat 1500-1 instances minimum vertex covers consist
higher degree vertices effectively solved greedy heuristics.
DIMACS MANN instances large proportion plateaus instance searchspace, thus greedy heuristics unsuitable solve them.
BHOSLIB instances minimum vertex covers consisting vertices whose
distribution vertex degree closely matches complete graph. difficult
instances greedy diversification heuristics.
Graph
Instance
brock400 2
C4000.5
MANN a45
p hat 1500-1
frb50-23-1
frb50-23-2
frb50-23-3
frb53-24-1
frb53-24-2
frb53-24-3
frb56-25-1
frb56-25-2
frb56-25-3
frb59-26-1
frb59-26-2
frb59-26-3

V C
371
3982
690
1488
1100
1100
1100
1219
1219
1219
1344
1344
1344
1475
1475
1475

suc
96
100
100
100
100
100
95
86
100
100
100
97
100
88
37
96

time
572
252
86
3.75
38
177
606
895
205
51
470
659
121
843
1677
636

NuMVC
steps
645631471
7802785
90642150
445830
24628019
113569606
386342329
514619149
117980833
29376406
259903023
350048132
67043078
440874471
875964146
325417225

#steps/sec (105 )
11.3
0.3
10.5
1.2
6.5
6.4
6.4
5.7
5.8
5.8
5.5
5.3
5.5
5.2
5.2
5.1

suc
19
100
100
100
100
100
63
45
100
100
72
52
100
45
21
69

time
1861
607
564
13.24
88
499
1312
1595
557
106
1088
1499
253
1572
1853
1545

NuMVC0
steps
837844749
6343304
186350533
381762
18125042
104841043
262559614
286396840
105863802
19685358
184323492
254973016
43062419
251520339
315425608
247273810

#steps/sec (105 )
4.5
0.1
3.3
0.3
2.1
2.1
2.0
1.8
1.9
1.9
1.7
1.7
1.7
1.6
1.7
1.6

Table 7: Comparative performance NuMVC NuMVC0 selects two vertices
exchanging simultaneously. results based 100 independent runs solver
instance.
comparative results NuMVC NuMVC0 presented Table 7. results show
NuMVC significantly outperforms NuMVC0 terms averaged run time, primarily due
much lower complexity per step. second, NuMVC performs 3-4 times steps
708

fiN U MVC: N E FFICIENT L OCAL EARCH LGORITHM INIMUM V ERTEX C

NuMVC0 , supports conjecture complexity per step NuMVC 2
times lower NuMVC0 .
turn attention comparing NuMVC NuMVC0 terms step performance,
independent complexity per step. brock MANN graphs difficult
greedy heuristics, NuMVC significantly better step performance NuMVC0 .
hand, greedy-friendly graphs C4000.5 p hat 1500-1, NuMVC needs
steps converge optimal solution NuMVC0 does. observations support
argument two-stage exchange strategy less greedy one selects two vertices
exchanging simultaneously, NuMVC0 does.
also observe step performance NuMVC0 better NuMVC
BHOSLIB instances. instance, BHOSLIB instances algorithms
100% success rate, NuMVC needs 1.2 times steps NuMVC0 find optimal
solution. expect cannot yet explain. Nevertheless, NuMVC makes
rather rapid modifications solution, little degrade step performance hurt.
Graph
Instance
C4000.5
MANN a45
p hat 1500-1
frb53-24-5
frb56-25-5
frb59-26-5

PLS
#steps/sec
85,318
1,546,625
170,511
841,346
801,282
706,436

COVER
#steps/sec
8,699
279,514
19,473
128,971
116,618
108,534

EWCC
#steps/sec
11,927
578,656
34,111
219,038
199,441
189,536

NuMVC
#steps/sec
30,963
1,053,978
118,888
570,425
522,561
511,014

Table 8: Complexity per step selected instances
demonstrate low complexity per step NuMVC, compare number
search steps per second NuMVC state-of-the-art heuristic solvers
representative instances. indicated Table 8, NuMVC executes many steps second
two MVC local search solvers COVER EWCC do. instances Table
8, second NuMVC executes 4-6 times steps COVER, 3-4 times steps
EWCC. indicates two-stage exchange strategy significantly accelerate MVC
local search algorithms. Although PLS performs steps per second NuMVC, MC
local search algorithm whose search scheme essentially different MVC local search
algorithms.
7.3 Effectiveness Forgetting Mechanism
study effectiveness forgetting mechanism NuMVC, compare NuMVC
two alternative algorithms NuMVC1 NuMVC2 , obtained NuMVC modifying
edge weighting scheme below.
NuMVC1 works way NuMVC, except using forgetting mechanism,
is, deleting line 18 Algorithm 1.
NuMVC2 adopts forgetting mechanism used DLS-MC (Pullan & Hoos, 2006)
weighting scheme. specifically, NuMVC2 increases weights uncovered edges
709

fiC AI , U , L UO & ATTAR

one end step, performs forgetting operation every pd steps decreasing
weights one edges whose weights greater one. Note pd instancedependent parameter.
experiments carried representative instances benchmarks.
DIMACS benchmark, select brock400 2, C4000.5, keller6, MANN a45,
different classes appropriate difficulty. BHOSLIB benchmark,
select three instances three largest-sized instance groups respectively.
Graph
Instance Vertices
brock400 2
400
C4000.5
4000
keller6
3361
MANN a45
1035
frb53-24-1
1272
frb53-24-2
1272
frb53-24-3
1272
frb56-25-1
1400
frb56-25-2
1400
frb56-25-3
1400
frb59-26-1
1534
frb59-26-2
1534
frb59-26-3
1534

C

V
371
3982
3302
690
1219
1219
1219
1344
1344
1344
1475
1475
1475

NuMVC
suc time
96
572
100
252
100 2.51
100
86
86
895
100
205
100
51
100
470
97
659
100
121
88
843
37 1677
96
636

NuMVC1
suc time
22 1781
100
270
100 2.95
65 1187
60
925
100
243
100
49
85
914
63 1209
100
111
64 1229
21 1894
83
997

NuMVC2
pd (102 ) suc time
15 100
21
60 100
327
750 100 4.26
8 100
113
100
78
901
100 100
201
100 100
52
130
91
595
130
81
739
130 100
117
150
85
907
150
45 1439
150
97
652

Table 9: Comparative performance NuMVC two alternatives NuMVC1 NuMVC2 .
algorithm performed 100 times instance.

apparent observation Table 9 two algorithms forgetting mechanisms
(i.e., NuMVC NuMVC2 ) outperform NuMVC1 almost instances. Particularly, due
missing forgetting mechanism, NuMVC1 performs significantly worse two
algorithms brock MANN graphs. hand, Table 9 demonstrates NuMVC
NuMVC2 exhibit competitive performance BHOSLIB benchmark, dominate different
types DIMACS instances. specifically, NuMVC outperforms NuMVC2 C4000.5,
keller6 MANN a45, performs significantly worse NuMVC2 brock400 2.
order find genuine performance NuMVC2 brock instances, test NuMVC2
larger brock800 2 brock800 4 instances. results show two large brock
instances substantially difficult two brock400 instances, NuMVC2 also fails
solve neither them.
Although NuMVC2 shows competitive performance NuMVC, performance given
optimizing pd parameter instance. Moreover, DLS-MC (Pullan & Hoos, 2006),
NuMVC2 considerably sensitive pd parameter. example, experiments show
frb53-24 instances, NuMVC2 performs quite well pd = 10000, fails find
optimal solution pd set value less 7000. Comparatively, NuMVC
710

fiN U MVC: N E FFICIENT L OCAL EARCH LGORITHM INIMUM V ERTEX C

parameter setting performs quite well types instances brock family. Actually,
show next section NuMVC sensitive parameters.
also interesting compare NuMVC alternatives replace forgetting
mechanism smoothing techniques similar local search SAT. Indeed, earlier
versions NuMVC use smoothing techniques similar SAT local search,
good performance compared NuMVC. would interesting find
reasons success forgetting mechanism failure smoothing techniques
MVC edge weighting local search algorithms NuMVC.
7.4 Parameters Forgetting Mechanism

(0.3|V |, 0.1)
(0.3|V |, 0.2)
(0.3|V |, 0.3)
(0.3|V |, 0.4)
(0.3|V |, 0.5)

brock400 2
100% (382)
100% (361)
100% (362)
95% (490)
90% (507)

MANN a45
100% (153)
100% (164)
100% (131)
100% (208)
100% (90)

C4000.5
100% (262)
100% (265)
100% (272)
100% (270)
100% (268)

frb53-24-1
80% (904)
85% (918)
70% (1058)
80% (995)
65% (1316)

frb53-24-2
100% (348)
100% (279)
100% (156)
100% (191)
100% (431)

frb56-25-1
100% (338)
90% (671)
95% (826)
100% (602)
100% (490)

frb56-25-2
80% (997)
70% (1197)
85% (819)
100% (885)
95% (922)

(0.4|V |, 0.1)
(0.4|V |, 0.2)
(0.4|V |, 0.3)
(0.4|V |, 0.4)
(0.4|V |, 0.5)

100% (261)
90% (736)
100% (402)
95% (375)
90% (612)

100% (133)
100% (207)
100% (176)
100% (169)
100% (190)

100% (250)
100% (245)
100% (258)
100% (253)
100% (264)

80% (899)
80% (860)
75% (1047)
70% (1009)
65% (1059)

100% (158)
100% (443)
100% (260)
100% (394)
100% (137)

90% (464)
85% (611)
90% (976)
90% (885)
95% (428)

70% (1601)
80% (851)
90% (1055)
85% (1019)
100% (851)

(0.5|V |, 0.1)
(0.5|V |, 0.2)
(0.5|V |, 0.3)
(0.5|V |, 0.4)
(0.5|V |, 0.5)

100% (523)
85% (950)
96% (572)
90% (499)
90% (968)

100% (107)
100% (69)
100% (86)
100% (169)
100% (148)

100% (262)
100% (259)
100% (252)
100% (251)
100% (249)

70% (1007)
75% (1061)
86% (850)
70% (931)
90% (805)

100% (416)
100% (482)
100% (205)
100% (219)
100% (361)

90% (714)
95% (706)
100% (470)
90% (632)
85% (933)

75% (1064)
70% (1228)
97% (625)
80% (1027)
85% (983)

(0.6|V |, 0.1)
(0.6|V |, 0.2)
(0.6|V |, 0.3)
(0.6|V |, 0.4)
(0.6|V |, 0.5)

100% (527)
80% (713)
75% (976)
100% (710)
85% (742)

100% (203)
100% (172)
100% (92)
100% (142)
100% (125)

100% (255)
100% (279)
100% (272)
100% (276)
100% (288)

70% (1109)
75% (944)
70% (1130)
75% (907)
80% (947)

100% (267)
100% (254)
100% (298)
100% (170)
100% (192)

100% (828)
90% (704)
90% (689)
100% (592)
100% (647)

90% (878)
70% (1306)
75% (862)
85% (1028)
80% (1109)

(0.7|V |, 0.1)
(0.7|V |, 0.2)
(0.7|V |, 0.3)
(0.7|V |, 0.4)
(0.7|V |, 0.5)

100% (410)
95% (781)
90% (826)
75% (1219)
90% (707)

100% (87)
100% (128)
100% (125)
100% (101)
100% (92)

100% (273)
100% (284)
100% (266)
100% (272)
100% (280)

65% (1186)
70% (1035)
75% (916)
85% (700)
70% (1085)

100% (358)
100% (220)
100% (206)
100% (338)
100% (352)

75% (1014)
80% (713)
80% (878)
100% (536)
90% (736)

75% (934)
90% (510)
80% (971)
85% (769)
70% (1044)

Table 10: Comparative performance NuMVC various parameter combinations (, )
forgetting mechanism. instance, NuMVC performed 20 times
parameter combination, except one adopted work (0.5|V |, 0.3),
results based 100 runs. keller6, NuMVC performs almost
various parameters, success rate (100%) tiny difference averaged
run time (less 1 second), thus results reported table.

711

fiC AI , U , L UO & ATTAR

NuMVC algorithm two parameters , specify forgetting mechanism.
Specifically, averaged weight edges achieves threshold , edge weights
multiplied constant factor (0 < < 1). subsection, investigate NuMVC
performs different settings two parameters. investigation carried
DIMACS BHOSLIB benchmarks. DIMACS benchmark, select four instances
used preceding subsection reasons. BHOSLIB benchmark, select
frb53-24-1, frb53-24-2, frb56-25-1 frb56-25-2, different sizes
appropriate hardness.
Table 10 presents performance NuMVC various parameter combinations
representative instances. see Table 10, parameter combination
(0.5|V |, 0.3) yields relatively good performance instances, exhibits better robustness
instances parameter combinations do.
hand, observe NuMVC various parameter combinations performs
comparably tested instances. example, parameter settings, NuMVC achieves
success rate 100% keller6, MANN a45, C4000.5 well frb53-24-1,
averaged run time difference instances significant. instances,
difference success rate never exceeds 25% two parameter settings. observation
indicates NuMVC seems sensitive two parameters. Actually, mentioned
before, NuMVC exhibits good performance DIMACS BHOSLIB benchmarks
fixed parameter setting. advantage compared forgetting mechanisms
one used DLS-MC (Pullan & Hoos, 2006), sensitive parameter.
algorithms sensitive parameters, considerable parameter tuning required order
get good performance certain instance, usually costs much time solving
instance.

8. Conclusions Future Work
paper, presented two new local search strategies minimum vertex cover (MVC)
problem, namely two-stage exchange edge weighting forgetting. two-stage exchange
strategy yields efficient two-pass move operator MVC local search algorithms,
significantly reduces time complexity per step. forgetting mechanism enhances
edge weighting scheme decreasing weights averaged weight reaches threshold,
periodically forget earlier weighting decisions. Based two strategies, designed
slight, yet effective MVC local search algorithm called NuMVC. NuMVC algorithm
evaluated best known heuristic algorithms MVC (MC, MIS) standard benchmarks,
i.e., DIMACS BHOSLIB benchmarks. experimental results show NuMVC
largely competitive DIMACS benchmark dramatically outperforms state-of-the-art
heuristic algorithms BHOSLIB instances.
Furthermore, showed NuMVC characterized exponential RTDs, means
robust w.r.t. cutoff parameters restart time, hence close-to-optimal parallelization
speedup. also performed investigations provide insights two new
strategies effectiveness. Finally, conducted experiment study performance
NuMVC different parameter settings, results indicate NuMVC sensitive
parameters.
712

fiN U MVC: N E FFICIENT L OCAL EARCH LGORITHM INIMUM V ERTEX C

two-stage exchange strategy lower time complexity per step, also
flexibility allow us employ specific heuristics different stages. interesting research
direction thus apply idea combinatorial problems whose essential tasks also
seek optimal subset fixed cardinality.

Acknowledgments
work supported 973 Program 2010CB328103, ARC Future Fellowship FT0991785,
National Natural Science Foundation China (61073033, 61003056 60903054),
Fundamental Research Funds Central Universities China (21612414). would like
thank editor anonymous reviewers valuable comments earlier versions
paper. would also like thank Yanyan Xu proofreading paper.

References
Aggarwal, C., Orlin, J., & Tai, R. (1997). Optimized crossover independent set problem.
Operations Research, 45, 226234.
Andrade, D. V., Resende, M. G. C., & Werneck, R. F. F. (2008). Fast local search maximum
independent set problem. Workshop Experimental Algorithms, pp. 220234.
Barbosa, V. C., & Campos, L. C. D. (2004). novel evolutionary formulation maximum
independent set problem. J. Comb. Optim., 8(4), 419437.
Bartz-Beielstein, T., Chiarandini, M., Paquete, L., & Preuss, M. (Eds.). (2010). Experimental
Methods Analysis Optimization Algorithms. Springer, Berlin, Heidelberg, New
York.
Battiti, R., & Protasi, M. (2001). Reactive local search maximum clique problem.
Algorithmica, 29(4), 610637.
Busygin, S., Butenko, S., & Pardalos, P. M. (2002). heuristic maximum independent set
problem based optimization quadratic sphere. J. Comb. Optim., 6(3), 287297.
Cai, S., & Su, K. (2011). Local search configuration checking SAT. Proc. ICTAI-11,
pp. 5966.
Cai, S., & Su, K. (2012). Configuration checking aspiration local search SAT. Proc.
AAAI-12, pp. 434440.
Cai, S., Su, K., & Chen, Q. (2010). EWLS: new local search minimum vertex cover. Proc.
AAAI-10, pp. 4550.
Cai, S., Su, K., & Sattar, A. (2011). Local search edge weighting configuration checking
heuristics minimum vertex cover. Artif. Intell., 175(9-10), 16721696.
Cai, S., Su, K., & Sattar, A. (2012). Two new local search strategies minimum vertex cover.
Proc. AAAI-12, pp. 441447.
Carraghan, R., & Pardalos, P. (1990). exact algorithm maximum clique problem.
Operations Research Letters, 9(6), 375382.
713

fiC AI , U , L UO & ATTAR

Dinur, I., & Safra, S. (2005). hardness approximating minimum vertex cover. Annals
Mathematics, 162(2), 439486.
Evans, I. (1998). evolutionary heuristic minimum vertex cover problem. Proceedings
Seventh International Conference Evolutionary Programming(EP), pp. 377386.
Fahle, T. (2002). Simple fast: Improving branch-and-bound algorithm maximum clique.
Proc. European Symposium Algorithms (ESA)-02, pp. 485498.
Feige, U. (2004). Approximating maximum clique removing subgraphs. SIAM J. Discrete Math.,
18(2), 219225.
Finkelstein, L., Markovitch, S., & Rivlin, E. (2003). Optimal schedules parallelizing anytime
algorithms: case shared resources. J. Artif. Intell. Res. (JAIR), 19, 73138.
Gajurel, S., & Bielefeld, R. (2012). fast near optimal vertex cover algorithm (novca).
International Journal Experimental Algorithms (IJEA), 3, 918.
Garey, M., & Johnson, D. (1979). Computers Intractability: Guide Theory NPcompleteness. Freeman, San Francisco, CA, USA.
Glover, F. (1989). Tabu search part i. ORSA Journal Computing, 1(3), 190206.
Grosso, A., Locatelli, M., & Pullan, W. J. (2008). Simple ingredients leading efficient
heuristics maximum clique problem. J. Heuristics, 14(6), 587612.
Halperin, E. (2002). Improved approximation algorithms vertex cover problem graphs
hypergraphs. SIAM Journal Computing, 31(5), 15081623.
Hastad, J. (1999). Clique hard approximate within n1 . Acta Math, 182, 105142.
Hastad, J. (2001). optimal inapproximability results. J. ACM, 48(4), 798859.
Hoaglin, D. C., Mosteller, F., & Tukey, J. W. (Eds.). (2000). Understanding Robust Exploratory
Data Analysis. Wiley Classics Library, Wiley, New York, NY.
Hoos, H., & Stutzle, T. (2004). Stochastic Local Search: Foundations Applications. Morgan
Kaufmann, San Francisco, CA, USA.
Hoos, H. H., & Stutzle, T. (1999). Towards characterisation behaviour stochastic local
search algorithms SAT. Artif. Intell., 112(1-2), 213232.
Hutter, F., Tompkins, D. A. D., & Hoos, H. H. (2002). Scaling probabilistic smoothing: Efficient
dynamic local search SAT. Proc. CP-02, pp. 233248.
Ishtaiwi, A., Thornton, J., Sattar, A., & Pham, D. N. (2005). Neighbourhood clause weight
redistribution local search SAT. Proc. CP-05, pp. 772776.
Johnson, D. S., & Trick, M. (Eds.). (1996). Cliques, Coloring, Satisfiability: Second DIMACS
Implementation Challenge, 1993, Vol. 26 DIMACS Series Discrete Mathematics
Theoretical Computer Science. American Mathematical Society, Providence, RI, USA.
Karakostas, G. (2005). better approximation ratio vertex cover problem. Proc.
ICALP-05, pp. 10431050.
Katayama, K., Sadamatsu, M., & Narihisa, H. (2007). Iterated k-opt local search maximum
clique problem. Proc. EvoCOP-07, pp. 8495.
714

fiN U MVC: N E FFICIENT L OCAL EARCH LGORITHM INIMUM V ERTEX C

Li, C. M., Manya, F., & Planes, J. (2007). New inference rules max-sat. J. Artif. Intell. Res.
(JAIR), 30, 321359.
Li, C. M., & Quan, Z. (2010a). Combining graph structure exploitation propositional reasoning
maximum clique problem. Proc. ICTAI-10, pp. 344351.
Li, C. M., & Quan, Z. (2010b). efficient branch-and-bound algorithm based maxsat
maximum clique problem. Proc. AAAI-10, pp. 128133.
Michiels, W., Aarts, E. H. L., & Korst, J. H. M. (2007). Theoretical aspects local search. Springer.
Minton, S., Johnston, M. D., Philips, A. B., & Laird, P. (1992). Minimizing conflicts: heuristic
repair method constraint satisfaction scheduling problems. Artif. Intell., 58(1-3), 161
205.
Morris, P. (1993). breakout method escaping local minima. Proc. AAAI-93, pp.
4045.
Ostergard, P. R. J. (2002). fast algorithm maximum clique problem. Discrete Applied
Mathematics, 120(1-3), 197207.
Papadimitriou, C. H. (1991). selecting satisfying truth assignment. Proc. FOCS-91, pp.
163169.
Pullan, W. (2006). Phased local search maximum clique problem. J. Comb. Optim., 12(3),
303323.
Pullan, W. (2009). Optimisation unweighted/weighted maximum independent sets minimum
vertex covers. Discrete Optimization, 6, 214219.
Pullan, W., & Hoos, H. H. (2006). Dynamic local search maximum clique problem. J. Artif.
Intell. Res. (JAIR), 25, 159185.
Pullan, W., Mascia, F., & Brunato, M. (2011). Cooperating local search maximum clique
problem. J. Heuristics, 17(2), 181199.
Regin, J. C. (2003). Using constraint programming solve maximum clique problem. Proc.
CP-03, pp. 634648.
Richter, S., Helmert, M., & Gretton, C. (2007). stochastic local search approach vertex cover.
Proc. KI-07, pp. 412426.
Schuurmans, D., Southey, F., & Holte, R. C. (2001). exponentiated subgradient algorithm
heuristic boolean programming. Proc. IJCAI-01, pp. 334341.
Shyu, S. J., Yin, P., & Lin, B. M. T. (2004). ant colony optimization algorithm minimum
weight vertex cover problem. Annals OR, 131(1-4), 283304.
Smyth, K., Hoos, H. H., & Stutzle, T. (2003). Iterated robust tabu search max-sat. Proc.
Canadian Conference AI-03, pp. 129144.
Taillard, E. D. (1994). Parallel taboo search techniques job shop scheduling problem.
INFORMS Journal Computing, 6(2), 108117.
Thornton, J., Pham, D. N., Bain, S., & Jr., V. F. (2004). Additive versus multiplicative clause
weighting SAT. Proc. AAAI-04, pp. 191196.
715

fiC AI , U , L UO & ATTAR

Tomita, E., & Kameda, T. (2009). efficient branch-and-bound algorithm finding maximum
clique computational experiments. J. Global Optimization, 44(2), 311.
Watson, J.-P., Whitley, L. D., & Howe, A. E. (2005). Linking search space structure, run-time
dynamics, problem difficulty: step toward demystifying tabu search. J. Artif. Intell.
Res. (JAIR), 24, 221261.
Wu, Q., Hao, J.-K., & Glover, F. (2012). Multi-neighborhood tabu search maximum weight
clique problem. Annals OR, 196(1), 611634.
Wu, Z., & Wah, B. W. (2000). efficient global-search strategy discrete lagrangian methods
solving hard satisfiability problems. Proc. AAAI/IAAI-00, pp. 310315.
Xu, K., Boussemart, F., Hemery, F., & Lecoutre, C. (2005). simple model generate hard
satisfiable instances. Proc. IJCAI-05, pp. 337342.
Xu, K., Boussemart, F., Hemery, F., & Lecoutre, C. (2007). Random constraint satisfaction: Easy
generation hard (satisfiable) instances. Artif. Intell., 171(8-9), 514534.
Xu, K., & Li, W. (2000). Exact phase transitions random constraint satisfaction problems. J.
Artif. Intell. Res. (JAIR), 12, 93103.
Xu, K., & Li, W. (2006). Many hard examples exact phase transitions. Theoretical Computer
Science, 355, 291302.
Yugami, N., Ohta, Y., & Hara, H. (1994). Improving repair-based constraint satisfaction methods
value propagation. AAAI, pp. 344349.
Zuckerman, D. (2006). Linear degree extractors inapproximability max clique
chromatic number. Proc. STOC-06, pp. 681690.

716

fiJournal Artificial Intelligence Research 46 (2013) 165-201

Submitted 7/12; published 2/13

Generating Extractive Summaries Scientific Paradigms
Vahed Qazvinian

vahed@umich.edu

Department EECS,
University Michigan, Ann Arbor, MI, 48109

Dragomir R. Radev

radev@umich.edu

Department EECS & School Information,
University Michigan, Ann Arbor, MI, 48109

Saif M. Mohammad

saif.mohammad@nrc-cnrc.gc.ca

National Research Council Canada,
Ottawa, Ontario, Canada, K1A 0R6

Bonnie Dorr
David Zajic
Michael Whidby
Taesun Moon

bonnie@umiacs.umd.edu
dmzajic@umiacs.umd.edu
mawhidby@umd.edu
tsmoon@umiacs.umd.edu

Institute Advanced Computer Studies & Computer Science,
University Maryland, College Park, MD, 20742

Abstract
Researchers scientists increasingly find position
quickly understand large amounts technical material. goal effectively serve
need using bibliometric text mining summarization techniques generate
summaries scientific literature. show use citations produce automatically generated, readily consumable, technical extractive summaries. first propose
C-LexRank, model summarizing single scientific articles based citations,
employs community detection extracts salient information-rich sentences. Next,
extend experiments summarize set papers, cover scientific topic. generate extractive summaries set Question Answering (QA)
Dependency Parsing (DP) papers, abstracts, citation sentences show
citations unique information amenable creating summary.

1. Introduction
todays rapidly expanding disciplines, scientists scholars constantly faced
daunting task keeping knowledge field. addition, increasingly
interconnected nature real-world tasks often requires experts one discipline rapidly
learn areas short amount time. Cross-disciplinary research requires
scientists areas linguistics, biology, sociology learn computational
approaches applications computational linguistics, biological modeling,
social networks. Authors journal articles books must write accurate summaries
previous work, ranging short summaries related research in-depth historical notes.
Interdisciplinary review panels often called upon review proposals wide range
c
2013
AI Access Foundation. rights reserved.

fiQazvinian et Al.

areas, may unfamiliar panelists. Thus, must learn new
discipline fly order relate expertise proposal.
goal effectively serve needs combining two currently available technologies: (1) bibliometric lexical link mining exploits structure citations (2)
summarization techniques exploit content material citing
cited papers.
generally agreed upon manually written abstracts good summaries individual papers. recently, Qazvinian Radev (2008) argued citation sentences
(i.e., set sentences appear papers cite given article) useful
creating summary important contributions research paper. Kaplan, Iida,
Tokunaga (2009) introduced citation-site block text includes citation
discusses cited text. work used machine learning method extracting citations
research papers evaluates result using annotated corpus 38 papers citing
4 articles. Moreover, Qazvinian Radev (2010) showed usefulness using implicit
citations (i.e., context sentences, sentences occur citation sentence
explicitly cite target paper, discuss contributions) summary generation. Teufel (2005) argued citations could contain subjective content,
content exploited summary generation. Additional work (Mohammad et al.,
2009) demonstrated usefulness citations producing multi-document summaries
scientific articles. Follow-up work indicated improvements citation handling
enables production fluent summaries (Whidby, 2012).
work, develop summarization systems exploit citations. Specifically,
compare contrast usefulness abstracts citations automatically
generating technical summary given topic multiple research papers.
findings suggest abstracts citations overlapping information
also significant amount unique summary-amenable information. Particularly, provide evidence citation sentences contain crucial information
available, hard extract, abstracts papers alone.
propose C-LexRank, graph based summarization system. method models
set citing sentences network vertices sentences edges represent
lexical similarity. C-LexRank identifies vertex communities (clusters)
network, selects sentences different communities increase diversity
summary. Using 30 different sets citation sentences extracted 6 different
NLP topics ACL1 Anthology Network, show C-LexRank effective
producing summary papers contributions. compare C-LexRank
wide range state-of-the-art summarization systems leverage diversity (MMR,
DivRank, MASCS), employ graph structure (DivRank, LexRank), employ sentence
compression (MASCS) produce summary.
extend experiments summarizing contributions single article
generating summaries scientific topics. evaluation experiments extractive
summary generation applied set 10 papers research area Question
Answering (QA) another set 16 papers Dependency Parsing (DP).
1. Association Computational Linguistics

166

fiGenerating Extractive Summaries Scientific Paradigms

provide background work including primary features technical
summary also types input used study (full papers, abstracts,
citation sentences).
1.1 Background
Automatically creating technical extractive summaries significantly distinct traditional multi-document summarization. describe primary characteristics
technical extractive summary present different types input texts used
production extractive summaries.
1.1.1 Technical Extractive Summaries
case multi-document summarization, goal produce readable presentation multiple documents, whereas case technical summary creation, goal
convey key features basic underpinnings particular field, early late
developments, important contributions findings, contradicting positions may reverse trends start new sub-fields, basic definitions examples enable rapid
understanding field non-experts.
prototypical example technical summary chapter notes, i.e., short
(50500 word) descriptions sub-areas found end chapters textbooks,
Jurafsky Martins (2008). One might imagine producing descriptions automatically, hand-editing refining use actual textbook.
Previously Mohammad et al. (2009) conducted human analysis chapter notes
revealed set conventions, outline provided (with example
sentences italics):
1. Introductory/opening statement: earliest computational use X Y, considered many foundational work area.
2. Definitional follow up: X defined Y.
3. Elaboration definition (e.g., example): early algorithms based
Z.
4. Deeper elaboration, e.g., pointing issues initial approaches: Unfortunately,
model seems wrong.
5. Contrasting definition: Algorithms since then...
6. Introduction additional specific instances / historical background citations:
Two classic approaches described Q.
7. References summaries: R provides comprehensive guide details behind
X.
notion text level categories zoning technical papersrelated summary
components enumerated abovehas investigated previously work Teufel
Moens (2002) Nanba, Kando, Okumura (2000). earlier works focused
167

fiQazvinian et Al.

analysis scientific papers based rhetorical structure determining
portions papers contain new results, comparisons earlier work, etc. work
described focuses synthesis technical summary based knowledge gleaned
rhetorical structure unlike work earlier researchers, guided
structural patterns along lines conventions listed above.
Although current approach summary creation yet incorporate fully
pattern-based component, ultimate objective apply patterns guide
creation refinement final output. first step toward goal, use citation
sentences (closest structure patterns identified convention 7 above) pick
important content summary creation.
1.1.2 Scholarly Texts
Published research particular topic summarized two different kinds
sources: (1) author describes work (2) others describe
authors work (usually relation work). authors description
work found paper. others perceive work spread across papers
cite work.
Traditionally, technical summary generation tackled summarizing set
research papers pertaining topic. However, individual research papers usually come
manually-created summariestheir abstracts. abstract paper may
sentences set context, state problem statement, mention problem
approached, bottom-line resultsall 200 500 words. Thus, using
abstracts (instead full papers) input summarization system worth exploring.
Whereas abstract paper presents authors think important
aspects paper, citations paper capture others field perceive
broader contributions paper. two perspectives expected
overlap content, citations also contain additional information found
abstracts (Elkiss, Shen, Fader, Erkan, States, & Radev, 2008; Nakov & Hearst, 2012).
example, authors may describe particular methodology one paper combined
another different paper overcome drawbacks each. Citations
also indicators contributions described paper influential time.
Another feature distinguishes citations abstracts citations tend
certain amount redundant information. multiple papers may describe
contributions target paper. redundancy exploited automatic
systems determine important contributions target paper.
goal test hypothesis effective technical summary reflect information research perspective authors also perspective
others use, commend, discredit, add it. describing experiments
technical papers, abstracts, citations, first summarize relevant prior work used
sources information input.
rest paper organized follows. reviewing related work,
present analysis citations demonstrate contain summary-amenable
information. process, develop C-LexRank, citation-based summarization system.
Section 5, show state-of-the-art automatic summarization systems create
168

fiGenerating Extractive Summaries Scientific Paradigms

contentful summaries citations individual documents created simply
random sampling. also show C-LexRank performs better state-of-theart summarization systems producing 100- 200-word extracts. Section 6,
extend experiments summarize set papers representing scientific
topic using source texts well citations topic papers. Additionally, show
usefulness citation sentences automatically generating technical summary
given topic. observe that, expected, abstracts useful summary creation, but,
notably, also conclude citations contain crucial information present (or
least, easily extractable from) abstracts. discover abstracts authorbiased thus complementary broader perspective inherent citation sentences;
differences enable use range different levels types information
summary.

2. Related Work
section, review related prior work two categories. First, review previous
research citation analysis, discuss prior work capturing diversity
automatic text summarization.
2.1 Citation Analysis
Previous work analyzed citation collaboration networks (Teufel, Siddharthan, &
Tidhar, 2006; Newman, 2001) scientific article summarization (Teufel & Moens, 2002).
Bradshaw (2002, 2003) benefited citations determine content articles
introduce Reference Directed Indexing improve results search engine. Nanba,
Abekawa, Okumura, Saito (2004) Nanba et al. (2000) analyzed citation sentences
automatically categorize citations three groups using 160 pre-defined phrase-based
rules. categorization used build tool help researchers analyze citations
write scientific summaries. Nanba Okumura (1999) also discussed citation
categorization support system writing survey. Nanba Okumura (1999)
Nanba et al. (2000) reported co-citation implies similarity showing textual
similarity co-cited papers proportional proximity citations citing
article.
Previous work shown importance citation sentences understanding
scientific contributions. Elkiss et al. (2008) performed large-scale study citations
importance. conducted several experiments set 2, 497 articles
free PubMed Central (PMC) repository2 66 ACM digital library. Results
experiment confirmed average cosine sentences set citations
article consistently higher abstract. also reported number
much greater average cosine citation sentences randomly chosen
document, well citation sentences abstract. Finally, concluded
content citing sentences much greater uniformity content
corresponding abstract, implying citations focused contain additional
information appear abstracts.
2. http://www.pubmedcentral.gov

169

fiQazvinian et Al.

Nakov Hearst (2012) performed detailed manual study citations area
molecular interactions found set citations given target paper cover
information found abstract article, well 20% concepts, mainly
related experimental procedures.
Kupiec, Pedersen, Chen (1995) used abstracts scientific articles target
summary. used 188 Engineering Information summaries mostly indicative
nature. Kan, Klavans, McKeown (2002) used annotated bibliographies cover
certain aspects summarization suggest guidelines summaries also include
metadata critical document features well prominent content-based features.
Siddharthan Teufel (2007) described new reference task show high human
agreement well improvement performance argumentative zoning (Teufel,
2005). argumentative zoninga rhetorical classification taskseven classes (Own, Other,
Background, Textual, Aim, Basis, Contrast) used label sentences according
role authors argument.
problem automatic related work summarization addressed Hoang Kan
(2010). work, Hoang Kan used set keywords representing hierarchy
paper topics assigned score input sentence construct extractive summary.
Athar (2011) addressed problem identifying positive negative sentiment polarity citations scientific papers. Similarly, Athar Teufel (2012) used context-enriched
citations classify scientific sentiment towards target paper.
2.2 Leveraging Diversity Summarization
summarization, number previous methods focused diversity perspectives. Mei, Guo, Radev (2010) introduced DivRank, diversity-focused ranking
methodology based reinforced random walks information networks. random
walk model, incorporates rich-gets-richer mechanism PageRank reinforcements transition probabilities vertices, showed promising results
Document Understanding Conference (DUC) 2004 dataset. DivRank state-of-the-art
graph-based method leverages diversity perspectives summarization. Therefore, chose algorithm important baseline experiments discuss
detail Section 4.
similar ranking algorithm, described Zhu, Goldberg, Van Gael, Andrzejewski
(2007), Grasshopper ranking model, leverages absorbing random walk.
model starts regular time-homogeneous random walk, step vertex
highest weight set absorbing state. Paul, Zhai, Girju (2010) addressed
problem summarizing opinionated text using Comparative LexRank, random walk model
inspired LexRank (Erkan & Radev, 2004). Comparative LexRank first assigns different
sentences clusters based contrastiveness other. modifies
graph based cluster information performs LexRank modified cosine similarity
graph.
Perhaps well-known summarization method address diversity summarization Maximal Marginal Relevance (MMR) (Carbonell & Goldstein, 1998). method
based greedy algorithm selects sentences step least similar
170

fiGenerating Extractive Summaries Scientific Paradigms

summary far. compare summarization output MMR
discuss algorithm details Section 4.
prior work evaluating independent contributions content generation, Voorhees
(1998) studied IR systems showed relevance judgments differ significantly
humans relative rankings show high degrees stability across annotators.
work, van Halteren Teufel (2004) asked 40 Dutch students 10 NLP researchers
summarize BBC news report, resulting 50 different summaries. also used 6
DUC-provided summaries, annotations 10 student participants 4 additional
researchers, create 20 summaries another news article DUC datasets.
calculated Kappa statistic (Carletta, 1996; Krippendorff, 1980) observed high agreement, indicating task atomic semantic unit (factoid) extraction robustly
performed naturally occurring text, without copy-editing.
diversity perspectives growth factoid inventory (Qazvinian &
Radev, 2011b) also affects evaluation text summarization. Evaluation methods either extrinsic, summaries evaluated based quality performing
specific task (Sparck-Jones, 1999) intrinsic quality summary
evaluated, regardless applied task (van Halteren & Teufel, 2003; Nenkova & Passonneau, 2004). evaluation methods assess information content summaries
generated automatically.

3. Citation-Based Summarization
ACL Anthology Network3 (AAN) manually curated resource built top
ACL Anthology4 (Bird, Dale, Dorr, Gibson, Joseph, Kan, Lee, Powley, Radev, & Tan,
2008). AAN includes papers published ACL related organizations well
Computational Linguistics journal period four decades. AAN consists
18, 000 papers 14, 000 authors, distinguished unique
ACL ID, together full-texts, abstracts, citation information. also includes
valuable metadata author affiliations, citation collaboration networks,
various centrality measures (Radev, Muthukrishnan, & Qazvinian, 2009; Joseph & Radev,
2007).
study citations across different areas within Computational Linguistics, first extracted six different sets papers AAN corresponding 6 different NLP topics: Dependency Parsing (DP), Phrase-based Machine Translation (PBMT), Text Summarization
(Summ), Question Answering (QA), Textual Entailment (TE), Conditional Random
Fields (CRF). build set, matched topic phrase title content AAN papers, extracted 5 highest cited papers. Table 1 shows number
articles number citation sentences topic5 . number citations
set shows number sentences used input various summarization
systems experiments.
3. http://clair.si.umich.edu/anthology/
4. http://www.aclweb.org/anthology-new/
5. number incoming citations AANs 2008 release.

171

fiCRF

TE

QA

Summ

PBMT

DP

Qazvinian et Al.

ACL ID
C96-1058
P97-1003
P99-1065
P05-1013
P05-1012
N03-1017
W03-0301
J04-4002
N04-1033
P05-1033
A00-1043
A00-2024
C00-1072
W00-0403
W03-0510
A00-1023
W00-0603
P02-1006
D03-1017
P03-1001
D04-9907
H05-1047
H05-1079
W05-1203
P05-1014
N03-1023
N04-1042
W05-0622
P06-1009
W06-1655

Title
Three New Probabilistic Models Dependency Parsing ...
Three Generative, Lexicalized Models Statistical Parsing
Statistical Parser Czech
Pseudo-Projective Dependency Parsing
On-line Large-Margin Training Dependency Parsers
Statistical Phrase-Based Translation
Evaluation Exercise Word Alignment
Alignment Template Approach Statistical Machine Translation
Improvements Phrase-Based Statistical Machine Translation
Hierarchical Phrase-Based Model Statistical Machine Translation
Sentence Reduction Automatic Text Summarization
Cut Paste Based Text Summarization
Automated Acquisition Topic Signatures ...
Centroid-Based Summarization Multiple Documents ...
Potential Limitations Automatic Sentence Extraction ...
Question Answering System Supported Information Extraction
Rule-Based Question Answering System Reading ...
Learning Surface Text Patterns Question Answering System
Towards Answering Opinion Questions: Separating Facts Opinions ...
Offline Strategies Online Question Answering ...
Scaling Web-Based Acquisition Entailment Relations
Semantic Approach Recognizing Textual Entailment
Recognising Textual Entailment Logical Inference
Measuring Semantic Similarity Texts
Distributional Inclusion Hypotheses Lexical Entailment
Weekly Supervised Natural Language Learning ...
Accurate Information Extraction Research Papers ...
Semantic Role Labelling Tree CRFs
Discriminative Word Alignment Conditional Random Fields
Hybrid Markov/Semi-Markov CRF Sentence Segmentation

Year
1996
1997
1999
2005
2005
2003
2003
2004
2004
2005
2000
2000
2000
2000
2003
2000
2002
2002
2003
2003
2004
2005
2005
2005
2005
2003
2004
2005
2006
2006

# citations
66
50
54
40
71
172
11
49
23
65
19
20
19
28
14
13
19
72
39
27
12
7
9
17
10
29
24
9
33
20

Table 1: Papers extracted 6 different NLP topics AAN: Dependency Parsing
(DP), Phrase-based Machine Translation (PBMT), Text Summarization (Summ),
Question Answering (QA), Textual Entailment (TE), Conditional Random
Fields (CRF). set consists 5 highest cited papers AANs 2008 release
whose title content matched corresponding topic phrase.

172

fiGenerating Extractive Summaries Scientific Paradigms

describe approach citation analysis, including calculation interjudge agreement. describe C-LexRank method extracting citation sentences.
3.1 Citation Analysis
analyze citations, designed annotation task requires explicit definitions
distinguish phrases represent different information units. Unfortunately, little consensus literature definitions. Therefore, following van
Halteren Teufel (2003), Qazvinian Radev (2011b) made following distinction. define nugget phrasal information unit (i.e., phrase would
contain information contributions cited paper). Different nuggets
may represent atomic semantic unit, refer factoid.
context citations, factoid refers unique contribution target paper mentioned
citation sentence. example, following set citations Eisners (1996) famous parsing paper illustrate set factoids paper suggest different
authors cite particular paper may discuss different contributions (factoids)
paper.
context DPs, edge based factorization method proposed Eisner (1996).
Eisner (1996) gave generative model cubic parsing algorithm based edge
factorization trees.
Eisner (1996) proposed O(n3 ) parsing algorithm PDG.
parse projective, Eisners (1996) bottom-up-span algorithm used
search.
example also suggests different authors use different wordings (nuggets)
represent factoids. instance, cubic parsing O(n3 ) parsing algorithm
two nuggets represent factoid (Eisner, 1996). similar example,
use throughout paper, paper Cohn Blunsom (2005) (identified
ACL ID W05-0622 Table 1). paper cited 9 different sentences within AAN.
sentences listed Table 2. sentence, nuggets extracted
annotators underlined. table suggests, citation sentence may discuss
contributions cited paper. instance, last sentence contain
factoids Cohn Blunsoms (2005) work. nuggets identified using
citation paper (Cohn & Blunsom, 2005) account total number 3 factoids
(contributions) identified paper: f1 , tree structures; f2 , semantic role labeling;
f3 , pipelined approach.
Following examples, asked two annotators background Natural Language Processing review citing sentence extract list phrases represent
contribution cited paper.6 Moreover, ensure extracted nuggets
explicitly mentioned citations, asked annotators rely merely set
citations task background topic source
6. One annotators author paper.

173

fiQazvinian et Al.

cited paper. Finally, reviewed list collapsed phrases represent
contribution (factoid).
Finding agreement annotated well-defined nuggets straightforward
calculated terms Kappa. However, nuggets extracted
annotators, task becomes less obvious. calculate agreement, annotated
5 randomly selected citation sets twice (1 paper NLP areas Table 1),
designed simple evaluation scheme based Kappa. n-gram, w, given
citation sentence, determine w part nugget either human annotations. w
occurs neither, two annotators agree it, otherwise not.
Based agreement setup, formalize statistic as:
=

Pr(a) Pr(e)
1 Pr(e)

(1)

P r(a) relative observed agreement among annotators, P r(e) probability annotators agree chance annotator randomly assigning categories.
Table 3 shows unigram, bigram, trigram-based two human annotators (Human1, Human2) five datasets annotated twice. results
suggest human annotators reach substantial agreement trigram nuggets
examined, reasonable agreement unigram bigram nuggets.
3.2 C-LexRank
section describe C-LexRank method extract citing sentences cover
diverse set factoids. method works modeling set citations network
sentences identifying communities sentences cover similar factoids.
good division sentences made, extract salient sentences different communities.
Figure 1 illustrates representative example depicts C-LexRanks process.
3.2.1 Citation Summary Network
first step (as shown Figure 1 (a)), model set sentences cite specific
paper network vertices represent citing sentences undirected weighted
edges show degree semantic relatedness vertex pairs, normally quantified
similarity measure. refer network Citation Summary Network
article. similarity function ideally assign high scores sentence pairs
factoids, assign low scores sentences talk different
contributions target paper.
Previously, Qazvinian Radev (2008) examined 7 different similarity measures including TF-IDF various IDF databases, longest common sub-sequence, generation
probability (Erkan, 2006), Levenstein distance training set citations.
showed cosine similarity measure employs TF-IDF vectors assigns higher similarities pairs contain factoids. Following Qazvinian Radev (2008),
use cosine similarity TF-IDF vector models employ general IDF corpus7
construct citation summary network article.
7. use IDF corpus Mead summarization system (Radev et al., 2004), generated using
English portion Hong Kong News parallel corpus (Ma, 2000).

174

fiGenerating Extractive Summaries Scientific Paradigms

1

factoid
f1

Citation Sentence
parsing model based conditional random field model, however,
unlike previous TreeCRF work, e.g., (Cohn & Blunsom, 2005; Jousse et
al., 2006), assume particular tree structure, instead find
likely structure labeling.

2

f3

researchers (Xue & Palmer, 2004; Koomen et al., 2005; Cohn & Blunsom, 2005; Punyakanok et al., 2008; Toutanova et al., 2005, 2008) used
pipelined approach attack task.

3

f2

used tree labelling, XML tree labelling (Jousse et al.,
2006) semantic role labelling tasks (Cohn & Blunsom, 2005).

4

f1

Finally, probabilistic models also applied produce structured output, example, generative models (Thompson, Levy, & Manning,
2003), sequence tagging classifiers (Marquez et al., 2005; Pradhan et al.,
2005), Conditional Random Fields tree structures (Cohn & Blunsom
2005).

5

f3

SRL news, researchers used pipelined approach, i.e., dividing task several phases argument identification, argument
classification, global inference, etc., conquering individually (Xue &
Palmer, 2004; Koomen et al., 2005; Cohn & Blunsom, 2005; Punyakanok
et al., 2008; Toutanova et al., 2005, 2008).

6

f1 , f2

Although T-CRFs relatively new models, already applied
several NLP tasks, semantic role labeling, semantic annotation, word
sense disambiguation, image modeling (Cohn & Blunsom, 2005; Tang et
al., 2006; Jun et al., 2009; Awasthi et al., 2007).

7

f2

model used tasks like syntactic parsing (Finkel et al., 2008)
semantic role labeling (Cohn & Blunsom, 2005).

8

f1

Regarding novel learning paradigms applied previous shared tasks,
find Relevant Vector Machine (RVM), kernel-based linear discriminant inside framework Sparse Bayesian Learning (Johansson & Nugues,
2005) Tree Conditional Random Fields (T-CRF) (Cohn & Blunsom,
2005), extend sequential CRF model tree structures.

9

N/A

use CRFs models tasks (Cohn & Blunsom, 2005).

Table 2: AAN paper W05-0622 CRF Cohn & Blunsom (2005) cited nine
different AAN sentences. citation sentence, nuggets extracted
annotators underlined.

175

fiQazvinian et Al.

Average
unigram bigram
Human1 vs. Human2
A00-1023
1.000
0.615
H05-1079
0.889
0.667
P05-1013
0.427
0.825
W03-0301
0.455
0.636
W05-0622
0.778
0.667
Average
0.710
0.682

trigram
0.923
0.556
0.975
0.818
0.778
0.810

Table 3: Agreement different annotators terms Kappa 5 citation sets.

(a) Citation summary network

(b) Community structure

(c) C-LexRank output

Figure 1: C-LexRank method extracts citing sentences cover diverse set factoids. citation summary network (a) models set sentences cite
specific paper, vertices represent citing sentences (weighted) edges
show degree semantic relatedness vertex pairs. community
structure (b) corresponds clustered sets representative sentences extracted
citation sentences. C-LexRank output (c) corresponds candidate sentences different clusters used building summary.

3.2.2 Community Structure
second step (as shown Figure 1 (b)), extract vertex communities
citation summary network generate summaries. generate summaries extracting
representative sentences citation summary network. Intuitively, good summary
include sentences represent different contributions paper. Therefore, good
sentence selection citation summary network include vertices similar
many vertices similar other. hand,
bad selection would include sentences representing small set vertices
graph. similar concept maximizing social influence social
networks (Kempe, Kleinberg, & Eva Tardos, 2003). Figure 2 shows example
selected two vertices citation summary networks represent small subset vertices
(left) larger subset vertices (right). work try select vertices
176

fiGenerating Extractive Summaries Scientific Paradigms

maximize size set vertices represent. achieve detecting
different vertex communities citation summary network.

(a) Bad sentence selection

(b) Good sentence selection

Figure 2: Summaries produced using vertex coverage select set representative
vertices corresponding sentences. Selecting two similar vertices cause
summary cover fewer contributions target paper (a), selecting
less similar vertices summary increase coverage summary
(b).

order find vertex communities thus good sentence selection, exploit
small-world property citation summary networks. network called small-world,
vertices neighbors other, reached one another
small number steps (Watts & Strogatz, 1998). Recent research shown
wide range natural graphs biological networks (Ravasz, Somera, Mongru, Oltvai,
& Barabasi, 2002), food webs (Montoya & Sole, 2002), brain neurons (Bassett & Bullmore,
2006) human languages (Ferrer Cancho & Sole, 2001) exhibit small-world property.
common characteristic detected using two basic statistical properties:
clustering coefficient C, average shortest path length `. clustering coefficient
graph measures number closed triangles graph. describes likely
two neighbors vertex connected (Newman, 2003). Watts Strogatz
(1998) define clustering coefficient average local clustering values
vertex.
Pn
ci
C = i=1
(2)
n
local clustering coefficient ci ith vertex number triangles connected
vertex divided total possible number triangles connected vertex i. Watts
Strogatz (1998) show small-world networks highly clustered obtain relatively short paths (i.e., ` small). Previous work (Qazvinian & Radev, 2011a) shows
citation summary networks highly clustered. networks small shortest paths
obtain clustering coefficient values significantly larger random networks.
Moreover, Qazvinian Radev suggest community structure,
177

fiQazvinian et Al.

{8} T-CRF
{8} T-CRF

{4} tree structures
{3} semantic role labeling

{4} tree structures

{1} TreeCRF
{9}

{7} semantic role labeling

{6} semantic role labeling; T-CRF

{2} pipelined approach
{6} semantic role labeling; T-CRF

{7} semantic role labeling
{3} semantic role labeling
{9}

{5} pipelined approach
{1} TreeCRF

{5} pipelined approach
{2} pipelined approach

Pajek

(a) Citation summary network

Pajek

(b) Latent community structure

{8} T-CRF

{8} T-CRF

{4} tree structures

{4} tree structures

{1} TreeCRF

{1} TreeCRF

{6} semantic role labeling; T-CRF

{6} semantic role labeling; T-CRF

{7} semantic role labeling
{3} semantic role labeling

{7} semantic role labeling
{3} semantic role labeling

{9}

{5} pipelined approach
{2} pipelined approach

{9}

{5} pipelined approach
{2} pipelined approach

Pajek

(c) Clustering output

Pajek

(d) C-LexRank ranking

Figure 3: C-LexRank algorithm operates follows Cohn Blunsoms (2005)
citation summary network: network (a), vertices citation sentences
(annotated nuggets Table 2), edge cosine similarity corresponding node pairs. (b) shows network
underlying structure captured C-LexRank (c). Finally, (d) shows
C-LexRank output vertex size proportional LexRank value
within cluster.

community composed set highly connected vertices small number
edges fall communities.
Figure 3 (a) illustrates real citation summary network built using citation sentences
Table 2 vertex labeled corresponding nugget. re178

fiGenerating Extractive Summaries Scientific Paradigms

arrangement vertices Figure 3 (b), becomes clear citation summary
network paper underlying community structure sentences cover
similar factoids closer form communities. instance, network
least 3 observable communities: one f1 : tree structure, one
f2 : semantic role labeling last one f3 : pipelined approach
proposed Cohn Blunsom (2005).
order detect communities automatically use modularity. Modularity,
(Newman, 2004a), measure evaluate divisions community detection algorithm generates. division g groups, define matrix egg whose component
eij fraction edges original network connect vertices components i, j.
modularity Q defined as:
X
X
Q=
eii
eij eki
(3)


ijk

Intuitively, Q fraction edges embedded within communities minus
expected value quantity network degrees
edges placed random regardless community structure. Newman Girvan
(2004) Newman (2004b) showed across wide range simulated real-world networks larger Q values correlated better graph clusterings. also shown
Newman (2004b) edges exist connect vertices across different clusters
Q = 1, conversely number inter-cluster edges better random
Q = 0. work (Smyth & White, 2005) showed empirically modularity works
well practice terms (a) finding good clusterings vertices networks
community structure evident, (b) indicating appropriate number clusters
k graph.
C-LexRank uses clustering algorithm Clauset, Newman, Moore (2004),
exploits modularity detect vertex communities network. network clustering
method, discussed Clauset et al. (2004) hierarchical agglomeration algorithm,
works greedily optimizing modularity linear running time sparse
graphs. particularly, method continuously merges vertex cluster pairs
highest similarity stops modularity reaches maximum value. clustering
algorithm efficient (O(n log2 n) number nodes, n) require predetermined number clusters. two characteristics makes community detection
algorithm particularly useful.
Figure 3 (c) shows clustering algorithm detects factoid communities Cohn
Blunsoms (2005) citation summary network. figure, color-coded vertices
based community. clustering algorithm assigns sentences 1, 4 8 (which
tree structures) one cluster; sentences 3, 6 7 (which
semantic role labeling) another cluster; finally assigns sentences 2, 5 9 (sentences
2 5 pipelined approach) last cluster. figure also shows
sentence 6, discusses two factoids (semantic role labeling T-CRF) connects
two vertex communities (corresponding 2 factoids) bridge.
evaluate well clustering method works datasets, calculated
purity normalized mutual information (NMI) divisions
citation set, extracted using community detection algorithm. Purity (Zhao & Karypis,
179

fiQazvinian et Al.

2001) method cluster assigned class majority vote
cluster, accuracy assignment measured dividing number
correctly assigned documents N . formally:
purity(, C) =

1 X
max |k cj |
j
N

(4)

k

= {1 , 2 , . . . , K } set clusters C = {c1 , c2 , . . . , cJ } set
classes. k interpreted set documents cluster k cj set
documents class cj .
also calculate normalized mutual information (NMI). Manning, Raghavan,
Schutze (2008) describe NMI follows. Let us assume = {1 , 2 , . . . , K } set
clusters C = {c1 , c2 , . . . , cJ } set classes. Then,
NMI(, C) =

I(; C)
[H() + H(C)]/2

(5)

I(; C) mutual information:
I(, C) =

XX
k

=

P (k cj ) log

j

X X |k cj |
k

N

j

log

P (k cj )
P (k )P (cj )

N |k cj |
|k ||cj |

(6)
(7)

P (k ), P (cj ), P (k cj ) probabilities document cluster
k , class cj , intersection k cj , respectively; H entropy:
X
H() =
P (k ) log P (k )
(8)
k

=

X |k |
k

N

log

|k |
N

(9)

I(; C) Equation 6 measures amount information would lose
classes without cluster assignments. normalization factor ([H() + H(C)]/2)
Equation 5 enables us trade quality clustering number
clusters, since entropy tends increase number clusters. example, H()
reaches maximum document assigned separate cluster. NMI
normalized, use compare cluster assignments different numbers clusters.
Moreover, [H() + H(C)]/2 tight upper bound I(; C), making NMI obtain values
0 1. Table 4 lists average Purity NMI across papers collected
dataset, along analogous numbers division size vertices
randomly assigned clusters.
3.2.3 Ranking
third step C-LexRank process (as shown Figure 1 (c)) applied
graph clustered communities formed. produce C-LexRank output,
180

fiGenerating Extractive Summaries Scientific Paradigms

purity(, C)
purity(random , C)
NMI(, C)
NMI(random , C)

average
0.461
0.389
0.312
0.182

95% Confidence Interval
[0.398, 0.524]
[0.334, 0.445]
[0.251, 0.373]
[0.143, 0.221]

Table 4: average purity (in boldface) normalized mutual information (NMI) values
shown papers collected dataset, along analogous values
division size vertices randomly assigned clusters.

extract sentences different clusters build summary. start largest
cluster extract sentences using LexRank (Erkan & Radev, 2004) within cluster.
words, cluster made lexical network sentences cluster
(Ni ). Using LexRank find central sentences Ni salient sentences
include main summary. choose, cluster , salient sentence
, reached summary length limit, second
salient sentences cluster, on. cluster selection order decreasing
size. Figure 3 (d) shows Cohn Blunsoms (2005) citation summary network,
vertex plotted size proportional LexRank value within cluster.
figure shows C-LexRank emphasizes selecting diverse set sentences covering
diverse set factoids.
Previously, mentioned factoids higher weights appear greater number
sentences, clustering aims cluster fact-sharing sentences communities. Thus, starting largest community important ensure system
summary first covers factoids frequently mentioned citation
sentences thus important.
last sentence example Table 2 follows. use CRFs models
tasks (Cohn & Blunsom, 2005). sentence shows citation may
cover contributions target paper. sentences assigned community
detection algorithm C-LexRank clusters semantically similar.
intuition behind employing LexRank within cluster try avoid extracting
sentences summary, since LexRank within cluster enforces extracting
central sentence cluster. order verify this, also try variant C-LexRank
select sentences clusters based salience cluster,
rather round-robin fashion, sentences within cluster equally likely
selected. call variant C-RR.
Table 5 shows 100-word summary constructed using C-LexRank exemplar
paper, different nuggets illustrated bold. summary perfect summary
terms covering different factoids paper. includes citing sentences
talk tree CRF, pipelined approach, Semantic Role Labeling,
indeed Cohn Blunsoms (2005) three main contributions.
181

fiQazvinian et Al.

parsing model based conditional random field model, however, unlike
previous TreeCRF work, e.g., (Cohn & Blunsom, 2005; Jousse et al., 2006),
assume particular tree structure, instead find likely structure
labeling.
researchers (Xue & Palmer, 2004; Koomen et al., 2005; Cohn & Blunsom,
2005; Punyakanok et al., 2008; Toutanova et al., 2005, 2008) used pipelined
approach attack task.
model used tasks like syntactic parsing (Finkel et al., 2008)
Semantic Role Labeling (Cohn & Blunsom, 2005).
Table 5: 100-word summary constructed using C-LexRank Cohn Blunsoms (2005) citation summary network. Factoids shown bold face.

4. Methods
experiments Section 5 compare C-LexRank number summarization
systems. compare C-LexRank random summarization find lower-bound
pyramid scores experiments. use LexRank C-RR two variants CLexRank investigate usefulness community detection salient vertex selection
C-LexRank. evaluate DivRank state art graph-based summarization system
leverages diversity well MMR widely used diversity-based summarization
system. Finally, use Multiple Alternate Sentence Compression Summarizer (MASCS)
system rely merely extraction, rather produces list candidates
applying pre-defined sentence-compression rules.
4.1 Random
method simply chooses citations random order without replacement. Since citing
sentence may cover information cited paper (as last sentence Table 2),
randomization drawback selecting citations valuable information
them. Moreover, random selection procedure prone produce redundant
summaries citing sentences discuss factoid may selected.
4.2 LexRank
One systems compare C-LexRank LexRank (Erkan & Radev, 2004).
works first building graph documents (di ) cluster. edges
corresponding vertices (di ) represent cosine similarity cosine value
threshold (0.10 following Erkan & Radev, 2004). network built,
system finds central sentences performing random walk graph.

p(dj ) = (1 )

X
1
+
p(di )P (di dj )
|D|
di

182

(10)

fiGenerating Extractive Summaries Scientific Paradigms

Equation 10 shows probability random walker would visit dj depends
random jump element well probability random walk visits
neighbors (di ) times transition probability di dj , P (di dj ).
Comparing C-LexRank summaries ones LexRank gives insight
benefit detecting communities citation sets. Essentially, C-LexRank
LexRank vertices assigned cluster. construction,
C-LexRank produce diverse summaries covering different perspectives capturing communities sentences discuss factoids.
4.3 MMR
Maximal Marginal Relevance (MMR) proposed Carbonell Goldstein (1998)
widely used algorithm generating summaries reflect diversity perspectives
source documents (Das & Martins, 2007). MMR uses pairwise cosine similarity
matrix greedily chooses sentences least similar already
summary. particular,
h

R = argminDi DA max Sim(Di , Dj )
Dj

(11)

set documents summary, initialized = . Equation 11,
sentence Di summary chosen highest similarity
summary sentences maxDj Sim(Di , Dj ) minimum among unselected sentences.
4.4 DivRank
also compare C-LexRank state-of-the-art graph-based summarization system
leverages diversity, DivRank. DivRank based calculating stationary distribution
vertices using modified random walk model. Unlike time-homogeneous random
walks (e.g., PageRank), DivRank assume transition probabilities remain
constant time.
DivRank uses vertex-reinforced random walk model rank graph vertices based
diversity based centrality. basic assumption DivRank transition probability vertex reinforced number previous visits target
vertex (Mei et al., 2010). Particularly, let us assume pT (u, v) transition probability
vertex u vertex v time . Then,
pT (di , dj ) = (1 ).p (dj ) + .

p0 (di , dj ).NT (dj )
DT (di )

(12)

NT (dj ) number times walk visited dj time and,
DT (di ) =

X

p0 (di , dj )NT (dj )

(13)

dj V

Here, p (dj ) prior distribution determines preference visiting vertex
dj , p0 (u, v) transition probability u v prior reinforcement. Mei et
al. argue random walk could stay current state time, therefore
183

fiQazvinian et Al.

assumes hidden link vertex itself, thus defining p0 (u, v) as:
(
. w(u,v)
u 6= v
deg(u)
p0 (u, v) =
1
u = v

(14)

Here, try two variants algorithm: DivRank, p (dj ) uniform,
DivRank priors p (dj ) l(Dj ) , l(Dj ) number words
document Dj parameter (0.1 experiments). prior distribution
assigns larger probabilities shorter sentences increase likelihood
salient. cause sentences included summary, might increase
factoid coverage. experiments, follow Mei et al. (2010) set = 0.90
= 0.25.
4.5 MASCS
last summarization system use baseline Multiple Alternate Sentence
Compression Summarizer (MASCS) (Zajic, Dorr, Lin, & Schwartz, 2007). Similar previous previous baseline systems, MASCSs goal leverage diversity summarization.
MASCS performs preprocessing sentences transforms new sentences,
thus expanding pool candidates available inclusion summary beyond
set sentences occur source documents. makes MASCS somewhat
non-extractive. addition, preprocessing used MASCS experiments
specifically adapted genre citation sentences scientific papers (Whidby, 2012).
particularly, MASCS summarization system utilizes Trimmers (Zajic
et al., 2007) sentence compression candidates create summaries single set
documents. Summarization MASCS performed three stages. first stage,
MASCS generates several compressed sentence candidates every sentence document
cluster. second stage involves calculating various ranking features
compressed sentence candidates. final stage, sentence candidates chosen
inclusion summary, chosen based linear combination features.
Trimmer leverage output constituency parser uses Penn Treebank
conventions. present, Stanford Parser (Klein & Manning, 2003) used. set
compressions ranked according set features may include metadata
source sentences, details compression process generated compression,
externally calculated features compression.
Summaries constructed iteratively adding compressed sentences candidate pool length threshold met. Candidates chosen implementation
MMR uses features directly calculated candidate, metadata candidates
source compression history, relevance candidate topic redundancy
candidate already selected candidates. redundancy score MASCS uses
index words (w) document set:
X

log(.P (w|summary) + (1 ).P (w|corpus))

w

weighting factor (set 0.3 experiments).
184

(15)

fiGenerating Extractive Summaries Scientific Paradigms

5. Experiments
used 30 sets citations listed Table 1 employ C-LexRank produce two
extractive summaries different summary lengths (100 200 words) set.
addition C-LexRank C-RR, also performed experiments
baseline methods described Section 4, aimed leveraging diversity
summarization.
5.1 Evaluation
evaluate system, use pyramid evaluation method (Nenkova & Passonneau,
2004). factoid citations paper corresponds summary content unit
(SCU) (Nenkova & Passonneau, 2004).
score given pyramid method summary ratio sum
weights factoids sum weights optimal summary. score ranges
0 1, high scores show summary content contains heavily weighted
factoids. factoid appears citation sentences another factoid,
important, thus assigned higher weight. weight factoids build
pyramid, factoid falls tier. tier shows number sentences factoid
appears in. Thus, number tiers pyramid equal citation summary size.
factoid appears sentences, falls higher tier. So, factoid fi appears
|fi | times citation summary assigned tier T|fi | .
pyramid score formula use computed follows. Suppose pyramid
n tiers, Ti , tier Tn top T1 bottom. weight factoids
tier Ti (i.e. appeared sentences). |Ti | denotes number factoids
tier Ti , Di number factoids summary appear Ti , total
factoid weight summary follows.
D=

n
X

Di

(16)

i=1

Additionally, optimal pyramid score summary X factoids,
ax =

n
X

|Ti | + j (X

i=j+1

n
X

|Ti |)

(17)

i=j+1

P
j = maxi ( nt=i |Tt | X). Subsequently, pyramid score summary calculated follows.

P =
(18)
ax
5.2 Results Discussion
Table 6 shows average pyramid score summaries generated using different methods
different lengths. Longer summaries result higher pyramid scores since amount
information cover greater shorter summaries. random sentence
extraction baseline, repeat experiment 100 different randomly generated seed
values report average pyramid score summaries Table 6. table shows
185

fiQazvinian et Al.

Method
Random
MMR
LexRank
DivRank
DivRank (with priors)
MASCS
C-RR
C-LexRank
C.I.=Confidence Interval

Length: 100 words
pyramid
95% C.I.
0.535
[0.526,0.645]
0.600
[0.501,0.699]
0.604
[0.511,0.696]
0.644
[0.580,0.709]
0.632
[0.545,0.719]
0.571
[0.477,0.665]
0.513
[0.436,0.591]
0.647
[0.565,0.730]

Length: 200 words
pyramid
95% C.I.
0.748
[0.740,0.755]
0.761
[0.685,0.838]
0.784
[0.725,0.844]
0.769
[0.704,0.834]
0.778
[0.716,0.841]
0.772
[0.705,0.840]
0.755
[0.678,0.832]
0.799
[0.732,0.866]

Table 6: Average pyramid scores shown two different summary lengths (100 words
200 words) eight different methods, including summary generator based
random citation sentence selection. C-LexRank outperforms methods
leverage diversity well random summaries LexRank. Highest scores
input source shown bold.

C-LexRank outperforms methods leverage diversity well random
summaries LexRank. results table also suggest employing LexRank
within cluster essential selection salient citing sentences, average
pyramid scores C-RR, sentences picked round-robin fashion, lower.
5.2.1 Effect Community Detection
community detection C-LexRank employs assigns highly similar citing sentences
cluster. enables C-LexRank produce diverse summary selecting
sentences different clusters. selection done assigning score vertex
using LexRank within cluster. modularity-based clustering method described
Section 3.2.2, works maximizing modularity clustering, always produce
least 2 clusters. Intuitively, network vertices assigned
community, fraction edges embedded within community equal
expected value quantity network edges placed random.
make Q obtain lower-bound, Q = 0.
However, hypothetical case vertices belong cluster, CLexRank LexRank (i.e., perform LexRank entire network).
Therefore, comparing C-LexRank LexRank helps us understand effect clustering
summary quality. Table 6 shows C-LexRank produces summaries obtain
higher pyramid scores 100 words 200 words. Table 7 shows 100-word
summary constructed using LexRank Cohn Blunsoms (2005) citations.
summary, unlike one produced C-LexRank (Table 5), cover
factoids target paper (e.g., pipelined approach). Moreover, summary
redundant information (e.g., TreeCRF vs. T-CRF) contains citation sentence
cover contributions Cohn Blunsom.
186

fiGenerating Extractive Summaries Scientific Paradigms

model used tasks like syntactic parsing (Finkel et al., 2008)
semantic role labeling (Cohn & Blunsom, 2005).
use CRFs models tasks (Cohn & Blunsom, 2005).
parsing model based conditional random field model, however, unlike
previous TreeCRF work, e.g., (Cohn & Blunsom, 2005; Jousse et al., 2006),
assume particular tree structure, instead find likely structure
labeling.
Although T-CRFs relatively new models, already applied
several NLP tasks, semantic role labeling, semantic annotation, word
sense disambiguation, image modeling (Cohn & Blunsom, 2005; Tang et al., 2006;
Jun et al., 2009; Awasthi et al., 2007).
Table 7: summary constructed using LexRank Cohn Blunsoms (2005) citation
sentences. Compared C-LexRank summary (in Table 5), LexRank
produce summary Cohn Blunsoms (2005) contributions (The summary
truncated clarity).

5.2.2 Salient Vertex Extraction
Selecting representative sentences (vertices) different clusters done using LexRank
C-LexRank algorithm. particularly, cluster, C-LexRank first extracts
subgraph network representing vertices edges cluster, employs
LexRank assign salience score vertex. alternative idea could selecting
sentences clusters random (C-RR). C-RR, traverse clusters
round-robin fashion randomly select previously unselected sentence cluster
include summary.
Comparing C-LexRank C-RR enables us understand effect salience selection within communities. Selecting vertices good representative cluster
may result picking sentences cover contributions target paper (e.g.,
sentence 9 Table 2 vertex 9 Figure 3 (d)). fact, Table 6 shows C-LexRank
produces summaries relatively 20% 5% higher pyramid scores C-RR
extracting 100 200 word summaries respectively. Moreover, C-RR performs better
longer summaries produced since extracts greater number sentences
cluster increasing likelihood covering different factoids captured different clusters.

6. Summaries Scientific Topics
previous sections, described C-LexRank method identify communities
citations discuss scientific contributions. showed C-LexRank
effective summarizing contributions single scientific papers. However, ultimate
goal work investigate whether citations summary-amenable information
also build end-to-end system receives query representing scientific topic
(such dependency parsing) produces citation-based automatic summary
given topic.
187

fiQazvinian et Al.

section, extend experiments using tools explained previous
sections automatic summarization scientific topics. evaluation experiments
extractive summary generation set papers research area Question
Answering (QA) another set papers Dependency Parsing (DP). two sets
papers compiled selecting papers AAN words Question
Answering Dependency Parsing, respectively, title content.
10 papers QA set 16 papers DP set. also compiled citation
sentences 10 QA papers citation sentences 16 DP papers.
6.1 Data Preparation
goal determine citations indeed useful information one want
put summary so, much information available original papers abstracts. evaluate automatically generated
summaries using two separate approaches: nugget-based pyramid evaluation ROUGE.
Recall-Oriented Understudy Gisting Evaluation (ROUGE) metric evaluates
automatic summaries comparing set human written references (Lin,
2004).
Two sets gold standard data manually created QA DP citation
sentences abstracts, respectively:8 (1) asked three annotators9 background
Natural Language Processing identify important nuggets information worth including
summary. (2) asked four NLP researchers10 write 250-word summaries
QA DP datasets. determined well different automatically generated
summaries perform gold standards. citations contain redundant information respect abstracts original papers, summaries citations
perform better others.
6.1.1 Nugget Annotations
first evaluation approach used nugget-based evaluation methodology (Voorhees,
2003; Nenkova & Passonneau, 2004; Hildebrandt, Katz, & Lin, 2004; Lin & DemnerFushman, 2006). asked three annotators background Natural Language Processing review citation sentences and/or abstract sets papers QA
DP sets manually extract prioritized lists 28 factoids, main contributions,
supplied paper. factoid assigned weight based frequency
listed annotators well priority assigned case.
automatically generated summaries scored based number weight
nuggets covered.
particularly, annotators two distinct tasks QA set, one
DP set: (1) extract nuggets 10 QA papers, based citations
papers; (2) extract nuggets 10 QA papers, based abstracts
papers; (3) extract nuggets 16 DP papers, based
citations papers.
8. Creating gold standard data complete papers fairly arduous, pursued.
9. Two annotators authors paper.
10. annotators authors paper.

188

fiGenerating Extractive Summaries Scientific Paradigms

Human Performance: Pyramid score
Human1 Human2 Human3 Human4 Average
Input: QA citations
QACT nuggets
0.524
0.711
0.468
0.695
0.599
QAAB nuggets
0.495
0.606
0.423
0.608
0.533
Input: QA abstracts
QACT nuggets
0.542
0.675
0.581
0.669
0.617
QAAB nuggets
0.646
0.841
0.673
0.790
0.738
Input: DP citations
DPCT nuggets
0.245
0.475
0.378
0.555
0.413
Table 8: Pyramid scores computed human-created summaries QA DP data.
summaries evaluated using nuggets drawn QA citation sentences
(QACT), QA abstracts (QAAB), DP citation sentences (DPCT).

One annotator completed three tasks full remaining two annotators
jointly completed tasks 1 3, providing us two complete annotations QA
DP citation sets one annotation QA abstract set. task, annotators
constructed lists 28 prioritized nuggets per paper. gave us 81 distinct nuggets
QA citation set, 45 nuggets QA abstract set, 144 nuggets DP
citation set. collapsing similar nuggets, able identify 34 factoids QA
citation set, 27 factoids QA abstract set, 57 factoids DP citation set.
obtained weight factoid reversing priority 8 (e.g., factoid listed
priority 1 assigned weight 8, nugget listed priority 2 assigned
weight 7, etc.) summing weights listing factoid.11
6.1.2 Expert Summaries
addition nugget annotations, asked four NLP researchers write 250-word summaries QA citation set, QA abstract set DP citation set.
Table 8 gives pyramid scores 250-word summaries manually produced
experts. summaries evaluated using nuggets drawn QA citations, QA
abstracts, DP citations. average scores (listed rightmost column)
may considered good score aim automatic summarization methods.
Additionally, Table 9 presents ROUGE scores (Lin, 2004) expert-written 250word summaries (e.g., Human1 versus others forth).
average (last column) could considered ceiling performance automatic
summarization systems.

11. Results obtained weighting schemes ignored priority ratings multiple mentions
nugget single annotator showed trends ones shown selected weighting
scheme.

189

fiQazvinian et Al.

Human Performance: ROUGE-2
human1 human2 human3 human4 average
Input: QA citations
QACT refs.
0.181
0.196
0.076
0.202
0.163
QAAB refs.
0.112
0.140
0.071
0.158
0.120
Input: QA abstracts
QACT refs.
0.131
0.110
0.122
0.115
0.120
QA-AB refs.
0.265
0.198
0.180
0.254
0.224
Input: DP citations
DPCT refs.
0.155
0.126
0.120
0.165
0.142
Table 9: ROUGE-2 scores obtained manually created summaries
using three reference. ROUGE-1 ROUGE-L followed similar
patterns.

6.2 Automatic Extractive Summaries
used four summarization systems summary-creation approach: C-LexRank, CRR, LexRank MASCS.
automatically generated summaries QA DP three different types
documents: (1) full papers QA DP setsQA DP full papers (PA),
(2) abstracts QA DP papersQA DP abstracts (AB), (3)
citation sentences corresponding QA DP papersQA DP citations (CT).
generated 24 (4 3 2) summaries, length 250 words, applying MASCS,
LexRank, C-LexRank three data types (citation sentences, abstracts, full
papers) QA DP. (Table 10 shows fragment one automatically
generated summaries QA citation sentences.) created six (3 2) additional 250word summaries randomly choosing sentences citations, abstracts, full papers
QA DP. refer random summaries.
work QA paraphrasing focused folding paraphrasing knowledge
question analyzer answer locater (Rinaldi et al., 2003; Tomuro, 2003).
addition, number researchers built systems take reading comprehension
examinations designed evaluate childrens reading levels (Charniak et al., 2000;
Hirschman et al., 1999; Ng et al., 2000; Riloff & Thelen, 2000; Wang et al., 2000).
So-called definition questions recent TREC evaluations (Voorhees,
2005) serve good examples.
better facilitate user information needs, recent trends QA research shifted
towards complex, context-based, interactive question answering (Voorhees,
2001; Small et al., 2003; Harabagiu et al., 2005).
Table 10: fragment one MASCS-generated summaries illustrated using
QA citation sentences input.

190

fiGenerating Extractive Summaries Scientific Paradigms

System Performance: Pyramid score
Random C-LexRank C-RR LexRank MASCS
Input: QA citations
QACT nuggets
0.321
0.434
0.268
0.295
0.616
QAAB nuggets
0.305
0.388
0.349
0.320
0.543
Input: QA abstracts
QACT nuggets
0.452
0.383
0.480
0.441
0.404
QAAB nuggets
0.623
0.484
0.574
0.606
0.622
Input: QA full papers
QACT nuggets
0.239
0.446
0.299
0.190
0.199
QAAB nuggets
0.294
0.520
0.387
0.301
0.290
Input: DP citations
DPCT nuggets
0.219
0.231
0.170
0.372
0.136
Input: DP abstracts
DPCT nuggets
0.321
0.301
0.263
0.311
0.312
Input: DP full papers
DPCT nuggets
0.032
0.000
0.144
*
0.280
Table 11: Pyramid scores computed automatic summaries QA DP data.
summaries evaluated using nuggets drawn QA citation sentences (QA
CT), QA abstracts (QAAB), DP citation sentences (DPCT). LexRank
computationally intensive run DP-PA dataset, indicated
* (about 4000 sentences). Highest scores input source shown
bold.

Table 11 gives pyramid score values summaries generated four automatic summarizers, evaluated using nuggets drawn QA citation sentences, QA
abstracts, DP citation sentences. table also includes results baseline random
summaries.
used nuggets abstracts set evaluation, summaries created
abstracts scored higher corresponding summaries created citations
papers. Further, best summaries generated citations outscored best summaries
generated papers. used nuggets citation sets evaluation, best
automatic summaries generated citations outperform generated abstracts
full papers. pyramid results demonstrate citations contain useful
information available abstracts original papers, abstracts
contain useful information available citations full papers.
Among various automatic summarizers, MASCS performed best task,
two cases exceeding average human performance. Note also random summarizer outscored automatic summarizers cases nuggets taken
source different used generate summary. However, one two summarizers
still tended well. indicates difficulty extracting overlapping summaryamenable information across two sources.
191

fiQazvinian et Al.

System Performance: ROUGE-2
Random C-LexRank C-RR LexRank MASCS
Input: QA citations
QACT refs.
0.116
0.170
0.095
0.135
0.170
QAAB refs.
0.083
0.117
0.076
0.070
0.103
Input: QA abstracts
QACT refs.
0.045
0.059
0.061
0.054
0.041
QAAB refs.
0.121
0.136
0.122
0.203
0.134
Input: QA full papers
QACT refs.
0.030
0.036
0.036
0.282
0.040
QAAB refs.
0.046
0.059
0.050
0.105
0.075
Input: DP citations
DPCT refs.
0.107
0.132
0.087
0.049
0.101
Input: DP abstracts
DPCT refs.
0.070
0.073
0.053
0.203
0.072
Input: DP full papers
DPCT refs.
0.038
0.025
0.034
*
0.046
Table 12: ROUGE-2 scores automatic summaries QA DP data. summaries
evaluated using human references created QA citation sentences
(QACT), QA abstracts (QAAB), DP citation sentences (DPCT).
results obtained Jack-knifing human references values
compared Table 4. LexRank computationally intensive
run DP full papers set, indicated * (about 4,000 sentences).
Highest scores input source shown bold.

192

fiGenerating Extractive Summaries Scientific Paradigms

evaluated random summaries generated four
summarization systems references. Table 12 lists ROUGE scores summaries
manually created 250-word summary QA citation sentences, summary
QA abstracts, summary DP citation sentences, used gold standard.
use manually created citation summaries reference, summaries
generated citations obtained significantly better ROUGE scores summaries
generated abstracts full papers (p < 0.05) [Result 1]. confirms crucial information, amenable creating summary present citation sentences
available, hard extract, abstracts papers alone. Further, summaries
generated abstracts performed significantly better generated full
papers (p < 0.05) [Result 2]. suggests abstracts citations generally
denser summary-amenable information full papers.
use manually created abstract summaries reference, summaries
generated abstracts obtained significantly better ROUGE scores summaries
generated citations full papers (p < 0.05) [Result 3]. Further, importantly, summaries generated citations performed significantly better
generated full papers (p < 0.05) [Result 4]. Again, suggests abstracts
citations richer summary-amenable information. results also show
abstracts papers citations overlapping information (Result 2 Result 4), also significant amount unique summary-amenable information
(Result 1 Result 3).
Among automatic summarizers, C-LexRank LexRank perform best.
unlike results found nugget-evaluation method, MASCS performed
best. suggests MASCS better identifying useful nuggets information,
C-LexRank LexRank better producing unigrams bigrams expected
summary. extent may due MASCSs compression preprocessing,
breaks large, complex sentences smaller, finer-grained units content correspond
better amount content nugget.

7. Conclusion
paper, investigated usefulness directly summarizing citation sentences
(set sentences cite paper) automatic creation technical summaries.
proposed C-LexRank, graph-based summarization model generated summaries
30 single scientific articles selected 6 different topics ACL Anthology Network
(AAN). also generated summaries set Question Answering (QA) Dependency
Parsing (DP) papers, abstracts, citation sentences using four state-of-theart summarization systems (C-LexRank, C-RR, LexRank, MASCS). used two
different approaches, nugget-based pyramid ROUGE, evaluate summaries.
results approaches four summarization systems show citation
sentences abstracts unique summary-amenable information. results also
demonstrate multidocument summarizationespecially technical summary creation
benefits considerably citations.
next plan generate summaries using citation sentences abstracts together
input. Given overlapping content abstracts citation sentences, discovered
193

fiQazvinian et Al.

current study, clear redundancy detection integral component
future work. Creating readily consumable technical summaries hard task, especially
using raw text simple summarization techniques. Therefore, intend
combine summarization bibliometric techniques suitable visualization methods towards creation iterative technical survey toolssystems present summaries
bibliometric links visually convenient manner incorporate user feedback
produce even better summaries.
Current work generating topic summaries focused almost exclusively extracting
diverse factoid-rich summaries. Meanwhile, fluency produced summaries
mostly ignored. future work, plan employ post-processing techniques
reference scope extraction sentence simplification, described Abu-Jbara
Radev (2011), generate readable cohesive summaries.

8. Acknowledgments
would like thank Ahmed Hassan, Rahul Jha, Pradeep Muthukrishan, Arzucan
Ozgur annotations Melissa Egan preliminary developments. also grateful
Ben Shneiderman, Judith Klavans, Jimmy Lin fruitful discussions anonymous reviewers insightful readings constructive guidance. following authors,
Vahed Qazvinian, Dragomir R. Radev, Saif M. Mohammad, Bonnie Dorr, David Zajic,
Michael Whidby supported, part, National Science Foundation
Grant No. IIS-0705832 (iOPENER: Information Organization PENning Expositions
Research) awarded University Michigan University Maryland.
opinions, findings, conclusions recommendations expressed material
authors necessarily reflect views National Science Foundation.
following authors, Michael Whidby Taesun Moon supported, part,
Intelligence Advanced Research Projects Activity (IARPA) via Department Interior National Business Center (DoI/NBC) contract number D11PC20153. U.S. Government
authorized reproduce distribute reprints Governmental purposes withstanding copyright annotation thereon. Disclaimer: views conclusions contained
herein authors interpreted necessarily representing
official policies endorsements, either expressed implied, IARPA, DoI/NBC,
U.S. Government.

References
Abu-Jbara, A., & Radev, D. (2011). Coherent citation-based summarization scientific
papers. Proceedings 49th Annual Conference Association Computational Linguistics (ACL-11), pp. 500509.
Athar, A. (2011). Sentiment analysis citations using sentence structure-based features.
Proceedings ACL 2011 Student Session, HLT-SS 11, pp. 8187.
Athar, A., & Teufel, S. (2012). Context-enhanced citation sentiment detection. Proceedings 2012 Conference North American Chapter Association
Computational Linguistics: Human Language Technologies, pp. 597601, Montreal,
Canada. Association Computational Linguistics.
194

fiGenerating Extractive Summaries Scientific Paradigms

Awasthi, P., Gagrani, A., & Ravindran, B. (2007). Image modeling using tree structured
conditional random fields. Proceedings 20th international joint conference
Artifical intelligence, IJCAI07, pp. 20602065.
Bassett, D. S., & Bullmore, E. (2006). Small-world brain networks. neuroscientist,
12 (6), 512523.
Bird, S., Dale, R., Dorr, B. J., Gibson, B., Joseph, M., Kan, M.-Y., Lee, D., Powley, B.,
Radev, D. R., & Tan, Y. F. (2008). acl anthology reference corpus: reference
dataset bibliographic research computational linguistics. Proceedings
International Conference Language Resources Evaluation, LREC 2008, 26
May - 1 June 2008, Marrakech, Morocco.
Bradshaw, S. (2002). Reference Directed Indexing: Indexing Scientific Literature
Context Use. Ph.D. thesis, Northwestern University.
Bradshaw, S. (2003). Reference directed indexing: Redeeming relevance subject search
citation indexes. Proceedings 7th European Conference Research
Advanced Technology Digital Libraries.
Carbonell, J. G., & Goldstein, J. (1998). use MMR, diversity-based reranking
reordering documents producing summaries. Proceedings 21st Annual
International ACM SIGIR Conference Research Development Information
Retrieval (SIGIR-98), pp. 335336.
Carletta, J. (1996). Assessing agreement classification tasks: kappa statistic. Computational Linguistics, 22 (2), 249254.
Charniak, E., Altun, Y., Braz, R. d. S., Garrett, B., Kosmala, M., Moscovich, T., Pang,
L., Pyo, C., Sun, Y., Wy, W., Yang, Z., Zeller, S., & Zorn, L. (2000). Reading
comprehension programs statistical-language-processing class. Proceedings
2000 ANLP/NAACL Workshop Reading comprehension tests evaluation computer-based language understanding sytems - Volume 6, ANLP/NAACLReadingComp 00, pp. 15.
Clauset, A., Newman, M. E. J., & Moore, C. (2004). Finding community structure
large networks. Phys. Rev. E, 70 (6), 066111.
Cohn, T., & Blunsom, P. (2005). Semantic role labelling tree conditional random
fields. Proceedings Ninth Conference Computational Natural Language
Learning, pp. 169172. Association Computational Linguistics.
Das, D., & Martins, A. (2007). survey automatic text summarization. Literature
Survey Language Statistics II course CMU, 4, 192195.
Eisner, J. (1996). Three new probabilistic models dependency parsing: exploration.
Proceedings 34th Annual Conference Association Computational
Linguistics (ACL-96), pp. 340345. Association Computational Linguistics.
Elkiss, A., Shen, S., Fader, A., Erkan, G., States, D., & Radev, D. R. (2008). Blind men
elephants: citation summaries tell us research article?. Journal
American Society Information Science Technology, 59 (1), 5162.
195

fiQazvinian et Al.

Erkan, G. (2006). Language model-based document clustering using random walks.
Proceedings HLT-NAACL conference, pp. 479486, New York City, USA. Association Computational Linguistics.
Erkan, G., & Radev, D. R. (2004). Lexrank: Graph-based centrality salience text
summarization. Journal Artificial Intelligence Research (JAIR).
Ferrer Cancho, R., & Sole, R. V. (2001). small-world human language. Proceedings
Royal Society London B, 268 (1482), 22612265.
Finkel, J. R., Kleeman, A., & Manning, C. D. (2008). Efficient, feature-based, conditional
random field parsing. Proceedings ACL-08: HLT, pp. 959967, Columbus, Ohio.
Association Computational Linguistics.
Gildea, D., & Jurafsky, D. (2002). Automatic labeling semantic roles. Comput. Linguist.,
28 (3), 245288.
Harabagiu, S., Hickl, A., Lehmann, J., & Moldovan, D. (2005). Experiments interactive
question-answering. Proceedings 43rd Annual Meeting Association
Computational Linguistics, ACL 05, pp. 205214.
Hildebrandt, W., Katz, B., & Lin, J. (2004). Overview TREC 2003 question-answering
track. Proceedings North American Chapter Association Computational Linguistics - Human Language Technologies (HLT-NAACL 04).
Hirschman, L., Light, M., Breck, E., & Burger, J. D. (1999). Deep read: reading comprehension system. Proceedings 37th annual meeting Association
Computational Linguistics Computational Linguistics, ACL 99, pp. 325332.
Hoang, C. D. V., & Kan, M.-Y. (2010). Towards automated related work summarization.
Proceedings 23nd International Conference Computational Linguistics
(COLING-10), pp. 427435, Beijing, China. Coling 2010 Organizing Committee.
Johansson, R., & Nugues, P. (2005). Sparse bayesian classification predicate arguments.
Proceedings Ninth Conference Computational Natural Language Learning
(CoNLL-2005), pp. 177180, Ann Arbor, Michigan. Association Computational
Linguistics.
Joseph, M. T., & Radev, D. R. (2007). Citation analysis, centrality, ACL Anthology. Tech. rep. CSE-TR-535-07, University Michigan. Department Electrical
Engineering Computer Science.
Jousse, F., Gilleron, R., Tellier, I., & Tommasi, M. (2006). Conditional random fields
xml trees. Workshop Mining Learning Graphs.
Jun Hatori, Y. M., & Tsujii, J. contribution sense dependencies word sense
disambiguation. Journal Natural Language Processing.
Jurafsky, D., & Martin, J. H. (2008). Speech Language Processing: Introduction
Natural Language Processing, Speech Recognition, Computational Linguistics
(2nd edition). Prentice-Hall.
Kan, M.-Y., Klavans, J. L., & McKeown, K. R. (2002). Using Annotated Bibliography
Resource Indicative Summarization. International Conference
Language Resources Evaluation (LREC), Las Palmas, Spain.
196

fiGenerating Extractive Summaries Scientific Paradigms

Kaplan, D., Iida, R., & Tokunaga, T. (2009). Automatic extraction citation contexts
research paper summarization: coreference-chain based approach. Proceedings
2009 Workshop Text Citation Analysis Scholarly Digital Libraries, pp.
8895, Suntec City, Singapore.
Kempe, D., Kleinberg, J., & Eva Tardos (2003). Maximizing spread influence
social network. Proceedings 15th ACM SIGKDD international conference
Knowledge Discovery Data Mining (KDD-09), pp. 137146. ACM.
Klein, D., & Manning, C. D. (2003). Accurate unlexicalized parsing. Proceedings
41st Annual Conference Association Computational Linguistics (ACL-03),
pp. 423430.
Koomen, P., Punyakanok, V., Roth, D., & Yih, W.-t. (2005). Generalized inference
multiple semantic role labeling systems. Proceedings Ninth Conference
Computational Natural Language Learning (CoNLL-2005), pp. 181184, Ann Arbor,
Michigan. Association Computational Linguistics.
Krippendorff, K. (1980). Content Analysis: Introduction Methodology. Beverly
Hills: Sage Publications.
Kupiec, J., Pedersen, J., & Chen, F. (1995). trainable document summarizer. Proceedings 18th Annual International ACM SIGIR Conference Research
Development Information Retrieval (SIGIR-95), pp. 6873.
Lin, C.-Y. (2004). ROUGE: package automatic evaluation summaries. Proceedings ACL workshop Text Summarization Branches Out.
Lin, J. J., & Demner-Fushman, D. (2006). Methods automatically evaluating answers
complex questions. Information Retrieval, 9 (5), 565587.
Ma, X. (2000). Hong kong news parallel text. Linguistic Data Consortium, Philadelphia.
Manning, C. D., Raghavan, P., & Schutze, H. (2008). Introduction Information Retrieval.
Cambridge University Press.
Marquez, L., Comas, P., Gimenez, J., & Catala, N. (2005). Semantic role labeling
sequential tagging. Proceedings Ninth Conference Computational Natural
Language Learning, CONLL 05, pp. 193196.
Mei, Q., Guo, J., & Radev, D. (2010). Divrank: interplay prestige diversity
information networks. Proceedings 16th ACM SIGKDD international
conference Knowledge Discovery Data Mining (KDD-10), pp. 10091018.
Mohammad, S., Dorr, B., Egan, M., Hassan, A., Muthukrishan, P., Qazvinian, V., Radev,
D., & Zajic, D. (2009). Using citations generate surveys scientific paradigms.
Proceedings North American Chapter Association Computational Linguistics - Human Language Technologies (HLT-NAACL 09), pp. 584592, Boulder,
Colorado.
Montoya, J. M., & Sole, R. V. (2002). Small world patterns food webs. Journal
theoretical biology, 214 (3), 405412.
197

fiQazvinian et Al.

Nakov, P., D. A., & Hearst, M. (2012). peers see paper authors?.
Advances Bioinformatics, special issue Literature Mining Solutions Life
Science Research.
Nanba, H., Abekawa, T., Okumura, M., & Saito, S. (2004). Bilingual PRESRI: Integration
multiple research paper databases. Proceedings RIAO 2004, pp. 195211,
Avignon, France.
Nanba, H., Kando, N., & Okumura, M. (2000). Classification research papers using
citation links citation types: Towards automatic review article generation.
Proceedings 11th SIG Classification Research Workshop, pp. 117134, Chicago,
USA.
Nanba, H., & Okumura, M. (1999). Towards multi-paper summarization using reference
information. Proceedings 16th International Joint Conference Artificial
Intelligence (IJCAI-99), pp. 926931.
Nenkova, A., & Passonneau, R. (2004). Evaluating content selection summarization:
pyramid method. Proceedings North American Chapter Association
Computational Linguistics - Human Language Technologies (HLT-NAACL 04).
Newman, M. E. J. (2001). structure scientific collaboration networks. PNAS, 98 (2),
404409.
Newman, M. E. J. (2003). structure function complex networks. SIAM Review,
45 (2), 167256.
Newman, M. E. J. (2004a). Analysis weighted networks. Physical Review E, 70056131.
Newman, M. E. J. (2004b). Fast algorithm detecting community structure networks.
Phys. Rev. E, 69, 066133.
Newman, M. E. J., & Girvan, M. (2004). Finding evaluating community structure
networks. Phys. Rev. E, 69, 026113.
Ng, H. T., Teo, L. H., & Kwan, J. L. P. (2000). machine learning approach answering questions reading comprehension tests. Proceedings 2000 Joint
SIGDAT conference Empirical methods natural language processing
large corpora: held conjunction 38th Annual Meeting Association
Computational Linguistics - Volume 13, EMNLP 00, pp. 124132.
Paul, M., Zhai, C., & Girju, R. (2010). Summarizing contrastive viewpoints opinionated
text. Proceedings Conference Empirical Methods Natural Language
Processing (EMNLP-10), pp. 6676.
Pradhan, S., Hacioglu, K., Ward, W., Martin, J. H., & Jurafsky, D. (2005). Semantic
role chunking combining complementary syntactic views. Proceedings Ninth
Conference Computational Natural Language Learning, CONLL 05, pp. 217220.
Punyakanok, V., Roth, D., & Yih, W. (2008). importance syntactic parsing
inference semantic role labeling. Computational Linguistics, 34 (2).
Qazvinian, V., & Radev, D. R. (2008). Scientific paper summarization using citation summary networks. Proceedings 22nd International Conference Computational
Linguistics (COLING-08), Manchester, UK.
198

fiGenerating Extractive Summaries Scientific Paradigms

Qazvinian, V., & Radev, D. R. (2010). Identifying non-explicit citing sentences citationbased summarization.. Proceedings 48th Annual Conference Association Computational Linguistics (ACL-10), pp. 555564, Uppsala, Sweden.
Qazvinian, V., & Radev, D. R. (2011a). Exploiting phase transition latent networks
clustering. Proceedings Association Advancement Artificial
Intelligence (AAAI-11).
Qazvinian, V., & Radev, D. R. (2011b). Learning collective human behavior introduce diversity lexical choice. Proceedings 49th Annual Conference
Association Computational Linguistics (ACL-11), pp. 10981108.
Radev, D., Allison, T., Blair-Goldensohn, S., Blitzer, J., Celebi, A., Dimitrov, S., Drabek,
E., Hakim, A., Lam, W., Liu, D., Otterbacher, J., Qi, H., Saggion, H., Teufel, S.,
Topper, M., Winkel, A., & Zhang, Z. (2004). MEAD - platform multidocument
multilingual text summarization. LREC 2004, Lisbon, Portugal.
Radev, D. R., Muthukrishnan, P., & Qazvinian, V. (2009). ACL anthology network
corpus. ACL workshop Natural Language Processing Information Retrieval
Digital Libraries, Singapore.
Ravasz, E., Somera, A., Mongru, D., Oltvai, Z., & Barabasi, A. (2002). Hierarchical organization modularity metabolic networks. Science, 297 (5586), 1551.
Riloff, E., & Thelen, M. (2000). rule-based question answering system reading comprehension tests. Proceedings 2000 ANLP/NAACL Workshop Reading
comprehension tests evaluation computer-based language understanding sytems
- Volume 6, ANLP/NAACL-ReadingComp 00, pp. 1319.
Rinaldi, F., Dowdall, J., Kaljurand, K., Hess, M., & Molla, D. (2003). Exploiting paraphrases question answering system. Proceedings second international
workshop Paraphrasing - Volume 16, PARAPHRASE 03, pp. 2532.
Siddharthan, A., & Teufel, S. (2007). Whose idea this, matter?
attributing scientific work citations. Proceedings North American Chapter
Association Computational Linguistics - Human Language Technologies
(HLT-NAACL 07).
Small, S., Liu, T., Shimizu, N., & Strzalkowski, T. (2003). Hitiqa: interactive question
answering system: preliminary report. Proceedings ACL 2003 Workshop
Multilingual Summarization Question Answering.
Smyth, S., & White, S. (2005). spectral clustering approach finding communities
graphs. Proceedings 5th SIAM International Conference Data Mining,
pp. 7684.
Sparck-Jones, K. (1999). Automatic summarizing: factors directions. Mani, I., &
Maybury, M. T. (Eds.), Advances automatic text summarization, chap. 1, pp. 1
12. MIT Press.
Tang, J., Hong, M., Li, J., & Liang, B. (2006). Tree-structured conditional random fields
semantic annotation. Proceedings 5th International Semantic Web Conference (ISWC06), pp. 640653.
199

fiQazvinian et Al.

Teufel, S. (2005). Argumentative Zoning Improved Citation Indexing. Computing Attitude Affect Text: Theory Applications, 159170.
Teufel, S., & Moens, M. (2002). Summarizing scientific articles: experiments relevance
rhetorical status. Computational Linguistics, 28 (4), 409445.
Teufel, S., Siddharthan, A., & Tidhar, D. (2006). Automatic classification citation function. Proceedings Conference Empirical Methods Natural Language
Processing (EMNLP-06), pp. 103110, Sydney, Australia.
Thompson, C., Levy, R., & Manning, C. (2003). generative model FrameNet semantic
role labeling. Proceedings Fourteenth European Conference Machine
Learning ECML-2003, Croatia.
Tomuro, N. (2003). Interrogative reformulation patterns acquisition question paraphrases. Proceedings second international workshop Paraphrasing - Volume 16, PARAPHRASE 03, pp. 3340.
Toutanova, K., Haghighi, A., & Manning, C. (2005). Joint learning improves semantic role
labeling. Proceedings 43rd Annual Meeting Association Computational Linguistics (ACL05), pp. 589596, Ann Arbor, Michigan. Association
Computational Linguistics.
Toutanova, K., Haghighi, A., & Manning, C. D. (2008). global joint model semantic
role labeling. Comput. Linguist., 34 (2), 161191.
van Halteren, H., & Teufel, S. (2003). Examining consensus human summaries:
initial experiments factoid analysis. Proceedings HLT-NAACL 03
Text summarization workshop, pp. 5764, Morristown, NJ, USA.
van Halteren, H., & Teufel, S. (2004). Evaluating information content factoid analysis: human annotation stability. Proceedings Conference Empirical
Methods Natural Language Processing (EMNLP-04), Barcelona.
Voorhees, E. M. (1998). Variations relevance judgments measurement retrieval
effectiveness. Proceedings 21st Annual International ACM SIGIR Conference
Research Development Information Retrieval (SIGIR-98), pp. 315323.
Voorhees, E. M. (2001). Overview TREC 2001 question answering track. Text
REtrieval Conference.
Voorhees, E. M. (2003). Overview TREC 2003 question answering track. Proceedings Twelfth Text Retrieval Conference (TREC-2003).
Voorhees, E. M. (2005). Using question series evaluate question answering system effectiveness. HLT/EMNLP 2005.
Wang, W., J., A., Parasuraman, R., Zubarev, I., Brandyberry, D., & Harper, M. (2000).
Question Answering System Developed Project Natural Language Processing Course. ANLP/NAACL Workshop Reading Comprehension Tests
Evaluation ComputerBased Language Understanding Systems.
Watts, D. J., & Strogatz, S. (1998). Collective dynamics small-world networks. Nature,
393, 440442.
200

fiGenerating Extractive Summaries Scientific Paradigms

Whidby, M. A. (2012). Citation handling: Processing citation texts scientific documents.
Masters thesis, University Maryland, Department Computer Science, College
Park, MD.
Xue, N., & Palmer, M. (2004). Calibrating features semantic role labeling. Lin, D., &
Wu, D. (Eds.), Proceedings EMNLP 2004, pp. 8894, Barcelona, Spain. Association
Computational Linguistics.
Zajic, D. M., Dorr, B. J., Lin, J., & Schwartz, R. (2007). Multi-candidate reduction: Sentence
compression tool document summarization tasks. Information Processing
Management (Special Issue Summarization).
Zhao, Y., & Karypis, G. (2001). Criterion functions document clustering: Experiments
analysis. Technical report TR #0140, Department Computer Science, University Minnesota, Minneapolis, MN.
Zhu, X., Goldberg, A., Van Gael, J., & Andrzejewski, D. (2007). Improving diversity
ranking using absorbing random walks. Proceedings North American Chapter
Association Computational Linguistics - Human Language Technologies
(HLT-NAACL 07), pp. 97104.

201

fiJournal Artificial Intelligence Research 46 (2013) 413-447

Submitted 12/12; published 03/13

Qualitative Order Magnitude Energy-Flow-Based Failure
Modes Effects Analysis
Neal Snooke

nns@aber.ac.uk

Department Computer Science, Aberystwyth University,
Penglais, Aberystwyth, Ceredigion, SY23 3DB, U.K.

Mark Lee

mhl@aber.ac.uk

Department Computer Science, Aberystwyth University.

Abstract
paper presents structured power energy-flow-based qualitative modelling approach applicable variety system types including electrical fluid flow.
modelling split two parts. Power flow global phenomenon therefore naturally represented analysed network comprised relevant structural elements
components system. power flow analysis platform higher-level
behaviour prediction energy related aspects using local component behaviour models
capture state-based representation global time. primary application
Failure Modes Effects Analysis (FMEA) form exaggeration reasoning used,
combined order magnitude representation derive worst case failure modes.
novel aspects work order magnitude(OM) qualitative network
analyser represent power domain topology, including multiple power sources,
feature required earlier specialised electrical versions approach.
Secondly, representation generalised energy related behaviour state-based local
models presented modelling strategy vivid intuitive range
topologically complex applications qualitative equation-based representations.
two-level modelling strategy allows broad system behaviour coverage qualitative
simulation exploited FMEA task, limiting difficulties qualitative
ambiguity explanation arise abstracted numerical models. used
method support automated FMEA system examples aircraft fuel system
domestic heating system discussed paper.

1. Introduction
Qualitative representations (QR) reasoning number well documented advantages Failure Modes Effects Analysis. Three important analysis
performed early design life cycle, broad coverage system behaviour faults, results level abstraction readily maps
system functional states.
QR widely applied electrical systems (de Kleer, 1984; Mauss & Neumann,
1996; Price, Snooke, & Lewis, 2006; Flores & Farley, 1999) used variety design
analysis applications including design concept analysis (gaining overview system behaviour), diagnosis, FMEA safety analysis, incremental multiple fault FMEA analysis
(Price & Taylor, 1997), fault tree analysis (FTA) (Price, Wilson, Timmis, & Cain, 1996),
sneak circuit analysis (Price, Snooke, & Landry, 1996; Savakoor, Bowles, & Bonnell,
2013 AI Access Foundation. rights reserved.

fiSnooke & Lee

1993). qualitative information enabling technique allowing significant system
nominal failure states identified distinguished easily efficiently.
qualitative results limitations, significant behavioural ambiguity alternative behaviours predicated simulation. often seen
problem qualitative techniques, ambiguity turned advantage, provided predict real physical behaviours since indicate key design parameters
exist. One benefits modelling structure developed paper many
ambiguities constrained therefore enhance design knowledge rather
analysis limitation.
typical approaches qualitative modelling reasoning use qualitative versions
numerical equations derived component models. forms sound platform,
burdensome extract relevant unimportant standard equations
(e.g fluid flow), deal landmarks, integrals deviations etc. Given appeal
QR abstracted explanation ability provide broad analysis system state,
propose specialised view applied wide range engineering domains
based relationships generalised physical variables shown Figure 1.
power
network

ine
(en rtia
flux linkage
erg
y)
pressure momentum
momentum

resista

momentum

EMF, voltage (electrical)
pressure (hydraulic)
force (mechanical)

ce
tan
aci y)
cap nerg
(e

n ce

effort

flow

displacement
charge
volume
displacement

current
volumetric flow rate
velocity

Figure 1: Tetrahedron state

paper provides consistent framework several earlier implementations, develops supporting qualitative order magnitude (OM) capability (Section 2.1) throughout
models simulation. OM representation derived many valued resistance
representation (Lee, 2000b) although many valued concept modelling
reasoning benefits completely applicable OM representation, actual technique
solving circuits work based path labelling (Lee, 1999),
general solution applicable network topology. present paper provides general
solution extends technique purely electrical systems domains.
necessary part effort energy-flow-based formalisation adopted, comprising
global instantaneous power network (Section 2) local component models include
notions energy time (Section 3).
414

fiQualitative Energy-Flow-Based FMEA

Previous work authors (Snooke, 2007) modelling fluid flow systems
based electrical network analyser (Lee, 1999) could deal series parallel
reducible circuits, becomes limitation topologically complex systems.
incorporation equivalent resistance reduction using stardelta (Y-) transforms (Mauss &
Neumann, 1996) OM representation (Section 2.2) allows effort flow variables
resistive networks topology solved reduction flow assignment expansion
(Section 2.3), completes work started Lee. Complex network topologies due
part need represent concepts atmosphere fluid flow systems,
allow failure modes leaks closed systems, well vented elements
systems. unique zero node common several domains generalised
present work Section 2.4.
Applying techniques derived Lee (1999) non electrical systems reveals two
limitations. Firstly, analysis supports single effort source network,
secondly concept substance flowing network. Section 2.5 extends
analysis include multiple power sources applying principle superposition allow
system decomposed multiple resistive networks. Section 2.6 considers
representation substances networks behaviour dependent upon substance
carrying effort flow. final part section 2 deals common special
cases simplify analysis maintain vividness representation avoiding
unnecessary (Y-) transforms.
Section 3 paper devoted local component concept captures temporal
information thus models displacement momentum aspects Figure 1.
OM modelling time subject section 3.1 underlies Finite State Machine
(FSM) based modelling approach. representation improvement restricted predecessor used electrical component behaviour (Snooke, 1999),
foundation variety automated electrical design analysis techniques. Sections 3.2
3.3 provide concrete examples component modelling system modelling respectively.
Section 4 considers use OM representation perform exaggeration reasoning
failure analysis section 5 provides brief overview strategy (Price, 1998; Lee,
Bell, & Coghill, 2001) used process multiple simulation results FMEA output.
provide two case study examples sections 6 7. first case study illustrates
behaviour simulation fluid flow system interesting characteristics, second
outlines use simulation results automatically produce completed FMEA
report industrial system supplied sponsors work.
paper utilise two-level modelling strategy. lower level (centre section
Figure 1) provides network-based global qualitative solver linear resistive networks
using graph-based methods determine instantaneous power (effort flow) network. higher level utilises results lower level network analysis decide
localized component behaviour state-based models qualitative representation
time global parameter.
two-level strategy gives significant benefits approaches modelling
kinds electrical hydraulic systems dealt here. two-level modelling allows
separation inherently global, instantaneous (single state) power variables,
local energy-based behaviour components. separation particularly suitable
qualitative representation qualitative behaviour fundamentally state-based;
415

fiSnooke & Lee

qualitative simulation produce sequence states power flows systems
components. Power flow global phenomenon cause effect undetermined
efficiently analysed network, avoiding problems causal instability
ambiguity arising systems equations generated interacting components
local behaviours (Skorstad, 1992a). Many QR approaches use local propagation method
connecting behaviours components appeal generality, however,
often require way including additional global information equivalent circuits (Sussman & Steele Jr, 1980), mechanisms propagating connectivity information
(Struss, Malik, & Sachenbacher, 1995). approach allows users build reusable models
components placed library used describing components
linked together.
Bond graphs share underlying energy-flow-based concept lower level
approach, popular domain-independent graphical method describing
system first described Paynter (1961) developed numerous others.
excellent way generating systems differential equations describe system
behaviour. Qualitative versions bond graphs produced (Ghiaus, 1999), however
qualitative versions standard numerical equations therefore suffer
difficulties general equation-based constraint methods. qualitative
version bond graphs successfully used model energy flows associated people
building (Tsai & Gero, 2010), movement people determined
specific physical laws thus choice provide ad-hoc model based
high-level knowledge building use. observed qualitative models
often result many fault candidates, reasoning scheme based past experience ad-hoc system dynamics difficult obtain proper models
quantitative methods (Samantaray & Ould, 2011). widely-used qualitative
equation-based constraint method QSIM (Kuipers, 1986). Kuipers acknowledged problems QSIM many applications - chattering, uncontrolled branching
possibilities, asymptotic behaviours (Fouche & Kuipers, 1990). techniques
developed minimising problems (Clancy & Kuipers, 1997), problems
approach remain practical systems. observed (Mosterman & Biswas, 2000),
reducing model complexity eliminating higher-order derivatives non-linear effects
leads discontinuities system (i.e. state changes) careful analysis underlying physical nature system required constructing models order ensure
simplified models correspond real behaviour. kind careful analysis
possible linking QSIM-style qualitative equation-based constraint methods physical
components, certainly wanting able simulate faulty components.
two-level approach achieved practical success even less sophisticated versions present approach (Price & Struss, 2004) avoiding direct translation
differential equations qualitative versions, rather representing phenomena explicitly state-based representations local component behaviour level providing
constrained global representation. limit applicability
property model discrete continuous rather inherent property
system (Struss, 2003), real question kind model appropriate
reasoning task hand. modelling paper requires minimal algebraic effort,
representing component behaviour abstract level qualitative behaviour
416

fiQualitative Energy-Flow-Based FMEA

Effort, E
0
0
0
u
u
u

Resistance, R
0
r

0
r


Flow, F
?
0
0

f
0

Table 1: Qualitative electrical current assignment

predicted nominal failure modes topologically complex systems many
component states.

2. Qualitative Power Network
power network (system circuit) represented using resistances, R, structural
component. simulation task derive effort across resistance flow
resistance given power (effort flow) source network. minimal
useful qualitative quantity space uses [0, r, ] represent resistance [0, u] represent
effort, E , supply terminals (Lee, 1999). linear network uses generalised version Ohms Law effort, flow, resistance, E = F R, provides current
assignment Table 1. first row qualitatively ambiguous flow
zero resistance produces effort loss. physically possible effort drop
across zero resistance hence flow fourth row shown indicate
impossible case.
power network considered graph G(T, A) containing nodes edges
connect exactly two nodes, represent circuit resistances. edge e
resistance value R(e) connects pair nodes e = ht1 T, t2 i. Effort measured
two nodes E(t1 , t2 ) flow measured edge F (e). degree
node number connections node. Looped edges ends connected
node therefore increase degree node two.
2.1 Order Magnitude Representation
order magnitude representation R allows detailed modelling without introducing qualitative ambiguity separating artefacts significantly different characteristics (Raiman, 1991; Lee, 2000a). enhancement improves ability represent
nominal behaviour distinguishing signal level power actuator power electrical circuits, gravitational head pressure system pump selected fluid
transfer systems. modelling faults also improved allowing exaggerated faults
modelled, thus producing effects faults effects would otherwise qualitatively
indistinguishable nominal operation.
define O(M ) qualitative resistance values R = [r1 = 0, r2 , . . . , ri1 , rm = ]
ri+1 ri i. addition ri /ri+1 = rj /rj+1 i, j N. Physically
interpret mean number rm valued resistors series dominated
417

fiSnooke & Lee


0
0
0
r 3n
r 3n


b
0
r 3n

r 3m



ab
0
0
?
r 3(n+m)



Table 2: OM Multiplication

single rm+1 valued resistor series segment, addition,
magnitudes resistances qualitatively equal spacing. OM representation
proposed benefit generate additional qualitative landmarks
lead potential ambiguity model, indeed purpose allow qualitative distinctions characteristics known significantly different magnitude.
qualitative OM approach somewhat analogous base 10 logarithmic scale used
magnitude approximation numerical reasoning, two differences. qualitative
assumption hold generally mapping numerical specifications qualitative ones practical systems, coarser magnitude scale 10 required. Therefore,
models distinguish values based numerical equivalent three order magnitude prefix ranges, A, mA, electrical current, work well. Secondly, may
interpreted specific application type domain. example assume
fluid system pressure system created gravitational head vertical
pipe always dominated pump(s) working it, regardless number
pipe sections system, consider different qualitative magnitudes, even
though numerically clearly case. case consistent set qualitative
magnitudes, models, interpretations required application domain
separates phenomena considered satisfy target systems.
qualitative magnitude notation q3n introduced convenience lower
magnitudes indicate quantity n orders magnitude q. Similarly, convienience, q2n indicates quantity n orders magnitude q therefore q3n = q2(n) .
OM calculations follow usual rules sign algebra (Trave-Massuyes, Ironi, & Dague,
2004) allows domination one magnitude another shown Tables 2 3
positive multiplication addition respectively. Table 1 u replaced
u3n r replaced r 3m resulting f 3(nm) flow result row 5.
2.2 Network Reduction
OM representation utilised resistance, effort, flow variables
power network. aim derive flow effort values throughout entire circuit,
achieved reducing circuit single equivalent resistance, assigning flow
value expanding network assigning effort flow values level expansion
according qualitative structures present. network reduction performed
series parallel (SP) circuit simplification OM version qualitative star
delta (Y-) transformation (Mauss & Neumann, 1996) non SP cases follows.
418

fiQualitative Energy-Flow-Based FMEA

+
0

0
0

+p3n
+p3n

+p3m

+p3m

+p3min(m,n)

p

p

3m

3m


?


?

+p3n n <
p3m < n
? n =

?

p3n
p3n
3n
p
n <
+p3m < n
? n =




?
?



?


p3min(m,n)

?

?

?

?
?

Table 3: OM addition
Within network graph G, nodes degree two removed
connected edges e1 , e2 replaced equivalent series edge resistance:
R(e1 e2 ) = max(R(e1 ), R(e2 ))

(1)

edges e1 , e2 , . . . , en share pair nodes replaced single
equivalent parallel edge resistance:
R(e1 ||e2 || . . . ||en ) = min(R(e1 ), R(e2 ), . . . , R(ei ))

(2)

facilitate vividness derived model use convention e1 ||e2 label edges
representing parallel combination edges e1 e2 indicate series combination.
Iterative application produce tree equivalent resistances result either
single resistance R0 supply nodes tfi non-SP reducible circuit
fragment.
majority circuits SP reducible, particularly consider zero resistance
edges removed network simplify topology (Section 2.7).
remainder Y- transformation applied shown Figure 2. introduction
Y- resistances unfortunately reduces vividness representation since
directly related original component structure; however, provides general
solution assign flow direction network, unlike earlier work (Lee, 2000a).
qualitative signs version transformation utilised (Mauss & Neumann,
1996), however, OM representation introduces possibility additional levels
resistance transformed node requires detailed analysis.
Edges e1 . . . en connected non SP reducible degree n star node replaced new
edges ejk form equivalent network 1 j n, 1 k n, k > j. Using
RR (e) notate numerical resistance edge e, 3 node Y- transformation defined
(symmetrically edges) as:
RR (e12 ) =

RR (e1 )RR (e2 ) + RR (e1 )R(e3 ) + RR (e2 )RR (e3 )
RR (e3 )

(3)

Equation 3 generalised star mesh transform defined star nodes order as:
RR (ejk ) = RR (ej )RR (ek )

n
X
m=1

419

1/RR (em )

(4)

fiSnooke & Lee

Noting OM qualitative values:


1 1
1
max
, , ... =
b
min(a, b, ...)

(5)

qualitative version equation 4 1 n is:
R(ejk ) =

R(ej )R(ek )
min(R(em ))

(6)

denominator , resistances must R(ejk ) = .
star resistances, R(em ) = 0, result undefined. Hence, efficiency, zero
resistance edges removed combining associated nodes super node
discussed next subsection. numerator contains 0 , value
result. cases result determined magnitude indices values.
2c R(e ) = r 2a R(e ) = r 2b ,
min(R(em )) = rm
j
k
j
j
2(a+bc)
rjk

=

e1

2b
r2a
j rk

(7)

r2c


e12

e13
e2

e3
e23

Figure 2: -Y transformation 3, 4 5 nodes

2.3 Flow Effort Assignment
reduced network flow (or effort flow sources) assigned source
directly OM extended version Table 1. R = 0 case reported immediately
since specific physical interpretation circumstances, electrical
short circuit. R = 0 contains another source (Section 2.5) presently
consideration, edge ignored. determine flow specific circuit component,
edge hierarchy expanded required edge. sign flow determines
420

fiQualitative Energy-Flow-Based FMEA

direction relative arbitrary terminal order resistor, using convention t1 t2
f t2 t1 -f.
edges e1 , e2 part series pair flow simply flow
pair: F (e1 ) = F (e2 ) = F (e1 e2 ) effort across edge E(e1 ) =
F (e1 )R(e1 ) (noting f 3n r 3m = u3(m+n) ). R(e) = , F (e) = 0 E(e)
undetermined effort equation (see rows 3 6 Table 1); however, unless
R(e1 ) = R(e2 ) = 0, E(e) = E(e1 e2 ).
edges parallel, E(e1 ) = E(e2 ) = E(e1||e2 ) flow F (e) = E(e)/R(e).
R = 0 physically impossible E 6= 0, unless short circuit
supply, treated special situation. R = F = 0 Table 1.
Y- edges sum flows,
Fek =

k1
X

F (emk )

m=1

n
X

F (ekm )

(8)

m=k+1

addition mixed signs leads possibility ambiguous flow (Table 3), would
caused balanced bridge configuration. indicates qualitative behaviour,
possibly, future state system depends numerical values resistances
within one order magnitude, signalling analysis try obtain detailed
information. level resistances, ambiguous flow value necessarily lead
reasoning impasse higher-level tasks FMEA higher-level behaviour
dependent value analysis tool reporting require it. Finally,
E(eek ) = F (eek )R(eek ).
Figure 3 exemplifies number concepts preceding subsections. notation
fi used identify positive negative supply nodes, using subscript identify
source necessary systems multiple power sources. Working left
right Figure, sequence circuit reductions performed obtain final single
equivalent resistance value. overall flow value computed distributed amongst
circuit elements right left Figure described above. Finally flows
calculated flows using equation 8, noting positive flow
directions defined away star centre node.
F (e1 ) = F (e14 ) F (e13 ) = f 31 f 34 = f 31
F (e4 ) = F (e14 ) F (e43 ) = f 31 f 33 = f 31
F (e3 ) = F (e13 ) F (e43 ) = f 34 + f 33 = f 33
2.4 Distinguished Node
Effort values measured two nodes convenience common practice
identify one distinguished node network make measurements relative
node. allows effort measured node implicit assumption
second node distinguished node. commonly case electrical systems
distinguished node called ground earth provides reference node
voltage measurement often defined negative supply terminal single
source system.
421

fiSnooke & Lee

u

e0

e0
r

<1

r

<1

t1

>1
f
r

<1

e1

r

e13

<1
>4
f

t3

e3
r

<3

r

r

<1

e5
r

>1
f

r

<3

r

>2
f
r

r

<1

>1
f

t1

e0 : e14
|| ((e2 || e13)
: (e5 || e34))

r

e5
<2

r
>2
f

<2

<1
r
e14
|| ((e2 || e13)
: (e5 || e34))

e14

<1 e14

<3
r

<1

>2
f

e34

>2
f

r

t1

t3

t3

r

<1

e2 || e13

<1

<1 e14
>3
f

e4

r

<1

e0

t1
e2

>2
f

>3
f

t2

r

t1
e2

e0

e0

>2
f

<1

>1
f

r

<2

>2
f

r

<1

>1
f

>1
f

e2
|| e13
: e5 || e34

e5 || e34

0

non SP-reducible

star delta reduction

parallel reduction

series reduction

parallel reduction

Figure 3: Operation qualitative circuit solver showing reduction flow assignment

generality define symbol Z distinguished zero node network
type specific instances, represent atmosphere fluid flow systems.
Notice Z provide another landmark value qualitative effort space;
provides structural reference point resistive network.
identification Z allows definition absolute effort relative point.
qualitative effort levels [0, u] supplemented voltage (Lee, 1999)
two additional symbols represent structural features circuit emerge
simulation. additions useful interpretation simulation,
change quantity space itself. used indicate floating effort present circuit
fragments disconnected source used indicate effort
supply terminals voltage, i.e. E(, ) = E(, fi) within magnitude.
associated source less useful systems multiple active effort sources.
always useful distinguish 0 ? generally undefined would
provide meaningful measurement, although may read 0 measured. qualitative
value ? different ? qualitatively undetermined value within
scope modelled system, whereas value within modelled system.
2.5 Multiple Effort Sources
network solver calculates power consumed (P = E F ) single source. linear
resistive network use qualitative version principle superposition
one source connected single network (noting single system schematic
may, instant, actually comprise many isolated networks, possibly different domains,
even though components schematic appear connected). network analysed
separately source inhibiting effort sources summing results.
P
sources s1 . . . sn , qualitative flow edge e given ssn1 F (e) =
max(Fs1 (e) . . . Fsn (e)). Clearly two opposing flows magnitude exist,
422

fiQualitative Energy-Flow-Based FMEA

edge suffers qualitative ambiguity. failure configurations, ambiguous result
useful highlighting engineer range possible failure behaviour occur.
Simple relational constraints may used resolve ambiguity common special
case zero resistance one supply nodes s1 , fis1 one
s2 , fis2 . example statement relative power two pumps allow direction
flow reduced circuit derived, thereby allowing flows expanded network
resistances calculated.
Figure 4 illustrates circuit schematic electrical system two effort sources
resistances magnitude r31 r. individual flow contributions calculated
source s1 shown coarse broken line, flows s2 fine broken line
annotated flow magnitudes. flow sum shown resistor.
partial flow ambiguity circuit, shown double-headed arrows next resistors
centre section opposing flow contributions occur magnitude.
state-based functional simulation requires flow (direction), power effort
resistors, necessary know values resistors.
s1

-

+
f<0

u>0

f<1

f<0

0z

r>1

-

r>1

f<1

f<0

u>0

+
r

f<0

s2

f<0

f<0
f<0

f<1
f<0

s2 flow

f<1

r>1

s1 flow

Figure 4: Multiple Source Example

2.6 Representation Substance Within System
discussion far considered nature flow. domains flow,
electrical current, implicit component models. thermal domain E
temperature difference entropy flow F . product heat thermal
power, P , resistance component selected represent reciprocal
thermal conductivity. Fluid transfer hydraulic systems include possibility
one substance associated network, particularly faults present.
concrete example given fuel distribution system. supply tank becomes
empty, air enter system behaviour pumps component may
change.
substance-dependent behaviour represented component level. power
network provides instantaneous view effort flow, cannot participate propa423

fiSnooke & Lee

gation flowing substance, is, however, necessary components able obtain
knowledge substance interface components. network nodes
considered zero volume points connection instantly propagate substances
component outflows inflows. local component behaviour provides substance information nodes appropriate qualitative timeframe according substance
present inflows, capacitance behaviour. several connections
node flow direction may change simulation. reasons new concept
included circuit solver; list substances maintained node contains
output substances resistance connected instant simulation.
Figure 5 depicts node connected three resistances. flow directions likely
cause e1 e2 modify associated SUBSTANCE lists t1. e3 requests
substance t1 using S(t1), {S0, S1, S3} obtained.
assign
S(e1.t1) = {S0,S1}
S(e2.t1) = {S3,S0}
evaluate
S(t1)
result {S0, S1, S3}

t1 SUBSTANCE list
e1 {S0, S1}
e2 {S3, S0}

e1
t1

e2

e3

Figure 5: Node substance representation

presence one substance node result substance mixing
virtue substances present provided component uses node
input. might consider using flow magnitude information indicate ratio
substance present, however, reality usually many factors involved,
leading process diminishing returns modelling effort required.
modelling substance work deliberately simple provides qualitative
capabilities match conventional bond graphs address systems free energy (mechanical, electrical, magnetic, incompressible fluid) elements conservative
except resistance, energy dissipated. Sophisticated techniques added
Bond graph methodology (Brown, 2010), allow numerical models thermofluidic phenomena constructed; however, complexity required models
benefits broad-based qualitative modelling system engineering analysis
FMEA lost. alternative approaches qualitative reasoning considered
changes material state example plug-based ontology (Skorstad, 1992b)
able model phase transitions steam boiler tube, however, modelling required
necessarily detailed 13 state envisionment produced illustrates complexity
phenomena. Another approach avoid complexity used Ghiaus provides
qualitative model Carnot refrigeration cycle (Ghiaus, 1999) based equations
derived thermal bond graph thus include substance properties
system assumed equilibrium, precluding analysis many failure modes.
Within proposed ontology, compressible fluids require representation state
substance would require substance state parameterised models.
424

fiQualitative Energy-Flow-Based FMEA

Enthalpy flow could potentially modelled providing multiple forms substance (including phase changes) related changes containing volumes pressures,
providing effort thermal circuit. modelling enterprise, choice made
range phenomena worth modelling, based required analysis.
Complex thermodynamic aspects one area considered automated
analysis overall system engineering level worth modelling effort
clear sufficient range faults supported models provide qualitatively distinct useful behaviour predictions. possibly area future
research may indeed provide fruitful results, make claims current time.
2.7 Computational Enhancements
network reduction flow assignment detailed previous sections sufficient
solve OM effort flow parameters network topology. are, however,
several additional concepts (Lee, 1999; Lee et al., 2001) remain applicable using
SPS technique, provide additional vividness representation well computational benefit. enhancements deal common special cases avoid
need use Y- transformation, produces resistances associated
original circuit components straightforward way.
nodes connected 0 valued resistances aggregated single supernode,
E(e) = 0 across edges subsumed supernode. edge e = hti , tj i,
ti , tj , R(e) = 0 supernode tij created label ti .tj ,
edge removed active graph. generated supernode participate
SP Star (SPS) reduction, however, upon expansion circuit possible
directly allocate flow edges represented subsumed edges done
SPS expansions.
flow within supernode edges deduced unambiguous (non-ladder network)
cases using qualitative version Kirchoffs current law. exception
source nodes, node connected edges flow F (e):
X

F (e) = 0

{eA:e=ht,xi e=hx,ti}

supernode edge flow values, f1 3m1 ... fn 3mn , flows F 0 = {fx 3mx : mx = max (m1 ...m2 )}
dominate. node |F | 1 edges flowing node unassigned flow edge
must node flow equal flow magnitude F 0 towards node.
dual exists flows direction.
Two additional enhancements may used improve implementation performance
network analyser. Edges e = hti , ti i, ti loop removed
active graph since loop by-passed zero resistance path, hence assigned zero
flow upon expansion. Loops typically represent shorted parts electrical circuit
example.
Degree one nodes, e {ht, i, h, ti}, , known dangling edges also
removed graph together connected edge occur
result reductions.
425

fiSnooke & Lee

3. Local Component Models OM Time
resistive network calculates power consumption, P , cannot model component
stores energy, En, since En = P time . Displacement momentum
two domain independent characteristics resulting inclusion time model
(Figure 1). Energy, displacement momentum fundamentally local characteristics
components derived power variables time. representation time follows
OM approach described Section 2.1. time t3n flow f 3n , d3(n+m) = f 3n t3n
defines displacement quantity substance.
FSM representation local component behaviour proved sufficient abstract
behaviours failure analysis task. Time represented state changes explicitly represented state transitions capture qualitative integration effort flow
variables. state power network used trigger transitions, input events
outside system (external interactions). change state component may
cause structure resistive parameters power network change, thus triggering
power network simulation sequence events components. results
system interacting state machines, sharing time common variable sequences
events.
use specialised subset UML language state chart notation (OMG, 2012)
describe FSM models. model comprised set states S, events ,
transitions = S. addition s0 defines initial (default) state
component. Output actions may associated e also entry actions
associated S. UML provides guard conditions events refine
conditions produce e = (t, Tc , Dc , Fc , A). represents temporal condition
transition qualitative OM duration transition occur Tc
satisfied, provided Fc true time. i.e. Tc triggers transition Fc allows
complete (fire). Dc condition must satisfied transition,
satisfaction Tc Fc . conditions may appeal values network model,
may change resistance values network model. States used represent
qualitatively significant values variables derived integration
efforts flows time higher-level states components. example capacitor
charged/discharged, relay activated/deactivated. following sections describe
component level simulation component model syntax.
3.1 Component Level Simulation
presence multiple components system results set independent interacting
FSMs. global variable level time, OM representation provides
sequencing component behaviour different timescales. definition OM applied
time requires sequence (non-cyclic) events t3(n1) occur event
t3n .
processing events carried maintaining time-ordered priority list
component events satisfied conditions. events ranked order
magnitude time delay periods (referred time-slots subsequently) following way:
events e Tc satisfied added end Q priority specified
event 0 unspecified.
426

fiQualitative Energy-Flow-Based FMEA

Candidate events fire n N N Q t(n) = t3x min(x)
N 6= , i.e events lowest order non-empty timeslot.
events n Fc (n) satisfied fired, events Fc (n) removed
queue.
events Dc satisfied removed queue.
|n| = 0 system reached steady state changes
state. |n| > 1 non-determinism system. important question
FMEA longer term impact alternative behaviours potential worst case
faults usually depends whether alternative behaviours diverge significantly
different functional (external) effects alternate paths internal states
converge common state. often latter case. example two relays wired
parallel may switch exactly moment, resulting two possible behaviour
paths two intermediate states, neither significant cases.
systems possible make concurrency assumption specifies
final state reached end time period t3x independent ordering events
time period. presence race conditions feedback loops mutually
interacting components allow concurrency assumption, allowing system
reach qualitatively distinct state dependent detailed numerical timing events.
case qualitative representation time lacks enough detail,
qualitative ambiguity detected concurrency assumption falsely applied.
systems certainly reasonable assume = 0 events concurrent,
provided events associated power network = 0 rarely
causal cycles, unless specific examples bistable logic gate configurations
created. Usually memory features would represented higher-level state
variables (e.g. electronic control unit), leaving domain-based modelling non cyclic
causality.
Generally, possible ordering events time-slot must considered
determine branches (eventually) reach state and, case,
generate number alternative behaviour paths. achieved breadth-first
search determine converging cyclic behaviours system reaches state
identical previously encountered state. assumption made
behaviour; necessary simulation detect cyclic behaviour allow termination
appropriate reporting. Alternatively simulation must seek additional information
advice engineer, if, example, converging state found within reasonable
length behaviour path.
3.2 Component Modelling Examples
graphical notation used describe component models, example Figure 6. State
entry actions placed inside rectangle representing state, event syntax
[if Tc [during]] event name [after [Fc ]][/A] used [x] represents optional element
x. keyword used specify Dc = Tc , also Fc = Tc Fc
unspecified. omitted Dc = >. Fc omitted Fc = >. keyword
427

fiSnooke & Lee

used specify t. specified, event = 0, i.e. immediate
occurrence.
example assume following OM time values F = {mS, Sec, hour, day}
flow levels = {low, normal, high}, Sec normal given OM index zero.
tank given volume may defined event changes empty full
state given time, e.g. F (tank inlet)==normal filling hour provides implicit
volume 21, one order magnitude bigger nominal volume, given chosen
flow time qualitative space. explicitly including volume event conditions
made represent number possible transitions different durations,
example, F (tank inlet)>0 filling tank volume/F (tank inlet), tank volume
defined component qualitative volume value magnitude 21.
flow condition prevents event requires infinite amount time fill tank (0
effectively normal2 )
Figure 6 shows two-part model tank. two levels model shown
left, graphic icons right used display simulation results
schematic containing instance component. structure single zero
resistance tank dissipates energy filling. capacity tank
defined local component variable volume, used combination flow, F(tk),
control change state, allowing several different capacity instances tank
created parameterising model.
Behaviour model - (energy)
OM parameters:
volume

empty
tk.level = 0
S(inlet.tk) =S(vent)

Component
schematic graphics
Tank

F(tk) > 0
S(inlet)=="fluid"during

-F(tk) > 0
emptying volume/F(tk)

filling volume/F(tk)

vent

Tank

full
tk.level = MAX
S(vent.tk) = S(inlet)

inlet

Structure model - (power)
inlet

tk
R=0



vent

Figure 6: Basic 3 tank, energy storage

tank Figure 6 store energy effects gravity ignored
hence increase potential energy tank fills. aspect could
included modelling making tank small magnitude effort source
level non-zero. include capacitance well displacement (zero stored energy
volume) provided duration flow continues prior change state. Atmospheric
pressure often significant qualitative value fluid flow systems behaviour
system may depend upon pressure difference atmospheric pressure
428

fiQualitative Energy-Flow-Based FMEA

point. defined fluid flow domain specialisation Z provides global
node accessible component model represents connection atmosphere.
Examples include vented tank leaking pipe.
different component used illustrate component containing dependent effort
source. Figure 7 shows model (non-horizontal) pipe contains either liquid
gas (air). resistance R(pipe) limits flow pipe represents combined
measure smoothness, length, Reynolds number etc. pipe model parameterised;
ideal pipe produced length = 0. pipe blockage fault model could set R(pipe) = .
filled pipe represents small magnitude effort source. pipe additional
resistance represent pipe wall facilitate fault modelling providing possible
connection A. event conditions cause pipe fill rate inversely proportional
flow it. rightmost event specifies pipe prime itself, ie.
fluid present top inlet (only), fill without externally imposed flow.
Behaviour model - (energy)
OM parameters
length; width

empty
S(upper.pipe) = "air"
S(lower.pipe) = "air"
E(gravity) = 0
F(pipe) > 0
"fluid" S(lower)
lower_filling
length*width/F(tk)

R(pipe) = length/width

-F(pipe) > 0
"fluid" S(upper)
upper_filling
length*width/F(tk)

F(pipe) == 0
"fluid" S(upper)
prime
length*width/F(tk)

upper
leak
lower
leak

full
S(upper.pipe) = "fluid"
S(lower.pipe) = "fluid"
>1
E(gravity) = e

upper

Structure model - (power)
pipe
gravity

8

8



lower

Figure 7: Partial model vertical pipe
Figure 8 illustrates general ways component local behaviour may interact
structure several common component types. Real component models
additional relationships constraints associated qualitative values device states,
dependent details behaviour required.
3.3 System Modelling Circuit Topology
Figure 9 artificial system components similar described previous
section used illustrate topological aspects modelling simulation.
addition valve closed (R = ) open (R = 0) states related external
position input included, together simple pump may activated via activate
deactivate external events. electrical aspects system included
429

fiSnooke & Lee

Relay (power controller)

Transformer

Pump

(Gyrator)

output
structure

r

0/

coil

behaviour
relationship F(coil)

r

switch

primary

R(switch)

r

secondary

motor

E(coil)
E(secondary)
maintain P(coil) = P(secondary)

F(motor)

pump
input

E(pump)

Figure 8: Local/global relationships several types component

pump model would include electrical resistance events would triggered
level power (or current flow) electrical resistance, thus setting effort
level pump appropriately.
qualitative aspects pump easily included model create
different types pump. details design pump may allow flow
inactive case here, however, easy include state-controlled resistance
set pump allow flow inactive. Similarly, additional
condition events, pump made non-self priming fluid present
input. bi-directional pump type requires additional state selection correct
substance input.

Tank
c=0

Tank C

Tank

0

c=1
Tank C

>1

0

>0

Pipe

Pipe

Valve
Pipe
Pipe B

>1

Valve

Pipe B

0/

8

Pipe


Pump

P

Pump
Pipe C

Pipe C

>1

Tank B
Tank B
c=0

0

Figure 9: Pumped System

Consider simulation system Figure 9 state top Table
4 pump off. table summarises changes state component
430

fiQualitative Energy-Flow-Based FMEA

Tank
full

Tank B
empty

Tank C
empty

Pipe
empty
prime, t32
full, = u31

Pipe B
empty

Pipe C
empty

start emptying
part filled
[emptying, t31 ]
prime, t32
full
prime, t32
full, = u31
start filling
part filled
[filling, t31 ]
Chronological non-determinism time t31 : 1 Tank emptying; 2 Tank B filling
Choose > 1
[emptying, t31 ]
empty
drain, t32
empty, = 0
drain, t32
drain, t32
empty, = 0
delete [filling, t31 ]
(S(inlet)==fluid)
Choose > 2
[filling, t31 ]
full
Fluid present
[emptying, t31 ]
continues 1

Table 4: Simulation sequence system Figure 9

horizontal lines points network simulation takes place. Pump Valve
Pipe D, change state maintaining states off, closed empty respectively.
example resistance changes, changes effort sources. Initially
vertical pipes prime fill sequence fluid propagated inflow nodes outflow
nodes, creating two pressure sources cause flow upper lower tank
flow air vent lower tank upper tank.
qualitative ambiguity results two events duration, raising
question could answered without knowing numerical sizes tanks
(and quantity full empty start). case simulation
allowed try alternative behaviours. first Tank becomes empty
Tank B fills steady state reached Tank empty Tank B part filled.
second possibility lower tank becomes filled upper one empties.
causes fluid reach node, built model atmosphere
reports abnormal condition substance air present. Finally
steady state reached Tank empty Tank B full.
431

fiSnooke & Lee

pump switched valve opened, pump effort source causes flow
top tanks pump larger magnitude effort gravity
pipes, however Tank C order size larger simulation essentially reverse
previous example. Tank overfilling spurious prediction simulation
able reason fluid originally came Tank A.
illustration flow ambiguity occurs Tank full valve open
pump off. simulation starts Pipe B filled, Pipe becomes
filled bottom due effort Pipe A. Pipe becomes pressure source
ambiguous opposing effort Pipe A. PipeA PipeB supply
nodes connected supernode allowing specification E(Pipe D.gravity) <
E(Pipe A.gravity) establish flow direction; Pipe fill flow Tank
C, would occur Tank C lower Tank A. large never becomes full,
subsequently drains PipeD.
E(Pipe D.gravity) > E(Pipe A.gravity), Pipe drains (producing air
valve) whereupon cycle repeats. simulation concludes pipe oscillating
full empty state state undetermined modelling resolution. may physical oscillation, however, usually indicates system
state cannot represented level abstraction used.
final possibility E(Pipe D.gravity) = E(Pipe A.gravity), (it filled
height Tank A) flow Pipe D. cases flow Tank C
either full part filled steady state.
last examples deliberately chosen limit representation,
occur FMEA nominal operation, clear indication
detailed numerical model required state behaviour; occur
component fault signal failure mode encountered
requires detailed investigation.
equation-based qualitative model system tanks (Dressler, Bottcher, Montag,
& Brinkop, 1993) allows comparisons drawn approach. Firstly,
authors note important feature also relevant approach; diagnosis (and also
FMEA) complete behaviour description necessary. fact, model containing
much detail likely produce many qualitative ambiguities (or branching behaviour)
unimportant behaviour aspects. problems described detail (Clancy,
Brajnik, & Kay, 1997) strategies avoid problem model revision proposed,
various tools used support revision (QSIM) models. may eventually
result desired simulation result, strategies prove problematic FMEA due
range behaviour likely encountered automatic insertion component
failure models system; need abstracted models nevertheless grounded
basic physics fully compositional.
avoid difficulties simplified numerical model containing relevant behavioural phenomena, used (Dressler et al., 1993) subsequently refined
qualitative one. refinement process, variables values mapped
onto qualitative values (0, +, ); however, values required treated differently various landmarks identified namely height liquid tanks
atmospheric pressure. illustrates problem using general equations,
since numerical equation provide landmarks represent states
432

fiQualitative Energy-Flow-Based FMEA

volume tank, global features A. suggest explicit state identification, temporal state changes, provides vivid qualitative model, within
broadly applicable domain-independent generalised framework. components used
Dressler, valve, require mappings input command flow
valve, using expression set valve flows (denoted i) input output follows:
valve.status = :close valve.i1 = valve.i2 = 0. also note model open valve
defined valve.i1 = valve.i2 therefore propagates flow locally valve
open. requires predetermination cause effect power network, unlike
global network model. approach state description valve (relay Figure
8 analogous electric valve) controls qualitative resistance vivid
(and physically correct) controlling flow, since flow depends pressure,
pressure cannot determined locally. OM model used, partly blocked valve
represented OM higher resistance normal. situation feasible
set flow value based valve control state, depends external system.
node important circuit-based representation pumped system
example, however general Z node may also lead power network ambiguity unless
following condition satisfied. zero node must partition graph two disjoint
sets edges sharing Z supply nodes. topology naturally exists many
practical fluid flow circuits, connection negative (suction)
positive (pressure) parts system pump atmosphere.
others contrived example Figure 10, analysis inherently limited since
impossible determine qualitatively leak Pipe E would cause liquid egress
air ingress. ambiguity indicated direction flow leak
(represented dashed resistances right figure) either end pipe
different respect atmosphere, shown arrows.
example also illustrates two additional resistances used represent leak failure
mode qualitative difference behaviour dependent upon leak position.
resistance magnitude used indicate severity leak zero would produce
complete fracture. arrows leaks PipeC PipeD demonstrate unambiguous qualitative behaviour, however PipeE shows conflicting leak flows respect A.
qualitative result exactly would expect absence numerical information, explicitly signals limit behaviour predictions possible limited
information system. Circuits resistances bridge Z node also
cannot effort values assigned relative Z.

4. Faults Exaggeration Reasoning
Exaggeration reasoning (Weld, 1988b, 1990, 1988a) provides alternative qualitative technique explanation worst case effects without need differential qualitative
calculus. form reasoning therefore suitable detailed system equations, or, global power network, reasoning causality
performed. secondary advantage purpose exaggeration reasoning often
produces concise explanations. drawback, however, qualitative deviation
analysis guaranteed sound, whereas exaggeration reasoning lead false predictions.
433

fiSnooke & Lee

med
P

0

Pipe

Pipe

Pipe B

Pipe B

P

l

l
Pipe E

Pipe E
0

Pipe C



Pipe C

?
Pipe




l1 l2

l




0
Tank

0

Tank

Tank B



l

Pipe



Tank B



Figure 10: forming disjoint graphs

purpose FMEA two reasons exaggerating faults reasonable
strategy: firstly, worst case effects required, results fault models
extremes behaviour; secondly, FMEA intended aid engineer
therefore externally verified.
Using simple electrical torch example, corroded battery contact fault may raise
contact resistance torch may dim, however, resistance increase
change qualitative behaviour. exaggerated fault would OM increase
battery contact simulation predicts OM reduction light output. course
OM reduction light would likely visible, FMEA effect
corrosion leads reduction light reasonable provides significant distinction
fracture effect light circuit activity present.
OM representation provides exaggerated forms faults model significant
differences behaviour. example small leak fluid system pipe may allow air
sucked in, causing mixture fluid air output; fracture position
may result output pump fails operate air.
common approach reasoning qualitatively use qualitative constraint-based deviation model propagate deviation system. Reasoning
deviation works well parameter value changes, less good structure
state changes occur, air ingress example new system state
occurs due air pump. approach strengths, might use
absolute/exaggeration reasoning determine impact faults major states
operating modes (or regions linear behaviour), followed deviations isolate finer
grained effects expected direction value drift due fault.
434

fiQualitative Energy-Flow-Based FMEA

5. Generating FMEA
automated FMEA typically based behaviour simulation many component faults
operational scenarios. Producing FMEA previously described detail
elsewhere (Price, 1998), however, since end goal modelling simulation
effort, summarise main steps follows:
system simulated failed components expected operating conditions.
system simulated component failure modes contained component type models every component instance, operating conditions.
Multiple faults may also considered high failure rate component combinations.
output simulation qualitative value system variables
step (state) simulation. used completely separate functional
model identifies specific (output) behaviour state identified system
functions. Functions one four states (Achieved, Failed, Unexpected
Behaviour, Inoperative) based truth Boolean trigger effect expressions
evaluate simulation output variables. functional model lightweight,
capture hierarchy system functions, including temporal aspects. Full details
functional model previously presented (Bell, Snooke, & Price, 2007).
nominal failure functional states compared, used indicate high
level abstraction, highest risk failed system functions unexpected function
effects associated component fault.
presentation, selection, ranking function states risk factors computed information associated function states component reliability
allows FMEA produced. structure typical automatically generated
FMEA shown Figure 17 second example, discussed Section 7.
simulation step process interest paper following
sections provide two example systems illustrations.

6. Case Study Example: Domestic Heating System
Figure 12 shows schematic part simple domestic central heating system.
complete model includes electrical microcontroller controls voltage pumps
activators, addition thermal system, however focus fluid flow aspect.
gas boiler three way valve retrofitted years initial installation.
Either gas boiler wood burning stove supply hot water. radiators
modelled R = r, pipes ideal (length=0) boilers hot water cylinder
R = r31 much larger diameter radiators. components also
include thermal element flow considered entropy flow rate temperature
effort. connection components compound connection including
fluid thermal circuits. space detail complete set models, however,
Figure 13 shows horizontal pipe model includes fluid thermal aspects.
435

fiSnooke & Lee

fluid flow element resistive propagates fluid based flow direction.
thermal element represented resistance. length pipe determines
thermal resistance pipe conduction flow,
flow, thermal resistance heated substance transported. Thermal
power provided boiler (by combustion) create temperature difference (effort)
inlet outlet. thermal effort applied across radiators
provide power (heat) surrounding air.
aspect heating system interaction fluid flow
temperature networks. flow rate returning temperature fluid
substantially higher ambient air, series connected radiators consume ratio
thermal power based dimensions. low flow rates assumption may
hold. example, represent situation low fluid flow allow
thermal power dissipate, additional resistance included thermal radiator
pipe models allows direct specification based low flow rate, outlet
temperature close inlet temperature boiler (or atmosphere). Figure
13 low flow resistance, controlled flow. Figure 11 shows effect low
fluid flow rate, series radiators. low flow elements changed r31 .
power decreases OM source. first radiator hot (if effort source
higher flow lower), next one OM cooler, on.
valve
8

partial blockage directly limits
fluid flow (not thermal flow)
pipe

pipe

/0
f>1

0

8

/0

E=u

Gas
Boiler


f>2

radiator

pipe
0

pipe
r

0

low_flow

f>2

radiator

radiator
pipe

r

0

low_flow

r>1

r
low_flow

r>1

r>1

pipe
0

Figure 11: Thermal flow circuit low fluid flow situation

Consider small leak vpipe0 directly pump + output. Figure 12
operation gas boiler valve heating position, fluid flows
radiators heat transferred radiators. small amount fluid enters atmosphere, causing small flow fluid header tank replaced water
external water supply.
try holiday scenario simulation results summarised Table 5.
external water supply isolated (in case freezing). flow model pump off,
derived component models connections, Figure 14.
ambiguity concerning gravity sources pipes p1, p7, p11(names abbreviated)
oppose p0, p3, p6, p12, p13 flow magnitude. user provide
436

fiQualitative Energy-Flow-Based FMEA

Synchron
motorised
3 way valve

tank level

heating

+

water

-

Header tank


vpipe16

hpipe2
vpipe1

Air Bleed
Valve

hpipe4

vpipe6

loft level

vpipe12

hpipe8

hpipe9

vpipe
13

-

boiler
upper

P

Manual
valve

+
vpipe0
boiler
lower

Vertical pipe
vpipe3

+

vpipe14

vpipe11



Gas Boiler

vpipe7

P
vpipe15



room level




Radiator

Radiator

Hot Water
Storage

Wood Burning
Stove

Figure 12: Domestic heating system

inactive
8

R(thermal) =
F(pipe) > 0
"fluid" S(in)
flow_normal
actions
S(out) = S(out);

;

F(pipe) < 0
"fluid" S(out)
flow_reverse
actions
S(in) = S(out);

F(pipe) == 0
"fluid" S(out)
"fluid" S(in)
no_flow

active
R(thermal) = 0;
F(pipe) < f<2 actions R(low_flow) = F(pipe);
pipe



R(pipe) = length/width

leak


low_flow
thermal

8

temperature_in

8



OM parameters
length; width



8

/0

temperature_out

Figure 13: Horizontal pipe thermal aspect

437

fiSnooke & Lee

relationship E(vpipe3.gravity) E(vpipe11.gravity), = similarly
two sets (p6=p7, p3=p11, p1=p0+p13+p12). result flow header tank
leak via p16, p12 p13. Air enters p16 longer effort source
(Table 5, row 3). addition secondary low flow exists via p1 p0, flows
opposition other, however constraint allows p1 source predominate.
four sources (p0, p1, p12, p13) turns three flow patterns.
Figure 15 shows flow system zero resistances dead branches removed,
using different style arrow show contributing flow patterns. radiator section
higher resistance therefore main flow due effort p12 p13, drawing
air boiler, p13, pump leak air reaches p12 effort
becomes zero. secondary flow pulls air toward radiators cold side pipe
(p11) becomes empty, whereupon ambiguity effort p3 p0.
specifying gravity sources p3>p1, flow direction reverses p0 p1 fill
air, p11 fills fluid p3. Since pipes length diameter
several possible behaviours; table provide additional information rather
generate alternative behaviours would lead uncertainty fill state
p0, p1, p2. p0, p1, p2, p12, p13 contain air. p3, p11, p6, p7 contain fluid opposing
equal sources system stable.
Returning home turning water causes header tank refill, however,
pump produce flow self prime. p16 prime create low
flow header tank take t31 propagate pump. Hopefully
winter.
subsequent step FMEA scenario wood burning stove pump started
valve opened. large flow water pulled header tank three way
valve wrong way, air water mix return pipe pumped past air
bleed valve, air removed. gas boiler pump restarted
heating system works correctly - small flow header tank leak.
resulting FMEA report highlight effect failure heat output
radiator holiday scenario step severe faults (a
burner fault example) permanent.

7. Case Study Example: Aircraft Fuel System
section describes fuel system provided sponsor work. modelling
simulation described forms part wider objective automate manually created FMEA
reports used generate fault effect relationships input Bayesian network based
diagnostic system. effort is, turn, part recent 32m UK government sponsored
programme seeking research, develop validate necessary technologies use
unmanned aerial systems (ASTRAEA, 2009).
system considered fuel system twin engine light aircraft shown Figure 16
also available physical laboratory simulation configurable rig allowed
validation results using fault insertion via additional components valves
represent leaks. requirements system include thermal aspects
due orientation changes system pressure created pumps
required, resulting little qualitative ambiguity, valves placed
438

fiQualitative Energy-Flow-Based FMEA

u

Pump

8

r>1
0

vpipe1_leak
vpipe0



u>1

u>1

vpipe1
0
boiler
3 way valve

hpipe4

0
0/
Air Bleed
water
Valve
r<1
0

8

8

0/

heating

8

0

hpipe2
0

vpipe14



0

u>1

vpipe6

0
vpipe3

u>1

Hot Water
r<1 Storage

u>1

Radiator
r
r

Radiator

u>1

vpipe15
0

u>1
u>1

vpipe7

vpipe11
0

0
hpipe8
0

hpipe10

0

vpipe12 0
0
r<1

hpipe9

Gas Boiler
u>1

u>1

vpipe16
0

0
vpipe13
u>1

0
u

Pump

Header tank



Figure 14: Flow model derived Figure 12 (joints omitted)

abnormal configurations. created qualitative simulation model physical
laboratory model, resulting aircraft engines represented tanks example.
system involves number tanks, valves pumps allow fuel stored
transferred around aircraft supply engines maintain aircraft trim
flight.
system comprised left right fuel tanks situated aircraft wings (e.g.
OC WT LH) left right auxiliary tanks (e.g. TK LH). engines represented
physical test rig used convenience, tanks EH LH EH RH. wing
tanks connected engines via pumps (e.g. CP FL LH) pressure sensors (e.g.
439

fiSnooke & Lee

vpipe1_leak
>2
f

>2
f

r>1

u>1

>2
f

vpipe0

>2
f

vpipe13
>2
f

u>1

u>1

vpipe1

>2
f

Radiator
r

>2
f

Pump

Radiator
r
>2 f>2 >2
f
f

r

<1

Gas Boiler

>2
>3 f
f

u>1
>2
f

vpipe12

Header
tank



Figure 15: Simplified heating system leak

PT FL LH) flow metres (e.g. FT FL LH), also modelled tank mimic
hardware test rig used, excess fuel returned tank
drawn. Control fuel distribution provided four three-position valves (e.g.
TVL FL...), slaved pairs left right subsystems. basic operation
system supply fuel wing tanks corresponding engine setting
valves normal position. valves also allow fuel supplied
left tank right engine allowing engines fed one tank (crossover
operation). possible feed engines opposite tanks desired, although
normal operating mode. addition fuel transferred wing tanks
auxiliary tanks wing tanks, although possible return fuel
auxiliary tanks. Failure modes provided component categories including
pipe tank leaks, pump failures stuck leaking valve failures every component
instance.
portion resulting FMEA output shown Figure 17. textual descriptions derived functional model (Bell et al., 2007; Price, 1998) provide
easily understood explanation fault effects risk priority. Functions also interpret exaggerated behaviours human-friendly phrases, example return line
tank OM lower flow nominal outflow nominal, virtual relative
level sensor value lower expected. course faults may provide absolute
qualitative value tank level sensor example becomes empty (0) part
filled (l 30 ) expected. consistency fully automated FMEA analysis allows
automated tasks performed diagnosability analysis (Snooke, 2009;
Snooke & Price, 2012). qualitative analysis allows entire FMEA regenerated
following system modification matter seconds differences system
effects presented engineer incremental FMEA. allows unforeseen
implications design changes easily detected.
first row Figure 17 describes effects blocked fuel return pipe near RH
engine different operating modes system called steps output. main
440

fiQualitative Energy-Flow-Based FMEA

Boiler Radiator1
Pipe0
Pipe1
Pipe3
Pipe11
Pipe12 Pipe13 Pipe16 vpipe1 leak
Pump

full
full
full
full
full
full
full
F =f
F =f
F = f
F = f
F =f
F =f
F = f F = f 22 F = f 22
=u
= u31 = u31 = u31 = u31 = u31 = u31 = u31 = u31
Close supply header tank, switch boiler pump.
Flow non-determinism: F = f E(Pipe3)E(Pipe11); F = f E(Pipe6) E(Pipe7);
Flow non-determinism: F = f E(Pipe1) E(Pipe0), E(Pipe12), E(Pipe13);
Resolve> Pipe3=Pipe11; Pipe6=Pipe7; Pipe1=Pipe0+Pipe13+Pipe12

empty F = f 23 F = f 23 F = f 23 F = f 23 F = f 22 F = f 22 empty
=0
empty
=0
empty
=0
empty
F =0
F =0
F = f 23
=0
Flow non-determinism: F = f 23 E(Pipe3) E(Pipe0);
Resolve> Pipe3>Pipe1;
F = f 23 F = f 23 F = f 23 F = f 23
F = f 23
Event non-determinism: Pipe0empty Pipe11full
Resolve> Pipe0;
empty
=0
Event non-determinism: Pipe1empty Pipe11full
Resolve> Pipe11;
F = f 23
full
= u31
full
empty
full
full
full
empty
empty
empty

Table 5: Extract simulation leak fault heating system 15

functional effect RH engine supply malfunction (too much fuel), effect
excess fuel returned tank. also indication RH wing tank
level might lower expected; course theoretical qualitative worst case.
second row deals fracture pipe RH pump. function
effect engine supply failes normal operating mode, additionally
see flow flow transducer, RH wing tank level higher
expected, although probably primary consideration, could used
indicate fuel could diverted remaining engine. fault different effect
cross feed mode (fuel taken opposite wing tank), case LH
tank level higher expected due potential lack returned fuel. Part
RH valve fault shown see fuel returned wrong tank LH
engine run.
qualitative simulation also used generate sets symptoms relate
qualitative measurements, symptoms failures (Snooke, 2009) order allow diagnosability analysis performed aim assisting sensor selection. structure
system greatest influence tasks exploited purely structural
441

fiSnooke & Lee

approach related work (Rosich, 2012; Krysander & Frisk, 2008). complex models
using high order differential equations, dealing sensing individual components
valve example (Krysander & Frisk, 2008) structure approach provides tractability. system product wide sensor placement analysis early design investigation
benefits addition (abnormal) behaviour response multiple interacting
components presence fault, especially pertinent indirect sensing
possible. less detailed qualitative approach allows aspects used,
maintaining tractability. comprehensive coverage system behaviour linked
functional states system allows automated fault isolation activities
subject another paper one authors (Snooke & Price, 2012). qualitative analysis important tasks captures broad regions similar system
behaviour meaningful level abstraction.
Multiple faults used simulation noting combinations fault
may produce qualitative ambiguity, representing critical tipping points system.
example fuel system pump normally operates segments system partitioned
valves. Multiple valve faults result pumps working opposition
complex pipe topology almost certain numerical information required
determine actual flows non-linearity abstracted qualitative
component states. FMEA resulting answer predicting several possible behaviours
reasonable, since highest risk effects significant enough, highlighted
engineer detailed analysis.
Single fault FMEA norm effort involved multiple fault effect
determination. automated FMEA combinations faults feasible, although
selection fault combinations still usually necessary alleviate computational
complexity O(N ) associated exploring concurrent faults previously
discussed (Price & Taylor, 1997).

8. Conclusions
Qualitative simulation powerful modelling concept support wide range
reasoning tasks. range electrical circuit design analysis tools based
approach authors electrical qualitative simulator known mcirq regular
industrial use.
structural behavioural models compositional encode system
functional information make assumptions use. course necessary
decide range phenomena included modelling, library
models typically created specific application area (e.g. automotive electrical, aircraft
fuel system, general plumbing etc), include set qualitative variables
interest, (suitably labelled) magnitudes relevant component failure modes.
models reusable components available systems within application
area. qualitative nature components makes far less complex numerical
equivalents also makes reusable within application area possibly
also application areas. models provide major behaviours relevant
objective high-level reasoning potential effects, detailed analysis
system performance, demonstrated examples.
442

fiQualitative Energy-Flow-Based FMEA

FL2-TR
FL-TR-LH

FL1-TR

FL6-TR

FL5-TR

TJ-TR-LH

TVL-TR-LH

FL7-TR
FL3-TR

CP-TR

TK-AT-LH

CP-AT-LH

TJ-TR-RH

TVL-TR-RH

FL4-TR

TP1-AT-LH

TK-AT-RH

TP2-AT-LH

TP2-AT-RH

DV-AT-LH

DV-AT-RH

TP1-AT-RH
CP-AT-RH

TP3-AT-LH

TP3-AT-RH
RP2-4-RL-LH

OC-WT-LH

RP1-RL-LH

TP1-WT-LH

TP2-WT-LH

DV-WT-LH

F-WT-LH

FL-TR-RH

CK-TR

TJ-AT-LH

RP2-4-RL-RH

FC-RL-RH

RP2-3-RL-LH

TP2-WT-RH

RP2-3-RL-RH

F-WT-RH
TJ-RL-LH
RL-FS-LH

RL-FS-RH

RL2-2-RL-LH

TP1-WT-RH
DV-WT-RH

TJ-RL-RH

RP2-2-RL-RH

TVL-RL-LH

FL1-1-FS-LH

TVL-RL-RH

FL1-1-FS-RH

FL1-2-FS-LH

FL1-2-FS-RH

FL1-3-FS-LH

FL1-3-FS-RH

TVL-FL-LH

TJ-FL-FS-RH

TVL-FL-RH

TP1-FL-LH

TP1-FL-RH

F-FL-LH

F-FL-RH

TP2-FL-LH

TP2-FL-RH

TP4-FL-LH
TP4-FL-RH
CP-FL-LH
CP-FL-RH

RP2-1-RL-LH

TP5-FL-LH
FT-FL-LH

TP5-FL-RH

RP2-1-RL-RH

FT-FL-RH

TP6-FL-LH

TP6-FL-RH

PT-FL-LH

PT-FL-RH

TP7-FL-LH

TP7-FL-RH

IV_FL_LH

TJ-FL-LH

TP3-FL-LH

TP8-FL-LH

IV_FL_RH

TP3-FL-RH

TJ-FL-FS-LH

OC-WT-RH

RP1-RL-RH

TJ-AT-RH

FC-RL-LH

TP8-FL-RH
TJ-FL-RH

Name

BravoFuelSyst em.vdx

Rev

4

Hist ory nst - original version 7/ 1/ 2008
nst - name changes 11/ 1/ 2008
nst - changed engines 25/ 1/ 2008
nst - changed engine ret urn. 8/ 4/ 2008

EH_RH

EH_LH

Bravo
ADTFschematic
Generic
fuel
system

Figure 16: Example fuel system

443

fiSnooke & Lee

Figure 17: Portion FMEA output fuel system

paper makes two notable contributions. Firstly, provides improved circuit
reasoning algorithm gives complete solution possible circuit topologies.
achieved solving problem non series/parallel reducible circuits. Also restriction
single sources removed resultant simulator called (m2 cirq), Multiple
source mcirq.
Secondly, qualitative network modelling method placed context modelling ontology based separating global local behaviour based power flow.
component models fluid systems involve aspects electrical circuits
paper introduces several additional fundamental concepts necessary global level
fluid flow modelling, including distinguished zero node propagation substances
network. techniques illustrated modelling range common fluid
flow components simulation.
ability QR make predictions across multiple system states operating
modes complementary techniques. example fault tree analysis diagnosis
similar fuel system example presented Section 7 performed (Hurdle,
Bartletta, & Andrews, 2009) uses pattern recognition deal multiple states.
production fault trees scenario (state) identification labour intensive manually
performed process described exactly qualitative terms produced QR
behaviour prediction high flow. therefore likely accuracy coverage
FTA might improved reduction effort using QR behaviour predictions
instead manual effort.
444

fiQualitative Energy-Flow-Based FMEA

Many software tools developed perform variety design analysis
electrical systems, mentioned introduction paper. substituting
enhanced simulator tools analyses performed types
complex topology systems multiple domain systems qualitative behaviours
used answer failure mode questions.
8.1 Acknowledgments
work supported Aberystwyth University, Welsh Assembly Government,
BAE Systems DTI ASTRAEA Programme. also thank anonymous reviewers
helpful comments.

References
ASTRAEA (2009). http://www.projectastraea.co.uk/. ASTRAEA project homepage. Accessed 15 February 2013.
Bell, J., Snooke, N. A., & Price, C. J. (2007). language functional interpretation
model based simulation. Advanced Engineering Informatics, 21 (4), 398409.
Brown, F. T. (2010). Bond-graph based simulation thermodynamic models. Journal
Dynamic Systems, Measurement, Control, 132 (6), 064501.
Clancy, D. J., Brajnik, G., & Kay, H. (1997). Model revision: Techniques tools
analyzing simulation results revising qualitative models. 11th International
Workshop Qualitative Reasoning, Cortona, Siena Italy.
Clancy, D. J., & Kuipers, B. (1997). Dynamic chatter abstraction: scalable technique
avoiding irrelevant distinctions qualitative simulation. 11th International
Workshop Qualitative Reasoning Physical Systems (QR 97), pp. 6776.
de Kleer, J. (1984). circuits work. Artificial Intelligence, 24, 205280.
Dressler, O., Bottcher, C., Montag, M., & Brinkop, A. (1993). Qualitative quantitative models model-based diagnosis system ballast tank systems. International Conference Fault Diagnosis (TOOLDIAG), Toulouse. Extended report
version available http://mqm.in.tum.de/ dressler/arm-1-93.ps.
Flores, J. J., & Farley, A. M. (1999). Reasoning linear circuits: model-based
approach. Artificial Intelligence Communications, 12 (1-2), 6177.
Fouche, P., & Kuipers, B. (1990). assessment current qualitative simulation techniques. 4th International Workshop Qualitative Reasoning Physical Systems (QR-90), pp. 195209.
Ghiaus, C. (1999). Fault diagnosis air conditioning systems based qualitative bond
graph. Energy Buildings, 30 (3), 221 232.
Hurdle, E., Bartletta, L., & Andrews, J. (2009). Fault diagnostics dynamic system
operation using fault tree based method. Reliability Engineering System Safety,
94 (9), 13711380.
445

fiSnooke & Lee

Krysander, M., & Frisk, E. (2008). Sensor placement fault diagnosis. Systems, Man
Cybernetics, Part A: Systems Humans, IEEE Transactions on, 38 (6), 1398
1410.
Kuipers, B. J. (1986). Qualitative simulation. Artificial Intelligence, 29, 289338.
Lee, M. H. (1999). Qualitative circuit models failure analysis reasoning. Artificial Intellligence, 111, 239276.
Lee, M. H. (2000a). Many-valued logic qualitative modelling electrical circuits.
Proceedings 14th International Workshop Qualitative Reasoning, (QR-2000).
Lee, M. H. (2000b). Qualitative modelling linear networks engineering applications.
Proceedings 14th European Conference Artificial Intelligence ECAI 2000, pp.
161165, Berlin.
Lee, M. H., Bell, J., & Coghill, G. M. (2001). Ambiguities deviations qualitative circuit analysis. Proceedings 15th International Workshop Qualitative Reasoning,
QR 01, pp. 5158.
Mauss, J., & Neumann, B. (1996). Qualitative reasoning electrical circuits using
series-parallel-star trees. Proceedings 10th International Workshop Qualitative
Reasoning, QR-96, pp. 147153.
Mosterman, P. J., & Biswas, G. (2000). comprehensive methodology building hybrid
models physical systems. Artificial Intelligence, 121, 171 209.
OMG (2012). Documents Associated Unified Modeling Language (UML) Version 2.5.
Object Mangement Group, http://www.omg.org/spec/UML/2.5/Beta1/PDF.
Paynter, H. M. (1961). Analysis Design Engineering Systems. MIT Press.
Price, C. J. (1998). Function-directed electrical design analysis. Artificial Intelligence
Engineering, 12 (4), 445456.
Price, C. J., Snooke, N. A., & Landry, J. (1996). Automated sneak identification. Engineering Applications Artificial Intelligence, 9 (4), 423427.
Price, C. J., Snooke, N. A., & Lewis, S. D. (2006). layered approach automated
electrical safety analysis automotive environments. Computers Industry, 57 (5),
451461.
Price, C. J., & Taylor, N. S. (1997). Multiple fault diagnosis FMEA. Proc.
Ninth Conference Innovative Applications Artificial Intelligence (IAAI 97), pp.
10521057. AAAI.
Price, C. J., & Struss, P. (2004). Model-based systems automotive industry. AI
Magazine, 24 (4), 1734.
Price, C. J., Wilson, M. S., Timmis, J., & Cain, C. (1996). Generating fault trees
FMEA. 7th International Workshop Principles Diagnosis, pp. 183190, Val
Morin, Canada.
Raiman, O. (1991). Order magnitude reasoning. Artificial Intelligence, 51, 1138.
446

fiQualitative Energy-Flow-Based FMEA

Rosich, A. (2012). Sensor placement fault detection isolation based structural
models. 8th IFAC Symposium Fault Detection, Supervision Safety Technical Processes (SAFEPROCESS), pp. 391396. IFAC.
Samantaray, & Ould (2011). Bond Graph Modelling Engineering Systems, chap. Bond
Graph Model-Based Fault Diagnosis. Springer. ISBN 978-1-4419-9368-7.
Savakoor, D. S., Bowles, J. B., & Bonnell, R. D. (1993). Combining sneak circuit analysis failure modes effects analysis. Proceedings Annual Reliability
Maintainability Symposium, pp. 199205.
Skorstad, G. (1992a). Finding stable causal interpretations equations. Faltings, &
Struss (Eds.), Recent advances qualitative physics, pp. 399413. MIT Press.
Skorstad, G. (1992b). Towards qualitative lagrangian theory fluid flow. Proceedings
tenth national conference Artificial intelligence, AAAI92, pp. 691696. AAAI
Press.
Snooke, N. A. (1999). Simulating electrical devices complex behaviour. AI Communications, 12 (1,2), 4558.
Snooke, N. A. (2007). M2 CIRQ: Qualitative fluid flow modelling aerospace fmea applications. Proceedings 21st international workshop qualitative reasoning, pp.
161169.
Snooke, N. A. (2009).
automated failure modes effects analysis based
visual matrix approach sensor selection diagnosability assessment.
online proc. Prognostics Health Management Conference (PHM09),
http://www.phmsociety.org/references/proceedings. PHM Society.
Snooke, N., & Price, C. (2012). Automated FMEA based diagnostic symptom generation.
Advanced Engineering Informatics, 26 (4), 870 888.
Struss, P. (2003). discrete charm diagnosis based continuous models. IFAC
Safeprocess 03. International Federation Automatic Control.
Struss, P., Malik, A., & Sachenbacher, M. (1995). Qualitative modelling key.
Workshop Notes 6th International Workshop Principles Diagnosis (DX-95),
pp. 99106.
Sussman, G. J., & Steele Jr, G. L. (1980). CONSTRAINTS: language expressing
almost-hierarchical descriptions. Artificial Intelligence, 14, 139.
Trave-Massuyes, L., Ironi, L., & Dague, P. (2004). Mathematical foundations qualitative
reasoning. AI Magazine., 24 (4), 91106.
Tsai, J. J.-H., & Gero, J. S. (2010). qualitative approach energy-based unified
representation building design. Automation Construction, 19 (1).
Weld, D. S. (1988a). Choices comparative analysis: DQ analysis exaggeration?. Artificial Intelligence Engineering, 3 (3), 174 180.
Weld, D. S. (1988b). Comparative analysis. Artificial Intelligence, 36 (3), 333 373.
Weld, D. S. (1990). Exaggeration. Artificial Intelligence, 43 (3), 311 368.

447

fiJournal Artificial Intelligence Research 46 (2013) 607-650

Submitted 9/12; published 4/13

Efficient Computation Shapley Value
Game-Theoretic Network Centrality
Tomasz P. Michalak

Tomasz.Michalak@cs.ox.ac.uk

Department Computer Science, University Oxford
OX1 3QD, UK
Institute Informatics, University Warsaw
02-097 Warsaw, Poland

Karthik .V. Aadithya

Aadithya@eecs.berkeley.edu

Department Electrical Engineering Computer Sciences
University California
Berkeley, CA 94720-4505, United States, USA

Piotr L. Szczepaski

P.Szczepanski@stud.elka.pw.edu.pl

Institute Informatics
Warsaw University Technology
00-661 Warsaw, Poland

Balaraman Ravindran

Ravi@cse.iitm.ac.in

Computer Science Engineering
Indian Institute Technology Madras
Chennai, 600 036, India

Nicholas R. Jennings

NRJ@ecs.soton.ac.uk

School Electronics Computer Science
University Southampton
SO17 1BJ Southampton, UK

Abstract
Shapley valueprobably important normative payoff division scheme coalitional gameshas recently advocated useful measure centrality networks.
However, although approach variety real-world applications (including social
organisational networks, biological networks communication networks), computational properties widely studied. date, practicable approach
compute Shapley value-based centrality via Monte Carlo simulations
computationally expensive guaranteed give exact answer.
background, paper presents first study computational aspects Shapley
value network centralities. Specifically, develop exact analytical formulae Shapley value-based centrality weighted unweighted networks develop efficient
(polynomial time) exact algorithms based them. empirically evaluate algorithms two real-life examples (an infrastructure network representing topology
Western States Power Grid collaboration network field astrophysics)
demonstrate deliver significant speedups Monte Carlo approach.
instance, case unweighted networks algorithms able return exact
solution 1600 times faster Monte Carlo approximation, even allow
generous 10% error margin latter method.
c
2013
AI Access Foundation. rights reserved.

fiMichalak, Aadithya, Szczepaski, Ravindran, & Jennings

1. Introduction
many network applications, important determine nodes edges
critical others. Classic examples include identifying important hubs road
network (Schultes & Sanders, 2007), critical functional entities protein network
(Jeong, Mason, Barabasi, & Oltvai, 2001), influential people social network
(Kempe, Kleinberg, & Tardos, 2003). Consequently, concept centrality, aims
quantify importance individual nodes/edges, extensively studied
literature (Koschtzki, Lehmann, Peeters, Richter, Tenfelde-Podehl, & Zlotowski, 2005;
Brandes & Erlebach, 2005).
v6

v4
v1
v5

v9

v7

v2
v10

v8
v12

v3

v11

v13

Figure 1: Sample network 13 nodes
Generally speaking, centrality analysis aims create consistent ranking nodes within
network. end, centrality measures assign score node way corresponds importance node given particular application. Since importance
depends context problem hand, many different centrality measures
developed. Three well-known widely applied are: degree centrality,
closeness centrality betweenness centrality.1 paper, refer measures
conventional/standard centrality. Degree centrality, brief, quantifies power
node degree, i.e., number adjacent edges. instance, sample
network Figure 1, nodes v1 v2 degree 5 and, judged degree centrality,
important nodes within entire network. Conversely, closeness centrality
focuses distances among nodes gives high value nodes close
nodes. measure, node v8 Figure 1 ranked top. last measure
betweenness centralityconsiders shortest paths (i.e., paths use minimal number
links) two nodes network. shortest paths node belongs to,
important is. measure, v2 Figure 1 important
nodes (including v1 v8 , chosen measures important
node). Clearly, measures expose different characteristics node. Consider,
instance, epidemiology application, aim identify people (i.e., nodes)
social network biggest influence spread disease
become focal point prevention emergency measures. Here, degree centrality
1. Koschtzki et al. (2005) Brandes Erlebach (2005) give good overview
centrality measures.

608

fiComputation Shapley Value Game-Theoretic Network Centrality

ranks top nodes biggest immediate sphere influencetheir infection would lead
highest number adjacent nodes exposed disease. hand,
closeness centrality identifies nodes whose infection would lead fastest spread
disease throughout society. Finally, betweenness centrality reveals nodes
play crucial role passing disease one person network another.2
common feature aforementioned standard measures assess
importance node focusing role node plays itself. However,
many applications approach inadequate synergies may occur
functioning nodes considered groups. Referring Figure 1
epidemiology example, vaccination individual node v6 (or v7 v8 ) would prevent
spread disease left right part network (or vice versa).
However, simultaneous vaccination v6 , v7 v8 would achieve goal. Thus,
particular context, nodes v6 , v7 v8 play significant role individually,
together do. quantify importance groups nodes, notion
group centrality introduced Everett Borgatti (1999). Intuitively, group centrality
works broadly way standard centrality, focus functioning
given group nodes, rather individual nodes. instance, Figure 1, group
degree centrality {v1 , v2 } 7 7 distinct adjacent nodes.
Although concept group centrality addresses issue synergy functions
played various nodes, suffers fundamental deficiency. focuses particular,
priori determined, groups nodes clear construct consistent ranking
individual nodes using group results. Specifically, nodes
valuable group ranked top? important nodes belong
group highest average value per node? focus nodes
contribute every coalition join? fact, many possibilities
choose from.
framework address issue game theoretic network centrality measure.
detail, allows consistent ranking individual nodes computed way
accounts various possible synergies occurring within possible groups nodes (Grofman
& Owen, 1982; Gmez, Gonzlez-Arangena, Manuel, Owen, Del Pozo, & Tejada, 2003;
Suri & Narahari, 2008). Specifically, concept builds upon cooperative game theorya
part game theory agents (or players) allowed form coalitions order
increase payoffs game. Now, one fundamental questions cooperative
game theory distribute surplus achieved cooperation among agents.
end, Shapley (1953) proposed remunerate agents payoffs correspond
individual marginal contributions game. detail, given agent,
individual marginal contribution measured weighted average marginal increase
payoff coalition agent could potentially join. Shapley famously proved
conceptknown since Shapley valueis division scheme
meets certain desirable normative properties.3 Given this, key idea game theoretic
network centrality define cooperative game network agents
nodes, coalitions groups nodes, payoffs coalitions defined meet
2. differences interpretation standard centralities see work Borgatti Everett (2006).
3. See Section 3 details.

609

fiMichalak, Aadithya, Szczepaski, Ravindran, & Jennings

requirements given application. means Shapley value agent
game interpreted centrality measure represents average
marginal contribution made node every coalition nodes.4
words, Shapley value answers question construct consistent ranking
individual nodes groups nodes evaluated.
detail, Shapley value-based approach centrality is, one hand, much
sophisticated conventional measures, accounts group nodes
Shapley value derives consistent ranking individual nodes.
hand, confers high degree flexibility cooperative game network
defined variety ways. means many different versions Shapley value-based
centrality developed depending particular application consideration,
well features network analyzed. prominent example,
specific Shapley value-based centrality measure developed crafted particular
application, consider work Suri Narahari (2008) study problem selecting
top-k nodes social network. problem relevant applications
key issue choose group nodes together biggest influence
entire network. include, example, analysis co-authorship networks,
diffusion information, viral marketing. new approach problem, Suri
Narahari define cooperative game value group nodes equal
number nodes within, adjacent to, group. words, assumed
agents sphere influence reaches immediate neighbors group. Whereas
definition game natural extension (group) degree centrality discussed
above, Shapley value nodes game constitutes new centrality metric is,
arguably, qualitatively better standard degree centrality far nodes influence
concerned. intuition behind visible even small network Figure 1. terms
influence, node v1 important v2 , node connected
v4 v5 . Without v1 impossible influence v4 v5 , neighbor v2
accessible node. Thus, unlike standard degree centrality, evaluates
v1 v2 equally, centrality based Shapley value game defined Suri
Narahari recognizes difference influence assigns higher value v1 v2 .
Unfortunately, despite advantages Shapley value-based centrality conventional
approaches, efficient algorithms compute yet developed. Indeed, given
network G(V, E), V set nodes E set edges, using original
Shapley value formula involves computing marginal contribution every node every
coalition O(2|V | ). exponential computation clearly prohibitive bigger
networks (of, e.g, 100 1000 nodes). networks, feasible approach currently
outlined literature Monte-Carlo sampling (e.g., Suri & Narahari, 2008; Castro,
Gomez, & Tejada, 2009). However, method inexact, also
time-consuming. instance, shown simulations, weighted network
16,000 nodes 120,000 edges, Monte Carlo approach iterate 300, 000
4. note division schemes power indices cooperative game theory, Banzhaf
power index (Banzhaf, 1965), could also used centrality measures (see, instance, discussion
work Grofman & Owen, 1982). However, like literature, focus Shapley
value due desirable properties.

610

fiComputation Shapley Value Game-Theoretic Network Centrality

b)

a)
v3

v5

v4

1
10

v1
v4

v2
v6

v1
v5

1

3

v6

2

v7

1
v2

v3

Figure 2: Sample unweighted weighted networks 6 7 nodes, respectively.
times entire network produce approximation Shapley value
40% error margin.5 Moreover, exponentially iterations needed reduce
error margin.
background, develop polynomial-time algorithms compute Shapley valuebased centrality. Specifically, focus five underlying games defined network;
games extend, various directions, standard notions degree closeness centrality.
starting point, consider game defined Suri Narahari propose
exact, linear-time algorithm compute corresponding Shapley value-based centrality.
denote game g1 . analyse computational properties four
games defined networks. denote g2 , g3 , g4 , g5 , respectively.
games captures different flavor centrality, all, similarly game
Suri Narahari, embrace one fundamental centrality idea: given group nodes C,
function defines value C game must somehow quantify sphere
influence C nodes network. particular:
g2 game value coalition C function size number
nodes immediately reachable least k different ways C.
game inspired Bikhchandani, Hirshleifer, Welch (1992) instance
general threshold model introduced Kempe, Kleinberg, Tardos (2005).
natural interpretation: agent becomes influenced (with ideas, information,
marketing message, etc.) least k neighbors already become
influenced. instance, given k = 2, value coalition {v1 , v2 } Figure 2a
4 coalition size 2 two neighbors less 2 edges
adjacent coalition.
g3 game concerns weighted graphs (unlike g1 g2 ). Here, value coalition C
depends size set nodes within cutoff distance C, measured
shortest path lengths weighted graph. example, Figure 2b,
cutoff set 8 coalition {v2 } value 4 able influence 3 nodes v3 ,
v6 , v7 8 away {v2 }. cutoff distance
interpreted radius sphere influence coalition.
5. See Section 5 exact definition error margin.

611

fiMichalak, Aadithya, Szczepaski, Ravindran, & Jennings

Game Graph
Value coalition C, i.e., (C)
Complexity
Accuracy
g1
U W (C) number nodes C
O(|V | + |E|)
exact
immediately reachable C
g2
U W (C) number nodes C
O(|V | + |E|)
exact
immediately reachable C,
via least k different edges
g3
W (C) number nodes C O(|V ||E| + |V |2 log|V |) exact
dcutoff away
g4
W (C) sum f (.)s nonO(|V ||E| + |V |2 log|V |) exact
-increasing functions distance
C nodes
g5
W (C) number nodes C
O(|V ||E|)
approx.
directly connected C via edges
5-10%
sum weights exceeds Wcutoff
Table 1: Games considered paper results (U W denotes unweighted graphs
W weighted).
g4 game generalizes g3 allowing value C specified size
arbitrary non-increasing function f (.) distance C nodes
network. instance, value {v1 , v3 } Figure 2b function
1
1
f (d) = 1+d
2 1 + 3 21 + 1 31 + 1 14 = 4 12
. intuition
coalition influence closer nodes awaya property
cannot expressed standard closeness centrality. Thus, g4 seen
extension closeness centrality.
g5 last game extension g2 case weighted networks. Here, value
C depends adjacent nodes connected coalition weighted
edges whose sum exceeds given threshold wcutof f (recall g2 threshold
defined simply integer k). Whereas g3 g4 weights edges interpreted
distance, g5 interpreted power influence. example,
Figure 2b, threshold vertex 5, value coalition {v1 , v3 } 3
coalition size two enough power influence one additional
node v2 .
computation Shapley value five games (see Table 1
overview) main focus paper. Shapley values extensions either
degree closeness centrality metrics applications settings
influence nodes nodes network evaluated. results
summarized follows:

demonstrate possible exactly efficiently compute number
Shapley value-based network centrality measures. methods take advantage
612

fiComputation Shapley Value Game-Theoretic Network Centrality

network structure, well specifics underlying game defined
network.
first four games, derive closed-form expressions Shapley values.
Based these, provide exact linear polynomial-time algorithms efficiently
compute Shapley values, i.e., without need enumerate possible coalitions.
Specifically, algorithms run O(|V | + |E|) g1 g2 O(|V ||E| +
|V |2 log|V |) g3 g4 . Furthermore, fifth measure centrality, develop
closed-form polynomial time computable Shapley value approximation. algorithm
running time O(|V ||E|) experiments show approximation error
5% large networks. summary algorithms performance
found Table 1.
evaluate algorithms two real-life examples: infrastructure network representing topology Western States Power Grid collaboration network
field astrophysics. results show algorithms deliver significant
speedups Monte Carlo simulations. instance, given unweighted network
Western States Power Grid, algorithms return exact Shapley value g1
g2 1600 times faster Monte Carlo method returns approximation
10% error margin.
remainder paper organized follows. Section 2 discuss related work.
Notation preliminary definitions presented Section 3. Section 4 analyse
five types centrality-related coalitional games propose polynomial time Shapley value
algorithms them. results numerical simulations presented Section 5
(with details simulation setup presented Appendix A). Conclusions future
work follow. Finally, Appendix B provides summary key notational conventions.

2. Related Literature
issue centrality one fundamental research directions network analysis
literature. particular, Freeman (1979) first formalise notion centrality
presenting conventional centrality measures: degree, closeness betweenness. Many
authors subsequently worked developing new centrality measures, refining existing
ones (e.g., Bonacich, 1972; Noh & Rieger, 2004; Stephenson & Zelen, 1989), developing
algorithms efficient centrality computation (e.g., Brandes, 2001; Eppstein & Wang, 2001).
context, Grofman Owen (1982) first apply game theory topic
centrality, focused Banzhaf power index (Banzhaf, 1965). followup work, Gmez et al. (2003) combined Myersons (1977) idea graph-restricted games
(in feasible coalition induced subgraph graph) concept
centrality proposed new Shapley value-based network centrality measures. contrast
Gmez et al., Suri Narahari (2008, 2010) assumed coalitions feasible,
approach also adopt paper.
fundamental problem conventional models coalitional games, i.e., exponential complexity number agents, tackle paper, also
613

fiMichalak, Aadithya, Szczepaski, Ravindran, & Jennings

1

2
v1

1
v2

4
3
1
v3
Figure 3: Induced-subgraph representation sample coalitional game 3 players.
game, values coalitions {v1 }, {v2 }, {v3 }, {v1 , v2 }, {v1 , v3 }, {v2 , v3 }, {v1 , v2 , v3}
1, 1, 1, 1+1+2, 1+1+3, 1+1+4, 1+1+1+2+3+4, respectively.
studied literature algorithmic aspects coalitional games. Indeed, since seminal work Deng Papadimitriou (1994), issue received considerable attention
computer scientists. Specifically, alternative straightforward (but exponential) listing possible coalitions, number authors proposed efficient
representations coalitional games. representations fall two main categories
(Wooldridge & Dunne, 2006):
give characteristic function specific interpretation terms combinatorial structures graphs. approach adopted by, instance,
Deng Papadimitriou (1994), Greco, Malizia, Palopoli, Scarcello (2009),
Wooldridge Dunne (2006) advantage ensuing representation
always guaranteed succinct. However, disadvantage always
fully expressive, i.e., cannot represent coalitional games.
try find succinct, still fully expressive, representation. is,
instance, approach adopted Conitzer Sandholm (2004), Ieong Shoham
(2005), Elkind, Goldberg, Goldberg, Wooldridge (2009). representations general completely capture coalitional games interest,
although always guaranteed succinct.
Unfortunately, even succinctly representable games, computing Shapley value
shown NP-Hard (or even worse, #P-Complete) many domains, including weighted voting games (Deng & Papadimitriou, 1994), threshold network flow games
(Bachrach & Rosenschein, 2009) minimum spanning tree games (Nagamochi, Zeng,
Kabutoya, & Ibaraki, 1997). Similarly, Aziz, Lachish, Paterson, Savani (2009a) obtained negative results related problem computing Shapley-Shubik power index
spanning connectivity games based undirected, unweighted multigraphs.
Also, Bachrach, Rosenschein, Porat (2008b) showed computation Banzhaf
index connectivity games, agents vertices control adjacent edges
aim become connected certain set primary edges, #P-Complete.
Fortunately, positive results also discovered. Probably known
among due Deng Papadimitriou (1994) Ieong Shoham (2005).
614

fiComputation Shapley Value Game-Theoretic Network Centrality

detail, Deng Papadimitriou proposed representation based weighted graphs,
node interpreted agent, weight edge interpreted value
cooperation two agents connected edge.6 value
coalition defined sum weights internal edges, or, words,
weights edges belonging subgraph induced members coalition. threeplayer example formalism, called induced-subgraph representation, found
Figure 3. downside representation fully expressive. However,
upside that, games formalised weighted graphs, representation
always concise. Furthermore, allows Shapley value computed time linear
number players. Specifically, case, Shapley value given
following formula:

Shapley Value(vi ) = vi self-loop weight +
vj

weight edge vi vj
.
2
neighbours v
X

(1)



Ieong Shoham (2005) developed representation consisting finite set logical rules
following form: Boolean Expression Real Number, agents atomic
boolean variables. representation, value coalition equal sum
right sides rules whose left sides satisfied coalition. representation, called marginal contribution networks (or MC-Nets short) (i) fully expressive
(i.e., used model game), (ii) exponentially concise games,
importantly, (iii) allows Shapley value computed time linear
size representation, provided boolean expressions rules conjunctions
(either positive negative) atomic literals. MC-Nets, rules interesting
game-theoretic interpretation, rule directly specifies incremental marginal contribution made agents featured rule. Now, using additivity axiom met
Shapley value, possible consider every rule separate simple game,
using axioms straightforwardly compute Shapley value simple game,
and, finally, sum results simple games obtained Shapley value. Building this, Elkind et al. (2009) developed extensions MC-Nets sophisticated
(read-once) boolean expressions, Michalak, Marciniak, Samotulski, Rahwan, McBurney, Wooldridge, Jennings (2010a), Michalak, Rahwan, Marciniak, Szamotulski,
Jennings (2010b) developed generalizations coalitional games externalities. Another
recently proposed representation formalism coalitional games allows polynomial
calculations Shapley value decision diagrams (Bolus, 2011; Aadithya, Michalak, &
Jennings, 2011; Sakurai, Ueda, Iwasaki, Minato, & Yokoo, 2011). Now, MC-Nets offer
fully-expressive representation works arbitrary coalitional games, possible
speed Shapley value computation focusing specific (not necessarily fully expressive) classes games. One particular class games investigated detail
weighted voting, approximate (but strictly polynomial) (Fatima, Wooldridge,
& Jennings, 2007) exact (but pseudo-polynomial) algorithms (Mann & Shapley, 1962;
Matsui & Matsui, 2000) proposed. Chalkiadakis, Elkind, Wooldridge (2011)
provided comprehensive discussion literature.
6. Also self-loops allowed.

615

fiMichalak, Aadithya, Szczepaski, Ravindran, & Jennings

Whereas choice representation foremost consideration efficient Shapley value computation context conventional coalitional games, paper,
face rather different set challenges:
Unlike conventional coalitional games, conciseness usually issue networks context. games aim capture network centrality
notions completely specified (a) underlying network compactly represented
graph, (b) concise closed-form characteristic function expression evaluating coalition values (see next section example). Rather, issue
exact specification characteristic function dictated computational considerations, real-world application game theoretic network
centrality. words, choice representation Shapley value computation
already fixed centrality consideration.
games paper designed reflect network centrality, characteristic function definition often depends highly non-trivial way underlying
graph structure. Specifically, value assigned characteristic function
subset nodes depends subgraph induced nodes, also
relationship subgraph rest network. example,
value assigned coalition nodes may based shortest path lengths
nodes outside coalition, may depend relationship coalition
neighbors.
Therefore, specific challenge tackle efficiently compute Shapley value, given
network game defined it, coalition values game given
closed-form expression depends non-trivially network. key question
take advantage (a) network structure, (b) functional form
coalition values, compute Shapley values efficiently, i.e., without need
enumerate possible coalitions.
Finally, conclude section mentioning Shapley value solution concepts game theory applied network-related problems. instance,
application Shapley value (and Nucleolus) problem cost allocation
electric market transmission system considered Zolezzi Rudnick (2002),
though computational aspects discussed. problem maximizing probability hitting strategically chosen hidden virtual network placing wiretap
single link communication network analysed Aziz, Lachish, Paterson, Savani
(2009b). problem viewed two-player win-lose (zero-sum) game, wiretap
game. authors provide polynomial-time computational results game,
also show one (key) strategies nucleolus simple cooperative
spanning connectivity game (Aziz et al., 2009a) mentioned above.

3. Preliminaries Notation
section formally introduce basic concepts graph theory cooperative
game theory used throughout paper. look closely sample coalitional
616

fiComputation Shapley Value Game-Theoretic Network Centrality

game defined network Shapley value game used
centrality measure.
graph (or network ) G consists vertices (or nodes) edges, sets
denoted V (G) E(G), respectively. Every edge set E(G) connects two vertices
set V (G).7 (u, v) denote edge connecting vertices u, v V (G). number
edges incident vertex called vertex degree. neighboring vertices v V
vertices connected v graph. weighted network weight (label)
associated every edge E(G). path is, informally, sequence connected edges.
shortest path problem find path two given vertices sum
edge weights minimized.
formalize notions coalitional games Shapley value. Specifically, let
us denote = {a1 , . . . , a|A| } set players participate coalitional game.
characteristic function : R assigns every coalition C real number representing
quality performance, assumed () = 0. characteristic function
game tuple (A, ). Assuming grand coalition, i.e., coalition
agents game, highest value formed, one key questions
coalitional game theory distribute gain cooperation among agents
meet certain normative/positive criteria. end, Shapley (1953) proposed
evaluate role agent game computing weighted average marginal
contributions agent possible coalitions belong to. importance
Shapley value stems fact unique division scheme meets four
desirable criteria:
(i) efficiency wealth available agents grand coalition distributed
among them;
(ii) symmetry payoffs agents depend identity;
(iii) null player agents zero marginal contributions coalitions receive zero
payoff;
(iv) additivity values two games sum value computed sum
games.
order formalize concept, let (A) denote permutation agents A,
let C (i) denote coalition made predecessors agent ai . formally,
denote (j) location aj , then: C (i) = {aj : (j) < (i)}. Shapley
value ai , denoted SVi (), defined average marginal contribution ai
coalition C (i) (Shapley, 1953):
SVi () =

1 X
[(C (i) {ai }) (C (i))].
|A|!

(2)



7. Whereas main focus paper undirected graphs, also show results
readily extended case directed graphs.

617

fiMichalak, Aadithya, Szczepaski, Ravindran, & Jennings

Shapley provides following intuition behind formula: imagine players
arrive meeting point random order, every player ai arrives receives
marginal contribution arrival would bring already meeting point.
average contributions possible orders arrival, obtain SVi (),
ai payoff game.
formula (2) also stated equivalent, computationally less involved,
form:
X |C|!(|A| |C| 1)!
SVi () =
[(C {ai }) (C)].
(3)
|A|!
CA\{ai }

network context, use G define coalitional game (V (G), ) set
agents = V (G) characteristic function . agents coalitional game
vertices graph G. Thus, coalition agents C simply subset V (G).
Furthermore, characteristic function : 2V (G) R function depends
graph G long satisfies condition () = 0. use phrase value
coalition C informally refer (C).
first consider sample characteristic function game defined network, well
Shapley value becomes centrality measure. discuss advantages
game theoretic network centrality conventional measures.
detail, consider notion closeness centrality node graph G(V, E),
traditionally defined reciprocal average distance node
(reachable) nodes graph (Koschtzki et al., 2005). definition captures
intuitive idea node close proximity many nodes valuable
virtue central location, hence assigned higher centrality score.
measure, however, fails recognize importance combinations nodes.
example, consider typical application closeness centrality: disseminating piece
information nodes network. time point dissemination process,
define random variable Ct subset nodes actively involved propagating
information. situation, new node added Ct would make maximum contribution
diffusion information close proximity nodes currently
close proximity node Ct . Thus, conventional closeness centrality
takes account average proximity nodes, actual importance node
actual applications based different measure: proximity nodes
close proximity random variable Ct .
show coalitional game theory used construct centrality measure
faithfully models situation. Let C arbitrary subset nodes
given network G(V, E). Then, every C, assign value (C) given
(C) =

X
vV (G)

1
,
1 + min{d(u, v)|u C}

d(u, v) distance nodes u v (measured shortest path length
u v graph G).
618

fiComputation Shapley Value Game-Theoretic Network Centrality

map defined captures fundamental centrality notion: intrinsic value
subset nodes C context application information dissemination
proportional overall proximity nodes C nodes network.
effect, map carries original definition closeness centrality global level,
measure importance assigned every possible combination nodes.
map therefore characteristic function coalitional game,
vertex network viewed agent playing game. follows node v
high Shapley value game, likely v would contribute arbitrary
randomly chosen coalition nodes C terms increasing proximity C
nodes network. Thus, computing Shapley values game yields centrality
score vertex much-improved characterization closeness centrality.
difficulty adopting game-theoretically inspired centrality measure
previously mentioned problem exponential complexity number agents.
next section, show overcome difficulty compute Shapley value
many centrality applications (including formulation) time polynomial
size network.

4. Algorithms Shapley Value-Based Network Centrality
section, present five characteristic function formulations (C), designed
convey specific centrality notion. already mentioned introduction, common
element formulations aim quantify, albeit different way,
sphere influence coalition C nodes network. Specifically,
first game formulation, start simplest possible idea sphere influence
coalition nodes C set nodes immediately reachable (within one hop)
C. Subsequent games generalize notion. particular, second formulation
specifies sophisticated sphere influence: one includes nodes
immediately reachable least k different ways C. three formulations
extend notion sphere influence weighted graphs. third game defines sphere
influence set nodes within cutoff distance C (as measured shortest
path lengths weighted graph). fourth formulation extreme generalization:
allows sphere influence C specified arbitrary function f (.)
distance C nodes. final formulation straightforward extension
second game, case weighted networks.
relationships among five games graphically presented Figure 4.
4.1 Game 1: 1 (C) = #agents 1 degree away
Let G(V, E) unweighted, undirected network. first define fringe subset
C V (G) set {v V (G) : v C (or) u C (u, v) E(G)}, i.e.,
fringe coalition includes nodes reachable coalition one hop.
Based fringe, define coalitional game g1 (V (G), 1 ) respect network
G(V, E) characteristic function 1 : 2V (G) R given
619

fiMichalak, Aadithya, Szczepaski, Ravindran, & Jennings

Figure 4: Euler diagram showing relationships among five games considered
paper. Specifically, game g2 generalizes g1 ; g3 generalizes g1 generalized
g4 ; g5 generalizes g2 . Finally, note certain instances games
represented g3 , g4 g5 .

(
0
1 (C) =
size(fringe(C))

C =
otherwise.

.
game applied Suri Narahari (2008) find influential nodes
social networks shown deliver promising results concerning target
set selection problem (see Kempe, Kleinberg, & Tardos, 2003). therefore desired
compute Shapley values nodes game. shall present exact
formula computation rather obtaining Monte Carlo simulation
done Suri Narahari.
detail, evaluate Shapley value node vi , consider possible permutations
nodes vi would make positive marginal contribution coalition
nodes occurring itself. Let set nodes occurring node vi random
permutation nodes denoted Ci . Let neighbours node vi graph G(V, E)
denoted NG (vi ) degree node vi denoted degG (vi ).
key question ask is: necessary sufficient condition node vi
marginally contribute node vj NG (vi ) {vi } fringe(Ci )? Clearly, happens
neither vj neighbours present Ci . Formally, (NG (vj ){vj })Ci =
.
going show condition holds probability

1
1+degG (vj ) .

Proposition 1. probability random permutation none vertices
NG (vj ) {vj } occurs vi , vi vj neighbours, 1+deg1G (vj ) .
620

fiComputation Shapley Value Game-Theoretic Network Centrality

Algorithm 1: Computing Shapley value Game 1
Input: Unweighted graph G(V, E)
Output: Shapley values nodes V (G) game g1
foreach v V (G)
SV[v] = 1+deg1 G (v) ;
foreach u NG (v)
SV[v] += 1+deg1 G (u) ;
end
end
return SV;

Proof. need count number permutations satisfy:
v(NG (vj ){vj }) (vi ) < (v).

(4)

end:
Let us choose |(NG (vj ) {vj }| positions sequence elements V .

|V |
1+deg
ways.
G (vj )
Then, last degG (vj ) chosen positions, place elements (NG (vj ) {vj }) \
{vi }. Directly these, place element vi . number line-ups
(degG (vj ))!.
remaining elements arrange (|V | (1 + degG (vj ))! different ways.
all, number permutations satisfying condition (4) is:

|V |
(degG (vj ))!(|V
1+degG (vj )

| (1 + degG (vj ))! =

|V |!
1+degG (vj ) ;

thus, probability one permutations randomly chosen

1
1+degG (vj ) .

Now, denote Bvi ,vj Bernoulli random variable vi marginally contributes vj
fringe(Ci ). above, have:
E[Bvi ,vj ] = Pr[(NG (vj ) {vj }) Ci = ] =

1
.
1 + degG (vj )

Therefore, SVg1 (vi ), expected marginal contribution vi , given by:
SVg1 (vi ) =

X

E[Bvi ,vj ] =

X
vj {vi }NG (vi )

vj {vi }NG (vi )

621

1
,
1 + degG (vj )

(5)

fiMichalak, Aadithya, Szczepaski, Ravindran, & Jennings

exact closed-form expression computing Shapley value node
network.
possible derive intuition formula. node high degree,
number terms Shapley value summation also high. terms
inversely related degree neighboring nodes. gives
intuition node high centrality degree high, also
whenever degree tends higher comparison degree neighboring nodes.
words, power comes connected powerless, fact
well-recognized centrality literature (e.g., Bonacich, 1987). Following
reasoning, also easily predict dynamic changes network, adding
removing edge, would influence Shapley value.8 Adding edge powerful
powerless node add even power former decrease power
latter. Naturally, removing edge would reverse effect.
Interestingly, although game g1 quite different induced-subgraph representation
Deng Papadimitriou (1994), formula SVg1 (vi ) is, extent, similar
formula (1). particular, cases, Shapley value node depends solely set
immediate neighbours. Moreover, cases, linear combination fractions
involving numerator weight edges node neighbours.9
difference denominators, case depends degree involved
nodes. see next two games considered paper yield comparable g1
closed-form expressions Shapley value.
Algorithm 1 directly implements expression (5) compute exact Shapley values
nodes network. cycles nodes neighbours, running time
O(|V | + |E|).
Finally, note Algorithm 1 adopted directed graphs couple simple
modifications. Specifically, order capture many nodes access given
node from, degree node replaced indegree. Furthermore, set
neighbours given node v consist nodes edge directed
v.
4.2 Game 2: 2 (C) = #agents least k neighbors C
consider general game formulation unweighted graph G(V, E),
value coalition includes number agents either coalition
adjacent least k agents coalition. Formally, consider game g2
characterised 2 : 2V (G) R,
(
0
2 (C) =
|{v : v C (or) |NG (v) C| k}|

C =
otherwise.

8. Many real-life networks fact dynamic challenge developing fast streaming algorithms
recently attracted considerable attention literature (Lee, Lee, Park, Choi, & Chung, 2012).
9. Note case g1 weight edge 1.

622

fiComputation Shapley Value Game-Theoretic Network Centrality

second game instance General Threshold Model widely studied
literature (e.g., Kempe et al., 2005; Granovetter, 1978). Intuitively, model
node become active monotone activation function reaches threshold.
instance problem proposed Goyal, Bonchi, Lakshmanan (2010),
authors introduced method learning influence probabilities social networks (from
users action logs). However, many realistic situations much less information available
network possible assess specific probabilities individual
nodes become active. Consequently, much simpler models studied. Bikhchandani et al.
(1992), instance, consider teenager deciding whether try drugs. strong
motivation trying drugs fact friends so. Conversely, seeing friends
reject drugs could help persuade teenager stay clean. situation modelled
second game; threshold node k activation function f (S) =
|S|. Another example viral marketing innovation diffusion analysis. Again,
application, often assumed agent influenced least k
neighbors already convinced (Valente, 1996). Note game reduces
game g1 k = 1.
Adopting notation previous subsection, ask: necessary
sufficient condition node vi marginally contribute node vj NG (vi ) {vi } value
coalition Ci ?
Clearly, degG (vj ) < k, E[Bvi ,vj ] = 1 vi = vj 0 otherwise. degG (nj ) k,
split argument two cases. vj 6= vi , condition marginal contribution
exactly (k 1) neighbors vj already belong Ci vj
/ Ci .
hand, vj = vi , marginal contribution occurs Ci originally consisted
(k 1) neighbors vj . degG (vj ) k vj 6= vi , need determine
appropriate probability.
Proposition 2. probability random permutation exactly k 1 neighbours vj
1+degG (vj )k
occur vi , vj occurs vi , is: degG (vj )(1+deg
, vj vi neighbors
G (vj ))
degG (vj ) k.
Proof. need count number permutations satisfy:

n


!KNG (vj ) |K| = k 1 vK (v) < (vi )



vNG (vj )\K (vi ) (v) (vi ) < (vj ) .

(6)

end:
Let us choose |(NG (vj ) {vj }| positions sequence elements V .

|V |
1+deg
ways.
(v
)
G j
Then, choose k 1 elements set (NG (vj ) \ {vi }. number choices
(vj )1
degGk1
.
623

fiMichalak, Aadithya, Szczepaski, Ravindran, & Jennings

Algorithm 2: Computing Shapley value Game 2
Input: Unweighted graph G(V, E), positive integer k
Output: Shapley value nodes V (G) game g2
foreach v V (G)
SV[v] = min(1, 1+degk G (v) );
foreach u NG (v)
G (u)k+1
SV[v] += max(0, degGdeg
(u)(1+degG (u)) );
end
end
return SV;

Then, first k 1 chosen positions, place elements chosen previous step.
Directly those, place element vi , remaining vertices chosen
first step. number line-ups (k 1)!(1 + degG (vj ) k)!.
remaining elements arrange (|V | degG (vj ) 1)! different ways.
Taking together, number permutations satisfying condition (6) is:
degG 1
|V |
k1 (k
1+degG (vj )

1)!(1 + degG (vj ) k)!(|V | degG (vj ) 1)! =

|V |!(1+degG (vj )k)
degG (vj )(1+degG (vj )) ;

thus, probability one permutations randomly chosen

1+degG (vj )k
degG (vj )(1+degG (vj )) .

Using Proposition 2 obtain:
E[Bvi ,vj ] =

1 + degG (vj ) k
.
degG (vj )(1 + degG (vj ))

degG (vi ) k vj = vi , have:

E[Bvi ,vi ] =

k1
X
n=0

1
k
=
.
1 + degG (vi )
1 + degG (vi )

before, Shapley values given substituting formulae into:
SVg2 (vi ) =

X

E[Bvi ,vj ].

vj NG (vi ){vi }

Although game generalization game g1 , still solved obtain Shapley
values nodes O(|V | + |E|) time, formalised Algorithm 2.
624

fiComputation Shapley Value Game-Theoretic Network Centrality

even general formulation game possible allowing k function
agent, i.e., node vi V (G) assigned unique attribute k(vi ).
translates application form: agent convinced least ki
neighbors convinced, frequently used model literature (Valente, 1996).
argument use fact k constant across nodes.
generalized formulation solved simple modification original Shapley value
expression:
SV (vi ) =

k(vi )
+
1 + degG (vi )

X
vj NG (vi )

1 + degG (vj ) k(vj )
.
degG (vj )(1 + degG (vj ))

equation (which also implementable O(|V |+|E|) time) assumes k(vi )
1+degG (vi ) nodes vi . condition assumed without loss generality
cases still modelled (we set k(vi ) = 1 + degG (vi ) extreme case node
vi never convinced matter many neighbors already convinced).
Finally, note Algorithm 2 adapted case directed graphs along
lines Algorithm 1.
4.3 Game 3: 3 (C) = #agents dcutoff away
Hitherto, games confined unweighted networks. many applications,
necessary model real-world networks weighted graphs. example, coauthorship network mentioned introduction, edge often assigned weight
proportional number joint publications corresponding authors produced
(Newman, 2001).
subsection extends game g1 case weighted networks. Whereas game g1 equates
(C) number nodes located within one hop node C, formulation
subsection equates (C) number nodes located within distance dcutoff
node C. Here, distance two nodes measured length shortest
path nodes given weighted graph G(V, E, W ), W : E R+
weight function.
Formally, define game g3 , coalition C V (G),
(
0
3 (C) =
size({vi : vj C | distance(vi , vj ) dcutoff })

C =
otherwise.

Clearly, g3 used settings g1 applicable; instance, diffusion
information social networks analyse research collaboration networks (e.g., Suri &
Narahari, 2010, 2008). Moreover, general game, g3 provides additional modelling
opportunities. instance, Suri Narahari (2010) suggest intelligent way
sieving nodes neighbourhood would improve algorithm solving target
selection problem (top-k problem). Now, g3 allows us define different cutoff distance
625

fiMichalak, Aadithya, Szczepaski, Ravindran, & Jennings

Algorithm 3: Computing Shapley value Game 3
Input: Weighted graph G(V, E, W ), dcutoff > 0
Output: Shapley value nodes G game g3
foreach v V (G)
DistanceVector = Dijkstra(v,G);
extNeighbors(v) = ; extDegree(v) = 0;
foreach u V (G) u 6= v
D(u) dcutoff
extNeighbors(v).push(u);
extDegree(v)++;
end
end
end
foreach v V (G)
1
SV[v] = 1+extDegree(v)
;
foreach u extN eighbors(v)
1
SV[v] += 1+extDegree(u)
;
end
end
return SV;

node Suri Naraharis setting. Furthermore, g3 specific case
general model g4 discussed next subsection.
shall show even highly general centrality game g3 amenable analysis
yields exact formula Shapley value. However, case algorithm
implementing formula linear size network, O(|V ||E| +
|V |2 log|V |) complexity.
deriving exact Shapley value formula, introduce extra notation. Define
extended neighborhood NG (vj , dcutoff ) = {vk 6= vj : distance(vk , vj ) dcutoff }, i.e.,
set nodes whose distance vj dcutoff . Denote size NG (vj , dcutoff )
degG (vj , dcutoff ). notation, necessary sufficient condition node vi
marginally contribute node vj value coalition Ci is: distance(vi , vj ) dcutoff
distance(vj , vk ) > dcutoff vk Ci . is, neither vj node extended
neighborhood present Ci . discussion previous subsections
Proposition 1, know probability event exactly 1+degG (v1j ,dcutoff ) . Therefore, exact formula Shapley value node vi game g3 is:
SVg3 (vi ) =

X
vj {vi }NG (vi ,dcutoff )

1
.
1 + degG (vj , dcutoff )

Algorithm 3 works follows: node v network G(V, E), extended neighborhood NG (v, dcutoff ) size degG (v, dcutoff ) first computed using Dijkstras algorithm
626

fiComputation Shapley Value Game-Theoretic Network Centrality

O(|E| + |V |log|V |) time (Cormen, 2001). results used directly implement
equation, takes maximum time O(|V |2 ). practice step runs much
faster worst case situation occurs every node reachable every
node within dcutoff . Overall complexity O(|V ||E| + |V |2 log|V |).
Furthermore, deal directed graphs need redefine notion extDegree
extN eighbors given node u Algorithm 3. former number vertices
distance u smaller than, equal to, dcutoff . latter set
nodes whose distance u dcutoff .
Finally, make observation proof depend dcutoff
constant across nodes. Indeed, node vi V (G) may assigned unique
value dcutoff (vi ), (C) would number agents vi within distance
dcutoff (vi ) C. case, proof gives:
SV (vi ) =

X

vj :distance(vi ,vj )
dcutoff (vj )

4.4 Game 4: 4 (C) =

1
.
1 + degG (vj , dcutoff (vj ))

P

vi V (G) f (distance(vi , C))

subsection generalizes game g3 , taking motivation real-life network
problems. game g3 , agents distances dagent dcutoff contributed equally
value coalition. However, assumption may always hold true
applications intuitively expect agents closer coalition contribute value.
instance, expect Facebook user exert influence immediate circle
friends friends friends, even though may satisfy dcutoff criterion.
Similarly, expect virus-affected computer infect neighboring computer quickly
computer two hops away.
general, expect agent distance coalition would contribute f (d)
value, f (.) positive valued decreasing function argument. formally,
define game g4 , value coalition C given by:
(
0
4 (C) = P

vi V (G) f (d(vi , C))

C =
otherwise,

d(vi , C) minimum distance: min{distance(vi , vj )|vj C}.
note possible solve Shapley value formulation
constructing MC-Nets representation (see Section 2 details formalism).
Indeed, combinatorial structure networks certain extent similar structure
MC-Nets. Consequently, existence polynomial algorithm compute Shapley
value MC-Nets strongly suggests polynomial algorithms could developed
games defined networks. results paper demonstrate indeed
case. However, underlined approach compute Shapley
627

fiMichalak, Aadithya, Szczepaski, Ravindran, & Jennings

value different applied MC-Nets. solutions focus
computing expected contribution every node random permutation nodes
disaggregating game collection simple, easily solvable, games
done MC-Nets. difference approaches clearly visible case
g3 . Here, MC-Nets would O(|V |3 ) rules, whereas discussion below,
propose efficient algorithm g3 runs O(|V ||E| + |V |2 log|V |).
considerable improvement real-world networks sparse, i.e., E O(|V |)
(Reka & Barabsi, 2002).
case g3 , key question ask is: expected value marginal
contribution vi node vj 6= vi value coalition Ci ? Let marginal
contribution denoted C(vi , vj ). Clearly:
(
0
C(vi , vj ) =
f (distance(vi , vj )) f (d(vj , Ci ))

distance(vi , vj ) d(vj , Ci )
otherwise.

Let Dvj = {d1 , d2 ...d|V |1 } distances node vj nodes network,
sorted increasing order. Let nodes corresponding distances {w1 , w2 ...w|V |1 },
respectively. Let kij + 1 number nodes (out |V | 1) whose distances
vj distance(vi , vj ). Let wkij +1 = vi (i.e., among nodes distance
vj vi , vi placed last increasing order).
use literal wi mean wi Ci literal wi mean wi
/ Ci . Define sequence
boolean variables pk = vj w1 w2 ... wk 0 k |V | 1. Finally denote
expressions form C(vi , vj |F ) mean marginal contribution vi Ci
vj given coalition Ci satisfies boolean expression F .

C(vi , vj |pkij +1 wkij +2 ) = f (dkij +1 ) f (dkij +2 ),
C(vi , vj |pkij +2 wkij +3 ) = f (dkij +1 ) f (dkij +3 ),
..
..
..
.
.
.
C(vi , vj |p|V |2 w|V |1 ) = f (dkij +1 ) f (d|V |1 ),
C(vi , vj |p|V |1 ) = f (dkij +1 ).
notation, obtain expressions C(vi , vj ) splitting mutually
exclusive exhaustive (i.e., covering possible non-zero marginal contributions) cases.
Now, need determine probability Pr(pk wk+1 ).
Proposition 3. probability random permutation none nodes
1
.
{vj , w1 , . . . , wk } occur vi node wk+1 occurs vi (k+1)(k+2)
Proof. Let us count number permutations satisfy:
v{vj ,w1 ,...,wk } (vi ) < (v) (vi < (wk+1 ).
end:
628

(7)

fiComputation Shapley Value Game-Theoretic Network Centrality

Let us choose |{vj , w1 , . . . , wk }{vj }{wk+1 }| positions sequence elements
|V |
V . k+3
ways.
Then, last k + 1 chosen positions, place elements {vj , w1 , . . . , wk }.
Directly these, place element vi , vertex wk+1 . number
line-ups (k + 1)!.
remaining elements arrange (|V | (k + 3)! different ways.
Thus, number permutations satisfying (7) is:
|V |
k+3 (k

+ 1)!(|V | (k + 3))! =

|V |!
(k+1)(k+2) ,

probability one permutations randomly chosen

1
(k+1)(k+2) .

proposition find that:
Pr(pk wk+1 )

1
1 + kij k |V | 2.
(k + 1)(k + 2)

Using C(vi , vj ) equations probabilities Pr(pk wk+1 ):




X f (distance(vi , vj )) f (dk+1 )
+ f (distance(vi , vj ))
E[M C(vi , vj )] =
(k + 1)(k + 2)
|V |
|V |2

k=1+kij

f (distance(vi , vj ))

=
kij + 2

|V |2

X
k=kij +1

f (dk+1 )
.
(k + 1)(k + 2)

vi = vj , similar analysis produces:
|V |2

E[M C(vi , vi )] = f (0)

X
k=0

f (dk+1 )
.
(k + 1)(k + 2)

Finally exact Shapley value given by:
SVg4 (vi ) =

X

E[M C(vi , vj )].

vj V (G)

Algorithm 4 implements formulae. vertex v, vector distances
every vertex first computed using Dijkstras algorithm (Cormen, 2001). yields
vector Dv already sorted increasing order. vector traversed
629

fiMichalak, Aadithya, Szczepaski, Ravindran, & Jennings

Algorithm 4: Computing Shapley value Game 4
Input: Weighted graph G(V, E, W ), function f : R+ R+
Output: Shapley value nodes G game g4
Initialise: v V (G) set SV[v] = 0;
foreach v V (G)
[Distances D, Nodes w] = Dijkstra(v,G);
sum = 0; index = |V|-1; prevDistance = -1, prevSV = -1;
index > 0
D(index) == prevDistance
currSV = prevSV;
else
currSV = f (D(index))
1+index sum;
end
SV[w(index)] += currSV;
sum +=

f (D(index))
index(1+index) ;

prevDistance = D(index), prevSV = currSV;
index--;
end
SV[v] += f(0) sum;
end
return SV;

P f (dk+1 )
reverse, compute backwards cumulative sum
(k+1)(k+2) . step backward traversal, Shapley value appropriate node w updated according
E[M C(w, v)] equation. traversal, Shapley value v updated according
E[M C(v, v)] equation. process repeated nodes v end
algorithm, Shapley value computed exactly O(|V ||E| + |V |2 log|V |) time.
final observation Algorithm 4 works also directed graphs long use
appropriate version Dijkstras algorithm (see, e.g., Cormen, 2001).
4.5 Game 5: 5 (C) = #agents

P
(weights inside C) Wcutoff (agent)

subsection, generalize game g2 case weighted networks. Given positive
weighted network G(V,
P E, W ) value Wcutoff (vi ) every node vi V (G), first
define W (vj , C) = vi C W (vj , vi ) every coalition C, W (vi , vj ) weight
edge nodes vi vj (or 0 edge). notation,
define game g5 characteristic function:
(
0
5 (C) =
size({vi : vi C (or) W (vi , C) Wcutoff (vi )})
630

C =
otherwise.

fiComputation Shapley Value Game-Theoretic Network Centrality

Algorithm 5: Computing Shapley value Game 5
Input: Weighted network G(V, E, W ), cutoffs Wcutoff (vi ) vi V (G)
Output: Shapley value nodes G game g5
foreach vi V (G)
compute store ;
end
foreach vi V (G)
SV[vi ] = 0;
foreach 0 degG (vi )
ii ), = (X ii ), p = Pr{N (, 2 ) < W
compute = (Xm
cutoff (vi )};

p
SV[vi ] += 1+degG (vi ) ;
end
foreach vj NG (vi )
p = 0;
foreach 0 degG (vj ) 1
ij
ij
ij
compute = (Xm
), = (Xm
) z = Zm
;
deg (v )m

G j
p += z degG (vj )(deg
;
G (vj )+1)

end
SV[vi ] += p;
end
end
return SV;

formulation applications in, instance, analysis information diffusion, adoption innovations, viral marketing. Indeed, many cascade models
phenomena weighted graphs proposed (e.g., Granovetter, 1978; Kempe et al.,
2003; Young, 2006) work assuming agent change state inactive
active sum weights active neighbors least equal
agent-specific cutoff.
Although able come exact formula Shapley value
game10 , analysis yields approximate formula found accurate
practice.
detail, observe node vi marginally contributes node vj NG (vi ) value
coalition Ci vj
/ Ci Wcutoff (vj ) W (vi , vj ) W (vj , Ci ) < Wcutoff (vj ).
Let us denote Bvi ,vj Bernoulli random variable corresponding event.
need following additional notation:
let NG (vj ) = {vi , w1 , w2 ...wdegG (vj )1 };
10. Computing Shapley value game involves determining whether sum weights specific
edges, adjacent random coalition, exceeds threshold. problem seems least hard
computing Shapley value weighted voting games, #P-Complete (Elkind et al., 2009).

631

fiMichalak, Aadithya, Szczepaski, Ravindran, & Jennings

let weights edges vj nodes NG (vj ) Wj =
{W (vi , vj ), W1 , W2 ...WdegG (vj )1 } order;
let j sum weights Wj j sum squares
weights Wj ;
let kij number nodes NG (vj ) occur vi Ci ;
let Xtij sum t-subset Wj \ {W (vi , vj )} drawn uniformly random
set possible t-subsets; finally
let Ymij event {kij = vj
/ Ci }.
Then:
degG (vj )1

E[Bvi ,vj ] =

X

ij
Pr(Ymij ) Pr{Xm
[Wcutoff (vj ) W (vi , vj ), Wcutoff (vj ))},

m=0

Pr(Ymij ) obtained Proposition 2:
Pr(Ymij )



degG (vj )
degG (vj ) 1 m! (degG (vj ) m)!
=
.
=
(degG (vj ) + 1)!
degG (vj )(degG (vj ) + 1)


ij
Evaluating Pr{Xm
[Wcutoff (vj ) W (vi , vj ), Wcutoff (vj ))} much difficult
ij
complicated function degG (vj ) 1 numbers Wj \
distribution Xm
ij
{W (vi , vj )}. However, obtain analytical expressions mean (Xm
) variance
ij
2 (Xm
). given by:


(j W (vi , vj ))
degG (vj ) 1
(j W (vi , vj ))2
m(degG (vj ) 1 m)
ij
)=
(j W (vi , vj )2
).
2 (Xm
(degG (vj ) 1)(degG (vj ) 2)
degG (vj ) 1
ij
(Xm
)=

ij
Knowing mean variance (not exact distribution) Xm
, propose
approximation:

ij
ij
ij
Xm
N ((Xm
), 2 (Xm
)),

N (, 2 ) denotes Gaussian random variable mean variance 2 .
approximation similar randomised approach proposed tested
Fatima et al. (2007).
approximation, have:
632

fiComputation Shapley Value Game-Theoretic Network Centrality

ij
ij
Zm
= Pr{Xm
[Wcutoff (vj ) W (vi , vj ), Wcutoff (vj ))}

given

ij
Zm

"
1
erf

2

ij
Wcutoff (vj ) (Xm
)

ij
2(Xm )

!
erf

ij
Wcutoff (vj ) W (vi , vj ) (Xm
)

ij
2(Xm )

!#
.

allows us write:
degG (vj )1

E[Bvi ,vj ] =

X
m=0

degG (vj )
Z ij .
degG (vj )(degG (vj ) + 1)

equations true vj 6= vi . vj = vi have:

1
E[Bvi ,vi ]
1 + degG (vi )

degG (vi )

X

ii
ii
Pr{N ((Xm
), 2 (Xm
)) < Wcutoff (vi )},

m=0


ii
(Xm
)=



degG (vi )



ii
2 (Xm
)=

i2
(degG (vi ) m)
(i
).
degG (vi ) (degG (vi ) 1)
degG (vi )

P
Finally Shapley value node vi given vj {vi }NG (vi ) E[Bvi ,vj ].
P
inPeach graph
P holds vi V (G) degG (vi ) 2|E|, Algorithm 5 implements
O(|V | + vi V (G) vj NG (vi ) degG (vj )) O(|V | + |V ||E|) = O(|V ||E|) solution compute
Shapley value agents game g5 using approximation.
Furthermore, make following observation: approximation discrete random
ij
variable Xm
continuous Gaussian random variable good degG (vj ) large.
small degG (vj ), one might well use brute force computation determine E[Bvi ,vj ]
O(2degG (vj )1 ) time.
far directed graphs concerned, calculations Algorithm 5 consider
indegree node instead degree. Furthermore, set neighbours node u
defined set nodes vi connected directed edge (u, vi ).
633

fiMichalak, Aadithya, Szczepaski, Ravindran, & Jennings

5. Simulations
section evaluate time performance exact algorithms games g1
g4 approximation algorithm game g5 . detail, compare exact
algorithms method approximating Shapley value via Monte Carlo sampling
feasible approach compute game-theoretic network centrality
available date literature. First, provide detailed description simulation
setup; then, present data sets simulation results.
5.1 Simulation Setup
approximation methods Shapley value recently
proposed literature. divided three groupseach referring specific
subclass coalitional games consideration:
1. First, let us consider method proposed Fatima et al. (2007) elaborated
Fatima, Wooldridge, Jennings (2008). approach concerns weighted
voting games. games, player certain number votes (or
words, weight). coalition winning number votes coalition
exceeds specific threshold, losing otherwise. Fatima et al. propose
following method approximate Shapley value weighted voting games. Instead
finding marginal contributions players 2n coalitions, authors consider
n randomly-selected coalitions, one size (i.e., 1 n).
n coalitions players expected marginal contributions calculated
average contributions yields approximation Shapley value. Whereas
Fatima et al. method certainly attractive, applicable games
value coalition depends sum associated weights bounds.
case games g1 g4 .11
2. Another method proposed Bachrach, Markakis, Procaccia, Rosenschein,
Saberi (2008a) context simple coalitional games 12 characteristic function binaryi.e., coalition value either zero one.
games, Bachrach et al. extend approach suggested Mann Shapley
(1960) provide rigorous statistical analysis. particular, Mann Shapley
described Monte Carlo simulations estimate Shapley value random
sample coalitions. Bachrach al. use technique compute Banzhaf power
index suggested using random sample permutations players
order compute Shapley-Shubik index simple coalitional games.13
computation confidence interval, crucial approach, hinges
upon binary form characteristic function simple coalitional games.
11. Recall approximation algorithm g5 builds upon Fatima et al. method.
game marginal contribution node depends weights assigned incident edges.
12. Note weighted voting games simple coalitional games.
13. Shapley-Shubik index well-known application Shapley value evaluates power
individuals voting (Shapley & Shubik, 1954).

634

fiComputation Shapley Value Game-Theoretic Network Centrality

Algorithm 6: Monte Carlo method approximate Shapley value
Input:
Characteristic function v, maximum iteration maxIter
Output: Aproximation Shapley value game v
vi V (G)
SV[vi ] = 0 ;
end
= 1 maxIter
shuffle(V (G));
Marginal Contribution block
P=;
vi V (G)
SV[vi ] += v(P {vi }) - v(P) ;
P = P {vi } ;
end
end
vi V (G)
SV[vi ]
SV[vi ] = maxIter
;
end
return SV ;

method general one proposed Fatima et al. (2007)as weighted
voting games subset simple coalitional gamesbut still cannot effectively
used games g1 g4 , characteristic functions binary.
3. Unlike first two methods, last method described Castro et al. (2009)
efficiently applied coalitional games characteristic function game form,
assuming worth every coalition computed polynomial time. Here,
approximating Shapley value involves generating permutations players
computing marginal contribution player set players occurring
it. solution precision increases (statistically) every new permutation
analysed. Furthermore, authors show estimate appropriate size
permutation sample order guarantee low error. Given broad applicability,
method used simulations comparison benchmark.
detail, preliminary step, test maximum number Monte
Carlo iterations performed reasonable time given game.
maximum number iterations, denoted maxIter, becomes input Algorithm 6
Monte Carlo sampling. algorithm, one maxIter iterations, random
permutation nodes generated. Then, using characteristic function set
{1 , 2 , 3 , 4 , 5 }, calculates marginal contribution node set P
635

fiMichalak, Aadithya, Szczepaski, Ravindran, & Jennings

nodes occurring given node random permutation.14 Finally, algorithm
divides aggregated sum contributions node number iterations
performed. time complexity algorithm O(maxIter con), con denotes
number operations necessary computing Marginal Contribution block.
block specifically tailored particular form characteristic function
games g1 g5 . particular, game g1 (see Algorithm 6), constructed follows.
Recall that, game, node vi makes positive contribution coalition P
adjacent node u two conditions. Firstly, neither vi u P .
Secondly, edge P vi u. check conditions Algorithm 6
store nodes already contributed value coalition P array
called: Counted. node vi , algorithm iterates set neighbours
adjacent node checks whether adjacent node counted array
Counted. not, marginal contribution node vi increased one. Appendix
describe Marginal Contribution block games g2 , . . . , g5 , respectively.15
details Algorithm 6 applied generate Shapley value approximations
games g1 g4 , propose exact polynomial solutions, differ g5 ,
developed approximate solution. Specifically, games g1 g4 :
1. use exact algorithm proposed paper compute Shapley value.
2. Then, run Monte Carlo simulations 30 times.16 every run:
perform maxIter Monte Carlo iterations.
every five iterations, compare approximation Shapley value
obtained via Monte Carlo simulation exact Shapley value obtained
algorithm.
record algorithms runtime error, error defined
maximum discrepancy actual Shapley value Monte Carlobased approximation Shapley value.
3. Finally, compute confidence interval using iterations (0.95% confidence
level).17
case game g5 cannot determine exact Shapley value larger networks.
Therefore, performed two levels simulation: one level small networks one level
large networks. Specifically:
1. small networks, generate 30 random instances weighted complete graphs
6 nodes (denoted K6 ) number graphs 12 nodes (denoted
14. Recall characteristic functions v1 , v2 , . . . , v5 correspond games g1 , g2 , . . . , g5 , respectively.
15. software package C++ containing exact/approximation algorithms, well Monte
Carlo approximation algorithms available www.tomaszmichalak.net.
16. purpose comparison method, suffices use 30 iterations, standard errors
converge significantly indicate magnitude cost using Monte Carlo method.
17. Since g4 Monte Carlo iteration relatively time consuming, run once; thus,
confidence interval generated, i.e., third step omitted.

636

fiComputation Shapley Value Game-Theoretic Network Centrality

K12 ) weights drawn uniform distribution U (0, 1). Then, graph
two parameters Wcutoff (vi ) = 41 (vi ) Wcutoff (vi ) = 34 (vi ):18
compute exact Shapley value using formula (3).
Then, run approximation algorithm determine error approximation.
Finally, run 2000 6000 Monte Carlo iterations K6 K12 , respectively.
2. large networks, generate 30 random instances weighted complete
graphs, 1000 nodes (we denote K1000 ). Then, graph
three parameters Wcutoff (vi ) = 14 (vi ), Wcutoff (vi ) = 42 (vi ),
Wcutoff (vi ) = 43 (vi )):
run approximation algorithm Shapley value.
Then, run fixed number (200000) Monte Carlo iterations.
Finally, compute Monte Carlo solution converges results
approximation algorithm.
described simulation setup, discuss data sets and, finally,
simulation results.
5.2 Data Used Simulations
consider two networks already well-studied literature. Specifically,
games g1 g3 present simulations undirected, unweighted network representing
topology Western States Power Grid (WSPG).19 network (which 4940
nodes 6594 edges) studied many contexts (see, instance, Watts &
Strogatz, 1998) freely available online (see, e.g., http://networkdata.ics.uci.edu/
data.php?id=107). games g3 g5 (played weighted networks), used network
astrophysics collaborations (abbreviated henceforth APhC) Jan 1, 1995
December 31, 1999. network (which 16705 nodes 121251 edges) also freely
available online (see, e.g., http://networkdata.ics.uci.edu/ data.php?id=13)
used previous studies like Newman (2001).
5.3 Simulation Results
results presented section show exact algorithms are, general, much
faster Monte Carlo sampling, case even allow generous
error tolerance. Furthermore, requiring smaller Monte Carlo errors makes Monte Carlo
runtime exponentially slower exact solution.
18. Recall j sum weights Wj defined Section 4.5.
19. Note distance threshold dcutoff replaced hop threshold kcutoff , game g3 played
unweighted network.

637

fiMichalak, Aadithya, Szczepaski, Ravindran, & Jennings

2000

1600

1600

1400

1400

1200

1200

1000
800

1000
800

600

600

400

400

200

200
0.43 ms

0
100

90

80 70 60 50 40 30 20
Maximal allowable MC error (%)

10

2000

100

90

80 70 60 50 40 30 20
Maximal allowable MC error (%)

10

0

Figure 6: g2 , k = 2, WSPG (UW)
2000

Monte Carlo solution
exact solution

1800

0.55 ms

0

0

Figure 5: g1 , WSPG (UW)

Monte Carlo solution
exact solution

1800

1600

1600

1400

1400

1200

1200

Time (ms)

Time (ms)

Monte Carlo solution
exact solution

1800

Time (ms)

Time (ms)

2000

Monte Carlo solution
exact solution

1800

1000
800

1000
800

600

600

400

400

200

200
0.52 ms

0
100

90

80 70 60 50 40 30 20
Maximal allowable MC error (%)

Figure 7: g2 , ki =

degi
2 ,

10

0

0.53 ms

0
100

90

80 70 60 50 40 30 20
Maximal allowable MC error (%)

Figure 8: g2 , ki =

WSPG (UW)

3
4

10

0

degi , WSPG (UW)

detail, simulation results game g1 shown Figure 5. dotted line
shows performance exact algorithm needs 0.43ms compute Shapley
value. contrast, generating reasonable Monte Carlo result takes substantially longer
time (the solid line shows average shaded area depicts confidence interval
Monte Carlo simulations). particular, takes average 200ms achieve
20% error 2000ms required guarantee 5% error (which
4600 times slower exact algorithm).
3

Figures 6 - 8 concern game g2 different values k (k = 2, ki = deg
2 , ki = 4 degi ,
20
respectively, degi degree node vi ). advantage exact algorithm
Monte Carlo simulation exponential.

Replacing distance threshold dcutoff hop threshold kcutoff enables game g3
played unweighted network. Thus, similarly games g1 g2 , test
Western States Power Grid. results shown Figures 9 10 kcutoff
equal 2 3, respectively. third game clearly computationally challenging
g1 g2 (note vertical axis seconds instead milliseconds). Now,
20. Recall g2 meaning parameter k follows: value coalition C depends
number nodes network least k neighbours C.

638

fi28
27
26
25
24
23
22
21
20
19
18
17
16
15
14
13

Monte Carlo solution
exact solution

Time (s)

Time (s)

Computation Shapley Value Game-Theoretic Network Centrality

12.88
100

90

80 70 60 50 40 30 20
Maximal allowable MC error (%)

10

0

31
30
29
28
27
26
25
24
23
22
21
20
19
18
17
16
15
14
13

Monte Carlo solution
exact solution

13.03
100

Figure 9: g3 , kcutoff = 2, WSPG (UW)

90

80 70 60 50 40 30 20
Maximal allowable MC error (%)

10

0

Figure 10: g3 , kcutoff = 3, WSPG (UW)

exact algorithm takes 13s complete. much lower speedups exact
methods respect Monte Carlo approach stem fact algorithms
start Dijkstras algorithm. Although algorithm run
cases takes 12.5s considered network. means exact
solution slower orders magnitude (compared games g1 g2 ). Monte Carlo
approach also slower, slowdown much less significant relative terms.
Figures 11 12 show performance algorithms game g3 astrophysics
collaboration network that, unlike Western States Power Grid, weighted network.

davg
observe increasing value dcutoff (here dcutoff = avg
8 dcutoff = 4 )
significantly worsens performance Monte Carlo-based algorithm.
increasing number nodes taken account computing marginal
contributions (see inner loop Algorithm 8) time consuming, also
increases Monte Carlo error.
1
game g4 performance algorithms shown Figures 13 - 15 (for f (d) = 1+d
,
1

f (d) = 1+d2 f (d) = e , respectively). Whereas Monte Carlo methods
first three games able achieve reasonable error bound seconds minutes,
fourth game takes 40 hours approach 50% error.
inner loop Marginal Contribution block (see Algorithm 9) iterates nodes
network. Due time consuming performance run simulations once.
Interestingly, observe error Monte Carlo method sometimes increases
slightly iterations performed. confirms error Monte
Carlo method approximate Shapley value proposed Castro et al. (2009)
statistically decreasing time. Certain new randomly chosen permutations actually
increase error.

Figures 16, 17, 18 19 present comparisons approximation algorithm game g5
Monte Carlo sampling small networks (for exact Shapley value
computed definition formula (3)). figures, horizontal dotted
line shows running time solution, vertical dotted line shows average
approximation error shaded area confidence interval. previously,
639

fi48
46
44
42
40
38
36
34
32
30
28
26
24
22
20
18
16
14
12
10
8
6
4

Monte Carlo solution
exact solution

160
140
120
100
80
60
40
20
3.47 min
100

90

80 70 60 50 40 30 20
Maximal allowable MC error (%)

Figure 11: g3 , dcutoff =

40

davg
8 ,

10

4.41 min

0

100

35

35

30

30

25

25

20

15

10

10

5

5
8.78 min
90

80 70 60 50 40 30 20
Maximal allowable MC error (%)

Figure 13: g4 , f (d) =

1
1+d ,

10

40

APhC (W)

8.88 min

0
90

80 70 60 50 40 30 20
Maximal allowable MC error (%)

Figure 14: g4 , f (d) =

APhC (W)

0

Monte Carlo solution
exact solution

100

0

davg
4 ,

10

20

15

100

80 70 60 50 40 30 20
Maximal allowable MC error (%)

40

Monte Carlo solution
exact solution

0

90

Figure 12: g3 , dcutoff =

APhC (W)

Time (h)

Time (h)

Monte Carlo solution
exact solution

180

Time (min)

Time (min)

Michalak, Aadithya, Szczepaski, Ravindran, & Jennings

1
,
1+d2

10

0

APhC (W)

Monte Carlo solution
exact solution

35
30

Time (h)

25
20
15
10
5
9.18 min

0
100

90

80 70 60 50 40 30 20
Maximal allowable MC error (%)

10

0

Figure 15: g4 , f (d) = ed , APhC (W)
solid line shows average, shaded area depicts confidence interval
Monte Carlo simulations. see Figures 16, 17 18 approximation error
proposed algorithm well-contained small networks. Specifically, K6
10%; whereas bigger network K12 5%. However, notice that,
higher values Wcutoff , Monte Carlo method may slightly outperform solution. See
640

fi7
6.5
6
5.5
5
4.5
4
3.5
3
2.5
2
1.5
1
0.5
0

14
10.57
6. .74 %
91 %
%

Monte Carlo solution
approximation solution

Time (ms)

Time (ms)

14
10.43
6. .29 %
15 %
%

Computation Shapley Value Game-Theoretic Network Centrality

0.38 ms
100 90

80 70 60 50 40 30 20 10
Maximal allowable MC error (%)

7
6.5
6
5.5
5
4.5
4
3.5
3
2.5
2
1.5
1
0.5
0

0

Monte Carlo solution
approximation solution

0.38 ms
100 90

4.
7
4. 1 %
9
5. 1 %
12
%

Monte Carlo solution
approximation solution

Time (ms)

Time (ms)

26
24
22
20
18
16
14
12
10
8
6
4
2
0

2.82 ms
100 90

80 70 60 50 40 30 20 10
Maximal allowable MC error (%)

0

Figure 17: g5 , Wcutoff = 34 , K6 (W),

5.
2
5. 6 %
5
5. 4 %
81
%

Figure 16: g5 , Wcutoff = 14 , K6 (W)

80 70 60 50 40 30 20 10
Maximal allowable MC error (%)

0

26
24
22
20
18
16
14
12
10
8
6
4
2
0

Monte Carlo solution
approximation solution

2.74 ms
100 90

Figure 18: g5 , Wcutoff = 14 , K12 (W)

80 70 60 50 40 30 20 10
Maximal allowable MC error (%)

0

Figure 19: g5 , Wcutoff = 43 , K12 (W)

Figure 17 average approximation error Monte Carlo sampling achieved
0.38ms lower average error achieved method. Already K12 effect
occur (see Figure 19).
large networks, exact Shapley value cannot obtained, naturally
unable compute exact approximation error. believe error may higher
values obtained K6 K12 . However, mixed strategy, discussed
Section 4 uses approximation large degree vertices, work
towards containing error within practical tolerance bounds. far believe
Monte Carlo gives good results, Figure 20, infer approximation solution
large networks gives good results (within 5%) least two times faster
Monte Carlo algorithm.
summarise, exact solutions outperform Monte Carlo simulations even relatively
wide error margins allowed. However, always case approximation
algorithm game g5 . Furthermore, underlined centrality metrics
consideration cannot described games g1 g4 exact
algorithms available, Monte Carlo simulations still viable option.
641

fiMichalak, Aadithya, Szczepaski, Ravindran, & Jennings

approximation solution
Monte Carlo solution Wcutoff=0.25
Monte Carlo solution Wcutoff=0.50
Monte Carlo solution Wcutoff=0.75

22
20
18

Time (min)

16
14
12
10.09 min

10
8
6

0.75

4

0.50

2

0.25

0
100

90

80 70 60 50 40 30 20
Maximal allowable MC error (%)

10

0

Figure 20: g5 , Wcutoff = 14 , Wcutoff = 24 , Wcutoff = 43 , K1000 (W)

6. Conclusions Future Work
key finding paper Shapley value many centrality-related cooperative
games interest played networks solved analytically. resulting algorithms
error-free, also run polynomial time and, practice, much faster
Monte Carlo methods. Approximate closed-form expressions algorithms also
constructed classes games played weighted networks. Simulation results
show approximations acceptable range situations.
number directions future work. one hand, Shapley value-based
extensions centrality notions, suit particular applications, developed.
step direction, first study Shapley value-based betweenness centrality
recently presented Szczepaski, Michalak, Rahwan (2012). hand,
would interesting analyze coalitional games defined network would
better reflect centrality nodes certain real-life applications. spirit, recent works
del Pozo, Manuel, Gonzlez-Arangena, Owen (2011) Amer, Gimnez, Magana
(2012) focus generalized coalitional games order agents forming coalitions
matter. Nevertheless, still classes coalitional games, games
either positive negative externalities (Yi, 1997), extensively studied
game theory may yield interesting results applied network centrality.
Another interesting application new class coalitional games defined
network could developed problem influence maximization, already mentioned
introduction.
also interesting analyse properties game-theoretic network centralities constructed solution concepts cooperative game theory Shapley value.
particular, game defined network belongs class simple coalitional
games (i.e., binary characteristic function) Banzhaf power index (Banzhaf,
1965) could also used centrality metric. Otherwise, general solution concepts
core (Osborne & Rubinstein, 1994) nucleolus (Schmeidler, 1969) could
applied.
642

fiComputation Shapley Value Game-Theoretic Network Centrality

Ultimately, would interesting develop formal general approach
would allow us construct coalitional games defined networks correspond
known centrality metrics even entire families them.21 approach would
involve developing group centrality first building characteristic function
coalitional game upon it. course, developing new centrality metrics based
coalitional games, one keep mind computational properties proposed
solutions. Although able obtain satisfactory computational results games
considered paper, computation game-theoretic network centrality may
become much challenging complex definitions characteristic function.

Acknowledgments
would like thank three anonymous reviewers comments earlier version
paper helped improve considerably. Also, would like thank dr Talal
Rahwan dr Suri Rama Narayanam proofreading, helpful comments suggestions.
Tomasz Michalak partially supported European Research Council Advanced Grant 291528 (RACE). Nicholas R. Jennings (and partially Tomasz Michalak)
supported ORCHID Project, funded EPSRC (Engineering Physical Research
Council) grant EP/I011587/1.

Appendix A. Marginal Contribution Blocks Algorithm 6 g2 -g5
Algorithm 7: Marginal Contribution block Algorithm 6 g2
Counted false ;
Edges 0 ;
foreach vi V (G)
foreach u NG (vi ) {vi }
Edges[u]++ ;
!Counted[u] ( Edges[u] k[u] u = vi )
SV[vi ]++ ;
Counted[u] = true ;
end
end
end
games considered paper Marginal Contribution block Algorithm 6 takes slightly different form. main text explained functioning
block g1 . appendix, discuss block remaining four games.
particular:
g2 : Here, node vi makes positive contribution coalition P
adjacent node u also two conditions. Firstly, neither vi u
21. thank anonymous reviewer suggestion.

643

fiMichalak, Aadithya, Szczepaski, Ravindran, & Jennings

Algorithm 8: Marginal Contribution block Algorithm 6 g3
Counted false ;
foreach vi V (G)
foreach u extN eighbors(vi ) {vi }
!Counted[u]
SV[vi ]++ ;
Counted[u] = true ;
end
end
end
Algorithm 9: Marginal Contribution block Algorithm 6 g4
dist infinity ;
foreach vi V (G)
foreach u V (G)
D[u] < dist[u]
SV[vi ] += f(D[u]) - f(dist[u]) ;
dist[u] = D[u] ;
end
end
SV[vi ] += f(dist[u]) - f(0) ;
dist[vi ] = 0 ;
end

P . Secondly, less k edges P vi exactly k 1 edges
P u. order check first condition Algorithm 7 use array
Counted, check second one, use array Edges. node vi ,
algorithm iterates set neighbours adjacent node
checks whether adjacent node meets two conditions. so, marginal
contribution node vi increased one.
g3/4 : Marginal Contribution blocks games g3 g4 (Algorithms 8 9),
values dependent distance (extN eighbours D) calculated
using Dijkstras algorithm stored memory. pre-computations allow us
significantly speed Monte Carlo methods. Now, g3 node vi makes positive
contribution coalition P adjacent node u
two conditions. Firstly, neither vi u P . Secondly, edge length
dcutoff P vi u. check conditions Algorithm 8 use
array Counted. node vi , algorithm iterates set extended
neighbours checks whether neighbour meets conditions.
so, marginal contribution node vi increased one. game g4 , node
vi makes positive contribution coalition P node (including itself)
closer vi P . Algorithm 9 use array Dist store distances
644

fiComputation Shapley Value Game-Theoretic Network Centrality

Algorithm 10: Marginal Contribution block Algorithm 6 g5
Counted false ;
Weights 0 ;
foreach vi V (G)
foreach u NG (vi ) {vi }
weights[u]+= W (vi , u);
!Counted[u] ( weights[u] Wcutoff (u) u = vi )
SV[vi ]++ ;
Counted[u] = true ;
end
end
end

coalition P nodes graph array store distances vi
nodes. node vi , algorithm iterates nodes
graph, node u, distance vi u smaller P u,
algorithm computes marginal contribution f (D[u]) f (Dist[u]). value
Dist[u] updated D[u]this new distance P u.
g5 : game g5 , extension g2 weighted graphs, node vi makes positive
contribution coalition P (both adjacent node u)
two conditions. Firstly, neither vi u P . Secondly, sum weights
edges P vi less Wcutoff (vi ) sum weights edges P u
greater than, equal to, Wcutoff (u)W (vi , u) smaller Wcutoff (vi )+W (vi , u).
order check first condition Algorithm 10 use array Counted,
check second one, use array W eights. node vi , algorithm
iterates set neighbours adjacent node checks whether
adjacent node meets two conditions. so, marginal contribution
node vi increased one.

Appendix B: Main Notation Used Paper


set players.

ai

player A.

C

coalition.
value coalition, characteristic function.

(C)
(A, )/gi
SVgj (vi )
G = (V, E)
G = (V, E, W )
W (v, u)

coalitional game.
Shapley value od vertex vi game gj .
Unweighted graph/network consisting set vertices V edges E.
Weighted graph/network.
Weight edge v u.
645

fiMichalak, Aadithya, Szczepaski, Ravindran, & Jennings

V (G)/V, E(G)/E
vi V

set vertices edges graph G.

deg(vi )

vertex set V .
Degree vertex vi .

NG (vi )

Set neighbours vertex vi G.

distance(v, u)/d(v, u) distance vertices v u.
Extended neighbourhood: NG (vj , dcutoff ) = {vk 6= vj : distance(vk , vj )
NG (vi , dcutof f )
dcutoff }.
Marginal contribution vertex u makes vertex v.
C(u, v)
(A)
(A)
(i)
C (i)

set orders players A.
single ordering agents A.
position th element ordering .
{aj : (j) < (i)}.

E[]

{v V (G) : v C (or) u C (u, v) E(G)}.
number assigned vertex v used Game 2. minimum number
adjacent nodes necessary influence node vi .
number assigned vertex v used Game 5. Minimum sum weights
adjacent edges necessary influence node vi .
expectation operator.

P[]

probability operator.

O()

big complexity notation.

f ringe(C)
k(vi )/ki
Wcutoff (vi )

B, X,
N (, 2 )
erf ()

Random variables.
Normal distribution mean variance 2 .

j

error function.
sum weights incident edges vertex vj .

j

sum squares weights incident edges vertex vj .

f (.)

positive valued decreasing function.

Ki

complete graph (clique) nodes.

References
Aadithya, K., Michalak, T., & Jennings, N. (2011). Representation coalitional games
algebraic decision diagrams. AAMAS 11: Proceedings 10th International
Joint Conference Autonomous Agents Multi-Agent Systems, pp. 11211122.
Amer, R., Gimnez, J., & Magana, A. (2012). Accessibility measures nodes directed
graphs using solutions generalized cooperative games. Mathematical Methods
Operations Research, 75, 105134.
Aziz, H., Lachish, O., Paterson, M., & Savani, R. (2009a). Power indices spanning connectivity games. AAIM 09: Proceedings 5th International Conference
Algorithmic Aspects Information Management, pp. 5567.
646

fiComputation Shapley Value Game-Theoretic Network Centrality

Aziz, H., Lachish, O., Paterson, M., & Savani, R. (2009b). Wiretapping hidden network.
WINE 09: Proceedings 5th Workshop Internet & Network Economics,
pp. 438446.
Bachrach, Y., Markakis, E., Procaccia, A. D., Rosenschein, J. S., & Saberi, A. (2008a).
Approximating power indices. AAMAS 08: Proceedings 7th International
Joint Conference Autonomous Agents Multi-Agent Systems, pp. 943950.
Bachrach, Y., Rosenschein, J. S., & Porat, E. (2008b). Power stability connectivity
games. AAMAS 08: Proceedings 7th International Joint Conference
Autonomous Agents Multi-Agent Systems, pp. 9991006.
Bachrach, Y., & Rosenschein, J. (2009). Power threshold network flow games. Autonomous
Agents Multi-Agent Systems, 18 (1), 106132.
Banzhaf, J. F. (1965). Weighted Voting Doesnt Work: Mathematical Analysis. Rutgers
Law Rev., 19, 317343.
Bikhchandani, S., Hirshleifer, D., & Welch, I. (1992). theory fads, fashion, custom,
cultural change informational cascades. Journal Political Economy, 100 (5),
9921026.
Bolus, S. (2011). Power indices simple games vector-weighted majority games
means binary decision diagrams. European Journal Operational Research, 210 (2),
258272.
Bonacich, P. (1972). Factoring weighting approaches status scores clique identification. Journal Mathematical Sociology, 2 (1), 113120.
Bonacich, P. (1987). Power centrality: family measures. American Journal
Sociology, 92 (5), 11701182.
Borgatti, S. P., & Everett, M. (2006). graph-theoretic framework classifying centrality
measures. social networks. Social Networks, 28(4), 466484.
Brandes, U. (2001). faster algorithm betweenness centrality. Journal Mathematical
Sociology, 25 (2), 163177.
Brandes, U., & Erlebach, T. (2005). Network Analysis: Methodological Foundations. Lecture
notes computer science: Tutorial. Springer.
Castro, J., Gomez, D., & Tejada, J. (2009). Polynomial calculation shapley value
based sampling. Computers & Operations Research, 36 (5), 17261730.
Chalkiadakis, G., Elkind, E., & Wooldridge, M. (2011). Computational Aspects Cooperative Game Theory. Synthesis Lectures Artificial Intelligence Machine Learning.
Morgan & Claypool Publishers.
Conitzer, V., & Sandholm, T. (2004). Computing Shapley Values, manipulating value division schemes checking core membership multi-issue domains. AAAI 04:
Proceedings Nineteenth National Conference Artificial Intelligence, pp. 219
225.
Cormen, T. (2001). Introduction algorithms. MIT Press.
647

fiMichalak, Aadithya, Szczepaski, Ravindran, & Jennings

del Pozo, M., Manuel, C., Gonzlez-Arangena, E., & Owen, G. (2011). Centrality directed
social networks. game theoretic approach. Social Networks, 33 (3), 191200.
Deng, X., & Papadimitriou, C. (1994). complexity cooperative solution concepts.
Mathematics Operations Research, 19 (2), 257266.
Elkind, E., Goldberg, L., Goldberg, P., & Wooldridge, M. (2009). tractable expressive
class marginal contribution nets applications. Mathematical Logic Quarterly,
55 (4), 362376.
Eppstein, D., & Wang, J. (2001). Fast approximation centrality. SODA 01: Proceedings
Twelfth Annual ACM-SIAM Symposium Discrete Algorithms, pp. 228229.
Everett, M. G., & Borgatti, S. P. (1999). centrality groups classes.. Journal
Mathematical Sociology, 23 (3), 181201.
Fatima, S. S., Wooldridge, M., & Jennings, N. (2007). randomized method shapley
value voting game. AAMAS 07: Proceedings 11th International Joint
Conference Autonomous Agents Multi-Agent Systems, pp. 955962.
Fatima, S. S., Wooldridge, M., & Jennings, N. (2008). linear approximation method
shapley value. Artificial Intellilgence, 172 (14), 16731699.
Freeman, L. (1979). Centrality social networks: Conceptual clarification. Social Networks,
1 (3), 215239.
Gmez, D., Gonzlez-Arangena, E., Manuel, C., Owen, G., Del Pozo, M., & Tejada, J.
(2003). Centrality power social networks: game theoretic approach. Mathematical Social Sciences, 46 (1), 2754.
Goyal, A., Bonchi, F., & Lakshmanan, L. V. (2010). Learning influence probabilities
social networks. WSDM 10: Proceedings 3rd ACM international conference
Web search data mining, pp. 241250.
Granovetter, M. (1978). Threshold models collective behavior. American Journal
Sociology, 83 (6), 14201443.
Greco, G., Malizia, E., Palopoli, L., & Scarcello, F. (2009). complexity compact
coalitional games. IJCAI 09: Proceedings Twenty First International Joint
Conference Artifical Intelligence, pp. 147152.
Grofman, B., & Owen, G. (1982). game-theoretic approach measuring centrality
social networks. Social Networks, 4, 213224.
Ieong, S., & Shoham, Y. (2005). Marginal contribution nets: compact representation
scheme coalitional games. EC 05: Proceedings Sixth ACM Conference
Electronic Commerce, pp. 193202.
Irwin, M., & Shapley, L. S. (1960). Values Large Games, IV : Evaluating Electoral
College Montecarlo Techniques. CA : RAND Corporation, Santa Monica.
Jeong, H., Mason, S. P., Barabasi, A. L., & Oltvai, Z. N. (2001). Lethality centrality
protein networks. Nature, 411 (6833), 4142.
Kempe, D., Kleinberg, J., & Tardos, . (2003). Maximizing spread influence
social network. KDD 03: Proceedings Ninth ACM SIGKDD International
Conference Knowledge Discovery Data Mining, pp. 137146.
648

fiComputation Shapley Value Game-Theoretic Network Centrality

Kempe, D., Kleinberg, J., & Tardos, . (2005). Influential nodes diffusion model
social networks. Automata, Languages Programming, 3580, 99.
Koschtzki, D., Lehmann, K., Peeters, L., Richter, S., Tenfelde-Podehl, D., & Zlotowski, O.
(2005). Centrality indices. Network analysis, Vol. 3418 Lecture Notes Computer
Science, pp. 1661. Springer.
Lee, M.-J., Lee, J., Park, J. Y., Choi, R. H., & Chung, C.-W. (2012). Qube: quick
algorithm updating betweenness centrality. Mille, A., Gandon, F. L., Misselis,
J., Rabinovich, M., & Staab, S. (Eds.), WWW, pp. 351360. ACM.
Mann, I., & Shapley, L. (1962). Values large games, VI: Evaluating electoral college
exactly.. RAND Research Memorandum.
Matsui, T., & Matsui, Y. (2000). survey algorithms calculating power indices
weighted majority games. Journal Operations Research Society Japan, 43,
7186.
Michalak, T., Marciniak, D., Samotulski, M., Rahwan, T., McBurney, P., Wooldridge, M.,
& Jennings, N. (2010a). logic-based representation coalitional games externalities. AAMAS 10: Proceedings 9th International Joint Conference
Autonomous Agents Multi-Agent Systems, pp. 125132.
Michalak, T., Rahwan, T., Marciniak, D., Szamotulski, M., & Jennings, N. (2010b). Computational aspects extending Shapley Value coalitional games externalities.
ECAI 10: Proceedings Nineteenth European Conference Artificial Intelligence.
Myerson, R. (1977). Graphs cooperation games. Mathematics Operations Research,
2 (3), 225229.
Nagamochi, H., Zeng, D., Kabutoya, N., & Ibaraki, T. (1997). Complexity minimum
base game matroids. Mathematics Operations Research, 22 (1), 146164.
Newman, M. (2001). Scientific collaboration networks. II. Shortest paths, weighted networks,
centrality. Physical Review E, 64 (1), 016132(17).
Noh, J., & Rieger, H. (2004). Random walks complex networks. Physical Review Letters,
92 (11), 118701(14).
Osborne, M. J., & Rubinstein, A. (1994). Course Game Theory, Vol. 1 MIT Press
Books. MIT Press.
Reka, A., & Barabsi, A. (2002). Statistical mechanics complex networks. Reviews
Modern Physics, 74, 4797.
Sakurai, Y., Ueda, S., Iwasaki, A., Minato, S., & Yokoo, M. (2011). compact representation
scheme coalitional games based multi-terminal zero-suppressed binary decision
diagrams. PRIMA 11: Public Risk Management Association, pp. 418.
Schmeidler, D. (1969). nucleolus characteristic function game. SIAM Journal
Applied Mathematics, 11631170.
Schultes, D., & Sanders, P. (2007). Dynamic highway-node routing. SEA 07: Proceedings
6th Workshop Experimental Efficient Algorithms. LNCS, pp. 6679. Springer.
649

fiMichalak, Aadithya, Szczepaski, Ravindran, & Jennings

Shapley, L. S. (1953). value n-person games. Kuhn, H., & Tucker, A. (Eds.),
Contributions Theory Games, volume II, pp. 307317. Princeton University
Press.
Shapley, L. S., & Shubik, M. (1954). Method Evaluating Distribution Power
Committee System. American Political Science Review, 48 (3), 787792.
Stephenson, K., & Zelen, M. (1989). Rethinking centrality: Methods examples. Social
Networks, 11 (1), 137.
Suri, N., & Narahari, Y. (2008). Determining top-k nodes social networks using
shapley value. AAMAS 08: Proceedings 7th International Joint Conference
Autonomous Agents Multi-Agent Systems, pp. 15091512.
Suri, N., & Narahari, Y. (2010). Shapley Value based approach discover influential
nodes social networks. IEEE Transaction Automation Science Engineering,
99, 118.
Szczepaski, P. L., Michalak, T., & Rahwan, T. (2012). new approach betweenness
centrality based shapley value. AAMAS 12: Proceedings 11th International Joint Conference Autonomous Agents Multi-Agent Systems, pp.
239246.
Valente, T. (1996). Social network thresholds diffusion innovations. Social Networks,
18 (1), 6989.
Watts, D., & Strogatz, S. (1998). Collective dynamics small-world networks. Nature,
393 (6684), 440442.
Wooldridge, M., & Dunne, P. (2006). computational complexity coalitional resource games. Artificial Intelligence, 170 (10), 835871.
Yi, S.-S. (1997). Stable coalition structures externalities. Games Economic Behavior, 20 (2), 201237.
Young, H. P. (2006). Diffusion Innovations Social Networks. Blume, B. L., &
Durlauf, S. N. (Eds.), Economy evolving complex system, Vol. 3 Proceedings
volume Santa Fe Institute studies sciences complexity Santa Fe Institute
Studies Sciences Complexity, pp. 267282. Oxford University Press US.
Zolezzi, J. M., & Rudnick, H. (2002). Transmission cost allocation cooperative games
coalition formation. IEEE Transactions Power Systems, 17, 10081015.

650

fiJournal Artificial Intelligence Research 46 (2013) 343-412

Submitted 08/12; published 03/13

Hybrid LPRPG Heuristic Modelling Numeric
Resource Flows Planning
Amanda Coles
Andrew Coles
Maria Fox
Derek Long

amanda.coles@kcl.ac.uk
andrew.coles@kcl.ac.uk
maria.fox@kcl.ac.uk
derek.long@kcl.ac.uk

Department Informatics
Kings College London
Strand Building
London, WC2R 2LS, UK

Abstract
Although use metric fluents fundamental many practical planning problems,
study heuristics support fully automated planners working fluents
remains relatively unexplored. widely used heuristic relaxation metric
fluents interval-valued variables idea first proposed decade ago. heuristics
depend domain encodings supply additional information fluents,
capacity constraints resource-related annotations.
particular challenge approaches handling interactions metric
fluents represent exchange, transformation quantities raw materials
quantities processed goods, trading money materials. usual relaxation
metric fluents often poor situations, since recognise
resources, spent, longer available spent again.
present heuristic numeric planning problems building propositional
relaxed planning graph, using mathematical program numeric reasoning.
define class producerconsumer planning problems demonstrate numeric
constraints modelled mixed integer program (MIP). MIP
combined metric Relaxed Planning Graph (RPG) heuristic produce integrated
hybrid heuristic. MIP tracks resource use accurately usual relaxation,
relaxes ordering actions, RPG captures causal propositional aspects
problem. discuss two components interact produce single unified
heuristic go explore numeric features planning problems
integrated MIP. show encoding limited subset propositional problem augment MIP yield accurate guidance, partly exploiting structure
propositional landmarks propositional resources. results show
use heuristic enhances scalability problems numeric resource interaction
key finding solution.

1. Introduction
Domain-independent planning research last decade focussed, part,
propositional planning, leading important discoveries powerful new heuristics
planning propositional domains. Relatively little effort invested planning
metric fluents, despite importance representing many practical planning problems.
c
2013
AI Access Foundation. rights reserved.

fiColes, Coles, Fox & Long

Numbers essential efficiently encoding resources, money, fuel materials.
state-of-the-art remains influential approach proposed Hoffmann (2003),
extends ignore-delete-effects relaxation propositional fluents metric fluents
tracking accumulating upper bound increasing fluents ignoring decreasing (negative) effects. symmetrical treatment fluents purposes determining
lower bound leads representation equivalent interval metric
fluent, goals preconditions satisfied provided value interval
sufficient satisfy condition. Although lpg (Gerevini, Saetti, & Serina, 2006)
mips (Edelkamp, 2003) capable handling metric fluents, depend
relaxation heuristic offer search guidance. approaches explored
(Do & Kambhampati, 2001; Koehler, 1998), comparatively less successful.
Planners using MetricFF heuristic generally effective solving problems complex interactions values numeric resources,
exchange quantities one materials production others.
contrast, solving problems numbers heart operational research mathematical programming techniques. Many powerful solvers developed solving
Linear Programming problems (LPs) Mixed Integer Programming problems (MIPs),
problems expressed linear constraints variables (which, case
MIPs, integers). Although efforts exploit linear programming
propositional planning, either schedule actions (Long & Fox, 2003a) directly,
heuristic (van den Briel, Benton, Kambhampati, & Vossen, 2007), relatively little work
considered exploitation linear programming techniques improve behaviour
numeric domain-independent planners (Kautz & Walser, 2000; Shin & Davis, 2005; Wolfman & Weld, 2000; Benton, van den Briel, & Kambhampati, 2007) (this work
considered Section 1.1).
paper, revisit issue planning numeric resources, beginning
Metric Relaxed Planning Graph (RPG) heuristic (Hoffmann, 2003). focus specifically
domains exclusively exhibit call producer-consumer behaviour (defined
Section 2.3), actions increase decrease numeric resources fixed quantities.
course, represents subset possible numeric behaviours, common
intuitive one. Furthermore, easy recognise syntactically domain encoding,
simple resort alternative strategies domains conform
constraint, might include use producer-consumer relaxations approximations
domains complex numeric behaviour.
explore behaviour RPG heuristic, demonstrating typical
patterns interactions producer-consumer numeric planning domains lead highly
uninformative heuristic guidance, particularly domains offer opportunities exchanges metric variables. address this, introduce novel heuristic based
mixed integer program (MIP), used alongside RPG, better capture numeric
constraints. described MIP constructed, used complement RPG, discuss extensions improve identified weakness, also
encode information propositional behaviour problem. evaluate
lp-rpg heuristic exploring spectrum between, one end, strict separation
numbers propositions MIP RPG components; and, other, discarding
344

fiA Hybrid LP-RPG Heuristic Planning

RPG entirely encoding preconditions effects actions entirely MIP.
so, determine best trade-off two lies.
work report paper extension earlier work reporting development lp-rpg (Coles, Fox, Long, & Smith, 2008). extends work
additional detail several variants core heuristic, exploring impact
tighter integration propositional metric fluents heuristic.
1.1 Related Work
integration linear programming (LP) MIP techniques planning
considered number contexts. relevant present work use
LP basis heuristic propositional over-subscription planning problems (Benton,
Do, & Kambhampati, 2005). setting, goal planning find plan
maximum utility, defined terms reward goals achieved, minus costs
occurred achieving them. Benton et al. use LP optimisation tool help
decide set goals planner satisfy order achieve maximum reward.
work Benton et al. work described paper exploit relaxation
action ordering rather effects, employ LP well RPG
structure. two key differences focus work using MIP
capture interactions within numeric planning problems, rely conventional
Relaxed Planning Graph (Hoffmann & Nebel, 2001) propositional reasoning, rather
also encoding structure MIP. work van den Briel et al. (van den Briel et al.,
2007; van den Briel, Vossen, & Kambhampati, 2008) also explores use mathematical
programming encode solve planning problems.
structure LP MIP models proposed Benton et al. (2007) van den
Briel et al. (2008) makes time consuming solve, requiring actions selected
satisfy preconditions effects actions, delete effects paired
add effects. contrast, MIP LP models propose attempt capture
causal plan structure, making construction solutions programs
state much feasible. difference interaction two
components: integration MIP RPG lp-rpg much tighter,
MIP used graph building indicate variable bounds, relaxed plan
extraction indicate actions use. comparison, earlier approach, MIP
used solely introduce bias RPG action selection, giving preference actions
used solution MIP.
Linear programming exploited planning work. Lpsat (Wolfman
& Weld, 2000) uses planning-as-satisfiability approach, linked use LP solver
ensure literals representing (linear) constraints metric fluents maintained
plan construction. heuristic guidance search, based
standard DPLL search satisfying assignment combined confirmation
corresponding LP satisfiable. Ip-sat (Kautz & Walser, 2000) uses MIP encoding
planning problems basis solving them, Vossen et al. (1999), similar way
later work van den Briel et al. (van den Briel et al., 2008). planners MIP
used directly heart solver, planning problems translated MIPs
rather used guide search. Tm-lpsat (Shin & Davis, 2005) uses lpsat
345

fiColes, Coles, Fox & Long

system solve planning problems continuous processes. Kongming (Li & Williams,
2008) another example planner exploits compilation planning problems
mathematical programs, solved using CPLEX, tackle hybrid mixed-continuous planning
problems. colin system (Coles, Coles, Fox, & Long, 2012) also uses LP encodings
manage reasoning effects continuous processes.
completely different use linear mixed integer programming planning lies
work Ono Williams (2008) also Blackmore, Ono Williams (2011) uses
mixed integer programming foundation solving problem risk allocation
plan-level control systems.
Alternative approaches handling numeric variables planning include implemented MetricFF, discussed detail Section 3, Sapa (Do & Kambhampati, 2001)
Resource-ipp (Koehler, 1998). Sapa heuristic cost estimates generated using relaxed plan extraction supplemented additional costs representing minimal set
additional resource producing actions required achieve resource requirements
relaxed plans. approach straightforward implement interesting
modification pure relaxed plan heuristic, separates problem producing
resources solution rest problem, consequence relaxed
plan using steps high resource demands constructed preference
longer plan lower demands. heuristic value state distorted
penalty attached relaxed plan achieve high resource requirement, potentially hugely overestimating true distance state goal. Resource-ipp depends
identification consumers producers, paper, builds
resource time map tracks production consumption resources
Graphplan-based search plan. approach leads extension mutex
relation used constrain search Graphplan (Blum & Furst, 1995). However,
iterative-deepening search used Graphplan-based planners scalable solve
large problems forward state-space search proved dominant strategy past
decade.

2. Problem Definition
section define class planning problems consider paper.
subset general class pddl 2.1 non-temporal, numeric planning problems,
represent linear producerconsumer problems. include example Settlers
domain, used throughout paper illustrate ideas presented.
2.1 PDDL 2.1 Numeric Planning Problems
work, concerned finding sequential plans solve non-temporal, numeric
planning problems, defined using (a subset of) pddl 2.1 (Fox & Long, 2003). Within
pddl, class problems defined follows:1
1. pddl2.1 also supports specification objective function measure plan quality, defined
numeric variables planning problem, work focus minimising plan length.

346

fiA Hybrid LP-RPG Heuristic Planning

initial state, state consists set propositions, and/or assignment values set numeric variables. notational convenience, refer
vector numeric values given state v propositional facts F .
A, set actions. tuple hpre, eff i:
pre preconditions a: conditions must hold state
executed.
eff effects a: applied, state updated according
effects. eff consists of:
eff , propositions deleted state;
eff + , propositions added state;
eff n , effects acting upon numeric variables.
G, goal: set propositions F ? set conditions numeric variables,
N ? . sets may empty. state hF, vi goal state F ? F v
satisfies condition N ? .
general case, pddl numeric conditions (as used pre N ? ) expressed
form:
hf (v), op, ci
s.t.
op {, <, =, >, }, c <
Numeric effects (as eff n) expressed as:
hv, op, f (v)i

s.t.

op {=, +=, =, =, =}

common Hoffmanns work MetricFF (2003) restrict attention
preconditions expressed Linear Normal Form (LNF). is, expression
f (v) within preconditions must form weighted sum state variables
plus constant, w.v + k. Likewise, consider numeric effects f (v) LNF,
op {+=, =, =}. restrictions guarantee termination construction
RPG evaluating state: introducing non-LNF preconditions, scaling effects,
lead asymptotic numeric behaviour certain conditions satisfied
infinite limit. lp-rpg heuristic describe work, require
numeric behaviour actions represented producerconsumer behaviour. is,
effects cause constant increments decrements variables affect and, apart
specific circumstances, permit assignment effects. precisely define
notions circumstances allow assignment effects later paper.
solution planning problem (sequential) plan: sequence actions transforms initial state goal state, respecting preconditions action application.
state hF, vi, application action effects eff ,eff + ,eff n yields successor
state hF 0 , v0 i, where:
F 0 = (F \ eff ) eff +
v0 [x] op (w.v + c) hv, op, w.v + ci eff n
v0 [x] = v[x]
otherwise
347

fiColes, Coles, Fox & Long

2.2 Example Problem: Settlers
Settlers domain, introduced 2002 International Planning Competition (IPC) (Long
& Fox, 2003b) used 2004 (Hoffmann & Edelkamp, 2005), good example
problem exhibiting interesting use metric fluents. aim Settlers problems
build transport building infrastructure extraction, refinement
transportation materials. numeric structure domain perhaps sophisticated IPC domains date. First, six numeric resources several
actions act upon each. available resources, effects actions upon
(consumption resource shown negative value production shown
positive value) shown Table 12 . Another interesting feature domain
resources directly produced: whilst raw materials Timber, Stone
Ore directly extracted, Wood, Coal Iron must refined respective
raw form. Finally, domain contains transferable resources. addition actions
shown table, resources refined consumed fuel transportation,
resources loaded unloaded vehicles. effect load unload
actions increase decrease amount resource vehicle, decrease
increase amount stored given location. Apart consuming producing (i.e.
releasing) remaining cargo space vehicle, resource produced consumed
loading unloading moved. However, expressing model pddl
requires pair effects described, decreasing one variable increasing another,
indistinguishable combination production consumption.
2.3 ProducerConsumer Problems
define constrained producerconsumer numeric behaviour considered
paper. first define producer consumer actions, two categories producer. Using
define notion producerconsumer variable. identification
consumers producers new idea common identify resource producers
consumers scheduling (for example, Laborie work scheduling
resource constraints, see Laborie, 2003).
2.3.1 ProducerConsumer Actions
simple production action defined follows:
Definition 2.1 Simple Producer
ground action simple producer given numeric variable v iff:
effect (increase (v) c) (where c positive constant)
precondition refers v.
definition two important consequences:
2. table represents debugged version original domain available http://sourceforge.net/
projects/tsgp/files/

348

fiA Hybrid LP-RPG Heuristic Planning

Action
Move cart
Move train
Move ship
Fell timber
Quarry stone
Mine ore
Saw wood
Make coal
Smelt iron
Build cabin
Build quarry
Build mine
Build saw-mill
Build iron-works
Build coal-stack
Build dock
Build wharf
Build house
Build cart
Build train
Build ship
Build rail

Timber

Stone

Ore

Wood

Coal

Iron

-1
-2
+1
+1
+1
-1
-1

+1
+1
-2

-1

+1

-2
-2
-2

-2

-2
-2
-1

-2

-1
-2
-1

-1
-1

-2
-4
-1

Table 1: Production consumption Settlers domain
1. simple producer produces uniformly: state satisfies preconditions,
effect upon v always increase value constant amount, c,
irrespective precise details s.
2. potential maximum value v attained use
restricted value v itself: minimum maximum bounds v
must hold allow production.
define bounded producer follows:
Definition 2.2 Bounded Producer
ground action bounded producer numeric variable v iff:
effect (increase (v) c) (where c positive constant),
precondition (<= (v) (- ub c))
preconditions depending v.
bounded producer, a, applied v (ub c). Therefore, maximum
amount v attained using a, denoted max prod (a, v), ub, achieved
applying state v = (ub c). (In practice, state might reachable
actual upper bound value v reachable using might lower ub).
simple producer, a0 , assume max prod (a0 , v) = .
349

fiColes, Coles, Fox & Long

define bounded consumer follows:
Definition 2.3 Bounded Consumer
ground action consumer respect given numeric variable v iff:
precondition (>= (v) (+ lb c))),
effect (decrease (v) c), c constant
preconditions depending v.
definition analogous bounded producer, since requires v exceed
minimum value allowing consumption. consequence, lb minimum amount
v attained using a, denoted min cons(a, v) (by applying state
v = lb + c).
are, course, many resource use behaviours might encoded
planning domains. producer-consumer behaviour identify natural intuitive
one (a producer produces fixed quantity resource consumer consumes fixed
quantity depends availability quantity). variants
compiled form (e.g. consumers must leave fixed sized store resource
untouched simply translate origin resource measurement) also consider,
below, possible extensions basic behaviour. Nonetheless, must emphasise
heuristic develop paper targeted producer-consumer behaviour
usefulness depends common domains practice. frequent
occurrence model scheduling resources suggests natural
useful behaviour.
2.3.2 ProducerConsumer Variables
definitions (bounded) producer consumer actions, define properties
variables manipulate:
Definition 2.4 ProducerConsumer Variable
variable v denotes resource produced/consumed iff:
set prod (v) actions increase value v contains bounded producers,
set cons(v) actions decrease value v contains consumers,
upper bound v bounded producers v
lower bound v consumers v.

350

fiA Hybrid LP-RPG Heuristic Planning

2.3.3 Handling Integer Resources
Consumer actions (Definition 2.3) require amount resource available
consumption must least much consumer actually consumes.
domain encodings, behaviour essentially consistent producerconsumer patterns
represented using precondition consumer action consumes c units v
form v > k rather v c (and k < c). general case, v <, know
v k + (where epsilon infinitesimal positive), c units v consumed,
suggesting lower bound v (k c + ). However, case consumers
consume integral quantities, rewrite strict inequality since must 1
case. Consider, example, fragment load action Settlers:
:precondition (> (available timber l1) 0)
:effect
(decrease (available timber l1) 1)
effects change quantities available resources integral,
rewritten:
:precondition (>= (available timber l1) 1)
:effect
(decrease (available timber l1) 1)
similar transformation used constant effects variable
rational, simply finding least common multiple, LCM , denominators
fractions involved using = 1/LCM .

3. MetricFF Revisited
section briefly review way MetricFF (Hoffmann, 2003) handles metric
fluents highlight weaknesses approach faced particular
kinds numeric behaviours planning domains.
3.1 Metric Relaxed Planning Graph Heuristic
Metric RPG heuristic based performing relaxed reachability analysis forwards
state evaluated, reachability analysis captured planning
graph (Blum & Furst, 1995) structure. Two elements domain relaxed: delete
conditions actions ignored optimistic upper lower bounds used record
interval possible values metric fluent may reach. Positive effects metric
variable increase upper bound reachable values negative effects decrease
lower bound. Satisfaction preconditions tested checking value interval
variable satisfies metric condition precondition. interesting note
preconditions tested individually, possible, principle, single value could
satisfy conditions simultaneously, even though condition separately satisfied
value. Conjunctions convex preconditions, includes linear conditions,
satisfiable value case condition satisfiable interval
associated variable, except case conjunction inconsistent,
likely arise erroneous domain encodings.
351

fiColes, Coles, Fox & Long

MetricFF allows preconditions combine multiple variables, effects depend
values variables. lp-rpg allow linear combinations variables
preconditions, effects must conform producerconsumer definitions
allow constant increases decreases.
Heuristic evaluation state using Metric RPG heuristic undertaken two
phases: graph expansion phase solution extraction phase. remind
reader two processes convenience reference discussion follows.
3.1.1 Metric RPG Expansion
Graph expansion concisely defined follows:
Definition 3.1 RPG Expansion
Let F (i) denote fact layer, comprising:
F P (i), set propositions;
F V (i), array upper- lower- bound pairs task numeric variable v
A(i) denotes action layer, consisting list ground actions. RPG begins
fact layer, F (0), defined based state evaluated:
F P (0) contains propositions hold S;
entry hLBv , UBv F V (0) set hS[v], S[v]i, i.e. value v S.
RPG expanded adding successive action layers, followed new fact layers:
Action layer A(i + 1) contains actions A, that:
propositional preconditions F P (i);
numeric preconditions satisfied values variables
F V (i).
Fact layer F P (i + 1) determined A(i + 1):
propositions F P (i + 1) F P (i), plus new facts added
action A(i + 1);
values numeric values F V (i + 1) first set F V (i), updated
extending interval variable include values achieved
maximum minimum possible assignment effects, action A(i + 1)
turn.
termination condition met, RPG expanded actionlayerfact-layer pairs.
reachability analysis therefore consists alternate steps: determining actions
applicable, hence instantiating next action layer, using extend
next fact layer. process presented graphically Figure 1, small problem
facts f0 ...fn numeric variables v0 , v1 . Considering first propositions:
352

fiA Hybrid LP-RPG Heuristic Planning

Fi

i+1

F i+1

f0

i+2

F i+2

f0

f0

f1



f1



f1

f2

B

f2

B

f2

f3

f3

f4

v0[0,0]

v0[0,2]
2

v1[2,2]

+2

f4

>=

+2

2

C

>=

2

C

v0[0,4]
2

v1[0,2]

v1[2,2]

Figure 1: Portion relaxed planning graph, C produces 2 units v0, consumes
2 units v1
Arrows fact action layers denote precondition dependencies actions
instance, action appear layer A(i + 1) f0 present F (i).
Arrows action fact layers denote effects instance, f3 F (i + 1)
added A.
numeric variables v0 , v1 , bounds shown square brackets. Action C
seen one precondition (v1 2) two effects (increase v0 2, decrease v1
2). C exhibits producerconsumer behaviour consumes 2 units v1 , produces
2 units v0 . preconditions satisfied F (i) therefore effects applied
layer A(i + 1), upper bound v0 increased lower bound v1
decreased. Moreover, bounds change layer F (i + 2), possible
application C.
variable bounds continue diverge way, RPG expansion needs
well-defined termination condition. positive case, terminate success
first layer F (i) goal propositions F P (i) goal numeric expressions
satisfied F V (i) (in relaxed sense). negative case, terminate failure
F (i) three following hold:
1. actions appear A(i + 1) present A(i) (and hence new propositions would present F P (i + 1)),
2. hitherto unsatisfied preconditions v c, action, U B(v) would change
F V (i) F V (i + 1),
3. hitherto unsatisfied preconditions v c, action, LB(v) would change
F V (i) F V (i + 1).
353

fiColes, Coles, Fox & Long

Algorithm 1: Adding action relaxed plan
Data: R metric RPG, action include relaxed plan, q subgoal
queue
1 foreach propositional precondition pre
2
l layer pre first appears;
3
l > 0 insert pre q[l].prop;
4
5
6

foreach numeric precondition pre
l layer pre first appears;
l > 0 insert pre q[l].num;

intuition behind conditions monotonic expansion RPG
implies that, new facts appearing numeric preconditions could become satisfied future layer, graph expansion stagnated relaxed problem
unsolvable.
3.1.2 Metric RPG Solution Extraction
expanded planning graph found relaxed solution exists (all
goals appeared), next step extract relaxed solution plan. done
regressing planning graph, using priority queue intermediate sub-goals
(latest layer first). subgoal, achieving action added relaxed plan
preconditions added queue goals achieved earlier layer.
relaxed plan extraction algorithm shown Algorithm 2. lines 3 6, priority
queue initialised top-level goals problem. propositional numeric
goals added priority queue achieved earliest fact layer
appeared. priority queue seeded, solution extraction proceeds regressing
layer-by-layer. propositions, suffices find action adds fact
increment heuristic value one add preconditions achieving action
queue, using Algorithm 1. numeric preconditions, process slightly
involved:
subgoal achieve v k v k layer F (l), action A(l)
assigns value k v, action chosen satisfy subgoal.
Otherwise, v k (or v k) must achieved layer F (l), actions increasing
(decreasing) v chosen A(l) residual value k (i.e.
original value k adjusted take account effects selected actions)
small enough (large enough) reachable layer F (l 1). residual condition
v k (v k) added queue subgoal achieved F (l 1),
modified value k.
Note lines 13, 19, 25, 32 39, actions chosen action layer 1
recorded adding set ha. used basis helpful action set:
action effect common action set ha considered helpful. Helpful
actions important element performance MetricFF: actions achieve
effects exploited relaxed solution state promoted search
state.
354

fiA Hybrid LP-RPG Heuristic Planning

Algorithm 2: Relaxed plan extraction

5

Data: R - metric RPG; F ? , N ? - problem goals
Result: ha - helpful actions, h - heuristic value
ha , h 0;
q deepest-first priority queue goal layers;
foreach p F ?
l layer p first appears;
insert p q[l].prop;

6
7
8

foreach f N ?
l layer f first holds;
insert f q[l].num;

9

q empty
(l, hprop, numi) pop(q);
foreach p prop
h h + 1; achiever p;
action layer 1 add ha;
prop prop \ add effects a;
call Algorithm 1 R, a, q;

1
2
3
4

10
11
12
13
14
15
16
17
18
19
20
21

foreach (v c) num
action A(l) assigned v = k, k c
h h + 1;
l = 1 add ha;
call Algorithm 1 R, a, q;
remove (v c0 ), c0 k (v c0 ), c0 k num;

22
23
24
25
26

foreach (v c) num
action A(l) assigned v = k, k c
h h + 1;
l = 1 add ha;
call Algorithm 1 R, a, q;
remove conditions (v c), c k num;

27
28
29
30
31
32
33
34
35
36
37
38
39
40
41

foreach (v c) num
F V (l 1)[v].upper < c
h h + 1; next increaser v;
decrease c (v, a);
l 1 add ha;
call Algorithm 1 R, a, q;
l > 0 insert (v c) q[l 1].num;
foreach (v c) num
F V (l 1)[v].lower > c
h h + 1; next decreaser v;
increase c (v, a);
l 1 add ha;
call Algorithm 1 R, a, q;
l > 0 insert (v c) q[l 1].num;

355

fiColes, Coles, Fox & Long

3.1.3 Use Relaxed Plan Search
relaxed plan, computed heuristic calculation, used two ways search.
MetricFF makes use two-stage search approach, number actions
relaxed plan used heuristic goal-distance estimate. first search phase,
enforced hill-climbing (EHC), greedy hill climbing search approach. thought
performing breadth first search forward initial state I, state progression
action application, state new global-best heuristic value found.
state, S, found, states discarded EHC search continues
manner S. search strategy incomplete due greedy nature,
discarding states could lead loss solution. such, followed
complete WA* search, order guarantee completeness planner (subject
sufficient time memory).
Since EHC phase already incomplete, designed find solutions quickly,
MetricFF makes use another completeness-sacrificing technique order attempt
guide planner solutions quickly. technique referred helpful action
pruning. Here, actions helpful action set state considered
successor generation: actions helpful discarded. Note pruning
used best-first search would compromise completeness. practice helpful
action pruning improves performance MetricFF many domains, however,
lead difficulties actions needed find solution given state appear
helpful action set. attempt compensate this, EHC terminates
considering helpful actions, returns state last global-best heuristic
value, searches considering applicable actions, either terminates
(leading WA*) state new global-best heuristic value found,
point helpful-action pruning re-enabled, EHC continues.
3.2 Problems Metric RPG Heuristic
Although Metric RPG powerful tool support planning metric fluents,
common situations planning problems heuristic gives flawed
guidance. problems include resource persistence cyclical resource transfer.
discuss phenomena result misleading heuristic guidance
relaxed plan length poorly approximating actual solution length helpful
action distortion.
3.2.1 Resource Persistence
Resource persistence consequence using relaxation ignores negative effects.
resource consumed disappear relaxation negative effects.
opportunity reuse resource suggest significantly shorter plan
available case reality. Although problem occurs propositional
metric fluents, fact metric fluents commonly encode resources must
carefully managed means problem often acute domains metric
fluents. example, state Settlers domain 2 units resource
produced ship required (either goal means travel
356

fiA Hybrid LP-RPG Heuristic Planning

otherwise inaccessible location), relaxed plan require production
resources (see Table 1).
One approach approximating number missing resource production actions
introduced planner Sapa (Do & Kambhampati, 2001). v = state
evaluated, relaxed plan consumes c units v, produces p,
(c p) > s, shortfall production v, would necessitate additional
actions added relaxed plan. case, maximum amount v
produced single action v, heuristic value increased by:


cps
.
v
increase lower bound number additional actions needed and,
indicate actions might be, serves increase heuristic value states whose
relaxed plans resource production shortfalls. does, however, two main limitations. First, relaxed plan extraction (such approach shown Algorithm 2) chooses
actions without consideration undesirable resource consumption side-effects.
good search choice might lead state worse heuristic value purely
accident choice achieving actions (consuming resource unnecessarily). Second,
increasing heuristic value without adding specific additional actions relaxed plan,
helpful actions record fact appropriate resource production helpful.
3.2.2 Cyclical Resource Transfer
phenomenon Cyclical Resource Transfer (CRT) consequence encoding
actions move resources around, combined relaxation negative effects.
encode movement resource, removed one location added another.
removal encoded decrease relaxed building Metric RPG.
result, moving resources appears generate new resource destination, making
movement spuriously attractive alternative production. Consider state 1
unit timber cart location, p1, goal 2 units timber
p1. Clearly, solution plan must involve production timber. However,
valid relaxed plan solution, found using Metric RPG heuristic, is:
0:
1:

(load v1 p1 timber)
(unload v1 p1 timber)

3.2.3 Helpful Action Distortion
problems resource persistence CRT important consequence EHC
search. result relaxed plans misleadingly short lengths,
relaxed plans typically contain production actions useless transfer actions.
situation, production actions often appear helpful action set,
therefore included EHC search, even states could conveniently
applied. refer problem helpful action distortion. illustrate arises,
reference Settlers domain, consider state unit
timber location a, goal unit timber location b. relaxed
plan use timber construct cart load timber onto cart
transport it. planner therefore consider producing timber.
357

fiColes, Coles, Fox & Long

4. Compiling ProducerConsumer Behaviour Mathematical
Program
section describe mathematical program built characterise
interaction numeric variables action choices producerconsumer framework.
4.1 Constraints ProducerConsumer Variables Actions
definition producerconsumer variables (Definition 2.4) implies actions
useful property preconditions variables derived effects
actions, together global variable bounds. Specifically, action a:
produces c units v precondition requiring v d, = ub(v) c,
ub(v) global upper bound v applied. condition
expresses effect upper bound precondition value
v (since v ub(v)).
consumes c units v v must satisfy v lb(v) + c action
applied. Again, expression leads effect precondition tied
one constraint.
ordering actions relaxed (that is, causal relations force
ordered ignored) value v series actions applied, v 0 ,
given by:
X
v0 = v +
Ca .(v, a)
(1)
aA

Ca non-negative (count) variable indicating many times action
applied, v 0 [lb(v), ub(v)] (v, a) defined follows:
produces c units v (v, a) = c;
consumes c units v (v, a) = c;
Otherwise, (v, a) = 0.
Note equation linear, since Definitions 2.2 2.3 require (v, a) constant
v a.
equations support construction mathematical program consisting one
variable action, a, one variable flow equation producerconsumer
variable, v. program is, fact, mixed integer program (MIP), variables
(the action counts, a) represent applications actions, integral. However,
relaxation exploited allow action count variables take non-integral
values, yielding linear program (LP). significant potential benefit
LPs solved far efficiently MIPs.
358

fiA Hybrid LP-RPG Heuristic Planning

4.2 Bounding Action Variables
Within equation associated state variable (i.e. Equation 1), action
corresponding variable denoting many times applied. general
limit number times action applied. However, numeric decrease
effects propositional delete effects may impose constraints practice, due limited
availability resources. example, applying action increases v expense
decreasing w, w resource producer, limit
Ca implicit consequence instance Equation 1 governing value w.
Specifically, ai never exceed w/(w, a): value w divided change causes
w. Moreover, w monotonically decreasing (w, a) constant action
a, global upper bound Ca set w(I)/(w, a), w(I) value w
initial state, I.
action increases v expense irreversibly deleting fact p, fact
precondition, clearly applied one-shot action (Coles,
Coles, Fox, & Long, 2009). However, contrast numeric delete effects discussed
above, captured producerconsumer constraints concerned
numeric change. However, constraint use captured
setting upper bound Ca 1. Moreover, collection actions a0 ...an1
depend fact p irreversibly delete it, say form one-shot
action set p, add constraint:
Ca0 + Ca1 + ... + Can1 1

(2)

4.3 Assignment Constraints
general, direct assignments values variables cannot represented directly constraints following form Equation 1. Assignments correspond, effectively, statedependent increases decreases. instance, assignment value 2 variable
v, state v = 0, equivalent producing 2 units, equivalent consuming
1 unit applied state v = 3. However, producerconsumer equations
notion state, coefficients action variables denote production
consumption. Therefore, state variables subject constant-valued change
MIP cannot used encode general assignment effects without extending
allow quadratic constraints (that is, constraints involving products pairs variables).
However, specific conditions assignments safely modelled
within mathematical program retaining linearity constraints.
One class assignment effects encoded MIP
actions assignment effects variable, v = k, applied states
v = c known constant, c. case, effect rewritten
increase effect v k c, making assignment actions follow standard pattern
producer/consumer actions. particular case rewriting made possible
following conditions hold set actions A:
1. action depend upon affect v condition satisfied
achieved actions actions A.
2. Applying action precludes assignments v.
359

fiColes, Coles, Fox & Long

conditions ensure set actions assign v form one-shot action
set value v assumed 0 prior application one actions
set, action set rewritten increase v assignment value.
situation one arises encodings objects created certain actions
objects associated metric variables initialised object creation
(such capacity newly created vehicle Settlers domain).

5. Linear ProgrammingRelaxed Planning Graph Heuristic
defined producerconsumer behaviour shown supports construction
MIP action ordering relaxed. MIP is, principle, hard solve (constructing
solutions NP-hard), basis heuristic evaluation states seems sensible
relax integrality constraints action variables reduce problem
linear program. first consider LP used two stages heuristic
evaluation state (reachability analysis relaxed plan extraction)
reconsider question whether relaxation linear program necessary practice
compromise might full MIP LP.
5.1 Overview
context use LP forward state-space search planner. task
intend use heuristic evaluation states. Thus, assume
state (a complete assignment variables define problem,
propositional numeric) interested estimating number actions
required transition goal state. approach use based
strategy used MetricFF: first construct reachability analysis using layered graph
alternating facts actions extract relaxed plan. determine whether
action applied reachability stage check propositional preconditions
usual way numeric preconditions checked determining whether values
reachable ranges recorded metric variables satisfy condition.
reachable ranges calculated using LP described, explain below.
Extraction relaxed plan involves determining actions support required
conditions (both goals preconditions selected actions), conditions
involve numeric variables use LP decide actions required
many used.
5.2 Using LP Graph Expansion
graph expansion phase calculation Metric RPG heuristic seen
layer-by-layer relaxed propositional reachability analysis, synchronised relaxed
numeric bounds analysis. Definition 3.1 shows numeric variable bounds appear
graph expansion algorithm two places. First, F (i) used determine
actions appear A(i + 1), next action layer (those whose preconditions within
reachable range). Then, actions deemed applicable used update variable
bounds subsequent fact layer, F (i + 1).
360

fiA Hybrid LP-RPG Heuristic Planning

maximise: v00
v00 = 0 + 2.CC
v10 = 2 2.CC
v00 0
v10 0
C0

v00
1
1

v10

1

CC
-2
2

=
=




1
1
1

(a) Equations

max
0
2
0
0
0

v00 [0, 2]
v10 [0, 2]
(c) Solutions min/max

(b) RowColumn

Figure 2: LP maximise value v0 layer F (i + 2) Figure 1 (treating layer Fi
initial state construction)

Due relaxed nature way numeric values considered, metric RPG
tends produce highly optimistic bounds numeric values. Returning Figure 1,
see that, effect, action C converts two units v1 two units v0 and,
initially, two units v1 present. Hence, most, could hope produce two
v1 , F V (i + 2), upper bound v1 already 4. Ignoring consumption effect
C makes possible produce arbitrary amounts v0 : C applicable upper
bound v1 greater equal two, reality true long C
yet applied.
accurate estimate variable bounds layer F V (i) found using
LP encoding described Section 4.1. model parameterised follows:
action variables corresponding actions A(i) used, ensures
reachable actions considered computing resource bounds. relevant
one-shot action constraints included. absence restriction number
action applications contrasts constraint used Metric FF action
may applied per action layer. practice prevent LP variables
becoming unbounded, set finite, large maximum value action variables.
initial value variable v set state, S, evaluated.
post-value variable, v 0 , F V (i)[v], range values could reach
F V (i). always, v 0 [lb(v), ub(v)].
Substituting parameters producerconsumer equation (Equation 1) yields:
X
F V (i)[v] = S[v] +
Ca .(v, a)
(3)
aA(i)

model, use LP solver find upper lower bounds
F V (i)[v], setting objective function accordingly.
Returning example, consider finding upper bounds variables F (i+2)
Figure 1 starting state corresponding one given Fi figure (thus,
considering constraints layer 2 following state treating starting
point). corresponding LP shown Figure 2, primed variables ones
361

fiColes, Coles, Fox & Long

use represent values numeric variables layer interest (i.e.
case layer 2 ahead state evaluated). Maximising v00 , i.e. using upper
bound F V (i)[v0 ], yields result 2: greater value possible, since setting C
value greater 1 (and thus producing v0 ) would lead violation constraint
v10 0. ranges variables computed, using LP four times (minimising
maximising two variables), shown Figure 2c. seen,
improve bounds calculated situation MetricFF (Figure 1),
v00 [0, 4] v10 [2, 2] respectively.
5.2.1 Notes LP Efficiency
LP solved twice per variable per layer expanding planning
graph, important steps taken minimise computational cost. reduce
costs using combination techniques, avoid needing solve LP
computing bound given variable, others reduce cost solving
LP itself.
1. First, consequence termination criteria RPG expansion (Section 3.1.1),
need compute upper (lower) bound given variable current
value large enough (small enough) satisfy preconditions goals
appears. case, avoid solve LP determine variable
bound, instead re-use bound computed previous layer, without
affecting behaviour heuristic.
2. variable never appears numeric precondition goal, entirely
excluded LP.
3. bounds variables change monotonically additional layers added
planning graph. Therefore, computing new upper (lower) bound variable v temporarily add LP constraints corresponding bounds
computed preceding layer (each added separately variable first minimised maximised). so, refuse admit tighter variable bound
previous layer.
4. Finally, actions increase (decrease) effect variable yet
added LP, need compute upper (lower) bound variable,
effect bound variable increased (decreased)
beyond value state evaluated.
5.3 Basic Use LP Solution Extraction
consider LP used give guidance action selection relaxed
plan extraction. First, observe LP directly affected propositions,
hence cannot used find actions able achieve given fact. Thus,
concern LP used identify actions use attain
numeric subgoals either top-level numeric goals, numeric preconditions
actions chosen solution extraction.
362

fiA Hybrid LP-RPG Heuristic Planning

Algorithm 3: Adding weighted action relaxed plan

1
2
3
4
5
6
7
8
9
10
11
12

Data: R metric RPG, action include relaxed plan, q subgoal
queue, w weight
foreach propositional precondition pre
l layer pre first appears;
l > 0
(pre, k) q[l].prop
k < w k w;
else insert (pre, w) q[l].prop;
foreach numeric precondition pre
l layer pre first appears;
l > 0
({pre}, k) q[l].num
k < w k w;
else insert ({pre}, w) q[l].num;

first key difference using LP relaxed plan extraction concerns
choice actions achieve numeric preconditions. original metric RPG heuristic,
given numeric precondition (e.g. x c) fact layer + 1 regressed
beneficial numeric effects layer i, giving residual numeric precondition (e.g. x c0 )
achieved fact layer i. (This process shown lines 28 41 Algorithm 2).
lp-rpg case, shown Algorithm 4, numeric precondition layer l (temporarily)
added LP generated layer l (as constraint (line 22). find actions use
achieve this, LP solved (line 23), objective minimise weighted
sum across action variables (one possible weighting scheme suffice
purpose minimise sum action variables, though return question
appropriate weighting schemes later paper). Finally, actions whose corresponding
variables non-zero (line 24) added relaxed plan (lines 25 34).
Second, must accommodate fact LP, relaxation underlying
MIP (in action variables integers), may solved applying actions nonintegral number times. simple example, several actions increment
given variable, alone would suffice achieve goal value variable,
valid optimal solution LP sum variables corresponding actions 1. every action non-zero action count variable
solution considered applied relaxed plan length could greatly over-estimate
required number actions. mitigate problem, subgoal (e.g. x c) arising
solution extraction associated weight, weights, along
values given action variables LP, used update relaxed plan length.
weights manipulated throughout Algorithms 3 4, use summarised
follows:

Initially, goal fact added subgoal queue associated weight 1, i.e.
achieved, entirely. Also note that, contrast Algorithm 1,
363

fiColes, Coles, Fox & Long

Algorithm 4: Relaxed plan extraction LP

1
2
3
4
5

Data: R - metric RPG; P G - propositional goals;
N G - numeric goals
Result: ha - helpful actions, h - heuristic value
ha , h 0;
q deepest-first priority queue goal layers;
foreach p P G
l layer p first appears;
insert (p, 1) q[l].prop;

|N G| > 1
l final layer R;
8
insert (N G, 1) q[l].num;

6
7

9
10
11
12

else
f fact N G;
l layer f first holds;
insert ({f }, 1) q[l].num;

q empty
(l, hprop, numi) pop(q);
foreach (p, w) prop
h h + w;
achiever p;
action layer 1 add ha;
19
call Algorithm 3 R, a, q, w;
20
prop prop \ add effects a;

13
14
15
16
17
18

21
22
23
24
25
26
27
28
29
30
31
32
33
34

foreach (G, w) num
LP LP(l) + constraint(s) G;
solve LP, minimising weighted action sum;
av {action variable (a = c) LP | c 6= 0};
foreach av
h h + w.c;
layer 1 add ha;
foreach propositional precondition pre
l layer pre first appears;
c min[c, 1];
(pre, k) q[l].prop
k < w.c k w.c;
remove (pre, k) q[l].prop;
else insert (pre, w.c) q[l].prop;

subgoal queue records layer goal introduced well
associated weight.
action chosen applied Ca times achieve queued propositional/numeric sub-goal g, either:
chosen applied (i.e. Ca = 1) achiever propositional
subgoal g, associated weight w;
364

fiA Hybrid LP-RPG Heuristic Planning

action applied (given non-zero value Ca ) solving LP
achieve numeric subgoal g, associated weight w.
cases, relaxed plan length incremented Ca .w.
weight given preconditions w0 = w.min[Ca , 1]. weights
preconditions used update weight attached achieving corresponding sub-goals earlier layers:
propositional precondition p already recorded subgoal earlier
layer, weight k, weight updated max[k, w0 ]. Otherwise,
p added subgoal RPG, satisfied first layer
appears, weight w0 .
case added support propositional sub-goal: numeric
precondition p already recorded subgoal earlier layer,
weight k, weight updated max[k, w0 ]; otherwise, p added
subgoal RPG, satisfied first layer appears,
weight w0 .
5.4 Consequences Use LP Solution Extraction
Use LP aid identification selection actions support achievement
numeric goals subgoals extraction relaxed solution lead
important consequences heuristic guidance offered relaxed solution.
already noted problem non-integral fragments actions might combined
achieve numeric effects indicated managed handling fractional
preconditions fractional action costs. However, potential problems
discuss.
5.4.1 Partially Applied Helpful Actions
Consider situation precisely five possible ways achieve particular
numeric goal, uses three actions. simple example small
problem Settlers domain, five carts unit timber available
location A: goal one unit timber location B achieved loading
timber onto carts A, moving B, unloading B.
metric RPG heuristic used achieve goal, three actions would used.
Working backwards goal, selected actions would be:
action layer three, unload action (from cart c) increase amount timber
B;
action layer two, action move cart c B;
action layer one, action load unit timber onto cart c.
solving LP achieve goal (ignoring propositional preconditions),
objective minimising sum action variables, solution returned
365

fiColes, Coles, Fox & Long

objective value 2. denote relevant load/unload action variable pair
cart (li , ui ), pool solutions could returned satisfying:


li = ui

i[1..5]

(

X

(li + ui )) = 2

i[1..5]

Then, non-zero variable ui = k relevant action move cart B
also added relaxed plan, weight k (line 19, Algorithm 4).
purposes providing contribution relaxed plan length, unimportant
solutions returned: sum action variables 2,
sum move actions added 1, giving total relaxed plan length 3. However,
discussed Section 3.1.2, relaxed plan also used determine set helpful actions:
effect common actions relaxed plan chosen
action layer one. example, action layer one consists action (or actions)
load unit timber onto cart A. original metric RPG case, exactly one action
would used. However, using LP five actions could (fractionally) used.
consequence within pool LP solutions could returned,
lead search much greater branching factor, case factor
five greater.
source problem relaxation integrality constraints action
variables extent affects search depends precise solution returned
LP solver: different solvers may greater lesser tendency return solutions
action variables assigned non-integral values. extreme response
problem would revert MIP require action variables integral rather
real-valued. Alternatively, focussing issue identified here, one could require
action variables corresponding actions action layer one integers. either
case, result mixed integer programming problem, since cost MIP-solving
exponential number integer variables, difference variants
significant. change need made point switch using
LP solution extraction rather graph expansion as, prior this, assignments
action variables unimportant (only value objective function used).
course, price pay potentially significant, MIP-solving NP-hard,
LP-solving polynomial. However, exchange shift complexity,
example given above, left single helpful action, longer possible
fractionally load timber onto cart: one cart must chosen. extent
two possible integer-modifications affect search performance considered later
evaluation.
5.4.2 Preferring Earlier Actions
Within Metric RPG heuristic explicit preference using actions
appear earlier relaxed planning graph. shown Algorithm 1, fact
needed, either goal satisfy precondition action chosen insertion
relaxed plan, queued sub-goal satisfied first fact layer
appeared. Then, action chosen support fact, amongst
earliest possible achievers. intuition behind preference earlier actions based
366

fiA Hybrid LP-RPG Heuristic Planning

observation that, later action appears relaxed planning graph,
greater number actions need added relaxed plan support
preconditions. Therefore, preferring earlier actions usually leads shorter relaxed plans
hence closer approximations optimal relaxed plan length.
Within LP, objective set minimise sum action variables (i.e. use
actions possible), distinction actions appear earlier
RPG appear later. Recalling LP disregards propositional
preconditions actions, failing take account action first added
RPG lead LP-based relaxed plan extraction generating poor quality solutions
and, consequently, poor search guidance.
address this, pressure generated within LP objective function
tuned prefer actions need fewer supporting actions relaxed plan.
achieved forcing LP favour actions appear RPG earlier. encode
preference earlier actions weighting scheme action variables
objective function. Actions appearing earlier given smaller weights
appear later. propose (and, later, evaluate) two ways achieving this. first,
simpler, use geometric series dictate coefficient given action variable
based layer l first appears. case, objective coefficient
is:
kl
k < k > 1.
value k controls extent earlier actions preferred, interpreted treating k.n actions selected layer l exactly good selecting n actions
layer l + 1 (so anything less k.n actions layer l preferable selecting
n actions layer l + 1). Throughout remainder paper, refer scheme
layer-weighting value k.
second option record, cost action, estimate number
actions needed support propositional preconditions, use weight
objective function. achieved using RPG cost propagation algorithm
Sapa (Do & Kambhampati, 2003). achieve this, planning graph expanded,
costs fact action recorded updated layer. Initially, fact
p state evaluated, cost fact layer zero, cost(p, 0), zero. (For fact
p true time zero, cost(p, 0) = .) fact costs used derive action
costs, using rules akin used hadd /hmax (Bonet & Geffner, 2001). cost
action layer t, cost(a, t) defined according one either:
cost max (a, t) = max cost(p, 1)
ppre(a)

cost sum (a, t) =

X

cost(p, 1).

ppre(a)

action costs, turn, used update costs proposition
subsequent fact layer, cost fact reduced cheaper
way achieve it. action layer t, potentially reduce cost
propositions p adds:
cost(p, t) = cost(a, t) + 1 iff (cost(a, t) + 1) < cost(p, 1)
= cost(p, 1) otherwise
367

fiColes, Coles, Fox & Long

planning graph expanded, process alternating action cost estimation,
fact cost estimation, used propagate cost information RPG.
setting objective LP, use cost action, cost(a, t), coefficient
action variable corresponding a. Using cost propagation described,
costs derived solely basis propositional preconditions effects. Therefore,
cost(a, t) estimate number actions needed support preconditions
a. purposes, desirable: LP will, itself, add actions support numeric
preconditions, estimate number actions needed support propositional
preconditions action measure cost impact upon relaxed plan due
action selected.

6. Adding Propositions LP
Benton et al. (2005) explore idea using LP guide search propositional planning
problems context over-subscription planning. work LP used
determine goal subset achieve, gain maximum utility. Whilst successful
achieving aim, authors observe use LP heuristic guide search
expensive, indeed expensive feasible. propositions encoded
LP way propose, task solving LP becomes equivalent solving
entire planning problem relaxed action ordering non-integer action variables.
section, reconsider inclusion propositions LP, considering spectrum
possibilities including propositions way include propositions.
Even though focus work numeric problems, including propositions
LP might still interest. instance, supporting propositional goal might
require consumption numeric resources. worst case, one could compile
problem numeric goals become preconditions action achieves
dummy propositional fact goal, modify problem goal goal. Since
numeric goals modified problem, LP used solve
individual preconditions dummy action (inevitably) chosen, rather
requiring goals satisfied conjunction, described Section 7.1. Since
dummy-goal model merely reformulation original problem, information
theoretically accessible conveyed LP. generally, hope
identify intermediate landmark propositions, well final goals, could usefully
encoded LP.
6.1 Adding Propositional Goals LP
Although LP describe Section 4 contain specific reference propositions, hence propositional goals, formulate constraints act proxy
them, considering actions achieve them. need introduce additional
variables. Instead, add constraints ensure least one achiever chosen
propositional goal. goal fact g true state evaluated,
list actions [a0 ..an1 ] achieve g, add constraint LP
recent layer planning graph:
a0 + ... + an1 1.
368

fiA Hybrid LP-RPG Heuristic Planning

is, least one achieving action must used or, specifically, given actions
partially applied, total least one achieving action must used. LP
containing constraints used augment positive termination criteria
graph expansion, detailed Section 3.1.2. terminate first fact layer where:
1. goal propositions F ? F P (i) (as before);
2. goal numeric expressions N ? satisfied (individually) F V (i) (as before);
3. LP used compute numeric bounds layer F (i) still solvable
constraints propositional goals added.
Use goal-checking LP two key consequences. First, actions layer
F (i) cannot used satisfy goals whilst respecting numeric constraints
LP, additional layers added planning graph necessary actions
appeared (or termination criterion reached). Thus, reasoning resource
persistence (Section 3.2.1), heuristic better able recognise cases where, although propositional goals might appear individually reachable, either additional
production needed meet collectively, alternative actions need used,
state dead-end. Second, solution LP used confirm point (3) above, used
indicate actions add relaxed plan achieve propositional goals.
propositional preconditions actions satisfied usual way (line 28
Algorithm 4).
requiring sum action variables selected achieve goals
least one, allowing variables real-valued, LP could, theory, provide
weaker guidance RPG. similar issue noted Section 5.4.1
considering helpful actions, could ameliorated similar manner, namely making
goal-achieving action variables integral. return issue evaluation,
considering whether benefits search.
6.2 Using Landmarks LP
landmark fact (Hoffmann, Porteous, & Sebastia, 2004) propositional fact must
true point every solution plan given planning problem. first work
landmarks (Porteous, Sebastia, & Hoffmann, 2001) proposed method extracting
subset landmarks planning problem based regressing goals using
delete relaxation FF. Since introduction idea 2001 (Porteous et al., 2001),
landmarks come play increasingly important role planning. Recent development new techniques extracting landmarks (Richter, Helmert, & Westphahl, 2008;
Zhu & Givan, 2003) development heuristics based different relaxations (Richter
& Westphal, 2010; Domshlak, Katz, & Lefler, 2010; Helmert & Domshlak, 2009; Karpas &
Domshlak, 2009) allowed planning community exploit landmarks successfully.
relaxed plan extraction phase lp-rpg heuristic relaxes action ordering
propositional preconditions effects might benefit substantially delete-relaxation
landmarks. use landmark facts LP offers mechanism
tightly couple LP RPG, allowing increased information sharing
369

fiColes, Coles, Fox & Long

propositional numeric components heuristic. know landmark
fact must occur solution plan, yet appeared path state
evaluated, add constraints representing LP,
propositional goals. is, sum action variables [a0 ..an1 ] achieving
given landmark must greater equal 1. propositional goals, constraint introduces need provide numeric support action(s) chosen support
landmark. Goals special case landmarks, important feature goals
that, even achieved path current state, true
current state must reachieved. Constraints added LP
ensure this. reflect landmarks achieved path current state,
state modified record record updated new landmarks seen.
approach similar lama (Richter & Westphal, 2010).
set disjunctive landmarks set propositional facts, one must
true solution planning problem. extraction disjunctive landmarks
considered (Gregory, Cresswell, Long, & Porteous, 2004) even difficult
case conjunctive landmarks exploit planning systems.
knowledge certain fact must true allow planner infer certain actions
must present solution plans, inform heuristics. However, disjunctive
landmarks less informative. Disjunctive landmarks often arise problem symmetry.
example, might know order deliver package one place another
loaded truck, truck. disjunctive landmark
package truck generated: even know truck use,
know one disjunctive landmarks must hold. Thus, context numeric
resources, truck must fueled (assuming start empty), might entail
additional costs. therefore interest take account disjunctive landmarks
numeric reasoning.
able make use disjunctive landmarks LP constrain
problem, ensuring support given least one fact within (unreached) disjunctive landmark. dealing standard conjunctive landmarks, constraint
sum least one achiever must added propositional landmark.
disjunctive landmarks, however, constraint slightly different. disjunctive landmark
set L satisfied constituent landmarks satisfied. is, apply
actions:
achieves(L) = {a | eff + (a) L 6= }.
encode disjunctive landmarks LP (assuming yet met)
two possibilities. first add binary variable sf fact f L,
constraints sf take value 1 least total one action
adding f , least one variable sf take value 1. is, least
one disjunctive landmarks toP
fully met. alternative, potentially cheaper,
approach add constraint ( achieves(L)) 1. allows disjunctive
landmark considered satisfied sum across action variables supporting
constituent facts least 1. somewhat weaker constraint individual,
non-disjunctive landmarks, guarantee support least 1
individual constituent fact. instance, two-fact disjunctive landmark satisfied
support constituent fact 0.5. considered approaches
370

fiA Hybrid LP-RPG Heuristic Planning

found negligible difference performance (time taken nodes expanded)
two encodings real saving achieved using relaxed approach.
6.3 Managing Propositional Preconditions Effects
far, considered propositions must achieved planning problem due
goals landmarks. However, second class propositions: that, given
values assigned action variables LP, must also supporting actions added
solution relaxed plan.
extend LP capture propositional preconditions effects, first introduce
binary variable f (an integer whose value 0 1) fact f true
state evaluated. involved two constraints. First, actions
+
[a+
0 ...an1 ] add f :
+
a+
0 + ... + an1 f.
case proposition corresponding f goal, f = 1 hence one
achieving actions must positive value (since constraint expressed using
continuous variables, actions might partially applied relaxation). Then,
actions [ap0 ..apm1 ] f precondition:
N.f ap0 + ... + apm1
use N denote (sufficiently) large number. constraint ensures
least one actions depending proposition corresponding f used relaxed
plan, f must positive (that is, corresponding proposition required true
within relaxation). use N ensure f = 1 sufficient satisfy
preconditions many actions. pair constraints effectively conditional version
constraint meet propositional goal, described Section 6.1. cases
proposition neither goal landmark, constraints serve enforce least
one action adds f chosen (has positive value) LP action requiring f
chosen even partially.3
6.4 Recognising Propositional Resources
Finally, consider one case potentially useful model propositions
equivalent numeric form. pddl, finite domain integer resources modelled
two ways: numeric variables, set propositions. Consider following two
formulations fell-timber action Settlers Domain (for simplicity effect
metric tracking variable labour omitted):
(:action fell-timber
:parameters (?p - place)
:precondition (has-cabin ?p)
3. tempting consider replacing constraint, slightly troublesome N , constraints
form f api i. Unfortunately, appropriate action variable, api ,
greater 1 (due multiple applications action) yet f = 1 sufficient satisfy
precondition action applications.

371

fiColes, Coles, Fox & Long

:effect (increase (available timber ?p) 1)
)
(:action fell-timber
:parameters (?p - place ?n0 ?n1 - value)
:precondition (and (has-cabin ?p)
(timber ?p ?n0)
(less-than ?n0 ?n1))
:effect (and (not (timber ?p ?n0))
(timber ?p ?n1))
)
representations models situation, uses different mechanism so. first uses numeric variables, second uses propositions.
using either numeric propositional formulations MetricFF heuristic,
little practical difference guidance given. numeric case, fell-timber
action applied, upper bound amount timber place p increased.
means action consuming amount timber executed subsequent
layers, regardless many actions using resource also applied.
propositional case, delete effect timber (i.e. deleting fact
previously some) also relaxed so, again, number actions requiring amount
timber applied.
Turning attention lp-rpg heuristic, however, observe although
RPG part heuristic exhibits weakness propositional case,
different relaxation used LP numeric reasoning means consumption
timber would disregarded. LP relaxes action ordering, rather delete
effects (or production/consumption effects), resource modelled numerically,
interaction captured accounted for. therefore interests using
lp-rpg convert resources modelled propositionally numeric formulation,
reasoned LP, rather RPG.
Although formulation resources example instance common
idiom used capture numeric resources propositional encoding, situations
natural model resources propositionally outset. often
case binary resources: resources either present not. resources are,
course, special case general resource model described above. Consider
propositional numeric counterparts action switch water pump:
(:action activate
:parameters (?p - pump)
:precondition (off ?p)
:effect (and (not (off ?p))
(on ?p))
)
(:action activate
:parameters (?p - pump)
372

fiA Hybrid LP-RPG Heuristic Planning

:precondition (<= (pumping ?p) 0)
:effect (increase (pumping ?p) 1)
)
Corresponding actions similarly created switch pump (the fact (on
?p) deleted (off ?p) added, equivalently unit (pumping ?p) consumed).
many senses, natural formulation action first, using propositions.
way binary resources encoded benchmark domains. However,
second formulation equivalent (assuming value (pumping ?p) initial state
1 0). interaction propositional resource resources
identified planning problem, little motivation add LP, since
numeric support required. case binary resource impact another
numeric variable is, shall see, efficient model numeric resources.
Suppose water pumps control flow water. Two ways model
pddl shown below:
(:action activate
:parameters (?p - pump)
:precondition (off ?p)
:effect
(and (increase (water-flow) 1)
(not (off ?p))
(on ?p))
)
(:action activate
:parameters (?p - pump)
:precondition (<= (pumping ?p) 0)
:effect (and (increase (pumping ?p) 1)
(increase (water-flow) 1))
)
first two actions switches pump (a binary, propositional resource)
produces unit (water-flow). actions domain preconditions
water flow, action run water wheel precondition ( (water-flow) 3)
interaction propositional numeric variables problem.
use first model action, RPG capture propositional part
action (whether pump off) LP encode numeric part
action. Since RPG relaxes delete effects represent fact (off ?p)
longer true and, hence, prevent pump switched many times.
LP built using first formulation, action activate consume numeric
resources, used arbitrarily often increase water flow fact
switching necessary achieve increase ignored. Thus, mixing propositions
numeric resources action degrades information available LP.
Using second formulation, state pump appears LP variable,
use activate deactivate denote actions activating deactivating
373

fiColes, Coles, Fox & Long

pump, constraints pumping 0 variable are:
pumping 0 = init + activate deactivate
pumping 0 0
pumping 0 1
clear activate action applied once: applied
deactivate must applied, corresponding effect water-flow, order
satisfy last constraints. provides useful guidance, indicates
water flow cannot reach 3 units using actions: actions control pumps,
means increasing flow, need added LP, expansion
planning graph. means attain sufficient flow found, dead-end
discovered would otherwise wasted search effort.
Static-analysis techniques capable identifying propositional resources developed (such tim system described Long & Fox, 2000). used
preprocessing stage recognise propositional resources planning domains translate
equivalent numeric resources. use translation approach recognised
resources, resulting numeric preconditions effects included LP
way numeric variables. so, LP order-relaxation rather
RPG delete-relaxation used compute heuristic values, preventing impossible reuse
resource cases described.

7. Extending Scope Numeric Reasoning LP
Section 4 discussed LP encoding captures producerconsumer behaviour
actions, used first version LPRPG (Coles et al., 2008). section discuss
encoding enhanced, use numeric information representing
structure planning problem, improve guidance resulting heuristic
provide planner. address two key issues here: ensuring conjunctions
numeric goals satisfied, considering issue fractionally applied actions
LP.
7.1 Checking Numeric Goals alongside Propositional Goals
Section 6.1 noted constrain LP finds actions achieve
propositional goals. extend further, capture numeric goals, N ? , adding
numeric goal directly LP constraint. propositional goals,
clear advantages terms resource persistence (Section 3.2.1), insisting goals
simultaneously achievable, rather individually achievable. Additionally, though,
raises expressive power numeric goals able handle anything
expressed Linear Normal Form (LNF) LNF formula added
LP constraint.
7.2 Catalytic Resources
far considered numeric variables conform producerconsumer behaviour. However, another related class variables also expressed
374

fiA Hybrid LP-RPG Heuristic Planning

LP similar way producerconsumer variables. variables represent resources must present order action applied,
consumed. resources used support many actions4 . example
resource catalyst chemical reaction. catalyst must either created
reactions, bought, present, enables reactions, allows take
place quickly. catalytic reactions, resource must present,
consumed, though could non-catalytic reactions may consume resource.
Another example one might consider building unit plant, support
process. unit must order process occur, built
used many times enable actions without necessitating destruction. Often
planning problems, presence structures represented propositions,
need case. many indistinguishable processing units present, several
units catalyst needed, often makes sense represent numerically5 .
extend lp-rpg heuristic provide guidance problems,
actions require v c affect value v, encounter difficulty
LP encodes notion time: ordering actions relaxed, impossible ascertain
value v specific time order determine whether (catalyst) precondition
satisfied not. therefore add additional constraints LP determine upper
lower bounds v obtained optimistic pessimistic possible ordering
actions whose variables non-zero. find optimistic upper lower bound
v, v v respectively, add constraints:
X
v = v +
a. max((v, a), 0)
(4)
aA
v = v +

X

a. min((v, a), 0)

(5)

aA
upper bound equivalent ordering production actions consumption
actions. lower bound, reversed, equivalent consumption actions
ordered production actions. bounds computed
possible values resource variables reachability graph,
considering actions actually selected execution relaxed plan,
rather could possibly applied. seen, neither requires
explicit notion time.
Using bounds, action precondition v c applied, even
fractionally, must case v c, otherwise precondition action
could never met ordering actions chosen. course, v c, cannot
guarantee legal ordering producers consumers achieves
value. need least feasible opportunity satisfy precondition
added LP introduction binary ([0, 1] integer) variable pair
constraints. actions [a0 ..an1 ] requiring precondition v c, using N
4. note complementary class variables whose values must remain certain level
order actions applied, produced. seem less useful real problems,
nonetheless handled analogous way.
5. Section 6 show could captured LP even expressed propositionally.

375

fiColes, Coles, Fox & Long

denote large number, denote new binary variable, add pair constraints:
N.s a0 + ... + an1
v lb(v) + (c lb(v)).s

(6)

first constraint forces take value 1 actions requiring
precondition v c applied. second constraint determines lower bound
v based value s: = 0, lower bound v lb(v), global lower
bound v. Otherwise, = 1, since action needing precondition applied,
thus v c. constraint = 1 implies least one
actions [a0 ...an1 ] applied. However, since non-zero value makes
LP harder solve, pressure set = 1 reason
precondition must satisfied. important note constraints
added LP problems v c preconditions matched v = c effect.
modified heuristic able support planning models previously could not.
behaviour domains without characteristics entirely unaffected.

8. Results
section present thorough evaluation lp-rpg heuristic: comparing
state-of-the-art numeric planners, considering use different LP solvers performing
ablation studies determine effective many potential different configurations lp-rpg discussed paper. include weighting action variables
LP according RPG layer appear, inclusion propositions
numeric goals LP consideration variables LP remain
integer relaxed real numbers. tests run 3.4GHz
Pentium IV machines limited 30 minutes 1.5GB memory. planner
planner-configuration fails report solution within limits deemed
failed solve problem.
8.1 Evaluation Domains
First discuss selection evaluation domains. purpose selecting domains
select construct examples informative evaluating behaviour
heuristic. Domains numeric variables conform producer-consumer
behaviour identified syntactically different planning strategy
employed. Since syntactic analysis trivial, overhead making decision
negligible, assume performance domains approach
applicable consistent whichever alternative strategy selected deployment.
consider existing competition benchmarks producerconsumer behaviour,
introduce new domains exhibit behaviour. current benchmarks exhibit interesting producerconsumer behaviour, order make comparison informative possible make use do:
MPrime domain IPC 1;
Rovers domain (Numeric variant) IPC 3;
376

fiA Hybrid LP-RPG Heuristic Planning

Settlers domain IPC 3;
alternative encoding Settlers domain (described below);
Pathways domain IPC 5. (We developed metric domain derived
Metric Time variant, replacing durative actions comparable nontemporal actions.)
addition standard IPC problem set Settlers (problems 120), introduce
new problems make full use scope domain. domain allows
building ships transporting materials disjoint islands; however,
benchmark set none problems require this. building ships requires large amount
infrastructure, therefore add problems challenge planners,
materials must imported overseas order achieve goals. first problems
(21) requires merely building ship, 22 requires import timber overseas,
23 requires building housing overseas. 24 adds goals 23, requiring
planner achieve goals mainland build housing island. final
problem, 25, considers 3 disjoint islands resources must combined achieve
goal island. problems requires building much greater infrastructure
required original IPC 3 settlers problems. consider two variants
settlers domain: standard IPC 3 domain, encoding based representation
carts proposed Gregory Rendl (2008). Here, number carts given
location represented numeric variable (carts-at ?location). two
possible move actions carts: one moves cart unit resource one
location another; one moves cart without moving resources, i.e.
cart moves whilst empty. encoding possible carts transport
single unit material, necessary maintain specific named identities
capacities cart.
addition benchmark domains, use two domains created development lp-rpg:
Market Trader Domain (Coles et al., 2008);
Hydro Power Domain.
Market Trader Domain, trader begins small amount money,
goal increased certain level. must achieved travelling
markets, sells collection goods certain price
good type, buys another (lower) price. Money made buying items
cheaper transporting (via camel) locations command
higher price; moving associated cost food required camel.
representation general class real-world trading problems aim
make money buying, transporting selling goods. Hydro Power domain
also concerned financial gains, domain different structure: rather
transportation, concerned energy storage using hydroelectric reservoirs.
buying electricity pump water uphill periods low demand, electricity
cheaper, storing potential energy, electricity sold higher price
377

fiColes, Coles, Fox & Long

Domain
Market Trader
Hydro Power
Pathways Metric
Settlers
Settlers Numeric Carts
MPrime
Rovers Numeric
Sugar
Total

lp-rpg
20
27
30
18
23
30
15
18
181

lp-rpg-FF
0
3
21
7
13
30
13
7
94

MetricFF
0
1
13
8
8
28
10
14
82

lpgtd
0
5
0
19
7
30
20
20
101

Table 2: Coverage achieved different planners

times greater demand. domain encoding augmented take account energy
loss: purchasing one unit energy sufficient provide one unit energy later
losses storage process. problem, despite temporal axis,
interested profit made, force planner advance time
specific point: ask sufficient profit made planner
choose advance time necessary. general problem models continuous processes:
customer demand changes continuously. Here, simplify problem discretising
30 minute time intervals, using demand schedule transformer domain (Bell,
Coles, Coles, Fox, & Long, 2009), based UK National Grid figures. Like original
transformer model represent temporal features problem advance time
action, rather temporal pddl, since lp-rpg temporal planner.
final domain consider Sugar domain (Radzi, 2011). Here, objective
produce sugar industrial processes, refining raw cane. domain
taken set domains designed optimisation planning: several
paths goal, originally included allow planner choice trajectories
challenge find better quality solutions. set domains designed
challenging metric optimisation problems, domains set trivial
standard MetricFF: plan metric ignored almost problems solved
less 1 second6 . Therefore, domains, consider Sugar domain,
remains challenging MetricFF even optimisation required: number
paths appear lead goal large without good guidance difficult
find solution problem.

6. say domains uninteresting present interesting challenge
explored Radzi (2011), using carefully modified variant lp-rpg. However, challenge
find good quality plans, quality determined complex metric simple plan length;
neither MetricFF lp-rpg form discussed difficulty finding poor quality plans
problems.

378

fiA Hybrid LP-RPG Heuristic Planning

markettrader
100

hydropower
10000

LPRPG

1000

LPRPG
LPRPG-FF
FF
LPG

100
10

Time (s)

Time (s)

10

1

1
0.1
0.01
0.1

0.001
2

4

6

8

10
12
Problem Number

14

16

18

20

5

10

15
20
Problem Number

10000

LPRPG
LPRPG-FF
FF
Metric-LPG

100

30

sugar

pathwaysmetric
1000

25

1000

LPRPG
LPRPG-FF
FF
LPG

100
10

Time (s)

Time (s)

10

1

1

0.1
0.1
0.01

0.01

0.001

0.001
5

10

15
20
Problem Number

25

30

2

4

6

8

10
12
Problem Number

settlers
10000

1000

14

16

18

20

settlersnumeric
1000

LPRPG
LPRPG-FF
FF
LPG

LPRPG
LPRPG-FF
FF
LPG

100

Time (s)

Time (s)

100

10

10

1
1
0.1

0.1

0.01

0.01
5

10
15
Problem Number

20

25

5

10
15
Problem Number

mprime
1000

100

25

roversnumeric
1000

LPRPG
LPRPG-FF
FF
LPG

100

LPRPG
LPRPG-FF
FF
LPG

10
Time (s)

10
Time (s)

20

1

1

0.1

0.1

0.01

0.01

0.001

0.001
5

10

15
20
Problem Number

25

30

2

4

6

8

10
12
Problem Number

14

16

18

Figure 3: Comparison MetricFF lpgtd: time taken solve problems

379

20

fiColes, Coles, Fox & Long

Market Trader
200

Hydro Power
300

LPRPG
Best known

LPRPG
LPRPG-FF
FF
LPG
Best known

180
250
160
200
Solution Quality

Solution Quality

140
120
100

150

100
80
50
60
40

0
2

4

6

8

10
12
Problem Number

14

16

18

20

5

10

15
Problem Number

Pathways Metric
9000

7000

30

LPRPG
LPRPG-FF
FF
LPG
Best known

250

200
Solution Quality

6000
Solution Quality

25

Sugar
300

LPRPG
LPRPG-FF
FF
Metric-LPG
Best known

8000

20

5000
4000

150

3000

100

2000
50
1000
0

0
5

10

15
Problem Number

20

25

30

2

4

6

8

10
12
Problem Number

Settlers
900

700

1000

18

20

LPRPG
LPRPG-FF
FF
LPG
Best known

800
Solution Quality

600
Solution Quality

16

Settlers Num. Carts
1200

LPRPG
LPRPG-FF
FF
LPG
Best known

800

14

500
400
300

600

400

200
200
100
0

0
5

10

15
Problem Number

20

25

5

10

M-Prime
300

250

20

25

Rovers Numeric
300

LPRPG
LPRPG-FF
FF
LPG
Best known

250

LPRPG
LPRPG-FF
FF
LPG
Best known

200
Solution Quality

200
Solution Quality

15
Problem Number

150

150

100

100

50

50

0

0
5

10

15
Problem Number

20

25

30

2

4

6

8

10
12
Problem Number

14

Figure 4: Comparison MetricFF lpgtd: plan length

380

16

18

20

fiA Hybrid LP-RPG Heuristic Planning

8.2 Comparison Planners
first compare performance lp-rpg existing numeric planners. use
found strong (though uniformly best) configuration planner,
demonstrate subsequent sections:
Landmarks Propositional Goals added LP (as Section 6.1);
weight action variable objective function used solution extraction 3l , l layer appeared RPG expansion;
Action variables corresponding actions action layer 1 integral;
IBM ILOG CPLEX version 12.1.0 used LP solver.
compare two historically successful numeric planners: MetricFF (Hoffmann, 2003) lpgtd (Gerevini et al., 2006). remain state art, many
modern planners (e.g. lama) handle numeric preconditions, action costs.
clarify differences performance, also compare lp-rpg-FF: reimplementation MetricFF based lp-rpg code difference that, computing
upper- lower-bounds numeric variables RPG expansion, lp-rpg-FF allows
actions applied many times action layer, rather per action
layer MetricFF. Since publication earlier comparison lpgtd (Coles et al.,
2008), new improved version lpgtd produced. version lpgtd
performs much better earlier version, use results here. also
consider variant lpgtd, Metric-lpgtd (Gerevini, Saetti, & Serina, 2008), designed
responsive plan metrics based numeric variables. experiments showed
Metric-lpgtd perform significantly differently lpgtd generating first
feasible plans problems plan length metric, apart Pathways domain lpgtd crashes problems. Therefore, report performance lpgtd
domains except Pathways report figures Metric-lpgtd.
interesting pattern emerges relative performance planners across set
evaluation domains, shown Figure 3. domains organised top
strongly numeric, relying propositions, towards bottom
propositional structure consequently less numeric structure. two
heavily propositional domains (MPrime Rovers) lpgtd generally
successful planner, solving problems evaluation sets, often
fastest planner problems. pattern holds standard competition
problems (1-20) competition formulation Settlers domain. MetricFF also
performs quite well MPrime Rovers domains, struggles Settlers
domain due numeric structure present.
strongly numeric domains, however, lpgtd performs poorly: indeed fails
solve single problem Pathways Market Trader domains. due
crashing, rather planner searches exhausts resource limits without finding
solution. Hydro Power, lpgtd solves five easiest problems, able
scale beyond this. experiments observed lpgtd struggles domains
limited propositional structure generally, search guidance gets
numeric problems poor. Comparing two Settlers variants also gives
381

fiColes, Coles, Fox & Long

interesting insights: carts turned numeric resource, lpgtd struggles
much solving 7, instead 19 problems, whereas performance lp-rpg fact
improved. Note although lpgtd successful IPC 3 problems, cannot solve
richer problems ship building overseas transport required; whereas lp-rpg
capable solving instances.
Turning attention comparison MetricFF observe problems
solved planners generally solved quickly MetricFF, particularly
domains propositional structure. lp-rpg additional
overhead solving LP state (and partly, course, due highly efficient
MetricFF code-base). Occasionally general pattern broken, lp-rpg faster;
slight variations ordering heuristically equivalent states lead
significant differences performance. results lp-rpg-FF show similar
coverage MetricFF, although sometimes solving different problems (again
branch orderings different code base cause differences, marked
Pathways Sugar domains), serve demonstrate basic FF
implementation performing drastically differently standard MetricFF causing
gains observe.
Looking numeric domains particular, lp-rpg heuristic able provide much better guidance, allowing lp-rpg solve many problems MetricFF.
Notably, Market Trader domain neither MetricFF lp-rpg-FF solve
problems. due poor guidance standard RPG heuristic gives domain, relaxing transfer numeric resources. relaxed plan buy item
repeatedly sell item sufficient profit made. Again, Hydro Power,
similar situation occurs: one unit energy pumped up, unit
energy repeatedly sold future time day, making sufficient profit without
guidance. Pathways, chemical reactions must take place, relaxation used
MetricFF allow units substance used repeatedly, several different
reactions, despite fact used consumed. lp-rpg heuristic permit therefore gives much better search guidance, allowing lp-rpg
solve problems domain. numeric resource transfer present Settlers
domain leads poor guidance MetricFF heuristic MetricFF able solve
problems result. use LP effective domain allowing
problems solved. different formulations seem make little difference
coverage MetricFF, neither making problems easier MetricFF solve.
quality solutions (plan length) produced different planners displayed
Figure 4. emphasize lp-rpg current form making attempt
minimise general measure plan quality, results merely intended give
indication whether large degradation, indeed fortuitous increase,
quality moving using standard RPG heuristic hybrid lp-rpg approach.
potential improve plan quality using lp-rpg-style approach explored
work domains preferences (Coles & Coles, 2011) also range
different metrics (Radzi, 2011). None problems use specified metric functions
minimise, instead use number actions solution plan. value
RPG heuristic tend minimise. None planners run optimisation
mode (where available) simply report first plan found search.
382

fiA Hybrid LP-RPG Heuristic Planning

domains quality solutions produced lp-rpg comparable
produced MetricFF lpgtd. sugar domain lp-rpg heuristic compares
favourably lpgtd, although could perhaps hope running lpgtd quality mode
would enable produce better solutions. Pathways, lp-rpg produces particularly
long solutions, trade off, also able scale solve far problems.
return issue solution length domain considering weighting
action variables LP solution extraction.
summary, lpgtd seems generally successful domains sufficient propositional structure MetricFF generally efficient problems
capable solving. structure domain becomes heavily numeric
planners perform poorly. lp-rpg, however, able solve many problems
planners, making use search guidance LP captures numeric
interactions well.
8.3 LP Solvers
lp-rpg construction use LP performed using functions commonly
available wide range LP solvers: adding variables constraints model, setting
variable bounds, marking whether variables real integer valued, changing objective
function, on. current implementation employs minimal abstraction layer
lp-rpg LP solver itself, almost LP solver used.
section, consider use three LP solvers:
IBM ILOG CPLEX version 12.1.0, commercial mixed integer-linear programming
solver.
COIN-OR LP (CLP) version 1.12.0, open-source LP solver. models feature
integer variables, CLP used within COIN-OR Branch-and-Cut (CBC) version 2.5.0,
is, again, open-source.
LPSolve version 5.5.0.13, open-source mixed integer-linear programming solver.
experiments LP solvers, found CPLEX substantially
robust two, particularly LP extended include satisfying
propositional goals landmarks. Thus, purposes comparison here,
use configuration lp-rpg (equivalent used earlier paper (Coles et al.,
2008)) efficient presented elsewhere paper, robust
(caused CLP LPSolve crash less often) testing:
Propositional goals landmarks added LP: encodes numeric goals
only.
integer variables (potentially) helpful actions, assignment
effects.
layer-weighting scheme k = 1.1 used.
refer configuration limited-lp-rpg.
383

fiColes, Coles, Fox & Long

Market Trader
1000

Hydro Power
10

CPLEX
LPSolve
CLP

CPLEX
LPSolve
CLP

100
1
Time (s)

Time (s)

10

1
0.1
0.1

0.01

0.01
2

4

6

8

10
12
Problem Number

14

16

18

20

5

10

15
20
Problem Number

Pathways Metric
1000

100

25

30

Sugar
10000

CPLEX
LPSolve
CLP

CPLEX
LPSolve
CLP

1000

100

Time (s)

Time (s)

10

1

10
0.1
1

0.01

0.001

0.1
5

10

15

20

25

30

2

4

6

8

Problem Number

Settlers
10000

10

12

14

16

18

20

Problem Number

Settlers Numeric Carts
10000

CPLEX
LPSolve
CLP

1000

1000

CPLEX
LPSolve
CLP

100
Time (s)

Time (s)

100

10

10
1
1

0.1

0.1

0.01
5

10
15
Problem Number

20

25

5

10
15
Problem Number

M-Prime

25

Rovers Numeric
1000

CPLEX
LPSolve
CLP

100

100

10

10

Time (s)

Time (s)

1000

20

1

1

0.1

0.1

0.01

CPLEX
LPSolve
CLP

0.01
5

10

15
20
Problem Number

25

30

2

4

6

8

10
12
Problem Number

14

16

18

20

Figure 5: Time taken limited-lp-rpg solve problems using different LP solvers

384

fiA Hybrid LP-RPG Heuristic Planning

Market Trader
300

Hydro Power
300

CPLEX
CLP
LPSolve

250

200
Plan Length

200
Plan Length

CPLEX
CLP
LPSolve

250

150

150

100

100

50

50

0

0
2

4

6

8

10
12
Problem Number

14

16

18

20

5

10

15
Problem Number

Pathways Metric
3000

2500

25

30

Sugar
55

CPLEX
CLP
LPSolve

CPLEX
CLP
LPSolve

50
45
40

2000

35

Plan Length

Plan Length

20

1500

30
25

1000

20
15

500

10
0

5
5

10

15
20
Problem Number

25

30

2

4

6

8

Settlers
500
450

10
12
Problem Number

14

16

18

20

Settlers Num. Carts
800

CPLEX
CLP
LPSolve

700

CPLEX
CLP
LPSolve

400
600
350
Plan Length

Plan Length

300
250
200

500
400
300

150
200
100
100

50
0

0
5

10
15
Problem Number

20

25

5

10
15
Problem Number

M-Prime
60

50

25

Rovers Numeric
120

CPLEX
CLP
LPSolve

100

CPLEX
CLP
LPSolve

80
Plan Length

40
Plan Length

20

30

60

20

40

10

20

0

0
5

10

15
Problem Number

20

25

30

2

4

6

8

10
12
Problem Number

14

16

18

20

Figure 6: Lengths plans produced limited-lp-rpg using different LP solvers

385

fiColes, Coles, Fox & Long

Domain
Market Trader
Hydro Power
Pathways Metric
Sugar
Settlers
Settlers Numeric Carts
MPrime
Rovers Numeric
Total

CPLEX
20
29
28
9
12
21
28
13
160

CLP
16
30
1
10
11
10
30
12
120

LPSolve
15
30
2
9
15
8
30
12
121

Table 3: Coverage limited-lp-rpg different LP solvers

results tests shown Figure 5, Table 3. Beginning Market
Trader domain, quite clear CPLEX faster CLP domain. LPSolve,
turn, substantially out-performs CPLEX many problems, two orders magnitude.
speed, though, comes cost terms robustness: CPLEX solves 20 problems,
LPSolve solves 15. LPSolve also demonstrates strong performance Hydro
Power domain, CLP falls LPSolve CPLEX.
Pathways Metric domain illustrates robustness CPLEX extension beyond
basic producerconsumer model encoding. domain contains actions numeric
preconditions must true, affected action. described Section 7.2, encoding requires integer variable precondition. domain
also contains goals referring multiple numeric variables, e.g:
(>= (+ (available cycDp1) (available c-Myc-Max)) 3)))
seen, domain, CPLEX LP solver allows lp-rpg
solve anything smallest problems. Beyond problem 2, CLP unable solve
LP reach goals initial state. Using LPSolve, solvable LPs reported
unsolvable, whilst planner make attempt search, erroneous state pruning
happening result LPs falsely declared unsolvable renders unable find
solutions.
Sugar domain, solver leads planner performing particularly well,
10 problems solved. contrasts earlier results, shown Table 2,
different configuration lp-rpg, using CPLEX richer LPs, able solve
18 problems. However, noted start section, compromise
performance planner using CPLEX allow reasonable comparison CLP
LPSolve: using richer LP models here, CPLEX solves 18 problems LPSolve
CLP perform far worse (falsely claiming LPs unsolvable, returning suboptimal
solutions, detriment performance planner).
two different encodings Settlers domain, carts represented either explicitly using carts-at function, see CPLEX robust
alternative domain encodings. original IPC domain model, LPSolve performs
particularly well, CLP markedly different CPLEX. Using numeric-carts
386

fiA Hybrid LP-RPG Heuristic Planning

Domain

LPSolve
CLP
CPLEX
Build (ms)Solve (ms)Build (ms)Solve (ms)Build (ms)Solve (ms)
Market Trader
5.0
167.1
1.3
137.9
17.3
44.6
Hydro Power
0.7
1.4
0.3
5.9
9.4
6.6
Pathways Metric
0.0
1.3
4.3
8.8
4.4
3.1
Sugar
0.8
7.5
1.6
42.0
17.6
22.9
Settlers
2.6
31.9
2.2
170.6
165.5
87.6
Settlers Num. Carts
2.2
22.9
1.6
71.0
48.0
33.6
MPrime
3.1
5.4
5.3
11.5
76.2
6.3
Rovers Numeric
0.8
4.6
0.7
7.2
14.8
1.9
Average
1.9
30.3
2.2
56.8
44.1
25.8
Table 4: Time spent LP building solving using different LP solvers

encoding, however, CPLEX considerably better solvers, robust
change LP structure arising alternative domain encoding.
MPrime Rovers domain, CLP LPSolve consistently faster
CPLEX.
Summarising, see limited-lp-rpg solve problem using CLP
LPSolve, usually faster using CPLEX, CPLEX offers better coverage
greater robustness grants access richer encodings allow better performance.
order investigate detail planner takes much longer solve problems using CPLEX devised tests. Table 4 shows average time spent,
per state, building solving LP LP solver. (The building time time
required integrate constraints inserted lp-rpg internal model used
particular solver.) Note results necessarily directly comparable: planners necessarily take paths state space, might evaluating
different states. paths are, nonetheless, often similar times taken
strongly indicative. give fairest possible comparison, include table
data problems solved three configurations, data presented
planner across exactly problem set. startling observation CPLEX
typically spends order magnitude longer building LPs two solvers (in
Settlers two orders magnitude). So, often solves LPs quickly, total time spent handling LPs generally greater. Indeed, CPLEX, time spent
solving LPs is, 5 8 domains, dominated time spent building them.
indicates that, although CPLEX good choice use LP solver (it solves
LPs efficiently), practice solvers faster due substantial overheads
building large number LPs necessary (at least one per state). results suggest
robust LP solver low LP building overheads could dramatically improve
performance lp-rpg.
considering solution quality note way LP Solvers
direct search different trajectory solvers return different optimal solutions
LP point search. Recall planner configuration seeks
directly minimise plan length: objective function LPs uses weighted sum
387

fiColes, Coles, Fox & Long

Domain
Market Trader
Hydro Power
Pathways Metric
Sugar
Settlers
Settlers Numeric Carts
MPrime
Rovers Numeric
Total

1l
18
30
30
11
13
22
29
10
163

1.1l
20
29
28
9
19
22
30
13
170

3l
20
27
30
18
18
23
30
15
181

5l
20
27
30
18
17
21
30
15
178

10l
20
26
30
20
16
21
30
15
178

hmax
20
28
30
16
20
23
30
14
181

hadd
20
28
30
16
22
23
29
15
183

Table 5: Coverage varying LP objective function weighting schemes

number actions. Figure 6 shows similar picture arises plan length
time performance: LPSolve often leads planner shorter solutions CPLEX (also
helping explain often faster, since explores search tree smaller depth).
particular happens Rovers Numeric Sugar domains well Settlers
variants. Across domains little variation quality solutions produced.
8.4 LP Objective Function Weighting Schemes
using LP solution extraction phase, one open issue weighting
scheme use objective function. Since LP ignores propositional preconditions
actions, using simple objective minimising sum action variables (layerweighting scheme k = 1) gives LP solver freedom select equally actions
appearing layers RPG, regardless many actions subsequently need
added relaxed plan support them. discussed earlier, using layer-weighting
scheme k > 1, using weighting scheme based estimated costs achieving
preconditions actions, encourage LP solutions favour actions
cheaper apply. hmax hadd heuristics candidates base
estimates cost application actions. course, cases
choice earlier action, one lower costs achieve preconditions,
flawed choice, worse choice would made using k = 1
layer-weighting scheme: simply nature heuristics.
section, evaluate range LP action-variable layer-weighting schemes
also action-cost estimate schemes. use k 1, 1.1, 3, 5, 10 layer-weighting schemes.
consider setting action variable weights 1 plus cost meeting propositional
preconditions hmax hadd action-cost estimate schemes. parameters
planners set sensible defaults: action variables first action layer
integral, propositional goals landmarks included goal-checking LP.
Results showing coverage configurations shown Table 5. first
observation weighting scheme better using layer-weighting
k = 1 action variables. particularly noticeable Settlers domain,
several situations earlier actions preferred. example,
388

fiA Hybrid LP-RPG Heuristic Planning

Weight 1 vs Weight 1.1: Time
Sugar
Rovers Num.
M-Prime
Settlers Num. Carts
Settlers
Pathways Metric
Hydro Power
Market Trader

1000

Weight 1.1 (Nodes)

100

Weight 1.1 (Time (s))

Weight 1 vs Weight 1.1: Nodes Expanded

Sugar
Rovers Num.
M-Prime
Settlers Num. Carts
Settlers
Pathways Metric
Hydro Power
Market Trader

1000

10

1

100

10
0.1

0.01
0.01

1
0.1

1

10
Weight 1 (Time (s))

100

1000

1

10

Weight 1 vs Weight 3
Sugar
Rovers Num.
M-Prime
Settlers Num. Carts
Settlers
Pathways Metric
Hydro Power
Market Trader

1000

Weight3 (Nodes)

Weight 3 (Time (s))

100

1000

Weight 1 vs Weight 3: Nodes Expanded

Sugar
Rovers Num.
M-Prime
Settlers Num. Carts
Settlers
Pathways Metric
Hydro Power
Market Trader

1000

100
Weight 1 (Nodes)

10

1

100

10
0.1

0.01
0.01

1
0.1

1

10
Weight 1 (Time (s))

100

1000

1

10

100
Weight 1 (Nodes)

Weight 1 vs Weight 5: Time
Sugar
Rovers Num.
M-Prime
Settlers Num. Carts
Settlers
Pathways Metric
Hydro Power
Market Trader

1000

Weight5 (Nodes)

100

Weight 5 (Time (s))

Weight 1 vs Weight 5: Nodes Expanded

Sugar
Rovers Num.
M-Prime
Settlers Num. Carts
Settlers
Pathways Metric
Hydro Power
Market Trader

1000

1000

10

1

100

10
0.1

0.01
0.01

1
0.1

1

10
Weight 1 (Time (s))

100

1000

1

10

Weight 1 vs Weight 10: Time

Weight 10 (Time (s))

100

1000

Weight 1 vs Weight 10: Nodes Expanded

Sugar
Rovers Num.
M-Prime
Settlers Num. Carts
Settlers
Pathways Metric
Hydro Power
Market Trader

Sugar
Rovers Num.
M-Prime
Settlers Num. Carts
Settlers
Pathways Metric
Hydro Power
Market Trader

1000

Weight10 (Nodes)

1000

100
Weight 1 (Nodes)

10

1

100

10
0.1

0.01
0.01

1
0.1

1

10
Weight 1 (Time (s))

100

1000

1

10

100
Weight 1 (Nodes)

1000

Figure 7: Layer-weighting schemes LP (k = 1 versus k = x different values x)

389

fiColes, Coles, Fox & Long

Weight 1 vs H Max: Nodes Expanded

Weight 1 vs H Max: Time

H Max (Time (s))

100

Sugar
Rovers Numeric
M-Prime
Settlers Num. Carts
Settlers
Pathways Metric
Hydro Power
Market Trader

Sugar
Rovers Numeric
M-Prime
Settlers Num. Carts
Settlers
Pathways Metric
Hydro Power
Market Trader

1000

H Max (Nodes)

1000

10

1

100

10
0.1

0.01
0.01

1
0.1

1
10
Weight 1 (Time (s))

100

1

1000

10

100
Weight1 (Nodes)

Weight 1 vs H Add: Nodes Expanded

Weight 1 vs H Add: Time

H Add (Time (s))

100

Sugar
Rovers Numeric
M-Prime
Settlers Num. Carts
Settlers
Pathways Metric
Hydro Power
Market Trader

Sugar
Rovers Numeric
M-Prime
Settlers Num. Carts
Settlers
Pathways Metric
Hydro Power
Market Trader

1000

H Add (Nodes)

1000

1000

10

1

100

10
0.1

0.01
0.01

1
0.1

1
10
Weight 1 (Time (s))

100

1

1000

10

100
Weight 1 (Nodes)

1000

Figure 8: Action weighting schemes LP objective function (layer-weighting k = 1
versus action-cost estimate based schemes using hadd hmax )
supposing cart location A, unit resource, initially available
B, needs moved location C. two two-action solutions LP.
Annotating action layer appears, are:
load cart (0), unload cart C (1);
load cart B (1), unload cart C (1).
Using layer-weighting scheme k = 1 two solutions indistinguishable
(each cost 2). However, latter selected, two actions needed support
relaxed plan: moving cart B moving cart C. alternative action
variable weighting schemes set weight loading cart B higher
loading A, leading preference first solution, better outcome
search guidance. similar phenomenon occurs Rovers domain, favouring
earlier actions increases preference recharging rovers close current
locations. domain leads better avoidance dead-ends, since postponing
recharging actions risks possibility rover little charge left get
back recharging location.
data layer-weighting schemes show results peak k = 3, reasonable trade-off minimising number actions chosen LP solution
favouring earlier actions. overall performance hmax same, losing performance
390

fiA Hybrid LP-RPG Heuristic Planning

Time Taken
Nodes Expanded
Plan Length
k=
1.1 3 5 10 hmax hadd 1.1 3 5 10 hmax hadd 1.1 3 5 10 hmax hadd
k=1 X X X X X
X X X X X X
X 7 X X X X
X
k = 1.1
X X X 7
7
X X X 7
X
X X X X
X
k=3
7 7 X
X
7 7 X
X
X X X
X
k=5
X X
X
7 X
7
7 X
X
k = 10
X
X
X
7
X
X
hmax
7
X
7

Table 6: Results Two-Tailed Wilcoxon Signed Rank Tests comparing different LP weighting schemes. X indicates significance (0 = 0.05), colour indicates better performer
(faster, fewer nodes expanded shorter plans).

Sugar Rovers gaining Settlers Hydro Power. Using hadd gives
gains Settlers, leading slightly better coverage layer-weighting scheme
k = 3. difference performance k = 3 hadd weighting schemes
domain-dependent. hadd increases weight action 1 propositional preconditions true state evaluated. Settlers domain,
gives particularly good performance, sound approach, neatly capturing
example case discussed earlier section: loading unloading cart
current location preferable later locations. domains earlier actions
preferable even propositional preconditions require little support, hadd fails
give adequate bias, layer-weighting scheme higher k performs better.
instance, Sugar domain, best coverage obtained using k = 10,
k = 3 scheme also performs strongly hadd here.
Examining performance configurations detail, comparing time
taken find solution plans number nodes evaluated, scatterplots comparing
configurations tested layer-weighting k = 1 shown Figures 7
8, layer-weighting schemes action-cost estimate based schemes. lprpg usually exhausts time limit 30 minutes memory limit, time-taken
scatterplots closely similar coverage table. Coverage directly reflected
number points far right graphs, indicating layer-weighting scheme
k = 1 (the x-axis) unable find solution within 30 minutes, lp-rpg using
different weighting scheme able solve problem. points
k = 3 hadd schemes, points hadd appearing predominantly
Settlers domain k = 3 spread across domains.
Since always clear scatterplots whether one configuration better
whether differences significant, used Two-Tailed Wilcoxon
Signed Rank Test compare pair tested configurations terms time taken
nodes evaluated, also plan length.7 tests performed p = 0.05. results
tests summarised Table 6. number interesting observations made:
7. Wilcoxon signed-rank test non-parametric statistical test used compare set matched
samples (such pairs results two different planners sequence problems)
assess whether population mean ranks differ (i.e. paired difference test). useful
absolute values necessarily comparable samples drawn completely

391

fiColes, Coles, Fox & Long
1l vs 1.1l: Plan Length
10000

Sugar
Rovers Numeric
M-Prime
Settlers Num. Carts
Settlers
Pathways Metric
Hydro Power
Market Trader

Sugar
Rovers Numeric
M-Prime
Settlers Num. Carts
Settlers
Pathways Metric
Hydro Power
Market Trader

1000
3l (Plan Length)

1000
1.1l (Plan Length)

1l vs 3l: Plan Length
10000

100

100

10

10

1

1
1

10

100

1000

10000

1

10

100

1l (Plan Length)

1l vs 5l: Plan Length
10000

10l (Plan Length)

5l (Plan Length)

Sugar
Rovers Numeric
M-Prime
Settlers Num. Carts
Settlers
Pathways Metric
Hydro Power
Market Trader

1000

100

10

100

10

1

1
1

10

100

1000

10000

1

10

1l (Plan Length)

1000

10000

1000

10000

1l vs hadd: Plan Length
10000

Sugar
Rovers Numeric
M-Prime
Settlers Num. Carts
Settlers
Pathways Metric
Hydro Power
Market Trader

Sugar
Rovers Numeric
M-Prime
Settlers Num. Carts
Settlers
Pathways Metric
Hydro Power
Market Trader

1000

hadd

1000

100
1l (Plan Length)

1.1l vs hmax
10000

hmax (Plan Length)

10000

1l vs 10l: Plan Length
10000

Sugar
Rovers Numeric
M-Prime
Settlers Num. Carts
Settlers
Pathways Metric
Hydro Power
Market Trader

1000

1000

1l (Plan Length)

100

10

100

10

1

1
1

10

100

1000

10000

l

1

10

100
l

1.1 (Plan Length)

1 (Plan Length)

Figure 9: Effect quality varying action weighting schemes LP objective function

k = 3 gives better coverage k = 5 k = 10, take significantly
different amount time explore significantly different number nodes.
is, however, significant difference time taken nodes, favour k = 3,
compared configurations tested.
action-cost based weighting schemes slower layer-weighting schemes
k 3. Furthermore, compared k = 3, expand nodes.
unknown distribution. two-tailed version makes assumption two contenders
better performance (i.e. lower mean rank).

392

fiA Hybrid LP-RPG Heuristic Planning

key strength action-cost based weighting schemes lies quality
solutions found. general trend, layer-weighting schemes gain coverage lose
quality k increases. action-cost based schemes generate significantly shorter
plans configuration tested, including layer-weighting k = 1.
increase plan length plans found using layer-weighting scheme increasing k seen Figure 9. reason increase quite clear: basic
objective function LP minimise number actions. However, weightings
added actions according RPG layer appear, longer plans
make use earlier actions become attractive. increase plan length is,
however, largely restricted two domains. first, worst affected, Pathways Metric, increase k causes preference reactions less efficient (in terms
number actions needed) performed using actions earliest layers
planning graph. second Settlers Numeric Carts, increase k
leads solutions preference resource production refinement approaches
less efficient, comprise actions appear earlier planning graphs.
perhaps expected, two domains excluded statistical analysis,
significant difference plan length k = 3 action-cost based
schemes.
weighting schemes tested best results obtained using k = 3 hadd :
single best option. former faster, expands fewer nodes, gives good
balance coverage across domains used. latter less prone variations plan
quality domains, strong performance Settlers domain leads two
problems solved overall within test domains.
8.5 Use Integer Constraints
Section 5.4.1 discussed fact LP relaxation MIP, proposed
certain situations may beneficial relax action variables, making
integral. section, explore hypothesis, considering five configurations
lp-rpg:
1. Minimal Integers: actions assignment effects (as Section 4.3) integers.
2. First-layer: above, variables actions appearing RPG layer 1 (the potentially helpful actions) also integral.
3. Propositional-Goal Achievers: above, variables actions achieve
propositional goals landmarks also integral.
4. Numeric-Goal Achievers: above, additionally, variables actions affecting
numeric state variables appear numeric goals also integral.
5. All: (action) variables integral.
coverage configurations shown Tables 7 8, layerweighting using k = 1.1 k = 3, respectively. k = 3 results, coverage fairly
insensitive configuration used. peak coverage Numeric Goal
393

fiColes, Coles, Fox & Long

Domain

Minimal
First-Layer Prop. Goal Num. Goal

(Assignments) Actions
Achievers Achievers Variables
Market Trader
20
20
20
20
20
Hydro Power
23
29
29
29
29
Pathways Metric
30
28
28
30
29
Settlers
21
19
19
19
19
Settlers Num. Carts
22
22
22
22
21
MPrime
30
30
30
30
30
Rovers Numeric
14
13
13
13
15
Sugar
10
9
9
9
19
Total
170
170
170
172
182
Table 7: Coverage varying action variables integer MIP (layer-weighting
k = 1.1)

Domain

Minimal
First-Layer Prop. Goal Num. Goal

(Assignments) Actions
Achievers Achievers Variables
Market Trader
20
20
20
20
20
Hydro Power
23
27
27
29
29
Pathways Metric
30
30
30
30
30
Settlers
20
18
18
18
15
Settlers Num. Carts
22
23
23
23
23
MPrime
30
30
30
30
30
Rovers Numeric
15
15
15
15
15
Sugar
20
18
18
18
20
Total
180
181
181
183
182
Table 8: Coverage varying action variables integer MIP (layer-weighting
k = 3)

Achievers configuration, though difference worst configuration
3 problems. Using k = 1.1, marked increase coverage action
variables integral. due Sugar domain: compared preceding configuration table, additional 10 problems solved. domain,
using k = 3, though, even better coverage obtained using minimal number
integral action variables. Thus, appears need integral variables domain
reduced objective preference earlier actions sufficiently high. Disregarding
Sugar domain, results k = 1.1 domains close, k = 3
configuration.
interesting that, even many integers MIP, performance lp-rpg
terms coverage different using integers all. Considering
computational complexity solving MIP, rather solving LP, time spent
calculating heuristic considerably higher, rendering all-integers approach
394

fiA Hybrid LP-RPG Heuristic Planning

Minimal Ints vs First Layer Actions: Time

100

Minimal Ints vs First Layer Actions: Nodes Expanded

Sugar
Rovers Numeric
M-Prime
Settlers Num. Carts
Settlers
Pathways Metric
Hydro Power
Market Trader

Sugar
Rovers Numeric
M-Prime
Settlers Num. Carts
Settlers
Pathways Metric
Hydro Power
Market Trader

1000

First Layer Actions (Nodes)

First Layer Actions (Time (s))

1000

10

1

100

10

0.1

0.01
0.01

1
0.1

1
10
Minimal Ints (Time (s))

100

1000

1

10

Minimal Ints vs (First Layer Actions +) Prop Goal Achievers: Time

100

1000

Minimal Ints vs (First Layer Actions +) Prop Goal Achievers: Nodes Expanded

Sugar
Rovers Numeric
M-Prime
Settlers Num. Carts
Settlers
Pathways Metric
Hydro Power
Market Trader

Sugar
Rovers Numeric
M-Prime
Settlers Num. Carts
Settlers
Pathways Metric
Hydro Power
Market Trader

1000

Prop Goal Achievers (Nodes)

Prop Goal Achievers (Time (s))

1000

100
Minimal Ints (Nodes)

10

1

100

10

0.1

0.01
0.01

1
0.1

1
10
Minimal Ints (Time (s))

100

1000

1

Minimal Ints vs (First Layer Actions + Prop Goal Achievers) Numeric Goal Achievers: Time

100

100
Minimal Ints (Nodes)

1000

Minimal Ints vs (First Layer Actions + Prop Goal Achievers) Numeric Goal Achievers: Nodes Exp.

Sugar
Rovers Numeric
M-Prime
Settlers Num. Carts
Settlers
Pathways Metric
Hydro Power
Market Trader

Sugar
Rovers Numeric
M-Prime
Settlers Num. Carts
Settlers
Pathways Metric
Hydro Power
Market Trader

1000

Num. Goal Achievers (Nodes)

Num. Goal Achievers (Time (s))

1000

10

10

1

100

10

0.1

0.01
0.01

1
0.1

1
10
Minimal Ints (Time (s))

100

1000

1

10

Minimal Ints vs Ints: Time

Ints (Time (s))

100

1000

Minimal Ints vs Ints: Nodes Expanded

Sugar
Rovers Numeric
M-Prime
Settlers Num. Carts
Settlers
Pathways Metric
Hydro Power
Market Trader

Sugar
Rovers Numeric
M-Prime
Settlers Num. Carts
Settlers
Pathways Metric
Hydro Power
Market Trader

1000

Ints (Nodes)

1000

100
Minimal Ints (Nodes)

10

1

100

10
0.1

0.01
0.01

1
0.1

1
10
Minimal Ints (Time (s))

100

1000

1

10

100
Minimal Ints (Nodes)

Figure 10: Changing variables integer (k = 3)

395

1000

fiColes, Coles, Fox & Long

Time Taken
First Prop. Num.
Layer Goal Goal
Acts Ach. Ach.
Minimal Ints
X
X
X
First Layer Acts
7
X
Prop. Goal Ach.
X
Num. Goal Ach.

Nodes Expanded
Plan Length
First Prop. Num. First Prop. Num.
In- Layer Goal Goal In- Layer Goal Goal Ints Acts Ach. Ach. ts Acts Ach. Ach. ts
X
7
7
7
7
7
7
7
7
X
7
X X
7
7
7
X
X X
7
7
X
7
7

Table 9: Results Two-Tailed Wilcoxon Signed Rank Tests comparing different sets
variables integers MIP (using layer-weighting k = 1.1). X indicates significance
(p = 0.05), colour indicates better performer (faster, fewer nodes expanded shorter
plans).

Time Taken
First Prop. Num.
Layer Goal Goal
Acts Ach. Ach.
Minimal Ints
X
X
X
First Layer Acts
X
X
Prop. Goal Ach.
X
Num. Goal Ach.

Nodes Expanded
Plan Length
First Prop. Num. First Prop. Num.
In- Layer Goal Goal In- Layer Goal Goal Ints Acts Ach. Ach. ts Acts Ach. Ach. ts
X
7
7
7
7
7
7
7
7
X
7
X X
7
7
7
X
7
X
7
7
X
7
7

Table 10: Results Two-Tailed Wilcoxon Signed Rank Tests comparing different sets
variables integers MIP (using layer-weighting k = 3). X indicates significance
(p = 0.05), colour indicates better performer (faster, fewer nodes expanded shorter
plans).

impractical. Investigating further, scatterplots time taken number
nodes expanded shown Figure 10. (The results shown k = 3, overall
picture k = 1.1 used.) configuration compared Minimal
Integers configuration. general trend, one moves left column (increasing
proportion action variables integral), points drift line
= x, i.e. time taken solve problems increases. time, though, moving
right-hand column decrease number nodes evaluated. Thus,
increasing proportion integral action variables seems improve search guidance,
though sufficiently allow pay-off terms time taken solve problems.
confirm significance observations, applied Two-Tailed Wilcoxon Signed
Rank Tests, results Tables 9 10, k = 1.1 k = 3, respectively.
cases, consistent increase time taken solve problems
proportion integral action variables increased: difference significant every
case using k = 1.1 comparing First Layer Actions Propositional
Goal Achievers. significant difference plan length pair
configurations. results nodes somewhat less clear. appear
using Minimal Integers leads expansion significantly different number nodes,
396

fiA Hybrid LP-RPG Heuristic Planning

Domain

Minimal Ints
First Layer Actions
Ints
Build (ms)Solve (ms)Build (ms)Solve (ms)Build (ms)Solve (ms)
Market Trader(20)
26.4
27.7
27.8
52.1
27.2
55.3
Hydro Power (21)
8.8
1.8
9.2
6.5
10.8
6.2
Pathways Metric (30)
58.5
271.2
64.9
834.0
65.4
3468.9
Sugar (16)
24.9
33.8
26.1
46.6
24.7
329.2
Settlers (9)
282.3
96.1
453.8
201.6
295.2
204.7
Settlers Num. Carts (16) 399.4
101.6
520.9
462.8
408.8
222.1
MPrime (28)
78.8
4.8
75.6
6.3
78.9
6.1
Rovers Numeric (11)
57.7
1.7
42.4
2.1
59.0
2.8
Average
117.1
67.3
152.6
201.5
121.3
536.9
Table 11: Time spent building solving LP varying variables integer
MIP; numbers shown domains indicate many problem instances solved
used computing reported average

compared configuration. part due limitation
tests pairwise-solved problems included, difference coverage
reflected data analysed test. Considering configurations, though,
Integers evaluates fewer nodes First Layer Actions Propositional Goal
Achievers, regardless weight used, cannot shown expand fewer nodes
Numeric Goal Achievers, configuration offered better coverage Table 8.
Although seen increasing number integer variables makes planner
slower, somewhat surprising amount planner made slower
greater is. theory would expect making variables integer would
dramatically decrease performance planner, however, appears so.
investigate consider long spent building solving LP
state configuration. Table 11 gives indication seeing
surprising result: 5 8 domains solving time LP less building
time (for Ints configuration), showing key bottleneck using LP
solver is, fact, building LP. Building time is, course, likely vary
configurations (the variation due different states expanded) compared
solving time small. two three remaining domains, Pathways
Sugar, see increase cost solving LP first-layer actions made
integral, much larger increase actions made integral, pattern
expected. final domain, Market Trader, main increase occurs
first-layer action variables integral: structure domain means integral
first layer actions often causes variables layers also become integral. Solving
times does, however, remain within order magnitude building time, overhead
particularly large domain compared previous two.
397

fiColes, Coles, Fox & Long

8.6 Including Numeric Goal Conjunct
Section 7.1 discussed possibility including entire numeric goal conjunct
problem LP. well theoretically increasing ability detect dead-ends
insisting goals attainable time, rather individually also
allows arbitrary LNF goals used, found domains Pathways.
section, investigate impact extension. particular, explore whether
use numeric-goal-checking LP including numeric goal conjunct improves
worsens performance. ensure goal-checking LP case
numeric goal conjunction used, tests, configurations,
disable inclusion propositional goals landmarks LP. gain insights
impact numeric-goal-checking LP affected choice action-variable
layer-weighting schemes, consider two: k = 1.1 k = 3.
Domain

Market Trader
Hydro Power
Pathways Metric
Sugar
Settlers
Settlers Num. Carts
MPrime
Rovers Numeric
Total (Excl. Pathways)
Total


Num.Goal
(k = 3)
20
27
30
18
13
19
28
13
138
168

Without
Num. Goal
(k = 3)
20
27
0
14
14
18
29
13
135
135


Num. Goal
(k = 1.1)
20
29
28
9
12
21
28
13
132
160

Without
Num. Goal
(k = 1.1)
20
30
0
14
10
22
29
13
138
138

Table 12: Coverage varying whether Numeric Goal conjunct included LP (with)
(without)

coverage results running lp-rpg without numeric goal conjunct
shown Table 12. Looking results excluding Pathways domain (which
solved numeric goal conjunct included, since goals expressed
arbitrary LNF), one make two immediate observations: use numeric
goal conjunct improves performance actions weighted k = 3, solving 3 additional
problems, use worsens performance actions weighted k = 1.1, solving 6
fewer problems. difference impact interesting consequence relationship
RPG structure LP:
no-goal-conjunct case, goal appears fact layer l, LP used meet
goal LP (l) LP containing actions action layer l (Algorithm 4,
line 22). favours earlier actions RPG, precluding actions
layer l used.
numeric goal-conjunct case, LP extended layer l0 which,
first, goals appear, second, LP constrained meet goals, using
actions layer l0 , satisfied. individual goals, point may later
398

fiA Hybrid LP-RPG Heuristic Planning

vs Without Numeric Goal Conjunct: Nodes Expanded

vs Without Numeric Goal Conjunct: Time
Sugar
Rovers Numeric
M-Prime
Settlers Num. Carts
Settlers
Hydro Power
Market Trader

100

Numeric Goal Conjunct (Nodes)

Numeric Goal Conjunct (Time (s))

1000

10

1

0.1

0.01

0.001
0.001

0.01

0.1
1
10
100
Without Numeric Goal Conjunct (Time (s))

Sugar
Rovers Numeric
M-Prime
Settlers Num. Carts
Settlers
Hydro Power
Market Trader

1000

100

10

1

1000

1

Sugar
Rovers Numeric
M-Prime
Settlers Num. Carts
Settlers
Hydro Power
Market Trader

10

1

0.1

0.01

0.001
0.001

0.01

0.1
1
10
100
Without Numeric Goal Conjunct (Time (s))

Sugar
Rovers Numeric
M-Prime
Settlers Num. Carts
Settlers
Hydro Power
Market Trader

1000
Numeric Goal Conjunct (Nodes)

Numeric Goal Conjunct (Time (s))

100

1000

vs Without Numeric Goal Conjunct (Weight 3): Nodes Expanded

vs Without Numeric Goal Conjunct (Weight 3): Time
1000

10
100
Without Numeric Goal Conjunct (Nodes)

100

10

1

1000

1

10
100
Without Numeric Goal Conjunct (Nodes)

1000

Figure 11: Including numeric goals LP
layer l first appeared, case actions l0
l chosen meet goals, whereas previous case could not.
difference actions available support given goal makes
action-variable weighting scheme important. use k = 3 rather k = 1.1
means actions added layer l layer l0 (inclusive) available meet
goal first appeared layer l, objective function leads preference use
earlier actions.
coverage results experiments give unequivocal picture
impact feature performance. Figure 11 shows scatterplots time taken
solve problems, number nodes evaluated, numeric goal-conjunct
used not, k = 1.1 k = 3. appears general trend
inclusion numeric goal conjunct reduce number nodes evaluated. case
weighting k = 3, Two-Tailed Wilcoxon Signed Rank Test confirms use
numeric goal conjunct reduces number nodes evaluated (p = 0.05). Using k = 1.1
suggests similar trend, results significant.
weighting schemes, use numeric goal conjunct introduces small
statistically significant time overhead. due additional time taken
evaluate state: RPG must extended point goals
satisfied together, rather individually. Whether pays off, i.e. whether reduction
nodes evaluated sufficient offset this, depends domain appears
399

fiColes, Coles, Fox & Long

correlated extent numeric goals interact. one extreme,
Rovers domain, goals propositional difference performance.
hand, Sugar domain, Settlers domain numeric carts
used, beneficial. domains concern production reprocessing raw
materials, one form another, leading interaction goals. instance, unit
resource may sufficient satisfy goals individually, additional production may
required support both. cases use numeric goal conjunct improves
time performance. Inclusion numeric goal conjunct significant impact
length plans produced (according Two-Tailed Wilcoxon Signed Rank Test).
summarise results section, main benefit use numeric
goal conjunct able extend expressivity planner domains
goals written using arbitrary LNF. success approach domains
varies. terms coverage, whether better use numeric goal conjunct
evaluation domains depends weighting scheme. using layer-weighting
scheme, k = 3, inclusion numeric goal conjunct slightly beneficial lp-rpg
therefore set use configuration default.
8.7 Including Propositions LP
previous section, observed inclusion numeric goal conjunct
goal-checking LP variable impact performance, depending weighting scheme
used. Perhaps interesting use goal-checking LP using LP meet
propositional goals, landmarks, described Section 6.1. evaluate technique
consider four configurations:
1. propositions: using goal-checking LP containing numeric-goal conjunct.
2. Propositional goals: previous case, also including propositional goals
goal-checking LP.
3. Landmarks: previous case, also including landmarks.
4. propositions: previous case, also constrained ensure action
variable non-zero, actions added meet propositional preconditions (as
described Section 6.3).
form spectrum, case information propositions
included LP all, last goal-checking LP must meet
propositional goals, also preconditions actions chosen so. consider
two layer-weighting schemes (k = 1.1 k = 3), action variables actions
first action layer integral.
coverage results k = 1.1 shown Table 13 k = 3 Table 14.
seen, cases, general pattern emerges: coverage improves
including configuration landmarks included LP, declines
final Propositions configuration. Including propositions appears, however,
remain better including propositions all.
Scatterplots illustrating time taken number nodes evaluated solving
problems shown Figures 12 13 (for weights k = 1.1 k = 3, respectively).
400

fiA Hybrid LP-RPG Heuristic Planning

Props vs Propositional Goals: Time

100

Props vs Propositional Goals: Nodes Expanded
10000

Sugar
Rovers Num.
M-Prime
Settlers Num. Carts
settlers
Pathways Metric
Hydro Power
Market Trader

10

1

Sugar
Rovers Numeric
M-Prime
Settlers Numeric Carts
Settlers
Pathways Metric
Hydro Power
Market Trader

1000

Propositional Goals (Nodes)

Propositional Goals (Time (s))

1000

100

10

0.1

0.01
0.01

1
0.1

1
10
Props (Time (s))

100

1000

1

10

Props vs Landmarks: Time

Landmarks (Time (s))

100

1000

10000

Props vs Landmarks: Nodes Expanded
10000

Sugar
Rovers Num.
M-Prime
Settlers Num. Carts
settlers
Pathways Metric
Hydro Power
Market Trader

Sugar
Rovers Numeric
M-Prime
Settlers Numeric Carts
Settlers
Pathways Metric
Hydro Power
Market Trader

1000
Landmarks (Nodes)

1000

100
Props (Nodes)

10

1

100

10
0.1

0.01
0.01

1
0.1

1
10
Props (Time (s))

100

1000

1

10

Props vs Props: Time

Props (Time (s))

100

1000

10000

Props vs Props: Nodes Expanded
10000

Sugar
Rovers Num.
M-Prime
Settlers Num. Carts
settlers
Pathways Metric
Hydro Power
Market Trader

Sugar
Rovers Numeric
M-Prime
Settlers Numeric Carts
Settlers
Pathways Metric
Hydro Power
Market Trader

1000
Props (Nodes)

1000

100
Props (Nodes)

10

1

100

10
0.1

0.01
0.01

1
0.1

1
10
Props (Time (s))

100

1000

1

10

100
Props (Nodes)

1000

Figure 12: Varying propositions included LP (k = 1.1)

401

10000

fiColes, Coles, Fox & Long

Props vs Propositional Goals: Time

Prop Goals (Time (s))

100

Props vs Propositional Goals: Nodes Expanded

Sugar
Rovers Numeric
M-Prime
Settlers Num. Carts
Settlers
Pathways Metric
Hydro Power
Market Trader

Sugar
Rovers Numeric
M-Prime
Settlers Num. Carts
Settlers
Pathways Metric
Hydro Power
Market Trader

1000

Propositional Goals (Nodes)

1000

10

1

100

10

0.1

0.01
0.01

1
0.1

1
10
Props (Time (s))

100

1000

1

10

Props vs Landmarks: Time

Landmarks (Time (s))

100

1000

Props vs Landmarks: Nodes Expanded

Sugar
Rovers Numeric
M-Prime
Settlers Num. Carts
Settlers
Pathways Metric
Hydro Power
Market Trader

Sugar
Rovers Numeric
M-Prime
Settlers Num. Carts
Settlers
Pathways Metric
Hydro Power
Market Trader

1000

100

Landmarks

1000

100
Props (Nodes)

10

1
10
0.1

0.01
0.01

1
0.1

1
10
Props (Time (s))

100

1000

1

10

Props vs Props: Time

Props (Time (s))

100

1000

Props vs Props: Nodes Expanded

Sugar
Rovers Numeric
M-Prime
Settlers Num. Carts
Settlers
Pathways Metric
Hydro Power
Market Trader

Sugar
Rovers Numeric
M-Prime
Settlers Num. Carts
Settlers
Pathways Metric
Hydro Power
Market Trader

1000

Props (Nodes)

1000

100
Props

10

1

100

10
0.1

0.01
0.01

1
0.1

1
10
Props (Time (s))

100

1000

1

10

100
Props (Nodes)

Figure 13: Varying propositions included LP (k = 3)

402

1000

fiA Hybrid LP-RPG Heuristic Planning

Domain
Market Trader
Hydro Power
Pathways Metric
Sugar
Settlers
Settlers Numeric
MPrime
Rovers Numeric
Total

Props
20
29
28
9
12
21
28
13
160

Prop Goals
20
29
28
9
19
22
28
9
164

Landmarks
20
29
28
9
19
22
30
13
170

Props
19
30
30
14
11
21
27
15
167

Table 13: Coverage varying propositions included LP (using k = 1.1)

Domain
Market Trader
Hydro Power
Pathways Metric
Sugar
Settlers
Settlers Numeric
MPrime
Rovers Numeric
Total

Props
20
27
30
18
13
19
28
13
168

Prop Goals
20
27
30
18
18
23
28
13
177

Landmarks
20
27
30
18
18
23
30
15
181

Props
20
28
30
19
11
21
30
14
173

Table 14: Coverage varying propositions included LP (using k = 3)

scatterplot compares one configurations 24 enumerated list
configuration 1 (no propositions). Comparing Propositional Goals Landmarks
configurations suggests inclusion Landmarks leads problems
solved quickly compared propositions configuration. particularly
evident Settlers, Settlers Numeric Carts, MPrime Rovers Numeric domains.
Whether using Landmarks offer gains Propositional Goals alone depends structure problems. two Settlers variants, including propositional goals (such
(connected-by-rail p1 p2) (has-ironworks p3)) necessitates use specific actions ((build-rail p1 p2) (build-ironworks p3), respectively). Since actions
consume numeric resources, including propositional goals sufficient cause LP
introduce production actions necessary support resource consumption. Rovers,
goals communicated data generated rock samples, soil analyses
images. actions adding facts consume number units energy,
benefit including propositional goals LP. However, actions also
propositional preconditions: instance, rover communicate rock data,
must taken sample rock. propositional preconditions included
landmarks LP, thereby forcing production energy needed
403

fiColes, Coles, Fox & Long

communicate requisite data, also energy acquired it. leads
inclusion additional recharge actions or, suitable actions available, better deadend detection. similar situation arises MPrime: actions achieve goal facts
consume resources, actions achieve (landmark) preconditions
actions do, including landmarks improves informedness heuristic.
Two-Tailed Wilcoxon Signed Rank Tests shown Tables 15 16 confirm
(p = 0.05) either k = 1.1 k = 3 use Propositional Goals improves
Propositions, use Landmarks improves Propositional Goals. Indeed,
p = 0.05 total-ordering time-performance k = 3:
Propositions Propositional Goals Landmarks
results show similar behaviour numbers nodes evaluated (the right-hand
columns Figures 12 13) although differences statistically significant.
Wilcoxon tests also show neither k = 1.1 k = 3 difference
Propositional Goals Landmarks statistically significant. Note, however, using
Landmarks allows problems solved. comparisons Propositions,
Propositional Goals Landmarks, weight k = 1.1, significant, latter two
improving upon former. weight k = 3 tests inconclusive, although
using Landmarks allows 13 additional problems solved (recall comparisons
restricted problems solved variants comparison).

Props
Prop Goals
Landmarks

Time Taken
Prop LandAll
Goals marks Props
X
X
7
X
X
X

Nodes Expanded
Prop LandAll
Goals marks Props
X
X
X
7
X
X

Plan Length
Prop LandAll
Goals marks Props
7
7
X
7
X
X

Table 15: Results Two-Tailed Wilcoxon Signed Rank Tests comparing inclusion different propositions LP (using layer-weighting k = 1.1). X indicates significance
(p = 0.05), colour indicates better performer (faster, fewer nodes expanded shorter
plans).

Props
Prop Goals
Landmarks

Time Taken
Prop LandAll
Goals marks Props
X
X
7
X
X
X

Nodes Expanded
Prop LandAll
Goals marks Props
X
7
X
7
X
X

Plan Length
Prop LandAll
Goals marks Props
7
7
X
7
X
X

Table 16: Results Two-Tailed Wilcoxon Signed Rank Tests comparing inclusion different propositions LP (using layer-weighting k = 3). X indicates significance (p = 0.05),
colour indicates better performer (faster, fewer nodes expanded shorter plans).

404

fiA Hybrid LP-RPG Heuristic Planning

Domain

Props
Landmarks
Props
Build (ms)Solve (ms)Build (ms)Solve (ms)Build (ms)Solve (ms)
Market Trader (20)
27.8
52.1
28.0
52.3
29.7
74.5
Hydro Power (27)
9.3
6.6
9.8
6.6
11.7
5.5
Pathways Metric (30)
64.9
834.0
67.0
833.3
68.6
905.5
Sugar (18)
27.2
47.5
27.3
47.5
39.9
82.3
Settlers (8)
436.4
145.5
421.6
137.9
563.9
3153.0
Settlers Num. Carts (15) 204.5
434.8
252.5
133.0
244.2
2253.8
MPrime (28)
75.6
6.3
78.9
5.7
91.2
24.5
Rovers Numeric (11)
42.4
2.1
56.3
2.0
76.3
31.2
Average
111.0
191.1
117.7
152.3
140.7
816.3
Table 17: Time spent building solving LP, varying propositions included;
numbers shown domains indicate many problem instances solved used
computing reported average

results shown bottom Figures 12 13 indicate Propositions
configuration less consistent performance.
show, however, (p = 0.05) k = 1.1 k = 3, Propositions
configuration takes longer solve problems Propositional Goals configuration
Landmarks configuration. perhaps surprising feasible consider
including propositions LP manner, given results reported van den Briel
et al. (2007). key difference disregarding delete effects, fact
needed precondition need added once. work reported van
den Briel et al., however, cases fact required precondition also deleted,
delete effect must balanced equivalent number add effects (less one
fact true initially).
Looking number nodes evaluated using Propositions configuration
find important result: shown bottom right Figures 12 13, whichever
weight used, Propositions tends expand fewer nodes. Furthermore, Propositions expands fewer nodes three configurations overall
best configuration terms nodes expanded (significant result, p = 0.05). Unfortunately,
overhead associated node higher results poorer coverage
time performance. result indicates numericpropositional separation used
lp-rpg heuristic sensible trade-off, use RPG meet propositional
preconditions sacrificing performance terms nodes expanded exchange
reduction time taken solve problems.
Table 17 shows increase costs associated solving LPs including propositions.
results taken problems solved three configurations, although
remains case configurations expand exactly states solving
problems. three domains Hydro Power, MPrime Rovers Numeric
cost building LPs dominates cost solving them, significant
decreases performance domains. course, cost building LPs increases
propositions included since variables required. Settlers domain
405

fiColes, Coles, Fox & Long

order magnitude increase time spent solving LPs, indicating
including propositions LPs makes difficult solve. Sometimes
informed search guidance gleaned information allows planner find
solutions expanding far fewer nodes.
Adding landmarks LP appears make solving LP slightly faster:
partly side effect landmarks configuration needing solve fewer LPs per state,
since uses one LP meet goals instead one LP per goal. Propositions
configuration often needs solve one LP solution extraction phase. However,
size difficulty LP means benefits solving fewer LPs (in terms
time taken) negated.
Tables 15 16 show results statistical tests comparing lengths plans
found various configurations. significant results that, regardless
weight used, Propositions finds shorter plans configurations.
propositions included LP, typically single LP call made
solution extraction, simultaneously achieving goals action preconditions.
configurations, first, LP call made goals, then, action
added support propositional precondition action implied solution
LP, numeric preconditions met another LP call. Thus, several
LP calls rather one. fragmenting production relaxed plan
way, efficacy relaxed plan eroded. example, plan lengths improved
two variants Settlers. Here, production resources requires building
infrastructure: sawmill required refine timber wood, on. two units
wood made, LP knowledge propositional preconditions,
difference LP building two units wood location A, one unit
location B one location C need build one two sawmills, depending
option chosen, discovered actions chosen meet
propositional preconditions actions required solution LP. LP
includes Propositions, cases this, difference building
one sawmill two, LP prefer single-sawmill solution, ultimately leading
better search guidance. Thus, Propositions produces plans significantly shorter,
domains fact propositions outside LP disguises true costs
action choices solution LP.
8.8 Propositional Resource Analysis
end Section 6.4, identified conditions possible turn propositions model resources equivalent numeric representation. purposes,
lp-rpg, would allow resource, within heuristic, managed LP
rather RPG. end, use three domains containing propositional stacks
represent resources, evaluate encoding affects performance lp-rpg,
numeric planners. five planners evaluated MetricFF, lpgtd, lp-rpg (using
analysis translates propositional resource stacks equivalent numeric representation described Long & Fox, 2000), lp-rpg analysis disabled,
lp-rpg-FF. three domains use are:
406

fiA Hybrid LP-RPG Heuristic Planning

Domain

MetricFF

lpgtd

lp-rpg-FF

Settlers Prop. Timber
Settlers Prop. Carts
MPrime Prop.
Total

7
4
29
40

4
4
30
38

8
10
29
68

lp-rpg
Anal.
Anal.
5
16
13
22
29
30
47
47

Table 18: Coverage without propositional resource analysis

variant IPC3 Settlers domain, amount timber given location
(or given vehicle) represented stack propositions, range n0
n10.
variant Settlers Numeric Carts encoding, number carts
given location represented stack propositions.
MPrime domain IPC1, encoding represents fuel level stack
propositions.
first two these, propositional encoding enforces limit 10 amount
timber (respectively, number carts) given location. necessary limitation forced action grounding. numeric representation, decreasing one resource
increasing another done single ground action, appropriate numeric
effects conditions. ground action need stipulate precise levels
resource operation, long limits resource levels
respected. propositional case, however, one ground action needed pair
discrete levels two resources, parameters action stipulating pre-
post-values resource. level 10 chosen avoid placing overly restrictive
bounds resource levels, creating manageable number ground actions.
issue grounding also accounts limited choice domains: propositional stack
used represent resources take modest range discrete values.
Rovers domain, example, set reachable energy levels rover
range [0, 80]. Market Trader domain, amount money could held
given time real value one decimal place (this consequence choice
problem files), greater equal zero.
Results domains shown Table 18. Comparing, first, lp-rpg without
analysis lp-rpg analysis, see encoding resources LP
numbers grants consistent improvement performance. Figure 14 shows time
taken find solutions similarly improved, perhaps strikingly second Settlers
variant (with propositional cart levels).
MetricFF lpgtd perform well MPrime domain, two Settlers
encodings, lp-rpg resource analysis performs markedly better,
planners solving handful problems. contrasts earlier results, shown
Table 2, lpgtd particular performed well Settlers domain.
interesting contrast see here, Propositional Timber variant derived
IPC 3 model lpgtd performs well leads considerably worse performance.
407

fiColes, Coles, Fox & Long

Settlers Propositional Stack Timber
10000

1000

Settlers Propositional Stack Carts
10000

LPRPG-Analysis
LPRPG-FF
FF
LPG
LPRPG-No-Analysis

1000

Time (s)

100

Time (s)

100

LPRPG-Analysis
LPRPG-FF
FF
LPG
LPRPG-No-Analysis

10

10

1

1

0.1

0.1
5

10

15
Problem Number

20

25

5

10

15
Problem Number

20

25

M-Prime Propositional Resource Stack
10000

1000

LPRPG-Analysis
LPRPG-FF
FF
LPG
LPRPG-No-Analysis

Time (s)

100

10

1

0.1

0.01
5

10

15
Problem Number

20

25

30

Figure 14: Using propositional resource analysis

supports role resource analysis allowing lp-rpg robust
number almost-equivalent domain encodings used, mixed propositional
numeric encodings resource levels. observed significant change length
plans produced lp-rpg without propositional resource analysis.

9. Conclusions Future Work
modern planning systems ineffective reasoning numbers. However, managing complex numeric interactions important part driving AI planning towards
future real-world application. paper shown that, using linear program
model numeric resource flows, ability planners reason domains involving
complex numeric interactions greatly improved.
key contribution separation heuristic search control relaxed
planning graph, based delete-relaxation, linear program allows exact reasoning
numeric constraints relaxes action ordering.
explored different configurations heuristic, put
less information LP, impact performance planner whole.
exploration different LP solvers reveals less efficient handling
various combinations constraints. found that, LPSolve CLP, conjunction
limited version lp-rpg published 2008 (Coles et al., 2008), solve simple
408

fiA Hybrid LP-RPG Heuristic Planning

problems quickly, CPLEX coupled full power extended LPRPG needed
handle complex test instances.
work far focussed developing search control methods perform
well numeric planning problems particular character: producer-consumer
behaviour define Sectionsect:prodcondefinition. Although believe
common behaviour, practice, numeric domains exhibit range behaviours.
several possible ways exploit lp-rpg approach domains. One use
approach actions conform constraints producer-consumer behaviour,
pushing numeric behaviour metric RPG way currently
handle propositional goals preconditions separate RPG. would yield
benefits potentially better estimates reachable ranges action use costs
parts domain express producer-consumer behaviour. challenging
consider behaviours relaxed producer-consumer behaviour
obtain useful heuristic information. example, actions production effects vary
could encoded family producers increasing capability, discretising range
production options introducing reachability analysis increased
production capabilities become available. general, relaxations must make reachability
least permissive actual reachability (that is, action must applicable
relaxed reachability analysis least early action actually reachable)
relaxed plan extraction minimise estimated cost goal effectively (this
difficult relaxed plan optimal). Within constraints, believe
use LP approximations provide tool tackling wider range behaviours
explore paper.
exciting challenge future work integrate lp-rpg method
optimising plans according given objective function. recent 2008 2011
planning competitions highlighted importance optimising planning emphasis solution quality. development is, however, non-trivial: challenges lie
integration cost optimisation LP RPG well deciding
use heuristic trades goal distance quality search. first step
direction accomplished Radzi PhD thesis (Radzi, 2011), advances
numeric planning described paper open many possibilities extending
initial work.

References
Bell, K. R. W., Coles, A. J., Coles, A. I., Fox, M., & Long, D. (2009). role AI planning
decision support tool power substation management. Artificial Intelligence
Communications, 22 (1), 3757.
Benton, J., Do, M. B., & Kambhampati, S. (2005). Over-subscription planning numeric goals. Proceedings 19th International Joint Conference Artificial
Intelligence (IJCAI), pp. 12071213.
Benton, J., van den Briel, M., & Kambhampati, S. (2007). hybrid linear programming
relaxed plan heuristic partial satisfaction planning problems. Proceedings
17th International Conference Planning Scheduling (ICAPS).
409

fiColes, Coles, Fox & Long

Blackmore, L., Ono, M., & Williams, B. (2011). Chance-constrained optimal path planning
obstacles. Robotics, IEEE Transactions on, 27 (6), 1080 1094.
Blum, A., & Furst, M. (1995). Fast planning planning graph analysis. Proceedings
14th International Joint Conference Artificial Intelligence (IJCAI 95), pp.
16361642.
Bonet, B., & Geffner, H. (2001). Heuristic Search Planner 2.0. Artificial Intelligence Magazine, 22 (3), 7780.
Coles, A., Coles, A., Fox, M., & Long, D. (2012). COLIN: Planning continuous linear
numeric change. Journal Artificial Intelligence Research, 44, 196.
Coles, A. I., Fox, M., Long, D., & Smith, A. J. (2008). hybrid relaxed planning graph-lp
heuristic numeric planning domains. Proceedings Eighteenth International
Conference Automated Planning Scheduling (ICAPS 08).
Coles, A. J., & Coles, A. I. (2011). LPRPG-P: Relaxed plan heuristics planning
preferences. Proceedings 21st International Conference Automated Planning Scheduling (ICAPS).
Coles, A. J., Coles, A. I., Fox, M., & Long, D. (2009). Extending use inference
temporal planning forwards search. Proceedings 19th International
Conference Automated Planning Scheduling (ICAPS 09).
Do, M. B., & Kambhampati, S. (2001). Sapa: domain-independent heuristic metric temporal planner. Proceedings European Conference Planning (ECP01).
Do, M. B., & Kambhampati, S. (2003). SAPA: multi-objective metric temporal planner.
Journal Artificial Intelligence Research, 20, 155194.
Domshlak, C., Katz, M., & Lefler, S. (2010). abstractions met landmarks. Proceedings 20th International Conference Planning Scheduling (ICAPS).
Edelkamp, S. (2003). Taming numbers durations model checking integrated
planning system. Journal Artificial Intelligence Research, 20, 195238.
Fox, M., & Long, D. (2003). PDDL2.1: extension PDDL expressing temporal
planning domains. Journal Artificial Intelligence Research, 20, 61124.
Gerevini, A., Saetti, A., & Serina, I. (2006). approach temporal planning scheduling domains predictable exogenous events. Journal Artificial Intelligence
Research, 25, 187231.
Gerevini, A., Saetti, A., & Serina, I. (2008). approach efficient planning numerical
fluents multi-criteria plan quality. Artificial Intelligence, 172 (8-9), 899944.
Gregory, P., & Rendl, A. (2008). constraint model settlers planning domain.
Aylett, R. (Ed.), Proceedings UK Planning Special Interest Group (PlanSIG).
Herriot Watt University.
Gregory, P., Cresswell, S., Long, D., & Porteous, J. (2004). extraction disjunctive landmarks planning problems via symmetry reduction.. Proceedings
Conference Symmetry Search (SymCon 2004), pp. 3441.
410

fiA Hybrid LP-RPG Heuristic Planning

Helmert, M., & Domshlak, C. (2009). Landmarks, critical paths abstractions: Whats
difference anyway?. Proceedings 19th International Conference Planning Scheduling (ICAPS), pp. 162169.
Hoffmann, J. (2003). Metric-FF planning system: Translating ignoring delete lists
numeric state variables. Journal Artificial Intelligence Research, 20, 291341.
Hoffmann, J., & Edelkamp, S. (2005). deterministic part IPC-4: overview. Journal
Artificial Intelligence Research, 24, 519579.
Hoffmann, J., Porteous, J., & Sebastia, L. (2004). Ordered landmarks planning. Journal
Artificial Intelligence Research, 22, 215278.
Hoffmann, J., & Nebel, B. (2001). FF planning system: Fast plan generation
heuristic search. Journal Artificial Intelligence Research, 14, 253302.
Karpas, E., & Domshlak, C. (2009). Cost-optimal planning landmarks. Proceedings
21st International Joint Conference Artificial Intelligence (IJCAI 09), pp.
17281733.
Kautz, H., & Walser, J. (2000). Integer optimization models AI planning problems.
Knowledge Engineering Review, 15 (1), 101117.
Koehler, J. (1998). Planning resource constraints. Proceedings European
Conference Artificial Intelligence (ECAI98), pp. 489493.
Laborie, P. (2003). Algorithms propagating resource constraints AI planning
scheduling: Existing approaches new results. Artificial Intelligence, 143 (2), 151
188.
Li, H. X., & Williams, B. C. (2008). Generative planning hybrid systems based flow
tubes. Proceedings 18th International Conference Automated Planning
Scheduling, ICAPS, pp. 206213.
Long, D., & Fox, M. (2000). Automatic synthesis use generic types planning.
Proceedings Artificial Intelligence Planning Scheduling (AIPS), pp. 196205.
Long, D., & Fox, M. (2003a). Exploiting Graphplan framework temporal planning.
Proceedings International Conference Artificial Intelligence Planning
Scheduling (ICAPS).
Long, D., & Fox, M. (2003b). 3rd International Planning Competition: Results
analysis. Journal Artificial Intelligence Research, 20, 159.
Ono, M., & Williams, B. C. (2008). efficient motion planning algorithm stochastic
dynamic systems constraints probability failure. Proceedings 23rd
AAAI Conference Artificial Intelligence, AAAI, pp. 13761382.
Porteous, J., Sebastia, L., & Hoffmann, J. (2001). extraction, ordering, usage
landmarks planning. Proceedings 6th European Conference Planning
(ECP 01).
Radzi, N. H. M. (2011). Multi-Objective Planning using Linear Programming. Ph.D. thesis,
University Strathclyde.
411

fiColes, Coles, Fox & Long

Richter, S., Helmert, M., & Westphahl, M. (2008). Landmarks revisited. Proceedings
23rd AAAI Conference Artificial Intelligence (AAAI 08), pp. 975982.
Richter, S., & Westphal, M. (2010). LAMA Planner: Guiding Cost-Based Anytime
Planning Landmarks.. Journal Artificial Intelligence Research, 39, 127177.
Shin, J.-A., & Davis, E. (2005). Processes continuous change SAT-based planner.
Journal Artificial Intelligence Research, 166, 194253.
van den Briel, M., Benton, J., Kambhampati, S., & Vossen, T. (2007). LP-based heuristic
optimal planning. Principles Practice Constraint Programming (CP
2007), pp. 651665.
van den Briel, M. H. L., Vossen, T., & Kambhampati, S. (2008). Loosely coupled formulations automated planning: integer programming perspective. Journal
Artificial Intelligence Research, 31, 217257.
Vossen, T., Ball, M. O., Lotem, A., & Nau, D. S. (1999). use integer programming
models AI planning. Proceedings 16th International Joint Conference
Artificial Intelligence (IJCAI 99), pp. 304309.
Wolfman, S., & Weld, D. (2000). Combining linear programming satisfiability solving
resource planning. Knowledge Engineering Review, 15 (1).
Zhu, L., & Givan, R. (2003). Landmark extraction via planning graph propagation. Proceedings Doctoral Consortium 13th International Conference Planning
Scheduling (ICAPS-DC 03), pp. 156160.

412

fiJournal Artificial Intelligence Research 46 (2013) 235-262

Submitted 11/12; published 02/13

Toward Supervised Anomaly Detection
Nico Grnitz
Marius Kloft

NICO . GOERNITZ @ TU - BERLIN . DE
KLOFT @ TU - BERLIN . DE

Machine Learning Laboratory, Technische Universitt Berlin
Franklinstr. 28/29, Berlin, Germany
Computational Biology Center
Memorial Sloan-Kettering Cancer Center
New York City, USA

Konrad Rieck

KONRAD . RIECK @ UNI - GOETTINGEN . DE

University Gttingen, Dep. Computer Science
Goldschmidtstr. 7, 37077 Gttingen, Germany

Ulf Brefeld

BREFELD @ KMA . INFORMATIK . TU - DARMSTADT. DE

Technische Universitt Darmstadt
Hochschulstr. 10, 64289 Darmstadt, Germany
German Institute International Educational Research
Schlostr. 29, 60486 Frankfurt, Germany

Abstract
Anomaly detection regarded unsupervised learning task anomalies stem adversarial unlikely events unknown distributions. However, predictive performance
purely unsupervised anomaly detection often fails match required detection rates many
tasks exists need labeled data guide model generation. first contribution
shows classical semi-supervised approaches, originating supervised classifier, inappropriate hardly detect new unknown anomalies. argue semi-supervised anomaly
detection needs ground unsupervised learning paradigm devise novel algorithm
meets requirement. Although intrinsically non-convex, show
optimization problem convex equivalent relatively mild assumptions. Additionally,
propose active learning strategy automatically filter candidates labeling. empirical
study network intrusion detection data, observe proposed learning methodology requires much less labeled data state-of-the-art, achieving higher detection accuracies.

1. Introduction
Anomaly detection deals identifying unlikely rare events. classical approach
anomaly detection compute precise description normal data. Every newly arriving instance contrasted model normality anomaly score computed. score
describes deviations new instance compared average data instance and, deviation exceeds predefined threshold, instance considered anomaly outlier
processed adequately (Markou & Singh, 2003a; Chandola, Banerjee, & Kumar, 2009; Markou &
Singh, 2003b).
Identifying data exhibits irregular suspicious traits crucial many applications
medical imaging network security. particular, latter become vivid research area
computer systems increasingly exposed security threats, computer worms, network
c
2013
AI Access Foundation. rights reserved.

fiG RNITZ , K LOFT, R IECK , & B REFELD

Figure 1: Illustration two paradigms semi-supervised learning.

attacks, malicious code (Andrews & Pregibon, 1978). Network intrusion detection deals
detecting previously unknown threats attacks network traffic. Conventional security techniques intrusion detection based identifying known patterns misuse, called signatures (Roesch, 1999; Paxson, 1999) thusalthough effective known attacksfail
protect novel threats. brings anomaly detection focus security research (e.g.,
Eskin, Arnold, Prerau, Portnoy, & Stolfo, 2002; Kruegel, Vigna, & Robertson, 2005; Stolfo, Apap,
Eskin, Heller, Hershkop, Honig, & Svore, 2005; Perdisci, Ariu, Fogla, Giacinto, & Lee, 2009).
Thus, anomaly detection beneficial learning scenarios many regular data instances given, allows machine approximate underlying distribution well
leads concise model normality. contrast, outliers anomalies rare even
originate changing distributions (e.g., novel classes network attacks). Especially adversarial settings, network intrusion detection, differences training test distributions
eminent novel threats tactics continuously developed. consequence, anomaly
detection generally considered unsupervised task prominent learning methods, including
one-class support vector machines (Schlkopf, Platt, Shawe-Taylor, Smola, & Williamson, 2001)
support vector data descriptions (SVDD, Tax & Duin, 2004), implement spirit. However,
underlying assumptions unsupervised methods also major drawback many
application areas, unsupervised methods fail achieve required detection rates. Especially
adversarial application areas network intrusion detection, even single undetected outlier
may already suffice capture system. Therefore, goal article incorporate
feedback-loop terms labeled data make anomaly detection practical. so, knowledge historic threats anomalies included terms labels thus guide
model generation toward better generalizations.
article, cast anomaly detection paradigm semi-supervised learning
(Chapelle, Schlkopf, & Zien, 2006). Usually, semi-supervised methods deduced existing
supervised techniques, augmented appropriate bias take unlabeled data account.
instance, prominent bias assumes unlabeled data structured clusters closeness (with respect measure) proportional probability class label
(Vapnik, 1998; Joachims, 1999; Chapelle & Zien, 2005; Chapelle, Chi, & Zien, 2006; Sindhwani,
Niyogi, & Belkin, 2005). consequence, anomaly detection often rephrased (multi-class)
classification problem (Almgren & Jonsson, 2004; Stokes & Platt, 2008; Pelleg & Moore, 2004;
Mao, Lee, Parikh, Chen, & Huang, 2009). Although assuming cluster-structure data
236

fiT OWARD UPERVISED NOMALY ETECTION

80

100

60

AUC[0,0.01] [in %]

AUC[0,0.01] [in %]

80

60

40

40

20

unsupervised

unsupervised

supervised
semisupervised

20

SSAD
0

1 5 10 15

50

75

supervised
semisupervised

0

SSAD
1 5 10 15

100

50

75

100

% labeled data

% labeled data

Figure 2: Left: standard supervised classification scenario identical training test distributions. Right: anomaly detection setting two novel anomaly clusters
test distribution.

often well justified anomaly detection, recall supervised learning techniques focus discriminating concept classes unsupervised techniques rather focus data characterization.
article, show differences training test distributions well occurrence
previously unseen outlier classes render anomaly detection methods, derived supervised
technique, inappropriate likely miss novel previously unseen classes anomalies depicted. contrast, argue successful anomaly detection methods inherently need
ground unsupervised learning paradigm, see Figure 1. sum, making anomaly detection practical requires following key characteristics: (i) intrinsically following unsupervised
learning paradigm cope unknown events (ii) additionally exploiting label information
obtain state-of-the-art results.
Figure 2, show results controlled experiment visualizes different nature
semi-supervised methods derived supervised unsupervised paradigms, respectively.
left hand side, achieved accuracies standard supervised classification scenario,
training test distributions identical, shown. performance unsupervised
anomaly detection method clearly outperformed supervised semi-supervised approaches.
However, observe right hand side figure fragile latter methods
anomaly detection scenario. experimental setup identical former except
discard two anomaly clusters training set (see Figure 3). Note change
arbitrary modification inherent characteristic anomaly detection scenarios, anomalies stem novel previously unseen distributions. Unsurprisingly, (partially) supervised
methods fail detect novel outliers clearly outperformed unsupervised approach,
robustly performs around unimpressive detection rates 40%. Finally, methods
clearly outperformed novel semi-supervised anomaly detection (SSAD) devised
unsupervised learning paradigm allows incorporating labeled data.
237

fiG RNITZ , K LOFT, R IECK , & B REFELD

anomalies

normal
data
novel
anomalies

Figure 3: Left: training data stems two clusters normal data (gray) one small anomaly
cluster (red). Right: two additional anomaly clusters (red) appear test data set.

main contribution article provide mathematical sound methodology semisupervised anomaly detection. Carefully conducted experiments discussions show
importance distinguishing two semi-supervised settings depicted Figure 1.
meet requirement, propose novel semi-supervised anomaly detection technique
derived unsupervised learning paradigm, allows incorporating labeled data
training process. approach based support vector data description (SVDD) contains
original formulation special case. Although final optimization problem convex,
show equivalent convex formulation obtained relatively mild assumptions.
guide user labeling process, additionally propose active learning strategy
improve actual model quickly detect novel anomaly clusters. empirically evaluate
method network intrusion detection tasks. contribution proves robust scenarios
performance baseline approaches deteriorate due obfuscation techniques. addition,
active learning strategy shown useful standalone method threshold adaptation.
remainder article organized follows. Section 2 reviews related work. novel
semi-supervised anomaly detection methods presented Section 3 Section 4 introduces
active learning strategies. Section 5 gives insights proposed learning paradigm
report results real-world network intrusion scenarios Section 6. Section 7 concludes.

2. Related Work
Semi-supervised learning (Chapelle, Schlkopf et al., 2006) offers mathematical sound framework
learning partially labeled data. instance, transductive approaches semi-supervised
learning assume cluster structure data close points likely share label
points far away likely labeled differently. transductive support vector
machine (TSVM, Vapnik, 1998; Joachims, 1999; Chapelle & Zien, 2005) optimizes max-margin
hyperplane feature space implements cluster assumption. basic formulation,
TSVM non-convex integer programming problem top SVM. computationally
expensive Chapelle, Chi et al. (2006) propose efficient smooth relaxation
TSVM. Another related approach low density separation (LDS) Chapelle Zien (2005),
cluster structure modeled graph distances.
238

fiT OWARD UPERVISED NOMALY ETECTION

broad overview anomaly detection found work Chandola et al. (2009).
Anomaly detection regarded unsupervised learning task therefore surprising
exist large number applications employing unsupervised anomaly detection methods.
instance, finding anomalies network traffic (Eskin et al., 2002) program behaviour (Heller,
Svore, Keromytis, & Stolfo, 2003), denoising patterns (Park, Kang, Kim, Kwok, & Tsang, 2007)
annotating (Goh, Chang, & Li, 2005) classifying images (Lai, Tax, Duin, Zbieta, Ekalska, &
Ik, 2004) documents (Manevitz & Yousef, 2002; Onoda, Murata, & Yamada, 2006)
Fully-supervised approaches anomaly detection usually ignore unlabeled data
training-phase: example, Almgren Jonsson (2004) employ max-margin classifier separates innocuous data attacks. Stokes Platt (2008) present technique
combines approaches effective discrimination (Almgren & Jonsson, 2004) rare-class detection (Pelleg & Moore, 2004). Mao et al. (2009) take multi-view co-training approach based
Blum Mitchell (1998) learn labeled unlabeled data.
Support vector learning also extended many non-standard settings one-class
learning (Schlkopf et al., 2001) support vector data description (Tax & Duin, 2004).
idea latter learn hypersphere encloses bulk provided data
instances lie outside hypersphere considered anomalous. contrast, one-class
SVM learns hyperplane feature space divides data points origin
maximum-margin. translation-invariant kernel matrices, approaches equivalent.
exist semi-supervised methods based unsupervised techniques.
Blanchard, Lee, Scott (2010) propose method appealing option specifying upper threshold false-positives rate. However, method needs include test instances training time applicable online streaming scenarios anomaly
detection. holds true extension one-class SVM Munoz Mar, Bovolo,
Gmez-Chova, Bruzzone, Camp-Valls (2010) incorporates labeled examples graphLaplacian regularization term. Tax (2001) proposes straight-forward extension SVDD
semi-supervised anomaly detection, negatively labeled points required lie outside
hypersphereotherwise penalty incurred. advantage so-called SVDDneg approach
assumptions underlying data-generating probability distribution
manifold assumptions imposed. Unfortunately, primal SVDDneg problem convex
optimization problem, makes difficult accurately optimize. Moreover, dual optimization proposed work Tax (2001) cannot considered sound alternative due
possible duality gaps. However, Appendix show convex reformulation
SVDDneg translation-invariant kernels, RBF-kernels. new formulation
suffer duality gaps easily solved primal dual descent methods.
problem occurs related semi-supervised one-class methods proposed Liu Zheng (2006)
Wang, Neskovic, Cooper (2005).
Another broad class methods deals learning positive unlabeled examples
(LPUE). Intrinsically, one aims solving two-class problem data one class (the
positive class) given together unlabeled data points. LPUE thus applied problem setting hand identifying outlier class positively labeled data. Zhang Lee
(2005) show class methods viewed special case semi-supervised learning
emphasize SVDDneg (Tax, 2001) considered instance LPUE. Algorithmically, LPUE often solved iterative manner (i) identifying reliable set labeled
examples using classifier (ii) re-training classifier given new training set (Liu, Dai,
239

fiG RNITZ , K LOFT, R IECK , & B REFELD

Li, Lee, & Yu, 2003; Zhang & Lee, 2005; Blum & Mitchell, 1998). Though work addresses
learning non-i.i.d. data (e.g., Li & Liu, 2005), underlying assumption usually implies
training test sets drawn distribution.
present article builds upon previous paper authors (Grnitz, Kloft, & Brefeld,
2009). extends latter mathematical sound framework intuitive philosophical insights.
addition, present general problem formulation employing arbitrary convex loss functions computation dual representation thereof, new empirical analysis
comparisons larger variety baseline approaches.

3. Semi-supervised Anomaly Detection
anomaly detection tasks, given n observations x1 , . . . , xn X . underlying assumption bulk data stems (unknown) distribution call
part data normal. observations, however, originate different distributions
considered anomalies. anomalies could instance caused broken sensors
network attacks cannot sampled definition. goal anomaly detection detect
anomalies finding concise description normal data, deviating observations
become outliers. thus aim finding scoring function f : X R defines model
normality. Following principle empirical risk minimization, optimization problem takes
following form,
n
X
f = argmin (f ) +
l(f (xi )),
n
f
i=1

l : R R appropriate loss function, : Rd R+ regularizer f ,
trade-off parameter.
approach based SVDD, computes hypersphere radius R center
c encompasses data. hypersphere model normality anomaly score
instance x computed distance center c,
f (x) = ||(x) c||2 R2 .

(1)

Points lying outside ball (i.e., f (x) > 0) considered anomalous, points within
(f (x) < 0) treated normal data. corresponding optimization problem known
support vector data description (SVDD, Tax, 2001) following form
n
X
min R2 + u

R,c,

s.t.

i=1

ni=1
ni=1

: k(xi ) ck2 R2 +
: 0 ,

trade-off u balances minimization radius sum erroneously placed
points (that are, points lying outside normality radius). parameter u also serves
estimate ratio outliers normal data n training examples. resulting
Pproblem convex solved equivalently dual space using representation
c = ni=1 (xi ). consequence, input data expressed equivalently kernel
function k(xi , xj ) = (xi )T (xj ) X corresponds feature map : X F
reproducing kernel Hilbert space F (see, e.g., Mller, Mika, Rtsch, Tsuda, & Schlkopf, 2001).
240

fiT OWARD UPERVISED NOMALY ETECTION

3.1 Semi-supervised Anomaly Detection
propose novel approach semi-supervised anomaly detection. proposed method
generalizes vanilla SVDD processes unlabeled labeled examples. existing extensions SVDD employ dual optimization techniques inherently suffer duality gaps
due non-convexity, propose primal approach semi-supervised anomaly detection.
discussed earlier, translation-invariant kernels, one-class SVM contained framework
special case. addition n unlabeled examples x1 , . . . , xn X , given
) X denotes set class labels.
labeled observations (x1 , y1 ), . . . , (xm , ym
simplicity, focus = {+1, 1} = +1 encodes nominal data = 1
anomalies.
argued introduction, goal derive method grounds unsupervised
learning paradigm. therefore stick hypersphere model SVDD use latter
blueprint dealing unlabeled data. inclusion labeled examples follows simple
pattern: example x labeled nomial (y = +1), require lies within hypersphere. contrast, example anomaly member outlier class (y = 1), want
placed outside ball. straight-forward extension SVDD using labeled
unlabeled examples thus given

min

R,,c,

s.t.

2

R + u

n
X

+ l

i=1

ni=1

:

n+m
j=n+1

n+m
X

2

j

j=n+1
2

k(xi ) ck R +

: yj k(xj ) ck2 R2 + j

ni=1 :

(2)

0,


n+m
j=n+1 : j 0 ,

margin labeled examples , u , l trade-off parameters. Unfortunately, inclusion negatively labeled data renders optimization problem non-convex
optimization dual space prohibitive. remedy, following approach Chapelle
Zien (2005), translate Equation (2) unconstrained problem. thereby resolve slack
terms OP follows
= ` R2 ||(xi ) c||2
j = ` yj



(3)



R2 ||(xj ) c||2 .

example, put `(t) = max{t, 0} (i.e., common hinge loss), recover (2). Furthermore, application representer theorem, obtain support-vector expansion

c=

n
X
i=1

n+m
X

(xi ) +

j=n+1

241

j yj (xj )

(4)

fiG RNITZ , K LOFT, R IECK , & B REFELD

1
hinge loss
Huber loss

linear

loss

0.75

0.5

quadratic

0.25

linear

0
0

0.5

1
yf(x)

1.5

2

Figure 4: Non-differentiable hinge loss (dashed) differentiable Huber loss `=1,=0.5 (solid).
(see Appendix B detailed derivation). Combining (3) (4), re-formulate optimization
problem (2) solely terms kernels without constraints follows:
min

R,,

2

R + u
+ l

n
X

` R2 k(xi , xi ) + (2ei )0 K

i=1
n+m
X





` yj R2 k(xj , xj ) + (2ej )0 K .

(5)

j=n+1

Hereby K = (kij )1i,jn denotes kernel matrix given kij = k(xi , xj ) = h(xi ), (xj )i
e1 , . . . , en+m standard base Rn+m . rephrasing problem unconstrained
optimization problem, intrinsic complexity changed. Often, unconstrained optimization
easier implement constrained optimization. non-smooth optimization possible
via e.g. non-convex bundle methods described work (2010), smooth optimization
methods conjugate gradient Newtons method easier apply. obtain smooth
optimization technique, choose Hubers robust loss (Huber, 1972). Huber loss two parameters controlling quadratic approximation terms center witdh , see Figure
4. optimization function becomes differentiable off-the-shelf gradient-based optimization
tools applied. complete derivation gradients optimization problem (5) using
Hubers robust loss shown Appendix C.
3.2 Convex Semi-supervised Anomaly Detection
optimization problem previous section easy implement but, unfortunately, nonconvex. Therefore, optimizers may find good local optimum, several restarts necessary
verify quality solutions cases optimization might fail completely. show
that, rather mild assumptions, namely data processed unit norm feature
space (as fulfilled by, e.g., RBF kernels), optimization problem converted
equivalent convex one. derivation general postulates nothing convexity
loss function. approach based combination Lagrangian duality notion
Fenchel-Legendre conjugate function. Fenchel duality machine learning pioneered
242

fiT OWARD UPERVISED NOMALY ETECTION

Rifkin Lippert (2007) assumption full-rank kernels. approach general
allows us use kernel K. byproduct derivation, show classical
one-class SVM special case general class density level set estimators minimize
convex risk functional give general dual criterion class. aim, introduce
Legendre-Fenchel conjugate given loss l(t)
lc (z) = sup (zt l(t))


use slightly different formulation SSAD problem, is, eliminate hinge loss
slack variables , reformulate problem explicit loss functions:
n
n+m
X
X
1
2
||w|| + u
l(ti ) + l
l(tj )
2

min

,,w,t

i=1

ni=1 :
n+m
j=n+1

s.t.

j=n+1



ti = (w (xi ))
: tj =

(yj wT (xj ))



(P)
yj



0.
Note that, problem, auxiliary variables ti introduced deal non-differentiable
loss functions. Again, convex nature stated optimization problem,
solve dual space. aim, use Lagrange Theorem incorporate constraints
objective:
n
n+m
X
X
1
2
L = ||w|| + u
l(ti ) + l
l(tj )
2
i=1





n
X

j=n+1

((wT (xi )) ti )

i=1
n+m
X

(6)

j ((yj wT (xj )) yj tj )

j=n+1

optimal solution found solving Lagrangian saddle point problem
max min EQ6.
, ,,w,t

used standard Lagrangian ansatz, would compute derivate Lagrangian
respect primal variables. However, general loss function l() necessarily differentiable.
remedy, compute derivatives wrt w, . Setting zero, yields
optimality conditions
:

0 u

j :

0 j l
n
n+m
X
X
w =
(xi ) +
j yj (xj ).
i=1

j=n+1

243

(7)

fiG RNITZ , K LOFT, R IECK , & B REFELD

Inserting optimality conditions Lagrangian, saddle point problem translates





n
n+m
X
X
j
1


max K + u
min l(ti ) + ti + l
min
l(tj ) + tj .



2
u
l
i=1

j=n+1

Converting min max statement results




n
n+m
X
X
j
1


max K u
max ti l(ti ) l
max
tj l(tj ) .



2
u
l
i=1

j=n+1

Now, making use Legendre-Fenchel conjugate lc () described above, arrive following dual optimization problem
max


s.t.

n+m
n
X
X
j

1
lc ( )
lc ( ) l
K u
2
u
l
j=n+1

i=1

1=

n
X
i=1

+

n+m
X

j yj

j=n+1

(D)





n+m
X

j .

j=n+1

contrast existing semi-supervised approaches anomaly detection (Tax, 2001; Liu & Zheng,
2006; Wang et al., 2005), strong duality holds shown following proposition.
Proposition 3.1 optimization problems (P) (D) strong duality holds.
Proof follows convexity (P) Slaters condition, trivially fulfilled
adjusting tj : > 0 tj R : 0 = (yj wT (xj )) yj tj .

observe optimization problems (P) (D) contain non-convex variant
special case translation-invariant kernels. Difficulties may arise presence many equality
inequality constraints, increase computational requirements. However,
inherent; left hand-side constraint removed discarding variable initial
primal problemthis leaves regularization path optimization problem invariantand
right hand side inequality equivalently incorporated objective function
Lagrangian argument (e.g., Proposition 12 Kloft, Brefeld, Sonnenburg, & Zien, 2011). Note,
convex model intuitive interpretation normalized kernels. order deal
wider class kernels, need resort general non-convex formulation presented
Section 3.1.

4. Active Learning Semi-supervised Anomaly Detection
previous section, presented two optimization problems incorporate labeled data
unsupervised anomaly detection technique. However, yet addressed question
acquiring labeled examples. Many topical real-world applications involve millions training
instances (Sonnenburg, 2008) domain experts label small fraction unlabeled
data. Active learning deals finding instances that, labeled included training
244

fiT OWARD UPERVISED NOMALY ETECTION

set, lead largest improvement re-trained model. following, present active
learning strategy well-suited anomaly detection. core idea query low-confidence
decisions guide user labeling process.
approach works follows. First, initialize method training unlabeled
examples. training set augmented particular examples selected
active learning rule. candidates labeled domain expert added training
set. model retrained refined training set, consists unlabeled labeled
examples. Subsequently labeling- retraining-steps repeated required performance
reached.
active learning rule consists two parts. begin commonly used active
learning strategy simply queries borderline points. idea method choose
point closest decision hypersphere (Almgren & Jonsson, 2004; Warmuth, Liao, Rtsch,
Mathieson, Putta, & Lemmen, 2003) presented expert:


kf (x)k


x0 = argmin
= argmin R2 k(x) ck2 .
(8)
x{x1 ,...,xn } maxk kf (xk )k
x{x1 ,...,xn }
supervised support vector machines, strategy known margin strategy (Tong &
Koller, 2000). Figure 5 (a) shows illustratation semi-supervised anomaly detection.
dealing non-stationary outlier categories, beneficial identify novel anomaly
classes soon possible. translate requirement active learning strategy follows.
Let = (aij )i,j=1,...,n+m adjacency matrix training instances, obtained by, example,
k-nearest-neighbor approach, aij = 1 xi among k-nearest neighbors xj 0
otherwise. introduce extended labeling y1 . . . , yn+m examples defining yi = 0
unlabeled instances retaining labels labeled instances, i.e., yj = yj . Using pseudo
labels, Equation (9) returns unlabeled instance according
x0

=

n+m
1 X
(yj + 1) aij .
xi {x1 ,...,xn } 2k

argmin

(9)

j=1

strategy explores unknown clusters feature space thus labels orthogonal complementary instances illustrated Figure 5 (b).
Nevertheless, using Equation (9) alone may result querying points lying close center
actual hypersphere. points hardly contribute improvement hypersphere.
hand, using margin strategy alone allow querying novel regions
lie far away margin. words, combination strategies (8) (9)
guarantees points interest queried. final active learning strategy therefore given
x0 =

argmin
xi {x1 ,...,xn }

=

n+m
kf (x)k 1 X
+
(yj + 1) aij
c
2k

(10)

j=1

[0, 1]. combined strategy queries instances close boundary hypersphere lie potentially anomalous clusters respect k-nearest neighbor graph, see
Figure 5 (c) illustration. Depending actual value , strategy jumps cluster
cluster thus helps identify interesting regions feature space. special case
labeled points, combined strategy reduces margin strategy.
245

fiG RNITZ , K LOFT, R IECK , & B REFELD

(a) margin strategy

(b) cluster strategy

(c) combined strategy

Figure 5: Comparison active learning strategies (queried points marked blue): (a)
margin strategy queries data points closest decision boundary, (b) cluster
strategy queries points rarely labeled regions, (c) combined strategy queries
data points likely anomalies clusters near decision boundary.

Usually, active learning step followed optimization step semi-supervised
SVDD, update model respect recently labeled data. procedure course timeconsuming altered practical settings, instance querying couple points
performing model update. Irrespectively actual implementation, alternating
active learning updating model repeated desired predictive performance
obtained.

5. Illustration Proposed Learning Paradigm
section, illustrate weaknesses existing learning paradigms semi-supervised
anomaly detection settings means controlled experiment synthetic data. results,
already briefly sketched introduction (cf., Figure 2), discussed detail
here.
Table 1: Competitors toy data experiment.
two-class
supervised transductive
SVM
LDS

unsupervised
SVDD

one-class
LPUE
semi-supervised
neg
SVDD
SSAD

end, generate nominal training validation data two isotropic Gaussian
distributions R2 one anomaly cluster (shown Figure 3 (left)). However, testing time,
two novel anomaly clusters appear test data (shown Figure 3 (right)). reflects characteristic anomalies stem novel, previously unseen distributions. compare
newly-developed method SSAD following baseline approaches: unsupervised support
246

fiT OWARD UPERVISED NOMALY ETECTION

80
70

AUC[0,0.01] [in %]

60
50
40
30
SVDD
SVM
10
0

(unsupervised)

SVDDneg (LPUE)

20

1 5 10 15

(supervised)

LDS

(transductive)

SSAD

(proposed method)

50

75

100

% labeled data

Figure 6: Performance various unsupervised, supervised semi-supervised methods
anomaly detection setting.

vector domain description (SVDD, Tax & Duin, 2004), corrected semi-supervised SVDDneg
(Tax, 2001) described Appendix A, supervised support vector machine (SVM, Boser, Guyon,
& Vapnik, 1992; Cortes & Vapnik, 1995), semi-supervised low-density separation (LDS,
Chapelle & Zien, 2005), see Table 1. common anomaly detection setups, measure area
ROC curve interval [0, 0.01] report AUCs averaged 25 repetitions
distinct training, validation, test sets. every repetition, parameters u , l adjusted
respective validation set within interval [102 , 102 ]. experiments, used = 1.
Error bars correspond standard errors.
results shown Figure 6, horizontal axis shows different ratios labeled
randomly drawn unlabeled examples. Methods derived supervised learning paradigm
SVM LDS cannot cope novel outlier clusters perform poorly ratios
labeled unlabeled examples; performance remains unsupervised SVDD,
utilize labeled data thus unaffected incorporating labels training process. contrast, two semi-supervised methods derived unsupervised learning
paradigm clearly outperform baselines. However, SVDDneg benefits anomalous labeled data since sparse, needs big fraction labeled data increase
performance. semi-supervised method SSAD exploits every single labeled example needs
15% labels saturate around optimum.
Figure 7 visualizes typical contour lines hypotheses computed SSAD three different
scenarios. figure shows fully-supervised scenario instances correctly labeled
(left), semi-supervised solution 25% data labeled 75% remains unlabeled
(center), completely unsupervised one using unlabeled data (right). Unsurprisingly,
fully supervised solution discriminates perfectly normal data outliers
unsupervised solution recognizes latter normal data mistake. intermediate semisupervised solution uses little label information also achieve perfect separation
involved classes.
247

fiG RNITZ , K LOFT, R IECK , & B REFELD

(a)

(b)

(c)

Figure 7: Different solutions fully-supervised (left), semi-supervised (center), unsupervised
anomaly detection using RBF kernels. Colors indicate unlabeled data (green), labeled
outliers (red), labeled normal instances (violet).

0.3
SVDD

0.25

0.2

time

(unsupervised)

SVDDneg (LPUE)
LDS
SVM

(transductive)
(supervised)

SSAD

(proposed method)

0.15

0.1

0.05

0
50

100

150

200

examples

Figure 8: Execution times.

Figure 8 compares execution times different methods shows number training
examples versus training time. simplicity, discarded 50% labels training data
random. results show methods, SSAD, SVDDneg , SVDD, derived
unsupervised learning principle, perform similarly. SVM performs best uses
labeled part training data ignores unlabeled examples. Low density separation
(LDS) performs worst due transductive nature.
Based observations, draw following conclusions. Anomaly detection scenarios
render methods derived supervised learning paradigm inappropriate. Even unsupervised
methods ignoring label information may perform better supervised peers. Intuitively,
discarding label information sub-optimal. experiment shows semi-supervised methods
unsupervised learning paradigm effectively incorporate label information outperform
competitors. confirm findings Section 6.
248

fiT OWARD UPERVISED NOMALY ETECTION

6. Real-World Network Intrusion Detection
goal network intrusion detection identify attacks incoming network traffic. Classical
signature-based proven insufficient identification novel attacks, signatures
need manually crafted advance. Therefore machine learning approaches gaining
attention intrusion detection research community.
detection unknown novel attacks requires adequate representation network
contents. remainder, apply technique embedding network payloads vector spaces
derived concepts information retrieval (Salton, Wong, & Yang, 1975) recently
applied application domain network intrusion detection (Rieck & Laskov, 2007).
network payload x (the data contained network packet connection) mapped vector
space using set strings embedding function . string function
(x) returns 1 contained payload x 0 otherwise. applying (x)
elements obtain following map
: X R|S| ,

: x 7 (s (x))sS ,

(11)

X domain network payloads. Defining set relevant strings priori difficult typical patterns novel attacks available prior disclosure. alternative,
define set implicitly associate possible strings length n. resulting
set strings often referred n-grams.
consequence using n-grams, network payloads mapped vector space
n
256 dimensions, apparently contradicts efficient network intrusion detection. Fortunately,
payload length comprises (T n) different n-grams and, consequently, map
sparse, is, vast majority dimensions zero. sparsity exploited derive
linear-time algorithms extraction comparison embedded vectors. Instead operating
full vectors, non-zero dimensions considered, extracted strings associated
dimension maintained efficient data structures (Rieck & Laskov, 2008).
experiments, consider HTTP traffic recorded within 10 days Fraunhofer Institute
FIRST. data set comprises 145,069 unmodified connections average length 489 bytes.
incoming byte stream connection mapped vector space using 3-grams detailed
above. refer FIRST data normal pool. malicious pool contains 27 real attack
classes generated using Metasploit framework (Maynor, Mookhey, Cervini, & Beaver, 2007).
covers 15 buffer overflows, 8 code injections 4 attacks including HTTP tunnels crosssite scripting. Every attack recorded 26 different variants using virtual network environment
decoy HTTP server, attack payload adapted match characteristics normal
data pool. detailed description data set provided Rieck (2009).
study robustness approach realistic scenario, also consider techniques
obfuscate malicious content adapting attack payloads mimic benign traffic feature space
(Fogla, Sharif, Perdisci, Kolesnikov, & Lee, 2006; Perdisci et al., 2009). consequence,
extracted features deviate less normality classifier likely fooled attack.
purposes, already suffices study simple cloaking technique adding common HTTP
headers payload malicious body attack remains unaltered. apply
technique malicious pool refer obfuscated set attacks cloaked pool.
249

fiG RNITZ , K LOFT, R IECK , & B REFELD

6.1 Detection Performance
section, evaluate statistical performance SSAD intrusion detection, comparison baseline methods SVDD SVDDneg . addition, combined active learning
strategy compared random sampling.
focus two scenarios: normal vs. malicious normal vs. cloaked data. settings,
respective byte streams translated bag-of-3-grams representation. experiment,
randomly draw 966 training examples normal pool 34 attacks, depending
scenario, either malicious cloaked pool. Holdout test sets also drawn
random consist 795 normal connections 27 attacks, each. make sure attacks
attack class occur either training, test set both. report
10 repetitions distinct training, holdout, test sets measure performance area
ROC curve false-positive interval [0, 0.01] (AUC0.01 )
Figure 9(a) shows results normal vs. malicious data pools, x-axis depicts
percentage randomly drawn labeled instances. Irrespectively amount labeled data,
malicious traffic detected methods equally well intrinsic nature attacks well
captured bag-of-3-grams representation (cf., Wang, Parekh, & Stolfo, 2006; Rieck & Laskov,
2006). significant difference classifiers.
Figure 9(b) shows results normal vs. cloaked data. First all, performance
unsupervised SVDD drops 70%. obtain similar result SVDDneg ; incorporating
cloaked attack information training process SVDD leads increase 5%
far practical value. Notice SVDDneg cannot make use labeled data
normal class. Thus, moderate ascent terms number labeled examples credited
class ratio 966/34 random labeling strategy. bulk additional information
cannot exploited left out. contrast, semi-supervised method SSAD includes
labeled data training process clearly outperforms two baselines. 5%
labeled data, SSAD easily beats best baseline randomly labeling 15% available
data separates normal cloaked malicious traffic almost perfectly.
Nevertheless, labeling 15% data realistic practical applications. thus explore
benefit active learning inquiring label information borderline low-confidence points.
Figure 9(c) shows results normal vs. cloaked data, labeled data SVDDneg
SSAD chosen according active learning strategy Equation (10). unsupervised
SVDD make use labeled information unaffected setup, remaining
AUC0.01 70%. Compared results using random labeling strategy (Figure 9(b)),
performance SVDDneg increases significantly. ascent SVDDneg steeper
performance yields 85% 15% labeled data. However, SSAD also improves active
learning dominates baselines. Using active learning, need label 3% data
attaining almost perfect separation, compared 25% random labeling strategy.
conclude active learning strategy effectively improves performance reduces
manual labeling effort significantly.
Figure 10 impact active learning strategy given Equation (10) shown.
compare number outliers detected combined strategy margin-based strategy
Equation (8) (see also, Almgren & Jonsson, 2004) randomly drawing instances
unlabeled pool. sanity check, also included theoretical outcome random sampling.
250

fiT OWARD UPERVISED NOMALY ETECTION

1
1

AUC[0,0.01] [in %]

0.99
0.985
0.98
0.975
0.97
0.965

SVDD
SVDDneg
SSAD (proposed method)

0.8

0.7

SVDDneg
SSAD (proposed method)

0.955
0.95

0.9

SVDD

0.96

0

5

10

0

15

1

3

6

10

15

% labeled data

% labeled data

(a) Detection accuracies regular attacks.

(b) Detection accuracies cloaked attacks.

1

AUC[0,0.01] [in %]

AUC[0,0.01] [in %]

0.995

0.9

0.8

0.7

0

1

3

6

10

15

% labeled data

(c) Detection accuracies cloaked attacks using proposed active learning strategy SSAD.

Figure 9: Results network intrusion detection experiment. detection accuracies
regular attacks insignificantly different (see (a)), SSAD achieves 30% higher
accuracies baseline approaches cloaked data (see (b)). proposed activity
learning strategy increases accuracy labeled data rare (see (c)).

results show combined strategy effectively detects malicious traffic much faster
margin-based strategy.
6.2 Threshold Adaptation
previous experiments demonstrated advantages active learning network intrusion
detection. far, results obtained using method SSAD; however, active
learning techniques devised Section 4 also applicable calibrating learning-based
methods. herein focus vanilla SVDD parameter value = 1, corresponds
classical centroid-based anomaly detection (e.g., Shawe-Taylor & Cristianini, 2004),
251

fiG RNITZ , K LOFT, R IECK , & B REFELD

35
30

#outliers

25
20

optimal
active learning
margin
random (empirical)
random (theoretical)

15
10
5
0
0

1

2

3

4

5

7.5
10
labeled data %

15

Figure 10: Number novel attacks detected combined active learning strategy (blue line),
random sampling (red solid dotted line), margin strategy (purple line) upper
bound (light blue dotted line) single run.

results directly transfer anomaly detectors Anagram (Wang et al., 2006), McPad (Perdisci
et al., 2009) TokDoc (Krueger, Gehl, Rieck, & Laskov, 2010).
draw set 3,750 network connections pool normal data split
resulting set training set 2,500 connections test partition 1,250 events. sets
mixed cloaked attack instances. SVDD trained normal training set
delivering threshold R. application learned hypersphere test set, evaluate
different strategies determining radius R using random sampling active learning.
cases, selected connections labeled threshold obtained computing mean
labeled instances:






R
maxi d(xi )
R =
minj d(x
)

P
P j


d(xi )+ j d(xj )
#pos+#neg

: #pos = 0
: #pos > 0
: #pos = 0





#neg = 0
#neg = 0
#neg > 0

:



#neg > 0

#pos > 0

(12)
xi positive labeled examples, xj negative examples d(x) = ||(x) c||
denotes distance current sample x hyperspheres origin. Figure 11 shows
ROC curve SVDD computed thresholds various levels labeled data. Results
averaged 10 random draws working sets. One see even small amounts
labeled data active learning strategy finds reasonable radius random strategy
vanilla SVDD completely fail false-positive rate 0.5 1, respectively. result
demonstrates active learning strategies enable calibrating anomaly detectors significantly
252

fiT OWARD UPERVISED NOMALY ETECTION

1

TPR

0.8

0.6

0.4
ROC curve
SVDD threshold
active learning 5%
random sampling 5%

0.2

0
0

0.2

0.4

0.6

0.8

1

FPR

Figure 11: Results threshold adaption experiment: ROC curve SVDD (grey)
thresholds determined SVDD (green), proposed combined strategy (blue),
random sampling (red) shown (labeling 5% data).

reduced effort comparison random sampling hence provide valuable instrument
deploying learning methods practice.

7. Conclusion
article, developed framework semi-supervised anomaly detection, allows
inclusion prior expert knowledge. discussed conceptual difference semisupervised models derived unsupervised supervised techniques proposed
generalization support vector data description incorporate labeled data. optimization
problem semi-supervised anomaly detection (SSAD) unconstrained, continuous problem,
allows efficient optimization gradient-based methods convex equivalent
mild assumptions kernel function.
approached semi-supervised anomaly detection unsupervised learning paradigm.
proposed novel active learning strategy specially tailored anomaly detection.
strategy guides user labeling process querying instances close
boundary hypersphere, also likely contain instances novel outlier categories.
Empirically, applied semi-supervised anomaly detection application domain network intrusion detection. showed rephrasing unsupervised problem semi-supervised
task beneficial practice: SSAD proves robust scenarios performance baseline
approaches deteriorates due obfuscation techniques. Moreover, demonstrated effectiveness active learning strategy couple data sets observed SSAD significantly
improve prediction accuracy effectively exploiting limited amount labeled available.
observed handful labeled instances necessary boost performance.
characteristic especially appealing tasks labeling data costly network security
traffic inspected malicious patterns expert expert.
253

fiG RNITZ , K LOFT, R IECK , & B REFELD

many possibilities exploit extend learning approach well active
learning strategy. example, replacing `2 -norm regularization sparsity-inducing `1 -norm
incorporate automatic feature selection reduces dimensionality solution. Learning
sparse feature representations great interest computer security applications
signature generation. possible optimization strategy could linear programming (LP)
approach Campbell Bennett (2001) data domain description. However, choices
regularizers certainly possible including structured regularizers incorporate hierarchies
learning process non-isotropic norms encode additional domain knowledge. Incorporating
multiple labels rephrasing semi-supervised anomaly detection multi-task problem might
also improve accuracy complex application domains.

Acknowledgments
authors grateful Klaus-Robert Mller comments helped improving
manuscript. work supported part German Bundesministerium fr Bildung und
Forschung (BMBF) project PROSEC (FKZ 01BY1145), FP7-ICT Programme
European Community, PASCAL2 Network Excellence, German National Science Foundation (DFG) GA 1615/1-1, MU 987/6-1, MU 987/11-1 RA 1894/11. Furthermore, Marius Kloft acknowledges PhD scholarship German Academic Exchange
Service (DAAD) postdoctoral fellowship German Research Foundation (DFG) well
funding Ministry Education, Science, Technology, National Research
Foundation Korea Grant R31-10008. part work done Marius Kloft
Computer Science Division Department Statistics, University California, Berkeley,
CA 94720-1758, USA.

Appendix A. Analysis SVDDneg
appendix, point limitation previously published methods SVDDneg (Tax,
2001) well methods proposed Hoi, Chan, Huang, Lyu, King (2003), Liu Zheng
(2006), Wang et al. (2005), Yuan Casasent (2004). methods suffer potential
duality gaps optimized dual space butdepending training datarun risk
originating non-convex optimization problem. instance, case
single negatively labeled example included training set. issue addressed
aforementioned papers.
exemplarily illustrate problem SVDDneg . SVDDneg incorporates labeled examples outlier class otherwise unsupervised learning process. before, majority
(unlabeled) data points shall lie inside sphere labeled outliers constrained
lie outside normality ball. results two-class problem positive class
consists unlabeled data negative class formed labeled outliers. introducing
class labels {+1, 1} (where unlabeled data points receive class label yi = +1), primal
optimization problem given
min

R,c,

R2 +

n
X



i=1

s.t. ni=1 : yi k(xi ) ck2 yi R2 +
254

0,

(13)

fiT OWARD UPERVISED NOMALY ETECTION

4

5

x 10

objective

4

3

2

1

0
0

Primal
Dual

20

40

60

80

100

negatives

Figure 12: Exemplary duality gap SVDDneg using linear kernel = 100. horizontal axis
shows percentage negative (anomalous) points training set vertical axis
shows primal dual objective values.

corresponding optimization problem dual space given

max


s.t.

n
X
i=1
n
X

yi k(xi , xi )

n
X

j yi yj k(xi , xj )

(14)

i,j=1

yi = 1 0

= 1, . . . , n.

i=1

existence duality gap shown follows: second derivative primal constraints
g(xi ) = yi k(xi ) ck2 yi R2 0 given 2 g/c2 = 2yi negative outliers
label equals yi = 1. turns whole optimization problem non-convex. consequence,
optimal solutions primal dual problems may differ. Figure 12 shows exemplary
plot duality gap artificially generated data one nominal Gaussian surrounded
smaller anomalous Gaussian. labeling process data points receive
corresponding label negative examples present learning problem (horizontal
axis) larger duality gap larger difference two objective values (vertical
axis). Note duality gap necessarily monotonic function although behavior
likely case. Furthermore, maximization dual problem yields lower bound
primal objective (blue line), whereas latter always greater equal corresponding
dual (red line).
Nevertheless, following Theorem shows class translation-invariant kernel functions, exists equivalent convex re-formulation form one-class SVM (Schlkopf
et al., 2001).
Theorem A.1 solution found optimizing dual non-convex SVDDneg stated
Equations (14) identical dual corresponding convex one-class SVM problem
stated Equation (15) kernel translation-invariant, i.e., k(xi , xi ) = i, R+ .
255

fiG RNITZ , K LOFT, R IECK , & B REFELD

Proof dual one-class SVM given
max




n
1 X
j yi yj k(xi , xj ),
2

n
X

s.t.

i,j=1

yi = 1

0

i.

(15)

i=1

respective constraints already equivalent. dual SVDDneg objective translationinvariant kernel reduces
n
n
X
X
= argmax
yi k(xi , xi )
j yi yj k(xi , xj )


i=1

= argmax

ns



i,j=1

n
X

yi

i=1

ns

= argmax


n
X

j yi yj k(xi , xj )

i,j=1

n
X

j yi yj k(xi , xj ),

(16)

i,j=1

substituted equality constraint Eq. (15) last step. Finally, Equation (16)
precisely one-class SVM dual objective, scaled 12 shifted constant ns. However,
optimal solution affected transformation completes proof.


Appendix B. Representer Theorem SSAD
section, show applicability representer theorem semi-supervised anomaly
detection.
Theorem B.1 (Representer Theorem Schlkopf & Smola, 2002) Let H reproducing kernel Hilbert space kernel k : X X R, symmetric positive semi-definite function
compact domain. function L : Rn R, nondecreasing function : R R.

J := min J(f )f H := min f H{ ||f ||2H + L (f (x1 ), . . . , f (xn ))}
well-defined, exist 1 , . . . , n R,
f () =

n
X

k(xi , )

(17)

i=1

J .

achieves J(f ) =
Furthermore, increasing, minimizer J(f ) expressed
form Eq. (17).
Proposition B.2 representer theorem applied non-expanded version Equation
(5).
Proof Recall primal SSAD objective function given
2

J(R, , c) =R + u

n
X

` R2 ||(xi ) c||2



i=1

+ l

n+m
X



` yj R2 ||(xj ) c||2 .

j=n+1

256

fiT OWARD UPERVISED NOMALY ETECTION

Substituting := R2 ||c||2 leads new objective function
2

J(T, , c) =||c|| + + u

n
X

` ||(xi )||2 + 2(xi )0 c



i=1

+ l

n+m
X



` yj ||(xj )||2 + 2(xj )0 c .

j=n+1

Expanding center c terms labeled unlabeled input examples covered
representer theorem. optimization, easily re-substituted obtain primal
variables R, , c. completes proof.


Appendix C. Computing Gradients Eq. (5)
section compute gradients SSAD formulation given Eq. (5).
neccessary step implement gradient-based solver SSAD. end, consider
unconstrained optimization problem given
min

R,,

2

R + u

n
X

`, R2 k(xi , xi ) + (2ei )0 K

i=1
n+m
X

+ l





`, yj R2 k(xj , xj ) + (2ej )0 K ,

j=n+1

`, Huber loss given


(+t)2
`, (t) =
4

0

:
: t+
: otherwise.

notational convenience, focus Huber loss `=0, (t) move margin dependent
terms argument compute gradients several steps, follows: first, build
gradient respect primal variables R c, yields

= 2R`0 (R2 ||(xi ) c||2 )
R

= 2((xi ) c)`0 (R2 ||(xi ) c||2 ).
c

(18)

derivatives counterparts j labeled examples respect R, , c
given
j


= 2yj R`0 yj R2 ||(xj ) c||2
R
j


= `0 yj R2 ||(xj ) c||2

j


= 2yj ((xj ) c)`0 yj R2 ||(xj ) c||2 .
c
257

fiG RNITZ , K LOFT, R IECK , & B REFELD

Substituting partial gradients, resolve gradient Equation (5) respect primal
variables follows:
n
n+m
X
X j
EQ5

= 2R + u
+ l
,
R
R
R

(19)

n+m
X j
EQ5
= + l
,



(20)

n
n+m
X
X j

EQ5
= u
+ l
.
c
c
c

(21)

i=1

j=n+1

j=n+1

i=1

j=n+1

following, extend approach allow use kernel functions. application
representer theorem shows center c expanded
c=

n
X

(xi ) +

i=1

n+m
X

j yj (xj ).

(22)

j=n+1

According chain rule, gradient Equation (5) respect i/j given
EQ5
EQ5 c
=
.
i/j
c i/j
Using Equation (22), partial derivatives

c
i/j

resolve

c
= (xi )


c
= yj (xj ),
j

(23)

respectively. Applying chain-rule Equations (19),(20),(21), (23) gives gradients
Equation (5) respect i/j .

References
Almgren, M., & Jonsson, E. (2004). Using active learning intrusion detection. Proc. IEEE
Computer Security Foundation Workshop, pp. 8889.
Andrews, D. F., & Pregibon, D. (1978). Finding outliers matter. Journal Royal
Statistical Society. Series B (Methodological), 40(1), 8593. /
Blanchard, G., Lee, G., & Scott, C. (2010). Semi-Supervised Novelty Detection. Journal Machine
Learning Research, , 2973297330093009.
Blum, A., & Mitchell, T. (1998). Combining labeled unlabeled data co-training. COLT
98: Proc. eleventh annual conference Computational learning theory, pp. 92100,
New York, NY, USA. ACM.
Boser, B., Guyon, I., & Vapnik, V. (1992). training algorithm optimal margin classifiers.
Haussler, D. (Ed.), Proceedings 5th Annual ACM Workshop Computational Learning
Theory, pp. 144152.
258

fiT OWARD UPERVISED NOMALY ETECTION

Campbell, C., & Bennett, K. (2001). linear programming approach novelty detection. Leen,
T., Dietterich, T., & Tresp, V. (Eds.), Advances Neural Information Processing Systems,
Vol. 13, pp. 395401. MIT Press.
Chandola, V., Banerjee, A., & Kumar, V. (2009). Anomaly detection: survey. ACM Computing
Surveys, 41(3), 158.
Chapelle, O., Chi, M., & Zien, A. (2006). continuation method semi-supervised SVMs.
ICML, pp. 185192, New York, New York, USA. ACM.
Chapelle, O., & Zien, A. (2005). Semi-supervised classification low density separation. Proc.
International Workshop AI Statistics.
Chapelle, O., Schlkopf, B., & Zien, A. (2006). Semi-Supervised Learning (Adaptive Computation
Machine Learning). MIT Press.
Cortes, C., & Vapnik, V. (1995). Support vector networks. Machine Learning, 20, 273297.
Do, T.-M.-T. (2010). Regularized bundle methods large-scale learning problems application large margin training hidden Markov models. Ph.D. thesis, Pierre Marie
Curie University Paris.
Eskin, E., Arnold, A., Prerau, M., Portnoy, L., & Stolfo, S. (2002). Applications Data Mining
Computer Security, chap. geometric framework unsupervised anomaly detection:
detecting intrusions unlabeled data. Kluwer.
Fogla, P., Sharif, M., Perdisci, R., Kolesnikov, O., & Lee, W. (2006). Polymorphic blending attacks.
Proc. USENIX Security Symposium.
Goh, K.-S., Chang, E. Y., & Li, B. (2005). Using one-class two-class svms multiclass image
annotation. IEEE Transactions Knowledge Data Engineering, 17, 13331346.
Grnitz, N., Kloft, M., & Brefeld, U. (2009). Active semi-supervised data domain description.
ECML/PKDD (1), pp. 407422.
Heller, K., Svore, K., Keromytis, A., & Stolfo, S. (2003). One class support vector machines
detecting anomalous windows registry accesses. Proc. workshop Data Mining
Computer Security.
Hoi, C.-H., Chan, C.-H., Huang, K., Lyu, M., & King, I. (2003). Support vector machines class
representation discrimination. Proc. International Joint Conference Neural
Networks.
Huber, P. (1972). Robust statistics: review. Ann. Statist., 43, 1041.
Joachims, T. (1999). Transductive inference text classification using support vector machines.
International Conference Machine Learning (ICML), pp. 200209, Bled, Slowenien.
Kloft, M., Brefeld, U., Sonnenburg, S., & Zien, A. (2011). `p -norm multiple kernel learning. Journal
Machine Learning Research, 12, 953997.
Kruegel, C., Vigna, G., & Robertson, W. (2005). multi-model approach detection webbased attacks. Computer Networks, 48(5).
Krueger, T., Gehl, C., Rieck, K., & Laskov, P. (2010). TokDoc: self-healing web application
firewall. Proc. 25th ACM Symposium Applied Computing (SAC), pp. 18461853.
259

fiG RNITZ , K LOFT, R IECK , & B REFELD

Lai, C., Tax, D. M. J., Duin, R. P. W., Zbieta, E., Ekalska, P., & Ik, P. P. (2004). study
combining image representations image classification retrieval. International Journal
Pattern Recognition Artificial Intelligence, 18, 867890.
Li, X.-l., & Liu, B. (2005). Learning Positive Unlabeled Examples Different Data
Distributions. ECML.
Liu, B., Dai, Y., Li, X., Lee, W. S., & Yu, P. S. (2003). Building Text Classifiers Using Positive
Unlabeled Examples. IEEE International Conference Data Mining, pp. 179186. IEEE
Comput. Soc.
Liu, Y., & Zheng, Y. F. (2006). Minimum enclosing maximum excluding machine pattern
description discrimination. ICPR 06: Proc. 18th International Conference
Pattern Recognition, pp. 129132, Washington, DC, USA. IEEE Computer Society.
Munoz Mar, J., Bovolo, F., Gmez-Chova, L., Bruzzone, L., & Camp-Valls, G. (2010). SemiSupervised One-Class Support Vector Machines Classification Remote Sensing Data.
IEEE Transactions Geoscience Remote Sensing, 48(8), 31883197.
Manevitz, L. M., & Yousef, M. (2002). One-class svms document classification. J. Mach. Learn.
Res., 2, 139154.
Mao, C.-H., Lee, H.-M., Parikh, D., Chen, T., & Huang, S.-Y. (2009). Semi-supervised co-training
active learning based approach multi-view intrusion detection. SAC 09: Proc.
2009 ACM symposium Applied Computing, pp. 20422048, New York, NY, USA.
ACM.
Markou, M., & Singh, S. (2003a). Novelty detection: review part 1: statistical approaches. Signal
Processing, 83, 24812497.
Markou, M., & Singh, S. (2003b). Novelty detection: review part 2: neural network based
approaches. Signal Processing, 83, 24992521.
Maynor, K., Mookhey, K., Cervini, J. F. R., & Beaver, K. (2007). Metasploit Toolkit. Syngress.
Mller, K.-R., Mika, S., Rtsch, G., Tsuda, K., & Schlkopf, B. (2001). introduction kernelbased learning algorithms. IEEE Neural Networks, 12(2), 181201.
Onoda, T., Murata, H., & Yamada, S. (2006). One class classification methods based non-relevance
feedback document retrieval. WI-IATW 06: Proc. 2006 IEEE/WIC/ACM international conference Web Intelligence Intelligent Agent Technology, pp. 393396, Washington, DC, USA. IEEE Computer Society.
Park, J., Kang, D., Kim, J., Kwok, J. T., & Tsang, I. W. (2007). SVDD-based pattern denoising.
Neural Computation, 19, 19191938.
Paxson, V. (1999). Bro: System Detecting Network Intruders Real-Time. Elsevier Computer
Networks, 31(23-24), 24352463.
Pelleg, D., & Moore, A. (2004). Active learning anomaly rare-category detection. Proc.
Advances Neural Information Processing Systems, pp. 10731080.
Perdisci, R., Ariu, D., Fogla, P., Giacinto, G., & Lee, W. (2009). McPAD: multiple classifier
system accurate payload-based anomaly detection. Computer Networks, 5(6), 864881.
260

fiT OWARD UPERVISED NOMALY ETECTION

Rieck, K. (2009). Machine Learning Application-Layer Intrusion Detection. Ph.D. thesis, Berlin
Institute Technology (TU Berlin).
Rieck, K., & Laskov, P. (2006). Detecting unknown network attacks using language models.
Detection Intrusions Malware, Vulnerability Assessment, Proc. 3rd DIMVA
Conference, LNCS, pp. 7490.
Rieck, K., & Laskov, P. (2007). Language models detection unknown attacks network
traffic. Journal Computer Virology, 2(4), 243256.
Rieck, K., & Laskov, P. (2008). Linear-time computation similarity measures sequential data.
Journal Machine Learning Research, 9(Jan), 2348.
Rifkin, R. M., & Lippert, R. A. (2007). Value regularization fenchel duality. Journal Machine
Learning Research, 8, 441479.
Roesch, M. (1999). Snort: Lightweight intrusion detection networks. Proc. USENIX Large
Installation System Administration Conference LISA, pp. 229238.
Salton, G., Wong, A., & Yang, C. (1975). vector space model automatic indexing. Communications ACM, 18(11), 613620.
Schlkopf, B., & Smola, A. (2002). Learning Kernels. MIT Press, Cambridge, MA.
Schlkopf, B., Platt, J. C., Shawe-Taylor, J., Smola, A. J., & Williamson, R. C. (2001). Estimating
support high-dimensional distribution. Neural Computation, 13(7), 14431471.
Shawe-Taylor, J., & Cristianini, N. (2004). Kernel methods pattern analysis. Cambridge University Press.
Sindhwani, V., Niyogi, P., & Belkin, M. (2005). Beyond point cloud: transductive
semi-supervised learning. ICML, Vol. 1, pp. 824831. ACM.
Sonnenburg, S. (2008). Machine Learning Genomic Sequence Analysis. Ph.D. thesis, Fraunhofer
Institute FIRST. supervised K.-R. Mller G. Rtsch.
Stokes, J. W., & Platt, J. C. (2008). Aladin: Active learning anomalies detect intrusion. Tech.
rep., Microsoft Research.
Stolfo, S. J., Apap, F., Eskin, E., Heller, K., Hershkop, S., Honig, A., & Svore, K. (2005).
comparative evaluation two algorithms windows registry anomaly detection. Journal
Computer Security, pp. 659693.
Tax, D. M. J. (2001). One-class classification. Ph.D. thesis, Technical University Delft.
Tax, D. M. J., & Duin, R. P. W. (2004). Support vector data description. Machine Learning, 54,
4566.
Tong, S., & Koller, D. (2000). Support vector machine active learning applications text
classification. Proc. Seventeenth International Conference Machine Learning,
San Francisco, CA. Morgan Kaufmann.
Vapnik, V. (1998). Statistical Learning Theory. Wiley, New York.
Wang, J., Neskovic, P., & Cooper, L. N. (2005). Pattern classification via single spheres. Computer Science: Discovery Science (DS), Vol. 3735, pp. 241252.
261

fiG RNITZ , K LOFT, R IECK , & B REFELD

Wang, K., Parekh, J., & Stolfo, S. (2006). Anagram: content anomaly detector resistant mimicry
attack. Recent Adances Intrusion Detection (RAID), pp. 226248.
Warmuth, M. K., Liao, J., Rtsch, G., Mathieson, M., Putta, S., & Lemmen, C. (2003). Active
learning support vector machines drug discovery process. Journal Chemical
Information Computer Sciences, 43(2), 667673.
Yuan, C., & Casasent, D. (2004). Pseudo relevance feedback biased support vector machine.
Proc. International Joint Conference Neural Networks.
Zhang, D., & Lee, W. S. (2005). simple probabilistic approach learning positive
unlabeled examples. Proceedings 5th Annual UK Workshop . . . .

262

fiJournal Artificial Intelligence Research 46 (2013) 511-577

Submitted 12/12; published 03/13

Probabilistic Planning Continuous Dynamic Systems
Bounded Risk
Masahiro Ono

ONO @ APPI . KEIO . AC . JP

Keio University
3-14-1 Hiyoshi, Kohoku-ku
Yokohama, Kanagawa, 223-8522 Japan

Brian C. Williams

WILLIAMS @ MIT. EDU

Massachusetts Institute Technology
77 Massachusetts Avenue
Cambridge, 02139 USA

Lars Blackmore

LARS . BLACKMORE @ SPACEX . COM

SpaceX
1 Rocket Road
Hawthorne, CA 90250 USA

Abstract
paper presents model-based planner called Probabilistic Sulu Planner p-Sulu
Planner, controls stochastic systems goal directed manner within user-specified risk
bounds. objective p-Sulu Planner allow users command continuous, stochastic
systems, unmanned aerial space vehicles, manner intuitive safe.
end, first develop new plan representation called chance-constrained qualitative state
plan (CCQSP), users specify desired evolution plant state well
acceptable level risk. example CCQSP statement go B within 30
minutes, less 0.001% probability failure. develop p-Sulu Planner,
tractably solve CCQSP planning problem. order enable CCQSP planning, develop
following two capabilities paper: 1) risk-sensitive planning risk bounds, 2)
goal-directed planning continuous domain temporal constraints. first capability
ensures probability failure bounded. second capability essential planner
solve problems continuous state space vehicle path planning. demonstrate
capabilities p-Sulu Planner simulations two real-world scenarios: path planning
scheduling personal aerial vehicle well space rendezvous autonomous cargo
spacecraft.

1. Introduction
increasing need risk-sensitive optimal planning uncertain environments,
guaranteeing acceptable probability success. motivating example article
Boeing concept future aerial personal transportation system (PTS), shown Figure 1.
PTS consists fleet small personal aerial vehicles (PAV) enable flexible point-to-point
transportation individuals families.
c
2013
AI Access Foundation. rights reserved.

fiO , W ILLIAMS , & B LACKMORE

order provide safety, PTS highly automated. 2004, US, pilot error
listed primary cause 75.5% fatal general aviation accidents, according 2005 Joseph
T. Nall Report (Aircraft Owners Pilots Association Air Safety Foundation, 2005). Automated
path planning, scheduling, collision avoidance, traffic management significantly improve
safety PTS, well efficiency. challenges operating system include
adapting uncertainties environment, storms turbulence, satisfying
complicated needs users.
substantial body work planning uncertainty relevant. However,
approach distinctive three key respects. First, planner, p-Sulu Planner, allows
users explicitly limit probability constraint violation. capability particularly important risk-sensitive missions impact failure significant. Second, planner
goal-directed, mean achieves time-evolved goals within user-specified temporal constraints. Third, planner works continuous state space. continuous state space
representation fits naturally many real-world applications, planning aerial, space,
underwater vehicles. also important problems resources.

Figure 1: Personal Transportation System (PTS). (Courtesy Boeing Company)
Figure 2 shows sample PTS scenario. passenger PAV starts Provincetown,
wants go Bedford within 30 minutes. passenger also wants go scenic
area remain 5 10 minutes flight. no-fly zone (NFZ)
storm must avoided. However, storms future location uncertain; vehicles
location uncertain well, due control error exogenous disturbances. Thus risk
penetrating NFZ storm. passengers want limit risk 0.001%.
order handle planning problem, introduce novel planner called Probabilistic
Sulu Planner (p-Sulu Planner), building upon prior work model-based plan executive called
Sulu (Leaute & Williams, 2005). p-Sulu Planner provides following three capabilities,
order meet needs described scenario: 1) goal-directed planning continuous
domain, 2) near-optimal planning, 3) risk-sensitive planning risk bounds.
Goal-directed planning continuous domain p-Sulu Planner must plan actions
continuous effects achieve time evolved goals specified users. case PTS
scenario Figure 2, PAV must sequentially achieve two temporally extended goals, called
512

fiP ROBABILISTIC P LANNING C ONTINUOUS DYNAMIC YSTEMS B OUNDED R ISK

Figure 2: sample plan personal aerial vehicle (PAV)

episodes: going scenic area arriving Bedford. additional
temporal constraints goals inherent scenario; temporal constraints
come physical limitations, fuel capacity, others come passenger requirements.
Near-optimal stochastic planning Cost reduction performance improvement important issues system. PTS scenario, passengers may want minimize trip
time fuel usage. p-Sulu Planner finds near-optimal control sequence according
user-defined objective function, satisfying given constraints.
Risk-sensitive planning risk bounds Real-world systems subject various uncertainties, state estimation error, modeling uncertainty, exogenous disturbance.
case PAVs, position velocity vehicle estimated Kalman filter
typically involve Gaussian-distributed uncertainties; system model used planning
control perfect; vehicles subject unpredictable disturbances turbulence. uncertainty, executed result plan inevitably deviates
original plan hence involves risk constraint violation. Deterministic plan execution
particularly susceptible risk optimized order minimize given cost function,
since optimal plan typically pushes one constraint boundaries, hence
leaves margin error. example, shortest path PTS scenario shown Figure
2 cuts close NFZs storm, generally, constraint boundaries. Then,
tiny perturbation planned path may result penetration obstacles.
risk reduced setting safety margin path obstacles, cost
longer path length. However, often impossible guarantee zero risk, since typically non-zero probability disturbance large enough push vehicle
feasible region. Therefore, passengers vehicle must accept risk,
time need limit certain level. generally, users autonomous
system uncertainty able specify bounds risk. planner must
guarantee system able operate within bounds. constraints called
chance constraints.
513

fiO , W ILLIAMS , & B LACKMORE

1.1 Overview Planner
section describes inputs outputs p-Sulu Planner informally. rigorously
defined Section 2.
1.1.1 NPUTS
Initial Condition p-Sulu Planner plans control sequence starting current state,
typically estimated noisy sensor measurements. Therefore, p-Sulu Planner takes
probability distribution, instead point estimate, current state initial condition.
Stochastic Plant Model control community planning problem generate sequence
control inputs actuate physical system, called plant. action model plant
typically system real-valued equations control, state observable variables. pSulu Planner takes input linear stochastic plant model, specifies probabilistic state
transitions continuous domain. stochastic extension continuous plant model used
Leaute Williams (2005). paper limit focus Gaussian-distributed uncertainty.
Chance-constrained qualitative state plan (CCQSP) order provide users intuitive way command stochastic systems, develop new plan representation called chanceconstrained qualitative state plan (CCQSP). extension qualitative state plan (QSP), developed used Leaute Williams (2005), Hofmann Williams (2006), Blackmore,
Li, Williams (2006). CCQSP specifies desired evolution plant state time,
defined set discrete events, set episodes, impose constraints plant state
evolution, set temporal constraints events, set chance constraints specify
reliability constraints success sets episodes plan.
CCQSP may depicted directed acyclic graph, shown Figure 3. circles
represent events squares represent episodes. Flexible temporal constraints represented
simple temporal network (STN) (Dechter, Meiri, & Pearl, 1991), specifies upper lower
bounds duration two events (shown pairs numbers parentheses).
plan Figure 3 describes PTS scenario depicted Figure 2, stated informally as:
Start Provincetown, reach scenic region within 30 time units, remain
5 10 time units. end flight Bedford. probability
failure episodes must less 1%. times, remain safe region
avoiding no-fly zones storm. Limit probability penetrating
obstacles 0.0001%. entire flight must take 60 time units.
formal definition CCQSP given Section 2.4.3.
Objective function user p-Sulu Planner specify objective function (e.g., cost
function). assume convex function.
1.1.2 UTPUT
Executable control sequence p-Sulu Planner plans finite horizon. One two
outputs p-Sulu Planner executable control sequence horizon satisfies
constraints specified input CCQSP. case PTS scenario, outputs vehicles actuation inputs, acceleration ladder angle, result nominal paths shown
514

fiP ROBABILISTIC P LANNING C ONTINUOUS DYNAMIC YSTEMS B OUNDED R ISK

Figure 3: example chance-constrained qualitative state plan (CCQSP), new plan representation specify desired evolution plant state acceptable levels
risk. PTS scenario Figure 2, passengers PAV would like go
Provincetown Bedford, fly scenic region way. safe region
means entire state space except obstacles. Risk episodes must within
risk bounds specified chance constraints.

Figure 2. order control sequence executable, must dynamically feasible.
example, curvature PAVs path must exceed vehicles maneuverability.
Optimal schedule output p-Sulu Planner optimal schedule, set execution time steps events input CCQSP minimizes given cost function. case
PTS scenario shown Figure 3, schedule specifies leave scenic region
arrive Bedford, example. p-Sulu Planner finds schedule satisfies simple
temporal constraints specified CCQSP, minimizes cost function.
two outputs control sequence schedule must consistent other:
time-evolved goals achieved optimal schedule applying control sequence
given initial conditions.
1.2 Approach
p-Sulu Planner must solve difficult problem generating executable control sequence
CCQSP, involves combinatorial optimization discrete schedule non-convex
optimization continuous control sequence. approach article develop p-Sulu
Planner three technical steps, call spirals.
first spiral, described Section 4, solve special case CCQSP planning problem, feasible state space convex (e.g., path planning problem without obstacles)
schedule fixed, shown Figure 4-(a). problem transformed convex optimization problem risk allocation approach, presented previous work (Ono
& Williams, 2008a). obtain feasible, near-optimal solution CCQSP planning problem
optimally solving convex optimization using interior point method (Blackmore & Ono,
2009).
second spiral, presented Section 5, consider CCQSP problem
non-convex state space order include obstacles, Figure 4-(b). develop branch
bound-based algorithm, called non-convex iterative risk allocation (NIRA). Subproblems
branch-and-bound search NIRA convex chance-constrained optimal control problems,
solved first spiral. NIRA algorithm cannot handle problem flexible schedule.
515

fiO , W ILLIAMS , & B LACKMORE

third spiral, described Section 6, develop another branch boundbased algorithm, namely p-Sulu Planner, solve general CCQSP planning problem
flexible schedule obstacles. Subproblems branch-and-bound search pSulu Planner non-convex chance-constrained optimal control problems, solved
NIRA algorithm.
dp-SuluW (Section 6)

NIRA (Section 5)

(Ono & Williams 2008b) (Section 4)

Fixed schedule

Fixed schedule

t=5

Simple temporal
constraints

[2 4]

t=5
t=1

C

t=1

Goal

[1 3]

Goal

C

Obstacle
Start

Start

(a) Convex, fixed schedule

Obstacle

Waypoint

Waypoint

Waypoint
Start

(b) Non-convex, fixed schedule

Goal

[0 5]

(c) Non-convex, flexible schedule

Figure 4: Three-spiral approach CCQSP planning problem

1.3 Related Work
Recall CCQSP planning problem distinguished use time-evolved goals, continuous states actions, stochastic optimal solutions chance constraints. planning
control disciplines explored aspects problem, solution total novel,
approach solving problem efficiently risk allocation novel.
specifically, extensive literature planning discrete actions achieve
temporally extended goals (TEGs), TLPlan (Bacchus & Kabanza, 1998) TALPlan
(Kvarnstrom & Doherty, 2000), treat TEGs temporal domain control knowledge prune
search space progressing temporal formula. However, since TEG planners assume
discrete state spaces, cannot handle problems continuous states effects without discretization. Ignoring chance constraints, representation time evolved goals used TLPlan
p-Sulu Planner similar. TLPlan uses version metric interval temporal logic (MITL)
(Alur, Feder, & Henzinger, 1996) applied discrete states, p-Sulu Planner uses qualitative state plans (QSPs) (Leaute & Williams, 2005; Hofmann & Williams, 2006; Li, 2010)
continuous states. Li (2010) shows that, given state space, QSP expressed MITL.
key difference defines p-Sulu Planner addition chance constraints, together
use continuous variables.
Several planners, particularly employed components model-based executives, command actions continuous state space. example, Sulu (Leaute & Williams, 2005)
takes input deterministic linear model QSP, specifies desired evolution plant
state well flexible temporal constraints, outputs continuous control sequence. Chekhov
(Hofmann & Williams, 2006) also takes input QSP nonlinear deterministic system model,
outputs continuous control sequence. order enable fast real-time plan execution, Chekhov
precomputes flow tubes, sets continuous state trajectories end goal regions specified given plan. Kongming (Li, 2010) provides generative planning capability hybrid
516

fiP ROBABILISTIC P LANNING C ONTINUOUS DYNAMIC YSTEMS B OUNDED R ISK

systems, involving continuous discrete actions. employs compact representation
hybrid plans, called Hybrid Flow Graph, combines strengths Planning Graph
discrete actions flow tubes continuous actions. planners adapt effects uncertainty, explicitly reason effects uncertainty planning. example,
Sulu employs receding horizon approach, continuously replans control sequence using
latest measurements. Chekhovs flow tube representation feasible policies allows executive generate new control sequences response disturbances on-line. p-Sulu Planner
distinct continuous planners plans model uncertainty dynamics, instead reacting it. plan guarantees user-specified probability success explicitly
reasoning effects uncertainty.
AI planning literatures, planning domain description language, PDDL+, supports mixed
discrete-continuous planning domains (Fox & Long, 2006). Probabilistic PDDL (Younes & Littman,
2004) Relational Dynamic influence Diagram Language (RDDL) (Sanner, 2011) handle stochastic systems. Recently, Coles, Coles, Fox, Long (2012) developed forward-chaining
heuristic search planner named COLIN, deal continuous linear change durationdependent effects. However, planners handle chance constraints. note
outputs p-Sulu Planner continuous space discrete time. time-dependent MDP
developed Boyan Littman (2000) handle continuous time encoding time state.
Extension p-Sulu Planner continuous-time planning would interesting future direction.
work within AI community probabilistic planning focused planning discrete domains builds upon Markov decision process (MDP) framework. growing subcommunity focused extensions MDPs continuous domain. However, tractability
issue, since typically require partitioning approximation continuous state space.
straightforward partitioning continuous state action spaces discrete states actions often leads exponential blow-up running time. Furthermore, feasible state
space unbounded, impossible partition space finite number compact subspaces. alternative approach function approximation (Boyan & Moore, 1995),
convergence guaranteed approximation error bounded (Bertsekas & Tsitsiklis,
1996; Lagoudakis & Parr, 2003). Time-dependent MDPs (Boyan & Littman, 2000; Feng, Dearden,
Meuleau, & Washington, 2004) efficient partitioning continuous state space, make
assumption set available states actions finite (i.e., discrete). Hence, planning
MDPs continuous state space, Rn , requires approximate state space
finite number discrete states. approach essentially different MDP approaches
continuous variables directly optimized convex optimization without discretization continuous state space. Hence, continuity state space harm tractability
p-Sulu Planner.
second point comparison treatment risk. Like p-Sulu Planner, MDP
framework offers approach marrying utility risk. However, MDP algorithms balance
utility risk assigning large negative utility event constraint violation.
approach cannot guarantee bounds probability constraint violation. constrained MDP
approach (Altman, 1999) explicitly impose constraints. Dolgov Durfee (2005) showed
stationary deterministic policies constrained MDPs obtained solving mixed
integer linear program (MILP). However, constrained MDP framework impose bounds
expected value costs, again, cannot guarantee strict upper bounds probability
517

fiO , W ILLIAMS , & B LACKMORE

constraint violation. contrast, p-Sulu Planner allows users impose chance constraints,
explicitly restrict probability constraint violation. far authors know,
risk-sensitive reinforcement learning approach proposed Geibel Wysotzki (2005)
work considers chance constraints MDP framework. developed reinforcement
learning algorithm MDPs constraint probability entering error states. work
distinct p-Sulu Planner goal-directed, mean achieves
time-evolved goals within user-specified temporal constraints. summarize, prior MDP work
supports continuous state actions combination general continuous noise transitions
ensuring probability failure bounded.
Risk-sensitive control methods continuous domain extensively studied discipline control theory. example, celebrated H control method minimizes effect
disturbances output system guaranteeing stability system (Stoorvogel,
1992). Risk-sensitive control approaches allow users choose level risk averseness
minimization expected exponentiated cost function (Jacobson, 1973; Fleming & McEneaney, 1995). However, approaches address chance constraints optimal scheduling. Several methods proposed solving stochastic optimal control problems
continuous variables chance constraints. method proposed van Hessem (2004) turns
stochastic problem deterministic problem using conservative ellipsoidal relaxation.
Blackmore (2006) proposes sampling-based method called Particle Control, evaluates joint
chance constraints Monte-Carlo simulation, instead using conservative bound. result,
stochastic planning problem reduced MILP problem. Although theoretical guarantee obtain exactly optimal solution infinite number samples used,
computation time issue. Blackmore et al. (2006) Nemirovski Shapiro (2006) employed
Booles inequality decompose joint chance constraint individual chance constraints. Although Booles inequality less conservative ellipsoidal relaxation, approach still
non-negligible conservatism since fixes individual risk bound uniform value.
approach builds upon approach, modifications allow flexible individual risk bounds.
best authors knowledge, p-Sulu Planner first goal-directed planner
able plan continuous state space chance constraints.
1.4 Innovations
p-Sulu Planner enabled six innovations presented article.
First, order allow users command stochastic systems intuitively, develop new plan
representation, CCQSP (Section 2.4.3).
Second, order decompose chance constraint disjunctive clause disjunction
individual chance constraints, introduce risk selection approach (Section 5.1.2).
Third, order obtain lower bounds branch-and-bound search NIRA, develop
fixed risk relaxation (FRR), linear program relaxation subproblems (Section 5.4.2).
Fourth, minimize search space optimal schedule introducing new forward
checking method efficiently prunes infeasible assignment execution time steps (Section 6.2).
Fifth, order enhance computation time schedule optimization, introduce method
obtain lower bound branch-and-bound solving fixed-schedule planning problems
partial assignment schedule. (Section 6.3)
518

fiP ROBABILISTIC P LANNING C ONTINUOUS DYNAMIC YSTEMS B OUNDED R ISK

Sixth, order minimize number non-convex subproblems solved branch-andbound search, introduce variable ordering heuristic, namely convex-episode-first (CEF)
heuristic, explores episodes convex feasible state region ones
non-convex state region (Section 6.2.2).
rest article organized follows. Section 2 formally defines CCQSP
states CCQSP planning problem. Section 3 derives encoding problem chanceconstrained optimization problem, well encodings two limited versions CCQSP
planning problem: one fixed schedule convex state space, another fixed
schedule non-convex state space. Section 4 reviews solution fixed-schedule CCQSP
planning problem convex state space. Section 5 develops NIRA algorithm, solves
fixed-schedule CCQSP planning problem non-convex state space, Section 6 introduces
p-Sulu Planner, solves CCQSP planning problem flexible schedule nonconvex state space. Finally, Section 7 shows simulation results various scenarios, including
personal transportation system (PTS).

2. Problem Statement
Recall p-Sulu Planner takes input linear stochastic plant model, specifies
effects actions; initial state description, describing distribution initial states; CCQSP,
specifies desired evolutions state variables, well acceptable levels risk;
objective function. output executable control sequence optimal schedule. Planning
performed finite horizon, since p-Sulu Planner incorporated finite-horizon
optimal control. first define variables used problem formulations. define
elements inputs outputs.
2.1 Definition Time Step
consider series discretized finite time steps = 0, 1, 2, N fixed time interval
, integer N size planning horizon. Since time interval take
positive real value, suffices consider time steps integer indices approximate
systems dynamics. use term time step mean integer index discretized time
steps, using term time mean real-valued time. define sets follows:
:= {0, 1, 2, N }.


:= {0, 1, 2, N 1}.

(1)
(2)

limit scope article discrete-time stochastic system. optimizing
control sequence continuous-time stochastic system requires solving stochastic differential
equation (SDE) repeatedly. Performing computation tractable except simple
problems.
2.2 Definitions Events
event denotes start end episode behavior plan representation.
Definition 1. event e E instance executed certain time step T.
519

fiO , W ILLIAMS , & B LACKMORE

define two special events, start event e0 end event eE . Without loss generality,
assume e0 executed = 0. end event eE represents termination entire
plan.
2.3 Definitions Variables
Variables used problem formulation involve discrete schedule, continuous state vector,
continuous control vector.
formally define event well schedule follows:
Definition 2. execution time step s(e) integer-valued scalar represents
time step event e E executed. schedule := [s(e0 ), s(e1 ), s(eE )]
sequence execution time steps events e E. Finally, partial schedule :=
[(e) | e E E] ordered set execution time steps subset events E .
definition, start event executed = 0 i.e, s(e0 ) = 0. Following notation
schedule, denote (e) execution time event e E . See also definition
schedule (Definition 2).
consider continuous state space, state vector state sequence defined
follows:
Definition 3. state vector xt Rnx real-valued vector represents state plant
time step t. state sequence x0:N := [x0 xN ] vector state variables time step 0
N .
actions assignments continuous decision variables, referred control
vector:
Definition 4. control vector ut Rnu real-valued vector represents control input
system time step t. control sequence u0:N 1 := [u0 uN 1 ] vector control inputs
time 0 N 1.
2.4 Definitions Inputs
subsection defines four inputs p-Sulu Planner: initial condition, stochastic plant
model, CCQSP, objective function.
2.4.1 NITIAL C ONDITION
belief state beginning plan represented initial state, assumed
Gaussian distribution known mean x0 covariance matrix x0 :
x0 N (x0 , x0 ).

(3)

parameters (3) specified initial condition, defined follows:
Definition 5. initial condition pair = x0 , x0 , x0 mean initial state
x0 covariance matrix initial state.
520

fiP ROBABILISTIC P LANNING C ONTINUOUS DYNAMIC YSTEMS B OUNDED R ISK

2.4.2 TOCHASTIC P LANT ODEL
p-Sulu Planner controls dynamical systems actions correspond settings continuous control variables, whose effects continuous state variables. p-Sulu Planner
specifies actions effects plant model. plant model considered state
transition model continuous space. employ variant linear plant model additive
Gaussian uncertainty commonly used context chance-constrained stochastic optimal control (Charnes & Cooper, 1959; Nemirovski & Shapiro, 2006; Oldewurtel, Jones, & Morari,
2008; van Hessem, 2004), modification consider controller saturation. Specifically,
assume following plant model:
xt+1 = xt + B U (ut ) + wt

(4)

wt Rnx state-independent disturbance t-th time step zero-mean Gaussian
distribution given covariance matrix denoted wt :
wt N (0, wt ).

(5)

Although model prohibits state-dependent disturbance, types noise involved
target applications state independent. example, PTS scenario introduced Section
1, primary source uncertainty wind turbulence, typically dependent
state vehicle. space rendezvous scenario discussed Section 7.5, main sources
perturbations space craft tidal force unmodeled gravitational effects Sun, Moon,
planets (Wertz & Wiley J. Larson, 1999). noises modeled state-dependent
noise practice scale planned actions significantly smaller Solar
System.
dependent state space craft. note problem formulation encode
time-varying noise specifying different covariance matrices wt time step.
set U Rnu compact convex set represents continuous domain feasible
control inputs. infeasible control input ut
/ U given plant, actuators saturate.
function U () : Rnu 7 U (4) represents effect actuator saturation follows:
{
u
(if u U)
U (u) :=
,
PU (u) (otherwise)
PU (u) projection u U. example, u one-dimensional U = [l, u],
PU (u) = max(min(u, u), l). Note U introduces nonlinearity plant.
parameters (4) (5) specified stochastic plant model, defined
follows:
Definition 6. stochastic plant model four-tuple = A0:N 1 , B 0:N 1 , w0:N 1 , U,
A0:N 1 B 0:N 1 sets N matrices A0:N 1 := {A0 , A1 , 1 }, B 0:N 1 :=
{B 0 , B 1 , B N 1 }, w0:N 1 set N covariance matrices w0:N 1 = {w0 , w1 , , wN 1 },
U Rnu compact convex set represents domain feasible control inputs.
Note xt , well wt , random variable, ut deterministic variable. Figure
5 illustrates plant model. typical plant model, probability circles grow time since
disturbance wt added every time step, drawn figure. effect represents commonly
observed tendency distant future involves uncertainty near future.
521

fiO , W ILLIAMS , & B LACKMORE

x2

x3

99.9%
99%
90%

x1
x0

99.9%
99%

x1

99.9%
99%
90%

Nominal
path

x2

90%

x3

Figure 5: Illustration stochastic plant model used p-Sulu Planner.
order mitigate accumulation uncertainty, employ close-loop control approach,
generates control input ut incorporating nominal control input ut Rnu
error feedback, follows:
ut = ut + K (xt xt ),
(6)
K matrix representing constant stabilizing feedback gain time xt
nominal state vector. nominal state xt obtained following recursion:
x0 := x0
xt+1 = xt + B ut .

(7)
(8)

closed-loop control approach employed Geibel Wysotzki (2005) Oldewurtel
et al. (2008) context chance-constrained optimal control shown significantly
improves performance.
closed-loop planning method, nominal control input ut planned execution. actual control input ut computed real time using (6). feedback term
(6) linearly responds error xt xt . choosing feedback gain K appropriately,
growth probability circles Figure 5 slowed down. Neglecting effect controller
saturation (i.e., assuming U = Rnx ), follows (4) (6) xt Gaussian distribution
covariance matrix xt , evolves follows:
xt+1 = (At + B K )xt (At + B K )T + wt .

(9)

typical plant, eigenvalues one. Therefore, error feedback
(i.e., K = 0), size xt grows wt iteration. choosing K
norm largest eigenvalue (At + B K ) less one, covariance xt grow
continuously. feedback gain found using standard control techniques,
linear quadratic regulator (LQR) (Bertsekas, 2005). Since consider finite-horizon, discretetime planning problem, optimal time-varying LQR gain K obtained solving finitehorizon, discrete-time Riccati equation. practice, often suffices use steady-state (i.e.,
time-invariant) LQR gain, obtained solving infinite-horizon, discrete-time Riccati
equation simplicity. note feedback gain K also optimized real time.
approach often used robust stochastic model predictive controls (Goulart, Kerrigan, &
Maciejowski, 2006; Oldewurtel et al., 2008; Ono, 2012). However, extension beyond
scope paper.
522

fiP ROBABILISTIC P LANNING C ONTINUOUS DYNAMIC YSTEMS B OUNDED R ISK

issue that, error xt xt happens large, control input ut may exceed
feasible domain U, resulting actuator saturation. Therefore, (9) hold due
nonlinearity function U (). address issue risk allocation approach.
specifically, impose chance constraints control saturation, allocate risk state
control constraints. approach discussed detail Section 4.1.5.
2.4.3 C HANCE - CONSTRAINED Q UALITATIVE TATE P LAN (CCQSP)
qualitative state plan (QSP) (Leaute & Williams, 2005) temporally flexible plan specifies
desired evolution plant state. activities QSP called episodes specify
constraints plant state. CCQSP extension QSPs stochastic plans involve
chance constraints, defined follows:
Definition 7. chance-constrained qualitative state plan (CCQSP) four-tuple P = E, A, , C,
E set discrete events, set episodes, set simple temporal constraints,
C set chance constraints.
four elements CCQSP defined precisely moment. Like QSP, CCQSP
illustrated diagrammatically directed acyclic graph discrete events E
represented vertices, drawn circles, episodes arcs ovals. CCQSP start
event e0 end eE , corresponds beginning end mission, respectively.
example, Figure 3 shows CCQSP PTS scenario. state regions obstacles
CCQSP illustrated Figure 2. involves four events: E = {e0 , e1 , e2 , eE }. meanings
described follows.
1. start event e0 corresponds take PAV Provincetown.
2. second event e1 corresponds time step PAV reaches scenic region.
3. Event e2 associated time instant PAV left scenic region.
4. end event eE corresponds arrival PAV Bedford.
CCQSP four episodes = {a1 , a2 , a3 , a4 } two chance constraints C = {c1 , c2 }.
natural language expression CCQSP is:
Start Provincetown, reach scenic region within 30 time units, remain
5 10 time units. end flight Bedford. probability
failure activities must less 1%. times, remain safe region
avoiding no-fly zones storm. Limit probability penetrating
obstacles 0.0001%. entire flight must take 60 time units.
formally define three types constraints - episodes, temporal constraints,
chance constraint.
Episodes episode specifies desired state system control time
interval.

Definition 8. episode = eSa , eE
, (tS , tE ), Ra associated start event ea end
N
event eE
. Ra R region state space. set time steps state xt
must region Ra .

523

fiO , W ILLIAMS , & B LACKMORE

feasible region Ra subset RN . approximate Ra set linear
constraints later Section 3.1.1.
(tS , tE ) subset given function episodes start time step tS = s(eSa )
end time step tE = s(eE
). Different forms (tS , tE ) result various types episodes.
following three types episodes particularly interest us:
1. Start-in episode: (tS , tE ) = {tS }
2. End-in episode: (tS , tE ) = {tE }
3. Remain-in episode: (tS , tE ) = {tS , tS + 1, , tE }
given episode a, set time steps plant state must region Ra
obtained substituting s(eSa ) s(eE
), execution time steps start event end
event episode, tS and( tE . other) words, episode requires plant state
Ra time steps s(eSa ), s(eE
) . rest article, use following
abbreviated notation:
(
)
(s) := s(eSa ), s(eE
a) .
Using notation, episode equivalent following state constraint:

xt Ra .

(10)

ta (s)

example, CCQSP shown Figure 3, four episodes: a1 (Start [Provincetown]), a2 (Remain [Scenic region]), a3 (End Bedford), a4 (Remain [safe region]).
Section 6, solve relaxed optimization problem partial schedule (Definition 2)
order obtain lower bound optimal objective value. relaxed problems,
subset episodes relevant given partial schedule imposed. formally
define partial episode set partial schedule follows:
Definition 9. Given partial schedule , A() partial episode set, subset
involves episodes whose start event end event assigned execution time steps.
}
{
A() = | eSa E eE
E ,
definition E given Definition 2.
Chance constraint Recall chance constraint probabilistic constraint requires
constraints defining episode satisfied within user-specified probability. CCQSP
multiple chance constraints. chance constraint associated least one episode.
chance constraint formally defined follows:
Definition 10. chance constraint c = c , c constraint requiring that:



Pr
xt Ra 1 c ,

(11)

ac ta (s)

c user-specified risk bound c set episodes associated chance
constraint c.
524

fiP ROBABILISTIC P LANNING C ONTINUOUS DYNAMIC YSTEMS B OUNDED R ISK

Note every episode CCQSP must associated exactly one chance constraint.
episode must involved one chance constraint unassociated
chance constraint.
example, CCQSP shown Figure 3 two chance constraints, c1 c2 . associated episodes c1 = {a1 , a2 , a3 } c2 = {a4 }. Therefore, c1 requires probability
satisfying three episodes a1 , a2 , a3 (colored green) 99%, c2
requires probability satisfying episode a4 99.99999%.
make following assumption, necessary order guarantee convexity
constraints Section 4.2.
Assumption 1.
c 0.5
assumption requires risk bounds less 50%. claim assumption
constrain practical applications, since typically user autonomous system would
accept 50% risk.
Temporal constraint CCQSP includes simple temporal constraints (STCs) (Dechter et al.,
1991), impose upper lower bounds duration episodes temporal
distances two events E.
min max constraint, specifyDefinition 11. simple temporal constraint = eS , eE
, b , b
ing duration start event eS end event eE
real-valued interval
max ] [0, +].
[bmin
,
b



Temporal constraints represented diagrammatically arcs nodes, labeled
max ], labels episodes. example, CCQSP shown Figure 3
time bounds [bmin
, b
four simple temporal constraints. One requires time e0 e1 30
time units. One requires time e1 e2 least 5 units 10 units. One
requires time e2 eE 40 time units. One requires time e0
eE 60 time units.
schedule feasible satisfies temporal constraints CCQSP. number
feasible schedules finite, since discrete finite. denote SF domain feasible
schedules, formally defined follows:
SF = {s T|E| |


max
bmin
{s(eE
},

) s(e )} b

(12)

|E| number events CCQSP. temporal duration multiplied time
real-valued time, set discrete time steps T.
bmin
interval bmin


2.4.4 BJECTIVE F UNCTION
section, formally define objective function.
Definition 12. objective function J : UN XN SF 7 R real-valued function
nominal control sequence u0:N 1 , nominal state sequence x1:N , schedule s. assume
J convex function x1:N u0:N 1 .
525

fiO , W ILLIAMS , & B LACKMORE

typical example objective function quadratic sum control inputs, requires
total control efforts minimized:
J(u0:N 1 , x1:N , s) =

N
1


||ut ||2 .

t=0

Another example is:
J(u0:N 1 , x1:N , s) = s(eE ),

(13)

minimizes total plan execution time, requiring end event eE qualitative
state plan scheduled soon possible.
often need minimize expectation cost function. Note that, case,
expectation function x1:N u0:N 1 reduced function u0:N 1
follows (4)-(6) probability distributions x1:N u0:N 1 uniquely determined
u0:N 1 K . practice, often convenient express objective function
function u0:N 1 x1:N , rather function u0:N 1 . Since x1:N specified
u0:N 1 using (8), two expressions equivalent. conversion expectation cost
function function nominal values conducted priori.
controller saturation, conversion often obtained closed form.
conversion particularly straight forward cost function polynomial, since
expectation equivalent combination raw moments, readily derived
cumulants. Note third higher cumulants Gaussian distribution zero.
show examples conversion regarding three commonly-used cost functions: linear, quadratic,
Manhattan norm.
E[xt ] = xt
E[xTt Qxt ]

(14)
= xTt Qxt +

nx


E[||xt ||1 ] =

xt ,i

i=1

tr(Qxt )
(
)
x2t,i
2
1 1
,
1 F1 , ,

2 2 2x2t ,i

(15)
(16)

Q positive definite matrix, xt ,i ith diagonal element xt , 1 F1 ()
confluent hypergeometric function. functions convex. expectation function
ut also transformed function ut manner. Note second term
right hand side (15) constant. Hence, minimizing xTt Qxt yields solution
minimizing E[xTt Qxt ].
controller saturation, difficult obtain conversion closed-form due
nonlinearity U () (4). practice, use approximation assumes saturation.
Since closed-loop control approach explicitly limits probability controller saturation
small probability (see Section 4.1.5 detail), approximation error trivial. claim
empirically validated Section 7.2.4.
2.5 Definitions Outputs
output p-Sulu Planner optimal solution, consists optimal control sequence u0:N 1 UN optimal schedule SF .
526

fiP ROBABILISTIC P LANNING C ONTINUOUS DYNAMIC YSTEMS B OUNDED R ISK

Definition 13. optimal solution pair u0:N 1 , . solution satisfies constraints
given CCQSP (Definition 7), initial condition I, stochastic plant model .
solution minimizes given objective function J(u0:N 1 , x1:N , s) (Definition 12).
2.6 Problem Statement
formally define CCQSP planning problem.
Problem 1: CCQSP Planning Problem
Given stochastic plant model = A0:N 1 , B 0:N 1 , w0:N 1 , initial condition =
x0 , x0 , CCQSP P = E, A, , C, objective function J(u0:N 1 , x1:N , s), CCQSP
planning problem find optimal solution u0:N 1 , M, I, P , J.
note p-Sulu Planner gives near-optimal solution Problem 1. p-Sulu Planner
employs two approximations, namely risk allocation (Section 4.1.1) risk selection (Section
5.1.1), sake computational tractability. result, solution strictly optimal
general. However, empirically show Section 7 suboptimality due risk allocation
risk selection significantly smaller existing approximation methods.

3. Problem Encoding
section encodes CCQSP planning problem stated previous section mathematical programming problem. Sections 4 - 6 address solve form mathematical
problem. Recall build CCQSP planner, p-Sulu Planner, three spirals. first
present problem encoding general CCQSP planning problem non-convex state space
flexible schedule (Figure 4-(c)) Subsection 3.1. present encodings two
special cases CCQSP planning problem Subsections 3.2 3.3: one non-convex
state space fixed schedule (Figure 4-(b)), one convex state space fixed schedule (Figure 4-(a)).
3.1 Encoding CCQSP Planning Problem Non-convex State Space Flexible
Schedule
3.1.1 E NCODING F EASIBLE R EGIONS
order encode Problem 1 mathematical programming problem, geometric constraint
(11), xt Ra , must represented algebraic constraints. purpose, approximate
feasible state regions Ra set half-spaces, represented linear state
constraint.
Figure 6 shows two simple examples. feasible region (a) outside obstacle,
approximated triangle. feasible region (b) inside pickup region,
approximated triangle. feasible region approximated set linear constraints
follows:
(a)

3


hTi x g ,

(b)

i=1

3


hTi x g .

i=1

approximate feasible regions set linear constraints sufficient condition
original state constraint xt Ra .
527

fiO , W ILLIAMS , & B LACKMORE

Figure 6: Approximate representation feasible regions set linear constraints

assume set linear state constraints approximates feasible region
reduced conjunctive normal form (CNF) follows:




hTa,k,j xt ga,k,j 0,

(17)

kKa jJa,k

Ka = {1, 2, |Ka |} Jc,i = {1, 2, |Jc,i |} sets indices. replacing xt Ra
(11) (17), chance constraint c encoded follows:

Pr










hTc,a,k,j xt gc,a,k,j 0 1 c .

(18)

ac ta (s) kKa jJa,k

order simplify notation, merge indices c , (s),
k Ka new
index Ic (s), Ic (s) = {1, 2, |Ic (s)|} |Ic (s)| = |Ka | ac |a (s)|. let ai ,
ki , ti indices correspond combined index i, let hc,i,j = hc,ai ,ki ,j . Using
notations, three conjunctions (18) combined one, obtain following
encoding chance constraint:

Pr






hTc,i,j xti gc,i,j 0 1 c .

(19)

iIc (s) jJc,i

specification chance constraints given (19) requires |Ic (s)| disjunctive clauses
state constraints must satisfied probability 1 c . ith disjunctive clause cth
chance constraint composed |Jc,i | linear state constraints.
3.1.2 CCQSP P LANNING P ROBLEM E NCODING
Using (3), (4), (5), (6), (19), CCQSP planning problem (Problem 1), solved
third spiral, encoded follows:
528

fiP ROBABILISTIC P LANNING C ONTINUOUS DYNAMIC YSTEMS B OUNDED R ISK

Problem 2: General CCQSP Planning Problem
min
u0:N 1 ,s
s.t.

J(u0:N 1 , x1:N , s)

(20)

SF

(21)

xt+1 = xt + B U (ut ) + wt ,





(22)



ut = ut + K (xt xt ),




Pr
hTc,i,j xti gc,i,j 0 1 c .
cC

(23)
(24)

iIc (s) jJc,i

x0 N (x0 , x0 ),

wt N (0, wt ),



(25)

Recall SF , formally defined (12), set schedules satisfy temporal constraints given CCQSP. CCQSP execution problem hybrid optimization problem
discrete variables (schedule) continuous variables u0:N 1 (control sequence). Note
temporal constraints within Problem 2 solved Section 6. similar problem encoding
also employed chance-constraint MDP proposed Geibel Wysotzki (2005). However,
encoding differs Geibel Wysotzki two respects: 1) optimize continuous control sequence u0:N 1 also discrete schedule temporal constraints; 2)
allow joint chance constraints, require satisfaction multiple state constraints given
probability. Problem 2 solved Section 6.
3.2 Encoding CCQSP Planning Problem Non-convex State Space Fixed
Schedule
restricted version CCQSP planning problem fixed schedule, solved
second spiral, obtained fixing Problem 2 follows:
Problem 3: CCQSP Planning Problem Fixed Schedule
J (s) = min
u0:N 1
s.t.

J (u0:N 1 , x1:N )

(26)

xt+1 = xt + B U (ut ) + wt ,




ut = ut + K (xt xt ),




Pr
hTc,i,j xti gc,i,j 0 1 c ,
cC

(27)
(28)
(29)

iIc (s) jJc,i

x0 N (x0 , x0 ),

wt N (0, wt ),



(30)

J (s) optimal objective value CCQSP Planning problem schedule fixed
s. Note schedule s, decision variable Problem 2, treated constant
Problem 3. Therefore, objective function J function control sequence mean
529

fiO , W ILLIAMS , & B LACKMORE

state, since fixed schedule. Since assumed J convex function regarding
u0:N 1 x1:N , J also convex function. Section 5 solves Problem 3.
3.3 Encoding CCQSP Planning Problem Convex State Space Fixed Schedule
restrictive version CCQSP planning problem fixed schedule convex state
space, solved first spiral, obtained removing disjunctions chance
constraints Problem 3 follows:
Problem 4: CCQSP Planning Problem Fixed Schedule Convex State Space

min
u0:N 1

J (u0:N 1 , x1:N )

(31)

xt+1 = xt + B U (ut ) + wt ,




ut = ut + K (xt xt ),




Pr
hTc,i xti gc,i 0 1 c .
cC

(32)
(33)
(34)

iIc (s)

x0 N (x0 , x0 ),

wt N (0, wt ),



(35)

Section 4 solves Problem 4.

4. CCQSP Planning Convex State Space Fixed Schedule
section presents solution methods Problem 4, CCQSP planning problem
convex state space fixed schedule, shown Figure 4-(a). obstacles
environment execution time steps achieve time-evolved goals fixed, CCQSP
planning problem reduced convex chance-constrained finite-horizon optimal control problem.
past work presented risk allocation approach, conservatively approximates
chance-constrained finite-horizon optimal control problem tractable convex optimization
problem (Ono & Williams, 2008a, 2008b; Blackmore & Ono, 2009). Although optimal solution
approximated convex optimization problem exactly optimal solution original convex chance-constrained finite-horizon optimal control problem, suboptimality significantly smaller previous approaches. section gives brief overview risk allocation
approach, well solution convex chance-constrained finite-horizon optimal control
problem.
4.1 Deterministic Approximation Problem 4
Evaluating whether joint chance constraint (34) satisfied requires computing integral
multivariate probability distribution arbitrary region, since probability (34) involves
multiple constraints. integral cannot obtained closed form. address issue
decomposing intractable joint chance constraint (34) set individual chance constraints,
involves univariate probability distribution. key feature individual
530

fiP ROBABILISTIC P LANNING C ONTINUOUS DYNAMIC YSTEMS B OUNDED R ISK

Goal

Goal
Walls

Walls

Nominal path
Safety margin

Start

Start

(a) Uniform risk allocation

(b) Optimal risk allocation

Figure 7: Risk allocation strategies racing car example

chance constraint transformed equivalent deterministic constraint
evaluated analytically.
4.1.1 R ISK LLOCATION PPROACH
decomposition considered allocation risk. decomposition, risk
bound joint chance constraint distributed individual chance constraints.
many feasible risk allocations. problem find risk allocation results minimum
cost. offer readers intuitive understanding risk allocation approach using example
below.

Racing Car Example Consider racing car example, shown Figure 7. dynamics
vehicle Gaussian-distributed uncertainty. task plan path minimizes time
reach goal, guarantee probability crashing wall race less
0.1% (chance constraint). Planning control sequence equivalent planning nominal
path, shown solid lines Figure 7. limit probability crashing wall,
good driver would set safety margin, colored dark gray Figure 7, plan
nominal path outside safety margin.
driver wants set safety margin small possible order make nominal path
shorter. However, since probability crashing race bounded, certain
lower bound size safety margin. Given constraint, different ways
setting safety margin; Figure 7(a) width margin uniform; Figure 7(b) safety
margin narrow around corner, wide places.
intelligent driver would take strategy (b), since knows going closer wall
corner makes path shorter, straight line not. key observation
taking risk (i.e., setting narrow safety margin) corner results greater reward
(i.e. time saving) taking risk straight line. gives rise notion risk
allocation. good risk allocation strategy save risk reward small, taking
reward great. illustrated example, risk allocation must optimized
order minimize objective function joint chance-constrained stochastic optimization
problem.
531

fiO , W ILLIAMS , & B LACKMORE

4.1.2 ECOMPOSITION C ONJUNCTIVE J OINT C HANCE C ONSTRAINTS R ISK
LLOCATION
derive mathematical representation risk allocation reformulating chance constraint conjunction constraints conjunction chance constraints. reformulation
initially presented Prekopa (1999) introduced chance-constrained optimal control
Ono Williams (2008b). concept risk allocation originally developed Ono
Williams (2008a). Let Ci proposition either true false. following lemma
holds:
Lemma 1.
Pr

[N


]
Ci 1



N


0,

i=1

Pr [Ci ] 1

i=1

N




i=1

Proof.
Pr

[N


]
Ci 1 Pr

i=1





[N

i=1
N


]
Ci

(36)

[ ]
Pr Ci

(37)

cC i=1

0
0

N

i=1
N

i=1

N

[ ]
Pr Ci

i=1

Pr [Ci ] 1

N


.

(38)

i=1

overline C negation literal C. use following Booles inequality obtain (37)
(36):
]
[N
N


Pr[Cc,i ].
Pr
Cc,i
i=1

i=1

following result immediately follows Lemma 1 substituting linear constraint
hTc,i xti gc,i 0 Ci chance constraint c.
Corollary 1. following set constraints sufficient condition joint chance constraint
(34) Problem 4:





[
]
c,i 0
Pr hc,i xti gc,i 0 1 c,i
c,i c
(39)


cC

iIc (s)

iIc (s)

newly introduced variables c,i represent upper bounds probability violating
linear state constraint. refer individual risk bounds. individual risk bound,
532

fiP ROBABILISTIC P LANNING C ONTINUOUS DYNAMIC YSTEMS B OUNDED R ISK

c,i , viewed amount risk allocated ith clause. fact c,i bound
probability implies 0 c,i 1. second term (39) requires total amount risk
upper-bounded original risk bound c . find analogue resource allocation
problem, allocation resource optimized upper bound total amount
available resource. Likewise, allocation risk c,i must optimized order minimize
cost. Therefore, call decomposition method risk allocation.
4.1.3 C ONSERVATISM R ISK LLOCATION PPROACH
mentioned previously, risk allocation approach gives conservative approximation
original chance constraint. subsection evaluates level conservatism risk allocation
approach.
Let Pf ail true probability failure, defined probability solution violates
constraints (i.e., left hand side (34)). Since (39) sufficient necessary condition
(34), Pf ail smaller equal risk bound general: Pf ail . Hence,
conservatism introduced risk allocation represented
Pf ail .
best-case scenario risk allocation approach violations constraints
mutually exclusive, meaning solution violates one constraint always satisfies
constraints. case, (39) becomes necessary sufficient condition (34) hence,
risk allocation involve conservatism. Therefore,
Pf ail = 0.
hand, worst-case scenario constraints equivalent, meaning
solution violates one constraint always violates constraints. case,
Pf ail =

N 1
,
N

N number constraints.
practical problems lie somewhere best-case scenario worst-case scenario, typically closer best-case worst-case scenario. example,
two separate obstacles path planning problem, collisions two obstacles mutually
exclusive events. Collision obstacle one time step usually imply collisions
time steps. rough approximation real-world situation assume satisfaction constraints probabilistically independent. assumption, true probability
failure is:


Pf ail =
Pr [qc,i (u) 0] 1
(1 ),
iIc

iIc

Ic set index state constraints. Note . Therefore, conservatism introduced risk allocation second order :
Pf ail O(2 ).
example, = 1%, true probability failure approximately Pf ail 0.99%.
practical cases, users prefer set small risk bounds, typically less 1%. cases,
conservatism introduced risk allocation becomes small.
533

fiO , W ILLIAMS , & B LACKMORE

4.1.4 C ONVERSION ETERMINISTIC C ONSTRAINTS
individual chance constraint (39) involves single linear constraint. Furthermore,
assuming actuator saturation, xti Gaussian distribution covariance
matrix given (9). Hence, hTc,i xti univariate Gaussian distribution. following lemma
transforms individual chance constraint equivalent deterministic constraint involves
mean state variables, instead random state variables:
Lemma 2. following two conditions equivalent.
[
]
Pr hTc,i xti gc,i 0 1 c,i hTc,i xti gc,i mc,i (c,i )

mc,i (c,i ) =



2hTc,i x,ti hc,i erf 1 (2c,i 1).

(40)

Note erf1 inverse Gauss error function x,ti covariance matrix
xti . lemma holds mc,i () inverse cumulative distribution function
univariate, zero-mean Gaussian distribution variance hTc,i x,ti hc,i .
4.1.5 R ISK LLOCATION PPROACH C LOSED - LOOP C ONTROL P OLICY
close-loop control policy employed (i.e., K = 0 (6)), risk actuator
saturation. Since nonlinearity function U () (5) makes probability distribution
xti non-Gaussian, mc,i () cannot obtained (40). Although theoretically possible
derive mc,i () non-Gaussian distributions, difficult case since inverse
cumulative distribution function xti cannot obtained closed-form.
solution issue summarized Lemma 3 below, allows us assume xti
Gaussian-distributed hence use (40), even possibility actuator saturation.
approach enabled imposing additional chance constraints bound risk actuator
saturation follows:
Pr [ut U] 1 , ,
(41)
bound risk actuator saturation time step t. Using method presented
Section 3.1.2, approximate U polytope follows:

ut U
hU,i ut gU,i 0
iIU

Assuming xti Gaussian-distributed, use Lemma 2 transform (41) deterministic
constraints nominal control inputs follows:


hU,i ut gU,i mU,t,i (t,i )
t,i , ,
(42)
iIU

iIU


mU,t,i (c,i ) =



2hTU,i x,t hU,i erf 1 (2c,i 1).

following lemma holds:
534

(43)

fiP ROBABILISTIC P LANNING C ONTINUOUS DYNAMIC YSTEMS B OUNDED R ISK

Lemma 3. following set constraints sufficient condition joint chance constraint
(34) Problem 4:


c,i 0, 0
hTc,i xti gc,i mc,i (c,i )

cC iIc (s)

Tcmax




c,i +
t,i c

t=0 iIU
iIc (s)


hU,i ut gU,i mU,t,i (t,i ),
(44)
tT iIU

(45)
mc,i () mU,t,i given (40) (43). Tcmax last time step episodes
associated chance constraint c executed, given schedule s:
Tcmax = max s(eE
).
ac

Intuitively, constraint (44) requires that, probability 1 c , episode constraints
satisfied actuators saturate episodes associated c executed.
Proof. consider two plants: = A0:N 1 , B 0:N 1 , w0:N 1 , U
= A0:N 1 , B 0:N 1 , w0:N 1 , Rnu , U Rnu compact convex set (see Definition
6). difference two plants possibility actuator saturation,
not. result, probability distribution state variables non-Gaussian,
Gaussian. Note result different probability distributions xti
ut . order explicitly show plant model considered, use notations xM
ti


ut proof.
first consider . follows Lemmas 1 2 that:


max

Tc










Pr
hTc,i xM

g

0

u

U

1


.
(44) =
c,i
c
ti



cC

t=0

iIc (s)

Let w0:N 1 := [w0 wN 1 ]. define feasible disturbance set, Wc (v 0:N 1 , s) RN nx ,
follows:


max
fi
Tc

fi





fi




hTc,i xM
u

U
.

g

0

Wc (v 0:N 1 , s) := w0:N 1 RN nx fi
c,i
ti

fi


iIc (s)

t=0

(46)
Then, definition,


max
Tc





U = Pr [w0:N 1 Wc (v 0:N 1 , s)] .
Pr
hTc,i xM
uM
ti gc,i 0

iIc (s)

t=0

535

fiO , W ILLIAMS , & B LACKMORE

Next consider . Note identical long actuator saturations

(i.e., uM
U). Therefore, given w 0:N 1 Wc (v 0:N 1 , s), follows (46) xt =




xM
ut = ut . Hence,

max

Tc




w0:N 1 Wc (v 0:N 1 , s) =
hTc,i xM
uM
ti gc,i 0
U .
t=0

iIc (s)

Accordingly, given c C,




hTc,i xM
Pr
ti gc,i 0
iIc (s)





Pr





Tcmax


hTc,i xM
ti gc,i 0





uM
U

t=0

iIc (s)

Pr [w0:N 1 Wc (v 0:N 1 , s)]
max


Tc





uM
U
= Pr
hTc,i xM

ti gc,i 0
t=0

iIc (s)

1 c .
completes proof Lemma 3
note Lemma 3 probabilistic extension closed-loop robust model predictive
control (RMPC) methods proposed Acikmese, Carson III, Bayard (2011) Richards
(2006). methods avoid risk actuator saturation imposing tightened control
constraints ut . Since consider stochastic uncertainty, replace constraint tightening
chance constraints.
4.2 Convex Programming Solution Problem 4
Using Lemma 3, replace stochastic optimization problem, Problem 4, deterministic
convex optimization problem:
Problem 5: Deterministic Approximation Problem 4
min
u1:N ,c,i 0,t,i 0
s.t.

J (u1:N , x1:N )
,


(47)

xt+1

hc,i xti

= xt + B ut

(48)

gc,i mc,i (c,i )

(49)

cC iIc (s)



tT

hU,i ut gU,i mU,t,i (t,i )

(50)

iIU



Tcmax

c,i +

cC iIc (s)

536


t=0 iIU

t,i c .

(51)

fiP ROBABILISTIC P LANNING C ONTINUOUS DYNAMIC YSTEMS B OUNDED R ISK

follows immediately Corollaries 1 2 feasible solution Problem 5 always
feasible solution Problem 4. Furthermore, Blackmore Ono (2009) showed optimal
solution Problem 5 near-optimal solution Problem 4. following lemma guarantees
tractability Problem 5.
Lemma 4. Problem 5 convex optimization problem.
Proof. inverse error function erf1 (x) concave x. Since assume Section 2.4.3
c 0.5, feasible ranges upperbounded 0.5. Since safety margin function
mc,i (c,i ) mU,t,i (t,i ) convex 0 < c,i 0.5 0 < t,i 0.5, constraints (49)
(50) convex within feasible region. constraints also convex since
linear. Finally, objective function convex assumption (Section 2.4.4). Therefore, Problem
5 convex optimization problem.
Since Problem 5 convex optimization problem, solved interior point method
optimally efficiently. completes first spiral, planning CCQSPs fixed schedule
convex constraints. next section present solution method non-convex problem
branch-and-bound algorithm, whose subproblems convex problems.

5. CCQSP Planning Non-convex State Space
Next, consider second spiral, comprised Problem 3 Section 3.2, variant CCQSP
planning problem involves fixed schedule non-convex constraints, obstacles,
shown Figure 4-(b). again, encoded chance-constrained optimization problem,
addition obstacle avoidance constraints requires disjunctive state constraints. Hence,
problem results non-convex, chance-constrained optimization. section introduces
novel algorithm, called Non-convex Iterative Risk Allocation (NIRA), optimally solves deterministic approximation Problem 3.
solution CCQSP planning problem non-convex state space two-fold.
first step, described Section 5.1, obtain deterministic approximation Problem 3. order
handle disjunctive chance constraints, develop additional decomposition approach called
risk selection, reformulates chance constraint disjunction constraints disjunction individual chance constraints. chance constraints (29) decomposed
set individual chance constraints risk allocation risk selection, technique
Section 4.1.4 used obtain equivalent deterministic constraints. result, obtain
disjunctive convex programming problem (Problem 6 Section 5.1.3).
deterministic disjunctive convex programming problem solved second step, described Sections 5.2-5.4. introduce NIRA algorithm (Algorithm 1) significantly reduces computation time without making compromise optimality solution.
reduction computation time enabled new bounding approach, Fixed Risk Relaxation
(FRR). FRR relaxes nonlinear constraints subproblems branch-and-bound algorithm
linear constraints. many cases, FRR nonlinear subproblems formulated linear
programming (LP) approximated LP. NIRA obtains strictly optimal solution Problem
6 solving subproblems exactly without FRR unpruned leaf nodes search tree,
subproblems solved approximately FRR order reduce computation time.
537

fiO , W ILLIAMS , & B LACKMORE

5.1 Deterministic Approximation
Section 4, first obtain deterministic approximation Problem 3.
5.1.1 R ISK ELECTION PPROACH
deterministic approximation obtained decomposing non-convex joint chance constraint
(29) set individual chance constraints, risk allocation risk selection. revisit
race car example explain concept risk selection intuitively.

Figure 8: racing car example, risk selection approach guarantees 0.1% risk bound
paths, lets vehicle choose better one.

Racing Car Example consider example shown Figure 8, vehicle uncertain
dynamics plans path minimizes time reach goal. vehicle allowed choose
one two routes shown Figure 8. impose chance constraint limits probability
crashing wall mission 0.1%.
satisfaction chance constraint guaranteed following process. First,
routes, find safety margin limits probability crash throughout route
0.1% start goal. Then, let vehicle plan nominal path operates within
safety margins. Since routes 0.1% safety margin, chance constraint satisfied
matter route vehicle chooses. Therefore, vehicle optimize path choosing
route results smaller cost. optimization process considered selection
risk; vehicle given two options Figure 8, routes (a) (b), involve
level risk; vehicle selects one results less cost. Hence, name
decomposition approach risk selection.
5.1.2 ECOMPOSITION C ONJUNCTIVE J OINT C HANCE C ONSTRAINT R ISK
ELECTION
subsection, derive mathematical representation risk selection. Let Ci proposition either true false. following lemma holds:

538

fiP ROBABILISTIC P LANNING C ONTINUOUS DYNAMIC YSTEMS B OUNDED R ISK

Lemma 5.
Pr

[N


]
Ci 1

i=1

N


Pr [Ci ] 1

i=1

Proof. following inequality always holds:


Pr

[N


]
Ci Pr [Ci ] .

(52)

i=1

Hence,

Pr

[N


]
Ci 1 Pr [Ci ] 1

i=1

N


Pr [Ci ] 1 .

(53)

i=1

following corollary follows immediately Lemmas 3 5.
Corollary 2. following set constraints sufficient condition disjunctive joint chance
constraint (29) Problem 3:

c,i 0, 0



cC







iIc (s) jJc,i



Tcmax

c,i +



tT


t=0 iIU

iIc (s)



hTc,i,j xti gc,i,j mc,i (c,i )

t,i c





5

hU,i ut gU,i mU,t,i (t,i ).

(54)

iIU

Note resulting set constraints (54) sufficient condition original chance
constraint (29). Therefore, solution satisfies (54) guaranteed satisfy (29). Furthermore,
although (54) conservative approximation (29), conservatism introduced risk selection
generally small many practical applications. claim empirically validated Section
7.2.3.
5.1.3 ETERMINISTIC PPROXIMATION P ROBLEM 3
Using Corollary 2, non-convex fixed-schedule CCQSP planning problem (Problem 3) approximated following deterministic convex optimization problem. later convenience,
label part optimization problem (objective function), (plant model), C (chance
constraints states), (chance constraints control inputs), R (risk allocation constraint).
539

fiO , W ILLIAMS , & B LACKMORE

Problem 6: Deterministic Approximation Problem 3
min
u1:N ,c,i 0,t,i 0
s.t.

(O :)

J (u1:N , x1:N )

(55)

(M :) , xt+1 = xt + B ut

(C :)
hTc,i,j xti gc,i,j mc,i,j (c,i )

(56)
(57)

cC iIc (s) jJc,i

(D :)



hU,i ut gU,i mU,t,i (t,i )

(58)

tT iIU

(R :)



Tcmax

c,i +

cC iIc (s)



t,i c .

(59)

t=0 iIU

follows immediately Corollary 2 optimal solution Problem 6 guaranteed
feasible solution original problem regard satisfying chance constraints
(Problem 3). Furthermore, empirically demonstrate Section 7.2.3 near-optimal
solution Problem 3 applications.
5.2 NIRA: Branch Bound-Based Solution Problem 6
next present Non-convex Iterative Risk Allocation (NIRA) algorithm. Recall NIRA
optimally solves Problem 6 branch-and-bound algorithm. standard branch-and-bound
solution problems involving disjunctive nonlinear constraints, Problem 6,
use bounding approach nonlinear convex relaxed subproblems constructed
removing non-convex constraints corresponding disjunction. approach
used Balas (1979) Li Williams (2005) different problem known disjunctive linear
programming, whose subproblems LPs instead convex programmings. However, although
standard branch-and-bound algorithm guaranteed find globally optimal solution Problem 6,
computation time slow algorithm needs solve numerous nonlinear subproblems
order compute relaxed bounds.
new bounding approach, Fixed Risk Relaxation (FRR), addresses issue computing
lower bounds efficiently. observe relaxed subproblems nonlinear convex optimization problems. FRR relaxes nonlinear constraints linear constraints. Particularly,
objective function linear, FRR subproblem (Problem 8) LP,
efficiently solved. optimal objective value FRR subproblem lower bound
optimal objective value original subproblem.
NIRA solves FRRs subproblems order efficiently obtain lower bounds,
solving original subproblems exactly without relaxation unpruned leaf nodes order obtain
exact optimal solution. result, NIRA achieves significant reduction computation time,
without loss optimality.
5.2.1 NIRA LGORITHM OVERVIEW
Algorithm 1 shows pseudocode NIRA algorithm. input deterministic approximation non-convex chance-constrained optimal control problem (Problem 6), five-tuple
540

fiP ROBABILISTIC P LANNING C ONTINUOUS DYNAMIC YSTEMS B OUNDED R ISK

Algorithm 1 Non-convex Iterative Risk Allocation (NIRA) algorithm
function NIRA(problem) returns optimal solution Problem 6
1: Set queue FILO queue
2: Incumbent
3: rootSubproblem obtainRootSubproblem(problem)
4: queue rootSubproblem
5: queue empty
6:
subproblem last entry queue
7:
Remove subproblem queue
8:
lb obtainLowerBound(subproblem)
9:
lb Incumbent
10:
c = |C| = |Ic (s)|
11:
(J, U ) Solve(subproblem)
12:
J < Incumbent

13:
Incumbent J, U U //Update optimal solution
14:
end
15:
else
16:
ii+1
17:
> |Ic (s)|
18:
c c + 1, 1
19:
end
20:
j Jc,i
21:
newSubproblems Expand(subproblem,problem,c,i,j)
22:
Add newSubproblems queue
23:
end
24:
end
25:
end
26: end

27: return U

O, M, C, D, R, well fixed schedule s. output optimal nominal control sequence

U := [u0 uN 1 ].
node branch-and-bound search tree corresponds subproblem convex
chance-constrained optimization problem (Problem 5). use FILO queue store subproblems
search conducted depth-first manner (Line 1). node, corresponding
subproblem solved obtain lower bound objective value subsequent subproblems
(Line 8). details bounding approaches explained Subsection 5.4. lower bound
larger incumbent, algorithm prunes branch. Otherwise, branch expanded
(Line 21). branch expanded leaf without pruned, subproblems solved exactly
(Line 11). Subsection 5.3 explains expansion procedure detail. NIRA algorithm always

results globally optimal solution Problem 6, since solution U obtained solving
subproblems leaf nodes exactly. next two subsections introduces branching bounding
methods.
541

fiO , W ILLIAMS , & B LACKMORE

5.3 Branching
subsection explains NIRA constructs root subproblem (Line 3 Algorithm 1),
well expands nodes (Line 21 Algorithm 1). root subproblem convex
optimal CCQSP planning problem without chance constraints. node expanded,
subproblems children nodes constructed adding one constraint disjunction
subproblem parent node. order simplify notations, let Cc,i,j represent individual
chance constraint (57) Problem 6:
{
rue (if hTc,i,j gc,i,j xti mc,i,j (c,i ))
Cc,i,j :=
F alse (otherwise).
5.3.1 WALK - E XAMPLE
first present walk-through example intuitively explain branching procedure. example instance Problem 6, involves four individual chance constraints:


hT1,i,j xti g1,i,j m1,i,j (1,i )
(60)
i{1,2} j{1,2}

Using notation defined above, set individual chance constraints (57) represented
follows:
(C1,1,1 C1,1,2 ) (C1,2,1 C1,2,2 )

(61)

Figure 9-(a) shows tree obtained dividing original problem subproblems sequentially.
subproblems corresponding trees four leaf nodes (Nodes 4-7 Figure 9-(a)) exhaust
conjunctive (i.e., convex) combinations among chance constraints (61). hand,
subproblems corresponding three branch nodes (Nodes 1-3 Figure 9-(a)) involve disjunctive
(i.e., nonconvex) clauses chance constraints. relax non-convex subproblems convex
subproblems removing clauses contain disjunctions order obtain search tree
shown Figure 9-(b).

Figure 9: Branch-and-bound search tree sample disjunctive convex programming problem
(Problem 6) constraints (60). (a) Tree non-convex subproblems, (b) Tree relaxed convex subproblems.

542

fiP ROBABILISTIC P LANNING C ONTINUOUS DYNAMIC YSTEMS B OUNDED R ISK

non-convex problem (Problem 6) optimally solved repeatedly solving relaxed
convex subproblems using algorithms presented Section 4. following subsections introduce algorithms construct search tree relaxed convex subproblems, one
Figure 9-(b).
5.3.2 R ELAXED C ONVEX UBPROBLEM
formulation relaxed convex subproblems given Problem 7. represent index
j j(c, i) since convex relaxation chooses one disjunct disjunction specified
optimal objective value relaxed
(c, i). Let Ic set indices i. denote JSP
subproblem.
Problem 7: Convex Relaxed Subproblem NIRA


JSP
=

min
u1:N ,c,i 0,t,i 0
s.t.

(O :) J (u1:N , x1:N )
(M :)
(C :)

, xt+1 = xt + B ut

hTc,i,j(c,i) xti gc,i,j(c,i) mc,i,j(c,i) (c,i ) (62)
cC iIc

(D :)



hU,i ut gU,i mU,t,i (t,i )

(63)

tT iIU

(R :)



Tcmax

c,i +

cC iIc



t,i c .

(64)

t=0 iIU

Note Problem 7 identical Problem 5. Hence, algorithms introduced Section 4
used solve relaxed subproblems.
5.3.3 C ONSTRUCTION ROOT UBPROBLEM
root subproblem special case Problem 7 Ic empty set c C.
function presented Algorithm 2 used Line 3 NIRA algorithm (Algorithm 1)
construct root subproblem branch-and-bound tree. Note that, Algorithm 2,
use object-oriented notation, subproblem.O, represent objective function
subproblem. resulting root subproblem follows:
5.3.4 E XPANSION UBPROBLEMS
order create child subproblem subproblem, function described Algorithm 3
used Line 21 NIRA algorithm (Algorithm 1). adds individual chance constraint
specified indices (c, i, j) conjunct. Note resulting child subproblem still
convex optimization, individual chance constraint added conjunctively. NIRA
algorithm (Algorithm 1) enumerates children nodes disjuncts Jc,i (Lines 20-23).
543

fiO , W ILLIAMS , & B LACKMORE

Algorithm 2 Construction root subproblem NIRA
function obtainRootSubproblem(problem) returns root subproblem
1: rootSubproblem.O problem.O
2: rootSubproblem.M problem.M
3: rootSubproblem.D problem.D
4: c C
5:
rootSubproblem.Ic max
Tc
6:
rootSubproblem.Rc .lhs t=0
iIU t,i
7:
rootSubproblem.Rc .rhs problem.Rc .rhs
8: end
9: return rootSubproblem
Algorithm 3 Expansion subproblem NIRA
function
Expand(subproblem, problem, c, i, j)
lem
1: subproblem.Ic subproblem.Ic
2: subproblem.Rc .lhs subproblem.Rc .lhs + c,i
3: return subproblem

returns



child

subprob-

5.4 Bounding
subsection, present two implementations obtainLowerBound function Line 8
Algorithm 1. first one uses optimal solution convex subproblems (Problem 7)
lower bounds. approach typically results extensive computation time. second one solves
LP relaxation convex subproblems, called fixed risk relaxation (FRR). FRR dramatically
reduces computation time compared first implementation. NIRA algorithm employs
second implementation.
5.4.1 IMPLE B OUNDING
Algorithm 4 shows straightforward way obtain lower bounds. simply solves
convex relaxed subproblems (Problem 7) using methods presented Section 4.2. optimal
objective value relaxed subproblem gives lower bound optimal objective value
subproblems it. example, optimal solution relaxed subproblem Node 2
Figure 9-(b) gives lower bound objective value subproblems Nodes 4 5.
constraints relaxed subproblems always subset constraints
subproblems below. Note optimization problems formulated minimizations.
However, despite simplicity approach, computation time slow algorithm needs solve myriad subproblems. example, simple path planning problem
Algorithm 4 simple implementation obtainLowerBound function Line 8 Algorithm 1
function obtainLowerBound-Naive(subproblem) returns lower bound
1: Solve subproblem using algorithms presented Section 4.2
2: return optimal objective value

544

fiP ROBABILISTIC P LANNING C ONTINUOUS DYNAMIC YSTEMS B OUNDED R ISK

ten time steps one rectangular obstacle requires solution 410 = 1, 048, 576 worst
case, although branch-and-bound process often significantly reduces number subproblems
solved. Moreover, subproblems (Problem 7) nonlinear convex optimization problems
due nonlinearity mc,i,j mU,t,i (62) (63). general nonlinear optimization problem requires significantly solution time specific classes optimization problems,
linear programmings (LPs) quadratic programmings (QPs).
5.4.2 F IXED R ISK R ELAXATION
new relaxation approach, fixed risk relaxation (FRR), addresses issue. FRR linearizes
nonlinear constraints (62) (63) Problem 7 fixing individual risk allocations,
c,i t,i , upper bound . objective function linear, FRR LP.
FRR convex piecewise linear objective function also reformulated LP
introducing slack variables (See Section 7.1.1 example.). general convex objective function
approximated convex piecewise linear function. Hence, many cases, FRRs
subproblems result LPs, solved efficiently. fixed risk relaxation Problem
7 follows:
Problem 8: Fixed Risk Relaxation Problem 7

JF RR = min
u1:N
s.t.

J (u1:N , x1:N )
,


xt+1 = xt + B ut
hTc,i xti gc,i mc,i,j(c,i) (c )

(65)

cC iIc (s)



hU,i ut gU,i mU,t,i (c )

(66)

tT iIU

Note nonlinear terms (62) (63), mc,i,j mU,t,i , become constant fixing c,i
t,i c , constant. optimal objective value FRR provides tightest lower
bound among linear relaxations constraints (62) (63). following lemmas hold:
Lemma 6. Problem 8 gives lower bound optimal objective value Problem 7:

JF RR JSP

Proof. mc,i,j () mU,t,i () monotonically decreasing functions. Since c,i c t,i
c , individual chance constraints (65) (66) Fixed Risk Relaxation less stricter
first conjunct (62) (63). Therefore, cost optimal solution Fixed Risk
Relaxation less equal original subproblem.
Lemma 7. FRR gives tightest lower bound among linear relaxations constraints (62)
(63).
Proof. linear relaxation (62) (63) becomes tighter fixing c,i t,i lesser value.
However, setting c,i t,i values less c may exclude feasible solutions, one
545

fiO , W ILLIAMS , & B LACKMORE

Algorithm 5 FRR implementation obtainLowerBound function Line 8 Algorithm 1
function obtainLowreBound-FRR(subproblem) returns lower bound
1: (c, i, j) subproblem.C
2:
subproblem.Cc,i,j .rhs mc,i,j (c ) //Apply fixed risk relaxation
3: end
4: (t, i)
5:
subproblem.Dt,i .rhs mU,t,i //Apply fixed risk relaxation
6: end
7: Remove subproblem.R
8: Solve subproblem using LP solver
9: return optimal objective value

sets c,i = c (c, i). Hence, FRR tightest linear relaxation (62) (63),
resulting tightest lower bound.
Note optimal solution Fixed Risk Relaxation (Problem 8) typically infeasible
solution Problem 7, since setting c,i = t,i = c violates constraint (64).
Algorithm 5 implements fixed risk relaxation. LP relaxation solved LP solver,
optimal objective value returned.
completes second spiral, planning CCQSPs fixed schedule nonconvex
constraints. next section, turn final spiral, involves flexible temporal constraints.

6. CCQSP Planning Flexible Schedule
section presents complete p-Sulu Planner, efficiently solves general CCQSP
planning problem flexible schedule non-convex state space (Problem 2 Section
3.1.2). problem find schedule events satisfies simple temporal constraints,
well nominal control sequence u0:N 1 satisfies chance constraints minimizes cost.
approach first generate feasible schedule extend control sequence
schedule, iteratively improving candidate schedules using branch-and-bound.
build p-Sulu Planner upon NIRA algorithm presented previous section. Recall
NIRA optimizes nominal control sequence u0:N 1 given fixed schedule s. p-Sulu
Planner uses NIRA subroutine takes schedule input, outputs optimal
objective value well executable control sequence. denote optimal objective value
given schedule J (s). Using notation, CCQSP planning problem flexible
schedule (Problem 2) rewritten schedule optimization problem follows:
min J (s).

sSF

(67)

Recall domain feasible schedules SF (Definition 11) finite set, since consider
discretized, finite set time steps (see Section 2.1). Hence, schedule optimization problem
(67) combinatorial constraint optimization problem, constraints given form
simple temporal constraints.
546

fiP ROBABILISTIC P LANNING C ONTINUOUS DYNAMIC YSTEMS B OUNDED R ISK

Algorithm 6 p-Sulu Planner
function pSulu(ccqsp) returns optimal schedule control sequence
1: Incumbent =
2: Set queue FILO queue
3: E0 = {e0 }, 0 (e0 ) = 0
//initialize partial schedule
4: queue E0 , 0
5: queue empty
6:
E , last entry queue
7:
Remove E , queue
8:
[J , u0:N 1 ] obtainLowerBound(ccqsp, E , )
9:
J < Incumbent
10:
E = E
11:
Incumbent J , OptCtlSequence u0:N 1 , OptSchedule
12:
else
13:
expand(ccqsp, queue, e, E , )
14:
end
15:
end
16: end
17: return OptCtlSequence, OptSchedule

6.1 Algorithm Overview
solution approach use branch-and-bound algorithm. branch-and-bound
search, p-Sulu Planner incrementally assigns execution time step event order
find schedule minimizes J (s) (67). objective function evaluated solving
fixed schedule CCQSP planning problem using NIRA algorithm. Although combination
two branch-and-bound searches p-Sulu Planner NIRA equivalent one unified
branch-and-bound search practice, treat separately ease explanation.
shown Figure 12, branch-and-bound algorithm searches optimal schedule
incrementally assigning execution time steps event depth-first manner. node
search tree corresponds partial schedule (Definition 2), assigns execution time steps
subset events included CCQSP. partial schedule root node involves
assignment start node e0 . tree expanded assigning execution time step one new
event time. example, node (e1 ) = 2 Figure 12-(a) represents partial schedule
assigns execution time step = 0 event e0 = 2 e1 , leaving eE unassigned.
p-Sulu Planner obtains lower bound objective function value J (s) solving
CCQSP planning problem partial schedule extended s. p-Sulu
Planner minimizes search space dynamically pruning domain forward checking.
specifically, execution time assigned event iteration branch-andbound search, p-Sulu Planner runs shortest-path algorithm tighten real-valued upper
lower bounds execution time step unassigned events according newly assigned
execution time step.
Algorithm 6 shows pseudocode algorithm. node search tree, fixedschedule CCQSP planning problem solved given partial schedule. node
547

fiO , W ILLIAMS , & B LACKMORE

leaf tree optimal objective value less incumbent, optimal solution
updated (Line 11). node leaf, optimal objective value corresponding
subproblem lower bound optimal objective value subsequent nodes. lower
bound less incumbent, node expanded enumerating feasible execution time
assignments unassigned event (Line 13). Otherwise, node expanded, hence
pruned. Details branch-and-bound process described later subsections.

Figure 10: (a) example CCQSP; (b) plan satisfies CCQSP (a)

Figure 11: (a) directed distance graph representation CCQSP Figure 10-(a); (b) dgraph (a), shows shortest distances nodes; (c) updated d-graph
execution time = 2 assigned event e1 .

(e0) = 0
(e1)

(e0) = 0

0
1

2

(e1) = 2

3

(eE)

(eE)
(a)

0
1

2
4

3
5

(b)

Figure 12: Branch-and-bound search schedule s. assume time interval =
1.0. (a)
node (e0]) = 0 expanded; De1 () = {1, 2, 3} given (e0 ) = 0,
[ max
since de (), dmin
1) = 2
e () = [0.8, 3.9] Figure 11-(b); (b)
]
[ node (emin
()
=
(),

expanded; DeE () = 4, 5 given (e0 ) = 0 (e1 ) = 2, since dmax
e
e
[3.6, 5.5] Figure 11-(c).

548

fiP ROBABILISTIC P LANNING C ONTINUOUS DYNAMIC YSTEMS B OUNDED R ISK

Walk-through example present walk-through example give readers insight solution process. consider CCQSP shown Figure 10-(a). CCQSP specifies mission
go waypoint get goal region B avoiding obstacle C, shown
Figure 10-(b). assume time interval = 1.0.
Figures 11 12 illustrate solution process. p-Sulu Planner algorithm initialized
assigning execution time 0 start event e0 . Figure 11-(a) distance graph representation simple temporal constraints (Dechter, 2003) CCQSP. Note simple chance
constraint equivalently represented pair inequality constraints follows:
s(e) s(e ) [l, u] s(e) s(e ) u s(e ) s(e) l.
two inequality constraints represented two directional edges two nodes
distance graph. p-Sulu Planner runs all-pair shortest-path algorithm distance
graph obtain d-graph shown Figure 11-(b). d-graph completed distance graph
edge labeled shortest-path length. d-graph represents tightest temporal
constraints. algorithm enumerates feasible execution-time assignments event
e1 using d-graph. According d-graph, execution time event e1 must
0.8 3.9. Since consider discrete time steps time interval = 1.0, feasible
execution time steps e1 {1, 2, 3}. idea behind enumerating feasible execution time
steps assign event, thus tighten bounds unassigned events order ensure
feasibility.
node (e1 ) = 1, p-Sulu Planner solves FRR fixed-schedule CCQSP
planning problem End episode execution schedule (e1 ) = 1.
words, tries find optimal path goes = 1, neglects goal B
obstacle C. solution exists, optimal cost gives lower bound objective value
feasible paths go = 1. Assume solution exist. Then,
p-Sulu Planner prunes node (e1 ) = 1, goes next node (e1 ) = 2. solves
FRR corresponding fixed-schedule subproblem find best path goes
= 2. Assume p-Sulu Planner finds solution. Then, p-Sulu Planner expands
node following process. First, fixes execution time (e1 ) = 2 d-graph,
runs shortest-path algorithm order tighten temporal constraints (11-(c)). pSulu Planner uses updated d-graph enumerate feasible execution-time assignments
event eE , {4, 5}. visits nodes solves fixed-schedule subproblems exactly
episodes fully assigned schedule. example, node (eE ) = 5, computes
best path goes = 2 reaches B = 5 avoiding obstacle C,
shown Figure 10-(b). Assume optimal objective values subproblems 10.0
(eE ) = 4 8.0 (eE ) = 5. algorithm records solution (eE ) = 5 cost
8.0 incumbent.
algorithm backs visits node (e1 ) = 3, relaxed subproblem
End episode solved obtain lower bound objective value subsequent
nodes. lower bound turns 9.0, exceeds incumbent. Therefore, branch
pruned. Since nodes expand, algorithm terminated, incumbent
solution returned.
549

fiO , W ILLIAMS , & B LACKMORE

Algorithm 7 Implementation expand function Line 13 Algorithm 6
function expand(ccqsp, queue, e, E , )
1: Fix distance e0 e (e)T d-graph ccqsp
2: Update d-graph running shortest-path algorithm
3: Choose e E\E
//choose unassigned event
4: E := E e
max
5: De () := { | dmin
e () tT de ()}
6: De ()
{
(e) (e E )

7:
(e) :=
//update partial schedule

(e = e )
8:
queue E ,
9: end
6.2 Branching
Algorithm 7 outlines implementation expand() function Algorithm 6. takes partial
schedule input, adds queue set schedules assign execution time step
additional event e . words, domain newly added schedules E one
assigned event domain input partial schedule E . details Algorithm 7
explained following parts subsection.
6.2.1 E NUMERATION F EASIBLE IME TEP SSIGNMENTS USING - GRAPH
enumerating feasible time steps, simple temporal constraints must respected.
accomplish this, use d-graph translate bounds durations two events
bounds execution time step event. shown Dechter et al. (1991)
set feasible execution times event e bounded distance e e0 dgraph. d-graph directed graph, weights edges represent shortest distances
nodes, Figure 11-(b). order obtain d-graph representation, first translate
simple temporal constraints directed distance graph, Figure 11-(a). weight
edge two nodes (events) corresponds maximum duration time origin node
destination node, specified corresponding simple temporal constraint. distance
takes negative value represent lower bounds. d-graph (Figure 11-(b)) obtained
distance graph (Figure 11-(a)) running all-pair shortest-path algorithm (Dechter et al., 1991).
Forward checking d-graph p-Sulu Planner algorithm incrementally assigns
execution time step event, explained walk-through example. p-Sulu Planner
minimizes search space forward checking using d-graph. forward checking
methods Constraint Programming, method prunes values unassigned variables (i.e.,
execution times unassigned event) violate simple temporal constraints. different
normal forward checking back tracking performed, due decomposability
d-graph. forward checking conducted following process. execution time step
assigned event e (i.e., (e) = t), distance e0 e fixed tT , distance
e e0 fixed tT distance graph (Line 1 Algorithm 7). Recall
index discretized time steps fixed interval , temporal bounds given
real-valued times (Section 2.1). run shortest-path algorithm update d-graph (Line
550

fiP ROBABILISTIC P LANNING C ONTINUOUS DYNAMIC YSTEMS B OUNDED R ISK

2). Given partial schedule , denote updated shortest distance start event e0 e

min
d-graph dmax
e (), distance e e0 de ().
example, execution time 2 assigned event e1 Figure 11-(c) (i.e., (e1 ) = 2),
distance e0 e1 fixed 2 distance opposite direction fixed
2. run shortest-path algorithm update d-graph. result, obtain
min
updated distances dmax
eE () = 5.5 deE () = 3.6.
max
Dechter et al. (1991) showed de () corresponds upper bound feasible execution time unassigned event e , dmin
eE () corresponds negative lower bound.
Hence, partial schedule assigned events e E , updated domain unassigned
max
event e
/ E bounded dmin
e () de (). Note domain execution time steps
max
e included in, equal [dmin
e (), de ()], consider discrete execution
time steps finite set T. forward checking, p-Sulu Planner computes
max

real-valued bounds [dmin
e (), de ()]. feasible values unassigned variable e

enumerated search tree expanded e .
Enumerating domain execution time steps unassigned event readily
extract feasible execution time steps unassigned event e
/ E updated d-graph
partial schedule . Let De () domain execution time steps unassigned event
e
/ E , given partial schedule . finite domain De () obtained follows:
max
De () := { | dmin
e () tT de ()}.

Note De () may empty temporal constraints tight, even though feasible. user p-Sulu Planner must make small enough De empty.
example, Figure 11-(b) d-graph given partial schedule {(e0 ) = 0}. According
d-graph, e1 must executed 0.8 3.9. Assuming = 1, set feasible
execution time steps e1 De1 () = {1, 2, 3}, shown Figure 12-(a). Likewise, Figure 11-(c)
d-graph given partial schedule {(e0 ) = 0, (e1 ) = 2}; feasible execution time eE
3.6 5.5. Hence, set feasible execution time steps eE DeE () = {4, 5},
shown Figure 12-(b).
enumeration conducted Line 6 Algorithm 7. algorithm creates extensions
input partial schedule assigning time steps e (Line 7), puts extended
partial schedules queue (Line 8).
6.2.2 E FFICIENT VARIABLE RDERING B RANCH - -B OUND EARCH
choosing next event assign time step Line 3 Algorithm 7, two variable ordering
heuristics found effective order reduce computation time.
first heuristic new convex-episode-first (CEF) heuristic, prioritizes events
associated non-convex constraints. idea CEF heuristic based
observation subproblems branch-and-bound algorithm particularly difficult solve
episodes A(E ) involve non-convex state constraints. Remain R2 \C (2D
plane minus obstacle C) episode walk-through example Figures 10 example
non-convex episodes. Therefore, effective approach reduce computation time
p-Sulu Planner minimize number non-convex subproblems solved branch-andbound process. idea realized sorting events episodes convex
feasible region always examined branch-and-bound process episodes
551

fiO , W ILLIAMS , & B LACKMORE

non-convex feasible region. walk-through example, note visited event e1
event eE example. End episode involves convex state
constraint Remain R2 \C (2D plane minus obstacle C) non-convex.
second one well-known constrained variable heuristic. p-Sulu Planner
expands node, counts number feasible time steps unassigned events, chooses
one least number feasible time steps. second heuristic used break ties
first heuristic.

6.3 Bounding
next present implementation obtainLowerBound() function Line 8 Algorithm 6.
algorithm obtains lower bound solving relaxed CCQSP planning problem fixed
partial schedule.
Algorithm 8 outlines implementation obtainLowerBound() function. takes partial
schedule input, outputs lower bound objective function, well optimal
control sequence, given partial schedule . constructs relaxed optimization problem,
involves episodes whose start end events assigned execution time steps (Line 1).
optimization problem involves non-convex constraints, NIRA algorithm used obtain
solution problem (Line 3). Otherwise solve FRR convex optimization problem
obtain lower bound efficiently (Line 5). input fully assigned schedule (E = E),
corresponding node leaf node. case obtain exact solution CCQSP
planning problem fixed schedule running NIRA algorithm (Line 3). details
Algorithm 8 explained subsequent part subsection.

Algorithm 8 Implementation obtainLowerBound function Line 8 Algorithm 6
function obtainLowerBound(ccqsp, E , ) returns optimal objective value control sequence
1: subprblem Problem 9 given ccqsp
2: E = E A() episodes non-convex state regions,
3:
[J , u0:N 1 ] NIRA(subprblem) //Algorithm 1
4: else
5:
J obtainLowreBound-FRR(subprblem) //Algorithm 5
6:
u0:N 1
7: end
8: return [J , u0:N 1 ]

6.3.1 R ELAXED PTIMIZATION P ROBLEM PARTIAL CHEDULE
consider relaxed optimization problem follows:
552

fiP ROBABILISTIC P LANNING C ONTINUOUS DYNAMIC YSTEMS B OUNDED R ISK

Problem 9: Relaxed Optimization Problem Partial Schedule
J () =

min
u0:N 1 UN
s.t.

J(u0:N 1 , x1:N , )

(68)

, xt+1 = xt + B ut




(69)
hTc,a,k,j xt gc,a,k,j mc,a,k,j (c,a,k )

cC a(c A()) ta () kKa jJa,k

(70)



c,a,k 1 c ,

(71)

kKa ,a(c A())

J () optimal objective value relaxed subproblem partial schedule .
Recall A() partial episode set , involves episodes whose start
end nodes assigned execution time steps partial schedule (Definition 9).
notational simplicity, merge three conjunctions (70) obtain following:

hTc,i,j xti gc,i,j mc,i,j (c,i ).
cC iIc () jJc,i

Note chance constraint exactly (57), except partial schedule
specified instead fully assigned schedule s. Hence, Problem 9 instance non-convex
CCQSP planning problem fixed schedule (Problem 6), optimally solved
NIRA algorithm. Also note fully assigned schedule leaf node branch-andbound search tree.
optimal objective value Problem 9 gives lower bound optimal objective value
subsequent subproblems branch-and-bound tree. property formally stated
Lemma 8 below. order prove feature, first define concept extension
partial schedule follows:
Definition 14. schedule : E 7 extension partial schedule : E 7
assign time steps events domain :
(e) = s(e) e E .
example, Figure 12-(b), fully assigned schedule {s(e0 ) = 0, s(e1 ) = 2, s(eE ) = 4}
{s(e0 ) = 0, s(e1 ) = 2, s(eE ) = 5} extension partial schedule {(e0 ) = 0, (e1 ) = 2}.
following lemma always holds:
Lemma 8. schedule extension partial schedule , optimal objective value
Problem 9 lower bound optimal objective value s:
J () J (s).
Proof. Since partial schedule, E E, hence A() A. Also, since (e) = s(e)
e E , state constraints chance constraint (70) Problem 9 partial schedule
included problem full schedule s. means feasible state space
553

fiO , W ILLIAMS , & B LACKMORE

problem subset one . Hence, chance constraint (24) problem
satisfied, chance constraint (70) problem also satisfied. Therefore,
problem always results better (less) equal cost problem ,
former looser constraints.

example, Figure 12-(b), e1 assigned execution time step eE not.
Therefore, node (e1 ) = 2, chance-constrained optimization problem End
episode solved partial schedule {(e0 ) = 0, (e1 ) = 2} (see Figure 10-(a)). gives
lower bound cost problems fully assigned schedules {s(e0 ) = 0, s(e1 ) =
2, s(eE ) = 4} {s(e0 ) = 0, s(e1 ) = 2, s(eE ) = 5}.
Algorithm 8 obtains lower bound solving Problem 9 exactly using NIRA algorithm,
involves episodes non-convex state regions (Line 3). function called leaf node,
Problem 9 also solved exactly NIRA. solutions leaf subproblems
candidate solutions optimal solution overall problem. Hence, solving exactly,
ensure optimality branch-and-bound search.
6.3.2 F URTHER B OUNDING FRR
relaxed subproblem (Problem 9) convex, p-Sulu Planner solves FRR subproblem, instead solving exactly NIRA, order obtain lower bound efficiently
(Line 5 Algorithm 8). Many practical CCQSP execution problems one episode
non-convex feasible region. example, CCQSP planning problem shown Figures
2 3, safe region (R2 minus obstacles) non-convex, Provincetown (start
region), Scenic region, Bedford (goal region) convex. case subproblems
solved exactly leaf nodes, lower bounds always evaluated approximate
solutions FRRs subproblems non-leaf nodes.

7. Results
section empirically demonstrate p-Sulu Planner efficiently operate various
systems within given risk bound. first present simulation settings Section 7.1. Section 7.2 presents simulation results NIRA algorithm, validates claim
efficiently compute feasible near-optimal solution. Section 7.3 demonstrates p-Sulu Planner two different benchmark problems. simulation results highlight p-Sulu Planners
capability operate within user-specified risk bound. Section 7.4 deploys p-Sulu Planner
PTS scenarios, Section 7.5 applies p-Sulu Planner space rendezvous
autonomous cargo spacecraft International Space Station.
7.1 Simulation Settings
Recall that, stated Section 2.4, p-Sulu Planner takes four inputs: stochastic plant model
M, initial condition I, CCQSP P , objective function J. section specifies
J, commonly used problems Sections 7.2-7.4. specify P
problem corresponding section.
554

fiP ROBABILISTIC P LANNING C ONTINUOUS DYNAMIC YSTEMS B OUNDED R ISK

7.1.1 TOCHASTIC P LANT ODEL
section explains plant model used Sections 7.2 - 7.4. Section 7.5 uses different plant
model described detail Section 7.5.2. consider point-mass double-integrator plant,
shown (72)-(73). Parameters, umax , vmax , 2 , set individually
problem. plant model commonly assumed literatures unmanned aerial vehicle (UAV)
path planning (Kuwata & How, 2011; Leaute, 2005; Wang, Yadav, & Balakrishnan, 2007).
state vector xt consists positions velocities x directions, control
vector consists accelerations:
xt := [x vx vy ]T ,

ut := [ax ay ]T .

plant model specified following matrices:





1 0 0
t2 /2
0
0 1 0


0
t2 /2
, B =
, w =
A=
0 0 1




0
0 0 0
1
0

T, ||ut || umax , ||Cxt || vmax ,
(


C=

0 0 1 0
0 0 0 1

2 0
0 2
0 0
0 0

0
0
0
0


0
0
(72)
0
0
(73)

)
.

first constraint (73) imposed order limit acceleration. nonlinear constraint
approximated following set linear constraints:
T, r n ut umax (n = 1, 2, , Nr )
]
[
2n
2n
, sin
r n = cos
Nr
Nr
choose Nr = 16. second constraint (73) imposed order limit velocity. use
linear approximation above.
7.1.2 BJECTIVE F UNCTION
Sections 7.2.3, 7.3, 7.4, cost function Manhattan norm control input
planning horizon, follows:
J(xti , U , s) =




(|ux,t | + |uy,t |) .

t=1

cost function represents total change momentum, roughly proportional
fuel consumption aerial vehicle. Note minimization problem piece-wise linear
cost function equivalently replaced following minimization problem
linear cost function additional linear constraints introducing slack variables x,t y,t :
min




(x,t + y,t )

t=1

s.t.

T,

x,t ux,t x,t ux,t y,t uy,t y,t uy,t
555

fiO , W ILLIAMS , & B LACKMORE

Section 7.2.4, minimize expected quadratic cost follows:


[
]
J(xti , U , s) =
E u2x,t + u2y,t .

(74)

t=1

7.1.3 C OMPUTING ENVIRONMENT
simulations except ones Section 7.2 conducted machine dual-core
Intel Xeon CPU clocked 2.40 GHz, 16 GB RAM. algorithms implemented
C/C++, run Debian 5.0.8 OS. simulations Section 7.2 conducted machine
quad-core Intel Core i7 CPU clocked 2.67 GHz, 8 GB RAM. algorithms
implemented Matlab, run Windows 7 OS. used IBM ILOG CPLEX Optimization
Solver Academic Edition version 12.2 linear program solver, SNOPT version 7.2-9
convex optimization solver.
7.2 NIRA Simulation Results
first statistically compare performance NIRA prior art. Recall NIRA
solver CCQSP planning problems non-convex state constraints fixed schedule
(Problem 3), used subroutine p-Sulu Planner.
7.2.1 C OMPARED LGORITHMS
two existing algorithms solve problem:
1. Fixed risk allocation (Blackmore et al., 2006) - approach fixes risk allocation
uniform value. result, assumption cost function linear, Problem 6
reformulated mixed-integer linear programming (MILP) problem,
solved efficiently MILP solver, CPLEX.
2. Particle Control (Blackmore, 2006) - Particle Control sampling-based method,
uses finite number samples approximate joint chance constraints. control
sequence optimized number samples violate constraints less c Np ,
Np total number samples. optimization problem reformulated
MILP, assumption cost function linear.
also compare NIRA MDP Section 7.2.5. Although MDP solve
exactly problem NIRA, also avoid risk considering penalty cost constraint
violations. purpose comparison highlight capabilities chance-constrained
planning provide guarantee probability failure.
7.2.2 P ROBLEM ETTINGS
compare closed-loop open-loop NIRAs two algorithms 2-D path planning
problem randomized location obstacle, shown Figure 13. vehicle starts
[0, 0] heads goal [1.0, 1.0], avoiding rectangular obstacle. obstacle
edge length 0.6 placed random location within square region corners [0, 0],
[1, 0], [1, 1], [0, 1]. consider ten time steps time interval = 1.0. require
556

fiP ROBABILISTIC P LANNING C ONTINUOUS DYNAMIC YSTEMS B OUNDED R ISK

mean state = 10 [1.0, 1.0]. risk bound set = 0.01. set standard
deviation disturbance = 0.01. use expected quadratic cost function given (74).
steady-state LQR gain used closed-loop NIRA Q = I4 R = 10000I2 ,
n n identity matrix Q R cost matrices state control variables,
respectively.
1

NIRA (closed-loop)
NIRA (open-loop)
Fixed Risk Allocation
Particle Control

0.8

NIRA (closed-loop)
NIRA (open-loop)

1
0.8

0.6

0.6

0.4

0.4

0.2

0.2
0

0
0

0.2

0.4

0.6

0.8

1

0

(a) Nominal trajectories

0.2

0.4

0.6

0.8

1

(b) Nominal trajectories 3 ellipses

Figure 13: (a) instance 2-D path planning problem used 7.2.3. obstacle
fixed size randomly placed within unit square run. (b) mean
standard deviation closed-loop open-loop NIRAs.

7.2.3 P ERFORMANCE C OMPARISON
Recall solution NIRA algorithm, used p-Sulu Planner solve subproblems, exactly optimal solution Problem 3, since risk allocation (Section 4.1.1)
risk selection (Section 5.1.1) replace chance constraint (29) sufficient condition (57)
(59). Since chance constraint (29) difficult evaluate, previously proposed
methods solve optimization approximation. provide empirical evidence risk
allocation/selection approach results solution significantly closer optimal solution
prior art, satisfaction original constraint (29) guaranteed.
evaluate suboptimality solutions difference risk bound, =
0.001, resulting probability constraint violation, Pf ail , estimated Monte-Carlo simulation. 1 Pf ail equal left-hand-side value (29) Problem 3. Hence, chance
constraint (29) equivalent to:
Pf ail .
strictly optimal solution problem achieve Pf ail = , although exact
solution unavailable, since algorithm solve Problem 3 exactly. solution suboptimal Pf ail < , ratio /Pf ail represents degree suboptimality. solution
violates chance constraint Pf ail > .
557

fiO , W ILLIAMS , & B LACKMORE

Algorithm
NIRA (Closed-loop)
NIRA (Open-loop)
Fixed Risk Allocation
Particle Control
(100 particles)

Computation time
[sec]
54.8 36.9
25.0 13.1
0.42 0.04

Probability failure

Cost

0.0096 0.0008
0.0095 0.0008
(2.19 0.40) 104

0.666 0.061
0.672 0.068
0.726 0.113

41.7 12.8

0.124 0.036

0.635 0.048

Table 1: averages standard deviations computation time, probability constraint violation, cost four algorithms. algorithms run 100 times
random location obstacle. risk bound set = 0.01. Note Particle
Control results less cost two methods solutions violate
chance constraint.

Table 1 compares performance four algorithms. values table averages standard deviations 100 runs random locations obstacle. probability
constraint violation, Pf ail , evaluated Monte-Carlo simulations 106 samples.
Comparison closed-loop open-loop NIRAs comparing NIRA existing algorithms, first compare two variants NIRA: closed-loop open loop NIRAs. Table
1 shows closed-loop NIRA results less cost open-loop NIRA. Importantly,
former outperforms latter 100 test cases. reduction cost closed-loop
approach explained Figure 13-(b), shows 3 ellipses probability distribution
state. Since closed-loop NIRA assumes feedback control, future position less uncertain. result, plan generated closed-loop NIRA less conservative. fact, Table
1 shows Pf ail closed-loop NIRA closer risk bound open-loop
NIRA. However, closed-loop planning problem requires twice much solution time
open-loop one since complicated due additional chance constraints control input.
Comparison fixed risk allocation approach Table 1 shows closed open NIRAs
result average probabilities failure 0.0096 0.0095 respectively, within userspecified risk bound = 0.01. hand, fixed risk allocation approach results
conservative probability failure, Pf ail = 0.000219, 98% smaller .
result indicates solution NIRA significantly closer exactly optimal solution
fixed risk allocation approach. fact, NIRA algorithm results less cost fixed risk
allocation approach 100 runs. optimizes risk allocation
fixed risk allocation approach uses predetermined risk allocation.
Figure 14 shows suboptimality measure /Pf ail open-loop NIRA different settings risk bound . values , suboptimality NIRA significantly smaller
fixed risk allocation approach. graph shows tendency suboptimality NIRA
gets smaller less , suboptimality fixed risk allocation approach approximately constant.
NIRA achieves improvement solution optimality cost computation time; Table
1 shows NIRA takes longer computation time risk allocation approach factor
558

fiP ROBABILISTIC P LANNING C ONTINUOUS DYNAMIC YSTEMS B OUNDED R ISK

Pfail /

Figure 14: Suboptimality NIRA fixed risk allocation approach. Strictly optimal solution
/Pf ail = 1. smaller value /Pf ail indicates solution suboptimal.

two. Hence, NIRA fixed risk allocation approach provide users trade-off
suboptimality computation time.
Comparison Particle Control Table 1 shows average probability failure
Particle Control approach higher risk bound = 0.01, meaning approach
tends generate infeasible solutions. hand, NIRA guarantees satisfaction
chance constraint since employs conservative approximation joint chance constraint.
Particle Control guarantee solution converges optimal solution increasing number samples infinity. However, using large number samples impractical,
since computation time memory usage grow exponentially number samples increases.
example, used 100 samples analysis Table 1. using 300 samples, took
4596 seconds (about 1.5 hours) solve problem obstacles centered [0.5, 0.5].
Computation 1000 samples could conducted, shortage memory.
hand, computation time NIRA significantly shorter PC, guaranteeing
feasibility solution.
7.2.4 PTIMAL P LANNING E XPECTED C OST
Next demonstrate capability p-Sulu Planner handle expected cost, instead cost
expected trajectory, path planning problem presented above. Specifically,
consider expected quadratic cost function shown (74). conducting open-loop planning,
cost function transformed function nominal control inputs constant term
using equality (15). However, performing closed-loop planning, equality
exact, due controller saturation. Nevertheless, use (15) approximation expected
cost, explained Section 2.4.4. subsection empirically evaluate error
approximation.
559

fiO , W ILLIAMS , & B LACKMORE

Approximate expected cost
0.048434950 0.010130589

Actual expected cost
0.048434956 0.010130588

Table 2: Comparison approximate expected cost obtained closed-loop NIRA
actual expected cost. table shows mean variance 100 runs random
location obstacle.

Table 2 compares approximate expected cost function value obtained closed-loop
NIRA actual expected cost estimated Monte-Carlo simulation one million samples.
path planning problem solved 100 times randomized location obstacle. risk
bound set = 0.01. shown table, approximate cost almost exactly agrees
actual cost. closed-loop planning approach explicitly bounds risk
controller saturation.
7.2.5 C OMPARISON MDP
Next compare NIRA MDP formulation. sake tractability MDP,
consider single integrator dynamics two-dimensional state space two-dimensional
control input, specifies velocity vehicle. rest problem setting same,
except state space discretized 100-by-100 grid. implement finite-horizon
MDP-based path planner, imposes penalty c event failure minimizes
expected cost based explicit state dynamic programming. MDP-based path planner imposes
cost follows:
]
[
(
)
u2x,t + u2y,t + cI(xt ) ,
E
t=1

I(xt ) indicator function one xt obstacle zero otherwise.
resulting optimization problem solved via dynamic programming.
ran MDP-based path planner three values penalty c: 1, 10, 100.
choice c, conducted 100 simulations randomized obstacle position. Figure 14 shows
typical output MDP-based path planner. Note that, small penalty (c = 1), path
planner chooses take 100% risk failure ignoring obstacle. simply
penalty failure smaller expected reduction cost going obstacle.
issue utilitarian approaches MDPs minimization unconstrained cost
sometimes lead impractical solution.
Table 3 shows mean standard deviation path lengths, well maximum,
minimum, mean resulting probability failure among 100 runs. expected,
imposing larger penalty, MDP-path planner chooses risk-averse path,
longer nominal path length. sense, MDP also conduct trade-off cost
risk. MDP particularly useful primary concern user cost failure instead
probability failure. hand, user would like impose hard bound
probability failure, chance constrained planning approach advantage. Observe that,
even penalty value, MDP-based path planner results wide range failure
probabilities depending location obstacle. notably, c = 10,
560

fiP ROBABILISTIC P LANNING C ONTINUOUS DYNAMIC YSTEMS B OUNDED R ISK

paths move directly across obstacle, doing, accept 100% probability failure,
others go around obstacle. Undesirable behaviors, crossing obstacle, likely
suppressed imposing greater penalty, without guarantee. Moreover, imposing heavy
penalty failure often results overly conservative, risk averse solution. hand,
behavior NIRA regarding risk predictable, sense path guaranteed
go around obstacle, regardless location. chance constraint requires
exists margin path boundary obstacle. p-Sulu Planner
inherits property NIRA.

1

0.8

0.6

0.4

0.2

c=1
c=10
c=100

0
0

0.2

0.4

0.6

0.8

1

Figure 15: Optimal paths generated MDP-based planner different penalty levels , c.
red rectangle represents obstacle. Note path c = 1 cuts
obstacle.

Penalty c

path length

1
10
100

1.41 0.00
1.54 0.05
1.57 0.06

Probability failure
Max
Mean Min
1.000 1.000 1.000
1.000 0.375 0.096
0.1215 0.031 0.009

Table 3: 100 runs randomized obstacle location

7.3 p-Sulu Planner Simulation Results
Next present simulation results p-Sulu Planner two problems, order illustrate
capability planning schedule constraints. also empirically evaluate scalability
p-Sulu.
561

fiO , W ILLIAMS , & B LACKMORE

Figure 16: sample CCQSP personal aerial vehicles path planning scheduling problem.

Figure 17: Output p-Sulu Planner CCQSP Figure 16 three different settings
risk bound obs , compared path planned deterministic planner, Sulu,
consider chance constraints.

7.3.1 PATH P LANNING BSTACLES
simulation test p-Sulu Planner path planning problem environment shown
Figure 17. input CCQSP shown Figure 16. CCQSP requires vehicle arrive
goal region within 15 minutes, going Waypoint 1 Waypoint 2 temporal
constraints specified Figure 16. also imposes two chance constraints: one requires
vehicle achieve time-evolved goals 90% certainty, another requires vehicle
limit probability violating obstacles obs . set = 1 2 = 0.0025.
Figure 17 shows plans generated p-Sulu Planner three different risk bounds:
obs = 10%, 0.1%, 0.001%. computation times 79.9 seconds, 86.4 seconds,
88.1 seconds, respectively. Figure 17 also shows plan generated Sulu, deterministic planner
explicitly consider uncertainty (Leaute & Williams, 2005). Observe Sulu leaves
margin path obstacles. result, Sulu path results 94.1% probability
hitting obstacles, estimated Monte-Carlo simulation 107 samples. hand,
p-Sulu Planner leaves margins path obstacles order satisfy risk bound,
562

fiP ROBABILISTIC P LANNING C ONTINUOUS DYNAMIC YSTEMS B OUNDED R ISK

specified chance constraint. margins larger plans smaller risk bounds.
probabilities failure three plans generated p-Sulu Planner, estimated Monte-Carlo
simulations 107 samples, 9.53%, 0.0964%, 0.00095%, respectively. Hence chance
constraints satisfied. schedule optimized p-Sulu Planner {s(e0 ) = 0, s(e1 ) =
5, s(e2 ) = 10, s(eE ) = 15}, satisfies temporal constraints CCQSP.
Figure 16, appears path cuts across obstacle. due discretization
time; optimization problem requires vehicle locations discrete time step
satisfy constraints, consider state between. issue addressed
constraint-tightening method (Kuwata, 2003).
7.3.2 PATH P LANNING NDOOR E NVIRONMENT

Figure 18: sample CCQSP path planning problem indoor environment.

= 10%
= 1%
= 0.1%

1.2
1
Goal
0.8
0.6
0.4
0.2
0

Start

0.2
0.2

0

0.2

0.4

0.6

0.8

1

1.2

Figure 19: Output p-Sulu Planner CCQSP Figure 16 three different settings
risk bound obs .

next give p-Sulu Planner CCQSP shown Figure 18, simulates path planning problem indoor environment. vehicle must get goal region side
room three five seconds. Remain safe region episode requires vehicle stay
563

fiO , W ILLIAMS , & B LACKMORE

within room outside obstacle five-second planning horizon. CCQSP
imposes two chance constraints shown Figure 18. set = 0.5 2 = 5.0 105 .
Given CCQSP, planner faces choice: heading straight goal going
narrow passage left wall obstacle minimizes path length, involves
higher risk constraint violation; making detour around right side obstacle involves
less risk, results longer path.
Figure 19 shows p-Sulu Planners outputs obs = 10%, 1%, 0.1%. computation times 35.1 seconds, 84.5 seconds, 13.3 seconds, respectively. result consistent
intuition. p-Sulu Planner allowed 10% risk, planner chooses go straight
goal, resulting cost function value 1.21; user gives 1% 0.1% risk bound,
chooses risk-averse path, resulting cost function values 3.64 3.84, respectively.
example demonstrates p-Sulu Planners capability make intelligent choice order
minimize cost, limiting risks user-specified levels.
7.3.3 CALABILITY NALYSIS
subsection conduct empirical analysis scalability p-Sulu Planner,
environment becomes increasingly constrained.. shown Figure 20, measured computation time solve path planning problem different numbers obstacles waypoints.
simulations, path starts [0, 12] ends square region centered [24, 12]. Figure 20
shows twenty simulation results, zero three obstacles zero four waypoints. Obstacles
waypoints represented blue red squares figure, respectively. positions
center obstacles [6, 12], [12, 12], [18, 12], positions center
waypoints [9, 9], [9, 15], [15, 15], [15, 9]. computation time shown caption
subfigure Figure 20.
comparing results Figure 20 horizontally, observe exponential growth computation time number obstacles. result expected since number disjunctive
clauses state constraint p-Sulu Planner increases exponentially number
obstacles. Building tractable extension p-Sulu Planner large number obstacles
future work. hand, comparing results vertically, find computation
time number obstacles different number waypoints stays order
magnitude. adding extra waypoint increases number conjunctive
clauses state constraints.
remaining sections describe application psulu two real world problems, air
vehicle space vehicle control. third application, building energy management, using variant
p-Sulu Planner, reported Ono, Graybill, Williams (2012).
7.4 PTS Scenarios
Next, deploy p-Sulu Planner PTS scenarios, robotic air taxi system introduced
Section 1.
7.4.1 CENARIOS
consider three scenarios, specified CCQSPs shown Figure 21. Scenarios 1 2
similar scenic flight scenario introduced beginning paper (see Figure 1).
564

fiP ROBABILISTIC P LANNING C ONTINUOUS DYNAMIC YSTEMS B OUNDED R ISK

0.003 sec

0.173 sec

51.47 sec

677.2 sec

0.016 sec

0.518 sec

48.25 sec

648.4 sec

0.034 sec

1.047 sec

118.3 sec

4327 sec

0.076 sec

2.613 sec

159.1 sec

5686 sec

0.036 sec

3.873 sec

165.9 sec

6845 sec

15
12
9

15
12
9

15
12
9

15
12
9

15
12
9
0

9

15

24

0

9

15

24

0

9

15

24

0

9

15

24

Figure 20: Computation time p-Sulu Planner path planning problem different numbers obstacles waypoints.

Scenario 1, personal aerial vehicle (PAV) takes Runway 71 Provincetown Municipal
Airport (KPVC) Provincetown, Massachusetts, fly scenic region, lands Runway 23
Hanscom Field (KBED) Bedford, Massachusetts. vehicle required stay within
scenic region least 2 minutes 10 minutes. entire flight must take
13 minutes less 15 minutes. Scenario 2 Scenario 1, except runways
used take-off landing.
Scenario 3 simulates leisure flight coast Massachusetts. PAV takes Runway 7
Provincetown Municipal Airport, flies two regions whales often seen.
vehicle lands Runway 11 Hanscom Field.
place three no-fly zones, shown Figure 22. entire flight must take 13
minutes less 15 minutes. scenario three chance constraints, {c1 , c2 , c3 }, shown
Figure 21. first one, c1 , concerned vehicles operation; requires vehicle
take land right runways right airports less 10 % probability
failure. second chance constraint, c2 , concerned leisure activities; requires
vehicle fly scenic regions less 10 % probability failure. Finally, c3
concerned passengers safety; requires vehicle limit risk penetrating
no-fly zones 0.01 %.
1. runway airport specified number, represents clockwise angle north. example,
Runway 7 points 70 degrees away north.

565

fiO , W ILLIAMS , & B LACKMORE

7.4.2 P LANT PARAMETERS
set umax = 250 m/s, approximates maximum cruise speed private jet airplanes,
Gulfstream V. maximum acceleration determined maximum bank angle.
Assuming aircraft flying constant speed, lateral acceleration given function
bank angle follows:
= g tan ,
g acceleration gravity. Typically passenger aircraft limits bank angle 25
degrees passenger comfort, even though aircraft capable turning larger bank
angle. Hence, use:
umax = 9.8 m/s2 tan(25 ) = 4.6 m/s2 .
set = 100 = 60 seconds.
7.4.3 IMULATION R ESULTS
Figure 22 shows paths planned p-Sulu Planner three scenarios. scenarios,
episode requirements CCQSPs Figure 21 met within specified temporal
chance constraints.
Table 4 compares performance Sulu p-Sulu Planner. expected, Sulus plans
result excessive probabilities failure scenarios. Sulu consider
uncertainty planning process, although PAV subject disturbance reality.
hand, p-Sulu Planner successfully limits probability failure within user-specified
risk bounds three scenarios. Furthermore, although p-Sulu Planner significantly reduces
risk failure, cost higher Sulu 9.5 - 12.8 %. capability
limiting risk maximizing efficiency time desirable feature PTS,
transports passengers.
Scenario number
Planner
Computation time [sec]
Pf ail,1
Pf ail,2
Pf ail,3
Cost function value J

1
Sulu
2.58
0.999
0.807
0.373
24.2

2

p-Sulu
60.2
9.12 102
8.46 102
2.74 105
27.5

Sulu
2.00
0.996
0.813
0.227
21.0

p-Sulu
390
9.14 102
8.59 102
2.62 105
23.7

3
Sulu
5.17
0.999
0.603
0.372
20.0

p-Sulu
198
9.23 102
7.65 102
2.81 105
22.3

Table 4: Performance Comparison prior art, Sulu, p-Sulu Planner. Pf ail,1 , Pf ail,2 ,
Pf ail,3 represent probabilities failure regarding chance constraints c1 , c2 ,
c3 Figure 21, respectively.

566

fiP ROBABILISTIC P LANNING C ONTINUOUS DYNAMIC YSTEMS B OUNDED R ISK

Figure 21: CCQSPs PTS scenarios.

Figure 22: paths planned p-Sulu Planner.

567

fiO , W ILLIAMS , & B LACKMORE

shown Table 4, p-Sulu Planner typically takes several minutes compute plan.
length computation time would allowed PTS applications, since assume pSulu Planner used preplanning; take-off, passengers PAV specify requirements,
p-Sulu Planner creates risk-sensitive flight plan. assume real-time plan executive
executes plan take-off.
note desirable real-time risk-sensitive plan executive, since risk factors, location storms, change time. future work reduce computation
time p-Sulu Planner used real-time execution.
7.5 Space Rendezvous Scenario
p-Sulu Planner general planner whose application limited specific plant model.
order show generality planner, deployed p-Sulu Planner system whose
plant model significantly different PTS.
Specifically, chose autonomous space rendezvous scenario H-II Transfer Vehicle
(HTV), shown Figure 23, subject. HTV unmanned cargo spacecraft developed
Japanese Aerospace Exploration Agency (JAXA), used resupply International Space
Station (ISS). Collision vehicle ISS may result fatal disaster, even collision
speed low. example, August 1994, Russian unmanned resupply vehicle Progress M34 collided Mir space station failed attempt automatic rendezvous docking.
result, one modules Mir permanently depressurized. order avoid
accident, HTV required follow specified safety sequence automated rendezvous,
described following subsection.

Figure 23: H-II Transfer Vehicle (HTV), Japanese unmanned cargo vehicle, conducts autonomous
rendezvous International Space Station. Image courtesy NASA.

7.5.1 HTV R ENDEZVOUS EQUENCE
HTVs autonomous rendezvous mission, final approach phase starts Approach Initiation (AI) point, located 5 km behind ISS, shown Figure 24. First, HTV moves
R-bar Initiation (RI) point, located 500 ISS, guided relative GPS
568

fiP ROBABILISTIC P LANNING C ONTINUOUS DYNAMIC YSTEMS B OUNDED R ISK

navigation. RI point, HTV switches navigation mode Rendezvous Sensor (RVS) Navigation. RVS Navigation, HTV measures distance ISS precisely beaming laser
reflector placed nadir (earth-facing) side ISS. Then, HTV proceeds Hold Point
(HP), located 300 ISS. required hold HP order perform 180-degree
yaw-around maneuver. new orientation HTV allows vehicle abort rendezvous
quickly case emergency. yaw-around maneuver, HTV resumes approach,
holds Parking Point (PP), 30 ISS. Finally, HTV approaches
distance 10 meters ISS, stops within Capture Box (CB) ISSs robotic
arm. robotic arm grabs HTV docks ISS. Please refer report Japan
Aerospace Exploration Agency (2009) details rendezvous sequence.
RI

HP

PP CB


ISS

-300m

-30m -10m

ISS Orbit

-500

x

Earth
AI: Approach Initiation
RI: R-bar Initiation
HP: Hold Point
PP: Parking Point
CB: Capture Box

AI Point
-5000

Figure 24: HTVs final approach sequence (Japan Aerospace Exploration Agency, 2009).
rendezvous sequence described represented CCQSP shown Figure 25.
addition time-evolved goals specified actual rendezvous sequence, specify temporal
constraints chance constraints simulation, shown figure. require HTV hold
intermediate goal least 240 seconds. transition goals must take
least 600 seconds, order make sure vehicle moves slowly enough. entire mission
must completed within 4800 seconds (1 hour 20 minutes). require HTV stay within
Safe Zone, conic area ISS, RVS navigation phase 99.5% probability,
since otherwise laser may reflected back HTV properly. assume goals
square regions, 10 sides RI HP, 2 sides PP, 1 sides CB. Finally,
require HTV achieves time-evolved goals 99.5% success probability.
7.5.2 RBITAL DYNAMICS
rendezvous considered two-body problem, chaser spacecraft (e.g., HTV)
moves relation target spacecraft (e.g., ISS), circular orbit. problem,
convenient describe motion chaser spacecraft using rotating frame fixed
target space craft, known Hill coordinate frame (Schaub & Junkins, 2003). shown
Figure 24, set x-axis pointing away center earth y-axis along
569

fiO , W ILLIAMS , & B LACKMORE

Figure 25: CCQSP representation HTVs final approach sequence. assume
time-evolved goals ones used actual flight missions. temporal constraints
chance constraints added authors.

orbital velocity target spacecraft. Since HTVs path within x-y plane, dont consider
z-axis.
known relative motion chase spacecraft Hill coordinate frame described following Clohessy-Wiltshire (CW) equation (Vallado, 2001):
x = 2 + 3 2 x + Fx
= 2 x + Fy
angular speed target spacecrafts orbit, Fx Fy force per unit
mass, acceleration x directions. first terms right-hand sides represent
Coriolis force.
object follows CW equation moves unintuitive manner. unforced motion
straight line due Coriolis effect; general, object cannot stay position
without external force. example, Figure 26 shows fuel-optimal path visit two waypoints,
B, come back start. seen figure, optimal path typically
straight line. virtue p-Sulu Planner handle irregular dynamic systems
way regular systems, setting B matrices plant model (4)
appropriately.
state vector consists positions velocity x plane:
x = [x vx vy ]T
obtain discrete-time CW equation using impulse-invariant discretization:
xk+1 = Axk + Buk ,
570

fiP ROBABILISTIC P LANNING C ONTINUOUS DYNAMIC YSTEMS B OUNDED R ISK



Start

B

Figure 26: typical motion spacecraft Hill coordinate frame. solid line fuel
optimal path visits B returns Start 30 minutes. Note
optimal path straight line Hill coordinate frame.






=




B =


4 3 cos(T )
6{T sin(T )}
3 sin(T )
6{1 cos(T )}

0
1
0
0

sin(T )

2{1cos(T )}


2{1cos(T )}

4 sin(T )
3T


cos(T )
2 sin(T )


2 sin(T )
4 cos(T ) 3

sin(T )

2{1cos(T )}


2{1cos(T )}

4 sin(T )
3T


cos(T )
2 sin(T )

2 sin(T )
4 cos(T ) 3












use ISSs orbital angular speed, = 0.001164 rad/sec, station goes around
Earth 90 minutes. choose interval = 120 seconds. number time steps N
set 40. Hence, entire plan 4800 seconds (1 hour 20 minutes). discretization,
assumed impulse inputs follows:
[

Fx
Fy

]
=

N
1


(t k)uk ,

k=0

() Dirac delta function. assumption justified thrusters
Reaction Control System (RCS) spacecraft, used final approach maneuver,
operate short duration (0.01 5.0 seconds) burn (Wertz & Wiley J. Larson, 1999).
consider stochastic uncertainty w, added discrete-time dynamic equation:
xk+1 = Axk + Buk + w.
assumption additive uncertainty commonly used past research autonomous
rendezvous formation flight space (Shields, Sirlin, & Wette, 2002; Smith & Hadaegh, 2007;
571

fiO , W ILLIAMS , & B LACKMORE

Campbell & Udrea, 2002). assume w
following covariance matrix:
6
10
0
0
106
w =
0
0
0
0

zero-mean Gaussian distribution,

0
0
0
0


0
0
.
0
0

7.5.3 BJECTIVE F UNCTION
employ objective function J requires p-Sulu Planner minimize fuel consumption. follows Tsiolkovsky rocket equation fuel consumption spacecraft
proportional total change velocity, called Delta-V V (Wertz & Wiley J. Larson, 1999).
total fuel consumption summation fuel consumption reaction jets x
directions time steps. Hence objective function described follows:
J(u0:N ) = Vx + Vy
(N 1)T
=
|Fx | + |Fy |dt
0
fi
fi fi
fi
k=N
fi fi (N 1)T
fi
1 fifi (N 1)T
fi fi
fi
=
(t k)ux,k dtfi + fi
(t k)uy,k dtfi
fi
fi 0
fi fi 0
fi
=

k=0
k=N
1

|ux,k | + |uy,k |.

k=0

7.5.4 IMULATION R ESULT
Figure 27 shows planning result p-Sulu Planner. compare result Sulu,
well nominal planning approach, assume HTV moves AI RI using
two-impulse transition (called CW guidance law) (Matsumoto, Dubowsky, Jacobsen, & Ohkami,
2003; Vallado, 2001). RI CB, follows predetermined path goes center
Safe Zone, shown Figure 27-(b), constant speed.
shown Figure 27, optimal paths generated p-Sulu Planner Sulu
straight. curved paths exploit Coriolis effect minimize fuel consumption.
Table 5 compares performance three planning approaches. two rows regarding
probabilities failure correspond two chance constraints specified CCQSP, shown
Figure 25. probabilities evaluated Monte Carlo simulation one million samples.
expected, probabilities failure path generated p-Sulu Planner less
risk bounds specified CCQSP, shown Figure 25. hand, again,
Sulus path results almost 100% probability failure. Sulu minimizes fuel
consumption without considering uncertainty. resulting path pushes boundaries
feasible regions, evident Figure 27-(c). Also note that, although p-Sulu Planner
significantly reduces probability constraint violation compared Sulu, cost (Delta V)
higher Sulu 0.2%. p-Sulu Planner results significantly smaller cost (Delta
V) nominal planning approach. 1.42 m/sec reduction Delta V equivalent
11.9 kg saving fuel, assuming 16, 500 kg mass vehicle 200 sec specific impulse
572

fiP ROBABILISTIC P LANNING C ONTINUOUS DYNAMIC YSTEMS B OUNDED R ISK

Figure 27: Planning results Sulu, p-Sulu Planner, nominal planning approach.
input CCQSP shown Figure 25.

(ISP ) thrusters. Although p-Sulu Planner takes longer compute plan
two approaches, 11.4 second computation time negligible compared 1 hour 20
minute plan duration.

Computation time [sec]
Probability failure Pf ail (Navigation)
Probability failure Pf ail (Goals)
Cost function value (Delta V) J [m/sec]

Sulu
3.9
0.92
1.0
7.30

p-Sulu Planner
11.4
0.0024
0.0029
7.32

Nominal
0.09
< 106
< 106
8.73

Table 5: Performance comparison Sulu, p-Sulu Planner, nominal approach
HTV rendezvous scenario.

573

fiO , W ILLIAMS , & B LACKMORE

8. Conclusions
article introduced model-based planner, p-Sulu Planner, operates within userspecified risk bounds. p-Sulu Planner optimizes continuous control sequence discrete
schedule, given input continuous stochastic plant model, objective function, newly
developed plan representation, chance-constrained qualitative state plan (CCQSP). CCQSP
involves time-evolved goals, simple temporal constraints, chance constraints, specify
users acceptable levels risk subsets plan.
approach developing p-Sulu Planner two-fold. first step, developed
efficient algorithm, called non-convex iterative risk allocation (NIRA), plan nonconvex state space fixed schedule. solved problem based key concept
risk allocation risk selection, achieves tractability allocating specified risk individual constraints mapping result equivalent disjunctive convex program.
NIRA algorithm employs branch-and-bound algorithm solve disjunctive convex program.
subproblems fixed-schedule CCQSP problems convex state space, solved
previously developed algorithms (Blackmore & Ono, 2009). developed novel relaxation method called fixed risk relaxation (FRR), provides tightest linear relaxation
nonlinear constraints convex subproblems.
second step, developed p-Sulu Planner, solve CCQSP planning problem flexible schedule. scheduling problem formulated combinatorial constrained
optimization problem (COP), solved branch-and-bound algorithm. subproblem branch-and-bound search CCQSP planning problem fixed schedule,
solved NIRA. domain feasible schedule pruned running shortest-path algorithm d-graph representation given temporal constraints. lower bounds optimal objective value subproblems obtained solving fixed-schedule CCQSP planning
problems subset state constraints imposed. proposed efficient variable
ordering prioritizes convex subproblems non-convex ones. demonstrated p-Sulu
Planner various examples, personal aerial transportation system autonomous space
rendezvous, showed efficiently solve CCQSP planning problems small suboptimality, compared past algorithms.

Acknowledgments
paper based upon work supported part Boeing Company Grant No. MITBA-GTA-1 National Science Foundation Grant No. IIS-1017992. opinions,
findings, conclusions recommendations expressed publication authors
necessarily reflect view sponsoring agencies. would like thank Michael
Kerstetter, Scott Smith, Ronald Provine, Hui Li Boeing Company support. Thanks
also Robert Irwin advice draft.

References
Acikmese, B., Carson III, J. M., & Bayard, D. S. (2011). robust model predictive control algorithm
incrementally conic uncertain/nonlinear systems. International Journal Robust
Nonlinear Control, 21(5), 563590.
574

fiP ROBABILISTIC P LANNING C ONTINUOUS DYNAMIC YSTEMS B OUNDED R ISK

Aircraft Owners Pilots Association Air Safety Foundation (2005). 2005 Joseph T. Nall Report
- accident trands factors 2004..
Altman, E. (1999). Constrained Markov decision processes. Stochastic modeling. Chapman &
Hall/CRC.
Alur, R., Feder, T., & Henzinger, T. A. (1996). benefits relaxing punctuality. Journal
ACM, 43.
Bacchus, F., & Kabanza, F. (1998). Planning temporally extended goals. Annals Mathematics
Artificial Intelligence, pp. 527.
Balas, E. (1979). Disjunctive programming. Annals Discrete Mathematics.
Bertsekas, D. P. (2005). Dynamic Programming Optimal Control Volume (Third Edition).
Athena Scientific.
Bertsekas, D. P., & Tsitsiklis, J. N. (1996). Neuro-Dynamic Programming (1st edition). Athena
Scientific.
Blackmore, L. (2006). probabilistic particle control approach optimal, robust predictive control.
Proceedings AIAA Guidance, Navigation Control Conference.
Blackmore, L., Li, H., & Williams, B. C. (2006). probabilistic approach optimal robust path
planning obstacles. Proceedings American Control Conference.
Blackmore, L., & Ono, M. (2009). Convex chance constrained predictive control without sampling.
Proceedings AIAA Guidance, Navigation Control Conference.
Boyan, J. A., & Littman, M. L. (2000). Exact solutions time-dependent MDPs. Advances
Neural Information Processing Systems, pp. 10261032. MIT Press.
Boyan, J. A., & Moore, A. W. (1995). Generalization reinforcement learning: Safely approximating value function. Advances Neural Information Processing Systems 7.
Campbell, M. E., & Udrea, B. (2002). Collision avoidance satellite clusters. Proceedings
American Control Conference.
Charnes, A., & Cooper, W. W. (1959). Chance-constrained programming. Management Science, 6,
7379.
Coles, A. J., Coles, A., Fox, M., & Long, D. (2012). Colin: Planning continuous linear numeric
change. J. Artif. Intell. Res. (JAIR), 44, 196.
Dechter, R. (2003). Constraint Processing. Elsevier.
Dechter, R., Meiri, I., & Pearl, J. (1991). Temporal constraint networks. Artificial Intelligence, 49,
6195.
Dolgov, D., & Durfee, E. (2005). Stationary deterministic policies constrained MDPs multiple rewards, costs, discount factors. Proceedings Nineteenth International
Joint Conference Artificial Intelligence (IJCAI-05, pp. 13261331.
Feng, Z., Dearden, R., Meuleau, N., & Washington, R. (2004). Dynamic programming structured
continuous markov decision problems. Proceedings Proceedings Twentieth
Conference Annual Conference Uncertainty Artificial Intelligence (UAI-04), pp. 154
161, Arlington, Virginia. AUAI Press.
575

fiO , W ILLIAMS , & B LACKMORE

Fleming, W., & McEneaney, W. (1995). Risk-sensitive control infinite time horizon. SIAM
Journal Control Optimization, 33(6), 18811915.
Fox, M., & Long, D. (2006). Modelling mixed discrete-continuous domains planning. Journal
Artificial Intelligence Research, 27, 235297.
Geibel, P., & Wysotzki, F. (2005). Risk-sensitive reinforcement learning applied control
constraints. Journal Artificial Intelligence Research, 24, 81108.
Goulart, P. J., Kerrigan, E. C., & Maciejowski, J. M. (2006). Optimization state feedback
policies robust control constraints. Automatica, 42(4), 523 533.
Hofmann, A. G., & Williams, B. C. (2006). Robust execution temporally flexible plans bipedal
walking devices. Proceedings International Conference Automated Planning
Scheduling (ICAPS-06).
Jacobson, D. (1973). Optimal stochastic linear systems exponential performance criteria
relation deterministic differential games. Automatic Control, IEEE Transactions on,
18(2), 124 131.
Japan Aerospace Exploration Agency (2009). HTV-1 mission press kit. Available on-line http:
//www.jaxa.jp/countdown/h2bf1/pdf/presskit_htv_e.pdf.
Kuwata, Y., & How, J. P. (2011). Cooperative distributed robust trajectory optimization using receding horizon MILP. IEEE Transactions Control Systems Technology, 19(2), 423431.
Kuwata, Y. (2003). Real-time trajectory design unmanned aerial vehicles using receding horizon
control. Masters thesis, Massachusetts Institute Technology.
Kvarnstrom, J., & Doherty, P. (2000). Talplanner: temporal logic based forward chaining planner.
Annals Mathematics Artificial Intelligence.
Lagoudakis, M. G., & Parr, R. (2003). Least-squares policy iteration. Journal Machine Learning
Research, 4, 2003.
Leaute, T. (2005). Coordinating agile systems model-based execution temporal plans.
Masters thesis, Massachusetts Institute Technology.
Leaute, T., & Williams, B. C. (2005). Coordinating agile systems model-based execution temporal plans. Proceedings Twentieth National Conference Artificial
Intelligence (AAAI).
Li, H., & Williams, B. C. (2005). Generalized conflict learning hybrid discrete linear optimization. Proc. 11th International Conf. Principles Practice Constraint Programming.
Li, H. X. (2010). Kongming: Generative Planner Hybrid Systems Temporally Extended
Goals. Ph.D. thesis, Massachusetts Institute Technology.
Matsumoto, S., Dubowsky, S., Jacobsen, S., & Ohkami, Y. (2003). Fly-by approach guidance
uncontrolled rotating satellite capture. Proceedings AIAA Guidance, Navigation,
Control Conference Exhibit.
Nemirovski, A., & Shapiro, A. (2006). Convex approximations chance constrained programs.
SIAM Journal Optimization, 17, 969996.
576

fiP ROBABILISTIC P LANNING C ONTINUOUS DYNAMIC YSTEMS B OUNDED R ISK

Oldewurtel, F., Jones, C. N., & Morari, M. (2008). tractable approximation chance constrained
stochastic MPC based affine disturbance feedback. Proceedings Conference Decision Control.
Ono, M. (2012). Closed-loop chance-constrained MPC probabilistic resolvability. Proceedings IEEE Conference Decision Control.
Ono, M., Graybill, W., & Williams, B. C. (2012). Risk-sensitive plan execution connected sustainable home:. Proceedings 4th ACM Workshop Embedded Systems (BuildSys).
Ono, M., & Williams, B. C. (2008a). efficient motion planning algorithm stochastic dynamic
systems constraints probability failure. Proceedings Twenty-Third AAAI
Conference Artificial Intelligence (AAAI-08).
Ono, M., & Williams, B. C. (2008b). Iterative risk allocation: new approach robust model
predictive control joint chance constraint. Proceedings 47th IEEE Conference
Decision Control.
Prekopa, A. (1999). use discrete moment bounds probabilistic constrained stochastic
programming models. Annals Operations Research, 85, 2138.
Richards, A., & How, J. (2006). Robust stable model predictive control constraint tightening.
American Control Conference, 2006, p. 6 pp.
Sanner, S. (2011). Relational dynamic influence diagram language (RDDL): Language description.
Available http://users.cecs.anu.edu.au/ssanner/IPPC_2011/RDDL.
pdf.
Schaub, H., & Junkins, J. L. (2003). Analytical mechanics space systems. American Institute
Aeronautics Astronautics, Inc.
Shields, J., Sirlin, S., & Wette, M. (2002). Metrology sensor characterization pointing control
formation interferometer testbed (fit). Proceedings IEEE Aerospace Conference.
Smith, R., & Hadaegh, F. (2007). Distributed estimation, communication control deep space
formations. IET Control Theory Applications.
Stoorvogel, A. (1992). H Control Problem: State Space Approach. Prentice Hall.
Vallado, D. A. (2001). Fundamentals Astrodynamics Applications, Second Edition. Microcosm Press.
van Hessem, D. H. (2004). Stochastic inequality constrained closed-loop model predictive control
application chemical process operation. Ph.D. thesis, Delft University Technology.
Wang, X., Yadav, V., & Balakrishnan, S. N. (2007). Cooperative uav formation flying obstacle/collision avoidance. IEEE Transactions Control Systems Technology, 15(4).
Wertz, J. R., & Wiley J. Larson, e. (1999). Space Mission Analysis Design (Third Edition).
Microcosm/Springer.
Younes, H. L. S., & Littman, M. L. (2004). PPDDL1.0: extension pddl expressing planning
domains probabilistic effects. Tech. rep., Carnegie Mellon University.

577

fiJournal Artificial Intelligence Research 46 (2013) 1 45

Submitted 07/12; published 01/13

Short Long Supports Constraint Propagation
Peter Nightingale
Ian P. Gent
Christopher Jefferson
Ian Miguel

pwn1@st-andrews.ac.uk
ian.gent@st-andrews.ac.uk
caj21@st-andrews.ac.uk
ijm@st-andrews.ac.uk

School Computer Science, University St Andrews,
St Andrews, Fife KY16 9SX, UK

Abstract
Special-purpose constraint propagation algorithms frequently make implicit use short
supports examining subset variables, infer support (a justification
variable-value pair may still form part assignment satisfies constraint)
variables values save substantial work short supports
studied right. two main contributions paper identification short supports important constraint propagation, introduction
HaggisGAC, efficient effective general purpose propagation algorithm exploiting short supports. Given complexity HaggisGAC, present optimised
version simpler algorithm ShortGAC. Although experiments demonstrate efficiency ShortGAC compared general-purpose propagation algorithms
compact set short supports available, show theoretically experimentally
HaggisGAC even better. also find HaggisGAC performs better
GAC-Schema full-length supports. also introduce variant algorithm HaggisGACStable, adapted avoid work backtracking cases faster
significant reductions memory use. proposed algorithms excellent
propagating disjunctions constraints. experiments disjunctions found
algorithms faster Constructive GAC-Schema least order
magnitude, three orders magnitude.

1. Introduction
Constraint solvers typically employ systematic backtracking search, interleaving choice
assignment decision variable propagation constraints determine
consequences assignment made. Propagation algorithms broadly divided
two types. first specialised reason efficiently constraint patterns
occur frequently models. Examples include global cardinality constraint (Regin,
1996) element constraint (Gent, Jefferson, & Miguel, 2006b). feasible
support every possible constraint expression specialised propagator way,
case general-purpose constraint propagators, GAC-Schema (Bessiere & Regin,
1997), GAC2001/3.1 (Bessiere, Regin, Yap, & Zhang, 2005), STR2 (Lecoutre, 2011)
MDDC (Cheng & Yap, 2010) used. typically expensive specialised
propagators important tool specialised propagator available.
support constraint domain value variable justification value
may still form part assignment satisfies constraint. usually given terms
set literals: variable-value pairs corresponding possible assignments
c
2013
AI Access Foundation. rights reserved.

fiNightingale, Gent, Jefferson, & Miguel

variables constraint. One efficiencies typically found specialised propagators
use short supports: examining subset variables, infer support
variables values save substantial work. use typically implicit,
i.e. achieved specialised algorithm examine variables
cases. One contributions highlight general importance short supports.
example, consider element constraint xy = z, x0 , x1 , x2 , {0 . . . 2},
z {0 . . . 3}. constraint satisfied iff element position vector [x0 , x1 , x2 ]
equals z. Consider set literals = {x0 7 1, 7 0, z 7 1}. set clearly satisfies
definition constraint xy = z, contain literal variable.
extension valid literals variables x1 x2 support. example
short support.
previous work introduced ShortGAC (Nightingale, Gent, Jefferson, & Miguel,
2011), general-purpose propagation algorithm exploits short supports. introduction ShortGAC, general-purpose propagators relied upon supports involving
variables. paper develop concept introduce new algorithm
HaggisGAC,1 consistently efficient ShortGAC. available,
use compact sets short supports allows HaggisGAC outperform greatly existing general-purpose propagation algorithms. cases, HaggisGAC even approaches
performance special-purpose propagators. HaggisGAC also well suited
propagating disjunctions constraints, outperforms traditional Constructive
algorithm (Lagerkvist & Schulte, 2009; Wurtz & Muller, 1996) orders magnitude.
HaggisGAC also efficient GAC-Schema full-length supports. also
describe variant, HaggisGAC-Stable, supports need deleted
backtracking. Applied full-length supports, version greatly reduced memory
usage.
ShortGAC, HaggisGAC HaggisGAC-Stable instantiated function named findNewSupport (and similar GAC-Schema way). function
specific constraint, generate short supports procedurally. Alternatively,
generic findNewSupport retrieve short supports data structure.
Section 2 presents necessary background, Section 3 introduces concept
short support. Section 4 outlines basic idea used deal implicit supports
throughout paper. Section 5 gives full details ShortGAC, including complexity
key operations alternative implementations short supports provided list
form. Section 6 presents new algorithm HaggisGAC development ShortGAC.
ShortGAC HaggisGAC evaluated experimentally Section 7. Section 8
describes HaggisGAC-Stable, corresponding experiments Section 9. Finally,
Sections 10 11 discuss related work present conclusions.

1. HaggisGAC named legendary wild haggis Scotland, short legs long
legs walking around hills. Like namesake, HaggisGAC copes full-length shorter
supports originates Scotland. Details wild haggis found Wikipedia, http:
//en.wikipedia.org/wiki/Wild_haggis, Veterinary Record (King, Cromarty, Paterson, &
Boyd, 2007).

2

fiShort Long Supports Constraint Propagation

2. Supports, GAC, Triggers
constraint satisfaction problem (CSP) defined set variables X, function
maps variable domain, : X 2Z domain finite set, set
constraints C. constraint c C relation subset variables X.
scope constraint c, named scope(c), set variables c constrains.
solution CSP function : X Z maps variable x X value
D(x), every constraint c C, values scope(c) form tuple
c (i.e. constraint satisfied ).
systematic search solution CSP, values progressively removed
domains D. Therefore, distinguish initial domains current
domains. function refers current domains unless stated otherwise. literal
defined variable-value pair, written x 7 v. literal x 7 v valid v
current domain x (i.e. v D(x)).
Definition 2.1. [Support] support constraint c domains defined
set valid literals contains exactly one valid literal variable scope(c)
satisfies c. necessary disambiguation, call support full-length
support simply long support, contrast short supports defined later.
property commonly established constraint propagation algorithms generalised
arc consistency (GAC) (Mackworth, 1977). constraint c GAC exists
full-length support every valid literal every variable scope(c). GAC established
identifying literals x 7 v full-length support exists removing v
domain x. consider algorithms establishing GAC paper.
GAC propagation algorithm usually situated systematic search. Hence,
must operate three contexts: initialisation (at root node), support established
scratch; following deletion one domain values (as result branching
decision and/or propagation constraints), support must re-established
selectively; upon backtracking, data structures must restored correct
state point search. primary focus second context, operation
following value deletion, although discuss efficient backtracking Section 8.
GAC propagation algorithm would typically called deleted domain value
turn. algorithm called domain value, constraint
GAC.
propagation algorithms present concept active support, inspired
GAC-Schema (Bessiere & Regin, 1997). active support support currently
use support set literals. literal set active supports support
it. active support found invalid, removed. set
literal empty, say literal lost support. new support sought literal,
found new support becomes active. new support found, literal
support deleted.
propagation algorithms present, efficiency make use watched literals
provided Minion (Gent et al., 2006b), propagators need called every
deleted domain value establish GAC. say propagators attach remove triggers
literals. domain value v variable x deleted, propagator called
3

fiNightingale, Gent, Jefferson, & Miguel

trigger attached literal x 7 v. means literal
deleted attached trigger, zero work incurred. emphasise
use watched literals fundamental work. available
given solver, algorithms need minor adaptation. called literal
removal, may return immediately literal active support,
checked time O(1). Thus algorithms fit traditional fine-grained scheme
(Bessiere & Regin, 1997) except cases invoked
use watched literals.

3. Short Supports
concept short support generalisation full-length support. defined below.
Definition 3.1. [Short support] short support constraint c domains
defined set valid literals x 7 v x scope(c), x occurs S,
every superset contains one valid literal variable scope(c)
full-length support. strict short support short support full-length
support.
definition short support includes extremes. empty set short support constraint entailed (i.e. every tuple scope(c) within satisfies
constraint). Similarly, every full-length support necessarily short support,
superset itself. case studies see examples empty short
supports short supports also happen full length.
Short supports used maintain GAC. full-length support,
short support provides GAC support literal contained within it. call
explicit support literals. new feature short support also provides
support valid literals variables contained short support.
because, definition, every valid extension short support cover variables
scope(c) full-length support. say short support gives implicit GAC support
valid literals variables short support.
also define concept complete set short supports constraint.
Definition 3.2. [Short support set] short support set S(c, D) set short supports
constraint c domains D, every full-length support c
(not necessarily strict) superset least one short support 0 S(c, D).
constraint may many short support sets. gives us latitude implement one efficient compute.
natural ask identify correct short supports given constraint c.
simple fundamental result given Lemma 3.3.
Lemma 3.3. Given constraint c domains D, empty set {} short support
c iff GAC propagation constraint not(c) leads empty domain.
Proof. {} short support every valid assignment variables scope(c)
satisfies c. Every assignment satisfies c iff every assignment violates not(c). every assignment violates not(c), GAC propagation constraint not(c) leads empty
4

fiShort Long Supports Constraint Propagation

domain. complete last equivalence, note assignment violate
not(c), literals assignment supported, GAC propagation cannot cause
empty domain.
lemma two important consequences. First, check short support
correctness, empty support. check short support = {x1 7 v1 , . . . , xk 7
vk }, simply set D(x1 ) = {v1 }, . . . , D(xk ) = {vk }. assignments extend S,
short support iff {} is. Lemma 3.3 applies check correctness
propagating not(c) seeing domain emptied.
second consequence negative, however. Determining whether GAC propagation
empty domain polynomially equivalent actually performing GAC propagation
(Bessiere, Hebrard, Hnich, & Walsh, 2007). Since constraints NP hard GAC
propagate, follows easy even check empty set short support.
Thus cannot expect find method fast general finding short
supports constraint.
Given provable difficulty finding short supports set full-length supports,
construct sets short supports specifically three experimental case studies
Section 7. focus paper show value strict short supports
given system. situation analogous important area
constraints, namely exploiting symmetries constraint problems (Gent, Petrie,
& Puget, 2006). large majority research assumed sets symmetries
provided system, even though finding sets hard. inhibited
research exploiting symmetry, within automated detection symmetry
become important subarea (Mears, 2009; Puget, 2005): however leave automated
construction compact short support sets future research. Analogously patterns
matrix symmetries (Flener, Frisch, Hnich, Kiziltan, Miguel, Pearson, & Walsh, 2002),
least identify pattern often lets us identify strict short supports,
describe.
3.1 Short Supports Disjunction
Strict short supports arise naturally disjunctions. constraint expressed
disjunction shorter constraints, set strict short supports constructed
it. Suppose following constraint.
c(x1 , x2 , x3 , x4 ) c1 (x1 , x2 ) c2 (x2 , x3 ) c3 (c3 , x4 )
Suppose also = {x1 7 2, x2 7 1} valid assignment satisfies c1 . satisfy
c1 , satisfy c regardless values x3 x4 . Therefore = {x1 7 2, x2 7 1}
strict short support c.
Lemma 3.4. Given constraint c, domain set D, set constraints {c1 . . . ck }
ci {c1 . . . ck } : scope(ci ) scope(c) c c1 ck , following short support
set (where write fls(ci , D) mean full-length supports ci w.r.t. domains D):
S(c, D) = {S | fls(c1 , D) fls(ck , D)}
5

fiNightingale, Gent, Jefferson, & Miguel

Proof. (a) element S(c, D) short support according Definition 3.1
semantics disjunction. (b) S(c, D) short support set Definition 3.2. Every fulllength support c must satisfy disjunct ci , therefore full-length support contains
full-length support ci included S(c, D).
Lemma 3.4 allows short support set created disjunction, given initial
domains. two three case studies (for third, set prohibitively
large).
Using similar approach Lemma 3.4 create function generates short
supports demand. function takes valid literal x 7 v current domains
D, returns short support supports x 7 v (explicitly implicitly), Null
none exists. function constructed follows. create new domains D0
D0 (x) = {v}, otherwise D0 identical D. disjunct satisfiable D0 ,
function returns Null. Otherwise, function picks disjunct ck satisfiable
D0 , returns satisfying assignment ck valid D0 .
three case studies Section 7, created function follows scheme
optimisations.
Propagating disjunctions recognised important topic. Many papers
published area (Wurtz & Muller, 1996; Lhomme, 2003; Lagerkvist & Schulte,
2009; Jefferson, Moore, Nightingale, & Petrie, 2010). Exploiting strict short supports
algorithms ShortGAC, HaggisGAC HaggisGAC-Stable allows us outperform
traditional Constructive algorithm (Wurtz & Muller, 1996) orders magnitude.
3.2 Backtrack Stability Short Supports
Within search tree, propagation algorithms often spend significant time backtracking
data structures. Reducing eliminating backtracking improve efficiency. example,
avoiding backtracking triggers speed simple table propagator 2 times
(Gent et al., 2006b), MAC-6 MAC-7 much efficient (in space
time) backtracking avoided (Regin, 2005). two potential advantages
reducing use backtracking state: saves time restoring data structures, saves
space avoiding storing supports backtrack stack.
Definition 3.5. [Backtrack Stable] short support constraint c current domains
backtrack stable iff always remains short support (according Definition 3.1)
backtracking search tree.
short support may support variable x implicitly, backtrack may
add values back domain x consistent s, meaning
longer meets definition short support. give example below.
Example 3.1. Consider constraint b [x] = y, boolean variable b, array
variables variables x y. b assigned False, constraint entailed,
empty short support used support literals M, x y.
support backtrack stable, backtracking True restored domain
b, empty set longer short support.
6

fiShort Long Supports Constraint Propagation

support full length backtrack stable: whenever support valid
supports literals contains. Backtrack stable supports always exist use
full-length supports cases (as GAC-Schema), although may much longer
necessary.
Section 8 exploit backtrack stability define new algorithm.

4. ShortGAC: Overview
section summarises key ideas ShortGAC propagation algorithm, along
illustrative example.2
ShortGAC maintains set short supports sufficient support valid literals
variables scope constraint propagating. refer active
supports. algorithm rests exploiting observation that, using short supports,
support established literal two ways. First, usual, short support
contains literal supports literal. Second, literal x 7 v supported short
support contains literal variable x. Hence, short supports
support x 7 v contain literal x 7 w value w 6= v.
following data structures central operation ShortGAC algorithm:
numSupports total number active short supports.
supportsPerVar array (indexed [x]) indicating number active short supports
containing variable x.
supportListPerLit array (indexed [x 7 v]) lists active short supports containing literal x 7 v.
number supports containing variable x less total number
supports exists support contain x. Therefore, supports
literals x. algorithm spends time processing variables whose literals
known supported way. variables involved active supports
seek support literals active supports.
illustrate, consider element example introduction: xy = z,
x0 , x1 , x2 , {0 . . . 2}, z {0 . . . 3}. constraint satisfied iff element
position vector [x0 , x1 , x2 ] equals z. Suppose current state ShortGAC storing
one support: = {x0 7 1, 7 0, z 7 1}. data structures follows,
indicates literal valid.3
2. details present different presented previously (Nightingale et al., 2011),
optimised data structures algorithms compared previous work. two
significant changes are: longer keep count supports per literal, saving overhead maintaining
this; data stored one dimensional vector literal, instead two dimensional array
variable/value, saving space variables constraint different domain sizes. Experiments
Appendix demonstrate algorithms data structures presented perform better
previous implementation.
3. clarity, presented one-dimensional array supportListPerLit two-dimensional format.

7

fiNightingale, Gent, Jefferson, & Miguel

Supports:
supportListPerLit:
Value
0
1
2
3
supportsPerVar:
numSupports:

x0
7 1, 7 0, z
Variable
x1 x2

{} {} {A}
{} {}
{}
{} {}
{}


0
0
1
1

A:
x0
{}
{A}
{}

1

7 1
z
{}
{A}
{}
{}
1

values x1 x2 support, since supportsPerVar counters
less numSupports. Therefore ShortGAC algorithm ignore x1 x2
look new supports x0 , z. Consider finding new support literals
z. ShortGAC ignore literals least one support case z 7 1.
algorithm looks literals z 7 supportListPerLit[z, a] = {}. Here, z 7 0
literal, ShortGAC seeks new support it. possible new support
B = {x1 7 0, 7 1, z 7 0}. Following discovery, update data structures:
Supports:
supportListPerLit:
Value
0
1
2
3
supportsPerVar:
numSupports:

A:
B:
x0
{}
{A}
{}

1

x0
7 1, 7 0, z
x1
7 0, 7 1, z
Variable
x1
x2

{B} {} {A}
{}
{} {B}
{}
{}
{}



1
0
2
2

7 1
7 0
z
{B}
{A}
{}
{}
2

variable x0 also fully supported, since supportsPerVar[x0 ] < numSupports.
remain three literals support established: 7 2, z 7 2 z 7 3.
first two ShortGAC finds supports C = {x0 7 2, 7 0, z 7 2}
= {x2 7 0, 7 2, z 7 0}. support exists z 7 3, 3 deleted, giving:
Supports:

supportListPerLit:
Value
0
1
2
3
supportsPerVar:
numSupports:

A:
B:
C:
D:
x0
{}
{A}
{C}

2

7 1, 7 0, z
7 0, 7 1, z
7 2, 7 0, z
7 0, 7 2, z
Variable
x2

{D} {A, C}
{}
{B}
{}
{D}


1
4
4

x0
x1
x0
x2
x1
{B}
{}
{}

1

7 1
7 0
7 2
7 0
z
{B, D}
{A}
{C}

4

valid literals supported. Nothing need done change
state, removal value branching decision propagation.
8

fiShort Long Supports Constraint Propagation

5. ShortGAC: Details
key tasks implementing ShortGAC are: data structure update; iteration
variables supportsPerVar equals numSupports; iteration unsupported
values variable. section describes infrastructure allows us perform
tasks efficiently.
5.1 ShortGAC Data Structures
active short support arity k provides explicit support k literals
contains. Therefore, reference must appear k lists supportListPerLit.
this, represent two types object: ShortSupport ShortSupportCell.
ShortSupport object contains k ShortSupportCell objects, contains literal
x 7 v,4 reference parent ShortSupport. elements array supportListPerLit doubly-linked lists ShortSupportCells. reference parent
ShortSupport, iterate active short supports given literal.
algorithm iterates variables x supportsPerVar[x] equals numSupports.
following data structure represents partition variables number supports. allows constant time size checking linear-time iteration cell
partition, allows variable moved adjacent cell (i.e. number
supports increases decreases 1) constant time. inspired indexed
dependency array Gecode (Schulte & Tack, 2010).
varsBySupport array containing permutation variables. Variables ordered
non-decreasing number active supports (supportsPerVar[x]).
supportNumLowIdx array integers, indexed 0 number literals,
maximum number active supports possible. Either supportNumLowIdx[i]
smallest index varsBySupport active supports, (when
variables) supportNumLowIdx[i]= k k total number variables.
k acts sentinel value. set variables supports is:
varsBySupport[supportNumLowIdx[i] . . . supportNumLowIdx[i + 1] 1]
Initially, variables 0 active supports, supportNumLowIdx[0] = 0 rest
array set k.
following table illustrates partition data structure works (on different
example 11 variables). Suppose supportsPerVar[x2 ] changed 7 6. x2 y1
(boxed) swapped varsBySupport cell boundary moved x2
lower cell. Consequently, supportNumLowIdx[7] incremented 1.
varsBySupport[]
supportsPerVar
x2 updated

w1
6
w1

w2
6
w2

y1
7
x2

x1
7
x1

x2
7
y1

y2
7
y2

y3
7
y3

x3
7
x3

z1
8
z1

z2
8
z2

z3
8
z3

4. literal x 7 v represented using single integer i. mapping x 7 v i,
allows O(1) access x v vice-versa.

9

fiNightingale, Gent, Jefferson, & Miguel

Require: sup: ShortSupport
1: sc: ShortSupportCell sup
2:
(x 7 v) sc.literal
3:
supportListPerLit[x 7 v] = {}
4:
attachTrigger(x 7 v)
5:
Add sc doubly linked list supportListPerLit[x 7 v]
6:
supportsPerVar[x]++
7:
sx supportsPerVar[x]
8:
cellend supportNumLowIdx[sx ]1
9:
swap(x, varsBySupport[cellend ])
10:
supportNumLowIdx[sx ]-11: numSupports++

Procedure 1: addSupport(sup)
variable x supportsPerVar[x] = numSupports, ShortGAC iterates
values zero explicit supports. avoid iterating values, use set data
structure:
zeroLits array (indexed [x]) stacks containing literals variable x zero
explicit support, particular order.
inZeroLits array (indexed [x 7 v]) booleans indicating whether literal x 7
v zeroLits[x].
supportListPerLit[x 7 v] reduced empty list, inZeroLits[x 7 v]
false x 7 v pushed onto zeroLits[x] (and inZeroLits[x 7 v] set true).
optimisation, values eagerly removed set; removed lazily
set iterated. Also, set backtracked. iteration, non-zero value
removed swapping top stack, popping. lazy maintenance never
costs work overall because, value would removed eagerly,
removed next time set iterated, costing O(1). save work, may
never iterate list value would restored set again.
use free list manage set ShortSupport objects avoid cost unnecessary object construction/destruction. ShortSupport object retrieved free list
may contain ShortSupportCell objects, use resizable vector data structure.
size ever increased.
5.2 Adding Deleting Supports
support added deleted, data structures described must
updated. done Procedures 1 (addSupport) 2 (deleteSupport).
procedures iterate given short support, literal update
supportListPerLit, supportsPerVar, varsBySupport supportNumLowIdx. Procedure 2 also
inserts literal zeroLits necessary. briefly explain maintenance varsBySupport become important Section 6.2. Suppose adding support
literal x 7 v Procedure 1. additional support, x must moved
next cell varsBySupport. Line 8 finds end cell x in, swap x
10

fiShort Long Supports Constraint Propagation

Require: sup: ShortSupport
1: sc: ShortSupportCell sup
2:
(x 7 v) sc.literal
3:
Remove sc doubly-linked list supportListPerLit[x 7 v]
4:
supportsPerVar[x]-5:
supportListPerLit[x 7 v] = {}
6:
removeTrigger(x 7 v)
7:
inZeroLits[x 7 v]
8:
inZeroLits[x 7 v] true
9:
zeroLits[x].push(x 7 v)
10:
sx supportsPerVar[x]
11:
cellend supportNumLowIdx[sx +1]
12:
swap(x, varsBySupport[cellend ])
13:
supportNumLowIdx[sx +1]++
14: numSupports--

Procedure 2: deleteSupport(sup)

Require: x 67 v (where v pruned domain x)
1: supportListPerLit[x 7 v] 6= {}
2:
deleteSupport(supportListPerLit[x 7 v].pop())
3: repeat
4:
continueLoop false
5:
{supportNumLowIdx[numSupports]. . . supportNumLowIdx[numSupports+1]-1}
6:
varsBySupport[i]
7:
ShortGAC-variableUpdate(y) = true
8:
continueLoop true
9:
break loop Line 5
10: continueLoop = false

Procedure 3: ShortGAC-Propagate: propagate(x 67 v)

end cell using subroutine swap(xi , xj ). simple procedure (not given) locates
swaps two variables varsBySupport, leaving variables unaffected.
makes use second array, varsBySupInv, inverse mapping varsBySupport.
done this, cell boundary decremented (in new position), x
higher cell. Another point note addSupport add trigger x 7 v
sup active explicit support contain literal, deleteSupport
remove trigger deleted support support.
Finally, note special-purpose methods undo changes
backtracking. backtracking past point support added, simply
call deleteSupport, similarly call addSupport backtrack past supports
deletion.
11

fiNightingale, Gent, Jefferson, & Miguel

Require: variable x
1: (x 7 v) zeroLits[x]
2:
supportListPerLit[x 7 v] 6= {}
3:
Remove (x 7 v) zeroLits[x]
4:
else
5:
v D(x)
6:
sup findNewSupport(x 7 v)
7:
sup = Null
8:
prune(x 7 v)
9:
else
10:
addSupport(sup)
11:
supportListPerLit[x 7 v] 6= {}
12:
Remove (x 7 v) zeroLits[x]
13:
return true
14: return false

Procedure 4: ShortGAC-variableUpdate: (x). pseudocode abstract
detailed maintenance zeroLits inZeroLits data structures. might seem
test Line 11 must always succeed. However, although sup must support x 7 v,
contain x 7 v might implicit support. findNewSupport
function discussed Section 5.5.
5.3 Propagation Algorithm
ShortGAC propagator (Procedure 3) invoked literal contained
one active short supports pruned.5 first deletes supports involving
pruned literal. checks variables implicitly supported, i.e.
supportsPerVar[y]=numSupports (Line 5). variable checked Procedure 4
(ShortGAC-variableUpdate, described below). call results new support
found, data structures changed (ShortGAC-variableUpdate(y) returns
true indicate this) must break for-all-loop (Line 9) go round again.
Iteration therefore continues either new support necessary new support
found.
ShortGAC-variableUpdate (Procedure 4) used check status every variable
lacking implicit support. iterates zeroLits, i.e. literals variable might
zero explicit supports. Since zeroLits maintained lazily, iteration first
check literal indeed explicit support, correct zeroLits necessary
(Lines 23). important case literal indeed support. Then, provided
v current domain x, must seek new support calling findNewSupport
constraint. support, value v must pruned domain x,
found support update data structures calling addSupport.
initialise data structures root search, Lines 310 Procedure 3 invoked.
Notice lines refer parameter x 67 v, first calling
supports initial iteration Line 5 variables.
5. noted earlier, watched literals available solver, simple check made
start procedure, return immediately removed literal active support.

12

fiShort Long Supports Constraint Propagation

5.4 Complexity Analysis ShortGAC
section provide complexity analysis ShortGAC used incrementally search constraint solver. analysis parameters arity
constraint n, maximum domain size d, cost f calling findNewSupport.
assume attaching removing trigger literal O(1). case
Minion 0.12.
First observe swap procedure executes O(1) time: operation swap
O(1) loop. Secondly establish time complexity procedures
addSupport deleteSupport, key algorithm.
Lemma 5.1. Procedure 1 (addSupport) time complexity O(n).
Proof. outer loop Line 1 iterates literals short support. worst
case, n literals. consider steps within loop. list test
Line 3 O(1), call attachTrigger Line 4. Adding ShortSupportCell
doubly-linked list Line 5 O(1), following five array dereferences.
established above, swap procedure also O(1). Hence, addSupport O(n).
Lemma 5.2. Procedure 2 (deleteSupport) time complexity O(n).
Proof. Similarly add Support procedure, outer loop Line 1 n
iterations. removal doubly-linked list Line 3 O(1), array
dereferences Line 4 subsequently. list test Line 5 call removeTrigger Line 6 O(1), stack push operation Line 9. Recalling
swap procedure O(1), deleteSupport O(n).
Theorem 5.3. Procedure 3 (ShortGAC-propagate) time complexity O(n2 d2 +ndf ).
upper bound obtained, i.e. worst case time complexity (n2 d2 + ndf ).
Proof. Analysis first statement breaks three parts.
First, loop Line 1 elements supportListPerLit. worst case occurs
nd literals explicit support. supports, maximum (n 1)d + 1
involve particular literal, literal may short support every
literal every variable ((n 1)d), (1). cost body loop
O(n) Lemma 5.2, total O(n2 d). dominated next part.
second part loop lines 310. maximum number iterations
Line 5 n supports full length iteration Line 5 contains n
variables. Successive calls Procedure 4 Line 7 add O(d) new supports.
support addition triggers restart loop beginning Line 5 n
variables, total O(n2 d) calls Procedure 4. call involves O(d)
iterations loop Line 1 Procedure 4. Therefore innermost loop run
O(n2 d2 ) times.
complete proof first statement, consider cost innermost loop
Procedure 4. Within loop, operations O(1), exceptions call
findNewSupport Line 6 (cost f ) call addSupport Line 10 (cost n
Lemma 5.1). f dominating cost, since must least traverse new support
record it. However, n2 d2 iterations, nd calls findNewSupport,
13

fiNightingale, Gent, Jefferson, & Miguel

time valid literals explicit support. cost either O(n2 d2 )
O(ndf ), whichever greater. case cost O(n2 d2 + ndf ).
upper bounds ndf n2 d2 attained worst case. literal
needs new support, (ndf ) calls findNewSupport. cost (n2 d2 )
nd literals explicit support, size n, variable ends
(for example) d/2 values supported d/2 values deleted. worst case thus
(n2 d2 + ndf ).
Procedure 3 invoked n(d 1) times one branch search tree,
therefore complexity one branch O(n3 d3 + n2 d2 f ).
5.4.1 Second Complexity Analysis
analysis conservative total number, maximum size,
short supports small. Therefore, give another complexity analysis two additional
parameters: maximum length l short supports returned findNewSupport,
total number distinct short supports may returned findNewSupport.
analysis also pertains branch search rather single call propagate
algorithm.
first part complexity analysis concerns short supports length l.
short support may added active set once, may deleted
branch. short support must also found calling findNewSupport, cost O(f ).
Lemma 5.1 shows addSupport procedure takes O(n) time. lemma
re-stated terms l, loop addSupport iterate O(l) times, giving
total time O(l). also applies deleteSupport. Since short supports,
cost finding, adding deleting (collectively processing) short supports O(s(l + f ))
branch.
Secondly, algorithm may make calls findNewSupport return Null.
happen n(d 1) + 1 times, maximum number domain values
may deleted. Therefore cost O(ndf ).
addition, ShortGAC operations charged either
categories. analyse these, must top-down analysis algorithm.
Procedure 3 invoked O(s) times (each time short support invalidated). Lines 12
already charged processing short supports. body loop lines 310 may
executed times new support found, times new
support found, therefore O(s) times total branch search.
come inner loop lines 59. Lemma 5.4 (below), unless domain
empty always one active short support. Therefore, l variables
contained active short supports, l variables relevant
partition varsBySupport, loop body executed O(l) times.
Lemma 5.4. initialisation, Procedure 3 always least one active short support
variable domain empty.
Proof. Suppose opposite. algorithm invoked time literal active short
support pruned, therefore delete active short supports must contain one
literal x 7 v. active short supports contain variable x, values domain
14

fiShort Long Supports Constraint Propagation

x implicitly supported must explicitly supported. Therefore v must
last remaining value D(x). prune x 7 v empties domain
contradiction.
branch, causes O(sl) calls ShortGAC-variableUpdate, Line 7.
call ShortGAC-variableUpdate takes O(d) time may 1 invalid
literals explicitly supported literals zeroLits. time spent procedure
charged processing short supports, pruning domains. Therefore top-down
analysis cost O(sld).
Overall, time complexity O(s(l + f ) + ndf + sld), tighter bound cases
one given section above. example, SAT clause = n, f = n, l = 1
= 2, giving time complexity O(n2 ) branch search.
5.5 Instantiation findNewSupport
Similarly GAC-Schema (Bessiere & Regin, 1997), ShortGAC must instantiated
findNewSupport function. function takes valid literal, returns support one
exists, otherwise returns Null. One way write specialist findNewSupport
function constraint. empirical case studies below.
case, findNewSupport function much simpler propagator
constraint. use Lemma 3.4 build findNewSupport functions, reduces
task finding satisfying tuples simple constraints like x < x = y.
alternative write generic version findNewSupport case
short supports given list. detail two generic instantiations findNewSupport lists, case studies compare specialist functions.
5.5.1 findNewSupport-List
provide generic instantiation named findNewSupport-List (Procedure 5) takes
list short supports literal (supportList), including explicit implicit
short supports literal. analogous Positive instantiation GACSchema (Bessiere & Regin, 1997). FindNewSupport-List persistent state: listPos,
array integers indexed variable value, initially 0. indicates current
position supportList. algorithm simply iterates list supports,
seeking one literals valid. ListPos backtracked, consequence
end list reached, cannot fail immediately must search
start back listPos. branch search tree, particular element
list may looked once. However, algorithm optimal
time space across search tree (Gent, 2012). surprising result achieved
amortizing cost across branches. Practically, using listPos stops algorithm always
starting first element list, seems good tradeoff avoiding
provably unnecessary work much data structure maintenance.
constraint-specific findNewSupport sometimes find shorter supports findNewSupport-List. specific findNewSupport take advantage current
domains whereas supportList may contain supports given initial domains.
example, constraint becomes entailed, specific findNewSupport return
15

fiNightingale, Gent, Jefferson, & Miguel

Require: x, v, supportList
1: j {listPos[x, v]. . .(supportList[x, v].size-1)}
2:
sup supportList[x, v, j]
3:
literals sup valid
4:
listPos[x, v] j
5:
return sup
6: j {0 . . .listPos[x, v]1}
7:
sup supportList[x, v, j]
8:
literals sup valid
9:
listPos[x, v] j
10:
return sup
11: return Null

Procedure 5: findNewSupport-List: findNewSupport(x, v). first block searches
location previous support end support list. unsuccessful
search restarts start list second block. circular approach removes
need backtrack listPos.
empty support whereas list version presented cannot. exploit fact
Case Study 3 below.
5.5.2 findNewSupport-NDList
list instantiation two major disadvantages. First, inefficient
unable skip sets invalid tuples. literature contains many solutions
problem context full-length supports, example binary search (Lecoutre &
Szymanek, 2006) tries (Gent, Jefferson, Miguel, & Nightingale, 2007). Second,
require large amount memory. short support S, potentially nd
pointers S, pointer literal implicitly supports.
section give second generic list instantiation based NextDifference lists
(Gent et al., 2007). single list (named supportList) containing short supports (indexed integer), second list named NDList support
=supportList[j], literal support s[k], NDList[j][k] index next
support contain literal s[k]. Thus, searching list, algorithm
able jump sets short supports contain invalid literal.
version findNewSupport NextDifference lists given Procedure 6.
approach solves problems list instantiation: able jump
sets invalid short supports, usually requires substantially less memory. fact
optimal space (unlike list instantiation): given short supports length
l, NextDifference list O(tl). However uses one list supports, therefore
spend time searching short supports support desired literal.
5.6 Literals Assigned Variables
Suppose ShortGAC discovers new support contains literal x 7 v, x assigned v. Since x take value v, sound remove x 7 v
save overhead adding it. apply minor optimisation cases using
ShortGAC, also cases using HaggisGAC (described Section 6). How16

fiShort Long Supports Constraint Propagation

Require: x, v, supportList, NDList
1: j listPos[x, v]
2: j < supportList.size
3:
sup supportList[j]
4:
nextDiff NDList[j]
5:
k {0 . . . sup.size 1}
6:
(y 7 b) sup[k]
7:
b
/ D(y) (x = v 6= b)
8:
j nextDiff [k] {Jump next short support assigned different value.}
9:
continue loop Line 2
10:
listPos[x, v] j
11:
return sup
12: j 0
13: j < listPos[x, v]
14:
sup supportList[j]
15:
nextDiff NDList[j]
16:
k {0 . . . sup.size 1}
17:
(y 7 b) sup[k]
18:
b
/ D(y) (x = v 6= b)
19:
j nextDiff [k] {Jump next short support assigned different value.}
20:
continue loop Line 13
21:
listPos[x, v] j
22:
return sup
23: return Null

Procedure 6: findNewSupport-NDlist: findNewSupport(x, v)

ever optimisation cannot used HaggisGAC-Stable (described Section 8)
algorithm retains active supports backtracks, backtracking
literal x 7 v may longer assigned.

6. HaggisGAC: Dealing Full-Length Strict Short Supports
introduce HaggisGAC. show better theoretical properties
ShortGAC. Furthermore, experiments show runs substantially faster many cases
strict short supports ShortGAC (which specialised strict short supports),
substantially faster full-length supports GAC-Schema.
6.1 Introduction Motivating Example
ShortGAC designed exploit concept implicit support, inefficiencies dealing explicit supports especially full-length supports. Consider
example constraint AllDifferentExceptZero, constraint non-zero
values array must different, zero may occur freely. constraint might
used, example, timetabling problem classes taking place different rooms
must different, use zero represent room unused occur
multiple times. Suppose AllDifferentExceptZero([w, x, y, z]), variable initial domain {0, 1, 2, 3}. Supports constraint full-length supports every
17

fiNightingale, Gent, Jefferson, & Miguel

non-zero value different, three variables equalling zero last variable may
take value. Suppose execute ShortGAC reach following situation:
Supports:

supportListPerLit:
Value
0
1
2
3
supportsPerVar:
numSupports:

A:
B:
C:
D:
E:
w
{A, B, E}
{}
{}
{C}
4

w
7 0, x 7 2, 7 3, z 7 1
w
7 0, x 7 3, 7 2, z 7 1
w
7 3, x 7 0, 7 1, z 7 2
x 7 0, 7 0, z 7 0
w 7 0, x 7 1, 7 2, z 7 3
Variable
x

z
{C, D}
{D}
{D}
{E}
{C}
{A, B}
{A}
{B, E}
{C}
{B}
{A}
{E}
5
5
5
5

Notice lack explicit supports w 7 1 w 7 2 acceptable
supportsPerVar[w] = 4 < numSupports = 5. suppose literal 7 0 deleted
constraint. causes support deleted, causing following state:
Supports:

supportListPerLit:
Value
0
1
2
3
supportsPerVar:
numSupports:

A:
B:
C:
E:

7 0, x 7 2,
7 0, x 7 3,
7 3, x 7 0,
7 0, x 7 1,
Variable
x

{C}

{E}
{C}
{A} {B, E}
{B}
{A}
4
4
4

w
w
w
w

w
{A, B, E}
{}
{}
{C}
4

7 3, z
7 2, z
7 1, z
7 2, z

7 1
7 1
7 2
7 3

z
{}
{A, B}
{C}
{E}
4

point ShortGAC iterates zeroLits lists variables
supportsPerVar = numSupports, case four variables. discover must
find new supports w 7 1, w 7 2 z 7 0. However, inefficient two reasons.
First, need check zeroLits[z] discover z 7 0, support list
z 7 0 became empty deletion support D, could discovered then.
Second, need look zeroLits[w] deletion caused
w lose implicit support. need check zeroLits x, y, z
variables implicitly supported prior Ds deletion. Removing two
reasons inefficiency motivation behind development HaggisGAC.
example, focus directly literal z 7 0 set zeroLits[w] literals
potentially needing new support.
fundamental problem ShortGAC cannot efficiently detect
literal loses last support. Every variable implicit support checked every time
support deleted, ShortGAC take O(nd) time find single literal needs
new support discover literal. improve upon this, wish
18

fiShort Long Supports Constraint Propagation


varsBySupport[i]
supportsPerVar
x2 updated
x3 updated
z2 updated
x1 updated
z3 updated
z1 updated
supportsPerVar

0
w1
6
w1
w1
w1
w1
w1
w1
6

1
w2
6
w2
w2
w2
w2
w2
w2
6

2
y1
7
x2
x2
x2
x2
x2
x2
6

3
x1
7
x1
x3
x3
x3
x3
x3
6

4
x2
7
y1
y1
y1
x1
x1
x1
6

5
y2
7
y2
y2
y2
y2
y2
y2
7

6
y3
7
y3
y3
y3
y3
y3
y3
7

7
x3
7
x3
x1
x1
y1
y1
y1
7

8
z1
8
z1
z1
z2
z2
z2
z2
7

9
z2
8
z2
z2
z1
z1
z3
z3
7

10
z3
8
z3
z3
z3
z3
z1
z1
7

Figure 1: Illustration deleteSupport concentrates variables lost
last implicit support. See main text full description.

HaggisGAC able detect loss literals last explicit support time O(1),
loss variables last implicit support time O(1). Perhaps surprisingly,
goals achievable use data structures already existing ShortGAC.
6.2 Finding Literals Support Efficiently
two types support, detecting last explicit support literal lost
simpler task. delete support, Procedure 2 iterates literals
short support. literal removes ShortSupportCell corresponding
supportListPerLit updates data structures appropriately. list empty tested
Line 5 Procedure 2 literal lost last explicit support. add literal
scratch list literals lost last explicit support: describe
process scratch list. additional cost O(1) detect empty list.
inside existing test, zero additional cost literal
lost last support. contrasts ShortGAC tests (in Procedure 4) every
variable implicit support, worst case cost O(n) even literal
lost last explicit support.
subtle task detect variable (and thus literals involving it)
lost last implicit support. reason difficult seeking
variables involved support deleted, Procedure 2 iterate
literals support deleted. variables seek
x supportsPerVar[x] = numSupports support deletion,
supportsPerVar[x] < numSupports support deletion. (Variables
supportsPerVar[x] = numSupports deletion implicit support
now, lose implicit support deletion.) Fortunately, existing
maintenance data structures happens compact exactly variables particular
region varsBySupport, find easily efficiently. compaction
happens sequence calls Procedure swap made Procedure 2.
first show worked example prove general properties need.
Figure 1, suppose 11 variables constraint, currently 8
supports, deleting support involving variables x1 , x2 , x3 , z1 , z2 z3 ,
19

fiNightingale, Gent, Jefferson, & Miguel

literals deleted arbitrary order top (start) bottom (finish).
start, z variables already supportsPerVar = numSupports = 8; variables x
supportsPerVar = 7; variables w supportsPerVar = 6. process literals
deleteSupport, pairs variables swapped (marked boxes line)
boundaries move cells (marked vertical lines) variables equal supportsPerVar. end, w x variables still supportsPerVar = 6 < numSupports = 7.
z variables supportsPerVar=numSupports deletion.
variables lost last implicit support variables. crucial point
end lie precisely final boundary 6 7 supports
(from = 5), initial boundary 7 8 supports (from = 8). following
simple results show variables losing last implicit support always compacted
similar way.
Lemma 6.1. Suppose, delete support S, numSupports = p (and numSupports = p 1 afterwards). variable x lose last implicit support, p 1
explicit supports deletion S.
Proof. x initially fewer p1 explicit supports, x one implicit
support deleting removes one these. x initially p explicit supports,
involved (since involved supports) implicit support
lose. Hence, x must initially p 1 explicit supports one implicit support
must one implicit support. Therefore deletion S, x p 1 explicit
supports implicit supports.
Lemma 6.2. set p Lemma 6.1, value supportNumLowIdx[p]
deleteSupport called, j value supportNumLowIdx[p 1] deleteSupport
exits. deleteSupport finishes, variables lost last implicit support
call deleteSupport exactly set variables indices range [j, i)
varsBySupport.
Proof. variables implicit supports deleteSupport exits lie index j
greater varsBySupport. establishes lower bound index range.
variable z implicit support start call must p explicit
supports must index higher. z must support deleted,
supports. z updated deleteSupport, always swapped
variable index supportNumLowIdx[p]. index supportNumLowIdx[p] increases
deleteSupport, z stays index higher throughout. Thus variables
index upwards finish permutation start, meaning variables
lost last implicit support must range [j, i). Finally, variable
range [j, i) implicit support end call (as index j above)
implicit support start (as i). Therefore variables
lost last implicit support lie indices range [j, i).
Lemma 6.2, run deleteSupport trivial enumerate variables
lost last implicit support result. exactly variables
varsBySupport[k] k = j, j + 1, ...i 1 j defined Lemma. Enumerating
list additional work already done Procedure 2, have:
20

fiShort Long Supports Constraint Propagation

Corollary 6.3. Given constraint n variables, additional work identify variables
lost last implicit support O(1) variable
some, O(1) none.
Proof. already argued case variables lost implicit
support. variables, still O(1) work check range
empty.
low level complexity contrasts favourably ShortGAC. support deleted, Procedure 4 iterates variables numSupports explicit supports.
worst case O(n) work even variable lost last implicit support, compared O(1) work have. move details incorporating
optimisations full suite procedures maintaining GAC.
6.3 HaggisGAC: Details
Two issues complicate implementation HaggisGAC compared ShortGAC.
First, Lemmas depend literals support deleted single pass.
Therefore, instead acting immediately finding literal supports, keep list
literals lost supports later treatment. Second, two cases
might detect lost support lost support explicit implicit compared
single case ShortGAC, lost supports detected way.
introduce two simple data structures storing literals variables lost
explicit implicit support find them.
litsLostExplicitSupport set containing literals lost final explicit support
supported implicitly.
varsLostImplicitSupport set containing variables lost final implicit
support.
adapt deleteSupport procedure Procedure 2. new version
shown Procedure 7. find literal explicit support, immediately
check implicit support instead (Line 8). not, add
set litsLostExplicitSupport later processing find new support delete it. Variables
implicit support detected literals deleted. done
lines 15-16, justified Lemma 6.2.
new propagate procedure shown Procedure 8. Like earlier Procedure 3,
first delete supports involving literal deleted, rest procedure
different. first iterate literals lost last explicit support,
variables lost last implicit support.
lost explicit supports, call HaggisGAC-literalUpdate (Procedure 9).
procedure analogue ShortGAC, straightforward. point interest
still check whether literal supported, even though added
litsLostExplicitSupport not. reason support found unrelated
call findNewSupport might also support literal. done,
Procedure 9 calls findNewSupport. new support found added,
prune literal longer supported.
21

fiNightingale, Gent, Jefferson, & Miguel

Require: Short Support sup
1: oldIndex supportNumLowIdx[numSupports]
2: (x 7 v) sup
3:
Remove sup supportListPerLit[x 7 v]
4:
supportListPerLit[x 7 v] = {}
5:
detachTrigger(x,v)
6:
(x 7 v) 6 zeroLits[x]
7:
Add (x 7 v) zeroLits[x]
8:
supportsPerVar[x] = numSupports
9:
Add (x 7 v) litsLostExplicitSupport
10:
sPV supportsPerVar[x]
11:
swap(x, varsBySupport[sPV])
12:
supportNumLowIdx[sPV] supportNumLowIdx[sPV]+1
13:
supportsPerVar[x] sPV1
14: numSupports-15: {supportNumLowIdx[numSupports] . . . oldIndex 1}
16:
Add varsBySupport[i] varsLostImplicitSupport

Procedure 7: HaggisGAC-DeleteSupport: (sup). One subtlety must add (x 7
v) zeroLits (line 7) even also add litsLostExplicitSupport (line 9).
case matters seek find new implicit support, i.e. containing
x 7 v, later lost. later point Procedure 10 requires x 7 v zeroLits
x 7 v might still explicit support.
Require: x 67 v (where v pruned domain x)
1: litsLostExplicitSupport {}
2: varsLostImplicitSupport {}
3: supportListPerLit[x 7 v] 6= {}
4:
sup first element supportListPerLit[x 7 v]
5:
deleteSupport(sup)
6: (y 7 b) litsLostExplicitSupport
7:
HaggisGAC-literalUpdate(y 7 b)
8: z varsLostImplicitSupport
9:
HaggisGAC-variableUpdate(z)

Procedure 8: HaggisGAC-Propagate: propagate(x 67 v)
variables lost implicit supports, call HaggisGAC-variableUpdate (Procedure 10), similar Procedure 4. differences return statements
Procedure 4 omitted; check every iteration whether new implicit support
found x exit loop; remove x 7 v zeroLits
new explicit support found, allowing done lazily later call Line 5.
gain efficiency ShortGAC two reasons. First, variableUpdate
called variables lost implicit support. Second, outer loop
HaggisGAC-Propagate must restarted new support found,
Procedure 3. write number variables lost last
implicit support, reduced worst case number calls variableUpdate
HaggisGAC-Propagate O(n2 d) n arity constraint m. Since
n often much smaller n even zero, significant gain.
22

fiShort Long Supports Constraint Propagation

Require: x 7 v, last explicit support x 7 v deleted
1: v D(x) supportsPerVar[x] = numSupports
supportListPerLit[x 7 v] = {}
2:
sup findNewSupport(x, v)
3:
sup = Null
4:
prune(x 7 v)
5:
else
6:
addSupport(sup)

Procedure 9: HaggisGAC-literalUpdate(x 7 v)

Require: variable x
1: (x 7 v) zeroLits[x]
2:
supportsPerVar[x] < numSupports
3:
return
4:
supportListPerLit[x 7 v] 6= {}
5:
Remove (x 7 v) zeroLits[x]
6:
else
7:
v D(x)
8:
sup findNewSupport(x 7 v)
9:
sup = Null
10:
prune(x 7 v)
11:
else
12:
addSupport(sup)

Procedure 10: HaggisGAC-variableUpdate(x)

6.4 Dealing Efficiently Full-length Supports
full-length support added, ShortGAC increments numSupports supportsPerVar every variable. Since interested condition numSupports =
supportsPerVar[x], full-length support cannot change status variable. Therefore save overheads case add full-length support. achieved
case split HaggisGACs versions addSupport deleteSupport:
support full length update numSupports, supportsPerVar, related data
structures. Note test apply final support arity n,
initial one omission assigned literals optimisation correct
even assigned literals omitted. omit pseudocode optimisation,
changes straightforward. optimisation often improves performance instances
full-length supports 20%, important effect instances
runtimes within 2.5% without it. optimisation also applicable
ShortGAC, implement case address key
inefficiency algorithm has, i.e. repeated checking variables cannot
lost last implicit support. affect experimental results dramatically:
cases found improved performance HaggisGAC larger
optimisation provides.
23

fiNightingale, Gent, Jefferson, & Miguel

7. Experimental Evaluation ShortGAC HaggisGAC
Minion solver 0.12 (Gent, Jefferson, & Miguel, 2006a) used experiments,
changes additional propagators. experiments, compared
methods maintain GAC. Therefore, solver explores search space case.
Since number nodes searched invariant, compare rate search exploration,
measured search nodes per second.6
used 8-core machine 2.27GHz Intel Xeon E5520 CPUs 12GB memory,
running Ubuntu Linux. possible ran 12 processes parallel. combination problem instance propagator, report median 11 runs.7 cases
possible run 12 processes parallel exceed 1GB memory. these,
ran one process time, report median 5 runs. instances
marked tables results. one method exceeded 1GB, sometimes
ran comparable methods series well. allows consistent comparison
List NDList, different propagation algorithms. also means tables
necessarily indicate method uses 1GB memory. find
median robust measure performance, reasons described Appendix B.
cases, imposed time limit one hour, limit 1,000,000 search nodes
(whichever first). avoid short runs solver find solution easily,
searched solutions. report complete cpu times, i.e. attempted
measure time attributable given propagator include initialisation.
advantage automatically take account factors affecting runtime,
including aspects (e.g. cache usage) may realise affect runtime. however
mean results tend understate difference methods studied.
case study, implemented findNewSupport method ShortGAC
HaggisGAC specific constraint. also used generic list instantiation (Section 5.5.1) Next-Difference List instantiation (Section 5.5.2) comparison
possible. compare ShortGAC HaggisGAC special-purpose propagator
(when available).
also compare ShortGAC-Long (ShortGAC full-length supports),
HaggisGAC-Long, GAC-Schema (Bessiere & Regin, 1997) closest equivalent algorithm without strict short supports. discuss GAC-Schema Section 7.4.
GAC-Schema, ShortGAC-Long HaggisGAC-Long use (constraint-specific)
findNewSupport ShortGAC, subsequently extend short support full length
using minimum value extra variable.
case, constraint compactly represented disjunction. Therefore
compare ShortGAC HaggisGAC Constructive Or. algorithm used
based Lagerkvist Schultes (2009), without rule entailment detection.

6. Source code solver three algorithms available http://www.cs.st-andrews.ac.
uk/~pn/haggisgac-source.tgz problem instances experimental results http://www.cs.
st-andrews.ac.uk/~pn/haggisgac-data-instances.tgz.
7. preliminary investigations, found running 12 processes parallel gives consistent cpu time
results, consistency improved taking median.

24

fiShort Long Supports Constraint Propagation

3

element
element long
element list
element ndlist
lex
lex long
squarepack
squarepack long
squarepack list
squarepack ndlist

2.5

2

1.5

1.25
1.1
1
0.9
1

10

100

1000

10000

100000

Figure 2: Summary comparison ShortGAC HaggisGAC. x-axis median
nodes per second ShortGAC. y-axis speedup (or slowdown) HaggisGAC, i.e. ratio ShortGAC nodes per second HaggisGAC.
Hence 1 represents equal behaviour, 1 means HaggisGAC
faster.

implementation Minion fully incremental: disjunct propagated incrementally
branch search backtracked search backtracks.8
compare table constraints, described (for example) Gent et al.
(2007), constraints large. example, smallest element constraints
reported 638 allowed tuples, making impossible even generate store
list allowed tuples.
aid comparison HaggisGAC ShortGAC, addition tables
compare graphically Figure 2. figure shows relative speedup (or
cases slowdown) using HaggisGAC compared ShortGAC.
7.1 Case Study 1: Element
use quasigroup existence problem QG3 (Colton & Miguel, 2001) evaluate ShortGAC HaggisGAC element constraint. problem class one parameter
n, specifying size n n table (qg) variables domains {0 . . . n 1}. Rows,
columns one diagonal GAC allDifferent constraints, following Colton Miguels
model. element constraints represent QG3 property (i j) (j i) = (where
j members quasigroup quasigroup operator). translates
i, j : element(qg, aux[i, j], i), aux[i, j]= n qg[i, j] + qg[j, i], aux[i, j]
domain {0 . . . n n 1}.
8. Personal communication Pascal Van Hentenryck indicated unpublished optimisation
Constructive whereby disjuncts need propagated cases. implement
optimisation.

25

fiNightingale, Gent, Jefferson, & Miguel

n
6
7
8
9
10

Watch
Elt.
27,825
22,259
15,635
15,898
15,088

Specific
6,956
4,866
2,773
2,374

1,594

ShortGAC
List NDL
4,122 2,182
3,226 1,233
1,609
545
1,377
398


1,060
280

Long
25.9

8.5

3.6

2.2

1.6

Specific
11,131
9,035
5,652
5,419

4,227

HaggisGAC
List NDL
5,300 2,473
4,833 1,415
2,367
622
2,116
451


1,911
317

Long
36.5

15.2

6.2

3.7

2.6

GAC
Sch.
22.5
7.1

3.0

3.0
mem

Con

53.5
24.2
9.1

6.2

4.2

Table 1: Nodes searched per second quasigroup existence problems. mem indicates
running memory (>12 GB). Columns correspond propagation algorithms.
Watch Elt special-purpose propagator. ShortGAC HaggisGAC
four instantiations: Specific (special-purpose findNewSupport function
constraint), List, NDL (Next-Difference List), Long (as described text).
GAC-Sch GAC-Schema, Con Constructive Or.

constraint element(X, y, z), findNewSupport method ShortGAC returns
tuples form hxi 7 j, 7 i, z 7 ji, index vector X j
common value z xi . ShortGAC-list supports form. Constructive
Or, used (x0 = z = 0) (x1 = z = 1) .
compare ShortGAC HaggisGAC special-purpose Watched Element
propagator (Gent et al., 2006b), GAC-Schema Constructive Or. Table 1 presents
results QG3. general purpose methods, using short supports (with Specific, List
NDList instantiations) dramatically better alternative. example n = 10,
even HaggisGAC-List method (which slower HaggisGAC-Specific)
450 times faster Constructive Or, best methods.
ShortGAC-Long runs 1020% faster GAC-Schema n = 6 8, slower
n = 9 better n = 10 GAC-Schema uses memory. Recall
use findNewSupport method, fair comparison efficiently
exploit supports. contrast results reported previously (Nightingale
et al., 2011), ShortGAC half speed GAC-Schema. Two substantial differences account improvement: improved data structures described
Section 5; remove assigned literals full-length supports described
Section 5.6. HaggisGAC-Long consistently faster ShortGAC-Long
GAC-Schema.
much faster methods using full-length supports, list variants HaggisGACList HaggisGAC-NDList slower HaggisGAC-Element (and
true ShortGAC). expected neither specialised Element
constraint, deal data structures containing lists tuples.
two list variants, NDList variant runs much slowly. However, memory usage is,
expected, much less HaggisGAC-List. used less half much memory
n = 6, improving almost 10 times less memory n = 10.
HaggisGAC-Element approximately twice fast ShortGAC-Element
instances. believe two variables short supports index
result variables meaning always supported explicitly. seen
26

fiShort Long Supports Constraint Propagation

n

GACLex

3
4
5
6
7
8
9
10
12
14
16
18
20
22
24

104,955
103,950
95,420
80,841
72,307
66,445
64,267
57,208
48,146
36,751
30,057
22,432
16,625
12,450
9,526

ShortGAC
Specific Long
87,463 7,020
99,602 6,481
89,127 6,358
73,260 3,456
65,062 2,424
51,335 1,290
47,059
786
38,344
557
31,626
293
22,712
139
17,813
85.9
13,843
52.4
10,734
35.9
7,976
24.9
6,255
14.3

HaggisGAC
Specific Long
91,265 9,288
100,100 8,628
90,009 8,503
74,184 4,666
65,359 3,271
52,659 1,609
47,847
914
39,683
634
32,425
311
23,063
142
18,420
90.9
13,845
53.8
10,711
38.9
8,141
26.0
6,268
18.9

GACSchema
3,622
3,030
2,734
1,638
1,190
670
451
318
170
82.3
51.5
33.3
21.0
12.5

7.3

Con

5,735
4,997
4,104
2,109
1,188
456
263
184
105

99.1

62.6

48.3

36.7

27.0

21.8

Table 2: Nodes searched per second BIBDs. GACLex special-purpose propagator,
columns named Table 1.

Figure 2, List, NDList Long instantiations HaggisGAC also faster
instantiations ShortGAC smaller margin. special purpose
Watched Element propagator fastest method, 3.6 times faster n = 10.
Watched Element also appears scaling better n increases. Constructive
much slower methods exploit strict short supports, however faster
HaggisGAC-Long. Overall clear exploiting strict short supports
beneficial compared general purpose methods.
7.2 Case Study 2: Lex-ordering
use BIBD problem evaluate ShortGAC HaggisGAC lexicographic
ordering constraint. lex constraint placed rows columns, perform
Double Lex symmetry breaking method (Flener et al., 2002). use BIBD model
given Frisch, Hnich, Kiziltan, Miguel, Walsh (2002), GACLex propagator given Frisch, Hnich, Kiziltan, Miguel, Walsh (2006). use BIBDs
parameter values (4n + 3, 4n + 3, 2n + 1, 2n + 1, n).
constraint lexleq(X, ) arrays X , define mxi = min(Dom(xi ))
myi = max(Dom(yi )). findNewSupport method ShortGAC finds lowest
index {0 . . . n} mxi < myi , = n. case = n arises X cannot
lexicographically less , support sought X = . < n, support
contains xi 7 mxi , yi 7 myi . index j < i, mxj = myj , short support
contains xj 7 mxj , yj 7 myj otherwise valid support Null returned.
lex constraint two arrays length n domain size dn short
supports short support set, assignments two arrays equal
satisfy constraint cannot reduced. ShortGAC-List ShortGAC-NDList
27

fiNightingale, Gent, Jefferson, & Miguel

practical substantial constraint omit comparison.
Constructive use following representation n + 1 disjuncts: (x0 < y0 ) (x0 =
y0 x1 < y1 ) , including final case pairs equal.
Table 2 presents results experiments non-list based methods values
n 3 24. clear best method special-purpose GACLex propagator,
HaggisGAC coming second. problem, HaggisGAC ShortGAC perform similarly. HaggisGAC ShortGAC far best general purpose methods.
largest instances run 1.5 times slower special purpose method,
outperforming next best method almost 300 times. Again, HaggisGAC-Long
ShortGAC-Long outperform GAC-Schema, instances difference
even marked.
HaggisGAC-Long substantially faster ShortGAC-Long, seen
Figure 2: largely explained optimisation Section 6.4.
summarise, experiments Lex constraint clearly show benefit
HaggisGAC ShortGAC compared general-purpose propagation methods.
speed even approaches special purpose GACLex propagator.
7.3 Case Study 3: Rectangle Packing
rectangle packing problem (Simonis & OSullivan, 2008) (with parameters n, width
height) consists packing squares size 1 1 n n rectangle size
width height. modelled follows: variables x1 . . . xn y1 . . . yn ,
(xi , yi ) represents Cartesian coordinates lower-left corner square.
Domains xi variables {0 . . . width i}, yi variables {0 . . . height i}.
Variables branched decreasing order (to place largest square first),
xi yi , smallest value first. type constraint non-overlap squares
j: (xi + xj ) (xj + j xi ) (yi + yj ) (yj + j yi ). Minion
special-purpose non-overlap constraint (Simonis & OSullivan, 2008),
report comparison general-purpose methods. experiment used optimum
rectangle sizes reported Simonis OSullivan.
domains xn yn reduced break flip symmetries described Simonis
OSullivan (2008). focus performance non-overlap constraint,
implement commonly-used implied constraints.
findNewSupport function ShortGAC follows. four disjuncts
entailed given current domains, return empty support (indicating entailment). Otherwise, return support two literals satisfy one four disjuncts.
list used ShortGAC-List ShortGAC-NDList supports size 2.
Table 3, compare HaggisGAC ShortGAC general purpose
methods. see HaggisGAC fastest method, ShortGAC second.
HaggisGAC-List HaggisGAC-NDList (as well ShortGAC-List ShortGACNDList) performed well compared GAC-Schema Constructive Or. However
n = 20, HaggisGAC-List consumes 971MB memory HaggisGAC-NDList 496MB,
n > 20 possible run methods 12 processes parallel.
Interestingly, performance two List variants HaggisGAC reversed
Case Study 1: here, NDList significantly faster List cases. expected,
28

fiShort Long Supports Constraint Propagation

n-w-h
18-31-69
19-47-53
20-34-85
21-38-88
22-39-98
23-64-68
24-56-88
25-43-129
26-70-89
27-47-148

Specific
14,923
38,329
13,949
8,568
8,059
31,486
12,317
5,310
25,860
2,943

ShortGAC
List
NDL
6,339
6,919
4,446
8,460
3,181
3,911

2,668 2,781

1,865 1,889

1,226 2,805

1,717 2,238


1,007
986


909
1,977


1,034
786

Long
1,093
1,282
914
641
599
718
492
377
455
252

Specific
19,524
39,185
21,000
12,262
11,966
30,628
16,075
10,228
23,132
4,677

HaggisGAC
List
NDL
7,999
7,988
5,295
9,330
4,261
4,734

3,955 3,981

3,013 2,896

1,663 3,863

2,441 3,152

1,634 1,506

1,219 2,577

1,265 1,187

Long
1,471
1,684
1,296
886
858
971
702
583
584
400

GAC
Sch.
1,033
1,181
775
592
518
590
474
348
376
272

Con

441
478
276
245
185
349
167
96
245
74

Table 3: Nodes searched per second Rectangle Packing instances. columns named
Table 1.

NDList used less memory, though less dramatically before. used 30%
50% memory HaggisGAC-List.
methods, always least 10 times slower HaggisGAC.
HaggisGAC-Long faster GAC-Schema cases. Also ShortGAC-Long
faster GAC-Schema instances except 27-47-148 (this contradicts result
previously reported (Nightingale et al., 2011), explanation given
first case study).
Table 3 shows HaggisGAC (with SquarePack instantiation) substantially
faster ShortGAC instances, exception n = 23 n = 26
ShortGAC slightly faster. compared ShortGAC List, NDList,
Long instantiations Figure 2, see HaggisGAC mostly 10
50% faster. summary, results clearly show benefits using strict short
supports.
7.4 Comparing HaggisGAC GAC-Schema
Across experiments, HaggisGAC-Long runs significantly faster GACSchema minimum 20% faster three times faster even
though code contains overhead dealing strict short supports. compared
memory usage across experiments, found similar performance across instances. found HaggisGAC-Long uses less 5% memory except
BIBD instances, BIBD uses less 17% memory GAC-Schema.
However, comparison functional instantiations full-length supports, constraints admit strict short supports. section, broaden
comparison using list instantiations rather functional ones, using problem
instances used previously comparing table constraints.
compared GAC-Schema similar HaggisGAC
ShortGAC conceptually. three algorithms maintain list supports literal,
updated backtracked search. GAC-Schema carefully implemented
29

fiNightingale, Gent, Jefferson, & Miguel

Sports
CarSeq
Graceful
PQueens
BIBD

20

10

5

2

1

0.5
100

1000

10000

100000

Figure 3: Comparison GAC-Schema HaggisGAC-List full-length table constraints. x-axis nodes per second GAC-Schema, y-axis speedup
HaggisGAC-List.

following pseudocode original paper (Bessiere & Regin, 1997). code
shared among three algorithms, optimised independently. example,
GAC-Schema different implementation supportListPerLit, named SC (Bessiere &
Regin, 1997), specialised full-length supports.
contrast GAC-Schema, table constraint propagators STR2 (Lecoutre,
2011) MDDC (Cheng & Yap, 2010) entirely different HaggisGAC, would
difficult create truly comparable implementations them.
report use HaggisGAC-List only, searches supports
way GAC-Schema (with one difference discuss below.) used structured
instances Gent et al. (2007), except Semigroup class. addition, used Car
Sequencing instances Nightingale (2011), specifically model B instances numbered
60-79. instances contain large number ternary table constraints.
Figure 3 shows HaggisGAC-List almost always faster GAC-Schema
problems. BIBDs clear algorithm better. HaggisGAC
always least marginally faster Sports Scheduling, Prime Queens Graceful
Graphs instances, cases range 10-20% faster. HaggisGAC substantially
faster Car Sequencing. seek new supports, HaggisGAC calls Procedure 5,
finds new support stores index listPos. HaggisGAC backtrack
listPos described Section 5.5.1. GAC-Schema similar, backtrack listPos,
ensures optimality branch search iterating listPos
end list (Bessiere & Regin, 1997). Profiling shows GAC-Schema hindered
backtracking listPos (by block-copying memory) Car Sequencing,
large number table constraints (2000 instance 60) large domains (some size
30

fiShort Long Supports Constraint Propagation

1000). Alternative memory management techniques might speed GAC-Schema,
claim HaggisGAC fundamentally 10 times faster GAC-Schema.
7.5 Results Summary
summarise three case studies, HaggisGAC indeed outperform ShortGAC
many instances, sometimes two times commonly 25%.
ShortGAC rarely faster, one instance much 10%. Overall,
experiments, HaggisGAC clearly better algorithm ShortGAC. Furthermore,
HaggisGAC ShortGAC perform well compared Constructive GACSchema, result validates idea strict short supports.
Finally, shown experimentally HaggisGAC outperform GAC-Schema
problems containing full-length supports. discuss Appendix C major focus paper.

8. Backtrack Stability Short Supports
Within search tree, HaggisGAC often spends significant time backtracking data structures. Reducing eliminating backtracking improve efficiency. example MAC-6
MAC-7 much efficient (in space time) backtracking avoided
(Regin, 2005). section present new algorithm saves time deleting
short supports backtrack, saves memory bounding total number stored
short supports (including backtrack stack).
new algorithm requires short supports backtrack stability property.
short support backtrack stable iff remains short support backtracking (Section 3.2).
three case studies, find short supports construct element
lex constraints backtrack stable, rectangle packing not. rectangle
packing, generate empty support constraint entailed. empty support
backtrack stable unless constraint entailed root node search.
introduce algorithm HaggisGAC-Stable know short supports
backtrack stable. key change delete supports backtrack
past point introduction. stable, still correct ancestors
node introduced at. save time previous algorithms, since
sometimes need work backtracking. Also, show below, obtain
tight limits space usage stored supports.
present HaggisGAC-Stable, introduce notion prime support
deleted literal. prime support deleted literal support (either explicit implicit)
valid support literal literal restored backtracking.
invariant maintain deleting literal either labelled deleted
support backtrack stack prime support literals variable currently
implicitly supported. invariant, guarantee backtrack
point literal restored, must supported again: either prime support
restore, known implicit support.
task finding prime support literal naturally splits three cases.
simplest case HaggisGAC-Stable deletes literals able find
31

fiNightingale, Gent, Jefferson, & Miguel

necessary new support. prime support implicit explicit support
whose deletion caused fruitless search new support.
second case literal pruned constraint search
procedure, pruned literal explicit support constraint. explicit
supports must deleted longer valid, label arbitrary one
literals prime support: simply choose last one deleted.
third case unfortunately complicated. literal pruned outside
current constraint, literal implicit support explicit support.
difficult precisely pruned literal link implicit support.
Providing maintaining link throughout search would negate efficiencies
gained. solution problem lazy. variable pruned
literal implicitly supported. implicit support variable,
maintaining invariant described above. literal pruned need nothing
case. need work variable loses last implicit support,
ever does. happens, invalid literal explicit support must
definition relevant zeroLits list. Whereas previously ignored invalid literals
iterating zeroLits, label deleted implicit support prime
support invalid literal.
show Lemma 8.1 HaggisGAC-Stable stores time
O(z) supports, z total number literals. save lot memory
HaggisGAC ShortGAC may store O(z 2 ) supports,
O(z) deletions literals branch, deletion new set O(z) supports
may stored. experiments later show difference memory usage
significant practice. effective, memory usage reduced 20 times.
8.1 Details HaggisGAC-Stable
HaggisGAC-Stable, control great care deletion restoration
supports, instead (as rest paper) simply reversing addition deletion
support node respectively deleting adding back backtrack past
node. short never delete active support backtracking, add back
deleted support prime support literal current active support.
deleting support, setup counter numPrimeSupported. initially 0,
incremented time find support prime support. propagation
algorithm finishes, support numPrimeSupported = 0, support destroyed space reclaimed. Otherwise, place numPrimeSupported new pairs
backtrack stack. pair consists deleted support literal prime
support for. backtracking, pop pair, first check current support
already supports literal. so, simply decrement numPrimeSupported,
reduces 0, reclaim supports space. literal supported,
restore support via call addSupport. way literals support prime
guaranteed supported.
relatively minor difference iterate zeroLits delete invalid literals zeroLits. backtracking restore
32

fiShort Long Supports Constraint Propagation

Require: x 7 v, last explicit support x 7 v deleted
1: v D(x)
2:
supportsPerVar[x] = numSupports supportListPerLit[x 7 v] = {}
3:
sup findNewSupport(x, v)
4:
sup = Null
5:
prune(x 7 v)
6:
increment lastSupportPerLit[x 7 v].numPrimeSupported
7:
push hx 7 v, lastSupportPerLit[x 7 v]i onto BacktrackStack
8:
else
9:
addSupport(sup)
10: else
11:
increment lastSupportPerLit[x 7 v].numPrimeSupported
12:
push hx 7 v, lastSupportPerLit[x 7 v]i onto BacktrackStack

Procedure 11: HaggisGAC-Stable-literalUpdate: (x 7 v). comparison Procedure 9, update numPrimeSupported BacktrackStack.

zeroLits backtrack stack, enables space complexity
result Lemma 8.1.
HaggisGAC-Stable similar HaggisGAC. appropriate simply describe
differences save space. Procedure HaggisGAC-Stable-Propagate almost
Procedure 8, calling backtrack stable variants deleteSupport, literalUpdate
(Procedure 11) variableUpdate (Procedure 12). addition, end algorithm
destroy reclaim space deleted support numPrimeSupported = 0.
Procedure HaggisGAC-Stable-DeleteSupport (called support S) also
similar predecessor, Procedure 7, additions. First, initialises numPrimeSupported 0. Second, new data structures lastSupportPerLit deleted
literal x 7 lastSupportPerVar variable x. terms Procedure 7,
assigned Line 9 Line 16 (respectively). Note assignments
make prime support: checked later.
Procedure 11 analogous Procedure 9 enough differences show
detail here. identifies prime supports, necessary increments numPrimeSupported pushes invalid literal/support pairs onto backtrack stack. also present
Procedure 12 detail, analogue Procedure 10. identifies prime supports,
increments counter adds pairs BacktrackStack. One difficult case arises,
Line 17. Here, x 7 pruned, externally constraint.
pruned Procedure 11, would zeroLits. x 7 restored backtracking still need make sure support. Since explicit support (it
zeroLits), last support must implicit support deleting. Therefore store
support BacktrackStack. minor change note remove literals
zeroLits, Lines 13 19.
Whenever new search node (including root) entered, Null pushed onto
BacktrackStack. used marker procedure HaggisGAC-StableBacktrack (Procedure 13), processes literal/support pairs reaches Null.
restores prime supports literals put back domain backtracking,
support currently known. numPrimeSupported counter
33

fiNightingale, Gent, Jefferson, & Miguel

Require: variable x
1: (x 7 v) zeroLits[x]
2:
supportsPerVar[x] < numSupports
3:
return
4:
supportListPerLit[x 7 v] 6= {}
5:
Remove (x 7 v) zeroLits[x]
6:
else
7:
v D(x)
8:
sup findNewSupport(x, v)
9:
sup = Null
10:
prune(x 7 v)
11:
increment lastSupportPerVar[x].numPrimeSupported
12:
push hx 7 v, lastSupportPerVar[x]i onto BacktrackStack
13:
Remove (x 7 v) zeroLits[x]
14:
else
15:
addSupport(sup)
16:
else
17:
increment lastSupportPerVar[x].numPrimeSupported
18:
push hx 7 v, lastSupportPerVar[x]i onto BacktrackStack
19:
Remove (x 7 v) zeroLits[x]
Procedure 12: HaggisGAC-Stable-variableUpdate: (x). similar Procedure 10
addition maintenance numPrimeSupported BacktrackStack.
1: top element BacktrackStack Null
2:
pop hx 7 v, supi BacktrackStack
3:
sup yet restored
4:
supportsPerVar(x) = numSupports supportListPerLit[x 7 v] = {}
5:
HaggisGAC-Stable-AddSupport(sup)
6:
else
7:
{Another support exists x 7 v}
8:
decrement sup.numPrimeSupported
9:
sup.numPrimeSupported = 0
10:
destroy sup reclaim space
11:
supportListPerLit[x 7 v] = {}
12:
Add (x 7 v) zeroLits[x]
13: pop Null BacktrackStack

Procedure 13: HaggisGAC-Stable-Backtrack. Performs backtracking using BacktrackStack.

support becomes zero, support destroyed longer necessary. Note
literals put back zeroLits necessary Line 12, reversing deletion
Procedure 12.
cannot use optimisation described Section 5.6, deleting literals supports
variables assigned, may break backtrack stability property.
34

fiShort Long Supports Constraint Propagation

However, retain optimisation Section 6.4 full-length supports, omit
pseudocode showing interest focusing essential aspects algorithms.
8.2 Improved Space Complexity HaggisGAC-Stable
approach improves space complexity HaggisGAC-Stable compared HaggisGAC, following lemma shows.
Lemma 8.1. constraint involving z literals, 2z supports stored, either
active deleted supports backtrack stack.
Proof. define function supports literals. support still active,
found call findNewSupport specific literal, map support
literal. Similarly, support backtrack stack, pair least
one literal prime support for. Map support one literals. Every
stored support falls one two categories, support deleted
put onto backtrack stack, space reclaimed. three supports mapped
literal because:
valid literals, findNewSupport called existing active support
exists literal.
invalid literals, literal appears pair backtrack stack
twice. case literal appears often twice literal
prime support already stack processed variable loses last implicit
support. case, literal must zeroLits, newly deleted implicit
support added backtrack stack literal. happen
delete literal zeroLits first time happens.
Thus number supports bounded 2z.
bound 2z Lemma 8.1 would improve z maintained zeroLits eagerly
instead lazily, expense higher overheads elsewhere.

9. Experimental Evaluation HaggisGAC-Stable
compare HaggisGAC-Stable HaggisGAC using experimental setup
Section 7. well tables results, provide graphical comparison runtimes
HaggisGAC-Stable HaggisGAC Figure 4, memory usage Figure 5.
Table 4 Figure 4 shows results instances Section 7.1. present
four instantiations HaggisGAC-Stable, along fastest instantiation HaggisGAC, Watched Element special-purpose propagator, Constructive (which
faster GAC-Schema Table 1). element, observe 10% slowdown,
slight slowdown List variants. full-length supports, see almost
identical performance.
Table 5 shows results instances Section 7.2. HaggisGAC-Stable-Lex performs slightly worse HaggisGAC-Lex, though fact never 10% worse
slightly faster largest instances. might supports found
35

fiNightingale, Gent, Jefferson, & Miguel

deep search likely contain literals supports found earlier, meaning
backtrack longer supports retained instead replaced earlier
efficient short supports. so, advantage disappears Long variants. Indeed, HaggisGAC-Stable-Long performs much better HaggisGAC-Long,
improvement increases n, 4.5 times n = 24.
Rectangle Packing instantiation ShortGAC described Section 7 generates
empty support constraint becomes entailed, causing variables implicitly
supported point on. empty support backtrack stable, cannot
used HaggisGAC-Stable. implemented new backtrack stable variant
findNewSupport, empty support returned, otherwise
before. List Long variants affected return
empty support case. Table 6, use instances Section 7.3. Results show
significant slowdowns using backtrack stability rectangle packing, 2 times
n = 24. probably inability return empty support.
hand, see speedups 50% list variants, cases factor
2 speedup full-length supports.
see Figure 5 memory usage goes greatly stability used
full-length supports, possibly contributing speedups cases. greatest
reductions case element, two cases 20 times less memory.
hand, significant reduction memory usage non-long variant.
also tested HaggisGAC-Stable GAC-Schema Section 7.4. gave
similar performance HaggisGAC therefore better GAC-Schema:
omit detailed results. significant memory advantage compared HaggisGAC, Stable variant saving less 25%. therefore seem gain
advantages saw earlier backtrack stability full-length supports.
conclude backtrack stability speed HaggisGAC significantly,
greatly reduce memory usage using full-length supports. However, care must
used, backtrack stability harmful insisting backtrack stability increases
size returned supports.

10. Related Work
use counters count supports inspired AC4 (Mohr & Henderson, 1986).
study compressing tuples constraint compact data structure
order make propagation efficient. example, Gent et al. (2007) used tries,
Cheng Yap (2010) applied MDDs. also extensive study searching
list tuples find first valid tuple. Approaches include binary search (Lecoutre &
Szymanek, 2006), trie search (Gent et al., 2007), approaches similar skip lists
NDLists (Gent et al., 2007) hologram-tuples (Lhomme, 2004; Lhomme & Regin, 2005).
techniques orthogonal main focus paper assist
finding supports, maintaining set active supports. adapted NDLists
contain short supports Section 5.5.2; may also interesting adapt
approaches.
STR2 maintains sparse set valid satisfying tuples constraint (Lecoutre,
2011). Updated variable domains computed set time algorithm
36

fiShort Long Supports Constraint Propagation

n

WatchElt

6
7
8
9
10

27,825
22,259
15,635
15,898
15,088

HaggisGAC
Specific
11,131
9,035
5,652
5,419

4,227

HaggisGAC-Stable
Specific
List NDList Long
10,305
4,881
2,358
30.3
8,302
4,225
1,349 15.1

4,986
1,950
550
7.0

4,579
1,711
388
4.4



4,008 2,409
309
2.5

Con

53.5
24.2
9.1

6.2

4.2

Table 4: Nodes searched per second quasigroup existence problems. columns
named Table 1.
n

GACLex

3
4
5
6
7
8
9
10
12
14
16
18
20
22
24

104,955
103,950
95,420
80,841
72,307
66,445
64,267
57,208
48,146
36,751
30,057
22,432
16,625
12,450
9,526

HaggisGAC
Specific Long
91,265 9,288
100,100 8,628
90,009 8,503
74,184 4,666
65,359 3,271
52,659 1,609
47,847
914
39,683
634
32,425
311
23,063
142
18,420
90.9
13,845
53.8
10,711
38.9
8,141
26.0
6,268
18.9

HaggisGAC-Stable
Specific
Long
90,473
12,008
103,470
9,056
93,382
7,248
76,777
3,844
67,273
2,615
52,113
1,591
47,881
1,114
39,176
806
32,310
533
23,709
345
18,556
248
14,504
177
10,438
135
8,159
106
6,165
85

GACSchema
3,622
3,030
2,734
1,638
1,190
670
451
318
170
82.3
51.5
33.3
21.0
12.5

7.3

Con

5,735
4,997
4,104
2,109
1,188
456
263
184
105

99.1

62.6

48.3

36.7

27.0

21.8

Table 5: Nodes searched per second BIBDs. GACLex special-purpose propagator
Lex, columns named Table 1.
n-w-h
18-31-69
19-47-53
20-34-85
21-38-88
22-39-98
23-64-68
24-56-88
25-43-129
26-70-89
27-47-148

HaggisGAC
Specific
19,524
39,185
21,000
12,262
11,966
30,628
16,075
10,228
23,132
4,677

HaggisGAC-Stable
Specific
List NDList
16,950
9,544
8,383
22,580
4,663
8,264
12,865
4,950
4,840

5,827
9,783 6,492

8,798 4,744
4,319


28,987
2,377
4,511

6,741 3,894
3,998

5,706 2,405
2,199


27,507
1,689
4,024

3,996 1,591
1,735

Long
1,686
1,621
2,607
957
921
1,095
1,149
1,265
890
344

GACSchema
1,033
1,181
775
592
518
590
474
348
376
272

Table 6: Nodes searched per second Rectangle Packing instances. columns named
Table 1.

37

fiNightingale, Gent, Jefferson, & Miguel

element
element long
element list
element ndlist
lex
lex long
squarepack
squarepack long
squarepack list
squarepack ndlist

6
5
4
3
2
1.5
1.1
1
0.9
0.7
0.5
0.4
0.3
1

10

100

1000

10000

100000

1e+06

Figure 4: Summary comparison HaggisGAC HaggisGAC-Stable. x-axis
median nodes per second HaggisGAC. y-axis speedup (or slowdown)
HaggisGAC-Stable.

2

1

0.5

0.2

0.1

0.05

0.02
1000

element
element long
element list
element ndlist
lex
lex long
squarepack
squarepack long
squarepack list
squarepack ndlist
10000

100000

1e+06

1e+07

1e+08

Figure 5: Summary comparison memory usage (KiB) HaggisGAC HaggisGACStable. x-axis median memory usage HaggisGAC. y-axis
reduction (or increase) usage HaggisGAC-Stable, i.e. ratio HaggisGAC memory usage HaggisGAC-Stable. Hence 1 represents
equal behaviour, 1 means HaggisGAC-Stable used less memory.

38

fiShort Long Supports Constraint Propagation

invoked. concept maintaining support, seeking new support literal.
would interesting investigate adapting STR2 handle short supports. would
result entirely different algorithm ones presented paper, possibly
complementary strengths.
MDD propagator MDDC (Cheng & Yap, 2010) maintains MDD incrementally
search. MDD compressed representation satisfying tuples
constraint. time complexity MDDC linear initial size MDD, therefore
degree compression vital efficiency algorithm. cases,
constraint amenable strict short supports, also compress well MDD
(given appropriate variable ordering). example, lex constraint compresses well
partly (given variable order x1 , y1 , x2 , y2 , . . .) constraint satisfied
assigning prefix variables. Lex amenable short supports reason.
However, constraints small set short supports cannot compressed
effectively MDD. Suppose disjunction equality constraints pair
n variables domain size d. n 1 variables, MDD must Cn1 states.
Another property MDD compression might indicate interesting direction future
work. Lex also compresses well MDD multiple assignments prefix
variables lead subsequent vertex (e.g. {x1 7 1, y1 7 1} {x1 7 2, y1 7 2}).
something short support algorithms currently able exploit.
Katsirelos Walsh (2007) proposed different generalisation support, named ctuples. c-tuple contains set values variable scope constraint.
valid tuple whose values drawn c-tuple (full-length) support. Katsirelos
Walsh give outline modified version GAC-Schema directly stores c-tuples.
also present experiments based different propagator, GAC3.1r, demonstrating
modest speed improvement c-tuples compared conventional full-length supports.
c-tuple contains values variable, nevertheless recorded (in SC )
support value individually (Katsirelos & Walsh, 2007). algorithm
concept implicit support.
context Constructive Or, Lhomme (2003) observed support one
disjunct support values variable contained A. concept similar
short support albeit less general, length supports fixed
length disjuncts. presented non-incremental Constructive algorithm two
disjuncts.
algorithms similar flavour GAC-Schema (Bessiere & Regin, 1997),
natural compare GAC-Schema. However GAC algorithms
GAC2001/3.1 (Bessiere et al., 2005) would interesting compare
algorithms.

11. Conclusions
introduced detailed three general purpose propagation algorithms short
supports. either given specialised function find new supports
constraint, used function accepts explicit list short supports.
strict short supports available, three algorithms perform well, provide much
39

fiNightingale, Gent, Jefferson, & Miguel

better performance general purpose methods GAC-Schema Constructive Or.
shows value using strict short supports.
first algorithm studied ShortGAC, described improvements
compared earlier report algorithm (Nightingale et al., 2011). identified significant inefficiency ShortGAC dealing explicit supports.
introduced new algorithm, HaggisGAC corrects flaw, better theoretical
complexities, performs much better ShortGAC experiments. three
case studies, HaggisGAC far faster general purpose methods. best case
even achieved speeds 90% special purpose propagator. Perhaps
remarkably, able deal strict short full-length supports, HaggisGAC outperformed ShortGAC strict short supports GAC-Schema full-length
supports, i.e. cases algorithms respectively specialised for.
third algorithm, HaggisGAC-Stable, retain supports backtracking.
less effective HaggisGAC invalidates use certain strict short supports,
also significantly faster problems full-length supports, reduce
memory usage greatly cases.
proposed algorithms excellent propagating disjunctions constraints.
experiments disjunctions found algorithms faster Constructive
GAC-Schema least order magnitude, three orders magnitude.
summarise, shown value explicit use strict short supports
general purpose propagation algorithms generalised arc consistency. strict short
supports available, exploiting yields orders magnitude improvements generic
propagation algorithms. cases, even found generic algorithm come
close performance specialised propagator. Previously, short supports
seem recognised important right. overall contribution
correct focus short supports first class objects.

Acknowledgments
would like thank anonymous reviewers Bilal Syed Hussain comments,
EPSRC funding work grants EP/H004092/1 EP/E030394/1.

Appendix A. Comparison ShortGAC ShortGAC-IJCAI
Section 4, noted optimised data structures algorithms ShortGAC, compared previous presentation (Nightingale et al., 2011). demonstrate
indeed improvements, compared two implementations ShortGAC
three case studies used paper. use name ShortGAC-IJCAI
previous version. quoting results previous work (Nightingale et al.,
2011), rerun experiments using environment described Section 7.
also updated codebase Minion 0.12 instead Minion 0.10 earlier paper.
algorithm instance, report nodes searched per second peak memory use.
Table 7 shows results instances Section 7.1. clear results
ShortGAC makes much better use memory also faster ShortGAC-IJCAI
40

fiShort Long Supports Constraint Propagation

0.16

low memory, Sections 7 9
high memory, Sections 7 9
List + NDList, Section 7.4
GAC-Schema + Constructive

0.14
0.12
0.1
0.08
0.06
0.04
0.02
0
0.1

1

10

100

1000

10000

100000

1e+06

Figure 6: Scatterplot median nodes per second (x-axis) median absolute
deviation divided median (y-axis). distinguish
main experiments Sections 7 9, cases medians 5
runs, list variants used table constraints Section 7.4, data
paper GAC-Schema Constructive Or.

instances. Table 8 shows results instances Section 7.2. Element,
ShortGAC makes better use memory faster ShortGAC-IJCAI, although
improvements great before. Table 9, use instances Section 7.3.
previous two case studies, ShortGAC consistently better speed
memory use. conclude algorithms data structures used paper
indeed superior used previously (Nightingale et al., 2011).

Appendix B. Median Absolute Deviation Experiments
experiments report median either 11 5 runs. assess robust
median measure looked, combination instances algorithm,
median absolute deviation (MAD), i.e. median absolute difference data
points median. Figure 6 shows MAD algorithm/instance combinations
fraction median case. shows 511 algorithm/instance combinations
tested (including combinations reported detail paper). nodes per
second, maximum MAD found always less 15% median, worst
case 14.5%. HaggisGAC-Long n = 9 Table 1. four
cases MAD 8% median. Figures memory usage even
consistent, two cases (at 6.3% 6.1%) showing MAD 5% median
others 2%. major conclusions draw regard 10% change
behaviour one method another significant, therefore say
median robust measure performance.
41

fiNightingale, Gent, Jefferson, & Miguel

n
6
7
8
9
10

ShortGAC
node rate
6,956
4,866
2,773
2,374

1,594

ShortGAC-IJCAI
node rate
4,839
3,273
1,673
1,511

1,294

ShortGAC
memory
5,684
6,624
8,996
12,560

17,048

ShortGAC-IJCAI
memory
27,880
72,916
188,812
461,648

991,768

Table 7: Nodes searched per second memory use (KiB) quasigroup existence problems. Comparison ShortGAC ShortGAC-IJCAI.

n
3
4
5
6
7
8
9
10
12
14
16
18
20
22
24

ShortGAC
node rate
87,463
99,602
89,127
73,260
65,062
51,335
47,059
38,344
31,626
22,712
17,813
13,843
10,734
7,976
6,255

ShortGAC-IJCAI
node rate
83,964
98,135
89,286
74,184
63,091
50,480
45,085
36,179
29,455
20,868
16,087
12,356
9,614
7,208
5,398

ShortGAC
memory
7,476
11,680
16,408
22,568
31,348
42,420
55,660
74,348
120,024
181,252
263,792
360,500
493,368
632,064
811,104

ShortGAC-IJCAI
memory
8,392
12,992
18,512
26,260
36,356
49,012
65,684
85,700
138,496
209,492
308,400
422,536
570,188
735,548
939,796

Table 8: Nodes searched per second memory use BIBD problems. Comparison
ShortGAC ShortGAC-IJCAI.

n-w-h
18-31-69
19-47-53
20-34-85
21-38-88
22-39-98
23-64-68
24-56-88
25-43-129
26-70-89
27-47-148

ShortGAC
node rate
14,923
38,329
13,949
8,568
8,059
31,486
12,317
5,310
25,860
2,943

ShortGAC-IJCAI
node rate
10,892
29,647
10,288
6,109
5,821
24,528
8,386
3,828
21,146
2,086

ShortGAC
memory
11,876
10,172
13,988
16,100
18,868
13,988
17,548
27,580
19,796
39,848

ShortGAC-IJCAI
memory
24,568
19,680
33,020
38,828
46,344
31,700
43,708
74,064
49,512
106,144

Table 9: Nodes searched per second memory use rectangle packing. Comparison
ShortGAC ShortGAC-IJCAI.

42

fiShort Long Supports Constraint Propagation

Appendix C. Comparison GAC-Schema HaggisGAC
showed Section 7.4 HaggisGAC outperforms GAC-Schema dealing
full-length supports. despite fact HaggisGAC small overheads
dealing strict short supports even none exist. discuss briefly
may so.
GAC-Schema concept current supports literal one current support,
one active supports contain literal. additional data
structure S( ). active support , S( ) list literals
current support. Hence invalidated, GAC-Schema finds new current support
literal S( ) (or deletes literal). HaggisGAC dispensed
entirely. sign literal needs new support lost current support,
support list (supportListPerLit) empty. small potential saving
maintaining S( ).
second, possibly important, difference GAC-Schema eager
HaggisGAC. literal x 7 v loses current support, GAC-Schema check
active supports containing x 7 v valid, O(n) operation one.
invalid, GAC-Schema calls findNewSupport. returns Null x 7 v
deleted. HaggisGAC none this, avoiding completely cost checking validity. safe every support invalid, literal deletion support
cause call deleteSupport last result empty list, causing call
findNewSupport. approaches correct, GAC-Schemas wasteful
performs unnecessary validity checks. However, one cannot guarantee time saving, GAC-Schema perform deletions sooner, possibly affecting way propagator
interacts propagators.

References
Bessiere, C., Hebrard, E., Hnich, B., & Walsh, T. (2007). complexity reasoning
global constraints. Constraints, 12 (2), 239259.
Bessiere, C., & Regin, J.-C. (1997). Arc consistency general constraint networks: Preliminary results. Proceedings IJCAI 1997, pp. 398404.
Bessiere, C., Regin, J.-C., Yap, R. H. C., & Zhang, Y. (2005). optimal coarse-grained
arc consistency algorithm. Artificial Intelligence, 165 (2), 165185.
Cheng, K. C. K., & Yap, R. H. C. (2010). MDD-based generalized arc consistency
algorithm positive negative table constraints global constraints.
Constraints, 15 (2), 265304.
Colton, S., & Miguel, I. (2001). Constraint generation via automated theory formation.
Proceedings CP 2001, pp. 575579.
Flener, P., Frisch, A. M., Hnich, B., Kiziltan, Z., Miguel, I., Pearson, J., & Walsh, T. (2002).
Breaking row column symmetries matrix models. Proceedings CP 2002, pp.
462476.
Frisch, A. M., Hnich, B., Kiziltan, Z., Miguel, I., & Walsh, T. (2002). Global constraints
lexicographic orderings. Proceedings CP 2002, pp. 93108.
43

fiNightingale, Gent, Jefferson, & Miguel

Frisch, A. M., Hnich, B., Kiziltan, Z., Miguel, I., & Walsh, T. (2006). Propagation algorithms
lexicographic ordering constraints. Artificial Intelligence, 170 (10), 803834.
Gent, I. P. (2012). optimality result maintaining list pointers backtracking
search. Tech. rep. CIRCA preprint 2012/1, University St Andrews.
Gent, I. P., Jefferson, C., & Miguel, I. (2006a). Minion: fast scalable constraint solver.
Proceedings ECAI 2006, pp. 98102.
Gent, I. P., Jefferson, C., & Miguel, I. (2006b). Watched literals constraint propagation
Minion. Proceedings CP 2006, pp. 182197.
Gent, I. P., Jefferson, C., Miguel, I., & Nightingale, P. (2007). Data structures generalised
arc consistency extensional constraints. Proceedings AAAI 2007, pp. 191197.
Gent, I. P., Petrie, K., & Puget, J.-F. (2006). Handbook Constraint Programming (Foundations Artificial Intelligence), chap. Symmetry Constraint Programming, pp.
329376. Elsevier Science Inc., New York, NY, USA.
Jefferson, C., Moore, N. C. A., Nightingale, P., & Petrie, K. E. (2010). Implementing logical
connectives constraint programming. Artificial Intelligence, 174 (16-17), 14071429.
Katsirelos, G., & Walsh, T. (2007). compression algorithm large arity extensional
constraints. Proceedings CP 2007, pp. 379393.
King, A., Cromarty, L., Paterson, C., & Boyd, J. (2007). Applications ultrasonography
reproductive management dux magnus gentis venteris saginati. Veterinary
record, 160 (3), 94.
Lagerkvist, M. Z., & Schulte, C. (2009). Propagator groups. Proceedings CP 2009, pp.
524538.
Lecoutre, C. (2011). STR2: optimized simple tabular reduction table constraints. Constraints, 16 (4), 341371.
Lecoutre, C., & Szymanek, R. (2006). Generalized arc consistency positive table constraints. Proceedings CP 2006, pp. 284298.
Lhomme, O., & Regin, J.-C. (2005). fast arc consistency algorithm n-ary constraints.
Proceedings AAAI 2005, pp. 405410.
Lhomme, O. (2003). efficient filtering algorithm disjunction constraints.
Proceedings CP 2003, pp. 904908.
Lhomme, O. (2004). Arc-consistency filtering algorithms logical combinations constraints. Integration AI Techniques Constraint Programming
Combinatorial Optimization Problems (CP-AI-OR04), pp. 209224.
Mackworth, A. K. (1977). reading sketch maps. Reddy, R. (Ed.), IJCAI, pp. 598606.
William Kaufmann.
Mears, C. D. (2009). Automatic Symmetry Detection Dynamic Symmetry Breaking
Constraint Programming. Ph.D. thesis, Clayton School Information Technology,
Monash University.
Mohr, R., & Henderson, T. C. (1986). Arc path consistency revisited. Artificial Intelligence, 28 (2), 225233.
44

fiShort Long Supports Constraint Propagation

Nightingale, P. (2011). extended global cardinality constraint: empirical survey.
Artificial Intelligence, 175 (2), 586614.
Nightingale, P., Gent, I. P., Jefferson, C., & Miguel, I. (2011). Exploiting short supports
generalised arc consistency arbitrary constraints. Proceedings IJCAI 2011,
pp. 623628.
Puget, J.-F. (2005). Automatic detection variable value symmetries. Proceedings
CP 2005, pp. 475489.
Regin, J.-C. (1996). Generalized arc consistency global cardinality constraint. Proceedings AAAI 1996, pp. 209215.
Regin, J.-C. (2005). Maintaining arc consistency algorithms search without
additional space cost. Proceedings CP 2005, pp. 520533.
Rossi, F., van Beek, P., & Walsh, T. (Eds.). (2006). Handbook Constraint Programming.
Elsevier.
Schulte, C., & Tack, G. (2010). Implementing efficient propagation control. Proceedings
TRICS: Techniques Implementing Constraint programming Systems, conference
workshop CP 2010, St Andrews, UK.
Simonis, H., & OSullivan, B. (2008). Search strategies rectangle packing. Proceedings
CP 2008, pp. 5266.
Wurtz, J., & Muller, T. (1996). Constructive disjunction revisited. Proceedings
20th Annual German Conference Artificial Intelligence: Advances Artificial
Intelligence, KI 96, pp. 377386. Springer-Verlag.

45

fiJournal Artificial Intelligence Research 46 (2013) 651686

Submitted 10/12; published 04/13

Description Logic Knowledge Action Bases
Babak Bagheri Hariri
Diego Calvanese
Marco Montali

BAGHERI @ INF. UNIBZ .
CALVANESE @ INF. UNIBZ .
MONTALI @ INF. UNIBZ .

KRDB Research Centre Knowledge Data
Free University Bozen-Bolzano
Piazza Domenicani 3, 39100 Bolzano, Italy

Giuseppe De Giacomo
Riccardo De Masellis
Paolo Felli

DEGIACOMO @ DIS . UNIROMA 1.
DEMASELLIS @ DIS . UNIROMA 1.
FELLI @ DIS . UNIROMA 1.

Dipartimento di Ingegneria Informatica Automatica e Gestionale
Sapienza Universita di Roma
Via Ariosto 25, 00185 Roma, Italy

Abstract
Description logic Knowledge Action Bases (KAB) mechanism providing
semantically rich representation information domain interest terms description logic knowledge base actions change information time, possibly introducing
new objects. resort variant DL-Lite unique name assumption enforced
equality objects may asserted inferred. Actions specified sets
conditional effects, conditions based epistemic queries knowledge base
(TBox ABox), effects expressed terms new ABoxes. setting, address
verification temporal properties expressed variant first-order -calculus quantification across states. Notably, show decidability verification, suitable restriction inspired
notion weak acyclicity data exchange.

1. Introduction
Recent work business processes, services databases bringing forward need considering data processes first-class citizens process service design (Nigam & Caswell,
2003; Bhattacharya, Gerede, Hull, Liu, & Su, 2007; Deutsch, Hull, Patrizi, & Vianu, 2009; Vianu,
2009; Meyer, Smirnov, & Weske, 2011). particular, so-called artifact-centric approaches,
advocate sort middle ground conceptual formalization dynamic systems
actual implementation, promising effective practice (Cohn & Hull, 2009).
verification temporal properties presence data represents significant research challenge (for survey, see Calvanese, De Giacomo, & Montali, 2013), since taking account
data evolve time results systems infinite number states. Neither finite-state
model checking (Clarke, Grumberg, & Peled, 1999) current techniques infinitestate model checking, mostly tackle recursion (Burkart, Caucal, Moller, & Steffen, 2001),
apply case. Recently, advancements issue (Cangialosi, De Giacomo, De Masellis, & Rosati, 2010; Damaggio, Deutsch, & Vianu, 2011; Bagheri Hariri, Calvanese,
De Giacomo, De Masellis, & Felli, 2011; Belardinelli, Lomuscio, & Patrizi, 2011), context
suitably constrained relational database settings.
c
2013
AI Access Foundation. rights reserved.

fiBAGHERI H ARIRI , C ALVANESE , E G IACOMO , E ASELLIS , F ELLI , & ONTALI

work based maintaining information relational database,
sophisticated applications foreseen enrich data-intensive business processes semantic
level, information maintained semantically rich knowledge base allows
operating incomplete information (Calvanese, De Giacomo, Lembo, Montali, & Santoso, 2012;
Limonad, De Leenheer, Linehan, Hull, & Vaculin, 2012). leads us look combine
first-order data, ontologies, processes, maintaining basic inference tasks (specifically
verification) decidable. setting, capture domain interest terms semantically
rich formalisms provided ontological languages based Description Logics (DLs)
(Baader, Calvanese, McGuinness, Nardi, & Patel-Schneider, 2003). languages natively deal
incomplete knowledge modeled domain. additional flexibility comes added
cost, however: differently relational databases, evaluate queries need resort logical
implication. Moreover, incomplete information combined ability evolving system
actions results notoriously fragile setting w.r.t. decidability (Wolter & Zakharyaschev,
1999b, 1999a; Gabbay, Kurusz, Wolter, & Zakharyaschev, 2003). particular, due nature
DL assertions (which general definitions constraints models), get one
difficult kinds domain descriptions reasoning actions (Reiter, 2001),
amounts dealing complex forms state constraints (Lin & Reiter, 1994).
overcome difficulty, virtually solutions aim robustness based so-called
functional view knowledge bases (Levesque, 1984): KB provides ability querying
based logical implication (ask), ability progressing new KB forms
updates (tell) (Baader, Ghilardi, & Lutz, 2012; Calvanese, De Giacomo, Lenzerini, & Rosati,
2011). Notice functional view tightly related epistemic interpretation KB
(Calvanese, De Giacomo, Lembo, Lenzerini, & Rosati, 2007a). Indeed work also related
Epistemic Dynamic Logic (van Ditmarsch, van der Hoek, & Kooi, 2007), and, though
scope paper, decidability results presented could find application context
research well.
follow functional view KBs. However, key point work execution step external information incorporated system form new individuals (denoted
function terms), is, systems closed w.r.t. available information. makes
framework particularly interesting challenging. particular, presence individuals requires specific treatment equality, since system progresses new information
acquired, distinct function terms may inferred denote object.
Specifically, introduce so-called Knowledge Action Bases (KABs). KAB
equipped ontology or, precisely, TBox expressed, case, variant DLLiteA (Calvanese, De Giacomo, Lembo, Lenzerini, & Rosati, 2007b), extends core
Web Ontology Language OWL 2 QL (Motik, Cuenca Grau, Horrocks, Wu, Fokoue, & Lutz, 2012)
particularly well suited data management. TBox captures intensional information
domain interest, similarly UML class diagrams conceptual data models, though
software component used run-time. KAB includes also ABox, acts
storage state. ABox maintains data interest, accessed relying query answering based logical implication (certain answers). Notably, variant DL-LiteA without
unique name assumption (UNA), allow explicit equality assertions ABox.
way suitably treat function terms represent individuals acquired execution.
Technically, need dealing equality breaks first-order rewritability DL-LiteA query
answering, requires that, addition rewriting process, inference equality performed
652

fiD ESCRIPTION L OGIC K NOWLEDGE ACTION BASES

(Artale, Calvanese, Kontchakov, & Zakharyaschev, 2009). query language, use unions
conjunctive queries, possibly composing certain answers full FOL constructs.
gives rise epistemic query language asks known current KB (Calvanese et al., 2007a). Apart KB, KAB contains actions, whose execution changes
state KB, i.e., ABox. actions specified sets conditional effects,
conditions (epistemic) queries KB effects expressed terms new ABoxes.
Actions static pre-conditions, whereas process used specify actions
executed step. simplicity, model processes condition/action rules,
condition expressed query KB.
setting, address verification temporal/dynamic properties expressed firstorder variant -calculus (Park, 1976; Stirling, 2001), atomic formulae queries
KB refer constants function terms, controlled form
quantification across states allowed. Notice previous decidability results actions
DL KBs assumed information coming outside system, sense
new individual terms added executing actions (Calvanese et al., 2011; Baader et al., 2012;
Rosati & Franconi, 2012). paper, instead, allow arbitrary introduction new terms.
Unsurprisingly, show even simple KABs temporal properties, verification
undecidable. However, also show rich class KABs, verification fact decidable
reducible finite-state model checking. obtain result, following Cangialosi et al. (2010),
Bagheri Hariri et al. (2011), rely recent results data exchange finiteness
chase tuple-generating dependencies (Fagin, Kolaitis, Miller, & Popa, 2005), though, case,
need extend approach deal (i) incomplete information, (ii) inference equality,
(iii) quantification across states verification language.
paper organized follows. Section 2 give preliminaries DL-LiteA without
UNA , going knowledge base formalism. Section 3 describes KAB framework detail, Section 4 discusses execution semantics. Section 5 introduce
verification formalism KABs. Section 6, show verification KABs general undecidable, even considering simple temporal properties KABs. Section 7, give
main technical result: verification weakly acyclic KABs decidable E XP IME. Section 8,
extensively survey related work. Section 9 concludes paper.

2. Knowledge Base Formalism
Description Logics (DLs) (Baader et al., 2003) knowledge representation formalisms
tailored representing domain interest terms concepts (or classes), denoting sets
objects, roles (or relations), denoting binary relations objects. DL knowledge bases
(KBs) based alphabet concept role names, alphabet individuals.
DL KB formed two distinct parts: TBox, represents intensional level KB
contains description domain interest terms universal assertions concepts
roles; ABox, represents instance level KB contains extensional
information participation individuals concepts roles.
expressing KBs use DL-LiteNU , variant DL-LiteA language (Poggi, Lembo,
Calvanese, De Giacomo, Lenzerini, & Rosati, 2008; Calvanese, De Giacomo, Lembo, Lenzerini, &
Rosati, 2013) drop unique name assumption (UNA) line standard Web
Ontology Language (OWL 2) (Bao et al., 2012). Essentially, DL-LiteNU extends OWL 2 QL
653

fiBAGHERI H ARIRI , C ALVANESE , E G IACOMO , E ASELLIS , F ELLI , & ONTALI

profile OWL 2, including functionality assertions possibility state equality
individuals.
syntax concept role expressions DL-LiteNU follows:
R P | P ,
V R | R,

B N | R,
C B | B,

N denotes concept name, P role name, P inverse role.
Formally, DL-LiteNU KB (T, A), TBox finite set TBox assertions form
B v C,

R v V,

(funct R),

called respectively concept inclusions, role inclusions, functionality assertions. follow
usual assumption DL-Lite, according TBox may contain neither (funct P )
(funct P ) contains R v P R v P , role R (Poggi et al., 2008; Calvanese et al.,
2013). condition expresses roles functionality assertions cannot specialized.
DL-LiteNU TBoxes able capture essential features conceptual modeling formalisms,
UML Class Diagrams (or Entity-Relationship schemas), namely ISA classes
associations (relationships), disjointness classes associations, typing associations, association multiplicities (in particular, mandatory participation functionality).
main missing feature completeness hierarchies, would require introduction
disjunction would compromise good computational properties DL-Lite.
ABox DL-LiteNU KB (T, A) finite set ABox assertions form
N (t1 ),

P (t1 , t2 ),

t1 = t2 ,

called respectively, concept (membership) assertions, role (membership) assertions, equality
assertions, t1 , t2 terms denoting individuals (see below). presence equality assertions ABox requires specific treatment equality goes beyond usual reasoning
techniques DL-Lite based first-order rewritability, although reasoning remains polynomial
(Artale et al., 2009). hand, allow explicit disequality, though one use
membership disjoint concepts assert two individuals different.
DL-LiteNU admits complex terms denoting individuals. terms inductively defined
starting finite set constants, applying finite set (uninterpreted) functions
various arity greater 0. result, set individual terms countably infinite.
call function terms terms involving functions. Also, structure terms impact
inference equality, congruence relation structure terms, i.e., ti = t0i ,
{1, . . . , n}, f function symbol arity n, f (t1 , . . . , tn ) = f (t01 , . . . , t0n ). Apart
aspect related equality, treat individuals denoted terms simply ordinary
individual constants DLs.
adopt standard semantics DLs based FOL interpretations = (I , ),
interpretation domain interpretation function tI , N ,
P , term t,concept name N , role name P . Coherently congruence
relation terms, (f (t1 , . . . , tn ))I = (f (t01 , . . . , t0n ))I , whenever tIi = t0i ,
{1, . . . , n}.
Complex concepts roles interpreted follows:
(R)I
(B)I

= {o | o0 .(o, o0 ) RI },
= \ B ,

(P )I
(R)I
654

= {(o1 , o2 ) | (o2 , o1 ) P },
= \ R .

fiD ESCRIPTION L OGIC K NOWLEDGE ACTION BASES

interpretation satisfies assertion form:
B v C, B C ;
R v V , RI V ;
(funct R), o, o1 , o2 that, {(o, o1 ), (o, o2 )} RI , o1 = o2 ;
N (t1 ), tI1 N ;
P (t1 , t2 ), (tI1 , tI2 ) P ;
t1 = t2 , tI1 = tI2 .
model KB (T, A) satisfies assertions A. KB (T, A) satisfiable
model. also say ABox consistent w.r.t. TBox KB (T, A) satisfiable.
assertion logically implied KB (T, A), denoted (T, A) |= , every model (T, A)
satisfies well.
following characterization satisfiability logical implication DL-LiteNU easy
consequence results Artale et al. (2009).
Theorem 1 Checking satisfiability logical implication DL-LiteNU PT IME-complete.
Proof. PT IME lower bound immediate consequence lower bound established
Artale et al. (2009) DL-LiteNU allow use complex individual terms.
upper bound, Artale et al. (2009) provide PT IME algorithm based first using
functionality assertions exhaustively propagate equality, resorting PT IME algorithm
(in combined complexity) reasoning DL-Lite absence UNA. adapt
algorithm changing first step, propagate, PT IME, equality terms
active domain due functionalities, also due congruence.
Next introduce queries. usual (cf. OWL 2), answers queries formed constants/terms denoting individuals explicitly mentioned ABox. (active) domain
ABox A, denoted ADOM(A), (finite) set constants/terms appearing concept, role,
equality assertions A. (predicate) alphabet KB (T, A), denoted ALPH((T, A)) set
concept role names occurring A.
union conjunctive queries (UCQ) q KB (T, A) FOL formula form
~y1 .conj 1 (~x, ~y1 ) ~yn .conj n (~x, ~yn ) free variables ~x existentially quantified variables ~y1 , . . . , ~yn . conj (~x, y~i ) q conjunction atoms form N (z), P (z, z 0 )
N P respectively denote concept role name occurring ALPH((T, A)), z, z 0
constants ADOM(A) variables ~x y~i , {1, . . . , n}. certain answers q
(T, A) set ANS (q, T, A) substitutions1 free variables q constants/terms
ADOM(A) q evaluates true every model (T, A), i.e., q logically implied
(T, A). Following notation used assertions, denote (T, A) |= q. q free
variables, called boolean certain answers either empty substitution denoting
true nothing denoting false.
Again, easy consequence results Artale et al. (2009), obtain following
characterization query answering DL-LiteNU .
Theorem 2 Computing ANS (q, T, A) UCQ q DL-LiteNU KB (T, A) PT IME-complete
size A.
1. customary, view substitution simply tuple constants, assuming ordering free
variables q.

655

fiBAGHERI H ARIRI , C ALVANESE , E G IACOMO , E ASELLIS , F ELLI , & ONTALI

Proof. proof Theorem 1, first propagate PT IME equality terms
active domain using functionality congruence closure, resort query answering
DL-Lite presence UNA, PT IME combined size TBox
ABox resulting equality propagation (actually, AC0 size ABox).
also consider extension UCQs, called ECQs, queries query language
EQL-Lite(UCQ) (Calvanese et al., 2007a), is, FOL query language whose atoms UCQs
evaluated according certain answer semantics above. ECQ KB (T, A) possibly
open formula form
Q [q] | [x = y] | Q | Q1 Q2 | x.Q,
[q] denotes certain answers UCQ q (T, A), [x = y] denotes certain answers
x = (T, A), is, set {hx, yi ADOM(A) | (T, A) |= (x = y)}, logical operators
usual meaning, quantification ranges elements ADOM(A).
Formally define relation Q holds (T, A) substitution free variables
Q, written T, A, |= Q, inductively follows:
T, A,
T, A,
T, A,
T, A,
T, A,

|= [q]
|= [x = y]
|= Q
|= Q1 Q2
|= x.Q







(T, A) |= q,
(T, A) |= (x = y),
T, A, 6|= Q,
T, A, |= Q1 T, A, |= Q2 ,
exists ADOM(A) T, A, [x/t] |= Q,

[x/t] denotes substitution obtained assigning x constant/term (if x
already present value replaced t, not, pair x/t added substitution).
certain answer Q (T, A), denoted ANS (Q, T, A), set substitutions
free variables Q Q holds (T, A) , i.e.,
ANS (Q, T, A)

= { | T, A, |= Q}.

Following line proof Calvanese et al. (2007a), considering Theorem 2
basic step evaluating UCQ, get:
Theorem 3 Computing ANS (Q, T, A) ECQ Q DL-LiteNU KB (T, A) PT IMEcomplete size A.
recall DL-Lite enjoys rewritability property, states every UCQ q
every DL-Lite KB (T, A),
ANS (q, T, A)

= ANS (rew (q), , A),

rew (q) UCQ computed reformulation algorithm Calvanese et al. (2007b).
Notice that, way, compiled away TBox. result extended ECQs
well, i.e., every ECQ Q, ANS (Q, T, A) = ANS (rew (Q), , A) query rew (Q)
obtained Q substituting atom [q] (where q UCQ) [rew (q)] (Calvanese et al.,
2007a). setting, exploit rewritability, pre-processed
ABox (in PT IME) propagating equalities individual terms ADOM(A) according
functionality assertions congruence terms.
656

fiD ESCRIPTION L OGIC K NOWLEDGE ACTION BASES

say two ABoxes A1 A2 equivalent w.r.t. TBox predicate alphabet ,
denoted
A1 T, A2 ,
every ABox assertion 2 A2 either concept assertion N (t) N , role
assertion P (t1 , t2 ) P , equivalence assertion t1 = t2 , (T, A1 ) |= 2 ; viceversa, every ABox assertion 1 A1 , either concept assertion N (t) N ,
role assertion P (t1 , t2 ) P , equivalence assertion t1 = t2 , (T, A2 ) |= 1 .
Notice A1 T, A2 , every ECQ Q whose concept role names belong
ANS (Q, T, A1 ) = ANS (Q, T, A2 ). Notice also that, applying Theorem 3 boolean
query [] corresponding ABox assertion , A1 A2 , obtain ABox
equivalence checked PT IME.

3. Knowledge Action Bases
Knowledge Action Base (KAB) tuple K = (T, A0 , , ) A0 form
knowledge component (or knowledge base), form action component (or action
base). practice, K stores information interest KB, formed fixed TBox
initial ABox A0 , evolves executing actions according sequencing established
process . evolution new individuals acquired KB. individuals
witnesses new pieces information inserted KAB environment KAB
runs (i.e., external world). represent new objects function terms. KAB
evolves, identity individuals intuitively preserved induces necessity
remembering equalities terms denoting individuals discovered past. describe
detail components KAB.
3.1 TBox
DL-LiteNU TBox, used capture intensional knowledge domain interest.
TBox fixed all, evolve execution KAB.
3.2 ABox
A0 DL-LiteNU ABox, stores extensional information interest. Notice A0
ABox initial state KAB, KAB evolves due effect actions,
ABox, indeed state system, evolves accordingly store up-to-date information.
actions acquire new information external world using calls external
services represented functions. Given information services,
except name parameters passed them, functions remain uninterpreted.
assume result service calls depends passed parameters. Hence,
represent new individuals returned service calls function terms. presence
function terms impact treatment equality, since principle need close equality
w.r.t. congruence. closure generates infinite number logically implied equality
assertions, going keep assertions implicit, computing needed.
657

fiBAGHERI H ARIRI , C ALVANESE , E G IACOMO , E ASELLIS , F ELLI , & ONTALI

3.3 Actions
finite set actions. action modifies current ABox adding deleting
assertions, thus generating new ABox A0 . action form
act(~x) : {e1 , . . . , en },
act(~x) signature {e1 , . . . , en } (finite) set effects forming effect
specification . action signature constituted name act list ~x individual input
parameters, need instantiated actual individuals execution time.2 effect ei
form
[qi+ ] Q
A0i ,
(1)


qi+ UCQ, i.e., positive query, extracts bulk data process (obtained
certain answers qi+ ); free variables qi+ include action parameters;
+
Q
arbitrary ECQ, whose free variables occur among free variables qi ,
refines, using negation quantification, result qi+ . query [qi+ ] Q

whole extracts individual terms used form new state KAB (notice
UCQ-ECQ division also convenience readily available positive part
condition, exploit later);

A0i set (non-ground) ABox assertions, include terms: constants A0 , free
variables qi+ , function terms f (~x) arguments ~x free variables qi+ .
terms, grounded values extracted [qi+ ] Q
, give rise (ground) ABox
assertions, contribute form next state KAB.
precisely, given current ABox K substitution input parameters
action , denote action instantiated actual parameters coming .
firing state A, get new state A0 computed simultaneously applying
instantiated effects follows:
effect ei form (1) extracts set ANS (([qi+ ] Q
), T, A) tuples
terms ADOM(A) and, tuple , asserts set A0i ABox assertions obtained
A0i applying substitution free variables qi+ . function term
f (~x) appearing A0i , new ground term introduced form f (~x).
terms represent new constants coming external environment KAB running
in.
denote ei (A) overall set ABox assertions, i.e.,
[

ei (A) =

A0i .

ANS (([qi+ ]Q
),T,A)

2. disregard specific treatment output parameters, assume instead user freely pose queries
KB, extracting whatever information she/he interested in.

658

fiD ESCRIPTION L OGIC K NOWLEDGE ACTION BASES

Moreover, let EQ(T, A) = {t1 = t2 | ht1 , t2 ANS ([x1 = x2 ], T, A)}. Observe that, due
semantics queries, terms EQ(T, A) must appear explicitly ADOM(A),
is, possibly infinite number equalities due congruence appear EQ(T, A),
though logically implied. Hence, equalities EQ(T, A) equality assertions involving terms ADOM(A) either appear explicitly A, obtained
closing functionality congruence terms.
overall effect action parameter substitution new ABox A0 =
(T, A, )
[
ei (A).
(T, A, ) = EQ(T, A)
1in

Notice presence function terms action effects makes domain ABoxes obtained executing actions continuously changing general unbounded size. Notice also
persistence assumption equalities, i.e., implicitly copy equalities holding current state new one. implies that, system evolves, acquire new
information equalities terms, never lose information equalities already acquired.
Finally, observe execution mechanism persistence/frame assumption (except equality) made. principle every move substitute whole old state, i.e., ABox,
new one. hand, clear easily write effect specifications
copy big chunks old state new one. example, [P (x, y)]
P (x, y) copies
entire set assertions involving role P . sense, execution mechanism adopted
paper basic address elaboration tolerance issues typical
reasoning actions, frame problem, ramification problem qualification problem
(Reiter, 2001)3 . consider irrelevant, contrary, relevant
research issues desirable. adopt basic mechanism simply
general enough expose difficulties need overcome order get decidability
verification setting.
3.4 Process
process component KAB possibly nondeterministic program uses KAB ABoxes
store (intermediate final) computation results, actions atomic instructions.
ABoxes arbitrarily queried KAB TBox , updated
actions . specify process component adopt rule-based specification.
Specifically, process finite set condition/action rules. condition/action rule
expression form
Q 7 ,
action Q ECQ, whose free variables exactly parameters
. rule expresses that, tuple condition Q holds, action actual
parameters executed. Processes force execution actions constrain them:
user process able choose action rules forming process allow.
Moreover, processes inherit entirely states KAB knowledge component (TBox
ABox) (see, e.g., Cohn & Hull, 2009).
3. see also work Kowalski Sadri (2011).

659

fiBAGHERI H ARIRI , C ALVANESE , E G IACOMO , E ASELLIS , F ELLI , & ONTALI

Villain v Character
livesIn v Character
livesIn v City
Character v livesIn
(funct livesIn)
enemy v Villain
enemy v Superhero
defeated v Villain
defeated v Superhero
defeated v enemy
alterEgo v Superhero
alterEgo v Character
(funct alterEgo)

0..1

alterEgo
enemy

Superhero

Character

1..1

livesIn

City

Villain

{subset}
defeated

Figure 1: KABs TBox Example 1
observe adopt basic rule-based specification because, spite simplicity,
able expose difficulties setting. choices also possible, particular,
process could maintain state besides one KAB. long additional
state finite, embeddable KAB itself, results would easily extend case.
Example 1 Let us consider KAB K = (T, A0 , , ) describing super-heroes comics world,
cities characters live. Figure 1 shows TBox rendering
UML Class Diagram. relationship UML Class Diagrams Description Logics
general DL-Lite particular, refer work Berardi, Calvanese, De Giacomo
(2005) Calvanese, De Giacomo, Lembo, Lenzerini, Poggi, Rodrguez-Muro, Rosati
(2009). dynamics domain, characters superheroes (super)villains,
fight other. classic plot, superheroes help endeavors law enforcement
fighting villains threatening city live in. villain reveals perpetrating
nefarious purposes citys peace, consequently becomes declared enemy
superheroes living city. character lives one city time. common trait
superheroes secret identity: superhero said alter ego character,
identity common life. Hence, ABox assertion alterEgo(s, p) means superhero
alter ego character p. Villains always try unmask superheroes, i.e., find secret identity,
order exploit knowledge defeat them. Notice subtle difference here: use
alterEgo(s, p) assertion model fact alter ego p, whereas asserting = p
capture knowledge p semantically denote individual. may include
actions like following ones:
BecomeSH(p, c) : { [Character(p) livesIn(p, c) v.Villain(v) livesIn(v, c)]
{Superhero(sh(p)), alterEgo(sh(p), p)},
CopyAll }
states exists least one villain living city c, new superhero sh(p) created,
purpose protecting c. superhero p alter ego. CopyAll shortcut
explicitly copying concept role assertions new state (equality assertions always
660

fiD ESCRIPTION L OGIC K NOWLEDGE ACTION BASES

implicitly copied). Action
Unmask(s, p) : { [alterEgo(s, p)]
CopyAll }

{s = p},

states superhero s, alter ego p, gets unmasked asserting equality
p (it known = p). Action
Fight(v, s) : { p.[Villain(v) Character(p) alterEgo(s, p)] [s = p]
CopyAll }

{defeated(v, s)},

states villain v fights superhero s, defeats unmasked, i.e., known
equal alter ego. Action
Challenge(v, s) :
{ [Villain(v) Superhero(s) p.alterEgo(s, p) livesIn(p, sc)] [defeated(v, s)]
{livesIn(v, sc), enemy(v, s)},
CopyAll }
states villain v challenges superhero defeated him, next lives
city enemy s. Action
ThreatenCity(v, c) :
{ [Villain(v) Superhero(s) p.alterEgo(s, p) livesIn(p, c)]
{enemy(v, s) livesIn(v, c)}
CopyAll }
states villain v threatens city c, becomes enemy superheroes
live c.
process might include following rules:
[Character(p)] [s.Superhero(s) livesIn(s, c)]
[Superhero(s) Character(c)]
[enemy(v, s)] [v 0 .defeated(v 0 , s)]
[Villain(v) Superhero(s)]
[Villain(v) City(c)] v 0 ([Villain(v 0 ) livesIn(v 0 , c)] [v = v 0 ])

7
7

7
7
7

BecomeSH(p, c),
Unmask(s, c),
Fight(v, s),
Challenge(v, s),
ThreatenCity(v, c).

instance, first rule states character become superhero city already
one, whereas last one states villain threaten city, city
another villain (known be) distinct him/her.
Notice that, execution, reasoning KB performed. instance, consider
initial ABox
A0 = { Superhero(batman), Villain(joker), alterEgo(batman, bruce),
livesIn(bruce, gotham), livesIn(batman, gotham), livesIn(joker, city1) }.
state, bruce batman live city, batman alter-ego bruce,
known whether denote individual. Executing Challenge(joker, batman) A0 ,
indeed allowed process , generates new ABox added assertions enemy(joker,
batman), livesIn(joker, gotham), gotham = city1 implied functionality livesIn.

661

fiBAGHERI H ARIRI , C ALVANESE , E G IACOMO , E ASELLIS , F ELLI , & ONTALI

4. KAB Transition System
semantics KABs given terms possibly infinite transition systems represent
possible evolutions KAB time, actions executed according process. Notice
transition systems must equipped semantically rich states, since full KB associated them. Formally define kind transition system need follows: transition
system tuple form (U, T, , s0 , abox , ), where:
U countably infinite set terms denoting individuals, called universe;
TBox;
set states;
s0 initial state;
abox function that, given state returns ABox associated
individuals terms U, conforms ;
transition relation pairs states.
convenience,
introduce active domain whole transition system, defined
ADOM () = ADOM (abox (s)). Also introduce (predicate) alphabet ALPH ()
set concepts roles occurring co-domain abox .
KAB generates transition system form execution. Formally, given
KAB K = (T, A0 , , ), define (generated) transition system K = (U, T, , s0 , abox , )
follows:
U formed constants function terms inductively formed starting
ADOM (A0 ) applying functions occurring actions ;
TBox KAB;
abox identity function (i.e., state simply ABox);
s0 = A0 initial state;
defined mutual induction smallest sets satisfying following property: , rule Q 7 , evaluate Q and, tuple returned,
(T, abox (s), ) consistent w.r.t. , s0 s0 = (T, abox (s), ).
Notice alphabet ALPH(K ) K simply formed set ALPH(K) concepts
roles occur K.
KAB transition system K infinite tree infinitely many different ABoxes
nodes, general. fact, get transition system infinite, enough perform
indefinitely simple action adds new terms step, e.g., action form
() : { [C(x)]

{C(f (x))}, CopyAll }.

Hence classical results model checking (Clarke et al., 1999), developed finite
transition systems, cannot applied directly verifying KABs.
662

fiD ESCRIPTION L OGIC K NOWLEDGE ACTION BASES

5. Verification Formalism
specify dynamic properties KABs, use first-order variant -calculus (Stirling, 2001;
Park, 1976). -calculus virtually powerful temporal logic used model checking
finite-state transition systems, able express linear time logics LTL PSL,
branching time logics CTL CTL* (Clarke et al., 1999). main characteristic
-calculus ability expressing directly least greatest fixpoints (predicate-transformer)
operators formed using formulae relating current state next one. using fixpoint
constructs one easily express sophisticated properties defined induction co-induction.
reason virtually logics used verification considered fragments
-calculus. Technically, -calculus separates local properties, asserted current state
states immediate successors current one, properties talking states
arbitrarily far away current one (Stirling, 2001). latter expressed use
fixpoints.
work, use first-order variant -calculus, allow local properties
expressed ECQs, time allow arbitrary first-order quantification across
states. Given nature ECQs used formulating local properties, first-order quantification
ranges terms denoting individuals. Formally, introduce logic LA defined follows:
Q | | 1 2 | x. | hi | Z | Z.,
Q possibly open ECQ Z second order predicate variable (of arity 0). make use
following abbreviations: x. = (x.), 1 2 = (1 2 ), [] = hi,
Z. = Z.[Z/Z]. formulae Z. Z. respectively denote least
greatest fixpoint formula (seen predicate transformer Z.). usual -calculus,
formulae form Z. (and Z.) must obey syntactic monotonicity w.r.t. Z,
states every occurrence variable Z must within scope even number
negation symbols. ensures least fixpoint Z. (as well greatest fixpoint Z.)
always exists.
semantics LA formulae defined possibly infinite transition systems form
hU, T, , s0 , abox , seen above. Since LA also contains formulae individual
predicate free variables, given transition system , introduce individual variable valuation
v, i.e., mapping individual variables x U, predicate variable valuation V , i.e.,
mapping predicate variables Z subsets . three notions place,
assign meaning formulae associating , v, V extension function ()
v,V , maps

formulae subsets . Formally, extension function ()v,V defined inductively follows:
(Q)
v,V
()
v,V
(1 2 )
v,V
(x.)
v,V
(hi)
v,V
(Z)
v,V
(Z.)
v,V

=
=
=
=
=
=
=

{s | ANS (Qv, T, abox (s)) = true},
\ ()
v,V ,

(1 )
v,V (2 )v,V ,
{s | t.t ADOM(abox (s)) ()
v[x/t],V },

0
0
0
{s | .s ()v,V },
V (Z),

{E | ()
v,V [Z/E] E}.

Qv stands (boolean) ECQ obtained Q substituting free variables according
v. Intuitively, ()
v,V assigns constructs following meaning:
663

fiBAGHERI H ARIRI , C ALVANESE , E G IACOMO , E ASELLIS , F ELLI , & ONTALI

boolean connectives expected meaning.
quantification individuals done terms current ABox. Notice
terms referred later states associated ABox include
terms anymore.
extension hi consists states that, state s0 transition
s0 , formula holds s0 valuation v .
extension [] consists states that, states s0 transition s0 ,
formula holds s0 valuation v.
extension Z. smallest subset E that, assigning Z
extension E , resulting extension (under valuation v) contained E . is,
extension Z. least fixpoint operator ()
v,V [Z/E] , V [Z/E] denotes
predicate valuation obtained V forcing valuation Z E.
Similarly, extension Z. greatest subset E that, assigning
Z extension E , resulting extension contains E . is, extension
Z.


=
{E

|E
.
Formally,
(Z.)
greatest fixpoint operator ()
v,V
v,V [Z/E]
()
v,V [Z/E] }.
closed formula, ()
v,V depend v V , denote extension

simply () . closed formula holds state () . case, write
, |= . closed formula holds , denoted |= , , s0 |= . call model
checking problem verifying whether |= holds.
next example shows simple temporal properties expressed LA .
Example 2 Considering KAB Example 1, easily express temporal properties
following ones.
current superheroes live Gotham live Gotham forever (a form
safety):
x.[Superhero(x) livesIn(x, gotham)] Z.([livesIn(x, gotham)] []Z).
Eventually current superheroes unmasked (a form liveness):
x.[Superhero(x)] Z.([alterEgo(x, x)] []Z).
exists possible future situation current superheroes unmasked (another form liveness):
x.[Superhero(x)] Z.([alterEgo(x, x)] hiZ).
Along every future, always true, every superhero, exists evolution
eventually leads unmask (a form liveness holds every moment):
Y.(x.[Superhero(x)] Z.([alterEgo(x, x)] hiZ)) []Y.
664

fiD ESCRIPTION L OGIC K NOWLEDGE ACTION BASES

Consider two transition systems sharing universe predicate alphabet.
say behaviorally equivalent satisfy exactly LA formulas.
formally capture equivalence, make use notion bisimulation (Milner, 1971),
suitably extended deal query answering KBs.
Given two transition systems 1
=
hU, T, 1 , s01 , abox 1 , 1 2
=
hU, T, 2 , s02 , abox 2 , 2 sharing universe U, TBox ,
ALPH (1 ) = ALPH (2 ) = , bisimulation 1 2 relation B 1 2
(s1 , s2 ) B implies that:
1. abox (s1 ) T, abox (s2 );
2. s1 1 s01 , exists s02 s2 2 s02 (s01 , s02 ) B;
3. s2 2 s02 , exists s01 s1 1 s01 (s01 , s02 ) B.
say two states s1 s2 bisimilar exists bisimulation B (s1 , s2 )
B. Two transition systems 1 initial state s01 2 initial state s02 bisimilar
(s01 , s02 ) B. following theorem states formula evaluation LA indeed invariant
w.r.t. bisimulation, equivalently check bisimilar transition systems.
Theorem 4 Let 1 2 two transition systems share universe, TBox,
predicate alphabet, bisimilar. Then, two states s1 1 s2 2
(including initial ones) bisimilar, closed LA formulas ,
s1 ()1

iff

s2 ()2 .

Proof. proof analogous standard proof bisimulation invariance -calculus (Stirling, 2001), though taking account bisimulation, guarantees ECQs evaluated
identically bisimilar states. Notice assumption two transition systems share
universe predicate alphabet makes easy compare answers queries.
Making use notion bisimulation, can, example, redefine transition system
generated KAB K = (T, A0 , , ) maintaining bisimilarity, modifying definition
K = hU, T, , s0 , abox , given Section 4 follows.
(i) modify DO() function term t0 introduced generated ABox A0
current ABox4 already term (T, A) |= = t0 .
(ii) ABox A0 = DO(T, abox (s), ) obtained current state logically equivalent
ABox abox (s00 ), already generate state s00 , generate new state,
simply add s00 K .

6. Verification KABs
immediate see verification KABs undecidable general easy represent
Turing machines using KAB. Actually using fragment capabilities
KABs, shown next lemma.
Lemma 5 Checking formulas form Z.(N (a) hiZ), N atomic concept
individual occurring A0 , undecidable already KAB K = (T, A0 , , ) where:
4. Note terms present current ABox preserved new ABox, together equalities
terms.

665

fiBAGHERI H ARIRI , C ALVANESE , E G IACOMO , E ASELLIS , F ELLI , & ONTALI

{First(c)}
{value(c, x)}
{value(c, av0 )}
{cell(cr , aq0 )}
{cell(n(c), aq0 ), next(c, n(c)), Last(n(c))}
{Last(c)}
{cell(c, #)}
{cell(cr , #)}
{Stop(0)}

[First(c)]
[cell(c, #) value(c, x)]
[cell(c, aq ) value(c, av )]
[cell(c, aq ) value(c, av ) next(c, cr )]
[cell(c, aq ) value(c, av ) Last(c)]
[cell(c, #) Last(c)]
[cell(c, #) First(c)]
[cell(c, #) next(c, cr )]
[cell(c, aqf )]

Figure 2: Effects action used encode transition (q, v, q 0 , v 0 , R) Turing Machine
empty TBox,
actions make use negation equality,
trivial process poses restriction executability actions.
Proof. Given Turing machine = hQ, , q0 , , qf , i, show construct corresponding
KAB KM = (, A0 , , ) mimics behavior M. Specifically, encode halting
problem verification problem KM . Roughly speaking, KM maintains tape
state information (current) ABox, encodes transitions actions.
construction makes use tape initially contains unique cell, represented constant
0, extended on-the-fly needed: cells right 0 represented function terms
form n(n( (0) )), cells left 0 represented function terms form
p(p( (0) )). Then, make use one constant aq state q Q, one constant av
tape symbol value v , special constant #, following concepts roles:
cell(c, h) models cell tape, c cell identifier, h corresponds current
state M, head currently points c, # head currently point
c;
next(cl , cr ) models relative position cells, stating cr cell immediately following cl ;
value(c, v) models cell c currently contains value v, v ;
First(c) Last(c) respectively denote current first cell last cell portion
tape explored far.
Stop(c) used detect halts.
initial state KM contains unique cell defined
A0 = { cell(0, aq0 ), value(0, ), First(0), Last(0) }.
action component, contains action parameters transition ,
process poses restriction executability actions, i.e., contains rule true 7 ()
action .
provide specification actions, detailing case right shift transition
(q, v, q 0 , v 0 , R). corresponding action specification consists set effects shown
Figure 2. first effect maintains first position tape unaltered. second third
666

fiD ESCRIPTION L OGIC K NOWLEDGE ACTION BASES

effects deal cell values. remain except current cell, updated
according transition. next three effects deal right shift Turing Machine
state. current cell next cell therefore last one, head moved
next cell state change recorded there. case last cell remains
same. instead current cell last one, moving head tape must properly
extended. function n/1 used create identifier new successor cell, starting
identifier current one. Furthermore, since transition corresponds right shift one
cell, first cell cells immediately following cell marked # marked #
next state. Finally, last effect used identify case reached final state.
marked inserting new state special assertion Stop(0).
construction left shift transition done symmetrically, using function p/1 create
new predecessor cell. construction, KM satisfies conditions theorem. Observe that,
transition system KM generated KM , every action corresponding every transition
executed ABox/state KM , since empty, actually generate
successor state s. However, state, (unique) action corresponds actually
executed transition generate successor state containing ABox assertion form
cell(c, aq ), state q M. Therefore, ABoxes/states properly corresponding
configurations could eventually lead ABox/state KM Stop(0) holds.
latter happen halts. precisely, one show induction length
respectively halting computation shortest path initial state KM
state Stop(0) holds, halts KM |= Z.([Stop(0)] hiZ),
concludes proof.
previous lemma, shows undecidability already special case, immediately
obtain following result.
Theorem 6 Verification LA formulae KABs undecidable.
observe Lemma 5 uses KB constituted ABox containing concept
role assertions, makes use conjunctive queries defining actions effects. Moreover, formula check makes use quantification all, simply seen
propositional CTL formula form EF p, expressing proposition p eventually holds along
one path.

7. Verification Weakly Acyclic KABs
spite Theorem 6, next introduce notable class KABs verification arbitrary
LA properties decidable. so, rely syntactic restriction resembles notion
weak acyclicity data exchange (Fagin et al., 2005)5 , guarantees boundedness ABoxes
generated execution KAB and, turn, decidability verification.
ready introduce notion weak acyclicity context. introduce
edge-labeled directed dependency graph KAB K = (T, A0 , , ), defined follows. Nodes,
called positions, obtained TBox T: node every concept name N ,
two nodes every role name P , corresponding domain range P . Edges
5. use original definition weak acyclicity. However, results applied also variants weak
acyclicity (see discussion Section 9).

667

fiBAGHERI H ARIRI , C ALVANESE , E G IACOMO , E ASELLIS , F ELLI , & ONTALI

Villain

livesIn,1

livesIn,2

City

enemy,1

Character

alterEgo,2

defeats,2

*
defeats,1

*

*

alterEgo,1

*
SuperHero

enemy,2

Figure 3: Weakly acyclic dependency graph Example 1.
drawn considering every effect specification [q + ] Q
A0 action contained
, tracing values copied contribute generate new values system progresses.
particular, let p position corresponding concept/role component rewriting rew (q + )
q + variable x. every position p0 A0 variable x, include normal
edge p p0 . every position p00 A0 function term f (~t) x ~t, include

special edge p
p00 . say K weakly-acyclic dependency graph cycle going
special edge.
Example 3 KAB Example 1 weakly acyclic. dependency graph, shown Figure 3,
contain cycle going special edges. readability, self-loops shown
Figure (but present nodes), dashed edges used compactly represent
contributions given rewriting queries. E.g., dashed edge form Villain Character
denotes every outgoing edge Character, exists outgoing edge Villain
type target. Hence, w.r.t. weak acyclicity dashed edges simply replaced
normal edges.
ready state main result work, going prove remainder
section.
Theorem 7 Verification LA properties weakly acyclic KAB decidable E XP IME
size KAB.
observe restriction imposed weak acyclicity (or variants) severe,
many real cases KABs indeed weakly acyclic transformed weakly acyclic ones
cost redesign. Indeed, weakly acyclic KABs cannot indefinitely generate new values
old ones, depend chain unboundedly many previous values. words,
current values depend bounded number old values. unbounded systems exist
theory, e.g., Turing machines, higher level processes, business process management
service-oriented modeling, typically require boundedness practice. systematically
transform systems weakly acyclic ones remains open issue.
remainder section present proof Theorem 7. several steps:
1. Normalized KAB. First introduce normalized form K KAB K, isolates
contribution equalities TBox actions effects KAB. important point
normalizing KAB preserves weak acyclicity.
668

fiD ESCRIPTION L OGIC K NOWLEDGE ACTION BASES

2. Normalized DO(). Then, introduce normalized version NORM () DO(), avoids
consider equalities generating bulk set tuples used effects generate
next ABox. transition system K,NORM generated normalized version
NORM () () normalized KAB K bisimilar transition system K generated DO() K. Hence two transition systems satisfy LA formulae.

3. Positive dominant. next step introduce call positive dominant K++
normalized KAB K. obtained K essentially dropping equalities, negations,
TBox. However K++ contains enough information positive part that,
drop features, active domain transition system K++ generated
K++ overestimates active domain transition system K,NORM generated
normalized KAB K. Moreover, normalized (and hence original) KAB weakly
acyclic, positive dominant. Finally positive dominant weakly acyclic
size active domain transition system K++ polynomially bounded size
initial ABox, hence size active domain K,NORM . implies
size K,NORM finite exponential size initial ABox.
4. Putting together. Tying results together, get claim.
following, detail steps.
7.1 Normalized KAB
Given KAB K = (T, A0 , , ), build KAB K = (T, A0 , , ), called normalized form
K, applying sequence transformations preserve semantics K producing
KAB format easier study.
1. view ABox partitioned part collecting concept role assertions,
part collecting equality assertions. denote A6E Q former EQ(T, A)
latter, closed w.r.t. (the functionality assertions in) TBox . Notice
closure computed polynomial time size .
2. K individuals appearing equality assertions ABox also occur special concept assertions form Dummy(t), concept Dummy unrelated
concepts roles KAB. by:
adding concept assertions Dummy(t) equality assertion A0
appear elsewhere;
adding right-hand part action effect ei concept assertion Dummy(t)
equality assertion right-hand part ei ;
adding action effect specification form
[Dummy(x)]

{Dummy(x)}.

Notice that, result transformation, get ABoxes containing additional
concept Dummy, however never queried actions effects rules forming
process. impact transformation simply ADOM(A) ABoxes
669

fiBAGHERI H ARIRI , C ALVANESE , E G IACOMO , E ASELLIS , F ELLI , & ONTALI

KAB transition system readily identified set terms occurring
concept role assertion (without looking equality assertions).
Given ABox A, denote result two transformations, respectively add closure equalities extension Dummy.
3. manipulate resulting effect specification
[q + ] Q

A0

follows:
3.1. replace [q + ] Q [rew (q + )] rew (Q ) (Calvanese et al., 2007a), exploiting
results Calvanese et al. (2007b) Artale et al. (2009), guarantee that,
every ECQ Q every ABox equalities closed functionality
congruence,
ANS (Q, T, A)

= ANS (rew (Q), , A).

3.2. replace effect specification [rew (q + )] rew (Q )
A0 , resulting
+

Step 3.1, set effect specifications [qi ] rew (Q )
A0 , one CQ qi
+
UCQ rew (q ).
3.3. effect specification [qi+ ] rew (Q )
A0 , re-express qi+ make
equalities used join terms explicit remove constants qi+ . Specifically,
replace effect specification
[qi++ ] q = rew (Q )

A0 ,

where:
qi++ CQ without repeated variables obtained qi+ (i) replacing
variable x occurring qi+ , j-th occurrence x except first one, x[j] ;
(ii) replacing constant c new variable xc ;
V
V
q = = [x = x[j] ] [xc = c] (i) first conjunction contains one equality
[x = x[j] ] variable x qi+ variable x[j] introduced step
above, (ii) second conjunction contains one equality constant c
qi+ .
clarify latter consider following example:
Example 4 Given query
.
[qi+ ] = [N (x) P1 (x, y) P2 (c, x)],
Step 3.3 replaces [qi++ ] q = ,
.
qi++ = N (x) P1 (x[2] , y) P2 (xc , x[3] ),
670

.
q = = [x = x[2] ] [x = x[3] ] [xc = c].

fiD ESCRIPTION L OGIC K NOWLEDGE ACTION BASES

correctness
Step 3.3, immediate notice [qi+ ] equivalent [qi++
V
V
[j]
(x = x ) (xc = c)]. equivalence latter [qi++ ]q = consequence
construction Artale et al. (2009), shows query entailment presence
equalities reduced query evaluation saturating equalities w.r.t. transitivity,
reflexivity, symmetry, functionality.
Given action , denote action normalized above.
Since transformations preserve logical equivalence (as long query Dummy),

Lemma 8 DO(T, A, ) T,ALPH(K) DO(T, A, ).
Also normalization KAB preserves weak acyclicity, crucial consideration
later results.
Lemma 9 K weakly acyclic, also K weakly acyclic.
Proof. Consider effect specification [q + ] Q
A0 belonging action K.
contribution effect specification dependency graph G K limited CQ qi
UCQ rew (q + ), set concept role assertions A0 . observe
qi corresponds query qi++ K variable qi occurs exactly once. every free
variable x qi also appears A0 , every occurrence x qi itself, edge included
G. dependency graph G K, one edges appears, corresponding single
occurrence variable x qi++ .
Notice Dummy omitted dependency graph G since, definition K,
Dummy occur left-hand side effects except trivial effect [Dummy(x)]
{Dummy(x)}. true K, Dummy needed. Therefore, G indeed subgraph
G, hence weak acyclicity G implies weak acyclicity G.
7.2 Normalized DO()
Next give simplified version DO(), call NORM (). start observing
reformulate definition DO() given Section 3. that, first need define
suitable notion join two queries. Let q1 q2 two ECQs, may free variables
common, let A1 A2 two ABoxes. define ANS (q1 , , A1 ) ./ ANS (q2 , , A2 )
set substitutions free variables q1 q2 qi holds , Ai , i.e.,
, Ai , |= qi , {1, 2}. Then, given action parameters substitution ABox
A,
[
(T, A, ) =
APPLY (T, A, e, ),
e

effect specification e : [q ++ ] q = Q
APPLY (T, A, e, )

[

=

A0 ,
A0

ANS (q ++ ,,A)./ANS ((q = Q ),,A)

671

EQ(T, A).

fiBAGHERI H ARIRI , C ALVANESE , E G IACOMO , E ASELLIS , F ELLI , & ONTALI

Instead, define NORM ()
NORM (T, A, )

=

[

APPLY NORM (T, A, e, ),

e

where, effect specification e : [q ++ ] q = Q
APPLY NORM (T, A, e, )

A0 ,
[

=

A0

EQ(T, A).

ANS (q ++ ,,A6E Q )./ANS ((q = Q ),,A)

Notice difference DO() NORM () latter use A6E Q
instead compute answers CQs q ++ .
following lemma shows applications DO() NORM () give rise logically
equivalent ABoxes.
Lemma 10 DO(T, A, ) T,ALPH(K) NORM (T, A, ).
Proof. order prove claim, enough show concept/role assertion 2 NORM (T, A, ) whose concept/role name belongs ALPH(K),
(T, DO(T, A, )) |= 2 , concept/role assertion 1 DO(T, A, ) whose concept/role name belongs ALPH(K), (T, NORM (T, A, )) |= 1 . actually prove
slightly stronger result:
(1) ABox assertion 2 APPLY NORM (T, A, e, ), (T, APPLY(T, A, e, )) |=
2 .
(2) ABox assertion 1 APPLY(T, A, e, ), (T, APPLY NORM (T, A, e, )) |=
1 .
(1), monotonicity q ++ fact A6E Q A,
[
[
A0
contained
A0 ,
(ANS (q ++ ,,A6E Q )./ANS ((q = Q ),,A))

(ANS (q ++ ,,A)./ANS ((q = Q ),,A))

hence claim follows.
(2), consider ABox assertion APPLY(T, A, e, ). definition APPLY(), know
exists effect e : [q ++ ] q = Q
A0 assignment free variables
++
=
q
(which include also free variables q Q ) (ANS (q ++ , , A) ./
=
ANS ((q Q ), , A)) A0 . Let {x1 , . . . , xn } free variables q ++ ,
= {x1 t1 , . . . , xn tn , }. variable xi , let N (xi ) (unique) concept atom
q ++ xi occurs (similar considerations hold xi occurs role atom). Then,
either N (ti ) A6E Q , t0i , N (t0i ) A6E Q (ti = t0i ) EQ(T, A). former
case, let t00i denote ti , latter case let t00i denote t0i . Then, consider substitution
0 = {x1 t001 , . . . , xn t00n , }. construction, 0 ANS (q ++ , , A6E Q ), since
ANS ((q = Q ), , A), (t00i = ti ) EQ(T, A) {1, . . . , n}, also
0 ANS (q ++ , , A6E Q ) ./ ANS ((q = Q ), , A). Since
A0 ,
672

fiD ESCRIPTION L OGIC K NOWLEDGE ACTION BASES

0 identical modulo EQ(T, A)
EQ(T, A) APPLY NORM (T, A, e, ),
infer (T, APPLY NORM (T, A, e, )) |= . Hence claim holds.
combining Lemma 8 Lemma 10, get DO() K NORM () K behave
equivalently, starting equivalent ABoxes.
Lemma 11 A1 T,ALPH(K) A2 DO(T, A1 , ) T,ALPH(K) NORM (T, A2 , ).
Proof. claim direct consequence Lemma 8, Lemma 10, equivalence A1
A2 , observation logical equivalence transitive.
Given KAB K normalized version K, call transition system generated
way K , using NORM () K instead DO() K, normalized transition system
generated K, denote K,NORM .
Lemma 12 Given KAB K, transition systems K K,NORM bisimilar.
Proof. Let K = (U, T, , s0 , abox , ) K,NORM = (U, T, NORM , s0 , abox NORM , NORM ).
define relation B NORM follows: (s1 , s2 ) B iff abox (s1 ) T,ALPH(K)
abox NORM (s2 ) show B bisimulation. so, prove B closed
definition bisimulation itself. Indeed, (s1 , s2 ) B, then:
abox (s1 ) T,ALPH(K) abox (s2 ) definition.
s1 s01 exists action substitution s01 =
(T, abox (s1 ), ) (notice abox (s1 ) = s1 ) s01 consistent w.r.t. . let
us consider s02 = NORM (T, abox (s2 ), ). Since abox (s1 ) T,ALPH(K) abox (s2 ),
Lemma 11, s01 T,ALPH(K) s02 . Therefore, s02 consistent w.r.t. , hence
s2 NORM s02 , (s01 , s02 ) B.
Similarly, s2 NORM s02 exists action substitution
s02 = NORM (T, abox (s2 ), ) s02 consistent w.r.t. . let us consider s01 =
(T, abox (s1 ), ). Since s2 T,ALPH(K) s1 , Lemma 11, s02 T,ALPH(K)
s01 Therefore, s01 consistent w.r.t. , hence s1 s01 , and, considering equivalence
enjoys symmetry, (s01 , s02 ) B.
proves claim.
direct consequence lemma that, considering Bismulation Invariance
Theorem 4, faithfully check LA formulas K,NORM instead K .
7.3 Positive Dominant
next step show weakly acyclic KAB K, normalized transition system K,NORM
finite. considering another transition system, behaviorally unrelated
K,NORM , hence K , whose active domain bounds active domain K,NORM .
obtain transition system essentially ignoring negative information equalities.
allows us refer back literature data exchange show boundedness. call
transition system positive dominant.
Given normalized KAB K = (T, A0 , , ), define positive dominant K KAB
6E Q

K+ = (, A0 , { + }, {true 7 + }).
673

fiBAGHERI H ARIRI , C ALVANESE , E G IACOMO , E ASELLIS , F ELLI , & ONTALI

action + without parameters effect specification constituted CopyAll
one effect form
6E Q
[qi++ ]
A0i
effect [qi++ ] qi= Q
A0i every action . Observe parameters

actions become simply free variables + .
Notice + applicable every step process trivially always allows it.
resulting state always consistent, since K+ empty TBox. Moreover, equality assertion
ever generated. transition system K+ constituted single run, incrementally
accumulates facts derived iterated application + increasing
ABox. behavior closely resembles chase tuple-generating dependencies (TGDs) data
exchange, application + corresponds parallel chase step (Deutsch, Nash, &
Remmel, 2008).
technical point view, notice K+ already normalized form (i.e., K+ = K+ ),
DO() NORM () identical since neither equality negation considered. Hence
K+ = K+ ,NORM .
next lemma shows K+ preserves weak acyclicity K.
Lemma 13 K weakly acyclic also positive dominant K+ weakly acyclic.
Proof. claim follows fact that, construction, dependency graph G + K+
equal G. Indeed, qi++ connection Ai preserved K+ . Hence, get
claim.
Next show K+ weakly acyclic active domain ABoxes transition
system K+ polynomially bounded active domain initial ABox.
Lemma 14 K+ weakly acyclic, exists polynomial P()
6E Q

|ADOM(K+ )| < P(|ADOM(A0 )|).
Proof. observe exists strict connection execution K+ chase
set TGDs data exchange. Therefore, proof closely resembles one Fagin et al.
(2005, Thm. 3.9), shown weakly acyclic TGDs, every chase sequence bounded.
Let K+ = (U, , , A0 6E Q , abox , ), let G + = (V, E) dependency graph K+ ,
let n = |ADOM(A0 6E Q )|. every node p V , consider incoming path (finite
infinite) path ending p. Let rank (p) maximum number special edges
incoming path. Since K+ weakly acyclic hypothesis, G + contain cycles going
special edges, therefore rank (p) finite. Let r maximum among rank (pi )
nodes. observe r |V |; indeed path lead node twice using special
edges, otherwise G + would contain cycle going special edges, thus breaking weak
acyclicity hypothesis. Next observe partition nodes V according rank,
obtaining set sets {V0 , V1 , . . . , Vr }, Vi set nodes rank i.
Let us consider state obtained A0 6E Q applying action + contained
K+ arbitrary number times. prove, induction i, following claim:
every exists polynomial Pi total number distinct values c occur
positions Vi Pi (n).
674

fiD ESCRIPTION L OGIC K NOWLEDGE ACTION BASES

(Base case) Consider p V0 . definition, p incoming path containing special edges.
Therefore, new values stored p along run A0 6E Q A. Indeed p store
values part initial ABox A0 6E Q . holds nodes V0 hence fix
P0 (n) = n.
(Inductive step) Consider p Vi , {1, . . . , r}. first kind values may stored
inside p values stored inside p A0 6E Q . number values
n. addition, value may stored p two reasons: either copied
position p0 Vj 6= j, generated possibly new function term, built applying
effects contain function head.
first determine number fresh individuals generated function terms.
possibility generating storing new value p result action reflected
presence special edges. definition, special edge entering p must start node
0
p0 V0 Vi1 .
P induction hypothesis, number distinct values exist p
bounded H(n) = j{0,...,i1} Pj (n). Let ba maximum number special edges
enter position, positions TBox; ba bounds arity taken function term
contained . every choice ba values V0 Vi1 (one special edge
enter position), number new values generated position p bounded tf H(n)ba ,
tf total number facts contained effects + . Note number
depend data A0 6E Q . considering positions Vi , total number values
generated bounded F(n) = |Vi | tf H(n)ba . Clearly, F() polynomial,
tf ba determined + .
count next number distinct values copied positions Vi positions
Vj , j 6= i. copy represented graph normal edge going node Vj
node Vi , j 6= i. observe first normal edges start nodes
V0 Vi1 , is, cannot start nodes Vj j > i. prove contradiction.
Assume exists p0 p E, p Vi p0 Vj j > i. case,
rank p would j > i, contradicts fact p Vi . consequence, number
distinct values copied positions Vi bounded total number values
V0 Vi1 , corresponds H(n) previous consideration.
Putting together, define Pi (n) = n + F(n) + H(n). Since Pi () polynomial,
claim proven.
Notice that, claim, bounded r, constant. Hence, exists
fixed polynomial P() number distinct values exist every state
bounded P(n). K+ inflationary, + applied copies concept role
assertions current next state. Since K+ contains single run, P(n) bound
ADOM(K+ ) well.
following lemma shows key feature positive dominant.
Lemma 15 ADOM(K ) ADOM(K+ ).
6E Q

Proof. Let K = (T, A0 , , ) K+ = (, A0 , { + }, {true 7 + }).
first observe that, every ABox K , ADOM(A) = ADOM(A6E Q ) definition K
(this role special concept Dummy).
675

fiBAGHERI H ARIRI , C ALVANESE , E G IACOMO , E ASELLIS , F ELLI , & ONTALI

show induction construction K (U, T, 1 , A0 , abox , 1 ) K+ =
6E Q

(U, , 2 , A0 , abox , 2 ), state A1 1 exists state A2 2
A1 6E Q A2 .
6E Q
base case holds initial states A0 A0 two transition systems definition.
inductive case, show that, given A1 1 A2 2 A1 6E Q A2 ,
A01 1 A1 1 A01 , unique state A02 2 A2 2 A02 A01 A02 .
show this, note A1 1 A01 exists action K substitution parameters
A01 = NORM (T, A1 , ). Similarly, taking account + parameters
always executable K+ , A02 = DO(T, A2 , + ) = NORM (T, A2 , + ).
construction K+ , effect e1 form
e1 : [q ++ ] q = Q

A0e1 ,

effect e2 + form
e2 : [q ++ ]

6E Q

A0e1 ,

A0e1 6E Q obtained A0e1 removing equality assertions. induction hypothesis,
A1 6E Q A2 . observing ANS ([q ++ ], , A1 6E Q ) ./ ANS ((q = Q ), , A1 )
6E Q
ANS ([q ++ ], , A2 ), obtain A0e1
A0e2 , A0e1 = APPLY NORM (T, A1 , e1 , )
A0e2 = APPLY(, A2 , e2 , ). Hence, get claim A01 6E Q A02 .
since ABox K active domain ADOM(A) ADOM(A6E Q ) identical construction, since ADOM(K ) ADOM(K+ ) simply union active
domains generated ABoxes, get claim.
7.4 Putting Together
KAB K weakly acyclic, then, Lemma 9, normalized form K weakly acyclic well
and, Lemma 13, positive dominant K+ . Hence, Lemma 14, size active
domain ADOM(K+ ) transition system K+ K+ polynomially related size
initial ABox.
Now, Lemma15, implies also size active domain ADOM(K,NORM )
transition system K K polynomially related size initial ABox. Hence, number
possible states K finite, fact exponential size initial ABox.
follows checking LA formulae K done E XP IME w.r.t. size K.
Finally, Lemma 12, K K bisimilar, Bisimulation Invariance Theorem 4,
K K satisfy exactly LA formulae. Hence, check LA formula K
sufficient check K , done E XP IME. concludes proof
Theorem 7.

8. Related Work
provide detailed review work related framework results presented
previous sections.
676

fiD ESCRIPTION L OGIC K NOWLEDGE ACTION BASES

8.1 Combining Description Logics Temporal Logics
work deeply related research studies combinations description logics temporal logics. Indeed, actions progress knowledge time and, although temporal logics
mention actions, easily used describing progression mechanisms, including transition systems (see, e.g., Clarke et al., 1999; Calvanese, De Giacomo, & Vardi, 2002).
research mostly explored combination standard description logics standard
temporal logics level models, certainly natural form combination
logical point view. Technically, form combination gives rise combined logic
two-dimensional semantics, one dimension time DL domain
(Schild, 1993; Wolter & Zakharyaschev, 1999b, 1999a; Gabbay et al., 2003). Unfortunately,
computational point view, form combination suffers key undecidability result,
makes fragile many practical purposes: possibility specifying roles
preserve extension time (the called rigid roles) causes undecidability6 . Referring
domain interest Example 1, would result, example, undecidability
theories specify instance Character livesIn City forever. Moreover,
undecidability result already holds concept satisfiability w.r.t. fixed TBox (i.e.,
TBox axioms must hold time points), without ABoxes, single rigid role (Wolter
& Zakharyaschev, 1999b, 1999a; Gabbay et al., 2003). is, holds reasoning service
much simpler conjunctive query answering (Calvanese, De Giacomo, & Lenzerini, 2008),
even fixed TBox data (no ABox assertions, hence individual terms) one
simplest kinds temporal formulae, namely forever something true (safety) (Clarke et al.,
1999).
Decidability regained by: (i) dropping TBoxes altogether, decision problem
still hard non-elementary time (Gabbay et al., 2003); (ii) allowing temporal operators
concepts (Schild, 1993; Artale & Franconi, 1998, 2005; Gutierrez-Basulto, Jung, & Lutz, 2012;
Jamroga, 2012), case complexity depends crucially description logic; (iii) allowing temporal operators TBox ABox assertions (Lutz, Wolter, & Zakharyaschev,
2008; Baader et al., 2012). fact cases (ii) (iii) mixed (Baader & Laux, 1995; Wolter
& Zakharyaschev, 1998).
Allowing temporal operators assertions (case (iii) above), tightly related
functional approach adopted paper: fact admit temporal operators front
assertions allows us consider temporal models whose time points actually sets models
description logic assertions. Hence keeps temporal component distinct description
logic one, exactly here. particular, results Baader et al. (2012) directly
compared ours. Apart obvious differences formalism used, one key point get
decidability individual terms mentioned ABox assertions fixed priori.
possible that, adapting techniques presented here, results could extended allow
functions denoting terms, hence allowing adding fresh individual terms temporal
evolution.

6. lose decidability, suffices able specify/verify persistence binary predicates/roles, allows
one build infinite grid hence encode Turing-machine computation (Robinson, 1971; van Emde Boas,
1997).

677

fiBAGHERI H ARIRI , C ALVANESE , E G IACOMO , E ASELLIS , F ELLI , & ONTALI

8.2 Combining Description Logics Actions
Somehow hampered undecidability results mentioned beginning section, also
combinations description logics action theories studied years. particular,
Liu, Lutz, Milicic, Wolter (2006b, 2006a) study combinations description logics action
theories level models, w.r.t. two classical problems reasoning actions,
namely projection executability. problems require explicitly give sequence
actions check property resulting final state (projection), check executability
sequence actions, comes certain precondition (Reiter, 2001).
sophisticated temporal properties (in particular, forever something true mentioned above) would
lead undecidability. way, notice undecidability result also deeply questions
computational point view possibility adding (sound complete) automated reasoning
capabilities proposals OWL-S (Semantic Markup Web Services) (Martin, Paolucci,
McIlraith, Burstein, McDermott, McGuinness, Parsia, Payne, Sabou, Solanki, Srinivasan, & Sycara,
2004).
Possibly first proposal based implicitly functional view KB pioneering
work De Giacomo, Iocchi, Nardi, Rosati (1999), adopts epistemic description logic
(based certain answers) combined action formalism describe routines mobile
robot. Again, one important point individual terms bounded fixed priori.
functional view approach first spelled Calvanese, De Giacomo, Lenzerini,
Rosati (2007), Calvanese et al. (2011). work, projection executability
studied, however distinction KB states actions (there specified
updates), framework gives rise single transition system whose states labeled
KBs (in fact TBox fixed ABox changes state state). However, again,
individual terms considered fixed priori hence resulting transition system finite.
So, although studied work, sophisticated forms temporal properties proposed
readily verifiable setting. Interestingly, apart KBs action, work
also Golog-like programs considered. programs whose atomic actions defined
action formalism, combined using (usual less usual) programming constructs,
sequence, while-loop, if-then-else, nondeterministic pick value (Levesque, Reiter,
Lesperance, Lin, & Scherl, 1997; De Giacomo, Lesperance, & Levesque, 2000). important
characteristic programs finite number control states (notice
memory storage programs kept action theory, KB case). Although
scope paper, finiteness allows easily extending results program
well.
interesting alternative way combine description logics reasoning actions
one reported Gu Soutchanski (2010). There, description logics KB7 used special
FOL theory describing initial situation situation calculus basic action theory (Reiter, 2001).
Notice result, TBox assertions act state constraints (Lin & Reiter, 1994),
would lead undecidability discussed (Wolter & Zakharyaschev, 1999b, 1999a; Gabbay
et al., 2003), fact essentially persist way actions.

7. actually mainly focus concepts description logic includes universal role, allows
one express TBox assertions concepts (Baader et al., 2003).

678

fiD ESCRIPTION L OGIC K NOWLEDGE ACTION BASES

8.3 Description Logics Update
Observe effects action setting seen basic form update previous
state (Katsuno & Mendelzon, 1991). Although mechanism sidesteps semantic computational difficulties description logic KB update (Liu et al., 2006b; De Giacomo, Lenzerini, Poggi,
& Rosati, 2009; Calvanese, Kharlamov, Nutt, & Zheleznyakov, 2010; Lenzerini & Savo, 2012)
simply rejecting execution actions would lead inconsistent state. Adopting proper
forms update setting interesting issue future research.
8.4 Artifacts Data-Aware Processes
work also closely related research verification artifact-centric business processes
(Nigam & Caswell, 2003; Bhattacharya et al., 2007). Artifact-centric approaches model business
processes giving equal importance control-flow perspective data interest.
artifact typically represented tuple schema, models artifact type, together
set actions/services specify information maintained artifact
manipulated time. action usually represented terms pre- post-conditions
respectively used determine action eligible execution, relate
current artifact state successor state obtained action execution. Pre- postconditions modeled first-order formulae, post-conditions employ existentially quantified
variables account external inputs environment. Differently KABs,
approaches targeting artifact-centric processes assume complete information data, using
relational database maintain artifacts information. paper, aim works
verify whether relational artifact-centric process meets temporal/dynamic property,
formalized using first-order variants branching linear temporal logics.
work Deutsch et al. (2009), infinite domain artifacts database equipped
dense linear order, mentioned pre-conditions, post-conditions, properties.
Runs receive unbounded external input infinite domain. Decidability verification
achieved avoiding branching time properties, restricting formulae used specify
pre-, post-conditions properties. particular, approach refers read-only read-write
database relations differently, querying latter checking whether contain given tuple constants. authors show restriction tight, integrity constraints cannot
added framework, since even single functional dependency leads undecidability
verification. Damaggio et al. (2011) extend approach disallowing read-write relations,
allows extension decidability result integrity constraints expressed embedded
dependencies terminating chase, decidable arithmetic. major difference
approach, concepts KAB considered read-write relations,
arbitrarily queried determine progression system. Differently works,
Belardinelli et al. (2011) consider first-order variant CTL quantification across states
verification formalism. framework supports incorporation new values external environment parameters actions; corresponding execution semantics considers
possible actual values, thus leading infinite-state transition systems. decidability verification, authors show that, assumption state system
(constituted union artifacts relational instances) bounded active domain, possible construct faithful abstract transition system which, differently original one,
finite number states. Belardinelli, Lomuscio, Patrizi (2012) improve results Belar679

fiBAGHERI H ARIRI , C ALVANESE , E G IACOMO , E ASELLIS , F ELLI , & ONTALI

dinelli et al. (2011) introducing semantic property uniformity which, roughly speaking,
says transition system representing execution process study able
distinguish among states constants patterns data. assumptions uniformity state boundedness, decidability verification achieved richer
logic, namely CTL quantification across states, interpreted active domain semantics.
notion state boundedness also adopted independently developed framework
Bagheri Hariri, Calvanese, De Giacomo, Deutsch, Montali (2012, 2013), first-order
variants -calculus, similar one considered here, considered. There, beside differences
way data external information modeled, sufficient syntactic conditions guarantee state boundedness proposed. works developed within relational database
setting, extend trivially systems actions change DL knowledge bases.
connection data-/artifact-centric business processes data exchange exploit paper first established Cangialosi et al. (2010), De Giacomo, De Masellis,
Rosati (2012). transition relation described terms TGDs, map
current state, represented relational database instance, next one. Null values used
model incorporation new, unknown data system. process evolution essentially
form chase. suitable weak acyclicity conditions chase terminates, guaranteeing,
turn, system finite-state. Decidability shown first-order -calculus without
first-order quantification across states. approach extended Bagheri Hariri et al. (2011),
TGDs replaced actions rule-based process follow structure
KAB action component. revised framework, values imported external environment represented uninterpreted function terms, play role nulls
work Cangialosi et al. (2010), De Giacomo et al. (2012). Since Bagheri Hariri et al.
(2011), Cangialosi et al. (2010), De Giacomo et al. (2012) rely purely relational setting, choice leads ad-hoc interpretation equality, null value/function term
considered equal itself. Differently works, allow sophisticated
schema constraints, i.e., TBox itself, provide time fine-grained treatment
equality, individuals inferred equal due application schema
constraints and/or execution action. treatment equality differentiates work
also one Bagheri Hariri, Calvanese, De Giacomo, De Masellis (2011), introduces preliminary version framework presented, UNA assumed equality
considered. specifically, Bagheri Hariri et al. (2011) propose semantic artifacts
means represent artifacts corresponding processes higher level abstraction relational artifacts, representing artifact data semantically rich knowledge base operating
incomplete information. KABs constitute general framework, seamlessly
customized account semantic artifacts. major difference work Bagheri Hariri
et al. (2011) also constituted verification formalism. particular, works focus
form -calculus ECQs used query states system, Bagheri Hariri et al.
(2011) support quantification across states, done here.
Calvanese et al. (2012) investigate framework data-centric processes mixes approach proposed Bagheri Hariri et al. (2013) relational artifacts notion knowledge
bases used here. particular, semantically-governed data-aware processes introduced
mechanism model dynamic system working relational database, providing
time conceptual representation manipulated data terms DL-Lite knowledge base.
relying ontology-based data access (Calvanese et al., 2009), declarative mappings used
680

fiD ESCRIPTION L OGIC K NOWLEDGE ACTION BASES

connect knowledge base underlying relational database. Differently KABs,
system evolves relational layer, knowledge base used understand ultimately
govern execution higher level abstraction.
observe results presented fully subsume Bagheri Hariri et al. (2011),
underlying description logic OWL 2 QL. one hand, remove possibility
asserting functionality roles knowledge component, equating individuals result action action component, precisely obtain setting presented Bagheri Hariri
et al. (2011). hand, frameworks established complexity upper bounds
same.

9. Conclusions
paper studied verification knowledge action bases, dynamic systems
constituted knowledge base, expressed description logics, action specification
changes knowledge base time. obtained interesting decidability result
relying notion weak acyclicity, based connection theory chase TGDs
relational databases.
work, used original notion weak acyclicity. However, easy adopt
advanced forms acyclicity, since results depend ability finding finite
bound number distinct function terms generated (when applying chase).
majority approaches adopt forms weak-acyclicity focus databases (Marnette &
Geerts, 2010; Meier, Schmidt, Wei, & Lausen, 2010), Cuenca Grau, Horrocks, Krotzsch, Kupke,
Magka, Motik, Wang (2012) investigate sophisticated forms acyclicity context
knowledge bases without UNA. results thus seamlessly applied KABs. Interestingly,
manage impact equalities setting without UNA, resort singularization technique presented Marnette (2009), closely resembles normalization KABs introduced
Section 7.
Weak acyclicity allows us gain decidability bounding number distinct function
terms occur transition system. alternative approach gain decidability bound
number distinct terms occurring ABox assertions state. Variants notion state
boundedness proposed recently contexts (Belardinelli et al., 2012; De Giacomo,
Lesperance, & Patrizi, 2012; Bagheri Hariri et al., 2013). great interest explore
approach setting presented actions acting description logic knowledge base.
observe decidability result (as well ones commented Section 8),
comes algorithm verification exponential size initial ABox. precludes direct application techniques large-scale systems, without careful analysis
modularized small units verified (almost) separately. important
direction investigation.
Acknowledgments
research partially supported EU ICT Collaborative Project ACSI
(Artifact-Centric Service Interoperation), grant agreement n. FP7-257593, large-scale
integrating project (IP) Optique (Scalable End-user Access Big Data), grant agreement n. FP7318338.
681

fiBAGHERI H ARIRI , C ALVANESE , E G IACOMO , E ASELLIS , F ELLI , & ONTALI

References
Artale, A., Calvanese, D., Kontchakov, R., & Zakharyaschev, M. (2009). DL-Lite family
relations. J. Artificial Intelligence Research, 36, 169.
Artale, A., & Franconi, E. (1998). temporal description logic reasoning actions
plans. J. Artificial Intelligence Research, 9, 463506.
Artale, A., & Franconi, E. (2005). Temporal description logics. Gabbay, D., Fisher, M., & Vila, L.
(Eds.), Handbook Temporal Reasoning Artificial Intelligence, Foundations Artificial
Intelligence. Elsevier.
Baader, F., Calvanese, D., McGuinness, D., Nardi, D., & Patel-Schneider, P. F. (Eds.). (2003).
Description Logic Handbook: Theory, Implementation Applications. Cambridge University Press.
Baader, F., Ghilardi, S., & Lutz, C. (2012). LTL description logic axioms. ACM Trans.
Computational Logic, 13(3), 21:121:32.
Baader, F., & Laux, A. (1995). Terminological logics modal operators. Proc. 14th
Int. Joint Conf. Artificial Intelligence (IJCAI95), pp. 808814.
Bagheri Hariri, B., Calvanese, D., De Giacomo, G., & De Masellis, R. (2011). Verification
conjunctive-query based semantic artifacts. Proc. 24th Int. Workshop Description Logic (DL 2011), Vol. 745 CEUR Electronic Workshop Proceedings, http:
//ceur-ws.org/.
Bagheri Hariri, B., Calvanese, D., De Giacomo, G., De Masellis, R., & Felli, P. (2011). Foundations
relational artifacts verification. Proc. 9th Int. Conference Business Process
Management (BPM 2011), Vol. 6896 Lecture Notes Computer Science, pp. 379395.
Springer.
Bagheri Hariri, B., Calvanese, D., De Giacomo, G., Deutsch, A., & Montali, M. (2012). Verification relational data-centric dynamic systems external services. Corr technical report arXiv:1203.0024, arXiv.org e-Print archive. Available http://arxiv.org/abs/
1203.0024.
Bagheri Hariri, B., Calvanese, D., De Giacomo, G., Deutsch, A., & Montali, M. (2013). Verification
relational data-centric dynamic systems external services. Proc. 32nd ACM
SIGACT SIGMOD SIGART Symp. Principles Database Systems (PODS 2013).
Bao, J., et al. (2012). OWL 2 Web Ontology Language document overview (second edition). W3C
Recommendation, World Wide Web Consortium. Available http://www.w3.org/
TR/owl2-overview/.
Belardinelli, F., Lomuscio, A., & Patrizi, F. (2011). Verification deployed artifact systems via data
abstraction. Proc. 9th Int. Joint Conf. Service Oriented Computing (ICSOC 2011),
Vol. 7084 Lecture Notes Computer Science, pp. 142156. Springer.
Belardinelli, F., Lomuscio, A., & Patrizi, F. (2012). abstraction technique verification
artifact-centric systems. Proc. 13th Int. Conf. Principles Knowledge
Representation Reasoning (KR 2012), pp. 319328.
Berardi, D., Calvanese, D., & De Giacomo, G. (2005). Reasoning UML class diagrams. Artificial
Intelligence, 168(12), 70118.
682

fiD ESCRIPTION L OGIC K NOWLEDGE ACTION BASES

Bhattacharya, K., Gerede, C., Hull, R., Liu, R., & Su, J. (2007). Towards formal analysis artifactcentric business process models. Proc. 5th Int. Conference Business Process
Management (BPM 2007), Vol. 4714 Lecture Notes Computer Science, pp. 288234.
Springer.
Burkart, O., Caucal, D., Moller, F., & Steffen, B. (2001). Verification infinite structures..
Handbook Process Algebra. Elsevier Science.
Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., Poggi, A., Rodrguez-Muro, M., &
Rosati, R. (2009). Ontologies databases: DL-Lite approach. Tessaris, S., & Franconi, E. (Eds.), Reasoning Web. Semantic Technologies Informations Systems 5th Int.
Summer School Tutorial Lectures (RW 2009), Vol. 5689 Lecture Notes Computer Science, pp. 255356. Springer.
Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., & Rosati, R. (2007a). EQL-Lite: Effective first-order query processing description logics. Proc. 20th Int. Joint Conf.
Artificial Intelligence (IJCAI 2007), pp. 274279.
Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., & Rosati, R. (2007b). Tractable reasoning efficient query answering description logics: DL-Lite family. J. Automated
Reasoning, 39(3), 385429.
Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., & Rosati, R. (2013). Data complexity
query answering description logics. Artificial Intelligence, 195, 335360.
Calvanese, D., De Giacomo, G., Lembo, D., Montali, M., & Santoso, A. (2012). Ontology-based
governance data-aware processes. Proc. 6th Int. Conf. Web Reasoning Rule
Systems (RR 2012), Vol. 7497 Lecture Notes Computer Science, pp. 2541. Springer.
Calvanese, D., De Giacomo, G., & Lenzerini, M. (2008). Conjunctive query containment answering description logics constraints. ACM Trans. Computational Logic, 9(3),
22.122.31.
Calvanese, D., De Giacomo, G., Lenzerini, M., & Rosati, R. (2007). Actions programs description logic ontologies. Proc. 20th Int. Workshop Description Logic (DL 2007),
Vol. 250 CEUR Electronic Workshop Proceedings, http://ceur-ws.org/, pp. 29
40.
Calvanese, D., De Giacomo, G., Lenzerini, M., & Rosati, R. (2011). Actions programs
description logic knowledge bases: functional approach. Lakemeyer, G., & McIlraith,
S. A. (Eds.), Knowing, Reasoning, Acting: Essays Honour Hector Levesque. College
Publications.
Calvanese, D., De Giacomo, G., & Montali, M. (2013). Foundations data aware process analysis:
database theory perspective. Proc. 32nd ACM SIGACT SIGMOD SIGART Symp.
Principles Database Systems (PODS 2013).
Calvanese, D., De Giacomo, G., & Vardi, M. Y. (2002). Reasoning actions planning
LTL action theories. Proc. 8th Int. Conf. Principles Knowledge Representation Reasoning (KR 2002), pp. 593602.
Calvanese, D., Kharlamov, E., Nutt, W., & Zheleznyakov, D. (2010). Updating ABoxes DL-Lite.
Proc. 4th Alberto Mendelzon Int. Workshop Foundations Data Management
683

fiBAGHERI H ARIRI , C ALVANESE , E G IACOMO , E ASELLIS , F ELLI , & ONTALI

(AMW 2010), Vol. 619 CEUR Electronic Workshop Proceedings, http://ceur-ws.
org/, pp. 3.13.12.
Cangialosi, P., De Giacomo, G., De Masellis, R., & Rosati, R. (2010). Conjunctive artifact-centric
services. Proc. 8th Int. Joint Conf. Service Oriented Computing (ICSOC 2010),
Vol. 6470 Lecture Notes Computer Science, pp. 318333. Springer.
Clarke, E. M., Grumberg, O., & Peled, D. A. (1999). Model checking. MIT Press, Cambridge,
MA, USA.
Cohn, D., & Hull, R. (2009). Business artifacts: data-centric approach modeling business
operations processes. Bull. IEEE Computer Society Technical Committee Data
Engineering, 32(3), 39.
Cuenca Grau, B., Horrocks, I., Krotzsch, M., Kupke, C., Magka, D., Motik, B., & Wang, Z. (2012).
Acyclicity conditions application query answering description logics.
Proc. 13th Int. Conf. Principles Knowledge Representation Reasoning
(KR 2012), pp. 243253.
Damaggio, E., Deutsch, A., & Vianu, V. (2011). Artifact systems data dependencies
arithmetic. Proc. 14th Int. Conf. Database Theory (ICDT 2011), pp. 6677.
De Giacomo, G., De Masellis, R., & Rosati, R. (2012). Verification conjunctive artifact-centric
services. Int. J. Cooperative Information Systems, 21(2), 111139.
De Giacomo, G., Iocchi, L., Nardi, D., & Rosati, R. (1999). theory implementation
cognitive mobile robots. J. Logic Computation, 9(5), 759785.
De Giacomo, G., Lenzerini, M., Poggi, A., & Rosati, R. (2009). instance-level update erasure description logic ontologies. J. Logic Computation, Special Issue Ontology
Dynamics, 19(5), 745770.
De Giacomo, G., Lesperance, Y., & Levesque, H. J. (2000). ConGolog, concurrent programming
language based situation calculus. Artificial Intelligence, 121(12), 109169.
De Giacomo, G., Lesperance, Y., & Patrizi, F. (2012). Bounded situation calculus action theories
decidable verification. Proc. 13th Int. Conf. Principles Knowledge
Representation Reasoning (KR 2012), pp. 467477.
Deutsch, A., Hull, R., Patrizi, F., & Vianu, V. (2009). Automatic verification data-centric business
processes. Proc. 12th Int. Conf. Database Theory (ICDT 2009), pp. 252267.
Deutsch, A., Nash, A., & Remmel, J. B. (2008). chase revisited. Proc. 27th ACM
SIGACT SIGMOD SIGART Symp. Principles Database Systems (PODS 2008), pp. 149
158.
Fagin, R., Kolaitis, P. G., Miller, R. J., & Popa, L. (2005). Data exchange: Semantics query
answering. Theoretical Computer Science, 336(1), 89124.
Gabbay, D., Kurusz, A., Wolter, F., & Zakharyaschev, M. (2003). Many-dimensional Modal Logics:
Theory Applications. Elsevier Science Publishers.
Gu, Y., & Soutchanski, M. (2010). description logic based situation calculus. Ann. Mathematics
Artificial Intelligence, 58(1-2), 383.
684

fiD ESCRIPTION L OGIC K NOWLEDGE ACTION BASES

Gutierrez-Basulto, V., Jung, J. C., & Lutz, C. (2012). Complexity branching temporal description
logics. Proc. 20th Eur. Conf. Artificial Intelligence (ECAI 2012), pp. 390395.
Jamroga, W. (2012). Concepts, agents, coalitions alternating time. Proc. 20th Eur.
Conf. Artificial Intelligence (ECAI 2012), pp. 438443.
Katsuno, H., & Mendelzon, A. (1991). difference updating knowledge base
revising it. Proc. 2nd Int. Conf. Principles Knowledge Representation
Reasoning (KR91), pp. 387394.
Kowalski, R. A., & Sadri, F. (2011). Abductive logic programming agents destructive
databases. Ann. Mathematics Artificial Intelligence, 62(12), 129158.
Lenzerini, M., & Savo, D. F. (2012). Updating inconsistent description logic knowledge bases.
Proc. 20th Eur. Conf. Artificial Intelligence (ECAI 2012), pp. 516521.
Levesque, H. J., Reiter, R., Lesperance, Y., Lin, F., & Scherl, R. (1997). GOLOG: logic programming language dynamic domains. J. Logic Programming, 31, 5984.
Levesque, H. J. (1984). Foundations functional approach knowledge representation. Artificial
Intelligence, 23, 155212.
Limonad, L., De Leenheer, P., Linehan, M., Hull, R., & Vaculin, R. (2012). Ontology dynamic
entities. Proc. 31st Int. Conf. Conceptual Modeling (ER 2012).
Lin, F., & Reiter, R. (1994). State constraints revisited. J. Logic Programming, 4(5), 655678.
Liu, H., Lutz, C., Milicic, M., & Wolter, F. (2006a). Reasoning actions using description
logics general TBoxes. Proc. 10th Eur. Conference Logics Artificial
Intelligence (JELIA 2006), Vol. 4160 Lecture Notes Computer Science. Springer.
Liu, H., Lutz, C., Milicic, M., & Wolter, F. (2006b). Updating description logic ABoxes. Proc.
10th Int. Conf. Principles Knowledge Representation Reasoning (KR 2006),
pp. 4656.
Lutz, C., Wolter, F., & Zakharyaschev, M. (2008). Temporal description logics: survey. Proc.
15th Int. Symp. Temporal Representation Reasoning (TIME 2008), pp. 314.
Marnette, B. (2009). Generalized schema-mappings: termination tractability. Proc.
28th ACM SIGACT SIGMOD SIGART Symp. Principles Database Systems
(PODS 2009), pp. 1322.
Marnette, B., & Geerts, F. (2010). Static analysis schema-mappings ensuring oblivious termination. Proc. 13th Int. Conf. Database Theory (ICDT 2010), pp. 183195.
Martin, D., Paolucci, M., McIlraith, S., Burstein, M., McDermott, D., McGuinness, D., Parsia, B.,
Payne, T., Sabou, M., Solanki, Srinivasan, N., & Sycara, K. (2004). Bringing semantics
web services: OWL-S approach. Proc. 1st Int. Workshop Semantic Web
Services Web Process Composition (SWSWPC 2004).
Meier, M., Schmidt, M., Wei, F., & Lausen, G. (2010). Semantic query optimization presence
types. 111-122 (Ed.), Proc. 29th ACM SIGACT SIGMOD SIGART Symp.
Principles Database Systems (PODS 2010).
Meyer, A., Smirnov, S., & Weske, M. (2011). Data business processes. EMISA Forum, 31(3),
531.
685

fiBAGHERI H ARIRI , C ALVANESE , E G IACOMO , E ASELLIS , F ELLI , & ONTALI

Milner, R. (1971). algebraic definition simulation programs. Proc. 2nd Int.
Joint Conf. Artificial Intelligence (IJCAI71), pp. 481489.
Motik, B., Cuenca Grau, B., Horrocks, I., Wu, Z., Fokoue, A., & Lutz, C. (2012). OWL 2 Web Ontology Language profiles (second edition). W3C Recommendation, World Wide Web Consortium. Available http://www.w3.org/TR/owl2-profiles/.
Nigam, A., & Caswell, N. S. (2003). Business artifacts: approach operational specification.
IBM Systems Journal, 42(3), 428445.
Park, D. M. R. (1976). Finiteness Mu-ineffable. Theoretical Computer Science, 3(2), 173181.
Poggi, A., Lembo, D., Calvanese, D., De Giacomo, G., Lenzerini, M., & Rosati, R. (2008). Linking
data ontologies. J. Data Semantics, X, 133173.
Reiter, R. (2001). Knowledge Action: Logical Foundations Specifying Implementing
Dynamical Systems. MIT Press.
Robinson, R. (1971). Undecidability nonperiodicity tilings plane. Inventiones Math.,
12, 177209.
Rosati, R., & Franconi, E. (2012). Generalized ontology-based production systems. Proc.
13th Int. Conf. Principles Knowledge Representation Reasoning (KR 2012), pp.
435445. AAAI Press.
Schild, K. (1993). Combining terminological logics tense logic. Proc. 6th Portuguese
Conf. Artificial Intelligence (EPIA93), Vol. 727 Lecture Notes Computer Science,
pp. 105120. Springer.
Stirling, C. (2001). Modal Temporal Properties Processes. Springer.
van Ditmarsch, H., van der Hoek, W., & Kooi, B. (2007). Dynamic epistemic logic. Springer.
van Emde Boas, P. (1997). convenience tilings. Sorbi, A. (Ed.), Complexity, Logic,
Recursion Theory, Vol. 187 Lecture Notes Pure Applied Mathematics, pp. 331363.
Marcel Dekker Inc.
Vianu, V. (2009). Automatic verification database-driven systems: new frontier. Proc.
12th Int. Conf. Database Theory (ICDT 2009), pp. 113.
Wolter, F., & Zakharyaschev, M. (1998). Satisfiability problem description logics modal
operators. Proc. 6th Int. Conf. Principles Knowledge Representation
Reasoning (KR98), pp. 512523.
Wolter, F., & Zakharyaschev, M. (1999a). Modal description logics: Modalizing roles. Fundamenta
Informaticae, 39(4), 411438.
Wolter, F., & Zakharyaschev, M. (1999b). Temporalizing description logic. Gabbay, D., &
de Rijke, M. (Eds.), Frontiers Combining Systems, pp. 379402. Studies Press/Wiley.

686

fiJournal Artificial Intelligence Research 46 (2013) 303341

Submitted 09/12; published 03/13

Boolean Equi-propagation Concise Efficient SAT
Encodings Combinatorial Problems
Amit Metodi
Michael Codish

amit.metodi@gmail.com
mcodish@cs.bgu.ac.il

Department Computer Science
Ben-Gurion University Negev, Israel

Peter J. Stuckey

pjs@csse.unimelb.edu.au

Department Computer Science Software Engineering
NICTA Victoria Laboratory
University Melbourne, Australia

Abstract
present approach propagation-based SAT encoding combinatorial problems, Boolean equi-propagation, constraints modeled Boolean functions
propagate information equalities Boolean literals. information
applied simplify CNF encoding constraints. key factor considering
small fragment constraint model one time enables us apply stronger,
even complete, reasoning detect equivalent literals fragment. detected,
equivalences apply simplify entire constraint model facilitate reasoning
fragments. Equi-propagation combination partial evaluation constraint simplification provide foundation powerful approach SAT-based finite
domain constraint solving. introduce tool called BEE (Ben-Gurion Equi-propagation
Encoder) based ideas demonstrate variety benchmarks approach leads considerable reduction size CNF encodings subsequent
speed-ups SAT solving times.

1. Introduction
recent years, Boolean SAT solving techniques improved dramatically. Todays SAT
solvers considerably faster able manage larger instances yesterdays. Moreover, encoding modeling techniques better understood increasingly innovative.
SAT currently applied solve wide variety hard practical combinatorial problems, often outperforming dedicated algorithms. general idea encode (typically,
NP) hard problem instance, , Boolean formula, , satisfying assignments correspond solutions . Given encoding, SAT solver
applied solve .
Tailgating success SAT technology variety tools applied
specify compile problem instances corresponding SAT instances. general
objective tools facilitate process providing high-level descriptions
(constraint) problem hand solved. Typically, constraint-based modeling
language introduced used model instances. Drawing analogy programming languages, given description, compiler provide low-level executable
c
2013
AI Access Foundation. rights reserved.

fiMetodi, Codish, & Stuckey

underlying machine. Namely, context, formula underlying SAT
SMT solver.
example, Cadoli Schaerf (2005) introduce NP-SPEC, logic-based specification
language allows specifying combinatorial problems declarative way. core
system component translates specifications CNF formula. Similarly Sugar
(Tamura, Taga, Kitagawa, & Banbara, 2009) SAT-based constraint solver. solve
finite domain constraint satisfaction problem first modeled constraint language
(also called Sugar) encoded CNF formula solved using MiniSAT
solver (Een & Sorensson, 2003). MiniZinc (Nethercote, Stuckey, Becket, Brand, Duck, &
Tack, 2007) constraint modeling language compiled variety solvers
low-level target language FlatZinc exist many solvers. particular,
FlatZinc instances solved fzntini (Huang, 2008) encoding CNF
fzn2smt encoding SMT-LIB (Barrett, Stump, & Tinelli, 2010).
Simplifying CNF formulae prior application SAT solving utmost
importance wide range techniques applied achieve
goal. See example work Li (2003), Een Biere (2005), Heule, Jarvisalo,
Biere (2011), Manthey (2012), references therein work.
techniques exhibit clear trade-off amount simplification obtained
time requires. Moreover, stronger techniques become prohibitive SAT model
involves hundreds thousands variables millions clauses. CNF simplification
tools, time limits simplification techniques imposed and/or approximations used.
paper takes new approach CNF simplification. Typically, CNF random collection clauses, rather structure derived application specific
problem domain. SAT solving applied encode solve finite domain constraint
problems, original constraint model manifest structure. Usually, constraints discarded encoded CNF. advocate maintaining constraints
provides important structural information applied drive process CNF
simplification. specific, constraints model induce partitioning CNF
encoding conjunction sub-formulae call portions.
novelty approach CNF simplification instead considering CNF
whole, assume partitioned conjunction smaller portions.
simplification repeatedly applied individual portions. facilitates propagationbased process simplification one portion propagates information
portions information may trigger simplification portions.
portions typically much smaller entire CNF effectively apply
stronger simplification algorithms. introduce notion equi-propagation. Similar
unit propagation inferring unit clauses applied simplify
CNF formulae, equi-propagation inferring equational consequences literals
(and Boolean constants).
wide body research CNF simplification applied implement
equi-propagation sometimes called equivalent literal substitution, example
Gelder (2005). Techniques typically involve binary clause based simplifications using, among
others, hyper binary resolution binary implication graphs. See example, work
Heule et al. (2011) references therein. guiding principle works
304

fiBoolean Equi-propagation

techniques must simple efficient prohibitive size CNF
must apply.
approach different focus far richer forms inference even related
CNF structure formula. one extreme apply complete equi-propagation
detects equivalences implied formula. Clearly complete equi-propagation
NP-hard. However, complete equi-propagators feasible apply small
portions formula. complete equi-propagation slow consider ad-hoc
techniques. forms equi-propagation common driven
CNF structure (e.g. binary clauses) rather underlying constraint structure
CNF was, being, generated.
rest paper structured follows. Section 2 introduces modeling language
finite domain constraints consists 5 constraint constructs sufficient
illustrate contribution paper. argue constraints model induce
natural partition CNF encoding smaller portions partition
used drive simplification CNF encoding. Section 3 presents equi-propagation
first ingredient contribution. Equi-propagation learning information apply simplify CNF encodings. Section 4 describes practical basis
implementing equi-propagation. Section 5 introduces second ingredient: partial evaluation. Given information derived using equi-propagation, partial evaluation applies
simplify constraints particular remove Boolean variables CNF
encodings. Section 6 describes tool, called BEE (Metodi & Codish, 2012) (Ben-Gurion
Equi-propagation Encoder) based equi-propagation partial evaluation.
introduce full constraint language similar Sugar subset
FlatZinc relevant finite domain constraint problems. also spell special
treatment all-different constraint BEE. Section 7 demonstrates application
BEE. Section 8 presents experimental evaluation. Finally Section 9 presents
conclusion.
paper extends earlier work presented Metodi, Codish, Lagoon, Stuckey
(2011), first introduced equi-propagation, also BEE tool paper (Metodi &
Codish, 2012). BEE tool available download (Metodi, 2012).

2. Constraint Based Boolean Modeling
section provides basis contribution: constraint-based modeling language,
together Boolean interpretation constraint language. enables us
view constraint model conjunction Boolean formulae provides structure
drives subsequent encoding CNF.
first introduce simple small fragment typical finite domain constraint-based
modeling language. serves illustrate approach. Later, Section 6, show
full language. discuss several options Boolean representation integers.
paper adopt particular unary representation, called order encoding.
contribution independent choice, although equi-propagation works well it.
Finally finish section constraints language fragment
viewed Boolean formula, constraint model conjunction.
305

fiMetodi, Codish, & Stuckey

(1)
(2)
(3)
(4)
(5)

new int(I, c1 , c2 )
int neq(I1 , I2 )
allDiff([I1 , . . . , ])
int plus(I1 , I2 , I)
int array plus([I1 , . . . , ], I)

0 c1 c2
I1 6= I2
V
i<j Ii 6= Ij
I1 + I2 =
I1 + + =

Figure 1: core constraint language
2.1 Constraint Language Fragment
focus small fragment typical constraint modeling language detailed Figure 1.
serves present main ideas paper. Constraint (1) declaring finite
domain integer variables range [c1 ...c2 ]. simplicity presentation
assume c1 0. Constraints (23) difference integer variables,
constraints (45) sums integer variables. syntactic sugar also allow
writing integer constants constraints. example, int neq(I, 5) short
new int(I0 , 5, 5), int neq(I, I0 ).
2.2 Modeling Kakuro: Example
Kakuro puzzle n board black white cells. black cells contain hints
white cells filled numbers 1 9 (the bound 9 often
generalized larger value r). hints specify constraints sums values
blocks white cells right and/or hint. numbers assigned white
cells block required different. Figure 2 illustrates 4 4 Kakuro
puzzle (left) solution (right).
model Kakuro puzzle view set blocks (of white cells) block
B set integer variables associated corresponding integer value, hint(B).
block B associated two constraints: integers B must sum hint(B)
must all-different. Figure 3 illustrates constraints corresponding Kakuro
instance Figure 2.
2.3 Representing Integers
fundamental design choice encoding finite domain constraints concerns representation integer variables. Gavanelli (2007) surveys several possible choices (the

Figure 2: 4 4 Kakuro puzzle (right) solution (left).
306

fiBoolean Equi-propagation

new
new
new
new
new
new
new

int(I1 , 1, 9)
int(I2 , 1, 9)
int(I3 , 1, 9)
int(I4 , 1, 9)
int(I5 , 1, 9)
int(I6 , 1, 9)
int(I7 , 1, 9)

int
int
int
int
int
int

array
array
array
array
array
array

plus([I1 , I2 ], 13)
plus([I1 , I3 ], 5)
plus([I3 , I4 , I5 ], 12)
plus([I2 , I4 , I6 ], 19)
plus([I6 , I7 ], 3)
plus([I5 , I7 ], 4)

allDiff([I1 , I2 ])
allDiff([I1 , I3 ])
allDiff([I3 , I4 , I5 ])
allDiff([I2 , I4 , I6 ])
allDiff([I6 , I7 ])
allDiff([I5 , I7 ])

Figure 3: Constraints Kakuro instance Figure 2.

direct-, support- log- encodings) introduces log-support encoding. Given choice
representation constraints bit-blasted interpreted Boolean formulae. focus
use unary representation, so-called, order-encoding (see, e.g. Crawford
& Baker, 1994; Bailleux & Boufkhad, 2003) many nice properties applied
small finite domains.
order-encoding, integer variable X domain [0, . . . , n] represented
bit vector X = [x1 , . . . , xn ]. bit xi interpreted X particular bit
sequence X constitutes monotonic non-increasing Boolean sequence. example,
value 3 interval [0, 5] represented 5 bits [1, 1, 1, 0, 0].
important property Boolean representation finite domain integers
ability represent changes set values variable take. well-known
order-encoding facilitates propagation bounds. Consider integer variable
X = [x1 , . . . , xn ] values interval [0, n]. restrict X take values range
[a, b] (for 1 b n), sufficient assign xa = 1 xb+1 = 0 (if b < n).
variables xa0 xb0 0 a0 > b < b0 n determined true false,
respectively, unit propagation. example, given X = [x1 , . . . , x9 ], assigning x3 = 1
x6 = 0 propagates give X = [1, 1, 1, x4 , x5 , 0, 0, 0, 0], signifying dom(X) = {3, 4, 5}.
observe additional property order-encoding X = [x1 , . . . , xn ]: ability
specify variable cannot take specific value 0 v n domain equating
two variables: xv = xv+1 . indicates order-encoding well-suited
propagate lower upper bounds, also represent integer variables arbitrary,
finite set, domain. example, given X = [x1 , . . . , x9 ], equating x2 = x3 imposes
X 6= 2. Likewise x5 = x6 x7 = x8 impose X 6= 5 X 6= 7. Applying
equalities X gives, X = [x1 , x2 , x2 , x4 , x5 , x5 , x7 , x7 , x9 ] (note repeated literals),
signifying dom(X) = {0, 1, 3, 4, 6, 8, 9}.
order-encoding many additional nice features exploited simplify
constraints encodings CNF. illustrate one, consider constraint form
+ B = 5 B integer values range 0 5 represented
order-encoding. bit level (in order encoding) have: = [a1 , . . . , a5 ]
B = [b1 , . . . , b5 ]. constraint satisfied precisely B = [a5 , . . . , a1 ]. Instead
encoding constraint CNF, substitute bits b1 , . . . , b5 literals a5 , . . . , a1 ,
remove constraint. section 3 formalize process discovering equalities
literals implied constraint using simplify CNF encodings.
307

fiMetodi, Codish, & Stuckey

2.4 Bit Blasting
Given constraint model decision represent finite domain integer variables
bit level (we chose order encoding), bit-blasting process instantiating integer variables corresponding bit vectors interpreting constraints Boolean
formulae.
integer variable, I, declared constraint form new int(I, c1 , c2 )
0 c1 c2 represented bit-vector = [1, . . . , 1, Xc1 +1 , . . . , Xc2 ]. So, may view
constraint model consisting Boolean variables constraint c corresponds
Boolean formula denoted [[c]], bit-blasted version c. specific definition
[[]] important. illustration, note one could define
^
[[new int(I, c1 , c2 )]] =
(xi+1 xi )
c1 i<c2

= [1, . . . , 1, Xc1 +1 , . . . , Xc2 ] well
[[int neq(I1 , I2 )]] =

n
_

(xi xor yi )

i=1

simplify presentation assume I1 = [x1 , . . . , xn ] I2 = [y1 , . . . , yn ]
represented number bits. mapping [[]] extends natural way apply
conjunctions constraints. So, given constraint model one Figure 3,
integer variables instantiated unary (order encoding) bit vectors constraint
viewed Boolean formula. constraint model takes Boolean representation
conjunction formulae.

3. Boolean Equi-propagation
section present approach propagation-based SAT encoding, Boolean equipropagation, propagates information equalities Boolean literals (and
constants). prove Boolean equi-propagation stronger unit propagation
determines least many fixed literals unit propagation. demonstrate,
example, power equi-propagation show leads considerable reduction
size CNF encoding.
3.1 Boolean Equi-propagation
Let B set Boolean variables. literal Boolean variable b B negation
b. negation literal `, denoted `, defined b ` = b b ` = b.
Boolean constants 1 0 represent true false, respectively. set literals
denoted L L0,1 = L {0, 1}. set (free) Boolean variables appear
Boolean formula denoted vars(). extend vars function sets formulae
natural way.
assignment, A, partial mapping
Boolean
constants, often
fi
fi
variables
fi
fi
viewed following set literals: b A(b) = 1 b A(b) = 0 . formula
b B, denote [b] (likewise [b]) formula obtained substituting
308

fiBoolean Equi-propagation

occurrences b B true (false). notation extends natural way
sets literals. say satisfies vars() vars(A) [A] evaluates true.
Boolean Satisfiability (SAT) problem consists Boolean formula determines
exists assignment satisfies .
Boolean equality constraint ` = `0 `, `0 L0,1 . equi-formula E set
Boolean equalities understood conjunction. set Boolean equalities denoted
Leq
0,1 set equi-formulae denoted E.
Example 1. Suppose B = {x, y, z}. L0,1 = {0, 1, x, x, y, y, z, x}. example
assignment = {x, z}, B = {x, y, z, y} assignment (since includes
{y, y}). Given formula = x (y z) [x] formula 0 (y z)
equivalently z. formula [A] = 1 (y 1) equivalent true,
satisfy since vars() = {x, y, z} 6 {x, z} = vars(A). example equi-formula B
{x = 0, = z} equivalently x (y z).
3.1.1 Equi-propagation
process inferring equational consequences Boolean formula given equational information. equi-propagator formula extensive function : E E
defined E E,

n
fi
fi
E (E) e Leq
0,1 E |= e
is, conjunction equalities, least strong
true E. say
n E, made
eq fifi
equi-propagator complete (E) = e L0,1 E |= e . denote
complete equi-propagator . assume equi-propagators monotonic:
E1 E2 (E1 ) (E2 ). particular, follows, definition, complete
equi-propagators. Section 3.3 discuss several methods implement complete
incomplete equi-propagators.
Example 2. Consider constraint
C = new int(X, 0, 4) new int(Y, 0, 4) int neq(X, Y)
corresponding Boolean representation = [[C]] bit representation
X = [x1 , x2 , x3 , x4 ] = [y1 , y2 , y3 , y4 ]
Assume setting
E=



y1 = 1, y2 = 1, y3 = 0, y4 = 0



signifying = 2. Then, (E) = E {x2 = x3 } indicating X 6= 2. occurs
since E equivalent (x2 x1 ) (x3 x2 ) (x4 x3 ) (x1 x2 x3 x4 )
E |= x2 = x3 .
following theorem states complete equi-propagation least powerful
unit propagation.
309

fiMetodi, Codish, & Stuckey

Theorem 3. Let complete equi-propagator Boolean formula . Then,
literal made true unit propagation clausal representation using
equations E also determined true (E).
Proof. Let Boolean formula, E equi-formula, let C CE clausal
representations E respectively. Clearly |= C E |= CE . Let b
positive literal determined unit propagation C CE . correctness unit
propagation, C CE |= b. Hence, E |= b thus (E) |= b = 1. case
negative literal b same, except infer b = 0.
following example illustrates equi-propagation powerful unit
propagation.
Example 4. Consider = (x1 x2 ) (x1 x2 ) (x1 x2 x3 ). clausal
representation (x1 x2 )(x1 x2 )(x1 x2 )(x1 x2 x3 ) unit propagation
possible, since unit clauses. Equi-propagation (with additional equational
information) gives: () = {x1 = 1, x2 = 1, x3 = 0}.
3.1.2 Boolean Unifiers
sometimes convenient view equi-formula E generic solved-form Boolean
substitution, E , (most general) unifier equations E. Boolean substitutions generalize assignments variables bound also literals.fi Boolean

substitution idempotent mapping : B L0,1 dom() = b B fi (b) 6= b
finite. Note particular idempotence implies (b) 6= b every b B.
Note also defined B domain, dom(), includes elements

non-identity.
Boolean substitution, , viewed set =
fi


b 7 (b) fi b dom()
.


apply another substitution 0 , obtain substifi

tution ( 0 ) = b 7 (0 (b)) fi b dom() dom(0 ) . unifier equi-formula E
substitution |= (e), e E. most-general unifier E
substitution unifier 0 E, exists substitution 0 = .
Example 5. Consider equi-formula E {b1 = b2 , b3 = b4 , b5 = b6 , b6 = b4 , b7 =
1, b8 = b7 } unifier E {b2 7 b1 , b4 7 b3 , b5 7 b3 , b6 7 b3 , b7 7 1, b8 7 0}.
Note (E) trivially true equi-formula {b1 = b1 , b3 = b3 , b3 = b3 , b3 = b3 , 1 =
1, 0 = 1}.
Consider enumeration L0,1 = {0, 1, b1 , b1 , b2 , b2 , . . .} let total (strict)
order L0,1 0 1 b1 b1 b2 b2 . define canonical most-general
unifier unifyE satisfiable equi-formula E where:
fi


unifyE (b) = min ` L0,1 fi E |= b = `
is, substitution unifyE maps b smallest literal equivalent b given
E. compute unifyE almost linear (amortized) time using variation
union-find algorithm (Tarjan, 1975).
Example 6. equi-formula E substitution Example 5
unifyE = .
310

fiBoolean Equi-propagation

following proposition provides foundation equi-propagation based Boolean
simplification. allows us apply equational information simplify given formula.
particular, E equi-formula literals occurring unifyE () smaller
contains fewer variables.
Proposition 1. Let Boolean formula E E satisfiable equi-formula. Then,
a. E unifyE () E;
b. E satisfiable unifyE () satisfiable;
c. satisfying assignment unifyE () unifyE satisfying assignment
E.
Proof. (a) Let = unifyE assume satisfying assignment E,
view substitution, unifier E. Hence, since general unifier,
exists substitution = . Clearly (b) = (b) variables b range
. Hence, agree variables () implies (()) = (())
meaning (()) = (). So, satisfying assignment () E
satisfying assignment E. (b) () direction follows (a)
() direction (c). (c) Assume satisfying assignment unifyE (). Clearly
unifyE satisfies construction. Also unifyE satisfies E since unifyE (E)
trivial. Hence unifyE satisfying assignment E.
3.1.3 Equi-propagation Process
equi-propagation process presented central theme paper: Let =
1 n partitioning Boolean formula n portions, let 1 , . . . , n corresponding equi-propagators, take initial E = . Satisfiability determined
follows:
1. long possible, select (E) ) E update E = (E).
2. Finally, equi-propagators apply more, check unifyE () satisfiable.
3. satisfying assignment unifyE () unifyE satisfying assignment
.
typically apply equi-propagation theme Boolean representation =
1 n constraint model C = C1 Cn = [[Ci ]]. require
Ci small conjunction constraints. Typically, integer variables referred
Ci also declared Ci (sometimes requires duplicating variable
declarations). individual constraint c denote c+ conjunction constraints
including c declarations integer variables refers to. specifics
declarations clear context.
Example 7. Let C following constraint model:

new int(X, 1, 3) new int(Y, 1, 3) new int(Z, 1, 3)
C=
int plus(X, Y, 3) int plus(Y, Z, 4) int neq(Y, Z)

311



fiMetodi, Codish, & Stuckey

1. int plus+ (X, Y, 3) = int plus(X, Y, 3) new int(X, 1, 3) new int(Y, 1, 3),
2. int plus+ (Y, Z, 4) = int plus(Y, Z, 4) new int(Y, 1, 3) new int(Z, 1, 3),
3. int neq+ (Y, Z) = int neq(Y, Z) new int(Y, 1, 3) new int(Z, 1, 3).
basis equi-propagation take = 1 2 3 1 = [[int plus+ (X, Y, 3)]],
2 = [[int plus+ (Y, Z, 4)]], 3 = [[int neq+ (Y, Z)]]. Denoting X = [1, x2 , x3 ], =
[1, y2 , y3 ], Z = [1, z2 , z3 ] applying corresponding complete equi-propagators
starting E0 = have:
1. E1 = 1 (E0 ) = E0 {x3 = 0, y3 = 0, x2 = y2 };
2. E2 = 2 (E1 ) = E1 {z2 = 1, y2 = z3 };
3. E3 = 3 (E2 ) = E2 {y2 = 0}.
point equi-propagation applies more, unifyE3 = {x2 7 1, x3 7 0, y2 7
0, y3 7 0, z2 7 1, z3 7 1} . Now, unifyE3 () tautology (all Boolean variables
determent equi-propagation).
following theorem clarifies order equi-propagators applied
equi-propagation process influence final result.
Theorem 8. equi-propagation process confluent.
Proof. Let = 1 n Boolean formula 1 , . . . , n corresponding equipropagators. Let E1 = ir (ir1 (. . . i1 () . . .)) E2 = js (js1 (. . . j1 () . . .))
two different applications equi-propagation process. construction,
given equi-propagators, property (?): (E1 ) = E1 (E2 ) = E2 .
assume, contradiction, E1 6= E2 . w.l.o.g. exists e E2 e
E1 (swap roles E1 E2 E2 E1 ). E1 ( E2 . Let us focus first
step equi-propagation process leading E2 introduced equation e E2
introduced E1 : So, exists ` < E = j` (j`1 (. . . j1 () . . .)) E1
e `+1 (E) e 6 E1 . But, E E1 , monotonicity `+1 ,
`+1 (E) `+1 (E1 ) hence e `+1 (E1 ) contradiction construction
property (?).
following proposition provides alternative, efficient implement, definition
complete equi-propagation.
Proposition 2. Let Boolean formula complete equi-propagator .
Define E E,
n

fi
fi unify () |= e
(E) = E e Leq
E
0,1
Then, (E) = (E). is, implements complete equi-propagator .
312

fiBoolean Equi-propagation

Proof. Forthefi first direction, ():
definition, (E) E. also
(E) e fi unifyE () |= e
Proposition 1(a) E |= unifyE (). So,
(E) (E). direction, (): Let e (E). e E proof
straightforward. Otherwise, let unifyE () |= e assume contrary e 6 (E),
words E 6|= e. means exists assignment
satisfies E satisfy e. Lemma 1(a), also satisfies unifyE () E
particular satisfies unifyE (). assumption unifyE () |= e
satisfies e. Contradiction.
Computing considerably efficient since simply examine
formula application unifyE determine new Boolean equality consequences.
Finally comment: intention equi-propagation process applied
make SAT instance smaller also obtain easier solve representation.
However, decreasing size CNF main objective. fact, often explicitly
introduce redundancies improve SAT encoding. example, consider if-thenelse construct, xITE(s,t,f), propositional variable: indicates selector,
indicates true branch, f indicates false branch, x indicates result.
corresponding CNF {{s, t, x}, {s, t, x}, {s, f, x}, {s, f, x}}. Een Sorensson
(2006) propose add redundant clauses, {t, f, x} {t, f, x}. comment
improves encoding observe redundant clauses often introduced
achieve arc-consistency SAT encoding. show given clausal encoding
formula , application equi-propagation strengthen unit propagation.
Theorem 9. Let C set clauses, suppose C |= E E equi-formula.
unit propagation unifyE (C) least strong unit propagation C.
Proof. Unit propagation C starting assignment A0 repeatedly chooses clause
c {l} C {l0 | l0 c} Ai sets Ai+1 := Ai {l}. Unit propagation terminates
Ak clauses occur. Note failure detected Ak contains
literal negation.
show using order unit propagation unifyE (C) determined
occurs C starting assignment B0 = unifyE (A0 ) always obtain assignment
Bi Bi unifyE (Ai ). proof induction unit propagation steps C.
base case holds construction.
Assume c {l} C {l0 | l0 c} Ai . induction Bi unifyE (Ai )
{unifyE (l0 ) | l0 c}. Either unifyE (l) Bi case set Bi+1 = Bi
induction holds. unifyE (l) 6 Bi . since c {l} C {unifyE (l0 ) | l0
c}{unifyE (l)} unifyE (C). Hence unit propagation unifyE (C) Bi obtain
Bi+1 := Bi {unifyE (l)}. Hence induction holds.
Given unit propagation reaches unique fixpoint unit propagation order
unifyE (A0 ) end assignment B B Bk unify(Ak )
3.2 Power Equi-propagation
illustrate impact equi-propagation come back Kakuro example
Section 2.2 (recall Figure 2). fact solving puzzles via SAT encodings quite easy,
without equi-propagation. example viewed illustrating
313

fiMetodi, Codish, & Stuckey

a. 1

b. 2

c. 3

Figure 4: Applying complete equi-propagation Kakuro Instance using different models

impact equi-propagation size encoding. compare 3 different models
problem, give different equi-propagation.
consider, baseline discussion, following Boolean representation
derived constraint model declarations specified explicitly
form new int(I, 1, h) h smallest hint block includes
number 9 smaller.
^
^
1 =
[[int neq+ (Ii , Ij )]]
[[int array sum+ (B, hint(B))]]
{I1 , . . . , Ik } Blocks
1i<j k

B Blocks

Notice one int neq conjunct pair white cells block,
one int array sum conjunct block. Applying equi-propagation process
1 complete equi-propagators determines six integer values depicted Figure 4(a).
Figure 4(b) illustrates impact applying equi-propagation process
equi-propagators allDiff constraints instead individual int neq constraints. determines seven integer variables formalized taking following
Boolean representation constraint model (and introducing equi-propagator
conjunct).
^
^
2 =
[[allDiff+ (B)]]
[[int array sum+ (B, hint(B))]]
B Blocks

B Blocks

Figure 4(c) illustrates impact applying equi-propagation process equipropagators pairs, consisting allDiff constraint together corresponding sum constraint. form equi-propagation powerful. fixes integer
values white cells (in example). stress equi-propagation reasons
equalities Boolean literals constants. take model as:
^

3 =
[[allDiff+ (B)]] [[int array sum+ (B, hint(B))]]
B Blocks

demonstrate impact equi-propagation, Table 1 provides data 15
additional instances,1 categorized as: easy, medium hard. first two columns
table indicate instance category ID. five columns headed Integer
1. Instances available http://4c.ucc.ie/~hcambaza/page1/page7/page7.html (generated Helmut
Simonis).

314

fihard

medium

easy

Boolean Equi-propagation

ID
168
169
170
171
172
188
189
190
191
192
183
184
185
186
187

Integer Variables
init 1 2 3 BEE
init
484 439 280
0 385 3872
467 456 440
0 440 3736
494 485 469
0 469 3952
490 406 393
0 422 3920
506 495 484
0 492 4048
476 461 455
0 461 3808
472 437 425
62 449 3776
492 481 480
0 480 3936
478 452 448 161 448 3824
499 481 478 136 478 3992
490 365 345
0 371 3920
506 489 484
23 486 4048
482 482 455 206 467 3856
472 466 454
0 466 3776
492 475 473
69 473 3936
Average compilation time sec.

Boolean Variables
1
2
3
1440
843
0
1823 1682
0
1961 1798
0
1280 1148
0
1676 1573
0
1939 1915
0
2017 1911
81
1998 1920
0
1864 1821
197
2455 2417
214
1151 1059
0
1613 1495
21
2181 2111
220
2115 2062
0
1991 1959
48
3.739 2.981 0.916

BEE
1170
1692
1805
1341
1634
1934
1976
1936
1828
2420
1168
1545
2144
2086
1960
0.477

Table 1: Applying SAT-based complete equi-propagation Kakuro encoding
Variables, first four specify number unassigned white cells initial stage
three complete equi-propagation processes described above. five
columns headed Boolean variables, first four indicate corresponding information
regarding number Boolean variables bit representations integers. So,
smaller number table, variables removed due equipropagation. particular, 3 model completely solves 9 15 instances. two
columns titled BEE show corresponding information obtained using weaker form
equi-propagation described Section 4 below. last row table indicates
average time takes perform equi-propagation (in seconds) using three
schemes, 1 , 2 , 3 , weaker scheme titled BEE. come back discuss
later detailing equi-propagation performed. results table indicate
clear benefit performing equi-propagation based coarser portions model.
3.3 Implementing Equi-propagators
implement complete equi-propagators need infer Boolean equalities implied
given Boolean formula, , equi-formula, E. Based Proposition 2, sufficient
test condition
unifyE () |= (`1 `2 )
(1)
consider three techniques: using SAT solver, using BDDs, using ad-hoc rules
applied Boolean representations individual constraints.
straightforward implement complete equi-propagator using SAT solver.
test Condition (1) consider formula = (`1 6 `2 ). satisfiable,
Condition (1) holds. way, Condition (1) checked relevant equations
315

fiMetodi, Codish, & Stuckey

involving variables unifyE () (and constants 0,1). major obstacle SATbased approach testing single equivalence, `1 `2 , least hard testing
satisfiability . fact testing unsatisfiability typically expensive.
Hence importance assumption small fragment CNF
interest. practice SAT-based equi-propagation surprisingly fast. illustration,
last row Table 1 average times SAT-based complete equi-propagation
different models indicated columns 1 , 2 , 3 . interesting observe
strongest technique, using 3 , fastest. fewer (but
larger) conjuncts hence fewer queries SAT solver.
implement complete equi-propagator using binary decision diagrams (BDDs)
follows. construct BDD formula beginning equi-propagation.
new equational information E 0 added E simplify BDD conjoining
BDD BDD E 0 projecting variables longer appear
unifyE (). Note simplification increase size BDD. practice,
rather two steps, use Restrict operation Coudert Madre
(1990) (bdd simplify Somenzi, 2009) create new BDD efficiently.
Given BDD unifyE (), explicitly test Condition (1) using standard
BDD containment test (e.g., bddLeq Somenzi, 2009). SAT-based approach, test performed relevant equations involving variables unifyE ()
(and constants 0,1). Alternately use method Bagnara Schachte (1998)
(extended extract literal equalities opposed variable equalities) extract
fixed literals equivalent literal consequences BDD.
Example 10. Consider BDD shown Figure 5(a) represents formula:
new int(A, 0, 3) new int(B, 0, 3) int neq(A, B). Figure 5(b) depicts BDD
unifyE () E = {B1 = 1, B2 = 1, B3 = 0 }. easy see equipropagation determines A2 = A3 . Let E 0 = E {A2 = A3 }. Figure 5(c) shows
simplified BDD unifyE 0 ().
major obstacle BDD-based approach concerns size formula
unifyE (). constraints, corresponding BDD guaranteed polynomial (in size constraint). following result holds arbitrary constraint
, also holds unifyE ().
Proposition 3. Let c constraint k integer variables represented n bits
order encoding. Then, number nodes BDD representing [[c]] bound
O(nk ).
Proof. (Sketch) n + 1 legitimate states n bit unary variable,
BDD cannot nodes possible states.
Constraints like new int, int neq, int plus involve 3 integer variables
hence BDD-based complete equi-propagators polynomially bounded. However,
case global constraints allDiff int array plus
arity fixed. Moreover, well known allDiff constraint
polynomial sized BDD (Bessiere, Katsirelos, Narodytska, & Walsh, 2009).
316

fiBoolean Equi-propagation
7654
0123
A1:

:

:
7654
0123
7654
0123
B1
B1




0123
7654
0123 7654
7654
0123
r 2 rrr A2: A2
r
r
:
r r
rr
rrr rrrr
:
r
0123 7654
0123
0123
0123
7654
0123
B2 L 7654
B2
B2: 7654
B2: 7654
, LLB 2

::
L
:
L
:
,
LLL
::
:
, L
7654
0123
7654
0123
0123
,
A3: A3 l 7654
A3
, : l -ll
, l l : l
7654
0123
7654
0123
B3
B3;; - ;;
S;-

7654
0123
A1
.
.
.

.

.

0123
7654
0123
7654
A2
A2
.

.

.
.
.
0123
0123
7654
A3.
A3 > 7654
>>
>> .
>> .
>> .
>>
>.

7654
0123
A1
+
+







+



+

+
7654
0123
A2







(a) BDD int nequ (A, B)

(b) Simpld wrt B=[1, 1, 0]

(c) Simpld wrt A2 =A3

Figure 5: BDDs (a) new int3 (A, [0, 3]) new int3 (B, [0, 3]) int neq(A, B) (b)
unifyE () E = {B1 =1, B2 =1, B3 =0} (c) unifyE 0 () E 0 =
E {A2 =A3 }. Full (dashed) lines correspond true (false) edges. Edges
false node F omitted brevity.

Given potential exponential run-time performing SAT-based equi-propagation,
potential exponential size BDD-based equi-propagators, consider third
approach implement equi-propagation collection ad-hoc transition rules
type constraint. approach complete equations
implied constraint detected implementation fast, works well
practice. topic next section.

4. Ad-hoc Equi-Propagation
consider rule-based approach define equi-propagators. definition given
set ad-hoc rules specified type constraint. novelty approach
based CNF, previous works, rather driven bit blasted constraints
encoded CNF. presentation focuses case finite domain
integers represented order encoding. integer X = [x1 , . . . , xn ], often
write: X denote equation xi = 1, X < denote equation xi = 0, X 6=
denote equation xi = xi+1 , X = denote pair equations xi = 1, xi+1 = 0.
Moreover, simplify notation specifying rules below, view X = [x1 , . . . , xn ]
larger vector padded sentinel cells cells left x1 take value 1
cells right xn take value 0. Basically facilitates specification
end cases formalism. consider 5 constraints
language fragment presented Section 2.
317

fiMetodi, Codish, & Stuckey

c = new int([x1 , . . . , xn ], 0, n)
E
add c (E)
xi = 1 x1 = 1, . . . , xi1 = 1
xi = 0 xi+1 = 0, . . . , xn = 0

(a)

c = int neq(X, )
X = [x1 , . . . , xn ] = [y1 , . . . , yn ]

E
add c (E)
X=i
6=
xi = yi+1 , yi = xi+1
X 6= i, 6=
xi = yi+1 , yi = xi+1
X 6= i, 6=
(b)

Figure 6: Ad-hoc rules (a) new int (b) int neq
c = int plus(X, Y, Z) X = [x1 , . . . , xn ],
= [y1 , . . . , ym ], Z = [z1 , . . . , zn+m ]

c = allDiff([Z1 , Z2 , Z3 , . . . , Zn ])
E
add c (E)
Z1 , Z2 {i, j}

Z1 =
6 Z2 , Zk 6=
Zk =
6 j (k > 2)

E
X i, j
X < i, < j
Z k, X <
Z < k, X
X=i
Z=k

(a)

add c (E)
Z i+j
Z <i+j1
ki
<ki
zi+1 = y1 , . . . , zi+m = ym
x1 = yk , . . . , xk = y1
(b)

Figure 7: Ad-hoc rules (a) allDiff (b) int plus

(1) two rules Figure 6(a) derive monotonicity order encoding
representation. basically correspond unit propagation, constraint level.
(2) first rule Figure 6(b) considers cases X constant (the symmetric
case handled exchanging X ). two rules capture templates
commonly arise equi-propagation process. illustrate justification
third rule consider possible truth values variables xi xi+1 : (a) xi = 0
xi+1 = 1 integers relation take form [. . . , 0, 1, . . .] violating
specification ordered, possible. (b) xi = 1 xi+1 = 0
numbers take form [1, . . . , 1, 0, . . . , 0] equal, violating neq constraint.
possible bindings xi xi+1 xi = xi+1 .
(3) Figure 7(a) illustrate single rule allDiff constraint considers
Hall sets size 2. Zi represents integer order encoding focus
case Z1 Z2 restricted equations E take two possible
values, j. expressed E [x1 , . . . , xn ] {i, j} (for < j) means
xk = 1 k < i, xk = xk+1 k < j, xk = 0 j < k n. Z1 6= Z2
means adding single equation xi = yi (because Z1 Z2 take two values).
addition rule, apply rules int neq(Zi , Zj ) pair integers Zi
Zj constraint.
318

fiBoolean Equi-propagation




z1 =1, . . . , z4 =1,
E0 =
z5 =0, . . . , z18 =0
X = [x1 , . . . , x4 , x5 , x6 , . . . , x9 ],
= [y1 , . . . , y4 , y5 , y6 , . . . , y9 ],
Z = [1, 1, 1, 1, 0, . . . , 0]
E2 = E1 {y6 =0, . . . , y9 =0}
X = [x1 , . . . , x4 , x5 , x6 , . . . , x9 ],
= [y1 , . . . , y4 , 0, . . . , 0],
Z = [1, 1, 1, 1, 0, . . . , 0]
E4 = E3 {x6 =0, . . . , x9 =0}
X = [x1 , . . . , x4 , 0, . . . , 0],
= [y1 , . . . , y4 , 0, . . . , 0],
Z = [1, 1, 1, 1, 0, . . . , 0]

x2 =y3 ,
y2 =x3


int neq

Z<5, X0


int plus

Z<5, 0


int plus

Z=4


int plus

E1 = E0 {y5 =0}
X = [x1 , . . . , x4 , x5 , x6 , . . . , x9 ],
= [y1 , . . . , y4 , 0, y6 , . . . , y9 ],
Z = [1, 1, 1, 1, 0, . . . , 0]

E3 = E2 {x5 =0}
X = [x1 , . . . , x4 , 0, x6 , . . . , x9 ],
= [y1 , . . . , y4 , 0, . . . , 0],
Z = [1, 1, 1, 1, 0, . . . , 0]

E5 = E4 {x1 =y4 , . . . , x4 =y1 }
X = [x1 , x2 , x3 , x4 , 0, . . . , 0],
= [x4 , x3 , x2 , x1 , 0, . . . , 0],
Z = [1, 1, 1, 1, 0, . . . , 0]

y5 =0


new int

x =0

5
new int



E6 = E5 {x2 =x3 }
X = [x1 , x2 , x2 , x4 , 0, . . . , 0],
= [x4 , x2 , x2 , x1 , 0, . . . , 0],
Z = [1, 1, 1, 1, 0, . . . , 0]

Figure 8: Ad-hoc equi-propagation described Example 11
(4) first four rules Figure 7(b) capture standard propagation behavior
interval arithmetics. last two rules apply one integers relation
constant. symmetric cases replacing role X .
(5) special ad-hoc rules equi-propagation int array plus constraint. simply viewed decomposition set int plus constraints.
simplification performed level using rules int plus. decomposition
int array plus explained Section 6.
Example 11 (ad-hoc equi-propagation). Consider following (partial) constraint model,
context Kakuro example Section 2.2, represent variables X,
Z X = [x1 , . . . , x9 ], = [y1 , . . . , y9 ] Z = [z1 , . . . , z18 ] assume previous equi-propagation (on constraints) determined current equi-formula E0
specify integer variable Z = 4:

C=

new int(X, 0, 9) new int(Y, 0, 9) new int(Z, 0, 18)
int plus(X, Y, Z) int neq(X, Y)



Figure 8 illustrates, step-by-step, equi-propagation process C using ad-hoc rules
defined above. step corresponds application one defined ad-hoc
equi-propagation rules indicated label transition. stage illustrate
derived equations (top part) application (as unifier) state variables
X, Z (lower part).
319

fiMetodi, Codish, & Stuckey

c = ordered([x1 , . . . , xn ]) (new int)

replace
n1
true
x1 = 1
ordered([1, x2 . . . , xn ])
xn = 0
ordered([x1 , . . . , xn1 , 0])
, . . . , xn ])

xi = xi+1 ordered([x1 , . . . , xi ,
xi+1
Figure 9: Simplification rules new int (crossed elements removed).
summarize, let us come back Table 1. numbers presented two columns
headed BEE specify number variables remaining application ad-hoc equipropagation. also observe definition ad-hoc equi-propagation trivially
monotonic.

5. Constraint Model Partial Evaluation
Partial evaluation, together equi-propagation, second important component
approach compile constraint models CNF. Partial evaluation simplifying given constraint model view information becomes available due equipropagation. Typically, constraint simplification process, apply alternating steps
equi-propagation partial evaluation. Examples partial evaluation include constant
elimination removing constraints tautologies. section detail
partial evaluation rules apply five constraint types defined language
fragment presented Section 2.
(1) new int(I, c1 , c2 ) constraint specifies integer = [x1 , . . . , xn ] represented
order encoding particular corresponding bit sequence sorted (not
increasing). denote ordered([x1 , . . . , xn ]). Partial evaluation focuses
aspect constraint ignores bounds c1 , c2 specified constraint. table
Figure 9 specifies four simplification rules apply. first rule identifies tautologies,
second third rules remove leading ones trailing zeros, fourth removes
(one two) equated bits. figure, subsequent, crossed element
sequence, indicates removed sequence.
(2) simplification rules int neq constraint shown Figure 10(a) symmetric
exchanging role X . first two rules identify tautologies. third
rule X equal bit position i. corresponding bits
removed representation X , resulting shorter list bits
representations. last two rules removing leading ones trailing zeroes
illustrated following example.
Example 12. Figure 10(b) shows two steps partial evaluation, int neq constraint,
first removing leading ones, removing trailing zeroes.

320

fiBoolean Equi-propagation

c = int neq(X, )
X = [x1 , . . . , xn ] = [y1 , . . . , yn ]

replace
X = i, 6=
true
xi = yi
true
int neq(
x
[x1 , . . . ,
xi = yi
, . . . , xn ],
yi , . . . , yn ])
[y1 , . . . ,
int neq([1, xi+1 , . . . , xn ],
Xi2
[yi , yi+1 , . . . , yn ])
int neq([x1 , . . . , xi , 0],
Xi
[y1 , . . . , yi , yi+1 ])




int neq(
[x1 , . . . , x4 , 0, 0, 0], P.E

int neq
[1, 1, 1, y4 , . . . , y7 ])


int neq(
[x3 , x4 , 0, 0, 0], P.E

int neq
[1, y4 , . . . , y7 ])


int neq(
[x3 , x4 , 0],
[1, y4 , y5 ])

(a)

(b)

Figure 10: (a) Simplification rules int neq (b) example application.
c = allDiff([Z1 , . . . , Zn ])
Zi = [zi,1 , . . . , zi,m ] (1 n)

replace
n1
true
^ dom(Z1 )
allDiff([Z2 , . . . , Zn ])
dom(Zk ) =
k>1 [
|
dom(Zi )| = 2
allDiff([Z3 , . . . , Zn ])
i{1,2}

^
k

Zk 6=

allDiff(
[z1,1 , . . . ,
z1,i+1
, . . . , z1,m ]
...
[zn,1 , . . . ,
zn,i+1
, . . . , zn,m ])

Figure 11: Simplification rules allDiff

(3) Four rules simplifying allDiff constraints illustrated Figure 11. first,
detecting tautologies. second, identifies cases one integers
constraint (assume Z1 ) domain disjoint others. rule also captures
case Z1 constant. third rule removes Hall set size 2 (assume {Z1 , Z2 })
constraint. Note corresponding equi-propagation rule detects
values Z3 , . . . , Zn different values {Z1 , Z2 } next fourth rule
applies. fourth rule case none integers constraint take
certain value i. rule also captures case numbers leading
ones trailing zeroes. last two rules illustrated Example 14.
(4 & 5) simplification rules shown Figure 12 symmetric exchanging
role X . first two apply (at least) one X, Z constant.
already applied equi-propagation constraint, tautology. See
Example 13. last two rules apply remove leading ones trailing zeroes.
321

fiMetodi, Codish, & Stuckey

c = int plus(X, Y, Z) X = [x1 , . . . , xn ],
= [y1 , . . . , ym ], Z = [z1 , . . . , zn+m ]

replace
X=i
true
Z=k
true
int plus([xi+1 , . . . , xn ], Y,
X i, Z
[zi+1 , . . . , zn+m ])
int plus([x1 , . . . , xi ], Y,
X i, Z +
[z1 , . . . , zi+m ])

Figure 12: Simplification rules int plus.
(a) int plus(I1 , I2 , K)
(b) allDiff([I1 , I2 , I3 , I4 , I5 , I6 , I7 , I8 ])
(c) int array plus([I2 , I3 , I4 , I5 ], K)
Figure 13: Constraint Model Examples 1315
simplification rules int array plus constraint straightforward generalizations
ones int plus. See Example 15.
summarise rule based approach apply equi-propagation partial evaluation
present following sequence three examples focus simplification
three constraints given Figure 13 integer variables I1 , . . . , I8 defined
range 1 8 K = 14.
Example 13. Consider equi-propagation constraint (a) Figure 13 E0 specifies
K = 14:




k1 =1, . . . , k14 =1
k15 = 0, k16 = 0
I1 = [1, i1,2 , . . . , i1,8 ],
I2 = [1, i2,2 , . . . , i2,8 ],
K = [1, 1, . . . , 1, 0, 0]
| {z }



14

i1,2 =1, . . . , i1,6 =1,

i2,2 =1, . . . , i2,6 =1,


i1,7 =i2,8 , i1,8 =i2,7
I1 = [1, 1, 1, 1, 1, 1, i1,7 , i1,8 ],
I2 = [1, 1, 1, 1, 1, 1, i1,8 , i1,7 ],
K = [1, 1, . . . , 1, 0, 0]
| {z }
E1 = E0

E0 =

K=14


int plus

14

Given E1 , constraint tautology removed partial evaluation:



int plus(

P.E


[1, 1, 1, 1, 1, 1, i1,7 , i1,8 ],

int plus
[1, 1, 1, 1, 1, 1, i1,8 , i1,7 ], 14)



Example 14. Consider equi-propagation constraint (b) Figure 13 given E1
Example 13:
E1
I1 = [1, 1, 1, 1, 1, 1, i1,7 , i1,8 ],
I2 = [1, 1, 1, 1, 1, 1, i1,8 , i1,7 ]

i1,7 =i2,8 ,
i2,7 =i1,8


int neq

322

E2 = E1 {i1,7 =i1,8 }
I1 = [1, 1, 1, 1, 1, 1, i1,7 , i1,7 ],
I2 = [1, 1, 1, 1, 1, 1, i1,7 , i1,7 ]

fiBoolean Equi-propagation

Given E2 , equi-propagation rule allDiff detects {I1 , I2 } Hall set (where
two variables take values 6 8). adds E2 set equations, E 0 , specify
I3 , I4 , I5 , I6 , I7 , I8 6= 6, 8. result E3 = E2 E 0 result step gives
following bindings (where impact E 0 underlined):
I1
I2
I3
I4

= [1, 1, 1, 1, 1, 1, i1,7 , i1,7 ]
= [1, 1, 1, 1, 1, 1, i1,7 , i1,7 ]
= [1, i3,2 , i3,3 , i3,4 , i3,5 , i3,7 , i3,7 , 0]
= [1, i4,2 , i4,3 , i4,4 , i4,5 , i4,7 , i4,7 , 0]

I5
I6
I7
I8

= [1, i5,2 , i5,3 , i5,4 , i5,5 , i5,7 , i5,7 , 0]
= [1, i6,2 , i6,3 , i6,4 , i6,5 , i6,7 , i6,7 , 0]
= [1, i7,2 , i7,3 , i7,4 , i7,5 , i7,7 , i7,7 , 0]
= [1, i8,2 , i8,3 , i8,4 , i8,5 , i8,7 , i8,7 , 0]

Given E3 , partial evaluation constraint first removes Hall set:
P.E

[allDiff([I1 , I2 , I3 , I4 , I5 , I6 , I7 , I8 ])] [allDiff([I3 , I4 , I5 , I6 , I7 , I8 ])]
allDiff

applies remove three redundant bits underlying representation
remaining integer (which equal 0, 6, 8):











allDiff([
[1, i3,2 , i3,3 , i3,4 , i3,5 , i3,7 , i3,7 , 0],
[1, i4,2 , i4,3 , i4,4 , i4,5 , i4,7 , i4,7 , 0],
[1, i5,2 , i5,3 , i5,4 , i5,5 , i5,7 , i5,7 , 0],
[1, i6,2 , i6,3 , i6,4 , i6,5 , i6,7 , i6,7 , 0],
[1, i7,2 , i7,3 , i7,4 , i7,5 , i7,7 , i7,7 , 0],
[1, i8,2 , i8,3 , i8,4 , i8,5 , i8,7 , i8,7 , 0]])









P.E

allDifferent














allDiff([
[i3,2 , i3,3 , i3,4 , i3,5 , i3,7 ],
[i4,2 , i4,3 , i4,4 , i4,5 , i4,7 ],
[i5,2 , i5,3 , i5,4 , i5,5 , i5,7 ],
[i6,2 , i6,3 , i6,4 , i6,5 , i6,7 ],
[i7,2 , i7,3 , i7,4 , i7,5 , i7,7 ],
[i8,2 , i8,3 , i8,4 , i8,5 , i8,7 ]])












Example 15. Consider equi-propagation constraint (c) Figure 13 given E3
Example 14. rules apply derive decomposition int array plus
constraint int plus parts. dictate I3 , I4 , I5 5:
E3
I2
I3
I4
I5

= [1, 1, 1, 1, 1, 1, i1,7 , i1,7 ],
= [1, i3,2 , i3,3 , i3,4 , i3,5 , i3,7 , i3,7 , 0],
= [1, i4,2 , i4,3 , i4,4 , i4,5 , i4,7 , i4,7 , 0],
= [1, i5,2 , i5,3 , i5,4 , i5,5 , i5,7 , i5,7 , 0]


int array
plus

E4
I2
I3
I4
I5

= E3 {i3,7 =0, i4,7 =0, i5,7 =0}
= [1, 1, 1, 1, 1, 1, i1,7 , i1,7 ],
= [1, i3,2 , i3,3 , i3,4 , i3,5 , 0, 0, 0],
= [1, i4,2 , i4,3 , i4,4 , i4,5 , 0, 0, 0],
= [1, i5,2 , i5,3 , i5,4 , i5,5 , 0, 0, 0]

Applying partial evaluation simplifies constraint follows:







int array plus([
[1, 1, 1, 1, 1, 1, i1,7 , i1,7 ],
[1, i3,2 , i3,3 , i3,4 , i3,5 , 0, 0, 0],
[1, i4,2 , i4,3 , i4,4 , i4,5 , 0, 0, 0],
[1, i5,2 , i5,3 , i5,4 , i5,5 , 0, 0, 0]], 14 )







P.E


int array
plus








int array plus([
[i1,7 , i1,7 ],
[i3,2 , i3,3 , i3,4 , i3,5 ],
[i4,2 , i4,3 , i4,4 , i4,5 ],
[i5,2 , i5,3 , i5,4 , i5,5 ]], 5 )








summarize Examples 1315 observe initial constraint model 3 constraints
8 integers represented 56 bits. constraint simplification 2 constraints
remain 8 integers represented using 28 bits:
I1
I2
I3
I4

= [1, 1, 1, 1, 1, 1, i1,7 , i1,7 ]
= [1, 1, 1, 1, 1, 1, i1,7 , i1,7 ]
= [1, i3,2 , i3,3 , i3,4 , i3,5 , 0, 0, 0]
= [1, i4,2 , i4,3 , i4,4 , i4,5 , 0, 0, 0]

I5
I6
I7
I8
323

= [1, i5,2 , i5,3 , i5,4 , i5,5 , 0, 0, 0]
= [1, i6,2 , i6,3 , i6,4 , i6,5 , i6,7 , i6,7 , 0]
= [1, i7,2 , i7,3 , i7,4 , i7,5 , i7,7 , i7,7 , 0]
= [1, i8,2 , i8,3 , i8,4 , i8,5 , i8,7 , i8,7 , 0]

fiMetodi, Codish, & Stuckey

6. Compiling Constraints BEE
BEE (Ben-Gurion Equi-propagation Encoder) tool applies encode finite domain
constraint models CNF. BEE first introduced Metodi Codish (2012).
encoding process, BEE performs optimizations based equi-propagation partial
evaluation improve quality target CNF. BEE implemented (SWI) Prolog
applied conjunction CryptoMiniSAT solver (Soos, 2010)
Prolog interface (Codish, Lagoon, & Stuckey, 2008). CryptoMiniSAT offers direct support
xor clauses, BEE takes advantage feature. BEE downloaded (Metodi,
2012) one also find examples paper others.
source language BEE compiler also called BEE. constraint modeling
language similar FlatZinc (Nethercote et al., 2007), focus subset
language relevant finite domain constraint problems. Five constraint constructs
BEE language introduced Section 2.1. full language presented
Table 2.
BEE Boolean constants true false viewed (integer) values 1 0.
Constraints represented (a list of) Prolog terms. Boolean integer variables
represented Prolog variables, may instantiated simplifying constraints.
Table 2, X Xs (possibly subscripts) denote literal (a Boolean variable
negation) vector literals, (possibly subscript) denotes integer variable,
c (possibly subscript) denotes integer constant. right column
table brief explanations regarding constraints. table introduces 26 constraint
templates.
Constraints (1-2) variable declarations: Booleans integers. Constraint (3)
expresses Boolean integer value. Constraints (4-8) Boolean (and reified
Boolean) statements. special cases Constraint (5) bool array or([X1 , . . . , Xn ])
bool array xor([X1 , . . . , Xn ]) facilitate specification clauses xor clauses
(supported directly CryptoMiniSAT solver Soos, 2010). Constraint (8) specifies
sorting bit pair [X1 , X2 ] (decreasing order) results pair [X3 , X4 ]. basic
building block construction sorting networks (Batcher, 1968) used encode cardinality (linear Boolean) constraints compilation described Asn, Nieuwenhuis,
Oliveras, Rodrguez-Carbonell (2011) Codish Zazon-Ivry (2010). Constraints (9-14) integer relations operations. Constraints (15-20)
linear (Boolean, Pseudo Boolean, integer) operations. Constraints (21-26)
lexical orderings Boolean integer arrays.
main design choice BEE integer variables represented orderencoding. So, BEE suitable problems integer variables take small
medium sized values. compilation constraint model CNF using BEE goes
three phases.
1. Unary bit-blasting: integer variables (and constants) represented bit vectors
order-encoding.
2. Constraint simplification: three types actions applied: equi-propagation, partial
evaluation, decomposition constraints. Simplification applied repeatedly
rule applicable.
324

fiBoolean Equi-propagation

Declaring Variables
(1)
(2)
(3)

declare Boolean X
declare integer I, c1 c2
(X = 1) (X = 0)

new bool(X)
new int(I, c1 , c2 )
bool2int(X, I)

op {or, and, xor, iff}

Boolean (reified) Statements
(4)
(5)
(6)
(7)
(8)

bool eq(X1 , X2 ) bool eq(X1 , X2 )
bool array op([X1 , . . . , Xn ])
bool array op reif([X1 , . . . , Xn ], X)
bool op reif(X1 , X2 , X)
comparator(X1 , X2 , X3 , X4 )

X1 = X2 X1 = X2
X1 op X2 op Xn
X1 op X2 op Xn X
X1 op X2 X
sort([X1 , X2 ]) = [X3 , X4 ]

Integer relations (reified)
rel {leq, geq, eq, lt, gt, neq}
arithmetic
op {plus, times, div, mod, max, min}, op0 {plus, times, max, min}
(9)
(10)
(11)
(12)
(13)
(14)

int
int
int
int
int
int

rel(I1 , I2 )
rel reif(I1 , I2 , X)
array allDiff([I1 , . . . , ])
abs(I1 , I)
op(I1 , I2 , I)
array op0 ([I1 , . . . , ], I)

I1 rel I2

V1 rel I2 X
i<j Ii 6= Ij
|I1 | =
I1 op I2 =
I1 op0 op0 =

Linear Constraints
(15)
(16)
(17)
(18)
(19)
(20)

rel{leq, geq, eq, lt, gt}

bool array sum rel([X1 , . . . , Xn ], I)
bool array pb rel([c1 , . . . , cn ], [X1 , . . . , Xn ], I)
bool array sum modK([X1 , . . . , Xn ], c, I)
int array sum rel([I1 , . . . , ], I)
int array lin rel([c1 , . . . , cn ], [I1 , . . . , ], I)
int array sum modK([I1 , . . . , ], c, I)

( Xi ) rel
( ci Xi ) rel
(( Xi ) mod c) =
( Ii ) rel
( ci Ii ) rel
(( Ii ) mod c) =

Lexical Order
(21)
(22)
(23)
(24)
(25)
(26)

bool arrays lex(Xs1 , Xs2 )
bool arrays lexLt(Xs1 , Xs2 )
bool arrays lex reif(Xs1 , Xs2 , X)
bool arrays lexLt reif(Xs1 , Xs2 , X)
int arrays lex(Is1 , Is2 )
int arrays lexLt(Is1 , Is2 )

Xs1 precedes (leq) Xs2 lex order
Xs1 precedes (lt) Xs2 lex order
X Xs1 precedes (leq) Xs2 lex order
X Xs1 precedes (lt) Xs2 lex order
Is1 precedes (leq) Is2 lex order
Is1 precedes (lt) Is2 lex order

Table 2: Syntax BEE Constraints.
3. CNF encoding: best suited encoding technique applied simplified constraints.
Bit-blasting equi-propagation BEE follow general descriptions Sections 2.4 3.1. Bit-blasting implemented Prolog unification. declaration form new int(I, c1 , c2 ) triggers unification = [1, . . . , 1, Xc1 +1 , . . . , Xc2 ] (to
ease presentation assume integer variables represented positive interval
starting 0 limitation practice BEE also supports negatives
integers). BEE applies ad-hoc equi-propagators described Section 4. equality
form X = L (between variable literal constant) detected, equipropagation implemented unifying X L. unification applies occurrences
X sense propagates constraints involving X.
Decomposition replacing complex constraints (for example arrays)
simpler constraints (for example array elements). Consider, instance, constraint int array plus(As, Sum). decomposed list int plus constraints applying
straightforward divide conquer recursive definition. base case, As=[A]
325

fiMetodi, Codish, & Stuckey

c = allDiff ([Z1 , Z2 , Z3 , . . . , Zn ])
E
add c (E)
dom(Z1 )

/ dom(Zk ) (k > 1)

Z1 = authors

{i, j} dom(Zk ) =
(k > 2)

dom(Z1 ) {i, j}
dom(Z2 ) {i, j}
Z1 6= Z2
Zk 6= i, Zk 6= j
(k > 2)

Figure 14: Simplification rules allDiff .
constraint replaced constraint form int eq(A,Sum) equates bits
Sum, = [A1 , A2 ] replaced int plus(A1 , A2 , Sum). general
case split two halves, constraints generated sum halves,
additional int plus constraint introduced sum two sums.
another example, consider int plus(A1 , A2 , A) constraint. One approach, supported BEE, decomposes constraint odd-even merger (from context
odd-even sorting networks) (Batcher, 1968). Here, sorted sequences bits A1 A2
merged obtain sum A. results model O(n log n) comparator
constraints (and later encoding O(n log n) clauses). Another approach, also
supported BEE, decompose constraint encodes directly CNF
size O(n2 ), context so-called totalizers (Bailleux & Boufkhad, 2003). hybrid
approach, leaves choice BEE, depending size domains variables
involved. Finally, note user configure BEE fix way compiles
constraint (and others).
CNF encoding last phase compilation constraint model.
remaining simplified (bit-blasted) constraints encoded directly CNF. encodings
standard similar applied various tools. BEE encodings similar
applied Sugar (Tamura et al., 2009).
6.1 All-Different Constraint BEE
all-different constraint specifies set integer variables take different values
specified domains. constraint received much attention literature
(see example survey van Hoeve, 2001). BEE provides special treatment
constraint.
many applications, all-different constraints applied model special case
constraint permutation. Namely, [I1 , . . . , ] different may
take precisely n different values. BEE identifies special case applies two additional
ad-hoc equi-propagation rules case. table Figure 14 illustrates rules.
annotate constraint * emphasize detected
permutation. first rule case one integer (assume Z1 )
take value i. second rule case variables except two, assume Z1 ,
Z2 , cannot take two values, assume i, j. Now, constraint permutation,
326

fiBoolean Equi-propagation

determine Z1 Z2 must take two values j. illustrate second
rule consider following example.
Example 16. Consider constraint allDiff(I1 , . . . , I5 ) 5 integer variables taking values interval [0, 4] (exactly 5 values) E0 specifies I3 , I4 I5 cannot take
values 0 1. Therefore introduce equations restrict I1 I2 take
values 0 1, corresponding ad-hoc rule permutation applies:
E0
I1
I2
I3
I4
I5



x3,1 =1, x4,1 =1,
x5,1 =1, x3,2 =1,
=


x4,2 =1, x5,2 =1
= [x1,1 , . . . , x1,4 ],
= [x2,1 , . . . , x2,4 ],
= [1, 1, x3,3 , x3,4 ],
= [1, 1, x4,3 , x4,4 ],
= [1, 1, x5,3 , x5,4 ]

dom(Ik ) {0, 1}=
k>2




allDiff

E1
I1
I2
I3
I4
I5



x1,2 =0, . . . , x1,4 =0,
x2,2 =0, . . . , x2,4 =0
= E0


x1,1 =x2,1
= [x1,1 , 0, . . . , 0],
= [x1,1 , 0, . . . , 0],
= [1, 1, x3,3 , x3,4 ],
= [1, 1, x4,3 , x4,4 ],
= [1, 1, x5,3 , x5,4 ]

facilitate implementation ad-hoc equi-propagation all-different constraints,
BEE adopts dual representation integer variables occurring constraints combining order encoding the, so-called, direct encoding. essentially
encoding proposed Gent Nightingale (2004). declaring integer variable I, bit-blast order encoding applies corresponding unification
= [x1 , . . . , xn ]. encountering allDiff constraint, additional bit-blast
introduces I0 = [d0 , . . . , dn ] direct encoding, channeling formula channel(I, I0 )
introduced.
direct encoding unary representation I0 = [d0 , . . . , dn ] bit di true
I0 = i. So, exactly one bits takes value true. example, value
3 interval [0, 5] represented 6 bits [0, 0, 0, 1, 0, 0]. dual representation
following channeling formula captures relation two representations
integer variable = [x1 , . . . , xn ] I0 = [d0 , . . . , dn ].

channel([x1 , . . . , xn ], [d0 , . . . , dn ]) =

d0 = x1
dn = xn




n1
^

(di xi xi+1 )

i=1

Consider allDiff constraint integer variables take different values 0 n. constraint simplification, allDiff([I1 , . . . , Im ]) constraint
viewed direct encoding bit matrix row consists bits
[di0 , . . . , din ] Ii direct encoding. element dij true iff Ii takes value j.
j th column specifies Ii take value j hence, one variable
column may take value true. representation one main advantage:
direct encoding decompose allDiff([I1 , . . . , Im ]), conjunction n + 1 constraints, one column 0 j n, form bool array sum leq([d1j , . . . , dmj ], 1),
arc-consistent. soon di,j = 1 (Ii = j) di,j 0 = 0 (Ii 6= j 0 )
2
j0 6= j. contrast
fi order encoding alone decomposition O(m ) constraints
int neq(Ii , Ij ) fi < j
arc-consistent. illustrate advantage dual
encoding allDiff constraint Section 8.1.
327

fiMetodi, Codish, & Stuckey

:- use module(bee compiler, [bCompile/2]).
:- use module(sat solver, [sat/1]).
solve(Instance, Solution) :encode(Instance, Map, Constraints),
bCompile(Constraints, CNF),
sat(CNF),
decode(Map, Solution).

Figure 15: generic application BEE.

7. Using BEE
typical BEE application form depicted Figure 15 predicate solve/2
takes problem Instance provides Solution. specifics application
call encode/3 given Instance generates Constraints solve
together Map relating instance variables constraint variables. calls
bCompile/2 sat/1 compile constraints CNF solve applying SAT solver.
instance solution, SAT solver binds constraint variables accordingly.
Then, call decode/2, using Map, provides Solution terms Instance
variables. definitions encode/3 decode/2 application dependent provided user. predicates bCompile/2 sat/1 part tool provide
interface BEE underlying SAT solver.
7.1 Example BEE Application: Magic Graph Labeling
illustrate application BEE using Prolog modeling language solve graph
labeling problem. Graph labeling finding assignment integers vertices
edges graph subject certain conditions. Graph labellings introduced
60s hundreds papers wide variety related problems published
since then. See example survey Gallian (2011) 1200 references.
Graph labellings many applications. instance radars, X-ray crystallography,
coding theory, etc.
focus vertex-magic total labeling (VMTL) problem one
find graph G = (V, E) labeling one-to-one map V E {1, 2, . . . , |V | +
|E|} property sum labels vertex incident edges
constant K independent choice vertex. problem instance takes form
vmtl(G, K) specifying graph G constant K. context Figure 15,
query solve(vmtl(G, K), Solution) poses question: exist vmtl labeling
G magic constant K? binds Solution indicate labeling one exists,
unsat otherwise. Figure 16 illustrates example problem instance together
solution.
Figure 17 illustrates Prolog program implements encode/3 predicate
VMTL problem. call predicate declareInts/4 introduces constraints
declare integer variables vertex edge graph, generates
map. call predicate sumToK/5 introduces constraints require sum
labels vertex incident edges equals K. auxiliary predicate
328

fiBoolean Equi-propagation

Instance
Instance = vmtl(G, K),
G = (V, E),
V = [1, 2, 3, 4],
E = [(1, 2), (1, 3),
(2, 3), (3, 4)],
K = 14

Graph
4

2

36
666



Solution


1

V1
V2

V3
V4

= 4,
= 5,
= 1,
= 6,

E(1,2)
E(1,3)
E(2,3)
E(3,4)


= 7,
= 3,

= 2,
=8

Figure 16: VMTL instance solution.
encode(vmtl((Vs,Es),K),Map,Constraints):append(Vs,Es,VEs), length(VEs,N),
declareInts(VEs,N,Map,Constraints-Cs2),
sumToK(Vs,Es,Map,K,Cs2-Cs3),
getVars(VEs,Map,Vars),
Cs3=[int array allDiff(Vars)].
declareInts([], , ,Cs-Cs).
declareInts([ID|IDs],N,[(ID,X)|Map],[new int(X,1,N)|CsH]-CsT):declareInts(IDs,N,Map,CsH-CsT).
sumToK([], , , ,Cs-Cs).
sumToK([VID|Vs],Es,Map,K,[int array plus(Vars,K)|CsH]-CsT):findall((X,Y),(member((X,Y),Es),(X=VID ; Y=VID)),EsIDs),
getVars([VID|EsIDs],Map,Vars),
sumToK(Vs,Es,Map,K,CsH-CsT).
getVars([], ,[]).
getVars([ID|IDs],Map,[Var|Vars]):member((ID,Var),Map),
getVars(IDs,Map,Vars).

Figure 17: encode/3 predicate VMTL application BEE
Map
((1, 2), E1 ),(1, V1 ),
((1, 3), E2 ),(2, V2 ),
((2, 3), E3 ),(3, V3 ),
((3, 4), E4 ),(4, V4 )

Constraints
new int(V1 , 1, 8), new int(E1 , 1, 8), int array plus([V1 , E1 , E2 ], K),
new int(V2 , 1, 8), new int(E2 , 1, 8), int array plus([V2 , E1 , E3 ], K),
new int(V3 , 1, 8), new int(E3 , 1, 8), int array plus([V3 , E2 , E3 , E4 ], K),
new int(V4 , 1, 8), new int(E4 , 1, 8), int array plus([V4 , E4 ], K),
new int(K, 14, 14), allDiff([V1 , V2 , V3 , V4 , E1 , E2 , E3 , E4 ])

Figure 18: VMTL instance constraints map generated encode/3.

getVars/3 receives list identifiers (vertices edges) extracts corresponding
list integer variables map.
Given VMTL instance Figure 16, call predicate encode/3 Figure 17
generates map constraints detailed Figure 18.
329

fiMetodi, Codish, & Stuckey

Solving constraints Figure 18 binds Map follows, indicating solution
(in unary order encoding):


(1,
(2,
=
(3,
(4,

[1, 1, 1, 1, 0, 0, 0, 0]),
[1, 1, 1, 1, 1, 0, 0, 0]),
[1, 0, 0, 0, 0, 0, 0, 0]),
[1, 1, 1, 1, 1, 1, 0, 0]),

((1, 2),
((1, 3),
((2, 3),
((3, 4),



[1, 1, 1, 1, 1, 1, 1, 0]),
[1, 1, 1, 0, 0, 0, 0, 0]),
[1, 1, 0, 0, 0, 0, 0, 0]),
[1, 1, 1, 1, 1, 1, 1, 1])

Using BEE compile constraints Figure 18 generates CNF contains 301
clauses 48 Boolean variables. Encoding set constraints without applying
simplification rules generates larger CNF contains 642 clauses 97 Boolean
variables.
Section 8.3 report using BEE enables us solve interesting instances
VMTL problem previously solvable techniques.
7.2 BumbleBEE
BEE distribution includes also command line solver, call BumbleBEE.
BumbleBEE enables one specify BEE model input file line contains
single constraint model last line specifies type goal. BumbleBEE
reads input file, compiles constraint model CNF, solves CNF using
embedded CryptoMiniSAT solver (Soos, 2010) outputs set bindings declared
variables model (or message indicating constraints satisfiable).
Figure 19 contains left BumbleBEE input file VMTL instance
Figure 16 right BumbleBEE output, solution constraint
model. example, last line input file specifies goal solver.
options are:
1. solve satisfy: solve single satisfying assignment constraint model;
2. solve satisfy(c): solve (at most) c satisfying assignments constraint model
c integer value. c 0 option solve solutions.
3. solve minimize(I): solve solution minimizes value integer
variable I. solver outputs intermediate solutions (with decreasing values
I) encountered search minimum value I.
4. solve maximize(I): similar minimize, maximizes.
details examples found BEE distribution (Metodi & Codish,
2012).

8. Experiments
report experience applying BEE. appreciate ease use reader
encouraged view example encodings available tool (Metodi & Codish,
2012). experiments run Intel Core 2 Duo E8400 3.00GHz CPU 4GB memory
Linux (Ubuntu lucid, kernel 2.6.32-24-generic). BEE written Prolog run
330

fiBoolean Equi-propagation

Content BumbleBEE input file
new int(V1, 1, 8)
new int(V2, 1, 8)
new int(V3, 1, 8)
new int(V4, 1, 8)
new int(E1, 1, 8)
new int(E2, 1, 8)
new int(E3, 1, 8)
new int(E4, 1, 8)
int array plus([V1, E1, E2], 14)
int array plus([V2, E1, E3], 14)
int array plus([V3, E2, E3, E4], 14)
int array plus([V4, E4], 14)
int array allDiff([V1, V2, V3, V4, E1, E2, E3, E4])
solve satisfy

BumbleBEE output

V1 = 4
V2 = 5
V3 = 1
V4 = 6
E1 = 7
E2 = 3
E3 = 2
E4 = 8

==========

Figure 19: Solving VMTL instance using BumbleBEE.

using SWI Prolog v6.0.2 64-bits. Comparisons Sugar (v1.15.0) based use
identical constraint models, apply SAT solver (CryptoMiniSAT v2.5.1), run
machine. Times reported seconds.
8.1 Quasigroup Completion Problems
Quasigroup Completion Problem (QCP) proposed Gomes, Selman, Crato (1997)
constraint satisfaction benchmark, given n n board integer variables (in
range [1, n]) assigned integer values. task assign values
variables, column row contains value twice. constraint model
conjunction allDiff constraints. Ansotegui, del Val, Dotu, Fernandez, Manya
(2004) argue advantage direct encoding QCP.
consider 15 instances 2008 CSP competition.2 Table 3 considers three
settings: BEE dual encoding allDiff constraints, BEE using order
encoding (equivalent using int neq constraints instead allDiff), Sugar.
table shows: instance identifier (sat unsat), compilation time (comp) seconds,
clauses encoding (clauses), variables encoding (vars), SAT solving time
(SAT) seconds.
results indicate that: (1) Application BEE using dual representation
allDiff 38 times faster produces 20 times fewer clauses (in average)
using order-encoding alone (despite need maintain two encodings); (2) Without
dual representation, solving encodings generated BEE slightly faster
Sugar BEE still generates CNF encodings 4 times smaller (on average)
generated Sugar. Observe 3 instances found unsatisfiable BEE (indicated
2. http://www.cril.univ-artois.fr/CPAI08/. competition instances specified using binary disequalities, use model allDiff.

331

fiMetodi, Codish, & Stuckey

instance

25-264-0 sat
25-264-1 sat
25-264-2 sat
25-264-3 sat
25-264-4 sat
25-264-5 sat
25-264-6 sat
25-264-7 sat
25-264-8 sat
25-264-9 sat
25-264-10 unsat
25-264-11 unsat
25-264-12 unsat
25-264-13 unsat
25-264-14 unsat
Total

BEE (dual
comp clauses
(sec)
0.23 6509
0.20 7475
0.21 6531
0.21 6819
0.21 7082
0.21 7055
0.21 7712
0.21 7428
0.21 6603
0.21 6784
0.21 6491
0.12
1
0.16
1
0.12
1
0.23 5984

encoding)
vars
SAT
(sec)
1317
0.33
1508
3.29
1329
0.07
1374
0.83
1431
0.34
1431
3.12
1551
0.34
1496
0.13
1335
0.18
1350
0.19
1296
0.04
0
0.00
0
0.00
0
0.00
1210
0.07
8.93

BEE (order
comp clauses
(sec)
0.36 33224
0.30 34323
0.30 35238
0.29 32457
0.29 32825
0.30 33590
0.33 39015
0.30 36580
0.27 31561
0.27 35404
0.30 33321
0.28 37912
0.29 39135
0.29 35048
0.28 31093

encoding)
vars
SAT
(sec)
887
8.95
917
97.50
905
2.46
899
18.52
897
19.08
897
46.15
932
69.81
937
19.93
896
10.32
903
34.08
930
10.92
955
0.09
984
0.08
944
0.09
885
11.60
349.58

clauses

Sugar
vars

126733
127222
127062
127757
126777
126973
128354
127106
124153
128423
126999
125373
127539
127026
126628

10770
10798
10787
10827
10779
10784
10850
10794
10687
10853
10785
10744
10815
10786
10771

SAT
(sec)
34.20
13.93
8.06
44.03
85.92
41.04
12.67
7.01
9.69
38.80
57.75
0.47
0.57
0.56
15.93
370.63

Table 3: QCP results 25 25 instances 264 holes
CNF single clause variables). comment Sugar pre-processing
times higher BEE indicated table.
8.2 Word Design DNA
Problem 033 CSPLib seeks largest parameter n, exists
set n eight-letter words alphabet = {A, C, G, } following
properties: (1) word exactly 4 symbols {C, G}; (2) pair
distinct words differ least 4 positions; (3) every x, S: xR (the reverse
x) C (the word obtained replacing , C G, vice versa)
differ least 4 positions.
Mancini, Micaletto, Patrizi, Cadoli (2008) provide comparison several stateof-the-art solvers applied DNA word problem variety encoding techniques.
best reported result solution 87 DNA words, obtained 554 seconds, using
OPL (van Hentenryck, 1999) model lexicographic order break symmetry. Frutos,
Liu, Thiel, Sanner, Condon, Smith, Corn (1997) present strategy solve problem
four letters modeled bit-pairs [t, m]. eight-letter word
viewed combination t-part, [t1 , . . . , t8 ], bit-vector, m-part,
[m1 , . . . , m8 ], also bit-vector. authors report solution composed two pairs
(t-part m-part) sets3 [T1 , M1 ] [T2 , M2 ] |T1 | = 6, |M1 | = 16, |T2 | = 2,
|M2 | = 6. forms set (6 16) + (2 6) = 108 DNA words. Marc van Dongen
reports larger solution 112 words.4
Building approach described Frutos et al. (1997), pose conditions sets
t-parts m-parts, , Cartesian product =
satisfy requirements original problem. three conditions below,
required satisfy (10 ) (20 ), required satisfy (20 ) (30 ). set
3. notions t-part m-part slightly different ours.
4. See http://www.cs.st-andrews.ac.uk/~ianm/CSPLib/.

332

fiBoolean Equi-propagation

bit-vectors V , conditions are: (10 ) bit-vector V sums 4; (20 ) pair
distinct bit-vectors V differ least 4 positions; (30 ) pair bit-vectors
(not necessarily distinct) u, v V , uR (the reverse u) v C (the complement v)
differ least 4 positions. equivalent requiring (uR )C differs v
least 4 positions.
strategy model BEE encoding. instance takes form
dna(n1 , n2 ) signifying numbers bit-vectors, n1 n2 sets . Without
loss generality, impose, remove symmetries, lexicographically
ordered. solution Cartesian product = .
Using BEE, find, fraction second, sets t-parts size 14 m-parts
size 8. provides solution size 14 8 = 112 DNA word problem. Running
Comet (v2.0.1) find 112 word solution 10 seconds using model Hakan
Kjellerstrand.5 Using BEE, also prove exist set 15 t-parts (0.15
seconds), set 9 m-parts (4.47 seconds). facts unknown prior BEE.
Proving solution DNA word problem 112 words,
without restriction two part t-m strategy, still open problem.
8.3 Vertex Magic Total Labeling
MacDougall, Miller, Slamin, Wallis (2002) conjecture n vertex complete graph,
Kn , n 5 vertex magic total labeling magic constants specific range
values k, determined n. conjecture proved correct odd n verified
brute force n = 6. address cases n = 8 n = 10 involve 15 instances
(different values k) n = 8, 23 (different values k) n = 10. Starting
simple constraint model (illustrated example Figure 16), add additional
constraints exploit fact graphs symmetric: (1) assume edge
smallest label e1,2 ; (2) assume labels edges incident v1
ordered hence introduce constraints e1,2 < e1,3 < < e1,n ; (3) assume
label edge e1,3 smaller labels edges incident v2 (except e1,2 )
introduce constraints accordingly. setting BEE solve except 2 instances
4 hour timeout Sugar solve except 4.
Table 4 gives results 10 hardest instances K8 20 hardest instances K10
4 hour time-out. BEE compilation times order 0.5 sec/instance K8
2.5 sec/instance K10 . Sugar encoding times slightly larger. instances
indicated magic constant, k; columns BEE Sugar indicate SAT solving
times (in seconds). bottom two lines indicate average encoding sizes (numbers clauses
variables).
results indicate Sugar encodings (in average) 60% larger,
average SAT solving time BEE encodings 2 times faster (average excluding instances Sugar times-out).
address two VMTL instances solvable using BEE models described
(K10 magic labels 259 258), partition problem fixing values e1,2
e1,3 maintaining constraints. Analysis symmetry breaking
constraints indicates results 198 new instances two cases.
5. See http://www.hakank.org/comet/word_design_dna1.co.

333

fiMetodi, Codish, & Stuckey

instance
BEE
K8
k
SAT (sec)
143
1.26
142
10.14
141
7.64
140
14.68
139
25.60
138
12.99
137
22.91
136
14.46
135
298.54
134
331.80
Average CNF size:
clauses
248000
vars
5688

Sugar
SAT (sec)
2.87
1.62
2.94
6.46
6.67
2.80
298.58
251.82
182.90


instance
BEE
K10
k
SAT (sec)
277
5.31
276
7.11
275
13.57
274
4.93
273
45.94
272
22.74
271
7.35
270
6.03
269
5.20
268
94.44
267
88.51
266
229.80
265
1335.31
264
486.09
263
236.68
262
1843.70
261
2771.60
260
4873.99
259

258

Average CNF size:
clauses
1229000
vars
15529

402000
9370

Sugar
SAT (sec)
9.25
9.91
19.63
9.24
9.03
86.45
9.49
55.94
11.05
424.89
175.70
247.56
259.45
513.61
648.43
6429.25
7872.76



1966000
25688

Table 4: VMTL results K8 K10 (4 hour timeout)

original VMTL instance solved one 198 instances solved. So, solve
parallel. Fixing e1,2 e1,3 fuels compiler encodings considerably
smaller. instance k = 259 solved 1379.50 seconds e1,2 = 1 e1,3 = 6.
compilation time 2.09 seconds encoding consists 1 million clauses
15 thousand variables.
best knowledge, hard instances suite beyond reach
previous approaches program search magic labels. SAT based approach
presented Jager (2010) cannot handle these.6 comparison Sugar indicates
impact compiler.
8.4 Balanced Incomplete Block Designs
Problem 028 CSPlib (BIBD) instance defined 5-tuple positive
integers [v, b, r, k, ] requires partition v distinct objects b blocks
block contains k different objects, exactly r objects occur block, every two
distinct objects occur exactly blocks.
6. Personal communication (Gerold Jager), March 2012.

334

fiBoolean Equi-propagation

Figure 20: BIBD symmetry breaking.
naive model BIBD instance [v, b, r, k, ] introduces following constraints
v b Boolean incidence matrix: (1) exactly r ones row, (2) exactly k ones
column, (3) exactly ones scalar product two (different) rows.
model contain sufficient degree information trigger equipropagation process. order take advantage BEE simplifications added
symmetry breaking described Frisch, Jefferson, Miguel (2004) illustrated
Figure 20: row viewed sequence four parts . . . sizes , (r ),
(r ), (b 2r + ). first row fixed assigning parts B ones
(marked black) parts C zeros (marked white). second row
fixed assign parts C ones (marked black) parts B zeros
(marked white). third subsequent rows (marked gray), sum constraints decomposed summing part (A . . . D) summing results
follows: + B = , + C = , C + = r , B + = r . ensures
row contains exactly r ones scalar product first (and second) row
. denote constraint model SymB (for symmetry breaking).
instance
[v, b, r, k, ]
[7, 420, 180, 3, 60]
[7, 560, 240, 3, 80]
[12, 132, 33, 3, 6]
[15, 45, 24, 8, 12]
[15, 70, 14, 3, 2]
[16, 80, 15, 3, 2]
[19, 19, 9, 9, 4]
[19, 57, 9, 3, 1]
[21, 21, 5, 5, 1]
[25, 25, 9, 9, 3]
[25, 30, 6, 5, 1]
Total (sec)

comp
(sec)
1.65
3.73
0.95
0.51
0.56
0.81
0.23
0.34
0.02
0.64
0.10

BEE (SymB)
clauses
SAT
(sec)
698579
1.73
1211941 13.60
180238
0.73
116016
8.46
81563
0.39
109442
0.56
39931
0.09
113053
0.17
0
0.00
92059
1.33
24594
0.06
36.66

Sugar (SymB)
comp
clauses
SAT
(sec)
(sec)
12.01 2488136 13.24
11.74 2753113 36.43
83.37 1332241
7.09
4.24
466086

23.58
540089
1.87
64.81
623773
2.26
2.27
125976
0.49



31.91
3716
0.01
42.65
569007
8.52
16.02
93388
0.42
> 722.93

SatELite (SymB)
comp
clauses SAT
(sec)
(sec)
1.67
802576 2.18
2.73 1397188 5.18
1.18
184764 0.57
0.64
134146

1.02
79542 0.20
1.14
105242 0.35
0.4
44714 0.09
10.45
111869 0.14
0.01
0 0.00
1.01
97623 8.93
1.2
23828 0.05
> 219.14

Table 5: BIBD results (180 sec. timeout)
Table 5 shows results comparing BEE (compilation time, clauses encoding, SAT
solving time) Sugar using SymB model. also compare BEE SatELite (Een
335

fiMetodi, Codish, & Stuckey

& Biere, 2005), CNF minimizer, input SatELite CNF encoding
SymB model generated BEE without applying simplifications. compilation
time (comp) indicates SatELite pre-processing time. final row indicates total
compilation SAT solving time entire suite approach. cases
time measured seconds.
experiment indicates BEE generates significantly smaller CNF Sugar
affects SAT solving time. Moreover, Sugar compilation time extremely
long. comparing BEE SatELite see output CNF
similar size SatELite applied entire CNF, instances
compilation time significantly longer solving time.
instance
[v, b, r, k, ]
[7, 420, 180, 3, 60]
[7, 560, 240, 3, 80]
[12, 132, 33, 3, 6]
[15, 45, 24, 8, 12]
[15, 70, 14, 3, 2]
[16, 80, 15, 3, 2]
[19, 19, 9, 9, 4]
[19, 57, 9, 3, 1]
[21, 21, 5, 5, 1]
[25, 25, 9, 9, 3]
[25, 30, 6, 5, 1]
Total

BEE (SymB)
comp
SAT
1.65
1.73
3.73 13.60
0.95
0.73
0.51
8.46
0.56
0.39
0.81
0.56
0.23
0.09
0.34
0.17
0.02
0.00
0.64
1.33
0.10
0.06
36.66

[M06]
0.54
0.66
5.51

12.22
107.43
53.23

1.26


>900.00

Minion
SymB
1.36
1.77


1.42
13.40
38.30
1.71
0.67

1.37
>600.00

SymB+
0.42
0.52
1.76
75.87
0.31
0.35
0.31
0.35
0.15
0.92
0.31
81.24

Table 6: BIBD results, comparison Minion (times seconds; 180 sec. timeout).
Table 6 shows results comparing BEE using SymB model Minion constraint
solver (Gent, Jefferson, & Miguel, 2006). consider three different models Minion:
[M06] indicates results using BIBD model described Gent et al. (2006), SymB uses
model use SAT approach, SymB+ , enhanced symmetry breaking
model tricks applied also [M06] model. columns
timeouts show total times (for BEE includes compile time SAT solving). Note
using clever modeling problem improved also previous run-times
Minion.
experiment indicates BEE significantly faster Minion BIBD
models ([M06]). tailoring SymB model, Minion becomes competitive
ours.
8.5 Combining BEE SatELite
demonstrate impact combining BEE SatELite. describe experiments involving two benchmarks SatELite applied simplify output
BEE. idea first apply powerful, local, techniques, performed BEE.
reduces size CNF fast. apply SatELite takes global
considerations CNF whole. wish determine smaller, simplified,
336

fiBoolean Equi-propagation

CNF amenable simplification using SatELite. results indicate
although CNF size slightly decreased, solving times often increased, sometimes
drastically.
Tables 7 8 show results. tables four columns BEE
heading indicate: BEE compilation time, size encoding (clauses variables),
subsequent SAT solving time. Similarly, four columns SatELite heading
indicate application SatELite output BEE: SatELite processing time,
size resulting CNF (clauses variables), subsequent SAT solving time.
Table 7 illustrates results BIBD benchmark Section 8.4 Table 8, results
10 hardest VMTL instances K8 K10 described Section 8.3. Observe
applying SatELite output BEE decreases CNF size slightly
improve SAT solving time. fact, contrary, cases renders CNF
takes time solve. several cases, SAT solving time increases drastically
introduce timeout.
instance
[v, b, r, k, ]
[7, 420, 180, 3, 60]
[7, 560, 240, 3, 80]
[12, 132, 33, 3, 6]
[15, 45, 24, 8, 12]
[15, 70, 14, 3, 2]
[16, 80, 15, 3, 2]
[19, 19, 9, 9, 4]
[19, 57, 9, 3, 1]
[21, 21, 5, 5, 1]
[25, 25, 9, 9, 3]
[25, 30, 6, 5, 1]

comp
(sec)
1.65
3.73
0.95
0.51
0.56
0.81
0.23
0.34
0.02
0.64
0.10

BEE
clauses
vars
698579
1211941
180238
116016
81563
109442
39931
113053
0
92059
24594

41399
58445
31947
19507
19693
26223
9273
6576
0
22098
2160

SAT
(sec)
1.73
13.60
0.73
8.46
0.39
0.56
0.09
0.17
0.00
1.33
0.06

comp
(sec)
1.88
3.14
1.20
0.66
0.98
1.13
0.38
12.49
0.00
0.97
1.14

SatELite
clauses
vars
696914
1209788
179700
115938
78630
104760
39805
112314
0
91736
24028

38749
54043
28351
17642
15877
21116
7988
6230
0
18540
1926

SAT
(sec)
3.41
6.97
0.91

0.35
0.50
0.16
0.37
0.00
10.34
0.09

Table 7: BIBD results, BEE combined SatELite (180 sec. timeout)
results demonstrate application SatELite remove redundancies
CNF often non-beneficial. Presumably difference see application
SatELite CNF benchmarks results fact BEE produces highly
optimized CNF output, many CNF benchmarks significant inefficiency
original encoding. BEE removes variable CNF, also instantiates
variable, either constant equivalent variable, remove
potential propagations encoding, captured Theorem 9.

9. Conclusion
considerable body work CNF simplification techniques clear trade-off
amount reduction achieved invested time. approaches determine binary clauses implied CNF, certainly enough determine Boolean
equalities. problem determining binary clauses implied CNF
prohibitive SAT model may involve many (hundreds of) thousands variables.
337

fiMetodi, Codish, & Stuckey

instance

K8

K10

143
142
141
140
139
138
137
136
135
134
267
266
265
264
263
262
261
260
259
258

comp
(sec)
0.51
0.27
0.20
0.19
0.18
0.18
0.18
0.18
0.18
0.18
0.65
0.65
0.65
0.65
0.65
0.65
0.65
0.65
0.65
0.65

BEE
clauses
vars
248558
248414
248254
248078
247886
247678
247454
247214
246958
246686
1228962
1228660
1228338
1227996
1227634
1227252
1226850
1226428
1225986
1225524

5724
5716
5708
5700
5692
5684
5676
5668
5660
5652
15529
15529
15529
15529
15529
15529
15529
15529
15529
15529

SAT
(sec)
1.26
10.14
7.64
14.68
25.6
12.99
22.91
14.46
298.54
331.8
88.51
229.8
1335.31
486.09
236.68
1843.7
2771.6
4873.99



comp
(sec)
2.60
2.59
2.59
2.60
2.59
2.60
2.59
2.59
2.58
2.59
3.02
3.01
3.02
3.02
3.01
3.02
3.04
3.02
3.03
3.01

SatELite
clauses
vars
248250
248107
247947
247771
247579
247371
247147
246907
246651
246379
1228368
1228066
1227744
1227402
1227040
1226658
1226256
1225834
1225392
1224930

5452
5445
5437
5429
5421
5413
5405
5397
5389
5381
14990
14990
14990
14990
14990
14990
14990
14990
14990
14990

SAT
(sec)
0.98
3.22
32.81
3.50
6.18
12.18
77.16
97.69
705.48

430.00
259.55
540.48
63.74
1008.06
1916.73





Table 8: VTML results, BEE combined SatELite (4 hour timeout)

Typically implied binary clauses determined, visible
unit propagation. trade-off regulated choice techniques applied infer
binary clauses, considering power cost. See example work Een Biere
(2005) references therein. also approaches (Li, 2003) detect use
Boolean equalities run-time, complementary approach.
approach, beast tamed introducing notion locality.
consider full CNF. Instead, maintaining original representation, conjunction
constraints, viewed Boolean formula, apply powerful reasoning techniques
separate parts model maintain efficient pre-processing.
end, introduce BEE, compiler follows approach encode finite
domain constraints CNF. Applying optimizations based ad-hoc equi-propagation
partial evaluation rules high level view problem allows us simplify problem
aggressively possible CNF representation. resulting CNF models
significantly smaller resulting straight translation.
well-understood making CNF smaller ultimate goal: often smaller
CNFs harder solve. Indeed, one often introduces redundancies improve SAT
encodings: removing counterproductive. experience BEE reduces
size encoding way productive subsequent SAT solving.
particular, removing variables determined compile time definitely
equal (or definitely different) solution.
338

fiBoolean Equi-propagation

BEE uses ad-hoc equi-propagation partial evaluation rules keeps compilation
times typically small (measured seconds) even instances result several millions
CNF clauses. reduction SAT solving time larger orders magnitude.
Hence, believe Boolean equi-propagation makes important contribution
encoding CSPs SAT.
BEE currently tuned represent integers order encoding. Ongoing work
aims extend BEE binary additional number representations mixed radix
bases considered Een Sorensson (2006) Codish, Fekete, Fuhs,
Schneider-Kamp (2011).
Acknowledgments
thank Vitaly Lagoon many insightful discussions concerning research.
NICTA funded Australian Government represented Department
Broadband, Communications Digital Economy Australian Research Council ICT Centre Excellence Program.

References
Ansotegui, C., del Val, A., Dotu, I., Fernandez, C., & Manya, F. (2004). Modeling choices
quasigroup completion: SAT vs. CSP. McGuinness, D. L., & Ferguson, G. (Eds.),
AAAI, pp. 137142, San Jose, California, USA. AAAI Press / MIT Press.
Asn, R., Nieuwenhuis, R., Oliveras, A., & Rodrguez-Carbonell, E. (2011). Cardinality
networks: theoretical empirical study. Constraints, 16 (2), 195221.
Bagnara, R., & Schachte, P. (1998). Factorizing equivalent variable pairs ROBDDbased implementations Pos. Haeberer, A. M. (Ed.), Algebraic Methodology
Software Technology, 7th International Conference, AMAST 98, Amazonia, Brasil,
January 4-8, 1999, Proceedings, Vol. 1548 Lecture Notes Computer Science, pp.
471485.
Bailleux, O., & Boufkhad, Y. (2003). Efficient CNF encoding Boolean cardinality constraints. Rossi, F. (Ed.), CP, Vol. 2833 LNCS, pp. 108122, Kinsale, Ireland.
Springer.
Barrett, C., Stump, A., & Tinelli, C. (2010). Satisfiability Modulo Theories Library
(SMT-LIB). www.SMT-LIB.org.
Batcher, K. E. (1968). Sorting networks applications. AFIPS Spring Joint
Computing Conference, Vol. 32 AFIPS Conference Proceedings, pp. 307314, Atlantic City, NJ, USA. Thomson Book Company, Washington D.C.
Bessiere, C., Katsirelos, G., Narodytska, N., & Walsh, T. (2009). Circuit complexity
decompositions global constraints. Proceedings IJCAI 2009, pp. 412418.
Cadoli, M., & Schaerf, A. (2005). Compiling problem specifications SAT. Artificial
Intelligence, 162 (1-2), 89120.
339

fiMetodi, Codish, & Stuckey

Codish, M., Fekete, Y., Fuhs, C., & Schneider-Kamp, P. (2011). Optimal base encodings
pseudo-Boolean constraints. Abdulla, P. A., & Leino, K. R. M. (Eds.), TACAS,
Vol. 6605 Lecture Notes Computer Science, pp. 189204. Springer.
Codish, M., Lagoon, V., & Stuckey, P. J. (2008). Logic programming satisfiability.
TPLP, 8 (1), 121128.
Codish, M., & Zazon-Ivry, M. (2010). Pairwise cardinality networks. Clarke, E. M., &
Voronkov, A. (Eds.), LPAR (Dakar), Vol. 6355 Lecture Notes Computer Science,
pp. 154172. Springer.
Coudert, O., & Madre, J. C. (1990). unified framework formal verification
sequential circuits. ICCAD, pp. 126129.
Crawford, J. M., & Baker, A. B. (1994). Experimental results application satisfiability algorithms scheduling problems. Hayes-Roth, B., & Korf, R. E. (Eds.),
AAAI, Vol. 2, pp. 10921097, Seattle, WA, USA. AAAI Press / MIT Press.
Een, N., & Biere, A. (2005). Effective preprocessing SAT variable clause
elimination. Bacchus, F., & Walsh, T. (Eds.), SAT, Vol. 3569 Lecture Notes
Computer Science, pp. 6175. Springer.
Een, N., & Sorensson, N. (2003). extensible SAT-solver. Giunchiglia, E., & Tacchella, A. (Eds.), SAT, Vol. 2919 Lecture Notes Computer Science, pp. 502518.
Springer.
Een, N., & Sorensson, N. (2006). Translating pseudo-Boolean constraints SAT. JSAT,
2 (1-4), 126.
Frisch, A. M., Jefferson, C., & Miguel, I. (2004). Symmetry breaking prelude implied
constraints: constraint modeling pattern. Proc. 16th Euro. Conf. AI, 171175,
pp. 171175. Press.
Frutos, A. G., Liu, Q., Thiel, A. J., Sanner, A. M. W., Condon, A. E., Smith, L. M., &
Corn, R. M. (1997). Demonstration word design strategy DNA computing
surfaces. Journal Nucleic Acids Research, 25 (23), 47484757.
Gallian, J. A. (2011). dynamic survey graph labeling. Electronic Journal
Combinatorics, 18.
Gavanelli, M. (2007). log-support encoding CSP SAT. Bessiere, C. (Ed.),
CP, Vol. 4741 LNCS, pp. 815822, Providence, RI, USA. Springer.
Gelder, A. V. (2005). Toward leaner binary-clause reasoning satisfiability solver. Ann.
Math. Artif. Intell., 43 (1), 239253.
Gent, I. P., Jefferson, C., & Miguel, I. (2006). Minion: fast scalable constraint solver.
Brewka, G., Coradeschi, S., Perini, A., & Traverso, P. (Eds.), ECAI, Vol. 141
Frontiers Artificial Intelligence Applications, pp. 98102. IOS Press.
Gent, I. P., & Nightingale, P. (2004). new encoding alldifferent SAT. Proceedings
3rd International Workshop Modeling Reformulating Constraint Satisfaction Problems, http://www-users.cs.york.ac.uk/frisch/Reformulation/04/
proceedings.pdf.
340

fiBoolean Equi-propagation

Gomes, C. P., Selman, B., & Crato, N. (1997). Heavy-tailed distributions combinatorial
search. Smolka, G. (Ed.), CP, Vol. 1330 LNCS, pp. 121135. Springer.
Heule, M., Jarvisalo, M., & Biere, A. (2011). Efficient CNF simplification based binary
implication graphs. Sakallah, K. A., & Simon, L. (Eds.), SAT, Vol. 6695 Lecture
Notes Computer Science, pp. 201215. Springer.
Huang, J. (2008). Universal Booleanization constraint models. CP2008, Vol. 5202
Lecture Notes Computer Science, pp. 144158.
Jager, G. (2010). effective SAT encoding magic labeling. Faigle, U., Schrader, R.,
& Herrmann, D. (Eds.), CTW, pp. 97100.
Li, C. (2003). Equivalent literal propagation DLL procedure. Discrete Applied
Mathematics, 130 (2), 251276.
MacDougall, J., Miller, M., Slamin, M., & Wallis, W. (2002). Vertex-magic total labelings
graphs. Utilitas Mathematica, 61, 321.
Mancini, T., Micaletto, D., Patrizi, F., & Cadoli, M. (2008). Evaluating ASP commercial
solvers CSPLib. Constraints, 13 (4), 407436.
Manthey, N. (2012). Coprocessor 2.0 - flexible CNF simplifier - (tool presentation).
Cimatti, A., & Sebastiani, R. (Eds.), SAT, Vol. 7317 Lecture Notes Computer
Science, pp. 436441. Springer.
Metodi, A. (2012). BEE. http://amit.metodi.me/research/bee/.
Metodi, A., & Codish, M. (2012). Compiling finite domain constraints SAT BEE.
TPLP, 12 (4-5), 465483.
Metodi, A., Codish, M., Lagoon, V., & Stuckey, P. J. (2011). Boolean equi-propagation
optimized SAT encoding. Lee, J. H.-M. (Ed.), CP, Vol. 6876 LNCS, pp. 621636.
Springer.
Nethercote, N., Stuckey, P. J., Becket, R., Brand, S., Duck, G. J., & Tack, G. (2007).
Minizinc: Towards standard CP modeling language. Bessiere, C. (Ed.), CP2007,
Vol. 4741 Lecture Notes Computer Science, pp. 529543, Providence, RI, USA.
Springer-Verlag.
Somenzi, F. (2009). CUDD: Colorado University Decision Diagram package. (Online,
accessed 13 April 2011). http://vlsi.colorado.edu/~fabio/CUDD/.
Soos, M. (2010). CryptoMiniSAT, v2.5.1. http://www.msoos.org/cryptominisat2.
Tamura, N., Taga, A., Kitagawa, S., & Banbara, M. (2009). Compiling finite linear CSP
SAT. Constraints, 14 (2), 254272.
Tarjan, R. (1975). Efficiency good linear set union algorithm. JACM, 22 (2),
215225.
van Hentenryck, P. (1999). OPL Optimization Programming Language. MIT Press.
van Hoeve, W. J. (2001). alldifferent constraint: survey.. CoRR:http://arxiv.org/
abs/cs.PL/0105015.

341

fiJournal Artificial Intelligence Research 46 (2013) 89-127

Submitted 03/12; published 01/13

Automatic Aggregation Joint Modeling
Aspects Values
Christina Sauper
Regina Barzilay

csauper@csail.mit.edu
regina@csail.mit.edu

Computer Science Artificial Intelligence Laboratory
Massachusetts Institute Technology
32 Vassar St.
Cambridge, 02139 USA

Abstract
present model aggregation product review snippets joint aspect identification sentiment analysis. model simultaneously identifies underlying set
ratable aspects presented reviews product (e.g., sushi miso Japanese
restaurant) determines corresponding sentiment aspect. approach
directly enables discovery highly-rated inconsistent aspects product. generative model admits efficient variational mean-field inference algorithm. also easily
extensible, describe several modifications effects model structure
inference. test model two tasks, joint aspect identification sentiment analysis set Yelp reviews aspect identification alone set medical summaries.
evaluate performance model aspect identification, sentiment analysis,
per-word labeling accuracy. demonstrate model outperforms applicable
baselines considerable margin, yielding 32% relative error reduction aspect
identification 20% relative error reduction sentiment analysis.

1. Introduction
Online product reviews become increasingly valuable influential source information consumers. ability explore range opinions allows consumers
form general opinion product gather information positive negative
aspects (e.g., packaging battery life). However, reviews added time,
problem information overload gets progressively worse. example, hundreds
reviews restaurant, consumers read handful making decision.
work, goal summarize large number reviews discovering
informational product aspects associated user sentiment.
address need, online retailers often use simple aggregation mechanisms represent spectrum user sentiment. Many sites, Amazon, simply present
distribution user-assigned star ratings, approach lacks reasoning
products given rating. retailers use breakdowns specific
predefined domain-specific aspects, food, service, atmosphere restaurant.
breakdowns continue assist effective aggregation; however, aspects
predefined, generic particular domain explanation one aspect rated well poorly. Instead, truly informative aggregation,
c
2013
AI Access Foundation. rights reserved.

fiSauper & Barzilay

product needs assigned set fine-grained aspects specifically tailored
product.
goal work provide mechanism effective unsupervised content
aggregation able discover specific, fine-grained aspects associated values. Specifically,
represent data set collection entities; instance, represent
products domain online reviews. interested discovering fine-grained
aspects entity (e.g., sandwiches dessert restaurant). Additionally, would
like recover value associated aspect (e.g., sentiment product reviews).
summary input output found Figure 1. input consists short
text snippets multiple reviews several products. restaurant domain,
Figure 1, restaurants. assume snippet opinion-bearing
discusses one aspects relevant particular product. output
consists set dynamic (i.e., pre-specified) aspects product, snippets labeled
aspect discuss, sentiment values snippet individually
aspect whole. Figure 1, aspects identified Tasca Spanish Tapas include
chicken, dessert, drinks, snippets labeled aspects describe
correct polarity.
One way approach problem treat multi-class classification problem.
Given set predefined domain-specific aspects, would fairly straightforward
humans identify aspect particular snippet describes. However, task
discovering fine-grained entity-specific aspects, way know priori
aspects may present across entire data set provide training data each; instead,
must select aspects dynamically. Intuitively, one potential solution cluster
input snippets, grouping lexically similar without prior knowledge
aspects represent. However, without knowledge words represent
aspect given snippet, clusters may align ones useful cross-review analysis.
Consider, example, two clusters restaurant review snippets shown Figure 2.
clusters share many words among members, first describes
coherent aspect cluster, namely drinks aspect. snippets second cluster
discuss single product aspect, instead share expressions sentiment.
successfully navigate challenge, must distinguish words indicate aspect, words indicate sentiment, extraneous words neither.
aspect identification sentiment analysis, crucial know words within
snippet relevant task. Distinguishing straightforward, however.
work sentiment analysis relies predefined lexicon WordNet provide
hints, way anticipate every possible expression aspect sentiment,
especially user-generated data (e.g., use slang deeeeeee-lish delicious).
lieu explicit lexicon, attempt use information proxy,
part speech; example, aspect words likely nouns, value words
likely adjectives. However, show later paper, additional information
sufficient tasks hand.
Instead, propose approach analyze collection product review snippets
jointly induce set learned aspects, respective value (e.g., sentiment).
capture idea using generative Bayesian topic model set aspects
corresponding values represented hidden variables. model takes collection
90

fiAutomatic Aggregation Joint Modeling Aspects Values

Input

Output

Tasca Spanish Tapas

Tasca Spanish Tapas

Review 1
chicken cooked perfectly
dessert good

Chicken
+ chicken cooked perfectly
chicken tough tasty
+ Moist delicious chicken

Review 2
red wines cheap
excellent creme brulee
Review 3
used frozen small shrimp
chicken tough tasty
Pitcher sangria pretty good

Douzo Sushi Bar
Review 1
sushi creative pretty good
ponzu overpowering
Review 2
Real wasabi thats fresh!
torched roll tasted rather bland


Dessert
+ dessert good
+ excellent creme brulee





Drinks
red wines cheap
+ Pitcher sangria pretty good

Douzo Sushi Bar
Sushi
+ sushi creative pretty good
torched roll tasted rather bland
Condiments
ponzu overpowering
+ Real wasabi thats fresh!



Figure 1: example desired input output system restaurant
domain. input consists collection review snippets several restaurants.
output aggregation snippets aspect (e.g., chicken dessert) along
associated sentiment snippet. Note input data completely unannotated;
information given snippets describe restaurant.

snippets input explains observed text arises latent variables,
thereby connecting text fragments corresponding aspects values.
Specifically, begin defining sets sentiment word distributions aspect word
distributions. expect types sentiment words consistent across
products (e.g., product may labeled great terrible), allow positive
negative sentiment word distributions shared across products.
hand, case restaurant reviews similar domains, aspect words expected
quite distinct products. Therefore, assign product set aspect
word distributions. addition word distributions, model takes account
several factors. First, model idea particular aspect product
underlying quality; is, already 19 snippets praising particular aspect,
likely 20th snippet positive well. Second, account common
patterns language using transition distribution types words. example,
common see pattern Value Aspect, phrases like great pasta.
Third, model distributions parts speech type distribution.
91

fiSauper & Barzilay

Coherent aspect cluster
+

martinis
good.
:::::::::
drinks

wine :::::::::
martinis - tasty.
:::::::
:::::

-

wine
list pricey.
:::::::::
:::::
wine:::::::::::
selection horrible.

Incoherent aspect cluster

+

sushi :::::
best :::::
Ive :::::
ever:::::
had.
Best
paella
Id
ever
had.
:::::
::::::::::::::
fillet best
steak wed ever had.
:::::::::::::::::::::::::::
best
soup
Ive
ever had.
::::::::::::::::::::::::::

Figure 2: Example clusters restaurant review snippets generated lexical clustering
algorithm; words relevant clustering highlighted. first cluster represents coherent aspect underlying product, namely drinks aspect. latter cluster simply
shares common sentiment expression represent snippets discussing
product aspect. work, aim produce first type aspect cluster along
corresponding values.

covers intuition aspect words frequently nouns, whereas value words often
adjectives. describe factors model whole detail Section 4.
formulation provides several advantages: First, model require set
predefined aspects. Instead, capable assigning latent variables discover
appropriate aspects based data. Second, joint analysis aspect value
allows us leverage several pieces information determine words relevant
aspect identification used sentiment analysis, including part
speech global entity-specific distributions words. Third, Bayesian model
admits efficient mean-field variational inference procedure parallelized
run quickly even large numbers entities snippets.
evaluate approach domain restaurant reviews. Specifically, use set
snippets automatically extracted restaurant reviews Yelp. collection consists
average 42 snippets 328 restaurants Boston area, representing
wide spectrum opinions several aspects restaurant. demonstrate
model accurately identify clusters review fragments describe
aspect, yielding 32.5% relative error reduction (9.9 absolute F1 ) standalone clustering
baseline. also show model effectively identify snippet sentiment,
19.7% relative error reduction (4.3% absolute accuracy) applicable baselines. Finally,
test models ability correctly label aspect sentiment words, discovering
aspect identification high-precision, sentiment identification highrecall.
Additionally, apply slimmed-down version model focuses exclusively
aspect identification set lab- exam-related snippets medical summaries
provided Pediatric Environmental Health Clinic (PEHC) Childrens Hospital
Boston. summaries represent concise overviews patient information par92

fiAutomatic Aggregation Joint Modeling Aspects Values

ticular visit, relayed PEHC doctor childs referring physician. model
achieves 7.4% (0.7 absolute F1 ) standalone clustering baseline.
remainder paper structured follows. Section 2 compares work
previous work aspect identification sentiment analysis. Section 3 describes
specific problem formulation task setup concretely. Section 4 presents details
full model various model extensions, Section 5 describes inference procedure necessary adjustments extension. details data sets,
experimental formulation, results presented Section 6. summarize findings
consider directions future work Section 7. code data used paper
available online http://groups.csail.mit.edu/rbg/code/review-aggregation.

2. Related Work
work falls area multi-aspect sentiment analysis. section, first
describe approaches toward document-level sentence-level sentiment analysis (Section
2.1), provide foundation future work, including own. Then, describe
three common directions multi-aspect sentiment analysis; specifically, use
data-mining fixed-aspect analysis (Section 2.2.1), incorporate sentiment
analysis multi-document summarization (Section 2.2.2), finally, focused
topic modeling additional sentiment components (Section 2.2.3).
2.1 Single-Aspect Sentiment Analysis
Early sentiment analysis focused primarily identification coarse document-level sentiment (Pang, Lee, & Vaithyanathan, 2002; Turney, 2002; Pang & Lee, 2008). Specifically,
approaches attempted determine overall polarity documents. approaches included rule-based machine learning approaches: Turney (2002) used
rule-based method extract potentially sentiment-bearing phrases compared
sentiment known-polarity words, Pang et al. (2002) used discriminative
methods features unigrams, bigrams, part-of-speech tags, word position
information.
document-level sentiment analysis give us overall view opinion, looking individual sentences within document yields fine-grained analysis.
work sentence-level sentiment analysis focuses first identifying sentiment-bearing sentences determining polarity (Yu & Hatzivassiloglou, 2003; Dave, Lawrence,
& Pennock, 2003; Kim & Hovy, 2005, 2006; Pang & Lee, 2008). identification
sentiment-bearing sentences polarity analysis performed supervised
classifiers (Yu & Hatzivassiloglou, 2003; Dave et al., 2003) similarity known text (Yu
& Hatzivassiloglou, 2003; Kim & Hovy, 2005), measures based distributional
similarity using WordNet relationships.
recognizing connections parts document, sentiment analysis
improved (Pang & Lee, 2004; McDonald, Hannan, Neylon, Wells, & Reynar, 2007;
Pang & Lee, 2008). Pang Lee (2004) leverage relationship sentences
improve document-level sentiment analysis. Specifically, utilize subjectivity
individual sentences information strength connection sentences
min cut formulation provide better sentiment-focused summaries text. McDonald
93

fiSauper & Barzilay

et al. (2007) examine different connection, instead constructing hierarchical model
sentiment sentences documents. model uses complete labeling
subset data learn generalized set parameters improve classification accuracy
document-level sentence-level.
none approaches attempt identify aspects analyze sentiment
aspect-based fashion, intuitions provide key insight approaches take
work. example, importance distinguishing opinion sentences follows
intuition necessity identifying sentiment-bearing words within snippet.
2.2 Aspect-Based Sentiment Analysis
Following work single-aspect document-level sentence-level sentiment analysis
came intuition modeling aspect-based (also called feature-based) sentiment review analysis. divide approaches roughly three types systems based
techniques: systems use fixed-aspect approaches data-mining techniques
aspect selection sentiment analysis, systems adapt techniques multi-document
summarization, systems jointly model aspect sentiment probabilistic
topic models. Here, examine avenue work relevant examples contrast
work.
2.2.1 Data-Mining Fixed-Aspect Techniques Sentiment Analysis
One set approaches toward aspect-based sentiment analysis follow traditional techniques data mining (Hu & Liu, 2004; Liu, Hu, & Cheng, 2005; Popescu, Nguyen, &
Etzioni, 2005). systems may operate full documents snippets, generally require rule-based templates additional resources WordNet identify
aspects determine sentiment polarity. Another approach fix predetermined
relevant set aspects, focus learning optimal opinion assignment
aspects (Snyder & Barzilay, 2007). Below, summarize approach compare
contrast work.
One set work relies combination association mining rule-based extraction
nouns noun phrases aspect identification. Hu Liu (2004) Liu et al. (2005)
developed three-step system: First, initial aspects selected association miner
pruned series rules. Second, related opinions aspect identified
rule-based fashion using word positions, polarity determined WordNet
search based set seed words. Third, additional aspects identified similar
fashion based position selected polarity words. steps, part-ofspeech information provides key role extraction rules. later work,
additional component identify implicit aspects deterministic fashion; e.g., heavy
maps deterministically <weight> (Liu et al., 2005). task similar
utilize part-of-speech information important feature well, additionally
leverage distributional information identify aspects sentiment. Furthermore,
avoid reliance WordNet predefined rule mappings order preserve
generality system. Instead, joint modeling allows us recover relationships
without need additional information.
94

fiAutomatic Aggregation Joint Modeling Aspects Values

approaches also rely WordNet relationships identify sentiment
polarity, also aspects, using parts properties particular product class.
Popescu et al. (2005) first use relations generate set aspects given
product class (e.g., camera). Following that, apply relaxation labeling sentiment
analysis. procedure gradually expands sentiment individual words aspects
sentences, similar Cascade pattern mentioned work McDonald et al. (2007).
Like system Liu et al. (2005), system requires set manual rules several
outside resources. model require seed words, require
manual rules additional resources due joint formulation.
separate direction work relies predefined aspects focusing improvement
sentiment analysis prediction. Snyder Barzilay (2007) define set aspects specific
restaurant domain. Specifically define individual rating model aspect,
plus overall agreement model attempts determine whether resulting ratings
agree disagree. models jointly trained supervised fashion using
extension PRanking algorithm (Crammer & Singer, 2001) find best overall
star rating aspect. problem formulation differs significantly work
several dimensions: First, desire refined analysis using fine-grained aspects
instead coarse predefined features. Second, would like use little supervised
training data possible, rather supervised training required PRanking
algorithm.
work, attempt capture intuitions approaches reducing
need outside resources rule-based components. example, rather supplying
rule-based patterns extraction aspect sentiment, instead leverage distributional patterns across corpus infer relationships words different types.
Likewise, rather relying WordNet relationships synonymy, antonymy, hyponymy, hypernymy (Hu & Liu, 2004; Liu et al., 2005; Popescu et al., 2005), bootstrap
model small set seed words.
2.2.2 Multi-Document Summarization Application Sentiment
Analysis
Multi-document summarization techniques generally look repetition across documents
signal important information (Radev & McKeown, 1998; Barzilay, McKeown, & Elhadad,
1999; Radev, Jing, & Budzikowska, 2000; Mani, 2001). aspect-based sentiment analysis,
work focused augmenting techniques additional components sentiment
analysis (Seki, Eguchi, Kanodo, & Aono, 2005, 2006; Carenini, Ng, & Pauls, 2006; Kim &
Zhai, 2009). general, end goal approaches task forming coherent text
summaries using either text extraction natural language generation. Unlike work,
many approaches explicitly identify aspects; instead, extracted
repeated information. Additionally, model explicitly looks connection
content sentiment, rather treating secondary computation
information selected.
One technique incorporating sentiment analysis follows previous work identification opinion-bearing sentences. Seki et al. (2005, 2006) present DUC summarization
95

fiSauper & Barzilay

systems designed create opinion-focused summaries task topics.1 system,
employ subjectivity component using supervised SVM lexical features, similar
work Yu Hatzivassiloglou (2003) Dave et al. (2003). component
used identify subjective sentences and, work Seki et al. (2006), polarity,
task sentences selected response summary. However, like
previous work unlike task, aspect-based analysis summarization
task. also fully supervised, relying hand-annotated set 10,000 sentences
train SVM.
Another line work focuses augmenting summarization system aspect
selection similar data-mining approaches Hu Liu (2004), rather using
single-aspect analysis. Carenini, Ng, Zwart (2005) Carenini et al. (2006) augment
previous aspect selection user-defined hierarchical organization aspects; e.g.,
digital zoom part lens. Polarity aspect assumed given previous
work. aspects incorporated existing summarization systems MEAD*
sentence extraction (Radev et al., 2000) SEA natural language generation (Carenini &
Moore, 2006) form final summaries. Like work Seki et al. (2005, 2006), work
create new techniques aspect identification sentiment analysis; instead,
focus process integrating sources information summarization systems.
aspects produced comparable across reviews particular product,
highly-supervised nature means approach feasible large set products
corpus reviews many types restaurants. Instead, must able
dynamically identify relevant aspects.
final line related work relies traditional summarization technique identifying contrastive contradictory sentences. Kim Zhai (2009) focus generating
contrastive summaries identifying pairs sentences express differing opinions
particular product feature. this, define metrics representativeness (coverage opinions) contrastiveness (alignment quality) using semantic similarity
WordNet matches word overlap. comparison work, approach follows orthogonal goal, try find defining aspects instead
contradictory ones. Additionally, selected pairs hint disagreements rating,
identification many people agree side overall rating
particular aspect. work, aim produce concrete set aspects
user sentiment each, whether unanimous shows disagreement.
Overall, methods designed produce output summaries focus
subjective information, specifically targeted aspect-based analysis. Instead,
aspects identified supervised fashion (Carenini et al., 2005, 2006) defined
(Seki et al., 2005, 2006; Kim & Zhai, 2009). work, crucial
dynamically-selected aspects feasible preselect aspects supervised
fashion.
2.2.3 Probabilistic Topic Modeling Sentiment Analysis
work closest direction aspect-based analysis focuses use
probabilistic topic modeling techniques identification aspects. may aggre1. task examples, see work Dang (2005, 2006).

96

fiAutomatic Aggregation Joint Modeling Aspects Values

gated without specific sentiment polarity (Lu & Zhai, 2008) combined additional
sentiment modeling either jointly (Mei, Ling, Wondra, Su, & Zhai, 2007; Blei & McAuliffe,
2008; Titov & McDonald, 2008a) separate post-processing step (Titov & McDonald,
2008b). Like work, approaches share intuition aspects may represented
topics.
Several approaches focus extraction topics sentiment blog articles.
one approach, used expert articles aspect extraction combination
larger corpus user reviews. Lu Zhai (2008) introduce model semi-supervised
probabilistic latent semantic analysis (PLSA) identifies sentiment-bearing aspects
segmentation expert review. Then, model extracts compatible supporting
supplementary text aspect set user reviews. Aspect selection
constrained rule-based approaches; specifically, aspect words required
nouns. work differs work significantly. share common goal
identifying aggregating opinion-bearing aspects, additionally desire identify
polarity opinions, task addressed work. addition, obtaining aspects
expert review unnecessarily constraining; practice, expert reviewers may
mention key aspects, mention every aspect. crucial discover
aspects based entire set articles.
work direction aspect identification blog posts. example,
Mei et al. (2007) use variation latent Dirichlet allocation (LDA) similar
explicitly model topics sentiment, use hidden Markov model discover
sentiment dynamics across topic life cycles. general sentiment polarity distribution
computed combining distributions several separate labeled data sets (e.g., movies,
cities, etc.). However, work, sentiment measured document-level, rather
topic-level. Additionally, topics discovered model broad; example, processing query Da Vinci Code, returned topics may labeled
book, movie, religion, rather fine-grained aspects desire model,
representing major characters events. model expands work discovering fine-grained aspects associating particular sentiment individual
aspect. addition, tying sentiment aspects, able identify sentiment-bearing
words associated polarities without additional annotation required train
external sentiment model.
Sentiment may also combined LDA using additional latent variables
document order predict document-level sentiment. Blei McAuliffe (2008) propose
form supervised LDA (sLDA) incorporates additional response variable,
used represent sentiment star rating movie. jointly
model documents responses order find latent topics best predict
response variables future unlabeled documents. work significantly different
work, supervised predict multi-aspect framework.
Building approaches comes work fine-grained aspect identification sentiment analysis. Titov McDonald (2008a, 2008b) introduce multi-grain unsupervised
topic model, specifically built extension LDA. technique yields mixture
global local topics. Word distributions topics (both global local) drawn
global level, however; unlike model. consequence topics
easy compare across products corpus; however, topics gen97

fiSauper & Barzilay

eral less dynamic hope achieve must shared among every
product. One consequence defining global topics difficulty finding relevant topics
every product little overlap. example, case restaurant reviews,
Italian restaurants completely different set aspects Indian restaurants.
course, factors known, would possible run algorithm separately
subset restaurants, distinctions immediately clear priori. Increasing number topics could assist recovering additional aspects; however
aspects still global, still difficult identify restaurant-specific aspects.
sentiment analysis, PRanking algorithm Snyder Barzilay (2007) incorporated two ways: First, PRanking algorithm trained pipeline fashion
topics generated (Titov & McDonald, 2008b); later, incorporated model
inference joint formulation (Titov & McDonald, 2008a). However, cases,
original algorithm, set aspects fixed aspects corresponds
fixed set topics found model. Additionally, learning problem supervised.
fixed aspects, necessary additional supervision, global topic distribution, model formulation sufficient problem domain, requires
fine-grained aspects.
approaches structural similarity work present,
variations LDA. None, however, intent model. Mei et al. (2007)
model aspect sentiment jointly; however aspects vague, treat
sentiment document level rather aspect level. Likewise, Titov McDonald
(2008b, 2008a) model fine-grained aspects, still coarser aspects
require, even increase number aspects, distributions shared
globally. Finally, Lu Zhai (2008), Blei McAuliffe (2008), Titov McDonald
(2008b, 2008a) require supervised annotation supervised expert review
have. attempt solve issues joint formulation order proceed
minimal supervision discover truly fine-grained aspects.

3. Problem Formulation
explaining model details, describe random variables abstractions
model, well intuitions assumptions.2 visual explanation model
components shown Figure 3. present complete details generative story
Section 4.
3.1 Model Components
model composed five component types: entities, snippets, aspects, values,
word topics. Here, describe type provide examples.

2. Here, explain complete model value selection sentiment restaurant domain.
simplified case medical domain would like use aspects, may simply ignore
value-related components model.

98

fiAutomatic Aggregation Joint Modeling Aspects Values

Tasca Spanish Tapas
Entity
Aspects

Chicken
+

+

chicken cooked perfectly
chicken tough tasty
Moist delicious chicken
Snippets

Values

Dessert
+
+

dessert good
excellent creme brulee


Douzo Sushi Bar
Sushi
+


sushi creative pretty good
torched roll tasted rather bland



Figure 3: Labeled model components example Figure 1. Note aspects
never given explicit labels, ones shown presented purely ease
understanding; aspects exist simply groups snippets share common subject.
Also, word topics pictured here; word topic (Aspect, Value, Background)
assigned word snippet. model components described high level
Section 3.1 depth Section 4.

3.1.1 Entity
entity represents single object described review. restaurant
domain, represent individual restaurants, Tasca Spanish Tapas, Douzo Sushi
Bar, Outback Steakhouse.
3.1.2 Snippet
snippet user-generated short sequence words describing entity. snippets
provided user (for example, quick reaction box) extracted
complete reviews phrase extraction system one Sauper,
Haghighi, Barzilay (2010). assume snippet contains one single
aspect (e.g., pizza) one single value type (e.g., positive). restaurant domain,
corresponds giving opinion one particular dish category dishes. Examples
restaurant domain include pasta dishes perfection ,
fantastic drinks, lasagna rustica cooked perfectly.
99

fiSauper & Barzilay

3.1.3 Aspect
aspect corresponds one several properties entity. restaurant domain
entities represent restaurants, aspects may correspond individual dishes categories dishes, pizza alcoholic drinks. domain, entity
unique set aspects. allows us model aspects appropriate granularity.
example, Italian restaurant may dessert aspect pertains information variety cakes, pies, gelato. However, bakerys menu would
fall dessert aspect. Instead, present useful aspect-based summary,
would require separate aspects cakes, pies, on. aspects
entity-specific rather shared, ties restaurants aspects
common (e.g., sushi restaurants sashimi aspect); consider
point potential future work. Note still possible compare aspects across
entities (e.g., find best restaurant burger ) comparing respective word
distributions.
3.1.4 Value
Values represent information associated aspect. review domain, two
value types represent positive negative sentiment respectively. general, possible
use value represent distinctions; example, domain aspects
associated numeric value others associated text description,
set value type. intended distinctions may encouraged
use seed words (see Section 3.2), may left unspecified model assign
whatever finds best fit data. number value types must prespecified;
however, possible use either many types.
3.1.5 Word Topic
words snippet observed, word associated underlying
latent topic. possible latent topics correspond aspect, value, background
topic. example, review domain, latent topic words great terrible would
Value, words represent entity aspects pizza would Aspect,
stop words like in-domain white noise like food would Background.
3.2 Problem Setup
work, assume snippet words always observed, correlation
snippets entities known (i.e., know entity given snippet describes).
addition, assume part speech tags word snippet. final source
supervision, may optionally include small sets seed words lexical distribution,
order bias distribution toward intended meaning. example, sentiment
case, add seed words order bias one value distribution toward positive one
toward negative. Seed words certainly required; simply tool constrain
models use distributions fit prior expectations.
Note formulation, relevant aspects restaurant observed;
instead, represented lexical distributions induced inference time.
100

fiAutomatic Aggregation Joint Modeling Aspects Values

system output, aspects represented unlabeled clusters snippets.3 Given
formulation, goal work induce latent aspect value underlying
snippet.

4. Model
model generative formulation snippets corpus. section,
first describe detail general formulation notation model, discuss
novel changes enhancements particular corpora types. Inference model
discussed Section 5. mentioned previously, describe complete model
including aspect values.
4.1 General Formulation
model, assume collection snippet words entities, s. use si,j,w
denote wth word jth snippet ith entity. also assume fixed vocabulary
words W .
present summary notation Table 1, concise summary model
Figure 4, model diagram Figure 5. three levels model design:
global distributions common snippets entities collection, entity-level
distributions common snippets describing single entity, snippet- word-level
random variables. Here, describe turn.
4.1.1 Global Distributions
global level, draw set distributions common entities corpus.
include everything shared across domain, background stop-word distribution,
value types, word topic transitions.
Background Distribution global background word distribution B drawn represent stop-words in-domain white noise (e.g., food becomes white noise corpus
restaurant reviews). distribution drawn symmetric Dirichlet concentration parameter B ; experiments, set 0.2.
Value Distributions value word distribution Vv drawn value type v.
example, review domain positive negative sentiment types,
distribution words positive type one negative type. Seed words
Wseedv given additional probability mass value priors type v; specifically,
non-seed word receives hyperparameter, seed word receives + V ;
experiments, set 0.15.
Transition Distribution transition distribution drawn represent transition
probabilities underlying word topics. example, may likely
Value Aspect transition review domain, fits phrases like great pizza.
experiments, distribution given slight prior bias toward helpful transitions;
3. label desired, automatically extract one selecting highest probability words
particular aspect. simplicity exactness, provide manual cluster labels examples
paper.

101

fiSauper & Barzilay

Data Set

si,j,w
ti,j,w
W
Wseedv

Collection snippet words entities
wth word jth snippet ith entity
Part-of-speech tag corresponding si,j,w
Fixed vocabulary
Seed words value type v

Lexical Distributions
B
i,a
)

(A
v
V


Background word distribution
Aspect word distribution aspect entity
Value word distribution type v
Ignored words distribution

Distributions

i,a (a )
( )


Transition distribution word topics
Aspect-value multinomial aspect entity
Aspect multinomial entity
Part-of-speech tag distribution

Latent Variables
i,j
ZA
ZVi,j
i,j,w
ZW

Aspect selected si,j
Value type selected si,j
Word topic (A, V, B, ) selected si,j,w

Notation
K

V
B


Number aspects
Indicator corresponding
Indicator corresponding
Indicator corresponding
Indicator corresponding






aspect word
value word
background word
ignored word

Table 1: Notation used paper. Items marked
Section 4.2.



relate extensions mentioned

example, encouraging sticky behavior providing small boost self-transitions.
bias easily overridden data; however, provides useful starting point.
4.1.2 Entity-Specific Distributions
naturally variations aspects snippets describe many snippets
describe aspect. example, mobile device popular long battery life likely
snippets describing battery device known large screen.
domains may enormous variation aspect vocabulary; example, restaurant
reviews, two restaurants may serve food items compare. account
102

fiAutomatic Aggregation Joint Modeling Aspects Values

Global Level:
Draw background word distribution B Dirichlet(B W )
value type v,
Draw value word distribution Vv Dirichlet(W + V Wseedv )
Entity Level:
entity i,
i,a
Draw aspect word distributions
Dirichlet(A W ) = 1, . . . , K

Draw aspect value multinomial i,a Dirichlet(AV N ) = 1, . . . , K
Draw aspect multinomial Dirichlet(M K)
Snippet Level:
snippet j describing ith entity,
i,j

Draw snippet aspect ZA
i,j

Draw snippet value ZVi,j i,ZA

i,j,w1
i,j,w
|ZW
Draw sequence word topic indicators ZW
i,j
value ZVi,j
Draw snippet word given aspect ZA

si,j,w


i,Z i,j

,

Z i,j

V V ,



B ,

i,j,w
=A
ZW
i,j,w
=V
ZW
i,j,w
ZW = B

Figure 4: summary generative model presented Section 4.1. use Dirichlet(W ) denote finite Dirichlet prior hyper-parameter counts scalar
times unit vector vocabulary items. global value word distribution, prior
hyper-parameter counts vocabulary items V Wseedv , vector
vocabulary items set seed words value v.

variations, define set entity-specific distributions generate
aspect vocabulary popularity, well distribution value types aspect.
i,a
Aspect Distributions aspect word distribution
drawn aspect a.
represents distribution unigrams particular aspect. example,
domain restaurant reviews, aspects may correspond menu items pizza,
reviews cell phones, may correspond details battery life.

103

fiSauper & Barzilay

Value v
Background word
distribution

Transition
distribution

Value word
distributions

B



Vv

Entity

Aspect
Aspect
multinomial

Aspect word
distributions

Aspect-value
multinomial



Ai,a

i,a

Snippet j

Snippet aspect

Snippet value

ZAi,j

ZVi,j

HMM snippet words



i,j,w1
ZW

i,j,w
ZW

i,j,w+1
ZW

si,j,w1

si,j,w

si,j,w+1

ZAi,j , Ai,a
ZVi,j , Vv
B
Figure 5: graphical description model presented Section 4.1. written description generative process located Figure 4. Curved arrows indicate additional links
present model drawn readability.

104

fiAutomatic Aggregation Joint Modeling Aspects Values

aspect word distribution drawn symmetric Dirichlet prior hyperparameter
; experiments, set 0.075.
Aspect-Value Multinomials Aspect-value multinomials i,a determine likelihood
value type v corresponding aspect a. example, value types represent
positive negative sentiment, corresponds agreement sentiment across snippets.
Likewise, value types represent formatting integers, decimals, text, aspect
generally prefers type value. multinomials drawn symmetric
Dirichlet prior using hyperparameter AV ; experiments, set 1.0.
Aspect Multinomial aspect multinomial controls likelihood aspect
discussed given snippet. encodes intuition certain aspects
likely discussed others given entity. example, particular Italian
restaurant famous pizza, likely pizza aspect frequently
discussed reviews, drinks aspect may mentioned occasionally.
aspect multinomial encode higher likelihood choosing pizza snippet
aspect drinks. multinomial drawn symmetric Dirichlet distribution
hyperparameter ; experiments, set 1.0.
4.1.3 Snippet- Word-Specific Random Variables
Using distributions described above, draw random variables snippet
determine aspect value type described, well sequence
underlying word topics words.
i,j
snippet describe drawn aspect
Aspect single aspect ZA

multinomial . aspect words snippet (e.g., pizza corpus restaurant
i,j
i,ZA

reviews) drawn corresponding aspect word distribution

.

Value Type single value type ZVi,j drawn conditioned selected aspect
i,j
corresponding aspect-value multinomial i,ZA . value words snippet (e.g., great
Z i,j

review domain) drawn corresponding value word distribution V V .
i,j,1
i,j,m
generWord Topic Indicators sequence word topic indicators ZW
, . . . , ZW
ated using first-order Markov model parameterized transition matrix .
indicators determine unigram distribution generates word snippet.
i,j,w
example, ZW
= B, wth word snippet generated background word
distribution B .

4.2 Model Extensions
optional components model may improve performance
cases. briefly list here, present necessary modifications model
detail case. Modifications inference procedure presented Section 5.2.
First, corpora contain irrelevant snippets, may introduce additional word
distribution word topic Ignore allow model ignore certain snippets
pieces snippets altogether. Second, possible acquire part speech tags
105

fiSauper & Barzilay

snippets, using extra piece information quite beneficial. Finally, corpora
every entity expected share aspects, model altered use
set aspect distributions entities.
4.2.1 Ignoring Snippets
snippet data automatically extracted, may noisy, snippets may
violate initial assumptions one aspect one value. example, find
snippets mistakenly extracted neither aspect value.
extraneous snippets may difficult identify priori. compensate this, modify
model allow partial entire snippets ignored addition global
unigram distribution, namely Ignore distribution . distribution drawn
symmetric Dirichlet concentration parameter .
Ignore distribution differs Background distribution includes
common uncommon words. intended select whole snippets large portions
snippets, words may overlap Background distribution distributions. order successfully incorporate distribution model, must allow
i,j,w
consider Ignore topic I. Additionally, ensure
word topic indicator ZW
selects long segments text, give large boost prior Ignore Ignore
sequence transition distribution , similar boost self-transitions.
4.2.2 Part-of-Speech Tags
Part-of-speech tags provide valuable evidence determining snippet words
drawn distribution. example, aspect words often nouns, represent
concrete properties concepts domain. Likewise, domains, value words
describe aspects therefore tend expressed numbers adjectives.
intuition directly incorporated model form additional
outputs. Specifically, modify HMM produce words tags. Additionally,
, v , , similar corresponding unigram
define distributions tags
V
B
distributions.
4.2.3 Shared Aspects
domains regular, every entity expected express aspects
consistent set, beneficial share aspect information across entities. example,
medical domain, general set lab tests physical exam categories run
patients. Note quite unlike restaurant review case, restaurants
aspects completely different (e.g., pizza, curry, scones, on).
Sharing aspects way accomplished modifying aspect distributions
i,a
. Likewise, aspect-value multinomials i,a become

become global distributions
shared across entities . Treatment aspect multinomials depend
domain properties. distribution aspects expected across
entities, also made global; however, individual entity expected exhibit
variation number snippets related aspect, kept entityspecific. example, reviews set cell phones may expected focus varying
106

fiAutomatic Aggregation Joint Modeling Aspects Values

Value v
Background word
distribution

Transition
distribution

Value word
distributions

B



Vv

Entity

Aspect
Aspect
multinomial

Aspect word
distributions

Aspect-value
multinomial



Aa



Snippet j

Snippet aspect

Snippet value

ZAi,j

ZVi,j

HMM snippet words



i,j,w1
ZW

i,j,w
ZW

i,j,w+1
ZW

si,j,w1

si,j,w

si,j,w+1

ZAi,j , Ai,a
ZVi,j , Vv
B
Figure 6: graphical description model shared aspects presented Section 4.2.
Note similarities Figure 5; however version, aspects shared entire
corpus, rather entity-specific. would also possible share aspect
multinomial corpus-wide; case would indicate entities share
general distribution aspects, version individual entities allowed
completely different distributions.

parts, depending unique problematic phones. graphical
description changes compared original model shown Figure 6.
107

fiSauper & Barzilay

Mean-field Factorization
Q (B , V , , , , , Z)
= q (B ) q ()

N


!
q (Vv )

v=1




q





K



i,a
q
q i,a

!

a=1



!
i,j i,j i,j,w

q ZV q ZA
q ZW
w

j

Snippet Aspect Indicator
i,j
log q(ZA
= a) Eq(i ) log (a) +

X

i,j,w
i,a i,j,w
q(ZW
= A)Eq(i,a ) log
(s
)+


w

N
X

q(ZVi,j = v)Eq(i,a ) log i,a (v)

v=1

Snippet Value Type Indicator
X
X
i,j
i,j,w
log q(ZVi,j = v)
q(ZA
= a)Eq(i,a ) log i,a (v) +
q(ZW
= V )Eq(Vv ) log Vv (si,j,w )


w

Word Topic Indicator
X





i,j,w+1
i,j
i,j i,j,w
i,j,w1
i,j,w
+
= Eq(i,a ) log
, A, ZW
q ZA

log q ZW
= log P ZW = + Eq() log ZW

















i,j,w+1

i,j,w1
i,j,w
= V log P ZW = V + Eq() log ZW
, V V, ZW
log q ZW



+

X

q ZVi,j = v Eq(Vv ) log Vv si,j,w


v

log q

i,j,w
ZW

= B log P ZW = B + Eq() log


i,j,w1
,B
ZW

i,j,w+1
B, ZW



+ Eq(B ) log B si,j,w



Figure 7: mean-field variational algorithm used learning inference obtain posterior predictions snippet properties attributes, described Section 5.
Mean-field inference consists updating latent variable factors well
straightforward update latent parameters round robin fashion.

5. Inference
goal inference model predict aspect value snippet
product j, given text observed snippets, marginalizing remaining
hidden parameters:
i,j
P (ZA
, ZVi,j |s)
accomplish task using variational inference (Blei, Ng, & Jordan, 2003). Specifically, goal variational inference find tractable approximation Q() full
posterior model.
P (B , V , , , , , Z|s) Q(B , V , , , , , Z)
model, assume full mean-field factorization variational distribution,
shown Figure 7. variational approximation defined product factors q(),
assumed independent. approximation allows tractable inference
factor individually. obtain closest possible approximation, attempt set
108



fiAutomatic Aggregation Joint Modeling Aspects Values

q() factors minimize KL divergence true model posterior:
arg min KL(Q(B , V , , , , , Z)kP (B , V , , , , , Z|s))
Q()

5.1 Optimization
optimize objective using coordinate descent q() factors. Concretely,
update factor optimizing criterion factors fixed
current values:
q() EQ/q() log P (B , V , , , , , Z, s)
summary variational update equations given Figure 7, graphical
representation involved variables step presented Figure 8. Here,
present update factor.
5.1.1 Snippet Aspect Indicator
i,j
First, consider update snippet aspect indicator, ZA
(Figure 8a):
i,j
log q(ZA
= a) Eq(i ) log (a)
X
i,j,w
i,a i,j,w
+
q(ZW
= A)Eq(i,a ) log
(s
)

+

(1b)



w
N
X

(1a)

q(ZVi,j = v)Eq(i,a ) log i,a (v)

(1c)

v=1

optimal aspect particular snippet depends three factors. First, include
likelihood discussing aspect (Eqn. 1a). mentioned earlier, encodes
prior probability aspects discussed frequently others. Second,
examine likelihood particular aspect based words snippet (Eqn. 1b).
word identified aspect word, add probability discusses
aspect. Third, determine compatibility chosen aspect type
current aspect (Eqn. 1c). example, know value type likely integer,
assigned aspect accept integers.
5.1.2 Snippet Value Type Indicator
Next, consider update snippet value type indicator, ZVi,j (Figure 8b):
X
i,j
log q(ZVi,j = v)
q(ZA
= a)Eq(i,a ) log i,a (v)

(2a)



+

X

i,j,w
q(ZW
= V )Eq(Vv ) log Vv (si,j,w )

(2b)

w

best value type snippet depends two factors. First, like snippet aspect
indicator, must take consideration compatibility snippet aspect
value type (Eqn. 2a). Second, word identified value word, include
likelihood comes given value type.
109

fiSauper & Barzilay

v
B

v
Vv








i,a




j



ZV , V
ZA ,
B



j

ZVi,j

w1
ZW

w
ZW

w+1
ZW

sw1

sw

sw+1

ZV , V
ZA ,
B



Vv



j

i,j
ZA

w1
ZW



ZA
sw1

Z





j





w
ZW

w+1
ZW

sw1

sw

sw+1

Vv

i,a


w+1
ZW



w1
ZW


sw

sw+1

Z

ZV
sw1 V





Vv

i,a


i,a

B


i,j
ZA

w
ZW

w
i. ZW
=A

w1
ZW

v



i,a

ZVi,j



B


i,a


ZVi,j

v




i,a

(b) Inference procedure snippet value, ZVi,j

v


i,a


i,j
ZA



i,j
(a) Inference procedure snippet aspect, ZA

B

Vv



i,a

i,j
ZA



B



i,a

ZVi,j



j

w
ZW

w+1
ZW



sw

sw+1

Z



i,j
ZA

w1
ZW

ZVi,j
w
ZW

w+1
ZW

sw

sw+1



w
ii. ZW
=V

sw1 B

w
iii. ZW
=B

i,j,w
(c) Inference procedure word topic, ZW

Figure 8: Variational inference update steps latent variable. latent variable
currently updated shown double circle, variables relevant
update highlighted black. variables impact update
grayed out. Note snippet aspect (a) snippet value type (b), update takes
form possible aspect value type. However, word topic (c),
update symmetric relevant variables different possible word topic.

110

fiAutomatic Aggregation Joint Modeling Aspects Values

5.1.3 Word Topic Indicator
i,j,w
Finally, consider update word topic indicators, ZW
(Figure 8c). Unlike
previous indicators, possible topic slightly different equation, must
marginalize possible aspects value types.






i,j,w
i,j,w1
i,j,w+1
log q ZW
= log P ZW = + Eq() log ZW
, A, ZW
X

i,j
i,j i,j,w
+
q ZA
= Eq(i,a ) log


(3a)










i,j,w
i,j,w1
i,j,w+1
log q ZW
= V log P ZW = V + Eq() log ZW
, V V, ZW
X


+
q ZVi,j = v Eq(Vv ) log Vv si,j,w

(3b)

v






i,j,w+1
i,j,w1
i,j,w
, B B, ZW
= B log P ZW = B + Eq() log ZW
log q ZW

+ Eq(B ) log B si,j,w

(3c)

update topic composed prior probability topic, transition probabilities using topic, probability word coming appropriate unigram distribution, marginalized possibilities snippet aspect value
indicators.
5.1.4 Parameter Factors
Updates parameter factors variational inference derived simple
counts latent variables ZA , ZV , ZW . Note include partial counts;
i,j
= a1 ) = 0.35, would contribute 0.35
particular snippet aspect probability P (ZA

count (a1 ).
5.1.5 Algorithm Details
Given set update equations, update procedure straightforward. First, iterate
corpus computing updated values random variable, batch
update factors simultaneously. update algorithm run convergence.
practice, convergence achieved 50th iteration, algorithm quite efficient.
Note batch update means update computed using values
previous iteration, unlike Gibbs sampling uses updated values runs
corpus. difference allows variational update algorithm parallelized, yielding
nice efficiency boost. Specifically, parallelize algorithm, simply split set
entities evenly among processors. Updates entity-specific factors variables
computed pass data, updates global factors collected
combined end pass.
111

fiSauper & Barzilay

5.2 Inference Model Extensions
discussed Section 4.2, add additional components model improve
performance data certain attributes. Here, briefly discuss modifications
inference equations extension.
5.2.1 Ignoring Snippets
main modifications model extension addition unigram
distribution word topic I, chosen ZW . update equation ZW
modified addition following:
i,j,w
log q(ZW
= I) log P (ZW = I) + Eq(I ) log (si,j,w )

pieces equation (Eqn. 3), composed prior probability
word topic likelihood word generated .
addition, transition distribution must updated include transition probabilities I. mentioned earlier, II transition receives high weight,
transitions receive low weight.
5.2.2 Part-of-Speech Tags
add part speech tags, model updated include part-of-speech distributions
i,a
, V , B , one word topic. Note unlike unigram distributions
Vv , corresponding tag distributions dependent snippet entity, aspect,
value. included referenced updates ZW follows:





i,j,w+1
i,j,w
i,j,w1
, A, ZW
log q ZW
= log P ZW = + Eq() log ZW
X

i,j i,j,w
i,j
+ Eq(A ) log ti,j,w +
= Eq(i,a ) log

q ZA









i,j,w+1
i,j,w1
i,j,w
, V V, ZW
= V log P ZW = V + Eq() log ZW
log q ZW
X


q ZVi,j = v Eq(Vv ) log Vv si,j,w
+ Eq(V ) log V ti,j,w +
v






i,j,w
i,j,w1
i,j,w+1
log q ZW
= B log P ZW = B + Eq() log ZW
, B B, ZW


+ Eq(B ) log B ti,j,w + Eq(B ) log B si,j,w
Here, define set tags ti,j,w tag corresponding word si,j,w .
5.2.3 Shared Aspects
global set shared aspects simplification model reduces total
aspect-value
number parameters. model redefines aspect distributions
multinomials . Depending domain, may also redefine aspect multinomial
. resulting latent variable update equations same; parameter
112

fiAutomatic Aggregation Joint Modeling Aspects Values

factor updates changed. Rather collecting counts snippets describing single
entity, counts collected across corpus.

6. Experiments
perform experiments two tasks. First, test full model joint prediction
aspect sentiment corpus review data. Second, use simplified version
model designed identify aspects corpus medical summary data.
domains structured quite differently, therefore present different challenges
model.
6.1 Joint Identification Aspect Sentiment
first task test full model jointly predicting aspect sentiment
collection restaurant review data. Specifically, would like dynamically select
set relevant aspects restaurant, identify snippets correspond
aspect, recover polarity snippet individually aspect whole.
perform three experiments evaluate models effectiveness. First, test
quality learned aspects evaluating predicted snippet clusters. Second, assess
quality polarity classification. Third, examine per-word labeling accuracy.
6.1.1 Data Set
data set task consists snippets selected Yelp restaurant reviews
previous system (Sauper et al., 2010). system trained extract snippets containing
short descriptions user sentiment towards aspect restaurant.4 purpose
experiment, select snippets labeled system referencing food.
order ensure enough data meaningful analysis, ignore restaurants
fewer 20 snippets across reviews. model easily operate
restaurants fewer snippets, want ensure cases select evaluation
nontrivial; i.e., sufficient number snippets cluster make
valid comparison. 13,879 snippets total, taken 328 restaurants
around Boston/Cambridge area. average snippet length 7.8 words,
average 42.1 snippets per restaurant. use MXPOST tagger (Ratnaparkhi,
1996) gather POS tags data. Figure 9 shows example snippets.
domain, value distributions consist one positive one negative distribution. seeded using 42 33 seed words respectively. Seed words hand-selected
based restaurant review domain; therefore, include domain-specific words
delicious gross. complete list seed words included Table 2.
6.1.2 Domain Challenges Modeling Techniques
domain presents two challenging characteristics model. First, wide
variety restaurants within domain, including everything high-end Asian fusion
cuisine greasy burger fast food places. try represent using single
4. exact training procedures, please reference paper.

113

fiSauper & Barzilay

Positive
amazing
delightful
extraordinary
flavorful
generous
heaven
inexpensive
perfect
recommend
stimulating
wonderful

Negative
awesome
divine
fantastic
free
good
huge
love
phenomenal
rich
strong
yummy

best
enjoy
fav
fresh
great
incredible
nice
pleasant
sleek
tasty

delicious
excellent
favorite
fun
happy
interesting
outstanding
quality
stellar
tender

average
bland
disappointed
expensive
gross
lame
meh
poor
tacky
tiny
uninspiring

awful
boring
disgusting
fatty
horrible
less
mushy
pricey
tasteless
unappetizing
worse

bad
confused
dry
greasy
inedible
mediocre
overcooked
salty
terrible
underwhelming
worst

Table 2: Seed words used model restaurant corpus, 42 positive words 33
negative words total. words manually selected data set.
shared set aspects, number aspects required would immense, would
extremely difficult model make fine-grained distinctions them.
defining aspects separately restaurant mentioned Section 4, achieve
proper granularity aspects individual restaurant without overwhelming
overlapping selection choices. example, model able distinguish
Italian restaurant may need single dessert aspect, bakery requires separate
pie, cake, cookie aspects.
Second, usually fairly cohesive set words refer particular
aspect (e.g., pizza aspect might commonly seen words slice, pepperoni,
cheese), near-unlimited set potential sentiment words. especially
pronounced social media domain many novel words used express
sentiment (e.g., deeeeeeeelish substitute delicious). mentioned Section 4,
part-of-speech transition components model helps identify unknown
words likely sentiment words; however, additionally need identify polarity sentiment. this, leverage aspect-value multinomial,
represents likelihood positive negative sentiment particular aspect.
snippets given aspect positive, likely word deeeeeeeelish
represents positive sentiment well.
6.1.3 Cluster Prediction
i,j
goal task evaluate quality aspect clusters; specifically ZA
variable
Section 4. ideal clustering, predicted clusters cohesive (i.e., snippets
predicted discuss given aspect related other) comprehensive (i.e.,
snippets discuss aspect selected such). example, snippet
assigned aspect pizza snippet mentions aspect pizza,
crust, cheese, toppings.

Annotation experiment, use set gold clusters complete sets
snippets 20 restaurants, 1026 snippets total (an average 51.3 snippets per
restaurant). Cluster annotations provided graduate students fluent English.
annotator provided complete set snippets particular restaurant,
asked cluster naturally. 199 clusters total, yields average
114

fiAutomatic Aggregation Joint Modeling Aspects Values

noodles meat actually +:::::::
pretty ::::::
good.
+:::::::::::::
recommend chicken noodle pho.
soggy.
noodles ::::::
chicken pho also + good.
:::::
though.
spring rolls coffee + good,
:::::

spring roll wrappers little
dry tasting.
::::::::::::::::::
crispy
spring
rolls.
+ favorites


:::::::::
Crispy Tuna Spring Rolls + fantastic!
:::::::::





lobster roll mother ordered ::::
dry scant.
:::::
portabella mushroom + go-to
sandwich.
:::::
bread sandwich stale.
:::::
rather ::::::::
measly.
slice tomato :::::::

shumai california maki sushi +::::::::
decent.
+
spicy tuna roll eel roll perfect.
:::::::
:::
so::::::
great.
rolls spicy mayo ::::
+
love Thai rolls.
:::::

Figure 9: Example snippets data set, grouped according aspect. Aspect words
underlined colored blue, negative value words labeled - colored red,
positive value words labeled + colored green. grouping labeling
given data set must learned model.

10.0 clusters per restaurant. annotations high-quality; average annotator
agreement 81.9 MUC evaluation metric (described detail below). could
define different number clusters restaurant varying number aspect
distributions, simplicity ask baseline systems full model produce
10 aspect clusters per restaurant, matching average annotated number. Varying
number clusters simply cause existing clusters merge split; large
surprising changes clustering.
Baseline use two baselines task, using clustering algorithm weighted
TF*IDF implemented publicly available CLUTO package (Karypis, 2002),5
using agglomerative clustering cosine similarity distance metric (Chen, Branavan,
Barzilay, & Karger, 2009; Chen, Benson, Naseem, & Barzilay, 2011).
first baseline, Cluster-All, clusters entire snippets data set.
baseline put strong connection things lexically similar.
model uses aspect words tie together clusters, baseline may capture correlations
words model correctly identify aspect words.
5. Available http://glaros.dtc.umn.edu/gkhome/cluto/cluto/overview.

115

fiSauper & Barzilay

Cluster-All
Cluster-Noun
model

Precision

Recall

57.3
68.6
74.3

60.1
70.5
85.3

F1
58.7
69.5
79.4

Table 3: Results using MUC metric cluster prediction joint aspect value
identification task. MUC deficiency putting everything single
cluster artificially inflate score, models set use number clusters.
Note task, Cluster-Noun significantly outperforms Cluster-All
baseline, indicating part speech crucial piece information task.

second baseline, Cluster-Noun, works nouns snippets.
snippet POS-tagged using MXPOST (Ratnaparkhi, 1996),6 non-noun (i.e.,
NN, NNS, NNP, NNPS) words removed. expect aspects contain
least one noun, acts proxy aspect identification model.
Metric use MUC cluster evaluation metric task (Vilain, Burger, Aberdeen,
Connolly, & Hirschman, 1995). metric measures number cluster merges
splits required recreate gold clusters given models output. Therefore,
concisely show accurate clusters whole. would possible
artificially inflate score putting everything single cluster, parameters
model likelihood objective model prefers use available
clusters, number baseline system.
Results Results cluster prediction task Table 3. model shows strong
performance baseline, total error reduction 32% Cluster-Noun
baseline 50% Cluster-All baseline. common cause poor cluster
choices baseline systems inability distinguish words relevant
aspect words. example, Cluster-All baseline, many snippets use word
delicious, may end cluster based alone. Cluster-Noun
baseline able avoid pitfalls thanks built-in filter. able
avoid common value words adjectives also focus seems
concrete portion aspect (e.g., blackened chicken); however, still cannot make
correct distinctions assumptions broken. model capable
distinguishing words aspect words (i.e., words relevant clustering),
choose clusters make sense overall.
6.1.4 Sentiment Analysis
evaluate systems predictions snippet sentiment using predicted posterior
i,j
value distributions snippet (i.e., ZA
). task, consider binary
i,j
judgment simply one higher value q(ZA
) (see Section 5). goal
task evaluate whether model correctly distinguishes sentiment value words.
6. Available http://www.inf.ed.ac.uk/resources/nlp/local_doc/MXPOST.html.

116

fiAutomatic Aggregation Joint Modeling Aspects Values

Majority
Discriminative-Small
Seed
Discriminative-Large
model

Accuracy
60.7
74.1
78.2
80.4
82.5

Table 4: Sentiment prediction accuracy model compared Discriminative
Seed baselines, well Majority representing majority class (Positive) baseline.
One advantage system ability distinguish aspect words sentiment words
order restrict judgment relevant terms; another leverage gains
biasing unknown sentiment words follow polarity observed snippets
relating aspect.
Annotation task, use set 662 randomly selected snippets Yelp
reviews express opinions. get clear result, set specifically excludes neutral,
mixed, potentially ambiguous snippets fries salty tasty
blackened chicken spicy, make 10% overall data. set
split training set 550 snippets test set 112 snippets, snippet
manually labeled positive negative. one baseline, use set positive
negative seed words manually chosen model, shown Table 2. Note
before, model access full corpus unlabeled data plus seed words,
labeled examples.
Baseline use two baselines task, one based standard discriminative
classifier one based seed words model.
Discriminative baseline task standard maximum entropy discriminative binary classifier7 unigrams. Given enough snippets enough unrelated aspects,
classifier able identify words like great indicate positive sentiment
like bad indicate negative sentiment, words like chicken neutral
effect. illustrate effect training size, include results DiscriminativeSmall, uses 100 training examples, Discriminative-Large, uses 550
training examples.
Seed baseline simply counts number words positive
negative seed lists used model, Vseed+ Vseed , listed Table 2.
words Vseed+ , snippet labeled positive, words
Vseed , snippet labeled negative. tie seed words, split
prediction. seed word lists manually selected specifically restaurant
reviews (i.e., contain food-related sentiment words delicious), baseline
perform well.
Results overall sentiment classification accuracy system shown Table 4). model outperforms baselines. obvious flaw Seed baseline
7. Available https://github.com/lzhang10/maxent.

117

fiSauper & Barzilay

85

Accuracy

82.5
79.5

80

80.4

78.2 77.7
76.8

78.6

75
Discriminative
Seed
model

74.1
70

0

100 200 300 400 500 600
Number snippets training data

Figure 10: Discriminative baseline performance number training examples increases. performance generally increases, inconsistencies. main
issue baseline needs see examples words training data
improve; phenomenon seen plateau graph.

inability pre-specify every possible sentiment word. perform highly, due
tailoring restaurant domain good coverage frequent words (e.g.,
delicious, good, great), performance model indicates generalize
beyond seed words.
Discriminative-Large outperforms Seed baseline test set; however,
given smaller training set Discriminative-Small, performs worse. training
curve Discriminative baseline shown Figure 10. Discriminative
baseline system correctly identify polarity statements containing information
seen past, two main weaknesses. First, every sentiment word must
present training data. example, test data, rancid appears negative
sentence; however, appear training data, model labels example
incorrectly. problematic, way find training data every possible
sentiment word, especially social media data novel words typos frequent
occurrence. models ability generalize polarity snippets describing
particular aspect allows predict sentiment values words unknown polarity.
example, already several positive snippets describing particular aspect,
system guess snippet unknown polarity likely also positive.
6.1.5 Per-Word Labeling Accuracy
goal task evaluate whether word correctly identified aspect
word, value word, background word. distinction crucial order achieve
correctness clustering sentiment analysis, errors may help us identify
weaknesses model.
118

fiAutomatic Aggregation Joint Modeling Aspects Values

rolls also nt
well made .
:::::::: :::: ::::::
pita ::::::::
beyond dry
:::::::
tasted like
cardboard !
:::
:::::::::::::::
falafel !
Falafel King best
::::
rolls spicy mayo
good .
::: :: :::::
!
Ordered spicy tuna california roll amazing
:::::::::

Table 5: Correct annotation set phrases containing elements may confusing,
annotators tested allowed annotate actual test data. Aspect words colored blue underlined; value words colored orange underlined
wavy line. common mistakes include: annotating nt background (because
attached background word was), annotating cardboard aspect
noun, annotating Falafel King aspect subject position.
Annotation Per-word annotation acquired Mechanical Turk. per-word labeling task seems difficult Turk annotators, implement filtering procedure
ensure high-quality annotators allowed submit results. Specifically,
ask annotators produce labels set difficult phrases known labels (shown
Table 5). annotators successfully produced correct mostly-correct annotations allowed access annotation tasks containing new phrases.
unknown tasks presented 3 annotators, majority label taken word.
total, test 150 labeled phrases, total 7,401 labeled words.
Baseline baseline task relies intuition part-of-speech
useful proxy aspect value identification. know aspects usually represent
concrete entities, often nouns, value words descriptive counting,
often adjectives adverbs. Therefore, use MXPOST tagger find
POS word snippet. main baseline, Tags-Full, assign noun
(NN*) aspect label, numeral, adjective, adverb, verb participle (CD, RB*,
JJ*, VBG, VBN) value label. comparison, also present results smaller tagset,
Small-Tags, labeling nouns (NN*) aspect adjectives (JJ*) values. Note
tags added Tags-Full baseline beneficial baselines score.
Tree expansion full model baselines designed pick
relevant individual words rather phrases, may correspond well phrases
humans selected relevant. Therefore, also evaluate set expanded
labels identified parse trees Stanford Parser (Klein & Manning, 2003).8 Specifically, non-background word, identify largest containing noun phrase (for
aspects values) adjective adverb phrase (for values only)
also contain oppositely-labeled words. example, noun phrase blackened chicken,
chicken labeled aspect word blackened labeled background word,
labeled aspect words. However, noun phrase tasty chicken
tasty already labeled value, label changed expansion
attempted. final heuristic step, punctuation, determiners, conjunctions
8. Available http://nlp.stanford.edu/software/lex-parser.shtml.

119

fiSauper & Barzilay

Tree expansion procedure aspect words:
1. Find noun phrases contain aspect word (pork).


+ innovative

::::::::::::

appetizers pork apple glaze
NP1
NP2
NP3

+ highlights
:::::::::::

2. Select largest noun phrase contain value (sentiment) words.
NP1 valid; contain value words. However, largest valid NP.
NP2 valid; contain value words. largest valid NP, selected.
NP3 contains value word (+ innovative),
invalid.
::::::::::::

3. Convert background words within selected noun phrase aspect words
except punctuation, determiners, conjunctions.


+ innovative

::::::::::::

appetizers pork apple glaze

+ highlights
:::::::::::

Figure 11: tree expansion procedure value words, example snippet.
procedure similar aspect words, except adjective phrases adverb phrases also
considered expansion.
Aspect
Precision Recall

F1

Value
Precision Recall

F1

Tags-Small
Tree

79.9
74.0

79.5
83.0

79.7
78.2

78.5
79.2

45.0
57.4

57.2
66.5

Tags-Full
Tree

79.9
75.6

79.5
81.4

79.7
78.4

78.1
77.1

68.7
70.1

73.1
73.4

85.2
79.5

52.6
71.9

65.0
75.5

70.5
76.7

61.6
70.9

65.7
73.7

model
Tree

Table 6: Per-word labeling precision recall model compared Tags-Small
Tags-Full baselines, without expansion trees. model
precise aspect better recall value. Note general process expanding labels tree structure increases recall expense precision.
would newly labeled aspect value words ignored kept background
words. steps procedure illustrative example shown Figure 11.
Results evaluate systems precision recall aspect value separately.
Results systems shown Table 6. model without tree expansion
highly precise expense recall; however expansion performed, recall
improves tremendously, especially value words.
result initially disappointing, possible adjust model parameters
increase performance task; example, aspect words could put additional
120

fiAutomatic Aggregation Joint Modeling Aspects Values

moqueca delicious
:::::::
perfect winter food , ::::::
warm , filling hearty :::
:::
::::::
heavy .
:::::::::
bacon wrapped almond dates ::::::::
amazing plantains cheese boring
.
::::::
artichoke homemade pasta appetizers :::::
great

Table 7: High-precision, low-recall aspect word labeling full model. Note
human would likely identify complete phrases bacon wrapped almond dates
homemade pasta appetizers; however, additional noise degrades performance
clustering task.

start

V
B



0.06
0.19
0.02
0.22
0.00

V
0.00
0.03
0.32
0.26
0.00

B
0.94
0.77
0.47
0.43
0.01


0.00
0.01
0.01
0.17
0.99

end
0.00
0.00
0.18
0.06
0.00

Table 8: Learned transition distribution model. pattern high-precision
aspect words represented preference continuing string several aspect
words, causing model prefer single, precise aspect words. Likewise, better recall
value words indicated higher value V V transition, encourage
several words row marked value words.
i,j,w
mass prior ZW
= increase Dirichlet hyperparameter . However,
increases performance word labeling task, also decreases performance
correspondingly clustering task. examination data, correlation
perfectly reasonable. order succeed clustering task, model selects
relevant portions snippet aspect words. entire aspect value
identified, clustering becomes noisy. Table 7 shows examples high-precision
labeling achieves high clustering performance, Table 8 shows example
learned transition distribution creates labeling.

6.2 Aspect Identification Shared Aspects
second task uses simplified version model designed aspect identification
only. task, use corpus medical visit summaries. domain,
summary expected contain similar relevant information; therefore, set aspects
shared corpus-wide. evaluate model formulation, examine predicted
clusters snippets, full model.
6.2.1 Data Set
data set task consists phrases selected dictated patient summaries
Pediatric Environmental Health Clinic (PEHC) Childrens Hospital Boston, specializing
treatment children lead poisoning. Specifically, patients office visit lab
results completed, PEHC doctor dictates letter referring physician containing
121

fiSauper & Barzilay

information previous visits, current developmental family status, in-office exam
results, lab results, current diagnosis, plan future.
experiment, select phrases in-office exam lab results sections
summaries. Phrases separated heuristically commas semicolons. domain contains significant amount extraneous information, restaurant
domain, must extract phrases believe bear relevance task hand.
However, medical text dense nearly relevant, heuristic separation
sufficient extract relevant phrases. 6198 snippets total, taken 271
summaries. average snippet length 4.5 words, average 23 snippets
per summary. Yelp domain, use MXPOST tagger (Ratnaparkhi, 1996)
gain POS tags. Figure 12 shows example snippets. domain,
values; simply concentrate aspect-identification task. Unlike restaurant
domain, use seed words.
6.2.2 Domain Challenges Modeling Techniques
contrast restaurant domain, medical domain uses single global set aspects.
represent either individual lab tests (e.g., lead level, white blood cell count) particular body systems (e.g., lungs cardiovascular ). aspects far common
others, uncommon summary include one two snippets
given aspect. Therefore, mentioned Section 4.2, model aspect word
distributions aspect multinomial shared entities corpus.
Also contrast restaurant domain, aspects defined words taken
entire snippet. Rather aspects associated names measurements
(e.g., weight), units descriptions measurement (e.g., kilograms) also
relevant aspect definition. property extends numeric written measurements; example, aspect lungs commonly described clear auscultation
bilaterally. order achieve high performance, model must leverage
clues provide proper aspect identification name measurement missing
(e.g., patient 100 cm). part speech still important factor model,
predict greater importance additional parts speech
nouns.
Finally, data set noisy contains irrelevant snippets, section
headings (e.g., Physical examination review systems) extraneous information.
described Section 4.2, modify model ignore partial complete
snippets.
6.2.3 Cluster Prediction
joint aspect sentiment prediction, goal task evaluate quality
aspect identification. aspects shared across documents, clusters
generally much larger, set annotated snippets represents fraction
cluster.
Annotation experiment, use set gold clusters gathered 1,200 snippets, annotated doctor expert domain Pediatric Environmental Health Clinic Childrens Hospital Boston. Note mentioned before, clusters
122

fiAutomatic Aggregation Joint Modeling Aspects Values

113 cm height
Patients height 146.5 cm
Lungs: Clear bilaterally auscultation
lungs normal
Heart regular rate rhythm; murmurs
Heart normal S1 S2

Figure 12: Example snippets medical data set, grouped according aspect.
Aspect words underlined colored blue. grouping labeling given
data set must learned model.

Cluster-All
Cluster-Noun
model

Precision

Recall

88.2
88.4
89.1

93.0
83.9
93.4

F1
90.5
86.1
91.2

Table 9: Results using MUC metric cluster prediction aspect identification
task. Note Cluster-All baseline significantly outperforms Cluster-Noun,
opposite observe joint aspect value prediction task. due
dependence aspect identification name lab test,
units description test results, mentioned Section 6.2.2.

global domain (e.g., many patients snippets representing blood lead
level, grouped one cluster). doctor asked cluster 100
snippets time (spanning several patients), clustering entire set would
infeasible human annotator. 12 sets snippets clustered, resulting
clusters manually combined match similar clusters set. example, blood lead level cluster first set 100 snippets combined
corresponding blood lead level clusters set snippets. cluster
final set fewer 5 members removed. total, yields gold set
30 clusters. 1,053 snippets total, average 35.1 snippets per cluster.
match this, baseline systems full model asked produce 30 clusters across
full data set.
Baselines & Metric keep results consistent previous task,
use baselines evaluation metric. baselines rely TF*IDFweighted clustering algorithm, specifically implemented CLUTO package (Karypis,
2002) using agglomerative clustering cosine similarity distance metric. before,
Cluster-All represents baseline using unigrams snippets entire data set,
Cluster-Noun works nouns snippets. use
MUC cluster evaluation metric task. details baselines
evaluation metric, please see Section 6.1.3.
123

fiSauper & Barzilay

Results experiment, system demonstrates improvement 7%
Cluster-All baseline. Absolute performance relatively high systems
medical domain, indicating lexical clustering task less misleading
restaurant domain. interesting note unlike restaurant domain,
Cluster-All baseline outperforms Cluster-Noun baseline. mentioned Section 6.2.2, medical data notable relevance entire snippet clustering
(e.g., weight kilograms useful identify weight aspect).
property, using nouns cluster Cluster-Noun baseline hurts performance
significantly.

7. Conclusions Future Work
paper, presented approach fine-grained content aggregation using
probabilistic topic modeling techniques discover structure individual text snippets.
model able successfully identify clusters snippets data set discuss
aspect entity well associated values (e.g., sentiment). requires
annotation, small list seed vocabulary bias positive negative
distributions proper direction.
results demonstrate delving structure snippet assist
identifying key words important unique domain hand.
values learned, joint identification aspect value help improve
quality results. word labeling analysis reveals model learns different
type labeling task; specifically, strict, high-precision labeling clustering
task high-recall labeling sentiment. follows intuition important
identify specific main points clustering, sentiment analysis task,
may often several descriptions conflicting opinions presented need
weighed together determine overall sentiment.
model admits fast, parallelized inference procedure. Specifically, entire inference procedure takes roughly 15 minutes run restaurant corpus less 5
minutes medical corpus. Additionally, model neatly extensible adjustable
fit particular characteristics given domain.
limitations model improved future work:
First, model makes attempt explicitly model negation word interactions,
increasing difficulty aspect sentiment analysis model. performing error analysis, find negation common source error sentiment
analysis task. Likewise, aspect side, model make errors attempting
differentiate aspects ice cream cream cheese share common aspect
word cream, despite phrases occurring bigrams. using connections
stronger way, indicator variable negation higher-order HMM,
model could make informed decisions.
Second, defining aspects per-entity restaurant domain advantages
possible get fine-grained set applicable aspects, also fails leverage
potential information data set. Specifically, know restaurants sharing
type (e.g., Italian, Indian, Bakery, etc.) share common aspects;
however, ties current model. Likewise, even global
124

fiAutomatic Aggregation Joint Modeling Aspects Values

level, may aspects tie across restaurants. hierarchical version
model would able tie together identify different types aspects:
global (e.g., presentation), type-level (e.g., pasta Italian type), restaurant-level
(e.g., restaurants special dish).

Bibliographic Note
Portions paper published previously conference publication (Sauper,
Haghighi, & Barzilay, 2011); however paper significantly extends work. describe several model generalizations extensions (Section 4.2) effects
inference procedure (Section 5.2). present new experimental results, including additional baseline comparisons additional experiment (Section 6.1). also introduce
new domain, medical summary text, quite different domain restaurant
reviews therefore requires several fundamental changes model (Section 6.2).

Acknowledgments
authors acknowledge support NSF (CAREER grant IIS-0448168), NIH (grant
5-R01-LM009723-02), Nokia, DARPA Machine Reading Program (AFRL prime
contract no. FA8750-09-C-0172). Thanks Peter Szolovits MIT NLP group
helpful comments. opinions, findings, conclusions, recommendations expressed
paper authors, necessarily reflect views funding
organizations.

References
Barzilay, R., McKeown, K. R., & Elhadad, M. (1999). Information fusion context
multi-document summarization. Proceedings ACL, pp. 550557.
Blei, D. M., & McAuliffe, J. (2008). Supervised topic models. Advances NIPS, pp.
121128.
Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent Dirichlet allocation. Journal
Machine Learning Research, 3, 9931022.
Carenini, G., & Moore, J. D. (2006). Generating evaluating evaluative arguments.
Artificial Intelligence, 170, 925952.
Carenini, G., Ng, R., & Pauls, A. (2006). Multi-document summarization evaluative text.
Proceedings EACL, pp. 305312.
Carenini, G., Ng, R. T., & Zwart, E. (2005). Extracting knowledge evaluative text.
Proceedings K-CAP, pp. 1118.
Chen, H., Benson, E., Naseem, T., & Barzilay, R. (2011). In-domain relation discovery
meta-constraints via posterior regularization. Proceedings ACL, pp. 530540.
Chen, H., Branavan, S. R. K., Barzilay, R., & Karger, D. R. (2009). Global models
document structure using latent permutations. Proceedings ACL/HLT, pp. 371
379.
125

fiSauper & Barzilay

Crammer, K., & Singer, Y. (2001). Pranking ranking. Advances NIPS, pp.
641647. MIT Press.
Dang, H. T. (2005). Overview DUC 2005. Proceedings DUC EMNLP/HLT.
Dang, H. T. (2006). Overview DUC 2006. Proceedings DUC NAACL/HLT.
Dave, K., Lawrence, S., & Pennock, D. M. (2003). Mining peanut gallery: opinion
extraction semantic classification product reviews. Proceedings WWW,
pp. 519528.
Hu, M., & Liu, B. (2004). Mining summarizing customer reviews. Proceedings
SIGKDD, pp. 168177.
Karypis, G. (2002). CLUTO clustering toolkit. Tech. rep. 02-017, Dept. Computer
Science, University Minnesota. Available http://www.cs.umn.educluto.
Kim, H. D., & Zhai, C. (2009). Generating comparative summaries contradictory opinions
text. Proceedings CIKM, pp. 385394.
Kim, S., & Hovy, E. (2005). Automatic detection opinion bearing words sentences.
Proceedings IJCNLP, pp. 6166.
Kim, S.-M., & Hovy, E. (2006). Automatic identification pro con reasons online
reviews. Proceedings COLING ACL, pp. 483490.
Klein, D., & Manning, C. D. (2003). Accurate unlexicalized parsing. Proceedings ACL,
pp. 423430.
Liu, B., Hu, M., & Cheng, J. (2005). Opinion observer: Analyzing comparing opinions
web. Proceedings WWW, pp. 342351.
Lu, Y., & Zhai, C. (2008). Opinion integration semi-supervised topic modeling.
Proceedings WWW, pp. 121130.
Mani, I. (2001). Automatic summarization, Vol. 3. John Benjamins Pub Co.
McDonald, R., Hannan, K., Neylon, T., Wells, M., & Reynar, J. (2007). Structured models
fine-to-coarse sentiment analysis. Proceedings ACL, pp. 432439.
Mei, Q., Ling, X., Wondra, M., Su, H., & Zhai, C. (2007). Topic sentiment mixture: modeling
facets opinions weblogs. Proceedings WWW, pp. 171180.
Pang, B., & Lee, L. (2004). sentimental education: Sentiment analysis using subjectivity
summarization based minimum cuts. Proceedings ACL, pp. 271278.
Pang, B., & Lee, L. (2008). Opinion mining sentiment analysis. Foundations Trends
Information Retrieval, 2, 1135.
Pang, B., Lee, L., & Vaithyanathan, S. (2002). Thumbs up? Sentiment classification using
machine learning techniques. Proceedings EMNLP, pp. 7986.
Popescu, A.-M., Nguyen, B., & Etzioni, O. (2005). OPINE: Extracting product features
opinions reviews. Proceedings EMNLP/HLT, pp. 339346.
Radev, D., & McKeown, K. (1998). Generating natural language summaries multiple
on-line sources. Computational Linguistics, 24 (3), 469500.
126

fiAutomatic Aggregation Joint Modeling Aspects Values

Radev, D. R., Jing, H., & Budzikowska, M. (2000). Centroid-based summarization multiple documents: sentence extraction, utility-based evaluation, user studies.
Proceedings NAACL-ANLP Workshop Automatic Summarization, pp. 21
30.
Ratnaparkhi, A. (1996). maximum entropy model part-of-speech tagging. Proceedings EMNLP, pp. 133142.
Sauper, C., Haghighi, A., & Barzilay, R. (2010). Incorporating content structure text
analysis applications. Proceedings EMNLP, pp. 377387.
Sauper, C., Haghighi, A., & Barzilay, R. (2011). Content models attitude. Proceedings ACL, pp. 350358.
Seki, Y., Eguchi, K., Kanodo, N., & Aono, M. (2005). Multi-document summarization
subjectivity analysis DUC 2005. Proceedings DUC EMNLP/HLT.
Seki, Y., Eguchi, K., Kanodo, N., & Aono, M. (2006). Opinion-focused summarization
analysis DUC 2006. Proceedings DUC NAACL/HLT, pp. 122130.
Snyder, B., & Barzilay, R. (2007). Multiple aspect ranking using good grief algorithm.
Proceedings NAACL/HLT, pp. 300307.
Titov, I., & McDonald, R. (2008a). joint model text aspect ratings sentiment
summarization. Proceedings ACL, pp. 308316.
Titov, I., & McDonald, R. (2008b). Modeling online reviews multi-grain topic models.
Proceedings WWW, pp. 111120.
Turney, P. D. (2002). Thumbs thumbs down?: semantic orientation applied unsupervised classification reviews. Proceedings ACL, pp. 417424.
Vilain, M., Burger, J., Aberdeen, J., Connolly, D., & Hirschman, L. (1995). modeltheoretic coreference scoring scheme. Proceedings MUC, pp. 4552.
Yu, H., & Hatzivassiloglou, V. (2003). Towards answering opinion questions: Separating
facts opinions identifying polarity opinion sentences. Proceedings
EMNLP, pp. 129136. Association Computational Linguistics.

127

fiJournal Artificial Intelligence Research 46 (2013) 203-233

Submitted 06/12; published 02/13

Integrative Semantic Dependency Parsing via Ecient
Large-scale Feature Selection
Hai Zhao
Xiaotian Zhang

zhaohai@cs.sjtu.edu.cn
xtian.zh@gmail.com

Shanghai Jiao Tong University,
800 Dongchuan Road, Shanghai,China

Chunyu Kit

ctckit@cityu.edu.hk

City University Hong Kong,
Tat Chee Avenue, Kowloon, Hong Kong SAR, China

Abstract
Semantic parsing, i.e., automatic derivation meaning representation
instantiated predicate-argument structure sentence, plays critical role deep processing natural language. Unlike top systems semantic dependency parsing
rely pipeline framework chain series submodels specialized specic subtask, one presented article integrates everything one
model, hopes achieving desirable integrity practicality real applications
maintaining competitive performance. integrative approach tackles semantic parsing word pair classication problem using maximum entropy classier. leverage
adaptive pruning argument candidates large-scale feature selection engineering
allow largest feature space ever use far eld, achieves state-of-the-art
performance evaluation data set CoNLL-2008 shared task, top one
top pipeline system, conrming feasibility eectiveness.

1. Introduction
purpose semantic parsing derive meaning representation sentence,
usually taking syntactic parse input. popular formalism represent kind
meaning predicate-argument structure and, accordingly, parsing instantiate
predicate argument(s) structure properly actual words phrases
given sentence. context dependency parsing, becomes semantic dependency
parsing, takes syntactic dependency tree input outputs lled predicateargument structure predicate, argument word properly labeled
semantic role relation predicate.
Semantic role labeling (SRL) one core tasks semantic dependency parsing,
dependency constituent based. Conventionally, tackled mainly two
subtasks, namely, argument identication classication. Conceptually, former determines whether word true argument predicate, latter semantic
role plays relation predicate (or argument instantiates predicateargument structure). predicate given, two indispensable subtasks
predicate identication disambiguation, one identify word predicate
sentence determine predicate-argument structure identied
predicate particular context.
c
2013
AI Access Foundation. rights reserved.

fiZhao, Zhang & Kit

pipeline framework adopted almost previous researches handle subtasks one another. main reason dividing whole task semantic dependency
parsing multiple stages way twofold: maintaining computational eciency
adopting dierent favorable features subtask. general, joint learning system
multiple components slower pipeline system, especially training. also
reported Xue Palmer (2004) dierent features favor dierent subtasks
SRL, especially argument identication classication. results CoNLL
shared tasks 2005 2008 (Carreras & Marquez, 2005; Koomen, Punyakanok, Roth, &
Yih, 2005; Surdeanu, Johansson, Meyers, Marquez, & Nivre, 2008; Johansson & Nugues,
2008) seem suggest pipeline strategy benchmark technology
state-of-the-art performance specic NLP task.
SRL systems pipeline, integrated SRL system holds unique merits, e.g., integrity implementation, practicality real applications, single-stage feature
selection beneting whole system, all-in-one model outputting expected semantic role information, on. particular, takes account interactive eect
features favoring dierent subtasks hence holds comprehensive view
features working together whole. article intended present recent research explore feasibility constructing eective integrated system semantic
dependency parsing melds subtasks together one, including predicate identication/disambiguation argument identication/classication, verbal nominal
predicates, uses feature set subtasks. core research
verify, practical implementation empirical evaluation, methodological soundness eectiveness approach. success, however, rooted
solid technical foundation, i.e., large-scale engineering procedure ecient mining
eective feature templates huge set feature candidates, feature space far richer
others ever used before. piece engineering brings potentials
integrative approach full play. Another focus article hence illustrate
technical essentials.
Nevertheless, worth pointing term integrative, used opposite
pipeline, misleading mean subtasks carried jointly single run.
Instead, used highlight integrity model implementation uses
single representation feature set accommodate subtasks. Although
approach unique advantages simplifying system engineering feature selection,
model implemented present joint one accomplish
whole semantic parsing synchronous determination predicates
arguments. two types indispensable objects semantic parse tree recognized
succession decoding using trained model.
rest article organized follows. Section 2 gives brief overview related
work, providing background research. Section 4 presents approach adaptive
pruning argument candidates generate head-dependent word pairs training
decoding, underlies whole process semantic parsing. two key
procedures optimize parsing, namely, feature selection decoding, presented
Section 5 6, respectively. details evaluation, including evaluation data, experimental results comprehensive comparative analysis results, presented
204

fiSemantic Dependency Parsing

Section 7. Finally, Section 8 concludes research, highlighting contributions
practicality competitiveness approach.

2. Related Work
Note SRL almost become surrogate semantic dependency parsing literature recent years. recent research eorts eld, including CoNLL shared
tasks 2004 2005, focused verbal predicates, thanks availability
PropBank (Palmer, Gildea, & Kingsbury, 2005). complement PropBank, NomBank
(Meyers, Reeves, Macleod, Szekely, Zielinska, Young, & Grishman, 2004) annotates nominal predicates correspondent semantic roles using similar semantic framework.
Although oering challenges, SRL nominal predicates drawn relatively little
attention (Jiang & Ng, 2006). issue merging various treebanks, including PropBank,
NomBank others, discussed work Pustejovsky, Meyers, Palmer,
Poesio (2005). idea merging two treebanks put practice
CoNLL-2008 shared task (Surdeanu et al., 2008). best system CoNLL-2008 used two
dierent subsystems cope verbal nominal predicates, respectively (Johansson &
Nugues, 2008). Unfortunately, however, integrative approach
illustrate performance close system.
fact, research eorts direction, except recent one
joint identication predicates, arguments senses Meza-Ruiz Riedel (2009).
formulate problem Markov Logic Network, weights learnt via 1-best
MIRA (Crammer & Singer, 2003) Online Learning method, use Cutting Plane Inference
(Riedel, 2008) Integer Linear Programming (ILP) base solver ecient joint
inference best choice predicates, frame types, arguments role labels
maximal posteriori probability. Using CoNLL-2008 data, system achieves best
semantic F1 80.16% WSJ test set. 0.75 percentage point lower ours,
reported below, whole WSJ+Brown test set. Note trained CoNLL2008 training corpus, subset WSJ corpus, SRL system performance least 10
percentage points higher WSJ Brown test set (Surdeanu et al., 2008).
CoNLL-2008 -2009 shared tasks1 devoted joint learning syntactic
semantic dependencies, aimed testing whether SRL well performed using
dependency syntax input. research reported article focuses semantic
dependency parsing. conduct valid reliable evaluation, use data set
evaluation settings CoNLL-2008 compare integrated system, best
SRL system CoNLL-2009 (Zhao, Chen, Kit, & Zhou, 2009), top systems
CoNLL shared tasks (Surdeanu et al., 2008; Hajic, Ciaramita, Johansson, Kawahara, Mart,
Marquez, Meyers, Nivre, Pado, Stepanek, Stranak, Surdeanu, Xue, & Zhang, 2009).2 Note
systems achieved higher performance scores CoNLL-2008 CoNLL-2009.
integrative approach dependency semantic parsing pros cons.
deal main drawbacks, two key techniques need applied purpose
1. Henceforth referred CoNLL-2008 -2009, respectively.
2. CoNLL-2008 English-only task, CoNLL-2009 multilingual one. Although use
English corpus, except more-sophisticated structures former (Surdeanu et al., 2008),
main dierence semantic predicate identication required latter.

205

fiZhao, Zhang & Kit

eciency enhancement. One bring auxiliary argument labels enable
improvement argument candidate pruning. signicantly facilitates development
fast lightweight SRL system. apply greedy feature selection
algorithm perform task feature selection given set feature templates.
helps nd many features possible benet overall process
parsing. Many individual optimal feature template sets reported literature
achieved excellent performance specic subtasks SRL. rst time
integrated SRL system reported produce result close state
art SRL achieved pipelines individual sub-systems highly specialized
specic subtask specic type predicate.

3. System Architecture
Dependencies words sentence, syntactic semantic, formulated
individual edges abstract graph structure. practice, dependency edge
built, type (usually referred label) identied, proper learning
decoding. conventional syntactic parsing makes use property projectiveness stipulated well-formedness syntactic tree. contrast, dependency
parsing, new dependencies built regard existing ones. However,
case semantic parsing, semantic parsing results projective trees.
Instead, actually directed acyclic graphs, word serve
argument multiple predicates. Inevitably, learning model semantic parsing
take word pairs account exploring possible dependent relationships.
SRL specic task semantic dependency parsing formulated word pair
classication problem tackled various machine learning models, e.g., Maximum
Entropy (ME) model used Zhao Kit (2008). model also used
work probability estimation support global decoding given
Section 6, extends model beyond sequential model. Without constraint,
classier task deal word pairs input sequence thus
inevitably prone poor computational eciency also unsatisfactory performance.
straightforward strategy alleviate problems perform proper pruning
training sample test data.
word pair consists word semantic head another semantic dependent,
conventionally denoted p (for predicate) (for argument), respectively.
follow convention feature representation below. Since approach unies
two tasks SRL, namely, predicate identication/disambiguation argument identication/classication, one classication framework, need dierentiate
verbal non-verbal heads, handled way.
one unique characteristics integrated system.
overall architecture system depicted Figure 1. input sentence
data set use, training, development test set, parsed word pair
sequence word pair generator using pruning algorithm, e.g., adaptive pruning
described below, eliminate useless pairs. Word pairs generated sentence
training set used train word pair classier, supports decoding
formulated Section 6 search optimal set word pairs test sentence
206

fiSemantic Dependency Parsing

Data set

Feature template set

Word pair generator

Feature selection procedure
Training

Word pair sequence

Selected feature set

Word pair classifier

Predicate decoding
Argument decoding
Yes

predicate?

Next sentence



Figure 1: Illustration system architecture work ow training testing
form semantic parse tree. decoding rst recognizes predicates sentence
determines arguments predicate beam search argument role
labels. features used classier selected predened feature space
greedy selection procedure using training development set repeated training
testing rene candidate feature set performance gain achievable
(see Section 5). classier obtained way selected features tested
test set.

4. Adaptive Argument Pruning
Word pairs derived sentence classier following ways. (1)
predicate identication/disambiguation, word pair consists virtual root (VR)
semantic parse tree construction (whose root virtually preset), head,
predicate candidate dependent. Theoretically, words sentence question
predicate candidate. reduce number, opt simple POS tag pruning
strategy verbs nouns allowed predicate candidates. (2) argument
identication/classication, word pair consists identied predicate, head,
another word dependent (or argument, conventional term). Potentially,
word sentence argument candidate. Pruning many
argument candidates possible thus particularly signicant improving eciency
performance classier.
two ways collect argument candidates given predicate, one
syntactic dependency tree linear path input sentence.
former (referred synPth hereafter), use dependency version pruning
algorithm Xue Palmer (2004), given follows necessary modication
allow predicate also included argument candidate list,
nominal predicate sometimes takes argument.
207

fiZhao, Zhang & Kit

ID
1
2
3
4
5
6
7
8
a.
b.
c.
d.
e.

FORMa
Investor
focus
shifted
quickly
,
traders
said
.

LEMMA
investor
focus
shift
quickly
,
trader
say
.

POS
NN
NN
VBD
RB
,
NNS
VBD
.

HEADb
2
3
7
3
7
7
0
7

DEPRELc
NMOD
SBJ
OBJ
MNR
P
SBJ
ROOT
P

PREDd

ARG Labele
A0

focus.01
shift.01

A1
A1
AM-MNR
A0

say.01

Word form, token.
Syntactic head current token, identied ID.
Syntactic dependency relation current token HEAD.
Roleset semantic predicate.
Argument labels semantic predicates text order.

Table 1: example input sentence CoNLL-2008 shared task data set
Initialization: Given predicate current node syntactic dependency tree.
1. Collect syntactic children argument candidates, traversing children
left right.
2. Reset current node syntactic head repeat Step 1 till root tree.
3. Collect root stop.
algorithm eective collecting words path given predicate
root children argument candidates. However, ecient one still
needed lend stronger support SRL system designed tackle argument
identication/classication single stage. Following observation arguments
usually tend surround predicate close distance, auxiliary label noMoreArg
introduced signify pruning stops collecting argument candidates. training
sample generation, label assigned next word soon arguments
current predicate saturated previously collected words, light original
training data illustrated Table 1. Accordingly, pruning process stops collecting
candidates. decoding, signals decoder stop searching, along similar
traverse pruning, arguments identied predicate. adaptive
technique improves pruning eciency signicantly, saving 1/3 training time
memory cost missing true arguments pruning without
label, according experiments. training sample generated way
sentence Table 1, means POS pruning pruning algorithm,
illustrated Table 2, class labels third column.
collect argument candidates along linear path (referred linPth hereafter)
instead syntactic tree sentence, classier search words around
given predicate. way similar pruning along synPth improved, two
auxiliary labels, namely, noMoreLeftArg noMoreRightArg, introduced signify
adaptive pruning along linPth stops, skipping words far away
predicate. Given example illustrate two labels used, e
208

fiSemantic Dependency Parsing

Head-dependent word pair
VR
Investor
VR
focus
VR
shifted
VR
traders
VR
said
focus
Investor
focus
focus
shifted focus
shifted quickly
shifted said
said
shifted
said
,
said
traders
said
.

Label
NONE PRED
01
01
NONE PRED
01
A0
noMoreArg
A1
AM-MNR
noMoreArg
A1
NONE ARG
A0
NONE ARG

Table 2: example training sample generated via pruning

input sequence predicate two arguments, labeled A0 A1, respectively.
two labels assigned next two words c g, respectively, indicating
arguments farther predicate. Accordingly, word sequence c
g taken training sample.
b

c
e f
g
h .
noMoreLeftArg A1
A0 noMoreRightArg

total list class labels model, including CoNLL-2008 data set
auxiliary ones newly introduced purpose, provided Table 9 Appendix A.
labels three categories, namely, 22 PropBank sense labels predicate classes,
54 argument classes, 23 auxiliary labels extra classes, total 78-79. Pruning
along linPth needs one label along synPth. Note work
assume whether sense label training test set means
dierent words. tendency particular word form associate senses
statistically signicant way throughout data set allows classier predict sense
labels using word form features.
principle, auxiliary label assigned last item sample generated
predicate via pruning along traversal order, syntactic linear. is,
assigned rst item immediately last argument predicate
seen pruning. auxiliary label treated exactly way
argument labels training decoding, except extra utility signal
stop search.

5. Feature Generation Selection
Following many previous works (Gildea & Jurafsky, 2002; Carreras & Marquez, 2005;
Koomen et al., 2005; Marquez, Surdeanu, Comas, & Turmo, 2005; Dang & Palmer, 2005;
Pradhan, Ward, Hacioglu, Martin, & Jurafsky, 2005; Toutanova, Haghighi, & Manning,
209

fiZhao, Zhang & Kit

2005; Jiang & Ng, 2006; Liu & Ng, 2007; Surdeanu, Marquez, Carreras, & Comas, 2007;
Johansson & Nugues, 2008; Che, Li, Hu, Li, Qin, Liu, & Li, 2008), carefully examine
factors involved wide range features used facilitate
undertaking two SRL subtasks, verbal nominal predicates.
endeavor decompose factors fundamental elements,
largest possible space feature templates explored eective
novel combinations features.
5.1 Feature Element
features adopted work intended make full use elements,
mainly drawn word property syntactic connection node syntactic
parse tree input sentence. sequences sets tree nodes, whose basic elements
drawn form features via feature generation means many predened feature
templates, identied path family relations stipulated below.
Word Property type elements include word form (denoted form
split form spForm),3 lemma (as lemma spLemma), part-of-speech tag (as pos
spPos), syntactic semantic dependency labels (as dprel semdprel).4
Syntactic Connection includes syntactic head (as h), left/right farthest/nearest
child (as slm, ln, rm rn), high/low support verb noun. Note along
path given word root syntactic tree, rst/last verb called
low/high support verb, respectively. notion widely adopted eld (Toutanova
et al., 2005; Xue, 2006; Jiang & Ng, 2006).5 work, extend nouns
prepositions. Besides, also introduce another syntactic head feature pphead
given word question, retain left sibling headed preposition,
original head otherwise, aimed drawing utility fact preposition usually
carries little semantic information. positive eect new feature conrmed
experiments.
Path two basic types path argument candidate given
predicate p, namely, linear path linePath sequence input words
(inclusive) path dpPath (inclusive) syntactic
dependency tree. Given two paths root r tree meet
node r , common part dpPathShare r r, dierent parts
dpPathArgu dpPathPred p r , respectively, path dpPath
p. Similarly, dpPath two nodes syntactic tree.
Family Two child sets dierentiated given predicate argument candidate,
one (as children) including syntactic children (as noFarChildren) excluding leftmost rightmost one. latter introduced feature
dierentiate modiers (i.e., children) close head far away.
3. Note CoNLL-2008, many treebank tokens split position hyphen (-) forward
slash (/), resulting two types form each, namely, non-split split.
4. lemma pos, training test, directly pre-analyzed columns input
le, automatically generated organizer CoNLL shared tasks.
5. Note notion term support verb slightly dierent works. used refer
verb introduces long-distance argument nominal predicate outside noun
phrase headed nominal predicate.

210

fiSemantic Dependency Parsing

Others also number elements, besides categories,
play signicant role feature generation. Many derived inter-word
relationships. Listed number representative ones.
dpTreeRelation returns relationship p input syntactic tree.
possible values feature include parent, sibling, etc.
isCurPred checks whether word question current predicate, returns
predicate yes, default value otherwise.
existCross checks potential dependency relation given pair words
may cross existing relation semantic tree construction.
distance returns distance two words along given path, dpPath
linePath, number words.
existSemdprel checks whether given argument label predicate assigned word.
voice returns either Active Passive verb default value noun.
baseline small set simple rules6 used generate SRL output baseline
CoNLL evaluation (Carreras & Marquez, 2005). baseline output
selectively used features, two categories: baseline Ax tags head rst
NP predicate A0 A1, respectively, baseline Mod tags
modal verb dependent predicate AM-MOD.
number features existCross existSemdprel depend
semantic dependencies dependency labels existing part semantic parse tree
(re)construction sentence, training decoding. Note training
decoding rst take candidate word pairs given sentence input, illustrated
Table 2, undergo process selecting subset candidates (re)construct
semantic parse tree, consists root, predicate(s) child(ren),
argument(s) predicate(s) grandchild(ren). decoding infers optimal
semantic tree sentence aid trained model (see Section 6).
training reconstructs gold standard semantic tree input sentence scanning
word pairs sequence dierentiating true ones tree
others. true ones rebuild tree part part. features (including existCross
existSemdprel) extracted true ones, partially (re)built parts
tree, others current context fed model training.
words, feature generation based gold standard argument labels
training predicted ones decoding.
5.2 Feature Generation
Sequences syntactic tree nodes rst collected means paths and/or family
relations dened above. Three strategies applied combine elements
type (e.g., form, spPos) nodes feature via string concatenation. three
strategies concatenation are: (1) sequencing (as seq), concatenates given element
strings original order path, (2) unduplicating (as noDup), frees
6. Developed Erik K Sang, University Antwerp, Belgium.

211

fiZhao, Zhang & Kit

seq adjacent duplicates, (3) bagging (as bag), concatenates unique element
strings alphabetical order.
Given number typical feature templates illustrate individual
features derived ways described above, aid following operators:
x+y (the concatenation x y), x.y (the attribute x), x:y (the path x y),
x:y|z (the collection instances attribute z along path x y).
a.lm.lemma lemma leftmost child argument candidate a.
p.h.dprel dependency label syntactic head predicate candidate p.
p-1 .pos + p.pos concatenation POS tags two consecutive predicates.
a:p|dpPath.lemma.bag bag lemmas along dpPath p.
a:p.highSupportNoun|linePath.dprel.seq seq dependency labels along
linePath high support noun p.
way, set 781 feature templates,7 henceforth referred F , generated
specify allowable feature space feature selection. Many generated
analogy existing feature templates literature. example, given feature
template like a.lm.lemma used previous works, analogous ones
a.rm.lemma, a.rn.lemma a.ln.lemma included F .
Predicate sense labels data set also utilized type element various
feature templates F . However, worth noting sense label associated
dierent words, e.g., 02 take.02 say.02, assumed anything
common anything other. predicate disambiguation, however,
features always combine predicate sense word form, hence naturally
dierentiate sense label dierent words. predict predicate sense
label always predict association word form. is, sense label never
used separation word form. way, model gives high precision
sense label prediction according empirical results.
5.3 Feature Template Selection
complicated hence computationally expensive task extract optimal subset
feature templates large feature space. sake eciency, greedy procedure
feature selection applied towards goal, illustrated many previous
works, e.g., Jiang Ng (2006), Ding Chang (2008). algorithm
implemented purpose presented Algorithm 1 below, imposes fewer
assumptions previous works, aiming higher eciency. repeats two
main steps performance gain achievable given development set:
1. Include template rest F current set candidate templates
inclusion would lead performance gain.
2. Exclude template current set candidate templates exclusion
would lead deterioration performance. repeatedly adding/removing
7. Available http://bcmi.sjtu.edu.cn/zhaohai/TSRLENAllT.txt, macro language used
implementation, far readable notation illustrations given here.

212

fiSemantic Dependency Parsing

most/least useful template, algorithm aims return better smaller candidate
set next round.
Given n candidate feature templates, algorithm Ding Chang (2008) requires
O(n2 ) time execute training/test routine, whereas one Jiang Ng (2006)
requires O(n) time, assuming initial set feature templates good enough
others handled strictly incremental way. time complexity
algorithm also analyzed terms execution time training-and-test routine
scr(M (.)), subroutines sorting negligible compared
execution time. Algorithm 1, recruitMore rst calls routine |F S| n
times loop, shakeOff calls |Smax | n times prepare
sorting, followed another |Smax | times inner loop. Assuming
rst loop outer shakeOff iterate k1 k2 times, respectively,
algorithm O(k1 (|F S| + k2 (|Smax | + |Smax |))) = O(k1 k2 n) time.
Empirically, however, k1 , k2 << n, experiments seldom show
k1 > 5 k2 > 10, especially running 1/10 F randomly chosen initial S.
particular, rst loop often iterates 2-3 times, rst iteration k2
drops rapidly. observation k1 k2 varies limited range suggests
may O(k1 k2 n) = O(n) empirical estimation eciency algorithm
particular context. reasonable account rst loop
comprises two functions, namely, recruitMore recruit positive feature templates
shakeOff lter negative ones, improve model either case,
likely positive/negative ones remain positive/negative consistently throughout
looping. result, remain outside/inside candidate set
recruiting/ltering couple iterations loop.
eciency allows large-scale engineering feature selection accomplished
reasonable cost time. experiments 1/10 F randomly selected
initial S, greedy selection procedure performed along one two argument
candidate traverse schemes (i.e., synPth linPth) NomBank, PropBank
, Ss , Ss
l
l
l
combination, output six feature template sets SN
P
N+P , SN , SP SN+P ,
186, 87, 246, 120, 80 118 selected templates, respectively, performance evaluation
comparison. 5500 machine learning routines ran synPth scheme
nearly 7000 routines linPth. contrastive analysis template sets,
focus top 100 important templates them, presented
Appendix Tables 9-17, rank columns present rankings feature
templates terms importance respective feature template sets. importance
feature template template set measured terms performance change
adding removing template, performance model using template set
measured labeled F1 score given test set, following conventional practice
SRL evaluation CoNLL shared tasks.
interesting note six template sets tiny intersection 5
templates, listed Table 10, manifesting notable variance importance ranking
dierent sets. Excluding ve, rest overlap top 100 synPth
,
sets SN
P
N +P also small, 11 templates, contrast
l , l l
linPth sets SN
P
N +P , 4 times larger, 46 templates; listed
213

fiZhao, Zhang & Kit

Algorithm 1 Greedy Feature Selection
Input
training data set:
development data set:
set feature templates: FT
Denotation
(S) = (S, ), model using feature template set S, trained ;
scr(M ) = scr(M, D), evaluation score model D;
Since xed, let scr(M (S)) = scr(M (S, ), D) brevity.
Algorithm
1: = {f0 , f1 , ..., fk }, random subset F ;
F : globally accessible constant
2:
3:
Cr = recruitMore(S);
4:
Cr == {} return S;
5:
= shakeOff(S + Cr );
6:
scr(M (S)) scr(M (S )) return S;
7:
= S;
8: end
1: function recruitMore(S)
Retrieve positive templates F
2:
Cr = {}, p = scr(M (S));
3:
f F
4:
p < scr(M (S + {f })) Cr = Cr + {f };
5:
end
6:
return Cr ;
7: end function
1: function shakeOff(Smax )
Shake useless templates Smax
2:

3:
= S0 = Smax ;
4:
sort descending ordera scr(M (S {f })) f S;
5:
(S = {f0 }) = {}
6:
Smax = argmaxx{Smax , S} scr(M (x));
Drop f0 useless
7:
end
8:
S0 == Smax return S0 ;
none dropped
9:
end
10: end function
a. Namely ascending order importance f S, estimated scr(M (S)) scr(M (S {f })).

214

fiSemantic Dependency Parsing

Tables 11 12, respectively. Besides shared templates, six sets hold 84, 71,
84, 69, 29 67 others top 100, listed Tables 13-18, respectively,
negative/positive subscript denotes preceding/following word. example, a.lm -1 .lemma
returns lemma previous word left child.
rather small overlap six sets suggests greedy feature selection algorithm maintains stable eciency working template sets huge divergence,
lending evidence support empirical estimation above. Despite divergence,
template sets enables SRL model achieve state-of-the-art performance
CoNLL-2008 data set,8 indicating eectiveness approach,
details evaluation provided Section 7 below.

6. Decoding
Following exactly procedure generating training sample, classier,
training, outputs series labels sequence word pairs generated
input sentence, inferring predicates arguments one another. Dierent
existing SRL systems, instantiates integrative approach conducts
predication trained model. However, following common practice incorporating task-specic constraints global inference (Roth & Yih, 2004; Punyakanok,
Roth, Yih, & Zimak, 2004), opt developing decoding algorithm infer
optimal argument structure predicate identied way classier. main dierences work Punyakanok et al. (2004) (1)
use ILP joint inference, exact, use beam search, greedy
approximate, (2) constraints (e.g., duplicate argument label allowed)
impose arguments individual linear (in)equalities realized
constraint fulllment features (e.g., existCross existSemdprel).
Specically, decoding identify arguments among candidate words inferring best semantic role label candidate (cf. training sample Table
2 one label per word). Let = {a0 , a1 , ..., an1 } candidates predicate,
ai embodies available properties word, including candidate label,
let Ai = a0 a1 ... ai1 partial argument structure (of target search)
determined ready use context inferring next argument. Instead
counting best-rst search, simply keeps picking next best argument according
conditional probability p(ai |Ai ), resort beam search better approximation
global optimization maximal probability
= argmax


n


p(ai |Ai ),

(1)

i=0

Ai consists rst elements . Ideally, beam search returns
probable subset arguments predicate question. rests conditional
maximum entropy sequential model incorporating global features decoding infer
arguments necessarily sequential order. previous practice,
8. Note early version model also illustrated top-ranking performance CoNLL-2009
multilingual data sets (Zhao, Chen, Kit, & Zhou, 2009).

215

fiZhao, Zhang & Kit

model adopts tunable Gaussian prior (Chen & Rosenfeld, 1999) estimate p(ai |Ai )
applies L-BFGS algorithm (Nocedal, 1980; Nash & Nocedal, 1991) parameter
optimization.

7. Evaluation
evaluation SRL approach conducted various feature template sets
ocial training/development/test corpora CoNLL-2008 (Surdeanu et al., 2008).
data set derived merging dependency version Penn Treebank 3 (Marcus,
Santorini, & Marcinkiewicz, 1993) PropBank NomBank. Note CoNLL-2008
essentially joint learning task syntactic semantic dependencies. research presented article focused semantic dependencies, primary
evaluation measure semantic labeled F1 score (Sem-F1 ). scores, including
macro labeled F1 score (Macro-F1 ), used rank participating systems
CoNLL-2008, Sem-F1 /LAS, ratio labeled F1 score semantic dependencies labeled attachment score (LAS) syntactic dependencies, also provided
reference.
7.1 Syntactic Input
Two types syntactic input used examine eectiveness integrative SRL approach. One gold standard syntactic input available ocial data set
parsing results data set two state-of-the-art syntactic
parsers, namely, MSTparser9 (McDonald, Pereira, Ribarov, & Hajic, 2005; McDonald
& Pereira, 2006) parser Johansson Nugues (2008). However, instead using
original MSTparser, substantially enriched additional features, following
Chen, Kawahara, Uchimoto, Zhang, Isahara (2008), Koo, Carreras, Collins (2008),
Nivre McDonald (2008). latter one, henceforth referred J&N short,
second-order graph-based dependency parser takes advantage pseudo-projective
techniques resorts syntactic-semantic reranking rening nal outputs.
However, 1-best outputs reranking used evaluation, even
thought reranking slightly improve parsing performance. Note reward reranking joint-learning syntactic semantic parsing gained
huge computational cost. contrary, approach intended show highly
comparable results achieved much lower cost.
7.2 Experimental Results
eectiveness proposed adaptive approach pruning argument candidates
examined three syntactic inputs, results presented Table 3,10
coverage rate proportion true arguments pruning output. Note
using auxiliary labels aect rate, accounted choice
traverse path quality syntactic input, suggested dierence
synPth rows. results show pruning reduces 50% candidates along
9. Available http://mstparser.sourceforge.net.
10. Decimal gures tables herein percentages unless otherwise specied.

216

fiSemantic Dependency Parsing

Syntactic Input (LAS)
MST (88.39%)
J&N (89.28%)
Gold (100.0%)

Path
linP th
synP th
linP th
synP th
linP th
synP th

Original
5.29M
2.15M
5.28M
2.15M
5.29M
2.13M

Pruning
1.57M
1.06M
1.57M
1.06M
1.57M
1.05M

Reduction
-70.32
-50.70
-70.27
-50.70
-70.32
-50.70

Coverage
100.0
95.0
100.0
95.4
100.0
98.4

Table 3: Reduction argument candidates adaptive pruning

Path x
linPth
synPth
Reduction

x
SN
7,103
5,609
-21.03

SPx
7,214
5,470
-24.18

x
SN+P
7,146
5,572
-22.03

Table 4: Number executions training-and-test routine greedy feature selection
synPth, cost losing 1.6-4.6% true ones, 70% along linPth without
loss. Nevertheless, candidate set resulted synPth 1/3 smaller size
linPth.
number times training-and-test routine executed greedy selection
six feature sets presented Table 4, showing synPth saves 21%-24% execution
times. Given estimation time complexity selection algorithm O(k1 k2 n)
executing routine, empirically 7 < k1 k2 < 10 feature space size n = 781
experiments, verifying high eciency algorithm.
pointed Pradhan, Ward, Hacioglu, Martin, Jurafsky (2004), argument
identication (before classication) bottleneck problem way improving SRL
performance. Narrowing set predicate candidates much possible reliable way shown feasible means alleviate problem. eectiveness
adaptive pruning purpose examined comparative experiments
terms time reduction performance enhancement. results series
experiments presented Table 5, showing adaptive pruning saves training
test time 30% 60%, respectively, enhancing performance (in
Sem-F1 score) 23.9%24.8%, nearly quarter. results also conrm signicant
improvement upon non-adaptive origin (Xue & Palmer, 2004) twofold benet
pruning arguments far away predicates, follows assumption true arguments tend close predicates. straightforward
using noMoreArg label reduces training samples using (see Section 4)
hence leads greater reduction training time. Using label also decreases
test time remarkably. decoding, noMoreArg label, assigned probability
higher possible role labels current word pair, hints decoder
stop working next word pair, resulting test time reduction 18.521.0
percentage points upon non-adaptive pruning. particularly low performance without pruning also reects soundness motivation candidate pruning
217

fiZhao, Zhang & Kit

Bank

Features

PropBank

87

NomBank
+
PropBank

246

Pruninga

Adaptiveb
+Adaptive

Adaptive
+Adaptive

Training
122,469s
109,094s
83,208s
432,544s
392,216s
305,325s

Redu.
-10.9
-32.1
-9.3
-29.4

Test
747s
372s
234s
2,795s
1,615s
1,029s

Redu.
-50.2
-68.7
-42.2
-63.2

Sem-F1
66.85
80.59
82.80
64.85
79.77
80.91

a. Syntactic input: MST; Traverse scheme: synPth; Machine conguration: Four six-core
R
R
Xeon
X5690 3.46GHz processors, 48G memory.
Intel
b. original pruning Xue Palmer (2004), using noMoreArg.

Table 5: Time reduction performance enhancement adaptive pruning

Syn. Input
(LAS)
MST
(88.39%)

J&N
(89.28%)

Gold
(100%)

Feature
Set
Initial
Selected
Initial
Selected
Initial
Selected

Path x
linPth
synPth
linPth
synPth
linPth
synPth
linPth
synPth
linPth
synPth
linPth
synPth

NomiF1 xN
44.58
44.67
77.93
77.89
44.84
45.01
77.73
77.70
45.57
45.89
80.43
80.37

VerbF1 xP
58.83
63.24
82.72
82.80
58.84
63.26
83.21
83.90
61.79
67.63
89.44
90.37

NomiF1 xN+P
41.18
42.42
76.75
77.52
42.16
43.64
76.45
76.79
42.41
43.76
79.44
80.20

VerbF1 xN+P
56.34
61.28
82.30
83.24
56.40
61.36
82.70
83.71
59.09
65.51
89.07
90.27

SemF1 xN+P
51.14
54.79
80.05
80.91
51.36
55.12
80.15
80.88
53.12
57.77
84.99
86.02

SemF1 xN+P /LAS
57.86
61.99
90.56
91.54
57.53
61.74
89.77
90.59
53.12
57.77
84.99
86.02

Table 6: Performance random initial greedily selected feature sets
machine learning linguistic perspective. pruning provides balanced
training dataset classier training without pruning. Note without pruning,
word pairs generated training irrelevant far away current
predicate, inevitably interfering informative features truly relevant ones
small minority and, hence, leading unsatisfactory performance. Although
pruning, especially adaptive version, rooted linguistic insight gained empirical observations real data, previous works semantic parsing simply took
pruning indispensable step towards good parsing performance, seldom paying much
attention poor performance without pruning comparing performance
dierent pruning strategies.
Table 6 presents comprehensive results semantic dependency parsing
three syntactic inputs aforementioned dierent quality. number observations
made results. (1) greedy feature selection, encoded Algorithm 1
above, boosts SRL performance drastically, raising Sem-F1 scores synPth
rows 54.79%57.77% initial feature sets, baseline, 80.88%86.02%
218

fiSemantic Dependency Parsing

Syn. Input
(LAS)

Feature set

Path x

MST
(88.39%)

l
SN
+P - Sense

SN
+P - Sense
l
SN
+ SPl

SN
+ SPs

linP th
synP th
linP th
synP th

NomiF1 xN+P
76.51
76.76
76.78
76.60

VerbF1 xN+P
82.09
82.75
82.20
82.76

SemF1 xN+P
79.82
80.30
79.99
80.24

SemF1 xN+P /LAS
90.30
90.85
90.50
90.78

Loss
F1 xN+P
-0.29
-0.75
-0.07
-0.83

Table 7: Experimental results feature ablation feature set combination
selected feature sets, increment 46.73%48.90%. rise corresponding linPth
rows even larger. Among three inputs, largest increment gold standard,
suggesting feature selection greater eect input better quality.
(2) traverse scheme synPth leads better model linPth, reected
dierence Sem-F1 Sem-F1 /LAS scores them, indicating integrative
SRL approach sensitive path along argument candidates traversed.
dierence Sem-F1 /LAS scores, instance, range 7.14%8.75%
0.91%1.21% initial selected feature sets, respectively. signicant
advantage synPth conrmed consistently, even though optimized feature set narrows
performance discrepancy two radically. (3) result
Nomi-F1 xN Verb-F1 xP higher corresponding F1 xN +P consistently throughout
almost experimental settings except one shows feature selection separately
Nombank PropBank (for verbal nominal predicates, respectively) gives better
performance combination Nombank+PropBank both.
explained interference two data sets due heterogeneous
nature, namely, interference nominal verbal predicate samples. Hence,
optimizing feature set specically particular type predicates eective
both. (4) overall comparison systems SRL performance three syntactic
inputs dierent quality (as reected LAS) shows performance whole
varies accord quality input. exhibited contrast Sem-F1
scores inputs, even though small LAS dierence may necessarily lead
signicant performance dierence (for instance, MST LAS 0.89 percentage point
lower J&N gives Sem-F1 score high one four experimental settings).
table also shows LAS dierence 11.61 percentage points, 88.39% 100%,
corresponds Sem-F1 score dierence 5.14 percentage points, 80.88%
86.02%, best setting (i.e., using selected feature set taking synPth).
However, Sem-F1 scores cannot trusted faithfully reect competence
semantic parser, quality syntactic input also decisive factor decide
scores. reason, Sem-F1 /LAS ratio evaluation metric.
Interestingly, parsers scores ratio two syntactic inputs LAS 10.82
11.61 percentage points gold standard are, contrarily, 4.575.52 percentage points
higher. certainly mean parser able rescue, sense, true
semantic parses erroneous syntactic input. Instead, explained
parsers high tolerance imperfections syntactic input.
Table 7 presents experimental results feature ablation feature set combination. former examine eect sense features latter feature
219

fiZhao, Zhang & Kit

optimization. Along synPth, ablation sense feature mix two feature
sets respectively optimized (through greedy selection) NomBank PropBank
lead signicant performance loss 0.75%0.83%, comparison performance

feature set SN
+P optimized combination two treebanks given Table
6. Along linPth, lead much less signicant insignicant loss, respestively.
results show sense features greedy selection features
signicant joining adaptive pruning along synPth achieve performance gain.
7.3 Comparison Analysis
order evaluate parser impartially comparative manner, performance along
synPth compared state-of-the-art systems CoNLL-2008.
chosen comparison ranked among top four among participants
shared task using sophisticated joint learning techniques. one Titov,
Henderson, Merlo, Musillo (2009) adopts similar joint learning approach
Henderson, Merlo, Musillo, Titov (2008) also included, signicant
methodological dierence others. particular, former attained best
performance date direction genuine joint learning. reported performance
systems CoNLL-2008 test set terms series F1 scores presented
Table 8 comparison. signicantly better (t = 14.6, P < 0.025) others
except post-evaluation result Johansson Nugues (2008). Contrary best
three systems CoNLL-2008 (Johansson & Nugues, 2008; Ciaramita, Attardi, DellOrletta,
& Surdeanu, 2008; Che et al., 2008) use SRL pipelines, current work intended
integrate one. Another baseline, namely, current model using feature
set work Zhao Kit (2008), instead random set, also included
table comparison, showing signicant performance enhancement top previous
model and, then, enhancement greedy feature selection.
Although work draws necessary support basic techniques (especially
traverse along synP th) underlying previous systems CoNLL-2008 -2009
(Zhao & Kit, 2008; Zhao, Chen, Kit, & Zhou, 2009; Zhao, Chen, Kazama, Uchimoto, &
Torisawa, 2009), marks uniqueness SRL sub-tasks performed one
integrative model one selected feature set. previous systems dealt predicate
disambiguation separate sub-task. rst attempt fully integrated SRL
system.
fact integrated system yet give performance par postevaluation result Johansson Nugues (2008) seems attributable number factors,
including ad hoc features adopted work handle linguistic constructions
raising/control coordination. However, noticeable ones following
discrepancies two systems, addition pipeline vs. all-in-one integration.
(1) n-best syntactic candidates input, without doubt provide
useful information 1-best use. (2) Then, exploit reranking joint
learning strategy make fuller use n-best candidates intermediate semantic
result available, resulting gain 0.5% increment Sem-F1 score. (3)
use respective sub-systems deal verbal nominal predicates specic
manner, following observation adaptive optimization feature sets nominal
220

fiSemantic Dependency Parsing

Systemsa

LAS

Ours:Gold
Johansson:2008*d
Ours:MST
Ours:Johansson
Johansson:2008
Ours:Baselinee
Ciaramita:2008*
Che:2008
Zhao:2008*
Ciaramita:2008
Titov:2009
Zhao:2008
Henderson:2008*
Henderson:2008

100.0
89.32
88.39
89.28
89.32
88.39
87.37
86.75
87.68
86.60
87.50
86.66
87.64
86.91

SemF1
86.02
81.65
80.91
80.88
80.37
79.42
78.00
78.52
76.75
77.50
76.10
76.16
73.09
70.97

MacroF1
92.27
85.49
85.09
85.12
84.86
84.34
82.69
82.66
82.24
82.06
81.80
81.44
80.48
79.11

Sem-F1
/LAS
86.02
91.41
91.54
90.59
89.98
89.85
89.28
90.51
87.53
89.49
86.97
87.88
83.40
81.66

PredF1 b
89.25
87.22
87.15
86.47
85.40
86.60
83.46
85.31
78.52
83.46

78.26
81.42
79.60

ArguF1 c
84.54
79.04
78.01
78.29
78.02
76.08
75.35
75.27
75.93
74.56

75.18
69.10
66.83

VerbF1
90.27
84.78
83.23
83.71
84.45
81.71
80.93
80.46
78.81
80.15

77.67
75.84
73.80

NomiF1
80.20
77.12
77.52
76.79
74.32
76.07
73.80
75.18
73.59
73.17

73.28
68.90
66.26

a.
b.
c.
d.

Ranked according Sem-F1 , rst authors listed sake space limitation.
Labeled F1 predicate identication classication.
Labeled F1 argument identication classication.
superscript * indicates post-evaluation results, available ocial website CoNLL2008 shared task http://www.yr-bcn.es/dokuwiki/doku.php?id=conll2008:start.
e. Syntactic input traverse scheme: Ours:MST; Features: Zhao:2008

Table 8: Performance comparison best existing SRL systems
verbal predicates respectively likely give better performance
mix both. observation also conrmed evidence experimental results:
F1 xN F1 xP scores consistently higher respective F1 xN +P ones Table 6 above.
integrative nature approach, however, priority given
optimizing whole feature set verbal nominal predicates. nevertheless
understood point potential ways enhance system, e.g.,
taking advantage specialized feature sets various kinds words and/or utilizing
joint learning techniques syntactic-semantic reranking, way integrity
system maintained properly.
dierence joint learning work Johansson Nugues (2008)
Titov et al. (2009) worth noting. former kind cascade-style joint
learning rst syntactic submodel provide n-best syntactic trees
semantic submodel infer correspondent semantic structures, reranking model,
log probabilities syntactic trees semantic structures features, nd
best joint syntactic-semantic analysis, resulting improvement top individual
submodels. contrast former non-synchronous pipeline syntactic
semantic parsing, latter adopts stricter all-in-one strategy joint learning,
syntactic semantic dependencies learnt decoded synchronously, based
augmented version transition-based shift-reduce parsing strategy (Henderson et al.,
2008). Regrettably, however, performance approach still far top
ranked list Table 8, indicating particular signicance current work.
221

fiZhao, Zhang & Kit

Whether worth integrating form joint-learning integrative system
depends cost-eectiveness so. illustrated
joint learning lead certain performance improvement, CoNLL shared task
SRL successive works, e.g., Johansson Nugues (2008). However, great deal
computational cost paid order enable reranking procedure handle
multiple syntactic inputs. certainly makes impractical real applications,
mention integrative system born particularly strong demand integrity
preclude accommodating stand-alone submodel.

8. Conclusion
Semantic parsing, aims derive instantiate semantic structure sentence
via identifying semantic relations words, plays critical role deep processing
natural language. article, presented integrative approach semantic
dependency parsing form semantic role labeling, implementation all-inone word pair classier, comprehensive evaluation using three syntactic inputs
dierent quality. evaluation results conrm eectiveness practicality
approach. major contributions research following. exhibits signicant
success rst time integrative SRL system achieved performance next
best pipeline system, indicating potentials integrative approach
besides practicality real applications. large-scale feature selection engineering
underlying success work also demonstrates (1) largest feature space ever
use eld formed allowing wide range exible (re)combinations basic
elements extracted known features properties input words (2)
speedy adaptive feature selection procedure formulated applied select
eective set features allowable feature space.
core techniques contributed success developed based two
types traverse path, along syntactic tree branches vs. linear input word sequence.
argument candidate pruning feature selection performed along identical path.
strategy using auxiliary labels facilitate argument candidate pruning, following
observation true arguments tend close predicates, works well
traverse schemes. Interestingly, although feature selection procedure outputs two
dierent feature sets NomBank, PropBank combination whilst working
along two paths, feature sets lead SRL system close performance
test data, competitive performance top one best pipeline system,
conrming robustness eectiveness feature selection procedure.
Evidence also presented evaluation results reconrm nding previous works semantic parsing feature sets optimized specically verbal nominal
predicates outperform collective one both. However, competitive performance
collective one arrived also suggests harmonious rival feature set
types predicate whole reachable slight performance dierence
specic sets fairly acceptable unavoidable small cost exchange
higher integrity practicality integrative SRL system. competitiveness
attributable least two main factors. One large feature space use,
provides dozen times many feature templates previous
222

fiSemantic Dependency Parsing

works (e.g., see Xue & Palmer, 2004; Xue, 2006). classier
accommodate many features one model. According experience piece
work, model vulnerable use many overlapping features,
SVM margin-based learners usually suer lot.

Acknowledgments
research reported article partially supported Department Chinese, Translation Linguistics, City University Hong Kong, post-doctorate
research fellowship rst author research grant (CTL UNFD-GRF-144611)
third corresponding author, National Natural Science Foundation China
(Grants 60903119 61170114), National Basic Research Program China (Grant
2009CB320901), National High-Tech Research Program China (Grant 2008AA02Z315),
Research Grants Council HKSAR, China (Grant CityU 144410), City University Hong Kong (Grant 7002796). Special thanks owed Richard Johansson
kindly providing syntactic output CoNLL-2008 shared task, three anonymous
reviewers insightful comments John S. Y. Lee help.

Appendix A. Feature Templates Importance Rankings

Type
Predicate

Argument

A05
AA,
C-A04
R-A04
R-AA
AM-PRD
AM-PRT
AM-REC
AM-TM
AM-TMP

PropBank
0121 (21)
AM-ADV C-AM-ADV
AM-CAU C-AM-CAU
AM-DIR C-AM-DIR
AM-DIS C-AM-DIS
AM-EXT C-AM-EXT
AM-LOC C-AM-LOC
AM-MNR C-AM-MNR
AM-MOD C-AM-NEG
AM-NEG C-AM-PNC
AM-PNC C-AM-TMP

R-AM-ADV
R-AM-CAU
R-AM-DIR
R-AM-EXT
R-AM-LOC
R-AM-MNR
R-AM-PNC
R-AM-TMP
C-R-AM-TMP
SU (54)

Extra/Auxiliary
NONE PRED
NONE ARG

Total
22

noMoreArg
(for synPth)

56

noMoreLeftArg
noMoreRighArg
(for linPth)

57

Table 9: list class labels predicate argument

Template
Rank in:
p.lm.dprel
a:p|dpPath.dprel
a.lemma + p.lemma
a.lemma + a.dprel + a.h.lemma
a.spLemma + p.spLemma


SN
+P
41
35
10
55
4


SN
39
31
44
40
97

SPs
6
52
4
49
15

l
SN
+P
82
2
5
112
13

Table 10: Overlap six resulted feature template sets

223

l
SN
113
62
36
69
68

SPl
60
2
6
44
26

fiZhao, Zhang & Kit

Template
Rank in:
p1 .pos + p.pos
p1 .spLemma
p.spForm + p.lm.spPos + p.noFarChildren.spPos.bag + p.rm.spPos
a.isCurPred.lemma
a.isCurPred.spLemma
a:p|existCross
a:p|dpPath.dprel.bag
a:p|dpPathPred.spForm.bag
a:p|dpPath.spLemma.seq
a:p|linePath.spForm.bag
a.semdprel = A0 ?


SN
+P
2
27
7
83
36
48
47
97
67
85
50


SN
37
13
45
94
38
77
14
24
59
48
86

SPs
79
59
63
75
86
82
85
5
71
61
40



Table 11: Overlap SN
, SPs SN
+P besides Table 10

Template
Rank in:
p.spLemma + p.currentSense
p.currentSense + a.spLemma
p.voice + (a:p|direction)
p.children.dprel.noDup
p.rm.dprel
p.rm.form
p1 .spLemma + p.spLemma
p.voice
p.form + p.children.dprel.noDup
p.lm.form + p.noFarChildren.spPos.bag + p.rm.form
p.lemma
p.lemma + p1 .lemma
p.spForm
p.spForm + p.children.dprel.bag
p.spForm + p.lm.spForm + p.noFarChildren.spPos.bag + p.rm.spForm
p.splemma
p.spLemma + p.h.spForm
p.spLemma + p1 .spLemma
p1 .pos
a1 .isCurPred.lemma
a1 .isCurPred.lemma + a.isCurPred.lemma
a1 .isCurPred.spLemma + a.isCurPred.spLemma
a.isCurPred.Lemma + a1 .isCurPred.Lemma
a.isCurPred.spLemma + a1 .isCurPred.spLemma
a.spPos.baseline Ax + a.voice + (a:p|direction)
a.spPos.baseline Mod
a.h.children.dprel.bag
a.lm1 .spPos
a.lm1 .lemma
a.children.spPos.seq + p.children.spPos.seq
a.rm.dprel + a.pos
a.rm.dprel + a.spPos
a.rm1 .spPos
a.rm.lemma

224

l
SN
+P
18
33
65
11
60
113
38
26
96
88
4
7
39
91
104
9
100
72
76
67
42
14
29
50
24
86
97
47
49
19
21
30
6
36

l
SN
28
57
120
54
114
110
61
4
81
106
26
5
100
6
10
65
11
112
104
109
24
89
44
45
9
80
35
63
30
90
17
7
74
50

SPl
56
17
25
40
3
80
69
10
65
5
50
34
36
30
14
64
70
33
28
77
43
62
67
59
46
18
45
49
68
76
24
22
15
4

fiSemantic Dependency Parsing

28
27
3
75
12
53
32
94
16
79
43
110

a.rn.dprel + a.spPos
a1 .lemma + a.lemma
a:p|dpPathArgu.dprel.seq
a:p|dpPathArgu.pos.seq
a:p|dpPathPred.dprel.seq
a.form
a.form + a.pos
a.form + a1 .form
a.spForm + a.spPos
a.spForm + a1 .spForm
a.spLemma + a.dprel
a.spLemma + a.h.spForm

33
46
96
79
64
94
93
31
73
38
118
2

72
37
1
9
35
78
32
38
48
52
7
51

l
l
Table 12: Overlap SN
, SPl SN
+P besides Table 10

Template
p.lemma + p.currentSense
p.currentSense + a.lemma
a.form + p.semdprel ctype ?
a.form + p.semdprel rtype ?
p.lm.form
p1 .form + p.form
p2 .form
p2 .spForm + p1 .spForm
p.form + p.dprel
p.lemma + p.h.form
p.spForm + p.dprel
p.spLemma + p.children.dprel.noDup
p.spLemma + p1 .spLemma
a.voice + (a:p|direction)
leaf syntactic tree ?
a.lm.dprel + a.spPos
a.lm.pos + a.pos
a.pphead.spLemma
a.rm1 .form
a.rm1 .spPos
a.highSupportVerb.form
a.lowSupportPorp.form
a.lowSupportPorp.spLemma
a1 .pos
a1 .spForm
a:p|dpPath.distance
a:p|dpPathArgu.spLemma.bag
a:p|dpPathPred.spPos.bag
a:p|linePath.dprel.bag
a.form + a.children.pos.seq
a.form + a.pos
a.spForm + a.children.spPos.seq
a.spForm + a.spPos
a.spLemma
a.spLemma + a1 .spLemma

Rank
82
57
3
5
47
71
78
15
74
10
46
43
49
23
16
67
50
19
81
79
56
51
69
70
85
9
96
93
88
53
1
76
87
11
60

225

Template
p.spLemma + p.currentSense
p.currentSense + a.spLemma
a.form + p.ctypeSemdprel
a.form + p.rtypeSemdprel
p.lm.spForm
p1 .spLemma + p.spLemma
p2 .spForm
p.form
p.lemma
p.pos
p.spForm + p.children.dprel.bag
p.spLemma + p.h.spForm
p1 .pos
a.children.adv.bag
a.lm.dprel + a.form
a.lm1 .spLemma
a.lm.spPos
a.rm.dprel + a.spPos
a.rm1 .spForm
a.rn.dprel + a.spForm
a.highSupportVerb.spForm
a.lowSupportPorp.lemma
a1 .lemma + a1 .lemma
a1 .pos + a.pos
a1 .spPos + a1 .spPos
a:p|dpPath.spLemma.bag
a:p|dpPathPred.spLemma.bag
a:p|dpPathArgu.dprel.seq
a.semdprel = A2 ?
a.form + a.form
a.pos + a.children.spPos.seq
a.spForm + a.children.spPos.bag
a.spForm + a1 .spForm
a.spLemma + a.pphead.spForm
a.spPos + a.dprel + a.h.spPos

Rank
80
18
4
6
7
92
61
68
63
62
90
27
28
95
75
100
8
26
55
32
99
91
20
84
98
73
2
22
35
58
12
65
52
64
41

fiZhao, Zhang & Kit

a1 .form
54
a1 .spForm
a1 .spPos
33
(a:p|dpTreeRelation) + p.form
(a:p|dpTreeRelation) + p.spPos
29
(a:p|dpTreeRelation) + a.spPos
(a:p|dpPath.dprel.seq) + p.spForm
36
a1 .isCurPred.spLemma + a.isCurPred.spLemma
a.noFarChildren.spPos.bag + a.rm.spPos
a.children.spPos.seq + p.children.spPos.seq
a.highSupportNoun:p|dpPath.dprel.seq
(a.highSupportNoun:p|dpTreeRelation) + p.form
(a.highSupportVerb:p|dpTreeRelation) + a.spForm
(a.lowSupportVerb:p|dpTreeRelation) + a.spForm

83
25
30
17
21
34
89
66
72
42


Table 13: Feature templates SN
besides Tables 10 11

Template
Rank
Template
p.rm.dprel
47
p.dprel
p.children.dprel.bag
66
p.lm.spPos
p.children.pos.seq
70
p.rm.dprel
p2 .pos
23
p2 .spForm + p1 .spForm
p.dprel = OBJ ?
50
p.lemma + p.h.form
p.lemma+p1 .lemma
3
p.pos
p.spForm
76
p.spForm + p.children.dprel.noDup
p.splemma
9
p.spLemma+p1 .spLemma
p1 .spPos
21
a.lowSupportVerb:p|dpTreeRelation
a.children.adv.bag
20
a.dprel
a.children.dprel.bag
7
a.h.lemma
a.h.spLemma
72
a.lm.dprel + a.spPos
a.lm1 .spLemma
54
a.pphead.lemma
a.pphead.spLemma
46
a.rm.dprel + a.spPos
a1 .lemma + a1 .lemma
16
a1 .pos
a1 .spLemma + a.spLemma
29
a:p|linePath.distance
a:p|dpPath.distance
22
a:p|dpPathPred.dprel.bag
a:p|dpPath.spForm.seq
12
a:p|dpPathArgu.spForm.seq
a:p|dpPathArgu.spLemma.bag
84
a:p|dpPathPred.spLemma.bag
a:p|dpPathArgu.spLemma.seq
17
a:p|dpPath.spPos.bag
a:p|dpPathPred.spPos.bag
64
a:p|dpPathArgu.dprel.seq
a:p|linePath.spLemma.seq
42
a:p|linePath.spLemma.bag
a:p|dpPathPred.spPos
62
a.existSemdprel A0
a.existSemdprel A1
56
a.existSemdprel A2
a.semdprel = A2 ?
77
a.dprel = OBJ ?
a.form + a.children.pos.seq
10
a.pos + p.pos
a.spLemma + a.dprel
87
a.spLemma+a.dprel+a.h.spLemma
a1 .lemma
14
a1 .spPos
(a:p|dpTreeRelation) + a.spPos
81
(a:p|dpPath.dprel.seq) + p.spPos
a1 .isCurPred.spLemma + a.isCurPred.spLemma
a2 .isCurPred.lemma + a1 .isCurPred.lemma
a.isCurPred.spLemma + a1 .isCurPred.spLemma
a.lowSupportVerb:p|dpPath.dprel.seq
a.lowSupportVerb:p|dpPathArgu.dprel.seq
a.lowSupportVerb:p|dpPathArgu.spPos.seq
a.lowSupportVerb:p|dpPathShared.dprel.seq

226

Rank
25
48
51
43
68
26
60
1
32
13
8
31
80
78
24
55
53
11
2
65
28
27
67
57
73
45
19
69
18
41
58
74
33
34
35
36

fiSemantic Dependency Parsing

a.lowSupportVerb:p|dpPathShared.spPos.seq
a.lowSupportVerb:p|dpPathPred.dprel.seq
a.lowSupportVerb:p|dpPathPred.spPos.seq
a.highSupportNoun:p|dpPath.dprel.seq
a.lowSupportVerb:p|dpPath.dprel.seq
(a.highSupportVerb:p|dpTreeRelation) + a.spPos

37
38
39
83
30
44

Table 14: Feature templates SPs besides Tables 10 11

Template
Rank
Template
p.lemma + p.currentSense
100
p.currentSense + a.lemma
a.form + p.semdprel ctype ?
90
a.form + p.ctypeSemdprel
a.form + p.semdprel rtype ?
92
a.form + p.rtypeSemdprel
p.dprel
8
p.children.pos.seq
p.rm.dprel
46
p.lowSupportProp:p|dpTreeRelation
p1 .spForm + p.spForm
54
p.voice
p.lemma+p1 .lemma
18
p.pos + p.dprel
p.splemma
88
p.spLemma+p.h.spForm
p.spPos + p.children.dprel.bag
15
p.spPos + p1 .spPos
p1 .spForm
26
a1 .isCurPred.lemma
a.isCurPred.pos
84
a.isCurPred.spPos
a1 .isCurPred.Lemma
37
a1 .isCurPred.spLemma
a:p|direction
57
(a:p|dpPath.dprel.seq) + a.spForm
a.form.baseline Mod
73
a.pos.baseline Mod
a.spForm.baseline Mod
75
a.baseline Mod
a.lm.Lemma
59
a.lm.spForm
a.lm.spPos
65
a.rm.lemma
a.highSupportNoun.pos
62
a.highSupportNoun.spPos
a.highSupportVerb.spPos
42
a.lowSupportNoun.pos
a.lowSupportPorp.spLemma
98
a.lowSupportVerb.pos
a1 .spLemma+a.spLemma
38
a:p|dpPathPred.spLemma.seq
a:p|linePath.spForm.seq
80
a:p|linePath.spLemma.seq
a:p|linePath.spLemma.bag
86
a:p|linePath.spPos.seq
a:p|linePath.spPos.bag
66
a:p|dpPathPred.spPos
a.existSemdprel A0
49
a.existSemdprel A1
a.form
94
a.form = p.form ?
a.form + a.form
95
a.lemma
a.lemma + a.dprel
21
a.lemma + a.h.form
a.lemma + a.pphead.form
44
a.spForm = p.spForm ?
a.spLemma + a.pphead.spForm
24
a.spPos + a.spPos
(a:p|dpPath.dprel.seq) + p.form
45
(a:p|dpPath.dprel.seq) + p.spForm
(a:p|dpPath.dprel.seq) + a.form
13
p.lm.form + p.noFarChildren.spPos.bag + p.rm.form
a2 .isCurPred.lemma + a1 .isCurPred.lemma
a.isCurPred.pos + a1 .isCurPred.pos
a.isCurPred.spLemma + a1 .isCurPred.spLemma
a.form.baseline Ax + a.voice + (a:p|direction)
a.spForm.baseline Ax+ a.voice + (a:p|direction)
a.spPos.baseline Ax + a.voice + (a:p|direction)
a.highSupportNoun:p|dpPathShared.dprel.seq
a.highSupportVerb:p|dpPathShared.dprel.seq

227

Rank
61
91
93
6
12
9
5
3
14
28
96
22
11
74
76
60
81
20
87
56
63
53
51
39
1
40
43
29
89
82
25
52
64
99
23
77
78
79
30
68

fiZhao, Zhang & Kit

16
31
32
33
34
17
69
70
71
72
58
19

a.lowSupportNoun:p|dpPath.dprel.seq
a.lowSupportNoun:p|dpPathArgu.dprel.seq
a.lowSupportNoun:p|dpPathArgu.spPos.seq
a.lowSupportNoun:p|dpPathShared.dprel.seq
a.lowSupportNoun:p|dpPathShared.spPos.seq
a.lowSupportNoun:p|dpPathPred.dprel.seq
a.lowSupportVerb:p|dpPathArgu.dprel.seq
a.lowSupportVerb:p|dpPathArgu.spPos.seq
a.lowSupportVerb:p|dpPathShared.dprel.seq
a.lowSupportVerb:p|dpPathShared.spPos.seq
(a.highSupportVerb:p|dpTreeRelation) + a.form
(a.lowSupportNoun:p|dpTreeRelation) + p.spPos

Table 15: Feature templates SN
+P besides Tables 10 11

Template
Rank
Template
p1 .spLemma
74
p2 .form
p1 .spPos
19
a1 .isCurPred.Lemma
a1 .isCurPred.spLemma
53
a.children.dprel.bag
a.h.lemma
23
a.lm.dprel + a.pos
a.lm1 .lemma
31
a.lm.Lemma
a.pphead.lemma
27
a.pphead.spLemma
a.lowSupportNoun.spPos
8
a.lowSupportPorp.form
a.lowSupportPorp.lemma
47
a.lowSupportPorp.spForm
a.lowSupportPorp.spLemma
57
a1 .spPos
a1 .spPos + a1 .spPos
54
a.semdprel = A2 ?
(a:p|dpTreeRelation) + p.pos
41
(a:p|dpTreeRelation) + p.spPos
a2 .isCurPred.spLemma + a1 .isCurPred.spLemma
a.lowSupportPorp:p|dpPathShared.dprel.seq
a.lowSupportPorp:p|dpPathShared.spPos.seq
a.lowSupportVerb:p|dpPath.dprel.seq
(a.highSupportVerb:p|dpTreeRelation) + a.form
(a.lowSupportNoun:p|dpTreeRelation) + p.pos
(a.lowSupportNoun:p|dpTreeRelation) + p.spPos

Rank
55
71
42
63
29
39
73
79
58
20
21
61
12
13
16
11
75
66

Table 16: Feature templates SPl besides Tables 10 12

Template
p.rm.dprel
p.lowSupportNoun.spForm
p1 .form + p.form
p1 .pos+p.pos
p1 .spLemma
p2 .pos
p.dprel = OBJ ?
p.lemma + p.h.form
p.spPos + p1 .spPos
a.voice + (a:p|direction)
a.isCurPred.spLemma

Rank
88
16
103
32
13
18
59
42
34
75
29

228

Template
p.children.dprel.seq
p.lowSupportProp:p|dpTreeRelation
p1 .lemma + p.lemma
p1 .spForm + p.spForm
p2 .form + p1 .form
p2 .spForm
p.form + p.dprel
p.pos + p.dprel
p1 .spForm
a.isCurPred.lemma
a.lm.dprel + a.dprel

Rank
27
72
91
40
99
39
95
1
86
43
98

fiSemantic Dependency Parsing

a.lm.dprel + a.pos
76
a.lm1 .spLemma
a.lm.pos + a.pos
19
a.lm.spForm
a.lm.spPos
49
a.ln.dprel + a.pos
a.rm1 .spPos
21
a.highSupportNoun.lemma
a.highSupportNoun.pos
48
a.highSupportNoun.spPos
a.lowSupportVerb.pos
97
a.lowSupportVerb.spLemma
a.lowSupportVerb.spPos
12
a1 .lemma
a1 .spLemma+a.spLemma
77
a2 .pos
a:p|linePath.distance
67
a:p|dpTreeRelation
a:p|dpPathPred.spPos
115
a.dprel = OBJ ?
a.form + p.form
83
a.pos + p.pos
a.spForm + p.spForm
87
a.spForm + a.children.spPos.seq
a.spForm + a.children.spPos.bag
119
a.spLemma+a.dprel+a.h.spLemma
a.spLemma + a.pphead.spForm
66
a.spLemma + a1 .spLemma
a1 .pos
52
a1 .spPos
(a:p|dpTreeRelation) + p.form
111
(a:p|dpTreeRelation) + p.spForm
(a:p|dpTreeRelation) + a.form
84
(a:p|dpTreeRelation) + a.spForm
(a:p|dpTreeRelation) + a.spPos
15
(a:p|dpPath.dprel.seq) + p.form
(a:p|dpPath.dprel.seq) + p.spForm
108
(a:p|dpPath.dprel.seq) + a.form
(a:p|dpPath.dprel.seq) + a.spForm
37
p.spForm + p.lm.spPos + p.noFarChildren.spPos.bag + p.rm.spPos
a2 .isCurPred.lemma + a1 .isCurPred.lemma
(a1 :p|direction) + (a2 :p|direction)
a.noFarChildren.spPos.bag + a.rm.spPos
a.highSupportVerb:p|dpTreeRelation
(a.highSupportVerb:p|dpTreeRelation) + a.form
(a.lowSupportNoun:p|dpTreeRelation) + p.form
(a.lowSupportNoun:p|dpTreeRelation) + p.spForm

3
107
25
14
51
78
101
102
20
116
92
53
60
55
23
8
41
56
70
117
58
105
22
85
47
82
71

l
Table 17: Feature templates SN
besides Tables 10 12

Template
p.currentSense + a.spPos
p.lm.form
p.lowSupportNoun.spForm
p1 .form + p.form
p1 .spForm + p.spForm
p2 .pos
p.form + p.dprel
p.spPos + p1 .spPos
p1 .spPos
a.isCurPred.lemma
a1 .isCurPred.Lemma
a.children.dprel.bag
a.lm1 .lemma
a.lm.Lemma
a.lm.spForm
a.ln.dprel + a.pos
a.lowSupportNoun:p|dpTreeRelation
a1 .lemma
a1 .spPos

Rank
69
101
99
106
98
87
114
45
102
52
41
48
20
84
34
63
93
81
109

229

Template
p.rm.dprel
p.lm.spForm
p.lowSupportProp:p|dpTreeRelation
p1 .pos+p.pos
p2 .form + p1 .form
p2 .spForm
p.spForm + p.dprel
p1 .spForm
a.voice + (a:p|direction)
a.isCurPred.spLemma
a1 .isCurPred.spLemma
a.lm.dprel + a.dprel
a.lm1 .spLemma
a.lm.pos + a.pos
a.lm.spPos
a.rm1 .spPos
a.lowSupportVerb.spLemma
a1 .spLemma+a.spLemma
a1 .spPos + a1 .spPos

Rank
117
51
74
1
40
54
115
37
10
66
64
70
17
8
59
111
15
31
92

fiZhao, Zhang & Kit

a:p|linePath.distance
80
a:p|dpTreeRelation
a:p|dpPathPred.spPos
85
a.existSemdprel A2
a.semdprel = A2 ?
78
a.spForm + a.children.spPos.seq
a.spForm + a.children.spPos.bag
61
a.spLemma+a.dprel+a.h.spLemma
a.spLemma + a.pphead.spForm
62
a1 .lemma
a1 .spPos
44
(a:p|dpTreeRelation) + a.form
(a:p|dpTreeRelation) + a.spForm
73
(a:p|dpTreeRelation) + a.spPos
(a:p|dpPath.dprel.seq) + p.form
22
(a:p|dpPath.dprel.seq) + p.spForm
(a:p|dpPath.dprel.seq) + a.form
89
(a:p|dpPath.dprel.seq) + a.spForm
p.spForm + p.lm.spPos + p.noFarChildren.spPos.bag + p.rm.spPos
a2 .isCurPred.lemma + a1 .isCurPred.lemma
a2 .isCurPred.spLemma + a1 .isCurPred.spLemma
a.noFarChildren.spPos.bag + a.rm.spPos
a.highSupportNoun:p|dpPath.dprel.seq
a.lowSupportVerb:p|dpPath.dprel.seq
(a.highSupportNoun:p|dpTreeRelation) + p.form
(a.highSupportNoun:p|dpTreeRelation) + p.spForm
(a.lowSupportNoun:p|dpTreeRelation) + p.spPos
(a.lowSupportVerb:p|dpTreeRelation) + a.form
(a.lowSupportVerb:p|dpTreeRelation) + a.spForm

57
77
71
90
68
25
58
83
103
108
23
46
95
55
35
116
118
56
105
107

l
Table 18: Feature templates SN
+P besides Tables 10 12

References
Carreras, X., & Marquez, L. (2005). Introduction CoNLL-2005 shared task: Semantic role labeling. Proceedings Ninth Conference Computational Natural
Language Learning, pp. 152164, Ann Arbor, Michigan.
Che, W., Li, Z., Hu, Y., Li, Y., Qin, B., Liu, T., & Li, S. (2008). cascaded syntactic
semantic dependency parsing system. Proceedings Twelfth Conference
Computational Natural Language Learning, pp. 238242, Manchester.
Chen, S. F., & Rosenfeld, R. (1999). Gaussian prior smoothing maximum entropy
models. Technical report CMU-CS-99-108, School Computer Science, Carnegie
Mellon University.
Chen, W., Kawahara, D., Uchimoto, K., Zhang, Y., & Isahara, H. (2008). Dependency
parsing short dependency relations unlabeled data. Proceedings
Third International Joint Conference Natural Language Processing, Vol. 1, pp.
8894, Hyderabad, India.
Ciaramita, M., Attardi, G., DellOrletta, F., & Surdeanu, M. (2008). DeSRL: lineartime semantic role labeling system. Proceedings Twelfth Conference
Computational Natural Language Learning, pp. 258262, Manchester.
Crammer, K., & Singer, Y. (2003). Ultraconservative online algorithms multiclass problems. Journal Machine Learning Research, 3 (Jan), 951991.
230

fiSemantic Dependency Parsing

Dang, H. T., & Palmer, M. (2005). role semantic roles disambiguating verb senses. Proceedings 43rd Annual Meeting Association Computational
Linguistics, pp. 4249, Ann Arbor, Michigan.
Ding, W., & Chang, B. (2008). Improving Chinese semantic role classication hierarchical feature selection strategy. Proceedings 2008 Conference Empirical
Methods Natural Language Processing, pp. 324333, Honolulu, Hawaii.
Gildea, D., & Jurafsky, D. (2002). Automatic labeling semantic roles. Computational
Linguistics, 28 (3), 245288.
Hajic, J., Ciaramita, M., Johansson, R., Kawahara, D., Mart, M. A., Marquez, L., Meyers,
A., Nivre, J., Pado, S., Stepanek, J., Stranak, P., Surdeanu, M., Xue, N., & Zhang,
Y. (2009). CoNLL-2009 shared task: Syntactic semantic dependencies
multiple languages. Proceedings Thirteenth Conference Computational
Natural Language Learning: Shared Task, pp. 118, Boulder, Colorado.
Henderson, J., Merlo, P., Musillo, G., & Titov, I. (2008). latent variable model
synchronous parsing syntactic semantic dependencies. Proceedings
Twelfth Conference Computational Natural Language Learning, pp. 178182,
Manchester.
Jiang, Z. P., & Ng, H. T. (2006). Semantic role labeling NomBank: maximum entropy
approach. Proceedings 2006 Conference Empirical Methods Natural
Language Processing, pp. 138145, Sydney.
Johansson, R., & Nugues, P. (2008). Dependency-based syntacticsemantic analysis
PropBank NomBank. Proceedings Twelfth Conference Computational
Natural Language Learning, pp. 183187, Manchester.
Koo, T., Carreras, X., & Collins, M. (2008). Simple semi-supervised dependency parsing. Proceedings 46th Annual Meeting Association Computational
Linguistics: Human Language Technologies, pp. 595603, Columbus, Ohio.
Koomen, P., Punyakanok, V., Roth, D., & Yih, W.-T. (2005). Generalized inference
multiple semantic role labeling systems. Proceedings Ninth Conference
Computational Natural Language Learning, pp. 181184, Ann Arbor, Michigan.
Liu, C., & Ng, H. T. (2007). Learning predictive structures semantic role labeling
NomBank. Proceedings 45th Annual Meeting Association Computational Linguistics, pp. 208215, Prague.
Marcus, M. P., Santorini, B., & Marcinkiewicz, M. A. (1993). Building large annotated
corpus English: Penn Treebank. Computational Linguistics, Special Issue
Using Large Corpora: II, 19 (2), 313330.
Marquez, L., Surdeanu, M., Comas, P., & Turmo, J. (2005). robust combination strategy
semantic role labeling. Proceedings Human Language Technology Conference
Conference Empirical Methods Natural Language Processing, pp. 644651,
Vancouver.
McDonald, R., & Pereira, F. (2006). Online learning approximate dependency parsing
algorithms. Proceedings Eleventh Conference European Chapter
Association Computational Linguistics, pp. 8188, Trento, Italy.
231

fiZhao, Zhang & Kit

McDonald, R., Pereira, F., Ribarov, K., & Hajic, J. (2005). Non-projective dependency
parsing using spanning tree algorithms. Proceedings Human Language Technology
Conference Conference Empirical Methods Natural Language Processing, pp.
523530, Vancouver, British Columbia.
Meyers, A., Reeves, R., Macleod, C., Szekely, R., Zielinska, V., Young, B., & Grishman, R.
(2004). NomBank project: interim report. Meyers, A. (Ed.), HLT-NAACL
2004 Workshop: Frontiers Corpus Annotation, pp. 2431, Boston.
Meza-Ruiz, I., & Riedel, S. (2009). Jointly identifying predicates, arguments senses
using Markov logic. Proceedings Human Language Technologies: 2009 Annual Conference North American Chapter Association Computational
Linguistics, pp. 155163, Boulder, Colorado.
Nash, S. G., & Nocedal, J. (1991). numerical study limited memory BFGS method
truncated-Newton method large scale optimization. SIAM Journal Optimization, 1 (2), 358372.
Nivre, J., & McDonald, R. (2008). Integrating graph-based transition-based dependency parsers. Proceedings 46th Annual Meeting Association
Computational Linguistics: Human Language Technologies, pp. 950958, Columbus,
Ohio.
Nocedal, J. (1980). Updating quasi-Newton matrices limited storage. Mathematics
Computation, 35 (151), 773782.
Palmer, M., Gildea, D., & Kingsbury, P. (2005). Proposition Bank: annotated
corpus semantic roles. Computational Linguistics, 31 (1), 71106.
Pradhan, S., Ward, W., Hacioglu, K., Martin, J., & Jurafsky, D. (2005). Semantic role
labeling using dierent syntactic views. Proceedings 43rd Annual Meeting
Association Computational Linguistics, pp. 581588, Ann Arbor, Michigan.
Pradhan, S. S., Ward, W. H., Hacioglu, K., Martin, J. H., & Jurafsky, D. (2004). Shallow
semantic parsing using support vector machines. Proceedings Human Language Technology Conference North American Chapter Association
Computational Linguistics, pp. 233240, Boston.
Punyakanok, V., Roth, D., Yih, W., & Zimak, D. (2004). Semantic role labeling via integer
linear programming inference. Proceedings 20th International Conference
Computational Linguistics, pp. 13461352, Geneva.
Pustejovsky, J., Meyers, A., Palmer, M., & Poesio, M. (2005). Merging PropBank, NomBank, TimeBank, Penn Discourse Treebank coreference. Proceedings
Workshop Frontiers Corpus Annotations II: Pie Sky, pp. 512, Ann
Arbor, Michigan.
Riedel, S. (2008). Improving accuracy eciency map inference markov logic.
Proceedings Twenty-Fourth Conference Annual Conference Uncertainty
Artificial Intelligence, pp. 468475, Corvallis, Oregon.
Roth, D., & Yih, W. (2004). linear programming formulation global inference
natural language tasks. Proceedings Eighth Conference Computational
Natural Language Learning, pp. 18, Boston.
232

fiSemantic Dependency Parsing

Surdeanu, M., Johansson, R., Meyers, A., Marquez, L., & Nivre, J. (2008). CoNLL 2008
shared task joint parsing syntactic semantic dependencies. Proceedings
Twelfth Conference Computational Natural Language Learning, pp. 159177,
Manchester.
Surdeanu, M., Marquez, L., Carreras, X., & Comas, P. R. (2007). Combination strategies
semantic role labeling. Journal Artificial Intelligence Research, 29, 105151.
Titov, I., Henderson, J., Merlo, P., & Musillo, G. (2009). Online graph planarisation
synchronous parsing semantic syntactic dependencies. Proceedings
21st International Jont Conference Artifical Intelligence, pp. 15621567, Pasadena,
California.
Toutanova, K., Haghighi, A., & Manning, C. D. (2005). Joint learning improves semantic
role labeling. Proceedings 43rd Annual Meeting Association Computational Linguistics, pp. 589596, Ann Arbor, Michigan.
Xue, N. (2006). Semantic role labeling nominalized predicates Chinese. Proceedings
Human Language Technology Conference North American Chapter
Association Computational Linguistics, Main Conference, pp. 431438, New York.
Xue, N., & Palmer, M. (2004). Calibrating features semantic role labeling. Proceedings
2004 Conference Empirical Methods Natural Language Processing, pp. 88
94, Barcelona.
Zhao, H., Chen, W., Kazama, J., Uchimoto, K., & Torisawa, K. (2009). Multilingual dependency learning: Exploiting rich features tagging syntactic semantic dependencies. Proceedings Thirteenth Conference Computational Natural Language
Learning: Shared Task, pp. 6166, Boulder, Colorado.
Zhao, H., Chen, W., Kit, C., & Zhou, G. (2009). Multilingual dependency learning: huge
feature engineering method semantic dependency parsing. Proceedings
Thirteenth Conference Computational Natural Language Learning: Shared Task,
pp. 5560, Boulder, Colorado.
Zhao, H., & Kit, C. (2008). Parsing syntactic semantic dependencies two singlestage maximum entropy models. Proceedings Twelfth Conference Computational Natural Language Learning, pp. 203207, Manchester.

233

fiJournal Artificial Intelligence Research 46 (2013) 449509

Submitted 9/12; published 03/13

Incremental Clustering Expansion Faster
Optimal Planning Decentralized POMDPs
Frans A. Oliehoek

frans.oliehoek@maastrichtuniversity.nl

Maastricht University
Maastricht, Netherlands

Matthijs T.J. Spaan

m.t.j.spaan@tudelft.nl

Delft University Technology
Delft, Netherlands

Christopher Amato

camato@csail.mit.edu

Massachusetts Institute Technology
Cambridge, MA, USA

Shimon Whiteson

s.a.whiteson@uva.nl

University Amsterdam
Amsterdam, Netherlands

Abstract
article presents state-of-the-art optimal solution methods decentralized
partially observable Markov decision processes (Dec-POMDPs), general models
collaborative multiagent planning uncertainty. Building generalized multiagent A* ( GMAA*) algorithm, reduces problem tree one-shot collaborative
Bayesian games (CBGs), describe several advances greatly expand range DecPOMDPs solved optimally. First, introduce lossless incremental clustering
CBGs solved GMAA*, achieves exponential speedups without sacrificing
optimality. Second, introduce incremental expansion nodes GMAA* search
tree, avoids need expand children, number worst case
doubly exponential nodes depth. particularly beneficial little clustering
possible. addition, introduce new hybrid heuristic representations
compact thereby enable solution larger Dec-POMDPs. provide theoretical
guarantees that, suitable heuristic used, incremental clustering incremental expansion yield algorithms complete search equivalent. Finally,
present extensive empirical results demonstrating GMAA*-ICE, algorithm
synthesizes advances, optimally solve Dec-POMDPs unprecedented size.

1. Introduction
key goal artificial intelligence development intelligent agents interact
environment order solve problems, achieve goals, maximize utility.
agents sometimes act alone, researchers increasingly interested collaborative multiagent
systems, teams agents work together perform manner tasks. Multiagent
systems appealing, tackle inherently distributed problems,
facilitate decomposition problems complex tackled single
c
2013
AI Access Foundation. rights reserved.

fiOliehoek, Spaan, Amato, & Whiteson

agent (Huhns, 1987; Sycara, 1998; Panait & Luke, 2005; Vlassis, 2007; Busoniu, Babuska, &
De Schutter, 2008).
One primary challenges multiagent systems presence uncertainty. Even
single-agent systems, outcome action may uncertain, e.g., action may fail
probability. Furthermore, many problems state environment may
uncertain due limited noisy sensors. However, multiagent settings problems
often greatly exacerbated. Since agents access sensors, typically
small fraction complete system, ability predict agents
act limited, complicating cooperation. uncertainties properly addressed,
arbitrarily bad performance may result.
principle, agents use communication synchronize beliefs coordinate
actions. However, due bandwidth constraints, typically infeasible agents
broadcast necessary information agents. addition, many realistic
scenarios, communication may unreliable, precluding possibility eliminating uncertainty agents actions.
Especially recent years, much research focused approaches (collaborative)
multiagent systems deal uncertainty principled way, yielding wide variety
models solution methods (Pynadath & Tambe, 2002; Goldman & Zilberstein, 2004;
Seuken & Zilberstein, 2008). article focuses decentralized partially observable
Markov decision process (Dec-POMDP), general model collaborative multiagent planning uncertainty. Unfortunately, solving Dec-POMDP, i.e., computing optimal
plan, generally intractable (NEXP-complete) (Bernstein, Givan, Immerman, & Zilberstein,
2002) fact even computing solutions absolutely bounded error (i.e., -approximate
solutions) also NEXP-complete (Rabinovich, Goldman, & Rosenschein, 2003). particular,
number joint policies grows exponentially number agents observations
doubly exponentially respect horizon problem.1 Though complexity results preclude methods efficient problems, developing better optimal
solution methods Dec-POMDPs nonetheless important goal, several reasons.
First, since complexity results describe worst case, still great potential
improve performance optimal methods practice. fact, evidence
many problems solved much faster worst-case complexity bound indicates
(Allen & Zilberstein, 2007). article, present experiments clearly demonstrate
point: many problems, methods propose scale vastly beyond would
expected doubly-exponential dependence horizon.
Second, computer speed memory capacity increase, growing set small
medium-sized problems solved optimally. problems arise naturally
others result decomposition larger problems. instance, may possible
extrapolate optimal solutions problems shorter planning horizons, using
starting point policy search longer-horizon problems work Eker
Akn (2013), use shorter-horizon, no-communication solutions inside problems
communication (Nair, Roth, & Yohoo, 2004; Goldman & Zilberstein, 2008). generally,
optimal policies smaller problems potentially used find good solutions larger
problems. instance, transfer planning (Oliehoek, 2010; Oliehoek, Whiteson, & Spaan,
1. Surprisingly, number states Dec-POMDP less important, e.g., brute-force search depends
number states via policy evaluation routine, scales linearly number states.

450

fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPs

2013) employs optimal solutions problems agents better solve problems
many agents. performing (approximate) influence-based abstraction influence search
(Witwicki, 2011; Oliehoek, Witwicki, & Kaelbling, 2012), optimal solutions component
problems potentially used find (near-)optimal solutions larger problems.
Third, optimal methods offer important insights nature specific Dec-POMDP
problems solutions. instance, methods introduced article enabled
discovery certain properties BroadcastChannel benchmark problem make
much easier solve.
Fourth, optimal methods provide critical inspiration principled approximation methods. fact, almost successful approximate Dec-POMDP methods based optimal
ones (see, e.g., Seuken & Zilberstein, 2007b, 2007a; Dibangoye, Mouaddib, & Chai-draa, 2009;
Amato, Dibangoye, & Zilberstein, 2009; Wu, Zilberstein, & Chen, 2010a; Oliehoek, 2010)
locally optimal ones (Velagapudi, Varakantham, Scerri, & Sycara, 2011)2 , clustering technique presented article forms basis recently introduced approximate
clustering technique (Wu, Zilberstein, & Chen, 2011).
Finally, optimal methods essential benchmarking approximate methods. recent
years, huge advances approximate solution Dec-POMDPs, leading
development solution methods deal large horizons, hundreds agents
many states (e.g., Seuken & Zilberstein, 2007b; Amato et al., 2009; Wu et al., 2010a;
Oliehoek, 2010; Velagapudi et al., 2011).
However, since computing even -approximate
solutions NEXP-complete, method whose complexity doubly exponential cannot
guarantees absolute error solution (assuming EXP6=NEXP). such,
existing effective approximate methods quality guarantees.3
Consequently, difficult meaningfully interpret empirical performance without
upper bounds optimal methods supply. approximate methods also benchmarked lower bounds (e.g., approximate methods), comparisons cannot
detect method fails find good solutions. requires benchmarking
upper bounds and, unfortunately, upper bounds easier compute, QMDP
QPOMDP, loose helpful (Oliehoek, Spaan, & Vlassis, 2008). such,
benchmarking respect optimal solutions important part verification
approximate algorithm. Since existing optimal methods tackle small problems,
scaling optimal solutions larger problems critical goal.
1.1 Contributions
article presents state-of-the-art optimal solution methods Dec-POMDPs.
particular, describes several advances greatly expand horizon many DecPOMDPs solved optimally. addition, proposes evaluates complete algorithm
synthesizes advances. approach based generalized multiagent A*
(GMAA*) algorithm (Oliehoek, Spaan, & Vlassis, 2008), makes possible reduce
problem tree one-shot collaborative Bayesian games (CBGs). appeal
2. method Velagapudi et al. (2011) repeatedly computes best responses way similar DP-JESP
(Nair, Tambe, Yokoo, Pynadath, & Marsella, 2003). best response computation, however, exploits
sparsity interactions.
3. Note refer methods without quality guarantees approximate rather heuristic avoid
confusion heuristic search, used throughout article exact.

451

fiOliehoek, Spaan, Amato, & Whiteson

approach abstraction layer introduces, led various insights DecPOMDPs and, turn, improved solution methods describe.
specific contributions article are:4
1. introduce lossless clustering CBGs, technique reduce size CBGs
GMAA* enumerates possible solutions, preserving optimality.
exponentially reduce number child nodes GMAA* search tree, leading
huge increases efficiency. addition, applying incremental clustering (IC)
GMAA*, GMAA*-IC method avoid clustering exponentially sized CBGs.
2. introduce incremental expansion (IE) nodes GMAA* search tree. Although
clustering may reduce number children search node, number
worst case still doubly exponential nodes depth. GMAA*-ICE, applies
IE GMAA*-IC, addresses problem creating next child node
candidate expansion.
3. provide theoretical guarantees GMAA*-IC GMAA*-ICE. particular, show that, using suitable heuristic, algorithms complete
search equivalent.
4. introduce improved heuristic representation. Tight heuristics like based
underlying POMDP solution (QPOMDP ) value function resulting
assuming 1-step-delayed communication (QBG ) essential heuristic search methods like GMAA* (Oliehoek, Spaan, & Vlassis, 2008). However, space needed
store heuristics grows exponentially problem horizon. introduce hybrid representations compact thereby enable solution larger
problems.
5. present extensive empirical results show substantial improvements
current state-of-the-art. Whereas Seuken Zilberstein (2008) argued GMAA*
best optimally solve Dec-POMDPs one horizon brute-force
search, results demonstrate GMAA*-ICE much better. addition,
provide comparative overview results competitive optimal solution methods
literature.
primary aim techniques introduced article improve scalability
respect horizon. empirical results confirm techniques highly
successful regard. added bonus, experiments also demonstrate improvement
scalability respect number agents. particular, present first optimal
results general (non-special case) Dec-POMDPs three agents. Extensions
techniques achieve improvements respect number agents,
well promising ways combine ideas behind methods state-of-the-art
approximate approaches, discussed future work Section 7.
4. article synthesizes extends research already reported two conference papers (Oliehoek,
Whiteson, & Spaan, 2009; Spaan, Oliehoek, & Amato, 2011).

452

fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPs

1.2 Organization
article organized follows. Section 2 provides background Dec-POMDP model,
GMAA* heuristic search solution method, well suitable heuristics. Section 3,
introduce lossless clustering CBGs integration GMAA*. Section 4 introduces incremental expansion search nodes. empirical evaluation proposed
techniques reported Section 5. give treatment related work Section 6. Future
work discussed Section 7 conclusions drawn Section 8.

2. Background
Dec-POMDP, multiple agents must collaborate maximize sum common
rewards receive multiple timesteps. actions affect immediate
rewards also state transition. current state known
agents, timestep agent receives private observation correlated
state.


ff
Definition 1. Dec-POMDP tuple D, S, A, T, O, O, R, b0 , h ,
= {1, . . . ,n} finite set agents.


= s1 , . . . ,s|S| finite set states.

= Ai set joint actions = ha1 , . . . , i, Ai finite set actions
available agent i.
transition function specifying state transition probabilities Pr(s |s,a).
= Oi finite set joint observations. every stage one joint observation
= ho1 ,...,on received. agent observes component oi .
observation function, specifies observation probabilities Pr(o|a,s ).
R(s,a) immediate reward function mapping (s,a)-pairs real numbers.
b0 (S) initial state distribution time = 0, (S) denotes infinite
set probability distributions finite set S.
h horizon, i.e., number stages. consider case h finite.
stage = 0 . . . h 1, agent takes individual action receives individual
observation.
Example 1 (Recycling Robots). illustrate Dec-POMDP model, consider team robots tasked
removing trash office building, depicted Fig. 1. robots sensors find marked
trash cans, motors move around order look cans, well gripper arms grasp carry
can. Small trash cans light compact enough single robot carry, large trash cans
require multiple robots carry together. people use them, larger trash
cans fill quickly. robot must also ensure battery remains charged moving
charging station expires. battery level robot degrades due distance
robot travels weight item carried. robot knows battery level
robots location robots within sensor range. goal
problem remove much trash possible given time period.
problem represented Dec-POMDP natural way. states, S, consist
different locations robot, battery levels different amounts trash
cans. actions, Ai , robot consist movements different directions well decisions
453

fiOliehoek, Spaan, Amato, & Whiteson

Figure 1: Illustration Recycling Robots example, two robots remove
trash office environment three small (blue) trash cans two large (yellow) ones.
situation, left robot might observe large trash next full,
robot small trash empty. However, none sure trash
cans state due limited sensing capabilities, see state trash cans
away. particular, one robot knowledge regarding observations robot.
pick trash recharge battery (when range charging station).
observations, Oi , robot consist battery level, location, locations
robots sensor range amount trash cans within range. rewards, R, could consist
large positive value pair robots emptying large (full) trash can, small positive value
single robot emptying small trash negative values robot depleting battery
trash overflowing. optimal solution joint policy leads expected behavior (given
rewards properly specified). is, ensures robots cooperate empty
large trash cans appropriate small ones individually considering battery usage.

explanatory purposes, also consider much simpler problem, so-called decentralized tiger problem (Nair et al., 2003).
Example 2 (Dec-Tiger). Dec-Tiger problem concerns two agents find hallway two doors. Behind one door, treasure behind tiger. state
describes door tiger behindleft (sl ) right (sr )each occurring 0.5 probability
(i.e., initial state distribution b0 uniform). agent perform three actions: open left
door (aOL ), open right door (aOR ) listen (aLi ). Clearly, opening door treasure
yield reward, opening door tiger result severe penalty. greater reward
given agents opening correct door time. such, good strategy
probably involve listening first. listen actions, however, also minor cost (negative reward).
every stage agents get observation. agents either hear tiger behind left
(oHL ) right (oHR ) door, agent 15% chance hearing incorrectly (getting wrong
observation). Moreover, observation informative agents listen; either agent opens
door, agents receive uninformative (uniformly drawn) observation problem resets
sl sr equal probability. point problem continues, agents may
able open door treasure multiple times. Also note that, since two observations
agents get oHL , oHR , agents way detecting problem reset:
one agent opens door listens, agent able tell
door opened. complete specification, see discussion Nair et al. (2003).

Given Dec-POMDP, agents common goal maximize expected cumulative
reward return. planning task entails finding joint policy = h1 , . . . ,n
space joint policies , specifies individual policy agent i.
454

fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPs

individual policy general specifies individual action action-observation history
(AOH) ~it = (a0i ,o1i , . . . ,ait1 ,oti ), e.g., (~it ) = ati . However, possible restrict
attention deterministic pure policies, case maps observation history
~ action, e.g., (~o ) = . number policies
(OH) (o1i , . . . ,oti ) = ~oit



h 1)/(|O |1)
(|O
|

|Ai |
number joint policies therefore

n|O |h 1
|A | |O |1 ,
(2.1)
denote largest individual action observation sets. quality
particular joint policy expressed expected cumulative reward induces, also referred
value.
Definition 2. value V () joint policy
V () , E

h1
hX
t=0

fi

fi
R(st ,at )fi,b0 ,

(2.2)

expectation sequences states, actions observations.
planning problem Dec-POMDP find optimal joint policy , i.e., joint
policy maximizes value: = arg max V ().
individual policy depends local information ~oi available
agent, on-line execution phase truly decentralized: communication takes place
modeled via actions observations. planning however, may take place
off-line phase centralized. scenario consider article.
detailed introduction Dec-POMDPs see, e.g., work Seuken Zilberstein
(2008) Oliehoek (2012).
2.1 Heuristic Search Methods
recent years, numerous Dec-POMDP solution methods proposed.
methods fall one two categories: dynamic programming heuristic search methods.
Dynamic programming methods take backwards bottom-up perspective first considering policies last time step = h 1 using construct policies stage
= h 2, etc. contrast, heuristic search methods take forward top-down perspective
first constructing plans = 0 extending later stages.
article, focus heuristic search approach shown state-of-the-art
results. make clear section, method interpreted searching
tree collaborative Bayesian games (CBGs). CBGs provide convenient abstraction
layer facilitates explanation techniques introduced article.
section provides concise background heuristic search methods.
detailed description, see work Oliehoek, Spaan, Vlassis (2008). description dynamic programming methods relationship heuristic search methods,
see work Oliehoek (2012).
2.1.1 Multiagent A*
Szer, Charpillet, Zilberstein (2005) introduced heuristically guided policy search method
called multiagent A* (MAA*). performs A* search partially specified joint policies,
455

fiOliehoek, Spaan, Amato, & Whiteson

t=0

t=1

t=2

i0

aLi
=2

2i

oHR

oHL
aOL

i1

aOL

oHL

oHR

oHL

oHR

aLi

aLi

aOL

aLi

i2

=1
Figure 2: arbitrary policy Dec-Tiger problem. figure illustrates different
types partial policies used paper. shown past policy 2i consists two decision
rules i0 , i1 . Also shown two sub-tree policies =1 , =2 (introduced Section 3.1.2).
pruning joint policies guaranteed worse best (fully specified) joint policy
found far. Oliehoek, Spaan, Vlassis (2008) generalized algorithm making explicit
expand selection operators performed heuristic search. resulting algorithm,
generalized MAA* (GMAA*) offers unified perspective MAA* forward sweep
policy computation method (Emery-Montemerlo, 2005), differ implement
GMAA*s expand operator: forward sweep policy computation solves (i.e., finds best
policy for) collaborative Bayesian games, MAA* finds policies collaborative
Bayesian games, describe Section 2.1.2.
GMAA* algorithm considers joint policies partially specified respect
time. partially specified policies formalized follows.
Definition 3. decision rule agent decision stage mapping action~ Ai .
observation histories stage actions :

article, consider deterministic policies. Since policies need condition
actions observation histories, made decision rules map length~ Ai . joint decision rule = h , . . . ,nt specifies
observation histories actions: :
1

decision rule agent. Fig. 2 illustrates concept, well past policy,
introduce shortly. discussed below, decision rules allow partial policies
defined play crucial role GMAA* algorithms developed article.
Definition 4. partial past policy stage t, ti , specifies part agent policy
relates stages < t. is, specifies decision rules first stages:
ti = (i0 ,i1 , . . . ,it1 ). past policy stage h regular, fully specified, policy
hi = . past joint policy = ( 0 , 1 , . . . , t1 ) specifies joint decision rules first
stages.
GMAA* performs heuristic search partial joint policies constructing
search tree illustrated Fig. 3a. node q = ht , vi search tree specifies
past joint policy heuristic value v. heuristic value v node represents
optimistic estimate past joint policy Vb (t ), computed via
Vb (t ) = V 0...t1 (t ) + H t...h1 (t ),
456

(2.3)

fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPs

B(0 )

0
0

0

0

0
0

0
1

1
1

2

...

B(1 )

1

1

1

...

B(1 )

B(2 )

2

(a) MAA* perspective.

...

B(1 )

1

...

B(2 )

(b) CBG perspective.

Figure 3: Generalized MAA*. Associated every node heuristic value. search
trees two perspectives shown equivalent certain assumptions heuristic,
explained Section 2.2.
H t...h1 heuristic value remaining h stages V 0...t1 (t ) actual
expected reward achieves first stages (for definition, see Appendix A.3).
Clearly, H t...h1 admissible heuristica guaranteed overestimationso Vb (t ).5
Algorithm 1 illustrates GMAA*. starts creating node q 0 completely unspecified joint policy 0 placing open list L. Then, selects nodes (Algorithm 2)
expands (Algorithm 3), repeating process certain found
optimal joint policy.
Select operator returns highest ranked node, defined following comparison operator.
Definition 5. node comparison operator < defined two nodes q = ht ,vi, q =
ht ,v follows:



, v 6= v
v < v
q < q = depth(q) < depth(q ) , otherwise depth(q) 6= depth(q )
(2.4)



<
, otherwise.
is, comparison operator first compares heuristic values. equal,
compares depth nodes. Finally, nodes equal value equal depth,
lexically compares past joint policies. ranking leads A* behavior (i.e., selecting
node open list highest heuristic value) GMAA*, well guaranteeing
selection order incremental expansion technique (introduced Section 4).
Ranking nodes greater depth higher case equal heuristic value helps find tight
lower bounds early first expanding deeper nodes (Szer et al., 2005) also useful
incremental expansion.
5. formally, H underestimate value. Note that, unlike classical A* applications
path planningin admissible heuristic overestimatein setting maximize reward,
rather minimize cost.

457

fiOliehoek, Spaan, Amato, & Whiteson

Algorithm 1 Generalized multiagent A*.
Input: Dec-POMDP, admissible heuristic H, empty open list L
Output: optimal joint policy
1: vGM AA
2: q 0 h0 = (), v = +i
3: L.insert(q 0 )
4: repeat
5:
q Select(L)
6:
QExpand Expand(q, H)
7:
depth(q) = h 1
8:
{ QExpand contains fully specified joint policies, interested best one }
9:
h, vi BestJointPolicyAndValue(QExpand )
10:
v > vGM AA
11:

{found new best joint policy}
12:
vGM AA v
13:
L.Prune(vGM AA )
{(optionally) prune open list}
14:
end
15:
else


{add expanded children open list}
16:
L.insert( q QExpand | q .v > vGM AA )
17:
end
18:
PostProcessNode(q, L)
19: L empty
20: return

Algorithm 2 Select(L): Return highest ranked node open list.
Input: open list L, total order nodes <
Output: highest ranked node q
1: q q L s.t. q L (q 6= q = q < q)
2: return q

Expand operator constructs QExpand , set child nodes. is, given node
contains partial joint policy = ( 0 , 1 , . . . , t1 ), constructs t+1 , set
t+1 = ( 0 , 1 , . . . , t1 , ), appending possible joint decision rules next time
step t. t+1 , heuristic value computed node constructed.
expansion, algorithm checks (line 7) expansion resulted fully specified
joint policies. not, children sufficient heuristic value placed open list
Algorithm 3 Expand(q, H). expand operator plain MAA*.
Input: q = ht , vi search node expand, H admissible heuristic.
Output: QExpand set containing expanded child nodes.
1: QExpand {}
2: t+1 {t+1 | t+1 = (t , )}
3: t+1 t+1
4:
Vb (t+1 ) V 0...t (t+1 ) + H(t+1 )
5:
q ht+1 , Vb (t+1 )i
6:
QExpand .Insert(q )
7: end
8: return QExpand

458

{create child node}

fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPs

Algorithm 4 PostProcessNode(q,L)
Input: q expanded parent node, L open list.
Output: expanded node removed.
1: L.Pop(q)

(line 16). children fully specified, BestJointPolicyAndValue returns best
joint policy (and value) QExpand (see Algorithm 12 Appendix A.1 details
BestJointPolicyAndValue). GMAA* also maintains lower bound vGM AA corresponds actual value best fully-specified joint policy found far. newly
found joint policy higher value lower bound updated (lines 11 12). Also,
nodes partial joint policies t+1 upper bound lower best solution
far, Vb (t+1 ) < vGM AA , pruned (line 13). pruning takes additional time,
save memory. Finally, PostProcessNode simply removes parent node open
list (this procedure augmented incremental expansion Section 4). search ends
list becomes empty, point optimal joint policy found.
GMAA* complete, i.e., search finds solution. Therefore, theory,
GMAA* guaranteed eventually produce optimal joint policy (Szer et al., 2005).
However, practice, often infeasible larger problems. major source complexity
full expansion search node. number joint decision rules stage
form children node depth search tree6



|A |n(|O | ) ,
(2.5)
doubly exponential t. Comparing (2.1) (2.5), see worst case
complexity expanding node deepest level tree = h 1 comparable
brute force search entire Dec-POMDP. Consequently, Seuken Zilberstein
(2008) conclude MAA* best solve problems whose horizon 1 greater
already solved nave brute force search.
2.1.2 Bayesian Game Perspective
GMAA* makes possible interpret MAA* solution collection collaborative
Bayesian games (CBGs). employ approach throughout article, facilitates
improvements GMAA* introduce, results significant advances
state-of-the-art Dec-POMDP solutions.
Bayesian game (BG) models one-shot interaction number agents.
extension well-known strategic game (also known normal form game)
agent holds private information (Osborne & Rubinstein, 1994). CBG BG
agents receive identical payoffs. Bayesian game perspective, node q
GMAA* search tree, along corresponding partial joint policy , defines
CBG (Oliehoek, Spaan, & Vlassis, 2008). is, given state distribution b0 , ,
possible construct CBG B(b0 ,t ) represents decision-making problem
stage given followed first stages starting b0 . clear
b0 is, simply write B(t ).
6. follow convention root depth 0.

459

fiOliehoek, Spaan, Amato, & Whiteson

Definition 6. collaborative Bayesian game (CBG) B(b0 ,t ) = hD, A, , Pr(), ui modeling
stage Dec-POMDP, given initial state distribution b0 past joint policy , consists
of:
D, set agents {1 . . . n},
A, set joint actions,
, set joint types, specifies type agent =
h1 , . . . ,n i,
Pr(), probability distribution joint types,
u, (heuristic) payoff function mapping joint type action real number: u(,a).
Bayesian game, type agent represents private information holds.
instance, Bayesian game modeling job recruitment scenario, type agent
may indicate whether agent hard worker. CBG Dec-POMDP, agents
private information individual AOH. Therefore, type agent corresponds
~it , history actions observations: ~it . Similarly, joint type corresponds
joint AOH: ~ .
Consequently, u provide (heuristic) estimate long-term payoff
(~ ,a)-pair. words, payoff function corresponds heuristic Q-value: u(,a)
b ~ ,a). discuss compute heuristics Section 2.2. Given , b0 ,
Q(
correspondence joint types AOHs, probability distribution joint types is:
Pr() , Pr(~ |b0 ,t ),

(2.6)

latter probability marginal Pr(s,~ |b0 ,t ) defined (A.2) used
computation value partial joint policy V 0...t1 (t ) Appendix A.3. Note due
correspondence types AOHs, size CBG B(b0 ,t ) stage
exponential t.
CBG, agent uses Bayesian game policy maps individual types actions:
(i ) = ai . correspondence types AOHs, (joint) policy
CBG corresponds (joint) decision rule: . remainder article,
assume deterministic past joint policies , implies one ~ non-zero
probability given observation history ~o . Thus, effectively maps observation histories
actions. number B(b0 ,t ) given (2.5). value joint CBG
policy CBG B(b0 ,t ) is:
X
b ~ ,(~ )),
Pr(~ |b0 ,t )Q(
(2.7)
Vb () =
~


(~ ) = hi (~it )ii=1...n denotes joint action results application
individual CBG-policies individual AOH ~it specified ~ .
Example 3. Consider CBG Dec-Tiger given past joint policy 2 specifies listen first two stages. stage = 2, agent four possible observation histories:
~ 2 = {(oHL ,oHL ), (oHL ,oHR ), (oHR ,oHL ), (oHR ,oHR )} correspond directly possible types.


probabilities joint types given 2 listed Fig. 4a. Since joint OHs together
2 determine joint AOHs, also correspond so-called joint beliefs: probability distributions
states (introduced formally Section 2.2). Fig. 4b shows joint beliefs, serve
basis heuristic payoff function (as discussed Section 2.2).
460

fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPs

~o12
(oHL ,oHL )
(oHL ,oHR )
(oHR ,oHL )
(oHR ,oHR )

~o22
(oHL ,oHL )
0.261
0.047
0.047
0.016

(oHL ,oHR )
0.047
0.016
0.016
0.047

(oHR ,oHL )
0.047
0.016
0.016
0.047

(oHR ,oHR )
0.016
0.047
0.047
0.261

(a) joint type probabilities.

~o12
(oHL ,oHL )
(oHL ,oHR )
(oHR ,oHL )
(oHR ,oHR )

~o22
(oHL ,oHL )
0.999
0.970
0.970
0.5

(oHL ,oHR )
0.970
0.5
0.5
0.030

(oHR ,oHL )
0.970
0.5
0.5
0.030

(oHR ,oHR )
0.5
0.030
0.030
0.001

(b) induced joint beliefs. Listed probability Pr(sl |~ 2 ,b0 )
tiger behind left door.

Figure 4: Illustration Dec-Tiger problem past joint policy 2 specifies
listen actions first two stages.

Algorithm 5 Expand-CBG(q, H). expand operator GMAA* makes use CBGs.
Input: q = ht , vi search node expand.
b ~,a).
Input: H admissible heuristic form Q(
Output: QExpand set containing expanded child nodes.
b
1: B(b0 ,t ) ConstructBG(b0 ,t , Q)
2: QExpand GenerateAllChildrenForCBG(B(b0 ,t ))
3: return QExpand

{as explained Section 2.1.2}

solution CBG maximizes (2.7). CBG equivalent team
decision process finding solution NP-complete (Tsitsiklis & Athans, 1985). However,
Bayesian game perspective GMAA*, illustrated Fig. 3b, issue solving
CBG (i.e., finding highest payoff ) relevant need expand .
is, Expand operator enumerates appends form set
extended joint policies


t+1 = (t , ) | joint CBG policy B(b0 ,t )
uses set construct QExpand , set child nodes. heuristic value
child node q QExpand specifies t+1 = (t , ) given
Vb (t+1 ) = V 0...t1 (t ) + Vb ().

(2.8)

Expand operator makes use CBGs summarized Algorithm 5, uses
GenerateAllChildrenForCBG subroutine (Algorithm 13 Appendix A.1). Fig. 3b illustrates
Bayesian game perspective GMAA*.
461

fiOliehoek, Spaan, Amato, & Whiteson

2.2 Heuristics
perform heuristic search, GMAA* defines heuristic value Vb (t ) using (2.3). contrast, Bayesian game perspective uses (2.8). two formulations equivalent
b faithfully represents expected immediate reward (Oliehoek, Spaan, & Vlasthe heuristic Q
sis, 2008). consequence GMAA* via CBGs complete (and thus finds optimal
solutions) stated following theorem.
Theorem 1. using heuristic form
b ~ ,a) = Est [R(st ,a) | ~ ] + E~ t+1 [Vb (~ t+1 ) | ~ , a],
Q(


(2.9)

Vb (~ t+1 ) Q (~ t+1 , (~ t+1 )) overestimation value optimal joint
policy , GMAA* via CBGs complete.
Proof. See appendix.
theorem, Q (~ ,a) Q-value, i.e., expected future cumulative reward
performing ~ joint policy (Oliehoek, Spaan,P
& Vlassis, 2008). expectation

~
immediate reward also written R( ,a) = sS R(s,a) Pr(s|~ ,b0 ).
computed using Pr(s|~ ,b0 ), quantity refer joint belief resulting ~
also denote b. joint belief computed via repeated application
Bayes rule (Kaelbling, Littman, & Cassandra, 1998), conditional (A.2).
rest subsection reviews several heuristics used GMAA*.
2.2.1 QMDP
b ~,a) solve underlying MDP, i.e.,
One way obtain admissible heuristic Q(
assume joint action chosen single puppeteer agent observe true
state. approach, known QMDP (Littman, Cassandra, & Kaelbling, 1995), uses

MDP value function Qt,
(s ,a), computed using standard dynamic programming

b ~t
techniques (Puterman, 1994). order transform Qt,
(s ,a)-values QM ( ,a)-values,
compute:
X t,
b (~ ,a) =
(2.10)
Q
Q (s,a) Pr(s|~ ,b0 ).


sS

Solving underlying MDP time complexity linear h, makes it,
especially compared Dec-POMDP, easy compute. addition, necessary
store value (s,a)-pair, stage t. However, bound provides
optimal Dec-POMDP Q -value function loose (Oliehoek & Vlassis, 2007).
2.2.2 QPOMDP
Similar underlying MDP, one define underlying POMDP Dec-POMDP,
i.e., assuming joint action chosen single agent access joint observation.7
7. Alternatively one view POMDP multiagent POMDP agents instantaneously
broadcast private observations.

462

fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPs

Tree

Vector

t=0
t=1
t=2
t=3
Figure 5: Visual comparison tree vector-based Q representations.

resulting solution used heuristic, called QPOMDP (Szer et al., 2005; Roth,
Simmons, & Veloso, 2005). optimal QPOMDP value function satisfies:

QP (bt , a) = R(bt ,a) +

X

P (ot+1 |bt ,a) max QP (bt+1 ,at+1 ),
at+1

ot+1

(2.11)

P
bt joint belief, R(bt ,a) = sS R(s,a)bt (s) immediate reward, bt+1
joint belief resulting bt action joint observation ot+1 . use QPOMDP ,
b P (~ ,a) , Qt (b~t ,a).
~ , directly use value induced joint belief: Q
P

two approaches computing QPOMDP . One construct belief MDP
tree joint beliefs, illustrated Fig. 5 (left). Starting b0 (corresponding
empty joint AOH ~ 0 ), compute resulting ~ 1 corresponding
~1
belief b continue recursively. Given tree, possible compute values
nodes standard dynamic programming.
Another possibility apply vector-based POMDP techniques (see Fig. 5 (right)).
Q-value function stage QtP (b,a) represented using set vectors joint
} (Kaelbling et al., 1998). Qt (b,a) defined maximum
action V = {V1t , . . . ,V|A|
P
inner product:
QtP (b,a) , max b vat .
V
va


Given V h1 , vector representation last stage, compute V h2 , etc. order
limit growth number vectors, dominated vectors pruned.
Since QMDP upper bound POMDP value function (Hauskrecht, 2000), QPOMDP
provides tighter upper bound Q QMDP . However, also costly compute
store: tree-based vector-based approach may need store number
values exponential h.
463

fiOliehoek, Spaan, Amato, & Whiteson

2.2.3 QBG
third heuristic, called QBG , assumes agent team access
individual observation communicate 1-step delay.8 define QBG
X
QB (~ ,a) = R(~ ,a) + max
Pr(ot+1 |~ ,a)QB (~ t+1 ,(ot+1 )),
(2.12)


ot+1

t+1
= h1 (ot+1
1 ),...,n (on )i tuple individual policies : Oi Ai CBG

constructed ~ ,a. Like QPOMDP , QBG also represented using vectors (Varaiya &
Walrand, 1978; Hsu & Marcus, 1982; Oliehoek, Spaan, & Vlassis, 2008) two
manners computation (tree vector based) apply. yields tighter heuristic
QPOMDP , computation additional exponential dependence maximum
number individual observations (Oliehoek, Spaan, & Vlassis, 2008), particularly
troubling vector-based computation, since precludes effective application incremental pruning (A. Cassandra, Littman, & Zhang, 1997). overcome problem, Oliehoek
Spaan (2012) introduce novel tree-based pruning methods.

3. Clustering
GMAA* solves Dec-POMDPs repeatedly constructing CBGs expanding joint
BG policies them. However, number equal number regular
MAA* child nodes given (2.5) thus grows doubly exponentially horizon h.
section, propose new approach improving scalability respect h
clustering individual AOHs. reduces number therefore number
constructed child nodes GMAA* search tree.9
Previous research also investigated clustering: Emery-Montemerlo, Gordon,
Schneider, Thrun (2005) propose clustering types based profiles payoff
functions CBGs. However, resulting method ad hoc. Even given bounds
error clustering two types CBG, guarantees made quality
Dec-POMDP solution, bound respect heuristic payoff function.
contrast, propose cluster histories based probability histories induce
histories agents states. critical advantage criterion,
call probabilistic equivalence (PE), resulting clustering lossless:
solution clustered CBG used construct solution original CBG
values two CBGs identical. Thus, criterion allows clustering AOHs
CBGs represent Dec-POMDPs preserving optimality.10
Section 3.1, describe histories Dec-POMDPs clustered using
notions probabilistic best-response equivalence. allows histories clustered
8. name QBG stems fact 1-step delayed communication scenario modeled
CBG. Note, however, CBGs used compute QBG different form B(b0 ,t )
discussed Section 2.1.2: latter, types correspond length-t (action-) observation histories;
former, types correspond length-1 observation histories.
9. CBGs essential clustering, provide convenient level abstraction simplifies
exposition techniques. Moreover, level abstraction makes possible employ results
concerning CBGs outside context Dec-POMDPs.
10. probabilistic equivalence criterion lossless clustering introduced Oliehoek et al. (2009).
article presents new, simpler proof optimality clustering based PE.

464

fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPs

rational always choose action. Section 3.2, describe application
results GMAA*. Section 3.3 introduces improved heuristic representations
allow computation longer horizons.
3.1 Lossless Clustering Dec-POMDPs
section, discuss lossless clustering based notion probabilistic equivalence.
show clustering lossless demonstrating probabilistic equivalence implies
best response equivalence, describes conditions rational agent select
action two types. prove implication, show best response
depends multiagent belief (i.e., probability distribution states policies
agents), two probabilistically equivalent histories. Relations
equivalence notions discussed Section 6.
3.1.1 Probabilistic Equivalence Criterion
first introduce probabilistic equivalence criterion, used decide whether
two individual histories ~ia ,~ib clustered without loss value.
Criterion 1 (Probabilistic Equivalence). Two AOHs ~ia ,~ib agent probabilistically
equivalent (PE), written P E(~ia ,~ib ), following holds:
~6=i

Pr(s,~
6=i |~ia ) = Pr(s,~
6=i |~ib ).

(3.1)

probabilities computed conditional Pr(s,~ |b0 ,t ), defined (A.2).
subsections 3.1.23.1.4, formally prove PE sufficient criterion guarantee
clustering lossless. remainder Section 3.1.1 discuss key properties
PE criterion order build intuition.
Note criterion decomposed following two criteria:
~6=i
~6=i

Pr(~
6=i |~ia ) = Pr(~
6=i |~ib ),

(3.2)

Pr(s|~
6=i ,~ia ) = Pr(s|~
6=i ,~ib ).

(3.3)

criteria give natural interpretation: first says probability distribution
agents AOHs must identical ~ia ~ib . second demands
resulting joint beliefs identical.
probabilities well defined without initial state distribution b0
past joint policy . However, since consider clustering histories within particular CBG
(for stage t) constructed particular b0 ,t , implicitly specified. Therefore
drop arguments, clarifying notation.
Example 4. Example 3, types (oHL ,oHR ) (oHR ,oHL ) agent PE. see this, note
rows (columns second agent) histories identical Fig. 4a
Fig. 4b. Thus, specify distribution histories agents (cf. equation (3.2))
induced joint beliefs (cf. equation (3.3)).

Probabilistic equivalence convenient property algorithms exploit: holds
particular pair histories, also hold identical extensions
histories, i.e., propagates forwards regardless policies agents.
465

fiOliehoek, Spaan, Amato, & Whiteson

Definition 7 (Identical extensions). Given two AOHs ~ia,t ,~ib,t , respective extensions
~ a,t+1 = (~ a,t ,ai ,oi ) ~ b,t+1 = (~ b,t ,a ,o ) called identical extensions






ai = ai oi = oi .







Lemma 1 (Propagation PE). Given ~ia,t ,~ib,t PE, regardless decision rule
agents use ( t6=i ), identical extensions also PE:
ati ot+1 st+1 ~t+1


6=i

6=i

t+1

t+1 ~ t+1 ~ b,t t+1
, 6=i |i ,ai ,oi , 6=i ) (3.4)
Pr(st+1 ,~
6=i |~ia,t ,ati ,ot+1
, 6=i ) = Pr(s

Proof. proof listed appendix, holds intuitively probabilities
described before, also taking action
seeing observation.
Note that, probabilities defined (3.1) superficially resemble beliefs used
POMDPs, substantially different. POMDP, single agent compute
individual belief using AOH. use belief determine value
future policy, sufficient statistic history predict future rewards
(Kaelbling et al., 1998; Bertsekas, 2005). Thus, trivial show equivalence AOHs
induce individual belief POMDP. Unfortunately, Dec-POMDPs
problematic. next section elaborates issue discussing relation multiagent
beliefs.
3.1.2 Sub-Tree Policies, Multiagent Beliefs Expected Future Value
describe relationship multiagent beliefs probabilistic equivalence,
must first discuss policies agent may follow resulting values. begin
introducing concept sub-tree policies. illustrated Fig. 2 (on page 456),
(deterministic) policy represented tree nodes labeled using actions
edges labeled using observations: root node corresponds first action taken,
nodes specify action observation history encoded path root node.
such, possible define sub-tree policies, , correspond sub-trees agent
policy (also illustrated Fig. 2). particular, write
w
~o = =ht
(3.5)


sub-tree policy corresponding w
observation history ~oit specifies actions
last = h stages. refer policy consumption operator, since
w
consumes part policy corresponding ~oit . Similarly write =k ~o l = =kl

(note (3.5), = h-steps-to-go sub-tree policy) use similar notation,
=k , joint sub-tree policies. extensive treatment different forms
policy, refer discussion Oliehoek (2012).
Given concepts, define value = k-stages-to-go joint policy starting
state s:
XX
w
Pr(s ,o|s,a)V (s , =k ).
(3.6)
V (s, =k ) = R(s,a) +




Here, joint action specified roots individual sub-tree policies specified
=k stage = h k.
466

fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPs

definition, follows directly probability distribution states
sub-tree policies agents 6=i sufficient predict value sub-tree policy .
fact, distribution known multiagent belief bi (s, 6=i ) (Hansen, Bernstein, &
Zilberstein, 2004). value given
XX
V (bi ) = max
bi (s, 6=i )V (s,hi , 6=i i),
(3.7)




6=i

refer maximizing agent best response bi . illustrates
multiagent belief sufficient statistic: contains sufficient information predict value
sub-tree policy .
possible connect action observation histories multiagent beliefs fixing
policies agents. Given agents act according profile
policies 6=i , agent multiagent belief first stage Dec-POMDP: bi (s, 6=i ) =
b0 (s). Moreover, agent maintain multiagent belief execution. such,
given 6=i , history ~i induces multiagent belief, write bi (s, 6=i |~i , 6=i )
make dependence ~i , 6=i explicit. multiagent belief history defined
bi (s, 6=i |~i , 6=i ) , Pr(s, 6=i |~i , b0 , 6=i ),

(3.8)

induces best response via (3.7):
BR(~i | 6=i ) , arg max


XX


bi (s, 6=i |~i , 6=i )V (s, 6=i ,i ).

(3.9)

6=i

conclude two AOHs ~ia ,~ib clustered together induce
multiagent belief.
However, notion multiagent belief clearly quite different distributions
used notion PE. particular, establish whether two AOHs induce
multiagent belief, need full specification 6=i . Nevertheless, show two AOHs
PE also best response equivalent therefore cluster them.
crux show that, Criterion 1 satisfied, AOHs always induce
multiagent beliefs 6=i (consistent current past joint policy 6=i ).
3.1.3 Best-Response Equivalence Allows Lossless Clustering Histories
relate probabilistic equivalence multiagent belief follows.
Lemma 2 (PE implies multiagent belief equivalence). 6=i , probabilistic equivalence
implies multiagent belief equivalence:


P E(~ia ,~ib ) s,6=i bi (s, 6=i |~ia , 6=i ) = bi (s, 6=i |~ib , 6=i )
(3.10)
Proof. See appendix.
lemma shows two AOHs PE, produce multiagent belief.
Intuitively, gives us justification cluster AOHs together: since multiagent
belief sufficient statistic act multiagent belief,
since Lemma 2 shows ~ia ,~ib induces multiagent beliefs 6=i
PE, conclude always act histories. Formally,
prove ~ia ,~ib best-response equivalent PE.
467

fiOliehoek, Spaan, Amato, & Whiteson

Theorem 2 (PE implies best-response equivalence). Probabilistic equivalence implies bestresponse equivalence.


P E(~ia ,~ib ) 6=i BR(~ia | 6=i ) = BR(~ib | 6=i )
Proof. Assume arbitrary 6=i ,
BR(~ia | 6=i ) = arg max

XX

bi (s, 6=i |~ia )V (s, 6=i ,i )

= arg max

XX

bi (s, 6=i |~ib )V (s, 6=i ,i ) = BR(~ib | 6=i ),









6=i

6=i

Lemma 2 employed assert equality bi (|~ia ) bi (|~ib ).
theorem key demonstrates two AOHs ~ia ,~ib agent
PE, agent need discriminate future. Thus,
searching space joint policies, restrict search assign
sub-tree policy ~ia ~ib . such, directly provides intuition lossless
clustering possible. Formally, define clustered joint policy space follows.
Definition 8 (Clustered joint policy space). Let C subset joint policies
clustered: i.e., part C assigns sub-tree policy action
observation histories probabilistically equivalent.
Corollary 1 (Existence optimal clustered joint policy). exists optimal joint
policy clustered joint policy space:
max V () = max V ()

C



(3.11)

Proof. clear left hand side (3.11) upper bounded right hand side,
since C . suppose = arg max V () strictly higher value
best clustered joint policy. least one agent one pair PE histories ~ia , ~ib , must
assign different sub-tree policies ia 6= ib (otherwise would clustered). Without loss
generality assume one pair. follows directly Theorem 2
policy construct clustered policy C C (by assigning either ia ib
~ia , ~ib ) guaranteed value less , thereby contradicting
assumption strictly higher value best clustered joint policy.
formally proves restrict search C , space clustered joint
policies, without sacrificing optimality.
3.1.4 Clustering Commitment CBGs
Though clear two AOHs PE clustered, making result
operational requires additional step. end, use abstraction layer provided
Bayesian games. Recall CBG stage, AOHs correspond types.
468

fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPs

Therefore, want cluster types CBG. accomplish clustering two
types ia ,ib , introduce new type ic replace them, defining:
6=i Pr(ic , 6=i ) , Pr(ia , 6=i ) + Pr(ib , 6=i )
j

u(hic , 6=i ,a)



ff
Pr(ia , 6=i )u(hia , 6=i ,a) + Pr(ib , 6=i )u( ib , 6=i ,a)
.
,
Pr(ia , 6=i ) + Pr(ib , 6=i )

(3.12)
(3.13)

Theorem 3 (Reduction commitment). Given agent collaborative Bayesian
game B committed selecting policy assigns action two types
ia ,ib , i.e., selecting policy (ia ) = (ib ), CBG reduced without
loss value agents. is, result new CBG B agent employs
policy reflects clustering whose expected payoff original

CBG: V B (i , 6=i ) = V B (i , 6=i ).
Proof. See appendix.
theorem shows that, given agent committed taking action
types ia ,ib , reduce collaborative Bayesian game B smaller one B

translate joint CBG-policy found
B back joint CBG-policy B.
necessarily mean = , 6=i also solution B, best-response
agent 6=i may select action ia ,ib . Rather best-response
6=i given action needs taken ia ,ib .11
Even though Theorem 3 gives conditional statement depends agent
committed select action two types, previous subsection discussed
rational agent make commitment. Combining results gives
following corollary.
Corollary 2 (Lossless Clustering PE). Probabilistically equivalent histories ~ia ,~ib
clustered without loss heuristic value merging single type CBG.
Proof. Theorem 3 shows that, given agent committed take action
two types, types clustered without loss value. Since ~ia ,~ib PE,
best-response equivalent, means agent committed use
sub-tree policy hence action ai . Therefore directly apply clustering
without loss expected payoff, CBG stage Dec-POMDP means loss
expected heuristic value given (2.7).
Intuitively, maximizing action ~ia ~ib regardless (future)
joint policies 6=i agents use hence cluster without loss
heuristic value. Note depend heuristic used hence also
holds optimal heuristic (i.e., using optimal Q-value function gives
true value). directly relates probabilistic equivalence equivalence optimal value.12
11. Although focus CBGs, results generalize BGs individual payoff functions. Thus,
could potentially exploited algorithms general-payoff BGs. Developing methods
interesting avenue future work.
12. proof originally provided Oliehoek et al. (2009) based showing histories PE
induce identical Q-values.

469

fiOliehoek, Spaan, Amato, & Whiteson

Algorithm 6 ClusterCBG(B)
Input: CBG B
Output: Losslessly clustered CBG B
1: agent
2:
individual type B.i
3:
Pr(i ) = 0
4:
B.i B.i \i
5:
continue
6:
end
7:
individual type B.i
8:
isProbabilisticallyEquivalent true
9:
hs, 6=i
10:
Pr(s, 6=i |i ) 6= Pr(s, 6=i |i )
11:
isProbabilisticallyEquivalent false
12:
break
13:
end
14:
end
15:
isProbabilisticallyEquivalent
16:
B.i B.i \i
17:

18:
6=i
19:
u(i , 6=i ,a) min(u(i , 6=i ,a),u(i , 6=i ,a))
20:
Pr(i , 6=i ) Pr(i , 6=i ) + Pr(i , 6=i )
21:
Pr(i , 6=i ) 0
22:
end
23:
end
24:
end
25:
end
26:
end
27: end
28: return B

{Prune B:}

{Prune B:}
{ take lowest upper bound }

Note result establishes sufficient, necessary condition lossless clustering.
particular, given policies agents, many types best-response equivalent
clustered. However, far know, criterion must hold order guarantee
two histories best-response policy agents.
3.2 GMAA* Incremental Clustering
Knowing individual histories clustered together without loss value
potential speed many Dec-POMDP methods. article, focus application
within GMAA* framework.
Emery-Montemerlo et al. (2005) showed clustering incorporated every stage
algorithm: CBG stage constructed, clustering individual
histories (types) performed first afterwards (reduced) CBG solved.
approach employed within GMAA* modifying Expand procedure (Algorithm 5)
cluster CBG calling GenerateAllChildrenForCBG.
Algorithm 6 shows clustering algorithm. takes input CBG returns
clustered CBG. performs clustering performing pairwise comparison types
470

fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPs

b
Algorithm 7 ConstructExtendedBG(B, t1 , Q)

Input: CBG B stage 1, joint BG policy followed t1 .
b ~,a).
Input: admissible heuristic form Q(

Output: CBG B stage t.
1: B B
{make copy B subsequently alter}
2: agent
3:
B .i = ConstructExtendedTypeSet(i)
{overwrite individual type sets}
4: end
5: B . iD
{the new joint type set (does explicitly stored)}
6: joint type = ( t1 ,at1 ,ot ) B .
7:
state st
8:
Compute Pr(st |)
{from Pr(st1 | t1 ) via Bayes rule }
9:
end
10:
Pr() Pr(ot | t1 ,at1 ) Pr( t1 )
11:

12:
q
13:
history ~ represented
b ~ ,a))
b take lowest upper bound }
14:
q min(q,Q(
{ Q Q
15:
end
16:
B .u(,a) q
17:
end
18: end
19: return B

agent see satisfy criterion, yielding O(|i |2 ) comparisons agent i.
comparison involves looping hs, 6=i (line 9). many states, efficiency
could gained first checking (3.2) checking (3.3). Rather taking
average (3.13), line 19 take lowest payoff, done using
upper bound heuristic values.
following theorem demonstrates that, incorporating clustering GMAA*,
resulting algorithm still guaranteed find optimal solution.
Theorem 4. using heuristic form (2.9) clustering CBGs GMAA*
using PE criterion, resulting search method complete.
Proof. Applying clustering alter computation lower bound values. Also,
heuristic values computed expanded nodes admissible fact unaltered
guaranteed Corollary 2. Therefore, difference regular GMAA*
class considered joint policies restricted C , class clustered joint policies:
possible child nodes expanded, clustering effectively prunes away policies
would specify different actions AOHs PE thus clustered. However, Corollary 1
guarantees exists optimal joint policy restricted class.
modification Expand proposed rather naive. construct B(b0 ,t )
must first construct |Oi |t possible AOHs agent (given past policy ti ).
subsequent clustering involves pairwise comparison exponentially many types.
Clearly, tractable later stages.
However, PE AOHs propagates forwards (i.e., identical extensions PE histories also PE), efficient approach possible. Instead clustering exponentially
471

fiOliehoek, Spaan, Amato, & Whiteson

Algorithm 8 Expand-IC(q, H). expand operator GMAA*-IC.
Input: q = ht , vi search node expand.
b ~,a).
Input: H admissible heuristic form Q(
Output: QExpand set containing expanded child nodes.
1: B(t1 ) t1 .CBG
{retrieve previous CBG, note = (t1 , t1 )}
t1 b

t1
2: B( ) ConstructExtendedBG(B(
), , Q)
3: B(t ) ClusterBG(B(t ))
4: .CBG B(t )
{store pointer CBG}
5: QExpand GenerateAllChildrenForCBG(B(t ))
6: return QExpand

growing set types, simply extend already clustered types previous stages
CBG, shown Algorithm 7. is, given , set types agent previous
stage 1, it1 policy agent took stage, set types stage t, ,
constructed


= = (i ,it1 (i ),oti ) | ,oti Oi .
(3.14)
means size newly constructed set |i | = |i | |Oi | . type set
previous stage 1 much smaller set histories |i | |Oi |t1 ,
new type set also much smaller: |i | |Oi |t . way, bootstrap clustering
stage spend significantly less time clustering. refer algorithm
implements type clustering GMAA* Incremental Clustering (GMAA*-IC).
approach possible perform exact, value-preserving clustering
Lemma 1 guarantees identical extensions also clustered without loss
value. performing procedure lossy clustering scheme (e.g., EmeryMontemerlo et al., 2005), errors might accumulate, better option might re-cluster
scratch every stage.
Expansion GMAA*-IC node takes exponential time respect number
agents types, O(|A |n| | ) joint CBG-policies thus child nodes
GMAA*-IC search tree (A largest action set largest type set). Clustering
involves pairwise comparison types agent comparisons needs
check O(| |n1 |S|) numbers equality verify (3.1). total cost clustering
therefore written
O(n | |2 | |n1 |S|),
polynomial number types. clustering decreases number
types | |, therefore significantly reduce number child nodes thereby
overall time needed. However, clustering possible, overhead incurred.
3.3 Improved Heuristic Representation
Since clustering reduce number types, GMAA*-IC potential scale
larger horizons. However, important consequences computation
heuristics. Previous research shown upper bound provided QMDP often
loose effective heuristic search (Oliehoek, Spaan, & Vlassis, 2008). However,
space needed store tighter heuristics QPOMDP QBG grows exponentially
horizon. Recall Section 2.2.2 (see Fig. 5) two approaches computing
472

fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPs

b minimum size.
Algorithm 9 Compute Hybrid Q
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:

Qh1 {R1 , . . . ,R|A| }
z |A| |S|
= h 2 0
~ | |A|
|
z <
V VectorBackup(Qt+1 )
V Prune(V)
Qt V
z |V | |S|
end
z
Qt TreeBackup(Qt+1 )
end
end

{vector representation last stage}
{the size |A| vectors}
{size AOH representation}

{From z y}

QPOMDP QBG . first constructs tree joint AOHs heuristic values,
simple implement requires storing value (~ , a)-pair, number
grows exponentially t. second approach maintains vector-based representation,
common POMDPs. Though pruning provide leverage, worst case, pruning
possible number maintained vectors grows doubly exponentially h t,
number stages-to-go. Similarly, initial belief subsequently reachable beliefs
used reduce number vectors retained stage, number reachable
beliefs exponential horizon exponential complexity remains.
Oliehoek, Spaan, Vlassis (2008) used tree-based representation QPOMDP QBG heuristics. Since
computational cost solving Dec-POMDP bottleneck, inefficiencies representation could overlooked. However, approach longer feasible
longer horizons made possible GMAA*-IC.

Hybrid

t=0

mitigate problem, propose hybrid represent=1
tation heuristics, illustrated Fig. 6. main
insight exponential growth two existing representations occurs opposite directions. Therefore,
t=2
use low space-complexity side representations:
later stages, fewer vectors, use vector-based representation, earlier stages, fewer histot=3
ries, use history-based representation. similar
idea utilizing reachable beliefs reduce size Figure 6: illustration
vector representation described but, rather stor- hybrid representation.
ing vectors appropriate AOHs step,
values needed using tree-based representation.
Algorithm 9 shows how, mild assumptions, minimally-sized representation
computed. Starting last stage, algorithm performs vector backups, switching
tree backups become smaller option. last time step h 1, represent
473

fiOliehoek, Spaan, Amato, & Whiteson

Qt set immediate reward vectors13 , variable z (initialized line 2) keeps track
number parameters needed represent Qt vectors time step hand.
Note z depends effective vector pruning is, i.e., large parsimonious
representation piecewise linear convex value function is. Since problem
dependent, z updated pruning actually performed (line 9).
contrast y, number parameters tree representation, computed directly
Dec-POMDP (line 4). z > y, algorithm switches tree backups.14

4. Incremental Expansion
clustering technique presented previous section potential significantly
speed planning much clustering possible. However, little clustering possible,
number children GMAA* search tree still grow super-exponentially. section
presents incremental expansion, complementary technique deal problem.
Incremental expansion exploits recent improvements effectively solving CBGs. First
note expansion last stage = h 1 particular h1 ,
interested best child (h1 , h1, ), corresponds optimal solution
Bayesian game h1, . such, last stage, use new methods solving
CBGs (Kumar & Zilberstein, 2010b; Oliehoek, Spaan, Dibangoye, & Amato, 2010)
provide speedups multiple orders magnitude brute force search (enumeration).15
Unfortunately, improvements GMAA* afforded approach limited: order
guarantee optimality, still relies expansion (child nodes corresponding all)
joint CBG-policies intermediate stages, thus necessitating brute-force approach.
However, many expanded child nodes may low heuristic values Vb may therefore
never selected expansion.
Incremental expansion overcomes problem exploits following key observation: generate children decreasing heuristic order using admissible
heuristic, expand children. before, A* search performed
partially specified policies new CBG constructed extending CBG
parent node. However, rather fully expanding (i.e., enumerating CBG policies
thereby constructing children for) search node, instantiate incremental
CBG solver corresponding CBG. incremental solver returns one joint CBG
policy time, used construct single child t+1 = (t , ). revisiting
nodes, promising child nodes expanded incrementally.
Below, describe GMAA*-ICE, algorithm combines GMAA*-IC incremental expansion. establish theoretical guarantees describe modifications
BaGaBaB, CBG solver GMAA*-ICE employs, necessary deliver
child nodes decreasing order.
13. exceptional cases short horizon combined large state action spaces representing last time step vectors minimal. cases, algorithm trivially adapted.
14. assumes vector representation shrink earlier stages. Although unlikely
practice, cases would prevent algorithm computing minimal representation.
15. Kumar Zilberstein (2010b) tackle slightly different problem; introduce weighted constraint satisfaction approach solving point-based backup dynamic programming Dec-POMDPs. However,
point-based backup interpreted collection CBGs (Oliehoek et al., 2010).

474

fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPs

4.1 GMAA* Incremental Clustering Expansion
begin formalizing incremental expansion incorporating GMAA*-IC, yielding GMAA* incremental clustering expansion (GMAA*-ICE). core
incremental expansion lies following lemma:
Lemma 3. Given two joint CBG policies , CBG B(b0 ,t ), Vb () Vb ( ),
corresponding child nodes Vb (t+1 ) Vb (t+1 ).
Proof. holds directly definition Vb (t ) given (2.8):
Vb (t+1 ) = V 0...(t1) (t ) + Vb ()
V 0...(t1) (t ) + Vb ( ) = Vb (t+1 ).

follows directly that, B(b0 ,t ) use CBG solver generate sequence
policies , , . . .
Vb () Vb ( ) . . .

then, sequence corresponding children

Vb (t+1 ) Vb (t+1 ) . . . .

Exploiting knowledge, expand first child t+1 compute heuristic
value Vb (t+1 ) using (2.8). Since unexpanded siblings heuristic values less
equal that, modify GMAA*-IC reinsert node q open list L
act placeholder non-expanded children.
Definition 9. placeholder node least one child expanded.
placeholder heuristic value equal last expanded child.
Thus, expansion search node qs child, update q.v, heuristic value
node, Vb (t+1 ), value expanded child, i.e., set q.v Vb (t+1 ). such,
reinsert q L placeholder. mentioned above, correct
unexpanded siblings (for parent node q placeholder) heuristic values
lower equal Vb (t+1 ). Therefore next sibling q represented placeholder
always expanded time: q always created nodes lower heuristic value
selected expansion. keep track whether node previously expanded
placeholder not.
before, GMAA*-ICE performs A* search partially specified policies.
GMAA*-IC, new CBG constructed extending CBG parent node
applying lossless clustering. However, rather expanding children, GMAA*-ICE
requests next solution incremental CBG solver, single child
t+1 = (t , ) constructed. principle GMAA*-ICE use CBG solver able
incrementally deliver descending order Vb (). propose modification
BaGaBaB algorithm (Oliehoek et al., 2010), briefly discussed Section 4.3.
Fig. 7 illustrates process incremental expansion GMAA*-ICE, indexed
letters. First, CBG solver root node ha, 7i created, optimal solution
computed, value 6. results child hb, 6i, root replaced placeholder
node ha, 6i. per Definition 5 (the node comparison operator), b appears
475

fiOliehoek, Spaan, Amato, & Whiteson

Legend:

v




7


6

Root node


6




b
6

t+1

b
4


New B(a), Vb =6
c
4

t+2
ht , vi
open list

ha, 7i


5.5

New B(b), Vb =4

ha, 6i
hc, 4i
hb, 4i

hb, 6i
ha, 6i

b
4

c
4


5.5
Next solution
B(a), Vb =5.5

hd, 5.5i
ha, 5.5i
hc, 4i
hb, 4i

Figure 7: Illustration incremental expansion, nodes open list bottom.
Past joint policies indexed letters. Placeholder nodes indicated dashes.
open list hence selected expansion. best child hc, 4i added hb, 6i replaced
placeholder hb, 4i. search returns root node, second best solution
obtained CBG solver, leading child hd, 5.5i. Placeholder nodes retained
long unexpanded children; values updated.
using GMAA*-ICE, derive lower upper bounds CBG solution,
exploited incremental CBG solver. incremental CBG solver
B(t ) initialized lower bound
vCBG = vGM AA V 0...(t1) (t ),

(4.1)

vGM AA value current best solution, V 0...(t1) (t ) true expected
value first stages. Therefore, vCBG minimum value candidate
must generate remaining h stages order beat current best solution. Note
time incremental CBG solver queried solution, vCBG re-evaluated
(using (4.1)), vGM AA may changed.
used heuristic faitfully represents immediate reward (i.e., form
(2.9)), then, last stage = h 1, also specify upper bound solution
CBG
vCBG = Vb (h1 ) V 0...(h2) (h1 ).
(4.2)
upper bound attained, solutions required CBG solver.
upper bound holds since (2.8)
Vb () , Vb (h ) V 0...(h2) (h1 )

= V (h ) V 0...(h2) (h1 )
Vb (h1 ) V 0...(h2) (h1 ).

first step, Vb (h ) = V (h ), h fully specified policy heuristic value
given (2.8) equals actual value heuristic faithfully represents expected
476

fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPs

Algorithm 10 Expand-ICE(q, H). expand operator GMAA*-ICE.
Input: q = ht , vi search node expand.
b ~,a).
Input: H admissible heuristic form Q(
Output: QExpand set containing 0 1 expanded child nodes.
1: IsPlaceholder(q)
2:
B(t ) .CBG
{reuse stored CBG}
3: else
4:
B(t1 ) t1 .CBG
{retrieve previous CBG, note = (t1 , t1 )}
t1
b
5:
B(t ) ConstructExtendedBG(B(t1 ), , Q)


6:
B( ) ClusterBG(B( ))
7:
B(t ).Solver CreateSolver(B(t ))
8:
.CBG B(t )
{store pointer CBG}
9: end
{set lower bound CBG solution}
10: vCBG = vGM AA V 0...(t1) (t )
11: = h 1
12:
vCBG = Vb (h1 ) V 0...(h2) (h1 )
{upper bound used last stage CBG}
13: else
14:
vCBG = +
15: end
b ( )i B(t ).Solver.NextSolution(vCBG ,vCBG )
{compute next CBG solution}
16: h , V

17:
18:
t+1 (t , )
{create partial joint policy}

t+1
0...t1

b
b
19:
V ( ) V
( ) + V ( )
{compute heuristic value}
20:
q ht+1 , Vb (t+1 )i
{create child node}
21:
QExpand {q }
22: else
23:
QExpand
{fully expanded: exists solution s.t. V ( h1 ) vCBG }
24: end
25: return QExpand

Algorithm 11 PostProcessNode-ICE(q, L): Post processing node GMAA*-ICE.
Input: q last expanded node, L open list.
Output: q either removed updated.
1: L.Pop(q)
2: q fully expanded depth(q) = h 1
3:
Cleanup q
{delete node associated CBG Solver}
4:
return
5: else
6:
c last expanded child q
7:
q.v c.v
{update heuristic value parent node}
8:
IsPlaceholder(q) true
{remember q placeholder}
9:
L.Insert(q)
{reinsert appropriate position}
10: end

477

fiOliehoek, Spaan, Amato, & Whiteson

immediate reward used. implies Vb () lower bound. second step
V (h ) Vb (h1 ), Vb (h1 ) admissible. Therefore, stop expanding
find (lower bound) heuristic value equal upper bound vCBG . applies
last stage first step valid.
GMAA*-ICE implemented replacing Expand PostProcessNode
procedures Algorithms 8 4 Algorithms 10 11, respectively. Expand-ICE first
determines placeholder used either reuses previously constructed incremental CBG solver constructs new one. Then, new bounds calculated next
CBG solution obtained. Subsequently, single child node generated (rather
expanding children Algorithm 13). PostProcessNode-ICE removes last node
returned Select children expanded. Otherwise,
updates nodes heuristic value reinserts open list. See Appendix A.2
GMAA*-ICE shown single algorithm.
4.2 Theoretical Guarantees
section, prove GMAA*-IC GMAA*-ICE search-equivalent. direct
result establish GMAA*-ICE complete, means integrating incremental
expansion preserves optimality guarantees GMAA*-IC.
Definition 10. call two GMAA* variants search-equivalent select exactly
sequence non-placeholder nodes corresponding past joint policies expand
search tree using Select operator.
GMAA*-IC GMAA*-ICE show set selected nodes same.
However, set expanded nodes different; fact, precisely differences
incremental expansion exploits.
Theorem 5. GMAA*-ICE GMAA*-IC search-equivalent.
Proof. Proof listed Section A.4 appendix.
Note Theorem 5 imply computational space requirements
GMAA*-ICE GMAA*-IC identical. contrary, expansion,
GMAA*-ICE generates one child node stored open list. contrast,
GMAA*-IC generates number child nodes is, worst case, doubly exponential
depth selected node.16 However, GMAA*-ICE guaranteed
efficient GMAA*-IC. example, case child nodes still
generated, GMAA*-ICE slower due overhead incurs.
Corollary 3. using heuristic form (2.9) GMAA*-ICE complete.
Proof. stated conditions, GMAA*-IC complete (see Theorem 4).
GMAA*-ICE search equivalent GMAA*-IC, also complete.

Since

16. problem allows clustering, number child nodes grows less dramatically (see Section 3).

478

fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPs

4.3 Incremental CBG Solvers
Implementing GMAA*-ICE requires CBG solver incrementally deliver
descending order Vb (). end, propose modify Bayesian game Branch
Bound (BaGaBaB) algorithm (Oliehoek et al., 2010). BaGaBaB performs A*-search
(partially specified) CBG policies. Thus, applied within GMAA*-ICE, performs
second, nested A* search. expand node GMAA* search tree, nested A*
search computes next CBG solution.17 section briefly summarizes main ideas
behind BaGaBaB (for information, see Oliehoek et al., 2010) modifications.
BaGaBaB works creating search tree nodes correspond partially
specified joint CBG policies. particular, represents joint action vector, vector
h( 1 ), . . . ,( || )i joint actions specifies joint type. node g
BaGaBaB search tree represents partially specified vector thus partially specified
joint CBG policy. example, completely unspecified vector h, , . . . ,i corresponds
root node, internal node

g depth (root beingff depth 0) specifies joint
actions first joint types g = ( 1 ), . . . , ( ), , , . . . , . value node V (g)
value best joint CBG-policy consistent it. Since value known
advance, BaGaBaB performs A* search guided optimistic heuristic.
particular, compute upper bound value achievable
partially specified vector computing maximum value complete information joint
policy consistent (i.e., non-admissible joint policy selects maximizing
joint actions remaining joint types). Since value guaranteed upper bound
maximum value achievable consistent joint CBG policy, admissible heuristic.
propose modification BaGaBaB allow solutions incrementally delivered.
main idea retain search tree first call BaGaBaB particular CBG
B(t ) update subsequent calls, thereby saving computational effort.
Standard A* search terminates single optimal solution found.
behavior incremental BaGaBaB called first time B(t ).
However, standard A*, nodes whose upper bound lower best known lower
bound safely deleted, never lead optimal solution. contrast,
incremental setting nodes cannot pruned, could possibly result k-th
best solution therefore might need expanded subsequent calls BaGaBaB.
nodes returned solutions pruned order avoid returning solution
twice. modification requires memory affect A* search process
otherwise.
asked k-th solution, BaGaBaB resets internal lower bound value
next-best solution previously found returned (or vCBG defined
(4.1) solution found). starts A* search initialized using search
tree resulting (k 1)-th solution. essence, method similar searching
best k solutions, k incremented demand. Recently shown that,
fixed k, modification preserves theoretical guarantees (soundness, completeness,
17. GMAA*-ICE could also use incremental CGB solver, avoid enumerating
providing first result thus potential work incrementally. exception may
method Kumar Zilberstein (2010b), employs AND/OR branch bound search
EDAC heuristic (and thus limited two-agent case). heuristic search method, may
amenable incremental implementation though knowledge attempted.

479

fiOliehoek, Spaan, Amato, & Whiteson

optimal efficiency) A* algorithm (Dechter, Flerova, & Marinescu, 2012), results
trivially transfer setting k allowed increase.

5. Experiments
section, empirically test validate proposed techniques: lossless clustering
joint histories, incremental expansion search nodes, hybrid heuristic representations.
introducing experimental setup, compare performance GMAA*-IC
GMAA*-ICE GMAA* suite benchmark problems literature.
Next, compare performance proposed methods state-of-the-art optimal
approximate Dec-POMDP methods, followed case study scaling behavior
respect number agents. Finally, compare memory requirements
hybrid heuristic representation tree vector representations.
5.1 Experimental Setup
well-known Dec-POMDP benchmarks Dec-Tiger (Nair et al., 2003)
BroadcastChannel (Hansen et al., 2004) problems. Dec-Tiger discussed extensively
Section 2. BroadcastChannel, two agents transmit messages communication channel, agents transmit time collision occurs
noisily observed agents. FireFighting problem models team n firefighters
extinguish fires row nh houses (Oliehoek, Spaan, & Vlassis, 2008).
agent choose move houses fight fires location; two agents
house, completely extinguish fire there. (negative) reward
team firefighters depends intensity fire house; fires
extinguished, reward zero received. Hotel 1 problem (Spaan & Melo, 2008),
travel agents need assign customers hotels limited capacity. also send
customer resort yields lower reward. addition, also use following problems: Recycling Robots (Amato, Bernstein, & Zilberstein, 2007), scaled-down version
problem described Section 2; GridSmall two observations (Amato, Bernstein, &
Zilberstein, 2006) Cooperative Box Pushing (Seuken & Zilberstein, 2007a), larger
two-robot benchmark. Table 1 summarizes problems numerically, listing number
joint policies different planning horizons.
Experiments run Intel Core i5 CPU running Linux, GMAA*, GMAA*-IC,
GMAA*-ICE implemented code-base using MADP Toolbox (C++)
(Spaan & Oliehoek, 2008). vector-based QBG representation computed using variation Incremental Pruning (adapted computing Q-functions instead regular value functions), corresponding NaiveIP method described Oliehoek Spaan (2012).
implement pruning, employ Cassandras POMDP-solve software (A. R. Cassandra,
1998).
results Sections 5.2 5.3, limited process 2Gb RAM
maximum CPU time 3,600s. Reported CPU times averaged 10 independent runs
resolution 0.01s. Timings given MAA* search processes, since
480

fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPs

problem primitives

Dec-Tiger

num. h

n

|S|

|Ai |

|Oi |

2

4

6

2

2

3

2

7.29e2

2.06e14

1.31e60

BroadcastChannel

2

4

2

2

6.40e1

1.07e9

8.51e37

GridSmall

2

16

5

2

1.563e4

9.313e20

1.175e88

Cooperative Box Pushing

2

100

4

5

1.68e7

6.96e187

1.96e4703

Recycling Robots

2

4

3

2

7.29e2

2.06e14

1.31e60

Hotel 1

2

16

3

4

5.90e4

1.29e81

3.48e1302

FireFighting

2

432

3

2

7.29e2

2.06e14

1.31e60

Table 1: Benchmark problem sizes number joint policies different horizons.
computation heuristic methods amortized multiple
runs.18 problem definitions available via http://masplan.org.
5.2 Comparing GMAA*, GMAA*-IC, GMAA*-ICE
compared GMAA*, GMAA*-IC, GMAA*-ICE using hybrid QBG representation. methods compute optimal policy, expect GMAA*-IC efficient
GMAA* lossless clustering possible. Furthermore, expect GMAA*-ICE
provide improvements terms speedup scaling longer planning horizons.
results shown Table 2. entries report results, QBG heuristics
could computed, thanks hybrid representation. Consequently, performance
GMAA*-IC much better previously reported results, including Oliehoek
et al. (2009), often required resort QMDP larger problems and/or horizons.
entries marked show limits using QMDP instead QBG :
problems reach longer horizons QBG . FireFighting GMAA*-ICE
QMDP compute solutions higher h possible QBG (hence missing ,
showing GMAA*-ICE efficient using loose heuristic GMAA*-IC).
Furthermore, entries indicate horizon solve problem
tree-based QBG representation often much shorter.
results clearly illustrate GMAA*-IC leads significant improvement
performance. problems, GMAA*-IC able produce solution quickly
increase largest solvable horizon GMAA*. cases, GMAA*-IC able
drastically increase solvable horizon.
Furthermore, results clearly demonstrate incremental expansion allows significant additional improvements. fact, table demonstrates GMAA*-ICE significantly outperforms GMAA*-IC, especially problems little clustering possible.
results Table 2 also illustrate efficacy hybrid representation. problems
like GridSmall, Cooperative Box Pushing, FireFighting Hotel 1 neither
tree vector representation able provide compact QBG heuristic longer hori18. heuristics computation time ranges less second hours (for high h difficult
problems). Table 4 presents heuristic computation time results.

481

fiOliehoek, Spaan, Amato, & Whiteson

h
2
3
4
5
6
7

V TGMAA* (s)
Dec-Tiger
4.000000
0.01
5.190812
0.01
4.802755
563.09
7.026451

10.381625

TIC (s)

TICE (s)

h

0.01
0.01
0.27
21.03


0.01
0.01
0.01
0.02
46.43


2
3
4
5
6
10
15
18
20
30
40
50
60
70
80

FireFighting hnh = 3,nf = 3i
2 4.383496
0.09
0.01
0.01
3 5.736969
3.05
0.11
0.10
4 6.578834
1001.53 950.51
1.00
5 7.069874


4.40
6 7.175591
0.08
0.07
7
#
#
GridSmall
2
0.910000
0.01
0.01
0.01
3
1.550444
0.90
0.10
0.01
4
2.241577
*
1.77 0.01
5
2.970496

0.02
6
3.717168

0.04
7
#
#
Hotel 1
2 10.000000
0.0
0.01
0.01
3 16.875000
*
0.01
0.01
4 22.187500
0.01 0.01
5 27.187500
0.01
0.01
6 32.187500
0.01
0.01
7 37.187500
0.01
0.01
8 42.187500
0.01
0.01
9 47.187500
0.02
0.01
10
#
#
Cooperative Box Pushing
2
17.600000
0.02
0.01
0.01
3
66.081000

0.11 0.01
4
98.593613
313.07
5
#
#

2
3
4
5
6
7
10
20
25
30
40
50
53
100
250
500
600
700
800
900
1000

V TGMAA* (s) TIC (s) TICE (s)
Recycling Robots
7.000000
0.01 0.01
0.01
10.660125
0.01 0.01
0.01
13.380000
713.41 0.01
0.01
16.486000
0.01 0.01
19.554200
0.01
0.01
31.863889
0.01
0.01
47.248521
0.01
0.01
56.479290
0.01 0.01
62.633136
0.01
0.01
93.402367
0.08
0.05
124.171598
0.42
0.25
154.940828
2.02
1.27
185.710059
9.70
6.00
216.479290

28.66


BroadcastChannel
2.000000
0.01 0.01
0.01
2.990000
0.01 0.01
0.01
3.890000
0.01 0.01
0.01
4.790000
1.27 0.01
0.01
5.690000
0.01
0.01
6.590000
0.01 0.01
9.290000
0.01
0.01
18.313228
0.01
0.01
22.881523
0.01
0.01
27.421850
0.01
0.01
36.459724
0.01
0.01
45.501604
0.01
0.01
48.226420
0.01 0.01
90.760423
0.01
0.01
226.500545
0.06
0.07
452.738119
0.81
0.94
543.228071
11.63
13.84
633.724279
0.52
0.63


814.709393
9.57
11.11



Table 2: Experimental results comparing regular GMAA*, GMAA*-IC, GMAA*-ICE.
Listed computation times GMAA* (TGMAA* ), GMAA*-IC (TIC ),
GMAA*-ICE (TICE ), using hybrid QBG representation. use following symbols:
memory limit violations, time limit overruns, # heuristic computation exceeded memory time limits, maximum planning horizon using QMDP , maximum planning horizon
using tree-based QBG . Bold entries indicate methods proposed article
computed results.
zons. Apart Dec-Tiger FireFighting, computing storing QBG (or another
tight heuristic) longer horizons bottleneck scalability.
Together, algorithmic improvements lead first optimal solutions many
problem horizons. fact, vast majority problems tested, provide results
longer horizons previous work (the bold entries). improvements quite sub482

fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPs

h
2
3
4
5
2
3
4
5
6
2
3
4
2
3
4
5
6
7
8
9
2
3

|BGh1 |

|cBGt |
Dec-Tiger
4 1.0, 4.0
16 1.0, 4.0, 9.0
64 1.0, 4.0, 9.0, 23.14
256 1.0, 4.0, 9.0, 16.0, 40.43
FireFighting hnh = 3,nf = 3i
4 1.0, 4.0
16 1.0, 4.0, 16.0
64 1.0, 4.0, 16.0, 64.0
256 1.0, 4.0, 16.0, 64.0, 256.0
1024 1.0, 1.0, 2.0, 3.0, 6.0, 10.0
GridSmall
4 1.0, 4.0
16 1.0, 4.0, 10.50
64 1.0, 4.0, 10.50, 20.0
Hotel 1
16 1.0, 4.0
256 1.0, 4.0, 16.0
4096 1.0, 4.0, 8.0, 16.0
65536 1.0, 4.0, 4.0, 8.0, 16.0
1.05e6 1.0, 4.0, . . . , 4.0, 8.0, 16.0
1.68e7 1.0, 4.0, . . . , 4.0, 8.0, 16.0
2.68e8 1.0, 4.0, . . . , 4.0, 8.0, 16.0
4.29e9 1.0, 4.0, . . . , 4.0, 8.0, 16.0
Cooperative Box Pushing
25 1.0, 4.0
625 1.0, 4.0, 25.0

h
2
3
4
5
10
15
18
20
30
40
50
60
2
3
4
5
6
7
10
20
25
30
40
50
100
900

|BGh1 | |cBGt |
Recycling Robots
4 1.0, remaining stages
16 1.0, remaining stages
64 1.0, remaining stages
256 1.0, remaining stages
262144 1.0, remaining stages
2.68e8 1.0, remaining stages
1.72e10 1.0, remaining stages
2.75e11 1.0, remaining stages
2.88e17 1.0, remaining stages
1.0, remaining stages
1.0, remaining stages
1.0, remaining stages
BroadcastChannel
4 1.0 (for t)
16 1.0 (for t)
64 1.0 (for t)
256 1.0 (for t)
1024 1.0 (for t)
4096 1.0 (for t)
262144 1.0 (for t)
2.75e11 1.0 (for t)
2.81e14 1.0 (for t)
2.88e17 1.0 (for t)
1.0 (for t)
1.0 (for t)
1.0 (for t)
1.0 (for t)

4.0
4.0
4.0
4.0
4.0
4.0
4.0
4.0
4.0
4.0
4.0
4.0

Table 3: Experimental results detailing effectiveness clustering. Listed size
CBGs = h 1 without clustering (|BGh1 |), average CBG size stages
clustering (|cBGt |).
stantial, especially given lengthening horizon one increases problem difficulty
exponentially (cf. Table 1).
5.2.1 Analysis Clustering Histories
Table 3 provides additional details performance GMAA*-IC, listing
number joint types GMAA*-IC search, |cBGt |, stage t. averages
since algorithm forms CBGs different past policies, leading clusterings different
sizes.19 see impact clustering, table also lists |BGh1 |, number joint types
CBGs constructed last stage without clustering, constant.
Dec-Tiger, time needed GMAA*-IC 3 orders magnitude less
GMAA* horizon h = 4. h = 5, test problem 3.82e29 joint
policies, method able optimally solve it. GMAA*-IC, however,
able reasonable time. Dec-Tiger, clear symmetries
19. Note problem domains report smaller clusterings Oliehoek et al. (2009). Due
implementation mistake, clustering overly conservative, cases treat two histories
probabilistically equivalent, fact were.

483

fiOliehoek, Spaan, Amato, & Whiteson

observations allow clustering, demonstrated Fig. 4. Another key property
opening door resets problem, may also facilitate clustering.
FireFighting, short planning horizons lossless clustering possible
stage, such, clustering incurs overhead. However, GMAA*-IC still faster
GMAA* constructing BGs using bootstrapping previous CBG
takes less time constructing CBG scratch. Interesting counterintuitive results
occur h = 6, solved within memory limits, contrast h = 5. fact, using
QMDP could compute optimal values V h > 6, turns equal
h = 6. reason optimal joint policy guaranteed extinguish
fires 6 stages. subsequent stages, rewards 0.
influence clustering, analysis Table 3 reveals CBG instances encountered
h = 6 search happen cluster much better h = 5, possible
heuristics vary horizon. fact, h = 6 sends agents
middle house = 0, h = 5, agents dispatched different houses.
agents fight fires house, fire extinguished completely, resulting joint
observations provide new information. result, different joint types lead
joint belief, means clustered. agents visit different houses,
observations convey information, leading different possible joint beliefs (which cannot
clustered).
Hotel 1 allows large amount clustering, GMAA*-IC outperforms GMAA*
large margin, former reaching h = 9 latter h = 2. problem
transition observation independent (Becker, Zilberstein, Lesser, & Goldman, 2003;
Nair, Varakantham, Tambe, & Yokoo, 2005; Varakantham, Marecki, Yabu, Tambe, & Yokoo,
2007), facilitates clustering, discuss Section 5.5. Unlike methods
specifically designed exploit transition observation independence, GMAA*-IC exploits
structure without requiring predefined explicit representation it. scalability
limited computation heuristic.
BroadcastChannel, GMAA*-IC achieves even dramatic increase performance, allowing solution horizon h = 900. Analysis reveals CBGs
constructed stages fully clustered: contain one type agent.
reason follows. constructing CBG = 1, one joint type
previous CBG so, given 0 , solution previous CBG, uncertainty
respect previous joint action a0 . crucial property BroadcastChannel
(joint) observation reveals nothing new state, joint action
taken (e.g., collision agents chose send). result, different individual
histories clustered. CBG constructed stage = 2, one joint
type previous game. Therefore, given past policy, actions agents
perfectly predicted. observation conveys information process repeats. Thus, problem special property could described non-observable
given past joint policy. GMAA*-IC automatically exploits property. Consequently,
time needed solve CBG grow horizon. solution time, however, still increases super-linearly increased amount backtracking.
FireFighting, performance monotonic planning horizon. case however,
clustering clearly responsible difference. Rather, explanation
certain horizons, many near-optimal joint policies, leading backtracking
higher search cost.
484

fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPs

10

Nodes depth

10

DecTiger, h=6 Full Exp.
DecTiger, h=6 Inc. Exp.
GridSmall, h=6 Full Exp.
GridSmall, h=6 Inc. Exp.
FireFighting, h=5 Full Exp.
FireFighting, h=5 Inc. Exp.

5

10

0

10

0

1

2


3

4

Figure 8: Number expanded partial joint policies intermediate stages = 0, . . . ,h 2
(in log scale).

5.2.2 Analysis Incremental Expansion
Dec-Tiger h = 5, GMAA*-ICE achieves speedup three orders magnitude
compute solution h = 6, unlike GMAA*-IC. GridSmall, achieves large
speedup h = 4 fast solutions h = 5 6, GMAA*-IC runs memory. Similar positive results obtained FireFighting, Cooperative Box Pushing
Recycling Robots. fact, using QMDP , GMAA*-ICE able compute
solutions well beyond h = 1000 FireFighting problem, stands stark contrast GMAA*-IC computes solutions h = 3 heuristic. Note
BroadcastChannel problem GMAA*-IC (slightly) faster
GMAA*-ICE. problem exhibits clustering single joint type, overhead
incremental expansion pay off.
analyze incremental expansion, examined impact number nodes
expanded intermediate stages = 0, . . . ,h 2. Fig. 8 shows number nodes expanded
GMAA*-ICE number would expanded GMAA*-IC (which
easily computed since search-tree equivalent). clear relationship
results Fig. 8 Table 2, illustrating, e.g., GMAA*-IC runs memory
GridSmall h = 6. plots confirm hypothesis that, practice, small number
child nodes queried.
5.2.3 Analysis Hybrid Heuristic Representation
Fig. 9 illustrates memory requirements terms number parameters (i.e., real numbers) tree, vector, hybrid representations QBG , latter computed
following Algorithm 9. Results vector representation omitted representations grew beyond limits. effectiveness vector pruning depends problem
complexity value function, increase suddenly, instance happens Fig. 9c. results show that, several benchmark Dec-POMDPs, hybrid
representation allows significant savings memory, allowing computation tight
heuristics longer horizons.
485

fiOliehoek, Spaan, Amato, & Whiteson

h

MILP

DP-LPC

DP-IPG

GMAA QBG
IC

ICE

heur

BroadcastChannel, ICE solvable h = 900
2
0.38
0.01
0.09
0.01
3
1.83
0.50
56.66
0.01
4
34.06

*
0.01
5
48.94
0.01

0.01
0.01
0.01
0.01

0.01
0.01
0.01
0.01

Dec-Tiger, ICE
2
0.69
3
23.99
4

5

0.01
0.01
0.01
0.02

0.01
0.01
0.03
0.09

solvable h = 6
0.05
0.32
60.73
55.46

2286.38


0.01
0.01
0.27
21.03

FireFighting (2 agents, 3 houses, 3 firelevels), ICE
2
4.45
8.13
10.34
0.01
3


569.27
0.11
4

950.51
GridSmall, ICE solvable h = 6
2
6.64
11.58
0.18
3


4.09
4
77.44

0.01
0.10
1.77

Recycling Robots, ICE solvable h = 70
2
1.18
0.05
0.30
0.01
3
*
2.79
1.07
0.01
4
2136.16
42.02
0.01
5

1812.15
0.01
Hotel
2
3
4
5
9
10
15

1, ICE solvable h = 9
1.92
6.14
0.22
315.16
2913.42
0.54


0.73
1.11
8.43
17.40
283.76

Cooperative Box Pushing
2
3.56
15.51
3
2534.08
4


0.01
0.01
0.01
0.01
0.02
#

solvable h 1000
0.01 0.01
0.10
0.07
1.00
0.65
0.01
0.01
0.01

0.01
0.42
67.39

0.01
0.01
0.01
0.01

0.01
0.01
0.02
0.02

0.01
0.01
0.01
0.01
0.01
#

0.03
1.51
3.74
4.54
20.26

(QPOMDP ), ICE solvable h = 4
1.07
0.01 0.01 0.01
6.43
0.91
0.02
0.15
1138.61
*
328.97 0.63

Table 4: Comparison runtimes methods. Total time GMAA* methods
given taking time method column (IC ICE) adding heuristic
computation time (heur). use following symbols: memory limit violations, *
time limit overruns, # heuristic computation exceeded memory time limits.

486

fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPs

10

Memory required

5

10

0

10

5

10

0

1

2

3
4
Horizon

5

10

6

(a) Dec-Tiger.

5

10

0

10

2

3
4
Horizon

5

10

6

(d) Recycling Robots.

10

0

1 2 3 4 5 6 7 8 9
Horizon

15

10

Tree
Vector
Hybrid

10

10

5

10

0

1 2 3 4 5 6 7 8 9 10
Horizon

Tree
Vector
Hybrid

(c) Hotel 1.

15

Tree
Vector
Hybrid

Memory required

Memory required

10

10

1

10

20

(b) FireFighting.

15

10

10

Tree
Vector
Hybrid

10

10
Memory required

Memory required

10

Tree
Vector
Hybrid

Memory required

10

10

Tree
Vector
Hybrid

10

10

5

10

0

1 2 3 4 5 6 7 8 9 10
Horizon

(e) BroadcastChannel.

10

1

2

3
4
Horizon

5

6

(f) GridSmall.

Figure 9: Hybrid heuristic representation. y-axis shows number real numbers stored
different representations QBG several benchmark problems (in log scale).
5.3 Comparing Methods
section, compare GMAA*-IC GMAA*-ICE methods literature. begin comparing runtimes methods following state-ofthe-art optimal Dec-POMDP methods: MILP20 (Aras & Dutech, 2010) converts DecPOMDP mixed integer linear program, numerous solvers available.
used MOSEK version 6.0. DP-LPC21 (Boularias & Chaib-draa, 2008) performs dynamic programming lossless policy compression, CPLEX 12.4 LP solver.
DP-IPG (Amato et al., 2009) performs exact dynamic programing incremental policy
20. results reported deviate reported Aras Dutech (2010). number
problems, Aras et al. employed solution method solves MILP series (a tree) smaller
MILPs branching continuous realization weight variables earlier stages. is, past
joint policy stage t, solve different MILP involving subset consistent sequences.
Additionally, FireFighting GridSmall, use benchmark versions standard literature
(Oliehoek, Spaan, & Vlassis, 2008; Amato et al., 2006), whereas Aras Dutech (2010) use non-standard
versions. explains difference results ones reported article (personal
communication, Raghav Aras).
21. goal Boularias Chaib-draa (2008) find non-dominated joint policies initial beliefs.
previously reported results concerned run-time compute non-dominated joint policies, without
performing pruning full-length joint policies. contrast, report time needed compute
actual optimal Dec-POMDP policy (given b0 ). additionally requires final round pruning
subsequently computing value remaining joint policies initial belief. additional overhead explains differences run time report previously
reported (personal communication, Abdeslam Boularias).

487

fiOliehoek, Spaan, Amato, & Whiteson

Problem
Dec-Tiger
Cooperative Box Pushing
GridSmall

h
6
3
5


7
3
3

VMBDP
9.91
53.04
2.32

V
10.38
66.08
2.97

Table 5: Comparison optimal (V ) approximate (VMBDP ) values.

generation exploits known start state knowledge states reachable
DP backup.
Table 4, shows results comparison, demonstrates that, almost cases,
total time GMAA*-ICE (given sum heuristic computation time time
GMAA*-phase) significantly less state-of-the-art methods.
Moreover, demonstrated Table 2, GMAA*-ICE compute solutions longer horizons problems, except Cooperative Box Pushing Hotel 1.22
problems, possible compute QBG longer horizons. Overcoming
problem could enable GMAA*-ICE scale horizons well.
DP-LPC algorithm proposed Boularias Chaib-draa (2008) also improves
efficiency optimal solutions form compression. performance algorithm,
however, weaker GMAA*-IC. two main explanations performance difference. First, DP-LPC uses compression compactly represent values
sets useful sub-tree policies, using sequence form representation. policies themselves, however, compressed: still specify actions every possible observation
history (for policy needs select exponential amount sequences make
policy). Hence, cannot compute solutions long horizons. Second, GMAA*-IC
exploit knowledge initial state distribution b0 .
Overall, GMAA*-ICE substantially improves state-of-the-art optimally solving
Dec-POMDPs. Previous methods typically improved feasible solution horizon
one (or provided speed-ups horizons could already solved). contrast,
GMAA*-ICE dramatically extends feasible solution horizon many problems.
also consider MBDP-based approaches, leading family approximate algorithms.
Table 5, reports VMBDP values produced PBIP-IPG (Amato et al., 2009) (with
typical maxTrees parameter setting m), demonstrates optimal solutions produced
GMAA*-IC GMAA*-ICE higher quality. PBIP-IPG chosen
MBDP algorithms parameters achieve value.
exhaustive, comparison illustrates even best approximate Dec-POMDP methods
practice provide inferior joint policies problems. Conducting analysis
possible optimal solutions computed. Clearly, data becomes
available, thorough comparisons made. Therefore, scalable optimal
solution methods GMAA*-ICE critical improving analyses.
488

fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPs

problem primitives

num. h

n

|S|

|A|

|O|

2

4

6

2

27

4

4

64

1.07e9

8.50e37

3

81

8

8

512

3.51e13

7.84e56

4

243

16

16

4.09e3

1.15e18

7.23e75

5

729

32

32

3.27e4

3.77e22

6.67e94

6

2187

64

64

2.62e5

9.80e55

6.15e113

Table 6: FireFightingGraph: number joint policies different numbers agents
horizons, 3 possible fire levels.
5.4 Scaling Agents
benchmark problems results presented far limited two agents. Here,
present case study FireFightingGraph (Oliehoek, Spaan, Whiteson, & Vlassis,
2008), variation FireFighting allowing agents, agent
fight fires two houses, instead them. Table 6 highlights size
problems, including total number joint policies different horizons. compared
GMAA*, GMAA*-IC, GMAA*-ICE (all using QMDP heuristic), BruteForceSearch,
DP-IPG, maximum run-time 12 hours running Intel Core i7 CPU,
averaged 10 runs. BruteForceSearch simple optimal algorithm enumerates
evaluates joint policies, implemented codebase GMAA*
variations. DP-IPG results use original implementation run Intel Xeon
computer. Hence, timing results directly comparable, overall trends
apparent. Also, since DP-IPG implementation limited 2 agents, results shown
agents.
Fig. 10 shows computation times FireFightingGraph across different numbers
agents planning horizons, Table 7 lists optimal values obtained. expected,
baseline BruteForceSearch performs poorly, scaling beyond h = 2 2
agents, DP-IPG reach h = 4. hand, regular GMAA* performs
relatively well, scaling maximum 5 agents. However, GMAA*-IC GMAA*-ICE
improve efficiency GMAA* 12 orders magnitude. such, substantially
outperform three methods, scale 6 agents. benefit incremental expansion clear n = 3,4, GMAA*-ICE reach higher horizon GMAA*-IC.
Hence, although article focuses scalability horizon, results show
methods propose also improve scalability number agents.
5.5 Discussion
Overall, empirical results demonstrate incremental clustering expansion offers
dramatic performance gains diverse set problems. addition, results Broad22. Hotel 1, DP-IPG performs particularly well problem structure limited reachability.
is, agent fully observe local state (but agent) local states
except one one action dominates others. result, DP-IPG generate small number
possibly optimal policies.

489

fiOliehoek, Spaan, Amato, & Whiteson

4

4

10

10

3

2

10

1

10

0

10

1

10

2

10

3

10

6

5

4

3

2

2

3

4

5

6

7

8

9

10

3

10

computation time (s)

computation time (s)

computation time (s)

4

10

3

10

2

10

1

10

0

10

1

10

2

10

3

10

6

5

4

h

#agents

3

2

2

3

5

4

7

6

8

9

10

10

2

10

1

10

0

10

1

10

2

10

3

10

6

5

h

#agents

(a) GMAA* results.

3

2

2

3

4

5

7

8

9

10

h

#agents

(b) GMAA*-IC results.

4

(c) GMAA*-ICE results.

4

10

10

3

10

2

10

1

10

0

10

1

10

2

10

3

10

6

5

4

3

2

2

3

4

5

6

7

8

9

10

computation time (s)

3

computation time (s)

4

6

10

2

10

1

10

0

10

1

10

2

10

3

10

6
h

#agents

5

4

3

2

2

3

4

5

6

7

8

9

10

h

#agents

(d) BruteForceSearch results.

(e) DP-IPG results.

Figure 10: Comparison GMAA*, GMAA*-IC, GMAA*-ICE, BruteForceSearch,
DP-IPG FireFightingGraph problem. Shown computation time (in log
scale) various number agents horizons. Missing bars indicate method
exceeded time memory limits. However, DP-IPG implementation supports 2
agents.

h
2
3
4
5
6

n=2
4.394252
5.806354
6.626555
7.093975
7.196444

n=3
5.213685
6.654551
7.472568

n=4
6.027319
7.391423
8.000277

n=5
6.846752

n=6
7.666185

Table 7: Value V optimal solutions FireFightingGraph problem, different
horizons numbers agents.

castChannel illustrate key advantage approach: problem possesses property makes large amount clustering possible, clustering method exploits
property automatically, without requiring predefined explicit representation it.
490

fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPs

course, problems admit great reductions via clustering. One domain property
allows clustering past joint policy encountered GMAA* makes
observations superfluous, BroadcastChannel FireFighting. Dec-Tiger,
see certain symmetries lead clustering. However clustering occur even
without properties. fact, problems nearly horizons tested,
size CBGs reduced. Moreover, accordance analysis Section 3.2,
improvements planning efficiency huge, even modest reductions CBG size.
One class problems say something priori amount clustering
possible class Dec-POMDPs transition observation independence
(Becker et al., 2003). problems, agents local states transitions
independent, two agents expressed
Pr(s1 , s2 |s1 , s2 , a1 , a2 ) = Pr(s1 |s1 , a1 ) Pr(s2 |s2 , a2 ).

(5.1)

Similarly, observations assumed independent, means agent
observation probability depends action local state: Pr(oi |ai , si ).
problems, probabilistic equivalence criterion (3.1) factors too. particular, due
transition observation independence23 , (3.2) holds true ~ia ,~ib . Moreover, (3.3)
factors product Pr(s1 , s2 |~1 , ~2 ) = Pr(s1 |~1 ) Pr(s2 |~2 ) thus holds Pr(s1 |~1a ) =
Pr(s1 |~1b ). is, two histories clustered induce local belief.
such, size CBGs directly corresponds product number reachable local
beliefs. Since transition observation independent Hotel 1 problem also locally
fully observable, local state spaces consist four states, four possible
local beliefs (which consistent CBG size 16 Table 3). Moreover, see
maximum size typically reached end search. good
policies defer sending customers hotel thus visit local states hotel
filled earlier stages.
general classes problems, even weakly coupled models (e.g., Becker,
Zilberstein, & Lesser, 2004; Witwicki & Durfee, 2010), criterion (3.1) factor,
hence direct correspondence number local beliefs. such,
applying clustering algorithm determine well problem clusters.
analogous to, e.g., state aggregation MDPs (e.g., discussed Givan, Dean, &
Greig, 2003) known predict priori large minimized model
be. Fortunately, empirical results demonstrate that, domains admit little
clustering, overhead small.
expected, incremental expansion helpful problems allow
much clustering. However, results for, e.g., Dec-Tiger illustrate limit
amount scaling method currently provide. bottleneck solution
large CBGs later stages: CBG solver solve large CBGs
returning first solution order guarantee optimality, takes takes long time.
expect improvements CBG solvers directly add efficacy
incremental expansion.
experiments also clearly demonstrate Dec-POMDP complexity results,
important, worst-case results. fact, scalability demonstrated experiments
clearly show many problems successfully scale dramatically beyond would
23. assumes external state variable s0 .

491

fiOliehoek, Spaan, Amato, & Whiteson

expected doubly-exponential dependence horizon. Even smallest problems,
doubly-exponential scaling horizon implies impossible compute solutions
beyond h = 4 all, indicated following simple calculation: let n = 2, |Ai | = 2
actions, |Oi | = 2| observations,
5

|Ai |(n(|Oi |

))

4

/|Ai |(n(|Oi |

))

= 4.2950e9.

Thus, even simplest possible case, see increase factor 4.2950e09 h = 4
h = 5. Similarly, next increment, h = 5 h = 6, increases size search
space factor 1.8447e19. However, experiments clearly indicate almost
cases, things dire. is, even though matters look bleak light
complexity results, many cases able perform substantially better worst
case.

6. Related Work
section, discuss number methods related proposed
article. methods already discussed earlier sections. Section 3,
indicated clustering method closely related approach Emery-Montemerlo
et al. (2005) also fundamentally different method lossless. Section 5.3,
discussed connections approach Boularias Chaib-draa (2008) clusters
policy values. contrasts approach clusters histories thus
policies themselves, leading greater scalability.
Section 3.1.2, discussed relationship notion probabilistic equivalence (PE) multiagent belief. However, yet another notion belief, employed
JESP solution method (Nair et al., 2003), superficially similar PE
distribution. JESP belief AOH ~i probability distribution Pr(s,~o6=i |~i , b0 , 6=i )
states observation histories agents given (deterministic) full policy
agents. sufficient statistic, since induces multiagent belief, thus also
allows clustering histories. crucial difference with, utility of, PE lies
fact PE criterion specified states AOHs given past joint policy.
is, (3.1) induce multiagent belief.
clustering approach also resembles number methods employ equivalence
notions. First, several approaches exploit notion behavioral equivalence (Pynadath &
Marsella, 2007; Zeng et al., 2011; Zeng & Doshi, 2012). consider, perspective
protagonist agent i, possible models another agent j. Since j affects
actions, i.e., behavior, agent cluster together models agent j lead
policy j agent. is, cluster models agent j
behaviorally equivalent. contrast, cluster models agents j, histories
agent agents, well environment, guaranteed behave
expectation, thus leading best response agent i. is, method
could seen clustering histories expected environmental behavior equivalent.
notion utility equivalence (Pynadath & Marsella, 2007; Zeng et al., 2011) closer
PE also takes account (value the) best-response agent (in particular,
clusters two models mj mj using BR(mj )the best response mj achieves
value mj ). However, remains form behavior equivalence
clusters models agents, histories protagonist agent.
492

fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPs

also connections PE work influence-based abstraction (Becker et
al., 2003; Witwicki & Durfee, 2010; Witwicki, 2011; Oliehoek et al., 2012), since influence
(or point parameter space, Becker et al., 2003) compact representation
agents policies. Models agents clustered lead influence
agent i. However, though fine-grained, ultimately still form behavioral
equivalence.
final relation equivalence notion work Dekel, Fudenberg, Morris
(2006), constructs distance measure topology space types
goal approximating infinite universal type space (the space possible beliefs
beliefs beliefs, etc.) one-shot Bayesian games. setting, however, considers
simple finite type space types directly correspond private histories (in
form AOHs) sequential problem. Thus, need approximate universal
type space; instead want know histories lead future dynamics
perspective agent. Dekel et al.s topology address question.
incremental expansion technique related approaches extending deal
large branching factors context multiple sequence alignment (Ikeda & Imai, 1999;
Yoshizumi, Miura, & Ishida, 2000). However, approach different
discard unpromising nodes rather provide mechanism generate necessary
ones. Also, proposing MAA*, Szer et al. (2005) developed superficially similar approach could applied last stage. particular, proposed generating
child nodes one one, time checking child found value equal
parents heuristic value. Since value child specifies full policy, value
lower bound therefore expansion remaining child nodes skipped. Unfortunately, number issues prevent approach providing substantial leverage
practice. First, cannot applied intermediate stages 0 < h 1 since lower bound
values expanded children available. Second, many problems unlikely
child node exists. Third, even does, Szer et al. specify efficient way
finding it. Incremental expansion overcomes issues, yielding approach that,
experiments demonstrate, significantly increases size Dec-POMDPs
solved optimally.
article focuses optimal solutions Dec-POMDPs finite horizon. part
evaluation, compare MILP approach (Aras & Dutech, 2010), DPILP (Boularias & Chaib-draa, 2008) DP-IPG (Amato et al., 2009), extension
exact dynamic programming algorithm (Hansen et al., 2004). Research finite-horizon DecPOMDPs considered many approaches bounded approximations (Amato,
Carlin, & Zilberstein, 2007), locally optimal solutions (Nair et al., 2003; Varakantham, Nair,
Tambe, & Yokoo, 2006) approximate methods without guarantees (Seuken & Zilberstein,
2007b, 2007a; Carlin & Zilberstein, 2008; Eker & Akn, 2010; Oliehoek, Kooi, & Vlassis, 2008;
Dibangoye et al., 2009; Kumar & Zilberstein, 2010b; Wu et al., 2010a; Wu, Zilberstein, &
Chen, 2010b).
particular, much research considered optimal and/or approximate solution
subclasses Dec-POMDPs. One subclass contains Dec-POMDPs
agents local states agents cannot influence. resulting models,
TOI-Dec-MDP (Becker et al., 2003; Dibangoye, Amato, Doniec, & Charpillet, 2013) NDPOMDP (Nair et al., 2005; Varakantham et al., 2007; Marecki, Gupta, Varakantham, Tambe,
& Yokoo, 2008; Kumar & Zilberstein, 2009), interpreted independent (PO)MDPs
493

fiOliehoek, Spaan, Amato, & Whiteson

agent coupled reward function (and possibly unaffectable state
feature). hand, event-driven interaction models (Becker et al., 2004) consider
agents individual rewards influence others transitions.
recently, models allow limited transition reward dependence
introduced. Examples interaction-driven Markov games (Spaan & Melo, 2008), DecMDPs sparse interactions (Melo & Veloso, 2011), distributed POMDPs coordination locales (Varakantham et al., 2009; Velagapudi et al., 2011), event-driven interactions
complex rewards (EDI-CR) (Mostafa & Lesser, 2011), transition decoupled Dec-POMDPs
(Witwicki & Durfee, 2010; Witwicki, 2011). methods developed models often exhibit better scaling behavior methods standard Dec-(PO)MDPs, typically
suitable agents extended interactions, e.g., collaborate transporting
item. Also, specialized models consider timing actions whose
ordering already determined (Marecki & Tambe, 2007; Beynier & Mouaddib, 2011).
Another body work addresses infinite-horizon problems (Amato, Bernstein, & Zilberstein, 2010; Amato, Bonet, & Zilberstein, 2010; Bernstein, Amato, Hansen, & Zilberstein,
2009; Kumar & Zilberstein, 2010a; Pajarinen & Peltonen, 2011), possible
represent policy tree. approaches represent policies using finite-state controllers
optimized various ways. Also, since infinite-horizon case undecidable
(Bernstein et al., 2002), approaches approximate optimal given particular controller size. exists boundedly optimal approach theoretically construct
controller within optimal, feasible small problems large
(Bernstein et al., 2009).
also great interest Dec-POMDPs explicitly take account communication. approaches try optimize meaning communication actions without
semantics (Xuan, Lesser, & Zilberstein, 2001; Goldman & Zilberstein, 2003; Spaan, Gordon,
& Vlassis, 2006; Goldman, Allen, & Zilberstein, 2007) others use fixed semantics (e.g.,
broadcasting local observations) (Ooi & Wornell, 1996; Pynadath & Tambe, 2002; Nair et
al., 2004; Roth et al., 2005; Oliehoek, Spaan, & Vlassis, 2007; Roth, Simmons, & Veloso, 2007;
Spaan, Oliehoek, & Vlassis, 2008; Goldman & Zilberstein, 2008; Becker, Carlin, Lesser, & Zilberstein, 2009; Williamson, Gerding, & Jennings, 2009; Wu et al., 2011). Since models used
first category (e.g., Dec-POMDP-Com) converted normal Dec-POMDPs
(Seuken & Zilberstein, 2008), contributions article applicable settings.
Finally, numerous models closely related Dec-POMDPs, POSGs
(Hansen et al., 2004), interactive POMDPs (I-POMDPs) (Gmytrasiewicz & Doshi, 2005),
graphical counterparts (Doshi, Zeng, & Chen, 2008). models general sense consider self-interested settings agent individual
reward function. I-POMDPs conjectured also require doubly exponential time (Seuken
& Zilberstein, 2008). However, I-POMDP number recent advances
(Doshi & Gmytrasiewicz, 2009). current paper makes clear link best-response
equivalence histories notion best-response equivalence beliefs I-POMDPs.
particular, article demonstrates two PE action-observation histories (AOHs) induce, given past joint policy, distribution states AOHs agents,
therefore induce multiagent belief future policies agents.
induced multiagent beliefs, turn, interpreted special cases I-POMDP beliefs
model agents sub-intentional models form fixed policy
tree. Rabinovich Rosenschein (2005) introduced method that, rather optimizing
494

fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPs

expected value joint policy, selects coordinated actions uncertainty tracking
dynamics environment. approach, however, requires model ideal system
dynamics input many problems, considered article, identifying
dynamics difficult.

7. Future Work
Several avenues future work made possible research presented article.
Perhaps promising development new approximate Dec-POMDP algorithms.
article focused optimal methods, GMAA*-ICE also seen framework approximate methods. methods could derived limiting amount
backtracking, employing approximate CBG solvers (Emery-Montemerlo, Gordon, Schneider,
& Thrun, 2004; Kumar & Zilberstein, 2010b; Wu et al., 2010a), integrating GMAA* methods factored Dec-POMDPs (Oliehoek, Spaan, Whiteson, & Vlassis, 2008; Oliehoek, 2010;
Oliehoek et al., 2013), performing lossy clustering (Emery-Montemerlo, 2005; Wu et al., 2011)
using bounded approximations heuristics. particular, seems promising combine approximate clustering approximate factored GMAA* methods.
Lossy clustering could achieved generalizing probabilistic equivalence criterion,
currently strict little clustering may possible many problems.
obvious approach cluster histories distributions states histories
agents merely similar, measured by, e.g., Kullback-Leibler divergence. Alternately,
histories could clustered induce individual belief states:
Pr(s|~i ) =

X

Pr(s,~
6=i |~i ).

(7.1)

~
6=i

individual beliefs sufficient statistics history, hypothesize
constitute effective metrics approximate clustering. Since individual belief simply
marginalizes agents histories probabilities used probabilistic
equivalence criterion, intuitive heuristic metric approximate clustering.
article focuses increasing scalability respect horizon, developing
techniques deal larger number agents important direction future work.
plan explore performing GMAA* using factored representations (Oliehoek, Spaan,
Whiteson, & Vlassis, 2008). previous work, could exploit factorization
last stage, since earlier stages required full expansions guarantee optimality. However,
larger problems, number joint BG policies (i.e., number child nodes)
directly large (earlier stages tightly coupled); therefore incremental expansion
crucial improving scalability optimal solution methods respect number
agents.
Another avenue future work generalize GMAA*-ICE. particular,
may possible flatten two nested searches single search.
could lead significant savings would obviate need solve entire CBG
expanding next one. work, employed plain algorithm basis,
promising direction future work investigate enhancements literature
(Edelkamp & Schrodl, 2012) benefit GMAA* most. particular, described
experiments, different past joint policies lead CBGs different sizes. One idea
495

fiOliehoek, Spaan, Amato, & Whiteson

first expand parts search tree lead small CBGs, biasing selection
operator (but pruning operator, maintain optimality).
Yet another important direction future work development tighter heuristics.
Though researchers addressing topic, results presented article underscore important heuristics solving larger problems. Currently, heuristic
bottleneck four seven problems considered. Moreover, two
problems bottleneck already solved long (h > 50) horizons.
Therefore, believe computing tight heuristics longer horizons single
important research direction improving scalability optimal Dec-POMDP
solution methods respect horizon.
different direction employ theoretical results clustering beyond DecPOMDP setting develop new solution methods CBGs. instance, well-known
method computing local optimum alternating maximization (AM): starting
arbitrary joint policy, compute best response agent given agents keep
policies fixed select another agents policy improve, etc. One idea start
completely clustered CBG, agents types clustered together thus
random joint CBG policy simple form: agent selects single action.
improving policy agent consider actual possible types compute
best response. Subsequently, cluster together types agent selects
action proceed next agent. addition, since clustering results
restricted collaborative setting, may also possible employ them, using similar
approach, develop new solution methods general-payoff BGs.
Finally, two contributions significant impact beyond problem
optimally solving Dec-POMDPs. First, idea incrementally expanding nodes introduced
GMAA*-ICE applied search methods. Incremental expansion
useful children generated order decreasing heuristic value without prohibitive
computational effort, problems large branching factor multiple sequence
alignment problems computational biology (Carrillo & Lipman, 1988; Ikeda & Imai, 1999).
Second, representing PWLC value functions hybrid tree set vectors
wider impact well, e.g., online search POMDPs (Ross, Pineau, Paquet, & Chaib-draa,
2008).

8. Conclusions
article presented set methods advance state-of-the-art optimal solution
methods Dec-POMDPs. particular, presented several advances aim extend
horizon optimal solutions found. advances build GMAA*
heuristic search approach include lossless incremental clustering CBGs solved
GMAA*, incremental expansion nodes GMAA* search tree, hybrid heuristic
representations. provided theoretical guarantees that, suitable heuristic used,
incremental clustering incremental expansion yield algorithms complete search equivalent. Finally, presented extensive empirical results demonstrating
GMAA*-ICE optimally solve Dec-POMDPs unprecedented size. significanty
increase planning horizons tackledin cases order
magnitude. Given increase horizon one results exponentially larger
search space, constitutes large improvement. Moreover, techniques also im496

fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPs

prove scalability respect number agents, leading first ever solutions
general Dec-POMDPs three agents. results also demonstrated
optimal techniques yield new insights particular Dec-POMDPs, incremental
clustering revealed properties BroadcastChannel make much easier solve.
addition facilitating optimal solutions, hope advances inspire new principled
approximation methods, incremental clustering already done (Wu et al., 2011),
enable meaningfully benchmarked.

Acknowledgments
thank Raghav Aras Abdeslam Boularias making code available us. Research supported part AFOSR MURI project #FA9550-09-1-0538 part NWO
CATCH project #640.005.003. M.S. funded FP7 Marie Curie Actions Individual
Fellowship #275217 (FP7-PEOPLE-2010-IEF).

Appendix A. Appendix
A.1 Auxiliary algorithms
Algorithm 12 implements BestJointPolicyAndValue function, prunes child
nodes fully specified. Algorithm 13 generates children particular CBG.
Algorithm 12 BestJointPolicyAndValue(QExpand ): Prune fully expanded nodes set
nodes QExpand returning best one value.
Input: QExpand set nodes fully specified joint policies.
Output: best full joint policy input set value.
1: v =
2: q QExpand
3:
QExpand .Remove(q)
4:
h, vi q
5:
v > v
6:
v v
7:

8:
end
9: end
10: return h , v

A.2 Detailed GMAA*-ICE algorithm
complete GMAA*-ICE algorithm shown Algorithm 14.
A.3 Computation V 0...t1 (t )
quantity V 0...t1 (t ) defined recursively via:
V 0...t1 (t ) = V 0...t2 (t1 ) + Est1 ,~t1 [R(st1 , t1 (~ t1 )) | b0 , ].
497

(A.1)

fiOliehoek, Spaan, Amato, & Whiteson

Algorithm 13 GenerateAllChildrenForCBG(B(t )).
Input: CBG B(t ).
Output: QExpand set containing expanded child nodes CBG.
1: QExpand {}
2: jointP
CBG policies B
3:
Vb () Pr()u(,())
4:
t+1 (t , )
{create partial joint policy}

t+1
0...t1

b
b
5:
V ( ) V
( ) + V ( )
{compute heuristic value}
6:
q ht+1 , Vb (t+1 )i
{create child node}
7:
QExpand .Insert(q )
8: end
9: return QExpand

expectation taken respect joint probability distribution states
joint AOHs induced :
X
Pr(st ,~ |b0 ,t ) =
Pr(ot |at1 ,st ) Pr(st |st1 ,at1 ) Pr(at1 |t ,~ t1 ) Pr(st1 ,~ t1 |b0 ,t ).
st1

(A.2)
Here, ~ = (~ t1 ,at1 ,ot ) Pr(at1 |t ,~ t1 ) probability specifies at1
AOH ~ t1 (which 0 1 case deterministic past joint policy ).
A.4 Proofs
Proof Theorem 1
Substituting (2.9) (2.7) yields
X
b ~ , (~ ))
Vb () = Vb ( ) =
Pr(~ |b0 ,t )Q(
~


=

X
~




Pr(~ |b0 ,t ) Est [R(st , (~ )) | ~ ] + E~t+1 [Vb (~ t+1 ) | ~ , (~ )]

= Est ,~t [R(st , (~ ) | b0 , ] + E~t+1 [Vb (~ t+1 ) | b0 , , ]

Est ,~t [R(st , (~ ) | b0 , ] + E~t+1 [Q (~ t+1 , (~ t+1 )) | b0 , t+1 = (t , )]
= Est ,~t [R(st , (~ ) | b0 , ] + H ,t+1...h1 (t+1 ),

H optimal admissible heuristic. Substituting (2.8) obtain
Vb (t+1 = (t , )) = V 0...t1 (t ) + Est ,~t [R(st , (~ ) | b0 , ] + E~t+1 [Vb (~ t+1 ) | b0 , , ]
V 0...t1 (t ) + Est ,~t [R(st , (~ )) | b0 , t+1 ] + H ,t+1...h1 (t+1 )

{via (A.1)} = V 0...t (t+1 ) + H ,t+1...h1 (t+1 ),
demonstrates heuristic value Vb (t ) used GMAA* via CBGs using heuristic
form (2.9) admissible, lower bounded actual value first plus
admissible heuristic. Since performs heuristic search admissible heuristic,
algorithm also complete.
498

fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPs

Algorithm 14 GMAA*-ICE
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:
28:
29:
30:
31:
32:
33:
34:
35:
36:
37:
38:
39:
40:
41:
42:
43:
44:
45:
46:
47:

vGM AA
0 ()
v +
q 0 h0 , vi
LIE {q 0 }
repeat
q Select(LIE )
{q = ht , vi}
IE
L .pop(q)
IsPlaceholder(q)
B(t ) .CBG
{reuse stored CBG}
else
{Construct extended BG solver:}
B(t1 ) t1 .CBG
{note = (t1 , t1 )}
t1

t1
B( ) ConstructExtendedBG(B( ), )
B(t ) ClusterBG(B(t ))
B(t ).Solver CreateSolver(B(t ))
.CBG B(t )
end
{Expand single child:}
vCBG = vGM AA V 0...(t1) (t )
vCBG = +
last stage = h 1
vCBG = Vb (h1 ) V 0...(h2) (h1 )
end
h , Vb ( )i B(t ).Solver.NextSolution(vCBG ,vCBG )

{fully expanded: solution s.t. V ( h1 ) vCBG }
delete q (and CBG + solver)
continue
{(i.e. goto line 8)}
end
t+1 (t , )
Vb (t+1 ) V 0...t1 (t ) + Vb ( )
last stage = h 1
{Note = t+1 , V () = Vb (t+1 ) }
V () > vGM AA
vGM AA V ()
{found new lower bound}

LIE .prune(vGM AA )
end
delete q (and CBG + solver)
else
q ht+1 , Vb (t+1 )i
LIE .insert(q )
q ht , Vb (t+1 )i
{ Update parent node q, placeholder }
LIE .insert(q)
end
LIE empty

499

fiOliehoek, Spaan, Amato, & Whiteson

Proof Lemma 1

t+1

t+1 ~
Proof. Assume arbitrary ati ,ot+1
6=i ,at6=i ,ot+1
6=i = (~
, 6=i ,s
6=i )).
t+1

~ a,t
Pr(st+1 ,~
6=i ,ot+1
|i ,ai , 6=i )
X


t+1
t+1
6=i |~ia,t )
Pr(ot+1
) Pr(st+1 |st ,ati ,at6=i ) Pr(at6=i |~
6=i , t6=i ) Pr(st ,~
=
,o6=i |ai ,a6=i ,s
st

=

X



t+1
t+1
6=i |~ib,t )
) Pr(st+1 |st ,ati ,at6=i ) Pr(at6=i |~
6=i , t6=i ) Pr(st ,~
Pr(ot+1
,o6=i |ai ,a6=i ,s

st

t+1
~ b,t
= Pr(st+1 ,~
6=i ,ot+1
|i ,ai , 6=i )
t+1

assumed arbitrary st+1 ,~
6=i ,ot+1
,
st+1 ,~t+1 ,ot+1
6=i



t+1
t+1 ~ t+1 t+1 ~ b,t
~ a,t
, 6=i ,oi |i ,ai , 6=i )
Pr(st+1 ,~
6=i ,ot+1
|i ,ai , 6=i ) = Pr(s

(A.3)

general
t+1

Pr(s

t+1

t+1

,~
6=i |~it ,ati ,ot+1
, 6=i ) =

~t
Pr(st+1 ,~
6=i ,ot+1
|i ,ai , 6=i )
Pr(ot+1 |~ ,at , )


=





6=i

t+1
~t
Pr(st+1 ,~
6=i ,ot+1
|i ,ai , 6=i )
P
t+1 t+1
t+1 ,~
~

t+1 Pr(s
6=i ,oi |i ,ai , 6=i )
t+1
~

,
6=i

Now, (A.3), numerator denominator substituting
~ia,t ,~ib,t equation. Consequently, conclude
t+1

t+1 ~ t+1 ~ b,t t+1
, 6=i |i ,ai ,oi , 6=i )
Pr(st+1 ,~
6=i |~ia,t ,ati ,ot+1
, 6=i ) = Pr(s
t+1


t+1 , ~
Finally, ati , ot+1
6=i arbitrarily chosen, conclude
, 6=i ,s
(3.4) holds.

Proof Lemma 2
Proof. Assume arbitrary 6=i ,s 6=i ,
bi (s, 6=i |~ia , 6=i ) , Pr(s, 6=i |~ia , 6=i ,b0 )
X
=
Pr(s, 6=i ,~
6=i |~ia , 6=i ,b0 )
~
6=i

{factoring joint distribution}

=

X

Pr(s,~
6=i |~ia , 6=i ,b0 ) Pr( 6=i |s,~
6=i , ~ia , 6=i ,b0 )

~
6=i

500

fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPs

{ 6=i depends ~
6=i , 6=i } =

X

Pr(s,~
6=i |~ia , 6=i ,b0 ) Pr( 6=i |~
6=i , 6=i )

{ s,~
6=i depend 6=i } =

X

Pr(s,~
6=i |~ia ,6=i ,b0 ) Pr( 6=i |~
6=i , 6=i )

{due PE} =

X

Pr(s,~
6=i |~ib ,6=i ,b0 ) Pr( 6=i |~
6=i , 6=i )

~
6=i

~
6=i

~
6=i

= [...] = Pr(s, 6=i |~ib , 6=i ,b0 ) = bi (s, 6=i |~ib , 6=i )
conclude holds 6=i ,s 6=i .
Proof Theorem 5 (Search Equivalence)
prove search equivalence, explicitly write node tuple q = ht , v, PHi,
past joint policy, v nodes heuristic value, PH boolean indicating whether
placeholder. consider equivalence maintained open lists. open list
L maintained GMAA*-IC contains non-expanded nodes q. contrast, open
list LIE GMAA*-ICE contains non-expanded nodes q placeholders (previously
expanded nodes), q. denote ordered subset LIE containing non-expanded nodes
Q containing placeholders Q. treat open lists ordered sets
heuristic values associated nodes.
Definition 11. L LIE equivalent, L LIE if:
1. Q L.
2. qs ordering: L.remove(L \ Q) = Q.24
3. Nodes q L Q placeholder q parent higher ranked
q:
q=ht ,vq ,falsei(L\Q)

q=ht1 ,vq ,trueiQ s.t. (t = (t1 , ) q < q).

4. placeholders.
Fig. 11 illustrates two equivalent lists past joint policies indexed letters.
Note placeholders LIE ranked higher nodes L represent.
Let us write IT-IC(L) IT-ICE(LIE ) one iteration (i.e., one loop main repeat
Algorithm 1) respective algorithms. Let IT-ICE* denote operation repeats
IT-ICE long placeholder selected (so ends q expanded).
Lemma 4. L LIE , executing IT-IC(L) IT-ICE*(LIE ) leads new open lists
equivalent: L LIE .
Proof. IT-ICE* selects placeholder q, generates child q already present
L (due properties 3 4 Definition 11) inserts it. Insertion occurs
relative location IT-IC algorithms use comparison operator
(Definition 5). Together facts guarantee insertion preserves properties 1 2.
24. A.remove(B) removes elements B without changing ordering.

501

fiOliehoek, Spaan, Amato, & Whiteson

LIE

L
Q
Vb



7
5
4.5

c

e

3
3
2.5
1
0.5

f
g
h

j

Q

Vb



5



3
3

f
g

Vb
8

4




b



placeholder {c,e,j}



nodes: position




placeholder {h,i}
consistent ordering
equal values

Figure 11: Illustration equivalent lists. Past joint policies indexed letters.
example, b expanded earlier (but yet fully expanded ICE-case).
remaining unexpanded children q, IT-ICE* reinserts q updated heuristic
value q.v q .v guaranteed upper bound value unexpanded siblings
q since q .v = Vb (q .) Vb (q .) = q .v (preserving properties 3 4).
IT-ICE* finally selects non-placeholder q, guaranteed q
selected IT-IC (due properties 1 2). Expansion ICE generates one child q (again
inserted relative location IC) inserts placeholder q = hq., q .v, truei
siblings q (again preserving properties 3 4).
Proof Theorem 5. fact GMAA*-ICE GMAA*-IC search-equivalent follows directly Lemma 4. Search equivalence means algorithms select
non-placeholders q expand. Since algorithms begin identical (and therefore trivially equivalent) open lists, maintain equivalent open lists throughout search. such,
property 2 Definition 11 ensures every time IT-ICE* selects non-placeholder, IT-IC
selects too.

References
Allen, M., & Zilberstein, S. (2007). Agent influence predictor difficulty decentralized
problem-solving. Proceedings Twenty-Second AAAI Conference Artificial
Intelligence.
Amato, C., Bernstein, D. S., & Zilberstein, S. (2006). Optimal fixed-size controllers
decentralized POMDPs. Proc. AAMAS Workshop Multi-Agent Sequential
Decision Making Uncertain Domains.
Amato, C., Bernstein, D. S., & Zilberstein, S. (2007). Optimizing memory-bounded controllers
decentralized POMDPs. Proc. Uncertainty Artificial Intelligence.
Amato, C., Bernstein, D. S., & Zilberstein, S. (2010). Optimizing fixed-size stochastic controllers POMDPs decentralized POMDPs. Autonomous Agents Multi-Agent
Systems, 21 (3), 293320.
502

fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPs

Amato, C., Bonet, B., & Zilberstein, S. (2010). Finite-state controllers based Mealy
machines centralized decentralized POMDPs. Proceedings TwentyFourth AAAI Conference Artificial Intelligence.
Amato, C., Carlin, A., & Zilberstein, S. (2007). Bounded dynamic programming decentralized POMDPs. Proc. AAMAS Workshop Multi-Agent Sequential
Decision Making Uncertain Domains.
Amato, C., Dibangoye, J. S., & Zilberstein, S. (2009). Incremental policy generation
finite-horizon DEC-POMDPs. Proc. International Conference Automated
Planning Scheduling.
Aras, R., & Dutech, A. (2010). investigation mathematical programming finite
horizon decentralized POMDPs. Journal Artificial Intelligence Research, 37 , 329
396.
Becker, R., Carlin, A., Lesser, V., & Zilberstein, S. (2009). Analyzing myopic approaches
multi-agent communication. Computational Intelligence, 25 (1), 3150.
Becker, R., Zilberstein, S., & Lesser, V. (2004). Decentralized Markov decision processes
event-driven interactions. Proc. International Conference Autonomous
Agents Multi Agent Systems.
Becker, R., Zilberstein, S., Lesser, V., & Goldman, C. V. (2003). Transition-independent
decentralized Markov decision processes. Proc. International Conference
Autonomous Agents Multi Agent Systems.
Bernstein, D. S., Amato, C., Hansen, E. A., & Zilberstein, S. (2009). Policy iteration
decentralized control Markov decision processes. Journal Artificial Intelligence
Research, 34 , 89132.
Bernstein, D. S., Givan, R., Immerman, N., & Zilberstein, S. (2002). complexity
decentralized control Markov decision processes. Mathematics Operations Research,
27 (4), 819840.
Bertsekas, D. P. (2005). Dynamic Programming Optimal Control (3rd ed., Vol. I). Athena
Scientific.
Beynier, A., & Mouaddib, A.-I. (2011). Solving efficiently decentralized MDPs temporal
resource constraints. Autonomous Agents Multi-Agent Systems, 23 (3), 486
539.
Boularias, A., & Chaib-draa, B. (2008). Exact dynamic programming decentralized
POMDPs lossless policy compression. Proc. International Conference
Automated Planning Scheduling.
Busoniu, L., Babuska, R., & De Schutter, B. (2008). comprehensive survey multi-agent
reinforcement learning. IEEE Transactions Systems, Man, Cybernetics, Part C:
Applications Reviews, 38 (2), 156172.
Carlin, A., & Zilberstein, S. (2008). Value-based observation compression DEC-POMDPs.
Proc. International Conference Autonomous Agents Multi Agent Systems.
503

fiOliehoek, Spaan, Amato, & Whiteson

Carrillo, H., & Lipman, D. (1988). multiple sequence alignment problem biology.
SIAM Journal Applied Mathematics, 48 (5), 10731082.
Cassandra, A., Littman, M. L., & Zhang, N. L. (1997). Incremental pruning: simple, fast,
exact method partially observable Markov decision processes. Proc. Uncertainty
Artificial Intelligence.
Cassandra, A. R. (1998). Exact Approximate Algorithms Partially Observable Markov
Decision Processes. Unpublished doctoral dissertation, Brown University.
Dechter, R., Flerova, N., & Marinescu, R. (2012). Search algorithms best solutions
graphical models. Proceedings Twenty-Sixth AAAI Conference Artificial
Intelligence.
Dekel, E., Fudenberg, D., & Morris, S. (2006). Topologies types. Theoretical Economics,
1 (3), 275309.
Dibangoye, J. S., Amato, C., Doniec, A., & Charpillet, F. (2013). Producing efficient errorbounded solutions transition independent decentralized MDPs. Proc. International Conference Autonomous Agents Multi Agent Systems. (Submitted
publication)
Dibangoye, J. S., Mouaddib, A.-I., & Chai-draa, B. (2009). Point-based incremental pruning heuristic solving finite-horizon DEC-POMDPs. Proc. International
Conference Autonomous Agents Multi Agent Systems.
Doshi, P., & Gmytrasiewicz, P. (2009). Monte Carlo sampling methods approximating
interactive POMDPs. Journal Artificial Intelligence Research, 34 , 297337.
Doshi, P., Zeng, Y., & Chen, Q. (2008). Graphical models interactive POMDPs: representations solutions. Autonomous Agents Multi-Agent Systems, 18 (3), 376416.
Edelkamp, S., & Schrodl, S. (2012). Heuristic search: theory applications. Morgan
Kaufmann.
Eker, B., & Akn, H. L. (2010). Using evolution strategies solve DEC-POMDP problems.
Soft ComputingA Fusion Foundations, Methodologies Applications, 14 (1), 35
47.
Eker, B., & Akn, H. L. (2013). Solving decentralized POMDP problems using genetic
algorithms. Autonomous Agents Multi-Agent Systems, 27 (1), 161196.
Emery-Montemerlo, R. (2005). Game-Theoretic Control Robot Teams. Unpublished
doctoral dissertation, Carnegie Mellon University.
Emery-Montemerlo, R., Gordon, G., Schneider, J., & Thrun, S. (2004). Approximate solutions partially observable stochastic games common payoffs. Proc.
International Conference Autonomous Agents Multi Agent Systems.
Emery-Montemerlo, R., Gordon, G., Schneider, J., & Thrun, S. (2005). Game theoretic
control robot teams. Proc. IEEE International Conference Robotics
Automation.
Givan, R., Dean, T., & Greig, M. (2003). Equivalence notions model minimization
Markov decision processes. Artificial Intelligence, 14 (12), 163223.
504

fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPs

Gmytrasiewicz, P. J., & Doshi, P. (2005). framework sequential planning multi-agent
settings. Journal Artificial Intelligence Research, 24 , 4979.
Goldman, C. V., Allen, M., & Zilberstein, S. (2007). Learning communicate decentralized environment. Autonomous Agents Multi-Agent Systems, 15 (1), 4790.
Goldman, C. V., & Zilberstein, S. (2003). Optimizing information exchange cooperative
multi-agent systems. Proc. International Conference Autonomous Agents
Multi Agent Systems.
Goldman, C. V., & Zilberstein, S. (2004). Decentralized control cooperative systems:
Categorization complexity analysis. Journal Artificial Intelligence Research, 22 ,
143174.
Goldman, C. V., & Zilberstein, S. (2008). Communication-based decomposition mechanisms
decentralized MDPs. Journal Artificial Intelligence Research, 32 , 169202.
Hansen, E. A., Bernstein, D. S., & Zilberstein, S. (2004). Dynamic programming partially observable stochastic games. Proc. National Conference Artificial
Intelligence.
Hauskrecht, M. (2000). Value-function approximations partially observable Markov decision processes. Journal Artificial Intelligence Research, 13 , 3394.
Hsu, K., & Marcus, S. (1982). Decentralized control finite state Markov processes. IEEE
Transactions Automatic Control , 27 (2), 426431.
Huhns, M. N. (Ed.). (1987). Distributed Artificial Intelligence. Pitman Publishing Ltd.
Ikeda, T., & Imai, H. (1999). Enhanced A* algorithms multiple alignments: optimal
alignments several sequences k-opt approximate alignments large cases. Theoretical Computer Science, 210 (2), 341374.
Kaelbling, L. P., Littman, M. L., & Cassandra, A. R. (1998). Planning acting partially
observable stochastic domains. Artificial Intelligence, 101 (1-2), 99134.
Kumar, A., & Zilberstein, S. (2009). Constraint-based dynamic programming decentralized
POMDPs structured interactions. Proc. International Conference
Autonomous Agents Multi Agent Systems.
Kumar, A., & Zilberstein, S. (2010a). Anytime planning decentralized POMDPs using
expectation maximization. Proc. Uncertainty Artificial Intelligence.
Kumar, A., & Zilberstein, S. (2010b). Point-based backup decentralized POMDPs: Complexity new algorithms. Proc. International Conference Autonomous
Agents Multi Agent Systems.
Littman, M., Cassandra, A., & Kaelbling, L. (1995). Learning policies partially observable environments: Scaling up. Proc. International Conference Machine
Learning.
Marecki, J., Gupta, T., Varakantham, P., Tambe, M., & Yokoo, M. (2008). agents
equal: scaling distributed POMDPs agent networks. Proc. International
Conference Autonomous Agents Multi Agent Systems.
505

fiOliehoek, Spaan, Amato, & Whiteson

Marecki, J., & Tambe, M. (2007). opportunistic techniques solving decentralized
Markov decision processes temporal constraints. Proc. International
Conference Autonomous Agents Multi Agent Systems.
Melo, F. S., & Veloso, M. (2011). Decentralized MDPs sparse interactions. Artificial
Intelligence, 175 (11), 17571789.
Mostafa, H., & Lesser, V. (2011). compact mathematical formulation problems
structured agent interactions. Proc. AAMAS Workshop Multi-Agent Sequential Decision Making Uncertain Domains.
Nair, R., Roth, M., & Yohoo, M. (2004). Communication improving policy computation
distributed POMDPs. Proc. International Conference Autonomous Agents
Multi Agent Systems.
Nair, R., Tambe, M., Yokoo, M., Pynadath, D. V., & Marsella, S. (2003). Taming decentralized POMDPs: Towards efficient policy computation multiagent settings. Proc.
International Joint Conference Artificial Intelligence.
Nair, R., Varakantham, P., Tambe, M., & Yokoo, M. (2005). Networked distributed POMDPs:
synthesis distributed constraint optimization POMDPs. Proc. National Conference Artificial Intelligence.
Oliehoek, F. A. (2010). Value-Based Planning Teams Agents Stochastic Partially Observable Environments. Amsterdam University Press. (Doctoral dissertation, University
Amsterdam)
Oliehoek, F. A. (2012). Decentralized POMDPs. M. Wiering & M. van Otterlo (Eds.),
Reinforcement learning: State art (Vol. 12). Springer Berlin Heidelberg.
Oliehoek, F. A., Kooi, J. F., & Vlassis, N. (2008). cross-entropy method policy search
decentralized POMDPs. Informatica, 32 , 341357.
Oliehoek, F. A., & Spaan, M. T. J. (2012). Tree-based solution methods multiagent
POMDPs delayed communication. Proceedings Twenty-Sixth AAAI Conference Artificial Intelligence.
Oliehoek, F. A., Spaan, M. T. J., Dibangoye, J., & Amato, C. (2010). Heuristic search identical payoff Bayesian games. Proc. International Conference Autonomous
Agents Multi Agent Systems.
Oliehoek, F. A., Spaan, M. T. J., & Vlassis, N. (2007). Dec-POMDPs delayed communication. Proc. AAMAS Workshop Multi-Agent Sequential Decision Making
Uncertain Domains.
Oliehoek, F. A., Spaan, M. T. J., & Vlassis, N. (2008). Optimal approximate Q-value
functions decentralized POMDPs. Journal Artificial Intelligence Research, 32 ,
289353.
Oliehoek, F. A., Spaan, M. T. J., Whiteson, S., & Vlassis, N. (2008). Exploiting locality
interaction factored Dec-POMDPs. Proc. International Conference
Autonomous Agents Multi Agent Systems.
Oliehoek, F. A., & Vlassis, N. (2007). Q-value functions decentralized POMDPs. Proc.
International Conference Autonomous Agents Multi Agent Systems.
506

fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPs

Oliehoek, F. A., Whiteson, S., & Spaan, M. T. J. (2009). Lossless clustering histories
decentralized POMDPs. Proc. International Conference Autonomous
Agents Multi Agent Systems.
Oliehoek, F. A., Whiteson, S., & Spaan, M. T. J. (2013). Approximate solutions factored Dec-POMDPs many agents. Proc. International Conference
Autonomous Agents Multi Agent Systems. (Submitted publication)
Oliehoek, F. A., Witwicki, S., & Kaelbling, L. P. (2012). Influence-based abstraction
multiagent systems. Proceedings Twenty-Sixth AAAI Conference Artificial
Intelligence.
Ooi, J. M., & Wornell, G. W. (1996). Decentralized control multiple access broadcast
channel: Performance bounds. Proc. 35th conference decision control.
Osborne, M. J., & Rubinstein, A. (1994). course game theory. MIT Press.
Pajarinen, J., & Peltonen, J. (2011). Efficient planning factored infinite-horizon DECPOMDPs. Proc. International Joint Conference Artificial Intelligence.
Panait, L., & Luke, S. (2005). Cooperative multi-agent learning: state art.
Autonomous Agents Multi-Agent Systems, 11 (3), 387434.
Puterman, M. L. (1994). Markov Decision ProcessesDiscrete Stochastic Dynamic Programming. John Wiley & Sons, Inc.
Pynadath, D. V., & Marsella, S. C. (2007). Minimal mental models. Proceedings
Twenty-Second AAAI Conference Artificial Intelligence.
Pynadath, D. V., & Tambe, M. (2002). communicative multiagent team decision problem:
Analyzing teamwork theories models. Journal Artificial Intelligence Research,
16 , 389423.
Rabinovich, Z., Goldman, C. V., & Rosenschein, J. S. (2003). complexity multiagent
systems: price silence. Proc. International Conference Autonomous
Agents Multi Agent Systems.
Rabinovich, Z., & Rosenschein, J. S. (2005). Multiagent coordination extended Markov
tracking. Proc. International Conference Autonomous Agents Multi
Agent Systems.
Ross, S., Pineau, J., Paquet, S., & Chaib-draa, B. (2008). Online planning algorithms
POMDPs. Journal Artificial Intelligence Research, 32 , 664704.
Roth, M., Simmons, R., & Veloso, M. (2005). Reasoning joint beliefs executiontime communication decisions. Proc. International Conference Autonomous
Agents Multi Agent Systems.
Roth, M., Simmons, R., & Veloso, M. (2007). Exploiting factored representations decentralized execution multi-agent teams. Proc. International Conference
Autonomous Agents Multi Agent Systems.
Seuken, S., & Zilberstein, S. (2007a). Improved memory-bounded dynamic programming
decentralized POMDPs. Proc. Uncertainty Artificial Intelligence.
507

fiOliehoek, Spaan, Amato, & Whiteson

Seuken, S., & Zilberstein, S. (2007b). Memory-bounded dynamic programming DECPOMDPs. Proc. International Joint Conference Artificial Intelligence.
Seuken, S., & Zilberstein, S. (2008). Formal models algorithms decentralized decision
making uncertainty. Autonomous Agents Multi-Agent Systems, 17 (2), 190
250.
Spaan, M. T. J., Gordon, G. J., & Vlassis, N. (2006). Decentralized planning uncertainty teams communicating agents. Proc. International Conference
Autonomous Agents Multi Agent Systems.
Spaan, M. T. J., & Melo, F. S. (2008). Interaction-driven Markov games decentralized
multiagent planning uncertainty. Proc. International Conference
Autonomous Agents Multi Agent Systems.
Spaan, M. T. J., & Oliehoek, F. A. (2008). MultiAgent Decision Process toolbox:
software decision-theoretic planning multiagent systems. Proc. AAMAS
Workshop Multi-Agent Sequential Decision Making Uncertain Domains.
Spaan, M. T. J., Oliehoek, F. A., & Amato, C. (2011). Scaling optimal heuristic search
Dec-POMDPs via incremental expansion. Proc. International Joint Conference
Artificial Intelligence.
Spaan, M. T. J., Oliehoek, F. A., & Vlassis, N. (2008). Multiagent planning uncertainty
stochastic communication delays. Proc. International Conference
Automated Planning Scheduling.
Sycara, K. P. (1998). Multiagent systems. AI Magazine, 19 (2), 7992.
Szer, D., Charpillet, F., & Zilberstein, S. (2005). MAA*: heuristic search algorithm
solving decentralized POMDPs. Proc. Uncertainty Artificial Intelligence.
Tsitsiklis, J., & Athans, M. (1985). complexity decentralized decision making
detection problems. IEEE Transactions Automatic Control , 30 (5), 440446.
Varaiya, P., & Walrand, J. (1978). delayed sharing patterns. IEEE Transactions
Automatic Control , 23 (3), 443445.
Varakantham, P., Kwak, J. young, Taylor, M. E., Marecki, J., Scerri, P., & Tambe, M. (2009).
Exploiting coordination locales distributed POMDPs via social model shaping.
Proc. International Conference Automated Planning Scheduling.
Varakantham, P., Marecki, J., Yabu, Y., Tambe, M., & Yokoo, M. (2007). Letting loose
SPIDER network POMDPs: Generating quality guaranteed policies. Proc.
International Conference Autonomous Agents Multi Agent Systems.
Varakantham, P., Nair, R., Tambe, M., & Yokoo, M. (2006). Winning back cup distributed POMDPs: planning continuous belief spaces. Proc. International
Conference Autonomous Agents Multi Agent Systems.
Velagapudi, P., Varakantham, P., Scerri, P., & Sycara, K. (2011). Distributed model shaping
scaling decentralized POMDPs hundreds agents. Proc. International Conference Autonomous Agents Multi Agent Systems.
Vlassis, N. (2007). Concise Introduction Multiagent Systems Distributed Artificial
Intelligence. Morgan & Claypool Publishers.
508

fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPs

Williamson, S. A., Gerding, E. H., & Jennings, N. R. (2009). Reward shaping valuing communications multi-agent coordination. Proc. International Conference
Autonomous Agents Multi Agent Systems.
Witwicki, S. J. (2011). Abstracting Influences Efficient Multiagent Coordination
Uncertainty. Unpublished doctoral dissertation, University Michigan, Ann Arbor,
Michigan, USA.
Witwicki, S. J., & Durfee, E. H. (2010). Influence-based policy abstraction weakly-coupled
Dec-POMDPs. Proc. International Conference Automated Planning
Scheduling.
Wu, F., Zilberstein, S., & Chen, X. (2010a). Point-based policy generation decentralized
POMDPs. Proc. International Conference Autonomous Agents Multi
Agent Systems.
Wu, F., Zilberstein, S., & Chen, X. (2010b). Rollout sampling policy iteration decentralized
POMDPs. Proc. Uncertainty Artificial Intelligence.
Wu, F., Zilberstein, S., & Chen, X. (2011). Online planning multi-agent systems
bounded communication. Artificial Intelligence, 175 (2), 487511.
Xuan, P., Lesser, V., & Zilberstein, S. (2001). Communication decisions multi-agent cooperation: Model experiments. Proc. International Conference Autonomous
Agents.
Yoshizumi, T., Miura, T., & Ishida, T. (2000). A* partial expansion large branching
factor problems. Proc. National Conference Artificial Intelligence.
Zeng, Y., & Doshi, P. (2012). Exploiting model equivalences solving interactive dynamic
influence diagrams. Journal Artificial Intelligence Research, 43 , 211255.
Zeng, Y., Doshi, P., Pan, Y., Mao, H., Chandrasekaran, M., & Luo, J. (2011). Utilizing
partial policies identifying equivalence behavioral models. Proceedings
Twenty-Fifth AAAI Conference Artificial Intelligence.

509

fiJournal Artificial Intelligence Research 46 (2013) 263302

Submitted 07/12; published 03/13

Parameterized Complexity Results
Exact Bayesian Network Structure Learning
Sebastian Ordyniak

ordyniak@fi.muni.cz

Masaryk University Brno, Czech Republic

Stefan Szeider

stefan@szeider.net

Vienna University Technology, Austria

Abstract
Bayesian network structure learning notoriously difficult problem discovering Bayesian network optimally represents given set training data.
paper study computational worst-case complexity exact Bayesian network structure learning graph theoretic restrictions (directed) super-structure.
super-structure undirected graph contains subgraphs skeletons solution
networks. introduce directed super-structure natural generalization undirected counterpart. results apply several variants score-based Bayesian network
structure learning score network decomposes local scores nodes.
Results: show exact Bayesian network structure learning carried
non-uniform polynomial time super-structure bounded treewidth, linear
time addition super-structure bounded maximum degree. Furthermore,
show directed super-structure acyclic, exact Bayesian network structure learning carried quadratic time. complement positive results
number hardness results. show restrictions (treewidth degree)
essential cannot dropped without loosing uniform polynomial time tractability
(subject complexity-theoretic assumption). Similarly, exact Bayesian network structure learning remains NP-hard almost acyclic directed super-structures. Furthermore,
show restrictions remain essential search globally optimal
network aim improve given network means k arc additions, arc
deletions, arc reversals (k-neighborhood local search).

1. Introduction
Bayesian Network Structure Learning (BNSL) important task discovering Bayesian
network represents given set training data. Unfortunately, solving problem
optimally (Exact BNSL) NP-complete (Chickering, 1996). common widely used
approach overcome complexity barrier exploit structure problem.
also popular direction BNSL two main kinds structural restrictions studied far, i.e., (1) restrictions probability distribution
generating input (2) restrictions resulting Bayesian network. help
restrictions several tractable classes BNSL identified. BNSL solvable non-uniform polynomial time distribution generating input bounded
treewidth (Narasimhan & Bilmes, 2004) bounded degree (Pieter, Daphne, & Andrew,
2006) solvable (non-uniform) polynomial time resulting BN branching (Chow & Liu, 1968) close branching (Gaspers, Koivisto, Liedloff, Ordyniak,
c
2013
AI Access Foundation. rights reserved.

fiOrdyniak & Szeider

& Szeider, 2012). positive results contrasted series negative results
mentioned restrictions, e.g., BNSL known NP-hard resulting BN
polytree (Dasgupta, 1999) directed path (Meek, 2001). Recently, novel approach
restrict structure BNSL introduced (Tsamardinos, Brown, & Aliferis,
2006; Perrier, Imoto, & Miyano, 2008). so-called super-structure, undirected
graph nodes resulting BN, used restrict search space BNSL
advance. super-structure obtained, usually using IT-based approach (Tsamardinos et al., 2006), one looks solution networks whose skeletons
contained super-structure. hence becomes important super-structure
sound, i.e., contains least one optimal solution.
two main questions concerning super-structure: First, suitable sound super-structure obtained efficiently? secondly, superstructure obtained, used guide search optimal solution?
goal paper provide theoretical analysis latter question, considering
super-structures arise given local score function adapting model Parviainen Koivisto (2010). consider various combinations restrictions systematic
way allows us draw broader picture complexity landscape BNSL. hope
analysis help understand boundaries tractable intractable
cases important problem. Furthermore, think results provide new insights help search efficient accurate heuristics. analysis,
use theoretical framework parameterized complexity (Downey & Fellows, 1999)
seems well suited investigating complexity BNSL allows take
structural properties (in terms parameters) account. best knowledge
parameterized complexity theory employed context before.
1.1 Results
section give brief overview results.
1.1.1 Exact BNSL Using Super-structure
first part paper study worst-case complexity Exact BNSL
graph-theoretic restrictions super-structure. One prominent restrictions
super-structure consider treewidth. Treewidth important widely
used parameter measures similarity graph tree (Bodlaender, 1993, 1997,
2005; Greco & Scarcello, 2010). Similarly trees, many otherwise intractable problems
become tractable graphs bounded treewidth. importantly, treewidth already
successfully applied context Bayesian Reasoning (Darwiche, 2001; Dechter,
1999; Kwisthout, Bodlaender, & van der Gaag, 2010). hence seems natural apply
treewidth (Exact) BNSL.
results follows:
(1) Exact BNSL solvable non-uniform polynomial time treewidth superstructure bounded arbitrary constant.
(2) Exact BNSL solvable linear time treewidth maximum degree
super-structure bounded arbitrary constants.
264

fiParameterized Complexity Exact BNSL

non-uniform mean order polynomial depends treewidth.
obtain results (1) (2) means dynamic programming algorithm along
decomposition tree super-structure.
show thatin certain senseboth results optimal:
(3) Exact BNSL instances super-structures maximum degree 4 (but unbounded
treewidth) solvable polynomial time unless P = NP. Thus, (1) (2)
cannot drop bound treewidth.
(4) Exact BNSL instances super-structures bounded treewidth (but unbounded
maximum degree) solvable uniform polynomial time unless FPT = W[1].
Thus, (2) cannot drop bound degree.
FPT 6= W[1] widely accepted complexity theoretic assumption (Downey & Fellows,
1999) often considered parameterized analog P 6= NP. provide
necessary background parameterized complexity fpt-reductions Section 2.2.
1.1.2 Local Search BNSL Using Super-structure
Since learning optimal Bayesian network computationally hard heuristic methods
used practice. popular heuristic BNSL so-called hill climbing procedure,
local search. particular, highly competitive algorithm learning large Bayesian networks (MMHC) uses local search find optimal solution inside previously constructed
super-structure (Tsamardinos et al., 2006). study worst case complexity wellknown generalization local search, k-Neighborhood Local Search (or k-Local Search
short). variant local search one allowed modify one k arcs
every step search. Hence adjusting k, one able balance speed accuracy. However, k-local search space order nO(k) , applying k-local search
especially desirable problems running time increase modestly
respect k. Similarly result (2) able show following:
(5) k-Local Search BNSL solvable linear time treewidth maximum
degree super-structure bounded arbitrary constants.
Clearly, result minor interest already solve Exact BNSL
problem restrictions super-structure (see result (2)). However,
contrast Exact BNSL problem one might able drop one restrictions
without losing uniform polynomial-time tractability. show unlikely.
(6) k-Local Search BNSL instances super-structures either bounded
treewidth bounded maximum degree solvable uniform polynomial time
unless FPT = W[1].

1.1.3 Exact BNSL Using Directed Super-structure
far, one considered super-structure undirected graph. introduce
directed super-structure expressive way restrict search space solutions,
i.e., directed super-structure fixed restrict search solutions whose
265

fiOrdyniak & Szeider

Problem: Exact BNSL

Complexity

Result

Restrictions Super-Structure
bounded treewidth
bounded treewidth

bounded max degree


linear time
XP, W[1]-hard



bounded max degree

NP-hard

Corollary 1
Corollary 1,
Theorem 3
Theorem 2

poly-time
NP-hard

Corollary 2
Theorem 7

Complexity

Result

linear time
XP, W[1]-hard
XP, W[1]-hard

Proposition 6
Theorem 5
Theorem 6

FPT
FPT

Theorem 4
Theorem 4

Restrictions Directed Super-Structure
acyclic
almost acyclic


bounded max degree

Problem: k-Local Search BNSL
Restrictions Super-Structure
bounded treewidth
bounded treewidth


bounded max degree

bounded max degree

Restrictions Operations
arc deletion
arc addition

Figure 1: Overview complexity results. complexity classes FPT XP contain
problems solvable uniform polynomial-time non-uniform polynomial-time,
respectively.

266

fiParameterized Complexity Exact BNSL

networks contained directed super-structure. Again, study complexity
Exact BNSL respect certain restrictions directed super-structure obtain
following result.
(7) Exact BNSL solvable quadratic time directed super-structure acyclic.
question arises whether extend result using general form
acyclicity. show, however, case.
(8) Exact BNSL NP-hard directed super-structures made acyclic
deleting one node. Hardness even holds maximum in-degree maximum
out-degree directed super-structure bounded 3.
systematic overview results (1)(8) given Figure 1.
1.2 Related Work
section present relevant related work BNSL. related work
found respective sections, i.e., present related work parameterized complexity
Section 2.2, related work treewidth tree decompositions Sections 2.3 3.1,
Section 7 present related work k-neighborhood local search.
1.2.1 Algorithms Exact BNSL
date handful exact algorithms BNSL proposed.
split three groups: (A) exact algorithms employ restrictions (Parviainen
& Koivisto, 2010; Koivisto, 2006; Yuan, Malone, & Wu, 2011; Ott, Imoto, & Miyano, 2004;
Silander & Myllymaki, 2006; Yuan et al., 2011), (B) exact algorithms using restrictions
generating target distribution (Pieter et al., 2006; Chechetka & Guestrin, 2007;
Friedman, Nachman, & Peer, 1999; Chow & Liu, 1968; Gaspers, Koivisto et al., 2012),
(C) exact algorithms use restrictions undirected super-structure (Friedman
et al., 1999; Kojima, Perrier, Imoto, & Miyano, 2010). Algorithms falling group (A)
suited small medium sized Bayesian networks restrict
search space way. hand, restrictions used algorithms
group (B) general restrictions coming undirected super-structure
non-uniform polynomial-time algorithms could obtained using
restrictions. best knowledge paper first employ in-depth
theoretical analysis parameterized complexity BNSL using restrictions
undirected super-structure. obtain first algorithm exact BNSL uniform
polynomial running-time respect structural restriction undirected superstructure. similar approach taken Kojima et al. (2010), authors
propose algorithm exact BNSL uses cluster-decomposition undirected
super-structure. Even though practical results quite promising authors provide
theoretical analysis worst-case complexity algorithm beyond trivial
bound also applies exact algorithms group (A). Apart exact algorithms
also exists variety approximation algorithms using tree decomposition degree-based
techniques (Pieter et al., 2006; Elidan & Gould, 2008; Karger & Srebro, 2001).
267

fiOrdyniak & Szeider

1.2.2 Hardness Results Exact BNSL
also number hardness results BNSL restrictions resulting
Bayesian network. particular, BNSL remains NP-hard in-degree resulting
Bayesian network bounded 2 (Chickering, 1996), resulting Bayesian network
poly tree (Dasgupta, 1999), directed path (Meek, 2001). However, best
knowledge, negative results BNSL restrictions (directed) super-structure
obtained.
1.3 Organization Prior Work
paper organized follows: Section 2 introduce basic concepts notions
use throughout paper. introduce main object study (BNSL)
Section 3. Section 4 shows use dynamic programming algorithm tree
decomposition order show results (1) (2). provide refined complexity analysis
algorithm Section 5. Section 6 show complexity boundaries Exact
BNSL using super-structure, i.e., obtain results (3) (4). introduce k-Local
Search BNSL Section 7 establish results (5) (6). introduce directed
super-structure Section 8 show results (7) (8). conclude Section 9.
appendix contains proofs technical claims.
preliminary shortened version paper appeared proceedings UAI
2010. Apart providing higher level detail readability giving examples
detailed proofs, paper extends previous version four ways: adding
section related work (Section 1.2), providing refined complexity analysis
main algorithmic result (Section 5), providing novel proof Theorem 6 allows
us decrease upper bound maximum degree super-structure 5 3,
introducing directed super-structure (Section 8).

2. Preliminaries
section introduce basic concepts notions use throughout
paper.
2.1 Basic Graph Theory
assume reader familiar basic graph theory (see, e.g., Diestel,
2000; Bang-Jensen & Gutin, 2009). consider undirected graphs directed graphs
(digraphs). dag directed acyclic graph. write V (G) = V E(G) = E
sets nodes edges (directed undirected) graph G = (V, E), respectively.
denote undirected edge nodes u v {u, v} directed edge (or arc),
directed u v (u, v). write NG (v) set neighbors node v V G,
i.e., NG (v) = { u : (v, u) E (u, v) E } G directed NG (v) = { u : {u, v} E }
G undirected. subset V 0 V write G[V 0 ] denote induced subgraph G0 =
(V 0 , E 0 ) E 0 = { e V 0 : e E } G undirected E 0 = { e V 0 V 0 : e E }
G directed. G digraph define PG (v) = { u V (G) : (u, v) E(G) } set
parents v G. Furthermore, two directed graphs D1 D2 define D1 D2
268

fiParameterized Complexity Exact BNSL

union D1 D2 , i.e., V (D1 D2 ) = V (D1 ) V (D2 ) E(D1 D2 ) = E(D1 ) E(D2 ).
Let G (directed undirected) graph e E(G) directed undirected edge G.
denote G e (undirected directed) graph, G e = (V (G), E(G) \ {e}).
Furthermore, subset X V (G) denote G X graph induced nodes
V \ X. X contains one node v also write G v instead G {v}. call
undirected graph G0 = (V 0 , E 0 ) skeleton directed graph G V 0 = V (G)
E 0 = { {u, v} : (u, v) E(G) }.
2.2 Parameterized Complexity
Parameterized complexity provides theoretical framework distinguish uniform
non-uniform polynomial-time tractability respect parameter.
introduced pioneered Downey Fellows (1999) receiving growing interest
reflected recent publication two monographs (Flum & Grohe, 2006;
Niedermeier, 2006) hundreds research papers (see references mentioned
monographs). 2008 Computer Journal devoted two special issues parameterized complexity order make key methods ideas known wide range
computer scientists (Downey, Fellows, & Langston, 2008).
instance parameterized problem pair (I, k) main part k
parameter ; latter usually non-negative integer. parameterized problem
fixed-parameter tractable exist computable function f constant c
instances (I, k) size n solved time O(f (k)nc ). FPT class
fixed-parameter tractable decision problems. Fixed-parameter tractable problems also
called uniform polynomial-time tractable k considered constant, instances
parameter k solved polynomial time order polynomial
independent k (in contrast non-uniform polynomial-time running times nk ).
Parameterized complexity offers completeness theory similar theory NPcompleteness. One uses fpt-reductions many-one reductions parameter
one problem maps parameter other. specifically, problem L
reduces problem L0 mapping R instances L instances L0
(i) (I, k) yes-instance L (I 0 , k 0 ) = R(I, k) yes-instance
L0 , (ii) k 0 g(k) computable function g, (iii) R computed time
O(f (k)nc ) f computable function, c constant, n denotes size
(I, k). parameterized complexity class W[1] considered parameterized analog
NP. particular, FPT = W[1] implies (unlikely) existence 2o(n) algorithm
n-variable 3SAT (Impagliazzo, Paturi, & Zane, 2001; Flum & Grohe, 2006). example,
parameterized problem W[1]-complete fpt-reductions parameterized
Maximum Clique problem (given graph G parameter k 0, G contain
complete subgraph k nodes?). Note exists trivial non-uniform polynomialtime nk algorithm Maximum Clique problems checks sets k nodes.
2.3 Tree Decompositions
Treewidth important graph parameter indicates certain sense treelikeness undirected graph (Bodlaender, 1993, 1997, 2005). graphs treewidth
bounded constant many otherwise intractable problems become tractable. Bucket
269

fiOrdyniak & Szeider

Elimination (Dechter, 1999) Recursive Conditioning (Darwiche, 2001) two important algorithmic concepts apply instances bounded treewidth.
treewidth graph G = (V, E) defined via following notion decomposition:
tree decomposition G pair (T, ) tree labeling function
(t) V every tree node following conditions hold:
1. Every node G occurs (t) tree node t.
2. every edge {u, v} G tree node u, v (t).
3. every node v G, let Tv subgraph induced nodes
v (t). Tv (connected) subtree (Connectedness Condition).
width tree decomposition (T, ) size largest set (t) minus 1 among
nodes . tree decomposition smallest width optimal. treewidth
graph G, denoted tw(G), width optimal tree decomposition G. known
(T, ) tree decomposition graph G, every clique G contained
(t) tree node V (T ) (Kloks, 1994, Lemma 2.2.2).
following proposition useful retrieve upper bound treewidth
graph.
Proposition 1. Let G undirected graph X V (G). graph G X contains
edge, i.e., nodes G X isolated, tw(G) |X|.
Proof. Let V (G) \ X contain nodes v1 , . . . , vn let tree node set
{t, t1 , . . . , tn } edge set { {t, ti } : 1 n }. together function
(t) = X (ti ) = X {vi } every 1 n tree decomposition G
width |X|.
main property tree decompositions allows efficient bottom-up dynamic
programming algorithms wide spectrum otherwise intractable problems wellknown separation property made precise following proposition.
Proposition 2. Let G graph, (T, ) tree decomposition G, {t0 , t00 } edge
, let 0 00 subtrees obtained deleting edge {t0 , t00 }
0
0
00
00
0
00

contains tSand contains . Furthermore, define = (t ) (t ), =
tV (T 0 ) (t) B =
tV (T 00 ) (t). following two statements hold:
1. B = S;
2. separates nodes nodes B, i.e., edge node
\ node B \ S.
Proof. first statement follows immediately Property (3) tree decomposition,
since subtree containing node v (A B) make use edge {t0 , t00 } order
connected hence v contained S.
see second statement, suppose contradiction edge
node \ node b B \ S. follows first statement proposition
Ta Tb disjoint, i.e.,otherwise either b S. contradicts
property (2) tree decomposition since {a, b} edge G.
270

fiParameterized Complexity Exact BNSL

Given graph G n nodes constant w, possible decide whether G
treewidth w, so, compute optimal tree decomposition G
time O(n) (Bodlaender, 1996). Furthermore exist powerful heuristics compute tree
decompositions small width practically feasible way (Koster, Bodlaender, & van
Hoesel, 2001; Gogate & Dechter, 2004; Dow & Korf, 2007). Recently, new randomized
heuristics studied context Bayesian reasoning (Kask, Gelfand, Otten, &
Dechter, 2011; Gelfand, Kask, & Dechter, 2011).

3. Bayesian Network Structure Learning
section define theoretical framework BNSL shall use
considerations. closely follow abstract framework used Parviainen Koivisto
(2010) encloses wide range score-based approaches structure learning.
assume input data specifies set N nodes (representing random variables)
local score function f assigns v N subset P N \ {v} nonnegative real number f (v, P ). Given local score function f set N nodes,
problem find dag N score f
X
f (D) :=
f (v, PD (v))
vN

large possible (the dag together certain local probability distributions forms
Bayesian network). setting accommodates several popular scores like BDe, BIC
AIC (Parviainen & Koivisto, 2010; Chickering, 1995).
consider following decision problem:
Exact Bayesian Network Structure Learning
Input:
local score function f defined set N nodes, real
number > 0.
Question: dag N f (D) s?
following complexity results need fix score function represented
problem input. cannot list values f (v, P ) nodes v N subsets
P N \ {v}, requires (2|N | ) space. Therefore, assume f (v, P ) given
different 0; call non-zero representation. representation
also used fields, instance constraint satisfaction, allowed tuples
constraints listed input (Tsang, 1993). alternative representation score
function would list input values f (v, P ) |P | c constant
c. Let us call arity-c representation. representation requires polynomial
space every node c parents, small constant c, reasonable
assumption. Given arity-c representation score function, clearly compute
linear time corresponding non-zero representation. Hence non-zero representation
general, therefore base complexity results encoding.
tractability results also carry arity-c representation.
size f (given non-zero representation) number bits needed
represent tuples f (v, P ) > 0 reasonable data structure, e.g., list
271

fiOrdyniak & Szeider

tuples tuple stores node, set parents, value score
function. Clearly, size f exceeds total number tuples. define
Pf (v) := { P N : f (v, P ) > 0} {} set potential parent sets v.
also define
f := max |Pf (v)|;
vN

important measurement worst-case analysis running times.
particular, assumption f represented implies following:
(*) Let = (N, f, s) instance Exact Bayesian Network Structure Learning. f bounded size f .
Let f local score function defined set N nodes. directed super-structure
f directed graph Sf = (N, Ef ) Ef contains arc (u, v) u
potential parent v, i.e., u P P Pf (v). (undirected) super-structure
f , denoted Sf = (N, Ef ), skeleton directed super-structure.
v
P
f (v, P )

{d}
1
{b, c, d}
0.5
b {a, f }
1
1
c
{e}


1
e
{b}
1
f
{d}
1
g {c, d}
1
f



c

b

e





f
Sf

g

c

b



e

f

g

Sf

Figure 2: local score function f together directed super-structure Sf
undirected super-structure Sf .

Example 1. Figure 2 shows example local-score function f defined set
N = {a, b, c, d, e, f, g} nodes. function f given table containing tuples
(v, P, f (v, P )) v N P N \ {v} f (v, P ) > 0. Note since f
non-negative holds f (v, P ) = 0 remaining pairs (v, P ), i.e., pairs
contained table. figure also shows directed super-structure Sf
unique super-structure Sf f .
say dag set N nodes admissible f skeleton
subgraph super-structure Sf . Furthermore, say dag N strictly
admissible f every node v N PD (v) Pf (v). Note every strictly
admissible dag also admissible. Furthermore, always exists (strictly) admissible
dag highest score shown following lemma.
272

fiParameterized Complexity Exact BNSL

Lemma 1. Let f local score function defined set N nodes let dag
N . strictly admissible dag D0 N score D.
Proof. (strictly) admissible, i.e., v N f (v, PD (v)) = 0,
delete arcs (w, v) w PD (v). decrease score since
f (v, ) f (v, PD (v)) = 0 every v.

b
e



c



f

g

b

Da

e



c



f

g

b
e

Db

Dc



c



f

g

b
e

c



f

g

Dd

Figure 3: Various dags correspondence local-score function f Example 1.
Example 2. Figure 3 shows four examples dags nodes a, b, c, d, e, f, g. Using
local-score function f defined Example 1, make following observations:
a) Da admissible, super-structure Sf contain edge {c, b}.
score Da f (Da ) = 4.
b) Db admissible strictly admissible, node parents c
f (a, {c, d}) = 0. score Db f (Db ) = 5.
c) Dc strictly admissible dag obtained Db described proof Lemma 1.
f (Dc ) = f (Db ) = 5.
d) Dd strictly admissible also optimal dag f . score Dd f (Dd ) =
7.
3.1 Tree Decompositions
presenting algorithms graphs bounded treewidth convenient consider
tree decompositions following normal form (Kloks, 1994): triple (T, , r) nice
tree decomposition graph G (T, ) tree decomposition G, tree rooted
node r, node one following four types:
1. leaf node: node children;
2. join node: node exactly two children t1 , t2 , (t) = (t1 ) = (t2 );
3. introduce node: node exactly one child t0 , (t) = (t0 ) {v}
node v G;
4. forget node: node exactly one child t0 , (t) = (t0 ) \ {v} node
v G.
273

fiOrdyniak & Szeider

convenience also assume (r) = root r . always
achieved adding forget nodes top root (see Figure 4 example). nice
tree decomposition (T, , r) define (t) union sets (t0 ) t0
contained subtree rooted t. Furthermore, denote Ft set nodes
already forgotten node t, i.e.,
Ft = (t) \ (t).
stated Section 2.3 one main properties tree decompositions allows
efficient algorithms well-known separator property made precise Proposition 2.
following propositions provide different versions separator property tree decompositions node types nice tree decomposition. propositions
well-known (Kloks, 1994) immediate consequences separator property
tree decompositions state without proofs. propositions summarize
algorithmic properties nice tree decompositions use design
algorithm Section 4.
Proposition 3. Let join node children t1 t2 . Ft1 Ft2 =
edge node u Ft1 node v Ft2 G.
Proposition 4. Let introduce node child t0 (t) = (t0 ) {v0 }.
edge v0 node v Ft G. Furthermore, v0
/ Ft0 .
Proposition 5. Let forget node child t0 (t) = (t0 ) \ {v0 }.
edge v0 node v V (G) \ (t) G.
Given tree decomposition graph G width w, one effectively obtain
time O(|V (G)|) nice tree decomposition G O(|V (G)|) nodes width
w (Kloks, 1994).
Example 3. Figure 4 shows tree decomposition corresponding nice tree decomposition super-structure Example 1.

4. Dynamic Programming Algorithm Exact Bayesian Network
Structure Learning
section present dynamic programming algorithm establish tractability
results. remainder section w denotes arbitrary fixed constant. Recall
previous section f maximum number potential parent sets node.
Theorem 1. Given set N nodes local score function f N whose superstructure Sf = (N, Ef ) treewidth bounded arbitrary constant w. find
2(w+1)
time O(f
|N |) dag N maximal score f (D).
devise algorithm show Theorem 1 state prove direct consequence theorem.
Corollary 1. Exact Bayesian Network Structure Learning decided polynomial time instances super-structure bounded treewidth. problem
decided linear time additionally super-structure bounded maximum degree.
274

fiParameterized Complexity Exact BNSL



a, b
a, b, c
a, b, c,

a, b, c,
b, c,

e, b, c

f, b,



g, c,

b, c,
b, c,

b, c,

b, c,

b, c

b,

c,

e, b, c

f, b,

g, c,

Figure 4: tree decomposition (left) corresponding nice tree decomposition (right)
super-structure Example 1.

Proof. first statement follows immediately theorem since f bounded
total input size instance w constant. Recall Section 3
local score function f given list tuples f non-zero hence f
bounded total input size instance. second statement follows since f
bounded whenever maximum degree super-structure bounded clearly
f 2d .
following assume given set nodes N local score
function f N together nice tree decomposition (T, , r) Sf width w.
going establish Theorem 1 means dynamic programming algorithm along
nice tree decomposition Sf , computing local information nodes tree
decomposition put together form optimal dag. algorithm
closely follows general approach used algorithms graphs (or structures) bounded
treewidth (Bodlaender & Koster, 2008).
partial solution tree node V (T ) dag obtained induced
subdigraph D[ (t)] strictly admissible dag f . tree node let D(t) denote
set partial solutions t. partial solution D(t) set
ft (D) =

X

f (v, PD (v)),

vFt

275

fiOrdyniak & Szeider

i.e., ft (D) sum scores nodes Ft . Recall previous section
Ft set forgotten nodes t.
main idea underlying algorithm reduce space required store partial
solution help so-called record. becomes possible properties
tree decomposition manifested Propositions 3, 4 5.
record tree node V (T ) triple R = (a, p, s) that:
1. mapping (t) Pf (v), i.e., every v (t) a(v) Pf (v);
2. p transitive binary relation (t);
3. non-negative real number.
Informally, tree node V (T ) record R = (a, p, s), mapping fixes
parent set every node (t), p compact representation reachability relation
nodes (t) (using directed paths nodes (t)), sum
scores nodes forgotten t, i.e., nodes Ft .
say record represents partial solution D(t) satisfies following
conditions:
1. a(v) V (D) = PD (v) every v (t).
2. every pair nodes v1 , v2 (t) holds (v1 , v2 ) p contains
directed path v1 v2 .
say record R = (a, p, s) tree node V (T ) valid represents dag
D(t) maximum score ft (D) dags D(t) represented R.
say partial solution represented R maximal respect R R valid
ft (D) = s. tree node V (T ) associate set R(t) valid records
representing partial solutions D(t).
certain sense, R(t) succinct representation optimal elements D(t),
using space depends w f , |N |.



b

e

c



f

b

(t)

g

c



e

Da

f

(t)

g

Db

Figure 5: Two partial solutions node nice tree decomposition Example 3.

Example 4. Figure 5 shows two partial solutions Da Db node nice
tree decomposition Example 3. Da Db represented records R = (a, p, s)
276

fiParameterized Complexity Exact BNSL

a(b) = {a, f }, a(c) = {e}, a(d) = p = {(b, c), (d, b), (d, c)}. ft (Da ) = 3 >
ft (Db ) = 2 easy see 3 maximum score partial solutions
represented R. Hence record R = (a, p, 3) valid record Da maximal
respect R, thus R R(t).
dynamic programming algorithm computes set valid records bottom
manner, i.e., starting leave nodes nice tree decomposition algorithm
proceeds root node. next three lemmas show compute set valid
records introduce, forget join nodes nice tree decomposition
valid records children. Informally, introduce node child t0
(t) = (t0 ) {v0 } compute set R(t) valid records checking
potential parent set P P(v0 ) v0 valid record R R(t0 ) t0 whether
combination P R constitutes valid record t.
Lemma 2 (introduce node). Let introduce node child t0 . R(t)
computed R(t0 ) time O(fw+1 ).
Proof. following denote v0 node introduced t, i.e., (t) = (t0 ) {v0 }.
going establish lemma help following claim whose proof
found appendix.
Claim 1. R(t) set records R = (a, p, s) set P Pf (v0 )
record R0 = (a0 , p0 , s0 ) R(t0 ) with:
1. a(v0 ) = P .
2. every v (t0 ) holds a(v) = a0 (v).
3. = s0 .
4. p transitive closure relation p0 { (u, v0 ) : u P } { (v0 , u) : u
(t0 ) v0 a0 (u) }.
5. p irreflexive.
follows R(t) computed checking every pair (P, R0 ), P Pf (v0 )
R0 R(t0 ), whether satisfies conditions (1)(5). Since f
possible sets P O(fw ) possible valid records t0 (observe |(t0 )| w)
lemma follows fact every pair (P ,R0 ) conditions checked
time depends w.
Informally, forget node child t0 (t) = (t0 )\{v0 } compute
set R(t) valid records projecting set R(t0 ) valid records t0
(t).
Lemma 3 (forget node). Let forget node child t0 . R(t) computed
R(t0 ) time O(fw+1 ).
277

fiOrdyniak & Szeider

Proof. following denote v0 forgotten node, i.e. (t) = (t0 ) \ {v0 }.
show R(t) obtained projection R(t0 ) (t).
need additional notation. Let R0 = (a0 , p0 , s0 ) R(t0 ). define projection R0
t, denoted R0 [t] record R = (a, p, s) that:
1. restriction a0 (t).
2. p = { (v, w) p0 : v, w (t) }.
3. = s0 + f (v0 , a0 (v0 )).
Furthermore, define projection R(t0 ) t, denoted R(t0 )[t], set records
R0 [t] R0 R(t0 ). say record R = (a, p, s) R(t0 )[t] maximal
s0 s0 > (a, p, s0 ) R(t0 )[t].
Again, going establish lemma help following claim whose
proof found appendix.
Claim 2. R(t) set maximal records R(t0 )[t].
Since R(t0 ) contains O(fw+1 ) records easy see R(t) computed
time O(fw+1 ).
Informally, join node children t1 t2 compute set R(t)
valid records checking record R1 R(t1 ) record R2 R(t2 )
whether combination R1 R2 constitutes valid record t.
Lemma 4 (join node). Let t1 , t2 children . R(t) computed
2(w+1)
R(t1 ) R(t2 ) time O(f
).
Proof. going establish lemma help following claim (the rather
technical proof claim found appendix).
Claim 3. R(t) set records R = (a, p, s) records R1 =
(a1 , p1 , s1 ) R(t1 ) R2 = (a2 , p2 , s2 ) R(t2 ) with:
1. = a1 = a2 .
2. = s1 + s2 .
3. p transitive closure p1 p2 .
4. p irreflexive, i.e., v (t) (v, v) p.
follows R(t) computed considering pairs records R1 R(t1 )
R2 R(t2 ) checking conditions (1)(4). Since O(fw+1 ) valid
records every V (T ) every pair records time required check
conditions (1)(4) depend w, follows running time procedure
2(w+1)
O(f
).
ready establish Theorem 1.
278

fiParameterized Complexity Exact BNSL

Proof Theorem 1. Let N set nodes, f local score function N
super-structure Sf treewidth w (a constant) |N | = n. compute nice tree
decomposition (T, , r) Sf width w O(n) nodes. accomplished
time O(n) (see discussion Section 2.3).
Next compute sets R(t) via bottom-up traversal . leaf node
compute R(t) considering strictly admissible dags w + 1 nodes
(t). clearly done time O(fw+1 ) use Lemmas 4, 2 3
2(w+1)

compute sets R(t) O(n) tree nodes time O(f
n).
Since (r) = , partial solutions root r exactly strictly admissible
dags f , fr (D) = f (D) dag D. computation
sets R(t) tree nodes t, set R(r) contains exactly one record R = (, , s).
considerations, follows largest score strictly admissible dags
f , and, noted Section 3, also largest score dag whose nodes belong
N . easy compute dag score f (D) = via top-down traversal
starting r using information previously stored node .
2(w+1)
also accomplished time O(f
n).
close section remark concerning relationship treewidth
Bayesian network treewidth super-structure. Bayesian reasoning one
usually associates dag Bayesian network moral graph (D)
skeleton plus edges joining nodes common child D. treewidth
Bayesian network treewidth moral graph (Darwiche, 2001; Dechter, 1999).
observe every Bayesian network bounded treewidth super-structure
bounded treewidth contains Bayesian network. Hence, Bayesian network
learned considering super-structures bounded treewidth. hand
Bayesian network contained super-structure bounded treewidth Bayesian
network bounded treewidth reasonable assumption nodes
bounded number parents. Consequently, Bayesian network learned
super-structure bounded treewidth reasonable assume Bayesian network
bounded treewidth well.

5. Refined Complexity Analysis
presenting algorithm Section 4 focused broad evaluation complexity. Section provide fine-grained analysis running time.
following assume N set nodes, f local score function N ,
(T, , r) nice tree decomposition Sf width w. improve
running time algorithm using following five ideas.
I1 keeping records node tree decomposition fixed order
2(w+1)
)
improve time needed join node tree decomposition O(f
w+1
O(f ) without additional cost sorting. achieve generating
new records ordered manner keeping track order records
stored.
279

fiOrdyniak & Szeider

I2 Parent sets supersets parent sets higher score
disregarded (de Campos, Zeng, & Ji, 2009). simple preprocessing rule usually
allows us consider fewer potentially 2d potential parent sets. sequel
denote preprocessed score function f 0 . Clearly, |Pf (v)| |Pf 0 (v)|
every v N .
I3 maximum in-degree resulting BN bounded advance one use
fact preprocess score function natural way. following
denote resulting score function f 00 . Clearly, |Pf 00 (v)| |Pf 0 (v)| every
v N.
I4 Every node network might different number potential parent sets.
Consequently, instead using one upper bound number f 00 potential parent
sets realistic consider actual number |Pf 00 (v)| potential parent sets
node v N .
I5 valid records need stored algorithm, i.e., records represent
acyclic networks.
Considering ideas I1I4 worst-case complexity algorithm refined
follows:
X

O(w2
|Pf 00 (v)|)
tV (T )

w2

v(t)

Q

Observe
v(t) |Pf 00 (v)| number potential records tree node t.
idea I5 general records need stored algorithm.
refined analysis suggests running time algorithm dominated
maximum number records need stored tree node V (T ).

6. Hardness Results Exact Bayesian Network Structure Learning
following result follows reduction due Chickering (1996).
Theorem 2. Exact Bayesian Network Structure Learning NP-hard instances super-structures maximum degree 4.
Proof. Since use well-known reduction Chickering, sketch argument. reduction Feedback Arc Set (FAS). problem asks whether
digraph = (V, E) made acyclic deleting k arcs (the deleted arcs form
feedback arc set D). problem NP-hard digraphs skeletons maximum
degree 4 (Karp, 1972). Given instance (D, k) FAS, skeleton maximum degree 4, construct set V 0 = V (D)E(D) nodes local score function f
V 0 setting f ((u, v), {u}) = 1 (u, v) E(D), f (v, { (u, v) : u PD (v) }) = |PD (v)|
v V (D), f (v, P ) = 0 cases. Clearly, super-structure Sf (recall
definition Sf Section 3) isomorphic undirected graph obtained
skeleton subdividing every edge once, hence maximum degree Sf
maximum degree 4. easy see feedback arc set
size k exists dag D0 whose skeleton spanning subgraph Sf
f (D0 ) 2 |E| k.
280

fiParameterized Complexity Exact BNSL

Theorem 3. Exact Bayesian Network Structure Learning parameterized
treewidth super-structure W[1]-hard.
Proof. devise fpt-reduction following problem, well-known
W[1]-complete (Pietrzak, 2003).
Partitioned Clique
Input:
Parameter:
Question:

k-partite graph G = (V, E) partition V1 , . . . , Vk
|Vi | = |Vj | = n 1 < j k.
integer k.
nodes v1 , . . . , vk vi Vi 1 k
{vi , vj } E 1 < j k? (The graph K =
({v1 , . . . , vk }, { {vi , vj } : 1 < j k }) k-clique G.)

Let G = (V, E) instance problem partition V1 , . . . , Vk , |V1 | = = |Vk | =
n parameter k. Informally, encode given instance Partitioned Clique
instance (N, f, s) Exact Bayesian Structure Learning G
k-clique Bayesian network f (D) s. achieve
introduce node nv every node v G one node aij every 1 6= j k.
node corresponds node Vi achieves maximum score parent set
contains nodes aij 1 j k. node aij achieves maximum score parent set
corresponds edge G node Vi node Vj . Hence, edges G
encoded local score function nodes aij 1 6= j k. choosing
proper scores nodes right threshold assure Bayesian
network whose score higher every node aij attain maximum score,
k nodes correspond nodes G achieve maximum score.
follows parent sets chosen nodes aij correspond edges k-clique
G.
order make later calculations easier assume k > 2 remainder
proof. Let = k 2 k 1, = 2k = nk + 1. construct set N nodes
local score function f N satisfying following claims.
Claim 4. tw(Sf ) k(k 1)/2
Claim 5. G k-clique dag f (D) s.
shown theorem establishing two claims.
set = { aij : 1 < j k }, N = V (G) A, Ai = { alk : l =
k = } every 1 k. ready define f . set f (v, Ai ) = every
v Vi , f (aij , {u, w}) = every 1 < j k, u Vi , w Vj , {u, w} E(G).
Furthermore, set f (v, P ) = 0 remaining combinations v P . See Figure 6
illustration. Now, Claim 4 follows Proposition 1 X = A. Hence, remains
show Claim 5.
go show Claim 5 give notation explanation. Let
0
0
E0
E(G). denote 0V (E ) set nodes incident edges E , i.e.,
set eE 0 e. say E representable every 1 < j k contains
one edge node Vi node Vj . define eij (E 0 ) = {vi , vj } E 0
contains edge vi Vi vj Vj eij = otherwise. define D(E 0 )
281

fiOrdyniak & Szeider

v11

v12

v13

v11

v12

a13
v33

v21
v32

7

a12

v33

v22
v31

v13

v21
v32

v23

v22

a23
v31

v23

Figure 6: example graph G super-structure Sf according construction used proof Theorem 3.
v11

v12

v13

v11

v12

a13
v33

v21
v32

7

a12

v33

v22
v31

v13

v21
v32

v23

v22

a23
v31

v23
D(E 0 )

G

Figure 7: example graph G Figure 6 together edge set E 0 , given
bold edges illustration G, resulting dag D(E 0 ).

directed graph node set N arc set { (v, aij ) : v eij (E 0 ) 1 < j
k } { (aij , v) : v
/ eij (E 0 ) 1 < j k }. Figure 7 shows D(E 0 ) representable
edge set example graph G Figure 6.
main idea show Claim 5 f (D) form D(E 0 )
representable edge set E 0 corresponds k-clique G. formally expressed
following claim whose proof found appendix.
Claim 6. following statements equivalent:
1. G k-clique.
2. dag f (D) s.
3. representable edge set E 0 E(G) f (D(E 0 )) s.
282

fiParameterized Complexity Exact BNSL

Note contrast Theorem 2, essential Theorem 3 super-structure
unbounded degree: degree treewidth bounded problem
fixed-parameter tractable Corollary 1 unlikely W[1]-hard.

7. k-Neighborhood Local Search
Important widely used algorithms BNSL based local search methods (Heckerman, Geiger, & Chickering, 1995). Usually local search algorithm tries improve
score given dag transforming new dag adding, deleting, reversing
one arc time (in symbols: add, del, rev, respectively). main obstacle
local search methods danger getting stuck poor local optimum. possibility
decreasing danger perform k > 1 elementary changes one step, known kNeighborhood Local Search k-Local Search short. BNSL, try improve
score dag n nodes, k-local search space order nO(k) . Therefore, carried
brute-force, k-local search costly even small values k. therefore
surprising practical local search algorithms BNSL consider 1-neighborhoods
only.
study parameterized complexity k-local search initiated Fellows
(2003). date collection positive negative results parameterized complexity
k-local search various combinatorial optimization problems known. instance,
k-local search already investigated combinatorial problems graphs (Khuller,
Bhatia, & Pless, 2003; Marx, 2008; Fellows, Rosamond, Fomin, Lokshtanov, Saurabh, & Villanger, 2009; Gaspers, Kim, Ordyniak, Saurabh, & Szeider, 2012), problem finding
minimum weight assignment Boolean constraint satisfaction instance (Krokhin &
Marx, 2012), stable marriage problem ties (Marx & Schlotter, 2011),
satisfiability problem (Szeider, 2011).
section show k-local search BNSL solved linear time
super-structure bounded treewidth bounded maximum degree. result
good agreement Theorem 1. However, contrast Exact BNSL might
still possible drop one restrictions without losing uniform polynomial-time
tractability, show case. also investigate k-Local Search BNSL
different combinations allowed operations reversal, addition deletion
arc. results mostly negative. fact, somewhat surprisingly, k-Local Search
BNSL remains hard even edge reversal allowed operation.
state show results define k-Local Search BNSL formally.
Let k 0 {add, del, rev}. Consider dag = (V, E). directed graph
D0 = (V 0 , E 0 ) k-O-neighbor
1. D0 dag,
2. V = V 0 ,
3. E 0 obtained E performing k operations set O.
{add, del, rev} consider following parameterized decision problem.
283

fiOrdyniak & Szeider

k-O-Local Search Bayesian Network Structure Learning
Input:
Question:

local score function f , dag admissible f ,
integer k.
k-O-neighbor D0 higher score D?

Note problem change require D0 admissible, always
avoid addition inadmissible arc.


c

b





e

f

7

g

c

b



e

f

g

D0



Figure 8: Two dags illustrating notion k-O-neighbor Example 5.
Example 5. Figure 8 shows two dags D0 D0 obtained
either reversing deleting adding reverse bold arcs D. follows
D0 3-{rev}-neighbor 6-{add, del}-neighbor D. directed graph obtained
reversing arc (a, d) contains cycle (on nodes {a, b, f, d}) D0
not. score D0 larger score D, since f (D) = 3 f (D0 ) = 7
(using score function f depicted Figure 2).
Proposition 6. k-Local Search Bayesian Network Structure Learning
decided linear time instances super-structure bounded treewidth
bounded maximum degree.
Proof. proof uses arguments proof Theorem 1 sketch
proof proposition.
following assume given instance = (D, f, k) k-OLocal Search Bayesian Network Structure Learning together nice tree
decomposition (T, , r) Sf width w let maximum degree Sf .
assume w constants. set denote [S] set
subsets S.
main difference proof Theorem 1 interested
solutions k-O-neighbors D. take account need slightly extend
concept record tree node V (T ). include integer c reflect cost
partial solution smallest number operations needed obtain D.
technical reasons also include mapping b assigns set forgotten children
every node contained (t). allows us compute value c forget node.
record tree node V (T ) quintuple R = (a, b, p, c, s) that:
284

fiParameterized Complexity Exact BNSL

1. mapping (t) Pf (v);
2. b mapping (t) [Ft ];
3. p transitive binary relation (t);
4. c non-negative integer;
5. non-negative real number.
say record represents partial solution Dp D(t) satisfies following
conditions:
1. a(v) V (Dp ) = PDp (v) every v (t).
2. b(v) = { u Ft : (v, u) E(Dp ) } every v (t).
3. every pair nodes v1 , v2 (t) holds (v1 , v2 ) p Dp contains
directed path v1 v2 .
4. c k smallest integer Dp [Ft ] c-O-neighbor D[Ft ].
say record R = (a, b, p, c, s) tree node V (T ) valid represents
dag Dp D(t) maximum score ft (D) dags D(t) represented R.
say partial solution represented R maximal respect R R
valid ft (D) = s. tree node V (T ) associate set R(t) valid
records representing partial solutions D(t).
straightforward adapt dynamic programming algorithm Section 4
new setting. Observe k + 1 possible values c. Furthermore,
every considered partial solution Dp admissible, number possible values
b(v) every v (t) bounded 2d . follows space requirement store
record (k + 1) 2d(w+1) times space requirement needed store record
defined Section 4. Using argumentation Section 4 leads overall
running time O((fw+1 (k + 1) 2d(w+1) )2 |V (D)|) O((dw+1 (k + 1) 2d(w+1) )2 |V (D)|).
Since w constants, constitutes linear running time.
Theorem 4. = {add} = {del}, k-O-Local Search Bayesian Network
Structure Learning solvable polynomial time.
Proof. consider = {add} proof = {del} analogous. Let
= (D, f, k) given instance k-{add}-Local Search Bayesian Network
Structure Learning. Since allowed add arcs every step must leave
acyclic. follows k-{add}-neighbor D0 f (D0 ) > f (D)
node v V (D) addition k incoming arcs increases
score v resulting digraph remains acyclic. Now, every P V (D) \ {v}
one easily check whether f (v, P ) > f (v, PD (v)) whether P obtained
PD (v) via addition k incoming arcs whether resulting digraph
acyclic.
285

fiOrdyniak & Szeider

view Theorem 4 let us define set {add, del, rev} non-trivial
/ {,
{add}, {del}}.
Theorem 5. Let {add, del, rev} non-trivial. k-O-Local Search Bayesian
Network Structure Learning W[1]-hard parameter tw(Sf ) + k.
Proof. slightly modify reduction given proof Theorem 3. Let = (G, k)
given instance Partioned Clique let N , f defined correspondence
proof Theorem 3. distinguish two cases depending whether
allowed reverse arc not, i.e., depending whether rev O.

case rev claim 0 = (f, D, k 0 ) = D() k 0 = k2
instance k 0 -O-Local Search Bayesian Network Structure Learning
G contains k-clique k 0 -O-neighbor D0 f (D0 ) > f (D).
see let K k-clique G. follows Claim 6 representable
edge set E 0 f (D(E 0 )) > f (D) = 1 since E 0 representable easy
see D(E 0 ) obtained reversal k 0 arcs D. Hence
D0 = D(E 0 ) k 0 -O-neighbor f (D0 ) > f (D). see reverse let D0
k 0 -O-neighbor f (D0 ) > f (D) = 1. Hence D0 dag since f (D0 ) integer
follows f (D0 ) s. Again, using Claim 6, k-clique G.
Now, remaining case, i.e., case = {add, del} claim
00 = (f, D, k 00 ) k 00 = 2k 0 instance k 0 -O-Local Search Bayesian Network
Structure Learning G contains k-clique k 00 -Oneighbor D0 f (D0 ) > f (D). proof uses arguments case
rev need twice many operations. is, replace
reversal arc (u, v) deletion arc (u, v) followed addition
arc (v, u).

preliminary version paper (Ordyniak & Szeider, 2010) showed following theorem parametrized reduction Red/Blue Non-Blocker,
claimed W[1]-complete graphs bounded degree (Downey & Fellows, 1995).
However, recently found problem fact fixed-parameter tractable
proof published work Downey Fellows (1995) contained mistake (Fellows, 2012). therefore use reduction Independent Set require
original instance bounded degree. even allows us strengthen result
decreasing upper bound maximum degree super-structure 5 3.
Theorem 6. Let {add, del, rev} non-trivial. k-O-Local Search Bayesian
Network Structure Learning W[1]-hard parameter k. Hardness even holds
super-structure Sf maximum degree 3.
Proof. devise fpt-reduction following problem known W[1]complete (Downey & Fellows, 1999).
286

fiParameterized Complexity Exact BNSL

Independent Set
Input:
undirected graph G = (V, E) integer k.
Parameter: integer k.
Question:
G independent set size least k, i.e.,
set V |S| k {u, v}
/ E every pair
nodes u, v S.
simplify initial construction first prove theorem case maximum
degree super-structure 5. later show refine proof superstructures maximum degree 3.
Let (G = (V, E), k) instance problem k 0 = 2k + 1 rev
0
k = 2(2k + 1) otherwise. construct dag local score function f G
independent set size least k k 0 -O-neighbor D0
higher score D.
dag obtained G applying following steps (see Figure 9
illustration):
1. replace every node v V two nodes v 1 v 2 arc (v 1 , v 2 ).
2. every node v V add binary tree Tv1 exactly |NG (v)| leaves. root
Tv1 v 1 arcs Tv1 directed away v 1 . Furthermore, define lv1
bijective mapping NG (v) leaves Tv1 .
3. every node v V add binary tree Tv2 exactly |NG (v)| leaves. root
Tv2 v 2 arcs Tv2 directed towards v 2 . Furthermore, define lv2
bijective mapping NG (v) leaves Tv2 .
4. every edge {u, v} E, add arcs (lu1 (v), lv2 (u)) (lv1 (u), lu2 (v)) D.
5. add binary tree T1 root r1 exactly |V | leaves, whose edges directed
away r1 . define lT1 bijective mapping V leaves T1 .
6. add binary tree T2 root r2 exactly |V | leaves, whose edges directed
towards r2 . define lT2 bijective mapping V leaves T2 .
7. every v V (G), add arcs (v 1 , lT1 (v)) (v 1 , lT2 (v)) D.
8. add arc (r2 , r1 ) D.
completes construction D. Next define local score function f V (D).
Let = k 1, = |V (G)| = 1.
1. every n V (D) \ { v 1 : v V (G) } {r1 } set f (n, PD (n)) = .
2. every v V (G) set f (v 1 , {v 2 , lT1 (v)}) = , f (v 2 , PD (v 2 ) \ {v 1 }) = ,
f (lT1 (v), PD (lT1 (v)) \ {v 1 }) = .
3. set f (r1 , PD (r1 )) = f (r2 , PD (r2 ) {r1 }) = .
4. remaining combinations n V (D) P V (D) set f (n, P ) = 0.
287

fiOrdyniak & Szeider



b

c

7
r1

lT1 (a)

lT1 (b)
lT2 (a)

lT1 (c)
lT2 (b)

lT2 (c)

r2

a1

b1

la1 (b)

la1 (c)

la2 (b)

la2 (c)

a2

lb1 (a)

lb2 (a)

c1
lb1 (c)

lc1 (b)

lc1 (a)

lb2 (c)

lc2 (b)

lc2 (a)

b2

c2

Figure 9: Top: example graph G. Bottom: dag resulting G using
construction proof Theorem 6.

Evidently acyclic f constructed G polynomial time.
Observe super-structure Sf exactly skeleton D. Hence, construction,
nodes v 1 v V (D) degree 5 nodes Sf degree
3 showing maximum degree Sf 5. Consequently,
establish theorem help following claim whose proof found
appendix.

288

fiParameterized Complexity Exact BNSL

0








































7
0





























0

0




0

0























D0



Figure 10: dag Figure 9 together k 0 -{rev}-neighbor D0 f (D0 ) >
f (D). k = 1, k 0 = 2k + 1 = 5 {a} independent set size k
graph G Figure 9. score every node given label.

Claim 7. G independent set size least k k 0 -O-neighbor
D0 higher score k 0 = 2k + 1 rev k 0 = 2(2k + 1) otherwise.
show alter construction obtain result maximum
degree 3. nodes Sf whose degree may exceed 3 nodes { v 1 : v V (G) }.
main idea reduce degree nodes split sets neighbors
using binary trees. new construction define DAG D0 local score function
f 0 follows. DAG D0 obtained applying following actions:
1. every v V (G), delete nodes Tv1 arcs incident nodes.
2. every v V (G), add nodes v 1a v 1b arcs (v 1a , v 1b ) (v 1b , v 2 ).
3. every v V (G) add binary tree Tv1a exactly |NG (v)| + 1 leaves.
root Tv1a v 1a arcs Tv1a directed away v 1a . Furthermore,
define lv1a bijective mapping NG (v) {lT2 (v)} leaves Tv1a .
4. every v V (G) add arcs (v 1b , lT1 (v)) (lv1a (lT2 (v)), lT2 (v)).
completes construction D0 . Next define local score function f 0 follows:
1. every n V (D0 ) \ { v 1a : v V (G) } {r1 } set f 0 (n, PD0 (n)) = .
2. every v V (G) set f 0 (v 1a , {v 1b }) = , f 0 (v 1b , {v 2 , lT1 (v)}) = , f 0 (v 2 , PD0 (v 2 ) \
{v 1b }) = , f 0 (lT1 (v), PD0 (lT1 (v)) \ {v 1b }) = .
289

fiOrdyniak & Szeider

3. set f 0 (r1 , PD0 (r1 )) = f 0 (r2 , PD0 (r2 ) {r1 }) = .
4. remaining combinations n V (D0 ) P V (D0 ) set f 0 (n, P ) = 0.
easy see Sf 0 maximum degree 3. Furthermore, using arguments
proof Claim 7 one show graph G independent set size k
DAG D0 k 0 -O-neighbor D00 higher score D0 (with respect
f 0 ) k 0 = 3k + 1 rev k 0 = 2(3k + 1) otherwise.
Theorem 6 provides surprising contrast similar study k-local search MAX-SAT
problem fixed-parameter tractable instances bounded degree (Szeider,
2011). possible explanation surprising hardness k-O-Local Search Bayesian
Structure Learning could that, contrast MAX-SAT, global property
entire instance (acyclicity) must checked.

8. Directed Super-structure
previous sections considered problem Exact BNSL k-Local Search
BNSL certain restrictions undirected super-structure. However, every strictly
admissible solution Exact BNSL actually contained restrictive directed
super-structure. natural question whether additional information entailed
directed super-structure used find new structural restrictions Exact
BNSL becomes tractable. well-known Exact BNSL becomes significantly easier
ordering variables given advance. instance, given ordering
variables BN, Exact BNSL becomes solvable polynomial time input given
arity-c representation (Teyssier & Koller, 2005). Fixing ordering variables
BN corresponds restricting search space acyclic directed super-structures.
first observation section Exact BNSL solvable polynomial time
directed super-structure dag input problem given general
non-zero representation. important note corresponding restriction
undirected super-structure, restricting directed super-structure
acyclic impose restrictions undirected super-structure. Considering
promising result becomes natural ask whether possible gradually generalize
class acyclic directed super-structures. natural approach would consider
directed super-structures made acyclic deleting small number k nodes.
approach looks promising known every fixed k directed superstructures made acyclic deleting k nodes recognized efficiently
(Chen, Liu, Lu, OSullivan, & Razgon, 2008). However, show approach
unlikely work Exact BNSL. Furthermore, correspondence results
previous sections, NP-hardness even holds additionally bound maximum in-degree
out-degree Sf .
Theorem 7. Let N set nodes f local score function N Sf
acyclic. find time O(|N |f ) dag maximal score f (D).
Proof. Sf acyclic, follows every strictly admissible directed graph
also acyclic. Hence order compute dag highest score, sufficient
290

fiParameterized Complexity Exact BNSL

compute every n N parent set highest score. clearly done
time O(|N |f ) result follows.
Corollary 2. Exact Bayesian Network Structure Learning solvable quadratic
time acyclic directed super-structures.
Proof. follows immediately Theorem 7 N f bounded
total input size problem. Recall Section 3 local score function f
given list tuples f non-zero hence f bounded total
input size instance.
Theorem 8. Exact Bayesian Network Structure Learning NP-hard instances Sf made acyclic deleting one node. Hardness even holds
additionally bound maximum in-degree maximum out-degree Sf 3.
Proof. devise polynomial reduction restricted version 3-SAT every
literal contained two clauses. version 3-SAT still NP-complete
(Garey & Johnson, 1979). Let 3-CNF formula variables x1 , . . . , xn clauses
C1 , . . . , Cm Cj = lj,1 lj,2 lj,3 , every 1 j m. construct set N nodes,
local score function f real number > 0.
N contains nodes d0 , . . . , dn t0 , . . . , tn+m additionally:
every 1 n nodes xi , xi , ai , bi .
every 1 j nodes lj,1 , lj,2 , lj,3 , Cj .
Let = n + 1 = 1. define f follows:
set f (d0 , {t0 }) = f (t0 , t1 ) = .
every 1 n set:
f (di , {di1 }) = .
f (ai , {di }) = f (xi , {ai }) = f (xi , {ai }) = .
f (bi , {xi }) = f (bi , {xi }) = .
f (ti , {ti+1 , bi }) = .
every 1 j set:
f (Cj , {lj,1 }) = f (Cj , {lj,2 }) = f (Cj , {lj,3 }) = .
f (tn+j , {tn+j+1 , Cj }) = j < f (tn+j , {Cj }) = j = m.
every 1 n, 1 j 1 l 3 set f (lj,l , {xi }) = lj,l = xi
f (lj,l , {xi }) = lj,l = xi .
combinations v N P N set f (v, P ) = 0. Furthermore, set
= (4n + 5m + 2) + n. example directed super-structure constructed
3-CNF formula shown Figure 11. establish theorem showing following
claims.
291

fiOrdyniak & Szeider

Claim 8. Sf made acyclic deleting one node.
Claim 9. Sf maximum in-degree maximum out-degree 3.
Claim 10. satisfiable dag score f (D) s.
d0

d1

d2

a1

a2

x1

x2

b1

l1,2

x3

l1,3

l2,1

b3

l2,2

l2,3

l3,1

C2
t1

x3

b2

C1
t0

a3

x2

x1

l1,1

d3

t2

t3

l3,2

l3,3

C3
t4

t5

t6

Figure 11: directed super-structure formula = (x1 x2 x3 ) (x1 x2 x3 )
(x1 x2 x3 ) proof Theorem 8.
easy see Sf d0 acyclic hence Claim 8 holds. Since every literal
occurs two clauses also easy verify Claim 9. proof Claim 10
straightforward found appendix.

9. Conclusion
studied computational complexity Bayesian Structure Learning (BNSL) various restrictions (directed) super-structure, considering Exact BNSL
k-Local Search BNSL. obtained positive negative results theoretical
worst-case complexities problems. main positive result states Exact BNSL
linear-time tractable super-structure bounded treewidth bounded maximum degree. contrasted positive results negative results, using techniques
concepts Parameterized Complexity. theoretical framework particularly
well-suited investigation allows fined-grained investigation takes
structural aspects problem instances account. results point combinations structural restrictions make problems tractable restrictions cannot
dropped without loosing tractability. Considering various combinations restrictions
systematic way allows us draw broader picture complexity landscape (see table
Section 1). hope results provide better understanding principles
292

fiParameterized Complexity Exact BNSL

BNSL contribute foundations. hope understanding also
useful development heuristic methods practical BNSL systems.

Acknowledgments
shorter preliminary version paper appeared UAI 2010. Research supported
European Research Council, grant reference 239962.

Appendix
Proof Claim 1 (Lemma 2). Let us first assume R = (a, p, s) valid record
let P = a(v0 ). Since R valid follows represents partial solution D(t)
maximal respect R. Now, D0 = D[ (t0 )] partial solution t0
follows Proposition 4 D0 = v0 furthermore ft0 (D0 ) = s. Hence,
D0 represented record R0 = (a0 , p0 , s0 ) R, R0 P satisfy
conditions claim. Furthermore, since R valid ft (D) = ft0 (D0 ) maximality
ft0 (D0 ) respect R0 follows maximality ft (D) respect R
hence R0 valid record t0 .
see converse let P Pf (v0 ) valid record R0 = (a0 , p0 , s0 ) R(t0 ) given.
Let R = (a, p, s) triple defined via conditions (1)-(4). Since R0 valid record
represents partial solution D0 D0 maximal respect R0 . easy
see digraph node set V (D0 ) {v0 } arc set E(D0 ) { (u, v0 ) : u
P } { (v0 , u) : u (t0 ) v0 a0 (u) } acyclic p satisfies condition
(5), i.e., p irreflexive. follows R represents p satisfies condition (5)
furthermore maximality respect R follows maximality D0
respect R0 . Hence, R valid record satisfies conditions
(1)(5).
Proof Claim 2 (Lemma 3). Let us first assume R = (a, p, s) valid record t.
Since R valid record represents solution D(t) maximal
respect R. Now, let R0 = (a0 , p0 , s0 ) a0 (v0 ) = PD (v0 ), a0 (v) = PD (v) every
v (t), p0 union p tuples (v0 , v) (v, v0 ) v (t)
directed path v0 v respectively v v0 s0 = f (v0 , PD (v0 )).
Note Proposition 5 assume PD (v0 ) Pf (v0 ) hence
also represented R0 . follows maximality respect R R0
maximal element R(t0 )[t].
see converse let R = (a, p, s) maximal element R(t0 )[t]. Since R R(t0 )[t]
follows record R0 = (a0 , p0 , s0 ) R(t0 ) R = R0 [t]. Hence,
partial solution represented R0 ft0 (D) = s0 maximal respect
partial solutions represented R0 . Clearly, also represented R maximality
respect R follows fact R maximal element R(t0 )[t].
Proof Claim 3 (Lemma 4). Let us first assume R = (a, p, s) valid record t.
Since R valid follows represents partial solution D(t)
maximal respect R. Let D1 = D[ (t1 )] D2 = D[ (t2 )], i.e., D1 D2
two subdigraphs induced nodes contained nodes subtrees
293

fiOrdyniak & Szeider

rooted t1 t2 , respectively. follows Proposition 3 = D1 D2
V (D1 ) V (D2 ) = (t). Hence, D1 D2 partial solutions t1 t2 , respectively.
{1, 2}, let Ri = (ai , pi , si ) = ai , (v, w) pi
directed path v w Di si = fti (Di ). follows directly definition
R1 R2 represent D1 D2 , respectively, since ft (D) = ft1 (D1 ) + ft2 (D2 )
maximality respect R implies maximality D1 D2 respect
R1 R2 , respectively. Hence, R1 R2 valid records t1 t2 , respectively,
easy see R, R1 R2 satisfy conditions claim.
see converse let us assume given R1 = (a1 , p1 , s1 ) R(t1 )
R2 = (a2 , p2 , s2 ) R(t2 ) triple R = (a, p, s) defined conditions (1)
(3) satisfies condition (4). Since R1 R2 valid follows represent
partial solutions D1 D2 D1 D2 maximal respect R1 R2 ,
respectively. Furthermore, using Proposition 3 follows V (D1 ) V (D2 ) = (t)
hence follows condition (4) = D1 D2 partial solution represented R.
Now, ft (D) = ft1 (D1 ) + ft2 (D2 ) = s1 + s2 = maximality respect
R follows maximality D1 D2 respect R1 R2 , respectively.
follows R valid record t.

Proof Claim 6 (Theorem 3). (1)(2) Suppose G k-clique K. f (D(E(K))) =
nk |V (K)| + |E(K)| = nk + k remains show D(E(K)) acyclic.
see note every cycle D(E(K)) use least one node V ,
D(E(K)) contain arc two nodes A. since K clique, every
node v V either sink, i.e., v incoming arcs, source, i.e., v
outgoing arcs hence cycle use node V .
(2)(3) Suppose dag f (D) s. Let A0 set nodes
f (a, PD (a)) = every A0 . follows definition f every aij A0
unique edge e node Vi node Vj G PD (aij ) = e.
Let E 0 E(G) set edges G correspond node A0 . follows
E 0 representable claim every node v N least local score
D(E 0 ) D. construction D(E 0 ) claim trivially satisfied every
node A. Similarly, every v V \ V (E 0 ) holds f (v, PD(E 0 ) (v)) = hence
f (v, PD(E 0 ) (v)) f (v, PD (v)). Furthermore, every v V (E 0 ) holds f (v, PD (v)) =
0 hence f (v, PD(E 0 ) (v)) f (v, PD (v)). follows f (D(E 0 )) f (D) s.
(3)(1) Suppose E 0 E(G) representable
edge set G f (D(E 0 )) s.

show |V (E 0 )| = k |E 0 | = k2 implies E 0 edge set
k-clique G.
294

fiParameterized Complexity Exact BNSL

E 0 edge set, holds |E 0 |

|V (E 0 )|
2



hence:

f (D(E 0 )) nk = |V (E 0 )| + |E 0 |


|V (E 0 )|
0
|V (E )| +

2


|V (E 0 )|
0
2
= |V (E )|(k k 1) +
2k
2
= |V (E 0 )|(k 2 k 1) + (|V (E 0 )|2 |V (E 0 )|)k
= |V (E 0 )|k 2 + |V (E 0 )|2 k + |V (E 0 )|k |V (E 0 )|k + |V (E 0 )|
= |V (E 0 )|k 2 + |V (E 0 )|2 k + |V (E 0 )|
f (D(E 0 )) s, follows |V (E 0 )|k 2 + |V (E 0 )|2 k + |V (E 0 )| 1 hence:
|V (E 0 )|k 2 + |V (E 0 )|2 k + |V (E 0 )| 1
k 2 + |V (E 0 )|k + 1
|V (E 0 )|

1
|V (E 0 )|
1
2
|V (E 0 )| + k 1
k

1
1
|V (E 0 )| k +
k |V (E 0 )|k
Since |V (E 0 )| integer k > 2, |V (E 0 )| k.

Furthermore, since E 0 representable contain k2 edges hence

f (D(E 0 ))nk |V (E 0 )| + k2 . Again, follows f (D(E 0 )) |V (E 0 )| +

k
2 1 and:

k
0
|V (E )| +
1
2
|V (E 0 )|(k 2 k 1) + k 3 k 2 1
|V (E 0 )|(k 2 k 1) k 3 + k 2 + 1
k 3 + k 2 + 1
|V (E 0 )|
k2 k 1
k+1
|V (E 0 )| k + 2
k k1
0
0
Since |V (E )| integer k > 2, follows |V (E )| k hence |V (E 0 )| = k.
Using f (D(E 0 )) nk 1 get:
k + |E 0 | 1
(k 3 k 2 k) + |E 0 |2k 1
k3 k2 k + 1
|E 0 |
2k
2
k k 1 + k1
|E 0 |
2

1 k1
k
|E 0 |

2
2
295

fiOrdyniak & Szeider

|E 0 | integer k > 2, follows |E 0 |
f (D(E 0 )) implies |V (E 0 )| = k |E 0 | =
k-clique G.

k
. Putting everything together
2
k
0
2 . Hence E edge set



Proof Claim 7 (Theorem 6). first show claim case rev
k 0 = 2k + 1.
Let us first assume G independent set V (G) size least k.
obtain D0 reversing k 0 arcs { (v 1 , v 2 ), (v 1 , lT1 (v)) : v } {(r2 , r1 )}.
decreases score r1 increases score nodes { v 1 : v }
score nodes remains unchanged. Hence, f (D0 ) =
f (D) + |S| + k = f (D) + 1 > f (D) remains show D0 acyclic.
see assume D0 contains cycle C. acyclic C must contain
least one newly created arcs D0 , i.e., C must contain either arc (v 2 , v 1 ), arc
(lT1 (v) , v 1 ) v arc (r1 , r2 ). r2 sink D0 , i.e., r2
outgoing arcs, cycle C cannot contain arc (r1 , r2 ). Similarly, D0
contain directed path v 1 lT1 (v) cycle C cannot contain arc (lT1 (v), v 1 )
v S. Hence cycle C must contain arc (v 2 , v 1 ) v S. suppose
C contains arc (v 2 , v 1 ) v S. D0 contains directed paths
node T2 node T1 follows C cannot leave node v 1 using arc
(v 1 , lT2 (v)). Consequently, cycle C must leave node v 1 towards w2 neighbor
w v G. independent set G follows w
/ hence node
w2 sink D0 contradicting existence cycle C.
see reverse direction assume D0 dag obtained reversing
k 0 arcs. Note nodes { v 1 : v V (G) } nodes whose scores
yet maximum. Hence, order D0 higher score score
least one nodes increased. Now, score node v 1
v V (G) increased reversing arcs (v 1 , v 2 ) (v 1 , lT1 (v)). easy
see reversing arc (v 1 , lT1 (v)) introduces cycle C uses nodes
V (T1 ) V (T2 ) {v 1 }. However, every cycle C contains arc (r2 , r1 )
destroy cycles additionally reversing arc (r2 , r1 ). reversing
(r2 , r1 ) decrease score r1 also cheapest way destroy
cycles. Now, = (k 1) follows order increase score scores
least k nodes { v 1 : v V (G) } increased . Let set nodes
V (G) score nodes { v 1 : v } increased manner.
mentioned |S| k remains show independent set G.
Suppose independent set let u, v {u, v} E(G).
arcs (v 2 , v 1 ) (u2 , u1 ) together directed path v 1 u2 (using arcs
Tv1 Tu2 ) directed path u1 v 2 (using arcs Tu1 Tv2 ) form
cycle D0 contradicting acyclicity D0 . follows independent set G
size least k.
Hence, shown theorem case rev O. remains show
theorem remaining non-trivial set rev
/ O, i.e., set = {add, del}.
k 0 = 2(2k + 1) idea replace every reversal arc (u, v) deletion
(of (u, v)) addition (of (v, u)).
296

fiParameterized Complexity Exact BNSL

Proof Claim 10 (Theorem 8). prove Claim 10 help following
claim.
Claim 11. f (D) acyclic satisfies following conditions:
1 contains least following arcs:
arc (t0 , d0 ).
every 1 n, arcs (di1 , di ), (di , ai ), (bi , ti ) (ti , ti1 ).
every 1 n, 1 j 1 l 3 arc (xi , lj,l ) lj,l = xi
similarly arc (xi , lj,l ) lj,l = xi .
every 1 j m, arcs (Cj , tn+j ) (tn+j , tn+j1 ) exactly one
arcs (lj,1 , Cj ), (lj,2 , Cj ) (lj,3 , Cj ).
2 every 1 n digraph contains arcs (ai , xi ) (xi , bi )
arcs (ai , xi ) (xi , bi ) contains arcs (ai , xi ) (xi , bi ) arcs
(ai , xi ) (xi , bi ).
3 1 n, 1 j 1 l 3, following holds:
lj,l = xi contains arc (lj,l , Cj ) contain arc
(ai , xi ).
lj,l = xi contains arc (lj,l , Cj ) contain arc
(ai , xi ).
first show previous claim used prove Claim 10. Suppose
satisfiable let satisfying assignment . Let digraph satisfies
condition 1 additionally:
every 1 n (xi ) = true contains arcs (ai , xi ) (xi , bi ),
otherwise contains arcs (ai , xi ) (xi , bi ).
every 1 j let lj,l literal clause Cj satisfied ; since
satisfying assignment every clause Cj contains literal. contains
arc (lj,l , Cj ).
follows satisfies conditions 2 3 hence (using Claim 11) f (D)
acyclic.
see reverse let dag f (D) s. follows Claim 11
satisfies conditions 1 3. claim assignment (xi ) = true
contain arc (ai , xi ) satisfying assignment . follows
condition 1 every 1 j digraph contains arc (lj,l , Cj )
1 l 3. W.l.o.g., assume lj,l = xi 1 n (the case lj,l = xi
analog). using condition 1 follows contains arc (xi , lj,l ).
condition 3 digraph contain arc (ai , xi ) hence (lj,l ) = true.
Hence remains show Claim 11. Let us first show every dag f (D)
satisfies conditions 1 3. see observe every node V = {x1 , x1 , . . . , xn , xn }
297

fiOrdyniak & Szeider

either score 0 . Similarly, every node V 0 = N \ V either score 0 .
follows every directed graph 4n + 5m + 2 nodes score
2n nodes score . Hence, maximum score every directed graph
(4n + 5m + 2) + 2n. > n f (D) follows every node
V 0 must score similarly least n 2n nodes V must score .
Hence satisfies condition 1.
show condition 2 observe every 1 n node bi must
score holds exactly one arcs (xi , bi ) (xi , bi ) D. Now,
contains arc (xi , bi ) 1 n cannot contain arc (ai , xi )
otherwise would contain cycle (d, ai , xi , bi , t, d). Similarly, contains arc (xi , bi )
1 n cannot contain arc (ai , xi ). follows every 1 n
least one arcs (ai , xi ) (ai , xi ) missing D. Since n nodes
V score 0 follows every 1 n exactly one arcs (ai , xi )
(ai , xi ) must D. follows satisfies condition 2.
see condition 3 suppose 1 n, 1 j 1 l 3 lj,l = xi
digraph contains arcs (lj,l , Cj ) (ai , xi ). follows would contain
cycle (d, ai , xi , lj,l , Cj , t, d), contradiction. case lj,l = xi analog hence
satisfies condition 3.
see reverse implication claim suppose digraph satisfies
conditions 1 3. easy see f (D) = hence remains show
acyclic. Sf (t0 , d0 ) acyclic follows every cycle use
arc (t0 , d0 ). Hence contains cycle directed path P d0 t0
D. follows condition 2 directed path d0 bi D,
1 3, hence P cannot contain node bi . Since nodes
arcs {t0 , . . . , tn+m } nodes C1 , . . . , Cm follows P use node Cj
1 j m. condition 1 node Cj exactly one incoming neighbor
(one lj,1 , lj,2 , lj,3 ) say lj,l . using condition 1 node lj,l exactly one incoming
neighbor xi xi lj,l = xi lj,l = xi , respectively. W.l.o.g. let us assume lj,l = xi .
follows condition 3 xi incoming neighbor hence contains directed
path P d0 t0 .

References
Bang-Jensen, J., & Gutin, G. (2009). Digraphs (Second edition). Springer Monographs
Mathematics. Springer-Verlag London Ltd., London.
Bodlaender, H. L. (1993). tourist guide treewidth. Acta Cybernetica, 11, 121.
Bodlaender, H. L. (2005). Discovering treewidth. Proceedings 31st Conference
Current Trends Theory Practice Computer Science (SOFSEM05), Vol.
3381 Lecture Notes Computer Science, pp. 116. Springer Verlag.
Bodlaender, H. L. (1996). linear-time algorithm finding tree-decompositions small
treewidth. SIAM J. Comput., 25 (6), 13051317.
Bodlaender, H. L. (1997). Treewidth: algorithmic techniques results. Mathematical foundations computer science 1997 (Bratislava), Vol. 1295 Lecture Notes
Computer Science, pp. 1936. Springer, Berlin.
298

fiParameterized Complexity Exact BNSL

Bodlaender, H. L., & Koster, A. M. C. A. (2008). Combinatorial optimization graphs
bounded treewidth. Comput. J., 51 (3), 255269.
Chechetka, A., & Guestrin, C. (2007). Efficient principled learning thin junction trees.
Platt, J. C., Koller, D., Singer, Y., & Roweis, S. T. (Eds.), Advances Neural Information Processing Systems 20, Proceedings Twenty-First Annual Conference
Neural Information Processing Systems, Vancouver, British Columbia, Canada,
December 3-6, 2007. MIT Press.
Chen, J., Liu, Y., Lu, S., OSullivan, B., & Razgon, I. (2008). fixed-parameter algorithm
directed feedback vertex set problem. J. ACM, 55 (5), Art. 21, 19.
Chickering, D. M. (1995). transformational characterization equivalent Bayesian network structures. Uncertainty artificial intelligence (Montreal, PQ, 1995), pp.
8798. Morgan Kaufmann, San Francisco, CA.
Chickering, D. M. (1996). Learning Bayesian networks NP-complete. Learning
data (Fort Lauderdale, FL, 1995), Vol. 112 Lecture Notes Statist., pp. 121130.
Springer Verlag.
Chow, C. I., & Liu, C. N. (1968). Approximating discrete probability distributions
dependence trees. IEEE Transactions Information Theory, 14, 462467.
Darwiche, A. (2001). Recursive conditioning. Artificial Intelligence, 126 (1-2), 541.
Dasgupta, S. (1999). Learning polytrees. Laskey, K. B., & Prade, H. (Eds.), UAI
99: Proceedings Fifteenth Conference Uncertainty Artificial Intelligence,
Stockholm, Sweden, July 30-August 1, 1999, pp. 134141. Morgan Kaufmann.
de Campos, C. P., Zeng, Z., & Ji, Q. (2009). Structure learning Bayesian networks using
constraints. Danyluk, A. P., Bottou, L., & Littman, M. L. (Eds.), Proceedings
26th Annual International Conference Machine Learning, ICML 2009, Montreal, Quebec, Canada, June 14-18, 2009, Vol. 382 ACM International Conference
Proceeding Series, p. 15. ACM.
Dechter, R. (1999). Bucket elimination: unifying framework reasoning. Artificial
Intelligence, 113 (1-2), 4185.
Diestel, R. (2000). Graph Theory (2nd edition)., Vol. 173 Graduate Texts Mathematics.
Springer Verlag, New York.
Dow, P. A., & Korf, R. E. (2007). Best-first search treewidth. Proceedings
Twenty-Second AAAI Conference Artificial Intelligence, July 22-26, 2007, Vancouver, British Columbia, Canada, pp. 11461151. AAAI Press.
Downey, R. G., & Fellows, M. R. (1999). Parameterized Complexity. Monographs Computer Science. Springer Verlag, New York.
Downey, R. G., & Fellows, M. R. (1995). Fixed-parameter tractability completeness.
II. completeness W [1]. Theoret. Comput. Sci., 141 (1-2), 109131.
Downey, R. G., Fellows, M. R., & Langston, M. A. (2008). computer journal special
issue parameterized complexity: Foreword guest editors. Computer
Journal, 51 (1), 16.
299

fiOrdyniak & Szeider

Elidan, G., & Gould, S. (2008). Learning bounded treewidth Bayesian networks. Koller,
D., Schuurmans, D., Bengio, Y., & Bottou, L. (Eds.), Advances Neural Information
Processing Systems 21, Proceedings Twenty-Second Annual Conference Neural Information Processing Systems, Vancouver, British Columbia, Canada, December
8-11, 2008, pp. 417424. MIT Press.
Fellows, M. R. (2003). Blow-ups, win/wins, crown rules: new directions FPT.
Bodlaender, H. L. (Ed.), Graph-Theoretic Concepts Computer Science (WG
2003), Vol. 2880 Lecture Notes Computer Science, pp. 112. Springer Verlag.
Fellows, M. R., Rosamond, F. A., Fomin, F. V., Lokshtanov, D., Saurabh, S., & Villanger,
Y. (2009). Local search: brute-force avoidable?. Boutilier, C. (Ed.), IJCAI
2009, Proceedings 21st International Joint Conference Artificial Intelligence,
Pasadena, California, USA, July 11-17, 2009, pp. 486491.
Fellows, M. R. (2012) Personal Communication.
Flum, J., & Grohe, M. (2006). Parameterized Complexity Theory, Vol. XIV Texts
Theoretical Computer Science. EATCS Series. Springer Verlag, Berlin.
Friedman, N., Nachman, I., & Peer, D. (1999). Learning Bayesian network structure
massive datasets: sparse candidate algorithm. Laskey, K. B., & Prade, H.
(Eds.), UAI 99: Proceedings Fifteenth Conference Uncertainty Artificial Intelligence, Stockholm, Sweden, July 30-August 1, 1999, pp. 206215. Morgan
Kaufmann.
Garey, M. R., & Johnson, D. R. (1979). Computers Intractability. W. H. Freeman
Company, New York, San Francisco.
Gaspers, S., Kim, E. J., Ordyniak, S., Saurabh, S., & Szeider, S. (2012). Dont strict
local search!. Proceedings Twenty-Sixth AAAI Conference Artificial
Intelligence, AAAI 2012, Toronto, Ontaria, Canada, July 22-26, 2012. AAAI Press.
appear.
Gaspers, S., Koivisto, M., Liedloff, M., Ordyniak, S., & Szeider, S. (2012). finding
optimal polytrees. appear AAAI 2012.
Gelfand, A., Kask, K., & Dechter, R. (2011). Stopping rules randomized greedy triangulation schemes. Burgard, W., & Roth, D. (Eds.), Proceedings Twenty-Fifth
AAAI Conference Artificial Intelligence, AAAI 2011, San Francisco, California,
USA, August 7-11, 2011. AAAI Press.
Gogate, V., & Dechter, R. (2004). complete anytime algorithm treewidth. Proceedings Proceedings Twentieth Conference Annual Conference Uncertainty
Artificial Intelligence (UAI-04), pp. 201208, Arlington, Virginia. AUAI Press.
Greco, G., & Scarcello, F. (2010). power structural decompositions graph-based
representations constraint problems. Artificial Intelligence, 174 (56), 382409.
Heckerman, D., Geiger, D., & Chickering, D. M. (1995). Learning Bayesian networks:
combination knowledge statistical data. Machine Learning, 20 (3), 197243.
Impagliazzo, R., Paturi, R., & Zane, F. (2001). problems strongly exponential
complexity?. J. Computer System Sciences, 63 (4), 512530.
300

fiParameterized Complexity Exact BNSL

Karger, D. R., & Srebro, N. (2001). Learning markov networks: maximum bounded treewidth graphs. SODA, pp. 392401.
Karp, R. M. (1972). Reducibility among combinatorial problems. Complexity computer computations (Proc. Sympos., IBM Thomas J. Watson Res. Center, Yorktown
Heights, N.Y., 1972), pp. 85103. Plenum, New York.
Kask, K., Gelfand, A., Otten, L., & Dechter, R. (2011). Pushing power stochastic
greedy ordering schemes inference graphical models. Burgard, W., & Roth, D.
(Eds.), Proceedings Twenty-Fifth AAAI Conference Artificial Intelligence,
AAAI 2011, San Francisco, California, USA, August 7-11, 2011. AAAI Press.
Khuller, S., Bhatia, R., & Pless, R. (2003). local search placement meters
networks. SIAM J. Comput., 32 (2), 470487.
Kloks, T. (1994). Treewidth: Computations Approximations. Springer Verlag, Berlin.
Koivisto, M. (2006). Advances exact Bayesian structure discovery Bayesian networks.
UAI 06, Proceedings 22nd Conference Uncertainty Artificial Intelligence, July 13-16 2006, Cambridge, MA, USA. AUAI Press.
Kojima, K., Perrier, E., Imoto, S., & Miyano, S. (2010). Optimal search clustered
structural constraint learning Bayesian network structure. J. Mach. Learn. Res.,
11, 285310.
Koster, A. M. C. A., Bodlaender, H. L., & van Hoesel, S. P. M. (2001). Treewidth: Computational experiments. Electronic Notes Discrete Mathematics, 8, 5457.
Krokhin, A. A., & Marx, D. (2012). hardness losing weight. ACM Transactions
Algorithms, 8 (2), 19.
Kwisthout, J., Bodlaender, H. L., & van der Gaag, L. C. (2010). necessity bounded
treewidth efficient inference Bayesian networks. ECAI, pp. 237242.
Marx, D. (2008). Searching k-change neighborhood TSP W[1]-hard. Oper. Res.
Lett., 36 (1), 3136.
Marx, D., & Schlotter, I. (2011). Stable assignment couples: Parameterized complexity
local search. Discrete Optimization, 8 (1), 2540.
Meek, C. (2001). Finding path harder finding tree. J. Artif. Intell. Res., 15,
383389.
Narasimhan, M., & Bilmes, J. A. (2004). PAC-learning bounded tree-width graphical models. Chickering, D. M., & Halpern, J. Y. (Eds.), UAI 04, Proceedings 20th
Conference Uncertainty Artificial Intelligence, July 7-11 2004, Banff, Canada,
pp. 410417. AUAI Press.
Niedermeier, R. (2006). Invitation Fixed-Parameter Algorithms. Oxford Lecture Series
Mathematics Applications. Oxford University Press, Oxford.
Ordyniak, S., & Szeider, S. (2010). Algorithms complexity results exact Bayesian
structure learning. Grunwald, P., & Spirtes, P. (Eds.), Proceedings UAI 2010,
26th Conference Uncertainty Artificial Intelligence, Catalina Island, California,
USA, July 8-11, 2010. AUAI Press, Corvallis, Oregon.
301

fiOrdyniak & Szeider

Ott, S., Imoto, S., & Miyano, S. (2004). Finding optimal models small gene networks.
Altman, R. B., Dunker, A. K., Hunter, L., Jung, T. A., & Klein, T. E. (Eds.), Biocomputing 2004, Proceedings Pacific Symposium, Hawaii, USA, 6-10 January
2004, pp. 557567. World Scientific.
Parviainen, P., & Koivisto, M. (2010). Bayesian structure discovery Bayesian networks
less space. J. Mach. Learn. Res., 9, 589596.
Perrier, E., Imoto, S., & Miyano, S. (2008). Finding optimal Bayesian network given
super-structure. J. Mach. Learn. Res., 9, 22512286.
Pieter, A., Daphne, K., & Andrew, Y. N. (2006). Learning factor graphs polynomial time
sample complexity. J. Mach. Learn. Res., 7, 17431788.
Pietrzak, K. (2003). parameterized complexity fixed alphabet shortest common supersequence longest common subsequence problems. J. Computer
System Sciences, 67 (4), 757771.
Silander, T., & Myllymaki, P. (2006). simple approach finding globally optimal Bayesian network structure. UAI 06, Proceedings 22nd Conference
Uncertainty Artificial Intelligence, July 13-16 2006, Cambridge, MA, USA. AUAI
Press.
Szeider, S. (2011). parameterized complexity k-flip local search SAT MAX
SAT. Discrete Optimization, 8 (1), 139145.
Teyssier, M., & Koller, D. (2005). Ordering-based search: simple effective algorithm
learning bayesian networks. UAI 05, Proceedings 21st Conference
Uncertainty Artificial Intelligence, Edinburgh, Scotland, July 26-29, 2005, pp. 548
549. AUAI Press.
Tsamardinos, I., Brown, L., & Aliferis, C. (2006). max-min hill-climbing Bayesian
network structure learning algorithm. Machine Learning, 65, 3178.
Tsang, E. P. K. (1993). Foundations Constraint Satisfaction. Academic Press.
Yuan, C., Malone, B., & Wu, X. (2011). Learning optimal Bayesian networks using A*
search. Walsh, T. (Ed.), IJCAI 2011, Proceedings 22nd International Joint
Conference Artificial Intelligence, Barcelona, Catalonia, Spain, July 16-22, 2011,
pp. 21862191. IJCAI/AAAI.

302

fiJournal Artificial Intelligence Research 46 (2013) 129-163

Submitted 09/12; published 01/13

Undominated Groves Mechanisms
Mingyu Guo

Mingyu.Guo@liverpool.ac.uk

University Liverpool, UK

Evangelos Markakis

markakis@gmail.com

Athens University Economics Business, Greece

Krzysztof R. Apt

apt@cwi.nl

CWI, Netherlands

Vincent Conitzer

conitzer@cs.duke.edu

Duke University, USA

Abstract
family Groves mechanisms, includes well-known VCG mechanism (also
known Clarke mechanism), family efficient strategy-proof mechanisms.
Unfortunately, Groves mechanisms generally budget balanced. is,
mechanisms, payments may flow system agents, resulting
deficits reduced utilities agents. consider following problem: within
family Groves mechanisms, want identify mechanisms give agents
highest utilities, constraint mechanisms must never incur deficits.
adopt prior-free approach. introduce two general measures comparing
mechanisms prior-free settings. say non-deficit Groves mechanism individually dominates another non-deficit Groves mechanism every type profile,
every agents utility less , holds strict
inequality least one type profile one agent. say non-deficit Groves
mechanism collectively dominates another non-deficit Groves mechanism every
type profile, agents total utility less , holds
strict inequality least one type profile. definitions induce two partial
orders non-deficit Groves mechanisms. study maximal elements corresponding
two partial orders, call individually undominated mechanisms
collectively undominated mechanisms, respectively.

1. Introduction
Mechanism design often employed coordinating group decision making among agents.
Often, mechanisms impose payments agents pay central authority.
Although maximizing revenue desirable objective many settings (for example,
mechanism auction designed seller), desirable situations
entity profiting payments. examples include public project problems well
certain resource allocation problems without seller (e.g., right use shared good
given time slot, assignment take-off slots among airline companies). cases,
would like mechanisms minimize payments (or, even better, achieve budget
balance), maintaining desirable properties, efficient, strategy-proof
non-deficit (i.e., mechanism need funded external source).
c
2013
AI Access Foundation. rights reserved.

fiGuo, Markakis, Apt, & Conitzer

family Groves mechanisms, includes well-known VCG mechanism (also
known Clarke mechanism), family efficient strategy-proof mechanisms.
many sufficiently general settings, including settings study paper,
Groves mechanisms efficient strategy-proof mechanisms (Holmstrom,
1979). Unfortunately though, Groves mechanisms generally budget balanced.
is, mechanisms, payments may flow system agents,
resulting deficits reduced utilities agents. Motivated consider
paper following problem: within family Groves mechanisms, want
identify mechanisms give agents highest utilities, constraint
mechanisms never incur deficits.1
adopt prior-free approach, agent knows valuation vi ,
prior probability distribution agents values. introduce
two natural measures comparing mechanisms prior-free settings. Given performance
indicator, say mechanism individually dominates mechanism every type
profile agents, performs worse perspective individual
agent, holds strict inequality least one type profile one agent.
say mechanism collectively dominates mechanism every type profile,
performs worse perspective set agents whole,
holds strict inequality least one type profile. paper, focus
maximizing agents utilities. Given specific performance indicator, individual
collective dominance determined comparing either individual utilities sum
agents utilities, respectively.
definitions induce two partial orders non-deficit Groves mechanisms.
goal work identify study maximal elements corresponding two
partial orders, call individually undominated (non-deficit Groves) mechanisms
collectively undominated (non-deficit Groves) mechanisms, respectively.
noted partial orders focus may different partial orders induced
performance indicators, e.g., criterion revenue extracted agents.
1.1 Structure Paper
presentation results structured follows: Sections 2 3, formally
define notions individual collective dominance, well family Groves
mechanisms, provide basic observations. also establish general
properties anonymous Groves mechanisms use later on, may
independent interest. begin study individual dominance Section 4,
give characterization individually undominated mechanisms. also propose
two techniques transforming given non-deficit Groves mechanism one
individually undominated.
Sections 5 6 study question finding collectively undominated mechanisms two settings. first (Section 5) auctions multiple identical units
unit-demand bidders. setting, VCG mechanism collectively dominated
1. agents utilities may increased also consider mechanisms outside Groves
family (Guo & Conitzer, 2008a; de Clippel, Naroditskiy, & Greenwald, 2009; Faltings, 2005; Guo,
Naroditskiy, Conitzer, Greenwald, & Jennings, 2011), paper take efficiency hard
constraint.

130

fiUndominated Groves Mechanisms

non-deficit Groves mechanisms, Bailey-Cavallo mechanism (Bailey, 1997;
Cavallo, 2006). obtain complete characterization collectively undominated mechanisms anonymous linear (meaning redistribution linear function
ordered type profile; see Section 5 definition). particular, show
collectively undominated mechanisms anonymous linear exactly
Optimal-in-Expectation Linear (OEL) redistribution mechanisms, include BaileyCavallo mechanism introduced Guo Conitzer (2010). second setting
(Section 6) public project problems, agents must decide whether
finance project. show case agents identical participation costs, VCG mechanism collectively undominated. hand,
participation costs different across agents, exist mechanisms collectively
dominate VCG. finally show participation costs different across agents,
VCG mechanism remains collectively undominated among pay-only mechanisms.
1.2 Related Work
efficiently allocate resources among group competing agents well-studied
topic economics literature. example, famous Myerson-Satterthwaite Theorem (Myerson & Satterthwaite, 1983) rules existence efficient, Bayes-Nash incentive compatible, budget-balanced, individually rational mechanisms. Cramton, Gibbons,
Klemperer (1987) characterized Bayes-Nash incentive compatible individually rational mechanisms dissolving partnership, gave necessary sufficient condition
possibility dissolving partnership efficiently.
main difference papers adopt prior-free approach. is, assume know prior distribution agents
valuations. result this, notion truthfulness strategy-proofness,
stronger Bayes-Nash incentive compatibility. many sufficiently general settings,
including settings study paper, Groves mechanisms
efficient strategy-proof mechanisms (Holmstrom, 1979). is, search undominated Groves mechanisms is, many settings, search efficient, strategy-proof,
non-deficit mechanisms closest budget-balance.
Recently, series works VCG redistribution mechanisms,
mechanisms make social decisions according efficient strategy-proof
VCG mechanism, redistribute VCG payments back agents,
certain constraints, agents redistribution independent
type (therefore ensuring strategy-proofness), total redistribution
never exceed total VCG payment (therefore ensuring non-deficit). Actually,
non-deficit Groves mechanism interpreted VCG-based redistribution
mechanism, (non-deficit) VCG redistribution mechanism corresponds nondeficit Groves mechanism (more details provided Section 2).
One example redistribution mechanism Bailey-Cavallo (BC) mechanism (Cavallo, 2006).2 BC mechanism, every agent, besides participating VCG
2. settings revenue monotonic, Cavallo (2006) mechanism coincides mechanism
discovered earlier Bailey (1997). Bailey-Cavallo mechanism single-item auction also
independently discovered Porter, Shoham, Tennenholtz (2004).

131

fiGuo, Markakis, Apt, & Conitzer

mechanism, also receives n1 times minimal VCG revenue could obtained
changing agents bid. settings (e.g., single-item auction), BC
mechanism successfully redistribute large portion VCG payments back
agents. is, settings, BC mechanism individually collectively
dominates VCG mechanism.
Guo Conitzer (2009) proposed another VCG redistribution mechanism called
worst-case optimal (WCO) redistribution mechanism, setting multi-unit auctions
nonincreasing marginal values. WCO optimal terms fraction total VCG
payment redistributed worst case.3 Moulin (2009) independently derived WCO
slightly different worst-case optimality notion (in restrictive setting multi-unit
auctions unit demand only). Guo Conitzer (2010) also proposed family VCG
redistribution mechanisms aim maximize expected amount VCG payment
redistributed, setting multi-unit auctions unit demand. members
family called Optimal-in-Expectation Linear (OEL) redistribution mechanisms.
Finally, paper closest study early work Moulin
collectively undominated non-deficit Groves mechanisms (Moulin, 1986). deals
problem selecting efficient public decision finitely many costless alternatives.4
agent submits central authority utility alternative. Subsequently,
central authority makes decision maximizes social welfare. Moulin (1986,
Lemma 2) showed VCG mechanism collectively undominated setting.
result generalizes earlier result case two public decisions Laffont
Maskin (1997).

2. Preliminaries
first briefly review payment-based mechanisms (see, e.g., Mas-Colell, Whinston, &
Green, 1995).
2.1 Payment-Based Mechanisms
Assume set possible outcomes decisions D, set {1, . . ., n} agents
n 2, agent i, set types (initial ) utility function
vi : R. Let := 1 n .
(direct revelation) mechanism, agent reports type based this,
mechanism selects outcome payment made every agent. Hence
mechanism given pair functions (f, t), f decision function
payment function determines agents payments, i.e., f : D, : Rn .
put ti () := (t())i , i.e., function ti computes payment agent i.
vector announced types, ti () 0, agent pays ti (), ti () < 0, receives
|ti ()|. true type agent announced type , final utility
function defined
ui ((f, t)(i , ), ) := vi (f (i , ), ) ti (i , ),
3. notion worst-case optimality also studied general settings (Gujar & Narahari, 2011;
Guo, 2011, 2012).
4. public project model, cost associated building project.

132

fiUndominated Groves Mechanisms

vector types announced agents.
2.2 Properties Payment-Based Mechanisms
say payment-based mechanism (f, t)
P
P
efficient D, ni=1 vi (f (), ) ni=1 vi (d, ),
P
budget-balanced ni=1 ti () = 0 ,
P
non-deficit ni=1 ti () 0 , i.e., mechanism need funded
external source,
pay-only ti () 0 {1, . . ., n},
strategy-proof , {1, . . ., n}, ,
ui ((f, t)(i , ), ) ui ((f, t)(i , ), ),
i.e., agent i, reporting false type, , profitable.
2.3 Individual Collective Dominance
consider prior-free settings, agent knows function vi ,
belief prior probability distribution regarding agents initial utilities.
Payment-based mechanisms naturally compared terms either effect
individual agent global effect whole set agents. therefore introduce
two measures comparing mechanisms. Given performance indicator5 , say
mechanism (f , ) individually dominates mechanism (f, t) every type profile,
(f , ) performs worse (f, t) perspective every agent, holds
strict inequality least one type profile one agent. say mechanism
(f , ) collectively dominates mechanism (f, t) every type profile, (f , ) performs
worse (f, t) perspective whole agent system, holds strict
inequality least one type profile. paper, focus maximizing agents
utilities. Given specific performance indicator, individual collective dominance
captured following definitions:
Definition 2.1 Given two payment-based mechanisms (f, t) (f , ), say (f , )
individually dominates (f, t)
{1, . . ., n}, ui ((f, t)(), ) ui ((f , )(), ),
{1, . . ., n}, ui ((f, t)(), ) < ui ((f , )(), ).
Definition 2.2 Given two payment-based mechanisms (f, t) (f , ), say (f , )
collectively dominates (f, t)
5. performance indicator mean function mechanisms outcome serves measure
comparing mechanisms. E.g., final utility agent, arbitrary function it,
function agents payment function depends decision rule
payment rule mechanism.

133

fiGuo, Markakis, Apt, & Conitzer

,

Pn

,

i=1 ui ((f, t)(), )

Pn



i=1 ui ((f, t)(), )

Pn

i=1 ui ((f

<

Pn

, )(),

i=1 ui ((f

),

, )(),

).

two payment-based mechanisms (f, t) (f , ), clearly (f , ) individually dominates (f, t), also collectively dominates (f, t). Theorem 3.4 shows reverse
implication however need hold, even limit special types
mechanisms. is, fact (f , ) collectively dominates (f, t) imply
(f , ) individually dominates (f, t).
Given set Z payment-based mechanisms, individual collective dominance induce
two partial orders Z, interested studying maximal elements
respect partial orders. maximal element respect first partial order
called individually undominated mechanism, i.e., mechanism
individually dominated mechanism Z. maximal element second
partial order called collectively undominated mechanism, i.e., mechanism
collectively dominated mechanism Z. maximal elements
respect two partial orders may differ particular, notion collectively
undominated mechanisms generally stronger notion. Clearly, (f , ) Z collectively
undominated, also individually undominated. reverse may true,
examples provided Section 4.2.
focus decision function f , individual collective dominance
strictly due difference payment functions. Hence, (f, ) individually dominates
(f, t) (or simply individually dominates t)
{1, . . ., n}, ti () ti (),
{1, . . ., n}, ti () > ti (),
collectively dominates
P
P
, ni=1 ti () ni=1 ti (),
P
P
, ni=1 ti () > ni=1 ti ().

define two transformations payment-based mechanisms originating
decision function. transformations build upon surplus-guarantee concept
(Cavallo, 2006) specific case VCG mechanism6 .
Consider payment-based mechanism
Pn (f, t). Given = (1 , . . ., n ), let ()
total amount payments, i.e., () := i=1 ti (). {1, . . ., n} let
SiBCGC (i ) := inf (i , ).


words, SiBCGC (i ) surplus guarantee independent report agent i.
define payment-based mechanism (f, tBCGC ) setting {1, . . ., n}
tBCGC
() := ti ()


SiBCGC (i )
.
n

6. first transformation originally defined Bailey (1997) Cavallo (2006) specific case
VCG mechanism Guo Conitzer (2008b) non-deficit Groves mechanisms. call
BCGC transformation authors papers (Bailey, Cavallo, Guo, Conitzer).

134

fiUndominated Groves Mechanisms

Also, fixed agent j, define payment-based mechanism (f, tBCGC(j) )
setting {1, . . ., n}

ti () SiBCGC (i )
= j
BCGC(j)
() :=
ti
ti ()
6= j
first transformation (from (f, t) (f, tBCGC )), every agent receives additional7 amount n1 times surplus guarantee independent type.
second transformation (from (f, t) (f, tBCGC(j) ), agent j chosen agent
receives additional amount. additional amount equals entirety surplus guarantee independent js type. transformations agents additional
payments independent types, thus strategy-proofness maintained:
(f, t) strategy-proof, (f, tBCGC ) (f, tBCGC(j) ) j.
following observations generalize earlier results Bailey (1997)
Cavallo (2006).
Proposition 2.3
(i) payment-based mechanism form tBCGC non-deficit.
(ii) non-deficit, either tBCGC coincide tBCGC individually (and hence
also collectively) dominates t.
Proof. (i) {1, . . ., n} () SiBCGC (i ),


BCGC

() =
=

n
X

i=1
n
X
i=1

tBCGC
()


= ()

n
X
BCGC (i )


i=1

n

() SiBCGC (i )
0.
n

(ii) non-deficit, {1, . . ., n} SiBCGC (i ) 0,
hence tBCGC
() ti ().
2

claims hold tBCGC(j) j {1, . . ., n}, equally simple proofs.

3. Groves Mechanisms
first briefly review Groves mechanisms.
3.1 Preliminaries
Recall Groves (1973) Mechanism payment-based mechanism (f, t)
following hold8 :
7. Receiving additional positive amount means paying less receiving additional negative amount
means paying more.
P
8. j6=i shorthand summation j {1, . . ., n}, j 6= i.

135

fiGuo, Markakis, Apt, & Conitzer

P
f () arg maxdD ni=1 vi (d, ), i.e., chosen outcome maximizes allocation
welfare (the agents total valuation),
ti : R defined ti () := hi (i ) gi (),
P
gi () := j6=i vj (f (), j ),
hi : R arbitrary function.

gi () represents allocation welfare decision f () agent ignored.
Recall following crucial result (see, e.g., Mas-Colell et al., 1995).
Groves Theorem (Groves, 1973) Every Groves mechanism efficient strategyproof.
several decision problems, efficient strategy-proof payment-based mechanisms Groves mechanisms. implied general result Holmstrom (1979),
covers two domains consider Sections 5 6, explains focus
Groves mechanisms. Hence on, use term mechanism refer
Groves mechanism.
Focusing set non-deficit Groves mechanisms, individually (respectively, collectively) undominated mechanisms mechanisms set individually
(respectively, collectively) dominated non-deficit Groves mechanism. mentioned earlier, matter domain set mechanisms consider, collective
undominance always implies individual undominance. Section 4.2 show two examples single-item auction scenarios, collective undominance strictly stronger
individual undominance, non-deficit Groves mechanisms. is, exists individually undominated non-deficit Groves mechanism collectively dominated.
Recall special Groves mechanism, called VCG Clarke (1971) mechanism,
obtained using9
X
vj (d, j ).
hi (i ) := max
dD

j6=i

case
ti () := max
dD

X

vj (d, j )

X

vj (f (), j ),

j6=i

j6=i

shows VCG mechanism pay-only.
follows introduce slightly different notation describe Groves mechanisms,
makes rest presentation convenient. First, denote payment
function ti VCG mechanism V CGi . Note Groves mechanism (f, t)
defined terms VCG mechanism setting ti () := V CGi ()ri (i ),
ri : R function . refer r := (r1 , . . ., rn ) redistribution
function. Hence Groves mechanism identified redistribution function r
viewed VCG mechanism combined redistribution. is,
r agents first participate VCG mechanism. Then, top that, agent also
9. below, whenever finite set, order ensure considered maximum exists,
assume f continuous, vi i, also set compact
subsets Rk .

136

fiUndominated Groves Mechanisms

receives redistribution
amountPequal ri (i ). definition, Groves mechanism r
P
non-deficit iff ni=1 V CGi () ni=1 ri (i ) .
3.2 Dominance Relations
Using new notation above, individual collective dominance (among non-deficit
Groves mechanisms) described follows:
Definition 3.1 non-deficit Groves mechanism r individually dominates another nondeficit Groves mechanism r
, ri (i ) ri (i ),
, ri (i ) > ri (i ).
Definition 3.2 non-deficit Groves mechanism r collectively dominates another nondeficit Groves mechanism r
P
P
, ri (i ) ri (i ),
P
P
, ri (i ) > ri (i ).
consider mechanism results applying BCGC transformation
VCG mechanism. refer Bailey-Cavallo mechanism simply
BC mechanism (Bailey, 1997; Cavallo, 2006). VCG mechanism characterized
constant redistribution function rVCG = (0, 0, . . ., 0). BCGC transformation,
every agent receives additional amount n1 times surplus guarantee SiBCGC (i ),
independent type. is, BC mechanism also Groves mechanism,
redistribution function
1
1
1
rBC = ( S1BCGC , S2BCGC , . . ., SnBCGC ).
n
n
n
Let := (1 , . . ., i1 , , i+1 , . . ., n ). starting VCG mechanism,



SiBCGC (i ) = inf



n
X

k=1

max
dD

X
j6=k

vj (d, j )

X
j6=k

vj (f ( ), j ) ,

is,


SiBCGC (i ) = inf


n
X

k=1

max
dD

X

vj (d, j ) (n 1)

j6=k

n
X

k=1



vk (f ( ), k )

(1)

many settings, i, SiBCGC (i ) = 0, consequently
VCG BC mechanisms coincide (e.g., see Proposition 6.1). Whenever not,
Proposition 2.3(ii), BC individually collectively dominates VCG. case
single-item auction, seen SiBCGC (i ) = [i ]2 , [i ]2
second-highest bid among bids agent bid.
137

fiGuo, Markakis, Apt, & Conitzer

3.3 Anonymous Groves Mechanisms
proofs main results obtained arguing first special class
Groves mechanisms, called anonymous Groves mechanisms. provide results
class utilize later sections. call function f : B
permutation independent permutations {1, . . ., n}, f = f . Following
Moulin (1986), call Groves mechanism r = (r1 , . . ., rn ) anonymous 10
type sets equal,
functions ri coincide permutation independent.
Hence, anonymous Groves mechanism uniquely determined single function r :
n1 R.
general, VCG mechanism anonymous. anonymous
type sets equal initial utility functions vi coincide. case
two domains consider later sections.
permutation {1, . . ., n} define letting
:= 1 (i) .
Denote (k) set permutations set {1, . . ., k}. Given Groves mechanism r := (r1 , . . ., rn ) type set every agent (and equal
set 0 ), construct function r : 0n1 R, following Moulin (1986),
setting
P
n

X
(n1) rj (x )

r (x) :=
,
n!
j=1

x

.


defined analogously
Note r permutation independent, r anonymous Groves mechanism.
following lemma, independent interest, shows properties
r transfer r .
Lemma 3.3 Consider Groves P
mechanism r corresponding anonymous Groves
n

mechanism r . Let V CG() :=
i=1 V CGi (), suppose V CG function
permutation independent. Then:
(i) r non-deficit, r .
(ii) anonymous Groves mechanism r0 collectively dominated r, collectively dominated r .
Proof.
n
X
i=1

ri (i )

=

Pn P
i=1

(n1)

Pn

n!

j=1 rj ((i )

)

=

10. definition slightly different one introduced Moulin (1986) conditions
put utility functions permutation independence refers redistribution function.

138

fiUndominated Groves Mechanisms

Pn P
i=1


(n) ri (i )

n!
last equality holds since terms aggregate applications ri
functions permutations n 1 elements .
Let payment functions mechanisms r r , respectively.
n
X

ti () = V CG()

n
X

ri (i )

i=1

i=1

(n)

n
X





ti ( ) = V CG( )

i=1

n
X


ri (i
).

i=1

Hence assumption V CG() follows
P
Pn
n

X
(n)
i=1 ti ( )

ti () =
n!

(2)

i=1

(i) immediate consequence (2).
prove (ii) let t0 payment function r0 . r collectively dominates r0 ,
(n)
n
n
X
X

t0i ( )
ti ( )
i=1

i=1

least one inequality strict. Hence
P
Pn
P
Pn 0

(n)
i=1 ti ( )
(n)
i=1 ti ( )

n!
n!
least one inequality strict.
fact r0 anonymous assumption V CG() imply
permutations {1, . . ., n}
n
X

n
X

t0i ( ) =

t0i (),

i=1

i=1

(2) inequality,
n
X
i=1

ti ()



n
X

t0i (),

i=1

least one inequality strict.

2

assumption Lemma 3.3 permutation independence V CG() satisfied
domains consider Sections 5 6. item (ii) states Groves
mechanism considered sequel collectively undominated, collectively
dominated anonymous Groves mechanism.
prove large class Groves mechanisms includes ones
study sequel introduced relations dominance differ.
139

fiGuo, Markakis, Apt, & Conitzer

Theorem 3.4 Suppose n 3. Assume sets types equal set 0
contains least n1 elements. two non-deficit anonymous Groves mechanisms
r r exist r collectively dominates r r individually dominate r .
Proof. Fix non-deficit anonymous Groves mechanism determined permutation
independent function r : 0n1 R.
Let a1 , . . ., an1 arbitrary different elements 0 . Define permutation independent
function q : 0n1 R putting

1 x permutation (a1 , . . ., an1 )
q(x) :=
2 otherwise
n0 two subsequences
may form permutation
Pn
(a1 , . . ., an1 ). n 3, ,
q(
)
0. implies

i=1
anonymous Groves mechanism determined function r := r q non-deficit.
Trivially, sum payments r less equal sum payments

r , since r redistributes
Pn money r . Moreover , instance
= (a1 , . . ., a1 ), i=1 q(i ) > 0. Finally, definition, q(a1 , . . ., an1 ) = 1.
imply r collectively dominated r individually dominated
r.
2

4. Individually Undominated Mechanisms: Characterization
Algorithmic Techniques General Domains
section, focus individually undominated non-deficit Groves mechanisms.
4.1 Non-deficit Groves Mechanisms
start characterization non-deficit Groves mechanisms.
Recall first
P
type profile , denote V CG() total VCG payment, ni=1 V CGi ().
Proposition 4.1 Groves mechanism r non-deficit ,
X

rj (j
)}
(3)
ri (i ) inf {V CG(i , )


j6=i


reported types agents j replaced .
Here, j

Proof. first prove
direction. , Equation 3 implies
P ri (i )
P




rj (j ) . let = , obtain
V CG(i , )
j rj (j )
j6=i
P
V CG(i , ) = V CGi (). Thus, non-deficit property holds.
prove direction. ToP
ensure non-deficit property, i,
) V CG( , ), equivalently

rj (j
, , must ri (i ) +

j6
=

P
). Since arbitrary, Equation 3 follows.
rj (j
2
ri (i ) V CG(i , )

j6=i

replacing Equation 3 =, get characterization individually
undominated non-deficit Groves mechanisms.
140

fiUndominated Groves Mechanisms

Theorem 4.2 Groves mechanism r non-deficit individually undominated
,
X

ri (i ) = inf {V CG(i , )
rj (j
)}
(4)


j6=i


Here, j
reported types agents j replaced .

Proof. prove direction first. Groves mechanism r satisfies Equation 4
non-deficit Proposition 4.1. suppose r individually dominated, is,
exists another non-deficit Groves mechanism r ,
ri (i ) ri (i ), , ri (i ) > ri (i ).
make inequality strict,
X

)}
rj (j
ri (i ) > ri (i ) = inf {V CG(i , )


j6=i

inf {V CG(i , )


X


rj (j
)},

j6=i

contradicts fact r must satisfy Equation 3. follows r individually undominated.
prove direction. Suppose Equation 4 P
satisfied. Then,
)}. Let

rj (j
exists ri (i ) < inf {V CG(i , )

j6
=

P
)} r ( ) (so > 0), let r r,
rj (j
= inf {V CG(i , )



j6=i

except aforementioned , ri (i ) = ri (i ) + a. show
break non-deficit constraint, consider type vector (i , )
(that is, type profile affected). Then,
X

rj (j
)}
ri (i ) = + ri (i ) = inf {V CG(i , )


= inf {V CG(i , )


j6=i

X


rj (j
)}.

j6=i

r

Thus, Proposition 4.1, non-deficit. contradicts r individually undominated. Hence, Equation 4 must hold.
2
give example individually undominated mechanism.
Example 4.3 Consider single-item auction n 3 agents. Agent bids [0, ).
Let []j jth highest type type profile . Let us consider anonymous
Groves mechanism characterized r(i ) = n1 [i ]2 . is, mechanism,
besides paying VCG payment, every agent receives n1 times second highest
bid. fact, mechanism BC mechanism single-item auctions. show
r individually undominated, suffices show Equation 4 satisfied. first observe
every agent, second highest bid second highest bid,
141

fiGuo, Markakis, Apt, & Conitzer

equals total VCG payment. is, r non-deficit. Hence, Equation 3 holds
agents type profiles. Moreover, every type profile , setting = [i ]2 ,
verify Equation 4 holds. follows BC mechanism individually
undominated single-item auctions.
follows, first show two examples single-item auction scenarios, collective undominance strictly stronger individual undominance non-deficit Groves
mechanisms. propose two techniques generating individually undominated
mechanisms starting known individually dominated mechanisms (if initial mechanism already individually undominated, techniques return mechanism). One technique immediately produces individually undominated mechanism.
However, preserve anonymity. second technique preserves anonymity,
repeated applications result converges individually undominated mechanism.
emphasize start non-deficit Groves mechanism, including BC
mechanism, Worst-Case Optimal mechanism (Guo & Conitzer, 2009), Optimal-inExpectation Linear mechanisms (Guo & Conitzer, 2010), VCG mechanism.
4.2 Collective Undominance Strictly Stronger Individual Undominance
use two examples show collective undominance is, general, strictly stronger
individual undominance.
Example 4.4 Consider single-item auction 4 agents. assume
agent, set allowed types same, namely, integers 0 3. Here, VCG
mechanism second-price auction.
Let us consider following two anonymous non-deficit Groves mechanisms,
computer-generated differentiating collective undominance individual undominance.
Mechanism 1: r(i ) = r([i ]1 , [i ]2 , [i ]3 ), function r given Table 1.
([i ]j jth highest type among types type.)
Mechanism 2: r (i ) = r ([i ]1 , [i ]2 , [i ]3 ), function r given Table 1.
characterization, mechanism
2 collectively dominates
P mechP
anism 1: example, type profile (3, 2, 2, 2), r(i ) = 1/2 < 1 = r (i ).
hand, mechanism 2 individually dominate mechanism 1: example, r(3, 3, 2) = 1 > 5/6 = r (3, 3, 2). fact, based characterization individually
undominated non-deficit Groves mechanisms (Theorem 4.2), able show mechanism 1 individually undominated.
Example 4.5 Consider single-item auction 5 agents. assume
agent, set allowed types [0, ). Here, VCG mechanism second-price
auction.
Let us consider following two anonymous non-deficit Groves mechanisms:
Mechanism 1:
142

fiUndominated Groves Mechanisms

r(0, 0, 0)
r(1, 0, 0)
r(1, 1, 0)
r(1, 1, 1)
r(2, 0, 0)
r(2, 1, 0)
r(2, 1, 1)
r(2, 2, 0)
r(2, 2, 1)
r(2, 2, 2)

0
0
1/4
1/4
0
1/12
0
1/2
0
1/2

r (0, 0, 0)
r (1, 0, 0)
r (1, 1, 0)
r (1, 1, 1)
r (2, 0, 0)
r (2, 1, 0)
r (2, 1, 1)
r (2, 2, 0)
r (2, 2, 1)
r (2, 2, 2)

0
0
1/4
1/4
0
7/24
1/6
1/2
1/4
1/2

r(3, 0, 0)
r(3, 1, 0)
r(3, 1, 1)
r(3, 2, 0)
r(3, 2, 1)
r(3, 2, 2)
r(3, 3, 0)
r(3, 3, 1)
r(3, 3, 2)
r(3, 3, 3)

0
1/4
0
2/3
1
0
2/3
0
1
0

r (3, 0, 0)
r (3, 1, 0)
r (3, 1, 1)
r (3, 2, 0)
r (3, 2, 1)
r (3, 2, 2)
r (3, 3, 0)
r (3, 3, 1)
r (3, 3, 2)
r (3, 3, 3)

0
1/4
1/4
2/3
19/24
1/6
5/6
7/12
5/6
1/2

Table 1: Computer-generated example mechanisms differentiating collective undominance individual undominance.

r(i ) = 0 four types identical.
r(i ) = [i ]1 /4 highest three types identical, strictly
higher lowest type .
r(i ) = [i ]1 /6 highest two types identical, strictly
higher third highest type .
r(i ) = 3[i ]2 /16 highest type strictly higher second highest
type , second highest type identical third highest type .
r(i ) = [i ]2 /5 highest three types different.
Mechanism 2 (BC):
r (i ) = [i ]2 /5.
characterization, mechanism
P 2 collectively dominates
mechanism 1: example,

type
profile
(3,
2,
2,
2,
2),
r(i ) = 4r(3, 2, 2, 2) +
P
r(2, 2, 2, 2) = 3/2 + 0 = 3/2 < r (i ) = 4r (3, 2, 2, 2) + r (2, 2, 2, 2) = 8/5 + 2/5 = 2.
hand, mechanism 2 individually dominate mechanism 1: example,
r(4, 4, 4, 1) = 1 > 4/5 = r (4, 4, 4, 1). fact, based characterization individually undominated non-deficit Groves mechanisms (Theorem 4.2), able show
mechanism 1 individually undominated.
4.3 Priority-Based Technique
Given non-deficit Groves mechanism r priority order agents , improve
r individually undominated mechanism follows:
1) Let : {1, . . . , n} {1, . . . , n} permutation representing priority order.
is, (i) agent priority value (the lower value, higher priority). 1 (k)
agent kth highest priority. high-level idea priority-based
technique go agents one one. first agent (the agent
highest priority), maximize redistribution function subject constraint
143

fiGuo, Markakis, Apt, & Conitzer

Proposition 4.1. later agents, same, take consideration earlier
agents redistribution functions updated. priority order arbitrary.
Generally, agents high priorities benefit technique, since earlier
agents, room improvement.
2) Let = 1 (1), update ri
X

ri (i ) = inf {V CG(i , )



rj (j
)}.

(j)>1

is, update ensures point r satisfies Equation 4 = 1 (1).
noted update, payment agent = 1 (1)
changed, changed
X

rj (j
)} ri (i )
ri (i ) ri (i ) = inf {V CG(i , )


= inf {V CG(i , )


(j)>1

X


rj (j
)} = SiBCGC (i ).

j

is, essentially, update amounts applying BCGC(i) transformation
r, = 1 (1).
3) consider remaining agents turn, according order .
kth step, update ri (i = 1 (k))
X

ri (i ) = inf {V CG(i , )



)
rj (j

X


)}.
rj (j

(j)<k

(j)>k

is, update ensures point r satisfies Equation 4 = 1 (k).
avoid breaking non-deficit property, make update, take previous
k 1 updates account. update, essentially applying
BCGC( 1 (k)) transformation resulting mechanism previous update.
Overall, every agent i,
ri (i ) = inf {V CG(i , )


X


)
rj (j

(j)>(i)

X


rj (j
)}.

(j)<(i)

new mechanism r satisfies following properties:
Proposition 4.6 , ri (i ) ri (i ).
Proof. First consider = 1 (1),
highest priority. ,
Pthe agent



rj (j )}. Since r non-deficit, Equation 3,
ri (i ) = inf {V CG(i , )

j6=i
P
)}. Hence r ( ) r ( ).
rj (j
ri (i ) inf {V CG(i , )





j6=i

144

fiUndominated Groves Mechanisms

6= 1 (1), ri (i ) equals
X

ri (i ) + inf {V CG(i , ) ri (i )


X


rj (j
)

(j)>(i)


rj (j
)}.

(j)<(i)

must show
inf {V CG(i , ) ri (i )



X

X


rj (j
)

(j)>(i)


rj (j
)} 0.

(5)

(j)<(i)

Consider p = 1 ((i) 1) (the agent immediately terms priority).
, ,
X
X
V CG(i , ) ri (i )
rj (j )
rj (j )
(j)>(i)

= V CG(i , )

X

rj (j )

X


rj (j
)

(j)>(p)

inf {V CG(p , p )
p p

(j)<(i)

X

rj (j ) rp (p )

X


rj (j
)} rp (p ) = 0.

(j)<(p)

(j)>(p)

(j)<(p)

set types reported agents j,
inequality, j
p

replaced p . arbitrary, Equation 5 follows. Therefore, ri (i ) ri (i )
.
2

Proposition 4.7 r individually undominated.
Proof. Let = 1 (n). ,
X
X

rj (j
)} ri (i ) = 0.
V CG()
rj (j ) inf {V CG(i , )


j=1,...,n

j6=i

Hence r never incurs deficit. So, r non-deficit.
Using Proposition 4.6, ,
X

rj (j
)
ri (i ) = inf {V CG(i , )


(j)>(i)

inf {V CG(i , )


X

X


rj (j
)}

(j)<(i)


rj (j
)}.

j6=i

r non-deficit, opposite inequality must also satisfied (Equation 3)hence
must equality, is, Equation 4 must hold. follows r individually
undominated.
2
noted technique, updates, need keep
track value ri (i ) . is, due space complexity,
technique suitable cases agents possible types. reduce
145

fiGuo, Markakis, Apt, & Conitzer

space complexity, update, could also recompute earlier updates recursive
fashion. so, later updates much difficult compute compared
earlier updates. Fortunately, earlier updates tend important,
generally room improvement earlier updates. Therefore,
reasonable approximation would update high-priority agents ignore
remaining agents low priorities.
4.4 Iterative Technique Preserves Anonymity
previous technique will, general, produce anonymous mechanism, even
input mechanism anonymous. agents higher priority order tend
benefit technique. Here, introduce another technique preserves
anonymity.
Given anonymous mechanism r, let r0 = r. , let
rk+1 (i ) =

X
n1 k
1


rk (j
)}.
r (i ) +
inf
{V
CG(
,

)



n
n
j6=i

easily seen induction rk mechanisms anonymous. rk anonymous, (n 1), rk ((i ) ) = rk (i ) i. also
V CG(i , (i ) ) = V CG(i , ) , , i. Finally, let ((i ) , )
type profile
permuted according , replaced
P types P
) , i, .
. j6=i rk (((i ) , )j ) = j6=i rk (j

implies rk+1 also permutation independent, thus anonymous.
noted rk rk+1 , agent payment changed

=
=
=

n1 k
n r (i )

+
1
n

1
n

rk+1 (i ) rk (i )
P k
, )
r (j )} rk (i )
inf
{V
CG(






inf {V



j6=i

CG(i , )



P

j
1 BCGC
(i ).
n Si

)}
rk (j

is, essentially, rk+1 resulting mechanism applying BCGC transform
rk .
next propositions immediately follow Proposition 2.3:
Proposition 4.8 r0 non-deficit, rk non-deficit k.
Proposition 4.9 , rk (i ) nondecreasing k.
Proposition 4.10 rk+1 = rk , rk individually undominated.
Proposition 4.11 rk individually undominated, rk+1 individually dominates
rk .
Finally, following proposition establishes convergence.
146

fiUndominated Groves Mechanisms

Proposition 4.12 k , rk converges (pointwise) individually undominated
mechanism.
Proof. Proposition 4.9, rk (i ) nondecreasing k, since every rk nondeficit Proposition 4.8, must bounded; hence must converge (pointwise).
, let
X

rk (j
)} rk (i ).
dk = inf {V CG(i , )


j6=i

Using Proposition 4.9, derive following inequality:
X

rk+1 (j
)} rk+1 (i )
dk+1 = inf {V CG(i , )


j6=i

inf {V CG(i , )


X


rk (j
)} rk+1 (i )

j6=i

= inf {V CG(i , )


=

X


rk (j
)}

j6=i

X
n1 k
1


r (i )
inf
{V
CG(
,

)

rk (j
)}


n
n
j6=i

=

X
n1 k
n1
n1


{V
CG(
,

)

rk (j
)}
inf
r (i ) =
dk .


n
n
n
j6=i

k , dk = inf {V CG(i , )


P

j6=i

)} r k ( ) 0. limit, Equark (j


tion 4 satisfied. Thus, rk converges (pointwise, linearly) individually undominated
mechanism.
2
Similar priority-based technique, iterative process, computing
rk , need value rk1 (i ), . is, due space complexity,
technique suitable cases agents possible types.
reduce space complexity could also recompute rk1 recursive fashion.
so, rk becomes much difficult compute large values k. Fortunately, earlier
iterative steps crucial, generally room improvement
earlier steps. Therefore reasonable approximation would compute
iterative steps.

5. Multi-Unit Auctions Unit Demand
section, consider auctions multiple identical units single
good agents unit demand, i.e., agent wants one unit (if
single unit good, simply standard single-item auction). focus
notion collectively undominated mechanisms relates individually
undominated mechanisms. particular, first obtain analytical characterization
147

fiGuo, Markakis, Apt, & Conitzer

collectively undominated Groves mechanisms non-deficit, anonymous,
linear payment functions, proving Optimal-in-Expectation Linear redistribution
mechanisms (OEL) (Guo & Conitzer, 2010), include BC mechanism,
collectively undominated Groves mechanisms anonymous linear. show
individual undominance collective undominance equivalent restrict
consideration Groves mechanisms anonymous linear setting multiunit auctions unit demand. Note even single-item auctions, examples
given Section 4.2 show equivalence hold restrict
linear anonymous mechanisms.
one mechanism collectively dominates another mechanism, first mechanism, agents expected total utility, prior distribution agents
valuations, must less second mechanism, strictly higher minimal conditions prior distribution. Therefore, good direction
look collectively undominated mechanisms start mechanisms
optimal-in-expectation.
Optimal-in-Expectation Linear (OEL) redistribution mechanisms (Guo & Conitzer,
2010), described below, special cases non-deficit Groves mechanisms anonymous linear. OEL mechanisms defined multi-unit auctions unit
demand. unit demand multi-unit auction, indistinguishable units sale,
agent interested one unit. agent i, type valuation
winning one unit. assume bids (announced types) bounded L
U , i.e., = [L, U ] (note L 0).
linear anonymous Groves mechanism characterized function r foln1
P
cj [i ]j (where [i ]j jth highest bid among ).
lowing form: r(i ) = c0 +
j=1

OEL mechanisms, cj chosen according one following options (indexed
integer parameter k, k ranges 0 n, k odd):
k = 0:




ni1
m1
ci = (1)
/
= 1, . . . , m,
nm1
i1




X
m1
mi n 1
(1)
c0 = U m/n U
/
,
nm1
i1
mi

i=1

ci = 0 values i.
k = 1, 2, . . . , m:




ni1
m1
ci = (1)
/
= k + 1, . . . , m,
nm1
i1




X
m1
mi n 1
(1)
ck = m/n
/
,
nm1
i1
mi

i=k+1

ci = 0 values i.
148

fiUndominated Groves Mechanisms

k = + 1, + 2, . . . , n 1:
ci = (1)

mi1



nm1
i1
= + 1, . . . , k 1,
/
ni1
m1



ck = m/n

k1
X

(1)

mi1

i=m+1





i1
nm1
/
,
m1
ni1

ci = 0 values i.
k = n:
ci = (1)mi1





i1
nm1
/
= + 1, . . . , n 1,
m1
ni1

c0 = Lm/n L

n1
X

(1)

mi1

i=m+1



nm1
i1
,
/
ni1
m1



ci = 0 values i.
example, k = + 1, cm+1 = m/n ci = 0 i.
specific OEL mechanism, r(i ) =
n [i ]m+1 . is, besides participating VCG
mechanism, every agent also receives amount equal m/n times (m + 1)th
highest bid agents. Actually, exactly BC mechanism multi-unit
auctions unit demand.
Besides non-deficit, one property OEL mechanisms always
budget balanced following scenarios.
[]1 = U k = 0
[]k+1 = []k k {1, . . ., n 1}
[]n = L k = n
Using property, prove OEL mechanisms collectively
undominated non-deficit Groves mechanisms anonymous linear.
first show OEL mechanisms collectively undominated.
Theorem 5.1 multi-unit auctions unit demand, non-deficit Groves
mechanism collectively dominates OEL mechanism.
using Lemma 3.3, need prove case anonymous Groves
mechanisms.
Lemma 5.2 multi-unit auctions unit demand, non-deficit anonymous
Groves mechanism collectively dominates OEL mechanism.
149

fiGuo, Markakis, Apt, & Conitzer

Proof. first prove: OEL mechanism index k {1, . . ., n 1} collectively
dominated non-deficit anonymous Groves mechanism.
Suppose non-deficit anonymous Groves mechanism r collectively dominates OEL
mechanism index k {1, . . ., n 1}. use rOEL denote OEL mechanism.
, define following function:
(i ) = r(i ) rOEL (i ).
P
Since r collectively dominates rOEL , , ni=1 (i ) 0.
also that, whenever []k+1 = []k , OEL mechanism budget balanced.
OEL , agents total payment 0; case, since r non-deficit,
is,
Pn r
must i=1 (i ) = 0.
claim (i ) = 0 . Let C(i ) number bids among
equal [i ]k . Hence, must show C(i ) 1,
(i ) = 0.
prove induction value C(i ) (backwards, n 1 1).
Base case: C(i ) = n 1.
Suppose C(i ) = n 1. is, bids identical.
also equal bids , allPbids []k+1 = []k .
Hence, earlier observation, nj=1 (j ) = 0. know j,
j set bids. Hence (i ) = 0 C(i ) = n 1.
Induction step.
Let us assume , C(i ) p (where p {2, . . ., n 1}), (i ) = 0.
consider
P C(i ) = p 1. equal [i ]k , []k =
[]k+1 , implies nj=1 (j ) = 0. j j = [i ]k , (
Pn j ) = (i ),
j, C(j ) = p. Therefore, induction assumption, j=1 (j )
positive multiple (i ), implies (i ) = 0.
induction, shown (i ) = 0 . implies r
rOEL identical. Hence, non-deficit anonymous Groves mechanism collectively
dominates OEL mechanism index k {1, . . ., n 1}.
prove: OEL mechanism index k = 0 collectively dominated
different non-deficit anonymous Groves mechanism.
Suppose non-deficit anonymous Groves mechanism r collectively dominates OEL
mechanism index k = 0. use rOEL denote OEL mechanism.
, define following function:
(i ) = r(i ) rOEL (i ).
P
Since r collectively dominates rOEL , , ni=1 (i ) 0. also
OEL , agents total payment 0; case,
that, whenever []1 = U , rP
r non-deficit, must ni=1 (i ) = 0.
claim (i ) = 0 . Let C(i ) number bids among
equal U . Hence, must show C(i ) 0, (i ) = 0.
prove induction value C(i ) (backwards, n 1 0).
150

fiUndominated Groves Mechanisms

Base case: C(i = n 1.
Suppose C(i ) = n 1. is, bids
Pnare equal U .
also equal bids U , earlier observation, j=1 (j ) = 0.
know j, (j ) value. Hence (i ) = 0
C(i ) = n 1.
Induction step.
Let us assume , C(i ) p (where p {2, . . ., n 1}), (i ) = 0.
consider
Pi C(i ) = p 1. equal U , []1 = U ,
implies nj=1 (j ) = 0. j j = U , (
Pnj ) = (i ),
j, C(j ) = p. Therefore, induction assumption, j=1 (j ) positive
multiple (i ), implies (i ) = 0.
induction, shown (i ) = 0 . implies r
rOEL identical. Hence, non-deficit anonymous Groves mechanism collectively
dominates OEL mechanism index k = 0.
remains prove: OEL mechanism index k = n collectively dominated different non-deficit anonymous Groves mechanism.
case similar case k = 0 omit here.
2
proceed show within family anonymous linear non-deficit
Groves mechanisms, OEL mechanisms ones collectively undominated. Actually, also ones individually undominated,
stronger claim since individually undominated weaker property.
Theorem 5.3 multi-unit auctions unit demand, anonymous linear nondeficit Groves mechanism individually undominated, must OEL mechanism.
proving theorem, let us introduce following lemma.
Lemma 5.4 Let set points (s1 , s2 , . . . , sk ) (U s1 s2 . . . sk L)
satisfy Q0 + Q1 s1 + Q2 s2 + . . . + Qk sk = 0 (the Qi constants). measure
positive (Lebesgue measure Rk ), Qi = 0 i.
Proof. Qi 6= 0 i, U s1 s2 . . . si1 si+1 . . . sk L,
make Q0 + Q1 s1 + Q2 s2 + . . . + Qk sk = 0, si take one value. result
measure must 0.
2
ready prove Theorem 5.3.
Proof. Let r non-deficit anonymous linear Groves mechanism. recall
Groves mechanism anonymous linear r linear function defined r(i ) =
n1
P
aj [i ]j (where [i ]j jth highest type among , aj constants).
a0 +
j=1

multi-unit auctions unit demand, total VCG payment equals m[]m+1
(m times (m + 1)th bid). r, agents total payment equals
m[]m+1

n
X

r(i ) = m[]m+1 na0

n n1
X
X
i=1 j=1

i=1

151

aj [i ]j .

fiGuo, Markakis, Apt, & Conitzer

total payment linear function terms types among . simplicity,
rewrite total payment C0 + C1 []1 + C2 []2 + . . . + Cn []n . Ci constants
determined ai .
C0 = na0
C1 = (n 1)a1
C2 = a1 (n 2)a2
C3 = 2a2 (n 3)a3
..
.
Cm = (m 1)am1 (n m)am
Cm+1 = mam (n 1)am+1 +
Cm+2 = (m + 1)am+1 (n 2)am+2
..
.
Cn1 = (n 2)an2 an1
Cn = (n 1)an1
Given , possible value , must
is, , inf

n
P

i=1

n
P

ti () 0 (non-deficit).

i=1

ti () 0. , inf

n
P

i=1

ti () >

( > 0), reduce payment agent without violating non-deficit
constraint, agents types . Therefore, mechanism individually
n
P
ti () = 0.
undominated, , inf
i=1

denote [i ]j sj (j = 1, . . . , n 1). is, s1 s2 . . . sn1 .
n
P
expression inf
ti () equals minimum following expressions:
i=1

inf

Li sn1

n
X

ti ()

i=1

inf

sn1 sn2

n
X

ti ()

i=1

..
.

inf

s2 s1

inf

s1 U

n
X

ti ()

n
X

ti ()

i=1

i=1

152

fiUndominated Groves Mechanisms

take closer look

inf

n
P

Li sn1 i=1

ti (). L sn1 , jth highest type

[]j = sj j = 1, . . . , n 1, nth highest type []n = (this case corresponds
agent agent lowest type).
inf

Li sn1

n
X

ti () =

i=1

inf

Li sn1

(C0 + C1 s1 + C2 s2 + . . . + Cn1 sn1 + Cn )

= min{C0 + C1 s1 + . . . + Cn1 sn1 + Cn L, C0 + C1 s1 + . . . + Cn1 sn1 + Cn sn1 }.
is, expression linear, minimum reached set either
lower bound L upper bound sn1 .
Similarly,
inf

sn1 sn2

n
X

ti () = min{C0 + C1 s1 + . . . + Cn2 sn2 + Cn1 sn1 + Cn sn1 ,

i=1

C0 + C1 s1 + C2 s2 + . . . + Cn2 sn2 + Cn1 sn2 + Cn sn1 },
..
.
inf

s2 s1

n
X

ti () = min{C0 + C1 s1 + C2 s1 + C3 s2 + . . . + Cn sn1 ,

i=1

C0 + C1 s1 + C2 s2 + C3 s2 + . . . + Cn sn1 },
inf

s1 U

n
X

ti () = min{C0 + C1 U + C2 s1 + . . . + Cn sn1 ,

i=1

C0 + C1 s1 + C2 s1 + . . . + Cn sn1 }.
Putting together, U s1 s2 . . . sn1 L,
minimum following expressions 0.
(n): C0 + C1 s1 + C2 s2 + . . . + Cn1 sn1 + Cn L
(n 1): C0 + C1 s1 + C2 s2 + . . . + Cn1 sn1 + Cn sn1
(n 2): C0 + C1 s1 + C2 s2 + . . . + Cn2 sn2 + Cn1 sn2 + Cn sn1
.
..
(2): C0 + C1 s1 + C2 s2 + C3 s2 + . . . + Cn sn1
(1): C0 + C1 s1 + C2 s1 + C3 s2 + . . . + Cn sn1
(0): C0 + C1 U + C2 s1 + C3 s2 + . . . + Cn sn1
153

fiGuo, Markakis, Apt, & Conitzer

expressions numbered 0 n. Let I(i) set points (s1 , . . . , sn1 )
(U s1 s2 . . . sn1 L) make expression (i) equal 0. must exist
least one measure I(i) positive. According Lemma 5.4, expression
(i) must constant 0.
expression (0) constant 0, total payment r 0 whenever highest
type equal upper bound U . is, , total payment C0 + C1 []1 +
C2 []2 + . . . + Cn []n must constant multiple U []1 (the total payment linear
function). C0 = U C1 Cj = 0 j 2. turns equalities
Cj completely determine values aj (the values aj solved
based Cj pure algebraic manipulations), corresponding mechanism
OEL mechanism index k = 0. expression (i) constant values i,
corresponding mechanism OEL mechanism another index.
2
Hence, following complete characterization context.
Corollary 5.5 multi-unit auctions unit demand, non-deficit anonymous linear
Groves mechanism individually / collectively undominated OEL
mechanism.
Proof. corollary proved combining Theorem 5.1 Theorem 5.3, well
fact collectively undominated mechanism also individually undominated. 2
corollary also shows consider Groves mechanisms
non-deficit, anonymous, linear setting multi-unit auctions unit demand,
individual undominance collective undominance equivalent. Thus,
characterized individually/collectively undominated Groves mechanisms nondeficit, anonymous, linear multi-unit auctions unit demand.

6. Public Project Problem
study well known class decision problems, namely public project problems (see,
e.g., Mas-Colell et al., 1995; Moulin, 1988; Moore, 2006). setting set n agents
needs decide financing project cost c. agents type private valuation
project takes place. consider two versions problem.
6.1 Equal Participation Costs
case project takes place, agent contributes share, c/n,
cover total cost. Hence participation costs agents same.
problem defined follows.
Public project problem
Consider (D, 1 , . . ., n , v1 , . . ., vn ),
= {0, 1} (reflecting whether project canceled takes place),
{1, . . ., n}, = [0, c], c > 0,
154

fiUndominated Groves Mechanisms

{1, . . ., n}, vi (d, ) := d(i nc ),
agents employ payment-based mechanism decide project,
addition c/n, agent also pay receive payment, ti (), imposed
mechanism. result Holmstrom (1979), efficient strategyproof payment-based mechanisms domain P
Groves mechanisms.
PnTo determine
efficient outcome given type vector , note ni=1 vi (d, P
) = d( i=1 c). Hence
efficiency mechanism (f, t) means f () = 1 ni=1 c f () = 0
otherwise, i.e., project takes place declared total value agents
project exceeds cost.
first observe following result.
Proposition 6.1 public project problem equal participation costs, BC mechanism coincides VCG.
Proof. suffices check equation (1) holds SiBCGC (i ) = 0
. Since VCG non-deficit mechanism, SiBCGC (i ) 0, term
SiBCGC (i ) sum payments type vector. Hence need show
value
makes expression (1) equal 0. Checking quite
P


simple. j6=i j < n1
n c, take := 0 otherwise := c. former case
efficient outcome implement project whereas latter case, opposite
occurs. easy check cases SiBCGC (i ) = 0.
2
show fact VCG cannot improved upon. stating result,
would
Plike note one ideally would like mechanism budget-balanced,
i.e., ti () = 0 , total agents pay cost project
more. However possible, since public project problem, mechanism
exists efficient, strategy-proof, budget balanced (Mas-Colell et al., 1995).
theorem considerably strengthens result, showing VCG optimal
respect minimizing total payment agents.
Theorem 6.2 public project problem exists non-deficit Groves mechanism
collectively dominates VCG mechanism.
case unit-demand auctions, first establish desired conclusion
anonymous Groves mechanisms extend arbitrary ones Lemma 3.3. Notice
VCG anonymous setting hence apply Lemma 3.3(ii).
Lemma 6.3 public project problem exists anonymous non-deficit Groves
mechanism collectively dominates VCG mechanism.
Proof. Suppose anonymous non-deficit Groves mechanism (r1 , ..., rn ) exists
collectively dominates VCG. anonymity, {1, . . ., n} ri = r, function
r : [0, c]n1 R. Hence
n
X
[0, c]n
r(i ) 0
(6)
i=1

155

fiGuo, Markakis, Apt, & Conitzer

show x [0, c]n1 , r(x) = 0 thus r coincides VCG.
divide proof two cases.
n1
X
n1
xi
Case 1: vector x satisfies
c.
n
i=1
Given x, define C(x) = |{i : xi = c}|, i.e., given vector x n 1 types, C(x)
number agents submitted c. Define following predicate:
P (k) : x [0, c]n1 ((C(x) = k

n1
X
i=1

xi

n1
c) r(x) = 0)
n

prove P (k) holds k {0, . . ., n 1}, using induction (going backwards n 1). Let ti () = V CGi () r(i ) payment function agent
mechanism r.
Base case.
Let x C(x) = n 1. Consider := (c, . . ., c) [0, c]n .
{1, . . ., n}, = x. Clearly f () = 1 agent paying anything VCG
mechanism instance, i.e., V CGi () = 0.
Since r non-deficit mechanism
n
n
n
n
X
X
X
X
r(i ) = nr(x),
r(i ) =
V CGi ()
ti () =
0
i=1

i=1

i=1

i=1

(6) r(x) = 0.
Induction step.
Assume P (k) holds k 1. prove P (k 1). Let x
C(x) = k 1 (note x may zero cs). Since r permutation independent,
assume without loss generality elements x sorted descending order (i.e.,
r(x) change reordering). Consider type vector = (c, x),
concatenation (c) x. Hence starts k cs rest like rest x. Note
{1, . . ., k}, = x C(i ) = k 1. {k
1, . . ., n}, C(i ) = k,
P+
n
therefore induction hypothesis, r(i ) = 0. means i=1 r(i ) = kr(x).
Furthermore, f () = 1 since least one c, agent paying payment
VCG mechanism. see this, k 2, every agent , another
agent submitted c hence agent
pivotal. k = 1, agent alter
P
decision outcome fact
xi n1
n c, hence agent pivotal case
well. Thus, {1, . . ., n}, V CGi () = 0, r non-deficit
0

n
X

ti () =

n
X

r(i ) = kr(x)

i=1

i=1

(6) r(x) = 0. concludes induction step consequently
r(x) = 0 vectors x belong Case 1.
n1
X

n1
c. proof case uses completely
n
i=0
symmetric argument Case 1. include sake completeness.
Case 2: vector x satisfies

xi <

156

fiUndominated Groves Mechanisms

Define C (x) = |{i : xi = 0}|. analogy predicate P (k) Case 1, define
following predicate:
P (k) : x [0, c]n1 ((C (x) = k

n1
X

xi <

i=0

prove
wards n 1).

P (k)

n1
c) r(x) = 0)
n

holds k {0, ..., n 1}, using induction (going back-

Base case.
Let x C (x) = n 1, i.e., zero vector. Consider := (0, ..., 0) [0, c]n .
{1, . . ., n}, = x. Clearly f () = 0 agent payingP
anything

ti () =
P VCG mechanism. Hence ti () payment paid agent i,
r(i ) = nr(x).
Since r non-deficit mechanism,
0

n
X

ti () = nr(x)

i=1

(6) implies r(x) = 0.
Induction step.
Suppose P (k) holds k 1. prove P (k 1). Let x
C (x) = k 1. Since r permutation independent, assume without loss generality
elements x sorted increasing order 0s left side
x (note may also x 0s, since k 1 maybe equal 0).
Consider type vector = (0, x). starts k 0s rest like rest x.
Note {1, . . ., k}, = x C (i ) = kP
1. {k + 1, . . ., n}, C (i ) = k
induction hypothesis, r(i ) = 0 hence
r(i ) = kr(x).
note f () = 0 also agent paying payment VCG
mechanism. see this, enough P
verify agent pivotal, follows
n1
fact case i=0
xi < n1
n c. Since = (0, x), agent
pivotal.P Therefore V CGi () = 0 every {1, . . ., n}. Since r non-deficit
0 r(i ) = kr(x). (6) r(x) = 0.
completes proof induction step hence Case 2. Since Cases 1 2
cover vectors x [0, c]n1 , proof Lemma complete.
2
using Lemma 6.3 Lemma 3.3(ii), proof Theorem 6.2 complete.
interesting open question whether mechanisms share properties VCG mechanism also collectively undominated. particular,
exhibited VCG pay-only anonymous mechanism. anonymous
pay-only mechanisms collectively undominated public project problem
equal participation costs?
start pay-only mechanisms. provide general observation holds
many domains public project problems, showing VCG mechanism dominates pay-only mechanisms.
157

fiGuo, Markakis, Apt, & Conitzer

Lemma 6.4 Let r Groves mechanism. Suppose following condition
{1, . . ., n}:

11

holds

bi V CGi (bi , ) ri (i ) = 0.
r individually dominates pay-only Groves mechanisms.
condition essentially says every agent always able make payment equal
0 type vector agents.
Proof. Suppose exists pay-only mechanism r = (r1 , . . ., rn ) different
r = (r1 , . . ., rn ) dominated r. Then, {1, . . .n}, ri (i ) >
ri (i ). Let bi type agent satisfies condition theorem. Consider
= (bi , ). V CGi ( ) = ri (i ).
payment agent mechanism r profile
ti ( ) = V CGi ( ) ri (i ) < V CGi ( ) ri (i ) = 0,
contradiction, r pay-only mechanism.

2

Theorem 6.5 Consider public project problem equal participation costs.
pay-only Groves mechanism r, following equivalent:
1. r individually undominated,
2. r VCG mechanism,
3. r collectively undominated.
Proof. 1 2. Consider pay-only individually undominated Groves mechanism r.
claim r VCG mechanism.
considered domain every agent i, given , force VCG payment 0
declaring bi = c/n. Indeed, would V CGi (c/n, ) = 0. Hence Lemma
6.4 VCG mechanism individually dominates pay-only mechanisms. means
individually undominated mechanism VCG.
2 3 holds Theorem 6.2 3 1 holds definition.

2

theorem shows public project problem equal participation
costs, VCG pay-only Groves mechanism individually/collectively undominated. Appendix A, show similar result anonymous Groves mechanisms,
case two agents. is, exactly two agents, VCG
anonymous Groves mechanism individually/collectively undominated. Further,
n 3, Herve Moulin (private communication) observed public project problems equal participation costs, VCG mechanism non-deficit Groves
mechanism collectively undominated.
11. slight generalization Potential Universal Relevance Nullification (PURN) condition
introduced Cavallo (2006). agent satisfies PURN make payment VCG
mechanism equal 0 type vector agents. Here, difference
consider Groves mechanisms instead VCG.

158

fiUndominated Groves Mechanisms

6.2 General Case
assumption made far public project problem agents
cost share may always realistic. Indeed, may argued richer
agents (such larger enterprises) contribute more. matter modify
formulation problem appropriately? answer yes. First, let us formalize
version problem. assume initial utility function form
vi (d, ) := d(i ci ),
P
{1, . . ., n}, ci > 0 ni=1 ci = c.
setting, ci share project cost financed agent i. call
resulting problem general public project problem. taken Moore (2006).
problem two results, concerning individual dominance relation.
Theorem 6.6 general public project problem VCG mechanism individually dominates pay-only Groves mechanisms.
Proof. Note , agent force VCG payment 0
declaring ci , since ti (ci , ) = 0. Lemma 6.4 proof complete.
2
theorem cannot extended non-deficit Groves mechanisms, illustrated following theorem. theorem also shows
individually undominated mechanism setting, cannot pay-only mechanism.
Theorem 6.7 n 3, instance general public project problem n
agents exists BC mechanism individually dominates VCG mechanism.
Proof. show n = 3. n > 3, fairly simple extend proof.
omit details. VCG mechanism non-deficit, hence suffices show
Proposition 2.3(ii) VCG BC mechanisms coincide, choice
c, c1 , c2 , c3 , c1 + c2 + c3 = c.
end need find 2 3 S1BCGC (2 , 3 ) > 0.
((R1 + R2 + R3 ) L),
S1BCGC (2 , 3 ) := min

1 1

:= (1 , 2 , 3 )
L := (n 1)

n
X

vk (f ( ), k ),

k=1

R1 = max

X

vj (d, j ) = max{0, 2 + 3 (c2 + c3 )},

R2 = max

X

vj (d, j ) = max{0, 1 + 3 (c1 + c3 )},

R3 = max

X

vj (d, j ) = max{0, 1 + 2 (c1 + c2 )}.

dD

dD

dD

j6=1

j6=2

j6=3

159

fiGuo, Markakis, Apt, & Conitzer

Now, take c = 100, c1 = 10, c2 = 40, c3 = 50 2 := 10, 3 := 70. R1 +R2 +R3 =
1 + 10 + max{0, 1 40}. Two cases arise.
Case 1 f ( ) = 0.
L = 0, (R1 + R2 + R3 ) L 10.
Case 2 f ( ) = 1.
L = 2(1 + 2 + 3 100) = 21 40,
(R1 + R2 + R3 ) L = 50 1 + max{0, 1 40}
(50 1 ) + (1 40) 10.
proves S1BCGC (2 , 3 ) 10. taking 1 [40, 100] see fact
= 10.
2

S1BCGC (2 , 3 )

virtue Theorem 6.6 BC mechanism proof pay-only.

7. Conclusions Future Work
family Groves mechanisms, includes well-known VCG mechanism (also
known Clarke mechanism), family efficient strategy-proof mechanisms.
Unfortunately, Groves mechanisms generally budget balanced. is,
mechanisms, payments may flow system agents, resulting
deficits reduced utilities agents. identify non-deficit Groves mechanisms
give agents highest utilities, introduced two general measures comparing
mechanisms prior-free settings. Specifically, say non-deficit Groves mechanism
individually dominates another non-deficit Groves mechanism every type profile, every agents utility less , holds strict
inequality least one type profile one agent. say non-deficit Groves
mechanism collectively dominates another non-deficit Groves mechanism every
type profile, agents total utility (social welfare) less
, holds strict inequality least one type profile. definitions
induce two partial orders non-deficit Groves mechanisms. paper mainly focused
studying maximal elements corresponding two partial orders.
number interesting open problems remain. Specifically,
provided Section 4.2 two examples showing collective undominance
strictly stronger individual undominance. One example involves discrete type
space, example involves discontinuous redistribution functions.
remains seen whether two definitions undominance coincide
type space smoothly connected redistribution functions continuous.
know Guo Conitzer (2010) OEL mechanisms
collectively undominated mechanisms multi-unit auctions unit demand, exist prior distributions mechanisms achieve strictly
higher expected social welfare. is, multi-unit auctions unit demand,
exist unknown collectively undominated mechanisms (based nonlinear
redistribution functions). However, remains seen whether also exist
collectively undominated mechanisms (other VCG) public project problems.
160

fiUndominated Groves Mechanisms

proposed two techniques generating individually undominated mechanisms.
also derive techniques generating collectively undominated mechanisms?
Acknowledgments
authors would like thank three reviewers useful comments. also
thank Herve Moulin valuable discussions. work supported project
DIACODEM Dutch organization scientific research (NWO), project
AGT research funding program THALIS (co-financed European Social FundESF Greek national funds). also thank National Science Foundation
Alfred P. Sloan Foundation support Awards IIS-0812113, IIS-0953756, CCF1101659, Sloan Fellowship.

Appendix A. Uniqueness VCG Case Two Agents
Theorem A.1 Consider public project problem equal participation costs.
number agents n = 2, non-deficit, anonymous Groves mechanism
r, following equivalent:
1. r individually undominated,
2. r VCG mechanism,
3. r collectively undominated.
Proof. proof Theorem 6.5 suffices show 1 2. take non-deficit,
anonymous, individually undominated Groves mechanism, determined function
r.
x [0, c], take := (x, x). x c/2, efficient outcome f () = 1
agent pivotal, hence total VCG payment 0. x < c/2, project
built agent pivotal. Hence cases VCG payment 0.
payment function corresponding r, t1 () + t2 () = 2r(x). Since
r non-deficit, every x [0, c], r(x) 0. since r individually
undominated, cannot case r(x) < 0 x, VCG
mechanism would dominate r. Hence r coincides VCG mechanism.
2

References
Apt, K., Conitzer, V., Guo, M., & Markakis, E. (2008). Welfare undominated Groves mechanisms. Proceedings Fourth Workshop Internet Network Economics
(WINE), pp. 426437, Shanghai, China.
Bailey, M. J. (1997). demand revealing process: distribute surplus. Public Choice,
91, 107126.
Cavallo, R. (2006). Optimal decision-making minimal waste: Strategyproof redistribution VCG payments. Proceedings International Conference Autonomous
Agents Multi-Agent Systems (AAMAS), pp. 882889, Hakodate, Japan.
161

fiGuo, Markakis, Apt, & Conitzer

Clarke, E. H. (1971). Multipart pricing public goods. Public Choice, 11, 1733.
Cramton, P., Gibbons, R., & Klemperer, P. (1987). Dissolving partnership efficiently.
Econometrica, 55 (3), 615632.
de Clippel, G., Naroditskiy, V., & Greenwald, A. (2009). Destroy save. Proceedings
ACM Conference Electronic Commerce (EC), pp. 207214, Stanford, CA,
USA.
Faltings, B. (2005). budget-balanced, incentive-compatible scheme social choice.
Agent-Mediated Electronic Commerce (AMEC), LNAI, 3435, pp. 3043.
Groves, T. (1973). Incentives teams. Econometrica, 41, 617631.
Gujar, S., & Narahari, Y. (2011). Redistribution mechanisms assignment heterogeneous objects. J. Artif. Intell. Res. (JAIR), 41, 131154.
Guo, M. (2011). VCG redistribution gross substitutes. Proceedings National
Conference Artificial Intelligence (AAAI), San Francisco, CA, USA.
Guo, M. (2012). Worst-case optimal redistribution VCG payments heterogeneousitem auctions unit demand. Proceedings Eleventh International Joint
Conference Autonomous Agents Multi-Agent Systems (AAMAS), Valencia,
Spain.
Guo, M., & Conitzer, V. (2008a). Better redistribution inefficient allocation multiunit auctions unit demand. Proceedings ACM Conference Electronic
Commerce (EC), pp. 210219, Chicago, IL, USA.
Guo, M., & Conitzer, V. (2008b). Undominated VCG redistribution mechanisms. Proceedings Seventh International Joint Conference Autonomous Agents
Multi-Agent Systems (AAMAS), pp. 10391046, Estoril, Portugal.
Guo, M., & Conitzer, V. (2009). Worst-case optimal redistribution VCG payments
multi-unit auctions. Games Economic Behavior, 67 (1), 6998.
Guo, M., & Conitzer, V. (2010). Optimal-in-expectation redistribution mechanisms. Artificial Intelligence, 174 (5-6), 363381.
Guo, M., Naroditskiy, V., Conitzer, V., Greenwald, A., & Jennings, N. R. (2011). Budgetbalanced nearly efficient randomized mechanisms: Public goods beyond.
Proceedings Seventh Workshop Internet Network Economics (WINE),
Singapore.
Holmstrom, B. (1979). Groves scheme restricted domains. Econometrica, 47 (5), 1137
1144.
Laffont, J., & Maskin, E. (1997). theory incentives: overview, in: W. Hildenbrand,
ed., Advances economics theory, Econometric Society Monograph Quantitative
Economics. Cambridge University Press.
Mas-Colell, A., Whinston, M., & Green, J. R. (1995). Microeconomic Theory. Oxford
University Press.
Moore, J. (2006). General Equilibrium Welfare Economics: Introduction. Springer.
Moulin, H. (1988). Axioms Cooperative Decision Making. Cambridge University Press.
162

fiUndominated Groves Mechanisms

Moulin, H. (1986). Characterizations pivotal mechanism. Journal Public Economics, 31 (1), 5378.
Moulin, H. (2009). Almost budget-balanced VCG mechanisms assign multiple objects.
Journal Economic Theory, 144 (1), 96119.
Myerson, R., & Satterthwaite, M. (1983). Efficient mechanisms bilateral trading. Journal
Economic Theory, 28, 265281.
Porter, R., Shoham, Y., & Tennenholtz, M. (2004). Fair imposition. Journal Economic
Theory, 118, 209228.

163

fiJournal Artificial Intelligence Research 46 (2012) 47-87

Submitted 07/12; published 01/13

Optimal Rectangle Packing:
Absolute Placement Approach
Eric Huang

ehuang@parc.com

Palo Alto Research Center
3333 Coyote Hill Road
Palo Alto, CA 94304 USA

Richard E. Korf

korf@cs.ucla.edu

UCLA Computer Science Department
4532E Boelter Hall
University California, Los Angeles
Los Angeles, CA 90095-1596 USA

Abstract
consider problem finding enclosing rectangles minimum area
contain given set rectangles without overlap. rectangle packer chooses xcoordinates rectangles y-coordinates. transform
problem perfect-packing problem empty space adding additional rectangles. determine y-coordinates, branch different rectangles
placed empty position. packer allows us extend known solutions
consecutive-square benchmark 27 32 squares. also introduce three new benchmarks, avoiding properties make benchmark easy, rectangles shared
dimensions. third benchmark consists rectangles increasingly high precision.
pack efficiently, limit rectangles coordinates bounding box dimensions
set subset sums rectangles dimensions. Overall, algorithms represent
current state-of-the-art problem, outperforming algorithms orders
magnitude, depending benchmark.

1. Introduction
Given set rectangles, problem find enclosing rectangles minimum area
contain without overlap. refer enclosing rectangle bounding box,
avoid confusion. optimization problem NP-hard, problem deciding
whether set rectangles packed given bounding box NP-complete, via
reduction bin-packing (Korf, 2003). consecutive-square benchmark simple set
increasingly difficult benchmarks problem, task find bounding
boxes minimum area contain set squares dimensions 1 1, 2 2, ...,
N N (Korf, 2003). example, Figure 1 optimal solution N =32.
use benchmark explain many ideas paper, techniques
limited packing squares, apply rectangles.
Rectangle packing many practical applications, including modeling scheduling problems tasks require resources allocated contiguous chunks.
example, consider task scheduling allocating contiguous memory addresses
programs. width rectangle represents length time program runs,
c
2012
AI Access Foundation. rights reserved.

fiHuang & Korf

Figure 1: optimal solution N =32 consecutive-square benchmark, packing
squares dimensions 1 1, 2 2, ..., 31 31, 32 32 bounding box minimum
area, 85 135.

48

fiOptimal Rectangle Packing: Absolute Placement Approach

height represents amount contiguous memory needs. rectangle packing solution
tells us programs run, well memory addresses
assigned. Similar problems include scheduling ships different length
berthed along single, long wharf (Li, Leong, & Quek, 2004), well allocation scheduling radio frequency spectra usage (Mitola & Maguire, 1999). Rectangle
packing also appears loading set rectangular objects pallet without stacking
them. cutting stock layout problems also contain rectangle packing subproblems.
1.1 Overview
remainder article organized follows. first introduce various benchmarks Section 2 specifically define rectangle packing instances solve.
Section 3, review state-of-the-art rectangle packers techniques,
provides foundation upon present new work. follow Section 4
data collected compare work previous state-of-the-art using previous
benchmarks. also compare difficulty previous benchmarks new ones.
Section 5, present benchmark rectangles successively higher precision
dimensions, new solution techniques handle this, follow experimental results.
compare methods competing search spaces used packing highprecision rectangles, show methods remain competitive.
Sections 6 7 explain various avenues future work, concluding article
summarizing contributions results. previously published much
work several conference papers (Huang & Korf, 2009, 2010, 2011).

2. Benchmarks
several reasons motivating benchmarks. First, benchmarks describe
instances single parameter N , allowing researchers easily reproduce instances.
Second, instances unique, optimal solutions reported easily
validated others. advantages many real-world instance libraries
randomly generated ones. Third, benchmarks define infinite set instances
successive instance harder previous. solver superior another solver
solve instance faster, larger instance amount time.
contrast, comparison using library instances may require counting number
instances completed within given time limit. Furthermore, instance libraries,
often one solver performs well one subset instances competing solver performs
well different subset, making comparisons inconclusive.
believe benchmarks capture difficult instances rectangle
packer may face investigate modeling generation random problems.
Although Clautiaux et al. (2007) others used random instances, non-random
benchmarks used Korf (2003) Simonis OSullivan (2008) better facilitated
comparison state-of-the-art packers. However, comprehensive overviews,
refer reader numerous surveys available (Lodi, Martello, & Vigo, 2002; Lodi,
Martello, & Monaci, 2002; Dowsland & Dowsland, 1992; Sweeney & Paternoster, 1992).
49

fiHuang & Korf

2.1 Previous Benchmarks
Several previous benchmarks used literature shown easier
benchmarks propose. Part due fact benchmarks, like solvers,
may also improved research, ensure cover various properties
rectangles, addition providing easy way compare performance among different
packers measure progress.
consecutive-square benchmark (Korf, 2003), simple set increasingly difficult
instances, task find bounding boxes minimum area contain set
squares sizes 1 1, 2 2, ..., N N . Prior work, many recent stateof-the-art packers used popular benchmark measure performance, including
Moffitt Pollack (2006), Korf, Moffitt, Pollack (2010), Simonis OSullivan
(2008). date, largest instance solved problem N =32, shown Figure 1,
using packer (Huang & Korf, 2009). consider problem packing squares
square benchmark gets much easier problem size increases, due
large differences areas consecutive square bounding boxes.
unoriented consecutive-rectangle benchmark (Korf et al., 2010), instance
set rectangles sizes 1 2, 2 3, ..., N (N + 1), rectangles may
rotated 90-degrees. subsequently explain, fact many pairs
rectangles instance share equal dimensions causes optimal solutions
leave empty space, making benchmark easy solve. include benchmark
completeness, note effective measure comparing different packers.
Finding first optimal solution another benchmark Simonis OSullivan
(2011) used conjunction problem instances unoriented consecutiverectangle benchmark. contrast problem finding optimal solutions,
measure time takes find first optimal solution, makes much
difficult reliably compare solvers unless focus research value
ordering tie-breaking among bounding boxes equal area.
example, Simonis OSullivan (2011) report find first solution
N =26 takes 3:28:20 (3 hours, 28 minutes, 20 seconds). shown Table 8 page
72, six solutions N =26: 42 156, 52 126, 56 117, 63 104, 72 91, 78 84,
requiring solver CPU times 0:32, 41:40, 53:19, 1:55:04, 1:33:22, 8:53:01,
respectively. smaller bounding boxes needed test optimal
solution empty space, used Simonis OSullivans termination criteria
returned first optimal solution, would need 32 seconds. Therefore, finding
minimum bounding boxes instead first one benchmark produces
harder problems larger N , better facilitates program comparisons.
2.2 Properties Easy Benchmarks Avoid
motivate new benchmarks, explain previous benchmarks tended
much easier comparison, constructed new benchmarks
describe instances consisting rectangles unique dimensions, without duplicates,
without area occupied rectangles.
50

fiOptimal Rectangle Packing: Absolute Placement Approach

(a) Solution 21 35 bounding
box unoriented instance 1 2,
2 3, ..., 11 12, 12 13.

(b) Solution 14 26 bounding
box unoriented instance 1
12, 2 11, ..., 11 2, 12 1.

Figure 2: Examples solutions instances rectangles equal dimensions.

2.2.1 Rectangles Equal Dimensions
unoriented consecutive-rectangle benchmark, rectangles share dimension
another rectangle. example, Figure 2a optimal solution N =12. optimal
solutions, rectangles equal dimensions tend line next other, forming larger
rectangles leaving little empty space. Figure 2a, 8 9 7 8 line up,
5 6 4 5, 3 4 2 3. fact, solutions
benchmark much smaller percentage empty space similar-sized instances
consecutive-square benchmark, rectangles unique dimensions.
also notice benchmarks duplicate rectangles, Figure 2b, solved
quickly.
2.2.2 Rectangles Small Area Small Dimensions
Figure 2b also example perfect packing, empty space
solution. Problems perfect packings tend easy two reasons. One
test bounding boxes increasing order area, test fewer boxes, since never test
box minimum area required. second problems,
rather deciding rectangle go bounding box,
efficient algorithm decide cell empty space rectangle occupy
51

fiHuang & Korf

it. soon small region empty space created cant accomodate remaining
rectangles, algorithm backtrack.
consecutive-square unoriented rectangle benchmarks, large
rectangles capture much total area instance. Thus, packer search
deeply using allowable empty space. little empty space, early
backtracking likely since cannot find place next rectangle. Therefore,
small rectangles benchmarks insignificant impact search effort.
previous benchmarks, consecutive-square benchmark, retangles
largest area also largest dimensions, making obvious rectangles
place first, largest rectangles constrained, impose
constraints remaining rectangles.
contrast, new benchmarks trade-off rectangles large
dimensions large area. widest rectangle oriented equal-perimeter benchmark, described below, smallest branching factor search xcoordinates. However, also least area, search wont constrain
placement remaining rectangles much. raises non-trivial question best
variable ordering non-square rectangles.
2.3 New Benchmarks
propose several new benchmarks difficult comparing instances
number rectangles. experimental results make use following
benchmarks, addition consecutive-square unoriented consecutive-rectangle
benchmarks described above.
2.3.1 Equal-Perimeter Rectangles
First, present oriented equal-perimeter rectangle benchmark, instance
set rectangles sizes 1 N , 2 (N 1), ..., (N 1) 2, N 1, rectangles may
rotated (see Figure 3). Given N , rectangles unique perimeter 2N +2.
experiments, benchmark much difficult either consecutive-square
benchmark unoriented consecutive-rectangle benchmark (Korf et al., 2010)
number rectangles. tested state-of-the-art packer (Huang & Korf, 2010)
old new benchmarks. N =22 oriented equal-perimeter benchmark
took nine hours solve, N =22 consecutive-square unoriented
consecutive-rectangle benchmarks took one second six seconds, respectively.
Second, present unoriented double-perimeter rectangle benchmark, instances described set rectangles 1 (2N 1), 2 (2N 2), ..., (N 1) (N + 1),
N N , rectangles may rotated 90-degrees. rectangles unique
perimeter 4N . benchmark difficult benchmarks
used previously literature, benchmark also difficult oriented
one introduced previous paragraph. experiments using techniques,
N =18 took two days solve.
far, benchmarks discussed low-precision integer dimensions.
property poses problem packer, enumerates various integer coordinate locations rectangle may placed. high-precision values, however,
52

fiOptimal Rectangle Packing: Absolute Placement Approach

Figure 3: optimal solution N =23 oriented equal-perimeter benchmark, packing
oriented rectangles dimensions 1 23, 2 22, ..., 22 2, 23 1 bounding box
minimum area, 38 61.

53

fiHuang & Korf

number distinct positions increases dramatically. motivates study packing rectangles high-precision dimensions. particular, propose unoriented
high-precision rectangle benchmark, instances described set rectangles
1
1 1
1
1
1
1 2 , 2 3 , ..., N N +1 . methods used solve benchmark quite
different used low-precision case.

3. Solution Techniques
section describe previous solution strategies well various new techniques
use rectangle packer. first describe techniques apply consecutive-square benchmark, oriented equal-perimeter benchmark, unoriented
double-perimeter benchmark. work unoriented high-precision rectangle benchmark included methods significantly different, deferred
Section 5.
3.1 Previous Work
earlier work focused optimal methods packing set rectangles
given bounding box motivated problem pallet loading. Dowsland (1987)
used depth-first search abstract graph representation search space solve
problem optimally problem sets modeled real-world pallet box dimensions.
Although problem instances contained average 30 rectangles 50,
benchmarks far easier consider here, rectangles
size, significant amount empty space solutions. Bhattacharya
et al. (1998) extended work additional lower bounds pruning techniques based
dominance conditions demonstrated work benchmarks.
examining rectangle packing instances rectangles different dimensions,
Onodera et al. (1991) used depth-first search, branching point search
space commitment particular non-overlap constraint two rectangles.
Lower bound graph reduction techniques applied prune search space, allowing optimally solve problems six rectangles.
Chan Markovs BloBB (2004) packer used branch-and-bound order find
minimum area bounding box contain set rectangles. solver could handle
eleven rectangles, observed instances duplicate rectangles
much easier, causing packer cluster rectangles together optimal solution.
Lesh et al.s solver (2004) used depth-first search, placing rectangle first bottommost left-most position fit (the bottom-left heuristic, see Chazelle, 1983),
determine whether set rectangles packed given enclosing rectangle.
able handle twenty-nine rectangles ten minutes average,
testbed consisted instances whose optimal solutions empty space.
Clautiaux et al. (2007) presented branch-and-bound method x-coordinates rectangles computed prior y-coordinates. assigning
x-coordinates, method uses relaxation similar cumulative constraint (Aggoun
& Beldiceanu, 1993) requires sum heights rectangles overlapping
particular x-coordinate cannot exceed height bounding box. y-coordinates
determined using search space derived bottom-left heuristic (Chazelle,
54

fiOptimal Rectangle Packing: Absolute Placement Approach

1983), using optimized data structures Martello Vigo (1998). Beldiceanu
Carlsson (2001) applied plane sweep algorithm used computational geometry detect violations non-overlap constraints, later adapted technique geometric
constraint kernel (Beldiceanu, Carlsson, Poder, Sadek, & Truchet, 2007). Lipovetskii (2008)
proposed branch-and-bound algorithm placed rectangles lower-left hand positions.
prior state-of-the-art, due Korf (2003, 2004) Simonis OSullivan (2008),
divide rectangle packing problem containment problem minimal
bounding box problem. former tries pack given set rectangles given bounding
box, latter finds bounding box least area contain given set
rectangles. packers algorithm minimal bounding box problem calls
algorithm containment problem subroutine.
3.2 Overall Search Strategy
Like Korf et al.s (2010) algorithm, minimum bounding box solver calls
containment problem solver, like Simonis OSullivan (2008), assign x-coordinates
prior y-coordinates.
Although use Simonis OSullivans (2008) ideas, take
constraint programming approach constraints specified general-purpose
solver like Prolog. Instead, implemented program scratch C++, allowing us
flexibly choose constraints use time naturally encode
search space use y-coordinates. implemented chronological backtracking
algorithm dynamic variable ordering. algorithm works five stages goes
root search tree leaves:
1. minimum bounding box algorithm generates initial candidate set bounding
boxes various widths heights.
2. containment solver called bounding box order increasing area,
infeasible bounding box, insert another back candidate set
bounding boxes height one unit greater. packing found, continue
testing boxes equal area find optimal solutions terminating.
3. containment solver first works x-coordinates model variables
rectangles values x-coordinate locations, using dynamic variable ordering
constraint detects infeasible subtrees.
4. x-coordinate solution found, problem transformed perfect packing instance.
5. searches set y-coordinates model variables empty corners
values rectangles.
describe detail steps.
55

fiHuang & Korf

3.3 Minimum Bounding Box Problem
One way solve minimum bounding box problem find minimum maximum
areas describing set candidate potentially optimal bounding boxes. Boxes
sizes generated areas within range, tested non-decreasing order
area solutions smallest area found. lower bound area
sum areas given rectangles. upper bound area determined
bounding box greedy solution found setting bounding box height
tallest rectangle, placing rectangles first available position
scanning left right, column scanning bottom top.
several techniques (Korf, 2003, 2004) use prune set bounding
boxes, review here. first generate set widths bounding boxes,
starting width widest rectangle width greedy solution
described above. width, generate feasible height using lower bounds
subsequently describe. resulting bounding boxes used initialize
min-heap sorted non-decreasing order area. search proceeds calling
containment solver bounding box minimal area heap. box infeasible,
increase height box one, insert new box back min-heap.
given bounding box width, initialize height maximum following
lower bounds. First, height must least height tallest rectangle
instance. Second, height must large enough accommodate total area
rectangles instance. Third, every pair rectangles, sum widths
exceed width bounding box, bounding box height must least
sum heights, since cant appear side-by-side, one must top
other. Fourth, set rectangles whose widths greater half width
bounding box must stacked vertically, including rectangle smallest height
whose width exactly half width bounding box. Finally, certain properties
exist given rectangle packing instance, force height greater equal
width break symmetry. example, one sufficient property instance
consisting squares, since solution W H bounding box easily transforms
another one H W bounding box. Another sufficient property every rectangle
dimensions w h correspond another one dimensions h w.
unoriented instances, given bounding box width, certain rectangles may forced
one orientation, improving lower bound bounding box height. Note
also break symmetry bounding box dimensions every unoriented instance.
3.3.1 Anytime Algorithm
problem instance many rectangles, immediate solution required,
Korf (2003) provides anytime algorithm bounding box problem, replacing one
described above, also calls containment problem solver. first find greedy
solution bounding box whose height equal tallest rectangle, described
previous section. repeatedly call containment problem solver following
way. previous attempt given bounding box resulted packing area
greater area best solution seen far, decrease width
one unit attempt solve resulting bounding box problem. instead previous
56

fiOptimal Rectangle Packing: Absolute Placement Approach

attempt infeasible, increase height bounding box one unit.
algorithm terminates width current bounding box less width
widest rectangle.
3.4 Containment Problem
Korfs (2003) absolute placement approach modeled rectangles variables positions
bounding box values. Rectangles placed turn depth-first search,
possible locations tested rectangle. contrast, Simonis OSullivans
(2008) packer assigned x-coordinates rectangles y-coordinates,
suggested Clautiaux et al. (2007), well using cumulative constraint (Aggoun
& Beldiceanu, 1993), improving performance orders magnitude. cumulative
constraint adds height rectangles overlap given x-coordinate location,
pruning values exceed height bounding box. constraint
checked exploring x-coordinates also exploring y-coordinates later on.
improved exploring y-coordinates differently, modeling candidate locations
variables, rectangles values (Huang & Korf, 2009), made packer
order magnitude faster Simonis OSullivans.
Simonis OSullivan (2008) furthermore applied least-commitment principle (Yap,
2004) constraint processing, first committing placement rectangles
interval x-coordinates instead single x-coordinate value. x-intervals
explored turn, constrain candidate individual x-coordinates explored later.
works committing x-interval induce pruning via cumulative constraint.
example, picking x-interval [a, b] size smaller width
rectangle wr , implies regardless x-coordinate rectangle eventually takes,
must contribute height x-coordinate within interval [b, + wr ]. Finally,
height bounding box constrains cumulative heights rectangles
given x-coordinate, similar ideas Beldiceanu et al. (2008). Larger intervals result
weaker constraint propagation (less pruning) smaller branching factor, smaller
intervals result stronger constraint propagation larger branching factor. size
intervals experimentally determined.
example, 4 2 rectangle x-coordinates restricted interval [0,2] contributes height 2 x-coordinates 2 3 even prior deciding exact x-coordinate
value. compulsory part (Lahrichi, 1982) constrains cumulative height rectangles may overlap x-coordinates 2 3 solution. interval assignments
infeasible, searching individual x-values futile. However, find
set interval assignments, still search set single x-coordinate
values. Simonis OSullivan (2008) assigned x-intervals, single x-coordinates, y-intervals,
single y-coordinates, order.
3.5 Assigning X-Intervals X-Coordinates
x-coordinates, propose pruning constraint adapted Korfs (2003) wastedspace pruning heuristic, dynamic variable order replace Beldiceanus (2008) fixed ordering, method optimize values assigned x-interval variables.
57

fiHuang & Korf

Figure 4: test violations cumulative constraint, remaining space
placing 3 2 rectangle x=2 represented vector h3, 3, 1, 1, 1, 3i.

3.5.1 Pruning Infeasible Subtrees
present constraint-based formulation Korfs (2003) two-dimensional wasted space
pruning algorithm, adapted one-dimensional case. Given partial solution, Korfs
algorithm computed lower bound amount wasted space, used
prune upper bound. contrast, compute numerical bounds
instead detect infeasibility single constraint.
rectangles placed bounding box, remaining empty space gets chopped
small irregular regions. Eventually empty space segmented small enough
chunks cannot accommodate remaining unplaced rectangles,
point backtrack. assigning x-coordinates bounding box height H,
keep histogram hv1 , v2 , . . . , vH i, vi number empty cells (units empty
space) empty columns height i. example, assume Figure 4
assigned x-coordinates 3 2 rectangle 6 3 bounding box. resulting
histogram would h3, 0, 9i, since 3 cells empty columns height 1, empty
cells columns height 2, 9 cells empty columns height 3.
Assume left place 2 3 2 2 rectangle. assign
six cells 2 3 rectangle empty cells v3 =9, leaving us remaining
empty cells h3, 0, 3i. point, cannot assign area 2 2,
3 empty cells accommodate height need 4, prune.
general, set unplaced rectangles R bounding box height H,

h,

X

wr hr

rR,hr h

H
X


vi ,

(1)

i=h

rectangle r R dimensions wr hr . is, every given height h,
amount space accommodate rectangles height h greater must least
cumulative area rectangles height h greater. check constraint
x-coordinate assignment.
58

fiOptimal Rectangle Packing: Absolute Placement Approach

(a) x=2 dominated position
4 4 square.

(b) x=0 undominated position
4 4 square.

Figure 5: Example dominance conditions.

3.5.2 Pruning Dominance Conditions
Korf (2003) introduced set dominance conditions prune positions large rectangles close sides bounding box. example, imagine must pack
squares 44, 33, 22, 11. Figure 5a, placement 44 square leaves
2 4 gap left side bounding box 3 3 square cannot fit.
2 2 1 1 squares fit within gap, fact placed
entirely within gap. Notice solution arrangement Figure 5a,
always rearrange Figure 5b without disturbing squares. Thus,
need try placing 4 4 square x=2 long tried placing
x=0. general, rectangle placement dominated leaves gap rectangles
individually fit also packed together gap without protruding
it. Although Korf hard-coded dominance rules consecutive-square benchmark,
dynamically generate every instance insignificant preprocessing overhead.
3.5.3 Variable Ordering
following subsections consider two variable orders work together packer.
use fixed ordering governs rectangle assigned next. ordering used
x-intervals independently use single x-coordinate variables.
point time, also must choose whether assign next x-interval next
single x-coordinate variable. Since ordering x-intervals single x-coordinate
variables simpler, present technique first.
Ordering X-Intervals X-Coordinates Area variable order
based observation placing rectangles larger area constraining
placing smaller area. times either choose assign single
x-coordinate rectangle previously assigned x-interval,
assign x-interval rectangle yet made assignments for. shown
Figure 4, either assignments decrease amount empty space represented
cumulative constraint vector. always pick next variable results
least remaining space.
59

fiHuang & Korf

Ordering Among Rectangles Branching Factor natural variable order
arises consecutive-square unoriented consecutive-rectangle benchmarks using strategy picking constrained variable next. example,
consecutive-square benchmark, largest rectangle clearly largest height,
width, area. However, new benchmarks rectangle largest width
smallest height, largest area, making good variable ordering non-obvious.
propose variable order rectangles various aspect ratios picking
variable fewest number values first, favor smaller branching factor closer
root search tree. oriented equal-perimeter benchmark, recall
assign intervals x-coordinates individual x-coordinates, like Simonis
Sullivan (2008) use constant factor times rectangle width define interval
size. branching factor x-interval variables given rectangle

Bw rw
1
Bw 1
b=
,
(2)
=
Crw
C rw
C
Bw bounding box width, rw rectangle width, C constant chosen
experimentally. numerator Bw rw number x-coordinate values
rectangle still fitting bounding box, denominator Crw
size interval assigning given rectangle. example, C=0.75
would assign intervals size three 4 2 rectangle.
may drop translational constant 1/C well positive scalar Bw /C since
interested relative ordering rectangles, leaving us 1/rw
means oriented benchmark place rectangles order decreasing
width. unoriented double-perimeter benchmark, packer first tries values
particular x-interval, rotates rectangle 90-degrees trying another set
x-interval values. case branching factor


Bw rw
Bw 1
Bw rh
1
2
b=
=
+
+
.
(3)
Crw
Crh
C rw
rh
C
mentioned before, drop scalar translational constant, giving us
1
rw + rh
1
=
+
.
(4)
rw
rh
rw rh
rectangles given instance perimeter definition,
numerator result Equation 4 constant. Therefore unoriented benchmark,
place rectangles order decreasing area.
3.5.4 Determining Sizes X-Intervals
consecutive-square benchmark, packer used interval size 0.35 times
width given rectangle. found larger interval sizes improve performance
packer new equal-perimeter benchmarks, use value C=0.55 instead.
assign larger intervals short wide rectangles, x-interval variables
rectangles tend branching factors three less. balance
sizes intervals values assigned equally constraining subtrees.
example, consider C=0.55, rectangle width 20, set possible x-coordinate
60

fiOptimal Rectangle Packing: Absolute Placement Approach

values [0,23]. Without balancing sizes intervals, packer would explore interval
sizes 20C = 11, x=[0,10], x=[11,21], finally remaining domain values
small interval x=[22,23]. results small compulsory parts therefore
large search subtrees first two branches, large compulsory part thus
small search subtree third.
Since must explore three branches anyway, balance sizes interval
assignments exploring x=[0,7], x=[8,15], x=[16,23]. eventual effect better
balance size search subtrees amongst branches. packer first computes
branching factor induced global interval parameter C=0.55 rectangle,
balances number values interval assignment.
Interactions Interval Assignment Dominance Conditions consecutive-square instances, squares several positions following x=0
dominated. Therefore, packer first branches assigning degenerate interval x=[0,0] exploring interval assignments undominated positions. Although
technique increased performance packer fivefold compared leaving out,
strategy slowed performance fivefold oriented unoriented doubleperimeter benchmark. reason degradation performance follows.
equal-perimeter benchmarks, 1 N rectangle always partially fit gaps
left rectangles, must always protrude gaps, thereby eliminating
dominance conditions previously described. Without dominated positions
account for, simply applying strategy used consecutive-squares new
benchmarks results packer committing single x-coordinate values situations
desirable include positions larger interval assignment.
avoid this, packer detects dominated positions dynamically
chooses whether assign degenerate interval x-coordinate assignment,
immediately begin interval assignments.
3.6 Perfect Packing Transformation
every complete x-coordinate solution, transform problem instance perfect
packing problem instance working y-coordinates. perfect packing instance
rectangle packing problem property solution empty space.
transformation done adding original set rectangles number 1 1
rectangles necessary increase total area rectangles bounding box.
Although new 1 1 rectangles increase problem size, hope ease
solving perfect packing instances offset difficulty packing rectangles. Next
describe search space perfect packing. show, methods rely
perfect packing property empty space.
3.7 Assigning Y-Coordinates
alternative asking rectangle go? ask rectangle
go here? former model, rectangles variables empty locations
values, whereas latter, empty locations variables rectangles values.
y-coordinates, search latter model. use 2D bitmap draw placed rectangles
61

fiHuang & Korf

test overlap, backtrack positions cannot accommodate remaining
rectangles, required Korfs (2003) wasted space pruning rule.
3.7.1 Empty Corner Model
perfect packing solutions, every rectangles lower-left corner fits lower-left
empty corner formed rectangles, sides bounding box, combination
both. model, one variable per empty corner. final solution, since
rectangle goes exactly one empty corner, number empty corner variables
equal number rectangles perfect packing instance. set values
set unplaced rectangles.
search space interesting property variables dynamically created
search x- y-coordinates empty corner known
rectangles create placed. Furthermore, placing rectangle empty corner
assigns x- y-coordinates.
Note empty corner model describe perfect packing solutions. Given
perfect packing solution, list unique sequence rectangles scanning
left right, bottom top lower-left corners rectangles. sequence
corresponds sequence assignments root search space leaf
tree. property also bounds maximum size search space N 0 ! N 0
number rectangles performed perfect packing transformation.
3.7.2 Duplicate Rectangles
Due additional 1 1 rectangles perfect packing transformation,
introduced additional redundancy problem. simple way handle
follows. particular empty corner, never place rectangle duplicate one
already tried position. method handling duplicates also applies
duplicate rectangles original problem instance.

4. Experimental Results
benchmarked packers Linux 2GHz AMD Opteron 246 2GB RAM.
packer call KMP10 (Korf et al., 2010) benchmarked machine,
quote published results. include data relative placement
packer competitive. Results Simonis OSullivans packer (2008),
call SS08, also quoted, obtained SICStus Prolog 4.0.2 Windows
3GHz Intel Xeon 5450 3.25GB RAM. Since machine faster ours,
comparisons conservative estimate relative performance.
4.1 Previous Benchmarks
consecutive-square benchmark unoriented consecutive-rectangle
benchmarks (Korf et al., 2010) used literature measure performance,
include data collected using two benchmarks.
62

fiOptimal Rectangle Packing: Absolute Placement Approach

Size
N

KMP10
Time

SS08
Time

FixedOrder
Time

20
21
22
23
24
25
26
27
28
29
30
31
32

1:32
9:54
37:03
3:15:23
10:17:02
2:02:58:36
8:20:14:51
34:04:01:03

:02
:07
:51
3:58
5:56
40:38
3:41:43
11:30:02

:00
:03
:02
:14
:40
2:27
10:25
1:08:55
2:18:12:13

HK09
Time
:00
:03
:02
:12
:37
2:14
9:39
35:12
4:39:31
8:06:03
2:17:32:52
4:16:03:42
33:11:36:23

Table 1: CPU times required various packers consecutive-square benchmark,
task pack squares 1 1 N N .

4.1.1 Consecutive Squares
Table 1 compares CPU runtimes four packers consecutive-square benchmark.
first column specifies instance size, number squares
size largest one. remaining columns specify CPU times required various
algorithms find optimal solutions format days, hours, minutes,
seconds. multiple boxes minimum area, N =27 listed Table
8 Appendix 4.4, report total time required find optimal bounding boxes.
two reasons. First, finding minimum area bounding boxes removes
question bounding box test first one area. Second,
providing optimal solutions, researchers working rectangle packing use
information verify correctness programs.
HK09 includes wasted space pruning rule x-coordinates, dynamic variable
ordering x-intervals x-coordinates, perfect packing transformation,
related search space inference rules. named packer consistent
previous work (Huang & Korf, 2009). SS08 refers previous state-of-the-art
packer (Simonis & OSullivan, 2008). largest problem previously solved N =27
took SS08 11 hours. solved problem 35 minutes solved five
open problems N =32. KMP10 refers Korf et al.s (2010) absolute placement
packer. FixedOrder assigns x-intervals single x-coordinates, includes
ideas. HK09s dynamic variable ordering x-coordinates order
magnitude faster FixedOrder N =28. order magnitude improvement
FixedOrder SS08 likely due use perfect packing assigning ycoordinates. include timing packer perfect packing disabled
competitive (e.g., N =20 took 2.5 hours).
63

fiHuang & Korf

Size
N
21
22
23
24
25
26
27
28
29
30
31
32

X-Coordinate
Solutions
665
283
391
870
193
1,026
244
2,715
11,129
10,244
73,614
37,742

Seconds
X
0.35
0.95
6.54
19.41
73.38
313.81
1,181.53
8,987.36
15,677.20
124,399.74
214,575.08
1,916,312.67

Seconds

1.04
0.18
0.31
1.08
0.14
1.39
0.60
23.40
28.82
17.97
254.42
102.59

Ratio
X:Y
0.3
5.3
21.1
18.0
524.1
225.8
1,969.2
384.1
544.0
6,922.6
843.4
18,679.3

Table 2: CPU times spent searching x- y-coordinates consecutive-square
benchmark

Table 2 second column number complete x-coordinate assignments
packer found entire run particular problem instance. third column
total time spent searching x-coordinates. fourth column total time
spent performing perfect packing transformation searching y-coordinates.
columns represent total CPU time entire run given problem instance.
last column ratio time third column fourth. Interestingly,
almost time spent x-coordinates opposed y-coordinates,
suggests could efficiently enumerate x-coordinate solutions, could also
efficiently solve rectangle packing. confirmed relatively x-coordinate
solutions exist even large instances. data Table 2 obtained Linux
2.93GHz Intel Core 2 Duo E7500 machine separate experiment Table 1,
total time spent given instance different.
4.1.2 Unoriented Consecutive Rectangles
Table 3 compares CPU times packer unoriented consecutive-rectangles
benchmark Korf et al. (2010). Although techniques due Simonis
OSullivan (2008) outperform Korf et al. consecutive-square benchmark,
previously published results benchmark besides Korf et al.
benchmark easier consecutive-square benchmark, break
contributions techniques, differences delineated
clearly previous section. primary differentiating feature benchmark
rectangles unoriented.
first column gives size problem instance. second column gives
performance previous state-of-the-art packer benchmark, using Korf et al.s
code (2010). third column gives performance packer benchmark.
64

fiOptimal Rectangle Packing: Absolute Placement Approach

Size
N

KMP10
Time

HK10
Time

16
17
18
19
20
21
22
23
24
25
26
27
28
29

:01
:05
:17
:07
8:11
15:00
1:09:45
8:51:46
11:53:17
7:17:00:03

:00
:00
:00
:00
:05
:06
:17
:47
13:38
2:21:10
6:31:51
4:07:37:08
1:16:43:02
6:04:47:06

Table 3: CPU times required two packers unoriented consecutive-rectangle benchmark, task pack unoriented rectangles sizes 12, 23, ..., N (N +1).

data table collected Linux 2.93GHz Intel Core 2 Duo E7500 machine,
except N =28 N =29, collected Linux 2.53GHz Intel Xeon E5630
12GB RAM, experiments revealed 20% faster former
machine.
benchmark techniques allowed us extend known solutions
N =25 N =29 allowed us solve N =25 80 times faster previous
state-of-the-art benchmark.
4.2 Oriented Equal-Perimeter Unoriented Double-Perimeter Rectangles
section uses new benchmarks compare techniques developed
non-square instances. techniques discuss here, including dynamic adjustment
interval sizes generalized variable order based branching factor, largely
affect performance packer consecutive-square benchmark. fact,
tested packer benchmark see effects extra overhead added
improvements. new packer resulted five percent speedup compared
packer without changes consecutive-square benchmark, likely due minor
improvements data structures, balancing interval sizes. Therefore, compare
effects techniques new benchmarks. techniques
developed new benchmarks improve performance oriented unoriented
cases, discuss together.
Table 4 compares performance packers oriented equal-perimeter benchmark Table 5 compares packers using unoriented double-perimeter
benchmark. first column refers problem size instance, number
65

fiHuang & Korf

Size
N

Boxes
Tested

HK09
Time

OptDom
Time

BrFactor
Time

C=0.55
Time

HK10
Time

13
14
15
16
17
18
19
20
21
22
23

7
7
10
9
8
12
12
11
9
15
16

:01
:02
:16
:57
5:56
1:06:32
6:35:48
1:18:51:34
3:21:31:46

:00
:01
:05
:16
1:21
14:47
1:26:16
7:36:09
13:33:16

:00
:00
:01
:02
:27
6:15
31:23
1:51:10
4:22:49

:00
:00
:00
:00
:03
:32
3:34
13:06
20:49
14:22:03

:00
:00
:00
:00
:02
:22
2:15
7:51
11:20
9:12:37
3:22:50:38

Table 4: CPU times required various packers oriented equal-perimeter rectangle
benchmark, task pack oriented rectangles sizes 1 N , 2 (N 1), ...,
(N 1) 2, N 1.

Size
N

Boxes
Tested

HK09
Time

OptDom
Time

BrFactor
Time

C=0.55
Time

HK10
Time

11
12
13
14
15
16
17
18

12
17
13
17
21
35
27
35

:01
:20
1:45
28:48
1:43:01
1:16:46:44

:00
:04
:21
4:53
11:36
4:13:34
1:12:40:14

:00
:04
:21
4:53
11:36
4:13:34
1:12:40:14

:00
:01
:06
1:19
3:33
1:16:02
9:44:14

:00
:01
:06
1:15
2:34
1:01:54
7:53:50
2:02:10:38

Table 5: CPU times required various packers unoriented double-perimeter rectangle benchmark, task pack unoriented rectangles sizes 1 (2N 1),
2 (2N 2), ..., (N 1) (N + 1), N N .

66

fiOptimal Rectangle Packing: Absolute Placement Approach

rectangles. second column gives number bounding boxes tested order
find optimal solutions. remaining columns represent CPU times different
versions packer format days, hours, minutes, seconds. wrote
packer C++ collected data Linux 2.93GHz Intel Core 2 Duo E7500 machine.
left right, successive packer improves previous one including
additional technique. column called HK09 data collected using techniques
developed specifically consecutive-square packing, include perfect packing
transformation related inference rules, dynamic variable ordering single
x-coordinates x-intervals, wasted space pruning rule x-coordinates
(Huang & Korf, 2009). compare new variable ordering rectangles
various aspect ratios, used order decreasing area default HK09.
OptDom improves upon HK09 dynamically detecting dominance rules apply
inapplicable, optimizes x-interval assignments knowledge. BrFactor
improves upon OptDom orders oriented equal-perimeter benchmark decreasing width unoriented double-perimeter benchmark decreasing area. C=0.55
improves upon BrFactor use interval size 0.55 instead C=0.35
consecutive-square benchmark. Finally, HK10 improves upon C=0.55 using
knowledge branching factor rebalance sizes interval assignments
x-coordinates.
Notice OptDom, BrFactor, C=0.55 introduce techniques reduce
branching factor, greater effect performance HK10, whose
new technique seeks make intervals assigned equally constraining. experiments
reveal techniques interact one another, note without including
dominated positions intervals, performance gained techniques
appears muted. interaction also tune global interval parameter C
including techniques affect branching factor.
Ordering branching factor improved performance oriented equal-perimeter
benchmark unoriented benchmark. latter case, seen Table 5,
technique ordering branching factor prescribes ordering decreasing area,
gave packer reasonable default. Therefore, difference
algorithm performance OptDom BrFactor columns Table 5.
Note unoriented double-perimeter benchmark requires packer try
twice many bounding boxes given parameter N required oriented benchmark. due 2N -1 largest dimension unoriented
benchmark N largest dimension oriented benchmark. larger
rectangles introduce higher precision problem, must try bounding
boxes. containment problem unoriented instance problem space
factor 2N larger oriented instance due two orientations
rectangle. Thus, instance N rectangles benchmark incomparable
instance N squares consecutive-square benchmark evaluating benchmark
difficulty.
summary, using techniques together, solve N =21 oriented
equal-perimeter benchmark 500 times faster N =16 unoriented doubleperimeter benchmark 40 times faster techniques presented optimized
consecutive squares.
67

fiHuang & Korf

Size

Boxes Tested

CPU Time

N

Squares

Perimeter

Squares

Perimeter

16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32

10
5
14
12
14
20
17
19
19
17
21
22
30
27
21
30
36

9
8
12
12
11
9
15
16

:00
:00
:00
:00
:00
:01
:01
:07
:20
1:14
5:15
19:42
2:30:11
4:21:46
1:10:33:38
2:11:40:29
22:04:20:15

:00
:02
:22
2:15
7:51
11:20
9:12:37
3:22:50:38

Table 6: Number bounding boxes tested CPU time required solve given instance
consecutive-square oriented equal-perimeter benchmarks.

68

fiOptimal Rectangle Packing: Absolute Placement Approach

4.3 Comparing Easy Hard Benchmarks
following tables compare difficulty various benchmarks using packer (Huang
& Korf, 2010) optimizations enabled.
4.3.1 Consecutive Squares vs. Equal-Perimeter Rectangles
Table 6, first column indicates number rectangles instance. second
third columns labeled Boxes Tested give number bounding boxes
tested finding optimal solutions consecutive-square benchmark
oriented equal-perimeter benchmark, respectively. fourth fifth columns give
performance rectangle packer benchmarks well. data point
table collected using Linux 2.93GHz Intel Core 2 Duo E7500 using one process, one
thread, one core.
Notice given instance number rectangles, oriented equalperimeter benchmark significantly harder consecutive-square benchmark.
due fact given problem size, consecutive-square benchmark contains
many little squares typically easy place property missing equalperimeter benchmark. fact, N =23 packer requires four orders magnitude
time find optimal solutions new benchmark compared instance
number items consecutive-square benchmark.
4.3.2 Unoriented Consecutive-Rectangles vs. Unoriented Double-Perimeter
Rectangles
Table 7 shows removing certain properties results successively difficult benchmarks. start unoriented consecutive-rectangle benchmark (Korf et al., 2010)
contains many easy properties. Doubly Scaled column pack 2 4, 4 6,
68, ..., (2N )(2N +2) rectangles, simply scales unoriented consecutive-rectangle benchmark factor two. benchmark difficult integers
higher magnitude lead x-coordinates search, turn increases branching factor problem. Unique Dimensions column pack rectangles
sizes 1 2, 3 4, 5 6, ..., (2N 1) (2N ), differs previous benchmark
dimensions unique. last column distributes area among rectangles
uniformly avoid consolidating area first rectangles.
column also culmination difficult properties identified
rectangle packing benchmark, call unoriented double-perimeter benchmark.
data points table collected using Linux 2.93GHz Intel Core 2 Duo E7500
machine without parallelization, except N =28 N =29, collected
Linux 2.53GHz Intel Xeon E5630 machine 12GB RAM, estimate
thirty percent faster.
4.4 Bounding Boxes Minimum Area
section list optimal bounding boxes various benchmarks found
program optimizations enabled. Notice duplicate data
69

fiHuang & Korf

Size
N
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29

Unoriented
ConsecutiveRectangles
:00
:00
:00
:00
:00
:00
:00
:00
:05
:06
:17
:47
13:38
2:21:10
6:31:51
4:07:37:08
1:16:43:02
6:04:47:06

Doubly
Scaled
:00
:00
:00
:00
:00
:00
:01
:01
:09
:10
:29
1:13
27:37
6:41:20
1:02:12:06

Unique
Dimensions
:00
:00
:01
:00
:01
:01
:03
:11
:50
3:00
15:34
3:21:36
12:23:37

Unoriented
DoublePerimeter
:01
:06
1:15
2:34
1:01:54
7:53:50
2:02:10:38

Table 7: CPU time required optimized packer various benchmarks increasing
difficulty.

70

fiOptimal Rectangle Packing: Absolute Placement Approach

unoriented high-precision rectangle benchmark, leave Table 10, Section 5.5.2,
since discussion refers data.
first column tables 8 9 refer size problem instance
respective benchmarks. columns called Optimal Solution give dimensions
optimal bounding boxes given instance. next column called Empty Space gives
percent empty space optimal solution. next column gives number
bounding boxes tested order find optimal solutions given instance.

5. Absolute Placement High-Precision Instances
Meir Moser (1968) first proposed problem finding smallest square
contain infinite series rectangles sizes 11 12 , 12 31 , 13 14 , ..., etc. rectangles
cannot overlap unoriented. unit square exactly enough area since total
area rectangles infinite series one. hand, space
wasted, suggesting task infeasible. Inspired problem, propose last
benchmark developed several new techniques.
introduce unoriented high-precision rectangle benchmark, task find
bounding boxes minimum area contain finite set unoriented rectangles
sizes 11 12 , 12 13 , ..., N1 N 1+1 . example, N =4 one must pack rectangles
sizes 11 12 , 21 13 , 13 41 , 14 51 . Alternatively, one may try pack rectangles sizes
60 30, 30 20, 20 15, 15 12 60 60 square, original instance
scaled factor 60, least common multiple rectangle denominators.
strategy required broad class recent rectangle-packers explore domain
integer x- y-coordinates rectangles quickly break higher N .
example, optimal solution N =15 400 billion unique coordinate pairs
rectangles assigned to. benchmark complements rather replaces
current low-precision benchmarks, neglected high-precision instances.
remainder section organized follows. first review previous
work proposing solution techniques may unaffected precision rectangle
dimensions. describe several adaptations low-precision techniques
high-precision case, along new techniques developed specifically high-precision
rectangle instances, finally follow experimental results.
5.1 Previous Work
relative placement approach Moffitt Pollack (2006) rectangle packing,
similar types search spaces used resource-constrained scheduling (Weglarz, 1999),
promises immune problem high-precision rectangle instances. However, since
many techniques described previous sections cannot
extended packer working relative placement search space, decided
stay within absolute placement framework attempt mitigate problems
introduced high-precision numbers.
71

fiHuang & Korf

Consecutive Squares

Consecutive Rectangles

Size
N

Optimal
Solutions

Empty
Space

Boxes
Tested

Optimal
Solutions

Empty
Space

Boxes
Tested

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26

11
23
35
57
512
911
1114, 722
1415
1520
1527
1927
2329
2238
2345
2355
2854, 2756
3946
3169
4753
3485
3888
3998
6468
5688
43129
7089

0.00%
16.7%
6.67%
14.3%
8.33%
8.08%
9.09%
2.86%
5.00%
4.94%
1.36%
2.55%
2.03%
1.93%
1.98%
1.06%
0.50%
1.40%
0.84%
0.69%
0.99%
0.71%
0.64%
0.58%
0.40%
0.47%

1
1
1
1
1
1
3
2
4
5
3
6
5
8
13
10
5
14
12
14
20
17
19
19
17
21

0.00%
0.00%
0.00%
0.00%
0.00%
1.75%
0.00%
0.00%
1.79%
0.45%
0.00%
0.95%
0.00%
0.00%
0.00%
0.00%
0.00%
0.00%
0.00%
0.00%
0.20%
0.00%
0.00%
0.00%
0.00%
0.00%

1
1
1
2
2
2
2
1
5
5
2
4
1
2
1
2
2
3
2
4
2
2
3
4
5
7

27
28
29
30
31
32

47148, 7494
63123
81106
51186
91110
85135

0.37%
0.45%
0.36%
0.33%
0.33%
0.31%

22
30
27
21
30
36

12
24
45
58, 410
514
619
1214
1516
1621, 1424
1726
2226
2135
2635
3235, 2840
3440
3251
3457
3076
3576, 3870
3588, 4470, 5556
3991
4492
40115, 46100
40130, 52100, 6580
45130, 6590, 7578
42156, 52126, 56117,
63104, 7291, 7884
63116
56145, 70116
62145

0.00%
0.00%
0.00%

3
3
2

Table 8: optimal solutions consecutive-square benchmark, task
pack squares sizes 11, 22, ..., N N , unoriented consecutive-rectangle
benchmark, task pack unoriented rectangles sizes 1 2, 2 3, ...,
N (N + 1).

72

fiOptimal Rectangle Packing: Absolute Placement Approach

Oriented Equal Perimeter

Unoriented Double Perimeter

Size
N

Optimal
Solutions

Empty
Space

Boxes
Tested

Optimal
Solutions

Empty
Space

Boxes
Tested

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16

11
23
34
46
67
610
811
816
1116
1121
1421
1329
1629
1930, 1538
2429
2336

0.00%
33.3%
16.7%
16.7%
16.7%
6.67%
4.55%
6.25%
6.25%
4.76%
2.72%
3.45%
1.94%
1.75%
2.30%
1.45%

1
1
1
1
4
2
2
5
6
8
6
7
7
7
10
9

0.00%
22.2%
8.33%
7.41%
6.86%
5.85%
3.08%
1.59%
1.50%
0.69%
1.15%
1.37%
0.71%
0.67%
0.67%
0.83%

1
1
2
2
8
9
11
8
13
8
12
17
13
17
21
35

17
18
19
20
21
22
23

2441
2448
3242, 2456
3742
3551
3460
3861

1.52%
1.04%
1.04%
0.90%
0.78%
0.78%
0.78%

8
12
12
11
9
15
16

11
33
38
69
617
919
1320
1821
1341
2430
2933
2159
3841
3851, 17114
4454
4564, 3096,
4072, 4860
3988, 5266
5574

0.44%
0.57%

27
35

Table 9: optimal solutions oriented equal-perimeter rectangle benchmark,
task pack oriented rectangles sizes 1 N , 2 (N 1), ..., (N 1) 2,
N 1, unoriented double-perimeter rectangle benchmark, task
pack unoriented rectangles sizes 1 (2N 1), 2 (2N 2), ..., (N 1) (N + 1),
N N.

73

fiHuang & Korf

(a)

(b)

Figure 6: Examples mapping solutions one rectangles left-most,
bottom-most positions.

5.2 Overall Strategy
Given instance high-precision benchmark described rational numbers,
multiply values least common multiple denominators get instance
integer dimensions. apply absolute placement solution techniques,
improvements subsequently explain, order find optimal solutions.
found, divide x- y-coordinates describing optimal solutions initial
scaling constant order obtain optimal solutions original problem.
Note map every solution one rectangles slid
left bottom much possible (Chazelle, 1983). example, solution
Figure 6a transformed Figure 6b. Since rectangles propped
left rectangles, rectangles x-coordinate sum
subset widths rectangles rectangles y-coordinate
sum subset heights rectangles. Similarly, width height
bounding box must sum subset widths heights rectangles,
respectively.
following subsections first explain techniques respect oriented
instances, follow handle unoriented case.
5.3 Minimum Bounding Box Problem
Since build initial set bounding boxes pairwise combinations widths
heights within given ranges, space pruned considering bounding box
widths heights equal subset sums rectangle widths, subset sums
rectangle heights, respectively. Recall Section 4.4 every bounding box
width, compute lower bound height. modify rounding
resulting bound next subset sum rectangle heights.
74

fiOptimal Rectangle Packing: Absolute Placement Approach

5.3.1 Precomputing Subset Sums
compute set subset sums prior searching. oriented rectangles
cannot rotated compute two sets: one based heights rectangles
representing candidate y-coordinates, one based widths representing
candidate x-coordinates. distinction generates fewer subset sums compared
single set subset sums generated widths heights.
5.3.2 Pruning Combinations Widths Heights
reject bounding boxes certain width height combinations
infeasible. pruning technique relies observation certain cases, may
one unique set rectangles generate specific width (height) bounding
box.
example, consider bounding box width generated unique
set rectangles. assume heights set rectangles also uniquely
determine subset sum specific bounding box height. say combination
bounding box width height incompatible. reason set rectangles
way bounding box given width, implies set
rectangles must appear solution laid horizontally one another. Thus,
set rectangles cannot appear stacked vertically solution. contradicts
implications bounding box given height. Note particular example,
compatible height maximum height rectangles.
5.3.3 Learning Infeasible Attempts
Recall algorithm solving minimal bounding box problem repeatedly calls
algorithm solve containment problem. Bounding boxes tested order
non-decreasing area first boxes solutions found. learn
infeasible attempts.
example, consider pack N rectangles {r1 , r2 , ..., rN }. Note use
pre-computed variable order rectangles. Let rd , < N rectangle corresponding
deepest search tree depth-first search able go, entire
search effort given bounding box. containmnet solver says bounding box
infeasible, next bounding box height consider next
greatest subset sum based smaller set {r1 , r2 , ..., rd+1 } instead considering N
rectangles. intuition behind since containment solver failed even
find arrangement first + 1 rectangles, doesnt make sense involve
remaining rectangles {rd+2 , ...rN } next largest subset sum bounding box
height.
method resembles conflict-directed backtracking. implementation, also
consider effect pruning using wasted space heuristic well.
5.4 Containment Problem
Similar low-precision methods, first assign x-coordinates rectangles,
conduct perfect packing transformation, finally work y-coordinates (Huang
75

fiHuang & Korf

& Korf, 2010). main difference high-precision methods lowprecision methods instead considering possible integers domain
x- y-coordinates, consider smaller set subset sums widths heights
rectangles. methods using x-intervals remain unchanged
describe search individual x-coordinates here.
5.4.1 Assigning X-Coordinates
oriented rectangles, choose x-coordinates set subset sums rectangle
widths. Instead precomputing set minimal bounding box problem,
generate dynamically every node search prior branching
various x-coordinate value assignments. set computed follows:
1. Initialize set value 0, represents placing rectangle
left side bounding box.
2. every rectangle r already assigned x-coordinate point search,
insert set sum x-coordinate width. represents placing
rectangle right side r.
3. every rectangle x-coordinate still unassigned, add width every
element set, insert new sums back set.
5.4.2 Perfect Packing Transformation
assigning x-coordinates, create number 1 1 rectangles account
empty space original instance. transformation results new instance,
empty space, consists original rectangles plus new 1 1 rectangles.
given empty corner partial solution, ask original unplaced rectangles
might fit there, 1 1 rectangle, essentially modeling empty corners variables
rectangles values.
high-precision benchmark, solving N =15 requires creating 1.5 billion 1 1
rectangles scaled problem large number. packer
simply requires much memory time. avoid problem creating fewer
much larger rectangles account empty space.
Widening Existing Rectangles Assume Figure 7a task pack three
rectangles. 10 20, 20 10, 40 10 rectangle 60 50 bounding
box, assume assigned x-coordinates y-coordinates. Given
x-coordinates already assigned, resulting packing solution space right
40 10 rectangle must always empty. Thus, replace 40 10 rectangle
60 10 rectangle, effectively widening original rectangle. Likewise, replace
20 10 rectangle 30 10 rectangle, 10 20 rectangle 30 20 rectangle,
Figure 7b. packer greedily attempts widen rectangles towards right
widening towards left. solving problem return
rectangles back original widths. avoids creating many 1 1 rectangles
perfect packing transformation represent empty space.
76

fiOptimal Rectangle Packing: Absolute Placement Approach

(a) partial solution xcoordinates known.

(b) result widening rectangles.

Figure 7: Widening existing rectangles.

(b) solution without 60 1 rectangles empty space.

(a) partial solution xcoordinates known.

Figure 8: Consolidating empty space horizontal strips.

Turning Empty Space Large Rectangles partial solution Figure 8a,
assigned x-coordinates rectangles 60 40 bounding box. Instead
creating three hundred 1 1 rectangles represent empty space indicated
single hash marks, use ten 30 1 rectangles without losing packing solutions.
Similarly, represent doubly-hashed empty space twenty 301 rectangles instead
six hundred 1 1 rectangles. Note cannot use 60 1 rectangles empty
space since would inadvertently prune potential solution Figure 8b.
5.4.3 Assigning Y-Coordinates
perfect packing transformation, assign y-coordinates asking rectangle
placed given empty corner. before, enforce constraint ycoordinate rectangle must subset sum rectangle heights. Note
rectangles create via perfect packing transformation included subset
sum calculations, since represent empty space.
5.4.4 Handling Unoriented Instances
unoriented instances, computing initial bounding box widths heights,
generate single set subset sums using widths heights rectangles
instance instead keeping widths separated heights. Likewise,
generating set candidate x- y-coordinates, must add fourth step
77

fiHuang & Korf

Size
N

Optimal
Solution

LCM

Bits
Precision

HK10
Boxes

Subsets
Boxes

Mutex
Boxes

HK11
Boxes

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15

1/21
1/24/3
1/219/12
5/61, 1/25/3
1/217/10
1/2107/60
1/2107/60
1/2163/90
1/2163/90
1/21817/990
1/27367/3960
1/267/36
1/2185/99
1/2169/90
1/279/42

2
6
12
60
60
420
840
2,520
2,520
27,720
27,720
360,360
360,360
360,360
720,720

2
6
8
12
12
18
20
23
23
30
30
37
37
37
39

1
1
2
30
20
1,979
4,033
39,357
13,571
2,682,948

1
1
2
5
7
59
151
693
1,083
7,489
31,196
66,425
289,217
549,135
1,171,765

1
1
2
4
7
44
107
465
755
4,901
22,822
38,827
162,507
382,059
651,041

1
1
2
4
7
29
46
124
192
585
1,641
2,366
5,027
9,548
15,334

Table 10: minimum-area bounding boxes number bounding boxes tested
packing unoriented rectangles 11 21 , 12 13 , 31 41 , ..., N1 N 1+1 .

bulleted list subsection 5.4.1 add height every rectangle
yet placed, every element set subset sums, represents possibility
rotating rectangle.
5.5 Experimental Results
present two different data tables, one relating improvements minimal bounding
box problem measured number bounding boxes tested, another one
overall CPU time solving entire rectangle-packing problem. separate
experiments way solution schema decouples minimal bounding box
problem containment problem.
5.5.1 Minimum Bounding Box Problem
Table 10 shows optimal solutions unoriented high-precision rectangle benchmark
along various properties corresponding instances. first two columns give
problem size dimensions optimal solutions, respectively. third gives
least common multiple first N +1 integers. fourth number bits required
represent area minimal bounding box. Note one optimal
solutions width 12 , since first rectangle much larger others.
N =12 larger, required precision exceeds 32-bit integer.
fifth eighth columns compare number bounding boxes various
packers test find optimal solutions unoriented high-precision rectangle bench78

fiOptimal Rectangle Packing: Absolute Placement Approach

Size
N
6
7
8
9
10
11
12
13
14
15

HK10
Time

Empty Space
Time

Dynamic
Time

HK11
Time

:00
:02
1:11
1:51

:00
:00
:00
:03
1:57
41:40
7:30:26

:00
:00
:00
:00
:02
:57
6:38
2:20:12
1:05:56:14

:00
:00
:00
:00
:01
:18
:33
16:41
46:56
4:28:20

Table 11: CPU times various packers find minimum-area bounding boxes containing
unoriented rectangles 11 12 , 21 13 , 13 41 , ..., N1 N 1+1 .
mark. column going left right, add one new technique minimal
bounding box problem.
HK10 number bounding boxes required simply scaling problem
instance described completely integers. column called Subsets improves upon
second testing bounding boxes whose dimensions constrained
subset sums technique. column called Mutex improves upon third rejecting
bounding boxes subset sum corresponding width mutually exclusive
subset sum corresponding height. HK11 improves upon previous packer using
information learned infeasible attempt reject future bounding boxes.
Using improvements, N =10 test 4,500 times fewer bounding boxes compared
previous state-of-the-art. instance HK10 ran memory last
bounding box sheer number 1 1 rectangles created perfect
packing transformation. introduction prime number 11 denominator
problem instance responsible increased difficulty N =9 N =10.
5.5.2 Containment Problem
Table 11 compares performance various packers using techniques.
decoupled minimal bounding box problem containment problem,
table use optimizations minimal bounding box problem,
compare individual techniques applied containment problem. Therefore,
performance data reported required solve overall problem using various
containment problem packers.
first column gives size problem instance high-precision rectangle
benchmark. previous tables, successive column left right improves upon
previous column additional technique. column called HK10 corresponds
using previous state-of-the-art improved minimal bounding box algorithm.
column called Empty Space improves upon HK10 precomputing subset
79

fiHuang & Korf

sums prior searching x-coordinates, uses techniques consolidate empty
space y-coordinates. column called Dynamic improves upon previous one
dynamically computing subset sums. Finally, last column called HK11 adds
ability learn unplaced rectangles exclude subset sums computation
exploring infeasible subtree. data collected using Linux eight core 3GHz
Intel Xeon X5460 without parallelization.
N =10, problem scaled 27,720 times dimensions, requiring HK10
create 6,597,361 1 1 units empty space perfect packing transformation
causing run memory. Empty Space could complete N =13 within
day sheer number subset sums must explored x-
y-coordinates, problem avoided Dynamic.
5.5.3 Comparison Relative Placement
interesting note number bounding boxes appears increasing exponentially, mostly likely due exponential growth number subset sums
introduced successive rectangle high-precision benchmark. difficulty
unoriented high-precision rectangle benchmark compounded fact
precision increases, branching factor single x- y-coordinate values
containment problem also increases.
contrast absolute placement technique, Moffitt Pollacks (2006) relative
placement techniques enumerate different exact locations rectangles,
therefore promise immune problem high-precision rectangles. used
variable every pair rectangles represent relations above, below, left, right.
search algorithm required least one non-overlapping constraints
true every pair rectangles. meta-CSP approach modeled work
Dechter, Meiri, Pearl (1991) solving binary constraint satisfaction problems,
included various pruning techniques model reduction, symmetry breaking,
graph-based pruning heuristics (Korf et al., 2010). solve minimum bounding box
problem branch-and-bound algorithm, evaluating size bounding box
non-overlapping relationships determined, keeping track bounding
box smallest area seen far.
Note contrast, solver tests bounding boxes order non-decreasing area.
Also, size formulation uses N 2 variables use N . Finally,
packer returns one optimal solution opposed ours, work
returning optimal solutions.
able benchmark code machine order provide
kind comparison methods ours. crude comparison,
cannot run packer unoriented high-precision rectangle benchmark since
hard-coded packer unoriented consecutive-rectangle benchmark, much
easier benchmark shown Table 7.
first column Table 12 refers problem size. second column called MP06
gives CPU time required Moffitt Pollacks code problem instances
unoriented consecutive-rectangle benchmark, uses low-precision rectangles. third
column called HK11 gives CPU time required packer problem instances
80

fiOptimal Rectangle Packing: Absolute Placement Approach

Size
N

MP06
Time

HK11
Time

10
11
12
13
14
15

:03
:13
2:26
17:40
1:48:09
7:27:42

:01
:18
:33
16:41
46:56
4:28:20

Table 12: CPU times required Moffitt Pollacks packer unoriented consecutiverectangle packer unoriented high-precision rectangle benchmarks.

unoriented high-precision rectangle benchmark. data point table
collected using eight core 3GHz Intel Xeon X5460 Linux without parallelization. Note
algorithm packs number rectangles somewhat faster Moffit
Pollacks.
5.6 Summary High-Precision Rectangles
section proposed new benchmark consisting instances rectangles
high-precision dimensions well techniques using subset sums limit number
positions must considered, rules filter subset sums minimal
bounding box containment problems, methods learn infeasible subtrees,
ways reduce number rectangles created perfect packing transformation.
techniques exploit special properties benchmark, useful
rectangles high-precision dimensions.
Using methods, solved six problems N =15 new benchmark compared using low-precision packer scaled instance. packer
two orders magnitude faster N =9 previous state-of-the-art, tests
4,500 times fewer bounding boxes. cursory comparison state-of-the-art using
relative placement search space shows perform slightly faster
Moffitt Pollacks packer, benchmark previously shown Section
4.3.2 significantly difficult unoriented consecutive-rectangle benchmark
Moffitt Pollacks program run on.

6. Future Work
Humans solve jigsaw puzzles asking particular piece go, well
asking piece go empty region. packer makes use
models, former x-coordinates latter y-coordinates. would
interesting see applicable dual formulation packing, layout,
scheduling problems. Currently, work x-coordinates asking
go?, work y-coordinates asking goes location?
method reduced time spent y-coordinates much time spent
81

fiHuang & Korf

working x-coordinates orders magnitude greater time spent working
y-coordinates. suggests performance might improved considering
models simultaneously.
another direction continued work, data indicates number bounding
boxes explored minimum bounding boxes solver main bottleneck solving
larger instances unoriented high-precision rectangle benchmark. observation
make across many bounding boxes, partial solutions
explored, resulting much redundant computation. Consequently, branch-and-bound
method starts large bounding box, gradually reduces dimensions
various packings explored would promising avenue research.

7. Conclusions
presented several new improvements previous state-of-the-art optimal
rectangle packing. Within schema assigning x-coordinates prior y-coordinates,
introduced dynamic variable order x-coordinates, constraint adapts
Korfs (2003) wasted space pruning heuristic one-dimensional case. ycoordinates work perfect packing transformation original problem,
using model assigns rectangles empty corners, inference rules reduce
models variables.
improvements search y-coordinates helped us solve N =27 consecutive-square benchmark order magnitude faster previous state-ofthe-art, improvements search x-coordinates also gave us another order
magnitude speedup N =28, compared leaving optimizations out.
techniques, 19 times faster previous state-of-the-art largest
problem solved date, allowing us extend known solutions consecutive-square
benchmark N =27 N =32. Furthermore, data show little time spent
searching y-coordinates, suggesting rectangle packing may largely reduced
problem determining x-coordinates.
techniques presented pick y-coordinates tightly coupled dual
view asking must go empty location. Furthermore, searching xcoordinates, pruning rule based analysis irregular regions empty space,
dynamic variable order also rests observation less empty space leads
constrained problem. success techniques rectangle packing make
worth exploring many packing, layout, scheduling problems.
also introduced two new benchmarks, one oriented one unoriented,
include rectangles various aspect ratios. new benchmarks avoid various properties
easy instances, identified, shown much harder
side-by-side comparison various benchmarks using state-of-the-art packer.
also proposed several search strategies improve performance new benchmarks. improved upon strategies used handle dominance conditions, proposed
variable ordering heuristic based increasing branching factor generalizes previous
strategies, tuned global interval parameter, introduced method balance sizes
intervals assigned x-coordinate variables.
82

fiOptimal Rectangle Packing: Absolute Placement Approach

experiments revealed takes orders magnitude time solve new
benchmarks compared instances consecutive-square benchmark
number rectangles. therefore advocate inclusion new, difficult
benchmarks suite benchmarks used research optimal rectangle packing. Finally, using techniques together, solved N =21 oriented equal-perimeter
benchmark 500 times faster, N =16 unoriented double-perimeter benchmark 40 times faster simply using methods tuned consecutive-squares.
order test limits rectangle packer, presented new high-precision benchmark specifically capturing pathological case successive rectangle
quickly increases precision required represent coordinate locations. presented
various techniques adapt absolute placement approach handle types instances, including dynamically using subset sums limit number coordinate values
must tested, mutex reasoning allows us reject certain combinations subset
sums used bounding boxs width height, general method rejecting future subset sums based previously infeasible search, finally memory-efficient adaptation
perfect packing transformation high-precision rectangle instances.
solved N =12 high-precision benchmark half minute, 800 times faster
basic version packer augmented high-precision version perfect
packing inference rules run memory. also first instance
requiring precision exceeding capacity 32-bit integer. techniques allowed us
solve N =15 compared N =9, largest instance low-precision techniques
alone could solve. methods also reduced number bounding boxes generated
factor 4,500. point solving problems require minimum 39 bits
precision, meet requirements many real-world problems.
provided comparison state-of-the-art relative placement packer showing
absolute-placement packer remains competitive even rectangles high-precision, reported promising avenues research may potentially give absolute
placement approach clear competitive edge relative-placement methods.
Although mainly focused obtaining optimal solutions benchmarks,
work may easily adapted applications requiring quick suboptimal solutions
simply replacing algorithm minimum bounding box problem alternatives
anytime algorithm described Section 3.3.1.
7.1 Comparison Constraint Programming Methodologies
clearly tradeoffs taking ground-up programming approach C++
taking constraint programming approach. latter provides quick prototyping
reuse constraint libraries researchers already implemented, also
forces problem expressed abstract constraint language. abstract
layer turns add unnecessary overhead algorithms data structures
one naturally uses solve problem optimal rectangle packing.
example, previously described, cumulative constraint, simply add
constant consecutive range integer array assign x-coordinate
rectangle. backtrack, scan array subtract constant.
Scanning manipulating arrays, iteration, fast pushing popping program
83

fiHuang & Korf

stack recursive algorithms precisely operations modern computer hardware
optimized for. significant explore two trillion search nodes
N =32 square-packing benchmark, fact solver spends 75%
time array manipulation operations alone. explain
orders magnitude speedup processing x-coordinate solutions 1D array
instead 2D bitmap Korf (2003). move 1D arrays, 2D bitmaps,
abstract representations variables values constraint programming, patterns
computation data structures simply become distant underlying
hardware optimized for.
optimal rectangle packing, happens algorithms data structures
naturally solve problem map nicely form function hardware modern
computers. Note one may always port code constraint module may
called constraint solver, still computational indirection
module backtracking control logic constraint solver. sacrifice make
approach, however, fact solver tailored specifically rectangle
packing problem defined it, would require implementation effort
reconfigure algorithms heuristics slightly different rectangle packing problem.
hope, however, latter problem ameliorated disciplined object-oriented,
modular software design.

8. Broader Lessons
Beyond specific problem rectangle packing, broader lessons learn
work? believe several.
One main applications rectangle packing scheduling. described
introduction, rectangle packing problem abstraction scheduling problem
different tasks take different amounts time, require different amounts onedimensional resource must allocated contiguously, memory computer.
width bounding box becomes total time, height total amount
resource available, job becomes rectangle width equal time duration,
height equal amount resource required.
found, however, vast majority time used rectangle packer
assigning x-coordinates rectangles, subject cumulative constraint,
every x-coordinate bounding box, sum heights rectangles overlap x-coordinate cannot exceed height bounding box.
important subpart rectangle-packing problem models much general problem
known resource-constrained scheduling problem. scheduling
problem described above, without constraint resource allocated contiguously. example, scheduling tasks planetary rover limited power budget,
sum power requirements tasks active given time cannot
exceed total power budget rover. Thus, subpart rectangle packer
used tackle general scheduling problem.
Another general lesson learned work absolute placement
approach various packing problems two, three, dimensions may effective
even problems high precision dimensions. One might expect absolute placement
84

fiOptimal Rectangle Packing: Absolute Placement Approach

would competitive relative placement approaches problems,
key success area instead considering possible placements,
consider placements correspond subset sums relevant dimensions.
guarantee approach work high-precision packing problems,
shown least worth considering, may effective.
Perhaps largest lesson learned encouraging discouraging.
problem rectangle packing extremely simple, understood played
game children. Yet research last decade described shows
efficient algorithms quite complex. best algorithms simple
problem complex, likely best algorithms complex problems
even complex, discouraging part. encouraging part
history research shown new idea result order magnitude
improvement previous state art larger problems, suggesting
still significant progress made problem, extension others like it.

Acknowledgments
wish thank Reza Ahmadi, Adnan Darwiche, Adam Meyerson advice
work. also thank Michael Moffitt making packer available. research funded part National Science Foundation grant number IIS0713178. source code optimal rectangle packer open sourced available
http://code.google.com/p/rectpack.

References
Aggoun, A., & Beldiceanu, N. (1993). Extending chip order solve complex scheduling
placement problems. Mathematical Computer Modelling, 17 (7), 5773.
Beldiceanu, N., & Carlsson, M. (2001). Sweep generic pruning technique applied
non-overlapping rectangles constraint. CP 01: Proceedings 7th International
Conference Principles Practice Constraint Programming, pp. 377391, London, UK. Springer-Verlag.
Beldiceanu, N., Carlsson, M., & Poder, E. (2008). New filtering cumulative constraint
context non-overlapping rectangles. Perron, L., & Trick, M. A. (Eds.),
CPAIOR, Vol. 5015 Lecture Notes Computer Science, pp. 2135. Springer.
Beldiceanu, N., Carlsson, M., Poder, E., Sadek, R., & Truchet, C. (2007). generic geometrical constraint kernel space time handling polymorphic k-dimensional
objects. Bessiere, C. (Ed.), CP, Vol. 4741 Lecture Notes Computer Science,
pp. 180194. Springer.
Bhattacharya, S., Roy, R., & Bhattacharya, S. (1998). exact depth-first algorithm
pallet loading problem. European Journal Operational Research, 110 (3), 610625.
Chan, H. H., & Markov, I. L. (2004). Practical slicing non-slicing block-packing without
simulated annealing. GLSVLSI 04: Proceedings 14th ACM Great Lakes
symposium VLSI, pp. 282287, New York, NY, USA. ACM.
85

fiHuang & Korf

Chazelle, B. (1983). bottomn-left bin-packing heuristic: efficient implementation.
IEEE Transactions Computers, C-32 (8), 697707.
Clautiaux, F., Carlier, J., & Moukrim, A. (2007). new exact method twodimensional orthogonal packing problem. European Journal Operational Research,
183 (3), 11961211.
Dechter, R., Meiri, I., & Pearl, J. (1991). Temporal constraint networks. Artificial Intelligence, 49 (1-3), 6195.
Dowsland, K. A. (1987). exact algorithm pallet loading problem. European
Journal Operational Research, 31 (1), 7884.
Dowsland, K. A., & Dowsland, W. B. (1992). Packing problems. European Journal
Operational Research, 56 (1), 214.
Huang, E., & Korf, R. E. (2009). New improvements optimal rectangle packing.
Boutilier, C. (Ed.), IJCAI, pp. 511516.
Huang, E., & Korf, R. E. (2010). Optimal rectangle packing non-square benchmarks.
AAAI10: Proceedings 24th National Conference Artificial intelligence, pp.
317324. AAAI Press.
Huang, E., & Korf, R. E. (2011). Optimal packing high-precision rectangles. Burgard,
W., & Roth, D. (Eds.), AAAI. AAAI Press.
Korf, R. E. (2003). Optimal rectangle packing: Initial results. Giunchiglia, E., Muscettola,
N., & Nau, D. S. (Eds.), ICAPS, pp. 287295. AAAI.
Korf, R. E. (2004). Optimal rectangle packing: New results. Zilberstein, S., Koehler, J.,
& Koenig, S. (Eds.), ICAPS, pp. 142149. AAAI.
Korf, R. E., Moffitt, M. D., & Pollack, M. E. (2010). Optimal rectangle packing. Annals
Operations Research, 179 (1), 261295.
Lahrichi, A. (1982). Scheduling: notions hump, compulsory parts use
cumulative problems. Comptes Rendus de Academie des Sciences, Paris, 294, 209211.
Lesh, N., Marks, J., McMahon, A., & Mitzenmacher, M. (2004). Exhaustive approaches
2d rectangular perfect packings. Information Processing Letters, 90 (1), 714.
Li, S. C., Leong, H. W., & Quek, S. K. (2004). New approximation algorithms dynamic storage allocation problems. Chwa, K.-Y., & Munro, J. I. (Eds.), COCOON,
Vol. 3106 Lecture Notes Computer Science, pp. 339348. Springer.
Lipovetskii, A. I. (2008). geometrical approach computation optimal solution
rectangle packing problem. American Mathematical Society Translations, 222,
93110.
Lodi, A., Martello, S., & Monaci, M. (2002). Two-dimensional packing problems: survey.
European Journal Operational Research, 141 (2), 241252.
Lodi, A., Martello, S., & Vigo, D. (2002). Recent advances two-dimensional bin packing
problems. Discrete Applied Mathematics, 123 (1-3), 379396.
Martello, S., & Vigo, D. (1998). Exact solution two-dimensional finite bin packing
problem. Management Science, 44 (3), 388399.
86

fiOptimal Rectangle Packing: Absolute Placement Approach

Meir, A., & Moser, L. (1968). packing squares cubes. Journal Combinatorial
Theory, 5 (2), 126134.
Mitola, J., & Maguire, G. (1999). Cognitive radio: making software radios personal.
IEEE Personal Communications Magazine, 6 (4), 1318.
Moffitt, M. D., & Pollack, M. E. (2006). Optimal rectangle packing: meta-csp approach.
Long, D., Smith, S. F., Borrajo, D., & McCluskey, L. (Eds.), ICAPS, pp. 93102.
AAAI.
Onodera, H., Taniguchi, Y., & Tamaru, K. (1991). Branch-and-bound placement building
block layout. DAC 91: Proceedings 28th ACM/IEEE Design Automation
Conference, pp. 433439, New York, NY, USA. ACM.
Simonis, H., & OSullivan, B. (2008). Search strategies rectangle packing. Stuckey,
P. J. (Ed.), CP, Vol. 5202 Lecture Notes Computer Science, pp. 5266. Springer.
Simonis, H., & OSullivan, B. (2011). Almost square packing. Achterberg, T., & Beck,
J. C. (Eds.), CPAIOR, Vol. 6697 Lecture Notes Computer Science, pp. 196209.
Springer.
Sweeney, P. E., & Paternoster, E. R. (1992). Cutting packing problems: categorized,
application-orientated research bibliography. Journal Operational Research
Society, 43 (7), 691706.
Weglarz, J. (1999). Project scheduling: recent models, algorithms applications. Springer,
Kluwer.
Yap, R. H. C. (2004). Constraint processing rina dechter, morgan kaufmann publishers,
2003, hard cover: Isbn 1-55860-890-7, xx + 481 pages. Theory Pract. Log. Program.,
4 (5-6), 755757.

87

fi
