Journal Artificial Intelligence Research 46 (2013) 579-605Submitted 4/12; published 4/13Predicting Behavior Unstructured BargainingProbability DistributionDavid H. Wolpertdavid.h.wolpert@gmail.comSanta Fe Institute,1399 Hyde Park Road, Santa Fe, NM 87501;Information Sciences Group,Los Alamos National Laboratory,MS B256, Los Alamos, NM 87545James W. Bonojwbono@gmail.comAbstractexperimental tests human behavior unstructured bargaining games, typicallymany joint utility outcomes found occur, one. suggests predictoutcome game probability distribution. contrastconventionally done (e.g, Nash bargaining solution), predict singleoutcome. show translate Nashs bargaining axioms provide distributionoutcomes rather single outcome. prove subset axioms forcesdistribution utility outcomes power-law distribution. Unlike Nashs originalresult, result holds even feasible set finite. feasible set convexcomprehensive, mode power law distribution Harsanyi bargainingsolution, require symmetry Nash bargaining solution. However,general modes joint utility distribution experimentalists Bayesoptimal predictions joint utility. bargains corresponding modesjoint utility distributions modes distribution bargains general, sinceone bargain may result joint utility. introducing distributionalbargaining solution concepts, show external regulator use optimallydesign unstructured bargaining scenario. Throughout demonstrate analysiscomputational experiments involving flight rerouting negotiations National AirspaceSystem. emphasize results formulated unstructured bargaining,also used make predictions noncooperative games modelerknows utility functions players possible outcomes game,know move spaces players use determine outcomes.1. Introductiongame theory, bargaining refers scenarios two people must comejoint agreement outcome. structured bargaining, scenario modelednoncooperative game, players making explicitly delineated alternating moves, e.g.,proposals counter-proposals (Osborne & Rubinstein, 1994; Aumann & Hart, 1992).contrast, unstructured bargaining, scenario modeled without explicit delineationalternating moves. Instead known modeler feasible setjoint-utilities would arise possible bargains humans might reach.Arguably, real-world bargaining scenarios, interaction bargainersc2013AI Access Foundation. rights reserved.fiWolpert & Bonofree form, requires unstructured bargaining analysis. Therefore, ablepredict potentially emulate behavior interacting humans, needaccurate model human unstructured bargaining.unstructured bargaining two spaces; space bargains spacejoint expected utility vectors players assign bargain. Note mayone bargain results outcome. shorthand, conventionalleave term expected implicit refer utility. Similar shorthandrefer joint (expected) utility vector outcome.game theoretic modeling date unstructured bargaining starts specification feasible set possible outcomes. knowledgemodeler concerning unstructured bargaining scenario. particular, modelerignorant spaces possible moves players may make reach bargaineven ignorant space possible bargains.Traditional unstructured bargaining concerned specifying map takesfeasible set single outcome x S, i.e., point-valued solution concept (Nash,1950; Harsanyi & Selten, 1972; Kalai & Smorodinsky, 1975; Kalai, 1977). example,case Nashs original work, showed specified S,axioms force unique prediction outcome x S. work regards mapnormatively, providing fair reasonable bargaining outcomes. work regardspositively, prediction agreement reached humans bargainingunstructured manner (Nydegger & Owen, 1974; Roth & Malouf, 1979; Camerer, 2003).paper concerned positive viewpoint. extensive discussioninterpretation Nashs solution, see work Rubinstein, Safra, Thomson (1982).contradiction theoretical positive work, experimental literature makes clearreal world one bargain non-zero probability outcomegiven unstructured bargaining problem (Camerer, 2003; Roth & Malouf, 1979).1accommodate this, paper consider maps take unstructured bargainingproblems feasible set probability distribution S, rather single elementS.2 this, derive parameterized set possible maps bargaining problemsdistributions problems. (Intuitively, parameters reflect bargainingpower players.) call maps distributional bargaining concepts, callimages utility distributions.many advantages using distributional bargaining concepts, additionaccording experimental data better point-valued solution concepts. majorone arises external regulator modify aspects bargaininggame utility function bargaining outcomes. modifying game,regulator changes associated distribution, therefore modifies valueexpected utility. Accordingly, calculate Bayes-optimal modificationgame.1. fact, given noisy nature human behavior, would stunning certain physically possibleoutcomes actually occurred exactly 0 probability, rather small, non-zero probability.2. approach allows either finite infinite; succinctness generically referprobability distribution even infinite properly refer probability densityfunction.580fiPredictive Unstructured Bargainingapproach deriving distributional bargaining concept translate Nashsaxioms unstructured bargaining conventionally applied maps produce singleutility outcome rather utility distribution apply distribution-valued maps.precise, use probabilistic versions Nashs axioms Scale Invariance (SI)Independence Irrelevant Alternatives (IIA). also use probabilistic versionaxiom Translation-Invariance utilities (TI).Adopting Bayesian perspective, view axioms formalizations ignorance modeler many scenarios, rather assumptions human behavior:us, SI means modeler know anything relative probabilities outcomes chosen players likely change one simply scales utilitiespossible outcomes. Similarly, us IIA means modeler know anythingrelative probabilities outcomes chosen players likely changesubset possible outcomes removed. us, TI means modelerknow anything relative probabilities outcomes chosenplayers likely change utilities possible bargains simply translatedconstant.axioms add extra one outcomes players strictlybetter default outcome Non-Zero probability (NZ). (NZ imposedholds experiments, due subject inattention nothing else.)work, which, like ours, also concerned distributions (Peters& Tijs, 1984). However, work Peters Tijs (1984) differs two importantways. First, also use probabilistic versions SI IIA, unlike us, alsouse axiom Pareto optimality (PAR). (As discussed conclusion, needimpose assumption binding contracts, therefore need assume joint utilityPareto-optimal.) addition, unlike us, use NZ, (explicitly) useTI. Second, Peters Tijs translate IIA SI different probabilistic versionsours.result difference, arrive different distributional solution conceptPeters Tijs. particular, axioms force distribution powerlaw set joint utility outcomes Pareto superior default outcome. contrast,axioms used Peters Tijs result tightly characterized solution, powerlaw otherwise. Accordingly, approach Peters Tijs used deriveBayes-optimal modification game might applied external regulatorgame.Perhaps importantly, version IIA means solution concept appliesbargaining game, even finite ones non-convex comprehensive.quite important positive bargaining solution concept. real-world unstructuredbargaining, common bargainers consider finite number possibleoutcomes. (Typically, real human bargainers field consider optiontossing weighted coin choose among possible bargains.) scenariosfeasible set finite.generally, since dispense PAR, nothing restricts analysisscenarios traditionally viewed bargaining. discuss conclusion,results also used predict outcomes noncooperative games whenevermodeler knows feasible set games joint-utility outcomes, cannot581fiWolpert & Bonotractably elaborate move spaces players. situations, best modelerprovide distribution final joint utility outcome. distributionalbargaining concept provides way this.current paper work Peters Tijs papers extendgame theory replacing solution concepts point-valued (or set-valued) solution concepts probability distributions. particular, work WolpertBono (2011) introduces distribution-valued solution concept noncooperative gamesmodeler know move spaces. work introduces mapinput specification arbitrary non-cooperative game. output mapdistribution possible mixed strategy profiles (i.e., player joint choices)input noncooperative game. contrast, present paper introduces map takingspecification unstructured bargaining game input, producing distributionplayer joint utilities output.also recent papers artificial intelligence literature that, like ours, focusnon-equilibrium solution concepts (Brafman & Tennenholtz, 2003; Rezek, Leslie, Reece,Roberts, Rogers, Dash, & Jennings, 2008; Aydogan & Yolum, 2012; Duan, Dogru, Ozen, &Beck, 2012). Rather merely invoking equilibrium, Brafman Tennenholtz (2003)employ reinforcement learning algorithm efficiently achieve coordination commoninterest stochastic games. Rezek et al. (2008) look game theoretic solution conceptsmachine learning perspective, i.e. assume players make inferencesopponents Bayesian framework derive novel fictitious play algorithm. Recentwork AI focuses bargaining, Aydogan Yolum (2012)Duan et al. (2012), particularly closely related current paper. main differencepaper focuses unstructured game, whereas focus previous workstructured games.1.1 Contribution Paperfirst contribution paper derive distributional bargaining concept,outlined above. focus two major advantages distributional bargainingconcept point-valued set-valued bargaining concepts: 1) ability modelerapply decision theory predict outcomes, 2) ability external regulatoremploy control theory optimally regulate system.elaborate first advantage, consider modeler outcomebargaining often loss function, measuring quality predictingjoint expected utility outcome x actual outcome x0 .3 Given lossfunction, given distribution outcomes, well-defined Bayes-optimalprediction single joint utility outcome. Bayes-optimal prediction varyloss function. Accordingly, whatever distributional bargaining concept used, generalassociated Bayes-optimal prediction neither likely joint expected joint utilityoutcome (the Harsanyi solution) likely bargain.Let vector-valued function taking bargains associated joint utility outcomes. general, need invertible. (E.g., feasible set set K possible3. shorthand, often abbreviate joint expected utility outcome joint utility outcome,even outcome.582fiPredictive Unstructured Bargainingjoint choices among N players, K N + 1.) cases cannot goprediction joint utility (whether made conventional bargaining conceptdistributional one) prediction joint bargain. One would need likelihood functionP (b | x) giving relative probabilities bargains b given joint utility x invertprediction joint utility prediction bargain reached.hand, invertible, distribution joint utilities fixdistribution bargains. However, even case, Jacobian non-uniform,likely joint utility likely joint bargain. Intuitively, mayconcentrate probability density regions possible joint utility outcomes (thosecorresponding relatively many possible bargains), diminish others.Note though, feasible set countable invertible, issue nonuniform Jacobian disappears. provides yet another benefit using distributionbargaining concept like applicable countable (and even finite) feasiblesets, convex comprehensive ones.illustration foregoing, consider scenario naturally modeled unstructured bargaining. Often path aircraft National Airspace Systemrenegotiated inflight, e.g., due unforeseen weather. negotiations followparticular protocol unstructured. Accordingly, use illustrateNash distributional bargaining model. emphasize fact even simple scenario, map bargains joint utility outcomes non-invertible. result,straightforward evaluate probability density function possible joint utilityoutcomes, cannot said evaluating relative probabilities various bargains.address second major advantage distribution-valued solution concept,i.e. ability external regulator employ control theory optimally regulatesystem, consider external regulator real-valued welfare function definedbargains players, modify parameters bargaininggame. show distributional solution concept allows external regulatorperform Bayes-optimal configuration bargaining game, i.e., regulatorset parameters game control optimize expected welfareresultant bargaining outcome. context Nash distributional bargaining concept,call approach setting parameters Nash distributional bargaining management.example, suppose regulator modify set allowed bargainswithin range. case, Nash distributional bargaining concept useddetermine regulators optimal modification. Similar external interventions wouldchanging default bargain, even modifying relative bargaining power players.complicated type external intervention regulator presentsnon-binding suggested bargain players. view suggested bargainSchelling-like focal point negotiations, introduce model mappingsuggested bargain associated distortion distribution bargaining outcomes.model provides regulator well-defined algorithm choosing suggestedbargain optimizes expected welfare outcome negotiations. useapplication flight rerouting negotiations demonstrate Nash distributional bargainingmanagement using suggested bargains.type optimal control strategic agents captured Nash distributionalbargaining management concept recent theme artificial intelligence literature.583fiWolpert & Bonoexample, growing literature focused optimally managing negotiationsstrategic, self-interested agents (Brafman & Tennenholtz, 1996; Chalamish & Kraus, 2012;Lopez-Carmona, Marsa-Maestre, Klein, & Ito, 2012). primary differencepaper current literature bargaining game unstructured, othersuse structured game. management unstructured flight rerouting negotiationsalso considered type constrained automated mechanism design, like frameworkstudied Vorobeychik, Reeves, Wellman (2012). is, mechanismpaper bargaining scenario modified (automated) external regulatorunique flight rerouting game. Similarly, domain air traffic management, workAgogino Tumer (2012) studies multi-agent approach managing air traffic flowsagents reinforcement learners. again, main differencepaper others difference controlling structured unstructuredgames.Despite recent interest optimal control strategic agents, foundationideas found conventional concepts correlated equilibrium. Ashlagi,Monderer, Tennenholtz (2008) point out, correlated equilibrium often conceptualizedarising external party provides recommended actions game participantscannot enforce recommendations. Ashlagi et al. look value correlation,i.e. welfare improvement arising external parties recommendations,directly analogous welfare improvement arising Nash distributional bargainingmanagement.1.2 Roadmap Papersection 1.3 start overview notation. section 2, providegeneral definition distributional bargaining concepts. focus Nashdistributional bargaining concept, defined NZ probabilistic versions TI,SI IIA. next prove Nash distributional bargaining concepts take formpower law distribution. result holds bargaining games, even non-convex,non-comprehensive ones, even ones finite set possible bargains.section 3, discuss Nash distributional bargaining concept. focusmathematical structure, physical meaning, normative implications externalregulator change feasible set. show mode Nash distributionalbargaining concept Harsanyi bargaining solution. point alsoimpose Nashs symmetry axiom, instead get Nash bargaining solution modedistribution outcomes.section 4 introduce model flight rerouting negotiations. showsetting yields feasible set convex, comprehensive, even connected.also demonstrate NDB model use prediction. also explore issuesinvertibility bargaining set mentioned above.section 5 develop concept Nash distributional bargaining management,external regulator make recommendation bargainers modifyaspect unstructured bargaining game. demonstrate concepts using flightrerouting scenario show external regulator make welfare improvements.conclude discussion results directions future research.584fiPredictive Unstructured Bargaining1.3 Notationconsider conventional N -dimensional unstructured bargaining problems. problem pair bounded feasible set, RN , special disagreement point, S.assume contains least two elements, exists xx (i.e, generalized strict inequality {xi > di i} holds). referproblem simple = 0 point x (i.e., set N utility valuesbargaining problem) component less 0. Given bargaining problem (S, d),use term outcome refer element S. refer (S, d) standardcontains open set convex comprehensive4 .define + B two sets A, B RN mean set x written+ b A, b B, similarly B. (So particular, B\ B.) also assume usual topology RN , etc. Given RNk RN k 0, define kA subset RN given replacingx Hadamard (i.e., component component) product kx , (k1 x1 , k2 x2 , . . .).Ndefine AB two sets A, B RR similarly.shorthand, throughout use R measure implicit. particular,feasible sets finite, expressions like dx . . . implicitly use point mass measure, i.e.,equivalent sums.2. Nash Distributional Bargaining ConceptsBayesian perspective, interested posterior probability density function,P (x | S, d). proceed decomposing distribution priorlikelihood. Instead, analogy Nashs approach, model P (x | S, d) directly, usingNashs axioms restrict form.convenient simplify notation, (again following Nash)talk terms maps (S, d) distributions x rather terms P (x | S, d):Definition 1. N -dimensional (distributional) bargaining concept mapN -dimensional bargaining problem (S, d) probability density function supportrestricted S. (When countable, image probability distribution ratherprobability density function, generically refer distribution implicit cardinality making clear whether mean probability distribution probabilitydensity function.)generically indicate distributional bargaining concept symbol ,indicate value problem (S, d) S,d . S,d (x) non-negative real numberproduced applying (S, d) evaluating resultant density function x.use unstructured bargaining approach make prediction human bargaining behavior particular physical setting (laboratory field), addition explicitly specifying bargaining problem (S, d), must also specificationphysical setting. Often specification implicit, make explicitly.restrict attention bargaining problems (S, d) associated physical settingsevery x physically possible. go also restrict attention4. Recall comprehensive x S, x d, S.585fiWolpert & Bonophysical settings physically possible x x occurnon-zero probability. justify noting laboratory field settings,even bargaining structured Pareto efficient outcome dominantNash equilibrium underlying game, would still non-zero probabilityoutcomes arise, due bounded rationality players. (A stronger versionreasoning invoke would require even outcomesplayer worse disagreement point occur non-zero probability.)Given foregoing, straightforward translate Nashs unstructured bargainingaxioms context distributional bargaining concepts. particular, SI, TI IIAget translated follows:Definition 2. N -dimensional distributional bargaining concept Nash distributional bargaining (NDB) concept RN ,pSI x, y, x, d, k 1,S,d (x)S,d (y)(kS,kd) (kx),(kS,kd) (ky)=pTI x, x d, RN , S,d (x) = S{a},da (x a),pIIA S, x, y, x, d,S,d (x)S,d (y)=T,d (x)T,d (y)NZ x, x d, S,d (x) 6= 0pSI means rescale coordinates problem get new problem,relative probability two points x original problemrelative probability rescaled versions points new (rescaled) problem.translation Nashs scaling invariance axiom, SI, context distributional bargaining concepts. results paper, could weakened pSIfollowing:pSI simple, standard, closed problem (S, 0), x, x, 0, k 1,S,0 (x)S,0 (y)=(kS,0) (kx)(kS,0) (ky)(1)expository simplicity though, use stronger version given Def. 2.pTI implements assumption translation-invariance utility functions. (Recallnotation, subtraction operator sets set subtraction.)pIIA means remove potential solutions x bargainingproblem, relative probabilities x dont change. translationNashs IIA axiom context distributional bargaining concepts. Finally,586fiPredictive Unstructured Bargainingmentioned earlier, NZ axiom says outcomes players strictly betterdefault outcome receive positive probability occurring.axiom requires solution concept conform physical reality human behavior.emphasize pIIA pSI assumptions human behavior. Ratherreflections ignorance modeler, prior probabilities Bayesianmodeling reflections ignorance. modeler know preferredscale predict player behavior, want use distributioninvariant rescalings (Jaynes & Bretthorst, 2003). pIIA formalizes.Similarly, modeler know removing set alternatives would affectrelative probabilities remaining ones, want use distributionaffect relative probabilities. pSI formalizes.5remainder section work derivation main mathematicalresult, presented Thm. 1. Loosely speaking, result says bargaining conceptmust product (over players) power law distributions, obey axiomspresented above.Given two N -dimensional problems (S, d) (T, d), define W . W .Therefore pIIA, N -dimensional Nash bargaining concept , x,x, d, S,d (x)/S,d (y) = W,d (x)/W,d (y). Similarly, W , therefore x,x, d, T,d (x)/T,d (y) = W,d (x)/W,d (y). establishes following:Lemma 1. Fix two N -dimensional problems (S, d) (T, d). x,x, d, N -dimensional NDB concept ,S,d (x)S,d (y)=T,d (x)T,d (y)(2)kinds geometric distortions underlying existence proofs Nash, KalaiSmorodinsky egalitarian solutions cannot applied (S, d) non-standard,especially contains finite number elements. inability handle finiteone major obstacles Nash bargaining theory. real-world unstructuredbargaining, quite common people bargain finite number possible outcomes, without ever considering possibility using randomization device decidefinal bargain.6contrast, Lemma 1 means establish form T,d particularclass problems (T, d) infinite , established form pair(S T, d). true even neither convex comprehensive. Indeed,even finite.7 following result provides form T,d particular classproblems (T, d).5. course, modeler knowledge preferred scales / humanplayers would change distribution alternatives removed, knowledgereplace pSI /or pIIA, respectively.6. Indeed, one could argue real bargaining scenarios cannot involve uncountably infinite numberbargaining outcomes. all, real human beings cannot specify digits real number giveninterval. communicating one another, real human beings able specifynumbersfinite precision, together finite set infinite-precision real numbers, like , 2, etc.7. authors examined bargaining solutions non-standard domains. example, many papersextend Nash solution non-convex (Kaneko, 1980; Herrero, 1989; Conley & Wilkie, 1996; Zhou,587fiWolpert & BonoTheorem 1. Let N -dimensional NDB concept closed, simple,standard problem (T, 0), T,0 (x) differentiable throughout interior .constants ai , = 1, . . . n, independentQ d, problems (S, d)x x d, S,d (x) (xi di )ai .Proof. Define 00 {d} x0 x d. use pTI write S,d (x) = 00 ,0 (x0 ).suffices prove theorem x0 0 bargaining problem (S 00 , 0).Let 0 {S 00 \ {x00 00 : x00i 0} {0}}. 0 00 without x00least one player better disagreement point, unioneddisagreement point. (S 0 , 0) simple bargaining problem. addition, pIIA,x0 , 0 00 ,0 (x0 )0 ,0 (y 0 )=00 ,0 (x0 )00 ,0 (y 0 )Therefore suffices prove theorem problem (S 0 , 0).Let convex, comprehensive closure 0 . (T, 0) simple, standardbargaining problem whose interior contains (S 0 , 0). Therefore pIIA, suffices proveproposition (T, 0).Let interior . Note hypothesis, T,0 (.) differentiable function.N -dimensional z 0, define N -dimensional vector ln[z] , (ln(z1 ), ln(z2 ), . . .).Make similar definitions ez sets ln[A] eA subset RN .Note exception points zero-valued components, pointwritten vector ez z RN , since (T, 0) simple bargaining problem.particular, every point written ez z RN .Say z point T,0 (ez ) non-zero (so ez ). Defineassociated scalar P(z) , ln[T,0 (ez )]. Note T,0 (.) non-zero Def. 2(i).addition, continuous derivative throughout hypothesis. Therefore P(.)continuous derivative throughout N ln[T ].Choose vector k whose components equal 1 except component i,interval (0, 1]. Define set k Ki . Note since simplestandard, Ki . Therefore ln[k] + N N .Combining pSI pIIA, x0 ,T,0 (kx0 )T,0 (ky)=T,0 (x0 )T,0 (y)Taking logarithms getP(ln[k] + ln[x0 ]) P(ln[k] + ln[y]) = P(ln[x0 ]) P(ln[y])x0 , , k Ki .1997; Mariotti, 1998a). generally replacing certain Nashs axioms. recentwork furthers analysis non-standard problems include finite domains (Mariotti, 1998b; Xu &Yoshihara, 2006; Kbrs & Sertel, 2007; Peters & Vermeulen, 2010). However, general conclusionwork goal reach; work typically fails find convincing solution conceptclosely resemble Nashs solution single-valued finite problems.588fiPredictive Unstructured BargainingGiven P must differentiable across N , take partial derivativesides equation respect ln[ki ]. Evaluate derivatives limitP(v)ki 1. allowed x0 , establishes P(u)ui vi = 0 u, v N , i.e.,establishesP(u)uiconstant across N . Therefore P linear coordinate across N .0Since true coordinates i, P(x0 ) = ln[S 0 ,0 (ex )] hyperplane acrossQ 0Nai.00Accordingly, 0 ,o (x ) must product monomials across , i.e., T,0 (x ) (xi )across . (The coefficients hyperplane give powers {ai } monomials.)Converting x0 back x gives claimed result.refer distributional bargaining concept meets conditions Thm. 1power law distributional bargaining concept.axioms giving power law distribution always hold real world.simple example, default bargain may serve focal point, case onemight presume S,d (d) > 0. cases, either differentiability assumptionTheorem 1 must relaxed, one axioms defining NDBs must be.generally, even differentiability assumed model focal point,may possible motivate distributional bargaining concepts besides powerlaw bargaining concept. particular, several axiomatic arguments motivatepredicting behavior single decision maker according logit distributionutilities (Train, 2003). Logit distributions utilities also motivated modelingbehavior multiple interacting players noncooperative game, e.g., argumentsoriginally used motivate logit Quantal Response Equilibrium (McKelvey & Palfrey,1995), arguments based maximum entropy inference (Wolpert, Harre, Olbrich,Bertschinger, & Jost, 2012). arguments suggest (but formallyderive) ideaQpredicting behavior product logit distributions, S,d (x) exp(xi ), ratherproduct monomials. (One advantage distributional bargaining conceptrequire specification d, since replacing xi xi di exponentschange value probability distribution.)rest paper, unless specified otherwise, restrict attention powerlaw distributional bargaining concepts. Note computational complexity evaluating power law distributional concept issue: one simply needs evaluateproduct monomials get relative probabilities. normalization constantalso desired, worst, gleaned via standard Monte Carlo methods.(a power law bargaining concept) S,d normalized standardproblem, ai must exceed 1. generally though, could countably infinite,utilities player given xi {1/k +di : k = 1, 2, . . .}. S,d normalizedcase, ai must exceed 0. Therefore Lemma 1, ai must always exceed 0.3. Discussion Power Law Distributional Bargaining Conceptssection discuss assorted mathematical characteristics power law distributionalbargaining concepts, together physical implications.589fiWolpert & Bono3.1 Relation Power Law Distributional Bargaining ConceptsPoint-Valued Bargaining Conceptsemphasized derive functional form Thm. 1 needanalogs last two Nashs unstructured bargaining axioms, PAR SYM. Howeverstraightforward incorporate desired. One natural way translate PARcontext distributional unstructured bargaining would require improvingevery players utility (i.e., changing x way shrinks component xi )cannot decrease probability density. Imposing NDB concept would ruleai less 0.8Similarly SYM translated mean ai must value.call power law bargaining concept meets additional requirement fullyNash distributional bargaining concept.fully NDB concept (S, d), S,DQsupportQis distribution (xi di ) scalar . fixed d, levelcurves x ofQ (xi di ) independent (assuming 6= 0). particularmaximum (xi di ) independent . Accordingly, find maximum,Qtake = 1. Accordingly, mode S,d (x) maximum x (xi di ).words, Nash bargaining solution bargaining problem (S, d) modefully NDB concept applied (S, d).addition, fixed > 0, send , probability event {xlies away Nash bargaining solution} 0. sense, solutionsNash solution become impossible limit.However, general feasible set possible bargains uncountable, functiontaking joint bargains joint utility outcomes non-uniform Jacobian.cases, likely bargain bargain corresponding Nash solution.Furthermore, finite , general Bayes-optimal guess x NashbargainingR solution. example, quadratic loss functions, Bayes-optimal guessx dx S,d (x)x. differs Nash solution, argmaxx S,d (x). Furthermore,consider expanding ith coordinate border S, leaving border alongaxis j unchanged. quadratic loss, change general changeBayes-optimal guess xj , even change Nash bargaining solution point.final comment, recall impose SYM requirement powerlaw bagaining concept, players may differ, i.e., players allowedheterogenous. case mode solution concept Nash bargainingsolution weighted Nash solution (Harsanyi & Selten, 1972), evaluatedconstants power law exponents NDB concept.3.2 Mathematical Structure Power Law Bargaining ConceptsNowhere definition power law bargaining concept explicitly refer differencesxi di . differences arise Thm. 1? Ultimately, reasondefinition bargaining problem requires specifying feasible set special8. extreme way impose PAR would make predictions using P AR(S),d , P AR(S)Pareto frontier S. Lemma 1, equivalent masking S,d Pareto frontier S,i.e., replacing new distribution 0S,d whose support restricted Pareto frontier S,x, x0 frontier, 0S,d (x)/0S,d (x0 ) = S,d (x)/S,d (x0 ).590fiPredictive Unstructured Bargainingdisagreement point within feasible set. Due this, translation invariance conditiontranslates point along point x gets evaluated. causesdifference two points invariant , reflected Thm. 1.Note also S,d product distribution support. sense, independenceplayers automatic. independence explicit part definitionpower law bargaining concept. Ultimately, independence arises factSI axiom Def. 2 involves scaling players utility independently.9One must careful interpreting player independence. meandistribution S,d , utility values players statistically independent.Unless box, general border serve couple players. example,= 0,YZS,d (xi ) =[ dxi S,d (xi , xi )]YZ[ dxi (xj )aj ]j6[xi ]= S,d (x),(3)i.e., product marginal distributions player utilities equal jointdistribution player utilities.example implications this, say border changedrange possible utility values player grow particular range utilitiesplayers, stay elsewhere.10 statistical couplingplayers generally change. Intuitively, modification S, inferlikely value xj6=i given particular value xi changed. (Thisdespite independence irrelevant alternatives axiom.)3.3 Physical Meaning Power Law Distributional Bargaining ConceptsThm. 1 specify values S,d (x) x 6 d. However, Qoften casex x 6 d, x0 x0 x, (x0i di )ai arbitrarilyclose zero.11 Moreover, given x0 x, would quite peculiar experimentscase S,d (x) > S,d (x0 ). strongly suggests meets conditionsThm. 1, stipulate S,d (x) = 0 x 6 d. (Formally though,need make requirement analysis paper.)9. example, modified axiom first rotating space RN , applying scaling operators,rotating back, would longer product distribution individual xi , ratherlinear combinations xi .10. Formally, specify box values xi : {xj [bj , tj ] : j 6= i, tj > bj j 6= i}.alone, extend associated border along coordinate i, i.e., xi , modifyexpanding set xi (xi , xi ) S.11. example, let sphere centered d, consider x x1 < 0time x2 , x3 , . . . > 0. distribution point (, x2 , x3 , . . .) arbitrarily close zerotaking small enough.591fiWolpert & Bonocomponents ai Thm. 1 physically interpreted determineddistribution S,d physically interpreted. Ultimately,physically mean unstructured bargaining scenario.One possible physical meaning S,d population average humansunstructured games feasible set disagreement point d.interpretation, one adopts power law bargaining concept, makes senseconstants ai identical. Another possibility distribution interpretedpopulation average, rather refer set N particular individuals involvedbargaining problem hand. case, one adopts power law bargaining concept,constants ai differ general. several reasons. One different peopledifferent bargaining styles, different powers persuasion. generally,bargaining scenarios certain players weak power affect outcome,least able affect aspects outcome.general two interpretations mutually consistent. Thats averaging (a population of) different distributions proportional product monomialsgive product monomials, general. interpretation one adopts ultimatelydepends interpretation one feels best characterizes bargaining scenarioconsideration.also third possible interpretation, one averagesbargainers, also set structured bargaining scenarios. interpretation,distribution unstructured bargaining interpreted average distributionsstructured bargaining scenarios. Nashs bargaining axioms would interpretedreflecting ignorance external modeler concerning structure gameplayers engaged in.3.4 Knitting Together 2-Player Distributions Get Multi-PlayerDistributionsproblematic aspects requiring IIA applies full joint distributionN players utilities n > 2. However seems less objectionable stipulateIIA applies distribution two players utilities, conditioned utilitiesplayer(s). example, require P (x1 , x2 | x3 , . . . xN , S, d) obeysIIA.12 also impose conditions defining power law bargaining conceptsconditional distribution. result conditional distributionproduct monomials, i.e., i, j 6= i,P (xi , xj | xi,j , S, d) (xi di )ai (xi,j ) (xj dj )aj (xi,j )(4)full generality exponents vary value xi,j .set n(n 1) equations involving full joint distribution S,d = P (x |S, d), parameterized matrices {ai (xi,j )}. infinite number jointdistributions P (x | S, d) obey equations simultaneously setmatrices {ai (xi,j )}: inspection, distribution power law bargaining12. Note condition concern scenario players 3 N somehow fixutilities values x3 , . . . xN , players 1 2 bargain. Rather concerns fullN -player bargaining scenario N players bargain together.592fiPredictive Unstructured BargainingQconcept (i.e., form S,d (x) (xi di )ai ) obeys equations degeneratecase matrices constants.generally, requiring Eq.s (4) simultaneously hold given setmatrices {ai (xi,j )} provides constraints conditioned two-player bargainingconcepts P (xi , xj | xi,j , S, d) knit together give full N -player bargaining concept.precise, given matrices {ai (xi,j )}, full joint distribution S,d (x) = P (x | S, d)must obey following equations:Zdx S,d (x) = 1,Zai (xi,j )aj (xi,j )dx0i,j S,d (xi , xj , x0i,j )i, j 6= i, S,d (x) (xi di )(xj dj )(5)Future work involves investigating properties knitting together conditionalbargaining concepts.4. NDB Flight Path Reroutingsection, demonstrate NDB using example flight rerouting negotiationsNational Airspace System. Flight rerouting negotiations take place humansaircrafts cockpit manning air traffic control (ATC) severe weatherair traffic result need changes scheduled flight path aircraftalready airborne. en route rerouting often referred tactical rerouting,distinguish strategic rerouting, takes place airlines operationcenter ATC flight must rerouted airborne.Tactical rerouting negotiations initiated either cockpit ATC. Thoughgenerally back-and-forth, offer/counter type feel, negotiationsfollow set protocol. Therefore, unstructured bargaining approach appropriatestudying tactical rerouting negotiations. (Strategic rerouting negotiations might alsowell suited unstructured bargaining approach.) addition, worth emphasizingalthough natural tendency pilot defer ATC, light greaterknowledge latter, legally responsibility ultimately lies pilot.Tactical rerouting negotiations generally result distance/heading pair determines way-point pilot agrees fly. example, negotiation mightlook like this:Cockpit: Denver Center, United 1492. Request 20 degrees right weather.Controller: United 1492, long need heading?Cockpit: Looks like 40 miles so.point controller might grant permission, might make counter proposal,accept 20 [degrees] left?. latter case, negotiations often continuemanner above. distance/heading pair summarized (l, ),593fiWolpert & Bono[90, 90] angle degrees, l [0, N ] distance miles.13 constitutesset bargains B, i.e., B = [0, E] [90, 90].cockpit ATC preferences bargains make,summarized utility functions set available bargains, B. example,cockpit doesnt want fly far course, also doesnt want fly closestorm center. many cases, ATC might want best cockpit. Yetcases, ATC might concerned air traffic things (e.g., impactrerouting traffic) cockpit know care about.evaluate utilities bargain (l, ) cockpit ATC, need evaluatecertain features flight path results it. determine flight pathresults (l, ) first translate bargain way-point Cartesian coordinatesystem. Cartesian coordinate system, say flights current positionw1 = (0, 0), flight pointing direction positive horizontal axis.Let r radians equivalent . means agreed way-point (l, ), locatedw2 = (l cos r , l sin r ) Cartesian coordinates.meeting way-point, w2 , flight return fix along originalflight path. rerouting negotiations, fix also part negotiated bargain.However, bargains relatively uncommon. simplify model, assume that,whatever bargain, flight return fix located along horizontal axispoint w3 = (E, 0) Cartesian coordinates. Using linear interpolation connect w1 , w2w3 , create constant-speed, constant-altitude 3D flight path, l, (t) = (el, (t), nl, (t)).order simplify notation, refer components l, (t) e(t) n(t)dependence (l, ) implicit.use l, (t) = (e(t), n(t)) calculate utility-relevant features bargain(l, ). features include total length l, (t), givenZe(t) 2 n(t) 2Ll, = dt+,whether l, (t) maintains safe distance weather center C radiusR,(1 mint ||l, (t) C|| > RDl, =0 otherwise.assume cockpits utility linear combination total length maintenancesafe distance, given by:sc (l, ) = L Ll, + Dl,real numbers L 0 0.assume ATCs utility linear combination maintenance safe distancenew flight paths impact existing air traffic. express traffic penaltyfunction H(e, n), defined Cartesian coordinates. total traffic penalty flightpath l, (t)ZHl, =dt H(e(t), n(t)).13. limit angle [90, 90] heading changes 90 degrees extremely uncommon.594fiPredictive Unstructured BargainingFeasible Set Utility Outcomes8001000Cockpit Utility1200140016001800200022006004002000200400ATC Utility60080010001200Figure 1: Feasible set joint utility outcomes rerouting unstructured bargainingproblem. feasible set non-convex comprehensive,even connected. Parameters ac = aatc = 1, L = 4, = 300, H = 0.01= 300. weather large circle center C = (150, 20) radiusR = 40. traffic penalty given H(e, n) = n.express ATCs utility satc (l, ) = + H Hl, real numbers 0H 0.thing left specify outcome negotiations break down, i.e.,default bargain, db . might that, event negotiations break down, cockpitchoose path maximizes objectives without getting clearance ATC.scenario, punishment cockpit receives result changing course withoutATC approval also enter cockpits objective. However, likely manycomplicated variable factors consider modeling effect punishment,pilot attitude, idiosyncratic airline rules, etc. Hence, simplicity, simply takedefault bargain original flight path, i.e., straight line (0, 0) (E, 0).Therefore, bargaining game given= {(xc , xatc ) R2 : xc = sc (l, ) xatc = satc (l, ) (l, ) B}= (sc (E, 0), satc (E, 0)).Figure 1 shows feasible set utility outcomes associated flight reroutingmodel. Note set neither convex comprehensive. Furthermore,breaks, feasible set consists disconnected subsets. Despite irregularitiesfeasible set, NDB distribution applied trivially, givingS,d (x) (xc dc )ac (xatc datc )aatc595fiWolpert & BonoIndifference Curves Multiple Intersections1.5Heading (radians)10.5ATCCockpit00.511.5050100150Distance200250300Figure 2: Indifference curves sc satc plotted space bargains flightpath rerouting problem. s(., .) invertible indifference curvesmultiple crossings. Parameters ac = aatc = 1, L = 4, = 300,H = 0.01 = 300. weather large circle center C = (150, 20)radius R = 40. traffic penalty given H(e, n) = n.x x d, S,d (x) = 0 otherwise.discussed introduction, want use NDB distribution jointutilities make predictions bargains b B, dont likelihoodfunction P (b | x), must able invert mapping = (sc , satc ). Unfortunately,invertible, seen noting figure 2 indifference curves scsatc multiple crossings.Recall map invertible, translate distributionjoint utilities distribution bargains would use Jacobian s.precise, (l, ) point B, NDB distribution bargains, BS,d , evaluated(l, ) givenBS,d (l, ) = S,d (sc (l, ), satc (l, )) |J(l, )|,J(l, ) determinant Jacobianfisccfifi lfi satc satcls,fififi.fiNote though even invertible, Jacobian might well-defined. Indeed,flight-rerouting contains discontinuities flight paths cross outside inside596fiPredictive Unstructured BargainingNDB Concept Translated Space Bargains806040Heading20020406080050100150Distance200250300Figure 3: NDB concept mapped back space bargains B 0 = [0, 1, ..., N ][90, 89, ..., 89, 90], ac = aatc = 1, L = 4, = 300, H = 0.01= 300. weather large circle center C = (150, 20) radiusR = 40. traffic penalty given H(e, n) = n.weather radius, variable Dl, jumps 1 zero. Partial derivatives scsatc exist points B.Say set bargains countable, e.g., grid imposed technologicalconstraints aircraft cognitive constraints pilots air traffic controllers.case would need worry discontinuities calculate Jacobian.might still non-invertible however, cannot transform distributionjoint utilities distribution bargains. fact happens flight-reroutingset bargains modified grid integer distances degrees, i.e.,B 0 = [0, 1, ..., N ] [90, 89, ..., 89, 90].Regardless cardinality B, one translate distribution joint utilitiesdistribution bargains even non-invertible, long one knows likelihoodbargains given joint utility outcomes. example, say B countable,bargains give rise particular joint utility x equally likely. case,|Bx | bargains give rise x, probability bargain simplyS,d (x)/|Bx |. Using assumption grid B 0 = [0, 1, ..., N ] [90, 89, ..., 89, 90],figure 3 shows NDB distribution flight-rerouting problem space bargains.597fiWolpert & Bono5. NDB ManagementSuppose regulator, external unstructured bargaining game, preferencesjoint utility bargain reached, formalized real-valued social welfare functionU (x S). Suppose change characteristics (S, d).example, might able change joint utility default bargain certainways, replace one several 0 S. cases, assuming model gameNDB S,d , regulator choose= argmax E (U (x))Z= argmax dx U (x)S ,d (x)(6)generally, may affect s(.), dB , and/or B, image (S, d).general setting regulator choose= argmax E (U )Z= argmax dx U (x)s (B ),s (d ) (x)B(7)generally still, social welfare function U might replaced function Wdefined space bargains B. long invertible , Eq. 7 still holdsvariant. (Just define U (x (B )) W (s1(x)).) However non-invertiblecertain , equation must modifed. situation, assumingassociated likelihood function, regulator choose action= argmax E (W (b))Z= argmax dxdb P (b | x)W (b)s (B ),s (d ) (x)B(8)addition types actions regulator, othersconsidered typical axiomatic analyses unstructured bargaining including analysispaper point. example regulators intervention simplysuggest bargain players start bargain, i.e., provide focalbargain bargaining. advise regulator cases, need modeleffects types action regulator, use model Eq. 7 .refer types optimal choice external regulator NDBmanagement. mean claim always possible implement NDBmanagement. situations external regulator may able anythingsubstantial effect distribution possible bargains players.rest section present preliminary analysis NDB management, concentratingexample regulators possible action provide focal bargain.5.1 NDB Management Rerouting Negotiationsdemonstrate simple form NDB management using example rerouting negotiations Section 4, related focal point concept introduced Schelling (1960).Specifically, assume regulator make suggestion flight rerouting598fiPredictive Unstructured Bargainingcockpit ATC begin rerouting negotiations. ideasuggestion serve focal bargain negotiations, effect raising relative probability bargains similar suggested one. suggestion externalregulator could implemented automated software operatescockpit ATC.14Use BS,d (l, ) indicate distribution (l, ) B induced NBD game(S, d). (So needed, BS,d (l, ) implicitly depends Jacobian likelihoodfunction P (b | x) = P (l, | x).) assume regulator models effectssuggested focal bargain (l0 , 0 ) distribution Bl0 ,0 (l, ) BPl0 ,0 (l, )S,d (l, )1Z(B)bivariate Gaussian distribution mean (l0 , 0 ) covariance matrix ,[0, 1] measures strength impact managers suggestion playersbehavior, Z(B) normalization constant . call NDB managementdistribution. clearly ad hoc model focal bargains; use simpleway demonstrate NDB management.assume minimize computational requirements, regulator requiresspace bargains grid integer distances degrees, i.e., B 0 = [0, 1, ..., E][90, 89, ..., 89, 90].15 mentioned Section 4, even finite set possible bargainsinstances multiple bargains map joint utility outcomes.usual, address must specify P (b | x); make simple assumptionlikelihood uniform b map x, zero others. Accordingly,BS,d (l, ) =S,d (s(l, )),|Bx ||Bx | number bargains B give rise outcome x.usual, regulator choose action means suggest headingdistance maximize resultant expected social welfare. Writing social welfarefunction W (l, ), optimal actionZargmax(l0 ,0 )B El0 ,0 ,S,d (W ) =dl W (l, )Pl0 ,0 (l, ).Bgeneral, W might depend impact rerouting traffic flow entireNational Airspace System, addition depending utility cockpit ATC.simplicity though, take W (l, ) = sc (l, ) + (1 )satc (l, ) [0, 1].Figure 4 depicts effect regulators recommendation predicted flight path.left panel NDB distribution bargains, BS,d . represents predictionregulator makes suggestion. NDB modal bargain produces detouredflight path denoted center panel sequence two lines joined open14. Experimental studies find empirical support theory focal points bargaining (Binmore,Swierzbinski, Hsu, & Proulx, 1993).15. practice, may need explicit requirement, since unlikely non-integer valueswould considered players.599fiWolpert & BonoNDB Distribution BargainsFlight PathsNDB Management Distribution808050606040304040200202010HeadingNorthSouthHeading2000102020404030406060508080050100150Distance200250300050100150WestEast200250300050100150200250300DistanceFigure 4: Left Panel: NDB distribution B 0 = [0, 1, ..., E] [90, 89, ..., 89, 90]ac = 1, aatc = 3, L = 4, = 300, H = 0.01 = 300. Center Panel:Weather large circle center C = (150, 20) radius R = 40, NDBmodal flight path (circle line), NDB management modal flight path (square line),NDB management recommended waypoint (triangle line). Right Panel: NDBmanagement distribution = .5, = .5, = 360 l = 1, 200.traffic penalty given H(e, n) = n.circle; flight begins detour point (0, 0) returns original flight path(300, 0). reason modal flight path pass closer weathertraffic penalty given H(e, n) = n, i.e., traffic denser north. means ATCrewarded flight paths pass south. since aatc = 3 > 1 = ac , ATCrelatively successful convincing cockpit accept bargains result longer,southerly flight paths.NDB management distribution shown right panel. represents prediction regulator made optimal suggestion. case, optimal suggestion(130, 10). path associated optimal suggestion denoted center panelpair lines joined triangle. modal bargain NDB management distribution induced suggestion denoted pair lines joined square. Noteregulators suggestion effect skewing distribution toward bargainsshorter distances l smaller heading changes. regulator assignsequal weight ATC cockpit utility, whereas NDB distribution skewed towardoutcomes better ATC. result, modal bargain shifts significantlynorth.600fiPredictive Unstructured Bargaining6. Conclusion Future Worklaboratory field unstructured bargaining experiments, typically foundone bargain arise. accommodate this, paper considermaps take unstructured bargaining problem probability distributionS, rather single element S. approach translate Nashs axiomsunstructured bargaining apply distribution-valued map. this, deriveNash distributional bargaining concept, maps feasible set joint utilityoutcomes power law set. power law nature intriguing dueubiquity empirical power laws real economy, e.g., wage distributions, city sizes,etc.Future work involves trying translate variants Nashs axioms distributionalbargaining concept. example, may prove possible translate weak monotonicityaxiom Kalai-Smorodinsky solution concept (Kalai & Smorodinsky, 1975) probabilistic terms. so, combining probabilistic versions remainingKalai-Smorodinsky axioms (which shared axioms Nash bargaining concept), may able produce Kalai-Smorodinsky distributional bargaining concept.many advantages using distributional bargaining concepts, additionaccording experimental fact multiple bargains arise givengame. One advantage concepts seamlessly accommodate feasible setsconvex comprehensive, even finite feasible sets. Another advantage arisesexternal regulator modify aspects bargaining gameutility function bargaining outcomes. modifying game, regulatorchanges associated distribution, therefore modifies value expectedutility. Accordingly, calculate Bayes-optimal modification game.emphasize nothing formal definition bargaining gametranslation Nashs axioms restricts analysis scenarios traditionallyviewed bargaining. particular, assume binding contracts. Thereforereason assume outcome game Pareto optimal. (Withoutbinding contract players behavior, player typically incentiveability change behavior way helps hurts opponent,thereby moves final outcome Pareto frontier.) reason, distributionalbargaining concepts applied model noncooperative game modelerknows feasible set games joint utility outcomes, know (orcannot tractably elaborate) full underlying extensive form game. modelingnoncooperative game behavior limited information, best modelerprovide distribution final joint utility outcome. Intuitively speaking,distribution amounts average extensive form games associatedfeasible set possible joint utilities. Nash distributional bargaining concept providesway form distribution outcomes, using axioms seem applicable manyscenarios traditionally viewed unstructured bargaining.1616. subtlety using Nash distributional bargaining concept way needsoutcome noncooperative game reasonably expected behave defaultpoint d. However even restriction dispensed using distributional bargainingconcepts, e.g., using logit one mentioned end Sec. 2.601fiWolpert & Bonoillustration, work Smith, Suchanek, Williams (1988) artificialspeculative market run rounds. created bubble burst, leavingtest subjects poorer richer. Predicting moves full extensive form gamesubjects engaged rounds laborious, best. alternative, one couldimagine predicting experiments outcome based knowing feasible set, i.e.,knowing ways money could end divided among subjects endexperiment. Future work involves analyzing Nash distributional bargaining conceptsituations.market experiments could prove useful source data testing alternativedistributional bargaining theories, addition experiments explicitly viewed termsunstructured bargaining. particular, possible produce Kalai-Smorodinskydistributional bargaining concept, discussed above, may possible comparepredictions alternative theory NDB concept using data marketexperiments. comparative study would analogous types meta-studiesconducted compare non-cooperative solution concepts using experimental data (Wright& Leyton-Brown, 2010).discussed above, function plays prominent role making predictionsbargains rather joint utilities. Arguably, knowledge even affectpredictions make joint utility outcomes. example, say set possiblebargains large players cannot expected examine completely,concentrates vast majority bargains onto single joint utility x, e.g., onePareto frontier. case, would seem reasonable ascribe higher probabilityx would single bargain mapped it. However traditional approachesunstructured bargaining ignore predicting joint utilities, relying axioms solelyconcerned space joint utilities. part, approachadopted here. (The section optimal recommendations diverged modelingeffect recommendation.) Future work involves incorporating knowledgedirectly predictions joint utility outcomes.Acknowledgmentswould like thank Sylvia Thoron helpful discussion.ReferencesAgogino, A., & Tumer, K. (2012). multiagent approach managing air traffic flow.Autonomous Agents Multi-Agent Systems, 24, 125. 10.1007/s10458-010-9142-5.Ashlagi, I., Monderer, D., & Tennenholtz, M. (2008). value correlation. JournalArtificial Intelligence Research, 33, 575613.Aumann, R., & Hart, S. (1992). Handbook Game Theory Economic Applications.North-Holland Press.602fiPredictive Unstructured BargainingAydogan, R., & Yolum, P. (2012). Learning opponents preferences effective negotiation:approach based concept learning. Autonomous Agents Multi-Agent Systems,24, 104140. 10.1007/s10458-010-9147-0.Binmore, K., Swierzbinski, J., Hsu, S., & Proulx, C. (1993). Focal points bargaining.International Journal Game Theory, 22, 381409. 10.1007/BF01240133.Brafman, R. I., & Tennenholtz, M. (1996). partially controlled multi-agent systems.Journal Artificial Intelligence Research, 4, 477507.Brafman, R. I., & Tennenholtz, M. (2003). Learning coordinate efficiently: modelbased approach. Journal Artificial Intelligence Research, 19, 1123.Camerer, C. (2003). Behavioral Game Theory: Experiments Strategic Interaction. Princeton University Press.Chalamish, M., & Kraus, S. (2012). Automed: automated mediator multi-issuebilateral negotiations. Autonomous Agents Multi-Agent Systems, 24, 536564.10.1007/s10458-010-9165-y.Conley, J. P., & Wilkie, S. (1996). extension nash bargaining solution nonconvexproblems. Games Economic Behavior, 13, 2638.Duan, L., Dogru, M., Ozen, U., & Beck, J. (2012). negotiation framework linkedcombinatorial optimization problems. Autonomous Agents Multi-Agent Systems,25, 158182. 10.1007/s10458-011-9172-7.Harsanyi, J., & Selten, R. (1972). generalized nash solution two-person bargaininggames incomplete information. Management Science, 18, 80106.Herrero, M. J. (1989). nash program: Non-convex bargaining problems. JournalEconomic Theory, 49 (2), 266 277.Jaynes, E. T., & Bretthorst, G. L. (2003). Probability Theory : Logic Science.Cambridge University Press.Kalai, E. (1977). Proportional solutions bargaining situations: Interpersonal utilitycomparisons. Econometrica, 45 (7), 16231630.Kalai, E., & Smorodinsky, M. (1975). solutions nashs bargaining problem. Econometrica, 43 (3), 513518.Kaneko, M. (1980). extension nash bargaining problem nash social welfarefunction. Theory Decision, 12, 135148. 10.1007/BF00154358.Kbrs, O., & Sertel, M. (2007). Bargaining finite set alternatives. Social ChoiceWelfare, 28, 421437. 10.1007/s00355-006-0178-z.Lopez-Carmona, M., Marsa-Maestre, I., Klein, M., & Ito, T. (2012). Addressing stability issues mediated complex contract negotiations constraint-based, nonmonotonic utility spaces. Autonomous Agents Multi-Agent Systems, 24, 485535.10.1007/s10458-010-9159-9.603fiWolpert & BonoMariotti, M. (1998a). Extending nashs axioms nonconvex problems. Games Economic Behavior, 22 (2), 377 383.Mariotti, M. (1998b). Nash bargaining theory number alternatives finite.Social Choice Welfare, 15, 413421. 10.1007/s003550050114.McKelvey, R. D., & Palfrey, T. R. (1995). Quantal response equilibria normal formgames. Games Economic Behavior, 10, 638.Nash, J. (1950). bargaining problem. Econometrica, 18 (2), 155162.Nydegger, R. V., & Owen, G. (1974). Two-person bargaining: experimentaltest nash axioms. International Journal Game Theory, 3, 239249.10.1007/BF01766877.Osborne, M., & Rubinstein, A. (1994). Course Game Theory. MIT Press, Cambridge,MA.Peters, H., & Tijs, S. (1984). Probabilistic bargaining solutions. Operations ResearchProceedings. Springer-Verlag.Peters, H., & Vermeulen, D. (2010). WPO, COR, IIA bargaining solutions. acceptedInternational Journal Game Theory.Rezek, I., Leslie, D. S., Reece, S., Roberts, S. J., Rogers, A., Dash, R. K., & Jennings,N. R. (2008). similarities inference game theory machine learning.Journal Artificial Intelligence Research, 33, 259283.Roth, A. E., & Malouf, M. W. K. (1979). Game-theoretic models role informationbargaining. Psychological Review, 86 (6), 574594.Rubinstein, A., Safra, Z., & Thomson, W. (1992). interpretation nash bargaining solution extension non-expected utility preferences. Econometrica,60 (5), 11711186.Schelling, T. (1960). strategy conflict. Harvard university press.Smith, V. L., Suchanek, G. L., & Williams, A. W. (1988). Bubbles, crashes, endogenousexpectations experimental spot asset markets. Econometrica, 56 (5), pp. 11191151.Train, K. E. (2003). Discrete Choice Methods Simulation. Cambridge University Press.Vorobeychik, Y., Reeves, D. M., & Wellman, M. P. (2012). Constrained automated mechanism design infinite games incomplete information. accepted JournalAutonomous Agents Multiagent Systems.Wolpert, D. H., Harre, M., Olbrich, E., Bertschinger, N., & Jost, J. (2012). Hysteresis effectschanging parameters noncooperative games. Physical Review E, 85, 036102. DOI:10.1103/PhysRevE.85.036102.604fiPredictive Unstructured BargainingWolpert, D. H., & Bono, J. W. (2011). Distribution-valued solution concepts. workingpaper.Wright, J. R., & Leyton-Brown, K. (2010). Beyond equilibrium: Predicting human behaviornormal form games. Twenty-Fourth Conference Artificial Intelligence (AAAI10). forthcoming.Xu, Y., & Yoshihara, N. (2006). Alternative characterizations three bargaining solutionsnonconvex problems. Games Economic Behavior, 57 (1), 86 92.Zhou, L. (1997). nash bargaining theory non-convex problems. Econometrica,65 (3), 681685.605fiJournal Artificial Intelligence Research 46 (2013) 687-716Submitted 12/12; published 04/13NuMVC: Efficient Local Search AlgorithmMinimum Vertex CoverShaowei CaiSHAOWEICAI . CS @ GMAIL . COMKey Laboratory High Confidence Software TechnologiesPeking University, Beijing, ChinaKaile SuK . SU @ GRIFFITH . EDU . AUInstitute Integrated Intelligent SystemsGriffith University, Brisbane, AustraliaChuan LuoCHUANLUOSABER @ GMAIL . COMKey Laboratory High Confidence Software TechnologiesPeking University, Beijing, ChinaAbdul Sattar. SATTAR @ GRIFFITH . EDU . AUInstitute Integrated Intelligent SystemsGriffith University, Brisbane, AustraliaAbstractMinimum Vertex Cover (MVC) problem prominent NP-hard combinatorialoptimization problem great importance theory application. Local search provedsuccessful problem. However, two main drawbacks state-of-the-art MVC localsearch algorithms. First, select pair vertices exchange simultaneously, timeconsuming. Secondly, although using edge weighting techniques diversify search,algorithms lack mechanisms decreasing weights. address issues, propose twonew strategies: two-stage exchange edge weighting forgetting. two-stage exchangestrategy selects two vertices exchange separately performs exchange two stages.strategy edge weighting forgetting increases weights uncovered edges, alsodecreases weights edge periodically. two strategies used designingnew MVC local search algorithm, referred NuMVC.conduct extensive experimental studies standard benchmarks, namely DIMACSBHOSLIB. experiment comparing NuMVC state-of-the-art heuristic algorithmsshow NuMVC least competitive nearest competitor namely PLSDIMACS benchmark, clearly dominates competitors BHOSLIB benchmark. Also,experimental results indicate NuMVC finds optimal solution much faster currentbest exact algorithm Maximum Clique random instances well structured ones.Moreover, study effectiveness two strategies run-time behaviourexperimental analysis.1. IntroductionMinimum Vertex Cover (MVC) problem consists of, given undirected graph G = (V, E),finding minimum sized vertex cover, vertex cover subset V everyedge G least one endpoint S. MVC important combinatorial optimization problemmany real-world applications, network security, scheduling, VLSI design industrialmachine assignment. equivalent two well-known combinatorial optimization problems:Maximum Independent Set (MIS) problem Maximum Clique (MC) problem,c2013AI Access Foundation. rights reserved.fiC AI , U , L UO & ATTARwide range applications areas information retrieval, experimental design, signaltransmission, computer vision, also bioinformatics problems aligning DNA proteinsequences (Johnson & Trick, 1996). Indeed, three problems seen three differentforms problem, viewpoint practical algorithms. Algorithms MVCdirectly used solve MIS MC problems. Due great importance theoryapplications, three problems widely investigated last several decades(Carraghan & Pardalos, 1990; Evans, 1998; Pullan & Hoos, 2006; Richter, Helmert, & Gretton,2007; Cai, Su, & Chen, 2010; Li & Quan, 2010b; Cai, Su, & Sattar, 2011).Theoretical analyses indicate three problems MVC, MIS, MC computationallyhard. NP-hard associated decision problems NP-complete (Garey &Johnson, 1979). Moreover, hard solve approximately. NP-hard approximateMVC within factor smaller 1.3606 (Dinur & Safra, 2005), although one achieveapproximation ratio 2 o(1) (Halperin, 2002; Karakostas, 2005). Besides inapproximabilityMVC, Hastad shows MIS MC approximable within |V |1 > 0,unless NP=ZPP1 (Hastad, 1999, 2001). Recently, conclusion enhanced MCapproximable within |V |1 > 0 unless NP=P (Zuckerman, 2006), derivedderandomization Hastads result. Moreover, currently best polynomial-time approximationalgorithm MC guaranteed find clique within factor O(n(loglogn)2 /(logn)3 )optimum (Feige, 2004).algorithms solve MVC (MIS, MC) fall two types: exact algorithms heuristicalgorithms. Exact methods mainly include branch-and-bound algorithms (Carraghan &Pardalos, 1990; Fahle, 2002; Ostergard, 2002; Regin, 2003; Tomita & Kameda, 2009; Li &Quan, 2010b, 2010a), guarantee optimality solutions find, may fail givesolution within reasonable time large instances. Heuristic algorithms, mainly include localsearch algorithms, cannot guarantee optimality solutions, find optimalsatisfactory near-optimal solutions large hard instances within reasonable time. Therefore,appealing use local search algorithms solve large hard MVC (MC, MIS) instances.Early heuristic methods Maximum Clique designed initial responsesSecond DIMACS Implementation Challenge (Johnson & Trick, 1996), Maximum Cliqueone three challenge problems. that, huge amount effort devoted designinglocal search algorithms MVC, MC MIS problems (Aggarwal, Orlin, & Tai, 1997; Battiti &Protasi, 2001; Busygin, Butenko, & Pardalos, 2002; Shyu, Yin, & Lin, 2004; Barbosa & Campos,2004; Pullan, 2006; Richter et al., 2007; Andrade, Resende, & Werneck, 2008; Cai et al., 2010,2011). review heuristic algorithms three problems found recent paperMVC local search (Cai et al., 2011).work devoted efficient local search algorithm MVC. Typically, local searchalgorithms MVC solve problem iteratively solving k-vertex cover problem. solvek-vertex cover problem, maintain current candidate solution size k, exchangetwo vertices iteratively becomes vertex cover. However, observe two drawbacksstate-of-the-art MVC local search algorithms. First, select pair vertices exchangingsimultaneously according heuristic (Richter et al., 2007; Cai et al., 2010, 2011),rather time-consuming, explained Section 3. second drawback edgeweighting techniques. basic concept edge weighting increase weights uncovered1. ZPP class problems solved expected polynomial time probabilistic algorithm zeroerror probability.688fiN U MVC: N E FFICIENT L OCAL EARCH LGORITHM INIMUM V ERTEX Cedges diversify search. Previous MVC local search algorithms utilize different edge weightingschemes. example, COVER (Richter et al., 2007) increases weights uncovered edgesstep, EWLS (Cai et al., 2010) EWCC (Cai et al., 2011) increase weights uncoverededges reaching local optima. However, algorithms mechanismdecrease weights. believe deficient weighting decisions made longago may mislead search.address two issues MVC local search algorithms, paper proposes two newstrategies, namely two-stage exchange edge weighting forgetting. two-stage exchangestrategy decomposes exchanging procedure two stages, i.e., removing stageadding stage, performs separately. first selects vertex removes currentcandidate solution, selects vertex random uncovered edge adds it. twostage exchange strategy yields efficient two-pass move operator MVC local search,first pass linear-time search vertex-to-remove, second pass lineartime search vertex-to-add. contrast standard quadratic, all-at-once moveoperator. Moreover, two-stage exchange strategy renders algorithm flexibleemploy different heuristics different stages. Indeed, NuMVC algorithm utilizeshighly greedy heuristic removing stage, adding stage, makes good usediversifying heuristic within framework similar focused random walk (Papadimitriou, 1991).second strategy propose edge weighting forgetting. increases weightsuncovered edges one step. Moreover, averaged edge weight achievesthreshold, reduces weights edges multiplying constant factor (0 < < 1) forgetearlier weighting decisions. best knowledge, first time forgettingmechanism introduced local search algorithms MVC.two strategies combined design new local search algorithm called NuMVC.carry detailed experimental study investigate performance NuMVC, comparePLS (Pullan, 2006), COVER (Richter et al., 2007) EWCC (Cai et al., 2011),leading heuristic algorithms MVC (MC, MIS). Experimental results show NuMVCcompetes well solvers DIMACS benchmark, shows dramatic improvementexisting results whole BHOSLIB benchmark. parts work publishedearly version paper (Cai, Su, & Sattar, 2012).paper, additionally carry experimental analyses providesinsights two strategies NuMVC. compare NuMVC exact algorithmMaxCLQdyn+EFL+SCR (Li & Quan, 2010a), best exact Maximum Clique algorithmfound literature. Experimental results indicate NuMVC finds optimal solutionmuch faster exact algorithm random instances well structured ones.importantly, conduct experimental investigations study run-time behaviour NuMVCeffectiveness two new strategies NuMVC.remainder paper organized follows. next section, introducedefinitions notations used paper. present two strategies: two-stage exchangeedge weighting forgetting. Section 5, describe NuMVC algorithm. Section 6presents experimental study NuMVC comparative results algorithms, includingheuristic exact algorithms. followed detailed investigations run-timebehaviour NuMVC effectiveness two new strategies Section 7. Finally,conclude paper summarizing main contributions future directions.689fiC AI , U , L UO & ATTAR2. Preliminariesundirected graph G = (V, E) consists vertex set V edge set E V V ,edge 2-element subset V . edge e = {u, v}, say vertices u vendpoints edge e. Two vertices neighbors belong commonedge. denote N (v) = {u V |{u, v} E}, set neighbors vertex v.undirected graph G = (V, E), independent set subset V pairwise nonadjacent elements clique subset V pairwise adjacent elements. maximumindependent set maximum clique problems find maximum sized independent setclique graph, respectively.note three problems MVC, MIS MC seen three different formsproblem, viewpoint experimental algorithms. vertex set independentset G V \S vertex cover G; vertex set K clique G V \Kvertex cover complementary graph G. find maximum independent set graphG, one find minimum vertex cover Cmin G return V \Cmin . Similarly, findmaximum clique graph G, one find minimum vertex cover Cmincomplementarygraph G, return V \Cmin.Given undirected graph G = (V, E), candidate solution MVC subset vertices.edge e E covered candidate solution X least one endpoint e belongsX. search procedure, NuMVC always maintains current candidate solution.convenience, rest paper, use C denote current candidate solution. statevertex v denoted sv {1, 0}, sv = 1 means v C, sv = 0 means v/ C.step neighboring candidate solution consists exchanging two vertices: vertex u Cremoved C, vertex v/ C put C. age vertex number steps sincestate last changed.state-of-the-art MVC local search algorithms, NuMVC utilizes edge weightingscheme. edge weighting local search, follow definitions notations EWCC (Caiet al., 2011). edge weighted undirected graph undirected graph G = (V, E) combinedweighting function w edge e E associated non-negative integer numberw(e) weight. use w denote mean value edge weights.Let w weighting function G. candidate solution X, set cost XXcost(G, X) =w(e)eE e covered Xindicates total weight edges uncovered X. take cost(G, X) evaluationf unction, NuMVC prefers candidate solutions lower costs.vertex v V ,dscore(v) = cost(G, C) cost(G, C )C = C\{v} v C, C = C {v} otherwise, measuring benefit changingstate vertex v. Obviously, vertex v C, dscore(v) 0, greater dscoreindicates less loss covered edges removing C. vertex v/ C,dscore(v) 0, higher dscore indicates greater increment covered edges addingC.690fiN U MVC: N E FFICIENT L OCAL EARCH LGORITHM INIMUM V ERTEX C3. Two-Stage Exchangesection, introduce two-stage exchange strategy, adopted NuMVCalgorithm exchange pair vertices.state-of-the-art MVC local search algorithms, NuMVC iterated k-vertexcover algorithm. finding k-vertex cover, NuMVC removes one vertex currentcandidate solution C goes search (k 1)-vertex cover. sense, coreNuMVC k-vertex cover algorithm given positive integer number k, searching k-sizedvertex cover. find k-vertex cover, NuMVC begins candidate solution C size k,exchanges two vertices iteratively C becomes vertex cover.local search algorithms MVC select pair vertices exchange simultaneouslyaccording certain heuristic. example, COVER selects pair vertices maximizegain(u, v) (Richter et al., 2007), EWLS (Cai et al., 2010) EWCC (Cai et al., 2011) selectrandom pair vertices score(u, v) > 0. strategy selecting two vertices exchangesimultaneously leads quadratic neighborhood candidate solutions. Moreover, evaluationpair vertices depends evaluations (such dscore) two vertices, alsoinvolves relationship two vertices, like belong edge. Therefore,rather time-consuming evaluate candidate pairs vertices.contrast earlier MVC local search algorithms, NuMVC selects two verticesexchanging separately exchanges two selected vertices two stages. iteration,NuMVC first selects vertex u C highest dscore removes it. that, NuMVCselects uniformly random uncovered edge e, chooses one endpoint v e higherdscore restrictions adds C. Note two-stage exchange strategyresembles respect min-conflicts hill-climbing heuristic CSP (Minton, Johnston,Philips, & Laird, 1992), shows surprisingly good performance N-queens problem.Selecting two vertices exchanging separately may cases miss greediervertex pairs consist two neighboring vertices. However, usual local searchalgorithms, trade-off accuracy heuristics complexity per step.Let R denote set candidate vertices removing adding separately. timecomplexity per step selecting exchanging vertex pair simultaneously |R| |A|;complexity per step selecting two vertices separately, NuMVC, |R| + |A|.worthy note that, heuristics local search algorithm often based intuitionexperience rather theoretically empirically derived principles insights, cannot saycertain less greedy good thing (Hoos & Stutzle, 2004). hand,lower time complexity always desirable.4. Edge Weighting Forgettingsection, present new edge weighting technique called edge weighting forgetting,plays important role NuMVC.proposed strategy edge weighting forgetting works follows. edgeassociated positive integer number weight, edge weight initialized one.iteration, edge weights uncovered edges increased one. Moreover,average weight achieves threshold, edge weights reduced forget earlier weightingdecisions using formula w(e) := w(e), constant factor 0 1.691fiC AI , U , L UO & ATTARNote edge weighting techniques MVC local search, including one work,fall general penalty idea optimization problems, dates back Morrisbreakout method (Morris, 1993) widely used local search algorithms constraintoptimization problems SAT (Yugami, Ohta, & Hara, 1994; Wu & Wah, 2000; Schuurmans,Southey, & Holte, 2001; Hutter, Tompkins, & Hoos, 2002). results therefore provideevidence effectiveness general applicability algorithmic technique.Edge weighting techniques successfully used improve MVC local searchalgorithms. example, COVER (Richter et al., 2007) updates edge weights step,EWLS (Cai et al., 2010) EWCC (Cai et al., 2011) update edge weights reaching localoptima. However, previous edge weighting techniques mechanism decreaseweights, limits effectiveness. strategy edge weighting forgettingwork introduces forgetting mechanism reduce edge weights periodically, contributesconsiderably NuMVC algorithm.intuition behind forgetting mechanism weighting decisions made long agolonger helpful may mislead search, hence considered less importantrecent ones. example, consider two edges e1 e2 w(e1 ) = 1000 w(e2 ) =100 step. use w(e) denote increase w(e). According evaluationfunction, next period time, algorithm likely cover e1 frequently e2 ,may assume period w(e1 ) = 50 w(e2 ) = 500, makes w(e1 ) =1000 + 50 = 1050 w(e2 ) = 100 + 500 = 600. Without forgetting mechanism, algorithmwould still prefer e1 e2 covered future search. reasonable,period e2 covered much fewer steps e1 is. Thus, e2 take priority coveredsake diversification. let us consider case forgetting mechanism (assuming= 0.3 setting experiments). Suppose w(e1 ) = 1000 w(e2 ) = 100algorithm performs forgetting. forgetting mechanism reduces edge weightsw(e1 ) = 10000.3 = 300 w(e2 ) = 1000.3 = 30. period time, w(e1 ) = 50w(e2 ) = 500, w(e1 ) = 300 + 50 = 350 w(e2 ) = 30 + 500 = 530. case,algorithm prefers cover e2 rather cover e1 future search, expect.Although inspired smoothing techniques clause weighting local search algorithmsSAT, forgetting mechanism NuMVC differs smoothing techniques SATlocal search algorithms. According way clause weights smoothed, threemain smoothing techniques clause weighting local search algorithms SAT bestknowledge: first pull clause weights mean value using formula wi :=wi + (1 ) w, ESG (Schuurmans et al., 2001), SAPS (Hutter et al., 2002) Swcca(Cai & Su, 2012); second subtract one clause weights greaterone, DLM (Wu & Wah, 2000) PAWS (Thornton, Pham, Bain, & Jr., 2004); lastemployed DDWF (Ishtaiwi, Thornton, Sattar, & Pham, 2005), transfers weightsneighbouring satisfied clauses unsatisfied ones. obvious forgetting mechanismNuMVC different smoothing techniques.Recently, forgetting mechanism proposed vertex weighting technique significantMC local search algorithm DLS-MC (Pullan & Hoos, 2006), important sub-algorithmPLS (Pullan, 2006) CLS (Pullan, Mascia, & Brunato, 2011). DLS-MC algorithm employsvertex weighting scheme increases weights vertices (by one) current cliquereaching local optimum, periodically decreases weights (by one) verticescurrently penalty. Specifically, utilizes parameter pd (penalty delay) specify692fiN U MVC: N E FFICIENT L OCAL EARCH LGORITHM INIMUM V ERTEX Cnumber penalty increase iterations must occur algorithm performs forgettingoperation. However, Pullan Hoos also observed DLS-MC sensitive pdparameter, optimal value pd varies considerably among different instances. Indeed,performance DLS-MC given optimizing pd parameter. contrast, forgettingmechanism NuMVC much less sensitive parameters (as shown Section 7.4),thus robust.also notice formula used forgetting mechanism NuMVC adoptedlong-term frequency-based learning mechanisms tabu search (Taillard, 1994). However,Taillars algorithm, parameter (using term work) always greater one,formula used penalizing move rather forgetting penalties.5. NuMVC Algorithmsection, present NuMVC algorithm, utilizes strategies two-stageexchange edge weighting forgetting.Algorithm 1: NuMVC1234567891011121314151617181920NuMVC (G,cutoff)Input: graph G = (V, E), cutoff timeOutput: vertex cover Gbegininitialize edge weights dscores vertices;initialize confChange array all-1 array;construct C greedily vertex cover;C := C;elapsed time < cutoffuncovered edgeC := C;remove vertex highest dscore C;continue;choose vertex u C highest dscore, breaking ties favor oldestone;C := C\{u}, confChange(u) := 0 confChange(z) := 1 z N (u);choose uncovered edge e randomly;choose vertex v e confChange(v) = 1 higher dscore, breaking tiesfavor older one;C := C {v}, confChange(z) := 1 z N (v);w(e) := w(e) + 1 uncovered edge e;w w(e) := w(e) edge e;return C ;endbetter understanding algorithm, first describe strategy called configuration checking(CC), used NuMVC. CC strategy (Cai et al., 2011) proposed handling693fiC AI , U , L UO & ATTARcycling problem local search, i.e., revisiting candidate solution visited recently(Michiels, Aarts, & Korst, 2007). strategy successfully applied local searchalgorithms MVC (Cai et al., 2011) well SAT (Cai & Su, 2011, 2012).CC strategy NuMVC works follows: vertex v/ C, neighboring verticesnever change states since last time v removed C, v addedback C. CC strategy seen prohibition mechanism, shares spiritdiffers well-known prohibition mechanism called tabu (Glover, 1989).implementation CC strategy maintain Boolean array confChange vertices.search procedure, vertices confChange value 0 forbiddenadd C. confChange array initialized all-1 array. that, vertex vremoved C, confChange(v) reset 0, vertex v changes state,z N (v), confChange(z) set 1.outline NuMVC algorithm Algorithm 1, described below. beginning, edgeweights initialized 1, dscores vertices computed accordingly; confChange(v)initialized 1 vertex v; current candidate solution C constructed iterativelyadding vertex highest dscore (ties broken randomly), becomes vertexcover. Finally, best solution C initialized C.initialization, loop (lines 7-18) executed given cutoff time reached.search procedure, uncovered edge, means C vertex cover,NuMVC updates best solution C C (line 9). removes one vertex highestdscore C (line 10), breaking ties randomly, go search vertex coversize |C| = |C | 1. note that, C, vertex highest dscore minimumabsolute value dscore since dscores negative.iteration loop, NuMVC swaps two vertices according strategy two-stageexchange (lines 12-16). Specifically, first selects vertex u C highest dscoreremove, breaking ties favor oldest one. removing u, NuMVC chooses uncoverededge e uniformly random, selects one es endpoints add C follows:one endpoint whose confChange 1, vertex selected; confChange valuesendpoints 1, NuMVC selects vertex higher dscore, breaking tiesfavor older one. exchange finished adding selected vertex C. Alongexchanging two selected vertices, confChange array updated accordingly.end iteration, NuMVC updates edge weights (lines 17-18). First, weightsuncovered edges increased one. Moreover, NuMVC utilizes forgetting mechanismdecrease weights periodically. detail, averaged weight edges achieves threshold, edge weights multiplied constant factor (0 < < 1) roundedinteger edge weights defined integers NuMVC. forgetting mechanism forgetsearlier weighting decision extent, past effects generally longer helpfulmay mislead search.conclude section following observation, guarantees executabilityline 15.Proposition 1. uncovered edge e, least one endpoint v edge econfChange(v) = 1.Proof: Let us consider arbitrary uncovered edge e = {v1 , v2 }. proof includes two cases.(a) least one v1 v2 never changes state initialization. Without694fiN U MVC: N E FFICIENT L OCAL EARCH LGORITHM INIMUM V ERTEX Closs generality, assume v1 vertex. initialization, confChange(v1 ) set1. that, removing v1 C (which corresponds vs state sv changing 01) make confChange(v1 ) 0, v1 never changes state initialization,confChange(v1 )= 1.(b) v1 v2 change states initialization. e uncovered, v1/ Cv2/ C. Without loss generality, assume last removing v1 happens lastremoving v2 . last time v1 removed, v2 C holds. Afterwards, v2 removed,means v2 changes state, confChange(v1 ) set 1 v1 N (v2 ).6. Empirical Resultssection, present detailed experimental study evaluate performance NuMVCstandard benchmarks literature, i.e., DIMACS BHOSLIB benchmarks. firstintroduce DIMACS BHOSLIB benchmarks, describe preliminariesexperiments. Then, divide experiments three parts. purpose first partdemonstrate performance NuMVC detail. second compare NuMVC state-ofthe-art heuristic algorithms. Finally, last part compare NuMVC state-of-the-art exactalgorithms.6.1 Benchmarksgood set benchmarks fundamental demonstrate effectiveness new solvers.use two standard benchmarks MVC (MIS, MC) research, DIMACS benchmarkBHOSLIB benchmark. DIMACS benchmark includes instances industrygenerated various models, BHOSLIB instances random ones high difficulty.6.1.1 DIMACS B ENCHMARKDIMACS benchmark taken Second DIMACS Implementation ChallengeMaximum Clique problem (1992-1993)2 . Thirty seven graphs selected organizerssummary indicate effectiveness algorithms, comprising Second DIMACS ChallengeTest Problems. instances generated real world problems coding theory,fault diagnosis, Kellers conjecture Steiner Triple Problem, etc, random graphsvarious models, brock p hat families. instances range size less50 vertices 1,000 edges greater 4,000 vertices 5,000,000 edges. Althoughproposed two decades ago, DIMACS benchmark remains popular benchmarkwidely used evaluating heuristic algorithms MVC (Richter et al., 2007; Pullan, 2009;Cai et al., 2011; Gajurel & Bielefeld, 2012), MIS (Andrade et al., 2008; Pullan, 2009) MCalgorithms (Pullan, 2006; Katayama, Sadamatsu, & Narihisa, 2007; Grosso, Locatelli, & Pullan,2008; Pullan et al., 2011; Wu, Hao, & Glover, 2012). particular, DIMACS benchmarkused evaluating COVER EWCC. convenient us use benchmark alsoconduct experiments comparing NuMVC COVER EWCC. Note DIMACSgraphs originally designed Maximum Clique problem, MVC algorithms testedcomplementary graphs.2. ftp://dimacs.rutgers.edu/pub/challenges695fiC AI , U , L UO & ATTAR6.1.2 BHOSLIB B ENCHMARKBHOSLIB3 (Benchmarks Hidden Optimum Solutions) instances generated randomlyphase transition area according model RB (Xu, Boussemart, Hemery, & Lecoutre,2005). Generally, phase-transition instances generated model RB provedhard theoretically (Xu & Li, 2006) practically (Xu & Li, 2000; Xu, Boussemart,Hemery, & Lecoutre, 2007). SAT version BHOSLIB benchmark extensively usedSAT competitions4 . Nevertheless, SAT solvers much weaker MVC solversproblems, remains justifiable referring results SAT Competition 2011benchmark. BHOSLIB benchmark famous hardness influential enoughstrongly recommended MVC (MC, MIS) community (Grosso et al., 2008; Cai et al., 2011).widely used recent literature reference point new local search solversMVC, MC MIS5 . Besides 40 instances, large instance frb100-40 4,000vertices 572,774 edges, designed challenging MVC (MC, MIS) algorithms.BHOSLIB benchmark designed MC, MVC MIS, graphsbenchmark expressed two formats, i.e., clq format mis format. BHOSLIBinstance, graph clq format one mis format complementary other. MCalgorithms tested graphs clq format, MVC MIS algorithms testedmis format.6.2 Experiment Preliminariesdiscuss experimental results, let us introduce preliminary informationexperiments.NuMVC implemented C++. codes NuMVC EWCC publicly availablefirst authors homepage6 . codes COVER downloaded online7 , PLSkindly provided authors. four solvers compiled g++ -O2 option.experiments carried machine 3 GHz Intel Core 2 Duo CPU E8400 4GBRAM Linux. execute DIMACS machine benchmarks8 , machine requires 0.19CPU seconds r300.5, 1.12 CPU seconds r400.5 4.24 CPU seconds r500.5.NuMVC, set = 0.5|V | = 0.3 runs, except challenging instancefrb100-40, = 5000 = 0.3. Note also parameters state-ofthe-art MVC (MC, MIS) algorithms, DLS-MC (Pullan & Hoos, 2006) EWLS (Cai et al.,2010). Moreover, parameters DLS-MC EWLS vary considerably different instances.instance, algorithm performed 100 independent runs different random seeds,run terminated upon reaching given cutoff time. cutoff time set 2000seconds instances except challenging instance frb100-40, cutofftime set 4000 seconds due significant hardness.NuMVC, report following information instance:optimal (or minimum known) vertex cover size (V C ).3.4.5.6.7.8.http://www.nlsde.buaa.edu.cn/kexu/benchmarks/graph-benchmarks.htmhttp://www.satcompetition.orghttp://www.nlsde.buaa.edu.cn/kexu/benchmarks/list-graph-papers.htmhttp://www.shaoweicai.net/research.htmlhttp://www.informatik.uni-freiburg.de/srichter/ftp://dimacs.rutgers.edu/pub/dsj/clique/696fiN U MVC: N E FFICIENT L OCAL EARCH LGORITHM INIMUM V ERTEX Cnumber successful runs (suc). run said successful solution size V Cfound.VC size shows min (average, max) vertex cover size found NuMVC100 runs.averaged run time 100 runs (time). run time successful runtime find V C solution, failed run considered cutoff time.instances NuMVC achieve 100% success rate, also report averagedrun time successful runs (suc time). run time measured CPU seconds.inter-quartile range (IQR) run time 100 runs. IQR difference75th percentile 25th percentile sample. IQR one famous robustmeasures data analysis (Hoaglin, Mosteller, & Tukey, 2000), recommendedmeasurement closeness sampling distribution community experimentalalgorithms (Bartz-Beielstein, Chiarandini, Paquete, & Preuss, 2010).number steps averaged 100 runs (steps). steps successful runneeded find V C solution, steps failed run executedrunning cut off. instances NuMVC achieve 100% successrate, also report averaged steps successful runs (suc steps).successful runs instance, time steps columns markedn/a. success rate solver instance less 75%, 75th percentilerun time sample cutoff time represent real 75th percentile. case,report IQR, instead mark n/a corresponding column. Actually,success rate solver certain instance less 75%, solver consideredrobust instance given cutoff time.6.3 Performance NuMVCsection, report detailed performance NuMVC two benchmarks.6.3.1 P ERFORMANCE N U MVCDIMACS B ENCHMARKperformance results NuMVC DIMACS benchmark displayed Table 1. NuMVCfinds optimal (or best known) solutions 35 37 DIMACS instances. Note 2 failedinstances brock graphs. Furthermore, among 35 successful instances, NuMVCconsistently (i.e., 100 runs) 32 instances, 24 solved within 1 second.Overall, NuMVC algorithm exhibits excellent performance DIMACS benchmark exceptbrock graphs. Remark brock graphs artificially designed defeat greedyheuristics explicitly incorporating low-degree vertices optimal vertex cover. Indeed,algorithms preferring higher-degree vertices GRASP, RLS, k-opt, COVER EWCCalso failed graphs.6.3.2 P ERFORMANCE N U MVCBHOSLIB B ENCHMARKTable 2, illustrate performance NuMVC BHOSLIB benchmark. NuMVCsuccessfully solves BHOSLIB instances terms finding optimal solution, size697fiC AI , U , L UO & ATTARGraphInstance Verticesbrock200 2brock200 4brock400 2brock400 4brock800 2brock800 4C125.9C250.9C500.9C1000.9C2000.5C2000.9C4000.5DSJC500.5DSJC1000.5gen200 p0.9 44gen200 p0.9 55gen400 p0.9 55gen400 p0.9 65gen400 p0.9 75hamming8-4hamming10-4keller4keller5keller6MANN a27MANN a45MANN a81p hat300-1p hat300-2p hat300-3p hat700-1p hat700-2p hat700-3p hat1500-1p hat1500-2p hat1500-3200200400400800800125250500100020002000400050010002002004004004002561024171776336137810353321300300300700700700150015001500V CsucVC size1881001009610000100100100100100110010010010010010010010010010010010010010010027100100100100100100100100100188183371(371.16,375)3677797799120644393219841920(1921.29,1922)398248798515614534533532524098416074933022526902221(2221.94,2223)2922752646896566381488143514061833713677767749120644393219841920398248798515614534533532524098416074933022526902221292275264689656638148814351406NuMVCtime(suc time)0.1261.259572.390(512.906)4.981n/an/a< 0.001< 0.0010.1282.0202.9351994.561(1393.303)252.8070.0120.615< 0.001< 0.0010.035< 0.001< 0.001< 0.0010.062< 0.0010.0382.51< 0.00186.3621657.880(732.897)0.003< 0.0010.0010.0110.0060.0083.7510.0710.060steps(suc steps)1376101705766645631471(585032783)6322882n/an/a13632561335951154155231778777848959(564895994)780278538001347961695693839815222031238534215269384026665190642150571607432(251509010)100981863124811032868445830528010668Table 1: NuMVC performance results, averaged 100 independent runs, DIMACSbenchmark instances. VC column marked asterisk means minimumknown vertex cover size proved optimal.698fiN U MVC: N E FFICIENT L OCAL EARCH LGORITHM INIMUM V ERTEX Cworst solution finds never exceeds V C + 1. NuMVC finds optimal solutions 100%success rate 33 40 instances, averaged success rate remaining 7instances 82.57%. results dramatically better existing results literaturebenchmark. Also, NuMVC finds sub-optimal solution size V C + 1 BSHOSLIBinstances quickly, always less 30 seconds. indicates NuMVC usedapproximate MVC problem efficiently even limited time.Besides 40 BHOSLIB instances Table 2, challenging instance frb100-40,hidden minimum vertex cover size 3900. designer BHOSLIB benchmarkconjectured instance solved PC less day within next twodecades9 . latest record challenging instance 3902-sized vertex cover foundEWLS, also EWCC.run NuMVC 100 independent trials within 4000 seconds frb100-40, = 5000= 0.3 (this parameter setting yields best performance among combinations= 2000, 3000, ..., 6000 = 0.1, 0.2, ..., 0.5). Among 100 runs, 4 runs find 3902-sizedsolution averaged time 2955 seconds, 93 runs find 3903-sized solution (including3902-sized) averaged time 1473 seconds. Also, interesting note NuMVClocate rather good approximate solution hard instance quickly: size vertexcovers NuMVC finds within 100 seconds 3903 3905.Generally, finding (k+1)-vertex cover much easier k-vertex cover. Hence,NuMVC, well MVC local search algorithms also solve MVC problemsolving k-vertex cover problem iteratively, majority running time used findingbest vertex cover C (of run), trying, without success, find vertex cover size(|C | 1).6.4 Comparison Heuristic Algorithmsrecent literature five leading heuristic algorithms MVC (MC, MIS), includingthree MVC algorithms COVER (Richter et al., 2007), EWLS (Cai et al., 2010) EWCC (Caiet al., 2011), two MC algorithms DLS-MC (Pullan & Hoos, 2006) PLS (Pullan, 2006).Note EWCC PLS improved versions EWLS DLS-MC respectively, showbetter performance original versions DIMACS BHOSLIB benchmarks. Therefore,compare NuMVC PLS, COVER EWCC.comparing NuMVC heuristic algorithms, report V C , suc, time wellIQR. averaged run time successful runs (suc time) cannot indicate comparativeperformance algorithms correctly unless evaluated algorithms close success rates,f (100suc)calculated time100cutof, report statistics. resultssucbold indicate best performance instance.6.4.1 C OMPARATIVE R ESULTSDIMACS B ENCHMARKcomparative results DIMACS benchmark shown Table 3. DIMACS instanceseasy solved solvers 100% success rate within 2 seconds, thusreported table. Actually, fact DIMACS benchmark reduced 11useful instances really emphasizes need make new benchmark.9. http://www.nlsde.buaa.edu.cn/kexu/benchmarks/graph-benchmarks.htm699fiC AI , U , L UO & ATTARGraphInstance Verticesfrb30-15-1frb30-15-2frb30-15-3frb30-15-4frb30-15-5frb35-17-1frb35-17-2frb35-17-3frb35-17-4frb35-17-5frb40-19-1frb40-19-2frb40-19-3frb40-19-4frb40-19-5frb45-21-1frb45-21-2frb45-21-3frb45-21-4frb45-21-5frb50-23-1frb50-23-2frb50-23-3frb50-23-4frb50-23-5frb53-24-1frb53-24-2frb53-24-3frb53-24-4frb53-24-5frb56-25-1frb56-25-2frb56-25-3frb56-25-4frb56-25-5frb59-26-1frb59-26-2frb59-26-3frb59-26-4frb59-26-545045045045045059559559559559576076076076076094594594594594511501150115011501150127212721272127212721400140014001400140015341534153415341534V C42042042042042056056056056056072072072072072090090090090090011001100110011001100121912191219121912191344134413441344134414751475147514751475sucVC sizeNuMVCtime (suc time)steps (suc steps)10010010010010010010010010010010010010010010010010010010010010010095100100861001001001001009710010010088389679100420420420420420560560560560560720720720720720900900900900900110011001100(1100.05,1101)110011001219(1219.14,1220)121912191219121913441344(1344.03,1345)1344134413441475(1475.12,1476)1475(1475.62,1476)1475(1475.04,1476)1475(1475.21,1476)14750.0450.0530.1910.0490.1180.5150.4470.1780.5630.2980.2424.0831.0762.75710.1412.7084.72713.7773.97310.66138.143176.589606.165(532.805)7.8919.529895.006(715.123)205.35251.227266.87139.893470.682658.961(617.485)121.29849.44626.761843.304(687.845)1677.801(1160.020)644.831(580.032)1004.550(741.208)61.90737963446321737084118910546838628733425512927942263821880020811536797709598742473081914271920295883605881104474443000680805923624628019113569606386342329(343518242)509207212690957514619149(416394360)1179808332937640615298273622817023259903023350048132(326853745)670430782603003114109165440874471(350993718)875964146(592010913)325417225(295226277)517521634(375976753)31682895Table 2: NuMVC performance results, averaged 100 independent runs, BHOSLIBbenchmark instances. BHOSLIB instances hidden optimal vertex cover,whose size shown VC column.indicated Table 3, NuMVC outperforms COVER EWCC instances,competitive complementary PLS. eight hard instances least onesolver fails achieve 100% success rate, PLS dominates brock graphs NuMVCdominates others, including two putatively hardest instances C2000.9 MANN a81(Richter et al., 2007; Grosso et al., 2008; Cai et al., 2011), well keller6 MANN a45.700fiN U MVC: N E FFICIENT L OCAL EARCH LGORITHM INIMUM V ERTEX CGraphInstanceV CsucPLStime (IQR)sucbrock400 2brock400 4brock800 2brock800 4C2000.9C4000.5gen400 p0.9 55keller6MANN a45MANN a81p hat1500-137136777677419203982345330269022211488100100100100010010092101000.15 (0.16)0.03 (0.03)3.89 (3.88)1.31 (1.52)n/a67 (59)15.17 (17)559 (515)1990 (n/a)n/a2.36 (3.07)382000100100100941100COVERtime (IQR)1947 (n/a)960 (988)n/an/an/a658 (290)0.35 (0.1)68 (6)714 (774)1995 (n/a)18.10 (17.23)sucEWCCtime (IQR)sucNuMVCtime (IQR)201000001001001008811001778 (n/a)25.38 (25.96)n/an/an/a739 (903)0.05 (0.04)3.76 (3.57)763 (766)1986 (n/a)9.79 (9.77)9610000110010010010027100572 (646)4.98 (6.14)n/an/a1994 (n/a)252 (97)0.03 (0.01)2.51 (0.76)86 (95)1657 (n/a)3.75 (3.19)Table 3: Comparison NuMVC state-of-the-art heuristic algorithms DIMACSbenchmark. VC column marked asterisk means minimum knownvertex cover size proved optimal.C2000.9, NuMVC finds 1920-sized solution, also finds 1921-sized solution70 runs, number 31, 6 32 PLS, COVER, EWCC respectively. NotePLS performs well brock family comprises three sub-algorithms, onefavors lower degree vertices.Table 3 indicates C2000.9 MANN a81 remain difficult modern algorithms,none algorithms solve good success rate reasonable time.hand, instances solved quickly (in less 100 seconds) least one algorithm, PLSNuMVC, low IQR value (always less 100), indicates quite stable performance.6.4.2 C OMPARATIVE R ESULTSBHOSLIB B ENCHMARKTable 4, present comparative results BHOSLIB benchmark. concentratingconsiderable gaps comparisons, report results two groups small instances(frb30 frb35), solved within several seconds solvers.results Table 4 illustrate NuMVC significantly outperforms algorithmsBHOSLIB instances, terms success rate averaged run time, alsodemonstrated Figure 1. take look comparison NuMVC EWCC,EWCC performs obviously better PLS COVER benchmark. NuMVC solves 33instances 40 100% success rate, 4 instances EWCC does. instancessolved algorithms 100% success rate, overall averaged run time 25 secondsNuMVC 74 seconds EWCC. instances, averaged success rate 90%NuMVC, compared 50% EWCC.excellent performance NuMVC underlined large gaps NuMVCsolvers hard instances. example, instances solvers failfind optimal solution 100% success rate, NuMVC achieves overall averaged successrate 82.57%, dramatically better PLS, COVER EWCC, 0.85%,17.43% 35.71% respectively. Obviously, experimental results show NuMVC delivers701fiC AI , U , L UO & ATTARGraphInstanceV CsucPLStime (IQR)sucfrb40-19-1frb40-19-2frb40-19-3frb40-19-4frb40-19-5frb45-21-1frb45-21-2frb45-21-3frb45-21-4frb45-21-5frb50-23-1frb50-23-2frb50-23-3frb50-23-4frb50-23-5frb53-24-1frb53-24-2frb53-24-3frb53-24-4frb53-24-5frb56-25-1frb56-25-2frb56-25-3frb56-25-4frb56-25-5frb59-26-1frb59-26-2frb59-26-3frb59-26-4frb59-26-572072072072072090090090090090011001100110011001100121912191219121912191344134413441344134414751475147514751475100100100100951001002110010030321007916202110100112700303010.42 (10.38)85.25 (72.75)9.06 (10.21)77.39 (90.56)496 (529.25)52.31 (55.5)170 (202.2)1737 (n/a)111 (130)261 (300)1658 (640)1956 (n/a)1989 (n/a)93 (80)967 (1305)1982 (n/a)1959 (n/a)1771 (n/a)1782 (n/a)1955 (n/a)1993 (n/a)n/an/a1915 (n/a)1719 (n/a)n/an/a1978 (n/a)n/a1708 (420)100100100100100100100100100100100483910010017509948952417979310016921398COVERtime (IQR)1.58 (0.55)17.18 (16.09)5.06 (4)11.79 (8.67)124 (131)14.34 (12.8)38 (35.4)110 (121)21 (18)105 (103 )268 (305)1325 (n/a)1486 (n/a)33 (25)168 (246)1796 (n/a)1279 (n/a)273 (223)1428 (n/a)423 (315)1698 (n/a)1598 (n/a)537 (692)476 (460)168 (128)1607 (n/a)1881 (n/a)1768 (n/a)1980 (n/a)431 (476)sucEWCCtime (IQR)suc1001001001001001001001001001001008256100100308110081100565210010010021764201000.55 (0.48)11.30 (14.21)2.97 (2.35)13.79 (16.05)41.71 (39.08)9.07 (9.3)15 (14.1)56 (70.4)15 (12.5)42 (40.1)124 (135)905 (1379)1348 (n/a)24 (27)85 (97)1696 (n/a)1006 (1270)117 (136)900 (1480)125 (115)1268 (n/a)1387 (n/a)285 (250)183 (188)80 (81)1778 (n/a)1930 (n/a)1294 (n/a)1745 (n/a)174 (182)10010010010010010010010010010010010095100100861001001001001009710010010088379679100NuMVCtime (IQR)0.24 (0.18)4.08 (3.77)1.07 (1.03)2.76 (2.83)10.14 (10.54)2.71 (2.6)5 (5.1)14 (11.9)4 (4.3)11 (10.9)38 (46)177 (149)606 (788)8 (7)19 (19)895 (1099)205 (200)51 (48)266 (311)40 (44)470 (466)659 (780)121 (118)50 (49)27 (23)843 (849)1677 (n/a)636 (788)1004 (1391)62 (70)Table 4: Comparison NuMVC state-of-the-art local search algorithmsBHOSLIB benchmark. BHOSLIB instances hidden optimal vertex cover,whose size shown VC column.702fiN U MVC: N E FFICIENT L OCAL EARCH LGORITHM INIMUM V ERTEX C1002000901800801600701400average run time (s)average success ratebest performance hard random benchmark, vastly improving existing performanceresults. also observe that, NuMVC always minimum IQR value instances,indicates apart efficiency, robustness NuMVC also better solvers.605040302012001000800PLSCOVEREWCCNuMVC600400100760(frb40)PLSCOVEREWCCNuMVC200945(frb45)0760(frb40)1150(frb50)1272(frb53) 1400(frb56) 1534(frb59)number vertices graph945(frb45)1150(frb50)1272(frb53) 1400(frb56) 1534(frb59)number vertices graphFigure 1: Comparison NuMVC local search algorithms BHOSLIB benchmarkterms success rate (left) averaged run time (right)also compare NuMVC COVER EWCC challenging instance frb100-40.Given failure PLS large BHOSLIB instances, run PLS instance.comparative results frb100-40 shown Table 5, indicates NuMVCsignificantly outperforms COVER EWCC challenging instance.Finally, would like remark performance NuMVC BHOSLIB benchmarkbetter four-core version CLS (Pullan et al., 2011), even divide run timeNuMVC 4 (the number cores utilized CLS). consider machine speed ratiodivide run time NuMVC 4, NuMVC would dramatically better CLSBHOSLIB benchmark.SizeVC39023903suc033COVERavg suc timen/a2768suc179EWCCavg suc time25862025suc493NuMVCavg suc time29551473Table 5: Comparative results frb100-40 challenging instance. solver executed100 times instance timeout 4000 seconds.6.5 Comparison Exact Algorithmssection, compare NuMVC state-of-the-art exact Maximum Clique algorithm.Generally, exact algorithms heuristic algorithms somewhat complementaryapplications. Usually, exact algorithms find solutions structured instances faster heuristicalgorithms faster random ones.703fiC AI , U , L UO & ATTARCompared MVC MIS, many exact algorithms designed Maximum Cliqueproblem (Carraghan & Pardalos, 1990; Fahle, 2002; Ostergard, 2002; Regin, 2003; Tomita &Kameda, 2009; Li & Quan, 2010b, 2010a). recent branch-and-bound MC algorithm MaxCLQ(Li & Quan, 2010b) utilizes MaxSAT inference technologies (Li, Manya, & Planes, 2007)improve upper bounds shows considerable progress. Experimental results MaxCLQ (Li &Quan, 2010b) random graphs DIMACS instances indicate MaxCLQ significantlyoutperforms previous exact MC algorithms. MaxCLQ algorithm improved using twostrategies called Extended Failed Literal Detection Soft Clause Relaxation, resulting betteralgorithm denoted MaxCLQdyn+EFL+SCR (Li & Quan, 2010a). Due great successMaxCLQdyn+EFL+SCR, compare algorithm MaxCLQdyn+EFL+SCR.compare NuMVC MaxCLQdyn+EFL+SCR DIMACS benchmark instances.results MaxCLQdyn+EFL+SCR taken previous work (Li & Quan, 2010a).MaxCLQdyn+EFL+SCR evaluated BHOSLIB benchmark much harderrequires effective technologies exact algorithms (Li & Quan, 2010a).run time results MaxCLQdyn+EFL+SCR obtained 3.33 GHz Intel Core 2 DuoCPU linux 4 Gb memory, required 0.172 seconds r300.5, 1.016 secondsr400.5 3.872 seconds r500.5 execute DIMACS machine benchmarks (Li & Quan,2010a). corresponding run time machine 0.19, 1.12 4.24 seconds. So, multiplyreported run time MaxCLQdyn+EFL+SCR 1.098 (=(4.24/3.872+1.12/1.016)/2=1.098,average two largest ratios). normalization based methodology establishedSecond DIMACS Implementation Challenge Cliques, Coloring, Satisfiability,widely used comparing different MaxClique algorithms (Pullan & Hoos, 2006; Pullan, 2006; Li& Quan, 2010b, 2010a).GraphInstancebrock400 2brock400 3brock400 4brock800 2brock800 3brock800 4keller5MANN a27MANN a45V C371369367776775774749252690NuMVCsuctime96100100000100100100572.398.254.98n/an/an/a0.04<0.00186.86MaxCLQdyn+EFL+SCR time125.06251.44119.245138.103298.392391.446884.460.1721.169GraphInstanceV CNuMVCsuctimep hat300-3p hat700-2p hat700-3p hat1000-2p hat1000-3p hat1500-1p hat1500-2sanr200 0.9sanr400 0.7264656638954932148814351583791001001001001001001001001000.0010.0060.0080.0190.0323.750.071<0.0010.008MaxCLQdyn+EFL+SCR time1.313.271141.92108.94113860.403.10866.515.2097.72Table 6: Comparison NuMVC state-of-the-art exact MaxClique algorithm MaxCLQdyn+EFL+SCR DIMACS benchmark.Table 6, present performance NuMVC MaxCLQdyn+EFL+SCRDIMACS instances. results indicate NuMVC finds optimal solution much fasterMaxCLQdyn+EFL+SCR random instances p hat sanr instances.believe similar results would hold hard random benchmarks like BHOSLIB ones,MaxCLQdyn+EFL+SCR evaluated instances due high hardness (Li & Quan,2010a), NuMVC performs well them.structured instances, note MaxCLQdyn+EFL+SCR mainly evaluatedbrock instances NuMVC performs worst, open DIMACS instances704fiN U MVC: N E FFICIENT L OCAL EARCH LGORITHM INIMUM V ERTEX CMANN a81, johnson32-2-4 keller6, remain difficult solve exactalgorithms (Li & Quan, 2010a). Although MaxCLQdyn+EFL+SCR overall performs better,NuMVC also finds optimal solution significantly faster MaxCLQdyn+EFL+SCRstructured instances, two brock instances keller5.Finally, would like note although heuristic solvers find optimal solutions fast,unable prove optimality solutions find. hand, run timeexact algorithm spent finding optimal solution also proving optimality.sense, heuristic exact algorithms cannot compared fair way. Nevertheless,experiments suggest heuristic approaches appealing solving large instances reasonableshort time.7. Discussionssection, first explore run-time distribution NuMVC representativeinstances, investigate effectiveness two-stage exchange strategyforgetting mechanism NuMVC. Finally, analyze performance NuMVC differentsettings two parameters forgetting mechanism, shows NuMVCsensitive parameters.7.1 Run-time Distributions NuMVCsubsection, conduct empirical study gain deeper insights run-time behaviorNuMVC. specifically, study run-time distribution NuMVC several representativeinstances. purpose comparison, also report run-time distribution EWCC,best competing MVC local search solver.Consider randomized algorithm solving given optimization problem instance, haltingsoon optimal solution found. run time algorithm viewedrandom variable, fully described distribution, commonly referred run-timedistribution (RTD) literature algorithm performance modeling (Hoos & Stutzle, 2004;Bartz-Beielstein et al., 2010). methodology studying run-time behavior algorithmsbased RTDs widely used empirical analysis heuristic algorithms (Hoos & Stutzle,1999; Finkelstein, Markovitch, & Rivlin, 2003; Watson, Whitley, & Howe, 2005; Pullan & Hoos,2006). also follow methodology study here.studying typical run-time behaviour, choose instances NuMVC reaches optimalsolution 100 runs, appropriate difficulty. DIMACS benchmark, selectbrock400 4 MANN a45, reasonable size hardness. Also, twoinstances represent two typical instance classes NuMVC, NuMVC poor performancebrock instances, dominates heuristic algorithms MANN instances.BHOSLIB benchmark, frb56-25-5 frb59-26-5 selected. appropriateinstances studying run-time behavior NuMVC, since neither easysolved short time difficult reach 100% success rate.empirical RTD graphs NuMVC EWCC shown Figure 2 (the RTDinstance based 100 independent runs reach respective optimal solution). Accordinggraphs, NuMVC shows large variability run time. investigation indicatesRTDs quite well approximated exponential distributions, labeled ed[m](x) = 1 2x/m ,median distribution. test goodness approximations, use705fiC AI , U , L UO & ATTAREmpirical RTD NuMVC EWCC MANN_a45Empirical RTD NuMVC EWCC brock400_410.90.80.710.9RTD NuMVCed[3.6]RTD EWCCed[12]0.80.70.6P(solve)P(solve)0.60.50.50.40.40.30.30.20.20.10.102101010121010runtime [CPU sec]100110310Empirical RTD NuMVC EWCC frb562550.80.70.9RTD NuMVCed[19]RTD EWCCed[53]0.80.72310410RTD NuMVCed[45]RTD EWCCed[116]0.6P(solve)P(solve)11010runtime [CPU sec]10.60.50.50.40.40.30.30.20.20.10.10110010Empirical RTD NuMVC EWCC frb5926510.9RTD NuMVCed[59]RTD EWCCed[532]010110runtime [CPU sec]2100110310010110runtime [CPU sec]210310Figure 2: Run-time distributions (RTDs) NuMVC EWCC applied two DIMACS instances(top) two BHOSLIB instances (bottom); empirical RTDs well approximatedexponential distributions, labeled ed[m](x) = 1 2x/m plots.Kolmogorov-Smirnov test, fails reject null hypothesis sampled run timestems exponential distributions shown figures standard confidence level= 0.05 p-values 0.19 0.88. EWCC, Kolmogorov-Smirnov testshows RTDs MANN a45 two BHOSLIB instances also exponential distributions,RTD brock400 4 exponential distribution.observation exponential RTDs NuMVC consistent similar resultshigh performance SLS algorithms, e.g., MaxClique (Pullan & Hoos, 2006), SAT (Hoos &Stutzle, 1999), MAXSAT (Smyth, Hoos, & Stutzle, 2003), scheduling problems (Watsonet al., 2005). arguments (Hoos & Stutzle, 1999; Hoos & Stutzle, 2004) made stochasticlocal search algorithms characterized exponential RTD, conclude that, NuMVC,probability finding optimal solution within fixed amount time (or steps) dependrun time past. Consequently, robust w.r.t. cutoff time thus, restart706fiN U MVC: N E FFICIENT L OCAL EARCH LGORITHM INIMUM V ERTEX Ctime. Therefore, performing multiple independent runs NuMVC parallel result closeto-optimal parallelization speedup. Similar observations made DIMACSinstances BHOSLIB instances.practical interest also RTD analysis NuMVC difficult instancesalgorithms experiments fail achieve high success rate (i.e., 40%). RTDscases would show algorithm stagnates suggest a-posteriori restart timealgorithm. purpose, select MANN a81 frb59-26-2 analysis. RTDsNuMVC two instances illustrated Figure 3. Interestingly, RTDsobserve obvious stagnation, confirms NuMVC robust w.r.t. cutoff timethus restart time. Therefore, increasing cutoff time, expect higher successrate algorithm difficult instances.Empirical RTD NuMVC MANN_a81 frb5926210.9RTD NuMVC MANN_a810.8RTD NuMVC frb592620.7P(solve)0.60.50.40.30.20.10110231010410runtime [CPU sec]Figure 3: Run-time distributions (RTDs) NuMVC MANN a81 frb59-26-2 instances,NuMVC finds optimal (or best known) solution less half runs.7.2 Effectiveness Two-Stage Exchangestudy effectiveness two-stage exchange strategy, compare NuMVCalternative algorithm NuMVC0 selects two vertices exchanging simultaneously.step, NuMVC0 first chooses uncovered edge e uniformly random, evaluates pairvertices u v u current candidate solution v one endpoint econf Change(v) = 1. evaluating benefit (i.e., decrement cost function)exchanging vertex pair u v, NuMVC0 first checks whether neighbors. uv neighbors, benefit dscore(u) + dscore(v) + w(e{u, v}); otherwise, benefitdscore(u) + dscore(v). NuMVC0 selects vertex pair greatest benefit exchange.NuMVC (and also NuMVC0 ) algorithm, two candidate vertices addcurrent candidate solution C (i.e., endpoints selected uncovered edge). Hence,worst case, NuMVC performs 2 + |C| evaluations, NuMVC0 evaluate 2 |C|pairs vertices. Moreover, NuMVC needs check dscore vertex (vertex)707fiC AI , U , L UO & ATTARevaluation, NuMVC0 performs vertex-pair evaluation involves pair verticesrelationship, thus time-consuming. Based analysis, conjecturecomplexity per step NuMVC least 2 times lower NuMVC0 . Also,mentioned Section 3, two-stage exchange strategy less greedy one selectingtwo vertices exchanging simultaneously, NuMVC0 does.investigation carried 4 DIMACS instances different families well12 BHOSLIB instances. DIMACS benchmark, select brock400 2, C4000.5,MANN a45, p hat 1500-1. instances different characteristics, described(Pullan et al., 2011). Note following conclusions DIMACS instancescomplementary DIMACS graphs.DIMACS brock instances minimum vertex covers consist medium lowerdegree vertices, designed defeat greedy heuristics.DIMACS C p hat 1500-1 instances minimum vertex covers consisthigher degree vertices effectively solved greedy heuristics.DIMACS MANN instances large proportion plateaus instance searchspace, thus greedy heuristics unsuitable solve them.BHOSLIB instances minimum vertex covers consisting vertices whosedistribution vertex degree closely matches complete graph. difficultinstances greedy diversification heuristics.GraphInstancebrock400 2C4000.5MANN a45p hat 1500-1frb50-23-1frb50-23-2frb50-23-3frb53-24-1frb53-24-2frb53-24-3frb56-25-1frb56-25-2frb56-25-3frb59-26-1frb59-26-2frb59-26-3V C37139826901488110011001100121912191219134413441344147514751475suc96100100100100100958610010010097100883796time572252863.7538177606895205514706591218431677636NuMVCsteps645631471780278590642150445830246280191135696063863423295146191491179808332937640625990302335004813267043078440874471875964146325417225#steps/sec (105 )11.30.310.51.26.56.46.45.75.85.85.55.35.55.25.25.1suc1910010010010010063451001007252100452169time186160756413.24884991312159555710610881499253157218531545NuMVC0steps8378447496343304186350533381762181250421048410432625596142863968401058638021968535818432349225497301643062419251520339315425608247273810#steps/sec (105 )4.50.13.30.32.12.12.01.81.91.91.71.71.71.61.71.6Table 7: Comparative performance NuMVC NuMVC0 selects two verticesexchanging simultaneously. results based 100 independent runs solverinstance.comparative results NuMVC NuMVC0 presented Table 7. results showNuMVC significantly outperforms NuMVC0 terms averaged run time, primarily duemuch lower complexity per step. second, NuMVC performs 3-4 times steps708fiN U MVC: N E FFICIENT L OCAL EARCH LGORITHM INIMUM V ERTEX CNuMVC0 , supports conjecture complexity per step NuMVC 2times lower NuMVC0 .turn attention comparing NuMVC NuMVC0 terms step performance,independent complexity per step. brock MANN graphs difficultgreedy heuristics, NuMVC significantly better step performance NuMVC0 .hand, greedy-friendly graphs C4000.5 p hat 1500-1, NuMVC needssteps converge optimal solution NuMVC0 does. observations supportargument two-stage exchange strategy less greedy one selects two verticesexchanging simultaneously, NuMVC0 does.also observe step performance NuMVC0 better NuMVCBHOSLIB instances. instance, BHOSLIB instances algorithms100% success rate, NuMVC needs 1.2 times steps NuMVC0 find optimalsolution. expect cannot yet explain. Nevertheless, NuMVC makesrather rapid modifications solution, little degrade step performance hurt.GraphInstanceC4000.5MANN a45p hat 1500-1frb53-24-5frb56-25-5frb59-26-5PLS#steps/sec85,3181,546,625170,511841,346801,282706,436COVER#steps/sec8,699279,51419,473128,971116,618108,534EWCC#steps/sec11,927578,65634,111219,038199,441189,536NuMVC#steps/sec30,9631,053,978118,888570,425522,561511,014Table 8: Complexity per step selected instancesdemonstrate low complexity per step NuMVC, compare numbersearch steps per second NuMVC state-of-the-art heuristic solversrepresentative instances. indicated Table 8, NuMVC executes many steps secondtwo MVC local search solvers COVER EWCC do. instances Table8, second NuMVC executes 4-6 times steps COVER, 3-4 times stepsEWCC. indicates two-stage exchange strategy significantly accelerate MVClocal search algorithms. Although PLS performs steps per second NuMVC, MClocal search algorithm whose search scheme essentially different MVC local searchalgorithms.7.3 Effectiveness Forgetting Mechanismstudy effectiveness forgetting mechanism NuMVC, compare NuMVCtwo alternative algorithms NuMVC1 NuMVC2 , obtained NuMVC modifyingedge weighting scheme below.NuMVC1 works way NuMVC, except using forgetting mechanism,is, deleting line 18 Algorithm 1.NuMVC2 adopts forgetting mechanism used DLS-MC (Pullan & Hoos, 2006)weighting scheme. specifically, NuMVC2 increases weights uncovered edges709fiC AI , U , L UO & ATTARone end step, performs forgetting operation every pd steps decreasingweights one edges whose weights greater one. Note pd instancedependent parameter.experiments carried representative instances benchmarks.DIMACS benchmark, select brock400 2, C4000.5, keller6, MANN a45,different classes appropriate difficulty. BHOSLIB benchmark,select three instances three largest-sized instance groups respectively.GraphInstance Verticesbrock400 2400C4000.54000keller63361MANN a451035frb53-24-11272frb53-24-21272frb53-24-31272frb56-25-11400frb56-25-21400frb56-25-31400frb59-26-11534frb59-26-21534frb59-26-31534CV37139823302690121912191219134413441344147514751475NuMVCsuc time96572100252100 2.51100868689510020510051100470976591001218884337 167796636NuMVC1suc time22 1781100270100 2.9565 118760925100243100498591463 120910011164 122921 189483997NuMVC2pd (102 ) suc time15 1002160 100327750 100 4.268 10011310078901100 100201100 100521309159513081739130 1001171508590715045 143915097652Table 9: Comparative performance NuMVC two alternatives NuMVC1 NuMVC2 .algorithm performed 100 times instance.apparent observation Table 9 two algorithms forgetting mechanisms(i.e., NuMVC NuMVC2 ) outperform NuMVC1 almost instances. Particularly, duemissing forgetting mechanism, NuMVC1 performs significantly worse twoalgorithms brock MANN graphs. hand, Table 9 demonstrates NuMVCNuMVC2 exhibit competitive performance BHOSLIB benchmark, dominate differenttypes DIMACS instances. specifically, NuMVC outperforms NuMVC2 C4000.5,keller6 MANN a45, performs significantly worse NuMVC2 brock400 2.order find genuine performance NuMVC2 brock instances, test NuMVC2larger brock800 2 brock800 4 instances. results show two large brockinstances substantially difficult two brock400 instances, NuMVC2 also failssolve neither them.Although NuMVC2 shows competitive performance NuMVC, performance givenoptimizing pd parameter instance. Moreover, DLS-MC (Pullan & Hoos, 2006),NuMVC2 considerably sensitive pd parameter. example, experiments showfrb53-24 instances, NuMVC2 performs quite well pd = 10000, fails findoptimal solution pd set value less 7000. Comparatively, NuMVC710fiN U MVC: N E FFICIENT L OCAL EARCH LGORITHM INIMUM V ERTEX Cparameter setting performs quite well types instances brock family. Actually,show next section NuMVC sensitive parameters.also interesting compare NuMVC alternatives replace forgettingmechanism smoothing techniques similar local search SAT. Indeed, earlierversions NuMVC use smoothing techniques similar SAT local search,good performance compared NuMVC. would interesting findreasons success forgetting mechanism failure smoothing techniquesMVC edge weighting local search algorithms NuMVC.7.4 Parameters Forgetting Mechanism(0.3|V |, 0.1)(0.3|V |, 0.2)(0.3|V |, 0.3)(0.3|V |, 0.4)(0.3|V |, 0.5)brock400 2100% (382)100% (361)100% (362)95% (490)90% (507)MANN a45100% (153)100% (164)100% (131)100% (208)100% (90)C4000.5100% (262)100% (265)100% (272)100% (270)100% (268)frb53-24-180% (904)85% (918)70% (1058)80% (995)65% (1316)frb53-24-2100% (348)100% (279)100% (156)100% (191)100% (431)frb56-25-1100% (338)90% (671)95% (826)100% (602)100% (490)frb56-25-280% (997)70% (1197)85% (819)100% (885)95% (922)(0.4|V |, 0.1)(0.4|V |, 0.2)(0.4|V |, 0.3)(0.4|V |, 0.4)(0.4|V |, 0.5)100% (261)90% (736)100% (402)95% (375)90% (612)100% (133)100% (207)100% (176)100% (169)100% (190)100% (250)100% (245)100% (258)100% (253)100% (264)80% (899)80% (860)75% (1047)70% (1009)65% (1059)100% (158)100% (443)100% (260)100% (394)100% (137)90% (464)85% (611)90% (976)90% (885)95% (428)70% (1601)80% (851)90% (1055)85% (1019)100% (851)(0.5|V |, 0.1)(0.5|V |, 0.2)(0.5|V |, 0.3)(0.5|V |, 0.4)(0.5|V |, 0.5)100% (523)85% (950)96% (572)90% (499)90% (968)100% (107)100% (69)100% (86)100% (169)100% (148)100% (262)100% (259)100% (252)100% (251)100% (249)70% (1007)75% (1061)86% (850)70% (931)90% (805)100% (416)100% (482)100% (205)100% (219)100% (361)90% (714)95% (706)100% (470)90% (632)85% (933)75% (1064)70% (1228)97% (625)80% (1027)85% (983)(0.6|V |, 0.1)(0.6|V |, 0.2)(0.6|V |, 0.3)(0.6|V |, 0.4)(0.6|V |, 0.5)100% (527)80% (713)75% (976)100% (710)85% (742)100% (203)100% (172)100% (92)100% (142)100% (125)100% (255)100% (279)100% (272)100% (276)100% (288)70% (1109)75% (944)70% (1130)75% (907)80% (947)100% (267)100% (254)100% (298)100% (170)100% (192)100% (828)90% (704)90% (689)100% (592)100% (647)90% (878)70% (1306)75% (862)85% (1028)80% (1109)(0.7|V |, 0.1)(0.7|V |, 0.2)(0.7|V |, 0.3)(0.7|V |, 0.4)(0.7|V |, 0.5)100% (410)95% (781)90% (826)75% (1219)90% (707)100% (87)100% (128)100% (125)100% (101)100% (92)100% (273)100% (284)100% (266)100% (272)100% (280)65% (1186)70% (1035)75% (916)85% (700)70% (1085)100% (358)100% (220)100% (206)100% (338)100% (352)75% (1014)80% (713)80% (878)100% (536)90% (736)75% (934)90% (510)80% (971)85% (769)70% (1044)Table 10: Comparative performance NuMVC various parameter combinations (, )forgetting mechanism. instance, NuMVC performed 20 timesparameter combination, except one adopted work (0.5|V |, 0.3),results based 100 runs. keller6, NuMVC performs almostvarious parameters, success rate (100%) tiny difference averagedrun time (less 1 second), thus results reported table.711fiC AI , U , L UO & ATTARNuMVC algorithm two parameters , specify forgetting mechanism.Specifically, averaged weight edges achieves threshold , edge weightsmultiplied constant factor (0 < < 1). subsection, investigate NuMVCperforms different settings two parameters. investigation carriedDIMACS BHOSLIB benchmarks. DIMACS benchmark, select four instancesused preceding subsection reasons. BHOSLIB benchmark, selectfrb53-24-1, frb53-24-2, frb56-25-1 frb56-25-2, different sizesappropriate hardness.Table 10 presents performance NuMVC various parameter combinationsrepresentative instances. see Table 10, parameter combination(0.5|V |, 0.3) yields relatively good performance instances, exhibits better robustnessinstances parameter combinations do.hand, observe NuMVC various parameter combinations performscomparably tested instances. example, parameter settings, NuMVC achievessuccess rate 100% keller6, MANN a45, C4000.5 well frb53-24-1,averaged run time difference instances significant. instances,difference success rate never exceeds 25% two parameter settings. observationindicates NuMVC seems sensitive two parameters. Actually, mentionedbefore, NuMVC exhibits good performance DIMACS BHOSLIB benchmarksfixed parameter setting. advantage compared forgetting mechanismsone used DLS-MC (Pullan & Hoos, 2006), sensitive parameter.algorithms sensitive parameters, considerable parameter tuning required orderget good performance certain instance, usually costs much time solvinginstance.8. Conclusions Future Workpaper, presented two new local search strategies minimum vertex cover (MVC)problem, namely two-stage exchange edge weighting forgetting. two-stage exchangestrategy yields efficient two-pass move operator MVC local search algorithms,significantly reduces time complexity per step. forgetting mechanism enhancesedge weighting scheme decreasing weights averaged weight reaches threshold,periodically forget earlier weighting decisions. Based two strategies, designedslight, yet effective MVC local search algorithm called NuMVC. NuMVC algorithmevaluated best known heuristic algorithms MVC (MC, MIS) standard benchmarks,i.e., DIMACS BHOSLIB benchmarks. experimental results show NuMVClargely competitive DIMACS benchmark dramatically outperforms state-of-the-artheuristic algorithms BHOSLIB instances.Furthermore, showed NuMVC characterized exponential RTDs, meansrobust w.r.t. cutoff parameters restart time, hence close-to-optimal parallelizationspeedup. also performed investigations provide insights two newstrategies effectiveness. Finally, conducted experiment study performanceNuMVC different parameter settings, results indicate NuMVC sensitiveparameters.712fiN U MVC: N E FFICIENT L OCAL EARCH LGORITHM INIMUM V ERTEX Ctwo-stage exchange strategy lower time complexity per step, alsoflexibility allow us employ specific heuristics different stages. interesting researchdirection thus apply idea combinatorial problems whose essential tasks alsoseek optimal subset fixed cardinality.Acknowledgmentswork supported 973 Program 2010CB328103, ARC Future Fellowship FT0991785,National Natural Science Foundation China (61073033, 61003056 60903054),Fundamental Research Funds Central Universities China (21612414). would likethank editor anonymous reviewers valuable comments earlier versionspaper. would also like thank Yanyan Xu proofreading paper.ReferencesAggarwal, C., Orlin, J., & Tai, R. (1997). Optimized crossover independent set problem.Operations Research, 45, 226234.Andrade, D. V., Resende, M. G. C., & Werneck, R. F. F. (2008). Fast local search maximumindependent set problem. Workshop Experimental Algorithms, pp. 220234.Barbosa, V. C., & Campos, L. C. D. (2004). novel evolutionary formulation maximumindependent set problem. J. Comb. Optim., 8(4), 419437.Bartz-Beielstein, T., Chiarandini, M., Paquete, L., & Preuss, M. (Eds.). (2010). ExperimentalMethods Analysis Optimization Algorithms. Springer, Berlin, Heidelberg, NewYork.Battiti, R., & Protasi, M. (2001). Reactive local search maximum clique problem.Algorithmica, 29(4), 610637.Busygin, S., Butenko, S., & Pardalos, P. M. (2002). heuristic maximum independent setproblem based optimization quadratic sphere. J. Comb. Optim., 6(3), 287297.Cai, S., & Su, K. (2011). Local search configuration checking SAT. Proc. ICTAI-11,pp. 5966.Cai, S., & Su, K. (2012). Configuration checking aspiration local search SAT. Proc.AAAI-12, pp. 434440.Cai, S., Su, K., & Chen, Q. (2010). EWLS: new local search minimum vertex cover. Proc.AAAI-10, pp. 4550.Cai, S., Su, K., & Sattar, A. (2011). Local search edge weighting configuration checkingheuristics minimum vertex cover. Artif. Intell., 175(9-10), 16721696.Cai, S., Su, K., & Sattar, A. (2012). Two new local search strategies minimum vertex cover.Proc. AAAI-12, pp. 441447.Carraghan, R., & Pardalos, P. (1990). exact algorithm maximum clique problem.Operations Research Letters, 9(6), 375382.713fiC AI , U , L UO & ATTARDinur, I., & Safra, S. (2005). hardness approximating minimum vertex cover. AnnalsMathematics, 162(2), 439486.Evans, I. (1998). evolutionary heuristic minimum vertex cover problem. ProceedingsSeventh International Conference Evolutionary Programming(EP), pp. 377386.Fahle, T. (2002). Simple fast: Improving branch-and-bound algorithm maximum clique.Proc. European Symposium Algorithms (ESA)-02, pp. 485498.Feige, U. (2004). Approximating maximum clique removing subgraphs. SIAM J. Discrete Math.,18(2), 219225.Finkelstein, L., Markovitch, S., & Rivlin, E. (2003). Optimal schedules parallelizing anytimealgorithms: case shared resources. J. Artif. Intell. Res. (JAIR), 19, 73138.Gajurel, S., & Bielefeld, R. (2012). fast near optimal vertex cover algorithm (novca).International Journal Experimental Algorithms (IJEA), 3, 918.Garey, M., & Johnson, D. (1979). Computers Intractability: Guide Theory NPcompleteness. Freeman, San Francisco, CA, USA.Glover, F. (1989). Tabu search part i. ORSA Journal Computing, 1(3), 190206.Grosso, A., Locatelli, M., & Pullan, W. J. (2008). Simple ingredients leading efficientheuristics maximum clique problem. J. Heuristics, 14(6), 587612.Halperin, E. (2002). Improved approximation algorithms vertex cover problem graphshypergraphs. SIAM Journal Computing, 31(5), 15081623.Hastad, J. (1999). Clique hard approximate within n1 . Acta Math, 182, 105142.Hastad, J. (2001). optimal inapproximability results. J. ACM, 48(4), 798859.Hoaglin, D. C., Mosteller, F., & Tukey, J. W. (Eds.). (2000). Understanding Robust ExploratoryData Analysis. Wiley Classics Library, Wiley, New York, NY.Hoos, H., & Stutzle, T. (2004). Stochastic Local Search: Foundations Applications. MorganKaufmann, San Francisco, CA, USA.Hoos, H. H., & Stutzle, T. (1999). Towards characterisation behaviour stochastic localsearch algorithms SAT. Artif. Intell., 112(1-2), 213232.Hutter, F., Tompkins, D. A. D., & Hoos, H. H. (2002). Scaling probabilistic smoothing: Efficientdynamic local search SAT. Proc. CP-02, pp. 233248.Ishtaiwi, A., Thornton, J., Sattar, A., & Pham, D. N. (2005). Neighbourhood clause weightredistribution local search SAT. Proc. CP-05, pp. 772776.Johnson, D. S., & Trick, M. (Eds.). (1996). Cliques, Coloring, Satisfiability: Second DIMACSImplementation Challenge, 1993, Vol. 26 DIMACS Series Discrete MathematicsTheoretical Computer Science. American Mathematical Society, Providence, RI, USA.Karakostas, G. (2005). better approximation ratio vertex cover problem. Proc.ICALP-05, pp. 10431050.Katayama, K., Sadamatsu, M., & Narihisa, H. (2007). Iterated k-opt local search maximumclique problem. Proc. EvoCOP-07, pp. 8495.714fiN U MVC: N E FFICIENT L OCAL EARCH LGORITHM INIMUM V ERTEX CLi, C. M., Manya, F., & Planes, J. (2007). New inference rules max-sat. J. Artif. Intell. Res.(JAIR), 30, 321359.Li, C. M., & Quan, Z. (2010a). Combining graph structure exploitation propositional reasoningmaximum clique problem. Proc. ICTAI-10, pp. 344351.Li, C. M., & Quan, Z. (2010b). efficient branch-and-bound algorithm based maxsatmaximum clique problem. Proc. AAAI-10, pp. 128133.Michiels, W., Aarts, E. H. L., & Korst, J. H. M. (2007). Theoretical aspects local search. Springer.Minton, S., Johnston, M. D., Philips, A. B., & Laird, P. (1992). Minimizing conflicts: heuristicrepair method constraint satisfaction scheduling problems. Artif. Intell., 58(1-3), 161205.Morris, P. (1993). breakout method escaping local minima. Proc. AAAI-93, pp.4045.Ostergard, P. R. J. (2002). fast algorithm maximum clique problem. Discrete AppliedMathematics, 120(1-3), 197207.Papadimitriou, C. H. (1991). selecting satisfying truth assignment. Proc. FOCS-91, pp.163169.Pullan, W. (2006). Phased local search maximum clique problem. J. Comb. Optim., 12(3),303323.Pullan, W. (2009). Optimisation unweighted/weighted maximum independent sets minimumvertex covers. Discrete Optimization, 6, 214219.Pullan, W., & Hoos, H. H. (2006). Dynamic local search maximum clique problem. J. Artif.Intell. Res. (JAIR), 25, 159185.Pullan, W., Mascia, F., & Brunato, M. (2011). Cooperating local search maximum cliqueproblem. J. Heuristics, 17(2), 181199.Regin, J. C. (2003). Using constraint programming solve maximum clique problem. Proc.CP-03, pp. 634648.Richter, S., Helmert, M., & Gretton, C. (2007). stochastic local search approach vertex cover.Proc. KI-07, pp. 412426.Schuurmans, D., Southey, F., & Holte, R. C. (2001). exponentiated subgradient algorithmheuristic boolean programming. Proc. IJCAI-01, pp. 334341.Shyu, S. J., Yin, P., & Lin, B. M. T. (2004). ant colony optimization algorithm minimumweight vertex cover problem. Annals OR, 131(1-4), 283304.Smyth, K., Hoos, H. H., & Stutzle, T. (2003). Iterated robust tabu search max-sat. Proc.Canadian Conference AI-03, pp. 129144.Taillard, E. D. (1994). Parallel taboo search techniques job shop scheduling problem.INFORMS Journal Computing, 6(2), 108117.Thornton, J., Pham, D. N., Bain, S., & Jr., V. F. (2004). Additive versus multiplicative clauseweighting SAT. Proc. AAAI-04, pp. 191196.715fiC AI , U , L UO & ATTARTomita, E., & Kameda, T. (2009). efficient branch-and-bound algorithm finding maximumclique computational experiments. J. Global Optimization, 44(2), 311.Watson, J.-P., Whitley, L. D., & Howe, A. E. (2005). Linking search space structure, run-timedynamics, problem difficulty: step toward demystifying tabu search. J. Artif. Intell.Res. (JAIR), 24, 221261.Wu, Q., Hao, J.-K., & Glover, F. (2012). Multi-neighborhood tabu search maximum weightclique problem. Annals OR, 196(1), 611634.Wu, Z., & Wah, B. W. (2000). efficient global-search strategy discrete lagrangian methodssolving hard satisfiability problems. Proc. AAAI/IAAI-00, pp. 310315.Xu, K., Boussemart, F., Hemery, F., & Lecoutre, C. (2005). simple model generate hardsatisfiable instances. Proc. IJCAI-05, pp. 337342.Xu, K., Boussemart, F., Hemery, F., & Lecoutre, C. (2007). Random constraint satisfaction: Easygeneration hard (satisfiable) instances. Artif. Intell., 171(8-9), 514534.Xu, K., & Li, W. (2000). Exact phase transitions random constraint satisfaction problems. J.Artif. Intell. Res. (JAIR), 12, 93103.Xu, K., & Li, W. (2006). Many hard examples exact phase transitions. Theoretical ComputerScience, 355, 291302.Yugami, N., Ohta, Y., & Hara, H. (1994). Improving repair-based constraint satisfaction methodsvalue propagation. AAAI, pp. 344349.Zuckerman, D. (2006). Linear degree extractors inapproximability max cliquechromatic number. Proc. STOC-06, pp. 681690.716fiJournal Artificial Intelligence Research 46 (2013) 165-201Submitted 7/12; published 2/13Generating Extractive Summaries Scientific ParadigmsVahed Qazvinianvahed@umich.eduDepartment EECS,University Michigan, Ann Arbor, MI, 48109Dragomir R. Radevradev@umich.eduDepartment EECS & School Information,University Michigan, Ann Arbor, MI, 48109Saif M. Mohammadsaif.mohammad@nrc-cnrc.gc.caNational Research Council Canada,Ottawa, Ontario, Canada, K1A 0R6Bonnie DorrDavid ZajicMichael WhidbyTaesun Moonbonnie@umiacs.umd.edudmzajic@umiacs.umd.edumawhidby@umd.edutsmoon@umiacs.umd.eduInstitute Advanced Computer Studies & Computer Science,University Maryland, College Park, MD, 20742AbstractResearchers scientists increasingly find positionquickly understand large amounts technical material. goal effectively serveneed using bibliometric text mining summarization techniques generatesummaries scientific literature. show use citations produce automatically generated, readily consumable, technical extractive summaries. first proposeC-LexRank, model summarizing single scientific articles based citations,employs community detection extracts salient information-rich sentences. Next,extend experiments summarize set papers, cover scientific topic. generate extractive summaries set Question Answering (QA)Dependency Parsing (DP) papers, abstracts, citation sentences showcitations unique information amenable creating summary.1. Introductiontodays rapidly expanding disciplines, scientists scholars constantly faceddaunting task keeping knowledge field. addition, increasinglyinterconnected nature real-world tasks often requires experts one discipline rapidlylearn areas short amount time. Cross-disciplinary research requiresscientists areas linguistics, biology, sociology learn computationalapproaches applications computational linguistics, biological modeling,social networks. Authors journal articles books must write accurate summariesprevious work, ranging short summaries related research in-depth historical notes.Interdisciplinary review panels often called upon review proposals wide rangec2013AI Access Foundation. rights reserved.fiQazvinian et Al.areas, may unfamiliar panelists. Thus, must learn newdiscipline fly order relate expertise proposal.goal effectively serve needs combining two currently available technologies: (1) bibliometric lexical link mining exploits structure citations (2)summarization techniques exploit content material citingcited papers.generally agreed upon manually written abstracts good summaries individual papers. recently, Qazvinian Radev (2008) argued citation sentences(i.e., set sentences appear papers cite given article) usefulcreating summary important contributions research paper. Kaplan, Iida,Tokunaga (2009) introduced citation-site block text includes citationdiscusses cited text. work used machine learning method extracting citationsresearch papers evaluates result using annotated corpus 38 papers citing4 articles. Moreover, Qazvinian Radev (2010) showed usefulness using implicitcitations (i.e., context sentences, sentences occur citation sentenceexplicitly cite target paper, discuss contributions) summary generation. Teufel (2005) argued citations could contain subjective content,content exploited summary generation. Additional work (Mohammad et al.,2009) demonstrated usefulness citations producing multi-document summariesscientific articles. Follow-up work indicated improvements citation handlingenables production fluent summaries (Whidby, 2012).work, develop summarization systems exploit citations. Specifically,compare contrast usefulness abstracts citations automaticallygenerating technical summary given topic multiple research papers.findings suggest abstracts citations overlapping informationalso significant amount unique summary-amenable information. Particularly, provide evidence citation sentences contain crucial informationavailable, hard extract, abstracts papers alone.propose C-LexRank, graph based summarization system. method modelsset citing sentences network vertices sentences edges representlexical similarity. C-LexRank identifies vertex communities (clusters)network, selects sentences different communities increase diversitysummary. Using 30 different sets citation sentences extracted 6 differentNLP topics ACL1 Anthology Network, show C-LexRank effectiveproducing summary papers contributions. compare C-LexRankwide range state-of-the-art summarization systems leverage diversity (MMR,DivRank, MASCS), employ graph structure (DivRank, LexRank), employ sentencecompression (MASCS) produce summary.extend experiments summarizing contributions single articlegenerating summaries scientific topics. evaluation experiments extractivesummary generation applied set 10 papers research area QuestionAnswering (QA) another set 16 papers Dependency Parsing (DP).1. Association Computational Linguistics166fiGenerating Extractive Summaries Scientific Paradigmsprovide background work including primary features technicalsummary also types input used study (full papers, abstracts,citation sentences).1.1 BackgroundAutomatically creating technical extractive summaries significantly distinct traditional multi-document summarization. describe primary characteristicstechnical extractive summary present different types input texts usedproduction extractive summaries.1.1.1 Technical Extractive Summariescase multi-document summarization, goal produce readable presentation multiple documents, whereas case technical summary creation, goalconvey key features basic underpinnings particular field, early latedevelopments, important contributions findings, contradicting positions may reverse trends start new sub-fields, basic definitions examples enable rapidunderstanding field non-experts.prototypical example technical summary chapter notes, i.e., short(50500 word) descriptions sub-areas found end chapters textbooks,Jurafsky Martins (2008). One might imagine producing descriptions automatically, hand-editing refining use actual textbook.Previously Mohammad et al. (2009) conducted human analysis chapter notesrevealed set conventions, outline provided (with examplesentences italics):1. Introductory/opening statement: earliest computational use X Y, considered many foundational work area.2. Definitional follow up: X defined Y.3. Elaboration definition (e.g., example): early algorithms basedZ.4. Deeper elaboration, e.g., pointing issues initial approaches: Unfortunately,model seems wrong.5. Contrasting definition: Algorithms since then...6. Introduction additional specific instances / historical background citations:Two classic approaches described Q.7. References summaries: R provides comprehensive guide details behindX.notion text level categories zoning technical papersrelated summarycomponents enumerated abovehas investigated previously work TeufelMoens (2002) Nanba, Kando, Okumura (2000). earlier works focused167fiQazvinian et Al.analysis scientific papers based rhetorical structure determiningportions papers contain new results, comparisons earlier work, etc. workdescribed focuses synthesis technical summary based knowledge gleanedrhetorical structure unlike work earlier researchers, guidedstructural patterns along lines conventions listed above.Although current approach summary creation yet incorporate fullypattern-based component, ultimate objective apply patterns guidecreation refinement final output. first step toward goal, use citationsentences (closest structure patterns identified convention 7 above) pickimportant content summary creation.1.1.2 Scholarly TextsPublished research particular topic summarized two different kindssources: (1) author describes work (2) others describeauthors work (usually relation work). authors descriptionwork found paper. others perceive work spread across paperscite work.Traditionally, technical summary generation tackled summarizing setresearch papers pertaining topic. However, individual research papers usually comemanually-created summariestheir abstracts. abstract paper maysentences set context, state problem statement, mention problemapproached, bottom-line resultsall 200 500 words. Thus, usingabstracts (instead full papers) input summarization system worth exploring.Whereas abstract paper presents authors think importantaspects paper, citations paper capture others field perceivebroader contributions paper. two perspectives expectedoverlap content, citations also contain additional information foundabstracts (Elkiss, Shen, Fader, Erkan, States, & Radev, 2008; Nakov & Hearst, 2012).example, authors may describe particular methodology one paper combinedanother different paper overcome drawbacks each. Citationsalso indicators contributions described paper influential time.Another feature distinguishes citations abstracts citations tendcertain amount redundant information. multiple papers may describecontributions target paper. redundancy exploited automaticsystems determine important contributions target paper.goal test hypothesis effective technical summary reflect information research perspective authors also perspectiveothers use, commend, discredit, add it. describing experimentstechnical papers, abstracts, citations, first summarize relevant prior work usedsources information input.rest paper organized follows. reviewing related work,present analysis citations demonstrate contain summary-amenableinformation. process, develop C-LexRank, citation-based summarization system.Section 5, show state-of-the-art automatic summarization systems create168fiGenerating Extractive Summaries Scientific Paradigmscontentful summaries citations individual documents created simplyrandom sampling. also show C-LexRank performs better state-of-theart summarization systems producing 100- 200-word extracts. Section 6,extend experiments summarize set papers representing scientifictopic using source texts well citations topic papers. Additionally, showusefulness citation sentences automatically generating technical summarygiven topic. observe that, expected, abstracts useful summary creation, but,notably, also conclude citations contain crucial information present (orleast, easily extractable from) abstracts. discover abstracts authorbiased thus complementary broader perspective inherent citation sentences;differences enable use range different levels types informationsummary.2. Related Worksection, review related prior work two categories. First, review previousresearch citation analysis, discuss prior work capturing diversityautomatic text summarization.2.1 Citation AnalysisPrevious work analyzed citation collaboration networks (Teufel, Siddharthan, &Tidhar, 2006; Newman, 2001) scientific article summarization (Teufel & Moens, 2002).Bradshaw (2002, 2003) benefited citations determine content articlesintroduce Reference Directed Indexing improve results search engine. Nanba,Abekawa, Okumura, Saito (2004) Nanba et al. (2000) analyzed citation sentencesautomatically categorize citations three groups using 160 pre-defined phrase-basedrules. categorization used build tool help researchers analyze citationswrite scientific summaries. Nanba Okumura (1999) also discussed citationcategorization support system writing survey. Nanba Okumura (1999)Nanba et al. (2000) reported co-citation implies similarity showing textualsimilarity co-cited papers proportional proximity citations citingarticle.Previous work shown importance citation sentences understandingscientific contributions. Elkiss et al. (2008) performed large-scale study citationsimportance. conducted several experiments set 2, 497 articlesfree PubMed Central (PMC) repository2 66 ACM digital library. Resultsexperiment confirmed average cosine sentences set citationsarticle consistently higher abstract. also reported numbermuch greater average cosine citation sentences randomly chosendocument, well citation sentences abstract. Finally, concludedcontent citing sentences much greater uniformity contentcorresponding abstract, implying citations focused contain additionalinformation appear abstracts.2. http://www.pubmedcentral.gov169fiQazvinian et Al.Nakov Hearst (2012) performed detailed manual study citations areamolecular interactions found set citations given target paper coverinformation found abstract article, well 20% concepts, mainlyrelated experimental procedures.Kupiec, Pedersen, Chen (1995) used abstracts scientific articles targetsummary. used 188 Engineering Information summaries mostly indicativenature. Kan, Klavans, McKeown (2002) used annotated bibliographies covercertain aspects summarization suggest guidelines summaries also includemetadata critical document features well prominent content-based features.Siddharthan Teufel (2007) described new reference task show high humanagreement well improvement performance argumentative zoning (Teufel,2005). argumentative zoninga rhetorical classification taskseven classes (Own, Other,Background, Textual, Aim, Basis, Contrast) used label sentences accordingrole authors argument.problem automatic related work summarization addressed Hoang Kan(2010). work, Hoang Kan used set keywords representing hierarchypaper topics assigned score input sentence construct extractive summary.Athar (2011) addressed problem identifying positive negative sentiment polarity citations scientific papers. Similarly, Athar Teufel (2012) used context-enrichedcitations classify scientific sentiment towards target paper.2.2 Leveraging Diversity Summarizationsummarization, number previous methods focused diversity perspectives. Mei, Guo, Radev (2010) introduced DivRank, diversity-focused rankingmethodology based reinforced random walks information networks. randomwalk model, incorporates rich-gets-richer mechanism PageRank reinforcements transition probabilities vertices, showed promising resultsDocument Understanding Conference (DUC) 2004 dataset. DivRank state-of-the-artgraph-based method leverages diversity perspectives summarization. Therefore, chose algorithm important baseline experiments discussdetail Section 4.similar ranking algorithm, described Zhu, Goldberg, Van Gael, Andrzejewski(2007), Grasshopper ranking model, leverages absorbing random walk.model starts regular time-homogeneous random walk, step vertexhighest weight set absorbing state. Paul, Zhai, Girju (2010) addressedproblem summarizing opinionated text using Comparative LexRank, random walk modelinspired LexRank (Erkan & Radev, 2004). Comparative LexRank first assigns differentsentences clusters based contrastiveness other. modifiesgraph based cluster information performs LexRank modified cosine similaritygraph.Perhaps well-known summarization method address diversity summarization Maximal Marginal Relevance (MMR) (Carbonell & Goldstein, 1998). methodbased greedy algorithm selects sentences step least similar170fiGenerating Extractive Summaries Scientific Paradigmssummary far. compare summarization output MMRdiscuss algorithm details Section 4.prior work evaluating independent contributions content generation, Voorhees(1998) studied IR systems showed relevance judgments differ significantlyhumans relative rankings show high degrees stability across annotators.work, van Halteren Teufel (2004) asked 40 Dutch students 10 NLP researcherssummarize BBC news report, resulting 50 different summaries. also used 6DUC-provided summaries, annotations 10 student participants 4 additionalresearchers, create 20 summaries another news article DUC datasets.calculated Kappa statistic (Carletta, 1996; Krippendorff, 1980) observed high agreement, indicating task atomic semantic unit (factoid) extraction robustlyperformed naturally occurring text, without copy-editing.diversity perspectives growth factoid inventory (Qazvinian &Radev, 2011b) also affects evaluation text summarization. Evaluation methods either extrinsic, summaries evaluated based quality performingspecific task (Sparck-Jones, 1999) intrinsic quality summaryevaluated, regardless applied task (van Halteren & Teufel, 2003; Nenkova & Passonneau, 2004). evaluation methods assess information content summariesgenerated automatically.3. Citation-Based SummarizationACL Anthology Network3 (AAN) manually curated resource built topACL Anthology4 (Bird, Dale, Dorr, Gibson, Joseph, Kan, Lee, Powley, Radev, & Tan,2008). AAN includes papers published ACL related organizations wellComputational Linguistics journal period four decades. AAN consists18, 000 papers 14, 000 authors, distinguished uniqueACL ID, together full-texts, abstracts, citation information. also includesvaluable metadata author affiliations, citation collaboration networks,various centrality measures (Radev, Muthukrishnan, & Qazvinian, 2009; Joseph & Radev,2007).study citations across different areas within Computational Linguistics, first extracted six different sets papers AAN corresponding 6 different NLP topics: Dependency Parsing (DP), Phrase-based Machine Translation (PBMT), Text Summarization(Summ), Question Answering (QA), Textual Entailment (TE), Conditional RandomFields (CRF). build set, matched topic phrase title content AAN papers, extracted 5 highest cited papers. Table 1 shows numberarticles number citation sentences topic5 . number citationsset shows number sentences used input various summarizationsystems experiments.3. http://clair.si.umich.edu/anthology/4. http://www.aclweb.org/anthology-new/5. number incoming citations AANs 2008 release.171fiCRFTEQASummPBMTDPQazvinian et Al.ACL IDC96-1058P97-1003P99-1065P05-1013P05-1012N03-1017W03-0301J04-4002N04-1033P05-1033A00-1043A00-2024C00-1072W00-0403W03-0510A00-1023W00-0603P02-1006D03-1017P03-1001D04-9907H05-1047H05-1079W05-1203P05-1014N03-1023N04-1042W05-0622P06-1009W06-1655TitleThree New Probabilistic Models Dependency Parsing ...Three Generative, Lexicalized Models Statistical ParsingStatistical Parser CzechPseudo-Projective Dependency ParsingOn-line Large-Margin Training Dependency ParsersStatistical Phrase-Based TranslationEvaluation Exercise Word AlignmentAlignment Template Approach Statistical Machine TranslationImprovements Phrase-Based Statistical Machine TranslationHierarchical Phrase-Based Model Statistical Machine TranslationSentence Reduction Automatic Text SummarizationCut Paste Based Text SummarizationAutomated Acquisition Topic Signatures ...Centroid-Based Summarization Multiple Documents ...Potential Limitations Automatic Sentence Extraction ...Question Answering System Supported Information ExtractionRule-Based Question Answering System Reading ...Learning Surface Text Patterns Question Answering SystemTowards Answering Opinion Questions: Separating Facts Opinions ...Offline Strategies Online Question Answering ...Scaling Web-Based Acquisition Entailment RelationsSemantic Approach Recognizing Textual EntailmentRecognising Textual Entailment Logical InferenceMeasuring Semantic Similarity TextsDistributional Inclusion Hypotheses Lexical EntailmentWeekly Supervised Natural Language Learning ...Accurate Information Extraction Research Papers ...Semantic Role Labelling Tree CRFsDiscriminative Word Alignment Conditional Random FieldsHybrid Markov/Semi-Markov CRF Sentence SegmentationYear199619971999200520052003200320042004200520002000200020002003200020022002200320032004200520052005200520032004200520062006# citations6650544071172114923651920192814131972392712791710292493320Table 1: Papers extracted 6 different NLP topics AAN: Dependency Parsing(DP), Phrase-based Machine Translation (PBMT), Text Summarization (Summ),Question Answering (QA), Textual Entailment (TE), Conditional RandomFields (CRF). set consists 5 highest cited papers AANs 2008 releasewhose title content matched corresponding topic phrase.172fiGenerating Extractive Summaries Scientific Paradigmsdescribe approach citation analysis, including calculation interjudge agreement. describe C-LexRank method extracting citation sentences.3.1 Citation Analysisanalyze citations, designed annotation task requires explicit definitionsdistinguish phrases represent different information units. Unfortunately, little consensus literature definitions. Therefore, following vanHalteren Teufel (2003), Qazvinian Radev (2011b) made following distinction. define nugget phrasal information unit (i.e., phrase wouldcontain information contributions cited paper). Different nuggetsmay represent atomic semantic unit, refer factoid.context citations, factoid refers unique contribution target paper mentionedcitation sentence. example, following set citations Eisners (1996) famous parsing paper illustrate set factoids paper suggest differentauthors cite particular paper may discuss different contributions (factoids)paper.context DPs, edge based factorization method proposed Eisner (1996).Eisner (1996) gave generative model cubic parsing algorithm based edgefactorization trees.Eisner (1996) proposed O(n3 ) parsing algorithm PDG.parse projective, Eisners (1996) bottom-up-span algorithm usedsearch.example also suggests different authors use different wordings (nuggets)represent factoids. instance, cubic parsing O(n3 ) parsing algorithmtwo nuggets represent factoid (Eisner, 1996). similar example,use throughout paper, paper Cohn Blunsom (2005) (identifiedACL ID W05-0622 Table 1). paper cited 9 different sentences within AAN.sentences listed Table 2. sentence, nuggets extractedannotators underlined. table suggests, citation sentence may discusscontributions cited paper. instance, last sentence containfactoids Cohn Blunsoms (2005) work. nuggets identified usingcitation paper (Cohn & Blunsom, 2005) account total number 3 factoids(contributions) identified paper: f1 , tree structures; f2 , semantic role labeling;f3 , pipelined approach.Following examples, asked two annotators background Natural Language Processing review citing sentence extract list phrases representcontribution cited paper.6 Moreover, ensure extracted nuggetsexplicitly mentioned citations, asked annotators rely merely setcitations task background topic source6. One annotators author paper.173fiQazvinian et Al.cited paper. Finally, reviewed list collapsed phrases representcontribution (factoid).Finding agreement annotated well-defined nuggets straightforwardcalculated terms Kappa. However, nuggets extractedannotators, task becomes less obvious. calculate agreement, annotated5 randomly selected citation sets twice (1 paper NLP areas Table 1),designed simple evaluation scheme based Kappa. n-gram, w, givencitation sentence, determine w part nugget either human annotations. woccurs neither, two annotators agree it, otherwise not.Based agreement setup, formalize statistic as:=Pr(a) Pr(e)1 Pr(e)(1)P r(a) relative observed agreement among annotators, P r(e) probability annotators agree chance annotator randomly assigning categories.Table 3 shows unigram, bigram, trigram-based two human annotators (Human1, Human2) five datasets annotated twice. resultssuggest human annotators reach substantial agreement trigram nuggetsexamined, reasonable agreement unigram bigram nuggets.3.2 C-LexRanksection describe C-LexRank method extract citing sentences coverdiverse set factoids. method works modeling set citations networksentences identifying communities sentences cover similar factoids.good division sentences made, extract salient sentences different communities.Figure 1 illustrates representative example depicts C-LexRanks process.3.2.1 Citation Summary Networkfirst step (as shown Figure 1 (a)), model set sentences cite specificpaper network vertices represent citing sentences undirected weightededges show degree semantic relatedness vertex pairs, normally quantifiedsimilarity measure. refer network Citation Summary Networkarticle. similarity function ideally assign high scores sentence pairsfactoids, assign low scores sentences talk differentcontributions target paper.Previously, Qazvinian Radev (2008) examined 7 different similarity measures including TF-IDF various IDF databases, longest common sub-sequence, generationprobability (Erkan, 2006), Levenstein distance training set citations.showed cosine similarity measure employs TF-IDF vectors assigns higher similarities pairs contain factoids. Following Qazvinian Radev (2008),use cosine similarity TF-IDF vector models employ general IDF corpus7construct citation summary network article.7. use IDF corpus Mead summarization system (Radev et al., 2004), generated usingEnglish portion Hong Kong News parallel corpus (Ma, 2000).174fiGenerating Extractive Summaries Scientific Paradigms1factoidf1Citation Sentenceparsing model based conditional random field model, however,unlike previous TreeCRF work, e.g., (Cohn & Blunsom, 2005; Jousse etal., 2006), assume particular tree structure, instead findlikely structure labeling.2f3researchers (Xue & Palmer, 2004; Koomen et al., 2005; Cohn & Blunsom, 2005; Punyakanok et al., 2008; Toutanova et al., 2005, 2008) usedpipelined approach attack task.3f2used tree labelling, XML tree labelling (Jousse et al.,2006) semantic role labelling tasks (Cohn & Blunsom, 2005).4f1Finally, probabilistic models also applied produce structured output, example, generative models (Thompson, Levy, & Manning,2003), sequence tagging classifiers (Marquez et al., 2005; Pradhan et al.,2005), Conditional Random Fields tree structures (Cohn & Blunsom2005).5f3SRL news, researchers used pipelined approach, i.e., dividing task several phases argument identification, argumentclassification, global inference, etc., conquering individually (Xue &Palmer, 2004; Koomen et al., 2005; Cohn & Blunsom, 2005; Punyakanoket al., 2008; Toutanova et al., 2005, 2008).6f1 , f2Although T-CRFs relatively new models, already appliedseveral NLP tasks, semantic role labeling, semantic annotation, wordsense disambiguation, image modeling (Cohn & Blunsom, 2005; Tang etal., 2006; Jun et al., 2009; Awasthi et al., 2007).7f2model used tasks like syntactic parsing (Finkel et al., 2008)semantic role labeling (Cohn & Blunsom, 2005).8f1Regarding novel learning paradigms applied previous shared tasks,find Relevant Vector Machine (RVM), kernel-based linear discriminant inside framework Sparse Bayesian Learning (Johansson & Nugues,2005) Tree Conditional Random Fields (T-CRF) (Cohn & Blunsom,2005), extend sequential CRF model tree structures.9N/Ause CRFs models tasks (Cohn & Blunsom, 2005).Table 2: AAN paper W05-0622 CRF Cohn & Blunsom (2005) cited ninedifferent AAN sentences. citation sentence, nuggets extractedannotators underlined.175fiQazvinian et Al.Averageunigram bigramHuman1 vs. Human2A00-10231.0000.615H05-10790.8890.667P05-10130.4270.825W03-03010.4550.636W05-06220.7780.667Average0.7100.682trigram0.9230.5560.9750.8180.7780.810Table 3: Agreement different annotators terms Kappa 5 citation sets.(a) Citation summary network(b) Community structure(c) C-LexRank outputFigure 1: C-LexRank method extracts citing sentences cover diverse set factoids. citation summary network (a) models set sentences citespecific paper, vertices represent citing sentences (weighted) edgesshow degree semantic relatedness vertex pairs. communitystructure (b) corresponds clustered sets representative sentences extractedcitation sentences. C-LexRank output (c) corresponds candidate sentences different clusters used building summary.3.2.2 Community Structuresecond step (as shown Figure 1 (b)), extract vertex communitiescitation summary network generate summaries. generate summaries extractingrepresentative sentences citation summary network. Intuitively, good summaryinclude sentences represent different contributions paper. Therefore, goodsentence selection citation summary network include vertices similarmany vertices similar other. hand,bad selection would include sentences representing small set verticesgraph. similar concept maximizing social influence socialnetworks (Kempe, Kleinberg, & Eva Tardos, 2003). Figure 2 shows exampleselected two vertices citation summary networks represent small subset vertices(left) larger subset vertices (right). work try select vertices176fiGenerating Extractive Summaries Scientific Paradigmsmaximize size set vertices represent. achieve detectingdifferent vertex communities citation summary network.(a) Bad sentence selection(b) Good sentence selectionFigure 2: Summaries produced using vertex coverage select set representativevertices corresponding sentences. Selecting two similar vertices causesummary cover fewer contributions target paper (a), selectingless similar vertices summary increase coverage summary(b).order find vertex communities thus good sentence selection, exploitsmall-world property citation summary networks. network called small-world,vertices neighbors other, reached one anothersmall number steps (Watts & Strogatz, 1998). Recent research shownwide range natural graphs biological networks (Ravasz, Somera, Mongru, Oltvai,& Barabasi, 2002), food webs (Montoya & Sole, 2002), brain neurons (Bassett & Bullmore,2006) human languages (Ferrer Cancho & Sole, 2001) exhibit small-world property.common characteristic detected using two basic statistical properties:clustering coefficient C, average shortest path length `. clustering coefficientgraph measures number closed triangles graph. describes likelytwo neighbors vertex connected (Newman, 2003). Watts Strogatz(1998) define clustering coefficient average local clustering valuesvertex.PnciC = i=1(2)nlocal clustering coefficient ci ith vertex number triangles connectedvertex divided total possible number triangles connected vertex i. WattsStrogatz (1998) show small-world networks highly clustered obtain relatively short paths (i.e., ` small). Previous work (Qazvinian & Radev, 2011a) showscitation summary networks highly clustered. networks small shortest pathsobtain clustering coefficient values significantly larger random networks.Moreover, Qazvinian Radev suggest community structure,177fiQazvinian et Al.{8} T-CRF{8} T-CRF{4} tree structures{3} semantic role labeling{4} tree structures{1} TreeCRF{9}{7} semantic role labeling{6} semantic role labeling; T-CRF{2} pipelined approach{6} semantic role labeling; T-CRF{7} semantic role labeling{3} semantic role labeling{9}{5} pipelined approach{1} TreeCRF{5} pipelined approach{2} pipelined approachPajek(a) Citation summary networkPajek(b) Latent community structure{8} T-CRF{8} T-CRF{4} tree structures{4} tree structures{1} TreeCRF{1} TreeCRF{6} semantic role labeling; T-CRF{6} semantic role labeling; T-CRF{7} semantic role labeling{3} semantic role labeling{7} semantic role labeling{3} semantic role labeling{9}{5} pipelined approach{2} pipelined approach{9}{5} pipelined approach{2} pipelined approachPajek(c) Clustering outputPajek(d) C-LexRank rankingFigure 3: C-LexRank algorithm operates follows Cohn Blunsoms (2005)citation summary network: network (a), vertices citation sentences(annotated nuggets Table 2), edge cosine similarity corresponding node pairs. (b) shows networkunderlying structure captured C-LexRank (c). Finally, (d) showsC-LexRank output vertex size proportional LexRank valuewithin cluster.community composed set highly connected vertices small numberedges fall communities.Figure 3 (a) illustrates real citation summary network built using citation sentencesTable 2 vertex labeled corresponding nugget. re178fiGenerating Extractive Summaries Scientific Paradigmsarrangement vertices Figure 3 (b), becomes clear citation summarynetwork paper underlying community structure sentences coversimilar factoids closer form communities. instance, networkleast 3 observable communities: one f1 : tree structure, onef2 : semantic role labeling last one f3 : pipelined approachproposed Cohn Blunsom (2005).order detect communities automatically use modularity. Modularity,(Newman, 2004a), measure evaluate divisions community detection algorithm generates. division g groups, define matrix egg whose componenteij fraction edges original network connect vertices components i, j.modularity Q defined as:XXQ=eiieij eki(3)ijkIntuitively, Q fraction edges embedded within communities minusexpected value quantity network degreesedges placed random regardless community structure. Newman Girvan(2004) Newman (2004b) showed across wide range simulated real-world networks larger Q values correlated better graph clusterings. also shownNewman (2004b) edges exist connect vertices across different clustersQ = 1, conversely number inter-cluster edges better randomQ = 0. work (Smyth & White, 2005) showed empirically modularity workswell practice terms (a) finding good clusterings vertices networkscommunity structure evident, (b) indicating appropriate number clustersk graph.C-LexRank uses clustering algorithm Clauset, Newman, Moore (2004),exploits modularity detect vertex communities network. network clusteringmethod, discussed Clauset et al. (2004) hierarchical agglomeration algorithm,works greedily optimizing modularity linear running time sparsegraphs. particularly, method continuously merges vertex cluster pairshighest similarity stops modularity reaches maximum value. clusteringalgorithm efficient (O(n log2 n) number nodes, n) require predetermined number clusters. two characteristics makes community detectionalgorithm particularly useful.Figure 3 (c) shows clustering algorithm detects factoid communities CohnBlunsoms (2005) citation summary network. figure, color-coded verticesbased community. clustering algorithm assigns sentences 1, 4 8 (whichtree structures) one cluster; sentences 3, 6 7 (whichsemantic role labeling) another cluster; finally assigns sentences 2, 5 9 (sentences2 5 pipelined approach) last cluster. figure also showssentence 6, discusses two factoids (semantic role labeling T-CRF) connectstwo vertex communities (corresponding 2 factoids) bridge.evaluate well clustering method works datasets, calculatedpurity normalized mutual information (NMI) divisionscitation set, extracted using community detection algorithm. Purity (Zhao & Karypis,179fiQazvinian et Al.2001) method cluster assigned class majority votecluster, accuracy assignment measured dividing numbercorrectly assigned documents N . formally:purity(, C) =1 Xmax |k cj |jN(4)k= {1 , 2 , . . . , K } set clusters C = {c1 , c2 , . . . , cJ } setclasses. k interpreted set documents cluster k cj setdocuments class cj .also calculate normalized mutual information (NMI). Manning, Raghavan,Schutze (2008) describe NMI follows. Let us assume = {1 , 2 , . . . , K } setclusters C = {c1 , c2 , . . . , cJ } set classes. Then,NMI(, C) =I(; C)[H() + H(C)]/2(5)I(; C) mutual information:I(, C) =XXk=P (k cj ) logjX X |k cj |kNjlogP (k cj )P (k )P (cj )N |k cj ||k ||cj |(6)(7)P (k ), P (cj ), P (k cj ) probabilities document clusterk , class cj , intersection k cj , respectively; H entropy:XH() =P (k ) log P (k )(8)k=X |k |kNlog|k |N(9)I(; C) Equation 6 measures amount information would loseclasses without cluster assignments. normalization factor ([H() + H(C)]/2)Equation 5 enables us trade quality clustering numberclusters, since entropy tends increase number clusters. example, H()reaches maximum document assigned separate cluster. NMInormalized, use compare cluster assignments different numbers clusters.Moreover, [H() + H(C)]/2 tight upper bound I(; C), making NMI obtain values0 1. Table 4 lists average Purity NMI across papers collecteddataset, along analogous numbers division size verticesrandomly assigned clusters.3.2.3 Rankingthird step C-LexRank process (as shown Figure 1 (c)) appliedgraph clustered communities formed. produce C-LexRank output,180fiGenerating Extractive Summaries Scientific Paradigmspurity(, C)purity(random , C)NMI(, C)NMI(random , C)average0.4610.3890.3120.18295% Confidence Interval[0.398, 0.524][0.334, 0.445][0.251, 0.373][0.143, 0.221]Table 4: average purity (in boldface) normalized mutual information (NMI) valuesshown papers collected dataset, along analogous valuesdivision size vertices randomly assigned clusters.extract sentences different clusters build summary. start largestcluster extract sentences using LexRank (Erkan & Radev, 2004) within cluster.words, cluster made lexical network sentences cluster(Ni ). Using LexRank find central sentences Ni salient sentencesinclude main summary. choose, cluster , salient sentence, reached summary length limit, secondsalient sentences cluster, on. cluster selection order decreasingsize. Figure 3 (d) shows Cohn Blunsoms (2005) citation summary network,vertex plotted size proportional LexRank value within cluster.figure shows C-LexRank emphasizes selecting diverse set sentences coveringdiverse set factoids.Previously, mentioned factoids higher weights appear greater numbersentences, clustering aims cluster fact-sharing sentences communities. Thus, starting largest community important ensure systemsummary first covers factoids frequently mentioned citationsentences thus important.last sentence example Table 2 follows. use CRFs modelstasks (Cohn & Blunsom, 2005). sentence shows citation maycover contributions target paper. sentences assigned communitydetection algorithm C-LexRank clusters semantically similar.intuition behind employing LexRank within cluster try avoid extractingsentences summary, since LexRank within cluster enforces extractingcentral sentence cluster. order verify this, also try variant C-LexRankselect sentences clusters based salience cluster,rather round-robin fashion, sentences within cluster equally likelyselected. call variant C-RR.Table 5 shows 100-word summary constructed using C-LexRank exemplarpaper, different nuggets illustrated bold. summary perfect summaryterms covering different factoids paper. includes citing sentencestalk tree CRF, pipelined approach, Semantic Role Labeling,indeed Cohn Blunsoms (2005) three main contributions.181fiQazvinian et Al.parsing model based conditional random field model, however, unlikeprevious TreeCRF work, e.g., (Cohn & Blunsom, 2005; Jousse et al., 2006),assume particular tree structure, instead find likely structurelabeling.researchers (Xue & Palmer, 2004; Koomen et al., 2005; Cohn & Blunsom,2005; Punyakanok et al., 2008; Toutanova et al., 2005, 2008) used pipelinedapproach attack task.model used tasks like syntactic parsing (Finkel et al., 2008)Semantic Role Labeling (Cohn & Blunsom, 2005).Table 5: 100-word summary constructed using C-LexRank Cohn Blunsoms (2005) citation summary network. Factoids shown bold face.4. Methodsexperiments Section 5 compare C-LexRank number summarizationsystems. compare C-LexRank random summarization find lower-boundpyramid scores experiments. use LexRank C-RR two variants CLexRank investigate usefulness community detection salient vertex selectionC-LexRank. evaluate DivRank state art graph-based summarization systemleverages diversity well MMR widely used diversity-based summarizationsystem. Finally, use Multiple Alternate Sentence Compression Summarizer (MASCS)system rely merely extraction, rather produces list candidatesapplying pre-defined sentence-compression rules.4.1 Randommethod simply chooses citations random order without replacement. Since citingsentence may cover information cited paper (as last sentence Table 2),randomization drawback selecting citations valuable informationthem. Moreover, random selection procedure prone produce redundantsummaries citing sentences discuss factoid may selected.4.2 LexRankOne systems compare C-LexRank LexRank (Erkan & Radev, 2004).works first building graph documents (di ) cluster. edgescorresponding vertices (di ) represent cosine similarity cosine valuethreshold (0.10 following Erkan & Radev, 2004). network built,system finds central sentences performing random walk graph.p(dj ) = (1 )X1+p(di )P (di dj )|D|di182(10)fiGenerating Extractive Summaries Scientific ParadigmsEquation 10 shows probability random walker would visit dj dependsrandom jump element well probability random walk visitsneighbors (di ) times transition probability di dj , P (di dj ).Comparing C-LexRank summaries ones LexRank gives insightbenefit detecting communities citation sets. Essentially, C-LexRankLexRank vertices assigned cluster. construction,C-LexRank produce diverse summaries covering different perspectives capturing communities sentences discuss factoids.4.3 MMRMaximal Marginal Relevance (MMR) proposed Carbonell Goldstein (1998)widely used algorithm generating summaries reflect diversity perspectivessource documents (Das & Martins, 2007). MMR uses pairwise cosine similaritymatrix greedily chooses sentences least similar alreadysummary. particular,hR = argminDi DA max Sim(Di , Dj )Dj(11)set documents summary, initialized = . Equation 11,sentence Di summary chosen highest similaritysummary sentences maxDj Sim(Di , Dj ) minimum among unselected sentences.4.4 DivRankalso compare C-LexRank state-of-the-art graph-based summarization systemleverages diversity, DivRank. DivRank based calculating stationary distributionvertices using modified random walk model. Unlike time-homogeneous randomwalks (e.g., PageRank), DivRank assume transition probabilities remainconstant time.DivRank uses vertex-reinforced random walk model rank graph vertices baseddiversity based centrality. basic assumption DivRank transition probability vertex reinforced number previous visits targetvertex (Mei et al., 2010). Particularly, let us assume pT (u, v) transition probabilityvertex u vertex v time . Then,pT (di , dj ) = (1 ).p (dj ) + .p0 (di , dj ).NT (dj )DT (di )(12)NT (dj ) number times walk visited dj time and,DT (di ) =Xp0 (di , dj )NT (dj )(13)dj VHere, p (dj ) prior distribution determines preference visiting vertexdj , p0 (u, v) transition probability u v prior reinforcement. Mei etal. argue random walk could stay current state time, therefore183fiQazvinian et Al.assumes hidden link vertex itself, thus defining p0 (u, v) as:(. w(u,v)u 6= vdeg(u)p0 (u, v) =1u = v(14)Here, try two variants algorithm: DivRank, p (dj ) uniform,DivRank priors p (dj ) l(Dj ) , l(Dj ) number wordsdocument Dj parameter (0.1 experiments). prior distributionassigns larger probabilities shorter sentences increase likelihoodsalient. cause sentences included summary, might increasefactoid coverage. experiments, follow Mei et al. (2010) set = 0.90= 0.25.4.5 MASCSlast summarization system use baseline Multiple Alternate SentenceCompression Summarizer (MASCS) (Zajic, Dorr, Lin, & Schwartz, 2007). Similar previous previous baseline systems, MASCSs goal leverage diversity summarization.MASCS performs preprocessing sentences transforms new sentences,thus expanding pool candidates available inclusion summary beyondset sentences occur source documents. makes MASCS somewhatnon-extractive. addition, preprocessing used MASCS experimentsspecifically adapted genre citation sentences scientific papers (Whidby, 2012).particularly, MASCS summarization system utilizes Trimmers (Zajicet al., 2007) sentence compression candidates create summaries single setdocuments. Summarization MASCS performed three stages. first stage,MASCS generates several compressed sentence candidates every sentence documentcluster. second stage involves calculating various ranking featurescompressed sentence candidates. final stage, sentence candidates choseninclusion summary, chosen based linear combination features.Trimmer leverage output constituency parser uses Penn Treebankconventions. present, Stanford Parser (Klein & Manning, 2003) used. setcompressions ranked according set features may include metadatasource sentences, details compression process generated compression,externally calculated features compression.Summaries constructed iteratively adding compressed sentences candidate pool length threshold met. Candidates chosen implementationMMR uses features directly calculated candidate, metadata candidatessource compression history, relevance candidate topic redundancycandidate already selected candidates. redundancy score MASCS usesindex words (w) document set:Xlog(.P (w|summary) + (1 ).P (w|corpus))wweighting factor (set 0.3 experiments).184(15)fiGenerating Extractive Summaries Scientific Paradigms5. Experimentsused 30 sets citations listed Table 1 employ C-LexRank produce twoextractive summaries different summary lengths (100 200 words) set.addition C-LexRank C-RR, also performed experimentsbaseline methods described Section 4, aimed leveraging diversitysummarization.5.1 Evaluationevaluate system, use pyramid evaluation method (Nenkova & Passonneau,2004). factoid citations paper corresponds summary content unit(SCU) (Nenkova & Passonneau, 2004).score given pyramid method summary ratio sumweights factoids sum weights optimal summary. score ranges0 1, high scores show summary content contains heavily weightedfactoids. factoid appears citation sentences another factoid,important, thus assigned higher weight. weight factoids buildpyramid, factoid falls tier. tier shows number sentences factoidappears in. Thus, number tiers pyramid equal citation summary size.factoid appears sentences, falls higher tier. So, factoid fi appears|fi | times citation summary assigned tier T|fi | .pyramid score formula use computed follows. Suppose pyramidn tiers, Ti , tier Tn top T1 bottom. weight factoidstier Ti (i.e. appeared sentences). |Ti | denotes number factoidstier Ti , Di number factoids summary appear Ti , totalfactoid weight summary follows.D=nXDi(16)i=1Additionally, optimal pyramid score summary X factoids,ax =nX|Ti | + j (Xi=j+1nX|Ti |)(17)i=j+1Pj = maxi ( nt=i |Tt | X). Subsequently, pyramid score summary calculated follows.P =(18)ax5.2 Results DiscussionTable 6 shows average pyramid score summaries generated using different methodsdifferent lengths. Longer summaries result higher pyramid scores since amountinformation cover greater shorter summaries. random sentenceextraction baseline, repeat experiment 100 different randomly generated seedvalues report average pyramid score summaries Table 6. table shows185fiQazvinian et Al.MethodRandomMMRLexRankDivRankDivRank (with priors)MASCSC-RRC-LexRankC.I.=Confidence IntervalLength: 100 wordspyramid95% C.I.0.535[0.526,0.645]0.600[0.501,0.699]0.604[0.511,0.696]0.644[0.580,0.709]0.632[0.545,0.719]0.571[0.477,0.665]0.513[0.436,0.591]0.647[0.565,0.730]Length: 200 wordspyramid95% C.I.0.748[0.740,0.755]0.761[0.685,0.838]0.784[0.725,0.844]0.769[0.704,0.834]0.778[0.716,0.841]0.772[0.705,0.840]0.755[0.678,0.832]0.799[0.732,0.866]Table 6: Average pyramid scores shown two different summary lengths (100 words200 words) eight different methods, including summary generator basedrandom citation sentence selection. C-LexRank outperforms methodsleverage diversity well random summaries LexRank. Highest scoresinput source shown bold.C-LexRank outperforms methods leverage diversity well randomsummaries LexRank. results table also suggest employing LexRankwithin cluster essential selection salient citing sentences, averagepyramid scores C-RR, sentences picked round-robin fashion, lower.5.2.1 Effect Community Detectioncommunity detection C-LexRank employs assigns highly similar citing sentencescluster. enables C-LexRank produce diverse summary selectingsentences different clusters. selection done assigning score vertexusing LexRank within cluster. modularity-based clustering method describedSection 3.2.2, works maximizing modularity clustering, always produceleast 2 clusters. Intuitively, network vertices assignedcommunity, fraction edges embedded within community equalexpected value quantity network edges placed random.make Q obtain lower-bound, Q = 0.However, hypothetical case vertices belong cluster, CLexRank LexRank (i.e., perform LexRank entire network).Therefore, comparing C-LexRank LexRank helps us understand effect clusteringsummary quality. Table 6 shows C-LexRank produces summaries obtainhigher pyramid scores 100 words 200 words. Table 7 shows 100-wordsummary constructed using LexRank Cohn Blunsoms (2005) citations.summary, unlike one produced C-LexRank (Table 5), coverfactoids target paper (e.g., pipelined approach). Moreover, summaryredundant information (e.g., TreeCRF vs. T-CRF) contains citation sentencecover contributions Cohn Blunsom.186fiGenerating Extractive Summaries Scientific Paradigmsmodel used tasks like syntactic parsing (Finkel et al., 2008)semantic role labeling (Cohn & Blunsom, 2005).use CRFs models tasks (Cohn & Blunsom, 2005).parsing model based conditional random field model, however, unlikeprevious TreeCRF work, e.g., (Cohn & Blunsom, 2005; Jousse et al., 2006),assume particular tree structure, instead find likely structurelabeling.Although T-CRFs relatively new models, already appliedseveral NLP tasks, semantic role labeling, semantic annotation, wordsense disambiguation, image modeling (Cohn & Blunsom, 2005; Tang et al., 2006;Jun et al., 2009; Awasthi et al., 2007).Table 7: summary constructed using LexRank Cohn Blunsoms (2005) citationsentences. Compared C-LexRank summary (in Table 5), LexRankproduce summary Cohn Blunsoms (2005) contributions (The summarytruncated clarity).5.2.2 Salient Vertex ExtractionSelecting representative sentences (vertices) different clusters done using LexRankC-LexRank algorithm. particularly, cluster, C-LexRank first extractssubgraph network representing vertices edges cluster, employsLexRank assign salience score vertex. alternative idea could selectingsentences clusters random (C-RR). C-RR, traverse clustersround-robin fashion randomly select previously unselected sentence clusterinclude summary.Comparing C-LexRank C-RR enables us understand effect salience selection within communities. Selecting vertices good representative clustermay result picking sentences cover contributions target paper (e.g.,sentence 9 Table 2 vertex 9 Figure 3 (d)). fact, Table 6 shows C-LexRankproduces summaries relatively 20% 5% higher pyramid scores C-RRextracting 100 200 word summaries respectively. Moreover, C-RR performs betterlonger summaries produced since extracts greater number sentencescluster increasing likelihood covering different factoids captured different clusters.6. Summaries Scientific Topicsprevious sections, described C-LexRank method identify communitiescitations discuss scientific contributions. showed C-LexRankeffective summarizing contributions single scientific papers. However, ultimategoal work investigate whether citations summary-amenable informationalso build end-to-end system receives query representing scientific topic(such dependency parsing) produces citation-based automatic summarygiven topic.187fiQazvinian et Al.section, extend experiments using tools explained previoussections automatic summarization scientific topics. evaluation experimentsextractive summary generation set papers research area QuestionAnswering (QA) another set papers Dependency Parsing (DP). two setspapers compiled selecting papers AAN words QuestionAnswering Dependency Parsing, respectively, title content.10 papers QA set 16 papers DP set. also compiled citationsentences 10 QA papers citation sentences 16 DP papers.6.1 Data Preparationgoal determine citations indeed useful information one wantput summary so, much information available original papers abstracts. evaluate automatically generatedsummaries using two separate approaches: nugget-based pyramid evaluation ROUGE.Recall-Oriented Understudy Gisting Evaluation (ROUGE) metric evaluatesautomatic summaries comparing set human written references (Lin,2004).Two sets gold standard data manually created QA DP citationsentences abstracts, respectively:8 (1) asked three annotators9 backgroundNatural Language Processing identify important nuggets information worth includingsummary. (2) asked four NLP researchers10 write 250-word summariesQA DP datasets. determined well different automatically generatedsummaries perform gold standards. citations contain redundant information respect abstracts original papers, summaries citationsperform better others.6.1.1 Nugget Annotationsfirst evaluation approach used nugget-based evaluation methodology (Voorhees,2003; Nenkova & Passonneau, 2004; Hildebrandt, Katz, & Lin, 2004; Lin & DemnerFushman, 2006). asked three annotators background Natural Language Processing review citation sentences and/or abstract sets papers QADP sets manually extract prioritized lists 28 factoids, main contributions,supplied paper. factoid assigned weight based frequencylisted annotators well priority assigned case.automatically generated summaries scored based number weightnuggets covered.particularly, annotators two distinct tasks QA set, oneDP set: (1) extract nuggets 10 QA papers, based citationspapers; (2) extract nuggets 10 QA papers, based abstractspapers; (3) extract nuggets 16 DP papers, basedcitations papers.8. Creating gold standard data complete papers fairly arduous, pursued.9. Two annotators authors paper.10. annotators authors paper.188fiGenerating Extractive Summaries Scientific ParadigmsHuman Performance: Pyramid scoreHuman1 Human2 Human3 Human4 AverageInput: QA citationsQACT nuggets0.5240.7110.4680.6950.599QAAB nuggets0.4950.6060.4230.6080.533Input: QA abstractsQACT nuggets0.5420.6750.5810.6690.617QAAB nuggets0.6460.8410.6730.7900.738Input: DP citationsDPCT nuggets0.2450.4750.3780.5550.413Table 8: Pyramid scores computed human-created summaries QA DP data.summaries evaluated using nuggets drawn QA citation sentences(QACT), QA abstracts (QAAB), DP citation sentences (DPCT).One annotator completed three tasks full remaining two annotatorsjointly completed tasks 1 3, providing us two complete annotations QADP citation sets one annotation QA abstract set. task, annotatorsconstructed lists 28 prioritized nuggets per paper. gave us 81 distinct nuggetsQA citation set, 45 nuggets QA abstract set, 144 nuggets DPcitation set. collapsing similar nuggets, able identify 34 factoids QAcitation set, 27 factoids QA abstract set, 57 factoids DP citation set.obtained weight factoid reversing priority 8 (e.g., factoid listedpriority 1 assigned weight 8, nugget listed priority 2 assignedweight 7, etc.) summing weights listing factoid.116.1.2 Expert Summariesaddition nugget annotations, asked four NLP researchers write 250-word summaries QA citation set, QA abstract set DP citation set.Table 8 gives pyramid scores 250-word summaries manually producedexperts. summaries evaluated using nuggets drawn QA citations, QAabstracts, DP citations. average scores (listed rightmost column)may considered good score aim automatic summarization methods.Additionally, Table 9 presents ROUGE scores (Lin, 2004) expert-written 250word summaries (e.g., Human1 versus others forth).average (last column) could considered ceiling performance automaticsummarization systems.11. Results obtained weighting schemes ignored priority ratings multiple mentionsnugget single annotator showed trends ones shown selected weightingscheme.189fiQazvinian et Al.Human Performance: ROUGE-2human1 human2 human3 human4 averageInput: QA citationsQACT refs.0.1810.1960.0760.2020.163QAAB refs.0.1120.1400.0710.1580.120Input: QA abstractsQACT refs.0.1310.1100.1220.1150.120QA-AB refs.0.2650.1980.1800.2540.224Input: DP citationsDPCT refs.0.1550.1260.1200.1650.142Table 9: ROUGE-2 scores obtained manually created summariesusing three reference. ROUGE-1 ROUGE-L followed similarpatterns.6.2 Automatic Extractive Summariesused four summarization systems summary-creation approach: C-LexRank, CRR, LexRank MASCS.automatically generated summaries QA DP three different typesdocuments: (1) full papers QA DP setsQA DP full papers (PA),(2) abstracts QA DP papersQA DP abstracts (AB), (3)citation sentences corresponding QA DP papersQA DP citations (CT).generated 24 (4 3 2) summaries, length 250 words, applying MASCS,LexRank, C-LexRank three data types (citation sentences, abstracts, fullpapers) QA DP. (Table 10 shows fragment one automaticallygenerated summaries QA citation sentences.) created six (3 2) additional 250word summaries randomly choosing sentences citations, abstracts, full papersQA DP. refer random summaries.work QA paraphrasing focused folding paraphrasing knowledgequestion analyzer answer locater (Rinaldi et al., 2003; Tomuro, 2003).addition, number researchers built systems take reading comprehensionexaminations designed evaluate childrens reading levels (Charniak et al., 2000;Hirschman et al., 1999; Ng et al., 2000; Riloff & Thelen, 2000; Wang et al., 2000).So-called definition questions recent TREC evaluations (Voorhees,2005) serve good examples.better facilitate user information needs, recent trends QA research shiftedtowards complex, context-based, interactive question answering (Voorhees,2001; Small et al., 2003; Harabagiu et al., 2005).Table 10: fragment one MASCS-generated summaries illustrated usingQA citation sentences input.190fiGenerating Extractive Summaries Scientific ParadigmsSystem Performance: Pyramid scoreRandom C-LexRank C-RR LexRank MASCSInput: QA citationsQACT nuggets0.3210.4340.2680.2950.616QAAB nuggets0.3050.3880.3490.3200.543Input: QA abstractsQACT nuggets0.4520.3830.4800.4410.404QAAB nuggets0.6230.4840.5740.6060.622Input: QA full papersQACT nuggets0.2390.4460.2990.1900.199QAAB nuggets0.2940.5200.3870.3010.290Input: DP citationsDPCT nuggets0.2190.2310.1700.3720.136Input: DP abstractsDPCT nuggets0.3210.3010.2630.3110.312Input: DP full papersDPCT nuggets0.0320.0000.144*0.280Table 11: Pyramid scores computed automatic summaries QA DP data.summaries evaluated using nuggets drawn QA citation sentences (QACT), QA abstracts (QAAB), DP citation sentences (DPCT). LexRankcomputationally intensive run DP-PA dataset, indicated* (about 4000 sentences). Highest scores input source shownbold.Table 11 gives pyramid score values summaries generated four automatic summarizers, evaluated using nuggets drawn QA citation sentences, QAabstracts, DP citation sentences. table also includes results baseline randomsummaries.used nuggets abstracts set evaluation, summaries createdabstracts scored higher corresponding summaries created citationspapers. Further, best summaries generated citations outscored best summariesgenerated papers. used nuggets citation sets evaluation, bestautomatic summaries generated citations outperform generated abstractsfull papers. pyramid results demonstrate citations contain usefulinformation available abstracts original papers, abstractscontain useful information available citations full papers.Among various automatic summarizers, MASCS performed best task,two cases exceeding average human performance. Note also random summarizer outscored automatic summarizers cases nuggets takensource different used generate summary. However, one two summarizersstill tended well. indicates difficulty extracting overlapping summaryamenable information across two sources.191fiQazvinian et Al.System Performance: ROUGE-2Random C-LexRank C-RR LexRank MASCSInput: QA citationsQACT refs.0.1160.1700.0950.1350.170QAAB refs.0.0830.1170.0760.0700.103Input: QA abstractsQACT refs.0.0450.0590.0610.0540.041QAAB refs.0.1210.1360.1220.2030.134Input: QA full papersQACT refs.0.0300.0360.0360.2820.040QAAB refs.0.0460.0590.0500.1050.075Input: DP citationsDPCT refs.0.1070.1320.0870.0490.101Input: DP abstractsDPCT refs.0.0700.0730.0530.2030.072Input: DP full papersDPCT refs.0.0380.0250.034*0.046Table 12: ROUGE-2 scores automatic summaries QA DP data. summariesevaluated using human references created QA citation sentences(QACT), QA abstracts (QAAB), DP citation sentences (DPCT).results obtained Jack-knifing human references valuescompared Table 4. LexRank computationally intensiverun DP full papers set, indicated * (about 4,000 sentences).Highest scores input source shown bold.192fiGenerating Extractive Summaries Scientific Paradigmsevaluated random summaries generated foursummarization systems references. Table 12 lists ROUGE scores summariesmanually created 250-word summary QA citation sentences, summaryQA abstracts, summary DP citation sentences, used gold standard.use manually created citation summaries reference, summariesgenerated citations obtained significantly better ROUGE scores summariesgenerated abstracts full papers (p < 0.05) [Result 1]. confirms crucial information, amenable creating summary present citation sentencesavailable, hard extract, abstracts papers alone. Further, summariesgenerated abstracts performed significantly better generated fullpapers (p < 0.05) [Result 2]. suggests abstracts citations generallydenser summary-amenable information full papers.use manually created abstract summaries reference, summariesgenerated abstracts obtained significantly better ROUGE scores summariesgenerated citations full papers (p < 0.05) [Result 3]. Further, importantly, summaries generated citations performed significantly bettergenerated full papers (p < 0.05) [Result 4]. Again, suggests abstractscitations richer summary-amenable information. results also showabstracts papers citations overlapping information (Result 2 Result 4), also significant amount unique summary-amenable information(Result 1 Result 3).Among automatic summarizers, C-LexRank LexRank perform best.unlike results found nugget-evaluation method, MASCS performedbest. suggests MASCS better identifying useful nuggets information,C-LexRank LexRank better producing unigrams bigrams expectedsummary. extent may due MASCSs compression preprocessing,breaks large, complex sentences smaller, finer-grained units content correspondbetter amount content nugget.7. Conclusionpaper, investigated usefulness directly summarizing citation sentences(set sentences cite paper) automatic creation technical summaries.proposed C-LexRank, graph-based summarization model generated summaries30 single scientific articles selected 6 different topics ACL Anthology Network(AAN). also generated summaries set Question Answering (QA) DependencyParsing (DP) papers, abstracts, citation sentences using four state-of-theart summarization systems (C-LexRank, C-RR, LexRank, MASCS). used twodifferent approaches, nugget-based pyramid ROUGE, evaluate summaries.results approaches four summarization systems show citationsentences abstracts unique summary-amenable information. results alsodemonstrate multidocument summarizationespecially technical summary creationbenefits considerably citations.next plan generate summaries using citation sentences abstracts togetherinput. Given overlapping content abstracts citation sentences, discovered193fiQazvinian et Al.current study, clear redundancy detection integral componentfuture work. Creating readily consumable technical summaries hard task, especiallyusing raw text simple summarization techniques. Therefore, intendcombine summarization bibliometric techniques suitable visualization methods towards creation iterative technical survey toolssystems present summariesbibliometric links visually convenient manner incorporate user feedbackproduce even better summaries.Current work generating topic summaries focused almost exclusively extractingdiverse factoid-rich summaries. Meanwhile, fluency produced summariesmostly ignored. future work, plan employ post-processing techniquesreference scope extraction sentence simplification, described Abu-JbaraRadev (2011), generate readable cohesive summaries.8. Acknowledgmentswould like thank Ahmed Hassan, Rahul Jha, Pradeep Muthukrishan, ArzucanOzgur annotations Melissa Egan preliminary developments. also gratefulBen Shneiderman, Judith Klavans, Jimmy Lin fruitful discussions anonymous reviewers insightful readings constructive guidance. following authors,Vahed Qazvinian, Dragomir R. Radev, Saif M. Mohammad, Bonnie Dorr, David Zajic,Michael Whidby supported, part, National Science FoundationGrant No. IIS-0705832 (iOPENER: Information Organization PENning ExpositionsResearch) awarded University Michigan University Maryland.opinions, findings, conclusions recommendations expressed materialauthors necessarily reflect views National Science Foundation.following authors, Michael Whidby Taesun Moon supported, part,Intelligence Advanced Research Projects Activity (IARPA) via Department Interior National Business Center (DoI/NBC) contract number D11PC20153. U.S. Governmentauthorized reproduce distribute reprints Governmental purposes withstanding copyright annotation thereon. Disclaimer: views conclusions containedherein authors interpreted necessarily representingofficial policies endorsements, either expressed implied, IARPA, DoI/NBC,U.S. Government.ReferencesAbu-Jbara, A., & Radev, D. (2011). Coherent citation-based summarization scientificpapers. Proceedings 49th Annual Conference Association Computational Linguistics (ACL-11), pp. 500509.Athar, A. (2011). Sentiment analysis citations using sentence structure-based features.Proceedings ACL 2011 Student Session, HLT-SS 11, pp. 8187.Athar, A., & Teufel, S. (2012). Context-enhanced citation sentiment detection. Proceedings 2012 Conference North American Chapter AssociationComputational Linguistics: Human Language Technologies, pp. 597601, Montreal,Canada. Association Computational Linguistics.194fiGenerating Extractive Summaries Scientific ParadigmsAwasthi, P., Gagrani, A., & Ravindran, B. (2007). Image modeling using tree structuredconditional random fields. Proceedings 20th international joint conferenceArtifical intelligence, IJCAI07, pp. 20602065.Bassett, D. S., & Bullmore, E. (2006). Small-world brain networks. neuroscientist,12 (6), 512523.Bird, S., Dale, R., Dorr, B. J., Gibson, B., Joseph, M., Kan, M.-Y., Lee, D., Powley, B.,Radev, D. R., & Tan, Y. F. (2008). acl anthology reference corpus: referencedataset bibliographic research computational linguistics. ProceedingsInternational Conference Language Resources Evaluation, LREC 2008, 26May - 1 June 2008, Marrakech, Morocco.Bradshaw, S. (2002). Reference Directed Indexing: Indexing Scientific LiteratureContext Use. Ph.D. thesis, Northwestern University.Bradshaw, S. (2003). Reference directed indexing: Redeeming relevance subject searchcitation indexes. Proceedings 7th European Conference ResearchAdvanced Technology Digital Libraries.Carbonell, J. G., & Goldstein, J. (1998). use MMR, diversity-based rerankingreordering documents producing summaries. Proceedings 21st AnnualInternational ACM SIGIR Conference Research Development InformationRetrieval (SIGIR-98), pp. 335336.Carletta, J. (1996). Assessing agreement classification tasks: kappa statistic. Computational Linguistics, 22 (2), 249254.Charniak, E., Altun, Y., Braz, R. d. S., Garrett, B., Kosmala, M., Moscovich, T., Pang,L., Pyo, C., Sun, Y., Wy, W., Yang, Z., Zeller, S., & Zorn, L. (2000). Readingcomprehension programs statistical-language-processing class. Proceedings2000 ANLP/NAACL Workshop Reading comprehension tests evaluation computer-based language understanding sytems - Volume 6, ANLP/NAACLReadingComp 00, pp. 15.Clauset, A., Newman, M. E. J., & Moore, C. (2004). Finding community structurelarge networks. Phys. Rev. E, 70 (6), 066111.Cohn, T., & Blunsom, P. (2005). Semantic role labelling tree conditional randomfields. Proceedings Ninth Conference Computational Natural LanguageLearning, pp. 169172. Association Computational Linguistics.Das, D., & Martins, A. (2007). survey automatic text summarization. LiteratureSurvey Language Statistics II course CMU, 4, 192195.Eisner, J. (1996). Three new probabilistic models dependency parsing: exploration.Proceedings 34th Annual Conference Association ComputationalLinguistics (ACL-96), pp. 340345. Association Computational Linguistics.Elkiss, A., Shen, S., Fader, A., Erkan, G., States, D., & Radev, D. R. (2008). Blind menelephants: citation summaries tell us research article?. JournalAmerican Society Information Science Technology, 59 (1), 5162.195fiQazvinian et Al.Erkan, G. (2006). Language model-based document clustering using random walks.Proceedings HLT-NAACL conference, pp. 479486, New York City, USA. Association Computational Linguistics.Erkan, G., & Radev, D. R. (2004). Lexrank: Graph-based centrality salience textsummarization. Journal Artificial Intelligence Research (JAIR).Ferrer Cancho, R., & Sole, R. V. (2001). small-world human language. ProceedingsRoyal Society London B, 268 (1482), 22612265.Finkel, J. R., Kleeman, A., & Manning, C. D. (2008). Efficient, feature-based, conditionalrandom field parsing. Proceedings ACL-08: HLT, pp. 959967, Columbus, Ohio.Association Computational Linguistics.Gildea, D., & Jurafsky, D. (2002). Automatic labeling semantic roles. Comput. Linguist.,28 (3), 245288.Harabagiu, S., Hickl, A., Lehmann, J., & Moldovan, D. (2005). Experiments interactivequestion-answering. Proceedings 43rd Annual Meeting AssociationComputational Linguistics, ACL 05, pp. 205214.Hildebrandt, W., Katz, B., & Lin, J. (2004). Overview TREC 2003 question-answeringtrack. Proceedings North American Chapter Association Computational Linguistics - Human Language Technologies (HLT-NAACL 04).Hirschman, L., Light, M., Breck, E., & Burger, J. D. (1999). Deep read: reading comprehension system. Proceedings 37th annual meeting AssociationComputational Linguistics Computational Linguistics, ACL 99, pp. 325332.Hoang, C. D. V., & Kan, M.-Y. (2010). Towards automated related work summarization.Proceedings 23nd International Conference Computational Linguistics(COLING-10), pp. 427435, Beijing, China. Coling 2010 Organizing Committee.Johansson, R., & Nugues, P. (2005). Sparse bayesian classification predicate arguments.Proceedings Ninth Conference Computational Natural Language Learning(CoNLL-2005), pp. 177180, Ann Arbor, Michigan. Association ComputationalLinguistics.Joseph, M. T., & Radev, D. R. (2007). Citation analysis, centrality, ACL Anthology. Tech. rep. CSE-TR-535-07, University Michigan. Department ElectricalEngineering Computer Science.Jousse, F., Gilleron, R., Tellier, I., & Tommasi, M. (2006). Conditional random fieldsxml trees. Workshop Mining Learning Graphs.Jun Hatori, Y. M., & Tsujii, J. contribution sense dependencies word sensedisambiguation. Journal Natural Language Processing.Jurafsky, D., & Martin, J. H. (2008). Speech Language Processing: IntroductionNatural Language Processing, Speech Recognition, Computational Linguistics(2nd edition). Prentice-Hall.Kan, M.-Y., Klavans, J. L., & McKeown, K. R. (2002). Using Annotated BibliographyResource Indicative Summarization. International ConferenceLanguage Resources Evaluation (LREC), Las Palmas, Spain.196fiGenerating Extractive Summaries Scientific ParadigmsKaplan, D., Iida, R., & Tokunaga, T. (2009). Automatic extraction citation contextsresearch paper summarization: coreference-chain based approach. Proceedings2009 Workshop Text Citation Analysis Scholarly Digital Libraries, pp.8895, Suntec City, Singapore.Kempe, D., Kleinberg, J., & Eva Tardos (2003). Maximizing spread influencesocial network. Proceedings 15th ACM SIGKDD international conferenceKnowledge Discovery Data Mining (KDD-09), pp. 137146. ACM.Klein, D., & Manning, C. D. (2003). Accurate unlexicalized parsing. Proceedings41st Annual Conference Association Computational Linguistics (ACL-03),pp. 423430.Koomen, P., Punyakanok, V., Roth, D., & Yih, W.-t. (2005). Generalized inferencemultiple semantic role labeling systems. Proceedings Ninth ConferenceComputational Natural Language Learning (CoNLL-2005), pp. 181184, Ann Arbor,Michigan. Association Computational Linguistics.Krippendorff, K. (1980). Content Analysis: Introduction Methodology. BeverlyHills: Sage Publications.Kupiec, J., Pedersen, J., & Chen, F. (1995). trainable document summarizer. Proceedings 18th Annual International ACM SIGIR Conference ResearchDevelopment Information Retrieval (SIGIR-95), pp. 6873.Lin, C.-Y. (2004). ROUGE: package automatic evaluation summaries. Proceedings ACL workshop Text Summarization Branches Out.Lin, J. J., & Demner-Fushman, D. (2006). Methods automatically evaluating answerscomplex questions. Information Retrieval, 9 (5), 565587.Ma, X. (2000). Hong kong news parallel text. Linguistic Data Consortium, Philadelphia.Manning, C. D., Raghavan, P., & Schutze, H. (2008). Introduction Information Retrieval.Cambridge University Press.Marquez, L., Comas, P., Gimenez, J., & Catala, N. (2005). Semantic role labelingsequential tagging. Proceedings Ninth Conference Computational NaturalLanguage Learning, CONLL 05, pp. 193196.Mei, Q., Guo, J., & Radev, D. (2010). Divrank: interplay prestige diversityinformation networks. Proceedings 16th ACM SIGKDD internationalconference Knowledge Discovery Data Mining (KDD-10), pp. 10091018.Mohammad, S., Dorr, B., Egan, M., Hassan, A., Muthukrishan, P., Qazvinian, V., Radev,D., & Zajic, D. (2009). Using citations generate surveys scientific paradigms.Proceedings North American Chapter Association Computational Linguistics - Human Language Technologies (HLT-NAACL 09), pp. 584592, Boulder,Colorado.Montoya, J. M., & Sole, R. V. (2002). Small world patterns food webs. Journaltheoretical biology, 214 (3), 405412.197fiQazvinian et Al.Nakov, P., D. A., & Hearst, M. (2012). peers see paper authors?.Advances Bioinformatics, special issue Literature Mining Solutions LifeScience Research.Nanba, H., Abekawa, T., Okumura, M., & Saito, S. (2004). Bilingual PRESRI: Integrationmultiple research paper databases. Proceedings RIAO 2004, pp. 195211,Avignon, France.Nanba, H., Kando, N., & Okumura, M. (2000). Classification research papers usingcitation links citation types: Towards automatic review article generation.Proceedings 11th SIG Classification Research Workshop, pp. 117134, Chicago,USA.Nanba, H., & Okumura, M. (1999). Towards multi-paper summarization using referenceinformation. Proceedings 16th International Joint Conference ArtificialIntelligence (IJCAI-99), pp. 926931.Nenkova, A., & Passonneau, R. (2004). Evaluating content selection summarization:pyramid method. Proceedings North American Chapter AssociationComputational Linguistics - Human Language Technologies (HLT-NAACL 04).Newman, M. E. J. (2001). structure scientific collaboration networks. PNAS, 98 (2),404409.Newman, M. E. J. (2003). structure function complex networks. SIAM Review,45 (2), 167256.Newman, M. E. J. (2004a). Analysis weighted networks. Physical Review E, 70056131.Newman, M. E. J. (2004b). Fast algorithm detecting community structure networks.Phys. Rev. E, 69, 066133.Newman, M. E. J., & Girvan, M. (2004). Finding evaluating community structurenetworks. Phys. Rev. E, 69, 026113.Ng, H. T., Teo, L. H., & Kwan, J. L. P. (2000). machine learning approach answering questions reading comprehension tests. Proceedings 2000 JointSIGDAT conference Empirical methods natural language processinglarge corpora: held conjunction 38th Annual Meeting AssociationComputational Linguistics - Volume 13, EMNLP 00, pp. 124132.Paul, M., Zhai, C., & Girju, R. (2010). Summarizing contrastive viewpoints opinionatedtext. Proceedings Conference Empirical Methods Natural LanguageProcessing (EMNLP-10), pp. 6676.Pradhan, S., Hacioglu, K., Ward, W., Martin, J. H., & Jurafsky, D. (2005). Semanticrole chunking combining complementary syntactic views. Proceedings NinthConference Computational Natural Language Learning, CONLL 05, pp. 217220.Punyakanok, V., Roth, D., & Yih, W. (2008). importance syntactic parsinginference semantic role labeling. Computational Linguistics, 34 (2).Qazvinian, V., & Radev, D. R. (2008). Scientific paper summarization using citation summary networks. Proceedings 22nd International Conference ComputationalLinguistics (COLING-08), Manchester, UK.198fiGenerating Extractive Summaries Scientific ParadigmsQazvinian, V., & Radev, D. R. (2010). Identifying non-explicit citing sentences citationbased summarization.. Proceedings 48th Annual Conference Association Computational Linguistics (ACL-10), pp. 555564, Uppsala, Sweden.Qazvinian, V., & Radev, D. R. (2011a). Exploiting phase transition latent networksclustering. Proceedings Association Advancement ArtificialIntelligence (AAAI-11).Qazvinian, V., & Radev, D. R. (2011b). Learning collective human behavior introduce diversity lexical choice. Proceedings 49th Annual ConferenceAssociation Computational Linguistics (ACL-11), pp. 10981108.Radev, D., Allison, T., Blair-Goldensohn, S., Blitzer, J., Celebi, A., Dimitrov, S., Drabek,E., Hakim, A., Lam, W., Liu, D., Otterbacher, J., Qi, H., Saggion, H., Teufel, S.,Topper, M., Winkel, A., & Zhang, Z. (2004). MEAD - platform multidocumentmultilingual text summarization. LREC 2004, Lisbon, Portugal.Radev, D. R., Muthukrishnan, P., & Qazvinian, V. (2009). ACL anthology networkcorpus. ACL workshop Natural Language Processing Information RetrievalDigital Libraries, Singapore.Ravasz, E., Somera, A., Mongru, D., Oltvai, Z., & Barabasi, A. (2002). Hierarchical organization modularity metabolic networks. Science, 297 (5586), 1551.Riloff, E., & Thelen, M. (2000). rule-based question answering system reading comprehension tests. Proceedings 2000 ANLP/NAACL Workshop Readingcomprehension tests evaluation computer-based language understanding sytems- Volume 6, ANLP/NAACL-ReadingComp 00, pp. 1319.Rinaldi, F., Dowdall, J., Kaljurand, K., Hess, M., & Molla, D. (2003). Exploiting paraphrases question answering system. Proceedings second internationalworkshop Paraphrasing - Volume 16, PARAPHRASE 03, pp. 2532.Siddharthan, A., & Teufel, S. (2007). Whose idea this, matter?attributing scientific work citations. Proceedings North American ChapterAssociation Computational Linguistics - Human Language Technologies(HLT-NAACL 07).Small, S., Liu, T., Shimizu, N., & Strzalkowski, T. (2003). Hitiqa: interactive questionanswering system: preliminary report. Proceedings ACL 2003 WorkshopMultilingual Summarization Question Answering.Smyth, S., & White, S. (2005). spectral clustering approach finding communitiesgraphs. Proceedings 5th SIAM International Conference Data Mining,pp. 7684.Sparck-Jones, K. (1999). Automatic summarizing: factors directions. Mani, I., &Maybury, M. T. (Eds.), Advances automatic text summarization, chap. 1, pp. 112. MIT Press.Tang, J., Hong, M., Li, J., & Liang, B. (2006). Tree-structured conditional random fieldssemantic annotation. Proceedings 5th International Semantic Web Conference (ISWC06), pp. 640653.199fiQazvinian et Al.Teufel, S. (2005). Argumentative Zoning Improved Citation Indexing. Computing Attitude Affect Text: Theory Applications, 159170.Teufel, S., & Moens, M. (2002). Summarizing scientific articles: experiments relevancerhetorical status. Computational Linguistics, 28 (4), 409445.Teufel, S., Siddharthan, A., & Tidhar, D. (2006). Automatic classification citation function. Proceedings Conference Empirical Methods Natural LanguageProcessing (EMNLP-06), pp. 103110, Sydney, Australia.Thompson, C., Levy, R., & Manning, C. (2003). generative model FrameNet semanticrole labeling. Proceedings Fourteenth European Conference MachineLearning ECML-2003, Croatia.Tomuro, N. (2003). Interrogative reformulation patterns acquisition question paraphrases. Proceedings second international workshop Paraphrasing - Volume 16, PARAPHRASE 03, pp. 3340.Toutanova, K., Haghighi, A., & Manning, C. (2005). Joint learning improves semantic rolelabeling. Proceedings 43rd Annual Meeting Association Computational Linguistics (ACL05), pp. 589596, Ann Arbor, Michigan. AssociationComputational Linguistics.Toutanova, K., Haghighi, A., & Manning, C. D. (2008). global joint model semanticrole labeling. Comput. Linguist., 34 (2), 161191.van Halteren, H., & Teufel, S. (2003). Examining consensus human summaries:initial experiments factoid analysis. Proceedings HLT-NAACL 03Text summarization workshop, pp. 5764, Morristown, NJ, USA.van Halteren, H., & Teufel, S. (2004). Evaluating information content factoid analysis: human annotation stability. Proceedings Conference EmpiricalMethods Natural Language Processing (EMNLP-04), Barcelona.Voorhees, E. M. (1998). Variations relevance judgments measurement retrievaleffectiveness. Proceedings 21st Annual International ACM SIGIR ConferenceResearch Development Information Retrieval (SIGIR-98), pp. 315323.Voorhees, E. M. (2001). Overview TREC 2001 question answering track. TextREtrieval Conference.Voorhees, E. M. (2003). Overview TREC 2003 question answering track. Proceedings Twelfth Text Retrieval Conference (TREC-2003).Voorhees, E. M. (2005). Using question series evaluate question answering system effectiveness. HLT/EMNLP 2005.Wang, W., J., A., Parasuraman, R., Zubarev, I., Brandyberry, D., & Harper, M. (2000).Question Answering System Developed Project Natural Language Processing Course. ANLP/NAACL Workshop Reading Comprehension TestsEvaluation ComputerBased Language Understanding Systems.Watts, D. J., & Strogatz, S. (1998). Collective dynamics small-world networks. Nature,393, 440442.200fiGenerating Extractive Summaries Scientific ParadigmsWhidby, M. A. (2012). Citation handling: Processing citation texts scientific documents.Masters thesis, University Maryland, Department Computer Science, CollegePark, MD.Xue, N., & Palmer, M. (2004). Calibrating features semantic role labeling. Lin, D., &Wu, D. (Eds.), Proceedings EMNLP 2004, pp. 8894, Barcelona, Spain. AssociationComputational Linguistics.Zajic, D. M., Dorr, B. J., Lin, J., & Schwartz, R. (2007). Multi-candidate reduction: Sentencecompression tool document summarization tasks. Information ProcessingManagement (Special Issue Summarization).Zhao, Y., & Karypis, G. (2001). Criterion functions document clustering: Experimentsanalysis. Technical report TR #0140, Department Computer Science, University Minnesota, Minneapolis, MN.Zhu, X., Goldberg, A., Van Gael, J., & Andrzejewski, D. (2007). Improving diversityranking using absorbing random walks. Proceedings North American ChapterAssociation Computational Linguistics - Human Language Technologies(HLT-NAACL 07), pp. 97104.201fiJournal Artificial Intelligence Research 46 (2013) 413-447Submitted 12/12; published 03/13Qualitative Order Magnitude Energy-Flow-Based FailureModes Effects AnalysisNeal Snookenns@aber.ac.ukDepartment Computer Science, Aberystwyth University,Penglais, Aberystwyth, Ceredigion, SY23 3DB, U.K.Mark Leemhl@aber.ac.ukDepartment Computer Science, Aberystwyth University.Abstractpaper presents structured power energy-flow-based qualitative modelling approach applicable variety system types including electrical fluid flow.modelling split two parts. Power flow global phenomenon therefore naturally represented analysed network comprised relevant structural elementscomponents system. power flow analysis platform higher-levelbehaviour prediction energy related aspects using local component behaviour modelscapture state-based representation global time. primary applicationFailure Modes Effects Analysis (FMEA) form exaggeration reasoning used,combined order magnitude representation derive worst case failure modes.novel aspects work order magnitude(OM) qualitative networkanalyser represent power domain topology, including multiple power sources,feature required earlier specialised electrical versions approach.Secondly, representation generalised energy related behaviour state-based localmodels presented modelling strategy vivid intuitive rangetopologically complex applications qualitative equation-based representations.two-level modelling strategy allows broad system behaviour coverage qualitativesimulation exploited FMEA task, limiting difficulties qualitativeambiguity explanation arise abstracted numerical models. usedmethod support automated FMEA system examples aircraft fuel systemdomestic heating system discussed paper.1. IntroductionQualitative representations (QR) reasoning number well documented advantages Failure Modes Effects Analysis. Three important analysisperformed early design life cycle, broad coverage system behaviour faults, results level abstraction readily mapssystem functional states.QR widely applied electrical systems (de Kleer, 1984; Mauss & Neumann,1996; Price, Snooke, & Lewis, 2006; Flores & Farley, 1999) used variety designanalysis applications including design concept analysis (gaining overview system behaviour), diagnosis, FMEA safety analysis, incremental multiple fault FMEA analysis(Price & Taylor, 1997), fault tree analysis (FTA) (Price, Wilson, Timmis, & Cain, 1996),sneak circuit analysis (Price, Snooke, & Landry, 1996; Savakoor, Bowles, & Bonnell,2013 AI Access Foundation. rights reserved.fiSnooke & Lee1993). qualitative information enabling technique allowing significant systemnominal failure states identified distinguished easily efficiently.qualitative results limitations, significant behavioural ambiguity alternative behaviours predicated simulation. often seenproblem qualitative techniques, ambiguity turned advantage, provided predict real physical behaviours since indicate key design parametersexist. One benefits modelling structure developed paper manyambiguities constrained therefore enhance design knowledge ratheranalysis limitation.typical approaches qualitative modelling reasoning use qualitative versionsnumerical equations derived component models. forms sound platform,burdensome extract relevant unimportant standard equations(e.g fluid flow), deal landmarks, integrals deviations etc. Given appealQR abstracted explanation ability provide broad analysis system state,propose specialised view applied wide range engineering domainsbased relationships generalised physical variables shown Figure 1.powernetworkine(en rtiaflux linkageergy)pressure momentummomentumresistamomentumEMF, voltage (electrical)pressure (hydraulic)force (mechanical)cetanaci y)cap nerg(en ceeffortflowdisplacementchargevolumedisplacementcurrentvolumetric flow ratevelocityFigure 1: Tetrahedron statepaper provides consistent framework several earlier implementations, develops supporting qualitative order magnitude (OM) capability (Section 2.1) throughoutmodels simulation. OM representation derived many valued resistancerepresentation (Lee, 2000b) although many valued concept modellingreasoning benefits completely applicable OM representation, actual techniquesolving circuits work based path labelling (Lee, 1999),general solution applicable network topology. present paper provides generalsolution extends technique purely electrical systems domains.necessary part effort energy-flow-based formalisation adopted, comprisingglobal instantaneous power network (Section 2) local component models includenotions energy time (Section 3).414fiQualitative Energy-Flow-Based FMEAPrevious work authors (Snooke, 2007) modelling fluid flow systemsbased electrical network analyser (Lee, 1999) could deal series parallelreducible circuits, becomes limitation topologically complex systems.incorporation equivalent resistance reduction using stardelta (Y-) transforms (Mauss &Neumann, 1996) OM representation (Section 2.2) allows effort flow variablesresistive networks topology solved reduction flow assignment expansion(Section 2.3), completes work started Lee. Complex network topologies duepart need represent concepts atmosphere fluid flow systems,allow failure modes leaks closed systems, well vented elementssystems. unique zero node common several domains generalisedpresent work Section 2.4.Applying techniques derived Lee (1999) non electrical systems reveals twolimitations. Firstly, analysis supports single effort source network,secondly concept substance flowing network. Section 2.5 extendsanalysis include multiple power sources applying principle superposition allowsystem decomposed multiple resistive networks. Section 2.6 considersrepresentation substances networks behaviour dependent upon substancecarrying effort flow. final part section 2 deals common specialcases simplify analysis maintain vividness representation avoidingunnecessary (Y-) transforms.Section 3 paper devoted local component concept captures temporalinformation thus models displacement momentum aspects Figure 1.OM modelling time subject section 3.1 underlies Finite State Machine(FSM) based modelling approach. representation improvement restricted predecessor used electrical component behaviour (Snooke, 1999),foundation variety automated electrical design analysis techniques. Sections 3.23.3 provide concrete examples component modelling system modelling respectively.Section 4 considers use OM representation perform exaggeration reasoningfailure analysis section 5 provides brief overview strategy (Price, 1998; Lee,Bell, & Coghill, 2001) used process multiple simulation results FMEA output.provide two case study examples sections 6 7. first case study illustratesbehaviour simulation fluid flow system interesting characteristics, secondoutlines use simulation results automatically produce completed FMEAreport industrial system supplied sponsors work.paper utilise two-level modelling strategy. lower level (centre sectionFigure 1) provides network-based global qualitative solver linear resistive networksusing graph-based methods determine instantaneous power (effort flow) network. higher level utilises results lower level network analysis decidelocalized component behaviour state-based models qualitative representationtime global parameter.two-level strategy gives significant benefits approaches modellingkinds electrical hydraulic systems dealt here. two-level modelling allowsseparation inherently global, instantaneous (single state) power variables,local energy-based behaviour components. separation particularly suitablequalitative representation qualitative behaviour fundamentally state-based;415fiSnooke & Leequalitative simulation produce sequence states power flows systemscomponents. Power flow global phenomenon cause effect undeterminedefficiently analysed network, avoiding problems causal instabilityambiguity arising systems equations generated interacting componentslocal behaviours (Skorstad, 1992a). Many QR approaches use local propagation methodconnecting behaviours components appeal generality, however,often require way including additional global information equivalent circuits (Sussman & Steele Jr, 1980), mechanisms propagating connectivity information(Struss, Malik, & Sachenbacher, 1995). approach allows users build reusable modelscomponents placed library used describing componentslinked together.Bond graphs share underlying energy-flow-based concept lower levelapproach, popular domain-independent graphical method describingsystem first described Paynter (1961) developed numerous others.excellent way generating systems differential equations describe systembehaviour. Qualitative versions bond graphs produced (Ghiaus, 1999), howeverqualitative versions standard numerical equations therefore sufferdifficulties general equation-based constraint methods. qualitativeversion bond graphs successfully used model energy flows associated peoplebuilding (Tsai & Gero, 2010), movement people determinedspecific physical laws thus choice provide ad-hoc model basedhigh-level knowledge building use. observed qualitative modelsoften result many fault candidates, reasoning scheme based past experience ad-hoc system dynamics difficult obtain proper modelsquantitative methods (Samantaray & Ould, 2011). widely-used qualitativeequation-based constraint method QSIM (Kuipers, 1986). Kuipers acknowledged problems QSIM many applications - chattering, uncontrolled branchingpossibilities, asymptotic behaviours (Fouche & Kuipers, 1990). techniquesdeveloped minimising problems (Clancy & Kuipers, 1997), problemsapproach remain practical systems. observed (Mosterman & Biswas, 2000),reducing model complexity eliminating higher-order derivatives non-linear effectsleads discontinuities system (i.e. state changes) careful analysis underlying physical nature system required constructing models order ensuresimplified models correspond real behaviour. kind careful analysispossible linking QSIM-style qualitative equation-based constraint methods physicalcomponents, certainly wanting able simulate faulty components.two-level approach achieved practical success even less sophisticated versions present approach (Price & Struss, 2004) avoiding direct translationdifferential equations qualitative versions, rather representing phenomena explicitly state-based representations local component behaviour level providingconstrained global representation. limit applicabilityproperty model discrete continuous rather inherent propertysystem (Struss, 2003), real question kind model appropriatereasoning task hand. modelling paper requires minimal algebraic effort,representing component behaviour abstract level qualitative behaviour416fiQualitative Energy-Flow-Based FMEAEffort, E000uuuResistance, R0r0rFlow, F?00f0Table 1: Qualitative electrical current assignmentpredicted nominal failure modes topologically complex systems manycomponent states.2. Qualitative Power Networkpower network (system circuit) represented using resistances, R, structuralcomponent. simulation task derive effort across resistance flowresistance given power (effort flow) source network. minimaluseful qualitative quantity space uses [0, r, ] represent resistance [0, u] representeffort, E , supply terminals (Lee, 1999). linear network uses generalised version Ohms Law effort, flow, resistance, E = F R, provides currentassignment Table 1. first row qualitatively ambiguous flowzero resistance produces effort loss. physically possible effort dropacross zero resistance hence flow fourth row shown indicateimpossible case.power network considered graph G(T, A) containing nodes edgesconnect exactly two nodes, represent circuit resistances. edge eresistance value R(e) connects pair nodes e = ht1 T, t2 i. Effort measuredtwo nodes E(t1 , t2 ) flow measured edge F (e). degreenode number connections node. Looped edges ends connectednode therefore increase degree node two.2.1 Order Magnitude Representationorder magnitude representation R allows detailed modelling without introducing qualitative ambiguity separating artefacts significantly different characteristics (Raiman, 1991; Lee, 2000a). enhancement improves ability representnominal behaviour distinguishing signal level power actuator power electrical circuits, gravitational head pressure system pump selected fluidtransfer systems. modelling faults also improved allowing exaggerated faultsmodelled, thus producing effects faults effects would otherwise qualitativelyindistinguishable nominal operation.define O(M ) qualitative resistance values R = [r1 = 0, r2 , . . . , ri1 , rm = ]ri+1 ri i. addition ri /ri+1 = rj /rj+1 i, j N. Physicallyinterpret mean number rm valued resistors series dominated417fiSnooke & Lee000r 3nr 3nb0r 3nr 3mab00?r 3(n+m)Table 2: OM Multiplicationsingle rm+1 valued resistor series segment, addition,magnitudes resistances qualitatively equal spacing. OM representationproposed benefit generate additional qualitative landmarkslead potential ambiguity model, indeed purpose allow qualitative distinctions characteristics known significantly different magnitude.qualitative OM approach somewhat analogous base 10 logarithmic scale usedmagnitude approximation numerical reasoning, two differences. qualitativeassumption hold generally mapping numerical specifications qualitative ones practical systems, coarser magnitude scale 10 required. Therefore,models distinguish values based numerical equivalent three order magnitude prefix ranges, A, mA, electrical current, work well. Secondly, mayinterpreted specific application type domain. example assumefluid system pressure system created gravitational head verticalpipe always dominated pump(s) working it, regardless numberpipe sections system, consider different qualitative magnitudes, eventhough numerically clearly case. case consistent set qualitativemagnitudes, models, interpretations required application domainseparates phenomena considered satisfy target systems.qualitative magnitude notation q3n introduced convenience lowermagnitudes indicate quantity n orders magnitude q. Similarly, convienience, q2n indicates quantity n orders magnitude q therefore q3n = q2(n) .OM calculations follow usual rules sign algebra (Trave-Massuyes, Ironi, & Dague,2004) allows domination one magnitude another shown Tables 2 3positive multiplication addition respectively. Table 1 u replacedu3n r replaced r 3m resulting f 3(nm) flow result row 5.2.2 Network ReductionOM representation utilised resistance, effort, flow variablespower network. aim derive flow effort values throughout entire circuit,achieved reducing circuit single equivalent resistance, assigning flowvalue expanding network assigning effort flow values level expansionaccording qualitative structures present. network reduction performedseries parallel (SP) circuit simplification OM version qualitative stardelta (Y-) transformation (Mauss & Neumann, 1996) non SP cases follows.418fiQualitative Energy-Flow-Based FMEA+000+p3n+p3n+p3m+p3m+p3min(m,n)pp3m3m??+p3n n <p3m < n? n =?p3np3n3npn <+p3m < n? n =???p3min(m,n)?????Table 3: OM additionWithin network graph G, nodes degree two removedconnected edges e1 , e2 replaced equivalent series edge resistance:R(e1 e2 ) = max(R(e1 ), R(e2 ))(1)edges e1 , e2 , . . . , en share pair nodes replaced singleequivalent parallel edge resistance:R(e1 ||e2 || . . . ||en ) = min(R(e1 ), R(e2 ), . . . , R(ei ))(2)facilitate vividness derived model use convention e1 ||e2 label edgesrepresenting parallel combination edges e1 e2 indicate series combination.Iterative application produce tree equivalent resistances result eithersingle resistance R0 supply nodes tfi non-SP reducible circuitfragment.majority circuits SP reducible, particularly consider zero resistanceedges removed network simplify topology (Section 2.7).remainder Y- transformation applied shown Figure 2. introductionY- resistances unfortunately reduces vividness representation sincedirectly related original component structure; however, provides generalsolution assign flow direction network, unlike earlier work (Lee, 2000a).qualitative signs version transformation utilised (Mauss & Neumann,1996), however, OM representation introduces possibility additional levelsresistance transformed node requires detailed analysis.Edges e1 . . . en connected non SP reducible degree n star node replaced newedges ejk form equivalent network 1 j n, 1 k n, k > j. UsingRR (e) notate numerical resistance edge e, 3 node Y- transformation defined(symmetrically edges) as:RR (e12 ) =RR (e1 )RR (e2 ) + RR (e1 )R(e3 ) + RR (e2 )RR (e3 )RR (e3 )(3)Equation 3 generalised star mesh transform defined star nodes order as:RR (ejk ) = RR (ej )RR (ek )nXm=14191/RR (em )(4)fiSnooke & LeeNoting OM qualitative values:1 11max, , ... =bmin(a, b, ...)(5)qualitative version equation 4 1 n is:R(ejk ) =R(ej )R(ek )min(R(em ))(6)denominator , resistances must R(ejk ) = .star resistances, R(em ) = 0, result undefined. Hence, efficiency, zeroresistance edges removed combining associated nodes super nodediscussed next subsection. numerator contains 0 , valueresult. cases result determined magnitude indices values.2c R(e ) = r 2a R(e ) = r 2b ,min(R(em )) = rmjkjj2(a+bc)rjk=e12br2aj rk(7)r2ce12e13e2e3e23Figure 2: -Y transformation 3, 4 5 nodes2.3 Flow Effort Assignmentreduced network flow (or effort flow sources) assigned sourcedirectly OM extended version Table 1. R = 0 case reported immediatelysince specific physical interpretation circumstances, electricalshort circuit. R = 0 contains another source (Section 2.5) presentlyconsideration, edge ignored. determine flow specific circuit component,edge hierarchy expanded required edge. sign flow determines420fiQualitative Energy-Flow-Based FMEAdirection relative arbitrary terminal order resistor, using convention t1 t2f t2 t1 -f.edges e1 , e2 part series pair flow simply flowpair: F (e1 ) = F (e2 ) = F (e1 e2 ) effort across edge E(e1 ) =F (e1 )R(e1 ) (noting f 3n r 3m = u3(m+n) ). R(e) = , F (e) = 0 E(e)undetermined effort equation (see rows 3 6 Table 1); however, unlessR(e1 ) = R(e2 ) = 0, E(e) = E(e1 e2 ).edges parallel, E(e1 ) = E(e2 ) = E(e1||e2 ) flow F (e) = E(e)/R(e).R = 0 physically impossible E 6= 0, unless short circuitsupply, treated special situation. R = F = 0 Table 1.Y- edges sum flows,Fek =k1XF (emk )m=1nXF (ekm )(8)m=k+1addition mixed signs leads possibility ambiguous flow (Table 3), wouldcaused balanced bridge configuration. indicates qualitative behaviour,possibly, future state system depends numerical values resistanceswithin one order magnitude, signalling analysis try obtain detailedinformation. level resistances, ambiguous flow value necessarily leadreasoning impasse higher-level tasks FMEA higher-level behaviourdependent value analysis tool reporting require it. Finally,E(eek ) = F (eek )R(eek ).Figure 3 exemplifies number concepts preceding subsections. notationfi used identify positive negative supply nodes, using subscript identifysource necessary systems multiple power sources. Working leftright Figure, sequence circuit reductions performed obtain final singleequivalent resistance value. overall flow value computed distributed amongstcircuit elements right left Figure described above. Finally flowscalculated flows using equation 8, noting positive flowdirections defined away star centre node.F (e1 ) = F (e14 ) F (e13 ) = f 31 f 34 = f 31F (e4 ) = F (e14 ) F (e43 ) = f 31 f 33 = f 31F (e3 ) = F (e13 ) F (e43 ) = f 34 + f 33 = f 332.4 Distinguished NodeEffort values measured two nodes convenience common practiceidentify one distinguished node network make measurements relativenode. allows effort measured node implicit assumptionsecond node distinguished node. commonly case electrical systemsdistinguished node called ground earth provides reference nodevoltage measurement often defined negative supply terminal singlesource system.421fiSnooke & Leeue0e0r<1r<1t1>1fr<1e1re13<1>4ft3e3r<3rr<1e5r>1fr<3r>2frr<1>1ft1e0 : e14|| ((e2 || e13): (e5 || e34))re5<2r>2f<2<1re14|| ((e2 || e13): (e5 || e34))e14<1 e14<3r<1>2fe34>2frt1t3t3r<1e2 || e13<1<1 e14>3fe4r<1e0t1e2>2f>3ft2rt1e2e0e0>2f<1>1fr<2>2fr<1>1f>1fe2|| e13: e5 || e34e5 || e340non SP-reduciblestar delta reductionparallel reductionseries reductionparallel reductionFigure 3: Operation qualitative circuit solver showing reduction flow assignmentgenerality define symbol Z distinguished zero node networktype specific instances, represent atmosphere fluid flow systems.Notice Z provide another landmark value qualitative effort space;provides structural reference point resistive network.identification Z allows definition absolute effort relative point.qualitative effort levels [0, u] supplemented voltage (Lee, 1999)two additional symbols represent structural features circuit emergesimulation. additions useful interpretation simulation,change quantity space itself. used indicate floating effort present circuitfragments disconnected source used indicate effortsupply terminals voltage, i.e. E(, ) = E(, fi) within magnitude.associated source less useful systems multiple active effort sources.always useful distinguish 0 ? generally undefined wouldprovide meaningful measurement, although may read 0 measured. qualitativevalue ? different ? qualitatively undetermined value withinscope modelled system, whereas value within modelled system.2.5 Multiple Effort Sourcesnetwork solver calculates power consumed (P = E F ) single source. linearresistive network use qualitative version principle superpositionone source connected single network (noting single system schematicmay, instant, actually comprise many isolated networks, possibly different domains,even though components schematic appear connected). network analysedseparately source inhibiting effort sources summing results.Psources s1 . . . sn , qualitative flow edge e given ssn1 F (e) =max(Fs1 (e) . . . Fsn (e)). Clearly two opposing flows magnitude exist,422fiQualitative Energy-Flow-Based FMEAedge suffers qualitative ambiguity. failure configurations, ambiguous resultuseful highlighting engineer range possible failure behaviour occur.Simple relational constraints may used resolve ambiguity common specialcase zero resistance one supply nodes s1 , fis1 ones2 , fis2 . example statement relative power two pumps allow directionflow reduced circuit derived, thereby allowing flows expanded networkresistances calculated.Figure 4 illustrates circuit schematic electrical system two effort sourcesresistances magnitude r31 r. individual flow contributions calculatedsource s1 shown coarse broken line, flows s2 fine broken lineannotated flow magnitudes. flow sum shown resistor.partial flow ambiguity circuit, shown double-headed arrows next resistorscentre section opposing flow contributions occur magnitude.state-based functional simulation requires flow (direction), power effortresistors, necessary know values resistors.s1-+f<0u>0f<1f<00zr>1-r>1f<1f<0u>0+rf<0s2f<0f<0f<0f<1f<0s2 flowf<1r>1s1 flowFigure 4: Multiple Source Example2.6 Representation Substance Within Systemdiscussion far considered nature flow. domains flow,electrical current, implicit component models. thermal domain Etemperature difference entropy flow F . product heat thermalpower, P , resistance component selected represent reciprocalthermal conductivity. Fluid transfer hydraulic systems include possibilityone substance associated network, particularly faults present.concrete example given fuel distribution system. supply tank becomesempty, air enter system behaviour pumps component maychange.substance-dependent behaviour represented component level. powernetwork provides instantaneous view effort flow, cannot participate propa423fiSnooke & Leegation flowing substance, is, however, necessary components able obtainknowledge substance interface components. network nodesconsidered zero volume points connection instantly propagate substancescomponent outflows inflows. local component behaviour provides substance information nodes appropriate qualitative timeframe according substancepresent inflows, capacitance behaviour. several connectionsnode flow direction may change simulation. reasons new conceptincluded circuit solver; list substances maintained node containsoutput substances resistance connected instant simulation.Figure 5 depicts node connected three resistances. flow directions likelycause e1 e2 modify associated SUBSTANCE lists t1. e3 requestssubstance t1 using S(t1), {S0, S1, S3} obtained.assignS(e1.t1) = {S0,S1}S(e2.t1) = {S3,S0}evaluateS(t1)result {S0, S1, S3}t1 SUBSTANCE liste1 {S0, S1}e2 {S3, S0}e1t1e2e3Figure 5: Node substance representationpresence one substance node result substance mixingvirtue substances present provided component uses nodeinput. might consider using flow magnitude information indicate ratiosubstance present, however, reality usually many factors involved,leading process diminishing returns modelling effort required.modelling substance work deliberately simple provides qualitativecapabilities match conventional bond graphs address systems free energy (mechanical, electrical, magnetic, incompressible fluid) elements conservativeexcept resistance, energy dissipated. Sophisticated techniques addedBond graph methodology (Brown, 2010), allow numerical models thermofluidic phenomena constructed; however, complexity required modelsbenefits broad-based qualitative modelling system engineering analysisFMEA lost. alternative approaches qualitative reasoning consideredchanges material state example plug-based ontology (Skorstad, 1992b)able model phase transitions steam boiler tube, however, modelling requirednecessarily detailed 13 state envisionment produced illustrates complexityphenomena. Another approach avoid complexity used Ghiaus providesqualitative model Carnot refrigeration cycle (Ghiaus, 1999) based equationsderived thermal bond graph thus include substance propertiessystem assumed equilibrium, precluding analysis many failure modes.Within proposed ontology, compressible fluids require representation statesubstance would require substance state parameterised models.424fiQualitative Energy-Flow-Based FMEAEnthalpy flow could potentially modelled providing multiple forms substance (including phase changes) related changes containing volumes pressures,providing effort thermal circuit. modelling enterprise, choice maderange phenomena worth modelling, based required analysis.Complex thermodynamic aspects one area considered automatedanalysis overall system engineering level worth modelling effortclear sufficient range faults supported models provide qualitatively distinct useful behaviour predictions. possibly area futureresearch may indeed provide fruitful results, make claims current time.2.7 Computational Enhancementsnetwork reduction flow assignment detailed previous sections sufficientsolve OM effort flow parameters network topology. are, however,several additional concepts (Lee, 1999; Lee et al., 2001) remain applicable usingSPS technique, provide additional vividness representation well computational benefit. enhancements deal common special cases avoidneed use Y- transformation, produces resistances associatedoriginal circuit components straightforward way.nodes connected 0 valued resistances aggregated single supernode,E(e) = 0 across edges subsumed supernode. edge e = hti , tj i,ti , tj , R(e) = 0 supernode tij created label ti .tj ,edge removed active graph. generated supernode participateSP Star (SPS) reduction, however, upon expansion circuit possibledirectly allocate flow edges represented subsumed edges doneSPS expansions.flow within supernode edges deduced unambiguous (non-ladder network)cases using qualitative version Kirchoffs current law. exceptionsource nodes, node connected edges flow F (e):XF (e) = 0{eA:e=ht,xi e=hx,ti}supernode edge flow values, f1 3m1 ... fn 3mn , flows F 0 = {fx 3mx : mx = max (m1 ...m2 )}dominate. node |F | 1 edges flowing node unassigned flow edgemust node flow equal flow magnitude F 0 towards node.dual exists flows direction.Two additional enhancements may used improve implementation performancenetwork analyser. Edges e = hti , ti i, ti loop removedactive graph since loop by-passed zero resistance path, hence assigned zeroflow upon expansion. Loops typically represent shorted parts electrical circuitexample.Degree one nodes, e {ht, i, h, ti}, , known dangling edges alsoremoved graph together connected edge occurresult reductions.425fiSnooke & Lee3. Local Component Models OM Timeresistive network calculates power consumption, P , cannot model componentstores energy, En, since En = P time . Displacement momentumtwo domain independent characteristics resulting inclusion time model(Figure 1). Energy, displacement momentum fundamentally local characteristicscomponents derived power variables time. representation time followsOM approach described Section 2.1. time t3n flow f 3n , d3(n+m) = f 3n t3ndefines displacement quantity substance.FSM representation local component behaviour proved sufficient abstractbehaviours failure analysis task. Time represented state changes explicitly represented state transitions capture qualitative integration effort flowvariables. state power network used trigger transitions, input eventsoutside system (external interactions). change state component maycause structure resistive parameters power network change, thus triggeringpower network simulation sequence events components. resultssystem interacting state machines, sharing time common variable sequencesevents.use specialised subset UML language state chart notation (OMG, 2012)describe FSM models. model comprised set states S, events ,transitions = S. addition s0 defines initial (default) statecomponent. Output actions may associated e also entry actionsassociated S. UML provides guard conditions events refineconditions produce e = (t, Tc , Dc , Fc , A). represents temporal conditiontransition qualitative OM duration transition occur Tcsatisfied, provided Fc true time. i.e. Tc triggers transition Fc allowscomplete (fire). Dc condition must satisfied transition,satisfaction Tc Fc . conditions may appeal values network model,may change resistance values network model. States used representqualitatively significant values variables derived integrationefforts flows time higher-level states components. example capacitorcharged/discharged, relay activated/deactivated. following sections describecomponent level simulation component model syntax.3.1 Component Level Simulationpresence multiple components system results set independent interactingFSMs. global variable level time, OM representation providessequencing component behaviour different timescales. definition OM appliedtime requires sequence (non-cyclic) events t3(n1) occur eventt3n .processing events carried maintaining time-ordered priority listcomponent events satisfied conditions. events ranked ordermagnitude time delay periods (referred time-slots subsequently) following way:events e Tc satisfied added end Q priority specifiedevent 0 unspecified.426fiQualitative Energy-Flow-Based FMEACandidate events fire n N N Q t(n) = t3x min(x)N 6= , i.e events lowest order non-empty timeslot.events n Fc (n) satisfied fired, events Fc (n) removedqueue.events Dc satisfied removed queue.|n| = 0 system reached steady state changesstate. |n| > 1 non-determinism system. important questionFMEA longer term impact alternative behaviours potential worst casefaults usually depends whether alternative behaviours diverge significantlydifferent functional (external) effects alternate paths internal statesconverge common state. often latter case. example two relays wiredparallel may switch exactly moment, resulting two possible behaviourpaths two intermediate states, neither significant cases.systems possible make concurrency assumption specifiesfinal state reached end time period t3x independent ordering eventstime period. presence race conditions feedback loops mutuallyinteracting components allow concurrency assumption, allowing systemreach qualitatively distinct state dependent detailed numerical timing events.case qualitative representation time lacks enough detail,qualitative ambiguity detected concurrency assumption falsely applied.systems certainly reasonable assume = 0 events concurrent,provided events associated power network = 0 rarelycausal cycles, unless specific examples bistable logic gate configurationscreated. Usually memory features would represented higher-level statevariables (e.g. electronic control unit), leaving domain-based modelling non cycliccausality.Generally, possible ordering events time-slot must considereddetermine branches (eventually) reach state and, case,generate number alternative behaviour paths. achieved breadth-firstsearch determine converging cyclic behaviours system reaches stateidentical previously encountered state. assumption madebehaviour; necessary simulation detect cyclic behaviour allow terminationappropriate reporting. Alternatively simulation must seek additional informationadvice engineer, if, example, converging state found within reasonablelength behaviour path.3.2 Component Modelling Examplesgraphical notation used describe component models, example Figure 6. Stateentry actions placed inside rectangle representing state, event syntax[if Tc [during]] event name [after [Fc ]][/A] used [x] represents optional elementx. keyword used specify Dc = Tc , also Fc = Tc Fcunspecified. omitted Dc = >. Fc omitted Fc = >. keyword427fiSnooke & Leeused specify t. specified, event = 0, i.e. immediateoccurrence.example assume following OM time values F = {mS, Sec, hour, day}flow levels = {low, normal, high}, Sec normal given OM index zero.tank given volume may defined event changes empty fullstate given time, e.g. F (tank inlet)==normal filling hour provides implicitvolume 21, one order magnitude bigger nominal volume, given chosenflow time qualitative space. explicitly including volume event conditionsmade represent number possible transitions different durations,example, F (tank inlet)>0 filling tank volume/F (tank inlet), tank volumedefined component qualitative volume value magnitude 21.flow condition prevents event requires infinite amount time fill tank (0effectively normal2 )Figure 6 shows two-part model tank. two levels model shownleft, graphic icons right used display simulation resultsschematic containing instance component. structure single zeroresistance tank dissipates energy filling. capacity tankdefined local component variable volume, used combination flow, F(tk),control change state, allowing several different capacity instances tankcreated parameterising model.Behaviour model - (energy)OM parameters:volumeemptytk.level = 0S(inlet.tk) =S(vent)Componentschematic graphicsTankF(tk) > 0S(inlet)=="fluid"during-F(tk) > 0emptying volume/F(tk)filling volume/F(tk)ventTankfulltk.level = MAXS(vent.tk) = S(inlet)inletStructure model - (power)inlettkR=0ventFigure 6: Basic 3 tank, energy storagetank Figure 6 store energy effects gravity ignoredhence increase potential energy tank fills. aspect couldincluded modelling making tank small magnitude effort sourcelevel non-zero. include capacitance well displacement (zero stored energyvolume) provided duration flow continues prior change state. Atmosphericpressure often significant qualitative value fluid flow systems behavioursystem may depend upon pressure difference atmospheric pressure428fiQualitative Energy-Flow-Based FMEApoint. defined fluid flow domain specialisation Z provides globalnode accessible component model represents connection atmosphere.Examples include vented tank leaking pipe.different component used illustrate component containing dependent effortsource. Figure 7 shows model (non-horizontal) pipe contains either liquidgas (air). resistance R(pipe) limits flow pipe represents combinedmeasure smoothness, length, Reynolds number etc. pipe model parameterised;ideal pipe produced length = 0. pipe blockage fault model could set R(pipe) = .filled pipe represents small magnitude effort source. pipe additionalresistance represent pipe wall facilitate fault modelling providing possibleconnection A. event conditions cause pipe fill rate inversely proportionalflow it. rightmost event specifies pipe prime itself, ie.fluid present top inlet (only), fill without externally imposed flow.Behaviour model - (energy)OM parameterslength; widthemptyS(upper.pipe) = "air"S(lower.pipe) = "air"E(gravity) = 0F(pipe) > 0"fluid" S(lower)lower_fillinglength*width/F(tk)R(pipe) = length/width-F(pipe) > 0"fluid" S(upper)upper_fillinglength*width/F(tk)F(pipe) == 0"fluid" S(upper)primelength*width/F(tk)upperleaklowerleakfullS(upper.pipe) = "fluid"S(lower.pipe) = "fluid">1E(gravity) = eupperStructure model - (power)pipegravity88lowerFigure 7: Partial model vertical pipeFigure 8 illustrates general ways component local behaviour may interactstructure several common component types. Real component modelsadditional relationships constraints associated qualitative values device states,dependent details behaviour required.3.3 System Modelling Circuit TopologyFigure 9 artificial system components similar described previoussection used illustrate topological aspects modelling simulation.addition valve closed (R = ) open (R = 0) states related externalposition input included, together simple pump may activated via activatedeactivate external events. electrical aspects system included429fiSnooke & LeeRelay (power controller)TransformerPump(Gyrator)outputstructurer0/coilbehaviourrelationship F(coil)rswitchprimaryR(switch)rsecondarymotorE(coil)E(secondary)maintain P(coil) = P(secondary)F(motor)pumpinputE(pump)Figure 8: Local/global relationships several types componentpump model would include electrical resistance events would triggeredlevel power (or current flow) electrical resistance, thus setting effortlevel pump appropriately.qualitative aspects pump easily included model createdifferent types pump. details design pump may allow flowinactive case here, however, easy include state-controlled resistanceset pump allow flow inactive. Similarly, additionalcondition events, pump made non-self priming fluid presentinput. bi-directional pump type requires additional state selection correctsubstance input.Tankc=0Tank CTank0c=1Tank C>10>0PipePipeValvePipePipe B>1ValvePipe B0/8PipePumpPPumpPipe CPipe C>1Tank BTank Bc=00Figure 9: Pumped SystemConsider simulation system Figure 9 state top Table4 pump off. table summarises changes state component430fiQualitative Energy-Flow-Based FMEATankfullTank BemptyTank CemptyPipeemptyprime, t32full, = u31Pipe BemptyPipe Cemptystart emptyingpart filled[emptying, t31 ]prime, t32fullprime, t32full, = u31start fillingpart filled[filling, t31 ]Chronological non-determinism time t31 : 1 Tank emptying; 2 Tank B fillingChoose > 1[emptying, t31 ]emptydrain, t32empty, = 0drain, t32drain, t32empty, = 0delete [filling, t31 ](S(inlet)==fluid)Choose > 2[filling, t31 ]fullFluid present[emptying, t31 ]continues 1Table 4: Simulation sequence system Figure 9horizontal lines points network simulation takes place. Pump ValvePipe D, change state maintaining states off, closed empty respectively.example resistance changes, changes effort sources. Initiallyvertical pipes prime fill sequence fluid propagated inflow nodes outflownodes, creating two pressure sources cause flow upper lower tankflow air vent lower tank upper tank.qualitative ambiguity results two events duration, raisingquestion could answered without knowing numerical sizes tanks(and quantity full empty start). case simulationallowed try alternative behaviours. first Tank becomes emptyTank B fills steady state reached Tank empty Tank B part filled.second possibility lower tank becomes filled upper one empties.causes fluid reach node, built model atmospherereports abnormal condition substance air present. Finallysteady state reached Tank empty Tank B full.431fiSnooke & Leepump switched valve opened, pump effort source causes flowtop tanks pump larger magnitude effort gravitypipes, however Tank C order size larger simulation essentially reverseprevious example. Tank overfilling spurious prediction simulationable reason fluid originally came Tank A.illustration flow ambiguity occurs Tank full valve openpump off. simulation starts Pipe B filled, Pipe becomesfilled bottom due effort Pipe A. Pipe becomes pressure sourceambiguous opposing effort Pipe A. PipeA PipeB supplynodes connected supernode allowing specification E(Pipe D.gravity) <E(Pipe A.gravity) establish flow direction; Pipe fill flow TankC, would occur Tank C lower Tank A. large never becomes full,subsequently drains PipeD.E(Pipe D.gravity) > E(Pipe A.gravity), Pipe drains (producing airvalve) whereupon cycle repeats. simulation concludes pipe oscillatingfull empty state state undetermined modelling resolution. may physical oscillation, however, usually indicates systemstate cannot represented level abstraction used.final possibility E(Pipe D.gravity) = E(Pipe A.gravity), (it filledheight Tank A) flow Pipe D. cases flow Tank Ceither full part filled steady state.last examples deliberately chosen limit representation,occur FMEA nominal operation, clear indicationdetailed numerical model required state behaviour; occurcomponent fault signal failure mode encounteredrequires detailed investigation.equation-based qualitative model system tanks (Dressler, Bottcher, Montag,& Brinkop, 1993) allows comparisons drawn approach. Firstly,authors note important feature also relevant approach; diagnosis (and alsoFMEA) complete behaviour description necessary. fact, model containingmuch detail likely produce many qualitative ambiguities (or branching behaviour)unimportant behaviour aspects. problems described detail (Clancy,Brajnik, & Kay, 1997) strategies avoid problem model revision proposed,various tools used support revision (QSIM) models. may eventuallyresult desired simulation result, strategies prove problematic FMEA duerange behaviour likely encountered automatic insertion componentfailure models system; need abstracted models nevertheless groundedbasic physics fully compositional.avoid difficulties simplified numerical model containing relevant behavioural phenomena, used (Dressler et al., 1993) subsequently refinedqualitative one. refinement process, variables values mappedonto qualitative values (0, +, ); however, values required treated differently various landmarks identified namely height liquid tanksatmospheric pressure. illustrates problem using general equations,since numerical equation provide landmarks represent states432fiQualitative Energy-Flow-Based FMEAvolume tank, global features A. suggest explicit state identification, temporal state changes, provides vivid qualitative model, withinbroadly applicable domain-independent generalised framework. components usedDressler, valve, require mappings input command flowvalve, using expression set valve flows (denoted i) input output follows:valve.status = :close valve.i1 = valve.i2 = 0. also note model open valvedefined valve.i1 = valve.i2 therefore propagates flow locally valveopen. requires predetermination cause effect power network, unlikeglobal network model. approach state description valve (relay Figure8 analogous electric valve) controls qualitative resistance vivid(and physically correct) controlling flow, since flow depends pressure,pressure cannot determined locally. OM model used, partly blocked valverepresented OM higher resistance normal. situation feasibleset flow value based valve control state, depends external system.node important circuit-based representation pumped systemexample, however general Z node may also lead power network ambiguity unlessfollowing condition satisfied. zero node must partition graph two disjointsets edges sharing Z supply nodes. topology naturally exists manypractical fluid flow circuits, connection negative (suction)positive (pressure) parts system pump atmosphere.others contrived example Figure 10, analysis inherently limited sinceimpossible determine qualitatively leak Pipe E would cause liquid egressair ingress. ambiguity indicated direction flow leak(represented dashed resistances right figure) either end pipedifferent respect atmosphere, shown arrows.example also illustrates two additional resistances used represent leak failuremode qualitative difference behaviour dependent upon leak position.resistance magnitude used indicate severity leak zero would producecomplete fracture. arrows leaks PipeC PipeD demonstrate unambiguous qualitative behaviour, however PipeE shows conflicting leak flows respect A.qualitative result exactly would expect absence numerical information, explicitly signals limit behaviour predictions possible limitedinformation system. Circuits resistances bridge Z node alsocannot effort values assigned relative Z.4. Faults Exaggeration ReasoningExaggeration reasoning (Weld, 1988b, 1990, 1988a) provides alternative qualitative technique explanation worst case effects without need differential qualitativecalculus. form reasoning therefore suitable detailed system equations, or, global power network, reasoning causalityperformed. secondary advantage purpose exaggeration reasoning oftenproduces concise explanations. drawback, however, qualitative deviationanalysis guaranteed sound, whereas exaggeration reasoning lead false predictions.433fiSnooke & LeemedP0PipePipePipe BPipe BPllPipe EPipe E0Pipe CPipe C?Pipel1 l2l0Tank0TankTank BlPipeTank BFigure 10: forming disjoint graphspurpose FMEA two reasons exaggerating faults reasonablestrategy: firstly, worst case effects required, results fault modelsextremes behaviour; secondly, FMEA intended aid engineertherefore externally verified.Using simple electrical torch example, corroded battery contact fault may raisecontact resistance torch may dim, however, resistance increasechange qualitative behaviour. exaggerated fault would OM increasebattery contact simulation predicts OM reduction light output. courseOM reduction light would likely visible, FMEA effectcorrosion leads reduction light reasonable provides significant distinctionfracture effect light circuit activity present.OM representation provides exaggerated forms faults model significantdifferences behaviour. example small leak fluid system pipe may allow airsucked in, causing mixture fluid air output; fracture positionmay result output pump fails operate air.common approach reasoning qualitatively use qualitative constraint-based deviation model propagate deviation system. Reasoningdeviation works well parameter value changes, less good structurestate changes occur, air ingress example new system stateoccurs due air pump. approach strengths, might useabsolute/exaggeration reasoning determine impact faults major statesoperating modes (or regions linear behaviour), followed deviations isolate finergrained effects expected direction value drift due fault.434fiQualitative Energy-Flow-Based FMEA5. Generating FMEAautomated FMEA typically based behaviour simulation many component faultsoperational scenarios. Producing FMEA previously described detailelsewhere (Price, 1998), however, since end goal modelling simulationeffort, summarise main steps follows:system simulated failed components expected operating conditions.system simulated component failure modes contained component type models every component instance, operating conditions.Multiple faults may also considered high failure rate component combinations.output simulation qualitative value system variablesstep (state) simulation. used completely separate functionalmodel identifies specific (output) behaviour state identified systemfunctions. Functions one four states (Achieved, Failed, UnexpectedBehaviour, Inoperative) based truth Boolean trigger effect expressionsevaluate simulation output variables. functional model lightweight,capture hierarchy system functions, including temporal aspects. Full detailsfunctional model previously presented (Bell, Snooke, & Price, 2007).nominal failure functional states compared, used indicate highlevel abstraction, highest risk failed system functions unexpected functioneffects associated component fault.presentation, selection, ranking function states risk factors computed information associated function states component reliabilityallows FMEA produced. structure typical automatically generatedFMEA shown Figure 17 second example, discussed Section 7.simulation step process interest paper followingsections provide two example systems illustrations.6. Case Study Example: Domestic Heating SystemFigure 12 shows schematic part simple domestic central heating system.complete model includes electrical microcontroller controls voltage pumpsactivators, addition thermal system, however focus fluid flow aspect.gas boiler three way valve retrofitted years initial installation.Either gas boiler wood burning stove supply hot water. radiatorsmodelled R = r, pipes ideal (length=0) boilers hot water cylinderR = r31 much larger diameter radiators. components alsoinclude thermal element flow considered entropy flow rate temperatureeffort. connection components compound connection includingfluid thermal circuits. space detail complete set models, however,Figure 13 shows horizontal pipe model includes fluid thermal aspects.435fiSnooke & Leefluid flow element resistive propagates fluid based flow direction.thermal element represented resistance. length pipe determinesthermal resistance pipe conduction flow,flow, thermal resistance heated substance transported. Thermalpower provided boiler (by combustion) create temperature difference (effort)inlet outlet. thermal effort applied across radiatorsprovide power (heat) surrounding air.aspect heating system interaction fluid flowtemperature networks. flow rate returning temperature fluidsubstantially higher ambient air, series connected radiators consume ratiothermal power based dimensions. low flow rates assumption mayhold. example, represent situation low fluid flow allowthermal power dissipate, additional resistance included thermal radiatorpipe models allows direct specification based low flow rate, outlettemperature close inlet temperature boiler (or atmosphere). Figure13 low flow resistance, controlled flow. Figure 11 shows effect lowfluid flow rate, series radiators. low flow elements changed r31 .power decreases OM source. first radiator hot (if effort sourcehigher flow lower), next one OM cooler, on.valve8partial blockage directly limitsfluid flow (not thermal flow)pipepipe/0f>108/0E=uGasBoilerf>2radiatorpipe0piper0low_flowf>2radiatorradiatorpiper0low_flowr>1rlow_flowr>1r>1pipe0Figure 11: Thermal flow circuit low fluid flow situationConsider small leak vpipe0 directly pump + output. Figure 12operation gas boiler valve heating position, fluid flowsradiators heat transferred radiators. small amount fluid enters atmosphere, causing small flow fluid header tank replaced waterexternal water supply.try holiday scenario simulation results summarised Table 5.external water supply isolated (in case freezing). flow model pump off,derived component models connections, Figure 14.ambiguity concerning gravity sources pipes p1, p7, p11(names abbreviated)oppose p0, p3, p6, p12, p13 flow magnitude. user provide436fiQualitative Energy-Flow-Based FMEASynchronmotorised3 way valvetank levelheating+water-Header tankvpipe16hpipe2vpipe1Air BleedValvehpipe4vpipe6loft levelvpipe12hpipe8hpipe9vpipe13-boilerupperPManualvalve+vpipe0boilerlowerVertical pipevpipe3+vpipe14vpipe11Gas Boilervpipe7Pvpipe15room levelRadiatorRadiatorHot WaterStorageWood BurningStoveFigure 12: Domestic heating systeminactive8R(thermal) =F(pipe) > 0"fluid" S(in)flow_normalactionsS(out) = S(out);;F(pipe) < 0"fluid" S(out)flow_reverseactionsS(in) = S(out);F(pipe) == 0"fluid" S(out)"fluid" S(in)no_flowactiveR(thermal) = 0;F(pipe) < f<2 actions R(low_flow) = F(pipe);pipeR(pipe) = length/widthleaklow_flowthermal8temperature_in8OM parameterslength; width8/0temperature_outFigure 13: Horizontal pipe thermal aspect437fiSnooke & Leerelationship E(vpipe3.gravity) E(vpipe11.gravity), = similarlytwo sets (p6=p7, p3=p11, p1=p0+p13+p12). result flow header tankleak via p16, p12 p13. Air enters p16 longer effort source(Table 5, row 3). addition secondary low flow exists via p1 p0, flowsopposition other, however constraint allows p1 source predominate.four sources (p0, p1, p12, p13) turns three flow patterns.Figure 15 shows flow system zero resistances dead branches removed,using different style arrow show contributing flow patterns. radiator sectionhigher resistance therefore main flow due effort p12 p13, drawingair boiler, p13, pump leak air reaches p12 effortbecomes zero. secondary flow pulls air toward radiators cold side pipe(p11) becomes empty, whereupon ambiguity effort p3 p0.specifying gravity sources p3>p1, flow direction reverses p0 p1 fillair, p11 fills fluid p3. Since pipes length diameterseveral possible behaviours; table provide additional information rathergenerate alternative behaviours would lead uncertainty fill statep0, p1, p2. p0, p1, p2, p12, p13 contain air. p3, p11, p6, p7 contain fluid opposingequal sources system stable.Returning home turning water causes header tank refill, however,pump produce flow self prime. p16 prime create lowflow header tank take t31 propagate pump. Hopefullywinter.subsequent step FMEA scenario wood burning stove pump startedvalve opened. large flow water pulled header tank three wayvalve wrong way, air water mix return pipe pumped past airbleed valve, air removed. gas boiler pump restartedheating system works correctly - small flow header tank leak.resulting FMEA report highlight effect failure heat outputradiator holiday scenario step severe faults (aburner fault example) permanent.7. Case Study Example: Aircraft Fuel Systemsection describes fuel system provided sponsor work. modellingsimulation described forms part wider objective automate manually created FMEAreports used generate fault effect relationships input Bayesian network baseddiagnostic system. effort is, turn, part recent 32m UK government sponsoredprogramme seeking research, develop validate necessary technologies useunmanned aerial systems (ASTRAEA, 2009).system considered fuel system twin engine light aircraft shown Figure 16also available physical laboratory simulation configurable rig allowedvalidation results using fault insertion via additional components valvesrepresent leaks. requirements system include thermal aspectsdue orientation changes system pressure created pumpsrequired, resulting little qualitative ambiguity, valves placed438fiQualitative Energy-Flow-Based FMEAuPump8r>10vpipe1_leakvpipe0u>1u>1vpipe10boiler3 way valvehpipe400/Air BleedwaterValver<10880/heating80hpipe20vpipe140u>1vpipe60vpipe3u>1Hot Waterr<1 Storageu>1RadiatorrrRadiatoru>1vpipe150u>1u>1vpipe7vpipe1100hpipe80hpipe100vpipe12 00r<1hpipe9Gas Boileru>1u>1vpipe1600vpipe13u>10uPumpHeader tankFigure 14: Flow model derived Figure 12 (joints omitted)abnormal configurations. created qualitative simulation model physicallaboratory model, resulting aircraft engines represented tanks example.system involves number tanks, valves pumps allow fuel storedtransferred around aircraft supply engines maintain aircraft trimflight.system comprised left right fuel tanks situated aircraft wings (e.g.OC WT LH) left right auxiliary tanks (e.g. TK LH). engines representedphysical test rig used convenience, tanks EH LH EH RH. wingtanks connected engines via pumps (e.g. CP FL LH) pressure sensors (e.g.439fiSnooke & Leevpipe1_leak>2f>2fr>1u>1>2fvpipe0>2fvpipe13>2fu>1u>1vpipe1>2fRadiatorr>2fPumpRadiatorr>2 f>2 >2ffr<1Gas Boiler>2>3 ffu>1>2fvpipe12HeadertankFigure 15: Simplified heating system leakPT FL LH) flow metres (e.g. FT FL LH), also modelled tank mimichardware test rig used, excess fuel returned tankdrawn. Control fuel distribution provided four three-position valves (e.g.TVL FL...), slaved pairs left right subsystems. basic operationsystem supply fuel wing tanks corresponding engine settingvalves normal position. valves also allow fuel suppliedleft tank right engine allowing engines fed one tank (crossoveroperation). possible feed engines opposite tanks desired, althoughnormal operating mode. addition fuel transferred wing tanksauxiliary tanks wing tanks, although possible return fuelauxiliary tanks. Failure modes provided component categories includingpipe tank leaks, pump failures stuck leaking valve failures every componentinstance.portion resulting FMEA output shown Figure 17. textual descriptions derived functional model (Bell et al., 2007; Price, 1998) provideeasily understood explanation fault effects risk priority. Functions also interpret exaggerated behaviours human-friendly phrases, example return linetank OM lower flow nominal outflow nominal, virtual relativelevel sensor value lower expected. course faults may provide absolutequalitative value tank level sensor example becomes empty (0) partfilled (l 30 ) expected. consistency fully automated FMEA analysis allowsautomated tasks performed diagnosability analysis (Snooke, 2009;Snooke & Price, 2012). qualitative analysis allows entire FMEA regeneratedfollowing system modification matter seconds differences systemeffects presented engineer incremental FMEA. allows unforeseenimplications design changes easily detected.first row Figure 17 describes effects blocked fuel return pipe near RHengine different operating modes system called steps output. main440fiQualitative Energy-Flow-Based FMEABoiler Radiator1Pipe0Pipe1Pipe3Pipe11Pipe12 Pipe13 Pipe16 vpipe1 leakPumpfullfullfullfullfullfullfullF =fF =fF = fF = fF =fF =fF = f F = f 22 F = f 22=u= u31 = u31 = u31 = u31 = u31 = u31 = u31 = u31Close supply header tank, switch boiler pump.Flow non-determinism: F = f E(Pipe3)E(Pipe11); F = f E(Pipe6) E(Pipe7);Flow non-determinism: F = f E(Pipe1) E(Pipe0), E(Pipe12), E(Pipe13);Resolve> Pipe3=Pipe11; Pipe6=Pipe7; Pipe1=Pipe0+Pipe13+Pipe12empty F = f 23 F = f 23 F = f 23 F = f 23 F = f 22 F = f 22 empty=0empty=0empty=0emptyF =0F =0F = f 23=0Flow non-determinism: F = f 23 E(Pipe3) E(Pipe0);Resolve> Pipe3>Pipe1;F = f 23 F = f 23 F = f 23 F = f 23F = f 23Event non-determinism: Pipe0empty Pipe11fullResolve> Pipe0;empty=0Event non-determinism: Pipe1empty Pipe11fullResolve> Pipe11;F = f 23full= u31fullemptyfullfullfullemptyemptyemptyTable 5: Extract simulation leak fault heating system 15functional effect RH engine supply malfunction (too much fuel), effectexcess fuel returned tank. also indication RH wing tanklevel might lower expected; course theoretical qualitative worst case.second row deals fracture pipe RH pump. functioneffect engine supply failes normal operating mode, additionallysee flow flow transducer, RH wing tank level higherexpected, although probably primary consideration, could usedindicate fuel could diverted remaining engine. fault different effectcross feed mode (fuel taken opposite wing tank), case LHtank level higher expected due potential lack returned fuel. PartRH valve fault shown see fuel returned wrong tank LHengine run.qualitative simulation also used generate sets symptoms relatequalitative measurements, symptoms failures (Snooke, 2009) order allow diagnosability analysis performed aim assisting sensor selection. structuresystem greatest influence tasks exploited purely structural441fiSnooke & Leeapproach related work (Rosich, 2012; Krysander & Frisk, 2008). complex modelsusing high order differential equations, dealing sensing individual componentsvalve example (Krysander & Frisk, 2008) structure approach provides tractability. system product wide sensor placement analysis early design investigationbenefits addition (abnormal) behaviour response multiple interactingcomponents presence fault, especially pertinent indirect sensingpossible. less detailed qualitative approach allows aspects used,maintaining tractability. comprehensive coverage system behaviour linkedfunctional states system allows automated fault isolation activitiessubject another paper one authors (Snooke & Price, 2012). qualitative analysis important tasks captures broad regions similar systembehaviour meaningful level abstraction.Multiple faults used simulation noting combinations faultmay produce qualitative ambiguity, representing critical tipping points system.example fuel system pump normally operates segments system partitionedvalves. Multiple valve faults result pumps working oppositioncomplex pipe topology almost certain numerical information requireddetermine actual flows non-linearity abstracted qualitativecomponent states. FMEA resulting answer predicting several possible behavioursreasonable, since highest risk effects significant enough, highlightedengineer detailed analysis.Single fault FMEA norm effort involved multiple fault effectdetermination. automated FMEA combinations faults feasible, althoughselection fault combinations still usually necessary alleviate computationalcomplexity O(N ) associated exploring concurrent faults previouslydiscussed (Price & Taylor, 1997).8. ConclusionsQualitative simulation powerful modelling concept support wide rangereasoning tasks. range electrical circuit design analysis tools basedapproach authors electrical qualitative simulator known mcirq regularindustrial use.structural behavioural models compositional encode systemfunctional information make assumptions use. course necessarydecide range phenomena included modelling, librarymodels typically created specific application area (e.g. automotive electrical, aircraftfuel system, general plumbing etc), include set qualitative variablesinterest, (suitably labelled) magnitudes relevant component failure modes.models reusable components available systems within applicationarea. qualitative nature components makes far less complex numericalequivalents also makes reusable within application area possiblyalso application areas. models provide major behaviours relevantobjective high-level reasoning potential effects, detailed analysissystem performance, demonstrated examples.442fiQualitative Energy-Flow-Based FMEAFL2-TRFL-TR-LHFL1-TRFL6-TRFL5-TRTJ-TR-LHTVL-TR-LHFL7-TRFL3-TRCP-TRTK-AT-LHCP-AT-LHTJ-TR-RHTVL-TR-RHFL4-TRTP1-AT-LHTK-AT-RHTP2-AT-LHTP2-AT-RHDV-AT-LHDV-AT-RHTP1-AT-RHCP-AT-RHTP3-AT-LHTP3-AT-RHRP2-4-RL-LHOC-WT-LHRP1-RL-LHTP1-WT-LHTP2-WT-LHDV-WT-LHF-WT-LHFL-TR-RHCK-TRTJ-AT-LHRP2-4-RL-RHFC-RL-RHRP2-3-RL-LHTP2-WT-RHRP2-3-RL-RHF-WT-RHTJ-RL-LHRL-FS-LHRL-FS-RHRL2-2-RL-LHTP1-WT-RHDV-WT-RHTJ-RL-RHRP2-2-RL-RHTVL-RL-LHFL1-1-FS-LHTVL-RL-RHFL1-1-FS-RHFL1-2-FS-LHFL1-2-FS-RHFL1-3-FS-LHFL1-3-FS-RHTVL-FL-LHTJ-FL-FS-RHTVL-FL-RHTP1-FL-LHTP1-FL-RHF-FL-LHF-FL-RHTP2-FL-LHTP2-FL-RHTP4-FL-LHTP4-FL-RHCP-FL-LHCP-FL-RHRP2-1-RL-LHTP5-FL-LHFT-FL-LHTP5-FL-RHRP2-1-RL-RHFT-FL-RHTP6-FL-LHTP6-FL-RHPT-FL-LHPT-FL-RHTP7-FL-LHTP7-FL-RHIV_FL_LHTJ-FL-LHTP3-FL-LHTP8-FL-LHIV_FL_RHTP3-FL-RHTJ-FL-FS-LHOC-WT-RHRP1-RL-RHTJ-AT-RHFC-RL-LHTP8-FL-RHTJ-FL-RHNameBravoFuelSyst em.vdxRev4Hist ory nst - original version 7/ 1/ 2008nst - name changes 11/ 1/ 2008nst - changed engines 25/ 1/ 2008nst - changed engine ret urn. 8/ 4/ 2008EH_RHEH_LHBravoADTFschematicGenericfuelsystemFigure 16: Example fuel system443fiSnooke & LeeFigure 17: Portion FMEA output fuel systempaper makes two notable contributions. Firstly, provides improved circuitreasoning algorithm gives complete solution possible circuit topologies.achieved solving problem non series/parallel reducible circuits. Also restrictionsingle sources removed resultant simulator called (m2 cirq), Multiplesource mcirq.Secondly, qualitative network modelling method placed context modelling ontology based separating global local behaviour based power flow.component models fluid systems involve aspects electrical circuitspaper introduces several additional fundamental concepts necessary global levelfluid flow modelling, including distinguished zero node propagation substancesnetwork. techniques illustrated modelling range common fluidflow components simulation.ability QR make predictions across multiple system states operatingmodes complementary techniques. example fault tree analysis diagnosissimilar fuel system example presented Section 7 performed (Hurdle,Bartletta, & Andrews, 2009) uses pattern recognition deal multiple states.production fault trees scenario (state) identification labour intensive manuallyperformed process described exactly qualitative terms produced QRbehaviour prediction high flow. therefore likely accuracy coverageFTA might improved reduction effort using QR behaviour predictionsinstead manual effort.444fiQualitative Energy-Flow-Based FMEAMany software tools developed perform variety design analysiselectrical systems, mentioned introduction paper. substitutingenhanced simulator tools analyses performed typescomplex topology systems multiple domain systems qualitative behavioursused answer failure mode questions.8.1 Acknowledgmentswork supported Aberystwyth University, Welsh Assembly Government,BAE Systems DTI ASTRAEA Programme. also thank anonymous reviewershelpful comments.ReferencesASTRAEA (2009). http://www.projectastraea.co.uk/. ASTRAEA project homepage. Accessed 15 February 2013.Bell, J., Snooke, N. A., & Price, C. J. (2007). language functional interpretationmodel based simulation. Advanced Engineering Informatics, 21 (4), 398409.Brown, F. T. (2010). Bond-graph based simulation thermodynamic models. JournalDynamic Systems, Measurement, Control, 132 (6), 064501.Clancy, D. J., Brajnik, G., & Kay, H. (1997). Model revision: Techniques toolsanalyzing simulation results revising qualitative models. 11th InternationalWorkshop Qualitative Reasoning, Cortona, Siena Italy.Clancy, D. J., & Kuipers, B. (1997). Dynamic chatter abstraction: scalable techniqueavoiding irrelevant distinctions qualitative simulation. 11th InternationalWorkshop Qualitative Reasoning Physical Systems (QR 97), pp. 6776.de Kleer, J. (1984). circuits work. Artificial Intelligence, 24, 205280.Dressler, O., Bottcher, C., Montag, M., & Brinkop, A. (1993). Qualitative quantitative models model-based diagnosis system ballast tank systems. International Conference Fault Diagnosis (TOOLDIAG), Toulouse. Extended reportversion available http://mqm.in.tum.de/ dressler/arm-1-93.ps.Flores, J. J., & Farley, A. M. (1999). Reasoning linear circuits: model-basedapproach. Artificial Intelligence Communications, 12 (1-2), 6177.Fouche, P., & Kuipers, B. (1990). assessment current qualitative simulation techniques. 4th International Workshop Qualitative Reasoning Physical Systems (QR-90), pp. 195209.Ghiaus, C. (1999). Fault diagnosis air conditioning systems based qualitative bondgraph. Energy Buildings, 30 (3), 221 232.Hurdle, E., Bartletta, L., & Andrews, J. (2009). Fault diagnostics dynamic systemoperation using fault tree based method. Reliability Engineering System Safety,94 (9), 13711380.445fiSnooke & LeeKrysander, M., & Frisk, E. (2008). Sensor placement fault diagnosis. Systems, ManCybernetics, Part A: Systems Humans, IEEE Transactions on, 38 (6), 13981410.Kuipers, B. J. (1986). Qualitative simulation. Artificial Intelligence, 29, 289338.Lee, M. H. (1999). Qualitative circuit models failure analysis reasoning. Artificial Intellligence, 111, 239276.Lee, M. H. (2000a). Many-valued logic qualitative modelling electrical circuits.Proceedings 14th International Workshop Qualitative Reasoning, (QR-2000).Lee, M. H. (2000b). Qualitative modelling linear networks engineering applications.Proceedings 14th European Conference Artificial Intelligence ECAI 2000, pp.161165, Berlin.Lee, M. H., Bell, J., & Coghill, G. M. (2001). Ambiguities deviations qualitative circuit analysis. Proceedings 15th International Workshop Qualitative Reasoning,QR 01, pp. 5158.Mauss, J., & Neumann, B. (1996). Qualitative reasoning electrical circuits usingseries-parallel-star trees. Proceedings 10th International Workshop QualitativeReasoning, QR-96, pp. 147153.Mosterman, P. J., & Biswas, G. (2000). comprehensive methodology building hybridmodels physical systems. Artificial Intelligence, 121, 171 209.OMG (2012). Documents Associated Unified Modeling Language (UML) Version 2.5.Object Mangement Group, http://www.omg.org/spec/UML/2.5/Beta1/PDF.Paynter, H. M. (1961). Analysis Design Engineering Systems. MIT Press.Price, C. J. (1998). Function-directed electrical design analysis. Artificial IntelligenceEngineering, 12 (4), 445456.Price, C. J., Snooke, N. A., & Landry, J. (1996). Automated sneak identification. Engineering Applications Artificial Intelligence, 9 (4), 423427.Price, C. J., Snooke, N. A., & Lewis, S. D. (2006). layered approach automatedelectrical safety analysis automotive environments. Computers Industry, 57 (5),451461.Price, C. J., & Taylor, N. S. (1997). Multiple fault diagnosis FMEA. Proc.Ninth Conference Innovative Applications Artificial Intelligence (IAAI 97), pp.10521057. AAAI.Price, C. J., & Struss, P. (2004). Model-based systems automotive industry. AIMagazine, 24 (4), 1734.Price, C. J., Wilson, M. S., Timmis, J., & Cain, C. (1996). Generating fault treesFMEA. 7th International Workshop Principles Diagnosis, pp. 183190, ValMorin, Canada.Raiman, O. (1991). Order magnitude reasoning. Artificial Intelligence, 51, 1138.446fiQualitative Energy-Flow-Based FMEARosich, A. (2012). Sensor placement fault detection isolation based structuralmodels. 8th IFAC Symposium Fault Detection, Supervision Safety Technical Processes (SAFEPROCESS), pp. 391396. IFAC.Samantaray, & Ould (2011). Bond Graph Modelling Engineering Systems, chap. BondGraph Model-Based Fault Diagnosis. Springer. ISBN 978-1-4419-9368-7.Savakoor, D. S., Bowles, J. B., & Bonnell, R. D. (1993). Combining sneak circuit analysis failure modes effects analysis. Proceedings Annual ReliabilityMaintainability Symposium, pp. 199205.Skorstad, G. (1992a). Finding stable causal interpretations equations. Faltings, &Struss (Eds.), Recent advances qualitative physics, pp. 399413. MIT Press.Skorstad, G. (1992b). Towards qualitative lagrangian theory fluid flow. Proceedingstenth national conference Artificial intelligence, AAAI92, pp. 691696. AAAIPress.Snooke, N. A. (1999). Simulating electrical devices complex behaviour. AI Communications, 12 (1,2), 4558.Snooke, N. A. (2007). M2 CIRQ: Qualitative fluid flow modelling aerospace fmea applications. Proceedings 21st international workshop qualitative reasoning, pp.161169.Snooke, N. A. (2009).automated failure modes effects analysis basedvisual matrix approach sensor selection diagnosability assessment.online proc. Prognostics Health Management Conference (PHM09),http://www.phmsociety.org/references/proceedings. PHM Society.Snooke, N., & Price, C. (2012). Automated FMEA based diagnostic symptom generation.Advanced Engineering Informatics, 26 (4), 870 888.Struss, P. (2003). discrete charm diagnosis based continuous models. IFACSafeprocess 03. International Federation Automatic Control.Struss, P., Malik, A., & Sachenbacher, M. (1995). Qualitative modelling key.Workshop Notes 6th International Workshop Principles Diagnosis (DX-95),pp. 99106.Sussman, G. J., & Steele Jr, G. L. (1980). CONSTRAINTS: language expressingalmost-hierarchical descriptions. Artificial Intelligence, 14, 139.Trave-Massuyes, L., Ironi, L., & Dague, P. (2004). Mathematical foundations qualitativereasoning. AI Magazine., 24 (4), 91106.Tsai, J. J.-H., & Gero, J. S. (2010). qualitative approach energy-based unifiedrepresentation building design. Automation Construction, 19 (1).Weld, D. S. (1988a). Choices comparative analysis: DQ analysis exaggeration?. Artificial Intelligence Engineering, 3 (3), 174 180.Weld, D. S. (1988b). Comparative analysis. Artificial Intelligence, 36 (3), 333 373.Weld, D. S. (1990). Exaggeration. Artificial Intelligence, 43 (3), 311 368.447fiJournal Artificial Intelligence Research 46 (2013) 607-650Submitted 9/12; published 4/13Efficient Computation Shapley ValueGame-Theoretic Network CentralityTomasz P. MichalakTomasz.Michalak@cs.ox.ac.ukDepartment Computer Science, University OxfordOX1 3QD, UKInstitute Informatics, University Warsaw02-097 Warsaw, PolandKarthik .V. AadithyaAadithya@eecs.berkeley.eduDepartment Electrical Engineering Computer SciencesUniversity CaliforniaBerkeley, CA 94720-4505, United States, USAPiotr L. SzczepaskiP.Szczepanski@stud.elka.pw.edu.plInstitute InformaticsWarsaw University Technology00-661 Warsaw, PolandBalaraman RavindranRavi@cse.iitm.ac.inComputer Science EngineeringIndian Institute Technology MadrasChennai, 600 036, IndiaNicholas R. JenningsNRJ@ecs.soton.ac.ukSchool Electronics Computer ScienceUniversity SouthamptonSO17 1BJ Southampton, UKAbstractShapley valueprobably important normative payoff division scheme coalitional gameshas recently advocated useful measure centrality networks.However, although approach variety real-world applications (including socialorganisational networks, biological networks communication networks), computational properties widely studied. date, practicable approachcompute Shapley value-based centrality via Monte Carlo simulationscomputationally expensive guaranteed give exact answer.background, paper presents first study computational aspects Shapleyvalue network centralities. Specifically, develop exact analytical formulae Shapley value-based centrality weighted unweighted networks develop efficient(polynomial time) exact algorithms based them. empirically evaluate algorithms two real-life examples (an infrastructure network representing topologyWestern States Power Grid collaboration network field astrophysics)demonstrate deliver significant speedups Monte Carlo approach.instance, case unweighted networks algorithms able return exactsolution 1600 times faster Monte Carlo approximation, even allowgenerous 10% error margin latter method.c2013AI Access Foundation. rights reserved.fiMichalak, Aadithya, Szczepaski, Ravindran, & Jennings1. Introductionmany network applications, important determine nodes edgescritical others. Classic examples include identifying important hubs roadnetwork (Schultes & Sanders, 2007), critical functional entities protein network(Jeong, Mason, Barabasi, & Oltvai, 2001), influential people social network(Kempe, Kleinberg, & Tardos, 2003). Consequently, concept centrality, aimsquantify importance individual nodes/edges, extensively studiedliterature (Koschtzki, Lehmann, Peeters, Richter, Tenfelde-Podehl, & Zlotowski, 2005;Brandes & Erlebach, 2005).v6v4v1v5v9v7v2v10v8v12v3v11v13Figure 1: Sample network 13 nodesGenerally speaking, centrality analysis aims create consistent ranking nodes withinnetwork. end, centrality measures assign score node way corresponds importance node given particular application. Since importancedepends context problem hand, many different centrality measuresdeveloped. Three well-known widely applied are: degree centrality,closeness centrality betweenness centrality.1 paper, refer measuresconventional/standard centrality. Degree centrality, brief, quantifies powernode degree, i.e., number adjacent edges. instance, samplenetwork Figure 1, nodes v1 v2 degree 5 and, judged degree centrality,important nodes within entire network. Conversely, closeness centralityfocuses distances among nodes gives high value nodes closenodes. measure, node v8 Figure 1 ranked top. last measurebetweenness centralityconsiders shortest paths (i.e., paths use minimal numberlinks) two nodes network. shortest paths node belongs to,important is. measure, v2 Figure 1 importantnodes (including v1 v8 , chosen measures importantnode). Clearly, measures expose different characteristics node. Consider,instance, epidemiology application, aim identify people (i.e., nodes)social network biggest influence spread diseasebecome focal point prevention emergency measures. Here, degree centrality1. Koschtzki et al. (2005) Brandes Erlebach (2005) give good overviewcentrality measures.608fiComputation Shapley Value Game-Theoretic Network Centralityranks top nodes biggest immediate sphere influencetheir infection would leadhighest number adjacent nodes exposed disease. hand,closeness centrality identifies nodes whose infection would lead fastest spreaddisease throughout society. Finally, betweenness centrality reveals nodesplay crucial role passing disease one person network another.2common feature aforementioned standard measures assessimportance node focusing role node plays itself. However,many applications approach inadequate synergies may occurfunctioning nodes considered groups. Referring Figure 1epidemiology example, vaccination individual node v6 (or v7 v8 ) would preventspread disease left right part network (or vice versa).However, simultaneous vaccination v6 , v7 v8 would achieve goal. Thus,particular context, nodes v6 , v7 v8 play significant role individually,together do. quantify importance groups nodes, notiongroup centrality introduced Everett Borgatti (1999). Intuitively, group centralityworks broadly way standard centrality, focus functioninggiven group nodes, rather individual nodes. instance, Figure 1, groupdegree centrality {v1 , v2 } 7 7 distinct adjacent nodes.Although concept group centrality addresses issue synergy functionsplayed various nodes, suffers fundamental deficiency. focuses particular,priori determined, groups nodes clear construct consistent rankingindividual nodes using group results. Specifically, nodesvaluable group ranked top? important nodes belonggroup highest average value per node? focus nodescontribute every coalition join? fact, many possibilitieschoose from.framework address issue game theoretic network centrality measure.detail, allows consistent ranking individual nodes computed wayaccounts various possible synergies occurring within possible groups nodes (Grofman& Owen, 1982; Gmez, Gonzlez-Arangena, Manuel, Owen, Del Pozo, & Tejada, 2003;Suri & Narahari, 2008). Specifically, concept builds upon cooperative game theoryapart game theory agents (or players) allowed form coalitions orderincrease payoffs game. Now, one fundamental questions cooperativegame theory distribute surplus achieved cooperation among agents.end, Shapley (1953) proposed remunerate agents payoffs correspondindividual marginal contributions game. detail, given agent,individual marginal contribution measured weighted average marginal increasepayoff coalition agent could potentially join. Shapley famously provedconceptknown since Shapley valueis division schememeets certain desirable normative properties.3 Given this, key idea game theoreticnetwork centrality define cooperative game network agentsnodes, coalitions groups nodes, payoffs coalitions defined meet2. differences interpretation standard centralities see work Borgatti Everett (2006).3. See Section 3 details.609fiMichalak, Aadithya, Szczepaski, Ravindran, & Jenningsrequirements given application. means Shapley value agentgame interpreted centrality measure represents averagemarginal contribution made node every coalition nodes.4words, Shapley value answers question construct consistent rankingindividual nodes groups nodes evaluated.detail, Shapley value-based approach centrality is, one hand, muchsophisticated conventional measures, accounts group nodesShapley value derives consistent ranking individual nodes.hand, confers high degree flexibility cooperative game networkdefined variety ways. means many different versions Shapley value-basedcentrality developed depending particular application consideration,well features network analyzed. prominent example,specific Shapley value-based centrality measure developed crafted particularapplication, consider work Suri Narahari (2008) study problem selectingtop-k nodes social network. problem relevant applicationskey issue choose group nodes together biggest influenceentire network. include, example, analysis co-authorship networks,diffusion information, viral marketing. new approach problem, SuriNarahari define cooperative game value group nodes equalnumber nodes within, adjacent to, group. words, assumedagents sphere influence reaches immediate neighbors group. Whereasdefinition game natural extension (group) degree centrality discussedabove, Shapley value nodes game constitutes new centrality metric is,arguably, qualitatively better standard degree centrality far nodes influenceconcerned. intuition behind visible even small network Figure 1. termsinfluence, node v1 important v2 , node connectedv4 v5 . Without v1 impossible influence v4 v5 , neighbor v2accessible node. Thus, unlike standard degree centrality, evaluatesv1 v2 equally, centrality based Shapley value game defined SuriNarahari recognizes difference influence assigns higher value v1 v2 .Unfortunately, despite advantages Shapley value-based centrality conventionalapproaches, efficient algorithms compute yet developed. Indeed, givennetwork G(V, E), V set nodes E set edges, using originalShapley value formula involves computing marginal contribution every node everycoalition O(2|V | ). exponential computation clearly prohibitive biggernetworks (of, e.g, 100 1000 nodes). networks, feasible approach currentlyoutlined literature Monte-Carlo sampling (e.g., Suri & Narahari, 2008; Castro,Gomez, & Tejada, 2009). However, method inexact, alsotime-consuming. instance, shown simulations, weighted network16,000 nodes 120,000 edges, Monte Carlo approach iterate 300, 0004. note division schemes power indices cooperative game theory, Banzhafpower index (Banzhaf, 1965), could also used centrality measures (see, instance, discussionwork Grofman & Owen, 1982). However, like literature, focus Shapleyvalue due desirable properties.610fiComputation Shapley Value Game-Theoretic Network Centralityb)a)v3v5v4110v1v4v2v6v1v513v62v71v2v3Figure 2: Sample unweighted weighted networks 6 7 nodes, respectively.times entire network produce approximation Shapley value40% error margin.5 Moreover, exponentially iterations needed reduceerror margin.background, develop polynomial-time algorithms compute Shapley valuebased centrality. Specifically, focus five underlying games defined network;games extend, various directions, standard notions degree closeness centrality.starting point, consider game defined Suri Narahari proposeexact, linear-time algorithm compute corresponding Shapley value-based centrality.denote game g1 . analyse computational properties fourgames defined networks. denote g2 , g3 , g4 , g5 , respectively.games captures different flavor centrality, all, similarly gameSuri Narahari, embrace one fundamental centrality idea: given group nodes C,function defines value C game must somehow quantify sphereinfluence C nodes network. particular:g2 game value coalition C function size numbernodes immediately reachable least k different ways C.game inspired Bikhchandani, Hirshleifer, Welch (1992) instancegeneral threshold model introduced Kempe, Kleinberg, Tardos (2005).natural interpretation: agent becomes influenced (with ideas, information,marketing message, etc.) least k neighbors already becomeinfluenced. instance, given k = 2, value coalition {v1 , v2 } Figure 2a4 coalition size 2 two neighbors less 2 edgesadjacent coalition.g3 game concerns weighted graphs (unlike g1 g2 ). Here, value coalition Cdepends size set nodes within cutoff distance C, measuredshortest path lengths weighted graph. example, Figure 2b,cutoff set 8 coalition {v2 } value 4 able influence 3 nodes v3 ,v6 , v7 8 away {v2 }. cutoff distanceinterpreted radius sphere influence coalition.5. See Section 5 exact definition error margin.611fiMichalak, Aadithya, Szczepaski, Ravindran, & JenningsGame GraphValue coalition C, i.e., (C)ComplexityAccuracyg1U W (C) number nodes CO(|V | + |E|)exactimmediately reachable Cg2U W (C) number nodes CO(|V | + |E|)exactimmediately reachable C,via least k different edgesg3W (C) number nodes C O(|V ||E| + |V |2 log|V |) exactdcutoff awayg4W (C) sum f (.)s nonO(|V ||E| + |V |2 log|V |) exact-increasing functions distanceC nodesg5W (C) number nodes CO(|V ||E|)approx.directly connected C via edges5-10%sum weights exceeds WcutoffTable 1: Games considered paper results (U W denotes unweighted graphsW weighted).g4 game generalizes g3 allowing value C specified sizearbitrary non-increasing function f (.) distance C nodesnetwork. instance, value {v1 , v3 } Figure 2b function11f (d) = 1+d2 1 + 3 21 + 1 31 + 1 14 = 4 12. intuitioncoalition influence closer nodes awaya propertycannot expressed standard closeness centrality. Thus, g4 seenextension closeness centrality.g5 last game extension g2 case weighted networks. Here, valueC depends adjacent nodes connected coalition weightededges whose sum exceeds given threshold wcutof f (recall g2 thresholddefined simply integer k). Whereas g3 g4 weights edges interpreteddistance, g5 interpreted power influence. example,Figure 2b, threshold vertex 5, value coalition {v1 , v3 } 3coalition size two enough power influence one additionalnode v2 .computation Shapley value five games (see Table 1overview) main focus paper. Shapley values extensions eitherdegree closeness centrality metrics applications settingsinfluence nodes nodes network evaluated. resultssummarized follows:demonstrate possible exactly efficiently compute numberShapley value-based network centrality measures. methods take advantage612fiComputation Shapley Value Game-Theoretic Network Centralitynetwork structure, well specifics underlying game definednetwork.first four games, derive closed-form expressions Shapley values.Based these, provide exact linear polynomial-time algorithms efficientlycompute Shapley values, i.e., without need enumerate possible coalitions.Specifically, algorithms run O(|V | + |E|) g1 g2 O(|V ||E| +|V |2 log|V |) g3 g4 . Furthermore, fifth measure centrality, developclosed-form polynomial time computable Shapley value approximation. algorithmrunning time O(|V ||E|) experiments show approximation error5% large networks. summary algorithms performancefound Table 1.evaluate algorithms two real-life examples: infrastructure network representing topology Western States Power Grid collaboration networkfield astrophysics. results show algorithms deliver significantspeedups Monte Carlo simulations. instance, given unweighted networkWestern States Power Grid, algorithms return exact Shapley value g1g2 1600 times faster Monte Carlo method returns approximation10% error margin.remainder paper organized follows. Section 2 discuss related work.Notation preliminary definitions presented Section 3. Section 4 analysefive types centrality-related coalitional games propose polynomial time Shapley valuealgorithms them. results numerical simulations presented Section 5(with details simulation setup presented Appendix A). Conclusions futurework follow. Finally, Appendix B provides summary key notational conventions.2. Related Literatureissue centrality one fundamental research directions network analysisliterature. particular, Freeman (1979) first formalise notion centralitypresenting conventional centrality measures: degree, closeness betweenness. Manyauthors subsequently worked developing new centrality measures, refining existingones (e.g., Bonacich, 1972; Noh & Rieger, 2004; Stephenson & Zelen, 1989), developingalgorithms efficient centrality computation (e.g., Brandes, 2001; Eppstein & Wang, 2001).context, Grofman Owen (1982) first apply game theory topiccentrality, focused Banzhaf power index (Banzhaf, 1965). followup work, Gmez et al. (2003) combined Myersons (1977) idea graph-restricted games(in feasible coalition induced subgraph graph) conceptcentrality proposed new Shapley value-based network centrality measures. contrastGmez et al., Suri Narahari (2008, 2010) assumed coalitions feasible,approach also adopt paper.fundamental problem conventional models coalitional games, i.e., exponential complexity number agents, tackle paper, also613fiMichalak, Aadithya, Szczepaski, Ravindran, & Jennings12v11v2431v3Figure 3: Induced-subgraph representation sample coalitional game 3 players.game, values coalitions {v1 }, {v2 }, {v3 }, {v1 , v2 }, {v1 , v3 }, {v2 , v3 }, {v1 , v2 , v3}1, 1, 1, 1+1+2, 1+1+3, 1+1+4, 1+1+1+2+3+4, respectively.studied literature algorithmic aspects coalitional games. Indeed, since seminal work Deng Papadimitriou (1994), issue received considerable attentioncomputer scientists. Specifically, alternative straightforward (but exponential) listing possible coalitions, number authors proposed efficientrepresentations coalitional games. representations fall two main categories(Wooldridge & Dunne, 2006):give characteristic function specific interpretation terms combinatorial structures graphs. approach adopted by, instance,Deng Papadimitriou (1994), Greco, Malizia, Palopoli, Scarcello (2009),Wooldridge Dunne (2006) advantage ensuing representationalways guaranteed succinct. However, disadvantage alwaysfully expressive, i.e., cannot represent coalitional games.try find succinct, still fully expressive, representation. is,instance, approach adopted Conitzer Sandholm (2004), Ieong Shoham(2005), Elkind, Goldberg, Goldberg, Wooldridge (2009). representations general completely capture coalitional games interest,although always guaranteed succinct.Unfortunately, even succinctly representable games, computing Shapley valueshown NP-Hard (or even worse, #P-Complete) many domains, including weighted voting games (Deng & Papadimitriou, 1994), threshold network flow games(Bachrach & Rosenschein, 2009) minimum spanning tree games (Nagamochi, Zeng,Kabutoya, & Ibaraki, 1997). Similarly, Aziz, Lachish, Paterson, Savani (2009a) obtained negative results related problem computing Shapley-Shubik power indexspanning connectivity games based undirected, unweighted multigraphs.Also, Bachrach, Rosenschein, Porat (2008b) showed computation Banzhafindex connectivity games, agents vertices control adjacent edgesaim become connected certain set primary edges, #P-Complete.Fortunately, positive results also discovered. Probably knownamong due Deng Papadimitriou (1994) Ieong Shoham (2005).614fiComputation Shapley Value Game-Theoretic Network Centralitydetail, Deng Papadimitriou proposed representation based weighted graphs,node interpreted agent, weight edge interpreted valuecooperation two agents connected edge.6 valuecoalition defined sum weights internal edges, or, words,weights edges belonging subgraph induced members coalition. threeplayer example formalism, called induced-subgraph representation, foundFigure 3. downside representation fully expressive. However,upside that, games formalised weighted graphs, representationalways concise. Furthermore, allows Shapley value computed time linearnumber players. Specifically, case, Shapley value givenfollowing formula:Shapley Value(vi ) = vi self-loop weight +vjweight edge vi vj.2neighbours vX(1)Ieong Shoham (2005) developed representation consisting finite set logical rulesfollowing form: Boolean Expression Real Number, agents atomicboolean variables. representation, value coalition equal sumright sides rules whose left sides satisfied coalition. representation, called marginal contribution networks (or MC-Nets short) (i) fully expressive(i.e., used model game), (ii) exponentially concise games,importantly, (iii) allows Shapley value computed time linearsize representation, provided boolean expressions rules conjunctions(either positive negative) atomic literals. MC-Nets, rules interestinggame-theoretic interpretation, rule directly specifies incremental marginal contribution made agents featured rule. Now, using additivity axiom metShapley value, possible consider every rule separate simple game,using axioms straightforwardly compute Shapley value simple game,and, finally, sum results simple games obtained Shapley value. Building this, Elkind et al. (2009) developed extensions MC-Nets sophisticated(read-once) boolean expressions, Michalak, Marciniak, Samotulski, Rahwan, McBurney, Wooldridge, Jennings (2010a), Michalak, Rahwan, Marciniak, Szamotulski,Jennings (2010b) developed generalizations coalitional games externalities. Anotherrecently proposed representation formalism coalitional games allows polynomialcalculations Shapley value decision diagrams (Bolus, 2011; Aadithya, Michalak, &Jennings, 2011; Sakurai, Ueda, Iwasaki, Minato, & Yokoo, 2011). Now, MC-Nets offerfully-expressive representation works arbitrary coalitional games, possiblespeed Shapley value computation focusing specific (not necessarily fully expressive) classes games. One particular class games investigated detailweighted voting, approximate (but strictly polynomial) (Fatima, Wooldridge,& Jennings, 2007) exact (but pseudo-polynomial) algorithms (Mann & Shapley, 1962;Matsui & Matsui, 2000) proposed. Chalkiadakis, Elkind, Wooldridge (2011)provided comprehensive discussion literature.6. Also self-loops allowed.615fiMichalak, Aadithya, Szczepaski, Ravindran, & JenningsWhereas choice representation foremost consideration efficient Shapley value computation context conventional coalitional games, paper,face rather different set challenges:Unlike conventional coalitional games, conciseness usually issue networks context. games aim capture network centralitynotions completely specified (a) underlying network compactly representedgraph, (b) concise closed-form characteristic function expression evaluating coalition values (see next section example). Rather, issueexact specification characteristic function dictated computational considerations, real-world application game theoretic networkcentrality. words, choice representation Shapley value computationalready fixed centrality consideration.games paper designed reflect network centrality, characteristic function definition often depends highly non-trivial way underlyinggraph structure. Specifically, value assigned characteristic functionsubset nodes depends subgraph induced nodes, alsorelationship subgraph rest network. example,value assigned coalition nodes may based shortest path lengthsnodes outside coalition, may depend relationship coalitionneighbors.Therefore, specific challenge tackle efficiently compute Shapley value, givennetwork game defined it, coalition values game givenclosed-form expression depends non-trivially network. key questiontake advantage (a) network structure, (b) functional formcoalition values, compute Shapley values efficiently, i.e., without needenumerate possible coalitions.Finally, conclude section mentioning Shapley value solution concepts game theory applied network-related problems. instance,application Shapley value (and Nucleolus) problem cost allocationelectric market transmission system considered Zolezzi Rudnick (2002),though computational aspects discussed. problem maximizing probability hitting strategically chosen hidden virtual network placing wiretapsingle link communication network analysed Aziz, Lachish, Paterson, Savani(2009b). problem viewed two-player win-lose (zero-sum) game, wiretapgame. authors provide polynomial-time computational results game,also show one (key) strategies nucleolus simple cooperativespanning connectivity game (Aziz et al., 2009a) mentioned above.3. Preliminaries Notationsection formally introduce basic concepts graph theory cooperativegame theory used throughout paper. look closely sample coalitional616fiComputation Shapley Value Game-Theoretic Network Centralitygame defined network Shapley value game usedcentrality measure.graph (or network ) G consists vertices (or nodes) edges, setsdenoted V (G) E(G), respectively. Every edge set E(G) connects two verticesset V (G).7 (u, v) denote edge connecting vertices u, v V (G). numberedges incident vertex called vertex degree. neighboring vertices v Vvertices connected v graph. weighted network weight (label)associated every edge E(G). path is, informally, sequence connected edges.shortest path problem find path two given vertices sumedge weights minimized.formalize notions coalitional games Shapley value. Specifically, letus denote = {a1 , . . . , a|A| } set players participate coalitional game.characteristic function : R assigns every coalition C real number representingquality performance, assumed () = 0. characteristic functiongame tuple (A, ). Assuming grand coalition, i.e., coalitionagents game, highest value formed, one key questionscoalitional game theory distribute gain cooperation among agentsmeet certain normative/positive criteria. end, Shapley (1953) proposedevaluate role agent game computing weighted average marginalcontributions agent possible coalitions belong to. importanceShapley value stems fact unique division scheme meets fourdesirable criteria:(i) efficiency wealth available agents grand coalition distributedamong them;(ii) symmetry payoffs agents depend identity;(iii) null player agents zero marginal contributions coalitions receive zeropayoff;(iv) additivity values two games sum value computed sumgames.order formalize concept, let (A) denote permutation agents A,let C (i) denote coalition made predecessors agent ai . formally,denote (j) location aj , then: C (i) = {aj : (j) < (i)}. Shapleyvalue ai , denoted SVi (), defined average marginal contribution aicoalition C (i) (Shapley, 1953):SVi () =1 X[(C (i) {ai }) (C (i))].|A|!(2)7. Whereas main focus paper undirected graphs, also show resultsreadily extended case directed graphs.617fiMichalak, Aadithya, Szczepaski, Ravindran, & JenningsShapley provides following intuition behind formula: imagine playersarrive meeting point random order, every player ai arrives receivesmarginal contribution arrival would bring already meeting point.average contributions possible orders arrival, obtain SVi (),ai payoff game.formula (2) also stated equivalent, computationally less involved,form:X |C|!(|A| |C| 1)!SVi () =[(C {ai }) (C)].(3)|A|!CA\{ai }network context, use G define coalitional game (V (G), ) setagents = V (G) characteristic function . agents coalitional gamevertices graph G. Thus, coalition agents C simply subset V (G).Furthermore, characteristic function : 2V (G) R function dependsgraph G long satisfies condition () = 0. use phrase valuecoalition C informally refer (C).first consider sample characteristic function game defined network, wellShapley value becomes centrality measure. discuss advantagesgame theoretic network centrality conventional measures.detail, consider notion closeness centrality node graph G(V, E),traditionally defined reciprocal average distance node(reachable) nodes graph (Koschtzki et al., 2005). definition capturesintuitive idea node close proximity many nodes valuablevirtue central location, hence assigned higher centrality score.measure, however, fails recognize importance combinations nodes.example, consider typical application closeness centrality: disseminating pieceinformation nodes network. time point dissemination process,define random variable Ct subset nodes actively involved propagatinginformation. situation, new node added Ct would make maximum contributiondiffusion information close proximity nodes currentlyclose proximity node Ct . Thus, conventional closeness centralitytakes account average proximity nodes, actual importance nodeactual applications based different measure: proximity nodesclose proximity random variable Ct .show coalitional game theory used construct centrality measurefaithfully models situation. Let C arbitrary subset nodesgiven network G(V, E). Then, every C, assign value (C) given(C) =XvV (G)1,1 + min{d(u, v)|u C}d(u, v) distance nodes u v (measured shortest path lengthu v graph G).618fiComputation Shapley Value Game-Theoretic Network Centralitymap defined captures fundamental centrality notion: intrinsic valuesubset nodes C context application information disseminationproportional overall proximity nodes C nodes network.effect, map carries original definition closeness centrality global level,measure importance assigned every possible combination nodes.map therefore characteristic function coalitional game,vertex network viewed agent playing game. follows node vhigh Shapley value game, likely v would contribute arbitraryrandomly chosen coalition nodes C terms increasing proximity Cnodes network. Thus, computing Shapley values game yields centralityscore vertex much-improved characterization closeness centrality.difficulty adopting game-theoretically inspired centrality measurepreviously mentioned problem exponential complexity number agents.next section, show overcome difficulty compute Shapley valuemany centrality applications (including formulation) time polynomialsize network.4. Algorithms Shapley Value-Based Network Centralitysection, present five characteristic function formulations (C), designedconvey specific centrality notion. already mentioned introduction, commonelement formulations aim quantify, albeit different way,sphere influence coalition C nodes network. Specifically,first game formulation, start simplest possible idea sphere influencecoalition nodes C set nodes immediately reachable (within one hop)C. Subsequent games generalize notion. particular, second formulationspecifies sophisticated sphere influence: one includes nodesimmediately reachable least k different ways C. three formulationsextend notion sphere influence weighted graphs. third game defines sphereinfluence set nodes within cutoff distance C (as measured shortestpath lengths weighted graph). fourth formulation extreme generalization:allows sphere influence C specified arbitrary function f (.)distance C nodes. final formulation straightforward extensionsecond game, case weighted networks.relationships among five games graphically presented Figure 4.4.1 Game 1: 1 (C) = #agents 1 degree awayLet G(V, E) unweighted, undirected network. first define fringe subsetC V (G) set {v V (G) : v C (or) u C (u, v) E(G)}, i.e.,fringe coalition includes nodes reachable coalition one hop.Based fringe, define coalitional game g1 (V (G), 1 ) respect networkG(V, E) characteristic function 1 : 2V (G) R given619fiMichalak, Aadithya, Szczepaski, Ravindran, & JenningsFigure 4: Euler diagram showing relationships among five games consideredpaper. Specifically, game g2 generalizes g1 ; g3 generalizes g1 generalizedg4 ; g5 generalizes g2 . Finally, note certain instances gamesrepresented g3 , g4 g5 .(01 (C) =size(fringe(C))C =otherwise..game applied Suri Narahari (2008) find influential nodessocial networks shown deliver promising results concerning targetset selection problem (see Kempe, Kleinberg, & Tardos, 2003). therefore desiredcompute Shapley values nodes game. shall present exactformula computation rather obtaining Monte Carlo simulationdone Suri Narahari.detail, evaluate Shapley value node vi , consider possible permutationsnodes vi would make positive marginal contribution coalitionnodes occurring itself. Let set nodes occurring node vi randompermutation nodes denoted Ci . Let neighbours node vi graph G(V, E)denoted NG (vi ) degree node vi denoted degG (vi ).key question ask is: necessary sufficient condition node vimarginally contribute node vj NG (vi ) {vi } fringe(Ci )? Clearly, happensneither vj neighbours present Ci . Formally, (NG (vj ){vj })Ci =.going show condition holds probability11+degG (vj ) .Proposition 1. probability random permutation none verticesNG (vj ) {vj } occurs vi , vi vj neighbours, 1+deg1G (vj ) .620fiComputation Shapley Value Game-Theoretic Network CentralityAlgorithm 1: Computing Shapley value Game 1Input: Unweighted graph G(V, E)Output: Shapley values nodes V (G) game g1foreach v V (G)SV[v] = 1+deg1 G (v) ;foreach u NG (v)SV[v] += 1+deg1 G (u) ;endendreturn SV;Proof. need count number permutations satisfy:v(NG (vj ){vj }) (vi ) < (v).(4)end:Let us choose |(NG (vj ) {vj }| positions sequence elements V .|V |1+degways.G (vj )Then, last degG (vj ) chosen positions, place elements (NG (vj ) {vj }) \{vi }. Directly these, place element vi . number line-ups(degG (vj ))!.remaining elements arrange (|V | (1 + degG (vj ))! different ways.all, number permutations satisfying condition (4) is:|V |(degG (vj ))!(|V1+degG (vj )| (1 + degG (vj ))! =|V |!1+degG (vj ) ;thus, probability one permutations randomly chosen11+degG (vj ) .Now, denote Bvi ,vj Bernoulli random variable vi marginally contributes vjfringe(Ci ). above, have:E[Bvi ,vj ] = Pr[(NG (vj ) {vj }) Ci = ] =1.1 + degG (vj )Therefore, SVg1 (vi ), expected marginal contribution vi , given by:SVg1 (vi ) =XE[Bvi ,vj ] =Xvj {vi }NG (vi )vj {vi }NG (vi )6211,1 + degG (vj )(5)fiMichalak, Aadithya, Szczepaski, Ravindran, & Jenningsexact closed-form expression computing Shapley value nodenetwork.possible derive intuition formula. node high degree,number terms Shapley value summation also high. termsinversely related degree neighboring nodes. givesintuition node high centrality degree high, alsowhenever degree tends higher comparison degree neighboring nodes.words, power comes connected powerless, factwell-recognized centrality literature (e.g., Bonacich, 1987). Followingreasoning, also easily predict dynamic changes network, addingremoving edge, would influence Shapley value.8 Adding edge powerfulpowerless node add even power former decrease powerlatter. Naturally, removing edge would reverse effect.Interestingly, although game g1 quite different induced-subgraph representationDeng Papadimitriou (1994), formula SVg1 (vi ) is, extent, similarformula (1). particular, cases, Shapley value node depends solely setimmediate neighbours. Moreover, cases, linear combination fractionsinvolving numerator weight edges node neighbours.9difference denominators, case depends degree involvednodes. see next two games considered paper yield comparable g1closed-form expressions Shapley value.Algorithm 1 directly implements expression (5) compute exact Shapley valuesnodes network. cycles nodes neighbours, running timeO(|V | + |E|).Finally, note Algorithm 1 adopted directed graphs couple simplemodifications. Specifically, order capture many nodes access givennode from, degree node replaced indegree. Furthermore, setneighbours given node v consist nodes edge directedv.4.2 Game 2: 2 (C) = #agents least k neighbors Cconsider general game formulation unweighted graph G(V, E),value coalition includes number agents either coalitionadjacent least k agents coalition. Formally, consider game g2characterised 2 : 2V (G) R,(02 (C) =|{v : v C (or) |NG (v) C| k}|C =otherwise.8. Many real-life networks fact dynamic challenge developing fast streaming algorithmsrecently attracted considerable attention literature (Lee, Lee, Park, Choi, & Chung, 2012).9. Note case g1 weight edge 1.622fiComputation Shapley Value Game-Theoretic Network Centralitysecond game instance General Threshold Model widely studiedliterature (e.g., Kempe et al., 2005; Granovetter, 1978). Intuitively, modelnode become active monotone activation function reaches threshold.instance problem proposed Goyal, Bonchi, Lakshmanan (2010),authors introduced method learning influence probabilities social networks (fromusers action logs). However, many realistic situations much less information availablenetwork possible assess specific probabilities individualnodes become active. Consequently, much simpler models studied. Bikhchandani et al.(1992), instance, consider teenager deciding whether try drugs. strongmotivation trying drugs fact friends so. Conversely, seeing friendsreject drugs could help persuade teenager stay clean. situation modelledsecond game; threshold node k activation function f (S) =|S|. Another example viral marketing innovation diffusion analysis. Again,application, often assumed agent influenced least kneighbors already convinced (Valente, 1996). Note game reducesgame g1 k = 1.Adopting notation previous subsection, ask: necessarysufficient condition node vi marginally contribute node vj NG (vi ) {vi } valuecoalition Ci ?Clearly, degG (vj ) < k, E[Bvi ,vj ] = 1 vi = vj 0 otherwise. degG (nj ) k,split argument two cases. vj 6= vi , condition marginal contributionexactly (k 1) neighbors vj already belong Ci vj/ Ci .hand, vj = vi , marginal contribution occurs Ci originally consisted(k 1) neighbors vj . degG (vj ) k vj 6= vi , need determineappropriate probability.Proposition 2. probability random permutation exactly k 1 neighbours vj1+degG (vj )koccur vi , vj occurs vi , is: degG (vj )(1+deg, vj vi neighborsG (vj ))degG (vj ) k.Proof. need count number permutations satisfy:n!KNG (vj ) |K| = k 1 vK (v) < (vi )vNG (vj )\K (vi ) (v) (vi ) < (vj ) .(6)end:Let us choose |(NG (vj ) {vj }| positions sequence elements V .|V |1+degways.(v)G jThen, choose k 1 elements set (NG (vj ) \ {vi }. number choices(vj )1degGk1.623fiMichalak, Aadithya, Szczepaski, Ravindran, & JenningsAlgorithm 2: Computing Shapley value Game 2Input: Unweighted graph G(V, E), positive integer kOutput: Shapley value nodes V (G) game g2foreach v V (G)SV[v] = min(1, 1+degk G (v) );foreach u NG (v)G (u)k+1SV[v] += max(0, degGdeg(u)(1+degG (u)) );endendreturn SV;Then, first k 1 chosen positions, place elements chosen previous step.Directly those, place element vi , remaining vertices chosenfirst step. number line-ups (k 1)!(1 + degG (vj ) k)!.remaining elements arrange (|V | degG (vj ) 1)! different ways.Taking together, number permutations satisfying condition (6) is:degG 1|V |k1 (k1+degG (vj )1)!(1 + degG (vj ) k)!(|V | degG (vj ) 1)! =|V |!(1+degG (vj )k)degG (vj )(1+degG (vj )) ;thus, probability one permutations randomly chosen1+degG (vj )kdegG (vj )(1+degG (vj )) .Using Proposition 2 obtain:E[Bvi ,vj ] =1 + degG (vj ) k.degG (vj )(1 + degG (vj ))degG (vi ) k vj = vi , have:E[Bvi ,vi ] =k1Xn=01k=.1 + degG (vi )1 + degG (vi )before, Shapley values given substituting formulae into:SVg2 (vi ) =XE[Bvi ,vj ].vj NG (vi ){vi }Although game generalization game g1 , still solved obtain Shapleyvalues nodes O(|V | + |E|) time, formalised Algorithm 2.624fiComputation Shapley Value Game-Theoretic Network Centralityeven general formulation game possible allowing k functionagent, i.e., node vi V (G) assigned unique attribute k(vi ).translates application form: agent convinced least kineighbors convinced, frequently used model literature (Valente, 1996).argument use fact k constant across nodes.generalized formulation solved simple modification original Shapley valueexpression:SV (vi ) =k(vi )+1 + degG (vi )Xvj NG (vi )1 + degG (vj ) k(vj ).degG (vj )(1 + degG (vj ))equation (which also implementable O(|V |+|E|) time) assumes k(vi )1+degG (vi ) nodes vi . condition assumed without loss generalitycases still modelled (we set k(vi ) = 1 + degG (vi ) extreme case nodevi never convinced matter many neighbors already convinced).Finally, note Algorithm 2 adapted case directed graphs alonglines Algorithm 1.4.3 Game 3: 3 (C) = #agents dcutoff awayHitherto, games confined unweighted networks. many applications,necessary model real-world networks weighted graphs. example, coauthorship network mentioned introduction, edge often assigned weightproportional number joint publications corresponding authors produced(Newman, 2001).subsection extends game g1 case weighted networks. Whereas game g1 equates(C) number nodes located within one hop node C, formulationsubsection equates (C) number nodes located within distance dcutoffnode C. Here, distance two nodes measured length shortestpath nodes given weighted graph G(V, E, W ), W : E R+weight function.Formally, define game g3 , coalition C V (G),(03 (C) =size({vi : vj C | distance(vi , vj ) dcutoff })C =otherwise.Clearly, g3 used settings g1 applicable; instance, diffusioninformation social networks analyse research collaboration networks (e.g., Suri &Narahari, 2010, 2008). Moreover, general game, g3 provides additional modellingopportunities. instance, Suri Narahari (2010) suggest intelligent waysieving nodes neighbourhood would improve algorithm solving targetselection problem (top-k problem). Now, g3 allows us define different cutoff distance625fiMichalak, Aadithya, Szczepaski, Ravindran, & JenningsAlgorithm 3: Computing Shapley value Game 3Input: Weighted graph G(V, E, W ), dcutoff > 0Output: Shapley value nodes G game g3foreach v V (G)DistanceVector = Dijkstra(v,G);extNeighbors(v) = ; extDegree(v) = 0;foreach u V (G) u 6= vD(u) dcutoffextNeighbors(v).push(u);extDegree(v)++;endendendforeach v V (G)1SV[v] = 1+extDegree(v);foreach u extN eighbors(v)1SV[v] += 1+extDegree(u);endendreturn SV;node Suri Naraharis setting. Furthermore, g3 specific casegeneral model g4 discussed next subsection.shall show even highly general centrality game g3 amenable analysisyields exact formula Shapley value. However, case algorithmimplementing formula linear size network, O(|V ||E| +|V |2 log|V |) complexity.deriving exact Shapley value formula, introduce extra notation. Defineextended neighborhood NG (vj , dcutoff ) = {vk 6= vj : distance(vk , vj ) dcutoff }, i.e.,set nodes whose distance vj dcutoff . Denote size NG (vj , dcutoff )degG (vj , dcutoff ). notation, necessary sufficient condition node vimarginally contribute node vj value coalition Ci is: distance(vi , vj ) dcutoffdistance(vj , vk ) > dcutoff vk Ci . is, neither vj node extendedneighborhood present Ci . discussion previous subsectionsProposition 1, know probability event exactly 1+degG (v1j ,dcutoff ) . Therefore, exact formula Shapley value node vi game g3 is:SVg3 (vi ) =Xvj {vi }NG (vi ,dcutoff )1.1 + degG (vj , dcutoff )Algorithm 3 works follows: node v network G(V, E), extended neighborhood NG (v, dcutoff ) size degG (v, dcutoff ) first computed using Dijkstras algorithm626fiComputation Shapley Value Game-Theoretic Network CentralityO(|E| + |V |log|V |) time (Cormen, 2001). results used directly implementequation, takes maximum time O(|V |2 ). practice step runs muchfaster worst case situation occurs every node reachable everynode within dcutoff . Overall complexity O(|V ||E| + |V |2 log|V |).Furthermore, deal directed graphs need redefine notion extDegreeextN eighbors given node u Algorithm 3. former number verticesdistance u smaller than, equal to, dcutoff . latter setnodes whose distance u dcutoff .Finally, make observation proof depend dcutoffconstant across nodes. Indeed, node vi V (G) may assigned uniquevalue dcutoff (vi ), (C) would number agents vi within distancedcutoff (vi ) C. case, proof gives:SV (vi ) =Xvj :distance(vi ,vj )dcutoff (vj )4.4 Game 4: 4 (C) =1.1 + degG (vj , dcutoff (vj ))Pvi V (G) f (distance(vi , C))subsection generalizes game g3 , taking motivation real-life networkproblems. game g3 , agents distances dagent dcutoff contributed equallyvalue coalition. However, assumption may always hold trueapplications intuitively expect agents closer coalition contribute value.instance, expect Facebook user exert influence immediate circlefriends friends friends, even though may satisfy dcutoff criterion.Similarly, expect virus-affected computer infect neighboring computer quicklycomputer two hops away.general, expect agent distance coalition would contribute f (d)value, f (.) positive valued decreasing function argument. formally,define game g4 , value coalition C given by:(04 (C) = Pvi V (G) f (d(vi , C))C =otherwise,d(vi , C) minimum distance: min{distance(vi , vj )|vj C}.note possible solve Shapley value formulationconstructing MC-Nets representation (see Section 2 details formalism).Indeed, combinatorial structure networks certain extent similar structureMC-Nets. Consequently, existence polynomial algorithm compute Shapleyvalue MC-Nets strongly suggests polynomial algorithms could developedgames defined networks. results paper demonstrate indeedcase. However, underlined approach compute Shapley627fiMichalak, Aadithya, Szczepaski, Ravindran, & Jenningsvalue different applied MC-Nets. solutions focuscomputing expected contribution every node random permutation nodesdisaggregating game collection simple, easily solvable, gamesdone MC-Nets. difference approaches clearly visible caseg3 . Here, MC-Nets would O(|V |3 ) rules, whereas discussion below,propose efficient algorithm g3 runs O(|V ||E| + |V |2 log|V |).considerable improvement real-world networks sparse, i.e., E O(|V |)(Reka & Barabsi, 2002).case g3 , key question ask is: expected value marginalcontribution vi node vj 6= vi value coalition Ci ? Let marginalcontribution denoted C(vi , vj ). Clearly:(0C(vi , vj ) =f (distance(vi , vj )) f (d(vj , Ci ))distance(vi , vj ) d(vj , Ci )otherwise.Let Dvj = {d1 , d2 ...d|V |1 } distances node vj nodes network,sorted increasing order. Let nodes corresponding distances {w1 , w2 ...w|V |1 },respectively. Let kij + 1 number nodes (out |V | 1) whose distancesvj distance(vi , vj ). Let wkij +1 = vi (i.e., among nodes distancevj vi , vi placed last increasing order).use literal wi mean wi Ci literal wi mean wi/ Ci . Define sequenceboolean variables pk = vj w1 w2 ... wk 0 k |V | 1. Finally denoteexpressions form C(vi , vj |F ) mean marginal contribution vi Civj given coalition Ci satisfies boolean expression F .C(vi , vj |pkij +1 wkij +2 ) = f (dkij +1 ) f (dkij +2 ),C(vi , vj |pkij +2 wkij +3 ) = f (dkij +1 ) f (dkij +3 ),.........C(vi , vj |p|V |2 w|V |1 ) = f (dkij +1 ) f (d|V |1 ),C(vi , vj |p|V |1 ) = f (dkij +1 ).notation, obtain expressions C(vi , vj ) splitting mutuallyexclusive exhaustive (i.e., covering possible non-zero marginal contributions) cases.Now, need determine probability Pr(pk wk+1 ).Proposition 3. probability random permutation none nodes1.{vj , w1 , . . . , wk } occur vi node wk+1 occurs vi (k+1)(k+2)Proof. Let us count number permutations satisfy:v{vj ,w1 ,...,wk } (vi ) < (v) (vi < (wk+1 ).end:628(7)fiComputation Shapley Value Game-Theoretic Network CentralityLet us choose |{vj , w1 , . . . , wk }{vj }{wk+1 }| positions sequence elements|V |V . k+3ways.Then, last k + 1 chosen positions, place elements {vj , w1 , . . . , wk }.Directly these, place element vi , vertex wk+1 . numberline-ups (k + 1)!.remaining elements arrange (|V | (k + 3)! different ways.Thus, number permutations satisfying (7) is:|V |k+3 (k+ 1)!(|V | (k + 3))! =|V |!(k+1)(k+2) ,probability one permutations randomly chosen1(k+1)(k+2) .proposition find that:Pr(pk wk+1 )11 + kij k |V | 2.(k + 1)(k + 2)Using C(vi , vj ) equations probabilities Pr(pk wk+1 ):X f (distance(vi , vj )) f (dk+1 )+ f (distance(vi , vj ))E[M C(vi , vj )] =(k + 1)(k + 2)|V ||V |2k=1+kijf (distance(vi , vj ))=kij + 2|V |2Xk=kij +1f (dk+1 ).(k + 1)(k + 2)vi = vj , similar analysis produces:|V |2E[M C(vi , vi )] = f (0)Xk=0f (dk+1 ).(k + 1)(k + 2)Finally exact Shapley value given by:SVg4 (vi ) =XE[M C(vi , vj )].vj V (G)Algorithm 4 implements formulae. vertex v, vector distancesevery vertex first computed using Dijkstras algorithm (Cormen, 2001). yieldsvector Dv already sorted increasing order. vector traversed629fiMichalak, Aadithya, Szczepaski, Ravindran, & JenningsAlgorithm 4: Computing Shapley value Game 4Input: Weighted graph G(V, E, W ), function f : R+ R+Output: Shapley value nodes G game g4Initialise: v V (G) set SV[v] = 0;foreach v V (G)[Distances D, Nodes w] = Dijkstra(v,G);sum = 0; index = |V|-1; prevDistance = -1, prevSV = -1;index > 0D(index) == prevDistancecurrSV = prevSV;elsecurrSV = f (D(index))1+index sum;endSV[w(index)] += currSV;sum +=f (D(index))index(1+index) ;prevDistance = D(index), prevSV = currSV;index--;endSV[v] += f(0) sum;endreturn SV;P f (dk+1 )reverse, compute backwards cumulative sum(k+1)(k+2) . step backward traversal, Shapley value appropriate node w updated accordingE[M C(w, v)] equation. traversal, Shapley value v updated accordingE[M C(v, v)] equation. process repeated nodes v endalgorithm, Shapley value computed exactly O(|V ||E| + |V |2 log|V |) time.final observation Algorithm 4 works also directed graphs long useappropriate version Dijkstras algorithm (see, e.g., Cormen, 2001).4.5 Game 5: 5 (C) = #agentsP(weights inside C) Wcutoff (agent)subsection, generalize game g2 case weighted networks. Given positiveweighted network G(V,P E, W ) value Wcutoff (vi ) every node vi V (G), firstdefine W (vj , C) = vi C W (vj , vi ) every coalition C, W (vi , vj ) weightedge nodes vi vj (or 0 edge). notation,define game g5 characteristic function:(05 (C) =size({vi : vi C (or) W (vi , C) Wcutoff (vi )})630C =otherwise.fiComputation Shapley Value Game-Theoretic Network CentralityAlgorithm 5: Computing Shapley value Game 5Input: Weighted network G(V, E, W ), cutoffs Wcutoff (vi ) vi V (G)Output: Shapley value nodes G game g5foreach vi V (G)compute store ;endforeach vi V (G)SV[vi ] = 0;foreach 0 degG (vi )ii ), = (X ii ), p = Pr{N (, 2 ) < Wcompute = (Xmcutoff (vi )};pSV[vi ] += 1+degG (vi ) ;endforeach vj NG (vi )p = 0;foreach 0 degG (vj ) 1ijijijcompute = (Xm), = (Xm) z = Zm;deg (v )mG jp += z degG (vj )(deg;G (vj )+1)endSV[vi ] += p;endendreturn SV;formulation applications in, instance, analysis information diffusion, adoption innovations, viral marketing. Indeed, many cascade modelsphenomena weighted graphs proposed (e.g., Granovetter, 1978; Kempe et al.,2003; Young, 2006) work assuming agent change state inactiveactive sum weights active neighbors least equalagent-specific cutoff.Although able come exact formula Shapley valuegame10 , analysis yields approximate formula found accuratepractice.detail, observe node vi marginally contributes node vj NG (vi ) valuecoalition Ci vj/ Ci Wcutoff (vj ) W (vi , vj ) W (vj , Ci ) < Wcutoff (vj ).Let us denote Bvi ,vj Bernoulli random variable corresponding event.need following additional notation:let NG (vj ) = {vi , w1 , w2 ...wdegG (vj )1 };10. Computing Shapley value game involves determining whether sum weights specificedges, adjacent random coalition, exceeds threshold. problem seems least hardcomputing Shapley value weighted voting games, #P-Complete (Elkind et al., 2009).631fiMichalak, Aadithya, Szczepaski, Ravindran, & Jenningslet weights edges vj nodes NG (vj ) Wj ={W (vi , vj ), W1 , W2 ...WdegG (vj )1 } order;let j sum weights Wj j sum squaresweights Wj ;let kij number nodes NG (vj ) occur vi Ci ;let Xtij sum t-subset Wj \ {W (vi , vj )} drawn uniformly randomset possible t-subsets; finallylet Ymij event {kij = vj/ Ci }.Then:degG (vj )1E[Bvi ,vj ] =XijPr(Ymij ) Pr{Xm[Wcutoff (vj ) W (vi , vj ), Wcutoff (vj ))},m=0Pr(Ymij ) obtained Proposition 2:Pr(Ymij )degG (vj )degG (vj ) 1 m! (degG (vj ) m)!=.=(degG (vj ) + 1)!degG (vj )(degG (vj ) + 1)ijEvaluating Pr{Xm[Wcutoff (vj ) W (vi , vj ), Wcutoff (vj ))} much difficultijcomplicated function degG (vj ) 1 numbers Wj \distribution Xmij{W (vi , vj )}. However, obtain analytical expressions mean (Xm) varianceij2 (Xm). given by:(j W (vi , vj ))degG (vj ) 1(j W (vi , vj ))2m(degG (vj ) 1 m)ij)=(j W (vi , vj )2).2 (Xm(degG (vj ) 1)(degG (vj ) 2)degG (vj ) 1ij(Xm)=ijKnowing mean variance (not exact distribution) Xm, proposeapproximation:ijijijXmN ((Xm), 2 (Xm)),N (, 2 ) denotes Gaussian random variable mean variance 2 .approximation similar randomised approach proposed testedFatima et al. (2007).approximation, have:632fiComputation Shapley Value Game-Theoretic Network CentralityijijZm= Pr{Xm[Wcutoff (vj ) W (vi , vj ), Wcutoff (vj ))}givenijZm"1erf2ijWcutoff (vj ) (Xm)ij2(Xm )!erfijWcutoff (vj ) W (vi , vj ) (Xm)ij2(Xm )!#.allows us write:degG (vj )1E[Bvi ,vj ] =Xm=0degG (vj )Z ij .degG (vj )(degG (vj ) + 1)equations true vj 6= vi . vj = vi have:1E[Bvi ,vi ]1 + degG (vi )degG (vi )XiiiiPr{N ((Xm), 2 (Xm)) < Wcutoff (vi )},m=0ii(Xm)=degG (vi )ii2 (Xm)=i2(degG (vi ) m)(i).degG (vi ) (degG (vi ) 1)degG (vi )PFinally Shapley value node vi given vj {vi }NG (vi ) E[Bvi ,vj ].PinPeach graphP holds vi V (G) degG (vi ) 2|E|, Algorithm 5 implementsO(|V | + vi V (G) vj NG (vi ) degG (vj )) O(|V | + |V ||E|) = O(|V ||E|) solution computeShapley value agents game g5 using approximation.Furthermore, make following observation: approximation discrete randomijvariable Xmcontinuous Gaussian random variable good degG (vj ) large.small degG (vj ), one might well use brute force computation determine E[Bvi ,vj ]O(2degG (vj )1 ) time.far directed graphs concerned, calculations Algorithm 5 considerindegree node instead degree. Furthermore, set neighbours node udefined set nodes vi connected directed edge (u, vi ).633fiMichalak, Aadithya, Szczepaski, Ravindran, & Jennings5. Simulationssection evaluate time performance exact algorithms games g1g4 approximation algorithm game g5 . detail, compare exactalgorithms method approximating Shapley value via Monte Carlo samplingfeasible approach compute game-theoretic network centralityavailable date literature. First, provide detailed description simulationsetup; then, present data sets simulation results.5.1 Simulation Setupapproximation methods Shapley value recentlyproposed literature. divided three groupseach referring specificsubclass coalitional games consideration:1. First, let us consider method proposed Fatima et al. (2007) elaboratedFatima, Wooldridge, Jennings (2008). approach concerns weightedvoting games. games, player certain number votes (orwords, weight). coalition winning number votes coalitionexceeds specific threshold, losing otherwise. Fatima et al. proposefollowing method approximate Shapley value weighted voting games. Insteadfinding marginal contributions players 2n coalitions, authors considern randomly-selected coalitions, one size (i.e., 1 n).n coalitions players expected marginal contributions calculatedaverage contributions yields approximation Shapley value. WhereasFatima et al. method certainly attractive, applicable gamesvalue coalition depends sum associated weights bounds.case games g1 g4 .112. Another method proposed Bachrach, Markakis, Procaccia, Rosenschein,Saberi (2008a) context simple coalitional games 12 characteristic function binaryi.e., coalition value either zero one.games, Bachrach et al. extend approach suggested Mann Shapley(1960) provide rigorous statistical analysis. particular, Mann Shapleydescribed Monte Carlo simulations estimate Shapley value randomsample coalitions. Bachrach al. use technique compute Banzhaf powerindex suggested using random sample permutations playersorder compute Shapley-Shubik index simple coalitional games.13computation confidence interval, crucial approach, hingesupon binary form characteristic function simple coalitional games.11. Recall approximation algorithm g5 builds upon Fatima et al. method.game marginal contribution node depends weights assigned incident edges.12. Note weighted voting games simple coalitional games.13. Shapley-Shubik index well-known application Shapley value evaluates powerindividuals voting (Shapley & Shubik, 1954).634fiComputation Shapley Value Game-Theoretic Network CentralityAlgorithm 6: Monte Carlo method approximate Shapley valueInput:Characteristic function v, maximum iteration maxIterOutput: Aproximation Shapley value game vvi V (G)SV[vi ] = 0 ;end= 1 maxItershuffle(V (G));Marginal Contribution blockP=;vi V (G)SV[vi ] += v(P {vi }) - v(P) ;P = P {vi } ;endendvi V (G)SV[vi ]SV[vi ] = maxIter;endreturn SV ;method general one proposed Fatima et al. (2007)as weightedvoting games subset simple coalitional gamesbut still cannot effectivelyused games g1 g4 , characteristic functions binary.3. Unlike first two methods, last method described Castro et al. (2009)efficiently applied coalitional games characteristic function game form,assuming worth every coalition computed polynomial time. Here,approximating Shapley value involves generating permutations playerscomputing marginal contribution player set players occurringit. solution precision increases (statistically) every new permutationanalysed. Furthermore, authors show estimate appropriate sizepermutation sample order guarantee low error. Given broad applicability,method used simulations comparison benchmark.detail, preliminary step, test maximum number MonteCarlo iterations performed reasonable time given game.maximum number iterations, denoted maxIter, becomes input Algorithm 6Monte Carlo sampling. algorithm, one maxIter iterations, randompermutation nodes generated. Then, using characteristic function set{1 , 2 , 3 , 4 , 5 }, calculates marginal contribution node set P635fiMichalak, Aadithya, Szczepaski, Ravindran, & Jenningsnodes occurring given node random permutation.14 Finally, algorithmdivides aggregated sum contributions node number iterationsperformed. time complexity algorithm O(maxIter con), con denotesnumber operations necessary computing Marginal Contribution block.block specifically tailored particular form characteristic functiongames g1 g5 . particular, game g1 (see Algorithm 6), constructed follows.Recall that, game, node vi makes positive contribution coalition Padjacent node u two conditions. Firstly, neither vi u P .Secondly, edge P vi u. check conditions Algorithm 6store nodes already contributed value coalition P arraycalled: Counted. node vi , algorithm iterates set neighboursadjacent node checks whether adjacent node counted arrayCounted. not, marginal contribution node vi increased one. Appendixdescribe Marginal Contribution block games g2 , . . . , g5 , respectively.15details Algorithm 6 applied generate Shapley value approximationsgames g1 g4 , propose exact polynomial solutions, differ g5 ,developed approximate solution. Specifically, games g1 g4 :1. use exact algorithm proposed paper compute Shapley value.2. Then, run Monte Carlo simulations 30 times.16 every run:perform maxIter Monte Carlo iterations.every five iterations, compare approximation Shapley valueobtained via Monte Carlo simulation exact Shapley value obtainedalgorithm.record algorithms runtime error, error definedmaximum discrepancy actual Shapley value Monte Carlobased approximation Shapley value.3. Finally, compute confidence interval using iterations (0.95% confidencelevel).17case game g5 cannot determine exact Shapley value larger networks.Therefore, performed two levels simulation: one level small networks one levellarge networks. Specifically:1. small networks, generate 30 random instances weighted complete graphs6 nodes (denoted K6 ) number graphs 12 nodes (denoted14. Recall characteristic functions v1 , v2 , . . . , v5 correspond games g1 , g2 , . . . , g5 , respectively.15. software package C++ containing exact/approximation algorithms, well MonteCarlo approximation algorithms available www.tomaszmichalak.net.16. purpose comparison method, suffices use 30 iterations, standard errorsconverge significantly indicate magnitude cost using Monte Carlo method.17. Since g4 Monte Carlo iteration relatively time consuming, run once; thus,confidence interval generated, i.e., third step omitted.636fiComputation Shapley Value Game-Theoretic Network CentralityK12 ) weights drawn uniform distribution U (0, 1). Then, graphtwo parameters Wcutoff (vi ) = 41 (vi ) Wcutoff (vi ) = 34 (vi ):18compute exact Shapley value using formula (3).Then, run approximation algorithm determine error approximation.Finally, run 2000 6000 Monte Carlo iterations K6 K12 , respectively.2. large networks, generate 30 random instances weighted completegraphs, 1000 nodes (we denote K1000 ). Then, graphthree parameters Wcutoff (vi ) = 14 (vi ), Wcutoff (vi ) = 42 (vi ),Wcutoff (vi ) = 43 (vi )):run approximation algorithm Shapley value.Then, run fixed number (200000) Monte Carlo iterations.Finally, compute Monte Carlo solution converges resultsapproximation algorithm.described simulation setup, discuss data sets and, finally,simulation results.5.2 Data Used Simulationsconsider two networks already well-studied literature. Specifically,games g1 g3 present simulations undirected, unweighted network representingtopology Western States Power Grid (WSPG).19 network (which 4940nodes 6594 edges) studied many contexts (see, instance, Watts &Strogatz, 1998) freely available online (see, e.g., http://networkdata.ics.uci.edu/data.php?id=107). games g3 g5 (played weighted networks), used networkastrophysics collaborations (abbreviated henceforth APhC) Jan 1, 1995December 31, 1999. network (which 16705 nodes 121251 edges) also freelyavailable online (see, e.g., http://networkdata.ics.uci.edu/ data.php?id=13)used previous studies like Newman (2001).5.3 Simulation Resultsresults presented section show exact algorithms are, general, muchfaster Monte Carlo sampling, case even allow generouserror tolerance. Furthermore, requiring smaller Monte Carlo errors makes Monte Carloruntime exponentially slower exact solution.18. Recall j sum weights Wj defined Section 4.5.19. Note distance threshold dcutoff replaced hop threshold kcutoff , game g3 playedunweighted network.637fiMichalak, Aadithya, Szczepaski, Ravindran, & Jennings2000160016001400140012001200100080010008006006004004002002000.43 ms01009080 70 60 50 40 30 20Maximal allowable MC error (%)1020001009080 70 60 50 40 30 20Maximal allowable MC error (%)100Figure 6: g2 , k = 2, WSPG (UW)2000Monte Carlo solutionexact solution18000.55 ms00Figure 5: g1 , WSPG (UW)Monte Carlo solutionexact solution1800160016001400140012001200Time (ms)Time (ms)Monte Carlo solutionexact solution1800Time (ms)Time (ms)2000Monte Carlo solutionexact solution1800100080010008006006004004002002000.52 ms01009080 70 60 50 40 30 20Maximal allowable MC error (%)Figure 7: g2 , ki =degi2 ,1000.53 ms01009080 70 60 50 40 30 20Maximal allowable MC error (%)Figure 8: g2 , ki =WSPG (UW)34100degi , WSPG (UW)detail, simulation results game g1 shown Figure 5. dotted lineshows performance exact algorithm needs 0.43ms compute Shapleyvalue. contrast, generating reasonable Monte Carlo result takes substantially longertime (the solid line shows average shaded area depicts confidence intervalMonte Carlo simulations). particular, takes average 200ms achieve20% error 2000ms required guarantee 5% error (which4600 times slower exact algorithm).3Figures 6 - 8 concern game g2 different values k (k = 2, ki = deg2 , ki = 4 degi ,20respectively, degi degree node vi ). advantage exact algorithmMonte Carlo simulation exponential.Replacing distance threshold dcutoff hop threshold kcutoff enables game g3played unweighted network. Thus, similarly games g1 g2 , testWestern States Power Grid. results shown Figures 9 10 kcutoffequal 2 3, respectively. third game clearly computationally challengingg1 g2 (note vertical axis seconds instead milliseconds). Now,20. Recall g2 meaning parameter k follows: value coalition C dependsnumber nodes network least k neighbours C.638fi28272625242322212019181716151413Monte Carlo solutionexact solutionTime (s)Time (s)Computation Shapley Value Game-Theoretic Network Centrality12.881009080 70 60 50 40 30 20Maximal allowable MC error (%)10031302928272625242322212019181716151413Monte Carlo solutionexact solution13.03100Figure 9: g3 , kcutoff = 2, WSPG (UW)9080 70 60 50 40 30 20Maximal allowable MC error (%)100Figure 10: g3 , kcutoff = 3, WSPG (UW)exact algorithm takes 13s complete. much lower speedups exactmethods respect Monte Carlo approach stem fact algorithmsstart Dijkstras algorithm. Although algorithm runcases takes 12.5s considered network. means exactsolution slower orders magnitude (compared games g1 g2 ). Monte Carloapproach also slower, slowdown much less significant relative terms.Figures 11 12 show performance algorithms game g3 astrophysicscollaboration network that, unlike Western States Power Grid, weighted network.davgobserve increasing value dcutoff (here dcutoff = avg8 dcutoff = 4 )significantly worsens performance Monte Carlo-based algorithm.increasing number nodes taken account computing marginalcontributions (see inner loop Algorithm 8) time consuming, alsoincreases Monte Carlo error.1game g4 performance algorithms shown Figures 13 - 15 (for f (d) = 1+d,1f (d) = 1+d2 f (d) = e , respectively). Whereas Monte Carlo methodsfirst three games able achieve reasonable error bound seconds minutes,fourth game takes 40 hours approach 50% error.inner loop Marginal Contribution block (see Algorithm 9) iterates nodesnetwork. Due time consuming performance run simulations once.Interestingly, observe error Monte Carlo method sometimes increasesslightly iterations performed. confirms error MonteCarlo method approximate Shapley value proposed Castro et al. (2009)statistically decreasing time. Certain new randomly chosen permutations actuallyincrease error.Figures 16, 17, 18 19 present comparisons approximation algorithm game g5Monte Carlo sampling small networks (for exact Shapley valuecomputed definition formula (3)). figures, horizontal dottedline shows running time solution, vertical dotted line shows averageapproximation error shaded area confidence interval. previously,639fi4846444240383634323028262422201816141210864Monte Carlo solutionexact solution160140120100806040203.47 min1009080 70 60 50 40 30 20Maximal allowable MC error (%)Figure 11: g3 , dcutoff =40davg8 ,104.41 min010035353030252520151010558.78 min9080 70 60 50 40 30 20Maximal allowable MC error (%)Figure 13: g4 , f (d) =11+d ,1040APhC (W)8.88 min09080 70 60 50 40 30 20Maximal allowable MC error (%)Figure 14: g4 , f (d) =APhC (W)0Monte Carlo solutionexact solution1000davg4 ,10201510080 70 60 50 40 30 20Maximal allowable MC error (%)40Monte Carlo solutionexact solution090Figure 12: g3 , dcutoff =APhC (W)Time (h)Time (h)Monte Carlo solutionexact solution180Time (min)Time (min)Michalak, Aadithya, Szczepaski, Ravindran, & Jennings1,1+d2100APhC (W)Monte Carlo solutionexact solution3530Time (h)2520151059.18 min01009080 70 60 50 40 30 20Maximal allowable MC error (%)100Figure 15: g4 , f (d) = ed , APhC (W)solid line shows average, shaded area depicts confidence intervalMonte Carlo simulations. see Figures 16, 17 18 approximation errorproposed algorithm well-contained small networks. Specifically, K610%; whereas bigger network K12 5%. However, notice that,higher values Wcutoff , Monte Carlo method may slightly outperform solution. See640fi76.565.554.543.532.521.510.501410.576. .74 %91 %%Monte Carlo solutionapproximation solutionTime (ms)Time (ms)1410.436. .29 %15 %%Computation Shapley Value Game-Theoretic Network Centrality0.38 ms100 9080 70 60 50 40 30 20 10Maximal allowable MC error (%)76.565.554.543.532.521.510.500Monte Carlo solutionapproximation solution0.38 ms100 904.74. 1 %95. 1 %12%Monte Carlo solutionapproximation solutionTime (ms)Time (ms)262422201816141210864202.82 ms100 9080 70 60 50 40 30 20 10Maximal allowable MC error (%)0Figure 17: g5 , Wcutoff = 34 , K6 (W),5.25. 6 %55. 4 %81%Figure 16: g5 , Wcutoff = 14 , K6 (W)80 70 60 50 40 30 20 10Maximal allowable MC error (%)026242220181614121086420Monte Carlo solutionapproximation solution2.74 ms100 90Figure 18: g5 , Wcutoff = 14 , K12 (W)80 70 60 50 40 30 20 10Maximal allowable MC error (%)0Figure 19: g5 , Wcutoff = 43 , K12 (W)Figure 17 average approximation error Monte Carlo sampling achieved0.38ms lower average error achieved method. Already K12 effectoccur (see Figure 19).large networks, exact Shapley value cannot obtained, naturallyunable compute exact approximation error. believe error may highervalues obtained K6 K12 . However, mixed strategy, discussedSection 4 uses approximation large degree vertices, worktowards containing error within practical tolerance bounds. far believeMonte Carlo gives good results, Figure 20, infer approximation solutionlarge networks gives good results (within 5%) least two times fasterMonte Carlo algorithm.summarise, exact solutions outperform Monte Carlo simulations even relativelywide error margins allowed. However, always case approximationalgorithm game g5 . Furthermore, underlined centrality metricsconsideration cannot described games g1 g4 exactalgorithms available, Monte Carlo simulations still viable option.641fiMichalak, Aadithya, Szczepaski, Ravindran, & Jenningsapproximation solutionMonte Carlo solution Wcutoff=0.25Monte Carlo solution Wcutoff=0.50Monte Carlo solution Wcutoff=0.75222018Time (min)16141210.09 min10860.7540.5020.2501009080 70 60 50 40 30 20Maximal allowable MC error (%)100Figure 20: g5 , Wcutoff = 14 , Wcutoff = 24 , Wcutoff = 43 , K1000 (W)6. Conclusions Future Workkey finding paper Shapley value many centrality-related cooperativegames interest played networks solved analytically. resulting algorithmserror-free, also run polynomial time and, practice, much fasterMonte Carlo methods. Approximate closed-form expressions algorithms alsoconstructed classes games played weighted networks. Simulation resultsshow approximations acceptable range situations.number directions future work. one hand, Shapley value-basedextensions centrality notions, suit particular applications, developed.step direction, first study Shapley value-based betweenness centralityrecently presented Szczepaski, Michalak, Rahwan (2012). hand,would interesting analyze coalitional games defined network wouldbetter reflect centrality nodes certain real-life applications. spirit, recent worksdel Pozo, Manuel, Gonzlez-Arangena, Owen (2011) Amer, Gimnez, Magana(2012) focus generalized coalitional games order agents forming coalitionsmatter. Nevertheless, still classes coalitional games, gameseither positive negative externalities (Yi, 1997), extensively studiedgame theory may yield interesting results applied network centrality.Another interesting application new class coalitional games definednetwork could developed problem influence maximization, already mentionedintroduction.also interesting analyse properties game-theoretic network centralities constructed solution concepts cooperative game theory Shapley value.particular, game defined network belongs class simple coalitionalgames (i.e., binary characteristic function) Banzhaf power index (Banzhaf,1965) could also used centrality metric. Otherwise, general solution conceptscore (Osborne & Rubinstein, 1994) nucleolus (Schmeidler, 1969) couldapplied.642fiComputation Shapley Value Game-Theoretic Network CentralityUltimately, would interesting develop formal general approachwould allow us construct coalitional games defined networks correspondknown centrality metrics even entire families them.21 approach wouldinvolve developing group centrality first building characteristic functioncoalitional game upon it. course, developing new centrality metrics basedcoalitional games, one keep mind computational properties proposedsolutions. Although able obtain satisfactory computational results gamesconsidered paper, computation game-theoretic network centrality maybecome much challenging complex definitions characteristic function.Acknowledgmentswould like thank three anonymous reviewers comments earlier versionpaper helped improve considerably. Also, would like thank dr TalalRahwan dr Suri Rama Narayanam proofreading, helpful comments suggestions.Tomasz Michalak partially supported European Research Council Advanced Grant 291528 (RACE). Nicholas R. Jennings (and partially Tomasz Michalak)supported ORCHID Project, funded EPSRC (Engineering Physical ResearchCouncil) grant EP/I011587/1.Appendix A. Marginal Contribution Blocks Algorithm 6 g2 -g5Algorithm 7: Marginal Contribution block Algorithm 6 g2Counted false ;Edges 0 ;foreach vi V (G)foreach u NG (vi ) {vi }Edges[u]++ ;!Counted[u] ( Edges[u] k[u] u = vi )SV[vi ]++ ;Counted[u] = true ;endendendgames considered paper Marginal Contribution block Algorithm 6 takes slightly different form. main text explained functioningblock g1 . appendix, discuss block remaining four games.particular:g2 : Here, node vi makes positive contribution coalition Padjacent node u also two conditions. Firstly, neither vi u21. thank anonymous reviewer suggestion.643fiMichalak, Aadithya, Szczepaski, Ravindran, & JenningsAlgorithm 8: Marginal Contribution block Algorithm 6 g3Counted false ;foreach vi V (G)foreach u extN eighbors(vi ) {vi }!Counted[u]SV[vi ]++ ;Counted[u] = true ;endendendAlgorithm 9: Marginal Contribution block Algorithm 6 g4dist infinity ;foreach vi V (G)foreach u V (G)D[u] < dist[u]SV[vi ] += f(D[u]) - f(dist[u]) ;dist[u] = D[u] ;endendSV[vi ] += f(dist[u]) - f(0) ;dist[vi ] = 0 ;endP . Secondly, less k edges P vi exactly k 1 edgesP u. order check first condition Algorithm 7 use arrayCounted, check second one, use array Edges. node vi ,algorithm iterates set neighbours adjacent nodechecks whether adjacent node meets two conditions. so, marginalcontribution node vi increased one.g3/4 : Marginal Contribution blocks games g3 g4 (Algorithms 8 9),values dependent distance (extN eighbours D) calculatedusing Dijkstras algorithm stored memory. pre-computations allow ussignificantly speed Monte Carlo methods. Now, g3 node vi makes positivecontribution coalition P adjacent node utwo conditions. Firstly, neither vi u P . Secondly, edge lengthdcutoff P vi u. check conditions Algorithm 8 usearray Counted. node vi , algorithm iterates set extendedneighbours checks whether neighbour meets conditions.so, marginal contribution node vi increased one. game g4 , nodevi makes positive contribution coalition P node (including itself)closer vi P . Algorithm 9 use array Dist store distances644fiComputation Shapley Value Game-Theoretic Network CentralityAlgorithm 10: Marginal Contribution block Algorithm 6 g5Counted false ;Weights 0 ;foreach vi V (G)foreach u NG (vi ) {vi }weights[u]+= W (vi , u);!Counted[u] ( weights[u] Wcutoff (u) u = vi )SV[vi ]++ ;Counted[u] = true ;endendendcoalition P nodes graph array store distances vinodes. node vi , algorithm iterates nodesgraph, node u, distance vi u smaller P u,algorithm computes marginal contribution f (D[u]) f (Dist[u]). valueDist[u] updated D[u]this new distance P u.g5 : game g5 , extension g2 weighted graphs, node vi makes positivecontribution coalition P (both adjacent node u)two conditions. Firstly, neither vi u P . Secondly, sum weightsedges P vi less Wcutoff (vi ) sum weights edges P ugreater than, equal to, Wcutoff (u)W (vi , u) smaller Wcutoff (vi )+W (vi , u).order check first condition Algorithm 10 use array Counted,check second one, use array W eights. node vi , algorithmiterates set neighbours adjacent node checks whetheradjacent node meets two conditions. so, marginal contributionnode vi increased one.Appendix B: Main Notation Used Paperset players.aiplayer A.Ccoalition.value coalition, characteristic function.(C)(A, )/giSVgj (vi )G = (V, E)G = (V, E, W )W (v, u)coalitional game.Shapley value od vertex vi game gj .Unweighted graph/network consisting set vertices V edges E.Weighted graph/network.Weight edge v u.645fiMichalak, Aadithya, Szczepaski, Ravindran, & JenningsV (G)/V, E(G)/Evi Vset vertices edges graph G.deg(vi )vertex set V .Degree vertex vi .NG (vi )Set neighbours vertex vi G.distance(v, u)/d(v, u) distance vertices v u.Extended neighbourhood: NG (vj , dcutoff ) = {vk 6= vj : distance(vk , vj )NG (vi , dcutof f )dcutoff }.Marginal contribution vertex u makes vertex v.C(u, v)(A)(A)(i)C (i)set orders players A.single ordering agents A.position th element ordering .{aj : (j) < (i)}.E[]{v V (G) : v C (or) u C (u, v) E(G)}.number assigned vertex v used Game 2. minimum numberadjacent nodes necessary influence node vi .number assigned vertex v used Game 5. Minimum sum weightsadjacent edges necessary influence node vi .expectation operator.P[]probability operator.O()big complexity notation.f ringe(C)k(vi )/kiWcutoff (vi )B, X,N (, 2 )erf ()Random variables.Normal distribution mean variance 2 .jerror function.sum weights incident edges vertex vj .jsum squares weights incident edges vertex vj .f (.)positive valued decreasing function.Kicomplete graph (clique) nodes.ReferencesAadithya, K., Michalak, T., & Jennings, N. (2011). Representation coalitional gamesalgebraic decision diagrams. AAMAS 11: Proceedings 10th InternationalJoint Conference Autonomous Agents Multi-Agent Systems, pp. 11211122.Amer, R., Gimnez, J., & Magana, A. (2012). Accessibility measures nodes directedgraphs using solutions generalized cooperative games. Mathematical MethodsOperations Research, 75, 105134.Aziz, H., Lachish, O., Paterson, M., & Savani, R. (2009a). Power indices spanning connectivity games. AAIM 09: Proceedings 5th International ConferenceAlgorithmic Aspects Information Management, pp. 5567.646fiComputation Shapley Value Game-Theoretic Network CentralityAziz, H., Lachish, O., Paterson, M., & Savani, R. (2009b). Wiretapping hidden network.WINE 09: Proceedings 5th Workshop Internet & Network Economics,pp. 438446.Bachrach, Y., Markakis, E., Procaccia, A. D., Rosenschein, J. S., & Saberi, A. (2008a).Approximating power indices. AAMAS 08: Proceedings 7th InternationalJoint Conference Autonomous Agents Multi-Agent Systems, pp. 943950.Bachrach, Y., Rosenschein, J. S., & Porat, E. (2008b). Power stability connectivitygames. AAMAS 08: Proceedings 7th International Joint ConferenceAutonomous Agents Multi-Agent Systems, pp. 9991006.Bachrach, Y., & Rosenschein, J. (2009). Power threshold network flow games. AutonomousAgents Multi-Agent Systems, 18 (1), 106132.Banzhaf, J. F. (1965). Weighted Voting Doesnt Work: Mathematical Analysis. RutgersLaw Rev., 19, 317343.Bikhchandani, S., Hirshleifer, D., & Welch, I. (1992). theory fads, fashion, custom,cultural change informational cascades. Journal Political Economy, 100 (5),9921026.Bolus, S. (2011). Power indices simple games vector-weighted majority gamesmeans binary decision diagrams. European Journal Operational Research, 210 (2),258272.Bonacich, P. (1972). Factoring weighting approaches status scores clique identification. Journal Mathematical Sociology, 2 (1), 113120.Bonacich, P. (1987). Power centrality: family measures. American JournalSociology, 92 (5), 11701182.Borgatti, S. P., & Everett, M. (2006). graph-theoretic framework classifying centralitymeasures. social networks. Social Networks, 28(4), 466484.Brandes, U. (2001). faster algorithm betweenness centrality. Journal MathematicalSociology, 25 (2), 163177.Brandes, U., & Erlebach, T. (2005). Network Analysis: Methodological Foundations. Lecturenotes computer science: Tutorial. Springer.Castro, J., Gomez, D., & Tejada, J. (2009). Polynomial calculation shapley valuebased sampling. Computers & Operations Research, 36 (5), 17261730.Chalkiadakis, G., Elkind, E., & Wooldridge, M. (2011). Computational Aspects Cooperative Game Theory. Synthesis Lectures Artificial Intelligence Machine Learning.Morgan & Claypool Publishers.Conitzer, V., & Sandholm, T. (2004). Computing Shapley Values, manipulating value division schemes checking core membership multi-issue domains. AAAI 04:Proceedings Nineteenth National Conference Artificial Intelligence, pp. 219225.Cormen, T. (2001). Introduction algorithms. MIT Press.647fiMichalak, Aadithya, Szczepaski, Ravindran, & Jenningsdel Pozo, M., Manuel, C., Gonzlez-Arangena, E., & Owen, G. (2011). Centrality directedsocial networks. game theoretic approach. Social Networks, 33 (3), 191200.Deng, X., & Papadimitriou, C. (1994). complexity cooperative solution concepts.Mathematics Operations Research, 19 (2), 257266.Elkind, E., Goldberg, L., Goldberg, P., & Wooldridge, M. (2009). tractable expressiveclass marginal contribution nets applications. Mathematical Logic Quarterly,55 (4), 362376.Eppstein, D., & Wang, J. (2001). Fast approximation centrality. SODA 01: ProceedingsTwelfth Annual ACM-SIAM Symposium Discrete Algorithms, pp. 228229.Everett, M. G., & Borgatti, S. P. (1999). centrality groups classes.. JournalMathematical Sociology, 23 (3), 181201.Fatima, S. S., Wooldridge, M., & Jennings, N. (2007). randomized method shapleyvalue voting game. AAMAS 07: Proceedings 11th International JointConference Autonomous Agents Multi-Agent Systems, pp. 955962.Fatima, S. S., Wooldridge, M., & Jennings, N. (2008). linear approximation methodshapley value. Artificial Intellilgence, 172 (14), 16731699.Freeman, L. (1979). Centrality social networks: Conceptual clarification. Social Networks,1 (3), 215239.Gmez, D., Gonzlez-Arangena, E., Manuel, C., Owen, G., Del Pozo, M., & Tejada, J.(2003). Centrality power social networks: game theoretic approach. Mathematical Social Sciences, 46 (1), 2754.Goyal, A., Bonchi, F., & Lakshmanan, L. V. (2010). Learning influence probabilitiessocial networks. WSDM 10: Proceedings 3rd ACM international conferenceWeb search data mining, pp. 241250.Granovetter, M. (1978). Threshold models collective behavior. American JournalSociology, 83 (6), 14201443.Greco, G., Malizia, E., Palopoli, L., & Scarcello, F. (2009). complexity compactcoalitional games. IJCAI 09: Proceedings Twenty First International JointConference Artifical Intelligence, pp. 147152.Grofman, B., & Owen, G. (1982). game-theoretic approach measuring centralitysocial networks. Social Networks, 4, 213224.Ieong, S., & Shoham, Y. (2005). Marginal contribution nets: compact representationscheme coalitional games. EC 05: Proceedings Sixth ACM ConferenceElectronic Commerce, pp. 193202.Irwin, M., & Shapley, L. S. (1960). Values Large Games, IV : Evaluating ElectoralCollege Montecarlo Techniques. CA : RAND Corporation, Santa Monica.Jeong, H., Mason, S. P., Barabasi, A. L., & Oltvai, Z. N. (2001). Lethality centralityprotein networks. Nature, 411 (6833), 4142.Kempe, D., Kleinberg, J., & Tardos, . (2003). Maximizing spread influencesocial network. KDD 03: Proceedings Ninth ACM SIGKDD InternationalConference Knowledge Discovery Data Mining, pp. 137146.648fiComputation Shapley Value Game-Theoretic Network CentralityKempe, D., Kleinberg, J., & Tardos, . (2005). Influential nodes diffusion modelsocial networks. Automata, Languages Programming, 3580, 99.Koschtzki, D., Lehmann, K., Peeters, L., Richter, S., Tenfelde-Podehl, D., & Zlotowski, O.(2005). Centrality indices. Network analysis, Vol. 3418 Lecture Notes ComputerScience, pp. 1661. Springer.Lee, M.-J., Lee, J., Park, J. Y., Choi, R. H., & Chung, C.-W. (2012). Qube: quickalgorithm updating betweenness centrality. Mille, A., Gandon, F. L., Misselis,J., Rabinovich, M., & Staab, S. (Eds.), WWW, pp. 351360. ACM.Mann, I., & Shapley, L. (1962). Values large games, VI: Evaluating electoral collegeexactly.. RAND Research Memorandum.Matsui, T., & Matsui, Y. (2000). survey algorithms calculating power indicesweighted majority games. Journal Operations Research Society Japan, 43,7186.Michalak, T., Marciniak, D., Samotulski, M., Rahwan, T., McBurney, P., Wooldridge, M.,& Jennings, N. (2010a). logic-based representation coalitional games externalities. AAMAS 10: Proceedings 9th International Joint ConferenceAutonomous Agents Multi-Agent Systems, pp. 125132.Michalak, T., Rahwan, T., Marciniak, D., Szamotulski, M., & Jennings, N. (2010b). Computational aspects extending Shapley Value coalitional games externalities.ECAI 10: Proceedings Nineteenth European Conference Artificial Intelligence.Myerson, R. (1977). Graphs cooperation games. Mathematics Operations Research,2 (3), 225229.Nagamochi, H., Zeng, D., Kabutoya, N., & Ibaraki, T. (1997). Complexity minimumbase game matroids. Mathematics Operations Research, 22 (1), 146164.Newman, M. (2001). Scientific collaboration networks. II. Shortest paths, weighted networks,centrality. Physical Review E, 64 (1), 016132(17).Noh, J., & Rieger, H. (2004). Random walks complex networks. Physical Review Letters,92 (11), 118701(14).Osborne, M. J., & Rubinstein, A. (1994). Course Game Theory, Vol. 1 MIT PressBooks. MIT Press.Reka, A., & Barabsi, A. (2002). Statistical mechanics complex networks. ReviewsModern Physics, 74, 4797.Sakurai, Y., Ueda, S., Iwasaki, A., Minato, S., & Yokoo, M. (2011). compact representationscheme coalitional games based multi-terminal zero-suppressed binary decisiondiagrams. PRIMA 11: Public Risk Management Association, pp. 418.Schmeidler, D. (1969). nucleolus characteristic function game. SIAM JournalApplied Mathematics, 11631170.Schultes, D., & Sanders, P. (2007). Dynamic highway-node routing. SEA 07: Proceedings6th Workshop Experimental Efficient Algorithms. LNCS, pp. 6679. Springer.649fiMichalak, Aadithya, Szczepaski, Ravindran, & JenningsShapley, L. S. (1953). value n-person games. Kuhn, H., & Tucker, A. (Eds.),Contributions Theory Games, volume II, pp. 307317. Princeton UniversityPress.Shapley, L. S., & Shubik, M. (1954). Method Evaluating Distribution PowerCommittee System. American Political Science Review, 48 (3), 787792.Stephenson, K., & Zelen, M. (1989). Rethinking centrality: Methods examples. SocialNetworks, 11 (1), 137.Suri, N., & Narahari, Y. (2008). Determining top-k nodes social networks usingshapley value. AAMAS 08: Proceedings 7th International Joint ConferenceAutonomous Agents Multi-Agent Systems, pp. 15091512.Suri, N., & Narahari, Y. (2010). Shapley Value based approach discover influentialnodes social networks. IEEE Transaction Automation Science Engineering,99, 118.Szczepaski, P. L., Michalak, T., & Rahwan, T. (2012). new approach betweennesscentrality based shapley value. AAMAS 12: Proceedings 11th International Joint Conference Autonomous Agents Multi-Agent Systems, pp.239246.Valente, T. (1996). Social network thresholds diffusion innovations. Social Networks,18 (1), 6989.Watts, D., & Strogatz, S. (1998). Collective dynamics small-world networks. Nature,393 (6684), 440442.Wooldridge, M., & Dunne, P. (2006). computational complexity coalitional resource games. Artificial Intelligence, 170 (10), 835871.Yi, S.-S. (1997). Stable coalition structures externalities. Games Economic Behavior, 20 (2), 201237.Young, H. P. (2006). Diffusion Innovations Social Networks. Blume, B. L., &Durlauf, S. N. (Eds.), Economy evolving complex system, Vol. 3 Proceedingsvolume Santa Fe Institute studies sciences complexity Santa Fe InstituteStudies Sciences Complexity, pp. 267282. Oxford University Press US.Zolezzi, J. M., & Rudnick, H. (2002). Transmission cost allocation cooperative gamescoalition formation. IEEE Transactions Power Systems, 17, 10081015.650fiJournal Artificial Intelligence Research 46 (2013) 343-412Submitted 08/12; published 03/13Hybrid LPRPG Heuristic Modelling NumericResource Flows PlanningAmanda ColesAndrew ColesMaria FoxDerek Longamanda.coles@kcl.ac.ukandrew.coles@kcl.ac.ukmaria.fox@kcl.ac.ukderek.long@kcl.ac.ukDepartment InformaticsKings College LondonStrand BuildingLondon, WC2R 2LS, UKAbstractAlthough use metric fluents fundamental many practical planning problems,study heuristics support fully automated planners working fluentsremains relatively unexplored. widely used heuristic relaxation metricfluents interval-valued variables idea first proposed decade ago. heuristicsdepend domain encodings supply additional information fluents,capacity constraints resource-related annotations.particular challenge approaches handling interactions metricfluents represent exchange, transformation quantities raw materialsquantities processed goods, trading money materials. usual relaxationmetric fluents often poor situations, since recogniseresources, spent, longer available spent again.present heuristic numeric planning problems building propositionalrelaxed planning graph, using mathematical program numeric reasoning.define class producerconsumer planning problems demonstrate numericconstraints modelled mixed integer program (MIP). MIPcombined metric Relaxed Planning Graph (RPG) heuristic produce integratedhybrid heuristic. MIP tracks resource use accurately usual relaxation,relaxes ordering actions, RPG captures causal propositional aspectsproblem. discuss two components interact produce single unifiedheuristic go explore numeric features planning problemsintegrated MIP. show encoding limited subset propositional problem augment MIP yield accurate guidance, partly exploiting structurepropositional landmarks propositional resources. results showuse heuristic enhances scalability problems numeric resource interactionkey finding solution.1. IntroductionDomain-independent planning research last decade focussed, part,propositional planning, leading important discoveries powerful new heuristicsplanning propositional domains. Relatively little effort invested planningmetric fluents, despite importance representing many practical planning problems.c2013AI Access Foundation. rights reserved.fiColes, Coles, Fox & LongNumbers essential efficiently encoding resources, money, fuel materials.state-of-the-art remains influential approach proposed Hoffmann (2003),extends ignore-delete-effects relaxation propositional fluents metric fluentstracking accumulating upper bound increasing fluents ignoring decreasing (negative) effects. symmetrical treatment fluents purposes determininglower bound leads representation equivalent interval metricfluent, goals preconditions satisfied provided value intervalsufficient satisfy condition. Although lpg (Gerevini, Saetti, & Serina, 2006)mips (Edelkamp, 2003) capable handling metric fluents, dependrelaxation heuristic offer search guidance. approaches explored(Do & Kambhampati, 2001; Koehler, 1998), comparatively less successful.Planners using MetricFF heuristic generally effective solving problems complex interactions values numeric resources,exchange quantities one materials production others.contrast, solving problems numbers heart operational research mathematical programming techniques. Many powerful solvers developed solvingLinear Programming problems (LPs) Mixed Integer Programming problems (MIPs),problems expressed linear constraints variables (which, caseMIPs, integers). Although efforts exploit linear programmingpropositional planning, either schedule actions (Long & Fox, 2003a) directly,heuristic (van den Briel, Benton, Kambhampati, & Vossen, 2007), relatively little workconsidered exploitation linear programming techniques improve behaviournumeric domain-independent planners (Kautz & Walser, 2000; Shin & Davis, 2005; Wolfman & Weld, 2000; Benton, van den Briel, & Kambhampati, 2007) (this workconsidered Section 1.1).paper, revisit issue planning numeric resources, beginningMetric Relaxed Planning Graph (RPG) heuristic (Hoffmann, 2003). focus specificallydomains exclusively exhibit call producer-consumer behaviour (definedSection 2.3), actions increase decrease numeric resources fixed quantities.course, represents subset possible numeric behaviours, commonintuitive one. Furthermore, easy recognise syntactically domain encoding,simple resort alternative strategies domains conformconstraint, might include use producer-consumer relaxations approximationsdomains complex numeric behaviour.explore behaviour RPG heuristic, demonstrating typicalpatterns interactions producer-consumer numeric planning domains lead highlyuninformative heuristic guidance, particularly domains offer opportunities exchanges metric variables. address this, introduce novel heuristic basedmixed integer program (MIP), used alongside RPG, better capture numericconstraints. described MIP constructed, used complement RPG, discuss extensions improve identified weakness, alsoencode information propositional behaviour problem. evaluatelp-rpg heuristic exploring spectrum between, one end, strict separationnumbers propositions MIP RPG components; and, other, discarding344fiA Hybrid LP-RPG Heuristic PlanningRPG entirely encoding preconditions effects actions entirely MIP.so, determine best trade-off two lies.work report paper extension earlier work reporting development lp-rpg (Coles, Fox, Long, & Smith, 2008). extends workadditional detail several variants core heuristic, exploring impacttighter integration propositional metric fluents heuristic.1.1 Related Workintegration linear programming (LP) MIP techniques planningconsidered number contexts. relevant present work useLP basis heuristic propositional over-subscription planning problems (Benton,Do, & Kambhampati, 2005). setting, goal planning find planmaximum utility, defined terms reward goals achieved, minus costsoccurred achieving them. Benton et al. use LP optimisation tool helpdecide set goals planner satisfy order achieve maximum reward.work Benton et al. work described paper exploit relaxationaction ordering rather effects, employ LP well RPGstructure. two key differences focus work using MIPcapture interactions within numeric planning problems, rely conventionalRelaxed Planning Graph (Hoffmann & Nebel, 2001) propositional reasoning, ratheralso encoding structure MIP. work van den Briel et al. (van den Briel et al.,2007; van den Briel, Vossen, & Kambhampati, 2008) also explores use mathematicalprogramming encode solve planning problems.structure LP MIP models proposed Benton et al. (2007) van denBriel et al. (2008) makes time consuming solve, requiring actions selectedsatisfy preconditions effects actions, delete effects pairedadd effects. contrast, MIP LP models propose attempt capturecausal plan structure, making construction solutions programsstate much feasible. difference interaction twocomponents: integration MIP RPG lp-rpg much tighter,MIP used graph building indicate variable bounds, relaxed planextraction indicate actions use. comparison, earlier approach, MIPused solely introduce bias RPG action selection, giving preference actionsused solution MIP.Linear programming exploited planning work. Lpsat (Wolfman& Weld, 2000) uses planning-as-satisfiability approach, linked use LP solverensure literals representing (linear) constraints metric fluents maintainedplan construction. heuristic guidance search, basedstandard DPLL search satisfying assignment combined confirmationcorresponding LP satisfiable. Ip-sat (Kautz & Walser, 2000) uses MIP encodingplanning problems basis solving them, Vossen et al. (1999), similar waylater work van den Briel et al. (van den Briel et al., 2008). planners MIPused directly heart solver, planning problems translated MIPsrather used guide search. Tm-lpsat (Shin & Davis, 2005) uses lpsat345fiColes, Coles, Fox & Longsystem solve planning problems continuous processes. Kongming (Li & Williams,2008) another example planner exploits compilation planning problemsmathematical programs, solved using CPLEX, tackle hybrid mixed-continuous planningproblems. colin system (Coles, Coles, Fox, & Long, 2012) also uses LP encodingsmanage reasoning effects continuous processes.completely different use linear mixed integer programming planning lieswork Ono Williams (2008) also Blackmore, Ono Williams (2011) usesmixed integer programming foundation solving problem risk allocationplan-level control systems.Alternative approaches handling numeric variables planning include implemented MetricFF, discussed detail Section 3, Sapa (Do & Kambhampati, 2001)Resource-ipp (Koehler, 1998). Sapa heuristic cost estimates generated using relaxed plan extraction supplemented additional costs representing minimal setadditional resource producing actions required achieve resource requirementsrelaxed plans. approach straightforward implement interestingmodification pure relaxed plan heuristic, separates problem producingresources solution rest problem, consequence relaxedplan using steps high resource demands constructed preferencelonger plan lower demands. heuristic value state distortedpenalty attached relaxed plan achieve high resource requirement, potentially hugely overestimating true distance state goal. Resource-ipp dependsidentification consumers producers, paper, buildsresource time map tracks production consumption resourcesGraphplan-based search plan. approach leads extension mutexrelation used constrain search Graphplan (Blum & Furst, 1995). However,iterative-deepening search used Graphplan-based planners scalable solvelarge problems forward state-space search proved dominant strategy pastdecade.2. Problem Definitionsection define class planning problems consider paper.subset general class pddl 2.1 non-temporal, numeric planning problems,represent linear producerconsumer problems. include example Settlersdomain, used throughout paper illustrate ideas presented.2.1 PDDL 2.1 Numeric Planning Problemswork, concerned finding sequential plans solve non-temporal, numericplanning problems, defined using (a subset of) pddl 2.1 (Fox & Long, 2003). Withinpddl, class problems defined follows:11. pddl2.1 also supports specification objective function measure plan quality, definednumeric variables planning problem, work focus minimising plan length.346fiA Hybrid LP-RPG Heuristic Planninginitial state, state consists set propositions, and/or assignment values set numeric variables. notational convenience, refervector numeric values given state v propositional facts F .A, set actions. tuple hpre, eff i:pre preconditions a: conditions must hold stateexecuted.eff effects a: applied, state updated accordingeffects. eff consists of:eff , propositions deleted state;eff + , propositions added state;eff n , effects acting upon numeric variables.G, goal: set propositions F ? set conditions numeric variables,N ? . sets may empty. state hF, vi goal state F ? F vsatisfies condition N ? .general case, pddl numeric conditions (as used pre N ? ) expressedform:hf (v), op, cis.t.op {, <, =, >, }, c <Numeric effects (as eff n) expressed as:hv, op, f (v)is.t.op {=, +=, =, =, =}common Hoffmanns work MetricFF (2003) restrict attentionpreconditions expressed Linear Normal Form (LNF). is, expressionf (v) within preconditions must form weighted sum state variablesplus constant, w.v + k. Likewise, consider numeric effects f (v) LNF,op {+=, =, =}. restrictions guarantee termination constructionRPG evaluating state: introducing non-LNF preconditions, scaling effects,lead asymptotic numeric behaviour certain conditions satisfiedinfinite limit. lp-rpg heuristic describe work, requirenumeric behaviour actions represented producerconsumer behaviour. is,effects cause constant increments decrements variables affect and, apartspecific circumstances, permit assignment effects. precisely definenotions circumstances allow assignment effects later paper.solution planning problem (sequential) plan: sequence actions transforms initial state goal state, respecting preconditions action application.state hF, vi, application action effects eff ,eff + ,eff n yields successorstate hF 0 , v0 i, where:F 0 = (F \ eff ) eff +v0 [x] op (w.v + c) hv, op, w.v + ci eff nv0 [x] = v[x]otherwise347fiColes, Coles, Fox & Long2.2 Example Problem: SettlersSettlers domain, introduced 2002 International Planning Competition (IPC) (Long& Fox, 2003b) used 2004 (Hoffmann & Edelkamp, 2005), good exampleproblem exhibiting interesting use metric fluents. aim Settlers problemsbuild transport building infrastructure extraction, refinementtransportation materials. numeric structure domain perhaps sophisticated IPC domains date. First, six numeric resources severalactions act upon each. available resources, effects actions upon(consumption resource shown negative value production shownpositive value) shown Table 12 . Another interesting feature domainresources directly produced: whilst raw materials Timber, StoneOre directly extracted, Wood, Coal Iron must refined respectiveraw form. Finally, domain contains transferable resources. addition actionsshown table, resources refined consumed fuel transportation,resources loaded unloaded vehicles. effect load unloadactions increase decrease amount resource vehicle, decreaseincrease amount stored given location. Apart consuming producing (i.e.releasing) remaining cargo space vehicle, resource produced consumedloading unloading moved. However, expressing model pddlrequires pair effects described, decreasing one variable increasing another,indistinguishable combination production consumption.2.3 ProducerConsumer Problemsdefine constrained producerconsumer numeric behaviour consideredpaper. first define producer consumer actions, two categories producer. Usingdefine notion producerconsumer variable. identificationconsumers producers new idea common identify resource producersconsumers scheduling (for example, Laborie work schedulingresource constraints, see Laborie, 2003).2.3.1 ProducerConsumer Actionssimple production action defined follows:Definition 2.1 Simple Producerground action simple producer given numeric variable v iff:effect (increase (v) c) (where c positive constant)precondition refers v.definition two important consequences:2. table represents debugged version original domain available http://sourceforge.net/projects/tsgp/files/348fiA Hybrid LP-RPG Heuristic PlanningActionMove cartMove trainMove shipFell timberQuarry stoneMine oreSaw woodMake coalSmelt ironBuild cabinBuild quarryBuild mineBuild saw-millBuild iron-worksBuild coal-stackBuild dockBuild wharfBuild houseBuild cartBuild trainBuild shipBuild railTimberStoneOreWoodCoalIron-1-2+1+1+1-1-1+1+1-2-1+1-2-2-2-2-2-2-1-2-1-2-1-1-1-2-4-1Table 1: Production consumption Settlers domain1. simple producer produces uniformly: state satisfies preconditions,effect upon v always increase value constant amount, c,irrespective precise details s.2. potential maximum value v attained userestricted value v itself: minimum maximum bounds vmust hold allow production.define bounded producer follows:Definition 2.2 Bounded Producerground action bounded producer numeric variable v iff:effect (increase (v) c) (where c positive constant),precondition (<= (v) (- ub c))preconditions depending v.bounded producer, a, applied v (ub c). Therefore, maximumamount v attained using a, denoted max prod (a, v), ub, achievedapplying state v = (ub c). (In practice, state might reachableactual upper bound value v reachable using might lower ub).simple producer, a0 , assume max prod (a0 , v) = .349fiColes, Coles, Fox & Longdefine bounded consumer follows:Definition 2.3 Bounded Consumerground action consumer respect given numeric variable v iff:precondition (>= (v) (+ lb c))),effect (decrease (v) c), c constantpreconditions depending v.definition analogous bounded producer, since requires v exceedminimum value allowing consumption. consequence, lb minimum amountv attained using a, denoted min cons(a, v) (by applying statev = lb + c).are, course, many resource use behaviours might encodedplanning domains. producer-consumer behaviour identify natural intuitiveone (a producer produces fixed quantity resource consumer consumes fixedquantity depends availability quantity). variantscompiled form (e.g. consumers must leave fixed sized store resourceuntouched simply translate origin resource measurement) also consider,below, possible extensions basic behaviour. Nonetheless, must emphasiseheuristic develop paper targeted producer-consumer behaviourusefulness depends common domains practice. frequentoccurrence model scheduling resources suggests naturaluseful behaviour.2.3.2 ProducerConsumer Variablesdefinitions (bounded) producer consumer actions, define propertiesvariables manipulate:Definition 2.4 ProducerConsumer Variablevariable v denotes resource produced/consumed iff:set prod (v) actions increase value v contains bounded producers,set cons(v) actions decrease value v contains consumers,upper bound v bounded producers vlower bound v consumers v.350fiA Hybrid LP-RPG Heuristic Planning2.3.3 Handling Integer ResourcesConsumer actions (Definition 2.3) require amount resource availableconsumption must least much consumer actually consumes.domain encodings, behaviour essentially consistent producerconsumer patternsrepresented using precondition consumer action consumes c units vform v > k rather v c (and k < c). general case, v <, knowv k + (where epsilon infinitesimal positive), c units v consumed,suggesting lower bound v (k c + ). However, case consumersconsume integral quantities, rewrite strict inequality since must 1case. Consider, example, fragment load action Settlers::precondition (> (available timber l1) 0):effect(decrease (available timber l1) 1)effects change quantities available resources integral,rewritten::precondition (>= (available timber l1) 1):effect(decrease (available timber l1) 1)similar transformation used constant effects variablerational, simply finding least common multiple, LCM , denominatorsfractions involved using = 1/LCM .3. MetricFF Revisitedsection briefly review way MetricFF (Hoffmann, 2003) handles metricfluents highlight weaknesses approach faced particularkinds numeric behaviours planning domains.3.1 Metric Relaxed Planning Graph HeuristicMetric RPG heuristic based performing relaxed reachability analysis forwardsstate evaluated, reachability analysis captured planninggraph (Blum & Furst, 1995) structure. Two elements domain relaxed: deleteconditions actions ignored optimistic upper lower bounds used recordinterval possible values metric fluent may reach. Positive effects metricvariable increase upper bound reachable values negative effects decreaselower bound. Satisfaction preconditions tested checking value intervalvariable satisfies metric condition precondition. interesting notepreconditions tested individually, possible, principle, single value couldsatisfy conditions simultaneously, even though condition separately satisfiedvalue. Conjunctions convex preconditions, includes linear conditions,satisfiable value case condition satisfiable intervalassociated variable, except case conjunction inconsistent,likely arise erroneous domain encodings.351fiColes, Coles, Fox & LongMetricFF allows preconditions combine multiple variables, effects dependvalues variables. lp-rpg allow linear combinations variablespreconditions, effects must conform producerconsumer definitionsallow constant increases decreases.Heuristic evaluation state using Metric RPG heuristic undertaken twophases: graph expansion phase solution extraction phase. remindreader two processes convenience reference discussion follows.3.1.1 Metric RPG ExpansionGraph expansion concisely defined follows:Definition 3.1 RPG ExpansionLet F (i) denote fact layer, comprising:F P (i), set propositions;F V (i), array upper- lower- bound pairs task numeric variable vA(i) denotes action layer, consisting list ground actions. RPG beginsfact layer, F (0), defined based state evaluated:F P (0) contains propositions hold S;entry hLBv , UBv F V (0) set hS[v], S[v]i, i.e. value v S.RPG expanded adding successive action layers, followed new fact layers:Action layer A(i + 1) contains actions A, that:propositional preconditions F P (i);numeric preconditions satisfied values variablesF V (i).Fact layer F P (i + 1) determined A(i + 1):propositions F P (i + 1) F P (i), plus new facts addedaction A(i + 1);values numeric values F V (i + 1) first set F V (i), updatedextending interval variable include values achievedmaximum minimum possible assignment effects, action A(i + 1)turn.termination condition met, RPG expanded actionlayerfact-layer pairs.reachability analysis therefore consists alternate steps: determining actionsapplicable, hence instantiating next action layer, using extendnext fact layer. process presented graphically Figure 1, small problemfacts f0 ...fn numeric variables v0 , v1 . Considering first propositions:352fiA Hybrid LP-RPG Heuristic PlanningFii+1F i+1f0i+2F i+2f0f0f1f1f1f2Bf2Bf2f3f3f4v0[0,0]v0[0,2]2v1[2,2]+2f4>=+22C>=2Cv0[0,4]2v1[0,2]v1[2,2]Figure 1: Portion relaxed planning graph, C produces 2 units v0, consumes2 units v1Arrows fact action layers denote precondition dependencies actionsinstance, action appear layer A(i + 1) f0 present F (i).Arrows action fact layers denote effects instance, f3 F (i + 1)added A.numeric variables v0 , v1 , bounds shown square brackets. Action Cseen one precondition (v1 2) two effects (increase v0 2, decrease v12). C exhibits producerconsumer behaviour consumes 2 units v1 , produces2 units v0 . preconditions satisfied F (i) therefore effects appliedlayer A(i + 1), upper bound v0 increased lower bound v1decreased. Moreover, bounds change layer F (i + 2), possibleapplication C.variable bounds continue diverge way, RPG expansion needswell-defined termination condition. positive case, terminate successfirst layer F (i) goal propositions F P (i) goal numeric expressionssatisfied F V (i) (in relaxed sense). negative case, terminate failureF (i) three following hold:1. actions appear A(i + 1) present A(i) (and hence new propositions would present F P (i + 1)),2. hitherto unsatisfied preconditions v c, action, U B(v) would changeF V (i) F V (i + 1),3. hitherto unsatisfied preconditions v c, action, LB(v) would changeF V (i) F V (i + 1).353fiColes, Coles, Fox & LongAlgorithm 1: Adding action relaxed planData: R metric RPG, action include relaxed plan, q subgoalqueue1 foreach propositional precondition pre2l layer pre first appears;3l > 0 insert pre q[l].prop;456foreach numeric precondition prel layer pre first appears;l > 0 insert pre q[l].num;intuition behind conditions monotonic expansion RPGimplies that, new facts appearing numeric preconditions could become satisfied future layer, graph expansion stagnated relaxed problemunsolvable.3.1.2 Metric RPG Solution Extractionexpanded planning graph found relaxed solution exists (allgoals appeared), next step extract relaxed solution plan. doneregressing planning graph, using priority queue intermediate sub-goals(latest layer first). subgoal, achieving action added relaxed planpreconditions added queue goals achieved earlier layer.relaxed plan extraction algorithm shown Algorithm 2. lines 3 6, priorityqueue initialised top-level goals problem. propositional numericgoals added priority queue achieved earliest fact layerappeared. priority queue seeded, solution extraction proceeds regressinglayer-by-layer. propositions, suffices find action adds factincrement heuristic value one add preconditions achieving actionqueue, using Algorithm 1. numeric preconditions, process slightlyinvolved:subgoal achieve v k v k layer F (l), action A(l)assigns value k v, action chosen satisfy subgoal.Otherwise, v k (or v k) must achieved layer F (l), actions increasing(decreasing) v chosen A(l) residual value k (i.e.original value k adjusted take account effects selected actions)small enough (large enough) reachable layer F (l 1). residual conditionv k (v k) added queue subgoal achieved F (l 1),modified value k.Note lines 13, 19, 25, 32 39, actions chosen action layer 1recorded adding set ha. used basis helpful action set:action effect common action set ha considered helpful. Helpfulactions important element performance MetricFF: actions achieveeffects exploited relaxed solution state promoted searchstate.354fiA Hybrid LP-RPG Heuristic PlanningAlgorithm 2: Relaxed plan extraction5Data: R - metric RPG; F ? , N ? - problem goalsResult: ha - helpful actions, h - heuristic valueha , h 0;q deepest-first priority queue goal layers;foreach p F ?l layer p first appears;insert p q[l].prop;678foreach f N ?l layer f first holds;insert f q[l].num;9q empty(l, hprop, numi) pop(q);foreach p proph h + 1; achiever p;action layer 1 add ha;prop prop \ add effects a;call Algorithm 1 R, a, q;1234101112131415161718192021foreach (v c) numaction A(l) assigned v = k, k ch h + 1;l = 1 add ha;call Algorithm 1 R, a, q;remove (v c0 ), c0 k (v c0 ), c0 k num;2223242526foreach (v c) numaction A(l) assigned v = k, k ch h + 1;l = 1 add ha;call Algorithm 1 R, a, q;remove conditions (v c), c k num;272829303132333435363738394041foreach (v c) numF V (l 1)[v].upper < ch h + 1; next increaser v;decrease c (v, a);l 1 add ha;call Algorithm 1 R, a, q;l > 0 insert (v c) q[l 1].num;foreach (v c) numF V (l 1)[v].lower > ch h + 1; next decreaser v;increase c (v, a);l 1 add ha;call Algorithm 1 R, a, q;l > 0 insert (v c) q[l 1].num;355fiColes, Coles, Fox & Long3.1.3 Use Relaxed Plan Searchrelaxed plan, computed heuristic calculation, used two ways search.MetricFF makes use two-stage search approach, number actionsrelaxed plan used heuristic goal-distance estimate. first search phase,enforced hill-climbing (EHC), greedy hill climbing search approach. thoughtperforming breadth first search forward initial state I, state progressionaction application, state new global-best heuristic value found.state, S, found, states discarded EHC search continuesmanner S. search strategy incomplete due greedy nature,discarding states could lead loss solution. such, followedcomplete WA* search, order guarantee completeness planner (subjectsufficient time memory).Since EHC phase already incomplete, designed find solutions quickly,MetricFF makes use another completeness-sacrificing technique order attemptguide planner solutions quickly. technique referred helpful actionpruning. Here, actions helpful action set state consideredsuccessor generation: actions helpful discarded. Note pruningused best-first search would compromise completeness. practice helpfulaction pruning improves performance MetricFF many domains, however,lead difficulties actions needed find solution given state appearhelpful action set. attempt compensate this, EHC terminatesconsidering helpful actions, returns state last global-best heuristicvalue, searches considering applicable actions, either terminates(leading WA*) state new global-best heuristic value found,point helpful-action pruning re-enabled, EHC continues.3.2 Problems Metric RPG HeuristicAlthough Metric RPG powerful tool support planning metric fluents,common situations planning problems heuristic gives flawedguidance. problems include resource persistence cyclical resource transfer.discuss phenomena result misleading heuristic guidancerelaxed plan length poorly approximating actual solution length helpfulaction distortion.3.2.1 Resource PersistenceResource persistence consequence using relaxation ignores negative effects.resource consumed disappear relaxation negative effects.opportunity reuse resource suggest significantly shorter planavailable case reality. Although problem occurs propositionalmetric fluents, fact metric fluents commonly encode resources mustcarefully managed means problem often acute domains metricfluents. example, state Settlers domain 2 units resourceproduced ship required (either goal means travel356fiA Hybrid LP-RPG Heuristic Planningotherwise inaccessible location), relaxed plan require productionresources (see Table 1).One approach approximating number missing resource production actionsintroduced planner Sapa (Do & Kambhampati, 2001). v = stateevaluated, relaxed plan consumes c units v, produces p,(c p) > s, shortfall production v, would necessitate additionalactions added relaxed plan. case, maximum amount vproduced single action v, heuristic value increased by:cps.vincrease lower bound number additional actions needed and,indicate actions might be, serves increase heuristic value states whoserelaxed plans resource production shortfalls. does, however, two main limitations. First, relaxed plan extraction (such approach shown Algorithm 2) choosesactions without consideration undesirable resource consumption side-effects.good search choice might lead state worse heuristic value purelyaccident choice achieving actions (consuming resource unnecessarily). Second,increasing heuristic value without adding specific additional actions relaxed plan,helpful actions record fact appropriate resource production helpful.3.2.2 Cyclical Resource Transferphenomenon Cyclical Resource Transfer (CRT) consequence encodingactions move resources around, combined relaxation negative effects.encode movement resource, removed one location added another.removal encoded decrease relaxed building Metric RPG.result, moving resources appears generate new resource destination, makingmovement spuriously attractive alternative production. Consider state 1unit timber cart location, p1, goal 2 units timberp1. Clearly, solution plan must involve production timber. However,valid relaxed plan solution, found using Metric RPG heuristic, is:0:1:(load v1 p1 timber)(unload v1 p1 timber)3.2.3 Helpful Action Distortionproblems resource persistence CRT important consequence EHCsearch. result relaxed plans misleadingly short lengths,relaxed plans typically contain production actions useless transfer actions.situation, production actions often appear helpful action set,therefore included EHC search, even states could convenientlyapplied. refer problem helpful action distortion. illustrate arises,reference Settlers domain, consider state unittimber location a, goal unit timber location b. relaxedplan use timber construct cart load timber onto carttransport it. planner therefore consider producing timber.357fiColes, Coles, Fox & Long4. Compiling ProducerConsumer Behaviour MathematicalProgramsection describe mathematical program built characteriseinteraction numeric variables action choices producerconsumer framework.4.1 Constraints ProducerConsumer Variables Actionsdefinition producerconsumer variables (Definition 2.4) implies actionsuseful property preconditions variables derived effectsactions, together global variable bounds. Specifically, action a:produces c units v precondition requiring v d, = ub(v) c,ub(v) global upper bound v applied. conditionexpresses effect upper bound precondition valuev (since v ub(v)).consumes c units v v must satisfy v lb(v) + c actionapplied. Again, expression leads effect precondition tiedone constraint.ordering actions relaxed (that is, causal relations forceordered ignored) value v series actions applied, v 0 ,given by:Xv0 = v +Ca .(v, a)(1)aACa non-negative (count) variable indicating many times actionapplied, v 0 [lb(v), ub(v)] (v, a) defined follows:produces c units v (v, a) = c;consumes c units v (v, a) = c;Otherwise, (v, a) = 0.Note equation linear, since Definitions 2.2 2.3 require (v, a) constantv a.equations support construction mathematical program consisting onevariable action, a, one variable flow equation producerconsumervariable, v. program is, fact, mixed integer program (MIP), variables(the action counts, a) represent applications actions, integral. However,relaxation exploited allow action count variables take non-integralvalues, yielding linear program (LP). significant potential benefitLPs solved far efficiently MIPs.358fiA Hybrid LP-RPG Heuristic Planning4.2 Bounding Action VariablesWithin equation associated state variable (i.e. Equation 1), actioncorresponding variable denoting many times applied. generallimit number times action applied. However, numeric decreaseeffects propositional delete effects may impose constraints practice, due limitedavailability resources. example, applying action increases v expensedecreasing w, w resource producer, limitCa implicit consequence instance Equation 1 governing value w.Specifically, ai never exceed w/(w, a): value w divided change causesw. Moreover, w monotonically decreasing (w, a) constant actiona, global upper bound Ca set w(I)/(w, a), w(I) value winitial state, I.action increases v expense irreversibly deleting fact p, factprecondition, clearly applied one-shot action (Coles,Coles, Fox, & Long, 2009). However, contrast numeric delete effects discussedabove, captured producerconsumer constraints concernednumeric change. However, constraint use capturedsetting upper bound Ca 1. Moreover, collection actions a0 ...an1depend fact p irreversibly delete it, say form one-shotaction set p, add constraint:Ca0 + Ca1 + ... + Can1 1(2)4.3 Assignment Constraintsgeneral, direct assignments values variables cannot represented directly constraints following form Equation 1. Assignments correspond, effectively, statedependent increases decreases. instance, assignment value 2 variablev, state v = 0, equivalent producing 2 units, equivalent consuming1 unit applied state v = 3. However, producerconsumer equationsnotion state, coefficients action variables denote productionconsumption. Therefore, state variables subject constant-valued changeMIP cannot used encode general assignment effects without extendingallow quadratic constraints (that is, constraints involving products pairs variables).However, specific conditions assignments safely modelledwithin mathematical program retaining linearity constraints.One class assignment effects encoded MIPactions assignment effects variable, v = k, applied statesv = c known constant, c. case, effect rewrittenincrease effect v k c, making assignment actions follow standard patternproducer/consumer actions. particular case rewriting made possiblefollowing conditions hold set actions A:1. action depend upon affect v condition satisfiedachieved actions actions A.2. Applying action precludes assignments v.359fiColes, Coles, Fox & Longconditions ensure set actions assign v form one-shot actionset value v assumed 0 prior application one actionsset, action set rewritten increase v assignment value.situation one arises encodings objects created certain actionsobjects associated metric variables initialised object creation(such capacity newly created vehicle Settlers domain).5. Linear ProgrammingRelaxed Planning Graph Heuristicdefined producerconsumer behaviour shown supports constructionMIP action ordering relaxed. MIP is, principle, hard solve (constructingsolutions NP-hard), basis heuristic evaluation states seems sensiblerelax integrality constraints action variables reduce problemlinear program. first consider LP used two stages heuristicevaluation state (reachability analysis relaxed plan extraction)reconsider question whether relaxation linear program necessary practicecompromise might full MIP LP.5.1 Overviewcontext use LP forward state-space search planner. taskintend use heuristic evaluation states. Thus, assumestate (a complete assignment variables define problem,propositional numeric) interested estimating number actionsrequired transition goal state. approach use basedstrategy used MetricFF: first construct reachability analysis using layered graphalternating facts actions extract relaxed plan. determine whetheraction applied reachability stage check propositional preconditionsusual way numeric preconditions checked determining whether valuesreachable ranges recorded metric variables satisfy condition.reachable ranges calculated using LP described, explain below.Extraction relaxed plan involves determining actions support requiredconditions (both goals preconditions selected actions), conditionsinvolve numeric variables use LP decide actions requiredmany used.5.2 Using LP Graph Expansiongraph expansion phase calculation Metric RPG heuristic seenlayer-by-layer relaxed propositional reachability analysis, synchronised relaxednumeric bounds analysis. Definition 3.1 shows numeric variable bounds appeargraph expansion algorithm two places. First, F (i) used determineactions appear A(i + 1), next action layer (those whose preconditions withinreachable range). Then, actions deemed applicable used update variablebounds subsequent fact layer, F (i + 1).360fiA Hybrid LP-RPG Heuristic Planningmaximise: v00v00 = 0 + 2.CCv10 = 2 2.CCv00 0v10 0C0v0011v101CC-22==111(a) Equationsmax02000v00 [0, 2]v10 [0, 2](c) Solutions min/max(b) RowColumnFigure 2: LP maximise value v0 layer F (i + 2) Figure 1 (treating layer Fiinitial state construction)Due relaxed nature way numeric values considered, metric RPGtends produce highly optimistic bounds numeric values. Returning Figure 1,see that, effect, action C converts two units v1 two units v0 and,initially, two units v1 present. Hence, most, could hope produce twov1 , F V (i + 2), upper bound v1 already 4. Ignoring consumption effectC makes possible produce arbitrary amounts v0 : C applicable upperbound v1 greater equal two, reality true long Cyet applied.accurate estimate variable bounds layer F V (i) found usingLP encoding described Section 4.1. model parameterised follows:action variables corresponding actions A(i) used, ensuresreachable actions considered computing resource bounds. relevantone-shot action constraints included. absence restriction numberaction applications contrasts constraint used Metric FF actionmay applied per action layer. practice prevent LP variablesbecoming unbounded, set finite, large maximum value action variables.initial value variable v set state, S, evaluated.post-value variable, v 0 , F V (i)[v], range values could reachF V (i). always, v 0 [lb(v), ub(v)].Substituting parameters producerconsumer equation (Equation 1) yields:XF V (i)[v] = S[v] +Ca .(v, a)(3)aA(i)model, use LP solver find upper lower boundsF V (i)[v], setting objective function accordingly.Returning example, consider finding upper bounds variables F (i+2)Figure 1 starting state corresponding one given Fi figure (thus,considering constraints layer 2 following state treating startingpoint). corresponding LP shown Figure 2, primed variables ones361fiColes, Coles, Fox & Longuse represent values numeric variables layer interest (i.e.case layer 2 ahead state evaluated). Maximising v00 , i.e. using upperbound F V (i)[v0 ], yields result 2: greater value possible, since setting Cvalue greater 1 (and thus producing v0 ) would lead violation constraintv10 0. ranges variables computed, using LP four times (minimisingmaximising two variables), shown Figure 2c. seen,improve bounds calculated situation MetricFF (Figure 1),v00 [0, 4] v10 [2, 2] respectively.5.2.1 Notes LP EfficiencyLP solved twice per variable per layer expanding planninggraph, important steps taken minimise computational cost. reducecosts using combination techniques, avoid needing solve LPcomputing bound given variable, others reduce cost solvingLP itself.1. First, consequence termination criteria RPG expansion (Section 3.1.1),need compute upper (lower) bound given variable currentvalue large enough (small enough) satisfy preconditions goalsappears. case, avoid solve LP determine variablebound, instead re-use bound computed previous layer, withoutaffecting behaviour heuristic.2. variable never appears numeric precondition goal, entirelyexcluded LP.3. bounds variables change monotonically additional layers addedplanning graph. Therefore, computing new upper (lower) bound variable v temporarily add LP constraints corresponding boundscomputed preceding layer (each added separately variable first minimised maximised). so, refuse admit tighter variable boundprevious layer.4. Finally, actions increase (decrease) effect variable yetadded LP, need compute upper (lower) bound variable,effect bound variable increased (decreased)beyond value state evaluated.5.3 Basic Use LP Solution Extractionconsider LP used give guidance action selection relaxedplan extraction. First, observe LP directly affected propositions,hence cannot used find actions able achieve given fact. Thus,concern LP used identify actions use attainnumeric subgoals either top-level numeric goals, numeric preconditionsactions chosen solution extraction.362fiA Hybrid LP-RPG Heuristic PlanningAlgorithm 3: Adding weighted action relaxed plan123456789101112Data: R metric RPG, action include relaxed plan, q subgoalqueue, w weightforeach propositional precondition prel layer pre first appears;l > 0(pre, k) q[l].propk < w k w;else insert (pre, w) q[l].prop;foreach numeric precondition prel layer pre first appears;l > 0({pre}, k) q[l].numk < w k w;else insert ({pre}, w) q[l].num;first key difference using LP relaxed plan extraction concernschoice actions achieve numeric preconditions. original metric RPG heuristic,given numeric precondition (e.g. x c) fact layer + 1 regressedbeneficial numeric effects layer i, giving residual numeric precondition (e.g. x c0 )achieved fact layer i. (This process shown lines 28 41 Algorithm 2).lp-rpg case, shown Algorithm 4, numeric precondition layer l (temporarily)added LP generated layer l (as constraint (line 22). find actions useachieve this, LP solved (line 23), objective minimise weightedsum across action variables (one possible weighting scheme sufficepurpose minimise sum action variables, though return questionappropriate weighting schemes later paper). Finally, actions whose correspondingvariables non-zero (line 24) added relaxed plan (lines 25 34).Second, must accommodate fact LP, relaxation underlyingMIP (in action variables integers), may solved applying actions nonintegral number times. simple example, several actions incrementgiven variable, alone would suffice achieve goal value variable,valid optimal solution LP sum variables corresponding actions 1. every action non-zero action count variablesolution considered applied relaxed plan length could greatly over-estimaterequired number actions. mitigate problem, subgoal (e.g. x c) arisingsolution extraction associated weight, weights, alongvalues given action variables LP, used update relaxed plan length.weights manipulated throughout Algorithms 3 4, use summarisedfollows:Initially, goal fact added subgoal queue associated weight 1, i.e.achieved, entirely. Also note that, contrast Algorithm 1,363fiColes, Coles, Fox & LongAlgorithm 4: Relaxed plan extraction LP12345Data: R - metric RPG; P G - propositional goals;N G - numeric goalsResult: ha - helpful actions, h - heuristic valueha , h 0;q deepest-first priority queue goal layers;foreach p P Gl layer p first appears;insert (p, 1) q[l].prop;|N G| > 1l final layer R;8insert (N G, 1) q[l].num;679101112elsef fact N G;l layer f first holds;insert ({f }, 1) q[l].num;q empty(l, hprop, numi) pop(q);foreach (p, w) proph h + w;achiever p;action layer 1 add ha;19call Algorithm 3 R, a, q, w;20prop prop \ add effects a;1314151617182122232425262728293031323334foreach (G, w) numLP LP(l) + constraint(s) G;solve LP, minimising weighted action sum;av {action variable (a = c) LP | c 6= 0};foreach avh h + w.c;layer 1 add ha;foreach propositional precondition prel layer pre first appears;c min[c, 1];(pre, k) q[l].propk < w.c k w.c;remove (pre, k) q[l].prop;else insert (pre, w.c) q[l].prop;subgoal queue records layer goal introduced wellassociated weight.action chosen applied Ca times achieve queued propositional/numeric sub-goal g, either:chosen applied (i.e. Ca = 1) achiever propositionalsubgoal g, associated weight w;364fiA Hybrid LP-RPG Heuristic Planningaction applied (given non-zero value Ca ) solving LPachieve numeric subgoal g, associated weight w.cases, relaxed plan length incremented Ca .w.weight given preconditions w0 = w.min[Ca , 1]. weightspreconditions used update weight attached achieving corresponding sub-goals earlier layers:propositional precondition p already recorded subgoal earlierlayer, weight k, weight updated max[k, w0 ]. Otherwise,p added subgoal RPG, satisfied first layerappears, weight w0 .case added support propositional sub-goal: numericprecondition p already recorded subgoal earlier layer,weight k, weight updated max[k, w0 ]; otherwise, p addedsubgoal RPG, satisfied first layer appears,weight w0 .5.4 Consequences Use LP Solution ExtractionUse LP aid identification selection actions support achievementnumeric goals subgoals extraction relaxed solution leadimportant consequences heuristic guidance offered relaxed solution.already noted problem non-integral fragments actions might combinedachieve numeric effects indicated managed handling fractionalpreconditions fractional action costs. However, potential problemsdiscuss.5.4.1 Partially Applied Helpful ActionsConsider situation precisely five possible ways achieve particularnumeric goal, uses three actions. simple example smallproblem Settlers domain, five carts unit timber availablelocation A: goal one unit timber location B achieved loadingtimber onto carts A, moving B, unloading B.metric RPG heuristic used achieve goal, three actions would used.Working backwards goal, selected actions would be:action layer three, unload action (from cart c) increase amount timberB;action layer two, action move cart c B;action layer one, action load unit timber onto cart c.solving LP achieve goal (ignoring propositional preconditions),objective minimising sum action variables, solution returned365fiColes, Coles, Fox & Longobjective value 2. denote relevant load/unload action variable paircart (li , ui ), pool solutions could returned satisfying:li = uii[1..5](X(li + ui )) = 2i[1..5]Then, non-zero variable ui = k relevant action move cart Balso added relaxed plan, weight k (line 19, Algorithm 4).purposes providing contribution relaxed plan length, unimportantsolutions returned: sum action variables 2,sum move actions added 1, giving total relaxed plan length 3. However,discussed Section 3.1.2, relaxed plan also used determine set helpful actions:effect common actions relaxed plan chosenaction layer one. example, action layer one consists action (or actions)load unit timber onto cart A. original metric RPG case, exactly one actionwould used. However, using LP five actions could (fractionally) used.consequence within pool LP solutions could returned,lead search much greater branching factor, case factorfive greater.source problem relaxation integrality constraints actionvariables extent affects search depends precise solution returnedLP solver: different solvers may greater lesser tendency return solutionsaction variables assigned non-integral values. extreme responseproblem would revert MIP require action variables integral ratherreal-valued. Alternatively, focussing issue identified here, one could requireaction variables corresponding actions action layer one integers. eithercase, result mixed integer programming problem, since cost MIP-solvingexponential number integer variables, difference variantssignificant. change need made point switch usingLP solution extraction rather graph expansion as, prior this, assignmentsaction variables unimportant (only value objective function used).course, price pay potentially significant, MIP-solving NP-hard,LP-solving polynomial. However, exchange shift complexity,example given above, left single helpful action, longer possiblefractionally load timber onto cart: one cart must chosen. extenttwo possible integer-modifications affect search performance considered laterevaluation.5.4.2 Preferring Earlier ActionsWithin Metric RPG heuristic explicit preference using actionsappear earlier relaxed planning graph. shown Algorithm 1, factneeded, either goal satisfy precondition action chosen insertionrelaxed plan, queued sub-goal satisfied first fact layerappeared. Then, action chosen support fact, amongstearliest possible achievers. intuition behind preference earlier actions based366fiA Hybrid LP-RPG Heuristic Planningobservation that, later action appears relaxed planning graph,greater number actions need added relaxed plan supportpreconditions. Therefore, preferring earlier actions usually leads shorter relaxed planshence closer approximations optimal relaxed plan length.Within LP, objective set minimise sum action variables (i.e. useactions possible), distinction actions appear earlierRPG appear later. Recalling LP disregards propositionalpreconditions actions, failing take account action first addedRPG lead LP-based relaxed plan extraction generating poor quality solutionsand, consequently, poor search guidance.address this, pressure generated within LP objective functiontuned prefer actions need fewer supporting actions relaxed plan.achieved forcing LP favour actions appear RPG earlier. encodepreference earlier actions weighting scheme action variablesobjective function. Actions appearing earlier given smaller weightsappear later. propose (and, later, evaluate) two ways achieving this. first,simpler, use geometric series dictate coefficient given action variablebased layer l first appears. case, objective coefficientis:klk < k > 1.value k controls extent earlier actions preferred, interpreted treating k.n actions selected layer l exactly good selecting n actionslayer l + 1 (so anything less k.n actions layer l preferable selectingn actions layer l + 1). Throughout remainder paper, refer schemelayer-weighting value k.second option record, cost action, estimate numberactions needed support propositional preconditions, use weightobjective function. achieved using RPG cost propagation algorithmSapa (Do & Kambhampati, 2003). achieve this, planning graph expanded,costs fact action recorded updated layer. Initially, factp state evaluated, cost fact layer zero, cost(p, 0), zero. (For factp true time zero, cost(p, 0) = .) fact costs used derive actioncosts, using rules akin used hadd /hmax (Bonet & Geffner, 2001). costaction layer t, cost(a, t) defined according one either:cost max (a, t) = max cost(p, 1)ppre(a)cost sum (a, t) =Xcost(p, 1).ppre(a)action costs, turn, used update costs propositionsubsequent fact layer, cost fact reduced cheaperway achieve it. action layer t, potentially reduce costpropositions p adds:cost(p, t) = cost(a, t) + 1 iff (cost(a, t) + 1) < cost(p, 1)= cost(p, 1) otherwise367fiColes, Coles, Fox & Longplanning graph expanded, process alternating action cost estimation,fact cost estimation, used propagate cost information RPG.setting objective LP, use cost action, cost(a, t), coefficientaction variable corresponding a. Using cost propagation described,costs derived solely basis propositional preconditions effects. Therefore,cost(a, t) estimate number actions needed support preconditionsa. purposes, desirable: LP will, itself, add actions support numericpreconditions, estimate number actions needed support propositionalpreconditions action measure cost impact upon relaxed plan dueaction selected.6. Adding Propositions LPBenton et al. (2005) explore idea using LP guide search propositional planningproblems context over-subscription planning. work LP useddetermine goal subset achieve, gain maximum utility. Whilst successfulachieving aim, authors observe use LP heuristic guide searchexpensive, indeed expensive feasible. propositions encodedLP way propose, task solving LP becomes equivalent solvingentire planning problem relaxed action ordering non-integer action variables.section, reconsider inclusion propositions LP, considering spectrumpossibilities including propositions way include propositions.Even though focus work numeric problems, including propositionsLP might still interest. instance, supporting propositional goal mightrequire consumption numeric resources. worst case, one could compileproblem numeric goals become preconditions action achievesdummy propositional fact goal, modify problem goal goal. Sincenumeric goals modified problem, LP used solveindividual preconditions dummy action (inevitably) chosen, ratherrequiring goals satisfied conjunction, described Section 7.1. Sincedummy-goal model merely reformulation original problem, informationtheoretically accessible conveyed LP. generally, hopeidentify intermediate landmark propositions, well final goals, could usefullyencoded LP.6.1 Adding Propositional Goals LPAlthough LP describe Section 4 contain specific reference propositions, hence propositional goals, formulate constraints act proxythem, considering actions achieve them. need introduce additionalvariables. Instead, add constraints ensure least one achiever chosenpropositional goal. goal fact g true state evaluated,list actions [a0 ..an1 ] achieve g, add constraint LPrecent layer planning graph:a0 + ... + an1 1.368fiA Hybrid LP-RPG Heuristic Planningis, least one achieving action must used or, specifically, given actionspartially applied, total least one achieving action must used. LPcontaining constraints used augment positive termination criteriagraph expansion, detailed Section 3.1.2. terminate first fact layer where:1. goal propositions F ? F P (i) (as before);2. goal numeric expressions N ? satisfied (individually) F V (i) (as before);3. LP used compute numeric bounds layer F (i) still solvableconstraints propositional goals added.Use goal-checking LP two key consequences. First, actions layerF (i) cannot used satisfy goals whilst respecting numeric constraintsLP, additional layers added planning graph necessary actionsappeared (or termination criterion reached). Thus, reasoning resourcepersistence (Section 3.2.1), heuristic better able recognise cases where, although propositional goals might appear individually reachable, either additionalproduction needed meet collectively, alternative actions need used,state dead-end. Second, solution LP used confirm point (3) above, usedindicate actions add relaxed plan achieve propositional goals.propositional preconditions actions satisfied usual way (line 28Algorithm 4).requiring sum action variables selected achieve goalsleast one, allowing variables real-valued, LP could, theory, provideweaker guidance RPG. similar issue noted Section 5.4.1considering helpful actions, could ameliorated similar manner, namely makinggoal-achieving action variables integral. return issue evaluation,considering whether benefits search.6.2 Using Landmarks LPlandmark fact (Hoffmann, Porteous, & Sebastia, 2004) propositional fact musttrue point every solution plan given planning problem. first worklandmarks (Porteous, Sebastia, & Hoffmann, 2001) proposed method extractingsubset landmarks planning problem based regressing goals usingdelete relaxation FF. Since introduction idea 2001 (Porteous et al., 2001),landmarks come play increasingly important role planning. Recent development new techniques extracting landmarks (Richter, Helmert, & Westphahl, 2008;Zhu & Givan, 2003) development heuristics based different relaxations (Richter& Westphal, 2010; Domshlak, Katz, & Lefler, 2010; Helmert & Domshlak, 2009; Karpas &Domshlak, 2009) allowed planning community exploit landmarks successfully.relaxed plan extraction phase lp-rpg heuristic relaxes action orderingpropositional preconditions effects might benefit substantially delete-relaxationlandmarks. use landmark facts LP offers mechanismtightly couple LP RPG, allowing increased information sharing369fiColes, Coles, Fox & Longpropositional numeric components heuristic. know landmarkfact must occur solution plan, yet appeared path stateevaluated, add constraints representing LP,propositional goals. is, sum action variables [a0 ..an1 ] achievinggiven landmark must greater equal 1. propositional goals, constraint introduces need provide numeric support action(s) chosen supportlandmark. Goals special case landmarks, important feature goalsthat, even achieved path current state, truecurrent state must reachieved. Constraints added LPensure this. reflect landmarks achieved path current state,state modified record record updated new landmarks seen.approach similar lama (Richter & Westphal, 2010).set disjunctive landmarks set propositional facts, one musttrue solution planning problem. extraction disjunctive landmarksconsidered (Gregory, Cresswell, Long, & Porteous, 2004) even difficultcase conjunctive landmarks exploit planning systems.knowledge certain fact must true allow planner infer certain actionsmust present solution plans, inform heuristics. However, disjunctivelandmarks less informative. Disjunctive landmarks often arise problem symmetry.example, might know order deliver package one place anotherloaded truck, truck. disjunctive landmarkpackage truck generated: even know truck use,know one disjunctive landmarks must hold. Thus, context numericresources, truck must fueled (assuming start empty), might entailadditional costs. therefore interest take account disjunctive landmarksnumeric reasoning.able make use disjunctive landmarks LP constrainproblem, ensuring support given least one fact within (unreached) disjunctive landmark. dealing standard conjunctive landmarks, constraintsum least one achiever must added propositional landmark.disjunctive landmarks, however, constraint slightly different. disjunctive landmarkset L satisfied constituent landmarks satisfied. is, applyactions:achieves(L) = {a | eff + (a) L 6= }.encode disjunctive landmarks LP (assuming yet met)two possibilities. first add binary variable sf fact f L,constraints sf take value 1 least total one actionadding f , least one variable sf take value 1. is, leastone disjunctive landmarks toPfully met. alternative, potentially cheaper,approach add constraint ( achieves(L)) 1. allows disjunctivelandmark considered satisfied sum across action variables supportingconstituent facts least 1. somewhat weaker constraint individual,non-disjunctive landmarks, guarantee support least 1individual constituent fact. instance, two-fact disjunctive landmark satisfiedsupport constituent fact 0.5. considered approaches370fiA Hybrid LP-RPG Heuristic Planningfound negligible difference performance (time taken nodes expanded)two encodings real saving achieved using relaxed approach.6.3 Managing Propositional Preconditions Effectsfar, considered propositions must achieved planning problem duegoals landmarks. However, second class propositions: that, givenvalues assigned action variables LP, must also supporting actions addedsolution relaxed plan.extend LP capture propositional preconditions effects, first introducebinary variable f (an integer whose value 0 1) fact f truestate evaluated. involved two constraints. First, actions+[a+0 ...an1 ] add f :+a+0 + ... + an1 f.case proposition corresponding f goal, f = 1 hence oneachieving actions must positive value (since constraint expressed usingcontinuous variables, actions might partially applied relaxation). Then,actions [ap0 ..apm1 ] f precondition:N.f ap0 + ... + apm1use N denote (sufficiently) large number. constraint ensuresleast one actions depending proposition corresponding f used relaxedplan, f must positive (that is, corresponding proposition required truewithin relaxation). use N ensure f = 1 sufficient satisfypreconditions many actions. pair constraints effectively conditional versionconstraint meet propositional goal, described Section 6.1. casesproposition neither goal landmark, constraints serve enforce leastone action adds f chosen (has positive value) LP action requiring fchosen even partially.36.4 Recognising Propositional ResourcesFinally, consider one case potentially useful model propositionsequivalent numeric form. pddl, finite domain integer resources modelledtwo ways: numeric variables, set propositions. Consider following twoformulations fell-timber action Settlers Domain (for simplicity effectmetric tracking variable labour omitted):(:action fell-timber:parameters (?p - place):precondition (has-cabin ?p)3. tempting consider replacing constraint, slightly troublesome N , constraintsform f api i. Unfortunately, appropriate action variable, api ,greater 1 (due multiple applications action) yet f = 1 sufficient satisfyprecondition action applications.371fiColes, Coles, Fox & Long:effect (increase (available timber ?p) 1))(:action fell-timber:parameters (?p - place ?n0 ?n1 - value):precondition (and (has-cabin ?p)(timber ?p ?n0)(less-than ?n0 ?n1)):effect (and (not (timber ?p ?n0))(timber ?p ?n1)))representations models situation, uses different mechanism so. first uses numeric variables, second uses propositions.using either numeric propositional formulations MetricFF heuristic,little practical difference guidance given. numeric case, fell-timberaction applied, upper bound amount timber place p increased.means action consuming amount timber executed subsequentlayers, regardless many actions using resource also applied.propositional case, delete effect timber (i.e. deleting factpreviously some) also relaxed so, again, number actions requiring amounttimber applied.Turning attention lp-rpg heuristic, however, observe althoughRPG part heuristic exhibits weakness propositional case,different relaxation used LP numeric reasoning means consumptiontimber would disregarded. LP relaxes action ordering, rather deleteeffects (or production/consumption effects), resource modelled numerically,interaction captured accounted for. therefore interests usinglp-rpg convert resources modelled propositionally numeric formulation,reasoned LP, rather RPG.Although formulation resources example instance commonidiom used capture numeric resources propositional encoding, situationsnatural model resources propositionally outset. oftencase binary resources: resources either present not. resources are,course, special case general resource model described above. Considerpropositional numeric counterparts action switch water pump:(:action activate:parameters (?p - pump):precondition (off ?p):effect (and (not (off ?p))(on ?p)))(:action activate:parameters (?p - pump)372fiA Hybrid LP-RPG Heuristic Planning:precondition (<= (pumping ?p) 0):effect (increase (pumping ?p) 1))Corresponding actions similarly created switch pump (the fact (on?p) deleted (off ?p) added, equivalently unit (pumping ?p) consumed).many senses, natural formulation action first, using propositions.way binary resources encoded benchmark domains. However,second formulation equivalent (assuming value (pumping ?p) initial state1 0). interaction propositional resource resourcesidentified planning problem, little motivation add LP, sincenumeric support required. case binary resource impact anothernumeric variable is, shall see, efficient model numeric resources.Suppose water pumps control flow water. Two ways modelpddl shown below:(:action activate:parameters (?p - pump):precondition (off ?p):effect(and (increase (water-flow) 1)(not (off ?p))(on ?p)))(:action activate:parameters (?p - pump):precondition (<= (pumping ?p) 0):effect (and (increase (pumping ?p) 1)(increase (water-flow) 1)))first two actions switches pump (a binary, propositional resource)produces unit (water-flow). actions domain preconditionswater flow, action run water wheel precondition ( (water-flow) 3)interaction propositional numeric variables problem.use first model action, RPG capture propositional partaction (whether pump off) LP encode numeric partaction. Since RPG relaxes delete effects represent fact (off ?p)longer true and, hence, prevent pump switched many times.LP built using first formulation, action activate consume numericresources, used arbitrarily often increase water flow factswitching necessary achieve increase ignored. Thus, mixing propositionsnumeric resources action degrades information available LP.Using second formulation, state pump appears LP variable,use activate deactivate denote actions activating deactivating373fiColes, Coles, Fox & Longpump, constraints pumping 0 variable are:pumping 0 = init + activate deactivatepumping 0 0pumping 0 1clear activate action applied once: applieddeactivate must applied, corresponding effect water-flow, ordersatisfy last constraints. provides useful guidance, indicateswater flow cannot reach 3 units using actions: actions control pumps,means increasing flow, need added LP, expansionplanning graph. means attain sufficient flow found, dead-enddiscovered would otherwise wasted search effort.Static-analysis techniques capable identifying propositional resources developed (such tim system described Long & Fox, 2000). usedpreprocessing stage recognise propositional resources planning domains translateequivalent numeric resources. use translation approach recognisedresources, resulting numeric preconditions effects included LPway numeric variables. so, LP order-relaxation ratherRPG delete-relaxation used compute heuristic values, preventing impossible reuseresource cases described.7. Extending Scope Numeric Reasoning LPSection 4 discussed LP encoding captures producerconsumer behaviouractions, used first version LPRPG (Coles et al., 2008). section discussencoding enhanced, use numeric information representingstructure planning problem, improve guidance resulting heuristicprovide planner. address two key issues here: ensuring conjunctionsnumeric goals satisfied, considering issue fractionally applied actionsLP.7.1 Checking Numeric Goals alongside Propositional GoalsSection 6.1 noted constrain LP finds actions achievepropositional goals. extend further, capture numeric goals, N ? , addingnumeric goal directly LP constraint. propositional goals,clear advantages terms resource persistence (Section 3.2.1), insisting goalssimultaneously achievable, rather individually achievable. Additionally, though,raises expressive power numeric goals able handle anythingexpressed Linear Normal Form (LNF) LNF formula addedLP constraint.7.2 Catalytic Resourcesfar considered numeric variables conform producerconsumer behaviour. However, another related class variables also expressed374fiA Hybrid LP-RPG Heuristic PlanningLP similar way producerconsumer variables. variables represent resources must present order action applied,consumed. resources used support many actions4 . exampleresource catalyst chemical reaction. catalyst must either createdreactions, bought, present, enables reactions, allows takeplace quickly. catalytic reactions, resource must present,consumed, though could non-catalytic reactions may consume resource.Another example one might consider building unit plant, supportprocess. unit must order process occur, builtused many times enable actions without necessitating destruction. Oftenplanning problems, presence structures represented propositions,need case. many indistinguishable processing units present, severalunits catalyst needed, often makes sense represent numerically5 .extend lp-rpg heuristic provide guidance problems,actions require v c affect value v, encounter difficultyLP encodes notion time: ordering actions relaxed, impossible ascertainvalue v specific time order determine whether (catalyst) preconditionsatisfied not. therefore add additional constraints LP determine upperlower bounds v obtained optimistic pessimistic possible orderingactions whose variables non-zero. find optimistic upper lower boundv, v v respectively, add constraints:Xv = v +a. max((v, a), 0)(4)aAv = v +Xa. min((v, a), 0)(5)aAupper bound equivalent ordering production actions consumptionactions. lower bound, reversed, equivalent consumption actionsordered production actions. bounds computedpossible values resource variables reachability graph,considering actions actually selected execution relaxed plan,rather could possibly applied. seen, neither requiresexplicit notion time.Using bounds, action precondition v c applied, evenfractionally, must case v c, otherwise precondition actioncould never met ordering actions chosen. course, v c, cannotguarantee legal ordering producers consumers achievesvalue. need least feasible opportunity satisfy preconditionadded LP introduction binary ([0, 1] integer) variable pairconstraints. actions [a0 ..an1 ] requiring precondition v c, using N4. note complementary class variables whose values must remain certain levelorder actions applied, produced. seem less useful real problems,nonetheless handled analogous way.5. Section 6 show could captured LP even expressed propositionally.375fiColes, Coles, Fox & Longdenote large number, denote new binary variable, add pair constraints:N.s a0 + ... + an1v lb(v) + (c lb(v)).s(6)first constraint forces take value 1 actions requiringprecondition v c applied. second constraint determines lower boundv based value s: = 0, lower bound v lb(v), global lowerbound v. Otherwise, = 1, since action needing precondition applied,thus v c. constraint = 1 implies least oneactions [a0 ...an1 ] applied. However, since non-zero value makesLP harder solve, pressure set = 1 reasonprecondition must satisfied. important note constraintsadded LP problems v c preconditions matched v = c effect.modified heuristic able support planning models previously could not.behaviour domains without characteristics entirely unaffected.8. Resultssection present thorough evaluation lp-rpg heuristic: comparingstate-of-the-art numeric planners, considering use different LP solvers performingablation studies determine effective many potential different configurations lp-rpg discussed paper. include weighting action variablesLP according RPG layer appear, inclusion propositionsnumeric goals LP consideration variables LP remaininteger relaxed real numbers. tests run 3.4GHzPentium IV machines limited 30 minutes 1.5GB memory. plannerplanner-configuration fails report solution within limits deemedfailed solve problem.8.1 Evaluation DomainsFirst discuss selection evaluation domains. purpose selecting domainsselect construct examples informative evaluating behaviourheuristic. Domains numeric variables conform producer-consumerbehaviour identified syntactically different planning strategyemployed. Since syntactic analysis trivial, overhead making decisionnegligible, assume performance domains approachapplicable consistent whichever alternative strategy selected deployment.consider existing competition benchmarks producerconsumer behaviour,introduce new domains exhibit behaviour. current benchmarks exhibit interesting producerconsumer behaviour, order make comparison informative possible make use do:MPrime domain IPC 1;Rovers domain (Numeric variant) IPC 3;376fiA Hybrid LP-RPG Heuristic PlanningSettlers domain IPC 3;alternative encoding Settlers domain (described below);Pathways domain IPC 5. (We developed metric domain derivedMetric Time variant, replacing durative actions comparable nontemporal actions.)addition standard IPC problem set Settlers (problems 120), introducenew problems make full use scope domain. domain allowsbuilding ships transporting materials disjoint islands; however,benchmark set none problems require this. building ships requires large amountinfrastructure, therefore add problems challenge planners,materials must imported overseas order achieve goals. first problems(21) requires merely building ship, 22 requires import timber overseas,23 requires building housing overseas. 24 adds goals 23, requiringplanner achieve goals mainland build housing island. finalproblem, 25, considers 3 disjoint islands resources must combined achievegoal island. problems requires building much greater infrastructurerequired original IPC 3 settlers problems. consider two variantssettlers domain: standard IPC 3 domain, encoding based representationcarts proposed Gregory Rendl (2008). Here, number carts givenlocation represented numeric variable (carts-at ?location). twopossible move actions carts: one moves cart unit resource onelocation another; one moves cart without moving resources, i.e.cart moves whilst empty. encoding possible carts transportsingle unit material, necessary maintain specific named identitiescapacities cart.addition benchmark domains, use two domains created development lp-rpg:Market Trader Domain (Coles et al., 2008);Hydro Power Domain.Market Trader Domain, trader begins small amount money,goal increased certain level. must achieved travellingmarkets, sells collection goods certain pricegood type, buys another (lower) price. Money made buying itemscheaper transporting (via camel) locations commandhigher price; moving associated cost food required camel.representation general class real-world trading problems aimmake money buying, transporting selling goods. Hydro Power domainalso concerned financial gains, domain different structure: rathertransportation, concerned energy storage using hydroelectric reservoirs.buying electricity pump water uphill periods low demand, electricitycheaper, storing potential energy, electricity sold higher price377fiColes, Coles, Fox & LongDomainMarket TraderHydro PowerPathways MetricSettlersSettlers Numeric CartsMPrimeRovers NumericSugarTotallp-rpg2027301823301518181lp-rpg-FF03217133013794MetricFF01138828101482lpgtd050197302020101Table 2: Coverage achieved different plannerstimes greater demand. domain encoding augmented take account energyloss: purchasing one unit energy sufficient provide one unit energy laterlosses storage process. problem, despite temporal axis,interested profit made, force planner advance timespecific point: ask sufficient profit made plannerchoose advance time necessary. general problem models continuous processes:customer demand changes continuously. Here, simplify problem discretising30 minute time intervals, using demand schedule transformer domain (Bell,Coles, Coles, Fox, & Long, 2009), based UK National Grid figures. Like originaltransformer model represent temporal features problem advance timeaction, rather temporal pddl, since lp-rpg temporal planner.final domain consider Sugar domain (Radzi, 2011). Here, objectiveproduce sugar industrial processes, refining raw cane. domaintaken set domains designed optimisation planning: severalpaths goal, originally included allow planner choice trajectorieschallenge find better quality solutions. set domains designedchallenging metric optimisation problems, domains set trivialstandard MetricFF: plan metric ignored almost problems solvedless 1 second6 . Therefore, domains, consider Sugar domain,remains challenging MetricFF even optimisation required: numberpaths appear lead goal large without good guidance difficultfind solution problem.6. say domains uninteresting present interesting challengeexplored Radzi (2011), using carefully modified variant lp-rpg. However, challengefind good quality plans, quality determined complex metric simple plan length;neither MetricFF lp-rpg form discussed difficulty finding poor quality plansproblems.378fiA Hybrid LP-RPG Heuristic Planningmarkettrader100hydropower10000LPRPG1000LPRPGLPRPG-FFFFLPG10010Time (s)Time (s)10110.10.010.10.00124681012Problem Number141618205101520Problem Number10000LPRPGLPRPG-FFFFMetric-LPG10030sugarpathwaysmetric1000251000LPRPGLPRPG-FFFFLPG10010Time (s)Time (s)10110.10.10.010.010.0010.0015101520Problem Number253024681012Problem Numbersettlers10000100014161820settlersnumeric1000LPRPGLPRPG-FFFFLPGLPRPGLPRPG-FFFFLPG100Time (s)Time (s)1001010110.10.10.010.0151015Problem Number202551015Problem Numbermprime100010025roversnumeric1000LPRPGLPRPG-FFFFLPG100LPRPGLPRPG-FFFFLPG10Time (s)10Time (s)20110.10.10.010.010.0010.0015101520Problem Number253024681012Problem Number141618Figure 3: Comparison MetricFF lpgtd: time taken solve problems37920fiColes, Coles, Fox & LongMarket Trader200Hydro Power300LPRPGBest knownLPRPGLPRPG-FFFFLPGBest known180250160200Solution QualitySolution Quality14012010015010080506040024681012Problem Number1416182051015Problem NumberPathways Metric9000700030LPRPGLPRPG-FFFFLPGBest known250200Solution Quality6000Solution Quality25Sugar300LPRPGLPRPG-FFFFMetric-LPGBest known80002050004000150300010020005010000051015Problem Number20253024681012Problem NumberSettlers90070010001820LPRPGLPRPG-FFFFLPGBest known800Solution Quality600Solution Quality16Settlers Num. Carts1200LPRPGLPRPG-FFFFLPGBest known800145004003006004002002001000051015Problem Number2025510M-Prime3002502025Rovers Numeric300LPRPGLPRPG-FFFFLPGBest known250LPRPGLPRPG-FFFFLPGBest known200Solution Quality200Solution Quality15Problem Number15015010010050500051015Problem Number20253024681012Problem Number14Figure 4: Comparison MetricFF lpgtd: plan length380161820fiA Hybrid LP-RPG Heuristic Planning8.2 Comparison Plannersfirst compare performance lp-rpg existing numeric planners. usefound strong (though uniformly best) configuration planner,demonstrate subsequent sections:Landmarks Propositional Goals added LP (as Section 6.1);weight action variable objective function used solution extraction 3l , l layer appeared RPG expansion;Action variables corresponding actions action layer 1 integral;IBM ILOG CPLEX version 12.1.0 used LP solver.compare two historically successful numeric planners: MetricFF (Hoffmann, 2003) lpgtd (Gerevini et al., 2006). remain state art, manymodern planners (e.g. lama) handle numeric preconditions, action costs.clarify differences performance, also compare lp-rpg-FF: reimplementation MetricFF based lp-rpg code difference that, computingupper- lower-bounds numeric variables RPG expansion, lp-rpg-FF allowsactions applied many times action layer, rather per actionlayer MetricFF. Since publication earlier comparison lpgtd (Coles et al.,2008), new improved version lpgtd produced. version lpgtdperforms much better earlier version, use results here. alsoconsider variant lpgtd, Metric-lpgtd (Gerevini, Saetti, & Serina, 2008), designedresponsive plan metrics based numeric variables. experiments showedMetric-lpgtd perform significantly differently lpgtd generating firstfeasible plans problems plan length metric, apart Pathways domain lpgtd crashes problems. Therefore, report performance lpgtddomains except Pathways report figures Metric-lpgtd.interesting pattern emerges relative performance planners across setevaluation domains, shown Figure 3. domains organised topstrongly numeric, relying propositions, towards bottompropositional structure consequently less numeric structure. twoheavily propositional domains (MPrime Rovers) lpgtd generallysuccessful planner, solving problems evaluation sets, oftenfastest planner problems. pattern holds standard competitionproblems (1-20) competition formulation Settlers domain. MetricFF alsoperforms quite well MPrime Rovers domains, struggles Settlersdomain due numeric structure present.strongly numeric domains, however, lpgtd performs poorly: indeed failssolve single problem Pathways Market Trader domains. duecrashing, rather planner searches exhausts resource limits without findingsolution. Hydro Power, lpgtd solves five easiest problems, ablescale beyond this. experiments observed lpgtd struggles domainslimited propositional structure generally, search guidance getsnumeric problems poor. Comparing two Settlers variants also gives381fiColes, Coles, Fox & Longinteresting insights: carts turned numeric resource, lpgtd strugglesmuch solving 7, instead 19 problems, whereas performance lp-rpg factimproved. Note although lpgtd successful IPC 3 problems, cannot solvericher problems ship building overseas transport required; whereas lp-rpgcapable solving instances.Turning attention comparison MetricFF observe problemssolved planners generally solved quickly MetricFF, particularlydomains propositional structure. lp-rpg additionaloverhead solving LP state (and partly, course, due highly efficientMetricFF code-base). Occasionally general pattern broken, lp-rpg faster;slight variations ordering heuristically equivalent states leadsignificant differences performance. results lp-rpg-FF show similarcoverage MetricFF, although sometimes solving different problems (againbranch orderings different code base cause differences, markedPathways Sugar domains), serve demonstrate basic FFimplementation performing drastically differently standard MetricFF causinggains observe.Looking numeric domains particular, lp-rpg heuristic able provide much better guidance, allowing lp-rpg solve many problems MetricFF.Notably, Market Trader domain neither MetricFF lp-rpg-FF solveproblems. due poor guidance standard RPG heuristic gives domain, relaxing transfer numeric resources. relaxed plan buy itemrepeatedly sell item sufficient profit made. Again, Hydro Power,similar situation occurs: one unit energy pumped up, unitenergy repeatedly sold future time day, making sufficient profit withoutguidance. Pathways, chemical reactions must take place, relaxation usedMetricFF allow units substance used repeatedly, several differentreactions, despite fact used consumed. lp-rpg heuristic permit therefore gives much better search guidance, allowing lp-rpgsolve problems domain. numeric resource transfer present Settlersdomain leads poor guidance MetricFF heuristic MetricFF able solveproblems result. use LP effective domain allowingproblems solved. different formulations seem make little differencecoverage MetricFF, neither making problems easier MetricFF solve.quality solutions (plan length) produced different planners displayedFigure 4. emphasize lp-rpg current form making attemptminimise general measure plan quality, results merely intended giveindication whether large degradation, indeed fortuitous increase,quality moving using standard RPG heuristic hybrid lp-rpg approach.potential improve plan quality using lp-rpg-style approach exploredwork domains preferences (Coles & Coles, 2011) also rangedifferent metrics (Radzi, 2011). None problems use specified metric functionsminimise, instead use number actions solution plan. valueRPG heuristic tend minimise. None planners run optimisationmode (where available) simply report first plan found search.382fiA Hybrid LP-RPG Heuristic Planningdomains quality solutions produced lp-rpg comparableproduced MetricFF lpgtd. sugar domain lp-rpg heuristic comparesfavourably lpgtd, although could perhaps hope running lpgtd quality modewould enable produce better solutions. Pathways, lp-rpg produces particularlylong solutions, trade off, also able scale solve far problems.return issue solution length domain considering weightingaction variables LP solution extraction.summary, lpgtd seems generally successful domains sufficient propositional structure MetricFF generally efficient problemscapable solving. structure domain becomes heavily numericplanners perform poorly. lp-rpg, however, able solve many problemsplanners, making use search guidance LP captures numericinteractions well.8.3 LP Solverslp-rpg construction use LP performed using functions commonlyavailable wide range LP solvers: adding variables constraints model, settingvariable bounds, marking whether variables real integer valued, changing objectivefunction, on. current implementation employs minimal abstraction layerlp-rpg LP solver itself, almost LP solver used.section, consider use three LP solvers:IBM ILOG CPLEX version 12.1.0, commercial mixed integer-linear programmingsolver.COIN-OR LP (CLP) version 1.12.0, open-source LP solver. models featureinteger variables, CLP used within COIN-OR Branch-and-Cut (CBC) version 2.5.0,is, again, open-source.LPSolve version 5.5.0.13, open-source mixed integer-linear programming solver.experiments LP solvers, found CPLEX substantiallyrobust two, particularly LP extended include satisfyingpropositional goals landmarks. Thus, purposes comparison here,use configuration lp-rpg (equivalent used earlier paper (Coles et al.,2008)) efficient presented elsewhere paper, robust(caused CLP LPSolve crash less often) testing:Propositional goals landmarks added LP: encodes numeric goalsonly.integer variables (potentially) helpful actions, assignmenteffects.layer-weighting scheme k = 1.1 used.refer configuration limited-lp-rpg.383fiColes, Coles, Fox & LongMarket Trader1000Hydro Power10CPLEXLPSolveCLPCPLEXLPSolveCLP1001Time (s)Time (s)1010.10.10.010.0124681012Problem Number141618205101520Problem NumberPathways Metric10001002530Sugar10000CPLEXLPSolveCLPCPLEXLPSolveCLP1000100Time (s)Time (s)101100.110.010.0010.1510152025302468Problem NumberSettlers10000101214161820Problem NumberSettlers Numeric Carts10000CPLEXLPSolveCLP10001000CPLEXLPSolveCLP100Time (s)Time (s)1001010110.10.10.0151015Problem Number202551015Problem NumberM-Prime25Rovers Numeric1000CPLEXLPSolveCLP1001001010Time (s)Time (s)100020110.10.10.01CPLEXLPSolveCLP0.015101520Problem Number253024681012Problem Number14161820Figure 5: Time taken limited-lp-rpg solve problems using different LP solvers384fiA Hybrid LP-RPG Heuristic PlanningMarket Trader300Hydro Power300CPLEXCLPLPSolve250200Plan Length200Plan LengthCPLEXCLPLPSolve25015015010010050500024681012Problem Number1416182051015Problem NumberPathways Metric300025002530Sugar55CPLEXCLPLPSolveCPLEXCLPLPSolve504540200035Plan LengthPlan Length20150030251000201550010055101520Problem Number25302468Settlers5004501012Problem Number14161820Settlers Num. Carts800CPLEXCLPLPSolve700CPLEXCLPLPSolve400600350Plan LengthPlan Length300250200500400300150200100100500051015Problem Number202551015Problem NumberM-Prime605025Rovers Numeric120CPLEXCLPLPSolve100CPLEXCLPLPSolve80Plan Length40Plan Length203060204010200051015Problem Number20253024681012Problem Number14161820Figure 6: Lengths plans produced limited-lp-rpg using different LP solvers385fiColes, Coles, Fox & LongDomainMarket TraderHydro PowerPathways MetricSugarSettlersSettlers Numeric CartsMPrimeRovers NumericTotalCPLEX202928912212813160CLP163011011103012120LPSolve1530291583012121Table 3: Coverage limited-lp-rpg different LP solversresults tests shown Figure 5, Table 3. Beginning MarketTrader domain, quite clear CPLEX faster CLP domain. LPSolve,turn, substantially out-performs CPLEX many problems, two orders magnitude.speed, though, comes cost terms robustness: CPLEX solves 20 problems,LPSolve solves 15. LPSolve also demonstrates strong performance HydroPower domain, CLP falls LPSolve CPLEX.Pathways Metric domain illustrates robustness CPLEX extension beyondbasic producerconsumer model encoding. domain contains actions numericpreconditions must true, affected action. described Section 7.2, encoding requires integer variable precondition. domainalso contains goals referring multiple numeric variables, e.g:(>= (+ (available cycDp1) (available c-Myc-Max)) 3)))seen, domain, CPLEX LP solver allows lp-rpgsolve anything smallest problems. Beyond problem 2, CLP unable solveLP reach goals initial state. Using LPSolve, solvable LPs reportedunsolvable, whilst planner make attempt search, erroneous state pruninghappening result LPs falsely declared unsolvable renders unable findsolutions.Sugar domain, solver leads planner performing particularly well,10 problems solved. contrasts earlier results, shown Table 2,different configuration lp-rpg, using CPLEX richer LPs, able solve18 problems. However, noted start section, compromiseperformance planner using CPLEX allow reasonable comparison CLPLPSolve: using richer LP models here, CPLEX solves 18 problems LPSolveCLP perform far worse (falsely claiming LPs unsolvable, returning suboptimalsolutions, detriment performance planner).two different encodings Settlers domain, carts represented either explicitly using carts-at function, see CPLEX robustalternative domain encodings. original IPC domain model, LPSolve performsparticularly well, CLP markedly different CPLEX. Using numeric-carts386fiA Hybrid LP-RPG Heuristic PlanningDomainLPSolveCLPCPLEXBuild (ms)Solve (ms)Build (ms)Solve (ms)Build (ms)Solve (ms)Market Trader5.0167.11.3137.917.344.6Hydro Power0.71.40.35.99.46.6Pathways Metric0.01.34.38.84.43.1Sugar0.87.51.642.017.622.9Settlers2.631.92.2170.6165.587.6Settlers Num. Carts2.222.91.671.048.033.6MPrime3.15.45.311.576.26.3Rovers Numeric0.84.60.77.214.81.9Average1.930.32.256.844.125.8Table 4: Time spent LP building solving using different LP solversencoding, however, CPLEX considerably better solvers, robustchange LP structure arising alternative domain encoding.MPrime Rovers domain, CLP LPSolve consistently fasterCPLEX.Summarising, see limited-lp-rpg solve problem using CLPLPSolve, usually faster using CPLEX, CPLEX offers better coveragegreater robustness grants access richer encodings allow better performance.order investigate detail planner takes much longer solve problems using CPLEX devised tests. Table 4 shows average time spent,per state, building solving LP LP solver. (The building time timerequired integrate constraints inserted lp-rpg internal model usedparticular solver.) Note results necessarily directly comparable: planners necessarily take paths state space, might evaluatingdifferent states. paths are, nonetheless, often similar times takenstrongly indicative. give fairest possible comparison, include tabledata problems solved three configurations, data presentedplanner across exactly problem set. startling observation CPLEXtypically spends order magnitude longer building LPs two solvers (inSettlers two orders magnitude). So, often solves LPs quickly, total time spent handling LPs generally greater. Indeed, CPLEX, time spentsolving LPs is, 5 8 domains, dominated time spent building them.indicates that, although CPLEX good choice use LP solver (it solvesLPs efficiently), practice solvers faster due substantial overheadsbuilding large number LPs necessary (at least one per state). results suggestrobust LP solver low LP building overheads could dramatically improveperformance lp-rpg.considering solution quality note way LP Solversdirect search different trajectory solvers return different optimal solutionsLP point search. Recall planner configuration seeksdirectly minimise plan length: objective function LPs uses weighted sum387fiColes, Coles, Fox & LongDomainMarket TraderHydro PowerPathways MetricSugarSettlersSettlers Numeric CartsMPrimeRovers NumericTotal1l18303011132229101631.1l2029289192230131703l20273018182330151815l202730181721301517810l2026302016213015178hmax2028301620233014181hadd2028301622232915183Table 5: Coverage varying LP objective function weighting schemesnumber actions. Figure 6 shows similar picture arises plan lengthtime performance: LPSolve often leads planner shorter solutions CPLEX (alsohelping explain often faster, since explores search tree smaller depth).particular happens Rovers Numeric Sugar domains well Settlersvariants. Across domains little variation quality solutions produced.8.4 LP Objective Function Weighting Schemesusing LP solution extraction phase, one open issue weightingscheme use objective function. Since LP ignores propositional preconditionsactions, using simple objective minimising sum action variables (layerweighting scheme k = 1) gives LP solver freedom select equally actionsappearing layers RPG, regardless many actions subsequently needadded relaxed plan support them. discussed earlier, using layer-weightingscheme k > 1, using weighting scheme based estimated costs achievingpreconditions actions, encourage LP solutions favour actionscheaper apply. hmax hadd heuristics candidates baseestimates cost application actions. course, caseschoice earlier action, one lower costs achieve preconditions,flawed choice, worse choice would made using k = 1layer-weighting scheme: simply nature heuristics.section, evaluate range LP action-variable layer-weighting schemesalso action-cost estimate schemes. use k 1, 1.1, 3, 5, 10 layer-weighting schemes.consider setting action variable weights 1 plus cost meeting propositionalpreconditions hmax hadd action-cost estimate schemes. parametersplanners set sensible defaults: action variables first action layerintegral, propositional goals landmarks included goal-checking LP.Results showing coverage configurations shown Table 5. firstobservation weighting scheme better using layer-weightingk = 1 action variables. particularly noticeable Settlers domain,several situations earlier actions preferred. example,388fiA Hybrid LP-RPG Heuristic PlanningWeight 1 vs Weight 1.1: TimeSugarRovers Num.M-PrimeSettlers Num. CartsSettlersPathways MetricHydro PowerMarket Trader1000Weight 1.1 (Nodes)100Weight 1.1 (Time (s))Weight 1 vs Weight 1.1: Nodes ExpandedSugarRovers Num.M-PrimeSettlers Num. CartsSettlersPathways MetricHydro PowerMarket Trader1000101100100.10.010.0110.1110Weight 1 (Time (s))1001000110Weight 1 vs Weight 3SugarRovers Num.M-PrimeSettlers Num. CartsSettlersPathways MetricHydro PowerMarket Trader1000Weight3 (Nodes)Weight 3 (Time (s))1001000Weight 1 vs Weight 3: Nodes ExpandedSugarRovers Num.M-PrimeSettlers Num. CartsSettlersPathways MetricHydro PowerMarket Trader1000100Weight 1 (Nodes)101100100.10.010.0110.1110Weight 1 (Time (s))1001000110100Weight 1 (Nodes)Weight 1 vs Weight 5: TimeSugarRovers Num.M-PrimeSettlers Num. CartsSettlersPathways MetricHydro PowerMarket Trader1000Weight5 (Nodes)100Weight 5 (Time (s))Weight 1 vs Weight 5: Nodes ExpandedSugarRovers Num.M-PrimeSettlers Num. CartsSettlersPathways MetricHydro PowerMarket Trader10001000101100100.10.010.0110.1110Weight 1 (Time (s))1001000110Weight 1 vs Weight 10: TimeWeight 10 (Time (s))1001000Weight 1 vs Weight 10: Nodes ExpandedSugarRovers Num.M-PrimeSettlers Num. CartsSettlersPathways MetricHydro PowerMarket TraderSugarRovers Num.M-PrimeSettlers Num. CartsSettlersPathways MetricHydro PowerMarket Trader1000Weight10 (Nodes)1000100Weight 1 (Nodes)101100100.10.010.0110.1110Weight 1 (Time (s))1001000110100Weight 1 (Nodes)1000Figure 7: Layer-weighting schemes LP (k = 1 versus k = x different values x)389fiColes, Coles, Fox & LongWeight 1 vs H Max: Nodes ExpandedWeight 1 vs H Max: TimeH Max (Time (s))100SugarRovers NumericM-PrimeSettlers Num. CartsSettlersPathways MetricHydro PowerMarket TraderSugarRovers NumericM-PrimeSettlers Num. CartsSettlersPathways MetricHydro PowerMarket Trader1000H Max (Nodes)1000101100100.10.010.0110.1110Weight 1 (Time (s))1001100010100Weight1 (Nodes)Weight 1 vs H Add: Nodes ExpandedWeight 1 vs H Add: TimeH Add (Time (s))100SugarRovers NumericM-PrimeSettlers Num. CartsSettlersPathways MetricHydro PowerMarket TraderSugarRovers NumericM-PrimeSettlers Num. CartsSettlersPathways MetricHydro PowerMarket Trader1000H Add (Nodes)10001000101100100.10.010.0110.1110Weight 1 (Time (s))1001100010100Weight 1 (Nodes)1000Figure 8: Action weighting schemes LP objective function (layer-weighting k = 1versus action-cost estimate based schemes using hadd hmax )supposing cart location A, unit resource, initially availableB, needs moved location C. two two-action solutions LP.Annotating action layer appears, are:load cart (0), unload cart C (1);load cart B (1), unload cart C (1).Using layer-weighting scheme k = 1 two solutions indistinguishable(each cost 2). However, latter selected, two actions needed supportrelaxed plan: moving cart B moving cart C. alternative actionvariable weighting schemes set weight loading cart B higherloading A, leading preference first solution, better outcomesearch guidance. similar phenomenon occurs Rovers domain, favouringearlier actions increases preference recharging rovers close currentlocations. domain leads better avoidance dead-ends, since postponingrecharging actions risks possibility rover little charge left getback recharging location.data layer-weighting schemes show results peak k = 3, reasonable trade-off minimising number actions chosen LP solutionfavouring earlier actions. overall performance hmax same, losing performance390fiA Hybrid LP-RPG Heuristic PlanningTime TakenNodes ExpandedPlan Lengthk=1.1 3 5 10 hmax hadd 1.1 3 5 10 hmax hadd 1.1 3 5 10 hmax haddk=1 X X X X XX X X X X XX 7 X X X XXk = 1.1X X X 77X X X 7XX X X XXk=37 7 XX7 7 XXX X XXk=5X XX7 X77 XXk = 10XXX7XXhmax7X7Table 6: Results Two-Tailed Wilcoxon Signed Rank Tests comparing different LP weighting schemes. X indicates significance (0 = 0.05), colour indicates better performer(faster, fewer nodes expanded shorter plans).Sugar Rovers gaining Settlers Hydro Power. Using hadd givesgains Settlers, leading slightly better coverage layer-weighting schemek = 3. difference performance k = 3 hadd weighting schemesdomain-dependent. hadd increases weight action 1 propositional preconditions true state evaluated. Settlers domain,gives particularly good performance, sound approach, neatly capturingexample case discussed earlier section: loading unloading cartcurrent location preferable later locations. domains earlier actionspreferable even propositional preconditions require little support, hadd failsgive adequate bias, layer-weighting scheme higher k performs better.instance, Sugar domain, best coverage obtained using k = 10,k = 3 scheme also performs strongly hadd here.Examining performance configurations detail, comparing timetaken find solution plans number nodes evaluated, scatterplots comparingconfigurations tested layer-weighting k = 1 shown Figures 78, layer-weighting schemes action-cost estimate based schemes. lprpg usually exhausts time limit 30 minutes memory limit, time-takenscatterplots closely similar coverage table. Coverage directly reflectednumber points far right graphs, indicating layer-weighting schemek = 1 (the x-axis) unable find solution within 30 minutes, lp-rpg usingdifferent weighting scheme able solve problem. pointsk = 3 hadd schemes, points hadd appearing predominantlySettlers domain k = 3 spread across domains.Since always clear scatterplots whether one configuration betterwhether differences significant, used Two-Tailed WilcoxonSigned Rank Test compare pair tested configurations terms time takennodes evaluated, also plan length.7 tests performed p = 0.05. resultstests summarised Table 6. number interesting observations made:7. Wilcoxon signed-rank test non-parametric statistical test used compare set matchedsamples (such pairs results two different planners sequence problems)assess whether population mean ranks differ (i.e. paired difference test). usefulabsolute values necessarily comparable samples drawn completely391fiColes, Coles, Fox & Long1l vs 1.1l: Plan Length10000SugarRovers NumericM-PrimeSettlers Num. CartsSettlersPathways MetricHydro PowerMarket TraderSugarRovers NumericM-PrimeSettlers Num. CartsSettlersPathways MetricHydro PowerMarket Trader10003l (Plan Length)10001.1l (Plan Length)1l vs 3l: Plan Length100001001001010111101001000100001101001l (Plan Length)1l vs 5l: Plan Length1000010l (Plan Length)5l (Plan Length)SugarRovers NumericM-PrimeSettlers Num. CartsSettlersPathways MetricHydro PowerMarket Trader10001001010010111101001000100001101l (Plan Length)1000100001000100001l vs hadd: Plan Length10000SugarRovers NumericM-PrimeSettlers Num. CartsSettlersPathways MetricHydro PowerMarket TraderSugarRovers NumericM-PrimeSettlers Num. CartsSettlersPathways MetricHydro PowerMarket Trader1000hadd10001001l (Plan Length)1.1l vs hmax10000hmax (Plan Length)100001l vs 10l: Plan Length10000SugarRovers NumericM-PrimeSettlers Num. CartsSettlersPathways MetricHydro PowerMarket Trader100010001l (Plan Length)100101001011110100100010000l110100l1.1 (Plan Length)1 (Plan Length)Figure 9: Effect quality varying action weighting schemes LP objective functionk = 3 gives better coverage k = 5 k = 10, take significantlydifferent amount time explore significantly different number nodes.is, however, significant difference time taken nodes, favour k = 3,compared configurations tested.action-cost based weighting schemes slower layer-weighting schemesk 3. Furthermore, compared k = 3, expand nodes.unknown distribution. two-tailed version makes assumption two contendersbetter performance (i.e. lower mean rank).392fiA Hybrid LP-RPG Heuristic Planningkey strength action-cost based weighting schemes lies qualitysolutions found. general trend, layer-weighting schemes gain coverage losequality k increases. action-cost based schemes generate significantly shorterplans configuration tested, including layer-weighting k = 1.increase plan length plans found using layer-weighting scheme increasing k seen Figure 9. reason increase quite clear: basicobjective function LP minimise number actions. However, weightingsadded actions according RPG layer appear, longer plansmake use earlier actions become attractive. increase plan length is,however, largely restricted two domains. first, worst affected, Pathways Metric, increase k causes preference reactions less efficient (in termsnumber actions needed) performed using actions earliest layersplanning graph. second Settlers Numeric Carts, increase kleads solutions preference resource production refinement approachesless efficient, comprise actions appear earlier planning graphs.perhaps expected, two domains excluded statistical analysis,significant difference plan length k = 3 action-cost basedschemes.weighting schemes tested best results obtained using k = 3 hadd :single best option. former faster, expands fewer nodes, gives goodbalance coverage across domains used. latter less prone variations planquality domains, strong performance Settlers domain leads twoproblems solved overall within test domains.8.5 Use Integer ConstraintsSection 5.4.1 discussed fact LP relaxation MIP, proposedcertain situations may beneficial relax action variables, makingintegral. section, explore hypothesis, considering five configurationslp-rpg:1. Minimal Integers: actions assignment effects (as Section 4.3) integers.2. First-layer: above, variables actions appearing RPG layer 1 (the potentially helpful actions) also integral.3. Propositional-Goal Achievers: above, variables actions achievepropositional goals landmarks also integral.4. Numeric-Goal Achievers: above, additionally, variables actions affectingnumeric state variables appear numeric goals also integral.5. All: (action) variables integral.coverage configurations shown Tables 7 8, layerweighting using k = 1.1 k = 3, respectively. k = 3 results, coverage fairlyinsensitive configuration used. peak coverage Numeric Goal393fiColes, Coles, Fox & LongDomainMinimalFirst-Layer Prop. Goal Num. Goal(Assignments) ActionsAchievers Achievers VariablesMarket Trader2020202020Hydro Power2329292929Pathways Metric3028283029Settlers2119191919Settlers Num. Carts2222222221MPrime3030303030Rovers Numeric1413131315Sugar1099919Total170170170172182Table 7: Coverage varying action variables integer MIP (layer-weightingk = 1.1)DomainMinimalFirst-Layer Prop. Goal Num. Goal(Assignments) ActionsAchievers Achievers VariablesMarket Trader2020202020Hydro Power2327272929Pathways Metric3030303030Settlers2018181815Settlers Num. Carts2223232323MPrime3030303030Rovers Numeric1515151515Sugar2018181820Total180181181183182Table 8: Coverage varying action variables integer MIP (layer-weightingk = 3)Achievers configuration, though difference worst configuration3 problems. Using k = 1.1, marked increase coverage actionvariables integral. due Sugar domain: compared preceding configuration table, additional 10 problems solved. domain,using k = 3, though, even better coverage obtained using minimal numberintegral action variables. Thus, appears need integral variables domainreduced objective preference earlier actions sufficiently high. DisregardingSugar domain, results k = 1.1 domains close, k = 3configuration.interesting that, even many integers MIP, performance lp-rpgterms coverage different using integers all. Consideringcomputational complexity solving MIP, rather solving LP, time spentcalculating heuristic considerably higher, rendering all-integers approach394fiA Hybrid LP-RPG Heuristic PlanningMinimal Ints vs First Layer Actions: Time100Minimal Ints vs First Layer Actions: Nodes ExpandedSugarRovers NumericM-PrimeSettlers Num. CartsSettlersPathways MetricHydro PowerMarket TraderSugarRovers NumericM-PrimeSettlers Num. CartsSettlersPathways MetricHydro PowerMarket Trader1000First Layer Actions (Nodes)First Layer Actions (Time (s))1000101100100.10.010.0110.1110Minimal Ints (Time (s))1001000110Minimal Ints vs (First Layer Actions +) Prop Goal Achievers: Time1001000Minimal Ints vs (First Layer Actions +) Prop Goal Achievers: Nodes ExpandedSugarRovers NumericM-PrimeSettlers Num. CartsSettlersPathways MetricHydro PowerMarket TraderSugarRovers NumericM-PrimeSettlers Num. CartsSettlersPathways MetricHydro PowerMarket Trader1000Prop Goal Achievers (Nodes)Prop Goal Achievers (Time (s))1000100Minimal Ints (Nodes)101100100.10.010.0110.1110Minimal Ints (Time (s))10010001Minimal Ints vs (First Layer Actions + Prop Goal Achievers) Numeric Goal Achievers: Time100100Minimal Ints (Nodes)1000Minimal Ints vs (First Layer Actions + Prop Goal Achievers) Numeric Goal Achievers: Nodes Exp.SugarRovers NumericM-PrimeSettlers Num. CartsSettlersPathways MetricHydro PowerMarket TraderSugarRovers NumericM-PrimeSettlers Num. CartsSettlersPathways MetricHydro PowerMarket Trader1000Num. Goal Achievers (Nodes)Num. Goal Achievers (Time (s))100010101100100.10.010.0110.1110Minimal Ints (Time (s))1001000110Minimal Ints vs Ints: TimeInts (Time (s))1001000Minimal Ints vs Ints: Nodes ExpandedSugarRovers NumericM-PrimeSettlers Num. CartsSettlersPathways MetricHydro PowerMarket TraderSugarRovers NumericM-PrimeSettlers Num. CartsSettlersPathways MetricHydro PowerMarket Trader1000Ints (Nodes)1000100Minimal Ints (Nodes)101100100.10.010.0110.1110Minimal Ints (Time (s))1001000110100Minimal Ints (Nodes)Figure 10: Changing variables integer (k = 3)3951000fiColes, Coles, Fox & LongTime TakenFirst Prop. Num.Layer Goal GoalActs Ach. Ach.Minimal IntsXXXFirst Layer Acts7XProp. Goal Ach.XNum. Goal Ach.Nodes ExpandedPlan LengthFirst Prop. Num. First Prop. Num.In- Layer Goal Goal In- Layer Goal Goal Ints Acts Ach. Ach. ts Acts Ach. Ach. tsX77777777X7X X777XX X77X77Table 9: Results Two-Tailed Wilcoxon Signed Rank Tests comparing different setsvariables integers MIP (using layer-weighting k = 1.1). X indicates significance(p = 0.05), colour indicates better performer (faster, fewer nodes expanded shorterplans).Time TakenFirst Prop. Num.Layer Goal GoalActs Ach. Ach.Minimal IntsXXXFirst Layer ActsXXProp. Goal Ach.XNum. Goal Ach.Nodes ExpandedPlan LengthFirst Prop. Num. First Prop. Num.In- Layer Goal Goal In- Layer Goal Goal Ints Acts Ach. Ach. ts Acts Ach. Ach. tsX77777777X7X X777X7X77X77Table 10: Results Two-Tailed Wilcoxon Signed Rank Tests comparing different setsvariables integers MIP (using layer-weighting k = 3). X indicates significance(p = 0.05), colour indicates better performer (faster, fewer nodes expanded shorterplans).impractical. Investigating further, scatterplots time taken numbernodes expanded shown Figure 10. (The results shown k = 3, overallpicture k = 1.1 used.) configuration compared MinimalIntegers configuration. general trend, one moves left column (increasingproportion action variables integral), points drift line= x, i.e. time taken solve problems increases. time, though, movingright-hand column decrease number nodes evaluated. Thus,increasing proportion integral action variables seems improve search guidance,though sufficiently allow pay-off terms time taken solve problems.confirm significance observations, applied Two-Tailed Wilcoxon SignedRank Tests, results Tables 9 10, k = 1.1 k = 3, respectively.cases, consistent increase time taken solve problemsproportion integral action variables increased: difference significant everycase using k = 1.1 comparing First Layer Actions PropositionalGoal Achievers. significant difference plan length pairconfigurations. results nodes somewhat less clear. appearusing Minimal Integers leads expansion significantly different number nodes,396fiA Hybrid LP-RPG Heuristic PlanningDomainMinimal IntsFirst Layer ActionsIntsBuild (ms)Solve (ms)Build (ms)Solve (ms)Build (ms)Solve (ms)Market Trader(20)26.427.727.852.127.255.3Hydro Power (21)8.81.89.26.510.86.2Pathways Metric (30)58.5271.264.9834.065.43468.9Sugar (16)24.933.826.146.624.7329.2Settlers (9)282.396.1453.8201.6295.2204.7Settlers Num. Carts (16) 399.4101.6520.9462.8408.8222.1MPrime (28)78.84.875.66.378.96.1Rovers Numeric (11)57.71.742.42.159.02.8Average117.167.3152.6201.5121.3536.9Table 11: Time spent building solving LP varying variables integerMIP; numbers shown domains indicate many problem instances solvedused computing reported averagecompared configuration. part due limitationtests pairwise-solved problems included, difference coveragereflected data analysed test. Considering configurations, though,Integers evaluates fewer nodes First Layer Actions Propositional GoalAchievers, regardless weight used, cannot shown expand fewer nodesNumeric Goal Achievers, configuration offered better coverage Table 8.Although seen increasing number integer variables makes plannerslower, somewhat surprising amount planner made slowergreater is. theory would expect making variables integer woulddramatically decrease performance planner, however, appears so.investigate consider long spent building solving LPstate configuration. Table 11 gives indication seeingsurprising result: 5 8 domains solving time LP less buildingtime (for Ints configuration), showing key bottleneck using LPsolver is, fact, building LP. Building time is, course, likely varyconfigurations (the variation due different states expanded) comparedsolving time small. two three remaining domains, PathwaysSugar, see increase cost solving LP first-layer actions madeintegral, much larger increase actions made integral, patternexpected. final domain, Market Trader, main increase occursfirst-layer action variables integral: structure domain means integralfirst layer actions often causes variables layers also become integral. Solvingtimes does, however, remain within order magnitude building time, overheadparticularly large domain compared previous two.397fiColes, Coles, Fox & Long8.6 Including Numeric Goal ConjunctSection 7.1 discussed possibility including entire numeric goal conjunctproblem LP. well theoretically increasing ability detect dead-endsinsisting goals attainable time, rather individually alsoallows arbitrary LNF goals used, found domains Pathways.section, investigate impact extension. particular, explore whetheruse numeric-goal-checking LP including numeric goal conjunct improvesworsens performance. ensure goal-checking LP casenumeric goal conjunction used, tests, configurations,disable inclusion propositional goals landmarks LP. gain insightsimpact numeric-goal-checking LP affected choice action-variablelayer-weighting schemes, consider two: k = 1.1 k = 3.DomainMarket TraderHydro PowerPathways MetricSugarSettlersSettlers Num. CartsMPrimeRovers NumericTotal (Excl. Pathways)TotalNum.Goal(k = 3)2027301813192813138168WithoutNum. Goal(k = 3)202701414182913135135Num. Goal(k = 1.1)202928912212813132160WithoutNum. Goal(k = 1.1)203001410222913138138Table 12: Coverage varying whether Numeric Goal conjunct included LP (with)(without)coverage results running lp-rpg without numeric goal conjunctshown Table 12. Looking results excluding Pathways domain (whichsolved numeric goal conjunct included, since goals expressedarbitrary LNF), one make two immediate observations: use numericgoal conjunct improves performance actions weighted k = 3, solving 3 additionalproblems, use worsens performance actions weighted k = 1.1, solving 6fewer problems. difference impact interesting consequence relationshipRPG structure LP:no-goal-conjunct case, goal appears fact layer l, LP used meetgoal LP (l) LP containing actions action layer l (Algorithm 4,line 22). favours earlier actions RPG, precluding actionslayer l used.numeric goal-conjunct case, LP extended layer l0 which,first, goals appear, second, LP constrained meet goals, usingactions layer l0 , satisfied. individual goals, point may later398fiA Hybrid LP-RPG Heuristic Planningvs Without Numeric Goal Conjunct: Nodes Expandedvs Without Numeric Goal Conjunct: TimeSugarRovers NumericM-PrimeSettlers Num. CartsSettlersHydro PowerMarket Trader100Numeric Goal Conjunct (Nodes)Numeric Goal Conjunct (Time (s))10001010.10.010.0010.0010.010.1110100Without Numeric Goal Conjunct (Time (s))SugarRovers NumericM-PrimeSettlers Num. CartsSettlersHydro PowerMarket Trader100010010110001SugarRovers NumericM-PrimeSettlers Num. CartsSettlersHydro PowerMarket Trader1010.10.010.0010.0010.010.1110100Without Numeric Goal Conjunct (Time (s))SugarRovers NumericM-PrimeSettlers Num. CartsSettlersHydro PowerMarket Trader1000Numeric Goal Conjunct (Nodes)Numeric Goal Conjunct (Time (s))1001000vs Without Numeric Goal Conjunct (Weight 3): Nodes Expandedvs Without Numeric Goal Conjunct (Weight 3): Time100010100Without Numeric Goal Conjunct (Nodes)1001011000110100Without Numeric Goal Conjunct (Nodes)1000Figure 11: Including numeric goals LPlayer l first appeared, case actions l0l chosen meet goals, whereas previous case could not.difference actions available support given goal makesaction-variable weighting scheme important. use k = 3 rather k = 1.1means actions added layer l layer l0 (inclusive) available meetgoal first appeared layer l, objective function leads preference useearlier actions.coverage results experiments give unequivocal pictureimpact feature performance. Figure 11 shows scatterplots time takensolve problems, number nodes evaluated, numeric goal-conjunctused not, k = 1.1 k = 3. appears general trendinclusion numeric goal conjunct reduce number nodes evaluated. caseweighting k = 3, Two-Tailed Wilcoxon Signed Rank Test confirms usenumeric goal conjunct reduces number nodes evaluated (p = 0.05). Using k = 1.1suggests similar trend, results significant.weighting schemes, use numeric goal conjunct introduces smallstatistically significant time overhead. due additional time takenevaluate state: RPG must extended point goalssatisfied together, rather individually. Whether pays off, i.e. whether reductionnodes evaluated sufficient offset this, depends domain appears399fiColes, Coles, Fox & Longcorrelated extent numeric goals interact. one extreme,Rovers domain, goals propositional difference performance.hand, Sugar domain, Settlers domain numeric cartsused, beneficial. domains concern production reprocessing rawmaterials, one form another, leading interaction goals. instance, unitresource may sufficient satisfy goals individually, additional production mayrequired support both. cases use numeric goal conjunct improvestime performance. Inclusion numeric goal conjunct significant impactlength plans produced (according Two-Tailed Wilcoxon Signed Rank Test).summarise results section, main benefit use numericgoal conjunct able extend expressivity planner domainsgoals written using arbitrary LNF. success approach domainsvaries. terms coverage, whether better use numeric goal conjunctevaluation domains depends weighting scheme. using layer-weightingscheme, k = 3, inclusion numeric goal conjunct slightly beneficial lp-rpgtherefore set use configuration default.8.7 Including Propositions LPprevious section, observed inclusion numeric goal conjunctgoal-checking LP variable impact performance, depending weighting schemeused. Perhaps interesting use goal-checking LP using LP meetpropositional goals, landmarks, described Section 6.1. evaluate techniqueconsider four configurations:1. propositions: using goal-checking LP containing numeric-goal conjunct.2. Propositional goals: previous case, also including propositional goalsgoal-checking LP.3. Landmarks: previous case, also including landmarks.4. propositions: previous case, also constrained ensure actionvariable non-zero, actions added meet propositional preconditions (asdescribed Section 6.3).form spectrum, case information propositionsincluded LP all, last goal-checking LP must meetpropositional goals, also preconditions actions chosen so. considertwo layer-weighting schemes (k = 1.1 k = 3), action variables actionsfirst action layer integral.coverage results k = 1.1 shown Table 13 k = 3 Table 14.seen, cases, general pattern emerges: coverage improvesincluding configuration landmarks included LP, declinesfinal Propositions configuration. Including propositions appears, however,remain better including propositions all.Scatterplots illustrating time taken number nodes evaluated solvingproblems shown Figures 12 13 (for weights k = 1.1 k = 3, respectively).400fiA Hybrid LP-RPG Heuristic PlanningProps vs Propositional Goals: Time100Props vs Propositional Goals: Nodes Expanded10000SugarRovers Num.M-PrimeSettlers Num. CartssettlersPathways MetricHydro PowerMarket Trader101SugarRovers NumericM-PrimeSettlers Numeric CartsSettlersPathways MetricHydro PowerMarket Trader1000Propositional Goals (Nodes)Propositional Goals (Time (s))1000100100.10.010.0110.1110Props (Time (s))1001000110Props vs Landmarks: TimeLandmarks (Time (s))100100010000Props vs Landmarks: Nodes Expanded10000SugarRovers Num.M-PrimeSettlers Num. CartssettlersPathways MetricHydro PowerMarket TraderSugarRovers NumericM-PrimeSettlers Numeric CartsSettlersPathways MetricHydro PowerMarket Trader1000Landmarks (Nodes)1000100Props (Nodes)101100100.10.010.0110.1110Props (Time (s))1001000110Props vs Props: TimeProps (Time (s))100100010000Props vs Props: Nodes Expanded10000SugarRovers Num.M-PrimeSettlers Num. CartssettlersPathways MetricHydro PowerMarket TraderSugarRovers NumericM-PrimeSettlers Numeric CartsSettlersPathways MetricHydro PowerMarket Trader1000Props (Nodes)1000100Props (Nodes)101100100.10.010.0110.1110Props (Time (s))1001000110100Props (Nodes)1000Figure 12: Varying propositions included LP (k = 1.1)40110000fiColes, Coles, Fox & LongProps vs Propositional Goals: TimeProp Goals (Time (s))100Props vs Propositional Goals: Nodes ExpandedSugarRovers NumericM-PrimeSettlers Num. CartsSettlersPathways MetricHydro PowerMarket TraderSugarRovers NumericM-PrimeSettlers Num. CartsSettlersPathways MetricHydro PowerMarket Trader1000Propositional Goals (Nodes)1000101100100.10.010.0110.1110Props (Time (s))1001000110Props vs Landmarks: TimeLandmarks (Time (s))1001000Props vs Landmarks: Nodes ExpandedSugarRovers NumericM-PrimeSettlers Num. CartsSettlersPathways MetricHydro PowerMarket TraderSugarRovers NumericM-PrimeSettlers Num. CartsSettlersPathways MetricHydro PowerMarket Trader1000100Landmarks1000100Props (Nodes)101100.10.010.0110.1110Props (Time (s))1001000110Props vs Props: TimeProps (Time (s))1001000Props vs Props: Nodes ExpandedSugarRovers NumericM-PrimeSettlers Num. CartsSettlersPathways MetricHydro PowerMarket TraderSugarRovers NumericM-PrimeSettlers Num. CartsSettlersPathways MetricHydro PowerMarket Trader1000Props (Nodes)1000100Props101100100.10.010.0110.1110Props (Time (s))1001000110100Props (Nodes)Figure 13: Varying propositions included LP (k = 3)4021000fiA Hybrid LP-RPG Heuristic PlanningDomainMarket TraderHydro PowerPathways MetricSugarSettlersSettlers NumericMPrimeRovers NumericTotalProps202928912212813160Prop Goals20292891922289164Landmarks202928919223013170Props1930301411212715167Table 13: Coverage varying propositions included LP (using k = 1.1)DomainMarket TraderHydro PowerPathways MetricSugarSettlersSettlers NumericMPrimeRovers NumericTotalProps2027301813192813168Prop Goals2027301818232813177Landmarks2027301818233015181Props2028301911213014173Table 14: Coverage varying propositions included LP (using k = 3)scatterplot compares one configurations 24 enumerated listconfiguration 1 (no propositions). Comparing Propositional Goals Landmarksconfigurations suggests inclusion Landmarks leads problemssolved quickly compared propositions configuration. particularlyevident Settlers, Settlers Numeric Carts, MPrime Rovers Numeric domains.Whether using Landmarks offer gains Propositional Goals alone depends structure problems. two Settlers variants, including propositional goals (such(connected-by-rail p1 p2) (has-ironworks p3)) necessitates use specific actions ((build-rail p1 p2) (build-ironworks p3), respectively). Since actionsconsume numeric resources, including propositional goals sufficient cause LPintroduce production actions necessary support resource consumption. Rovers,goals communicated data generated rock samples, soil analysesimages. actions adding facts consume number units energy,benefit including propositional goals LP. However, actions alsopropositional preconditions: instance, rover communicate rock data,must taken sample rock. propositional preconditions includedlandmarks LP, thereby forcing production energy needed403fiColes, Coles, Fox & Longcommunicate requisite data, also energy acquired it. leadsinclusion additional recharge actions or, suitable actions available, better deadend detection. similar situation arises MPrime: actions achieve goal factsconsume resources, actions achieve (landmark) preconditionsactions do, including landmarks improves informedness heuristic.Two-Tailed Wilcoxon Signed Rank Tests shown Tables 15 16 confirm(p = 0.05) either k = 1.1 k = 3 use Propositional Goals improvesPropositions, use Landmarks improves Propositional Goals. Indeed,p = 0.05 total-ordering time-performance k = 3:Propositions Propositional Goals Landmarksresults show similar behaviour numbers nodes evaluated (the right-handcolumns Figures 12 13) although differences statistically significant.Wilcoxon tests also show neither k = 1.1 k = 3 differencePropositional Goals Landmarks statistically significant. Note, however, usingLandmarks allows problems solved. comparisons Propositions,Propositional Goals Landmarks, weight k = 1.1, significant, latter twoimproving upon former. weight k = 3 tests inconclusive, althoughusing Landmarks allows 13 additional problems solved (recall comparisonsrestricted problems solved variants comparison).PropsProp GoalsLandmarksTime TakenProp LandAllGoals marks PropsXX7XXXNodes ExpandedProp LandAllGoals marks PropsXXX7XXPlan LengthProp LandAllGoals marks Props77X7XXTable 15: Results Two-Tailed Wilcoxon Signed Rank Tests comparing inclusion different propositions LP (using layer-weighting k = 1.1). X indicates significance(p = 0.05), colour indicates better performer (faster, fewer nodes expanded shorterplans).PropsProp GoalsLandmarksTime TakenProp LandAllGoals marks PropsXX7XXXNodes ExpandedProp LandAllGoals marks PropsX7X7XXPlan LengthProp LandAllGoals marks Props77X7XXTable 16: Results Two-Tailed Wilcoxon Signed Rank Tests comparing inclusion different propositions LP (using layer-weighting k = 3). X indicates significance (p = 0.05),colour indicates better performer (faster, fewer nodes expanded shorter plans).404fiA Hybrid LP-RPG Heuristic PlanningDomainPropsLandmarksPropsBuild (ms)Solve (ms)Build (ms)Solve (ms)Build (ms)Solve (ms)Market Trader (20)27.852.128.052.329.774.5Hydro Power (27)9.36.69.86.611.75.5Pathways Metric (30)64.9834.067.0833.368.6905.5Sugar (18)27.247.527.347.539.982.3Settlers (8)436.4145.5421.6137.9563.93153.0Settlers Num. Carts (15) 204.5434.8252.5133.0244.22253.8MPrime (28)75.66.378.95.791.224.5Rovers Numeric (11)42.42.156.32.076.331.2Average111.0191.1117.7152.3140.7816.3Table 17: Time spent building solving LP, varying propositions included;numbers shown domains indicate many problem instances solved usedcomputing reported averageresults shown bottom Figures 12 13 indicate Propositionsconfiguration less consistent performance.show, however, (p = 0.05) k = 1.1 k = 3, Propositionsconfiguration takes longer solve problems Propositional Goals configurationLandmarks configuration. perhaps surprising feasible considerincluding propositions LP manner, given results reported van den Brielet al. (2007). key difference disregarding delete effects, factneeded precondition need added once. work reported vanden Briel et al., however, cases fact required precondition also deleted,delete effect must balanced equivalent number add effects (less onefact true initially).Looking number nodes evaluated using Propositions configurationfind important result: shown bottom right Figures 12 13, whicheverweight used, Propositions tends expand fewer nodes. Furthermore, Propositions expands fewer nodes three configurations overallbest configuration terms nodes expanded (significant result, p = 0.05). Unfortunately,overhead associated node higher results poorer coveragetime performance. result indicates numericpropositional separation usedlp-rpg heuristic sensible trade-off, use RPG meet propositionalpreconditions sacrificing performance terms nodes expanded exchangereduction time taken solve problems.Table 17 shows increase costs associated solving LPs including propositions.results taken problems solved three configurations, althoughremains case configurations expand exactly states solvingproblems. three domains Hydro Power, MPrime Rovers Numericcost building LPs dominates cost solving them, significantdecreases performance domains. course, cost building LPs increasespropositions included since variables required. Settlers domain405fiColes, Coles, Fox & Longorder magnitude increase time spent solving LPs, indicatingincluding propositions LPs makes difficult solve. Sometimesinformed search guidance gleaned information allows planner findsolutions expanding far fewer nodes.Adding landmarks LP appears make solving LP slightly faster:partly side effect landmarks configuration needing solve fewer LPs per state,since uses one LP meet goals instead one LP per goal. Propositionsconfiguration often needs solve one LP solution extraction phase. However,size difficulty LP means benefits solving fewer LPs (in termstime taken) negated.Tables 15 16 show results statistical tests comparing lengths plansfound various configurations. significant results that, regardlessweight used, Propositions finds shorter plans configurations.propositions included LP, typically single LP call madesolution extraction, simultaneously achieving goals action preconditions.configurations, first, LP call made goals, then, actionadded support propositional precondition action implied solutionLP, numeric preconditions met another LP call. Thus, severalLP calls rather one. fragmenting production relaxed planway, efficacy relaxed plan eroded. example, plan lengths improvedtwo variants Settlers. Here, production resources requires buildinginfrastructure: sawmill required refine timber wood, on. two unitswood made, LP knowledge propositional preconditions,difference LP building two units wood location A, one unitlocation B one location C need build one two sawmills, dependingoption chosen, discovered actions chosen meetpropositional preconditions actions required solution LP. LPincludes Propositions, cases this, difference buildingone sawmill two, LP prefer single-sawmill solution, ultimately leadingbetter search guidance. Thus, Propositions produces plans significantly shorter,domains fact propositions outside LP disguises true costsaction choices solution LP.8.8 Propositional Resource Analysisend Section 6.4, identified conditions possible turn propositions model resources equivalent numeric representation. purposes,lp-rpg, would allow resource, within heuristic, managed LPrather RPG. end, use three domains containing propositional stacksrepresent resources, evaluate encoding affects performance lp-rpg,numeric planners. five planners evaluated MetricFF, lpgtd, lp-rpg (usinganalysis translates propositional resource stacks equivalent numeric representation described Long & Fox, 2000), lp-rpg analysis disabled,lp-rpg-FF. three domains use are:406fiA Hybrid LP-RPG Heuristic PlanningDomainMetricFFlpgtdlp-rpg-FFSettlers Prop. TimberSettlers Prop. CartsMPrime Prop.Total7429404430388102968lp-rpgAnal.Anal.516132229304747Table 18: Coverage without propositional resource analysisvariant IPC3 Settlers domain, amount timber given location(or given vehicle) represented stack propositions, range n0n10.variant Settlers Numeric Carts encoding, number cartsgiven location represented stack propositions.MPrime domain IPC1, encoding represents fuel level stackpropositions.first two these, propositional encoding enforces limit 10 amounttimber (respectively, number carts) given location. necessary limitation forced action grounding. numeric representation, decreasing one resourceincreasing another done single ground action, appropriate numericeffects conditions. ground action need stipulate precise levelsresource operation, long limits resource levelsrespected. propositional case, however, one ground action needed pairdiscrete levels two resources, parameters action stipulating pre-post-values resource. level 10 chosen avoid placing overly restrictivebounds resource levels, creating manageable number ground actions.issue grounding also accounts limited choice domains: propositional stackused represent resources take modest range discrete values.Rovers domain, example, set reachable energy levels roverrange [0, 80]. Market Trader domain, amount money could heldgiven time real value one decimal place (this consequence choiceproblem files), greater equal zero.Results domains shown Table 18. Comparing, first, lp-rpg withoutanalysis lp-rpg analysis, see encoding resources LPnumbers grants consistent improvement performance. Figure 14 shows timetaken find solutions similarly improved, perhaps strikingly second Settlersvariant (with propositional cart levels).MetricFF lpgtd perform well MPrime domain, two Settlersencodings, lp-rpg resource analysis performs markedly better,planners solving handful problems. contrasts earlier results, shownTable 2, lpgtd particular performed well Settlers domain.interesting contrast see here, Propositional Timber variant derivedIPC 3 model lpgtd performs well leads considerably worse performance.407fiColes, Coles, Fox & LongSettlers Propositional Stack Timber100001000Settlers Propositional Stack Carts10000LPRPG-AnalysisLPRPG-FFFFLPGLPRPG-No-Analysis1000Time (s)100Time (s)100LPRPG-AnalysisLPRPG-FFFFLPGLPRPG-No-Analysis1010110.10.151015Problem Number202551015Problem Number2025M-Prime Propositional Resource Stack100001000LPRPG-AnalysisLPRPG-FFFFLPGLPRPG-No-AnalysisTime (s)1001010.10.0151015Problem Number202530Figure 14: Using propositional resource analysissupports role resource analysis allowing lp-rpg robustnumber almost-equivalent domain encodings used, mixed propositionalnumeric encodings resource levels. observed significant change lengthplans produced lp-rpg without propositional resource analysis.9. Conclusions Future Workmodern planning systems ineffective reasoning numbers. However, managing complex numeric interactions important part driving AI planning towardsfuture real-world application. paper shown that, using linear programmodel numeric resource flows, ability planners reason domains involvingcomplex numeric interactions greatly improved.key contribution separation heuristic search control relaxedplanning graph, based delete-relaxation, linear program allows exact reasoningnumeric constraints relaxes action ordering.explored different configurations heuristic, putless information LP, impact performance planner whole.exploration different LP solvers reveals less efficient handlingvarious combinations constraints. found that, LPSolve CLP, conjunctionlimited version lp-rpg published 2008 (Coles et al., 2008), solve simple408fiA Hybrid LP-RPG Heuristic Planningproblems quickly, CPLEX coupled full power extended LPRPG neededhandle complex test instances.work far focussed developing search control methods performwell numeric planning problems particular character: producer-consumerbehaviour define Sectionsect:prodcondefinition. Although believecommon behaviour, practice, numeric domains exhibit range behaviours.several possible ways exploit lp-rpg approach domains. One useapproach actions conform constraints producer-consumer behaviour,pushing numeric behaviour metric RPG way currentlyhandle propositional goals preconditions separate RPG. would yieldbenefits potentially better estimates reachable ranges action use costsparts domain express producer-consumer behaviour. challengingconsider behaviours relaxed producer-consumer behaviourobtain useful heuristic information. example, actions production effects varycould encoded family producers increasing capability, discretising rangeproduction options introducing reachability analysis increasedproduction capabilities become available. general, relaxations must make reachabilityleast permissive actual reachability (that is, action must applicablerelaxed reachability analysis least early action actually reachable)relaxed plan extraction minimise estimated cost goal effectively (thisdifficult relaxed plan optimal). Within constraints, believeuse LP approximations provide tool tackling wider range behavioursexplore paper.exciting challenge future work integrate lp-rpg methodoptimising plans according given objective function. recent 2008 2011planning competitions highlighted importance optimising planning emphasis solution quality. development is, however, non-trivial: challenges lieintegration cost optimisation LP RPG well decidinguse heuristic trades goal distance quality search. first stepdirection accomplished Radzi PhD thesis (Radzi, 2011), advancesnumeric planning described paper open many possibilities extendinginitial work.ReferencesBell, K. R. W., Coles, A. J., Coles, A. I., Fox, M., & Long, D. (2009). role AI planningdecision support tool power substation management. Artificial IntelligenceCommunications, 22 (1), 3757.Benton, J., Do, M. B., & Kambhampati, S. (2005). Over-subscription planning numeric goals. Proceedings 19th International Joint Conference ArtificialIntelligence (IJCAI), pp. 12071213.Benton, J., van den Briel, M., & Kambhampati, S. (2007). hybrid linear programmingrelaxed plan heuristic partial satisfaction planning problems. Proceedings17th International Conference Planning Scheduling (ICAPS).409fiColes, Coles, Fox & LongBlackmore, L., Ono, M., & Williams, B. (2011). Chance-constrained optimal path planningobstacles. Robotics, IEEE Transactions on, 27 (6), 1080 1094.Blum, A., & Furst, M. (1995). Fast planning planning graph analysis. Proceedings14th International Joint Conference Artificial Intelligence (IJCAI 95), pp.16361642.Bonet, B., & Geffner, H. (2001). Heuristic Search Planner 2.0. Artificial Intelligence Magazine, 22 (3), 7780.Coles, A., Coles, A., Fox, M., & Long, D. (2012). COLIN: Planning continuous linearnumeric change. Journal Artificial Intelligence Research, 44, 196.Coles, A. I., Fox, M., Long, D., & Smith, A. J. (2008). hybrid relaxed planning graph-lpheuristic numeric planning domains. Proceedings Eighteenth InternationalConference Automated Planning Scheduling (ICAPS 08).Coles, A. J., & Coles, A. I. (2011). LPRPG-P: Relaxed plan heuristics planningpreferences. Proceedings 21st International Conference Automated Planning Scheduling (ICAPS).Coles, A. J., Coles, A. I., Fox, M., & Long, D. (2009). Extending use inferencetemporal planning forwards search. Proceedings 19th InternationalConference Automated Planning Scheduling (ICAPS 09).Do, M. B., & Kambhampati, S. (2001). Sapa: domain-independent heuristic metric temporal planner. Proceedings European Conference Planning (ECP01).Do, M. B., & Kambhampati, S. (2003). SAPA: multi-objective metric temporal planner.Journal Artificial Intelligence Research, 20, 155194.Domshlak, C., Katz, M., & Lefler, S. (2010). abstractions met landmarks. Proceedings 20th International Conference Planning Scheduling (ICAPS).Edelkamp, S. (2003). Taming numbers durations model checking integratedplanning system. Journal Artificial Intelligence Research, 20, 195238.Fox, M., & Long, D. (2003). PDDL2.1: extension PDDL expressing temporalplanning domains. Journal Artificial Intelligence Research, 20, 61124.Gerevini, A., Saetti, A., & Serina, I. (2006). approach temporal planning scheduling domains predictable exogenous events. Journal Artificial IntelligenceResearch, 25, 187231.Gerevini, A., Saetti, A., & Serina, I. (2008). approach efficient planning numericalfluents multi-criteria plan quality. Artificial Intelligence, 172 (8-9), 899944.Gregory, P., & Rendl, A. (2008). constraint model settlers planning domain.Aylett, R. (Ed.), Proceedings UK Planning Special Interest Group (PlanSIG).Herriot Watt University.Gregory, P., Cresswell, S., Long, D., & Porteous, J. (2004). extraction disjunctive landmarks planning problems via symmetry reduction.. ProceedingsConference Symmetry Search (SymCon 2004), pp. 3441.410fiA Hybrid LP-RPG Heuristic PlanningHelmert, M., & Domshlak, C. (2009). Landmarks, critical paths abstractions: Whatsdifference anyway?. Proceedings 19th International Conference Planning Scheduling (ICAPS), pp. 162169.Hoffmann, J. (2003). Metric-FF planning system: Translating ignoring delete listsnumeric state variables. Journal Artificial Intelligence Research, 20, 291341.Hoffmann, J., & Edelkamp, S. (2005). deterministic part IPC-4: overview. JournalArtificial Intelligence Research, 24, 519579.Hoffmann, J., Porteous, J., & Sebastia, L. (2004). Ordered landmarks planning. JournalArtificial Intelligence Research, 22, 215278.Hoffmann, J., & Nebel, B. (2001). FF planning system: Fast plan generationheuristic search. Journal Artificial Intelligence Research, 14, 253302.Karpas, E., & Domshlak, C. (2009). Cost-optimal planning landmarks. Proceedings21st International Joint Conference Artificial Intelligence (IJCAI 09), pp.17281733.Kautz, H., & Walser, J. (2000). Integer optimization models AI planning problems.Knowledge Engineering Review, 15 (1), 101117.Koehler, J. (1998). Planning resource constraints. Proceedings EuropeanConference Artificial Intelligence (ECAI98), pp. 489493.Laborie, P. (2003). Algorithms propagating resource constraints AI planningscheduling: Existing approaches new results. Artificial Intelligence, 143 (2), 151188.Li, H. X., & Williams, B. C. (2008). Generative planning hybrid systems based flowtubes. Proceedings 18th International Conference Automated PlanningScheduling, ICAPS, pp. 206213.Long, D., & Fox, M. (2000). Automatic synthesis use generic types planning.Proceedings Artificial Intelligence Planning Scheduling (AIPS), pp. 196205.Long, D., & Fox, M. (2003a). Exploiting Graphplan framework temporal planning.Proceedings International Conference Artificial Intelligence PlanningScheduling (ICAPS).Long, D., & Fox, M. (2003b). 3rd International Planning Competition: Resultsanalysis. Journal Artificial Intelligence Research, 20, 159.Ono, M., & Williams, B. C. (2008). efficient motion planning algorithm stochasticdynamic systems constraints probability failure. Proceedings 23rdAAAI Conference Artificial Intelligence, AAAI, pp. 13761382.Porteous, J., Sebastia, L., & Hoffmann, J. (2001). extraction, ordering, usagelandmarks planning. Proceedings 6th European Conference Planning(ECP 01).Radzi, N. H. M. (2011). Multi-Objective Planning using Linear Programming. Ph.D. thesis,University Strathclyde.411fiColes, Coles, Fox & LongRichter, S., Helmert, M., & Westphahl, M. (2008). Landmarks revisited. Proceedings23rd AAAI Conference Artificial Intelligence (AAAI 08), pp. 975982.Richter, S., & Westphal, M. (2010). LAMA Planner: Guiding Cost-Based AnytimePlanning Landmarks.. Journal Artificial Intelligence Research, 39, 127177.Shin, J.-A., & Davis, E. (2005). Processes continuous change SAT-based planner.Journal Artificial Intelligence Research, 166, 194253.van den Briel, M., Benton, J., Kambhampati, S., & Vossen, T. (2007). LP-based heuristicoptimal planning. Principles Practice Constraint Programming (CP2007), pp. 651665.van den Briel, M. H. L., Vossen, T., & Kambhampati, S. (2008). Loosely coupled formulations automated planning: integer programming perspective. JournalArtificial Intelligence Research, 31, 217257.Vossen, T., Ball, M. O., Lotem, A., & Nau, D. S. (1999). use integer programmingmodels AI planning. Proceedings 16th International Joint ConferenceArtificial Intelligence (IJCAI 99), pp. 304309.Wolfman, S., & Weld, D. (2000). Combining linear programming satisfiability solvingresource planning. Knowledge Engineering Review, 15 (1).Zhu, L., & Givan, R. (2003). Landmark extraction via planning graph propagation. Proceedings Doctoral Consortium 13th International Conference PlanningScheduling (ICAPS-DC 03), pp. 156160.412fiJournal Artificial Intelligence Research 46 (2013) 235-262Submitted 11/12; published 02/13Toward Supervised Anomaly DetectionNico GrnitzMarius KloftNICO . GOERNITZ @ TU - BERLIN . DEKLOFT @ TU - BERLIN . DEMachine Learning Laboratory, Technische Universitt BerlinFranklinstr. 28/29, Berlin, GermanyComputational Biology CenterMemorial Sloan-Kettering Cancer CenterNew York City, USAKonrad RieckKONRAD . RIECK @ UNI - GOETTINGEN . DEUniversity Gttingen, Dep. Computer ScienceGoldschmidtstr. 7, 37077 Gttingen, GermanyUlf BrefeldBREFELD @ KMA . INFORMATIK . TU - DARMSTADT. DETechnische Universitt DarmstadtHochschulstr. 10, 64289 Darmstadt, GermanyGerman Institute International Educational ResearchSchlostr. 29, 60486 Frankfurt, GermanyAbstractAnomaly detection regarded unsupervised learning task anomalies stem adversarial unlikely events unknown distributions. However, predictive performancepurely unsupervised anomaly detection often fails match required detection rates manytasks exists need labeled data guide model generation. first contributionshows classical semi-supervised approaches, originating supervised classifier, inappropriate hardly detect new unknown anomalies. argue semi-supervised anomalydetection needs ground unsupervised learning paradigm devise novel algorithmmeets requirement. Although intrinsically non-convex, showoptimization problem convex equivalent relatively mild assumptions. Additionally,propose active learning strategy automatically filter candidates labeling. empiricalstudy network intrusion detection data, observe proposed learning methodology requires much less labeled data state-of-the-art, achieving higher detection accuracies.1. IntroductionAnomaly detection deals identifying unlikely rare events. classical approachanomaly detection compute precise description normal data. Every newly arriving instance contrasted model normality anomaly score computed. scoredescribes deviations new instance compared average data instance and, deviation exceeds predefined threshold, instance considered anomaly outlierprocessed adequately (Markou & Singh, 2003a; Chandola, Banerjee, & Kumar, 2009; Markou &Singh, 2003b).Identifying data exhibits irregular suspicious traits crucial many applicationsmedical imaging network security. particular, latter become vivid research areacomputer systems increasingly exposed security threats, computer worms, networkc2013AI Access Foundation. rights reserved.fiG RNITZ , K LOFT, R IECK , & B REFELDFigure 1: Illustration two paradigms semi-supervised learning.attacks, malicious code (Andrews & Pregibon, 1978). Network intrusion detection dealsdetecting previously unknown threats attacks network traffic. Conventional security techniques intrusion detection based identifying known patterns misuse, called signatures (Roesch, 1999; Paxson, 1999) thusalthough effective known attacksfailprotect novel threats. brings anomaly detection focus security research (e.g.,Eskin, Arnold, Prerau, Portnoy, & Stolfo, 2002; Kruegel, Vigna, & Robertson, 2005; Stolfo, Apap,Eskin, Heller, Hershkop, Honig, & Svore, 2005; Perdisci, Ariu, Fogla, Giacinto, & Lee, 2009).Thus, anomaly detection beneficial learning scenarios many regular data instances given, allows machine approximate underlying distribution wellleads concise model normality. contrast, outliers anomalies rare evenoriginate changing distributions (e.g., novel classes network attacks). Especially adversarial settings, network intrusion detection, differences training test distributionseminent novel threats tactics continuously developed. consequence, anomalydetection generally considered unsupervised task prominent learning methods, includingone-class support vector machines (Schlkopf, Platt, Shawe-Taylor, Smola, & Williamson, 2001)support vector data descriptions (SVDD, Tax & Duin, 2004), implement spirit. However,underlying assumptions unsupervised methods also major drawback manyapplication areas, unsupervised methods fail achieve required detection rates. Especiallyadversarial application areas network intrusion detection, even single undetected outliermay already suffice capture system. Therefore, goal article incorporatefeedback-loop terms labeled data make anomaly detection practical. so, knowledge historic threats anomalies included terms labels thus guidemodel generation toward better generalizations.article, cast anomaly detection paradigm semi-supervised learning(Chapelle, Schlkopf, & Zien, 2006). Usually, semi-supervised methods deduced existingsupervised techniques, augmented appropriate bias take unlabeled data account.instance, prominent bias assumes unlabeled data structured clusters closeness (with respect measure) proportional probability class label(Vapnik, 1998; Joachims, 1999; Chapelle & Zien, 2005; Chapelle, Chi, & Zien, 2006; Sindhwani,Niyogi, & Belkin, 2005). consequence, anomaly detection often rephrased (multi-class)classification problem (Almgren & Jonsson, 2004; Stokes & Platt, 2008; Pelleg & Moore, 2004;Mao, Lee, Parikh, Chen, & Huang, 2009). Although assuming cluster-structure data236fiT OWARD UPERVISED NOMALY ETECTION8010060AUC[0,0.01] [in %]AUC[0,0.01] [in %]8060404020unsupervisedunsupervisedsupervisedsemisupervised20SSAD01 5 10 155075supervisedsemisupervised0SSAD1 5 10 151005075100% labeled data% labeled dataFigure 2: Left: standard supervised classification scenario identical training test distributions. Right: anomaly detection setting two novel anomaly clusterstest distribution.often well justified anomaly detection, recall supervised learning techniques focus discriminating concept classes unsupervised techniques rather focus data characterization.article, show differences training test distributions well occurrencepreviously unseen outlier classes render anomaly detection methods, derived supervisedtechnique, inappropriate likely miss novel previously unseen classes anomalies depicted. contrast, argue successful anomaly detection methods inherently needground unsupervised learning paradigm, see Figure 1. sum, making anomaly detection practical requires following key characteristics: (i) intrinsically following unsupervisedlearning paradigm cope unknown events (ii) additionally exploiting label informationobtain state-of-the-art results.Figure 2, show results controlled experiment visualizes different naturesemi-supervised methods derived supervised unsupervised paradigms, respectively.left hand side, achieved accuracies standard supervised classification scenario,training test distributions identical, shown. performance unsupervisedanomaly detection method clearly outperformed supervised semi-supervised approaches.However, observe right hand side figure fragile latter methodsanomaly detection scenario. experimental setup identical former exceptdiscard two anomaly clusters training set (see Figure 3). Note changearbitrary modification inherent characteristic anomaly detection scenarios, anomalies stem novel previously unseen distributions. Unsurprisingly, (partially) supervisedmethods fail detect novel outliers clearly outperformed unsupervised approach,robustly performs around unimpressive detection rates 40%. Finally, methodsclearly outperformed novel semi-supervised anomaly detection (SSAD) devisedunsupervised learning paradigm allows incorporating labeled data.237fiG RNITZ , K LOFT, R IECK , & B REFELDanomaliesnormaldatanovelanomaliesFigure 3: Left: training data stems two clusters normal data (gray) one small anomalycluster (red). Right: two additional anomaly clusters (red) appear test data set.main contribution article provide mathematical sound methodology semisupervised anomaly detection. Carefully conducted experiments discussions showimportance distinguishing two semi-supervised settings depicted Figure 1.meet requirement, propose novel semi-supervised anomaly detection techniquederived unsupervised learning paradigm, allows incorporating labeled datatraining process. approach based support vector data description (SVDD) containsoriginal formulation special case. Although final optimization problem convex,show equivalent convex formulation obtained relatively mild assumptions.guide user labeling process, additionally propose active learning strategyimprove actual model quickly detect novel anomaly clusters. empirically evaluatemethod network intrusion detection tasks. contribution proves robust scenariosperformance baseline approaches deteriorate due obfuscation techniques. addition,active learning strategy shown useful standalone method threshold adaptation.remainder article organized follows. Section 2 reviews related work. novelsemi-supervised anomaly detection methods presented Section 3 Section 4 introducesactive learning strategies. Section 5 gives insights proposed learning paradigmreport results real-world network intrusion scenarios Section 6. Section 7 concludes.2. Related WorkSemi-supervised learning (Chapelle, Schlkopf et al., 2006) offers mathematical sound frameworklearning partially labeled data. instance, transductive approaches semi-supervisedlearning assume cluster structure data close points likely share labelpoints far away likely labeled differently. transductive support vectormachine (TSVM, Vapnik, 1998; Joachims, 1999; Chapelle & Zien, 2005) optimizes max-marginhyperplane feature space implements cluster assumption. basic formulation,TSVM non-convex integer programming problem top SVM. computationallyexpensive Chapelle, Chi et al. (2006) propose efficient smooth relaxationTSVM. Another related approach low density separation (LDS) Chapelle Zien (2005),cluster structure modeled graph distances.238fiT OWARD UPERVISED NOMALY ETECTIONbroad overview anomaly detection found work Chandola et al. (2009).Anomaly detection regarded unsupervised learning task therefore surprisingexist large number applications employing unsupervised anomaly detection methods.instance, finding anomalies network traffic (Eskin et al., 2002) program behaviour (Heller,Svore, Keromytis, & Stolfo, 2003), denoising patterns (Park, Kang, Kim, Kwok, & Tsang, 2007)annotating (Goh, Chang, & Li, 2005) classifying images (Lai, Tax, Duin, Zbieta, Ekalska, &Ik, 2004) documents (Manevitz & Yousef, 2002; Onoda, Murata, & Yamada, 2006)Fully-supervised approaches anomaly detection usually ignore unlabeled datatraining-phase: example, Almgren Jonsson (2004) employ max-margin classifier separates innocuous data attacks. Stokes Platt (2008) present techniquecombines approaches effective discrimination (Almgren & Jonsson, 2004) rare-class detection (Pelleg & Moore, 2004). Mao et al. (2009) take multi-view co-training approach basedBlum Mitchell (1998) learn labeled unlabeled data.Support vector learning also extended many non-standard settings one-classlearning (Schlkopf et al., 2001) support vector data description (Tax & Duin, 2004).idea latter learn hypersphere encloses bulk provided datainstances lie outside hypersphere considered anomalous. contrast, one-classSVM learns hyperplane feature space divides data points originmaximum-margin. translation-invariant kernel matrices, approaches equivalent.exist semi-supervised methods based unsupervised techniques.Blanchard, Lee, Scott (2010) propose method appealing option specifying upper threshold false-positives rate. However, method needs include test instances training time applicable online streaming scenarios anomalydetection. holds true extension one-class SVM Munoz Mar, Bovolo,Gmez-Chova, Bruzzone, Camp-Valls (2010) incorporates labeled examples graphLaplacian regularization term. Tax (2001) proposes straight-forward extension SVDDsemi-supervised anomaly detection, negatively labeled points required lie outsidehypersphereotherwise penalty incurred. advantage so-called SVDDneg approachassumptions underlying data-generating probability distributionmanifold assumptions imposed. Unfortunately, primal SVDDneg problem convexoptimization problem, makes difficult accurately optimize. Moreover, dual optimization proposed work Tax (2001) cannot considered sound alternative duepossible duality gaps. However, Appendix show convex reformulationSVDDneg translation-invariant kernels, RBF-kernels. new formulationsuffer duality gaps easily solved primal dual descent methods.problem occurs related semi-supervised one-class methods proposed Liu Zheng (2006)Wang, Neskovic, Cooper (2005).Another broad class methods deals learning positive unlabeled examples(LPUE). Intrinsically, one aims solving two-class problem data one class (thepositive class) given together unlabeled data points. LPUE thus applied problem setting hand identifying outlier class positively labeled data. Zhang Lee(2005) show class methods viewed special case semi-supervised learningemphasize SVDDneg (Tax, 2001) considered instance LPUE. Algorithmically, LPUE often solved iterative manner (i) identifying reliable set labeledexamples using classifier (ii) re-training classifier given new training set (Liu, Dai,239fiG RNITZ , K LOFT, R IECK , & B REFELDLi, Lee, & Yu, 2003; Zhang & Lee, 2005; Blum & Mitchell, 1998). Though work addresseslearning non-i.i.d. data (e.g., Li & Liu, 2005), underlying assumption usually impliestraining test sets drawn distribution.present article builds upon previous paper authors (Grnitz, Kloft, & Brefeld,2009). extends latter mathematical sound framework intuitive philosophical insights.addition, present general problem formulation employing arbitrary convex loss functions computation dual representation thereof, new empirical analysiscomparisons larger variety baseline approaches.3. Semi-supervised Anomaly Detectionanomaly detection tasks, given n observations x1 , . . . , xn X . underlying assumption bulk data stems (unknown) distribution callpart data normal. observations, however, originate different distributionsconsidered anomalies. anomalies could instance caused broken sensorsnetwork attacks cannot sampled definition. goal anomaly detection detectanomalies finding concise description normal data, deviating observationsbecome outliers. thus aim finding scoring function f : X R defines modelnormality. Following principle empirical risk minimization, optimization problem takesfollowing form,nXf = argmin (f ) +l(f (xi )),nfi=1l : R R appropriate loss function, : Rd R+ regularizer f ,trade-off parameter.approach based SVDD, computes hypersphere radius R centerc encompasses data. hypersphere model normality anomaly scoreinstance x computed distance center c,f (x) = ||(x) c||2 R2 .(1)Points lying outside ball (i.e., f (x) > 0) considered anomalous, points within(f (x) < 0) treated normal data. corresponding optimization problem knownsupport vector data description (SVDD, Tax, 2001) following formnXmin R2 + uR,c,s.t.i=1ni=1ni=1: k(xi ) ck2 R2 +: 0 ,trade-off u balances minimization radius sum erroneously placedpoints (that are, points lying outside normality radius). parameter u also servesestimate ratio outliers normal data n training examples. resultingPproblem convex solved equivalently dual space using representationc = ni=1 (xi ). consequence, input data expressed equivalently kernelfunction k(xi , xj ) = (xi )T (xj ) X corresponds feature map : X Freproducing kernel Hilbert space F (see, e.g., Mller, Mika, Rtsch, Tsuda, & Schlkopf, 2001).240fiT OWARD UPERVISED NOMALY ETECTION3.1 Semi-supervised Anomaly Detectionpropose novel approach semi-supervised anomaly detection. proposed methodgeneralizes vanilla SVDD processes unlabeled labeled examples. existing extensions SVDD employ dual optimization techniques inherently suffer duality gapsdue non-convexity, propose primal approach semi-supervised anomaly detection.discussed earlier, translation-invariant kernels, one-class SVM contained frameworkspecial case. addition n unlabeled examples x1 , . . . , xn X , given) X denotes set class labels.labeled observations (x1 , y1 ), . . . , (xm , ymsimplicity, focus = {+1, 1} = +1 encodes nominal data = 1anomalies.argued introduction, goal derive method grounds unsupervisedlearning paradigm. therefore stick hypersphere model SVDD use latterblueprint dealing unlabeled data. inclusion labeled examples follows simplepattern: example x labeled nomial (y = +1), require lies within hypersphere. contrast, example anomaly member outlier class (y = 1), wantplaced outside ball. straight-forward extension SVDD using labeledunlabeled examples thus givenminR,,c,s.t.2R + unX+ li=1ni=1:n+mj=n+1n+mX2jj=n+12k(xi ) ck R +: yj k(xj ) ck2 R2 + jni=1 :(2)0,n+mj=n+1 : j 0 ,margin labeled examples , u , l trade-off parameters. Unfortunately, inclusion negatively labeled data renders optimization problem non-convexoptimization dual space prohibitive. remedy, following approach ChapelleZien (2005), translate Equation (2) unconstrained problem. thereby resolve slackterms OP follows= ` R2 ||(xi ) c||2j = ` yj(3)R2 ||(xj ) c||2 .example, put `(t) = max{t, 0} (i.e., common hinge loss), recover (2). Furthermore, application representer theorem, obtain support-vector expansionc=nXi=1n+mX(xi ) +j=n+1241j yj (xj )(4)fiG RNITZ , K LOFT, R IECK , & B REFELD1hinge lossHuber losslinearloss0.750.5quadratic0.25linear000.51yf(x)1.52Figure 4: Non-differentiable hinge loss (dashed) differentiable Huber loss `=1,=0.5 (solid).(see Appendix B detailed derivation). Combining (3) (4), re-formulate optimizationproblem (2) solely terms kernels without constraints follows:minR,,2R + u+ lnX` R2 k(xi , xi ) + (2ei )0 Ki=1n+mX` yj R2 k(xj , xj ) + (2ej )0 K .(5)j=n+1Hereby K = (kij )1i,jn denotes kernel matrix given kij = k(xi , xj ) = h(xi ), (xj )ie1 , . . . , en+m standard base Rn+m . rephrasing problem unconstrainedoptimization problem, intrinsic complexity changed. Often, unconstrained optimizationeasier implement constrained optimization. non-smooth optimization possiblevia e.g. non-convex bundle methods described work (2010), smooth optimizationmethods conjugate gradient Newtons method easier apply. obtain smoothoptimization technique, choose Hubers robust loss (Huber, 1972). Huber loss two parameters controlling quadratic approximation terms center witdh , see Figure4. optimization function becomes differentiable off-the-shelf gradient-based optimizationtools applied. complete derivation gradients optimization problem (5) usingHubers robust loss shown Appendix C.3.2 Convex Semi-supervised Anomaly Detectionoptimization problem previous section easy implement but, unfortunately, nonconvex. Therefore, optimizers may find good local optimum, several restarts necessaryverify quality solutions cases optimization might fail completely. showthat, rather mild assumptions, namely data processed unit norm featurespace (as fulfilled by, e.g., RBF kernels), optimization problem convertedequivalent convex one. derivation general postulates nothing convexityloss function. approach based combination Lagrangian duality notionFenchel-Legendre conjugate function. Fenchel duality machine learning pioneered242fiT OWARD UPERVISED NOMALY ETECTIONRifkin Lippert (2007) assumption full-rank kernels. approach generalallows us use kernel K. byproduct derivation, show classicalone-class SVM special case general class density level set estimators minimizeconvex risk functional give general dual criterion class. aim, introduceLegendre-Fenchel conjugate given loss l(t)lc (z) = sup (zt l(t))use slightly different formulation SSAD problem, is, eliminate hinge lossslack variables , reformulate problem explicit loss functions:nn+mXX12||w|| + ul(ti ) + ll(tj )2min,,w,ti=1ni=1 :n+mj=n+1s.t.j=n+1ti = (w (xi )): tj =(yj wT (xj ))(P)yj0.Note that, problem, auxiliary variables ti introduced deal non-differentiableloss functions. Again, convex nature stated optimization problem,solve dual space. aim, use Lagrange Theorem incorporate constraintsobjective:nn+mXX12L = ||w|| + ul(ti ) + ll(tj )2i=1nXj=n+1((wT (xi )) ti )i=1n+mX(6)j ((yj wT (xj )) yj tj )j=n+1optimal solution found solving Lagrangian saddle point problemmax min EQ6., ,,w,tused standard Lagrangian ansatz, would compute derivate Lagrangianrespect primal variables. However, general loss function l() necessarily differentiable.remedy, compute derivatives wrt w, . Setting zero, yieldsoptimality conditions:0 uj :0 j lnn+mXXw =(xi ) +j yj (xj ).i=1j=n+1243(7)fiG RNITZ , K LOFT, R IECK , & B REFELDInserting optimality conditions Lagrangian, saddle point problem translatesnn+mXXj1max K + umin l(ti ) + ti + lminl(tj ) + tj .2uli=1j=n+1Converting min max statement resultsnn+mXXj1max K umax ti l(ti ) lmaxtj l(tj ) .2uli=1j=n+1Now, making use Legendre-Fenchel conjugate lc () described above, arrive following dual optimization problemmaxs.t.n+mnXXj1lc ( )lc ( ) lK u2ulj=n+1i=11=nXi=1+n+mXj yjj=n+1(D)n+mXj .j=n+1contrast existing semi-supervised approaches anomaly detection (Tax, 2001; Liu & Zheng,2006; Wang et al., 2005), strong duality holds shown following proposition.Proposition 3.1 optimization problems (P) (D) strong duality holds.Proof follows convexity (P) Slaters condition, trivially fulfilledadjusting tj : > 0 tj R : 0 = (yj wT (xj )) yj tj .observe optimization problems (P) (D) contain non-convex variantspecial case translation-invariant kernels. Difficulties may arise presence many equalityinequality constraints, increase computational requirements. However,inherent; left hand-side constraint removed discarding variable initialprimal problemthis leaves regularization path optimization problem invariantandright hand side inequality equivalently incorporated objective functionLagrangian argument (e.g., Proposition 12 Kloft, Brefeld, Sonnenburg, & Zien, 2011). Note,convex model intuitive interpretation normalized kernels. order dealwider class kernels, need resort general non-convex formulation presentedSection 3.1.4. Active Learning Semi-supervised Anomaly Detectionprevious section, presented two optimization problems incorporate labeled dataunsupervised anomaly detection technique. However, yet addressed questionacquiring labeled examples. Many topical real-world applications involve millions traininginstances (Sonnenburg, 2008) domain experts label small fraction unlabeleddata. Active learning deals finding instances that, labeled included training244fiT OWARD UPERVISED NOMALY ETECTIONset, lead largest improvement re-trained model. following, present activelearning strategy well-suited anomaly detection. core idea query low-confidencedecisions guide user labeling process.approach works follows. First, initialize method training unlabeledexamples. training set augmented particular examples selectedactive learning rule. candidates labeled domain expert added trainingset. model retrained refined training set, consists unlabeled labeledexamples. Subsequently labeling- retraining-steps repeated required performancereached.active learning rule consists two parts. begin commonly used activelearning strategy simply queries borderline points. idea method choosepoint closest decision hypersphere (Almgren & Jonsson, 2004; Warmuth, Liao, Rtsch,Mathieson, Putta, & Lemmen, 2003) presented expert:kf (x)kx0 = argmin= argmin R2 k(x) ck2 .(8)x{x1 ,...,xn } maxk kf (xk )kx{x1 ,...,xn }supervised support vector machines, strategy known margin strategy (Tong &Koller, 2000). Figure 5 (a) shows illustratation semi-supervised anomaly detection.dealing non-stationary outlier categories, beneficial identify novel anomalyclasses soon possible. translate requirement active learning strategy follows.Let = (aij )i,j=1,...,n+m adjacency matrix training instances, obtained by, example,k-nearest-neighbor approach, aij = 1 xi among k-nearest neighbors xj 0otherwise. introduce extended labeling y1 . . . , yn+m examples defining yi = 0unlabeled instances retaining labels labeled instances, i.e., yj = yj . Using pseudolabels, Equation (9) returns unlabeled instance accordingx0=n+m1 X(yj + 1) aij .xi {x1 ,...,xn } 2kargmin(9)j=1strategy explores unknown clusters feature space thus labels orthogonal complementary instances illustrated Figure 5 (b).Nevertheless, using Equation (9) alone may result querying points lying close centeractual hypersphere. points hardly contribute improvement hypersphere.hand, using margin strategy alone allow querying novel regionslie far away margin. words, combination strategies (8) (9)guarantees points interest queried. final active learning strategy therefore givenx0 =argminxi {x1 ,...,xn }=n+mkf (x)k 1 X+(yj + 1) aijc2k(10)j=1[0, 1]. combined strategy queries instances close boundary hypersphere lie potentially anomalous clusters respect k-nearest neighbor graph, seeFigure 5 (c) illustration. Depending actual value , strategy jumps clustercluster thus helps identify interesting regions feature space. special caselabeled points, combined strategy reduces margin strategy.245fiG RNITZ , K LOFT, R IECK , & B REFELD(a) margin strategy(b) cluster strategy(c) combined strategyFigure 5: Comparison active learning strategies (queried points marked blue): (a)margin strategy queries data points closest decision boundary, (b) clusterstrategy queries points rarely labeled regions, (c) combined strategy queriesdata points likely anomalies clusters near decision boundary.Usually, active learning step followed optimization step semi-supervisedSVDD, update model respect recently labeled data. procedure course timeconsuming altered practical settings, instance querying couple pointsperforming model update. Irrespectively actual implementation, alternatingactive learning updating model repeated desired predictive performanceobtained.5. Illustration Proposed Learning Paradigmsection, illustrate weaknesses existing learning paradigms semi-supervisedanomaly detection settings means controlled experiment synthetic data. results,already briefly sketched introduction (cf., Figure 2), discussed detailhere.Table 1: Competitors toy data experiment.two-classsupervised transductiveSVMLDSunsupervisedSVDDone-classLPUEsemi-supervisednegSVDDSSADend, generate nominal training validation data two isotropic Gaussiandistributions R2 one anomaly cluster (shown Figure 3 (left)). However, testing time,two novel anomaly clusters appear test data (shown Figure 3 (right)). reflects characteristic anomalies stem novel, previously unseen distributions. comparenewly-developed method SSAD following baseline approaches: unsupervised support246fiT OWARD UPERVISED NOMALY ETECTION8070AUC[0,0.01] [in %]60504030SVDDSVM100(unsupervised)SVDDneg (LPUE)201 5 10 15(supervised)LDS(transductive)SSAD(proposed method)5075100% labeled dataFigure 6: Performance various unsupervised, supervised semi-supervised methodsanomaly detection setting.vector domain description (SVDD, Tax & Duin, 2004), corrected semi-supervised SVDDneg(Tax, 2001) described Appendix A, supervised support vector machine (SVM, Boser, Guyon,& Vapnik, 1992; Cortes & Vapnik, 1995), semi-supervised low-density separation (LDS,Chapelle & Zien, 2005), see Table 1. common anomaly detection setups, measure areaROC curve interval [0, 0.01] report AUCs averaged 25 repetitionsdistinct training, validation, test sets. every repetition, parameters u , l adjustedrespective validation set within interval [102 , 102 ]. experiments, used = 1.Error bars correspond standard errors.results shown Figure 6, horizontal axis shows different ratios labeledrandomly drawn unlabeled examples. Methods derived supervised learning paradigmSVM LDS cannot cope novel outlier clusters perform poorly ratioslabeled unlabeled examples; performance remains unsupervised SVDD,utilize labeled data thus unaffected incorporating labels training process. contrast, two semi-supervised methods derived unsupervised learningparadigm clearly outperform baselines. However, SVDDneg benefits anomalous labeled data since sparse, needs big fraction labeled data increaseperformance. semi-supervised method SSAD exploits every single labeled example needs15% labels saturate around optimum.Figure 7 visualizes typical contour lines hypotheses computed SSAD three differentscenarios. figure shows fully-supervised scenario instances correctly labeled(left), semi-supervised solution 25% data labeled 75% remains unlabeled(center), completely unsupervised one using unlabeled data (right). Unsurprisingly,fully supervised solution discriminates perfectly normal data outliersunsupervised solution recognizes latter normal data mistake. intermediate semisupervised solution uses little label information also achieve perfect separationinvolved classes.247fiG RNITZ , K LOFT, R IECK , & B REFELD(a)(b)(c)Figure 7: Different solutions fully-supervised (left), semi-supervised (center), unsupervisedanomaly detection using RBF kernels. Colors indicate unlabeled data (green), labeledoutliers (red), labeled normal instances (violet).0.3SVDD0.250.2time(unsupervised)SVDDneg (LPUE)LDSSVM(transductive)(supervised)SSAD(proposed method)0.150.10.05050100150200examplesFigure 8: Execution times.Figure 8 compares execution times different methods shows number trainingexamples versus training time. simplicity, discarded 50% labels training datarandom. results show methods, SSAD, SVDDneg , SVDD, derivedunsupervised learning principle, perform similarly. SVM performs best useslabeled part training data ignores unlabeled examples. Low density separation(LDS) performs worst due transductive nature.Based observations, draw following conclusions. Anomaly detection scenariosrender methods derived supervised learning paradigm inappropriate. Even unsupervisedmethods ignoring label information may perform better supervised peers. Intuitively,discarding label information sub-optimal. experiment shows semi-supervised methodsunsupervised learning paradigm effectively incorporate label information outperformcompetitors. confirm findings Section 6.248fiT OWARD UPERVISED NOMALY ETECTION6. Real-World Network Intrusion Detectiongoal network intrusion detection identify attacks incoming network traffic. Classicalsignature-based proven insufficient identification novel attacks, signaturesneed manually crafted advance. Therefore machine learning approaches gainingattention intrusion detection research community.detection unknown novel attacks requires adequate representation networkcontents. remainder, apply technique embedding network payloads vector spacesderived concepts information retrieval (Salton, Wong, & Yang, 1975) recentlyapplied application domain network intrusion detection (Rieck & Laskov, 2007).network payload x (the data contained network packet connection) mapped vectorspace using set strings embedding function . string function(x) returns 1 contained payload x 0 otherwise. applying (x)elements obtain following map: X R|S| ,: x 7 (s (x))sS ,(11)X domain network payloads. Defining set relevant strings priori difficult typical patterns novel attacks available prior disclosure. alternative,define set implicitly associate possible strings length n. resultingset strings often referred n-grams.consequence using n-grams, network payloads mapped vector spacen256 dimensions, apparently contradicts efficient network intrusion detection. Fortunately,payload length comprises (T n) different n-grams and, consequently, mapsparse, is, vast majority dimensions zero. sparsity exploited derivelinear-time algorithms extraction comparison embedded vectors. Instead operatingfull vectors, non-zero dimensions considered, extracted strings associateddimension maintained efficient data structures (Rieck & Laskov, 2008).experiments, consider HTTP traffic recorded within 10 days Fraunhofer InstituteFIRST. data set comprises 145,069 unmodified connections average length 489 bytes.incoming byte stream connection mapped vector space using 3-grams detailedabove. refer FIRST data normal pool. malicious pool contains 27 real attackclasses generated using Metasploit framework (Maynor, Mookhey, Cervini, & Beaver, 2007).covers 15 buffer overflows, 8 code injections 4 attacks including HTTP tunnels crosssite scripting. Every attack recorded 26 different variants using virtual network environmentdecoy HTTP server, attack payload adapted match characteristics normaldata pool. detailed description data set provided Rieck (2009).study robustness approach realistic scenario, also consider techniquesobfuscate malicious content adapting attack payloads mimic benign traffic feature space(Fogla, Sharif, Perdisci, Kolesnikov, & Lee, 2006; Perdisci et al., 2009). consequence,extracted features deviate less normality classifier likely fooled attack.purposes, already suffices study simple cloaking technique adding common HTTPheaders payload malicious body attack remains unaltered. applytechnique malicious pool refer obfuscated set attacks cloaked pool.249fiG RNITZ , K LOFT, R IECK , & B REFELD6.1 Detection Performancesection, evaluate statistical performance SSAD intrusion detection, comparison baseline methods SVDD SVDDneg . addition, combined active learningstrategy compared random sampling.focus two scenarios: normal vs. malicious normal vs. cloaked data. settings,respective byte streams translated bag-of-3-grams representation. experiment,randomly draw 966 training examples normal pool 34 attacks, dependingscenario, either malicious cloaked pool. Holdout test sets also drawnrandom consist 795 normal connections 27 attacks, each. make sure attacksattack class occur either training, test set both. report10 repetitions distinct training, holdout, test sets measure performance areaROC curve false-positive interval [0, 0.01] (AUC0.01 )Figure 9(a) shows results normal vs. malicious data pools, x-axis depictspercentage randomly drawn labeled instances. Irrespectively amount labeled data,malicious traffic detected methods equally well intrinsic nature attacks wellcaptured bag-of-3-grams representation (cf., Wang, Parekh, & Stolfo, 2006; Rieck & Laskov,2006). significant difference classifiers.Figure 9(b) shows results normal vs. cloaked data. First all, performanceunsupervised SVDD drops 70%. obtain similar result SVDDneg ; incorporatingcloaked attack information training process SVDD leads increase 5%far practical value. Notice SVDDneg cannot make use labeled datanormal class. Thus, moderate ascent terms number labeled examples creditedclass ratio 966/34 random labeling strategy. bulk additional informationcannot exploited left out. contrast, semi-supervised method SSAD includeslabeled data training process clearly outperforms two baselines. 5%labeled data, SSAD easily beats best baseline randomly labeling 15% availabledata separates normal cloaked malicious traffic almost perfectly.Nevertheless, labeling 15% data realistic practical applications. thus explorebenefit active learning inquiring label information borderline low-confidence points.Figure 9(c) shows results normal vs. cloaked data, labeled data SVDDnegSSAD chosen according active learning strategy Equation (10). unsupervisedSVDD make use labeled information unaffected setup, remainingAUC0.01 70%. Compared results using random labeling strategy (Figure 9(b)),performance SVDDneg increases significantly. ascent SVDDneg steeperperformance yields 85% 15% labeled data. However, SSAD also improves activelearning dominates baselines. Using active learning, need label 3% dataattaining almost perfect separation, compared 25% random labeling strategy.conclude active learning strategy effectively improves performance reducesmanual labeling effort significantly.Figure 10 impact active learning strategy given Equation (10) shown.compare number outliers detected combined strategy margin-based strategyEquation (8) (see also, Almgren & Jonsson, 2004) randomly drawing instancesunlabeled pool. sanity check, also included theoretical outcome random sampling.250fiT OWARD UPERVISED NOMALY ETECTION11AUC[0,0.01] [in %]0.990.9850.980.9750.970.965SVDDSVDDnegSSAD (proposed method)0.80.7SVDDnegSSAD (proposed method)0.9550.950.9SVDD0.9605100151361015% labeled data% labeled data(a) Detection accuracies regular attacks.(b) Detection accuracies cloaked attacks.1AUC[0,0.01] [in %]AUC[0,0.01] [in %]0.9950.90.80.701361015% labeled data(c) Detection accuracies cloaked attacks using proposed active learning strategy SSAD.Figure 9: Results network intrusion detection experiment. detection accuraciesregular attacks insignificantly different (see (a)), SSAD achieves 30% higheraccuracies baseline approaches cloaked data (see (b)). proposed activitylearning strategy increases accuracy labeled data rare (see (c)).results show combined strategy effectively detects malicious traffic much fastermargin-based strategy.6.2 Threshold Adaptationprevious experiments demonstrated advantages active learning network intrusiondetection. far, results obtained using method SSAD; however, activelearning techniques devised Section 4 also applicable calibrating learning-basedmethods. herein focus vanilla SVDD parameter value = 1, correspondsclassical centroid-based anomaly detection (e.g., Shawe-Taylor & Cristianini, 2004),251fiG RNITZ , K LOFT, R IECK , & B REFELD3530#outliers2520optimalactive learningmarginrandom (empirical)random (theoretical)1510500123457.510labeled data %15Figure 10: Number novel attacks detected combined active learning strategy (blue line),random sampling (red solid dotted line), margin strategy (purple line) upperbound (light blue dotted line) single run.results directly transfer anomaly detectors Anagram (Wang et al., 2006), McPad (Perdisciet al., 2009) TokDoc (Krueger, Gehl, Rieck, & Laskov, 2010).draw set 3,750 network connections pool normal data splitresulting set training set 2,500 connections test partition 1,250 events. setsmixed cloaked attack instances. SVDD trained normal training setdelivering threshold R. application learned hypersphere test set, evaluatedifferent strategies determining radius R using random sampling active learning.cases, selected connections labeled threshold obtained computing meanlabeled instances:Rmaxi d(xi )R =minj d(x)PP jd(xi )+ j d(xj )#pos+#neg: #pos = 0: #pos > 0: #pos = 0#neg = 0#neg = 0#neg > 0:#neg > 0#pos > 0(12)xi positive labeled examples, xj negative examples d(x) = ||(x) c||denotes distance current sample x hyperspheres origin. Figure 11 showsROC curve SVDD computed thresholds various levels labeled data. Resultsaveraged 10 random draws working sets. One see even small amountslabeled data active learning strategy finds reasonable radius random strategyvanilla SVDD completely fail false-positive rate 0.5 1, respectively. resultdemonstrates active learning strategies enable calibrating anomaly detectors significantly252fiT OWARD UPERVISED NOMALY ETECTION1TPR0.80.60.4ROC curveSVDD thresholdactive learning 5%random sampling 5%0.2000.20.40.60.81FPRFigure 11: Results threshold adaption experiment: ROC curve SVDD (grey)thresholds determined SVDD (green), proposed combined strategy (blue),random sampling (red) shown (labeling 5% data).reduced effort comparison random sampling hence provide valuable instrumentdeploying learning methods practice.7. Conclusionarticle, developed framework semi-supervised anomaly detection, allowsinclusion prior expert knowledge. discussed conceptual difference semisupervised models derived unsupervised supervised techniques proposedgeneralization support vector data description incorporate labeled data. optimizationproblem semi-supervised anomaly detection (SSAD) unconstrained, continuous problem,allows efficient optimization gradient-based methods convex equivalentmild assumptions kernel function.approached semi-supervised anomaly detection unsupervised learning paradigm.proposed novel active learning strategy specially tailored anomaly detection.strategy guides user labeling process querying instances closeboundary hypersphere, also likely contain instances novel outlier categories.Empirically, applied semi-supervised anomaly detection application domain network intrusion detection. showed rephrasing unsupervised problem semi-supervisedtask beneficial practice: SSAD proves robust scenarios performance baselineapproaches deteriorates due obfuscation techniques. Moreover, demonstrated effectiveness active learning strategy couple data sets observed SSAD significantlyimprove prediction accuracy effectively exploiting limited amount labeled available.observed handful labeled instances necessary boost performance.characteristic especially appealing tasks labeling data costly network securitytraffic inspected malicious patterns expert expert.253fiG RNITZ , K LOFT, R IECK , & B REFELDmany possibilities exploit extend learning approach well activelearning strategy. example, replacing `2 -norm regularization sparsity-inducing `1 -normincorporate automatic feature selection reduces dimensionality solution. Learningsparse feature representations great interest computer security applicationssignature generation. possible optimization strategy could linear programming (LP)approach Campbell Bennett (2001) data domain description. However, choicesregularizers certainly possible including structured regularizers incorporate hierarchieslearning process non-isotropic norms encode additional domain knowledge. Incorporatingmultiple labels rephrasing semi-supervised anomaly detection multi-task problem mightalso improve accuracy complex application domains.Acknowledgmentsauthors grateful Klaus-Robert Mller comments helped improvingmanuscript. work supported part German Bundesministerium fr Bildung undForschung (BMBF) project PROSEC (FKZ 01BY1145), FP7-ICT ProgrammeEuropean Community, PASCAL2 Network Excellence, German National Science Foundation (DFG) GA 1615/1-1, MU 987/6-1, MU 987/11-1 RA 1894/11. Furthermore, Marius Kloft acknowledges PhD scholarship German Academic ExchangeService (DAAD) postdoctoral fellowship German Research Foundation (DFG) wellfunding Ministry Education, Science, Technology, National ResearchFoundation Korea Grant R31-10008. part work done Marius KloftComputer Science Division Department Statistics, University California, Berkeley,CA 94720-1758, USA.Appendix A. Analysis SVDDnegappendix, point limitation previously published methods SVDDneg (Tax,2001) well methods proposed Hoi, Chan, Huang, Lyu, King (2003), Liu Zheng(2006), Wang et al. (2005), Yuan Casasent (2004). methods suffer potentialduality gaps optimized dual space butdepending training datarun riskoriginating non-convex optimization problem. instance, casesingle negatively labeled example included training set. issue addressedaforementioned papers.exemplarily illustrate problem SVDDneg . SVDDneg incorporates labeled examples outlier class otherwise unsupervised learning process. before, majority(unlabeled) data points shall lie inside sphere labeled outliers constrainedlie outside normality ball. results two-class problem positive classconsists unlabeled data negative class formed labeled outliers. introducingclass labels {+1, 1} (where unlabeled data points receive class label yi = +1), primaloptimization problem givenminR,c,R2 +nXi=1s.t. ni=1 : yi k(xi ) ck2 yi R2 +2540,(13)fiT OWARD UPERVISED NOMALY ETECTION45x 10objective432100PrimalDual20406080100negativesFigure 12: Exemplary duality gap SVDDneg using linear kernel = 100. horizontal axisshows percentage negative (anomalous) points training set vertical axisshows primal dual objective values.corresponding optimization problem dual space givenmaxs.t.nXi=1nXyi k(xi , xi )nXj yi yj k(xi , xj )(14)i,j=1yi = 1 0= 1, . . . , n.i=1existence duality gap shown follows: second derivative primal constraintsg(xi ) = yi k(xi ) ck2 yi R2 0 given 2 g/c2 = 2yi negative outlierslabel equals yi = 1. turns whole optimization problem non-convex. consequence,optimal solutions primal dual problems may differ. Figure 12 shows exemplaryplot duality gap artificially generated data one nominal Gaussian surroundedsmaller anomalous Gaussian. labeling process data points receivecorresponding label negative examples present learning problem (horizontalaxis) larger duality gap larger difference two objective values (verticalaxis). Note duality gap necessarily monotonic function although behaviorlikely case. Furthermore, maximization dual problem yields lower boundprimal objective (blue line), whereas latter always greater equal correspondingdual (red line).Nevertheless, following Theorem shows class translation-invariant kernel functions, exists equivalent convex re-formulation form one-class SVM (Schlkopfet al., 2001).Theorem A.1 solution found optimizing dual non-convex SVDDneg statedEquations (14) identical dual corresponding convex one-class SVM problemstated Equation (15) kernel translation-invariant, i.e., k(xi , xi ) = i, R+ .255fiG RNITZ , K LOFT, R IECK , & B REFELDProof dual one-class SVM givenmaxn1 Xj yi yj k(xi , xj ),2nXs.t.i,j=1yi = 10i.(15)i=1respective constraints already equivalent. dual SVDDneg objective translationinvariant kernel reducesnnXX= argmaxyi k(xi , xi )j yi yj k(xi , xj )i=1= argmaxnsi,j=1nXyii=1ns= argmaxnXj yi yj k(xi , xj )i,j=1nXj yi yj k(xi , xj ),(16)i,j=1substituted equality constraint Eq. (15) last step. Finally, Equation (16)precisely one-class SVM dual objective, scaled 12 shifted constant ns. However,optimal solution affected transformation completes proof.Appendix B. Representer Theorem SSADsection, show applicability representer theorem semi-supervised anomalydetection.Theorem B.1 (Representer Theorem Schlkopf & Smola, 2002) Let H reproducing kernel Hilbert space kernel k : X X R, symmetric positive semi-definite functioncompact domain. function L : Rn R, nondecreasing function : R R.J := min J(f )f H := min f H{ ||f ||2H + L (f (x1 ), . . . , f (xn ))}well-defined, exist 1 , . . . , n R,f () =nXk(xi , )(17)i=1J .achieves J(f ) =Furthermore, increasing, minimizer J(f ) expressedform Eq. (17).Proposition B.2 representer theorem applied non-expanded version Equation(5).Proof Recall primal SSAD objective function given2J(R, , c) =R + unX` R2 ||(xi ) c||2i=1+ ln+mX` yj R2 ||(xj ) c||2 .j=n+1256fiT OWARD UPERVISED NOMALY ETECTIONSubstituting := R2 ||c||2 leads new objective function2J(T, , c) =||c|| + + unX` ||(xi )||2 + 2(xi )0 ci=1+ ln+mX` yj ||(xj )||2 + 2(xj )0 c .j=n+1Expanding center c terms labeled unlabeled input examples coveredrepresenter theorem. optimization, easily re-substituted obtain primalvariables R, , c. completes proof.Appendix C. Computing Gradients Eq. (5)section compute gradients SSAD formulation given Eq. (5).neccessary step implement gradient-based solver SSAD. end, considerunconstrained optimization problem givenminR,,2R + unX`, R2 k(xi , xi ) + (2ei )0 Ki=1n+mX+ l`, yj R2 k(xj , xj ) + (2ej )0 K ,j=n+1`, Huber loss given(+t)2`, (t) =40:: t+: otherwise.notational convenience, focus Huber loss `=0, (t) move margin dependentterms argument compute gradients several steps, follows: first, buildgradient respect primal variables R c, yields= 2R`0 (R2 ||(xi ) c||2 )R= 2((xi ) c)`0 (R2 ||(xi ) c||2 ).c(18)derivatives counterparts j labeled examples respect R, , cgivenj= 2yj R`0 yj R2 ||(xj ) c||2Rj= `0 yj R2 ||(xj ) c||2j= 2yj ((xj ) c)`0 yj R2 ||(xj ) c||2 .c257fiG RNITZ , K LOFT, R IECK , & B REFELDSubstituting partial gradients, resolve gradient Equation (5) respect primalvariables follows:nn+mXX jEQ5= 2R + u+ l,RRR(19)n+mX jEQ5= + l,(20)nn+mXX jEQ5= u+ l.ccc(21)i=1j=n+1j=n+1i=1j=n+1following, extend approach allow use kernel functions. applicationrepresenter theorem shows center c expandedc=nX(xi ) +i=1n+mXj yj (xj ).(22)j=n+1According chain rule, gradient Equation (5) respect i/j givenEQ5EQ5 c=.i/jc i/jUsing Equation (22), partial derivativesci/jresolvec= (xi )c= yj (xj ),j(23)respectively. Applying chain-rule Equations (19),(20),(21), (23) gives gradientsEquation (5) respect i/j .ReferencesAlmgren, M., & Jonsson, E. (2004). Using active learning intrusion detection. Proc. IEEEComputer Security Foundation Workshop, pp. 8889.Andrews, D. F., & Pregibon, D. (1978). Finding outliers matter. Journal RoyalStatistical Society. Series B (Methodological), 40(1), 8593. /Blanchard, G., Lee, G., & Scott, C. (2010). Semi-Supervised Novelty Detection. Journal MachineLearning Research, , 2973297330093009.Blum, A., & Mitchell, T. (1998). Combining labeled unlabeled data co-training. COLT98: Proc. eleventh annual conference Computational learning theory, pp. 92100,New York, NY, USA. ACM.Boser, B., Guyon, I., & Vapnik, V. (1992). training algorithm optimal margin classifiers.Haussler, D. (Ed.), Proceedings 5th Annual ACM Workshop Computational LearningTheory, pp. 144152.258fiT OWARD UPERVISED NOMALY ETECTIONCampbell, C., & Bennett, K. (2001). linear programming approach novelty detection. Leen,T., Dietterich, T., & Tresp, V. (Eds.), Advances Neural Information Processing Systems,Vol. 13, pp. 395401. MIT Press.Chandola, V., Banerjee, A., & Kumar, V. (2009). Anomaly detection: survey. ACM ComputingSurveys, 41(3), 158.Chapelle, O., Chi, M., & Zien, A. (2006). continuation method semi-supervised SVMs.ICML, pp. 185192, New York, New York, USA. ACM.Chapelle, O., & Zien, A. (2005). Semi-supervised classification low density separation. Proc.International Workshop AI Statistics.Chapelle, O., Schlkopf, B., & Zien, A. (2006). Semi-Supervised Learning (Adaptive ComputationMachine Learning). MIT Press.Cortes, C., & Vapnik, V. (1995). Support vector networks. Machine Learning, 20, 273297.Do, T.-M.-T. (2010). Regularized bundle methods large-scale learning problems application large margin training hidden Markov models. Ph.D. thesis, Pierre MarieCurie University Paris.Eskin, E., Arnold, A., Prerau, M., Portnoy, L., & Stolfo, S. (2002). Applications Data MiningComputer Security, chap. geometric framework unsupervised anomaly detection:detecting intrusions unlabeled data. Kluwer.Fogla, P., Sharif, M., Perdisci, R., Kolesnikov, O., & Lee, W. (2006). Polymorphic blending attacks.Proc. USENIX Security Symposium.Goh, K.-S., Chang, E. Y., & Li, B. (2005). Using one-class two-class svms multiclass imageannotation. IEEE Transactions Knowledge Data Engineering, 17, 13331346.Grnitz, N., Kloft, M., & Brefeld, U. (2009). Active semi-supervised data domain description.ECML/PKDD (1), pp. 407422.Heller, K., Svore, K., Keromytis, A., & Stolfo, S. (2003). One class support vector machinesdetecting anomalous windows registry accesses. Proc. workshop Data MiningComputer Security.Hoi, C.-H., Chan, C.-H., Huang, K., Lyu, M., & King, I. (2003). Support vector machines classrepresentation discrimination. Proc. International Joint Conference NeuralNetworks.Huber, P. (1972). Robust statistics: review. Ann. Statist., 43, 1041.Joachims, T. (1999). Transductive inference text classification using support vector machines.International Conference Machine Learning (ICML), pp. 200209, Bled, Slowenien.Kloft, M., Brefeld, U., Sonnenburg, S., & Zien, A. (2011). `p -norm multiple kernel learning. JournalMachine Learning Research, 12, 953997.Kruegel, C., Vigna, G., & Robertson, W. (2005). multi-model approach detection webbased attacks. Computer Networks, 48(5).Krueger, T., Gehl, C., Rieck, K., & Laskov, P. (2010). TokDoc: self-healing web applicationfirewall. Proc. 25th ACM Symposium Applied Computing (SAC), pp. 18461853.259fiG RNITZ , K LOFT, R IECK , & B REFELDLai, C., Tax, D. M. J., Duin, R. P. W., Zbieta, E., Ekalska, P., & Ik, P. P. (2004). studycombining image representations image classification retrieval. International JournalPattern Recognition Artificial Intelligence, 18, 867890.Li, X.-l., & Liu, B. (2005). Learning Positive Unlabeled Examples Different DataDistributions. ECML.Liu, B., Dai, Y., Li, X., Lee, W. S., & Yu, P. S. (2003). Building Text Classifiers Using PositiveUnlabeled Examples. IEEE International Conference Data Mining, pp. 179186. IEEEComput. Soc.Liu, Y., & Zheng, Y. F. (2006). Minimum enclosing maximum excluding machine patterndescription discrimination. ICPR 06: Proc. 18th International ConferencePattern Recognition, pp. 129132, Washington, DC, USA. IEEE Computer Society.Munoz Mar, J., Bovolo, F., Gmez-Chova, L., Bruzzone, L., & Camp-Valls, G. (2010). SemiSupervised One-Class Support Vector Machines Classification Remote Sensing Data.IEEE Transactions Geoscience Remote Sensing, 48(8), 31883197.Manevitz, L. M., & Yousef, M. (2002). One-class svms document classification. J. Mach. Learn.Res., 2, 139154.Mao, C.-H., Lee, H.-M., Parikh, D., Chen, T., & Huang, S.-Y. (2009). Semi-supervised co-trainingactive learning based approach multi-view intrusion detection. SAC 09: Proc.2009 ACM symposium Applied Computing, pp. 20422048, New York, NY, USA.ACM.Markou, M., & Singh, S. (2003a). Novelty detection: review part 1: statistical approaches. SignalProcessing, 83, 24812497.Markou, M., & Singh, S. (2003b). Novelty detection: review part 2: neural network basedapproaches. Signal Processing, 83, 24992521.Maynor, K., Mookhey, K., Cervini, J. F. R., & Beaver, K. (2007). Metasploit Toolkit. Syngress.Mller, K.-R., Mika, S., Rtsch, G., Tsuda, K., & Schlkopf, B. (2001). introduction kernelbased learning algorithms. IEEE Neural Networks, 12(2), 181201.Onoda, T., Murata, H., & Yamada, S. (2006). One class classification methods based non-relevancefeedback document retrieval. WI-IATW 06: Proc. 2006 IEEE/WIC/ACM international conference Web Intelligence Intelligent Agent Technology, pp. 393396, Washington, DC, USA. IEEE Computer Society.Park, J., Kang, D., Kim, J., Kwok, J. T., & Tsang, I. W. (2007). SVDD-based pattern denoising.Neural Computation, 19, 19191938.Paxson, V. (1999). Bro: System Detecting Network Intruders Real-Time. Elsevier ComputerNetworks, 31(23-24), 24352463.Pelleg, D., & Moore, A. (2004). Active learning anomaly rare-category detection. Proc.Advances Neural Information Processing Systems, pp. 10731080.Perdisci, R., Ariu, D., Fogla, P., Giacinto, G., & Lee, W. (2009). McPAD: multiple classifiersystem accurate payload-based anomaly detection. Computer Networks, 5(6), 864881.260fiT OWARD UPERVISED NOMALY ETECTIONRieck, K. (2009). Machine Learning Application-Layer Intrusion Detection. Ph.D. thesis, BerlinInstitute Technology (TU Berlin).Rieck, K., & Laskov, P. (2006). Detecting unknown network attacks using language models.Detection Intrusions Malware, Vulnerability Assessment, Proc. 3rd DIMVAConference, LNCS, pp. 7490.Rieck, K., & Laskov, P. (2007). Language models detection unknown attacks networktraffic. Journal Computer Virology, 2(4), 243256.Rieck, K., & Laskov, P. (2008). Linear-time computation similarity measures sequential data.Journal Machine Learning Research, 9(Jan), 2348.Rifkin, R. M., & Lippert, R. A. (2007). Value regularization fenchel duality. Journal MachineLearning Research, 8, 441479.Roesch, M. (1999). Snort: Lightweight intrusion detection networks. Proc. USENIX LargeInstallation System Administration Conference LISA, pp. 229238.Salton, G., Wong, A., & Yang, C. (1975). vector space model automatic indexing. Communications ACM, 18(11), 613620.Schlkopf, B., & Smola, A. (2002). Learning Kernels. MIT Press, Cambridge, MA.Schlkopf, B., Platt, J. C., Shawe-Taylor, J., Smola, A. J., & Williamson, R. C. (2001). Estimatingsupport high-dimensional distribution. Neural Computation, 13(7), 14431471.Shawe-Taylor, J., & Cristianini, N. (2004). Kernel methods pattern analysis. Cambridge University Press.Sindhwani, V., Niyogi, P., & Belkin, M. (2005). Beyond point cloud: transductivesemi-supervised learning. ICML, Vol. 1, pp. 824831. ACM.Sonnenburg, S. (2008). Machine Learning Genomic Sequence Analysis. Ph.D. thesis, FraunhoferInstitute FIRST. supervised K.-R. Mller G. Rtsch.Stokes, J. W., & Platt, J. C. (2008). Aladin: Active learning anomalies detect intrusion. Tech.rep., Microsoft Research.Stolfo, S. J., Apap, F., Eskin, E., Heller, K., Hershkop, S., Honig, A., & Svore, K. (2005).comparative evaluation two algorithms windows registry anomaly detection. JournalComputer Security, pp. 659693.Tax, D. M. J. (2001). One-class classification. Ph.D. thesis, Technical University Delft.Tax, D. M. J., & Duin, R. P. W. (2004). Support vector data description. Machine Learning, 54,4566.Tong, S., & Koller, D. (2000). Support vector machine active learning applications textclassification. Proc. Seventeenth International Conference Machine Learning,San Francisco, CA. Morgan Kaufmann.Vapnik, V. (1998). Statistical Learning Theory. Wiley, New York.Wang, J., Neskovic, P., & Cooper, L. N. (2005). Pattern classification via single spheres. Computer Science: Discovery Science (DS), Vol. 3735, pp. 241252.261fiG RNITZ , K LOFT, R IECK , & B REFELDWang, K., Parekh, J., & Stolfo, S. (2006). Anagram: content anomaly detector resistant mimicryattack. Recent Adances Intrusion Detection (RAID), pp. 226248.Warmuth, M. K., Liao, J., Rtsch, G., Mathieson, M., Putta, S., & Lemmen, C. (2003). Activelearning support vector machines drug discovery process. Journal ChemicalInformation Computer Sciences, 43(2), 667673.Yuan, C., & Casasent, D. (2004). Pseudo relevance feedback biased support vector machine.Proc. International Joint Conference Neural Networks.Zhang, D., & Lee, W. S. (2005). simple probabilistic approach learning positiveunlabeled examples. Proceedings 5th Annual UK Workshop . . . .262fiJournal Artificial Intelligence Research 46 (2013) 511-577Submitted 12/12; published 03/13Probabilistic Planning Continuous Dynamic SystemsBounded RiskMasahiro OnoONO @ APPI . KEIO . AC . JPKeio University3-14-1 Hiyoshi, Kohoku-kuYokohama, Kanagawa, 223-8522 JapanBrian C. WilliamsWILLIAMS @ MIT. EDUMassachusetts Institute Technology77 Massachusetts AvenueCambridge, 02139 USALars BlackmoreLARS . BLACKMORE @ SPACEX . COMSpaceX1 Rocket RoadHawthorne, CA 90250 USAAbstractpaper presents model-based planner called Probabilistic Sulu Planner p-SuluPlanner, controls stochastic systems goal directed manner within user-specified riskbounds. objective p-Sulu Planner allow users command continuous, stochasticsystems, unmanned aerial space vehicles, manner intuitive safe.end, first develop new plan representation called chance-constrained qualitative stateplan (CCQSP), users specify desired evolution plant state wellacceptable level risk. example CCQSP statement go B within 30minutes, less 0.001% probability failure. develop p-Sulu Planner,tractably solve CCQSP planning problem. order enable CCQSP planning, developfollowing two capabilities paper: 1) risk-sensitive planning risk bounds, 2)goal-directed planning continuous domain temporal constraints. first capabilityensures probability failure bounded. second capability essential plannersolve problems continuous state space vehicle path planning. demonstratecapabilities p-Sulu Planner simulations two real-world scenarios: path planningscheduling personal aerial vehicle well space rendezvous autonomous cargospacecraft.1. Introductionincreasing need risk-sensitive optimal planning uncertain environments,guaranteeing acceptable probability success. motivating example articleBoeing concept future aerial personal transportation system (PTS), shown Figure 1.PTS consists fleet small personal aerial vehicles (PAV) enable flexible point-to-pointtransportation individuals families.c2013AI Access Foundation. rights reserved.fiO , W ILLIAMS , & B LACKMOREorder provide safety, PTS highly automated. 2004, US, pilot errorlisted primary cause 75.5% fatal general aviation accidents, according 2005 JosephT. Nall Report (Aircraft Owners Pilots Association Air Safety Foundation, 2005). Automatedpath planning, scheduling, collision avoidance, traffic management significantly improvesafety PTS, well efficiency. challenges operating system includeadapting uncertainties environment, storms turbulence, satisfyingcomplicated needs users.substantial body work planning uncertainty relevant. However,approach distinctive three key respects. First, planner, p-Sulu Planner, allowsusers explicitly limit probability constraint violation. capability particularly important risk-sensitive missions impact failure significant. Second, plannergoal-directed, mean achieves time-evolved goals within user-specified temporal constraints. Third, planner works continuous state space. continuous state spacerepresentation fits naturally many real-world applications, planning aerial, space,underwater vehicles. also important problems resources.Figure 1: Personal Transportation System (PTS). (Courtesy Boeing Company)Figure 2 shows sample PTS scenario. passenger PAV starts Provincetown,wants go Bedford within 30 minutes. passenger also wants go scenicarea remain 5 10 minutes flight. no-fly zone (NFZ)storm must avoided. However, storms future location uncertain; vehicleslocation uncertain well, due control error exogenous disturbances. Thus riskpenetrating NFZ storm. passengers want limit risk 0.001%.order handle planning problem, introduce novel planner called ProbabilisticSulu Planner (p-Sulu Planner), building upon prior work model-based plan executive calledSulu (Leaute & Williams, 2005). p-Sulu Planner provides following three capabilities,order meet needs described scenario: 1) goal-directed planning continuousdomain, 2) near-optimal planning, 3) risk-sensitive planning risk bounds.Goal-directed planning continuous domain p-Sulu Planner must plan actionscontinuous effects achieve time evolved goals specified users. case PTSscenario Figure 2, PAV must sequentially achieve two temporally extended goals, called512fiP ROBABILISTIC P LANNING C ONTINUOUS DYNAMIC YSTEMS B OUNDED R ISKFigure 2: sample plan personal aerial vehicle (PAV)episodes: going scenic area arriving Bedford. additionaltemporal constraints goals inherent scenario; temporal constraintscome physical limitations, fuel capacity, others come passenger requirements.Near-optimal stochastic planning Cost reduction performance improvement important issues system. PTS scenario, passengers may want minimize triptime fuel usage. p-Sulu Planner finds near-optimal control sequence accordinguser-defined objective function, satisfying given constraints.Risk-sensitive planning risk bounds Real-world systems subject various uncertainties, state estimation error, modeling uncertainty, exogenous disturbance.case PAVs, position velocity vehicle estimated Kalman filtertypically involve Gaussian-distributed uncertainties; system model used planningcontrol perfect; vehicles subject unpredictable disturbances turbulence. uncertainty, executed result plan inevitably deviatesoriginal plan hence involves risk constraint violation. Deterministic plan executionparticularly susceptible risk optimized order minimize given cost function,since optimal plan typically pushes one constraint boundaries, henceleaves margin error. example, shortest path PTS scenario shown Figure2 cuts close NFZs storm, generally, constraint boundaries. Then,tiny perturbation planned path may result penetration obstacles.risk reduced setting safety margin path obstacles, costlonger path length. However, often impossible guarantee zero risk, since typically non-zero probability disturbance large enough push vehiclefeasible region. Therefore, passengers vehicle must accept risk,time need limit certain level. generally, users autonomoussystem uncertainty able specify bounds risk. planner mustguarantee system able operate within bounds. constraints calledchance constraints.513fiO , W ILLIAMS , & B LACKMORE1.1 Overview Plannersection describes inputs outputs p-Sulu Planner informally. rigorouslydefined Section 2.1.1.1 NPUTSInitial Condition p-Sulu Planner plans control sequence starting current state,typically estimated noisy sensor measurements. Therefore, p-Sulu Planner takesprobability distribution, instead point estimate, current state initial condition.Stochastic Plant Model control community planning problem generate sequencecontrol inputs actuate physical system, called plant. action model planttypically system real-valued equations control, state observable variables. pSulu Planner takes input linear stochastic plant model, specifies probabilistic statetransitions continuous domain. stochastic extension continuous plant model usedLeaute Williams (2005). paper limit focus Gaussian-distributed uncertainty.Chance-constrained qualitative state plan (CCQSP) order provide users intuitive way command stochastic systems, develop new plan representation called chanceconstrained qualitative state plan (CCQSP). extension qualitative state plan (QSP), developed used Leaute Williams (2005), Hofmann Williams (2006), Blackmore,Li, Williams (2006). CCQSP specifies desired evolution plant state time,defined set discrete events, set episodes, impose constraints plant stateevolution, set temporal constraints events, set chance constraints specifyreliability constraints success sets episodes plan.CCQSP may depicted directed acyclic graph, shown Figure 3. circlesrepresent events squares represent episodes. Flexible temporal constraints representedsimple temporal network (STN) (Dechter, Meiri, & Pearl, 1991), specifies upper lowerbounds duration two events (shown pairs numbers parentheses).plan Figure 3 describes PTS scenario depicted Figure 2, stated informally as:Start Provincetown, reach scenic region within 30 time units, remain5 10 time units. end flight Bedford. probabilityfailure episodes must less 1%. times, remain safe regionavoiding no-fly zones storm. Limit probability penetratingobstacles 0.0001%. entire flight must take 60 time units.formal definition CCQSP given Section 2.4.3.Objective function user p-Sulu Planner specify objective function (e.g., costfunction). assume convex function.1.1.2 UTPUTExecutable control sequence p-Sulu Planner plans finite horizon. One twooutputs p-Sulu Planner executable control sequence horizon satisfiesconstraints specified input CCQSP. case PTS scenario, outputs vehicles actuation inputs, acceleration ladder angle, result nominal paths shown514fiP ROBABILISTIC P LANNING C ONTINUOUS DYNAMIC YSTEMS B OUNDED R ISKFigure 3: example chance-constrained qualitative state plan (CCQSP), new plan representation specify desired evolution plant state acceptable levelsrisk. PTS scenario Figure 2, passengers PAV would like goProvincetown Bedford, fly scenic region way. safe regionmeans entire state space except obstacles. Risk episodes must withinrisk bounds specified chance constraints.Figure 2. order control sequence executable, must dynamically feasible.example, curvature PAVs path must exceed vehicles maneuverability.Optimal schedule output p-Sulu Planner optimal schedule, set execution time steps events input CCQSP minimizes given cost function. casePTS scenario shown Figure 3, schedule specifies leave scenic regionarrive Bedford, example. p-Sulu Planner finds schedule satisfies simpletemporal constraints specified CCQSP, minimizes cost function.two outputs control sequence schedule must consistent other:time-evolved goals achieved optimal schedule applying control sequencegiven initial conditions.1.2 Approachp-Sulu Planner must solve difficult problem generating executable control sequenceCCQSP, involves combinatorial optimization discrete schedule non-convexoptimization continuous control sequence. approach article develop p-SuluPlanner three technical steps, call spirals.first spiral, described Section 4, solve special case CCQSP planning problem, feasible state space convex (e.g., path planning problem without obstacles)schedule fixed, shown Figure 4-(a). problem transformed convex optimization problem risk allocation approach, presented previous work (Ono& Williams, 2008a). obtain feasible, near-optimal solution CCQSP planning problemoptimally solving convex optimization using interior point method (Blackmore & Ono,2009).second spiral, presented Section 5, consider CCQSP problemnon-convex state space order include obstacles, Figure 4-(b). develop branchbound-based algorithm, called non-convex iterative risk allocation (NIRA). Subproblemsbranch-and-bound search NIRA convex chance-constrained optimal control problems,solved first spiral. NIRA algorithm cannot handle problem flexible schedule.515fiO , W ILLIAMS , & B LACKMOREthird spiral, described Section 6, develop another branch boundbased algorithm, namely p-Sulu Planner, solve general CCQSP planning problemflexible schedule obstacles. Subproblems branch-and-bound search pSulu Planner non-convex chance-constrained optimal control problems, solvedNIRA algorithm.dp-SuluW (Section 6)NIRA (Section 5)(Ono & Williams 2008b) (Section 4)Fixed scheduleFixed schedulet=5Simple temporalconstraints[2 4]t=5t=1Ct=1Goal[1 3]GoalCObstacleStartStart(a) Convex, fixed scheduleObstacleWaypointWaypointWaypointStart(b) Non-convex, fixed scheduleGoal[0 5](c) Non-convex, flexible scheduleFigure 4: Three-spiral approach CCQSP planning problem1.3 Related WorkRecall CCQSP planning problem distinguished use time-evolved goals, continuous states actions, stochastic optimal solutions chance constraints. planningcontrol disciplines explored aspects problem, solution total novel,approach solving problem efficiently risk allocation novel.specifically, extensive literature planning discrete actions achievetemporally extended goals (TEGs), TLPlan (Bacchus & Kabanza, 1998) TALPlan(Kvarnstrom & Doherty, 2000), treat TEGs temporal domain control knowledge prunesearch space progressing temporal formula. However, since TEG planners assumediscrete state spaces, cannot handle problems continuous states effects without discretization. Ignoring chance constraints, representation time evolved goals used TLPlanp-Sulu Planner similar. TLPlan uses version metric interval temporal logic (MITL)(Alur, Feder, & Henzinger, 1996) applied discrete states, p-Sulu Planner uses qualitative state plans (QSPs) (Leaute & Williams, 2005; Hofmann & Williams, 2006; Li, 2010)continuous states. Li (2010) shows that, given state space, QSP expressed MITL.key difference defines p-Sulu Planner addition chance constraints, togetheruse continuous variables.Several planners, particularly employed components model-based executives, command actions continuous state space. example, Sulu (Leaute & Williams, 2005)takes input deterministic linear model QSP, specifies desired evolution plantstate well flexible temporal constraints, outputs continuous control sequence. Chekhov(Hofmann & Williams, 2006) also takes input QSP nonlinear deterministic system model,outputs continuous control sequence. order enable fast real-time plan execution, Chekhovprecomputes flow tubes, sets continuous state trajectories end goal regions specified given plan. Kongming (Li, 2010) provides generative planning capability hybrid516fiP ROBABILISTIC P LANNING C ONTINUOUS DYNAMIC YSTEMS B OUNDED R ISKsystems, involving continuous discrete actions. employs compact representationhybrid plans, called Hybrid Flow Graph, combines strengths Planning Graphdiscrete actions flow tubes continuous actions. planners adapt effects uncertainty, explicitly reason effects uncertainty planning. example,Sulu employs receding horizon approach, continuously replans control sequence usinglatest measurements. Chekhovs flow tube representation feasible policies allows executive generate new control sequences response disturbances on-line. p-Sulu Plannerdistinct continuous planners plans model uncertainty dynamics, instead reacting it. plan guarantees user-specified probability success explicitlyreasoning effects uncertainty.AI planning literatures, planning domain description language, PDDL+, supports mixeddiscrete-continuous planning domains (Fox & Long, 2006). Probabilistic PDDL (Younes & Littman,2004) Relational Dynamic influence Diagram Language (RDDL) (Sanner, 2011) handle stochastic systems. Recently, Coles, Coles, Fox, Long (2012) developed forward-chainingheuristic search planner named COLIN, deal continuous linear change durationdependent effects. However, planners handle chance constraints. noteoutputs p-Sulu Planner continuous space discrete time. time-dependent MDPdeveloped Boyan Littman (2000) handle continuous time encoding time state.Extension p-Sulu Planner continuous-time planning would interesting future direction.work within AI community probabilistic planning focused planning discrete domains builds upon Markov decision process (MDP) framework. growing subcommunity focused extensions MDPs continuous domain. However, tractabilityissue, since typically require partitioning approximation continuous state space.straightforward partitioning continuous state action spaces discrete states actions often leads exponential blow-up running time. Furthermore, feasible statespace unbounded, impossible partition space finite number compact subspaces. alternative approach function approximation (Boyan & Moore, 1995),convergence guaranteed approximation error bounded (Bertsekas & Tsitsiklis,1996; Lagoudakis & Parr, 2003). Time-dependent MDPs (Boyan & Littman, 2000; Feng, Dearden,Meuleau, & Washington, 2004) efficient partitioning continuous state space, makeassumption set available states actions finite (i.e., discrete). Hence, planningMDPs continuous state space, Rn , requires approximate state spacefinite number discrete states. approach essentially different MDP approachescontinuous variables directly optimized convex optimization without discretization continuous state space. Hence, continuity state space harm tractabilityp-Sulu Planner.second point comparison treatment risk. Like p-Sulu Planner, MDPframework offers approach marrying utility risk. However, MDP algorithms balanceutility risk assigning large negative utility event constraint violation.approach cannot guarantee bounds probability constraint violation. constrained MDPapproach (Altman, 1999) explicitly impose constraints. Dolgov Durfee (2005) showedstationary deterministic policies constrained MDPs obtained solving mixedinteger linear program (MILP). However, constrained MDP framework impose boundsexpected value costs, again, cannot guarantee strict upper bounds probability517fiO , W ILLIAMS , & B LACKMOREconstraint violation. contrast, p-Sulu Planner allows users impose chance constraints,explicitly restrict probability constraint violation. far authors know,risk-sensitive reinforcement learning approach proposed Geibel Wysotzki (2005)work considers chance constraints MDP framework. developed reinforcementlearning algorithm MDPs constraint probability entering error states. workdistinct p-Sulu Planner goal-directed, mean achievestime-evolved goals within user-specified temporal constraints. summarize, prior MDP worksupports continuous state actions combination general continuous noise transitionsensuring probability failure bounded.Risk-sensitive control methods continuous domain extensively studied discipline control theory. example, celebrated H control method minimizes effectdisturbances output system guaranteeing stability system (Stoorvogel,1992). Risk-sensitive control approaches allow users choose level risk aversenessminimization expected exponentiated cost function (Jacobson, 1973; Fleming & McEneaney, 1995). However, approaches address chance constraints optimal scheduling. Several methods proposed solving stochastic optimal control problemscontinuous variables chance constraints. method proposed van Hessem (2004) turnsstochastic problem deterministic problem using conservative ellipsoidal relaxation.Blackmore (2006) proposes sampling-based method called Particle Control, evaluates jointchance constraints Monte-Carlo simulation, instead using conservative bound. result,stochastic planning problem reduced MILP problem. Although theoretical guarantee obtain exactly optimal solution infinite number samples used,computation time issue. Blackmore et al. (2006) Nemirovski Shapiro (2006) employedBooles inequality decompose joint chance constraint individual chance constraints. Although Booles inequality less conservative ellipsoidal relaxation, approach stillnon-negligible conservatism since fixes individual risk bound uniform value.approach builds upon approach, modifications allow flexible individual risk bounds.best authors knowledge, p-Sulu Planner first goal-directed plannerable plan continuous state space chance constraints.1.4 Innovationsp-Sulu Planner enabled six innovations presented article.First, order allow users command stochastic systems intuitively, develop new planrepresentation, CCQSP (Section 2.4.3).Second, order decompose chance constraint disjunctive clause disjunctionindividual chance constraints, introduce risk selection approach (Section 5.1.2).Third, order obtain lower bounds branch-and-bound search NIRA, developfixed risk relaxation (FRR), linear program relaxation subproblems (Section 5.4.2).Fourth, minimize search space optimal schedule introducing new forwardchecking method efficiently prunes infeasible assignment execution time steps (Section 6.2).Fifth, order enhance computation time schedule optimization, introduce methodobtain lower bound branch-and-bound solving fixed-schedule planning problemspartial assignment schedule. (Section 6.3)518fiP ROBABILISTIC P LANNING C ONTINUOUS DYNAMIC YSTEMS B OUNDED R ISKSixth, order minimize number non-convex subproblems solved branch-andbound search, introduce variable ordering heuristic, namely convex-episode-first (CEF)heuristic, explores episodes convex feasible state region onesnon-convex state region (Section 6.2.2).rest article organized follows. Section 2 formally defines CCQSPstates CCQSP planning problem. Section 3 derives encoding problem chanceconstrained optimization problem, well encodings two limited versions CCQSPplanning problem: one fixed schedule convex state space, another fixedschedule non-convex state space. Section 4 reviews solution fixed-schedule CCQSPplanning problem convex state space. Section 5 develops NIRA algorithm, solvesfixed-schedule CCQSP planning problem non-convex state space, Section 6 introducesp-Sulu Planner, solves CCQSP planning problem flexible schedule nonconvex state space. Finally, Section 7 shows simulation results various scenarios, includingpersonal transportation system (PTS).2. Problem StatementRecall p-Sulu Planner takes input linear stochastic plant model, specifieseffects actions; initial state description, describing distribution initial states; CCQSP,specifies desired evolutions state variables, well acceptable levels risk;objective function. output executable control sequence optimal schedule. Planningperformed finite horizon, since p-Sulu Planner incorporated finite-horizonoptimal control. first define variables used problem formulations. defineelements inputs outputs.2.1 Definition Time Stepconsider series discretized finite time steps = 0, 1, 2, N fixed time interval, integer N size planning horizon. Since time interval takepositive real value, suffices consider time steps integer indices approximatesystems dynamics. use term time step mean integer index discretized timesteps, using term time mean real-valued time. define sets follows::= {0, 1, 2, N }.:= {0, 1, 2, N 1}.(1)(2)limit scope article discrete-time stochastic system. optimizingcontrol sequence continuous-time stochastic system requires solving stochastic differentialequation (SDE) repeatedly. Performing computation tractable except simpleproblems.2.2 Definitions Eventsevent denotes start end episode behavior plan representation.Definition 1. event e E instance executed certain time step T.519fiO , W ILLIAMS , & B LACKMOREdefine two special events, start event e0 end event eE . Without loss generality,assume e0 executed = 0. end event eE represents termination entireplan.2.3 Definitions VariablesVariables used problem formulation involve discrete schedule, continuous state vector,continuous control vector.formally define event well schedule follows:Definition 2. execution time step s(e) integer-valued scalar representstime step event e E executed. schedule := [s(e0 ), s(e1 ), s(eE )]sequence execution time steps events e E. Finally, partial schedule :=[(e) | e E E] ordered set execution time steps subset events E .definition, start event executed = 0 i.e, s(e0 ) = 0. Following notationschedule, denote (e) execution time event e E . See also definitionschedule (Definition 2).consider continuous state space, state vector state sequence definedfollows:Definition 3. state vector xt Rnx real-valued vector represents state planttime step t. state sequence x0:N := [x0 xN ] vector state variables time step 0N .actions assignments continuous decision variables, referred controlvector:Definition 4. control vector ut Rnu real-valued vector represents control inputsystem time step t. control sequence u0:N 1 := [u0 uN 1 ] vector control inputstime 0 N 1.2.4 Definitions Inputssubsection defines four inputs p-Sulu Planner: initial condition, stochastic plantmodel, CCQSP, objective function.2.4.1 NITIAL C ONDITIONbelief state beginning plan represented initial state, assumedGaussian distribution known mean x0 covariance matrix x0 :x0 N (x0 , x0 ).(3)parameters (3) specified initial condition, defined follows:Definition 5. initial condition pair = x0 , x0 , x0 mean initial statex0 covariance matrix initial state.520fiP ROBABILISTIC P LANNING C ONTINUOUS DYNAMIC YSTEMS B OUNDED R ISK2.4.2 TOCHASTIC P LANT ODELp-Sulu Planner controls dynamical systems actions correspond settings continuous control variables, whose effects continuous state variables. p-Sulu Plannerspecifies actions effects plant model. plant model considered statetransition model continuous space. employ variant linear plant model additiveGaussian uncertainty commonly used context chance-constrained stochastic optimal control (Charnes & Cooper, 1959; Nemirovski & Shapiro, 2006; Oldewurtel, Jones, & Morari,2008; van Hessem, 2004), modification consider controller saturation. Specifically,assume following plant model:xt+1 = xt + B U (ut ) + wt(4)wt Rnx state-independent disturbance t-th time step zero-mean Gaussiandistribution given covariance matrix denoted wt :wt N (0, wt ).(5)Although model prohibits state-dependent disturbance, types noise involvedtarget applications state independent. example, PTS scenario introduced Section1, primary source uncertainty wind turbulence, typically dependentstate vehicle. space rendezvous scenario discussed Section 7.5, main sourcesperturbations space craft tidal force unmodeled gravitational effects Sun, Moon,planets (Wertz & Wiley J. Larson, 1999). noises modeled state-dependentnoise practice scale planned actions significantly smaller SolarSystem.dependent state space craft. note problem formulation encodetime-varying noise specifying different covariance matrices wt time step.set U Rnu compact convex set represents continuous domain feasiblecontrol inputs. infeasible control input ut/ U given plant, actuators saturate.function U () : Rnu 7 U (4) represents effect actuator saturation follows:{u(if u U)U (u) :=,PU (u) (otherwise)PU (u) projection u U. example, u one-dimensional U = [l, u],PU (u) = max(min(u, u), l). Note U introduces nonlinearity plant.parameters (4) (5) specified stochastic plant model, definedfollows:Definition 6. stochastic plant model four-tuple = A0:N 1 , B 0:N 1 , w0:N 1 , U,A0:N 1 B 0:N 1 sets N matrices A0:N 1 := {A0 , A1 , 1 }, B 0:N 1 :={B 0 , B 1 , B N 1 }, w0:N 1 set N covariance matrices w0:N 1 = {w0 , w1 , , wN 1 },U Rnu compact convex set represents domain feasible control inputs.Note xt , well wt , random variable, ut deterministic variable. Figure5 illustrates plant model. typical plant model, probability circles grow time sincedisturbance wt added every time step, drawn figure. effect represents commonlyobserved tendency distant future involves uncertainty near future.521fiO , W ILLIAMS , & B LACKMOREx2x399.9%99%90%x1x099.9%99%x199.9%99%90%Nominalpathx290%x3Figure 5: Illustration stochastic plant model used p-Sulu Planner.order mitigate accumulation uncertainty, employ close-loop control approach,generates control input ut incorporating nominal control input ut Rnuerror feedback, follows:ut = ut + K (xt xt ),(6)K matrix representing constant stabilizing feedback gain time xtnominal state vector. nominal state xt obtained following recursion:x0 := x0xt+1 = xt + B ut .(7)(8)closed-loop control approach employed Geibel Wysotzki (2005) Oldewurtelet al. (2008) context chance-constrained optimal control shown significantlyimproves performance.closed-loop planning method, nominal control input ut planned execution. actual control input ut computed real time using (6). feedback term(6) linearly responds error xt xt . choosing feedback gain K appropriately,growth probability circles Figure 5 slowed down. Neglecting effect controllersaturation (i.e., assuming U = Rnx ), follows (4) (6) xt Gaussian distributioncovariance matrix xt , evolves follows:xt+1 = (At + B K )xt (At + B K )T + wt .(9)typical plant, eigenvalues one. Therefore, error feedback(i.e., K = 0), size xt grows wt iteration. choosing Knorm largest eigenvalue (At + B K ) less one, covariance xt growcontinuously. feedback gain found using standard control techniques,linear quadratic regulator (LQR) (Bertsekas, 2005). Since consider finite-horizon, discretetime planning problem, optimal time-varying LQR gain K obtained solving finitehorizon, discrete-time Riccati equation. practice, often suffices use steady-state (i.e.,time-invariant) LQR gain, obtained solving infinite-horizon, discrete-time Riccatiequation simplicity. note feedback gain K also optimized real time.approach often used robust stochastic model predictive controls (Goulart, Kerrigan, &Maciejowski, 2006; Oldewurtel et al., 2008; Ono, 2012). However, extension beyondscope paper.522fiP ROBABILISTIC P LANNING C ONTINUOUS DYNAMIC YSTEMS B OUNDED R ISKissue that, error xt xt happens large, control input ut may exceedfeasible domain U, resulting actuator saturation. Therefore, (9) hold duenonlinearity function U (). address issue risk allocation approach.specifically, impose chance constraints control saturation, allocate risk statecontrol constraints. approach discussed detail Section 4.1.5.2.4.3 C HANCE - CONSTRAINED Q UALITATIVE TATE P LAN (CCQSP)qualitative state plan (QSP) (Leaute & Williams, 2005) temporally flexible plan specifiesdesired evolution plant state. activities QSP called episodes specifyconstraints plant state. CCQSP extension QSPs stochastic plans involvechance constraints, defined follows:Definition 7. chance-constrained qualitative state plan (CCQSP) four-tuple P = E, A, , C,E set discrete events, set episodes, set simple temporal constraints,C set chance constraints.four elements CCQSP defined precisely moment. Like QSP, CCQSPillustrated diagrammatically directed acyclic graph discrete events Erepresented vertices, drawn circles, episodes arcs ovals. CCQSP startevent e0 end eE , corresponds beginning end mission, respectively.example, Figure 3 shows CCQSP PTS scenario. state regions obstaclesCCQSP illustrated Figure 2. involves four events: E = {e0 , e1 , e2 , eE }. meaningsdescribed follows.1. start event e0 corresponds take PAV Provincetown.2. second event e1 corresponds time step PAV reaches scenic region.3. Event e2 associated time instant PAV left scenic region.4. end event eE corresponds arrival PAV Bedford.CCQSP four episodes = {a1 , a2 , a3 , a4 } two chance constraints C = {c1 , c2 }.natural language expression CCQSP is:Start Provincetown, reach scenic region within 30 time units, remain5 10 time units. end flight Bedford. probabilityfailure activities must less 1%. times, remain safe regionavoiding no-fly zones storm. Limit probability penetratingobstacles 0.0001%. entire flight must take 60 time units.formally define three types constraints - episodes, temporal constraints,chance constraint.Episodes episode specifies desired state system control timeinterval.Definition 8. episode = eSa , eE, (tS , tE ), Ra associated start event ea endNevent eE. Ra R region state space. set time steps state xtmust region Ra .523fiO , W ILLIAMS , & B LACKMOREfeasible region Ra subset RN . approximate Ra set linearconstraints later Section 3.1.1.(tS , tE ) subset given function episodes start time step tS = s(eSa )end time step tE = s(eE). Different forms (tS , tE ) result various types episodes.following three types episodes particularly interest us:1. Start-in episode: (tS , tE ) = {tS }2. End-in episode: (tS , tE ) = {tE }3. Remain-in episode: (tS , tE ) = {tS , tS + 1, , tE }given episode a, set time steps plant state must region Raobtained substituting s(eSa ) s(eE), execution time steps start event endevent episode, tS and( tE . other) words, episode requires plant stateRa time steps s(eSa ), s(eE) . rest article, use followingabbreviated notation:()(s) := s(eSa ), s(eEa) .Using notation, episode equivalent following state constraint:xt Ra .(10)ta (s)example, CCQSP shown Figure 3, four episodes: a1 (Start [Provincetown]), a2 (Remain [Scenic region]), a3 (End Bedford), a4 (Remain [safe region]).Section 6, solve relaxed optimization problem partial schedule (Definition 2)order obtain lower bound optimal objective value. relaxed problems,subset episodes relevant given partial schedule imposed. formallydefine partial episode set partial schedule follows:Definition 9. Given partial schedule , A() partial episode set, subsetinvolves episodes whose start event end event assigned execution time steps.}{A() = | eSa E eEE ,definition E given Definition 2.Chance constraint Recall chance constraint probabilistic constraint requiresconstraints defining episode satisfied within user-specified probability. CCQSPmultiple chance constraints. chance constraint associated least one episode.chance constraint formally defined follows:Definition 10. chance constraint c = c , c constraint requiring that:Prxt Ra 1 c ,(11)ac ta (s)c user-specified risk bound c set episodes associated chanceconstraint c.524fiP ROBABILISTIC P LANNING C ONTINUOUS DYNAMIC YSTEMS B OUNDED R ISKNote every episode CCQSP must associated exactly one chance constraint.episode must involved one chance constraint unassociatedchance constraint.example, CCQSP shown Figure 3 two chance constraints, c1 c2 . associated episodes c1 = {a1 , a2 , a3 } c2 = {a4 }. Therefore, c1 requires probabilitysatisfying three episodes a1 , a2 , a3 (colored green) 99%, c2requires probability satisfying episode a4 99.99999%.make following assumption, necessary order guarantee convexityconstraints Section 4.2.Assumption 1.c 0.5assumption requires risk bounds less 50%. claim assumptionconstrain practical applications, since typically user autonomous system wouldaccept 50% risk.Temporal constraint CCQSP includes simple temporal constraints (STCs) (Dechter et al.,1991), impose upper lower bounds duration episodes temporaldistances two events E.min max constraint, specifyDefinition 11. simple temporal constraint = eS , eE, b , bing duration start event eS end event eEreal-valued intervalmax ] [0, +].[bmin,bTemporal constraints represented diagrammatically arcs nodes, labeledmax ], labels episodes. example, CCQSP shown Figure 3time bounds [bmin, bfour simple temporal constraints. One requires time e0 e1 30time units. One requires time e1 e2 least 5 units 10 units. Onerequires time e2 eE 40 time units. One requires time e0eE 60 time units.schedule feasible satisfies temporal constraints CCQSP. numberfeasible schedules finite, since discrete finite. denote SF domain feasibleschedules, formally defined follows:SF = {s T|E| |maxbmin{s(eE},) s(e )} b(12)|E| number events CCQSP. temporal duration multiplied timereal-valued time, set discrete time steps T.bmininterval bmin2.4.4 BJECTIVE F UNCTIONsection, formally define objective function.Definition 12. objective function J : UN XN SF 7 R real-valued functionnominal control sequence u0:N 1 , nominal state sequence x1:N , schedule s. assumeJ convex function x1:N u0:N 1 .525fiO , W ILLIAMS , & B LACKMOREtypical example objective function quadratic sum control inputs, requirestotal control efforts minimized:J(u0:N 1 , x1:N , s) =N1||ut ||2 .t=0Another example is:J(u0:N 1 , x1:N , s) = s(eE ),(13)minimizes total plan execution time, requiring end event eE qualitativestate plan scheduled soon possible.often need minimize expectation cost function. Note that, case,expectation function x1:N u0:N 1 reduced function u0:N 1follows (4)-(6) probability distributions x1:N u0:N 1 uniquely determinedu0:N 1 K . practice, often convenient express objective functionfunction u0:N 1 x1:N , rather function u0:N 1 . Since x1:N specifiedu0:N 1 using (8), two expressions equivalent. conversion expectation costfunction function nominal values conducted priori.controller saturation, conversion often obtained closed form.conversion particularly straight forward cost function polynomial, sinceexpectation equivalent combination raw moments, readily derivedcumulants. Note third higher cumulants Gaussian distribution zero.show examples conversion regarding three commonly-used cost functions: linear, quadratic,Manhattan norm.E[xt ] = xtE[xTt Qxt ](14)= xTt Qxt +nxE[||xt ||1 ] =xt ,ii=1tr(Qxt )()x2t,i21 1,1 F1 , ,2 2 2x2t ,i(15)(16)Q positive definite matrix, xt ,i ith diagonal element xt , 1 F1 ()confluent hypergeometric function. functions convex. expectation functionut also transformed function ut manner. Note second termright hand side (15) constant. Hence, minimizing xTt Qxt yields solutionminimizing E[xTt Qxt ].controller saturation, difficult obtain conversion closed-form duenonlinearity U () (4). practice, use approximation assumes saturation.Since closed-loop control approach explicitly limits probability controller saturationsmall probability (see Section 4.1.5 detail), approximation error trivial. claimempirically validated Section 7.2.4.2.5 Definitions Outputsoutput p-Sulu Planner optimal solution, consists optimal control sequence u0:N 1 UN optimal schedule SF .526fiP ROBABILISTIC P LANNING C ONTINUOUS DYNAMIC YSTEMS B OUNDED R ISKDefinition 13. optimal solution pair u0:N 1 , . solution satisfies constraintsgiven CCQSP (Definition 7), initial condition I, stochastic plant model .solution minimizes given objective function J(u0:N 1 , x1:N , s) (Definition 12).2.6 Problem Statementformally define CCQSP planning problem.Problem 1: CCQSP Planning ProblemGiven stochastic plant model = A0:N 1 , B 0:N 1 , w0:N 1 , initial condition =x0 , x0 , CCQSP P = E, A, , C, objective function J(u0:N 1 , x1:N , s), CCQSPplanning problem find optimal solution u0:N 1 , M, I, P , J.note p-Sulu Planner gives near-optimal solution Problem 1. p-Sulu Planneremploys two approximations, namely risk allocation (Section 4.1.1) risk selection (Section5.1.1), sake computational tractability. result, solution strictly optimalgeneral. However, empirically show Section 7 suboptimality due risk allocationrisk selection significantly smaller existing approximation methods.3. Problem Encodingsection encodes CCQSP planning problem stated previous section mathematical programming problem. Sections 4 - 6 address solve form mathematicalproblem. Recall build CCQSP planner, p-Sulu Planner, three spirals. firstpresent problem encoding general CCQSP planning problem non-convex state spaceflexible schedule (Figure 4-(c)) Subsection 3.1. present encodings twospecial cases CCQSP planning problem Subsections 3.2 3.3: one non-convexstate space fixed schedule (Figure 4-(b)), one convex state space fixed schedule (Figure 4-(a)).3.1 Encoding CCQSP Planning Problem Non-convex State Space FlexibleSchedule3.1.1 E NCODING F EASIBLE R EGIONSorder encode Problem 1 mathematical programming problem, geometric constraint(11), xt Ra , must represented algebraic constraints. purpose, approximatefeasible state regions Ra set half-spaces, represented linear stateconstraint.Figure 6 shows two simple examples. feasible region (a) outside obstacle,approximated triangle. feasible region (b) inside pickup region,approximated triangle. feasible region approximated set linear constraintsfollows:(a)3hTi x g ,(b)i=13hTi x g .i=1approximate feasible regions set linear constraints sufficient conditionoriginal state constraint xt Ra .527fiO , W ILLIAMS , & B LACKMOREFigure 6: Approximate representation feasible regions set linear constraintsassume set linear state constraints approximates feasible regionreduced conjunctive normal form (CNF) follows:hTa,k,j xt ga,k,j 0,(17)kKa jJa,kKa = {1, 2, |Ka |} Jc,i = {1, 2, |Jc,i |} sets indices. replacing xt Ra(11) (17), chance constraint c encoded follows:PrhTc,a,k,j xt gc,a,k,j 0 1 c .(18)ac ta (s) kKa jJa,korder simplify notation, merge indices c , (s),k Ka newindex Ic (s), Ic (s) = {1, 2, |Ic (s)|} |Ic (s)| = |Ka | ac |a (s)|. let ai ,ki , ti indices correspond combined index i, let hc,i,j = hc,ai ,ki ,j . Usingnotations, three conjunctions (18) combined one, obtain followingencoding chance constraint:PrhTc,i,j xti gc,i,j 0 1 c .(19)iIc (s) jJc,ispecification chance constraints given (19) requires |Ic (s)| disjunctive clausesstate constraints must satisfied probability 1 c . ith disjunctive clause cthchance constraint composed |Jc,i | linear state constraints.3.1.2 CCQSP P LANNING P ROBLEM E NCODINGUsing (3), (4), (5), (6), (19), CCQSP planning problem (Problem 1), solvedthird spiral, encoded follows:528fiP ROBABILISTIC P LANNING C ONTINUOUS DYNAMIC YSTEMS B OUNDED R ISKProblem 2: General CCQSP Planning Problemminu0:N 1 ,ss.t.J(u0:N 1 , x1:N , s)(20)SF(21)xt+1 = xt + B U (ut ) + wt ,(22)ut = ut + K (xt xt ),PrhTc,i,j xti gc,i,j 0 1 c .cC(23)(24)iIc (s) jJc,ix0 N (x0 , x0 ),wt N (0, wt ),(25)Recall SF , formally defined (12), set schedules satisfy temporal constraints given CCQSP. CCQSP execution problem hybrid optimization problemdiscrete variables (schedule) continuous variables u0:N 1 (control sequence). Notetemporal constraints within Problem 2 solved Section 6. similar problem encodingalso employed chance-constraint MDP proposed Geibel Wysotzki (2005). However,encoding differs Geibel Wysotzki two respects: 1) optimize continuous control sequence u0:N 1 also discrete schedule temporal constraints; 2)allow joint chance constraints, require satisfaction multiple state constraints givenprobability. Problem 2 solved Section 6.3.2 Encoding CCQSP Planning Problem Non-convex State Space FixedSchedulerestricted version CCQSP planning problem fixed schedule, solvedsecond spiral, obtained fixing Problem 2 follows:Problem 3: CCQSP Planning Problem Fixed ScheduleJ (s) = minu0:N 1s.t.J (u0:N 1 , x1:N )(26)xt+1 = xt + B U (ut ) + wt ,ut = ut + K (xt xt ),PrhTc,i,j xti gc,i,j 0 1 c ,cC(27)(28)(29)iIc (s) jJc,ix0 N (x0 , x0 ),wt N (0, wt ),(30)J (s) optimal objective value CCQSP Planning problem schedule fixeds. Note schedule s, decision variable Problem 2, treated constantProblem 3. Therefore, objective function J function control sequence mean529fiO , W ILLIAMS , & B LACKMOREstate, since fixed schedule. Since assumed J convex function regardingu0:N 1 x1:N , J also convex function. Section 5 solves Problem 3.3.3 Encoding CCQSP Planning Problem Convex State Space Fixed Schedulerestrictive version CCQSP planning problem fixed schedule convex statespace, solved first spiral, obtained removing disjunctions chanceconstraints Problem 3 follows:Problem 4: CCQSP Planning Problem Fixed Schedule Convex State Spaceminu0:N 1J (u0:N 1 , x1:N )(31)xt+1 = xt + B U (ut ) + wt ,ut = ut + K (xt xt ),PrhTc,i xti gc,i 0 1 c .cC(32)(33)(34)iIc (s)x0 N (x0 , x0 ),wt N (0, wt ),(35)Section 4 solves Problem 4.4. CCQSP Planning Convex State Space Fixed Schedulesection presents solution methods Problem 4, CCQSP planning problemconvex state space fixed schedule, shown Figure 4-(a). obstaclesenvironment execution time steps achieve time-evolved goals fixed, CCQSPplanning problem reduced convex chance-constrained finite-horizon optimal control problem.past work presented risk allocation approach, conservatively approximateschance-constrained finite-horizon optimal control problem tractable convex optimizationproblem (Ono & Williams, 2008a, 2008b; Blackmore & Ono, 2009). Although optimal solutionapproximated convex optimization problem exactly optimal solution original convex chance-constrained finite-horizon optimal control problem, suboptimality significantly smaller previous approaches. section gives brief overview risk allocationapproach, well solution convex chance-constrained finite-horizon optimal controlproblem.4.1 Deterministic Approximation Problem 4Evaluating whether joint chance constraint (34) satisfied requires computing integralmultivariate probability distribution arbitrary region, since probability (34) involvesmultiple constraints. integral cannot obtained closed form. address issuedecomposing intractable joint chance constraint (34) set individual chance constraints,involves univariate probability distribution. key feature individual530fiP ROBABILISTIC P LANNING C ONTINUOUS DYNAMIC YSTEMS B OUNDED R ISKGoalGoalWallsWallsNominal pathSafety marginStartStart(a) Uniform risk allocation(b) Optimal risk allocationFigure 7: Risk allocation strategies racing car examplechance constraint transformed equivalent deterministic constraintevaluated analytically.4.1.1 R ISK LLOCATION PPROACHdecomposition considered allocation risk. decomposition, riskbound joint chance constraint distributed individual chance constraints.many feasible risk allocations. problem find risk allocation results minimumcost. offer readers intuitive understanding risk allocation approach using examplebelow.Racing Car Example Consider racing car example, shown Figure 7. dynamicsvehicle Gaussian-distributed uncertainty. task plan path minimizes timereach goal, guarantee probability crashing wall race less0.1% (chance constraint). Planning control sequence equivalent planning nominalpath, shown solid lines Figure 7. limit probability crashing wall,good driver would set safety margin, colored dark gray Figure 7, plannominal path outside safety margin.driver wants set safety margin small possible order make nominal pathshorter. However, since probability crashing race bounded, certainlower bound size safety margin. Given constraint, different wayssetting safety margin; Figure 7(a) width margin uniform; Figure 7(b) safetymargin narrow around corner, wide places.intelligent driver would take strategy (b), since knows going closer wallcorner makes path shorter, straight line not. key observationtaking risk (i.e., setting narrow safety margin) corner results greater reward(i.e. time saving) taking risk straight line. gives rise notion riskallocation. good risk allocation strategy save risk reward small, takingreward great. illustrated example, risk allocation must optimizedorder minimize objective function joint chance-constrained stochastic optimizationproblem.531fiO , W ILLIAMS , & B LACKMORE4.1.2 ECOMPOSITION C ONJUNCTIVE J OINT C HANCE C ONSTRAINTS R ISKLLOCATIONderive mathematical representation risk allocation reformulating chance constraint conjunction constraints conjunction chance constraints. reformulationinitially presented Prekopa (1999) introduced chance-constrained optimal controlOno Williams (2008b). concept risk allocation originally developed OnoWilliams (2008a). Let Ci proposition either true false. following lemmaholds:Lemma 1.Pr[N]Ci 1N0,i=1Pr [Ci ] 1i=1Ni=1Proof.Pr[N]Ci 1 Pri=1[Ni=1N]Ci(36)[ ]Pr Ci(37)cC i=100Ni=1Ni=1N[ ]Pr Cii=1Pr [Ci ] 1N.(38)i=1overline C negation literal C. use following Booles inequality obtain (37)(36):][NNPr[Cc,i ].PrCc,ii=1i=1following result immediately follows Lemma 1 substituting linear constrainthTc,i xti gc,i 0 Ci chance constraint c.Corollary 1. following set constraints sufficient condition joint chance constraint(34) Problem 4:[]c,i 0Pr hc,i xti gc,i 0 1 c,ic,i c(39)cCiIc (s)iIc (s)newly introduced variables c,i represent upper bounds probability violatinglinear state constraint. refer individual risk bounds. individual risk bound,532fiP ROBABILISTIC P LANNING C ONTINUOUS DYNAMIC YSTEMS B OUNDED R ISKc,i , viewed amount risk allocated ith clause. fact c,i boundprobability implies 0 c,i 1. second term (39) requires total amount riskupper-bounded original risk bound c . find analogue resource allocationproblem, allocation resource optimized upper bound total amountavailable resource. Likewise, allocation risk c,i must optimized order minimizecost. Therefore, call decomposition method risk allocation.4.1.3 C ONSERVATISM R ISK LLOCATION PPROACHmentioned previously, risk allocation approach gives conservative approximationoriginal chance constraint. subsection evaluates level conservatism risk allocationapproach.Let Pf ail true probability failure, defined probability solution violatesconstraints (i.e., left hand side (34)). Since (39) sufficient necessary condition(34), Pf ail smaller equal risk bound general: Pf ail . Hence,conservatism introduced risk allocation representedPf ail .best-case scenario risk allocation approach violations constraintsmutually exclusive, meaning solution violates one constraint always satisfiesconstraints. case, (39) becomes necessary sufficient condition (34) hence,risk allocation involve conservatism. Therefore,Pf ail = 0.hand, worst-case scenario constraints equivalent, meaningsolution violates one constraint always violates constraints. case,Pf ail =N 1,NN number constraints.practical problems lie somewhere best-case scenario worst-case scenario, typically closer best-case worst-case scenario. example,two separate obstacles path planning problem, collisions two obstacles mutuallyexclusive events. Collision obstacle one time step usually imply collisionstime steps. rough approximation real-world situation assume satisfaction constraints probabilistically independent. assumption, true probabilityfailure is:Pf ail =Pr [qc,i (u) 0] 1(1 ),iIciIcIc set index state constraints. Note . Therefore, conservatism introduced risk allocation second order :Pf ail O(2 ).example, = 1%, true probability failure approximately Pf ail 0.99%.practical cases, users prefer set small risk bounds, typically less 1%. cases,conservatism introduced risk allocation becomes small.533fiO , W ILLIAMS , & B LACKMORE4.1.4 C ONVERSION ETERMINISTIC C ONSTRAINTSindividual chance constraint (39) involves single linear constraint. Furthermore,assuming actuator saturation, xti Gaussian distribution covariancematrix given (9). Hence, hTc,i xti univariate Gaussian distribution. following lemmatransforms individual chance constraint equivalent deterministic constraint involvesmean state variables, instead random state variables:Lemma 2. following two conditions equivalent.[]Pr hTc,i xti gc,i 0 1 c,i hTc,i xti gc,i mc,i (c,i )mc,i (c,i ) =2hTc,i x,ti hc,i erf 1 (2c,i 1).(40)Note erf1 inverse Gauss error function x,ti covariance matrixxti . lemma holds mc,i () inverse cumulative distribution functionunivariate, zero-mean Gaussian distribution variance hTc,i x,ti hc,i .4.1.5 R ISK LLOCATION PPROACH C LOSED - LOOP C ONTROL P OLICYclose-loop control policy employed (i.e., K = 0 (6)), risk actuatorsaturation. Since nonlinearity function U () (5) makes probability distributionxti non-Gaussian, mc,i () cannot obtained (40). Although theoretically possiblederive mc,i () non-Gaussian distributions, difficult case since inversecumulative distribution function xti cannot obtained closed-form.solution issue summarized Lemma 3 below, allows us assume xtiGaussian-distributed hence use (40), even possibility actuator saturation.approach enabled imposing additional chance constraints bound risk actuatorsaturation follows:Pr [ut U] 1 , ,(41)bound risk actuator saturation time step t. Using method presentedSection 3.1.2, approximate U polytope follows:ut UhU,i ut gU,i 0iIUAssuming xti Gaussian-distributed, use Lemma 2 transform (41) deterministicconstraints nominal control inputs follows:hU,i ut gU,i mU,t,i (t,i )t,i , ,(42)iIUiIUmU,t,i (c,i ) =2hTU,i x,t hU,i erf 1 (2c,i 1).following lemma holds:534(43)fiP ROBABILISTIC P LANNING C ONTINUOUS DYNAMIC YSTEMS B OUNDED R ISKLemma 3. following set constraints sufficient condition joint chance constraint(34) Problem 4:c,i 0, 0hTc,i xti gc,i mc,i (c,i )cC iIc (s)Tcmaxc,i +t,i ct=0 iIUiIc (s)hU,i ut gU,i mU,t,i (t,i ),(44)tT iIU(45)mc,i () mU,t,i given (40) (43). Tcmax last time step episodesassociated chance constraint c executed, given schedule s:Tcmax = max s(eE).acIntuitively, constraint (44) requires that, probability 1 c , episode constraintssatisfied actuators saturate episodes associated c executed.Proof. consider two plants: = A0:N 1 , B 0:N 1 , w0:N 1 , U= A0:N 1 , B 0:N 1 , w0:N 1 , Rnu , U Rnu compact convex set (see Definition6). difference two plants possibility actuator saturation,not. result, probability distribution state variables non-Gaussian,Gaussian. Note result different probability distributions xtiut . order explicitly show plant model considered, use notations xMtiut proof.first consider . follows Lemmas 1 2 that:maxTcPrhTc,i xMg0uU1.(44) =c,icticCt=0iIc (s)Let w0:N 1 := [w0 wN 1 ]. define feasible disturbance set, Wc (v 0:N 1 , s) RN nx ,follows:maxfiTcfifihTc,i xMuU.g0Wc (v 0:N 1 , s) := w0:N 1 RN nx fic,itifiiIc (s)t=0(46)Then, definition,maxTcU = Pr [w0:N 1 Wc (v 0:N 1 , s)] .PrhTc,i xMuMti gc,i 0iIc (s)t=0535fiO , W ILLIAMS , & B LACKMORENext consider . Note identical long actuator saturations(i.e., uMU). Therefore, given w 0:N 1 Wc (v 0:N 1 , s), follows (46) xt =xMut = ut . Hence,maxTcw0:N 1 Wc (v 0:N 1 , s) =hTc,i xMuMti gc,i 0U .t=0iIc (s)Accordingly, given c C,hTc,i xMPrti gc,i 0iIc (s)PrTcmaxhTc,i xMti gc,i 0uMUt=0iIc (s)Pr [w0:N 1 Wc (v 0:N 1 , s)]maxTcuMU= PrhTc,i xMti gc,i 0t=0iIc (s)1 c .completes proof Lemma 3note Lemma 3 probabilistic extension closed-loop robust model predictivecontrol (RMPC) methods proposed Acikmese, Carson III, Bayard (2011) Richards(2006). methods avoid risk actuator saturation imposing tightened controlconstraints ut . Since consider stochastic uncertainty, replace constraint tighteningchance constraints.4.2 Convex Programming Solution Problem 4Using Lemma 3, replace stochastic optimization problem, Problem 4, deterministicconvex optimization problem:Problem 5: Deterministic Approximation Problem 4minu1:N ,c,i 0,t,i 0s.t.J (u1:N , x1:N ),(47)xt+1hc,i xti= xt + B ut(48)gc,i mc,i (c,i )(49)cC iIc (s)tThU,i ut gU,i mU,t,i (t,i )(50)iIUTcmaxc,i +cC iIc (s)536t=0 iIUt,i c .(51)fiP ROBABILISTIC P LANNING C ONTINUOUS DYNAMIC YSTEMS B OUNDED R ISKfollows immediately Corollaries 1 2 feasible solution Problem 5 alwaysfeasible solution Problem 4. Furthermore, Blackmore Ono (2009) showed optimalsolution Problem 5 near-optimal solution Problem 4. following lemma guaranteestractability Problem 5.Lemma 4. Problem 5 convex optimization problem.Proof. inverse error function erf1 (x) concave x. Since assume Section 2.4.3c 0.5, feasible ranges upperbounded 0.5. Since safety margin functionmc,i (c,i ) mU,t,i (t,i ) convex 0 < c,i 0.5 0 < t,i 0.5, constraints (49)(50) convex within feasible region. constraints also convex sincelinear. Finally, objective function convex assumption (Section 2.4.4). Therefore, Problem5 convex optimization problem.Since Problem 5 convex optimization problem, solved interior point methodoptimally efficiently. completes first spiral, planning CCQSPs fixed scheduleconvex constraints. next section present solution method non-convex problembranch-and-bound algorithm, whose subproblems convex problems.5. CCQSP Planning Non-convex State SpaceNext, consider second spiral, comprised Problem 3 Section 3.2, variant CCQSPplanning problem involves fixed schedule non-convex constraints, obstacles,shown Figure 4-(b). again, encoded chance-constrained optimization problem,addition obstacle avoidance constraints requires disjunctive state constraints. Hence,problem results non-convex, chance-constrained optimization. section introducesnovel algorithm, called Non-convex Iterative Risk Allocation (NIRA), optimally solves deterministic approximation Problem 3.solution CCQSP planning problem non-convex state space two-fold.first step, described Section 5.1, obtain deterministic approximation Problem 3. orderhandle disjunctive chance constraints, develop additional decomposition approach calledrisk selection, reformulates chance constraint disjunction constraints disjunction individual chance constraints. chance constraints (29) decomposedset individual chance constraints risk allocation risk selection, techniqueSection 4.1.4 used obtain equivalent deterministic constraints. result, obtaindisjunctive convex programming problem (Problem 6 Section 5.1.3).deterministic disjunctive convex programming problem solved second step, described Sections 5.2-5.4. introduce NIRA algorithm (Algorithm 1) significantly reduces computation time without making compromise optimality solution.reduction computation time enabled new bounding approach, Fixed Risk Relaxation(FRR). FRR relaxes nonlinear constraints subproblems branch-and-bound algorithmlinear constraints. many cases, FRR nonlinear subproblems formulated linearprogramming (LP) approximated LP. NIRA obtains strictly optimal solution Problem6 solving subproblems exactly without FRR unpruned leaf nodes search tree,subproblems solved approximately FRR order reduce computation time.537fiO , W ILLIAMS , & B LACKMORE5.1 Deterministic ApproximationSection 4, first obtain deterministic approximation Problem 3.5.1.1 R ISK ELECTION PPROACHdeterministic approximation obtained decomposing non-convex joint chance constraint(29) set individual chance constraints, risk allocation risk selection. revisitrace car example explain concept risk selection intuitively.Figure 8: racing car example, risk selection approach guarantees 0.1% risk boundpaths, lets vehicle choose better one.Racing Car Example consider example shown Figure 8, vehicle uncertaindynamics plans path minimizes time reach goal. vehicle allowed chooseone two routes shown Figure 8. impose chance constraint limits probabilitycrashing wall mission 0.1%.satisfaction chance constraint guaranteed following process. First,routes, find safety margin limits probability crash throughout route0.1% start goal. Then, let vehicle plan nominal path operates withinsafety margins. Since routes 0.1% safety margin, chance constraint satisfiedmatter route vehicle chooses. Therefore, vehicle optimize path choosingroute results smaller cost. optimization process considered selectionrisk; vehicle given two options Figure 8, routes (a) (b), involvelevel risk; vehicle selects one results less cost. Hence, namedecomposition approach risk selection.5.1.2 ECOMPOSITION C ONJUNCTIVE J OINT C HANCE C ONSTRAINT R ISKELECTIONsubsection, derive mathematical representation risk selection. Let Ci proposition either true false. following lemma holds:538fiP ROBABILISTIC P LANNING C ONTINUOUS DYNAMIC YSTEMS B OUNDED R ISKLemma 5.Pr[N]Ci 1i=1NPr [Ci ] 1i=1Proof. following inequality always holds:Pr[N]Ci Pr [Ci ] .(52)i=1Hence,Pr[N]Ci 1 Pr [Ci ] 1i=1NPr [Ci ] 1 .(53)i=1following corollary follows immediately Lemmas 3 5.Corollary 2. following set constraints sufficient condition disjunctive joint chanceconstraint (29) Problem 3:c,i 0, 0cCiIc (s) jJc,iTcmaxc,i +tTt=0 iIUiIc (s)hTc,i,j xti gc,i,j mc,i (c,i )t,i c5hU,i ut gU,i mU,t,i (t,i ).(54)iIUNote resulting set constraints (54) sufficient condition original chanceconstraint (29). Therefore, solution satisfies (54) guaranteed satisfy (29). Furthermore,although (54) conservative approximation (29), conservatism introduced risk selectiongenerally small many practical applications. claim empirically validated Section7.2.3.5.1.3 ETERMINISTIC PPROXIMATION P ROBLEM 3Using Corollary 2, non-convex fixed-schedule CCQSP planning problem (Problem 3) approximated following deterministic convex optimization problem. later convenience,label part optimization problem (objective function), (plant model), C (chanceconstraints states), (chance constraints control inputs), R (risk allocation constraint).539fiO , W ILLIAMS , & B LACKMOREProblem 6: Deterministic Approximation Problem 3minu1:N ,c,i 0,t,i 0s.t.(O :)J (u1:N , x1:N )(55)(M :) , xt+1 = xt + B ut(C :)hTc,i,j xti gc,i,j mc,i,j (c,i )(56)(57)cC iIc (s) jJc,i(D :)hU,i ut gU,i mU,t,i (t,i )(58)tT iIU(R :)Tcmaxc,i +cC iIc (s)t,i c .(59)t=0 iIUfollows immediately Corollary 2 optimal solution Problem 6 guaranteedfeasible solution original problem regard satisfying chance constraints(Problem 3). Furthermore, empirically demonstrate Section 7.2.3 near-optimalsolution Problem 3 applications.5.2 NIRA: Branch Bound-Based Solution Problem 6next present Non-convex Iterative Risk Allocation (NIRA) algorithm. Recall NIRAoptimally solves Problem 6 branch-and-bound algorithm. standard branch-and-boundsolution problems involving disjunctive nonlinear constraints, Problem 6,use bounding approach nonlinear convex relaxed subproblems constructedremoving non-convex constraints corresponding disjunction. approachused Balas (1979) Li Williams (2005) different problem known disjunctive linearprogramming, whose subproblems LPs instead convex programmings. However, althoughstandard branch-and-bound algorithm guaranteed find globally optimal solution Problem 6,computation time slow algorithm needs solve numerous nonlinear subproblemsorder compute relaxed bounds.new bounding approach, Fixed Risk Relaxation (FRR), addresses issue computinglower bounds efficiently. observe relaxed subproblems nonlinear convex optimization problems. FRR relaxes nonlinear constraints linear constraints. Particularly,objective function linear, FRR subproblem (Problem 8) LP,efficiently solved. optimal objective value FRR subproblem lower boundoptimal objective value original subproblem.NIRA solves FRRs subproblems order efficiently obtain lower bounds,solving original subproblems exactly without relaxation unpruned leaf nodes order obtainexact optimal solution. result, NIRA achieves significant reduction computation time,without loss optimality.5.2.1 NIRA LGORITHM OVERVIEWAlgorithm 1 shows pseudocode NIRA algorithm. input deterministic approximation non-convex chance-constrained optimal control problem (Problem 6), five-tuple540fiP ROBABILISTIC P LANNING C ONTINUOUS DYNAMIC YSTEMS B OUNDED R ISKAlgorithm 1 Non-convex Iterative Risk Allocation (NIRA) algorithmfunction NIRA(problem) returns optimal solution Problem 61: Set queue FILO queue2: Incumbent3: rootSubproblem obtainRootSubproblem(problem)4: queue rootSubproblem5: queue empty6:subproblem last entry queue7:Remove subproblem queue8:lb obtainLowerBound(subproblem)9:lb Incumbent10:c = |C| = |Ic (s)|11:(J, U ) Solve(subproblem)12:J < Incumbent13:Incumbent J, U U //Update optimal solution14:end15:else16:ii+117:> |Ic (s)|18:c c + 1, 119:end20:j Jc,i21:newSubproblems Expand(subproblem,problem,c,i,j)22:Add newSubproblems queue23:end24:end25:end26: end27: return UO, M, C, D, R, well fixed schedule s. output optimal nominal control sequenceU := [u0 uN 1 ].node branch-and-bound search tree corresponds subproblem convexchance-constrained optimization problem (Problem 5). use FILO queue store subproblemssearch conducted depth-first manner (Line 1). node, correspondingsubproblem solved obtain lower bound objective value subsequent subproblems(Line 8). details bounding approaches explained Subsection 5.4. lower boundlarger incumbent, algorithm prunes branch. Otherwise, branch expanded(Line 21). branch expanded leaf without pruned, subproblems solved exactly(Line 11). Subsection 5.3 explains expansion procedure detail. NIRA algorithm alwaysresults globally optimal solution Problem 6, since solution U obtained solvingsubproblems leaf nodes exactly. next two subsections introduces branching boundingmethods.541fiO , W ILLIAMS , & B LACKMORE5.3 Branchingsubsection explains NIRA constructs root subproblem (Line 3 Algorithm 1),well expands nodes (Line 21 Algorithm 1). root subproblem convexoptimal CCQSP planning problem without chance constraints. node expanded,subproblems children nodes constructed adding one constraint disjunctionsubproblem parent node. order simplify notations, let Cc,i,j represent individualchance constraint (57) Problem 6:{rue (if hTc,i,j gc,i,j xti mc,i,j (c,i ))Cc,i,j :=F alse (otherwise).5.3.1 WALK - E XAMPLEfirst present walk-through example intuitively explain branching procedure. example instance Problem 6, involves four individual chance constraints:hT1,i,j xti g1,i,j m1,i,j (1,i )(60)i{1,2} j{1,2}Using notation defined above, set individual chance constraints (57) representedfollows:(C1,1,1 C1,1,2 ) (C1,2,1 C1,2,2 )(61)Figure 9-(a) shows tree obtained dividing original problem subproblems sequentially.subproblems corresponding trees four leaf nodes (Nodes 4-7 Figure 9-(a)) exhaustconjunctive (i.e., convex) combinations among chance constraints (61). hand,subproblems corresponding three branch nodes (Nodes 1-3 Figure 9-(a)) involve disjunctive(i.e., nonconvex) clauses chance constraints. relax non-convex subproblems convexsubproblems removing clauses contain disjunctions order obtain search treeshown Figure 9-(b).Figure 9: Branch-and-bound search tree sample disjunctive convex programming problem(Problem 6) constraints (60). (a) Tree non-convex subproblems, (b) Tree relaxed convex subproblems.542fiP ROBABILISTIC P LANNING C ONTINUOUS DYNAMIC YSTEMS B OUNDED R ISKnon-convex problem (Problem 6) optimally solved repeatedly solving relaxedconvex subproblems using algorithms presented Section 4. following subsections introduce algorithms construct search tree relaxed convex subproblems, oneFigure 9-(b).5.3.2 R ELAXED C ONVEX UBPROBLEMformulation relaxed convex subproblems given Problem 7. represent indexj j(c, i) since convex relaxation chooses one disjunct disjunction specifiedoptimal objective value relaxed(c, i). Let Ic set indices i. denote JSPsubproblem.Problem 7: Convex Relaxed Subproblem NIRAJSP=minu1:N ,c,i 0,t,i 0s.t.(O :) J (u1:N , x1:N )(M :)(C :), xt+1 = xt + B uthTc,i,j(c,i) xti gc,i,j(c,i) mc,i,j(c,i) (c,i ) (62)cC iIc(D :)hU,i ut gU,i mU,t,i (t,i )(63)tT iIU(R :)Tcmaxc,i +cC iIct,i c .(64)t=0 iIUNote Problem 7 identical Problem 5. Hence, algorithms introduced Section 4used solve relaxed subproblems.5.3.3 C ONSTRUCTION ROOT UBPROBLEMroot subproblem special case Problem 7 Ic empty set c C.function presented Algorithm 2 used Line 3 NIRA algorithm (Algorithm 1)construct root subproblem branch-and-bound tree. Note that, Algorithm 2,use object-oriented notation, subproblem.O, represent objective functionsubproblem. resulting root subproblem follows:5.3.4 E XPANSION UBPROBLEMSorder create child subproblem subproblem, function described Algorithm 3used Line 21 NIRA algorithm (Algorithm 1). adds individual chance constraintspecified indices (c, i, j) conjunct. Note resulting child subproblem stillconvex optimization, individual chance constraint added conjunctively. NIRAalgorithm (Algorithm 1) enumerates children nodes disjuncts Jc,i (Lines 20-23).543fiO , W ILLIAMS , & B LACKMOREAlgorithm 2 Construction root subproblem NIRAfunction obtainRootSubproblem(problem) returns root subproblem1: rootSubproblem.O problem.O2: rootSubproblem.M problem.M3: rootSubproblem.D problem.D4: c C5:rootSubproblem.Ic maxTc6:rootSubproblem.Rc .lhs t=0iIU t,i7:rootSubproblem.Rc .rhs problem.Rc .rhs8: end9: return rootSubproblemAlgorithm 3 Expansion subproblem NIRAfunctionExpand(subproblem, problem, c, i, j)lem1: subproblem.Ic subproblem.Ic2: subproblem.Rc .lhs subproblem.Rc .lhs + c,i3: return subproblemreturnschildsubprob-5.4 Boundingsubsection, present two implementations obtainLowerBound function Line 8Algorithm 1. first one uses optimal solution convex subproblems (Problem 7)lower bounds. approach typically results extensive computation time. second one solvesLP relaxation convex subproblems, called fixed risk relaxation (FRR). FRR dramaticallyreduces computation time compared first implementation. NIRA algorithm employssecond implementation.5.4.1 IMPLE B OUNDINGAlgorithm 4 shows straightforward way obtain lower bounds. simply solvesconvex relaxed subproblems (Problem 7) using methods presented Section 4.2. optimalobjective value relaxed subproblem gives lower bound optimal objective valuesubproblems it. example, optimal solution relaxed subproblem Node 2Figure 9-(b) gives lower bound objective value subproblems Nodes 4 5.constraints relaxed subproblems always subset constraintssubproblems below. Note optimization problems formulated minimizations.However, despite simplicity approach, computation time slow algorithm needs solve myriad subproblems. example, simple path planning problemAlgorithm 4 simple implementation obtainLowerBound function Line 8 Algorithm 1function obtainLowerBound-Naive(subproblem) returns lower bound1: Solve subproblem using algorithms presented Section 4.22: return optimal objective value544fiP ROBABILISTIC P LANNING C ONTINUOUS DYNAMIC YSTEMS B OUNDED R ISKten time steps one rectangular obstacle requires solution 410 = 1, 048, 576 worstcase, although branch-and-bound process often significantly reduces number subproblemssolved. Moreover, subproblems (Problem 7) nonlinear convex optimization problemsdue nonlinearity mc,i,j mU,t,i (62) (63). general nonlinear optimization problem requires significantly solution time specific classes optimization problems,linear programmings (LPs) quadratic programmings (QPs).5.4.2 F IXED R ISK R ELAXATIONnew relaxation approach, fixed risk relaxation (FRR), addresses issue. FRR linearizesnonlinear constraints (62) (63) Problem 7 fixing individual risk allocations,c,i t,i , upper bound . objective function linear, FRR LP.FRR convex piecewise linear objective function also reformulated LPintroducing slack variables (See Section 7.1.1 example.). general convex objective functionapproximated convex piecewise linear function. Hence, many cases, FRRssubproblems result LPs, solved efficiently. fixed risk relaxation Problem7 follows:Problem 8: Fixed Risk Relaxation Problem 7JF RR = minu1:Ns.t.J (u1:N , x1:N ),xt+1 = xt + B uthTc,i xti gc,i mc,i,j(c,i) (c )(65)cC iIc (s)hU,i ut gU,i mU,t,i (c )(66)tT iIUNote nonlinear terms (62) (63), mc,i,j mU,t,i , become constant fixing c,it,i c , constant. optimal objective value FRR provides tightest lowerbound among linear relaxations constraints (62) (63). following lemmas hold:Lemma 6. Problem 8 gives lower bound optimal objective value Problem 7:JF RR JSPProof. mc,i,j () mU,t,i () monotonically decreasing functions. Since c,i c t,ic , individual chance constraints (65) (66) Fixed Risk Relaxation less stricterfirst conjunct (62) (63). Therefore, cost optimal solution Fixed RiskRelaxation less equal original subproblem.Lemma 7. FRR gives tightest lower bound among linear relaxations constraints (62)(63).Proof. linear relaxation (62) (63) becomes tighter fixing c,i t,i lesser value.However, setting c,i t,i values less c may exclude feasible solutions, one545fiO , W ILLIAMS , & B LACKMOREAlgorithm 5 FRR implementation obtainLowerBound function Line 8 Algorithm 1function obtainLowreBound-FRR(subproblem) returns lower bound1: (c, i, j) subproblem.C2:subproblem.Cc,i,j .rhs mc,i,j (c ) //Apply fixed risk relaxation3: end4: (t, i)5:subproblem.Dt,i .rhs mU,t,i //Apply fixed risk relaxation6: end7: Remove subproblem.R8: Solve subproblem using LP solver9: return optimal objective valuesets c,i = c (c, i). Hence, FRR tightest linear relaxation (62) (63),resulting tightest lower bound.Note optimal solution Fixed Risk Relaxation (Problem 8) typically infeasiblesolution Problem 7, since setting c,i = t,i = c violates constraint (64).Algorithm 5 implements fixed risk relaxation. LP relaxation solved LP solver,optimal objective value returned.completes second spiral, planning CCQSPs fixed schedule nonconvexconstraints. next section, turn final spiral, involves flexible temporal constraints.6. CCQSP Planning Flexible Schedulesection presents complete p-Sulu Planner, efficiently solves general CCQSPplanning problem flexible schedule non-convex state space (Problem 2 Section3.1.2). problem find schedule events satisfies simple temporal constraints,well nominal control sequence u0:N 1 satisfies chance constraints minimizes cost.approach first generate feasible schedule extend control sequenceschedule, iteratively improving candidate schedules using branch-and-bound.build p-Sulu Planner upon NIRA algorithm presented previous section. RecallNIRA optimizes nominal control sequence u0:N 1 given fixed schedule s. p-SuluPlanner uses NIRA subroutine takes schedule input, outputs optimalobjective value well executable control sequence. denote optimal objective valuegiven schedule J (s). Using notation, CCQSP planning problem flexibleschedule (Problem 2) rewritten schedule optimization problem follows:min J (s).sSF(67)Recall domain feasible schedules SF (Definition 11) finite set, since considerdiscretized, finite set time steps (see Section 2.1). Hence, schedule optimization problem(67) combinatorial constraint optimization problem, constraints given formsimple temporal constraints.546fiP ROBABILISTIC P LANNING C ONTINUOUS DYNAMIC YSTEMS B OUNDED R ISKAlgorithm 6 p-Sulu Plannerfunction pSulu(ccqsp) returns optimal schedule control sequence1: Incumbent =2: Set queue FILO queue3: E0 = {e0 }, 0 (e0 ) = 0//initialize partial schedule4: queue E0 , 05: queue empty6:E , last entry queue7:Remove E , queue8:[J , u0:N 1 ] obtainLowerBound(ccqsp, E , )9:J < Incumbent10:E = E11:Incumbent J , OptCtlSequence u0:N 1 , OptSchedule12:else13:expand(ccqsp, queue, e, E , )14:end15:end16: end17: return OptCtlSequence, OptSchedule6.1 Algorithm Overviewsolution approach use branch-and-bound algorithm. branch-and-boundsearch, p-Sulu Planner incrementally assigns execution time step event orderfind schedule minimizes J (s) (67). objective function evaluated solvingfixed schedule CCQSP planning problem using NIRA algorithm. Although combinationtwo branch-and-bound searches p-Sulu Planner NIRA equivalent one unifiedbranch-and-bound search practice, treat separately ease explanation.shown Figure 12, branch-and-bound algorithm searches optimal scheduleincrementally assigning execution time steps event depth-first manner. nodesearch tree corresponds partial schedule (Definition 2), assigns execution time stepssubset events included CCQSP. partial schedule root node involvesassignment start node e0 . tree expanded assigning execution time step one newevent time. example, node (e1 ) = 2 Figure 12-(a) represents partial scheduleassigns execution time step = 0 event e0 = 2 e1 , leaving eE unassigned.p-Sulu Planner obtains lower bound objective function value J (s) solvingCCQSP planning problem partial schedule extended s. p-SuluPlanner minimizes search space dynamically pruning domain forward checking.specifically, execution time assigned event iteration branch-andbound search, p-Sulu Planner runs shortest-path algorithm tighten real-valued upperlower bounds execution time step unassigned events according newly assignedexecution time step.Algorithm 6 shows pseudocode algorithm. node search tree, fixedschedule CCQSP planning problem solved given partial schedule. node547fiO , W ILLIAMS , & B LACKMOREleaf tree optimal objective value less incumbent, optimal solutionupdated (Line 11). node leaf, optimal objective value correspondingsubproblem lower bound optimal objective value subsequent nodes. lowerbound less incumbent, node expanded enumerating feasible execution timeassignments unassigned event (Line 13). Otherwise, node expanded, hencepruned. Details branch-and-bound process described later subsections.Figure 10: (a) example CCQSP; (b) plan satisfies CCQSP (a)Figure 11: (a) directed distance graph representation CCQSP Figure 10-(a); (b) dgraph (a), shows shortest distances nodes; (c) updated d-graphexecution time = 2 assigned event e1 .(e0) = 0(e1)(e0) = 0012(e1) = 23(eE)(eE)(a)012435(b)Figure 12: Branch-and-bound search schedule s. assume time interval =1.0. (a)node (e0]) = 0 expanded; De1 () = {1, 2, 3} given (e0 ) = 0,[ maxsince de (), dmin1) = 2e () = [0.8, 3.9] Figure 11-(b); (b)][ node (emin()=(),expanded; DeE () = 4, 5 given (e0 ) = 0 (e1 ) = 2, since dmaxee[3.6, 5.5] Figure 11-(c).548fiP ROBABILISTIC P LANNING C ONTINUOUS DYNAMIC YSTEMS B OUNDED R ISKWalk-through example present walk-through example give readers insight solution process. consider CCQSP shown Figure 10-(a). CCQSP specifies missiongo waypoint get goal region B avoiding obstacle C, shownFigure 10-(b). assume time interval = 1.0.Figures 11 12 illustrate solution process. p-Sulu Planner algorithm initializedassigning execution time 0 start event e0 . Figure 11-(a) distance graph representation simple temporal constraints (Dechter, 2003) CCQSP. Note simple chanceconstraint equivalently represented pair inequality constraints follows:s(e) s(e ) [l, u] s(e) s(e ) u s(e ) s(e) l.two inequality constraints represented two directional edges two nodesdistance graph. p-Sulu Planner runs all-pair shortest-path algorithm distancegraph obtain d-graph shown Figure 11-(b). d-graph completed distance graphedge labeled shortest-path length. d-graph represents tightest temporalconstraints. algorithm enumerates feasible execution-time assignments evente1 using d-graph. According d-graph, execution time event e1 must0.8 3.9. Since consider discrete time steps time interval = 1.0, feasibleexecution time steps e1 {1, 2, 3}. idea behind enumerating feasible execution timesteps assign event, thus tighten bounds unassigned events order ensurefeasibility.node (e1 ) = 1, p-Sulu Planner solves FRR fixed-schedule CCQSPplanning problem End episode execution schedule (e1 ) = 1.words, tries find optimal path goes = 1, neglects goal Bobstacle C. solution exists, optimal cost gives lower bound objective valuefeasible paths go = 1. Assume solution exist. Then,p-Sulu Planner prunes node (e1 ) = 1, goes next node (e1 ) = 2. solvesFRR corresponding fixed-schedule subproblem find best path goes= 2. Assume p-Sulu Planner finds solution. Then, p-Sulu Planner expandsnode following process. First, fixes execution time (e1 ) = 2 d-graph,runs shortest-path algorithm order tighten temporal constraints (11-(c)). pSulu Planner uses updated d-graph enumerate feasible execution-time assignmentsevent eE , {4, 5}. visits nodes solves fixed-schedule subproblems exactlyepisodes fully assigned schedule. example, node (eE ) = 5, computesbest path goes = 2 reaches B = 5 avoiding obstacle C,shown Figure 10-(b). Assume optimal objective values subproblems 10.0(eE ) = 4 8.0 (eE ) = 5. algorithm records solution (eE ) = 5 cost8.0 incumbent.algorithm backs visits node (e1 ) = 3, relaxed subproblemEnd episode solved obtain lower bound objective value subsequentnodes. lower bound turns 9.0, exceeds incumbent. Therefore, branchpruned. Since nodes expand, algorithm terminated, incumbentsolution returned.549fiO , W ILLIAMS , & B LACKMOREAlgorithm 7 Implementation expand function Line 13 Algorithm 6function expand(ccqsp, queue, e, E , )1: Fix distance e0 e (e)T d-graph ccqsp2: Update d-graph running shortest-path algorithm3: Choose e E\E//choose unassigned event4: E := E emax5: De () := { | dmine () tT de ()}6: De (){(e) (e E )7:(e) :=//update partial schedule(e = e )8:queue E ,9: end6.2 BranchingAlgorithm 7 outlines implementation expand() function Algorithm 6. takes partialschedule input, adds queue set schedules assign execution time stepadditional event e . words, domain newly added schedules E oneassigned event domain input partial schedule E . details Algorithm 7explained following parts subsection.6.2.1 E NUMERATION F EASIBLE IME TEP SSIGNMENTS USING - GRAPHenumerating feasible time steps, simple temporal constraints must respected.accomplish this, use d-graph translate bounds durations two eventsbounds execution time step event. shown Dechter et al. (1991)set feasible execution times event e bounded distance e e0 dgraph. d-graph directed graph, weights edges represent shortest distancesnodes, Figure 11-(b). order obtain d-graph representation, first translatesimple temporal constraints directed distance graph, Figure 11-(a). weightedge two nodes (events) corresponds maximum duration time origin nodedestination node, specified corresponding simple temporal constraint. distancetakes negative value represent lower bounds. d-graph (Figure 11-(b)) obtaineddistance graph (Figure 11-(a)) running all-pair shortest-path algorithm (Dechter et al., 1991).Forward checking d-graph p-Sulu Planner algorithm incrementally assignsexecution time step event, explained walk-through example. p-Sulu Plannerminimizes search space forward checking using d-graph. forward checkingmethods Constraint Programming, method prunes values unassigned variables (i.e.,execution times unassigned event) violate simple temporal constraints. differentnormal forward checking back tracking performed, due decomposabilityd-graph. forward checking conducted following process. execution time stepassigned event e (i.e., (e) = t), distance e0 e fixed tT , distancee e0 fixed tT distance graph (Line 1 Algorithm 7). Recallindex discretized time steps fixed interval , temporal bounds givenreal-valued times (Section 2.1). run shortest-path algorithm update d-graph (Line550fiP ROBABILISTIC P LANNING C ONTINUOUS DYNAMIC YSTEMS B OUNDED R ISK2). Given partial schedule , denote updated shortest distance start event e0 emind-graph dmaxe (), distance e e0 de ().example, execution time 2 assigned event e1 Figure 11-(c) (i.e., (e1 ) = 2),distance e0 e1 fixed 2 distance opposite direction fixed2. run shortest-path algorithm update d-graph. result, obtainminupdated distances dmaxeE () = 5.5 deE () = 3.6.maxDechter et al. (1991) showed de () corresponds upper bound feasible execution time unassigned event e , dmineE () corresponds negative lower bound.Hence, partial schedule assigned events e E , updated domain unassignedmaxevent e/ E bounded dmine () de (). Note domain execution time stepsmaxe included in, equal [dmine (), de ()], consider discrete executiontime steps finite set T. forward checking, p-Sulu Planner computesmaxreal-valued bounds [dmine (), de ()]. feasible values unassigned variable eenumerated search tree expanded e .Enumerating domain execution time steps unassigned event readilyextract feasible execution time steps unassigned event e/ E updated d-graphpartial schedule . Let De () domain execution time steps unassigned evente/ E , given partial schedule . finite domain De () obtained follows:maxDe () := { | dmine () tT de ()}.Note De () may empty temporal constraints tight, even though feasible. user p-Sulu Planner must make small enough De empty.example, Figure 11-(b) d-graph given partial schedule {(e0 ) = 0}. Accordingd-graph, e1 must executed 0.8 3.9. Assuming = 1, set feasibleexecution time steps e1 De1 () = {1, 2, 3}, shown Figure 12-(a). Likewise, Figure 11-(c)d-graph given partial schedule {(e0 ) = 0, (e1 ) = 2}; feasible execution time eE3.6 5.5. Hence, set feasible execution time steps eE DeE () = {4, 5},shown Figure 12-(b).enumeration conducted Line 6 Algorithm 7. algorithm creates extensionsinput partial schedule assigning time steps e (Line 7), puts extendedpartial schedules queue (Line 8).6.2.2 E FFICIENT VARIABLE RDERING B RANCH - -B OUND EARCHchoosing next event assign time step Line 3 Algorithm 7, two variable orderingheuristics found effective order reduce computation time.first heuristic new convex-episode-first (CEF) heuristic, prioritizes eventsassociated non-convex constraints. idea CEF heuristic basedobservation subproblems branch-and-bound algorithm particularly difficult solveepisodes A(E ) involve non-convex state constraints. Remain R2 \C (2Dplane minus obstacle C) episode walk-through example Figures 10 examplenon-convex episodes. Therefore, effective approach reduce computation timep-Sulu Planner minimize number non-convex subproblems solved branch-andbound process. idea realized sorting events episodes convexfeasible region always examined branch-and-bound process episodes551fiO , W ILLIAMS , & B LACKMOREnon-convex feasible region. walk-through example, note visited event e1event eE example. End episode involves convex stateconstraint Remain R2 \C (2D plane minus obstacle C) non-convex.second one well-known constrained variable heuristic. p-Sulu Plannerexpands node, counts number feasible time steps unassigned events, choosesone least number feasible time steps. second heuristic used break tiesfirst heuristic.6.3 Boundingnext present implementation obtainLowerBound() function Line 8 Algorithm 6.algorithm obtains lower bound solving relaxed CCQSP planning problem fixedpartial schedule.Algorithm 8 outlines implementation obtainLowerBound() function. takes partialschedule input, outputs lower bound objective function, well optimalcontrol sequence, given partial schedule . constructs relaxed optimization problem,involves episodes whose start end events assigned execution time steps (Line 1).optimization problem involves non-convex constraints, NIRA algorithm used obtainsolution problem (Line 3). Otherwise solve FRR convex optimization problemobtain lower bound efficiently (Line 5). input fully assigned schedule (E = E),corresponding node leaf node. case obtain exact solution CCQSPplanning problem fixed schedule running NIRA algorithm (Line 3). detailsAlgorithm 8 explained subsequent part subsection.Algorithm 8 Implementation obtainLowerBound function Line 8 Algorithm 6function obtainLowerBound(ccqsp, E , ) returns optimal objective value control sequence1: subprblem Problem 9 given ccqsp2: E = E A() episodes non-convex state regions,3:[J , u0:N 1 ] NIRA(subprblem) //Algorithm 14: else5:J obtainLowreBound-FRR(subprblem) //Algorithm 56:u0:N 17: end8: return [J , u0:N 1 ]6.3.1 R ELAXED PTIMIZATION P ROBLEM PARTIAL CHEDULEconsider relaxed optimization problem follows:552fiP ROBABILISTIC P LANNING C ONTINUOUS DYNAMIC YSTEMS B OUNDED R ISKProblem 9: Relaxed Optimization Problem Partial ScheduleJ () =minu0:N 1 UNs.t.J(u0:N 1 , x1:N , )(68), xt+1 = xt + B ut(69)hTc,a,k,j xt gc,a,k,j mc,a,k,j (c,a,k )cC a(c A()) ta () kKa jJa,k(70)c,a,k 1 c ,(71)kKa ,a(c A())J () optimal objective value relaxed subproblem partial schedule .Recall A() partial episode set , involves episodes whose startend nodes assigned execution time steps partial schedule (Definition 9).notational simplicity, merge three conjunctions (70) obtain following:hTc,i,j xti gc,i,j mc,i,j (c,i ).cC iIc () jJc,iNote chance constraint exactly (57), except partial schedulespecified instead fully assigned schedule s. Hence, Problem 9 instance non-convexCCQSP planning problem fixed schedule (Problem 6), optimally solvedNIRA algorithm. Also note fully assigned schedule leaf node branch-andbound search tree.optimal objective value Problem 9 gives lower bound optimal objective valuesubsequent subproblems branch-and-bound tree. property formally statedLemma 8 below. order prove feature, first define concept extensionpartial schedule follows:Definition 14. schedule : E 7 extension partial schedule : E 7assign time steps events domain :(e) = s(e) e E .example, Figure 12-(b), fully assigned schedule {s(e0 ) = 0, s(e1 ) = 2, s(eE ) = 4}{s(e0 ) = 0, s(e1 ) = 2, s(eE ) = 5} extension partial schedule {(e0 ) = 0, (e1 ) = 2}.following lemma always holds:Lemma 8. schedule extension partial schedule , optimal objective valueProblem 9 lower bound optimal objective value s:J () J (s).Proof. Since partial schedule, E E, hence A() A. Also, since (e) = s(e)e E , state constraints chance constraint (70) Problem 9 partial scheduleincluded problem full schedule s. means feasible state space553fiO , W ILLIAMS , & B LACKMOREproblem subset one . Hence, chance constraint (24) problemsatisfied, chance constraint (70) problem also satisfied. Therefore,problem always results better (less) equal cost problem ,former looser constraints.example, Figure 12-(b), e1 assigned execution time step eE not.Therefore, node (e1 ) = 2, chance-constrained optimization problem Endepisode solved partial schedule {(e0 ) = 0, (e1 ) = 2} (see Figure 10-(a)). giveslower bound cost problems fully assigned schedules {s(e0 ) = 0, s(e1 ) =2, s(eE ) = 4} {s(e0 ) = 0, s(e1 ) = 2, s(eE ) = 5}.Algorithm 8 obtains lower bound solving Problem 9 exactly using NIRA algorithm,involves episodes non-convex state regions (Line 3). function called leaf node,Problem 9 also solved exactly NIRA. solutions leaf subproblemscandidate solutions optimal solution overall problem. Hence, solving exactly,ensure optimality branch-and-bound search.6.3.2 F URTHER B OUNDING FRRrelaxed subproblem (Problem 9) convex, p-Sulu Planner solves FRR subproblem, instead solving exactly NIRA, order obtain lower bound efficiently(Line 5 Algorithm 8). Many practical CCQSP execution problems one episodenon-convex feasible region. example, CCQSP planning problem shown Figures2 3, safe region (R2 minus obstacles) non-convex, Provincetown (startregion), Scenic region, Bedford (goal region) convex. case subproblemssolved exactly leaf nodes, lower bounds always evaluated approximatesolutions FRRs subproblems non-leaf nodes.7. Resultssection empirically demonstrate p-Sulu Planner efficiently operate varioussystems within given risk bound. first present simulation settings Section 7.1. Section 7.2 presents simulation results NIRA algorithm, validates claimefficiently compute feasible near-optimal solution. Section 7.3 demonstrates p-Sulu Planner two different benchmark problems. simulation results highlight p-Sulu Plannerscapability operate within user-specified risk bound. Section 7.4 deploys p-Sulu PlannerPTS scenarios, Section 7.5 applies p-Sulu Planner space rendezvousautonomous cargo spacecraft International Space Station.7.1 Simulation SettingsRecall that, stated Section 2.4, p-Sulu Planner takes four inputs: stochastic plant modelM, initial condition I, CCQSP P , objective function J. section specifiesJ, commonly used problems Sections 7.2-7.4. specify Pproblem corresponding section.554fiP ROBABILISTIC P LANNING C ONTINUOUS DYNAMIC YSTEMS B OUNDED R ISK7.1.1 TOCHASTIC P LANT ODELsection explains plant model used Sections 7.2 - 7.4. Section 7.5 uses different plantmodel described detail Section 7.5.2. consider point-mass double-integrator plant,shown (72)-(73). Parameters, umax , vmax , 2 , set individuallyproblem. plant model commonly assumed literatures unmanned aerial vehicle (UAV)path planning (Kuwata & How, 2011; Leaute, 2005; Wang, Yadav, & Balakrishnan, 2007).state vector xt consists positions velocities x directions, controlvector consists accelerations:xt := [x vx vy ]T ,ut := [ax ay ]T .plant model specified following matrices:1 0 0t2 /200 1 00t2 /2, B =, w =A=0 0 100 0 010T, ||ut || umax , ||Cxt || vmax ,(C=0 0 1 00 0 0 12 00 20 00 0000000(72)00(73)).first constraint (73) imposed order limit acceleration. nonlinear constraintapproximated following set linear constraints:T, r n ut umax (n = 1, 2, , Nr )][2n2n, sinr n = cosNrNrchoose Nr = 16. second constraint (73) imposed order limit velocity. uselinear approximation above.7.1.2 BJECTIVE F UNCTIONSections 7.2.3, 7.3, 7.4, cost function Manhattan norm control inputplanning horizon, follows:J(xti , U , s) =(|ux,t | + |uy,t |) .t=1cost function represents total change momentum, roughly proportionalfuel consumption aerial vehicle. Note minimization problem piece-wise linearcost function equivalently replaced following minimization problemlinear cost function additional linear constraints introducing slack variables x,t y,t :min(x,t + y,t )t=1s.t.T,x,t ux,t x,t ux,t y,t uy,t y,t uy,t555fiO , W ILLIAMS , & B LACKMORESection 7.2.4, minimize expected quadratic cost follows:[]J(xti , U , s) =E u2x,t + u2y,t .(74)t=17.1.3 C OMPUTING ENVIRONMENTsimulations except ones Section 7.2 conducted machine dual-coreIntel Xeon CPU clocked 2.40 GHz, 16 GB RAM. algorithms implementedC/C++, run Debian 5.0.8 OS. simulations Section 7.2 conducted machinequad-core Intel Core i7 CPU clocked 2.67 GHz, 8 GB RAM. algorithmsimplemented Matlab, run Windows 7 OS. used IBM ILOG CPLEX OptimizationSolver Academic Edition version 12.2 linear program solver, SNOPT version 7.2-9convex optimization solver.7.2 NIRA Simulation Resultsfirst statistically compare performance NIRA prior art. Recall NIRAsolver CCQSP planning problems non-convex state constraints fixed schedule(Problem 3), used subroutine p-Sulu Planner.7.2.1 C OMPARED LGORITHMStwo existing algorithms solve problem:1. Fixed risk allocation (Blackmore et al., 2006) - approach fixes risk allocationuniform value. result, assumption cost function linear, Problem 6reformulated mixed-integer linear programming (MILP) problem,solved efficiently MILP solver, CPLEX.2. Particle Control (Blackmore, 2006) - Particle Control sampling-based method,uses finite number samples approximate joint chance constraints. controlsequence optimized number samples violate constraints less c Np ,Np total number samples. optimization problem reformulatedMILP, assumption cost function linear.also compare NIRA MDP Section 7.2.5. Although MDP solveexactly problem NIRA, also avoid risk considering penalty cost constraintviolations. purpose comparison highlight capabilities chance-constrainedplanning provide guarantee probability failure.7.2.2 P ROBLEM ETTINGScompare closed-loop open-loop NIRAs two algorithms 2-D path planningproblem randomized location obstacle, shown Figure 13. vehicle starts[0, 0] heads goal [1.0, 1.0], avoiding rectangular obstacle. obstacleedge length 0.6 placed random location within square region corners [0, 0],[1, 0], [1, 1], [0, 1]. consider ten time steps time interval = 1.0. require556fiP ROBABILISTIC P LANNING C ONTINUOUS DYNAMIC YSTEMS B OUNDED R ISKmean state = 10 [1.0, 1.0]. risk bound set = 0.01. set standarddeviation disturbance = 0.01. use expected quadratic cost function given (74).steady-state LQR gain used closed-loop NIRA Q = I4 R = 10000I2 ,n n identity matrix Q R cost matrices state control variables,respectively.1NIRA (closed-loop)NIRA (open-loop)Fixed Risk AllocationParticle Control0.8NIRA (closed-loop)NIRA (open-loop)10.80.60.60.40.40.20.20000.20.40.60.810(a) Nominal trajectories0.20.40.60.81(b) Nominal trajectories 3 ellipsesFigure 13: (a) instance 2-D path planning problem used 7.2.3. obstaclefixed size randomly placed within unit square run. (b) meanstandard deviation closed-loop open-loop NIRAs.7.2.3 P ERFORMANCE C OMPARISONRecall solution NIRA algorithm, used p-Sulu Planner solve subproblems, exactly optimal solution Problem 3, since risk allocation (Section 4.1.1)risk selection (Section 5.1.1) replace chance constraint (29) sufficient condition (57)(59). Since chance constraint (29) difficult evaluate, previously proposedmethods solve optimization approximation. provide empirical evidence riskallocation/selection approach results solution significantly closer optimal solutionprior art, satisfaction original constraint (29) guaranteed.evaluate suboptimality solutions difference risk bound, =0.001, resulting probability constraint violation, Pf ail , estimated Monte-Carlo simulation. 1 Pf ail equal left-hand-side value (29) Problem 3. Hence, chanceconstraint (29) equivalent to:Pf ail .strictly optimal solution problem achieve Pf ail = , although exactsolution unavailable, since algorithm solve Problem 3 exactly. solution suboptimal Pf ail < , ratio /Pf ail represents degree suboptimality. solutionviolates chance constraint Pf ail > .557fiO , W ILLIAMS , & B LACKMOREAlgorithmNIRA (Closed-loop)NIRA (Open-loop)Fixed Risk AllocationParticle Control(100 particles)Computation time[sec]54.8 36.925.0 13.10.42 0.04Probability failureCost0.0096 0.00080.0095 0.0008(2.19 0.40) 1040.666 0.0610.672 0.0680.726 0.11341.7 12.80.124 0.0360.635 0.048Table 1: averages standard deviations computation time, probability constraint violation, cost four algorithms. algorithms run 100 timesrandom location obstacle. risk bound set = 0.01. Note ParticleControl results less cost two methods solutions violatechance constraint.Table 1 compares performance four algorithms. values table averages standard deviations 100 runs random locations obstacle. probabilityconstraint violation, Pf ail , evaluated Monte-Carlo simulations 106 samples.Comparison closed-loop open-loop NIRAs comparing NIRA existing algorithms, first compare two variants NIRA: closed-loop open loop NIRAs. Table1 shows closed-loop NIRA results less cost open-loop NIRA. Importantly,former outperforms latter 100 test cases. reduction cost closed-loopapproach explained Figure 13-(b), shows 3 ellipses probability distributionstate. Since closed-loop NIRA assumes feedback control, future position less uncertain. result, plan generated closed-loop NIRA less conservative. fact, Table1 shows Pf ail closed-loop NIRA closer risk bound open-loopNIRA. However, closed-loop planning problem requires twice much solution timeopen-loop one since complicated due additional chance constraints control input.Comparison fixed risk allocation approach Table 1 shows closed open NIRAsresult average probabilities failure 0.0096 0.0095 respectively, within userspecified risk bound = 0.01. hand, fixed risk allocation approach resultsconservative probability failure, Pf ail = 0.000219, 98% smaller .result indicates solution NIRA significantly closer exactly optimal solutionfixed risk allocation approach. fact, NIRA algorithm results less cost fixed riskallocation approach 100 runs. optimizes risk allocationfixed risk allocation approach uses predetermined risk allocation.Figure 14 shows suboptimality measure /Pf ail open-loop NIRA different settings risk bound . values , suboptimality NIRA significantly smallerfixed risk allocation approach. graph shows tendency suboptimality NIRAgets smaller less , suboptimality fixed risk allocation approach approximately constant.NIRA achieves improvement solution optimality cost computation time; Table1 shows NIRA takes longer computation time risk allocation approach factor558fiP ROBABILISTIC P LANNING C ONTINUOUS DYNAMIC YSTEMS B OUNDED R ISKPfail /Figure 14: Suboptimality NIRA fixed risk allocation approach. Strictly optimal solution/Pf ail = 1. smaller value /Pf ail indicates solution suboptimal.two. Hence, NIRA fixed risk allocation approach provide users trade-offsuboptimality computation time.Comparison Particle Control Table 1 shows average probability failureParticle Control approach higher risk bound = 0.01, meaning approachtends generate infeasible solutions. hand, NIRA guarantees satisfactionchance constraint since employs conservative approximation joint chance constraint.Particle Control guarantee solution converges optimal solution increasing number samples infinity. However, using large number samples impractical,since computation time memory usage grow exponentially number samples increases.example, used 100 samples analysis Table 1. using 300 samples, took4596 seconds (about 1.5 hours) solve problem obstacles centered [0.5, 0.5].Computation 1000 samples could conducted, shortage memory.hand, computation time NIRA significantly shorter PC, guaranteeingfeasibility solution.7.2.4 PTIMAL P LANNING E XPECTED C OSTNext demonstrate capability p-Sulu Planner handle expected cost, instead costexpected trajectory, path planning problem presented above. Specifically,consider expected quadratic cost function shown (74). conducting open-loop planning,cost function transformed function nominal control inputs constant termusing equality (15). However, performing closed-loop planning, equalityexact, due controller saturation. Nevertheless, use (15) approximation expectedcost, explained Section 2.4.4. subsection empirically evaluate errorapproximation.559fiO , W ILLIAMS , & B LACKMOREApproximate expected cost0.048434950 0.010130589Actual expected cost0.048434956 0.010130588Table 2: Comparison approximate expected cost obtained closed-loop NIRAactual expected cost. table shows mean variance 100 runs randomlocation obstacle.Table 2 compares approximate expected cost function value obtained closed-loopNIRA actual expected cost estimated Monte-Carlo simulation one million samples.path planning problem solved 100 times randomized location obstacle. riskbound set = 0.01. shown table, approximate cost almost exactly agreesactual cost. closed-loop planning approach explicitly bounds riskcontroller saturation.7.2.5 C OMPARISON MDPNext compare NIRA MDP formulation. sake tractability MDP,consider single integrator dynamics two-dimensional state space two-dimensionalcontrol input, specifies velocity vehicle. rest problem setting same,except state space discretized 100-by-100 grid. implement finite-horizonMDP-based path planner, imposes penalty c event failure minimizesexpected cost based explicit state dynamic programming. MDP-based path planner imposescost follows:][()u2x,t + u2y,t + cI(xt ) ,Et=1I(xt ) indicator function one xt obstacle zero otherwise.resulting optimization problem solved via dynamic programming.ran MDP-based path planner three values penalty c: 1, 10, 100.choice c, conducted 100 simulations randomized obstacle position. Figure 14 showstypical output MDP-based path planner. Note that, small penalty (c = 1), pathplanner chooses take 100% risk failure ignoring obstacle. simplypenalty failure smaller expected reduction cost going obstacle.issue utilitarian approaches MDPs minimization unconstrained costsometimes lead impractical solution.Table 3 shows mean standard deviation path lengths, well maximum,minimum, mean resulting probability failure among 100 runs. expected,imposing larger penalty, MDP-path planner chooses risk-averse path,longer nominal path length. sense, MDP also conduct trade-off costrisk. MDP particularly useful primary concern user cost failure insteadprobability failure. hand, user would like impose hard boundprobability failure, chance constrained planning approach advantage. Observe that,even penalty value, MDP-based path planner results wide range failureprobabilities depending location obstacle. notably, c = 10,560fiP ROBABILISTIC P LANNING C ONTINUOUS DYNAMIC YSTEMS B OUNDED R ISKpaths move directly across obstacle, doing, accept 100% probability failure,others go around obstacle. Undesirable behaviors, crossing obstacle, likelysuppressed imposing greater penalty, without guarantee. Moreover, imposing heavypenalty failure often results overly conservative, risk averse solution. hand,behavior NIRA regarding risk predictable, sense path guaranteedgo around obstacle, regardless location. chance constraint requiresexists margin path boundary obstacle. p-Sulu Plannerinherits property NIRA.10.80.60.40.2c=1c=10c=100000.20.40.60.81Figure 15: Optimal paths generated MDP-based planner different penalty levels , c.red rectangle represents obstacle. Note path c = 1 cutsobstacle.Penalty cpath length1101001.41 0.001.54 0.051.57 0.06Probability failureMaxMean Min1.000 1.000 1.0001.000 0.375 0.0960.1215 0.031 0.009Table 3: 100 runs randomized obstacle location7.3 p-Sulu Planner Simulation ResultsNext present simulation results p-Sulu Planner two problems, order illustratecapability planning schedule constraints. also empirically evaluate scalabilityp-Sulu.561fiO , W ILLIAMS , & B LACKMOREFigure 16: sample CCQSP personal aerial vehicles path planning scheduling problem.Figure 17: Output p-Sulu Planner CCQSP Figure 16 three different settingsrisk bound obs , compared path planned deterministic planner, Sulu,consider chance constraints.7.3.1 PATH P LANNING BSTACLESsimulation test p-Sulu Planner path planning problem environment shownFigure 17. input CCQSP shown Figure 16. CCQSP requires vehicle arrivegoal region within 15 minutes, going Waypoint 1 Waypoint 2 temporalconstraints specified Figure 16. also imposes two chance constraints: one requiresvehicle achieve time-evolved goals 90% certainty, another requires vehiclelimit probability violating obstacles obs . set = 1 2 = 0.0025.Figure 17 shows plans generated p-Sulu Planner three different risk bounds:obs = 10%, 0.1%, 0.001%. computation times 79.9 seconds, 86.4 seconds,88.1 seconds, respectively. Figure 17 also shows plan generated Sulu, deterministic plannerexplicitly consider uncertainty (Leaute & Williams, 2005). Observe Sulu leavesmargin path obstacles. result, Sulu path results 94.1% probabilityhitting obstacles, estimated Monte-Carlo simulation 107 samples. hand,p-Sulu Planner leaves margins path obstacles order satisfy risk bound,562fiP ROBABILISTIC P LANNING C ONTINUOUS DYNAMIC YSTEMS B OUNDED R ISKspecified chance constraint. margins larger plans smaller risk bounds.probabilities failure three plans generated p-Sulu Planner, estimated Monte-Carlosimulations 107 samples, 9.53%, 0.0964%, 0.00095%, respectively. Hence chanceconstraints satisfied. schedule optimized p-Sulu Planner {s(e0 ) = 0, s(e1 ) =5, s(e2 ) = 10, s(eE ) = 15}, satisfies temporal constraints CCQSP.Figure 16, appears path cuts across obstacle. due discretizationtime; optimization problem requires vehicle locations discrete time stepsatisfy constraints, consider state between. issue addressedconstraint-tightening method (Kuwata, 2003).7.3.2 PATH P LANNING NDOOR E NVIRONMENTFigure 18: sample CCQSP path planning problem indoor environment.= 10%= 1%= 0.1%1.21Goal0.80.60.40.20Start0.20.200.20.40.60.811.2Figure 19: Output p-Sulu Planner CCQSP Figure 16 three different settingsrisk bound obs .next give p-Sulu Planner CCQSP shown Figure 18, simulates path planning problem indoor environment. vehicle must get goal region sideroom three five seconds. Remain safe region episode requires vehicle stay563fiO , W ILLIAMS , & B LACKMOREwithin room outside obstacle five-second planning horizon. CCQSPimposes two chance constraints shown Figure 18. set = 0.5 2 = 5.0 105 .Given CCQSP, planner faces choice: heading straight goal goingnarrow passage left wall obstacle minimizes path length, involveshigher risk constraint violation; making detour around right side obstacle involvesless risk, results longer path.Figure 19 shows p-Sulu Planners outputs obs = 10%, 1%, 0.1%. computation times 35.1 seconds, 84.5 seconds, 13.3 seconds, respectively. result consistentintuition. p-Sulu Planner allowed 10% risk, planner chooses go straightgoal, resulting cost function value 1.21; user gives 1% 0.1% risk bound,chooses risk-averse path, resulting cost function values 3.64 3.84, respectively.example demonstrates p-Sulu Planners capability make intelligent choice orderminimize cost, limiting risks user-specified levels.7.3.3 CALABILITY NALYSISsubsection conduct empirical analysis scalability p-Sulu Planner,environment becomes increasingly constrained.. shown Figure 20, measured computation time solve path planning problem different numbers obstacles waypoints.simulations, path starts [0, 12] ends square region centered [24, 12]. Figure 20shows twenty simulation results, zero three obstacles zero four waypoints. Obstacleswaypoints represented blue red squares figure, respectively. positionscenter obstacles [6, 12], [12, 12], [18, 12], positions centerwaypoints [9, 9], [9, 15], [15, 15], [15, 9]. computation time shown captionsubfigure Figure 20.comparing results Figure 20 horizontally, observe exponential growth computation time number obstacles. result expected since number disjunctiveclauses state constraint p-Sulu Planner increases exponentially numberobstacles. Building tractable extension p-Sulu Planner large number obstaclesfuture work. hand, comparing results vertically, find computationtime number obstacles different number waypoints stays ordermagnitude. adding extra waypoint increases number conjunctiveclauses state constraints.remaining sections describe application psulu two real world problems, airvehicle space vehicle control. third application, building energy management, using variantp-Sulu Planner, reported Ono, Graybill, Williams (2012).7.4 PTS ScenariosNext, deploy p-Sulu Planner PTS scenarios, robotic air taxi system introducedSection 1.7.4.1 CENARIOSconsider three scenarios, specified CCQSPs shown Figure 21. Scenarios 1 2similar scenic flight scenario introduced beginning paper (see Figure 1).564fiP ROBABILISTIC P LANNING C ONTINUOUS DYNAMIC YSTEMS B OUNDED R ISK0.003 sec0.173 sec51.47 sec677.2 sec0.016 sec0.518 sec48.25 sec648.4 sec0.034 sec1.047 sec118.3 sec4327 sec0.076 sec2.613 sec159.1 sec5686 sec0.036 sec3.873 sec165.9 sec6845 sec1512915129151291512915129091524091524091524091524Figure 20: Computation time p-Sulu Planner path planning problem different numbers obstacles waypoints.Scenario 1, personal aerial vehicle (PAV) takes Runway 71 Provincetown MunicipalAirport (KPVC) Provincetown, Massachusetts, fly scenic region, lands Runway 23Hanscom Field (KBED) Bedford, Massachusetts. vehicle required stay withinscenic region least 2 minutes 10 minutes. entire flight must take13 minutes less 15 minutes. Scenario 2 Scenario 1, except runwaysused take-off landing.Scenario 3 simulates leisure flight coast Massachusetts. PAV takes Runway 7Provincetown Municipal Airport, flies two regions whales often seen.vehicle lands Runway 11 Hanscom Field.place three no-fly zones, shown Figure 22. entire flight must take 13minutes less 15 minutes. scenario three chance constraints, {c1 , c2 , c3 }, shownFigure 21. first one, c1 , concerned vehicles operation; requires vehicletake land right runways right airports less 10 % probabilityfailure. second chance constraint, c2 , concerned leisure activities; requiresvehicle fly scenic regions less 10 % probability failure. Finally, c3concerned passengers safety; requires vehicle limit risk penetratingno-fly zones 0.01 %.1. runway airport specified number, represents clockwise angle north. example,Runway 7 points 70 degrees away north.565fiO , W ILLIAMS , & B LACKMORE7.4.2 P LANT PARAMETERSset umax = 250 m/s, approximates maximum cruise speed private jet airplanes,Gulfstream V. maximum acceleration determined maximum bank angle.Assuming aircraft flying constant speed, lateral acceleration given functionbank angle follows:= g tan ,g acceleration gravity. Typically passenger aircraft limits bank angle 25degrees passenger comfort, even though aircraft capable turning larger bankangle. Hence, use:umax = 9.8 m/s2 tan(25 ) = 4.6 m/s2 .set = 100 = 60 seconds.7.4.3 IMULATION R ESULTSFigure 22 shows paths planned p-Sulu Planner three scenarios. scenarios,episode requirements CCQSPs Figure 21 met within specified temporalchance constraints.Table 4 compares performance Sulu p-Sulu Planner. expected, Sulus plansresult excessive probabilities failure scenarios. Sulu consideruncertainty planning process, although PAV subject disturbance reality.hand, p-Sulu Planner successfully limits probability failure within user-specifiedrisk bounds three scenarios. Furthermore, although p-Sulu Planner significantly reducesrisk failure, cost higher Sulu 9.5 - 12.8 %. capabilitylimiting risk maximizing efficiency time desirable feature PTS,transports passengers.Scenario numberPlannerComputation time [sec]Pf ail,1Pf ail,2Pf ail,3Cost function value J1Sulu2.580.9990.8070.37324.22p-Sulu60.29.12 1028.46 1022.74 10527.5Sulu2.000.9960.8130.22721.0p-Sulu3909.14 1028.59 1022.62 10523.73Sulu5.170.9990.6030.37220.0p-Sulu1989.23 1027.65 1022.81 10522.3Table 4: Performance Comparison prior art, Sulu, p-Sulu Planner. Pf ail,1 , Pf ail,2 ,Pf ail,3 represent probabilities failure regarding chance constraints c1 , c2 ,c3 Figure 21, respectively.566fiP ROBABILISTIC P LANNING C ONTINUOUS DYNAMIC YSTEMS B OUNDED R ISKFigure 21: CCQSPs PTS scenarios.Figure 22: paths planned p-Sulu Planner.567fiO , W ILLIAMS , & B LACKMOREshown Table 4, p-Sulu Planner typically takes several minutes compute plan.length computation time would allowed PTS applications, since assume pSulu Planner used preplanning; take-off, passengers PAV specify requirements,p-Sulu Planner creates risk-sensitive flight plan. assume real-time plan executiveexecutes plan take-off.note desirable real-time risk-sensitive plan executive, since risk factors, location storms, change time. future work reduce computationtime p-Sulu Planner used real-time execution.7.5 Space Rendezvous Scenariop-Sulu Planner general planner whose application limited specific plant model.order show generality planner, deployed p-Sulu Planner system whoseplant model significantly different PTS.Specifically, chose autonomous space rendezvous scenario H-II Transfer Vehicle(HTV), shown Figure 23, subject. HTV unmanned cargo spacecraft developedJapanese Aerospace Exploration Agency (JAXA), used resupply International SpaceStation (ISS). Collision vehicle ISS may result fatal disaster, even collisionspeed low. example, August 1994, Russian unmanned resupply vehicle Progress M34 collided Mir space station failed attempt automatic rendezvous docking.result, one modules Mir permanently depressurized. order avoidaccident, HTV required follow specified safety sequence automated rendezvous,described following subsection.Figure 23: H-II Transfer Vehicle (HTV), Japanese unmanned cargo vehicle, conducts autonomousrendezvous International Space Station. Image courtesy NASA.7.5.1 HTV R ENDEZVOUS EQUENCEHTVs autonomous rendezvous mission, final approach phase starts Approach Initiation (AI) point, located 5 km behind ISS, shown Figure 24. First, HTV movesR-bar Initiation (RI) point, located 500 ISS, guided relative GPS568fiP ROBABILISTIC P LANNING C ONTINUOUS DYNAMIC YSTEMS B OUNDED R ISKnavigation. RI point, HTV switches navigation mode Rendezvous Sensor (RVS) Navigation. RVS Navigation, HTV measures distance ISS precisely beaming laserreflector placed nadir (earth-facing) side ISS. Then, HTV proceeds Hold Point(HP), located 300 ISS. required hold HP order perform 180-degreeyaw-around maneuver. new orientation HTV allows vehicle abort rendezvousquickly case emergency. yaw-around maneuver, HTV resumes approach,holds Parking Point (PP), 30 ISS. Finally, HTV approachesdistance 10 meters ISS, stops within Capture Box (CB) ISSs roboticarm. robotic arm grabs HTV docks ISS. Please refer report JapanAerospace Exploration Agency (2009) details rendezvous sequence.RIHPPP CBISS-300m-30m -10mISS Orbit-500xEarthAI: Approach InitiationRI: R-bar InitiationHP: Hold PointPP: Parking PointCB: Capture BoxAI Point-5000Figure 24: HTVs final approach sequence (Japan Aerospace Exploration Agency, 2009).rendezvous sequence described represented CCQSP shown Figure 25.addition time-evolved goals specified actual rendezvous sequence, specify temporalconstraints chance constraints simulation, shown figure. require HTV holdintermediate goal least 240 seconds. transition goals must takeleast 600 seconds, order make sure vehicle moves slowly enough. entire missionmust completed within 4800 seconds (1 hour 20 minutes). require HTV stay withinSafe Zone, conic area ISS, RVS navigation phase 99.5% probability,since otherwise laser may reflected back HTV properly. assume goalssquare regions, 10 sides RI HP, 2 sides PP, 1 sides CB. Finally,require HTV achieves time-evolved goals 99.5% success probability.7.5.2 RBITAL DYNAMICSrendezvous considered two-body problem, chaser spacecraft (e.g., HTV)moves relation target spacecraft (e.g., ISS), circular orbit. problem,convenient describe motion chaser spacecraft using rotating frame fixedtarget space craft, known Hill coordinate frame (Schaub & Junkins, 2003). shownFigure 24, set x-axis pointing away center earth y-axis along569fiO , W ILLIAMS , & B LACKMOREFigure 25: CCQSP representation HTVs final approach sequence. assumetime-evolved goals ones used actual flight missions. temporal constraintschance constraints added authors.orbital velocity target spacecraft. Since HTVs path within x-y plane, dont considerz-axis.known relative motion chase spacecraft Hill coordinate frame described following Clohessy-Wiltshire (CW) equation (Vallado, 2001):x = 2 + 3 2 x + Fx= 2 x + Fyangular speed target spacecrafts orbit, Fx Fy force per unitmass, acceleration x directions. first terms right-hand sides representCoriolis force.object follows CW equation moves unintuitive manner. unforced motionstraight line due Coriolis effect; general, object cannot stay positionwithout external force. example, Figure 26 shows fuel-optimal path visit two waypoints,B, come back start. seen figure, optimal path typicallystraight line. virtue p-Sulu Planner handle irregular dynamic systemsway regular systems, setting B matrices plant model (4)appropriately.state vector consists positions velocity x plane:x = [x vx vy ]Tobtain discrete-time CW equation using impulse-invariant discretization:xk+1 = Axk + Buk ,570fiP ROBABILISTIC P LANNING C ONTINUOUS DYNAMIC YSTEMS B OUNDED R ISKStartBFigure 26: typical motion spacecraft Hill coordinate frame. solid line fueloptimal path visits B returns Start 30 minutes. Noteoptimal path straight line Hill coordinate frame.=B =4 3 cos(T )6{T sin(T )}3 sin(T )6{1 cos(T )}0100sin(T )2{1cos(T )}2{1cos(T )}4 sin(T )3Tcos(T )2 sin(T )2 sin(T )4 cos(T ) 3sin(T )2{1cos(T )}2{1cos(T )}4 sin(T )3Tcos(T )2 sin(T )2 sin(T )4 cos(T ) 3use ISSs orbital angular speed, = 0.001164 rad/sec, station goes aroundEarth 90 minutes. choose interval = 120 seconds. number time steps Nset 40. Hence, entire plan 4800 seconds (1 hour 20 minutes). discretization,assumed impulse inputs follows:[FxFy]=N1(t k)uk ,k=0() Dirac delta function. assumption justified thrustersReaction Control System (RCS) spacecraft, used final approach maneuver,operate short duration (0.01 5.0 seconds) burn (Wertz & Wiley J. Larson, 1999).consider stochastic uncertainty w, added discrete-time dynamic equation:xk+1 = Axk + Buk + w.assumption additive uncertainty commonly used past research autonomousrendezvous formation flight space (Shields, Sirlin, & Wette, 2002; Smith & Hadaegh, 2007;571fiO , W ILLIAMS , & B LACKMORECampbell & Udrea, 2002). assume wfollowing covariance matrix:61000106w =0000zero-mean Gaussian distribution,000000.007.5.3 BJECTIVE F UNCTIONemploy objective function J requires p-Sulu Planner minimize fuel consumption. follows Tsiolkovsky rocket equation fuel consumption spacecraftproportional total change velocity, called Delta-V V (Wertz & Wiley J. Larson, 1999).total fuel consumption summation fuel consumption reaction jets xdirections time steps. Hence objective function described follows:J(u0:N ) = Vx + Vy(N 1)T=|Fx | + |Fy |dt0fifi fifik=Nfi fi (N 1)Tfi1 fifi (N 1)Tfi fifi=(t k)ux,k dtfi + fi(t k)uy,k dtfififi 0fi fi 0fi=k=0k=N1|ux,k | + |uy,k |.k=07.5.4 IMULATION R ESULTFigure 27 shows planning result p-Sulu Planner. compare result Sulu,well nominal planning approach, assume HTV moves AI RI usingtwo-impulse transition (called CW guidance law) (Matsumoto, Dubowsky, Jacobsen, & Ohkami,2003; Vallado, 2001). RI CB, follows predetermined path goes centerSafe Zone, shown Figure 27-(b), constant speed.shown Figure 27, optimal paths generated p-Sulu Planner Sulustraight. curved paths exploit Coriolis effect minimize fuel consumption.Table 5 compares performance three planning approaches. two rows regardingprobabilities failure correspond two chance constraints specified CCQSP, shownFigure 25. probabilities evaluated Monte Carlo simulation one million samples.expected, probabilities failure path generated p-Sulu Planner lessrisk bounds specified CCQSP, shown Figure 25. hand, again,Sulus path results almost 100% probability failure. Sulu minimizes fuelconsumption without considering uncertainty. resulting path pushes boundariesfeasible regions, evident Figure 27-(c). Also note that, although p-Sulu Plannersignificantly reduces probability constraint violation compared Sulu, cost (Delta V)higher Sulu 0.2%. p-Sulu Planner results significantly smaller cost (DeltaV) nominal planning approach. 1.42 m/sec reduction Delta V equivalent11.9 kg saving fuel, assuming 16, 500 kg mass vehicle 200 sec specific impulse572fiP ROBABILISTIC P LANNING C ONTINUOUS DYNAMIC YSTEMS B OUNDED R ISKFigure 27: Planning results Sulu, p-Sulu Planner, nominal planning approach.input CCQSP shown Figure 25.(ISP ) thrusters. Although p-Sulu Planner takes longer compute plantwo approaches, 11.4 second computation time negligible compared 1 hour 20minute plan duration.Computation time [sec]Probability failure Pf ail (Navigation)Probability failure Pf ail (Goals)Cost function value (Delta V) J [m/sec]Sulu3.90.921.07.30p-Sulu Planner11.40.00240.00297.32Nominal0.09< 106< 1068.73Table 5: Performance comparison Sulu, p-Sulu Planner, nominal approachHTV rendezvous scenario.573fiO , W ILLIAMS , & B LACKMORE8. Conclusionsarticle introduced model-based planner, p-Sulu Planner, operates within userspecified risk bounds. p-Sulu Planner optimizes continuous control sequence discreteschedule, given input continuous stochastic plant model, objective function, newlydeveloped plan representation, chance-constrained qualitative state plan (CCQSP). CCQSPinvolves time-evolved goals, simple temporal constraints, chance constraints, specifyusers acceptable levels risk subsets plan.approach developing p-Sulu Planner two-fold. first step, developedefficient algorithm, called non-convex iterative risk allocation (NIRA), plan nonconvex state space fixed schedule. solved problem based key conceptrisk allocation risk selection, achieves tractability allocating specified risk individual constraints mapping result equivalent disjunctive convex program.NIRA algorithm employs branch-and-bound algorithm solve disjunctive convex program.subproblems fixed-schedule CCQSP problems convex state space, solvedpreviously developed algorithms (Blackmore & Ono, 2009). developed novel relaxation method called fixed risk relaxation (FRR), provides tightest linear relaxationnonlinear constraints convex subproblems.second step, developed p-Sulu Planner, solve CCQSP planning problem flexible schedule. scheduling problem formulated combinatorial constrainedoptimization problem (COP), solved branch-and-bound algorithm. subproblem branch-and-bound search CCQSP planning problem fixed schedule,solved NIRA. domain feasible schedule pruned running shortest-path algorithm d-graph representation given temporal constraints. lower bounds optimal objective value subproblems obtained solving fixed-schedule CCQSP planningproblems subset state constraints imposed. proposed efficient variableordering prioritizes convex subproblems non-convex ones. demonstrated p-SuluPlanner various examples, personal aerial transportation system autonomous spacerendezvous, showed efficiently solve CCQSP planning problems small suboptimality, compared past algorithms.Acknowledgmentspaper based upon work supported part Boeing Company Grant No. MITBA-GTA-1 National Science Foundation Grant No. IIS-1017992. opinions,findings, conclusions recommendations expressed publication authorsnecessarily reflect view sponsoring agencies. would like thank MichaelKerstetter, Scott Smith, Ronald Provine, Hui Li Boeing Company support. Thanksalso Robert Irwin advice draft.ReferencesAcikmese, B., Carson III, J. M., & Bayard, D. S. (2011). robust model predictive control algorithmincrementally conic uncertain/nonlinear systems. International Journal RobustNonlinear Control, 21(5), 563590.574fiP ROBABILISTIC P LANNING C ONTINUOUS DYNAMIC YSTEMS B OUNDED R ISKAircraft Owners Pilots Association Air Safety Foundation (2005). 2005 Joseph T. Nall Report- accident trands factors 2004..Altman, E. (1999). Constrained Markov decision processes. Stochastic modeling. Chapman &Hall/CRC.Alur, R., Feder, T., & Henzinger, T. A. (1996). benefits relaxing punctuality. JournalACM, 43.Bacchus, F., & Kabanza, F. (1998). Planning temporally extended goals. Annals MathematicsArtificial Intelligence, pp. 527.Balas, E. (1979). Disjunctive programming. Annals Discrete Mathematics.Bertsekas, D. P. (2005). Dynamic Programming Optimal Control Volume (Third Edition).Athena Scientific.Bertsekas, D. P., & Tsitsiklis, J. N. (1996). Neuro-Dynamic Programming (1st edition). AthenaScientific.Blackmore, L. (2006). probabilistic particle control approach optimal, robust predictive control.Proceedings AIAA Guidance, Navigation Control Conference.Blackmore, L., Li, H., & Williams, B. C. (2006). probabilistic approach optimal robust pathplanning obstacles. Proceedings American Control Conference.Blackmore, L., & Ono, M. (2009). Convex chance constrained predictive control without sampling.Proceedings AIAA Guidance, Navigation Control Conference.Boyan, J. A., & Littman, M. L. (2000). Exact solutions time-dependent MDPs. AdvancesNeural Information Processing Systems, pp. 10261032. MIT Press.Boyan, J. A., & Moore, A. W. (1995). Generalization reinforcement learning: Safely approximating value function. Advances Neural Information Processing Systems 7.Campbell, M. E., & Udrea, B. (2002). Collision avoidance satellite clusters. ProceedingsAmerican Control Conference.Charnes, A., & Cooper, W. W. (1959). Chance-constrained programming. Management Science, 6,7379.Coles, A. J., Coles, A., Fox, M., & Long, D. (2012). Colin: Planning continuous linear numericchange. J. Artif. Intell. Res. (JAIR), 44, 196.Dechter, R. (2003). Constraint Processing. Elsevier.Dechter, R., Meiri, I., & Pearl, J. (1991). Temporal constraint networks. Artificial Intelligence, 49,6195.Dolgov, D., & Durfee, E. (2005). Stationary deterministic policies constrained MDPs multiple rewards, costs, discount factors. Proceedings Nineteenth InternationalJoint Conference Artificial Intelligence (IJCAI-05, pp. 13261331.Feng, Z., Dearden, R., Meuleau, N., & Washington, R. (2004). Dynamic programming structuredcontinuous markov decision problems. Proceedings Proceedings TwentiethConference Annual Conference Uncertainty Artificial Intelligence (UAI-04), pp. 154161, Arlington, Virginia. AUAI Press.575fiO , W ILLIAMS , & B LACKMOREFleming, W., & McEneaney, W. (1995). Risk-sensitive control infinite time horizon. SIAMJournal Control Optimization, 33(6), 18811915.Fox, M., & Long, D. (2006). Modelling mixed discrete-continuous domains planning. JournalArtificial Intelligence Research, 27, 235297.Geibel, P., & Wysotzki, F. (2005). Risk-sensitive reinforcement learning applied controlconstraints. Journal Artificial Intelligence Research, 24, 81108.Goulart, P. J., Kerrigan, E. C., & Maciejowski, J. M. (2006). Optimization state feedbackpolicies robust control constraints. Automatica, 42(4), 523 533.Hofmann, A. G., & Williams, B. C. (2006). Robust execution temporally flexible plans bipedalwalking devices. Proceedings International Conference Automated PlanningScheduling (ICAPS-06).Jacobson, D. (1973). Optimal stochastic linear systems exponential performance criteriarelation deterministic differential games. Automatic Control, IEEE Transactions on,18(2), 124 131.Japan Aerospace Exploration Agency (2009). HTV-1 mission press kit. Available on-line http://www.jaxa.jp/countdown/h2bf1/pdf/presskit_htv_e.pdf.Kuwata, Y., & How, J. P. (2011). Cooperative distributed robust trajectory optimization using receding horizon MILP. IEEE Transactions Control Systems Technology, 19(2), 423431.Kuwata, Y. (2003). Real-time trajectory design unmanned aerial vehicles using receding horizoncontrol. Masters thesis, Massachusetts Institute Technology.Kvarnstrom, J., & Doherty, P. (2000). Talplanner: temporal logic based forward chaining planner.Annals Mathematics Artificial Intelligence.Lagoudakis, M. G., & Parr, R. (2003). Least-squares policy iteration. Journal Machine LearningResearch, 4, 2003.Leaute, T. (2005). Coordinating agile systems model-based execution temporal plans.Masters thesis, Massachusetts Institute Technology.Leaute, T., & Williams, B. C. (2005). Coordinating agile systems model-based execution temporal plans. Proceedings Twentieth National Conference ArtificialIntelligence (AAAI).Li, H., & Williams, B. C. (2005). Generalized conflict learning hybrid discrete linear optimization. Proc. 11th International Conf. Principles Practice Constraint Programming.Li, H. X. (2010). Kongming: Generative Planner Hybrid Systems Temporally ExtendedGoals. Ph.D. thesis, Massachusetts Institute Technology.Matsumoto, S., Dubowsky, S., Jacobsen, S., & Ohkami, Y. (2003). Fly-by approach guidanceuncontrolled rotating satellite capture. Proceedings AIAA Guidance, Navigation,Control Conference Exhibit.Nemirovski, A., & Shapiro, A. (2006). Convex approximations chance constrained programs.SIAM Journal Optimization, 17, 969996.576fiP ROBABILISTIC P LANNING C ONTINUOUS DYNAMIC YSTEMS B OUNDED R ISKOldewurtel, F., Jones, C. N., & Morari, M. (2008). tractable approximation chance constrainedstochastic MPC based affine disturbance feedback. Proceedings Conference Decision Control.Ono, M. (2012). Closed-loop chance-constrained MPC probabilistic resolvability. Proceedings IEEE Conference Decision Control.Ono, M., Graybill, W., & Williams, B. C. (2012). Risk-sensitive plan execution connected sustainable home:. Proceedings 4th ACM Workshop Embedded Systems (BuildSys).Ono, M., & Williams, B. C. (2008a). efficient motion planning algorithm stochastic dynamicsystems constraints probability failure. Proceedings Twenty-Third AAAIConference Artificial Intelligence (AAAI-08).Ono, M., & Williams, B. C. (2008b). Iterative risk allocation: new approach robust modelpredictive control joint chance constraint. Proceedings 47th IEEE ConferenceDecision Control.Prekopa, A. (1999). use discrete moment bounds probabilistic constrained stochasticprogramming models. Annals Operations Research, 85, 2138.Richards, A., & How, J. (2006). Robust stable model predictive control constraint tightening.American Control Conference, 2006, p. 6 pp.Sanner, S. (2011). Relational dynamic influence diagram language (RDDL): Language description.Available http://users.cecs.anu.edu.au/ssanner/IPPC_2011/RDDL.pdf.Schaub, H., & Junkins, J. L. (2003). Analytical mechanics space systems. American InstituteAeronautics Astronautics, Inc.Shields, J., Sirlin, S., & Wette, M. (2002). Metrology sensor characterization pointing controlformation interferometer testbed (fit). Proceedings IEEE Aerospace Conference.Smith, R., & Hadaegh, F. (2007). Distributed estimation, communication control deep spaceformations. IET Control Theory Applications.Stoorvogel, A. (1992). H Control Problem: State Space Approach. Prentice Hall.Vallado, D. A. (2001). Fundamentals Astrodynamics Applications, Second Edition. Microcosm Press.van Hessem, D. H. (2004). Stochastic inequality constrained closed-loop model predictive controlapplication chemical process operation. Ph.D. thesis, Delft University Technology.Wang, X., Yadav, V., & Balakrishnan, S. N. (2007). Cooperative uav formation flying obstacle/collision avoidance. IEEE Transactions Control Systems Technology, 15(4).Wertz, J. R., & Wiley J. Larson, e. (1999). Space Mission Analysis Design (Third Edition).Microcosm/Springer.Younes, H. L. S., & Littman, M. L. (2004). PPDDL1.0: extension pddl expressing planningdomains probabilistic effects. Tech. rep., Carnegie Mellon University.577fiJournal Artificial Intelligence Research 46 (2013) 1 45Submitted 07/12; published 01/13Short Long Supports Constraint PropagationPeter NightingaleIan P. GentChristopher JeffersonIan Miguelpwn1@st-andrews.ac.ukian.gent@st-andrews.ac.ukcaj21@st-andrews.ac.ukijm@st-andrews.ac.ukSchool Computer Science, University St Andrews,St Andrews, Fife KY16 9SX, UKAbstractSpecial-purpose constraint propagation algorithms frequently make implicit use shortsupports examining subset variables, infer support (a justificationvariable-value pair may still form part assignment satisfies constraint)variables values save substantial work short supportsstudied right. two main contributions paper identification short supports important constraint propagation, introductionHaggisGAC, efficient effective general purpose propagation algorithm exploiting short supports. Given complexity HaggisGAC, present optimisedversion simpler algorithm ShortGAC. Although experiments demonstrate efficiency ShortGAC compared general-purpose propagation algorithmscompact set short supports available, show theoretically experimentallyHaggisGAC even better. also find HaggisGAC performs betterGAC-Schema full-length supports. also introduce variant algorithm HaggisGACStable, adapted avoid work backtracking cases fastersignificant reductions memory use. proposed algorithms excellentpropagating disjunctions constraints. experiments disjunctions foundalgorithms faster Constructive GAC-Schema least ordermagnitude, three orders magnitude.1. IntroductionConstraint solvers typically employ systematic backtracking search, interleaving choiceassignment decision variable propagation constraints determineconsequences assignment made. Propagation algorithms broadly dividedtwo types. first specialised reason efficiently constraint patternsoccur frequently models. Examples include global cardinality constraint (Regin,1996) element constraint (Gent, Jefferson, & Miguel, 2006b). feasiblesupport every possible constraint expression specialised propagator way,case general-purpose constraint propagators, GAC-Schema (Bessiere & Regin,1997), GAC2001/3.1 (Bessiere, Regin, Yap, & Zhang, 2005), STR2 (Lecoutre, 2011)MDDC (Cheng & Yap, 2010) used. typically expensive specialisedpropagators important tool specialised propagator available.support constraint domain value variable justification valuemay still form part assignment satisfies constraint. usually given termsset literals: variable-value pairs corresponding possible assignmentsc2013AI Access Foundation. rights reserved.fiNightingale, Gent, Jefferson, & Miguelvariables constraint. One efficiencies typically found specialised propagatorsuse short supports: examining subset variables, infer supportvariables values save substantial work. use typically implicit,i.e. achieved specialised algorithm examine variablescases. One contributions highlight general importance short supports.example, consider element constraint xy = z, x0 , x1 , x2 , {0 . . . 2},z {0 . . . 3}. constraint satisfied iff element position vector [x0 , x1 , x2 ]equals z. Consider set literals = {x0 7 1, 7 0, z 7 1}. set clearly satisfiesdefinition constraint xy = z, contain literal variable.extension valid literals variables x1 x2 support. exampleshort support.previous work introduced ShortGAC (Nightingale, Gent, Jefferson, & Miguel,2011), general-purpose propagation algorithm exploits short supports. introduction ShortGAC, general-purpose propagators relied upon supports involvingvariables. paper develop concept introduce new algorithmHaggisGAC,1 consistently efficient ShortGAC. available,use compact sets short supports allows HaggisGAC outperform greatly existing general-purpose propagation algorithms. cases, HaggisGAC even approachesperformance special-purpose propagators. HaggisGAC also well suitedpropagating disjunctions constraints, outperforms traditional Constructivealgorithm (Lagerkvist & Schulte, 2009; Wurtz & Muller, 1996) orders magnitude.HaggisGAC also efficient GAC-Schema full-length supports. alsodescribe variant, HaggisGAC-Stable, supports need deletedbacktracking. Applied full-length supports, version greatly reduced memoryusage.ShortGAC, HaggisGAC HaggisGAC-Stable instantiated function named findNewSupport (and similar GAC-Schema way). functionspecific constraint, generate short supports procedurally. Alternatively,generic findNewSupport retrieve short supports data structure.Section 2 presents necessary background, Section 3 introduces conceptshort support. Section 4 outlines basic idea used deal implicit supportsthroughout paper. Section 5 gives full details ShortGAC, including complexitykey operations alternative implementations short supports provided listform. Section 6 presents new algorithm HaggisGAC development ShortGAC.ShortGAC HaggisGAC evaluated experimentally Section 7. Section 8describes HaggisGAC-Stable, corresponding experiments Section 9. Finally,Sections 10 11 discuss related work present conclusions.1. HaggisGAC named legendary wild haggis Scotland, short legs longlegs walking around hills. Like namesake, HaggisGAC copes full-length shortersupports originates Scotland. Details wild haggis found Wikipedia, http://en.wikipedia.org/wiki/Wild_haggis, Veterinary Record (King, Cromarty, Paterson, &Boyd, 2007).2fiShort Long Supports Constraint Propagation2. Supports, GAC, Triggersconstraint satisfaction problem (CSP) defined set variables X, functionmaps variable domain, : X 2Z domain finite set, setconstraints C. constraint c C relation subset variables X.scope constraint c, named scope(c), set variables c constrains.solution CSP function : X Z maps variable x X valueD(x), every constraint c C, values scope(c) form tuplec (i.e. constraint satisfied ).systematic search solution CSP, values progressively removeddomains D. Therefore, distinguish initial domains currentdomains. function refers current domains unless stated otherwise. literaldefined variable-value pair, written x 7 v. literal x 7 v valid vcurrent domain x (i.e. v D(x)).Definition 2.1. [Support] support constraint c domains definedset valid literals contains exactly one valid literal variable scope(c)satisfies c. necessary disambiguation, call support full-lengthsupport simply long support, contrast short supports defined later.property commonly established constraint propagation algorithms generalisedarc consistency (GAC) (Mackworth, 1977). constraint c GAC existsfull-length support every valid literal every variable scope(c). GAC establishedidentifying literals x 7 v full-length support exists removing vdomain x. consider algorithms establishing GAC paper.GAC propagation algorithm usually situated systematic search. Hence,must operate three contexts: initialisation (at root node), support establishedscratch; following deletion one domain values (as result branchingdecision and/or propagation constraints), support must re-establishedselectively; upon backtracking, data structures must restored correctstate point search. primary focus second context, operationfollowing value deletion, although discuss efficient backtracking Section 8.GAC propagation algorithm would typically called deleted domain valueturn. algorithm called domain value, constraintGAC.propagation algorithms present concept active support, inspiredGAC-Schema (Bessiere & Regin, 1997). active support support currentlyuse support set literals. literal set active supports supportit. active support found invalid, removed. setliteral empty, say literal lost support. new support sought literal,found new support becomes active. new support found, literalsupport deleted.propagation algorithms present, efficiency make use watched literalsprovided Minion (Gent et al., 2006b), propagators need called everydeleted domain value establish GAC. say propagators attach remove triggersliterals. domain value v variable x deleted, propagator called3fiNightingale, Gent, Jefferson, & Migueltrigger attached literal x 7 v. means literaldeleted attached trigger, zero work incurred. emphasiseuse watched literals fundamental work. availablegiven solver, algorithms need minor adaptation. called literalremoval, may return immediately literal active support,checked time O(1). Thus algorithms fit traditional fine-grained scheme(Bessiere & Regin, 1997) except cases invokeduse watched literals.3. Short Supportsconcept short support generalisation full-length support. defined below.Definition 3.1. [Short support] short support constraint c domainsdefined set valid literals x 7 v x scope(c), x occurs S,every superset contains one valid literal variable scope(c)full-length support. strict short support short support full-lengthsupport.definition short support includes extremes. empty set short support constraint entailed (i.e. every tuple scope(c) within satisfiesconstraint). Similarly, every full-length support necessarily short support,superset itself. case studies see examples empty shortsupports short supports also happen full length.Short supports used maintain GAC. full-length support,short support provides GAC support literal contained within it. callexplicit support literals. new feature short support also providessupport valid literals variables contained short support.because, definition, every valid extension short support cover variablesscope(c) full-length support. say short support gives implicit GAC supportvalid literals variables short support.also define concept complete set short supports constraint.Definition 3.2. [Short support set] short support set S(c, D) set short supportsconstraint c domains D, every full-length support c(not necessarily strict) superset least one short support 0 S(c, D).constraint may many short support sets. gives us latitude implement one efficient compute.natural ask identify correct short supports given constraint c.simple fundamental result given Lemma 3.3.Lemma 3.3. Given constraint c domains D, empty set {} short supportc iff GAC propagation constraint not(c) leads empty domain.Proof. {} short support every valid assignment variables scope(c)satisfies c. Every assignment satisfies c iff every assignment violates not(c). every assignment violates not(c), GAC propagation constraint not(c) leads empty4fiShort Long Supports Constraint Propagationdomain. complete last equivalence, note assignment violatenot(c), literals assignment supported, GAC propagation cannot causeempty domain.lemma two important consequences. First, check short supportcorrectness, empty support. check short support = {x1 7 v1 , . . . , xk 7vk }, simply set D(x1 ) = {v1 }, . . . , D(xk ) = {vk }. assignments extend S,short support iff {} is. Lemma 3.3 applies check correctnesspropagating not(c) seeing domain emptied.second consequence negative, however. Determining whether GAC propagationempty domain polynomially equivalent actually performing GAC propagation(Bessiere, Hebrard, Hnich, & Walsh, 2007). Since constraints NP hard GACpropagate, follows easy even check empty set short support.Thus cannot expect find method fast general finding shortsupports constraint.Given provable difficulty finding short supports set full-length supports,construct sets short supports specifically three experimental case studiesSection 7. focus paper show value strict short supportsgiven system. situation analogous important areaconstraints, namely exploiting symmetries constraint problems (Gent, Petrie,& Puget, 2006). large majority research assumed sets symmetriesprovided system, even though finding sets hard. inhibitedresearch exploiting symmetry, within automated detection symmetrybecome important subarea (Mears, 2009; Puget, 2005): however leave automatedconstruction compact short support sets future research. Analogously patternsmatrix symmetries (Flener, Frisch, Hnich, Kiziltan, Miguel, Pearson, & Walsh, 2002),least identify pattern often lets us identify strict short supports,describe.3.1 Short Supports DisjunctionStrict short supports arise naturally disjunctions. constraint expresseddisjunction shorter constraints, set strict short supports constructedit. Suppose following constraint.c(x1 , x2 , x3 , x4 ) c1 (x1 , x2 ) c2 (x2 , x3 ) c3 (c3 , x4 )Suppose also = {x1 7 2, x2 7 1} valid assignment satisfies c1 . satisfyc1 , satisfy c regardless values x3 x4 . Therefore = {x1 7 2, x2 7 1}strict short support c.Lemma 3.4. Given constraint c, domain set D, set constraints {c1 . . . ck }ci {c1 . . . ck } : scope(ci ) scope(c) c c1 ck , following short supportset (where write fls(ci , D) mean full-length supports ci w.r.t. domains D):S(c, D) = {S | fls(c1 , D) fls(ck , D)}5fiNightingale, Gent, Jefferson, & MiguelProof. (a) element S(c, D) short support according Definition 3.1semantics disjunction. (b) S(c, D) short support set Definition 3.2. Every fulllength support c must satisfy disjunct ci , therefore full-length support containsfull-length support ci included S(c, D).Lemma 3.4 allows short support set created disjunction, given initialdomains. two three case studies (for third, set prohibitivelylarge).Using similar approach Lemma 3.4 create function generates shortsupports demand. function takes valid literal x 7 v current domainsD, returns short support supports x 7 v (explicitly implicitly), Nullnone exists. function constructed follows. create new domains D0D0 (x) = {v}, otherwise D0 identical D. disjunct satisfiable D0 ,function returns Null. Otherwise, function picks disjunct ck satisfiableD0 , returns satisfying assignment ck valid D0 .three case studies Section 7, created function follows schemeoptimisations.Propagating disjunctions recognised important topic. Many paperspublished area (Wurtz & Muller, 1996; Lhomme, 2003; Lagerkvist & Schulte,2009; Jefferson, Moore, Nightingale, & Petrie, 2010). Exploiting strict short supportsalgorithms ShortGAC, HaggisGAC HaggisGAC-Stable allows us outperformtraditional Constructive algorithm (Wurtz & Muller, 1996) orders magnitude.3.2 Backtrack Stability Short SupportsWithin search tree, propagation algorithms often spend significant time backtrackingdata structures. Reducing eliminating backtracking improve efficiency. example,avoiding backtracking triggers speed simple table propagator 2 times(Gent et al., 2006b), MAC-6 MAC-7 much efficient (in spacetime) backtracking avoided (Regin, 2005). two potential advantagesreducing use backtracking state: saves time restoring data structures, savesspace avoiding storing supports backtrack stack.Definition 3.5. [Backtrack Stable] short support constraint c current domainsbacktrack stable iff always remains short support (according Definition 3.1)backtracking search tree.short support may support variable x implicitly, backtrack mayadd values back domain x consistent s, meaninglonger meets definition short support. give example below.Example 3.1. Consider constraint b [x] = y, boolean variable b, arrayvariables variables x y. b assigned False, constraint entailed,empty short support used support literals M, x y.support backtrack stable, backtracking True restored domainb, empty set longer short support.6fiShort Long Supports Constraint Propagationsupport full length backtrack stable: whenever support validsupports literals contains. Backtrack stable supports always exist usefull-length supports cases (as GAC-Schema), although may much longernecessary.Section 8 exploit backtrack stability define new algorithm.4. ShortGAC: Overviewsection summarises key ideas ShortGAC propagation algorithm, alongillustrative example.2ShortGAC maintains set short supports sufficient support valid literalsvariables scope constraint propagating. refer activesupports. algorithm rests exploiting observation that, using short supports,support established literal two ways. First, usual, short supportcontains literal supports literal. Second, literal x 7 v supported shortsupport contains literal variable x. Hence, short supportssupport x 7 v contain literal x 7 w value w 6= v.following data structures central operation ShortGAC algorithm:numSupports total number active short supports.supportsPerVar array (indexed [x]) indicating number active short supportscontaining variable x.supportListPerLit array (indexed [x 7 v]) lists active short supports containing literal x 7 v.number supports containing variable x less total numbersupports exists support contain x. Therefore, supportsliterals x. algorithm spends time processing variables whose literalsknown supported way. variables involved active supportsseek support literals active supports.illustrate, consider element example introduction: xy = z,x0 , x1 , x2 , {0 . . . 2}, z {0 . . . 3}. constraint satisfied iff elementposition vector [x0 , x1 , x2 ] equals z. Suppose current state ShortGAC storingone support: = {x0 7 1, 7 0, z 7 1}. data structures follows,indicates literal valid.32. details present different presented previously (Nightingale et al., 2011),optimised data structures algorithms compared previous work. twosignificant changes are: longer keep count supports per literal, saving overhead maintainingthis; data stored one dimensional vector literal, instead two dimensional arrayvariable/value, saving space variables constraint different domain sizes. ExperimentsAppendix demonstrate algorithms data structures presented perform betterprevious implementation.3. clarity, presented one-dimensional array supportListPerLit two-dimensional format.7fiNightingale, Gent, Jefferson, & MiguelSupports:supportListPerLit:Value0123supportsPerVar:numSupports:x07 1, 7 0, zVariablex1 x2{} {} {A}{} {}{}{} {}{}0011A:x0{}{A}{}17 1z{}{A}{}{}1values x1 x2 support, since supportsPerVar countersless numSupports. Therefore ShortGAC algorithm ignore x1 x2look new supports x0 , z. Consider finding new support literalsz. ShortGAC ignore literals least one support case z 7 1.algorithm looks literals z 7 supportListPerLit[z, a] = {}. Here, z 7 0literal, ShortGAC seeks new support it. possible new supportB = {x1 7 0, 7 1, z 7 0}. Following discovery, update data structures:Supports:supportListPerLit:Value0123supportsPerVar:numSupports:A:B:x0{}{A}{}1x07 1, 7 0, zx17 0, 7 1, zVariablex1x2{B} {} {A}{}{} {B}{}{}{}10227 17 0z{B}{A}{}{}2variable x0 also fully supported, since supportsPerVar[x0 ] < numSupports.remain three literals support established: 7 2, z 7 2 z 7 3.first two ShortGAC finds supports C = {x0 7 2, 7 0, z 7 2}= {x2 7 0, 7 2, z 7 0}. support exists z 7 3, 3 deleted, giving:Supports:supportListPerLit:Value0123supportsPerVar:numSupports:A:B:C:D:x0{}{A}{C}27 1, 7 0, z7 0, 7 1, z7 2, 7 0, z7 0, 7 2, zVariablex2{D} {A, C}{}{B}{}{D}144x0x1x0x2x1{B}{}{}17 17 07 27 0z{B, D}{A}{C}4valid literals supported. Nothing need done changestate, removal value branching decision propagation.8fiShort Long Supports Constraint Propagation5. ShortGAC: Detailskey tasks implementing ShortGAC are: data structure update; iterationvariables supportsPerVar equals numSupports; iteration unsupportedvalues variable. section describes infrastructure allows us performtasks efficiently.5.1 ShortGAC Data Structuresactive short support arity k provides explicit support k literalscontains. Therefore, reference must appear k lists supportListPerLit.this, represent two types object: ShortSupport ShortSupportCell.ShortSupport object contains k ShortSupportCell objects, contains literalx 7 v,4 reference parent ShortSupport. elements array supportListPerLit doubly-linked lists ShortSupportCells. reference parentShortSupport, iterate active short supports given literal.algorithm iterates variables x supportsPerVar[x] equals numSupports.following data structure represents partition variables number supports. allows constant time size checking linear-time iteration cellpartition, allows variable moved adjacent cell (i.e. numbersupports increases decreases 1) constant time. inspired indexeddependency array Gecode (Schulte & Tack, 2010).varsBySupport array containing permutation variables. Variables orderednon-decreasing number active supports (supportsPerVar[x]).supportNumLowIdx array integers, indexed 0 number literals,maximum number active supports possible. Either supportNumLowIdx[i]smallest index varsBySupport active supports, (whenvariables) supportNumLowIdx[i]= k k total number variables.k acts sentinel value. set variables supports is:varsBySupport[supportNumLowIdx[i] . . . supportNumLowIdx[i + 1] 1]Initially, variables 0 active supports, supportNumLowIdx[0] = 0 restarray set k.following table illustrates partition data structure works (on differentexample 11 variables). Suppose supportsPerVar[x2 ] changed 7 6. x2 y1(boxed) swapped varsBySupport cell boundary moved x2lower cell. Consequently, supportNumLowIdx[7] incremented 1.varsBySupport[]supportsPerVarx2 updatedw16w1w26w2y17x2x17x1x27y1y27y2y37y3x37x3z18z1z28z2z38z34. literal x 7 v represented using single integer i. mapping x 7 v i,allows O(1) access x v vice-versa.9fiNightingale, Gent, Jefferson, & MiguelRequire: sup: ShortSupport1: sc: ShortSupportCell sup2:(x 7 v) sc.literal3:supportListPerLit[x 7 v] = {}4:attachTrigger(x 7 v)5:Add sc doubly linked list supportListPerLit[x 7 v]6:supportsPerVar[x]++7:sx supportsPerVar[x]8:cellend supportNumLowIdx[sx ]19:swap(x, varsBySupport[cellend ])10:supportNumLowIdx[sx ]-11: numSupports++Procedure 1: addSupport(sup)variable x supportsPerVar[x] = numSupports, ShortGAC iteratesvalues zero explicit supports. avoid iterating values, use set datastructure:zeroLits array (indexed [x]) stacks containing literals variable x zeroexplicit support, particular order.inZeroLits array (indexed [x 7 v]) booleans indicating whether literal x 7v zeroLits[x].supportListPerLit[x 7 v] reduced empty list, inZeroLits[x 7 v]false x 7 v pushed onto zeroLits[x] (and inZeroLits[x 7 v] set true).optimisation, values eagerly removed set; removed lazilyset iterated. Also, set backtracked. iteration, non-zero valueremoved swapping top stack, popping. lazy maintenance nevercosts work overall because, value would removed eagerly,removed next time set iterated, costing O(1). save work, maynever iterate list value would restored set again.use free list manage set ShortSupport objects avoid cost unnecessary object construction/destruction. ShortSupport object retrieved free listmay contain ShortSupportCell objects, use resizable vector data structure.size ever increased.5.2 Adding Deleting Supportssupport added deleted, data structures described mustupdated. done Procedures 1 (addSupport) 2 (deleteSupport).procedures iterate given short support, literal updatesupportListPerLit, supportsPerVar, varsBySupport supportNumLowIdx. Procedure 2 alsoinserts literal zeroLits necessary. briefly explain maintenance varsBySupport become important Section 6.2. Suppose adding supportliteral x 7 v Procedure 1. additional support, x must movednext cell varsBySupport. Line 8 finds end cell x in, swap x10fiShort Long Supports Constraint PropagationRequire: sup: ShortSupport1: sc: ShortSupportCell sup2:(x 7 v) sc.literal3:Remove sc doubly-linked list supportListPerLit[x 7 v]4:supportsPerVar[x]-5:supportListPerLit[x 7 v] = {}6:removeTrigger(x 7 v)7:inZeroLits[x 7 v]8:inZeroLits[x 7 v] true9:zeroLits[x].push(x 7 v)10:sx supportsPerVar[x]11:cellend supportNumLowIdx[sx +1]12:swap(x, varsBySupport[cellend ])13:supportNumLowIdx[sx +1]++14: numSupports--Procedure 2: deleteSupport(sup)Require: x 67 v (where v pruned domain x)1: supportListPerLit[x 7 v] 6= {}2:deleteSupport(supportListPerLit[x 7 v].pop())3: repeat4:continueLoop false5:{supportNumLowIdx[numSupports]. . . supportNumLowIdx[numSupports+1]-1}6:varsBySupport[i]7:ShortGAC-variableUpdate(y) = true8:continueLoop true9:break loop Line 510: continueLoop = falseProcedure 3: ShortGAC-Propagate: propagate(x 67 v)end cell using subroutine swap(xi , xj ). simple procedure (not given) locatesswaps two variables varsBySupport, leaving variables unaffected.makes use second array, varsBySupInv, inverse mapping varsBySupport.done this, cell boundary decremented (in new position), xhigher cell. Another point note addSupport add trigger x 7 vsup active explicit support contain literal, deleteSupportremove trigger deleted support support.Finally, note special-purpose methods undo changesbacktracking. backtracking past point support added, simplycall deleteSupport, similarly call addSupport backtrack past supportsdeletion.11fiNightingale, Gent, Jefferson, & MiguelRequire: variable x1: (x 7 v) zeroLits[x]2:supportListPerLit[x 7 v] 6= {}3:Remove (x 7 v) zeroLits[x]4:else5:v D(x)6:sup findNewSupport(x 7 v)7:sup = Null8:prune(x 7 v)9:else10:addSupport(sup)11:supportListPerLit[x 7 v] 6= {}12:Remove (x 7 v) zeroLits[x]13:return true14: return falseProcedure 4: ShortGAC-variableUpdate: (x). pseudocode abstractdetailed maintenance zeroLits inZeroLits data structures. might seemtest Line 11 must always succeed. However, although sup must support x 7 v,contain x 7 v might implicit support. findNewSupportfunction discussed Section 5.5.5.3 Propagation AlgorithmShortGAC propagator (Procedure 3) invoked literal containedone active short supports pruned.5 first deletes supports involvingpruned literal. checks variables implicitly supported, i.e.supportsPerVar[y]=numSupports (Line 5). variable checked Procedure 4(ShortGAC-variableUpdate, described below). call results new supportfound, data structures changed (ShortGAC-variableUpdate(y) returnstrue indicate this) must break for-all-loop (Line 9) go round again.Iteration therefore continues either new support necessary new supportfound.ShortGAC-variableUpdate (Procedure 4) used check status every variablelacking implicit support. iterates zeroLits, i.e. literals variable mightzero explicit supports. Since zeroLits maintained lazily, iteration firstcheck literal indeed explicit support, correct zeroLits necessary(Lines 23). important case literal indeed support. Then, providedv current domain x, must seek new support calling findNewSupportconstraint. support, value v must pruned domain x,found support update data structures calling addSupport.initialise data structures root search, Lines 310 Procedure 3 invoked.Notice lines refer parameter x 67 v, first callingsupports initial iteration Line 5 variables.5. noted earlier, watched literals available solver, simple check madestart procedure, return immediately removed literal active support.12fiShort Long Supports Constraint Propagation5.4 Complexity Analysis ShortGACsection provide complexity analysis ShortGAC used incrementally search constraint solver. analysis parameters arityconstraint n, maximum domain size d, cost f calling findNewSupport.assume attaching removing trigger literal O(1). caseMinion 0.12.First observe swap procedure executes O(1) time: operation swapO(1) loop. Secondly establish time complexity proceduresaddSupport deleteSupport, key algorithm.Lemma 5.1. Procedure 1 (addSupport) time complexity O(n).Proof. outer loop Line 1 iterates literals short support. worstcase, n literals. consider steps within loop. list testLine 3 O(1), call attachTrigger Line 4. Adding ShortSupportCelldoubly-linked list Line 5 O(1), following five array dereferences.established above, swap procedure also O(1). Hence, addSupport O(n).Lemma 5.2. Procedure 2 (deleteSupport) time complexity O(n).Proof. Similarly add Support procedure, outer loop Line 1 niterations. removal doubly-linked list Line 3 O(1), arraydereferences Line 4 subsequently. list test Line 5 call removeTrigger Line 6 O(1), stack push operation Line 9. Recallingswap procedure O(1), deleteSupport O(n).Theorem 5.3. Procedure 3 (ShortGAC-propagate) time complexity O(n2 d2 +ndf ).upper bound obtained, i.e. worst case time complexity (n2 d2 + ndf ).Proof. Analysis first statement breaks three parts.First, loop Line 1 elements supportListPerLit. worst case occursnd literals explicit support. supports, maximum (n 1)d + 1involve particular literal, literal may short support everyliteral every variable ((n 1)d), (1). cost body loopO(n) Lemma 5.2, total O(n2 d). dominated next part.second part loop lines 310. maximum number iterationsLine 5 n supports full length iteration Line 5 contains nvariables. Successive calls Procedure 4 Line 7 add O(d) new supports.support addition triggers restart loop beginning Line 5 nvariables, total O(n2 d) calls Procedure 4. call involves O(d)iterations loop Line 1 Procedure 4. Therefore innermost loop runO(n2 d2 ) times.complete proof first statement, consider cost innermost loopProcedure 4. Within loop, operations O(1), exceptions callfindNewSupport Line 6 (cost f ) call addSupport Line 10 (cost nLemma 5.1). f dominating cost, since must least traverse new supportrecord it. However, n2 d2 iterations, nd calls findNewSupport,13fiNightingale, Gent, Jefferson, & Migueltime valid literals explicit support. cost either O(n2 d2 )O(ndf ), whichever greater. case cost O(n2 d2 + ndf ).upper bounds ndf n2 d2 attained worst case. literalneeds new support, (ndf ) calls findNewSupport. cost (n2 d2 )nd literals explicit support, size n, variable ends(for example) d/2 values supported d/2 values deleted. worst case thus(n2 d2 + ndf ).Procedure 3 invoked n(d 1) times one branch search tree,therefore complexity one branch O(n3 d3 + n2 d2 f ).5.4.1 Second Complexity Analysisanalysis conservative total number, maximum size,short supports small. Therefore, give another complexity analysis two additionalparameters: maximum length l short supports returned findNewSupport,total number distinct short supports may returned findNewSupport.analysis also pertains branch search rather single call propagatealgorithm.first part complexity analysis concerns short supports length l.short support may added active set once, may deletedbranch. short support must also found calling findNewSupport, cost O(f ).Lemma 5.1 shows addSupport procedure takes O(n) time. lemmare-stated terms l, loop addSupport iterate O(l) times, givingtotal time O(l). also applies deleteSupport. Since short supports,cost finding, adding deleting (collectively processing) short supports O(s(l + f ))branch.Secondly, algorithm may make calls findNewSupport return Null.happen n(d 1) + 1 times, maximum number domain valuesmay deleted. Therefore cost O(ndf ).addition, ShortGAC operations charged eithercategories. analyse these, must top-down analysis algorithm.Procedure 3 invoked O(s) times (each time short support invalidated). Lines 12already charged processing short supports. body loop lines 310 mayexecuted times new support found, times newsupport found, therefore O(s) times total branch search.come inner loop lines 59. Lemma 5.4 (below), unless domainempty always one active short support. Therefore, l variablescontained active short supports, l variables relevantpartition varsBySupport, loop body executed O(l) times.Lemma 5.4. initialisation, Procedure 3 always least one active short supportvariable domain empty.Proof. Suppose opposite. algorithm invoked time literal active shortsupport pruned, therefore delete active short supports must contain oneliteral x 7 v. active short supports contain variable x, values domain14fiShort Long Supports Constraint Propagationx implicitly supported must explicitly supported. Therefore v mustlast remaining value D(x). prune x 7 v empties domaincontradiction.branch, causes O(sl) calls ShortGAC-variableUpdate, Line 7.call ShortGAC-variableUpdate takes O(d) time may 1 invalidliterals explicitly supported literals zeroLits. time spent procedurecharged processing short supports, pruning domains. Therefore top-downanalysis cost O(sld).Overall, time complexity O(s(l + f ) + ndf + sld), tighter bound casesone given section above. example, SAT clause = n, f = n, l = 1= 2, giving time complexity O(n2 ) branch search.5.5 Instantiation findNewSupportSimilarly GAC-Schema (Bessiere & Regin, 1997), ShortGAC must instantiatedfindNewSupport function. function takes valid literal, returns support oneexists, otherwise returns Null. One way write specialist findNewSupportfunction constraint. empirical case studies below.case, findNewSupport function much simpler propagatorconstraint. use Lemma 3.4 build findNewSupport functions, reducestask finding satisfying tuples simple constraints like x < x = y.alternative write generic version findNewSupport caseshort supports given list. detail two generic instantiations findNewSupport lists, case studies compare specialist functions.5.5.1 findNewSupport-Listprovide generic instantiation named findNewSupport-List (Procedure 5) takeslist short supports literal (supportList), including explicit implicitshort supports literal. analogous Positive instantiation GACSchema (Bessiere & Regin, 1997). FindNewSupport-List persistent state: listPos,array integers indexed variable value, initially 0. indicates currentposition supportList. algorithm simply iterates list supports,seeking one literals valid. ListPos backtracked, consequenceend list reached, cannot fail immediately must searchstart back listPos. branch search tree, particular elementlist may looked once. However, algorithm optimaltime space across search tree (Gent, 2012). surprising result achievedamortizing cost across branches. Practically, using listPos stops algorithm alwaysstarting first element list, seems good tradeoff avoidingprovably unnecessary work much data structure maintenance.constraint-specific findNewSupport sometimes find shorter supports findNewSupport-List. specific findNewSupport take advantage currentdomains whereas supportList may contain supports given initial domains.example, constraint becomes entailed, specific findNewSupport return15fiNightingale, Gent, Jefferson, & MiguelRequire: x, v, supportList1: j {listPos[x, v]. . .(supportList[x, v].size-1)}2:sup supportList[x, v, j]3:literals sup valid4:listPos[x, v] j5:return sup6: j {0 . . .listPos[x, v]1}7:sup supportList[x, v, j]8:literals sup valid9:listPos[x, v] j10:return sup11: return NullProcedure 5: findNewSupport-List: findNewSupport(x, v). first block searcheslocation previous support end support list. unsuccessfulsearch restarts start list second block. circular approach removesneed backtrack listPos.empty support whereas list version presented cannot. exploit factCase Study 3 below.5.5.2 findNewSupport-NDListlist instantiation two major disadvantages. First, inefficientunable skip sets invalid tuples. literature contains many solutionsproblem context full-length supports, example binary search (Lecoutre &Szymanek, 2006) tries (Gent, Jefferson, Miguel, & Nightingale, 2007). Second,require large amount memory. short support S, potentially ndpointers S, pointer literal implicitly supports.section give second generic list instantiation based NextDifference lists(Gent et al., 2007). single list (named supportList) containing short supports (indexed integer), second list named NDList support=supportList[j], literal support s[k], NDList[j][k] index nextsupport contain literal s[k]. Thus, searching list, algorithmable jump sets short supports contain invalid literal.version findNewSupport NextDifference lists given Procedure 6.approach solves problems list instantiation: able jumpsets invalid short supports, usually requires substantially less memory. factoptimal space (unlike list instantiation): given short supports lengthl, NextDifference list O(tl). However uses one list supports, thereforespend time searching short supports support desired literal.5.6 Literals Assigned VariablesSuppose ShortGAC discovers new support contains literal x 7 v, x assigned v. Since x take value v, sound remove x 7 vsave overhead adding it. apply minor optimisation cases usingShortGAC, also cases using HaggisGAC (described Section 6). How16fiShort Long Supports Constraint PropagationRequire: x, v, supportList, NDList1: j listPos[x, v]2: j < supportList.size3:sup supportList[j]4:nextDiff NDList[j]5:k {0 . . . sup.size 1}6:(y 7 b) sup[k]7:b/ D(y) (x = v 6= b)8:j nextDiff [k] {Jump next short support assigned different value.}9:continue loop Line 210:listPos[x, v] j11:return sup12: j 013: j < listPos[x, v]14:sup supportList[j]15:nextDiff NDList[j]16:k {0 . . . sup.size 1}17:(y 7 b) sup[k]18:b/ D(y) (x = v 6= b)19:j nextDiff [k] {Jump next short support assigned different value.}20:continue loop Line 1321:listPos[x, v] j22:return sup23: return NullProcedure 6: findNewSupport-NDlist: findNewSupport(x, v)ever optimisation cannot used HaggisGAC-Stable (described Section 8)algorithm retains active supports backtracks, backtrackingliteral x 7 v may longer assigned.6. HaggisGAC: Dealing Full-Length Strict Short Supportsintroduce HaggisGAC. show better theoretical propertiesShortGAC. Furthermore, experiments show runs substantially faster many casesstrict short supports ShortGAC (which specialised strict short supports),substantially faster full-length supports GAC-Schema.6.1 Introduction Motivating ExampleShortGAC designed exploit concept implicit support, inefficiencies dealing explicit supports especially full-length supports. Considerexample constraint AllDifferentExceptZero, constraint non-zerovalues array must different, zero may occur freely. constraint mightused, example, timetabling problem classes taking place different roomsmust different, use zero represent room unused occurmultiple times. Suppose AllDifferentExceptZero([w, x, y, z]), variable initial domain {0, 1, 2, 3}. Supports constraint full-length supports every17fiNightingale, Gent, Jefferson, & Miguelnon-zero value different, three variables equalling zero last variable maytake value. Suppose execute ShortGAC reach following situation:Supports:supportListPerLit:Value0123supportsPerVar:numSupports:A:B:C:D:E:w{A, B, E}{}{}{C}4w7 0, x 7 2, 7 3, z 7 1w7 0, x 7 3, 7 2, z 7 1w7 3, x 7 0, 7 1, z 7 2x 7 0, 7 0, z 7 0w 7 0, x 7 1, 7 2, z 7 3Variablexz{C, D}{D}{D}{E}{C}{A, B}{A}{B, E}{C}{B}{A}{E}5555Notice lack explicit supports w 7 1 w 7 2 acceptablesupportsPerVar[w] = 4 < numSupports = 5. suppose literal 7 0 deletedconstraint. causes support deleted, causing following state:Supports:supportListPerLit:Value0123supportsPerVar:numSupports:A:B:C:E:7 0, x 7 2,7 0, x 7 3,7 3, x 7 0,7 0, x 7 1,Variablex{C}{E}{C}{A} {B, E}{B}{A}444wwwww{A, B, E}{}{}{C}47 3, z7 2, z7 1, z7 2, z7 17 17 27 3z{}{A, B}{C}{E}4point ShortGAC iterates zeroLits lists variablessupportsPerVar = numSupports, case four variables. discover mustfind new supports w 7 1, w 7 2 z 7 0. However, inefficient two reasons.First, need check zeroLits[z] discover z 7 0, support listz 7 0 became empty deletion support D, could discovered then.Second, need look zeroLits[w] deletion causedw lose implicit support. need check zeroLits x, y, zvariables implicitly supported prior Ds deletion. Removing tworeasons inefficiency motivation behind development HaggisGAC.example, focus directly literal z 7 0 set zeroLits[w] literalspotentially needing new support.fundamental problem ShortGAC cannot efficiently detectliteral loses last support. Every variable implicit support checked every timesupport deleted, ShortGAC take O(nd) time find single literal needsnew support discover literal. improve upon this, wish18fiShort Long Supports Constraint PropagationvarsBySupport[i]supportsPerVarx2 updatedx3 updatedz2 updatedx1 updatedz3 updatedz1 updatedsupportsPerVar0w16w1w1w1w1w1w161w26w2w2w2w2w2w262y17x2x2x2x2x2x263x17x1x3x3x3x3x364x27y1y1y1x1x1x165y27y2y2y2y2y2y276y37y3y3y3y3y3y377x37x3x1x1y1y1y178z18z1z1z2z2z2z279z28z2z2z1z1z3z3710z38z3z3z3z3z1z17Figure 1: Illustration deleteSupport concentrates variables lostlast implicit support. See main text full description.HaggisGAC able detect loss literals last explicit support time O(1),loss variables last implicit support time O(1). Perhaps surprisingly,goals achievable use data structures already existing ShortGAC.6.2 Finding Literals Support Efficientlytwo types support, detecting last explicit support literal lostsimpler task. delete support, Procedure 2 iterates literalsshort support. literal removes ShortSupportCell correspondingsupportListPerLit updates data structures appropriately. list empty testedLine 5 Procedure 2 literal lost last explicit support. add literalscratch list literals lost last explicit support: describeprocess scratch list. additional cost O(1) detect empty list.inside existing test, zero additional cost literallost last support. contrasts ShortGAC tests (in Procedure 4) everyvariable implicit support, worst case cost O(n) even literallost last explicit support.subtle task detect variable (and thus literals involving it)lost last implicit support. reason difficult seekingvariables involved support deleted, Procedure 2 iterateliterals support deleted. variables seekx supportsPerVar[x] = numSupports support deletion,supportsPerVar[x] < numSupports support deletion. (VariablessupportsPerVar[x] = numSupports deletion implicit supportnow, lose implicit support deletion.) Fortunately, existingmaintenance data structures happens compact exactly variables particularregion varsBySupport, find easily efficiently. compactionhappens sequence calls Procedure swap made Procedure 2.first show worked example prove general properties need.Figure 1, suppose 11 variables constraint, currently 8supports, deleting support involving variables x1 , x2 , x3 , z1 , z2 z3 ,19fiNightingale, Gent, Jefferson, & Miguelliterals deleted arbitrary order top (start) bottom (finish).start, z variables already supportsPerVar = numSupports = 8; variables xsupportsPerVar = 7; variables w supportsPerVar = 6. process literalsdeleteSupport, pairs variables swapped (marked boxes line)boundaries move cells (marked vertical lines) variables equal supportsPerVar. end, w x variables still supportsPerVar = 6 < numSupports = 7.z variables supportsPerVar=numSupports deletion.variables lost last implicit support variables. crucial pointend lie precisely final boundary 6 7 supports(from = 5), initial boundary 7 8 supports (from = 8). followingsimple results show variables losing last implicit support always compactedsimilar way.Lemma 6.1. Suppose, delete support S, numSupports = p (and numSupports = p 1 afterwards). variable x lose last implicit support, p 1explicit supports deletion S.Proof. x initially fewer p1 explicit supports, x one implicitsupport deleting removes one these. x initially p explicit supports,involved (since involved supports) implicit supportlose. Hence, x must initially p 1 explicit supports one implicit supportmust one implicit support. Therefore deletion S, x p 1 explicitsupports implicit supports.Lemma 6.2. set p Lemma 6.1, value supportNumLowIdx[p]deleteSupport called, j value supportNumLowIdx[p 1] deleteSupportexits. deleteSupport finishes, variables lost last implicit supportcall deleteSupport exactly set variables indices range [j, i)varsBySupport.Proof. variables implicit supports deleteSupport exits lie index jgreater varsBySupport. establishes lower bound index range.variable z implicit support start call must p explicitsupports must index higher. z must support deleted,supports. z updated deleteSupport, always swappedvariable index supportNumLowIdx[p]. index supportNumLowIdx[p] increasesdeleteSupport, z stays index higher throughout. Thus variablesindex upwards finish permutation start, meaning variableslost last implicit support must range [j, i). Finally, variablerange [j, i) implicit support end call (as index j above)implicit support start (as i). Therefore variableslost last implicit support lie indices range [j, i).Lemma 6.2, run deleteSupport trivial enumerate variableslost last implicit support result. exactly variablesvarsBySupport[k] k = j, j + 1, ...i 1 j defined Lemma. Enumeratinglist additional work already done Procedure 2, have:20fiShort Long Supports Constraint PropagationCorollary 6.3. Given constraint n variables, additional work identify variableslost last implicit support O(1) variablesome, O(1) none.Proof. already argued case variables lost implicitsupport. variables, still O(1) work check rangeempty.low level complexity contrasts favourably ShortGAC. support deleted, Procedure 4 iterates variables numSupports explicit supports.worst case O(n) work even variable lost last implicit support, compared O(1) work have. move details incorporatingoptimisations full suite procedures maintaining GAC.6.3 HaggisGAC: DetailsTwo issues complicate implementation HaggisGAC compared ShortGAC.First, Lemmas depend literals support deleted single pass.Therefore, instead acting immediately finding literal supports, keep listliterals lost supports later treatment. Second, two casesmight detect lost support lost support explicit implicit comparedsingle case ShortGAC, lost supports detected way.introduce two simple data structures storing literals variables lostexplicit implicit support find them.litsLostExplicitSupport set containing literals lost final explicit supportsupported implicitly.varsLostImplicitSupport set containing variables lost final implicitsupport.adapt deleteSupport procedure Procedure 2. new versionshown Procedure 7. find literal explicit support, immediatelycheck implicit support instead (Line 8). not, addset litsLostExplicitSupport later processing find new support delete it. Variablesimplicit support detected literals deleted. donelines 15-16, justified Lemma 6.2.new propagate procedure shown Procedure 8. Like earlier Procedure 3,first delete supports involving literal deleted, rest proceduredifferent. first iterate literals lost last explicit support,variables lost last implicit support.lost explicit supports, call HaggisGAC-literalUpdate (Procedure 9).procedure analogue ShortGAC, straightforward. point intereststill check whether literal supported, even though addedlitsLostExplicitSupport not. reason support found unrelatedcall findNewSupport might also support literal. done,Procedure 9 calls findNewSupport. new support found added,prune literal longer supported.21fiNightingale, Gent, Jefferson, & MiguelRequire: Short Support sup1: oldIndex supportNumLowIdx[numSupports]2: (x 7 v) sup3:Remove sup supportListPerLit[x 7 v]4:supportListPerLit[x 7 v] = {}5:detachTrigger(x,v)6:(x 7 v) 6 zeroLits[x]7:Add (x 7 v) zeroLits[x]8:supportsPerVar[x] = numSupports9:Add (x 7 v) litsLostExplicitSupport10:sPV supportsPerVar[x]11:swap(x, varsBySupport[sPV])12:supportNumLowIdx[sPV] supportNumLowIdx[sPV]+113:supportsPerVar[x] sPV114: numSupports-15: {supportNumLowIdx[numSupports] . . . oldIndex 1}16:Add varsBySupport[i] varsLostImplicitSupportProcedure 7: HaggisGAC-DeleteSupport: (sup). One subtlety must add (x 7v) zeroLits (line 7) even also add litsLostExplicitSupport (line 9).case matters seek find new implicit support, i.e. containingx 7 v, later lost. later point Procedure 10 requires x 7 v zeroLitsx 7 v might still explicit support.Require: x 67 v (where v pruned domain x)1: litsLostExplicitSupport {}2: varsLostImplicitSupport {}3: supportListPerLit[x 7 v] 6= {}4:sup first element supportListPerLit[x 7 v]5:deleteSupport(sup)6: (y 7 b) litsLostExplicitSupport7:HaggisGAC-literalUpdate(y 7 b)8: z varsLostImplicitSupport9:HaggisGAC-variableUpdate(z)Procedure 8: HaggisGAC-Propagate: propagate(x 67 v)variables lost implicit supports, call HaggisGAC-variableUpdate (Procedure 10), similar Procedure 4. differences return statementsProcedure 4 omitted; check every iteration whether new implicit supportfound x exit loop; remove x 7 v zeroLitsnew explicit support found, allowing done lazily later call Line 5.gain efficiency ShortGAC two reasons. First, variableUpdatecalled variables lost implicit support. Second, outer loopHaggisGAC-Propagate must restarted new support found,Procedure 3. write number variables lost lastimplicit support, reduced worst case number calls variableUpdateHaggisGAC-Propagate O(n2 d) n arity constraint m. Sincen often much smaller n even zero, significant gain.22fiShort Long Supports Constraint PropagationRequire: x 7 v, last explicit support x 7 v deleted1: v D(x) supportsPerVar[x] = numSupportssupportListPerLit[x 7 v] = {}2:sup findNewSupport(x, v)3:sup = Null4:prune(x 7 v)5:else6:addSupport(sup)Procedure 9: HaggisGAC-literalUpdate(x 7 v)Require: variable x1: (x 7 v) zeroLits[x]2:supportsPerVar[x] < numSupports3:return4:supportListPerLit[x 7 v] 6= {}5:Remove (x 7 v) zeroLits[x]6:else7:v D(x)8:sup findNewSupport(x 7 v)9:sup = Null10:prune(x 7 v)11:else12:addSupport(sup)Procedure 10: HaggisGAC-variableUpdate(x)6.4 Dealing Efficiently Full-length Supportsfull-length support added, ShortGAC increments numSupports supportsPerVar every variable. Since interested condition numSupports =supportsPerVar[x], full-length support cannot change status variable. Therefore save overheads case add full-length support. achievedcase split HaggisGACs versions addSupport deleteSupport:support full length update numSupports, supportsPerVar, related datastructures. Note test apply final support arity n,initial one omission assigned literals optimisation correcteven assigned literals omitted. omit pseudocode optimisation,changes straightforward. optimisation often improves performance instancesfull-length supports 20%, important effect instancesruntimes within 2.5% without it. optimisation also applicableShortGAC, implement case address keyinefficiency algorithm has, i.e. repeated checking variables cannotlost last implicit support. affect experimental results dramatically:cases found improved performance HaggisGAC largeroptimisation provides.23fiNightingale, Gent, Jefferson, & Miguel7. Experimental Evaluation ShortGAC HaggisGACMinion solver 0.12 (Gent, Jefferson, & Miguel, 2006a) used experiments,changes additional propagators. experiments, comparedmethods maintain GAC. Therefore, solver explores search space case.Since number nodes searched invariant, compare rate search exploration,measured search nodes per second.6used 8-core machine 2.27GHz Intel Xeon E5520 CPUs 12GB memory,running Ubuntu Linux. possible ran 12 processes parallel. combination problem instance propagator, report median 11 runs.7 casespossible run 12 processes parallel exceed 1GB memory. these,ran one process time, report median 5 runs. instancesmarked tables results. one method exceeded 1GB, sometimesran comparable methods series well. allows consistent comparisonList NDList, different propagation algorithms. also means tablesnecessarily indicate method uses 1GB memory. findmedian robust measure performance, reasons described Appendix B.cases, imposed time limit one hour, limit 1,000,000 search nodes(whichever first). avoid short runs solver find solution easily,searched solutions. report complete cpu times, i.e. attemptedmeasure time attributable given propagator include initialisation.advantage automatically take account factors affecting runtime,including aspects (e.g. cache usage) may realise affect runtime. howevermean results tend understate difference methods studied.case study, implemented findNewSupport method ShortGACHaggisGAC specific constraint. also used generic list instantiation (Section 5.5.1) Next-Difference List instantiation (Section 5.5.2) comparisonpossible. compare ShortGAC HaggisGAC special-purpose propagator(when available).also compare ShortGAC-Long (ShortGAC full-length supports),HaggisGAC-Long, GAC-Schema (Bessiere & Regin, 1997) closest equivalent algorithm without strict short supports. discuss GAC-Schema Section 7.4.GAC-Schema, ShortGAC-Long HaggisGAC-Long use (constraint-specific)findNewSupport ShortGAC, subsequently extend short support full lengthusing minimum value extra variable.case, constraint compactly represented disjunction. Thereforecompare ShortGAC HaggisGAC Constructive Or. algorithm usedbased Lagerkvist Schultes (2009), without rule entailment detection.6. Source code solver three algorithms available http://www.cs.st-andrews.ac.uk/~pn/haggisgac-source.tgz problem instances experimental results http://www.cs.st-andrews.ac.uk/~pn/haggisgac-data-instances.tgz.7. preliminary investigations, found running 12 processes parallel gives consistent cpu timeresults, consistency improved taking median.24fiShort Long Supports Constraint Propagation3elementelement longelement listelement ndlistlexlex longsquarepacksquarepack longsquarepack listsquarepack ndlist2.521.51.251.110.9110100100010000100000Figure 2: Summary comparison ShortGAC HaggisGAC. x-axis mediannodes per second ShortGAC. y-axis speedup (or slowdown) HaggisGAC, i.e. ratio ShortGAC nodes per second HaggisGAC.Hence 1 represents equal behaviour, 1 means HaggisGACfaster.implementation Minion fully incremental: disjunct propagated incrementallybranch search backtracked search backtracks.8compare table constraints, described (for example) Gent et al.(2007), constraints large. example, smallest element constraintsreported 638 allowed tuples, making impossible even generate storelist allowed tuples.aid comparison HaggisGAC ShortGAC, addition tablescompare graphically Figure 2. figure shows relative speedup (orcases slowdown) using HaggisGAC compared ShortGAC.7.1 Case Study 1: Elementuse quasigroup existence problem QG3 (Colton & Miguel, 2001) evaluate ShortGAC HaggisGAC element constraint. problem class one parametern, specifying size n n table (qg) variables domains {0 . . . n 1}. Rows,columns one diagonal GAC allDifferent constraints, following Colton Miguelsmodel. element constraints represent QG3 property (i j) (j i) = (wherej members quasigroup quasigroup operator). translatesi, j : element(qg, aux[i, j], i), aux[i, j]= n qg[i, j] + qg[j, i], aux[i, j]domain {0 . . . n n 1}.8. Personal communication Pascal Van Hentenryck indicated unpublished optimisationConstructive whereby disjuncts need propagated cases. implementoptimisation.25fiNightingale, Gent, Jefferson, & Migueln678910WatchElt.27,82522,25915,63515,89815,088Specific6,9564,8662,7732,3741,594ShortGACList NDL4,122 2,1823,226 1,2331,6095451,3773981,060280Long25.98.53.62.21.6Specific11,1319,0355,6525,4194,227HaggisGACList NDL5,300 2,4734,833 1,4152,3676222,1164511,911317Long36.515.26.23.72.6GACSch.22.57.13.03.0memCon53.524.29.16.24.2Table 1: Nodes searched per second quasigroup existence problems. mem indicatesrunning memory (>12 GB). Columns correspond propagation algorithms.Watch Elt special-purpose propagator. ShortGAC HaggisGACfour instantiations: Specific (special-purpose findNewSupport functionconstraint), List, NDL (Next-Difference List), Long (as described text).GAC-Sch GAC-Schema, Con Constructive Or.constraint element(X, y, z), findNewSupport method ShortGAC returnstuples form hxi 7 j, 7 i, z 7 ji, index vector X jcommon value z xi . ShortGAC-list supports form. ConstructiveOr, used (x0 = z = 0) (x1 = z = 1) .compare ShortGAC HaggisGAC special-purpose Watched Elementpropagator (Gent et al., 2006b), GAC-Schema Constructive Or. Table 1 presentsresults QG3. general purpose methods, using short supports (with Specific, ListNDList instantiations) dramatically better alternative. example n = 10,even HaggisGAC-List method (which slower HaggisGAC-Specific)450 times faster Constructive Or, best methods.ShortGAC-Long runs 1020% faster GAC-Schema n = 6 8, slowern = 9 better n = 10 GAC-Schema uses memory. Recalluse findNewSupport method, fair comparison efficientlyexploit supports. contrast results reported previously (Nightingaleet al., 2011), ShortGAC half speed GAC-Schema. Two substantial differences account improvement: improved data structures describedSection 5; remove assigned literals full-length supports describedSection 5.6. HaggisGAC-Long consistently faster ShortGAC-LongGAC-Schema.much faster methods using full-length supports, list variants HaggisGACList HaggisGAC-NDList slower HaggisGAC-Element (andtrue ShortGAC). expected neither specialised Elementconstraint, deal data structures containing lists tuples.two list variants, NDList variant runs much slowly. However, memory usage is,expected, much less HaggisGAC-List. used less half much memoryn = 6, improving almost 10 times less memory n = 10.HaggisGAC-Element approximately twice fast ShortGAC-Elementinstances. believe two variables short supports indexresult variables meaning always supported explicitly. seen26fiShort Long Supports Constraint PropagationnGACLex34567891012141618202224104,955103,95095,42080,84172,30766,44564,26757,20848,14636,75130,05722,43216,62512,4509,526ShortGACSpecific Long87,463 7,02099,602 6,48189,127 6,35873,260 3,45665,062 2,42451,335 1,29047,05978638,34455731,62629322,71213917,81385.913,84352.410,73435.97,97624.96,25514.3HaggisGACSpecific Long91,265 9,288100,100 8,62890,009 8,50374,184 4,66665,359 3,27152,659 1,60947,84791439,68363432,42531123,06314218,42090.913,84553.810,71138.98,14126.06,26818.9GACSchema3,6223,0302,7341,6381,19067045131817082.351.533.321.012.57.3Con5,7354,9974,1042,1091,18845626318410599.162.648.336.727.021.8Table 2: Nodes searched per second BIBDs. GACLex special-purpose propagator,columns named Table 1.Figure 2, List, NDList Long instantiations HaggisGAC also fasterinstantiations ShortGAC smaller margin. special purposeWatched Element propagator fastest method, 3.6 times faster n = 10.Watched Element also appears scaling better n increases. Constructivemuch slower methods exploit strict short supports, however fasterHaggisGAC-Long. Overall clear exploiting strict short supportsbeneficial compared general purpose methods.7.2 Case Study 2: Lex-orderinguse BIBD problem evaluate ShortGAC HaggisGAC lexicographicordering constraint. lex constraint placed rows columns, performDouble Lex symmetry breaking method (Flener et al., 2002). use BIBD modelgiven Frisch, Hnich, Kiziltan, Miguel, Walsh (2002), GACLex propagator given Frisch, Hnich, Kiziltan, Miguel, Walsh (2006). use BIBDsparameter values (4n + 3, 4n + 3, 2n + 1, 2n + 1, n).constraint lexleq(X, ) arrays X , define mxi = min(Dom(xi ))myi = max(Dom(yi )). findNewSupport method ShortGAC finds lowestindex {0 . . . n} mxi < myi , = n. case = n arises X cannotlexicographically less , support sought X = . < n, supportcontains xi 7 mxi , yi 7 myi . index j < i, mxj = myj , short supportcontains xj 7 mxj , yj 7 myj otherwise valid support Null returned.lex constraint two arrays length n domain size dn shortsupports short support set, assignments two arrays equalsatisfy constraint cannot reduced. ShortGAC-List ShortGAC-NDList27fiNightingale, Gent, Jefferson, & Miguelpractical substantial constraint omit comparison.Constructive use following representation n + 1 disjuncts: (x0 < y0 ) (x0 =y0 x1 < y1 ) , including final case pairs equal.Table 2 presents results experiments non-list based methods valuesn 3 24. clear best method special-purpose GACLex propagator,HaggisGAC coming second. problem, HaggisGAC ShortGAC perform similarly. HaggisGAC ShortGAC far best general purpose methods.largest instances run 1.5 times slower special purpose method,outperforming next best method almost 300 times. Again, HaggisGAC-LongShortGAC-Long outperform GAC-Schema, instances differenceeven marked.HaggisGAC-Long substantially faster ShortGAC-Long, seenFigure 2: largely explained optimisation Section 6.4.summarise, experiments Lex constraint clearly show benefitHaggisGAC ShortGAC compared general-purpose propagation methods.speed even approaches special purpose GACLex propagator.7.3 Case Study 3: Rectangle Packingrectangle packing problem (Simonis & OSullivan, 2008) (with parameters n, widthheight) consists packing squares size 1 1 n n rectangle sizewidth height. modelled follows: variables x1 . . . xn y1 . . . yn ,(xi , yi ) represents Cartesian coordinates lower-left corner square.Domains xi variables {0 . . . width i}, yi variables {0 . . . height i}.Variables branched decreasing order (to place largest square first),xi yi , smallest value first. type constraint non-overlap squaresj: (xi + xj ) (xj + j xi ) (yi + yj ) (yj + j yi ). Minionspecial-purpose non-overlap constraint (Simonis & OSullivan, 2008),report comparison general-purpose methods. experiment used optimumrectangle sizes reported Simonis OSullivan.domains xn yn reduced break flip symmetries described SimonisOSullivan (2008). focus performance non-overlap constraint,implement commonly-used implied constraints.findNewSupport function ShortGAC follows. four disjunctsentailed given current domains, return empty support (indicating entailment). Otherwise, return support two literals satisfy one four disjuncts.list used ShortGAC-List ShortGAC-NDList supports size 2.Table 3, compare HaggisGAC ShortGAC general purposemethods. see HaggisGAC fastest method, ShortGAC second.HaggisGAC-List HaggisGAC-NDList (as well ShortGAC-List ShortGACNDList) performed well compared GAC-Schema Constructive Or. Howevern = 20, HaggisGAC-List consumes 971MB memory HaggisGAC-NDList 496MB,n > 20 possible run methods 12 processes parallel.Interestingly, performance two List variants HaggisGAC reversedCase Study 1: here, NDList significantly faster List cases. expected,28fiShort Long Supports Constraint Propagationn-w-h18-31-6919-47-5320-34-8521-38-8822-39-9823-64-6824-56-8825-43-12926-70-8927-47-148Specific14,92338,32913,9498,5688,05931,48612,3175,31025,8602,943ShortGACListNDL6,3396,9194,4468,4603,1813,9112,668 2,7811,865 1,8891,226 2,8051,717 2,2381,0079869091,9771,034786Long1,0931,282914641599718492377455252Specific19,52439,18521,00012,26211,96630,62816,07510,22823,1324,677HaggisGACListNDL7,9997,9885,2959,3304,2614,7343,955 3,9813,013 2,8961,663 3,8632,441 3,1521,634 1,5061,219 2,5771,265 1,187Long1,4711,6841,296886858971702583584400GACSch.1,0331,181775592518590474348376272Con4414782762451853491679624574Table 3: Nodes searched per second Rectangle Packing instances. columns namedTable 1.NDList used less memory, though less dramatically before. used 30%50% memory HaggisGAC-List.methods, always least 10 times slower HaggisGAC.HaggisGAC-Long faster GAC-Schema cases. Also ShortGAC-Longfaster GAC-Schema instances except 27-47-148 (this contradicts resultpreviously reported (Nightingale et al., 2011), explanation givenfirst case study).Table 3 shows HaggisGAC (with SquarePack instantiation) substantiallyfaster ShortGAC instances, exception n = 23 n = 26ShortGAC slightly faster. compared ShortGAC List, NDList,Long instantiations Figure 2, see HaggisGAC mostly 1050% faster. summary, results clearly show benefits using strict shortsupports.7.4 Comparing HaggisGAC GAC-SchemaAcross experiments, HaggisGAC-Long runs significantly faster GACSchema minimum 20% faster three times faster eventhough code contains overhead dealing strict short supports. comparedmemory usage across experiments, found similar performance across instances. found HaggisGAC-Long uses less 5% memory exceptBIBD instances, BIBD uses less 17% memory GAC-Schema.However, comparison functional instantiations full-length supports, constraints admit strict short supports. section, broadencomparison using list instantiations rather functional ones, using probleminstances used previously comparing table constraints.compared GAC-Schema similar HaggisGACShortGAC conceptually. three algorithms maintain list supports literal,updated backtracked search. GAC-Schema carefully implemented29fiNightingale, Gent, Jefferson, & MiguelSportsCarSeqGracefulPQueensBIBD20105210.5100100010000100000Figure 3: Comparison GAC-Schema HaggisGAC-List full-length table constraints. x-axis nodes per second GAC-Schema, y-axis speedupHaggisGAC-List.following pseudocode original paper (Bessiere & Regin, 1997). codeshared among three algorithms, optimised independently. example,GAC-Schema different implementation supportListPerLit, named SC (Bessiere &Regin, 1997), specialised full-length supports.contrast GAC-Schema, table constraint propagators STR2 (Lecoutre,2011) MDDC (Cheng & Yap, 2010) entirely different HaggisGAC, woulddifficult create truly comparable implementations them.report use HaggisGAC-List only, searches supportsway GAC-Schema (with one difference discuss below.) used structuredinstances Gent et al. (2007), except Semigroup class. addition, used CarSequencing instances Nightingale (2011), specifically model B instances numbered60-79. instances contain large number ternary table constraints.Figure 3 shows HaggisGAC-List almost always faster GAC-Schemaproblems. BIBDs clear algorithm better. HaggisGACalways least marginally faster Sports Scheduling, Prime Queens GracefulGraphs instances, cases range 10-20% faster. HaggisGAC substantiallyfaster Car Sequencing. seek new supports, HaggisGAC calls Procedure 5,finds new support stores index listPos. HaggisGAC backtracklistPos described Section 5.5.1. GAC-Schema similar, backtrack listPos,ensures optimality branch search iterating listPosend list (Bessiere & Regin, 1997). Profiling shows GAC-Schema hinderedbacktracking listPos (by block-copying memory) Car Sequencing,large number table constraints (2000 instance 60) large domains (some size30fiShort Long Supports Constraint Propagation1000). Alternative memory management techniques might speed GAC-Schema,claim HaggisGAC fundamentally 10 times faster GAC-Schema.7.5 Results Summarysummarise three case studies, HaggisGAC indeed outperform ShortGACmany instances, sometimes two times commonly 25%.ShortGAC rarely faster, one instance much 10%. Overall,experiments, HaggisGAC clearly better algorithm ShortGAC. Furthermore,HaggisGAC ShortGAC perform well compared Constructive GACSchema, result validates idea strict short supports.Finally, shown experimentally HaggisGAC outperform GAC-Schemaproblems containing full-length supports. discuss Appendix C major focus paper.8. Backtrack Stability Short SupportsWithin search tree, HaggisGAC often spends significant time backtracking data structures. Reducing eliminating backtracking improve efficiency. example MAC-6MAC-7 much efficient (in space time) backtracking avoided(Regin, 2005). section present new algorithm saves time deletingshort supports backtrack, saves memory bounding total number storedshort supports (including backtrack stack).new algorithm requires short supports backtrack stability property.short support backtrack stable iff remains short support backtracking (Section 3.2).three case studies, find short supports construct elementlex constraints backtrack stable, rectangle packing not. rectanglepacking, generate empty support constraint entailed. empty supportbacktrack stable unless constraint entailed root node search.introduce algorithm HaggisGAC-Stable know short supportsbacktrack stable. key change delete supports backtrackpast point introduction. stable, still correct ancestorsnode introduced at. save time previous algorithms, sincesometimes need work backtracking. Also, show below, obtaintight limits space usage stored supports.present HaggisGAC-Stable, introduce notion prime supportdeleted literal. prime support deleted literal support (either explicit implicit)valid support literal literal restored backtracking.invariant maintain deleting literal either labelled deletedsupport backtrack stack prime support literals variable currentlyimplicitly supported. invariant, guarantee backtrackpoint literal restored, must supported again: either prime supportrestore, known implicit support.task finding prime support literal naturally splits three cases.simplest case HaggisGAC-Stable deletes literals able find31fiNightingale, Gent, Jefferson, & Miguelnecessary new support. prime support implicit explicit supportwhose deletion caused fruitless search new support.second case literal pruned constraint searchprocedure, pruned literal explicit support constraint. explicitsupports must deleted longer valid, label arbitrary oneliterals prime support: simply choose last one deleted.third case unfortunately complicated. literal pruned outsidecurrent constraint, literal implicit support explicit support.difficult precisely pruned literal link implicit support.Providing maintaining link throughout search would negate efficienciesgained. solution problem lazy. variable prunedliteral implicitly supported. implicit support variable,maintaining invariant described above. literal pruned need nothingcase. need work variable loses last implicit support,ever does. happens, invalid literal explicit support mustdefinition relevant zeroLits list. Whereas previously ignored invalid literalsiterating zeroLits, label deleted implicit support primesupport invalid literal.show Lemma 8.1 HaggisGAC-Stable stores timeO(z) supports, z total number literals. save lot memoryHaggisGAC ShortGAC may store O(z 2 ) supports,O(z) deletions literals branch, deletion new set O(z) supportsmay stored. experiments later show difference memory usagesignificant practice. effective, memory usage reduced 20 times.8.1 Details HaggisGAC-StableHaggisGAC-Stable, control great care deletion restorationsupports, instead (as rest paper) simply reversing addition deletionsupport node respectively deleting adding back backtrack pastnode. short never delete active support backtracking, add backdeleted support prime support literal current active support.deleting support, setup counter numPrimeSupported. initially 0,incremented time find support prime support. propagationalgorithm finishes, support numPrimeSupported = 0, support destroyed space reclaimed. Otherwise, place numPrimeSupported new pairsbacktrack stack. pair consists deleted support literal primesupport for. backtracking, pop pair, first check current supportalready supports literal. so, simply decrement numPrimeSupported,reduces 0, reclaim supports space. literal supported,restore support via call addSupport. way literals support primeguaranteed supported.relatively minor difference iterate zeroLits delete invalid literals zeroLits. backtracking restore32fiShort Long Supports Constraint PropagationRequire: x 7 v, last explicit support x 7 v deleted1: v D(x)2:supportsPerVar[x] = numSupports supportListPerLit[x 7 v] = {}3:sup findNewSupport(x, v)4:sup = Null5:prune(x 7 v)6:increment lastSupportPerLit[x 7 v].numPrimeSupported7:push hx 7 v, lastSupportPerLit[x 7 v]i onto BacktrackStack8:else9:addSupport(sup)10: else11:increment lastSupportPerLit[x 7 v].numPrimeSupported12:push hx 7 v, lastSupportPerLit[x 7 v]i onto BacktrackStackProcedure 11: HaggisGAC-Stable-literalUpdate: (x 7 v). comparison Procedure 9, update numPrimeSupported BacktrackStack.zeroLits backtrack stack, enables space complexityresult Lemma 8.1.HaggisGAC-Stable similar HaggisGAC. appropriate simply describedifferences save space. Procedure HaggisGAC-Stable-Propagate almostProcedure 8, calling backtrack stable variants deleteSupport, literalUpdate(Procedure 11) variableUpdate (Procedure 12). addition, end algorithmdestroy reclaim space deleted support numPrimeSupported = 0.Procedure HaggisGAC-Stable-DeleteSupport (called support S) alsosimilar predecessor, Procedure 7, additions. First, initialises numPrimeSupported 0. Second, new data structures lastSupportPerLit deletedliteral x 7 lastSupportPerVar variable x. terms Procedure 7,assigned Line 9 Line 16 (respectively). Note assignmentsmake prime support: checked later.Procedure 11 analogous Procedure 9 enough differences showdetail here. identifies prime supports, necessary increments numPrimeSupported pushes invalid literal/support pairs onto backtrack stack. also presentProcedure 12 detail, analogue Procedure 10. identifies prime supports,increments counter adds pairs BacktrackStack. One difficult case arises,Line 17. Here, x 7 pruned, externally constraint.pruned Procedure 11, would zeroLits. x 7 restored backtracking still need make sure support. Since explicit support (itzeroLits), last support must implicit support deleting. Therefore storesupport BacktrackStack. minor change note remove literalszeroLits, Lines 13 19.Whenever new search node (including root) entered, Null pushed ontoBacktrackStack. used marker procedure HaggisGAC-StableBacktrack (Procedure 13), processes literal/support pairs reaches Null.restores prime supports literals put back domain backtracking,support currently known. numPrimeSupported counter33fiNightingale, Gent, Jefferson, & MiguelRequire: variable x1: (x 7 v) zeroLits[x]2:supportsPerVar[x] < numSupports3:return4:supportListPerLit[x 7 v] 6= {}5:Remove (x 7 v) zeroLits[x]6:else7:v D(x)8:sup findNewSupport(x, v)9:sup = Null10:prune(x 7 v)11:increment lastSupportPerVar[x].numPrimeSupported12:push hx 7 v, lastSupportPerVar[x]i onto BacktrackStack13:Remove (x 7 v) zeroLits[x]14:else15:addSupport(sup)16:else17:increment lastSupportPerVar[x].numPrimeSupported18:push hx 7 v, lastSupportPerVar[x]i onto BacktrackStack19:Remove (x 7 v) zeroLits[x]Procedure 12: HaggisGAC-Stable-variableUpdate: (x). similar Procedure 10addition maintenance numPrimeSupported BacktrackStack.1: top element BacktrackStack Null2:pop hx 7 v, supi BacktrackStack3:sup yet restored4:supportsPerVar(x) = numSupports supportListPerLit[x 7 v] = {}5:HaggisGAC-Stable-AddSupport(sup)6:else7:{Another support exists x 7 v}8:decrement sup.numPrimeSupported9:sup.numPrimeSupported = 010:destroy sup reclaim space11:supportListPerLit[x 7 v] = {}12:Add (x 7 v) zeroLits[x]13: pop Null BacktrackStackProcedure 13: HaggisGAC-Stable-Backtrack. Performs backtracking using BacktrackStack.support becomes zero, support destroyed longer necessary. Noteliterals put back zeroLits necessary Line 12, reversing deletionProcedure 12.cannot use optimisation described Section 5.6, deleting literals supportsvariables assigned, may break backtrack stability property.34fiShort Long Supports Constraint PropagationHowever, retain optimisation Section 6.4 full-length supports, omitpseudocode showing interest focusing essential aspects algorithms.8.2 Improved Space Complexity HaggisGAC-Stableapproach improves space complexity HaggisGAC-Stable compared HaggisGAC, following lemma shows.Lemma 8.1. constraint involving z literals, 2z supports stored, eitheractive deleted supports backtrack stack.Proof. define function supports literals. support still active,found call findNewSupport specific literal, map supportliteral. Similarly, support backtrack stack, pair leastone literal prime support for. Map support one literals. Everystored support falls one two categories, support deletedput onto backtrack stack, space reclaimed. three supports mappedliteral because:valid literals, findNewSupport called existing active supportexists literal.invalid literals, literal appears pair backtrack stacktwice. case literal appears often twice literalprime support already stack processed variable loses last implicitsupport. case, literal must zeroLits, newly deleted implicitsupport added backtrack stack literal. happendelete literal zeroLits first time happens.Thus number supports bounded 2z.bound 2z Lemma 8.1 would improve z maintained zeroLits eagerlyinstead lazily, expense higher overheads elsewhere.9. Experimental Evaluation HaggisGAC-Stablecompare HaggisGAC-Stable HaggisGAC using experimental setupSection 7. well tables results, provide graphical comparison runtimesHaggisGAC-Stable HaggisGAC Figure 4, memory usage Figure 5.Table 4 Figure 4 shows results instances Section 7.1. presentfour instantiations HaggisGAC-Stable, along fastest instantiation HaggisGAC, Watched Element special-purpose propagator, Constructive (whichfaster GAC-Schema Table 1). element, observe 10% slowdown,slight slowdown List variants. full-length supports, see almostidentical performance.Table 5 shows results instances Section 7.2. HaggisGAC-Stable-Lex performs slightly worse HaggisGAC-Lex, though fact never 10% worseslightly faster largest instances. might supports found35fiNightingale, Gent, Jefferson, & Migueldeep search likely contain literals supports found earlier, meaningbacktrack longer supports retained instead replaced earlierefficient short supports. so, advantage disappears Long variants. Indeed, HaggisGAC-Stable-Long performs much better HaggisGAC-Long,improvement increases n, 4.5 times n = 24.Rectangle Packing instantiation ShortGAC described Section 7 generatesempty support constraint becomes entailed, causing variables implicitlysupported point on. empty support backtrack stable, cannotused HaggisGAC-Stable. implemented new backtrack stable variantfindNewSupport, empty support returned, otherwisebefore. List Long variants affected returnempty support case. Table 6, use instances Section 7.3. Results showsignificant slowdowns using backtrack stability rectangle packing, 2 timesn = 24. probably inability return empty support.hand, see speedups 50% list variants, cases factor2 speedup full-length supports.see Figure 5 memory usage goes greatly stability usedfull-length supports, possibly contributing speedups cases. greatestreductions case element, two cases 20 times less memory.hand, significant reduction memory usage non-long variant.also tested HaggisGAC-Stable GAC-Schema Section 7.4. gavesimilar performance HaggisGAC therefore better GAC-Schema:omit detailed results. significant memory advantage compared HaggisGAC, Stable variant saving less 25%. therefore seem gainadvantages saw earlier backtrack stability full-length supports.conclude backtrack stability speed HaggisGAC significantly,greatly reduce memory usage using full-length supports. However, care mustused, backtrack stability harmful insisting backtrack stability increasessize returned supports.10. Related Workuse counters count supports inspired AC4 (Mohr & Henderson, 1986).study compressing tuples constraint compact data structureorder make propagation efficient. example, Gent et al. (2007) used tries,Cheng Yap (2010) applied MDDs. also extensive study searchinglist tuples find first valid tuple. Approaches include binary search (Lecoutre &Szymanek, 2006), trie search (Gent et al., 2007), approaches similar skip listsNDLists (Gent et al., 2007) hologram-tuples (Lhomme, 2004; Lhomme & Regin, 2005).techniques orthogonal main focus paper assistfinding supports, maintaining set active supports. adapted NDListscontain short supports Section 5.5.2; may also interesting adaptapproaches.STR2 maintains sparse set valid satisfying tuples constraint (Lecoutre,2011). Updated variable domains computed set time algorithm36fiShort Long Supports Constraint PropagationnWatchElt67891027,82522,25915,63515,89815,088HaggisGACSpecific11,1319,0355,6525,4194,227HaggisGAC-StableSpecificList NDList Long10,3054,8812,35830.38,3024,2251,349 15.14,9861,9505507.04,5791,7113884.44,008 2,4093092.5Con53.524.29.16.24.2Table 4: Nodes searched per second quasigroup existence problems. columnsnamed Table 1.nGACLex34567891012141618202224104,955103,95095,42080,84172,30766,44564,26757,20848,14636,75130,05722,43216,62512,4509,526HaggisGACSpecific Long91,265 9,288100,100 8,62890,009 8,50374,184 4,66665,359 3,27152,659 1,60947,84791439,68363432,42531123,06314218,42090.913,84553.810,71138.98,14126.06,26818.9HaggisGAC-StableSpecificLong90,47312,008103,4709,05693,3827,24876,7773,84467,2732,61552,1131,59147,8811,11439,17680632,31053323,70934518,55624814,50417710,4381358,1591066,16585GACSchema3,6223,0302,7341,6381,19067045131817082.351.533.321.012.57.3Con5,7354,9974,1042,1091,18845626318410599.162.648.336.727.021.8Table 5: Nodes searched per second BIBDs. GACLex special-purpose propagatorLex, columns named Table 1.n-w-h18-31-6919-47-5320-34-8521-38-8822-39-9823-64-6824-56-8825-43-12926-70-8927-47-148HaggisGACSpecific19,52439,18521,00012,26211,96630,62816,07510,22823,1324,677HaggisGAC-StableSpecificList NDList16,9509,5448,38322,5804,6638,26412,8654,9504,8405,8279,783 6,4928,798 4,7444,31928,9872,3774,5116,741 3,8943,9985,706 2,4052,19927,5071,6894,0243,996 1,5911,735Long1,6861,6212,6079579211,0951,1491,265890344GACSchema1,0331,181775592518590474348376272Table 6: Nodes searched per second Rectangle Packing instances. columns namedTable 1.37fiNightingale, Gent, Jefferson, & Miguelelementelement longelement listelement ndlistlexlex longsquarepacksquarepack longsquarepack listsquarepack ndlist654321.51.110.90.70.50.40.31101001000100001000001e+06Figure 4: Summary comparison HaggisGAC HaggisGAC-Stable. x-axismedian nodes per second HaggisGAC. y-axis speedup (or slowdown)HaggisGAC-Stable.210.50.20.10.050.021000elementelement longelement listelement ndlistlexlex longsquarepacksquarepack longsquarepack listsquarepack ndlist100001000001e+061e+071e+08Figure 5: Summary comparison memory usage (KiB) HaggisGAC HaggisGACStable. x-axis median memory usage HaggisGAC. y-axisreduction (or increase) usage HaggisGAC-Stable, i.e. ratio HaggisGAC memory usage HaggisGAC-Stable. Hence 1 representsequal behaviour, 1 means HaggisGAC-Stable used less memory.38fiShort Long Supports Constraint Propagationinvoked. concept maintaining support, seeking new support literal.would interesting investigate adapting STR2 handle short supports. wouldresult entirely different algorithm ones presented paper, possiblycomplementary strengths.MDD propagator MDDC (Cheng & Yap, 2010) maintains MDD incrementallysearch. MDD compressed representation satisfying tuplesconstraint. time complexity MDDC linear initial size MDD, thereforedegree compression vital efficiency algorithm. cases,constraint amenable strict short supports, also compress well MDD(given appropriate variable ordering). example, lex constraint compresses wellpartly (given variable order x1 , y1 , x2 , y2 , . . .) constraint satisfiedassigning prefix variables. Lex amenable short supports reason.However, constraints small set short supports cannot compressedeffectively MDD. Suppose disjunction equality constraints pairn variables domain size d. n 1 variables, MDD must Cn1 states.Another property MDD compression might indicate interesting direction futurework. Lex also compresses well MDD multiple assignments prefixvariables lead subsequent vertex (e.g. {x1 7 1, y1 7 1} {x1 7 2, y1 7 2}).something short support algorithms currently able exploit.Katsirelos Walsh (2007) proposed different generalisation support, named ctuples. c-tuple contains set values variable scope constraint.valid tuple whose values drawn c-tuple (full-length) support. KatsirelosWalsh give outline modified version GAC-Schema directly stores c-tuples.also present experiments based different propagator, GAC3.1r, demonstratingmodest speed improvement c-tuples compared conventional full-length supports.c-tuple contains values variable, nevertheless recorded (in SC )support value individually (Katsirelos & Walsh, 2007). algorithmconcept implicit support.context Constructive Or, Lhomme (2003) observed support onedisjunct support values variable contained A. concept similarshort support albeit less general, length supports fixedlength disjuncts. presented non-incremental Constructive algorithm twodisjuncts.algorithms similar flavour GAC-Schema (Bessiere & Regin, 1997),natural compare GAC-Schema. However GAC algorithmsGAC2001/3.1 (Bessiere et al., 2005) would interesting comparealgorithms.11. Conclusionsintroduced detailed three general purpose propagation algorithms shortsupports. either given specialised function find new supportsconstraint, used function accepts explicit list short supports.strict short supports available, three algorithms perform well, provide much39fiNightingale, Gent, Jefferson, & Miguelbetter performance general purpose methods GAC-Schema Constructive Or.shows value using strict short supports.first algorithm studied ShortGAC, described improvementscompared earlier report algorithm (Nightingale et al., 2011). identified significant inefficiency ShortGAC dealing explicit supports.introduced new algorithm, HaggisGAC corrects flaw, better theoreticalcomplexities, performs much better ShortGAC experiments. threecase studies, HaggisGAC far faster general purpose methods. best caseeven achieved speeds 90% special purpose propagator. Perhapsremarkably, able deal strict short full-length supports, HaggisGAC outperformed ShortGAC strict short supports GAC-Schema full-lengthsupports, i.e. cases algorithms respectively specialised for.third algorithm, HaggisGAC-Stable, retain supports backtracking.less effective HaggisGAC invalidates use certain strict short supports,also significantly faster problems full-length supports, reducememory usage greatly cases.proposed algorithms excellent propagating disjunctions constraints.experiments disjunctions found algorithms faster ConstructiveGAC-Schema least order magnitude, three orders magnitude.summarise, shown value explicit use strict short supportsgeneral purpose propagation algorithms generalised arc consistency. strict shortsupports available, exploiting yields orders magnitude improvements genericpropagation algorithms. cases, even found generic algorithm comeclose performance specialised propagator. Previously, short supportsseem recognised important right. overall contributioncorrect focus short supports first class objects.Acknowledgmentswould like thank anonymous reviewers Bilal Syed Hussain comments,EPSRC funding work grants EP/H004092/1 EP/E030394/1.Appendix A. Comparison ShortGAC ShortGAC-IJCAISection 4, noted optimised data structures algorithms ShortGAC, compared previous presentation (Nightingale et al., 2011). demonstrateindeed improvements, compared two implementations ShortGACthree case studies used paper. use name ShortGAC-IJCAIprevious version. quoting results previous work (Nightingale et al.,2011), rerun experiments using environment described Section 7.also updated codebase Minion 0.12 instead Minion 0.10 earlier paper.algorithm instance, report nodes searched per second peak memory use.Table 7 shows results instances Section 7.1. clear resultsShortGAC makes much better use memory also faster ShortGAC-IJCAI40fiShort Long Supports Constraint Propagation0.16low memory, Sections 7 9high memory, Sections 7 9List + NDList, Section 7.4GAC-Schema + Constructive0.140.120.10.080.060.040.0200.11101001000100001000001e+06Figure 6: Scatterplot median nodes per second (x-axis) median absolutedeviation divided median (y-axis). distinguishmain experiments Sections 7 9, cases medians 5runs, list variants used table constraints Section 7.4, datapaper GAC-Schema Constructive Or.instances. Table 8 shows results instances Section 7.2. Element,ShortGAC makes better use memory faster ShortGAC-IJCAI, althoughimprovements great before. Table 9, use instances Section 7.3.previous two case studies, ShortGAC consistently better speedmemory use. conclude algorithms data structures used paperindeed superior used previously (Nightingale et al., 2011).Appendix B. Median Absolute Deviation Experimentsexperiments report median either 11 5 runs. assess robustmedian measure looked, combination instances algorithm,median absolute deviation (MAD), i.e. median absolute difference datapoints median. Figure 6 shows MAD algorithm/instance combinationsfraction median case. shows 511 algorithm/instance combinationstested (including combinations reported detail paper). nodes persecond, maximum MAD found always less 15% median, worstcase 14.5%. HaggisGAC-Long n = 9 Table 1. fourcases MAD 8% median. Figures memory usage evenconsistent, two cases (at 6.3% 6.1%) showing MAD 5% medianothers 2%. major conclusions draw regard 10% changebehaviour one method another significant, therefore saymedian robust measure performance.41fiNightingale, Gent, Jefferson, & Migueln678910ShortGACnode rate6,9564,8662,7732,3741,594ShortGAC-IJCAInode rate4,8393,2731,6731,5111,294ShortGACmemory5,6846,6248,99612,56017,048ShortGAC-IJCAImemory27,88072,916188,812461,648991,768Table 7: Nodes searched per second memory use (KiB) quasigroup existence problems. Comparison ShortGAC ShortGAC-IJCAI.n34567891012141618202224ShortGACnode rate87,46399,60289,12773,26065,06251,33547,05938,34431,62622,71217,81313,84310,7347,9766,255ShortGAC-IJCAInode rate83,96498,13589,28674,18463,09150,48045,08536,17929,45520,86816,08712,3569,6147,2085,398ShortGACmemory7,47611,68016,40822,56831,34842,42055,66074,348120,024181,252263,792360,500493,368632,064811,104ShortGAC-IJCAImemory8,39212,99218,51226,26036,35649,01265,68485,700138,496209,492308,400422,536570,188735,548939,796Table 8: Nodes searched per second memory use BIBD problems. ComparisonShortGAC ShortGAC-IJCAI.n-w-h18-31-6919-47-5320-34-8521-38-8822-39-9823-64-6824-56-8825-43-12926-70-8927-47-148ShortGACnode rate14,92338,32913,9498,5688,05931,48612,3175,31025,8602,943ShortGAC-IJCAInode rate10,89229,64710,2886,1095,82124,5288,3863,82821,1462,086ShortGACmemory11,87610,17213,98816,10018,86813,98817,54827,58019,79639,848ShortGAC-IJCAImemory24,56819,68033,02038,82846,34431,70043,70874,06449,512106,144Table 9: Nodes searched per second memory use rectangle packing. ComparisonShortGAC ShortGAC-IJCAI.42fiShort Long Supports Constraint PropagationAppendix C. Comparison GAC-Schema HaggisGACshowed Section 7.4 HaggisGAC outperforms GAC-Schema dealingfull-length supports. despite fact HaggisGAC small overheadsdealing strict short supports even none exist. discuss brieflymay so.GAC-Schema concept current supports literal one current support,one active supports contain literal. additional datastructure S( ). active support , S( ) list literalscurrent support. Hence invalidated, GAC-Schema finds new current supportliteral S( ) (or deletes literal). HaggisGAC dispensedentirely. sign literal needs new support lost current support,support list (supportListPerLit) empty. small potential savingmaintaining S( ).second, possibly important, difference GAC-Schema eagerHaggisGAC. literal x 7 v loses current support, GAC-Schema checkactive supports containing x 7 v valid, O(n) operation one.invalid, GAC-Schema calls findNewSupport. returns Null x 7 vdeleted. HaggisGAC none this, avoiding completely cost checking validity. safe every support invalid, literal deletion supportcause call deleteSupport last result empty list, causing callfindNewSupport. approaches correct, GAC-Schemas wastefulperforms unnecessary validity checks. However, one cannot guarantee time saving, GAC-Schema perform deletions sooner, possibly affecting way propagatorinteracts propagators.ReferencesBessiere, C., Hebrard, E., Hnich, B., & Walsh, T. (2007). complexity reasoningglobal constraints. Constraints, 12 (2), 239259.Bessiere, C., & Regin, J.-C. (1997). Arc consistency general constraint networks: Preliminary results. Proceedings IJCAI 1997, pp. 398404.Bessiere, C., Regin, J.-C., Yap, R. H. C., & Zhang, Y. (2005). optimal coarse-grainedarc consistency algorithm. Artificial Intelligence, 165 (2), 165185.Cheng, K. C. K., & Yap, R. H. C. (2010). MDD-based generalized arc consistencyalgorithm positive negative table constraints global constraints.Constraints, 15 (2), 265304.Colton, S., & Miguel, I. (2001). Constraint generation via automated theory formation.Proceedings CP 2001, pp. 575579.Flener, P., Frisch, A. M., Hnich, B., Kiziltan, Z., Miguel, I., Pearson, J., & Walsh, T. (2002).Breaking row column symmetries matrix models. Proceedings CP 2002, pp.462476.Frisch, A. M., Hnich, B., Kiziltan, Z., Miguel, I., & Walsh, T. (2002). Global constraintslexicographic orderings. Proceedings CP 2002, pp. 93108.43fiNightingale, Gent, Jefferson, & MiguelFrisch, A. M., Hnich, B., Kiziltan, Z., Miguel, I., & Walsh, T. (2006). Propagation algorithmslexicographic ordering constraints. Artificial Intelligence, 170 (10), 803834.Gent, I. P. (2012). optimality result maintaining list pointers backtrackingsearch. Tech. rep. CIRCA preprint 2012/1, University St Andrews.Gent, I. P., Jefferson, C., & Miguel, I. (2006a). Minion: fast scalable constraint solver.Proceedings ECAI 2006, pp. 98102.Gent, I. P., Jefferson, C., & Miguel, I. (2006b). Watched literals constraint propagationMinion. Proceedings CP 2006, pp. 182197.Gent, I. P., Jefferson, C., Miguel, I., & Nightingale, P. (2007). Data structures generalisedarc consistency extensional constraints. Proceedings AAAI 2007, pp. 191197.Gent, I. P., Petrie, K., & Puget, J.-F. (2006). Handbook Constraint Programming (Foundations Artificial Intelligence), chap. Symmetry Constraint Programming, pp.329376. Elsevier Science Inc., New York, NY, USA.Jefferson, C., Moore, N. C. A., Nightingale, P., & Petrie, K. E. (2010). Implementing logicalconnectives constraint programming. Artificial Intelligence, 174 (16-17), 14071429.Katsirelos, G., & Walsh, T. (2007). compression algorithm large arity extensionalconstraints. Proceedings CP 2007, pp. 379393.King, A., Cromarty, L., Paterson, C., & Boyd, J. (2007). Applications ultrasonographyreproductive management dux magnus gentis venteris saginati. Veterinaryrecord, 160 (3), 94.Lagerkvist, M. Z., & Schulte, C. (2009). Propagator groups. Proceedings CP 2009, pp.524538.Lecoutre, C. (2011). STR2: optimized simple tabular reduction table constraints. Constraints, 16 (4), 341371.Lecoutre, C., & Szymanek, R. (2006). Generalized arc consistency positive table constraints. Proceedings CP 2006, pp. 284298.Lhomme, O., & Regin, J.-C. (2005). fast arc consistency algorithm n-ary constraints.Proceedings AAAI 2005, pp. 405410.Lhomme, O. (2003). efficient filtering algorithm disjunction constraints.Proceedings CP 2003, pp. 904908.Lhomme, O. (2004). Arc-consistency filtering algorithms logical combinations constraints. Integration AI Techniques Constraint ProgrammingCombinatorial Optimization Problems (CP-AI-OR04), pp. 209224.Mackworth, A. K. (1977). reading sketch maps. Reddy, R. (Ed.), IJCAI, pp. 598606.William Kaufmann.Mears, C. D. (2009). Automatic Symmetry Detection Dynamic Symmetry BreakingConstraint Programming. Ph.D. thesis, Clayton School Information Technology,Monash University.Mohr, R., & Henderson, T. C. (1986). Arc path consistency revisited. Artificial Intelligence, 28 (2), 225233.44fiShort Long Supports Constraint PropagationNightingale, P. (2011). extended global cardinality constraint: empirical survey.Artificial Intelligence, 175 (2), 586614.Nightingale, P., Gent, I. P., Jefferson, C., & Miguel, I. (2011). Exploiting short supportsgeneralised arc consistency arbitrary constraints. Proceedings IJCAI 2011,pp. 623628.Puget, J.-F. (2005). Automatic detection variable value symmetries. ProceedingsCP 2005, pp. 475489.Regin, J.-C. (1996). Generalized arc consistency global cardinality constraint. Proceedings AAAI 1996, pp. 209215.Regin, J.-C. (2005). Maintaining arc consistency algorithms search withoutadditional space cost. Proceedings CP 2005, pp. 520533.Rossi, F., van Beek, P., & Walsh, T. (Eds.). (2006). Handbook Constraint Programming.Elsevier.Schulte, C., & Tack, G. (2010). Implementing efficient propagation control. ProceedingsTRICS: Techniques Implementing Constraint programming Systems, conferenceworkshop CP 2010, St Andrews, UK.Simonis, H., & OSullivan, B. (2008). Search strategies rectangle packing. ProceedingsCP 2008, pp. 5266.Wurtz, J., & Muller, T. (1996). Constructive disjunction revisited. Proceedings20th Annual German Conference Artificial Intelligence: Advances ArtificialIntelligence, KI 96, pp. 377386. Springer-Verlag.45fiJournal Artificial Intelligence Research 46 (2013) 651686Submitted 10/12; published 04/13Description Logic Knowledge Action BasesBabak Bagheri HaririDiego CalvaneseMarco MontaliBAGHERI @ INF. UNIBZ .CALVANESE @ INF. UNIBZ .MONTALI @ INF. UNIBZ .KRDB Research Centre Knowledge DataFree University Bozen-BolzanoPiazza Domenicani 3, 39100 Bolzano, ItalyGiuseppe De GiacomoRiccardo De MasellisPaolo FelliDEGIACOMO @ DIS . UNIROMA 1.DEMASELLIS @ DIS . UNIROMA 1.FELLI @ DIS . UNIROMA 1.Dipartimento di Ingegneria Informatica Automatica e GestionaleSapienza Universita di RomaVia Ariosto 25, 00185 Roma, ItalyAbstractDescription logic Knowledge Action Bases (KAB) mechanism providingsemantically rich representation information domain interest terms description logic knowledge base actions change information time, possibly introducingnew objects. resort variant DL-Lite unique name assumption enforcedequality objects may asserted inferred. Actions specified setsconditional effects, conditions based epistemic queries knowledge base(TBox ABox), effects expressed terms new ABoxes. setting, addressverification temporal properties expressed variant first-order -calculus quantification across states. Notably, show decidability verification, suitable restriction inspirednotion weak acyclicity data exchange.1. IntroductionRecent work business processes, services databases bringing forward need considering data processes first-class citizens process service design (Nigam & Caswell,2003; Bhattacharya, Gerede, Hull, Liu, & Su, 2007; Deutsch, Hull, Patrizi, & Vianu, 2009; Vianu,2009; Meyer, Smirnov, & Weske, 2011). particular, so-called artifact-centric approaches,advocate sort middle ground conceptual formalization dynamic systemsactual implementation, promising effective practice (Cohn & Hull, 2009).verification temporal properties presence data represents significant research challenge (for survey, see Calvanese, De Giacomo, & Montali, 2013), since taking accountdata evolve time results systems infinite number states. Neither finite-statemodel checking (Clarke, Grumberg, & Peled, 1999) current techniques infinitestate model checking, mostly tackle recursion (Burkart, Caucal, Moller, & Steffen, 2001),apply case. Recently, advancements issue (Cangialosi, De Giacomo, De Masellis, & Rosati, 2010; Damaggio, Deutsch, & Vianu, 2011; Bagheri Hariri, Calvanese,De Giacomo, De Masellis, & Felli, 2011; Belardinelli, Lomuscio, & Patrizi, 2011), contextsuitably constrained relational database settings.c2013AI Access Foundation. rights reserved.fiBAGHERI H ARIRI , C ALVANESE , E G IACOMO , E ASELLIS , F ELLI , & ONTALIwork based maintaining information relational database,sophisticated applications foreseen enrich data-intensive business processes semanticlevel, information maintained semantically rich knowledge base allowsoperating incomplete information (Calvanese, De Giacomo, Lembo, Montali, & Santoso, 2012;Limonad, De Leenheer, Linehan, Hull, & Vaculin, 2012). leads us look combinefirst-order data, ontologies, processes, maintaining basic inference tasks (specificallyverification) decidable. setting, capture domain interest terms semanticallyrich formalisms provided ontological languages based Description Logics (DLs)(Baader, Calvanese, McGuinness, Nardi, & Patel-Schneider, 2003). languages natively dealincomplete knowledge modeled domain. additional flexibility comes addedcost, however: differently relational databases, evaluate queries need resort logicalimplication. Moreover, incomplete information combined ability evolving systemactions results notoriously fragile setting w.r.t. decidability (Wolter & Zakharyaschev,1999b, 1999a; Gabbay, Kurusz, Wolter, & Zakharyaschev, 2003). particular, due natureDL assertions (which general definitions constraints models), get onedifficult kinds domain descriptions reasoning actions (Reiter, 2001),amounts dealing complex forms state constraints (Lin & Reiter, 1994).overcome difficulty, virtually solutions aim robustness based so-calledfunctional view knowledge bases (Levesque, 1984): KB provides ability queryingbased logical implication (ask), ability progressing new KB formsupdates (tell) (Baader, Ghilardi, & Lutz, 2012; Calvanese, De Giacomo, Lenzerini, & Rosati,2011). Notice functional view tightly related epistemic interpretation KB(Calvanese, De Giacomo, Lembo, Lenzerini, & Rosati, 2007a). Indeed work also relatedEpistemic Dynamic Logic (van Ditmarsch, van der Hoek, & Kooi, 2007), and, thoughscope paper, decidability results presented could find application contextresearch well.follow functional view KBs. However, key point work execution step external information incorporated system form new individuals (denotedfunction terms), is, systems closed w.r.t. available information. makesframework particularly interesting challenging. particular, presence individuals requires specific treatment equality, since system progresses new informationacquired, distinct function terms may inferred denote object.Specifically, introduce so-called Knowledge Action Bases (KABs). KABequipped ontology or, precisely, TBox expressed, case, variant DLLiteA (Calvanese, De Giacomo, Lembo, Lenzerini, & Rosati, 2007b), extends coreWeb Ontology Language OWL 2 QL (Motik, Cuenca Grau, Horrocks, Wu, Fokoue, & Lutz, 2012)particularly well suited data management. TBox captures intensional informationdomain interest, similarly UML class diagrams conceptual data models, thoughsoftware component used run-time. KAB includes also ABox, actsstorage state. ABox maintains data interest, accessed relying query answering based logical implication (certain answers). Notably, variant DL-LiteA withoutunique name assumption (UNA), allow explicit equality assertions ABox.way suitably treat function terms represent individuals acquired execution.Technically, need dealing equality breaks first-order rewritability DL-LiteA queryanswering, requires that, addition rewriting process, inference equality performed652fiD ESCRIPTION L OGIC K NOWLEDGE ACTION BASES(Artale, Calvanese, Kontchakov, & Zakharyaschev, 2009). query language, use unionsconjunctive queries, possibly composing certain answers full FOL constructs.gives rise epistemic query language asks known current KB (Calvanese et al., 2007a). Apart KB, KAB contains actions, whose execution changesstate KB, i.e., ABox. actions specified sets conditional effects,conditions (epistemic) queries KB effects expressed terms new ABoxes.Actions static pre-conditions, whereas process used specify actionsexecuted step. simplicity, model processes condition/action rules,condition expressed query KB.setting, address verification temporal/dynamic properties expressed firstorder variant -calculus (Park, 1976; Stirling, 2001), atomic formulae queriesKB refer constants function terms, controlled formquantification across states allowed. Notice previous decidability results actionsDL KBs assumed information coming outside system, sensenew individual terms added executing actions (Calvanese et al., 2011; Baader et al., 2012;Rosati & Franconi, 2012). paper, instead, allow arbitrary introduction new terms.Unsurprisingly, show even simple KABs temporal properties, verificationundecidable. However, also show rich class KABs, verification fact decidablereducible finite-state model checking. obtain result, following Cangialosi et al. (2010),Bagheri Hariri et al. (2011), rely recent results data exchange finitenesschase tuple-generating dependencies (Fagin, Kolaitis, Miller, & Popa, 2005), though, case,need extend approach deal (i) incomplete information, (ii) inference equality,(iii) quantification across states verification language.paper organized follows. Section 2 give preliminaries DL-LiteA withoutUNA , going knowledge base formalism. Section 3 describes KAB framework detail, Section 4 discusses execution semantics. Section 5 introduceverification formalism KABs. Section 6, show verification KABs general undecidable, even considering simple temporal properties KABs. Section 7, givemain technical result: verification weakly acyclic KABs decidable E XP IME. Section 8,extensively survey related work. Section 9 concludes paper.2. Knowledge Base FormalismDescription Logics (DLs) (Baader et al., 2003) knowledge representation formalismstailored representing domain interest terms concepts (or classes), denoting setsobjects, roles (or relations), denoting binary relations objects. DL knowledge bases(KBs) based alphabet concept role names, alphabet individuals.DL KB formed two distinct parts: TBox, represents intensional level KBcontains description domain interest terms universal assertions conceptsroles; ABox, represents instance level KB contains extensionalinformation participation individuals concepts roles.expressing KBs use DL-LiteNU , variant DL-LiteA language (Poggi, Lembo,Calvanese, De Giacomo, Lenzerini, & Rosati, 2008; Calvanese, De Giacomo, Lembo, Lenzerini, &Rosati, 2013) drop unique name assumption (UNA) line standard WebOntology Language (OWL 2) (Bao et al., 2012). Essentially, DL-LiteNU extends OWL 2 QL653fiBAGHERI H ARIRI , C ALVANESE , E G IACOMO , E ASELLIS , F ELLI , & ONTALIprofile OWL 2, including functionality assertions possibility state equalityindividuals.syntax concept role expressions DL-LiteNU follows:R P | P ,V R | R,B N | R,C B | B,N denotes concept name, P role name, P inverse role.Formally, DL-LiteNU KB (T, A), TBox finite set TBox assertions formB v C,R v V,(funct R),called respectively concept inclusions, role inclusions, functionality assertions. followusual assumption DL-Lite, according TBox may contain neither (funct P )(funct P ) contains R v P R v P , role R (Poggi et al., 2008; Calvanese et al.,2013). condition expresses roles functionality assertions cannot specialized.DL-LiteNU TBoxes able capture essential features conceptual modeling formalisms,UML Class Diagrams (or Entity-Relationship schemas), namely ISA classesassociations (relationships), disjointness classes associations, typing associations, association multiplicities (in particular, mandatory participation functionality).main missing feature completeness hierarchies, would require introductiondisjunction would compromise good computational properties DL-Lite.ABox DL-LiteNU KB (T, A) finite set ABox assertions formN (t1 ),P (t1 , t2 ),t1 = t2 ,called respectively, concept (membership) assertions, role (membership) assertions, equalityassertions, t1 , t2 terms denoting individuals (see below). presence equality assertions ABox requires specific treatment equality goes beyond usual reasoningtechniques DL-Lite based first-order rewritability, although reasoning remains polynomial(Artale et al., 2009). hand, allow explicit disequality, though one usemembership disjoint concepts assert two individuals different.DL-LiteNU admits complex terms denoting individuals. terms inductively definedstarting finite set constants, applying finite set (uninterpreted) functionsvarious arity greater 0. result, set individual terms countably infinite.call function terms terms involving functions. Also, structure terms impactinference equality, congruence relation structure terms, i.e., ti = t0i ,{1, . . . , n}, f function symbol arity n, f (t1 , . . . , tn ) = f (t01 , . . . , t0n ). Apartaspect related equality, treat individuals denoted terms simply ordinaryindividual constants DLs.adopt standard semantics DLs based FOL interpretations = (I , ),interpretation domain interpretation function tI , N ,P , term t,concept name N , role name P . Coherently congruencerelation terms, (f (t1 , . . . , tn ))I = (f (t01 , . . . , t0n ))I , whenever tIi = t0i ,{1, . . . , n}.Complex concepts roles interpreted follows:(R)I(B)I= {o | o0 .(o, o0 ) RI },= \ B ,(P )I(R)I654= {(o1 , o2 ) | (o2 , o1 ) P },= \ R .fiD ESCRIPTION L OGIC K NOWLEDGE ACTION BASESinterpretation satisfies assertion form:B v C, B C ;R v V , RI V ;(funct R), o, o1 , o2 that, {(o, o1 ), (o, o2 )} RI , o1 = o2 ;N (t1 ), tI1 N ;P (t1 , t2 ), (tI1 , tI2 ) P ;t1 = t2 , tI1 = tI2 .model KB (T, A) satisfies assertions A. KB (T, A) satisfiablemodel. also say ABox consistent w.r.t. TBox KB (T, A) satisfiable.assertion logically implied KB (T, A), denoted (T, A) |= , every model (T, A)satisfies well.following characterization satisfiability logical implication DL-LiteNU easyconsequence results Artale et al. (2009).Theorem 1 Checking satisfiability logical implication DL-LiteNU PT IME-complete.Proof. PT IME lower bound immediate consequence lower bound establishedArtale et al. (2009) DL-LiteNU allow use complex individual terms.upper bound, Artale et al. (2009) provide PT IME algorithm based first usingfunctionality assertions exhaustively propagate equality, resorting PT IME algorithm(in combined complexity) reasoning DL-Lite absence UNA. adaptalgorithm changing first step, propagate, PT IME, equality termsactive domain due functionalities, also due congruence.Next introduce queries. usual (cf. OWL 2), answers queries formed constants/terms denoting individuals explicitly mentioned ABox. (active) domainABox A, denoted ADOM(A), (finite) set constants/terms appearing concept, role,equality assertions A. (predicate) alphabet KB (T, A), denoted ALPH((T, A)) setconcept role names occurring A.union conjunctive queries (UCQ) q KB (T, A) FOL formula form~y1 .conj 1 (~x, ~y1 ) ~yn .conj n (~x, ~yn ) free variables ~x existentially quantified variables ~y1 , . . . , ~yn . conj (~x, y~i ) q conjunction atoms form N (z), P (z, z 0 )N P respectively denote concept role name occurring ALPH((T, A)), z, z 0constants ADOM(A) variables ~x y~i , {1, . . . , n}. certain answers q(T, A) set ANS (q, T, A) substitutions1 free variables q constants/termsADOM(A) q evaluates true every model (T, A), i.e., q logically implied(T, A). Following notation used assertions, denote (T, A) |= q. q freevariables, called boolean certain answers either empty substitution denotingtrue nothing denoting false.Again, easy consequence results Artale et al. (2009), obtain followingcharacterization query answering DL-LiteNU .Theorem 2 Computing ANS (q, T, A) UCQ q DL-LiteNU KB (T, A) PT IME-completesize A.1. customary, view substitution simply tuple constants, assuming ordering freevariables q.655fiBAGHERI H ARIRI , C ALVANESE , E G IACOMO , E ASELLIS , F ELLI , & ONTALIProof. proof Theorem 1, first propagate PT IME equality termsactive domain using functionality congruence closure, resort query answeringDL-Lite presence UNA, PT IME combined size TBoxABox resulting equality propagation (actually, AC0 size ABox).also consider extension UCQs, called ECQs, queries query languageEQL-Lite(UCQ) (Calvanese et al., 2007a), is, FOL query language whose atoms UCQsevaluated according certain answer semantics above. ECQ KB (T, A) possiblyopen formula formQ [q] | [x = y] | Q | Q1 Q2 | x.Q,[q] denotes certain answers UCQ q (T, A), [x = y] denotes certain answersx = (T, A), is, set {hx, yi ADOM(A) | (T, A) |= (x = y)}, logical operatorsusual meaning, quantification ranges elements ADOM(A).Formally define relation Q holds (T, A) substitution free variablesQ, written T, A, |= Q, inductively follows:T, A,T, A,T, A,T, A,T, A,|= [q]|= [x = y]|= Q|= Q1 Q2|= x.Q(T, A) |= q,(T, A) |= (x = y),T, A, 6|= Q,T, A, |= Q1 T, A, |= Q2 ,exists ADOM(A) T, A, [x/t] |= Q,[x/t] denotes substitution obtained assigning x constant/term (if xalready present value replaced t, not, pair x/t added substitution).certain answer Q (T, A), denoted ANS (Q, T, A), set substitutionsfree variables Q Q holds (T, A) , i.e.,ANS (Q, T, A)= { | T, A, |= Q}.Following line proof Calvanese et al. (2007a), considering Theorem 2basic step evaluating UCQ, get:Theorem 3 Computing ANS (Q, T, A) ECQ Q DL-LiteNU KB (T, A) PT IMEcomplete size A.recall DL-Lite enjoys rewritability property, states every UCQ qevery DL-Lite KB (T, A),ANS (q, T, A)= ANS (rew (q), , A),rew (q) UCQ computed reformulation algorithm Calvanese et al. (2007b).Notice that, way, compiled away TBox. result extended ECQswell, i.e., every ECQ Q, ANS (Q, T, A) = ANS (rew (Q), , A) query rew (Q)obtained Q substituting atom [q] (where q UCQ) [rew (q)] (Calvanese et al.,2007a). setting, exploit rewritability, pre-processedABox (in PT IME) propagating equalities individual terms ADOM(A) accordingfunctionality assertions congruence terms.656fiD ESCRIPTION L OGIC K NOWLEDGE ACTION BASESsay two ABoxes A1 A2 equivalent w.r.t. TBox predicate alphabet ,denotedA1 T, A2 ,every ABox assertion 2 A2 either concept assertion N (t) N , roleassertion P (t1 , t2 ) P , equivalence assertion t1 = t2 , (T, A1 ) |= 2 ; viceversa, every ABox assertion 1 A1 , either concept assertion N (t) N ,role assertion P (t1 , t2 ) P , equivalence assertion t1 = t2 , (T, A2 ) |= 1 .Notice A1 T, A2 , every ECQ Q whose concept role names belongANS (Q, T, A1 ) = ANS (Q, T, A2 ). Notice also that, applying Theorem 3 booleanquery [] corresponding ABox assertion , A1 A2 , obtain ABoxequivalence checked PT IME.3. Knowledge Action BasesKnowledge Action Base (KAB) tuple K = (T, A0 , , ) A0 formknowledge component (or knowledge base), form action component (or actionbase). practice, K stores information interest KB, formed fixed TBoxinitial ABox A0 , evolves executing actions according sequencing establishedprocess . evolution new individuals acquired KB. individualswitnesses new pieces information inserted KAB environment KABruns (i.e., external world). represent new objects function terms. KABevolves, identity individuals intuitively preserved induces necessityremembering equalities terms denoting individuals discovered past. describedetail components KAB.3.1 TBoxDL-LiteNU TBox, used capture intensional knowledge domain interest.TBox fixed all, evolve execution KAB.3.2 ABoxA0 DL-LiteNU ABox, stores extensional information interest. Notice A0ABox initial state KAB, KAB evolves due effect actions,ABox, indeed state system, evolves accordingly store up-to-date information.actions acquire new information external world using calls externalservices represented functions. Given information services,except name parameters passed them, functions remain uninterpreted.assume result service calls depends passed parameters. Hence,represent new individuals returned service calls function terms. presencefunction terms impact treatment equality, since principle need close equalityw.r.t. congruence. closure generates infinite number logically implied equalityassertions, going keep assertions implicit, computing needed.657fiBAGHERI H ARIRI , C ALVANESE , E G IACOMO , E ASELLIS , F ELLI , & ONTALI3.3 Actionsfinite set actions. action modifies current ABox adding deletingassertions, thus generating new ABox A0 . action formact(~x) : {e1 , . . . , en },act(~x) signature {e1 , . . . , en } (finite) set effects forming effectspecification . action signature constituted name act list ~x individual inputparameters, need instantiated actual individuals execution time.2 effect eiform[qi+ ] QA0i ,(1)qi+ UCQ, i.e., positive query, extracts bulk data process (obtainedcertain answers qi+ ); free variables qi+ include action parameters;+Qarbitrary ECQ, whose free variables occur among free variables qi ,refines, using negation quantification, result qi+ . query [qi+ ] Qwhole extracts individual terms used form new state KAB (noticeUCQ-ECQ division also convenience readily available positive partcondition, exploit later);A0i set (non-ground) ABox assertions, include terms: constants A0 , freevariables qi+ , function terms f (~x) arguments ~x free variables qi+ .terms, grounded values extracted [qi+ ] Q, give rise (ground) ABoxassertions, contribute form next state KAB.precisely, given current ABox K substitution input parametersaction , denote action instantiated actual parameters coming .firing state A, get new state A0 computed simultaneously applyinginstantiated effects follows:effect ei form (1) extracts set ANS (([qi+ ] Q), T, A) tuplesterms ADOM(A) and, tuple , asserts set A0i ABox assertions obtainedA0i applying substitution free variables qi+ . function termf (~x) appearing A0i , new ground term introduced form f (~x).terms represent new constants coming external environment KAB runningin.denote ei (A) overall set ABox assertions, i.e.,[ei (A) =A0i .ANS (([qi+ ]Q),T,A)2. disregard specific treatment output parameters, assume instead user freely pose queriesKB, extracting whatever information she/he interested in.658fiD ESCRIPTION L OGIC K NOWLEDGE ACTION BASESMoreover, let EQ(T, A) = {t1 = t2 | ht1 , t2 ANS ([x1 = x2 ], T, A)}. Observe that, duesemantics queries, terms EQ(T, A) must appear explicitly ADOM(A),is, possibly infinite number equalities due congruence appear EQ(T, A),though logically implied. Hence, equalities EQ(T, A) equality assertions involving terms ADOM(A) either appear explicitly A, obtainedclosing functionality congruence terms.overall effect action parameter substitution new ABox A0 =(T, A, )[ei (A).(T, A, ) = EQ(T, A)1inNotice presence function terms action effects makes domain ABoxes obtained executing actions continuously changing general unbounded size. Notice alsopersistence assumption equalities, i.e., implicitly copy equalities holding current state new one. implies that, system evolves, acquire newinformation equalities terms, never lose information equalities already acquired.Finally, observe execution mechanism persistence/frame assumption (except equality) made. principle every move substitute whole old state, i.e., ABox,new one. hand, clear easily write effect specificationscopy big chunks old state new one. example, [P (x, y)]P (x, y) copiesentire set assertions involving role P . sense, execution mechanism adoptedpaper basic address elaboration tolerance issues typicalreasoning actions, frame problem, ramification problem qualification problem(Reiter, 2001)3 . consider irrelevant, contrary, relevantresearch issues desirable. adopt basic mechanism simplygeneral enough expose difficulties need overcome order get decidabilityverification setting.3.4 Processprocess component KAB possibly nondeterministic program uses KAB ABoxesstore (intermediate final) computation results, actions atomic instructions.ABoxes arbitrarily queried KAB TBox , updatedactions . specify process component adopt rule-based specification.Specifically, process finite set condition/action rules. condition/action ruleexpression formQ 7 ,action Q ECQ, whose free variables exactly parameters. rule expresses that, tuple condition Q holds, action actualparameters executed. Processes force execution actions constrain them:user process able choose action rules forming process allow.Moreover, processes inherit entirely states KAB knowledge component (TBoxABox) (see, e.g., Cohn & Hull, 2009).3. see also work Kowalski Sadri (2011).659fiBAGHERI H ARIRI , C ALVANESE , E G IACOMO , E ASELLIS , F ELLI , & ONTALIVillain v CharacterlivesIn v CharacterlivesIn v CityCharacter v livesIn(funct livesIn)enemy v Villainenemy v Superherodefeated v Villaindefeated v Superherodefeated v enemyalterEgo v SuperheroalterEgo v Character(funct alterEgo)0..1alterEgoenemySuperheroCharacter1..1livesInCityVillain{subset}defeatedFigure 1: KABs TBox Example 1observe adopt basic rule-based specification because, spite simplicity,able expose difficulties setting. choices also possible, particular,process could maintain state besides one KAB. long additionalstate finite, embeddable KAB itself, results would easily extend case.Example 1 Let us consider KAB K = (T, A0 , , ) describing super-heroes comics world,cities characters live. Figure 1 shows TBox renderingUML Class Diagram. relationship UML Class Diagrams Description Logicsgeneral DL-Lite particular, refer work Berardi, Calvanese, De Giacomo(2005) Calvanese, De Giacomo, Lembo, Lenzerini, Poggi, Rodrguez-Muro, Rosati(2009). dynamics domain, characters superheroes (super)villains,fight other. classic plot, superheroes help endeavors law enforcementfighting villains threatening city live in. villain reveals perpetratingnefarious purposes citys peace, consequently becomes declared enemysuperheroes living city. character lives one city time. common traitsuperheroes secret identity: superhero said alter ego character,identity common life. Hence, ABox assertion alterEgo(s, p) means superheroalter ego character p. Villains always try unmask superheroes, i.e., find secret identity,order exploit knowledge defeat them. Notice subtle difference here: usealterEgo(s, p) assertion model fact alter ego p, whereas asserting = pcapture knowledge p semantically denote individual. may includeactions like following ones:BecomeSH(p, c) : { [Character(p) livesIn(p, c) v.Villain(v) livesIn(v, c)]{Superhero(sh(p)), alterEgo(sh(p), p)},CopyAll }states exists least one villain living city c, new superhero sh(p) created,purpose protecting c. superhero p alter ego. CopyAll shortcutexplicitly copying concept role assertions new state (equality assertions always660fiD ESCRIPTION L OGIC K NOWLEDGE ACTION BASESimplicitly copied). ActionUnmask(s, p) : { [alterEgo(s, p)]CopyAll }{s = p},states superhero s, alter ego p, gets unmasked asserting equalityp (it known = p). ActionFight(v, s) : { p.[Villain(v) Character(p) alterEgo(s, p)] [s = p]CopyAll }{defeated(v, s)},states villain v fights superhero s, defeats unmasked, i.e., knownequal alter ego. ActionChallenge(v, s) :{ [Villain(v) Superhero(s) p.alterEgo(s, p) livesIn(p, sc)] [defeated(v, s)]{livesIn(v, sc), enemy(v, s)},CopyAll }states villain v challenges superhero defeated him, next livescity enemy s. ActionThreatenCity(v, c) :{ [Villain(v) Superhero(s) p.alterEgo(s, p) livesIn(p, c)]{enemy(v, s) livesIn(v, c)}CopyAll }states villain v threatens city c, becomes enemy superheroeslive c.process might include following rules:[Character(p)] [s.Superhero(s) livesIn(s, c)][Superhero(s) Character(c)][enemy(v, s)] [v 0 .defeated(v 0 , s)][Villain(v) Superhero(s)][Villain(v) City(c)] v 0 ([Villain(v 0 ) livesIn(v 0 , c)] [v = v 0 ])77777BecomeSH(p, c),Unmask(s, c),Fight(v, s),Challenge(v, s),ThreatenCity(v, c).instance, first rule states character become superhero city alreadyone, whereas last one states villain threaten city, cityanother villain (known be) distinct him/her.Notice that, execution, reasoning KB performed. instance, considerinitial ABoxA0 = { Superhero(batman), Villain(joker), alterEgo(batman, bruce),livesIn(bruce, gotham), livesIn(batman, gotham), livesIn(joker, city1) }.state, bruce batman live city, batman alter-ego bruce,known whether denote individual. Executing Challenge(joker, batman) A0 ,indeed allowed process , generates new ABox added assertions enemy(joker,batman), livesIn(joker, gotham), gotham = city1 implied functionality livesIn.661fiBAGHERI H ARIRI , C ALVANESE , E G IACOMO , E ASELLIS , F ELLI , & ONTALI4. KAB Transition Systemsemantics KABs given terms possibly infinite transition systems representpossible evolutions KAB time, actions executed according process. Noticetransition systems must equipped semantically rich states, since full KB associated them. Formally define kind transition system need follows: transitionsystem tuple form (U, T, , s0 , abox , ), where:U countably infinite set terms denoting individuals, called universe;TBox;set states;s0 initial state;abox function that, given state returns ABox associatedindividuals terms U, conforms ;transition relation pairs states.convenience,introduce active domain whole transition system, definedADOM () = ADOM (abox (s)). Also introduce (predicate) alphabet ALPH ()set concepts roles occurring co-domain abox .KAB generates transition system form execution. Formally, givenKAB K = (T, A0 , , ), define (generated) transition system K = (U, T, , s0 , abox , )follows:U formed constants function terms inductively formed startingADOM (A0 ) applying functions occurring actions ;TBox KAB;abox identity function (i.e., state simply ABox);s0 = A0 initial state;defined mutual induction smallest sets satisfying following property: , rule Q 7 , evaluate Q and, tuple returned,(T, abox (s), ) consistent w.r.t. , s0 s0 = (T, abox (s), ).Notice alphabet ALPH(K ) K simply formed set ALPH(K) conceptsroles occur K.KAB transition system K infinite tree infinitely many different ABoxesnodes, general. fact, get transition system infinite, enough performindefinitely simple action adds new terms step, e.g., action form() : { [C(x)]{C(f (x))}, CopyAll }.Hence classical results model checking (Clarke et al., 1999), developed finitetransition systems, cannot applied directly verifying KABs.662fiD ESCRIPTION L OGIC K NOWLEDGE ACTION BASES5. Verification Formalismspecify dynamic properties KABs, use first-order variant -calculus (Stirling, 2001;Park, 1976). -calculus virtually powerful temporal logic used model checkingfinite-state transition systems, able express linear time logics LTL PSL,branching time logics CTL CTL* (Clarke et al., 1999). main characteristic-calculus ability expressing directly least greatest fixpoints (predicate-transformer)operators formed using formulae relating current state next one. using fixpointconstructs one easily express sophisticated properties defined induction co-induction.reason virtually logics used verification considered fragments-calculus. Technically, -calculus separates local properties, asserted current statestates immediate successors current one, properties talking statesarbitrarily far away current one (Stirling, 2001). latter expressed usefixpoints.work, use first-order variant -calculus, allow local propertiesexpressed ECQs, time allow arbitrary first-order quantification acrossstates. Given nature ECQs used formulating local properties, first-order quantificationranges terms denoting individuals. Formally, introduce logic LA defined follows:Q | | 1 2 | x. | hi | Z | Z.,Q possibly open ECQ Z second order predicate variable (of arity 0). make usefollowing abbreviations: x. = (x.), 1 2 = (1 2 ), [] = hi,Z. = Z.[Z/Z]. formulae Z. Z. respectively denote leastgreatest fixpoint formula (seen predicate transformer Z.). usual -calculus,formulae form Z. (and Z.) must obey syntactic monotonicity w.r.t. Z,states every occurrence variable Z must within scope even numbernegation symbols. ensures least fixpoint Z. (as well greatest fixpoint Z.)always exists.semantics LA formulae defined possibly infinite transition systems formhU, T, , s0 , abox , seen above. Since LA also contains formulae individualpredicate free variables, given transition system , introduce individual variable valuationv, i.e., mapping individual variables x U, predicate variable valuation V , i.e.,mapping predicate variables Z subsets . three notions place,assign meaning formulae associating , v, V extension function ()v,V , mapsformulae subsets . Formally, extension function ()v,V defined inductively follows:(Q)v,V()v,V(1 2 )v,V(x.)v,V(hi)v,V(Z)v,V(Z.)v,V======={s | ANS (Qv, T, abox (s)) = true},\ ()v,V ,(1 )v,V (2 )v,V ,{s | t.t ADOM(abox (s)) ()v[x/t],V },000{s | .s ()v,V },V (Z),{E | ()v,V [Z/E] E}.Qv stands (boolean) ECQ obtained Q substituting free variables accordingv. Intuitively, ()v,V assigns constructs following meaning:663fiBAGHERI H ARIRI , C ALVANESE , E G IACOMO , E ASELLIS , F ELLI , & ONTALIboolean connectives expected meaning.quantification individuals done terms current ABox. Noticeterms referred later states associated ABox includeterms anymore.extension hi consists states that, state s0 transitions0 , formula holds s0 valuation v .extension [] consists states that, states s0 transition s0 ,formula holds s0 valuation v.extension Z. smallest subset E that, assigning Zextension E , resulting extension (under valuation v) contained E . is,extension Z. least fixpoint operator ()v,V [Z/E] , V [Z/E] denotespredicate valuation obtained V forcing valuation Z E.Similarly, extension Z. greatest subset E that, assigningZ extension E , resulting extension contains E . is, extensionZ.={E|E.Formally,(Z.)greatest fixpoint operator ()v,Vv,V [Z/E]()v,V [Z/E] }.closed formula, ()v,V depend v V , denote extensionsimply () . closed formula holds state () . case, write, |= . closed formula holds , denoted |= , , s0 |= . call modelchecking problem verifying whether |= holds.next example shows simple temporal properties expressed LA .Example 2 Considering KAB Example 1, easily express temporal propertiesfollowing ones.current superheroes live Gotham live Gotham forever (a formsafety):x.[Superhero(x) livesIn(x, gotham)] Z.([livesIn(x, gotham)] []Z).Eventually current superheroes unmasked (a form liveness):x.[Superhero(x)] Z.([alterEgo(x, x)] []Z).exists possible future situation current superheroes unmasked (another form liveness):x.[Superhero(x)] Z.([alterEgo(x, x)] hiZ).Along every future, always true, every superhero, exists evolutioneventually leads unmask (a form liveness holds every moment):Y.(x.[Superhero(x)] Z.([alterEgo(x, x)] hiZ)) []Y.664fiD ESCRIPTION L OGIC K NOWLEDGE ACTION BASESConsider two transition systems sharing universe predicate alphabet.say behaviorally equivalent satisfy exactly LA formulas.formally capture equivalence, make use notion bisimulation (Milner, 1971),suitably extended deal query answering KBs.Given two transition systems 1=hU, T, 1 , s01 , abox 1 , 1 2=hU, T, 2 , s02 , abox 2 , 2 sharing universe U, TBox ,ALPH (1 ) = ALPH (2 ) = , bisimulation 1 2 relation B 1 2(s1 , s2 ) B implies that:1. abox (s1 ) T, abox (s2 );2. s1 1 s01 , exists s02 s2 2 s02 (s01 , s02 ) B;3. s2 2 s02 , exists s01 s1 1 s01 (s01 , s02 ) B.say two states s1 s2 bisimilar exists bisimulation B (s1 , s2 )B. Two transition systems 1 initial state s01 2 initial state s02 bisimilar(s01 , s02 ) B. following theorem states formula evaluation LA indeed invariantw.r.t. bisimulation, equivalently check bisimilar transition systems.Theorem 4 Let 1 2 two transition systems share universe, TBox,predicate alphabet, bisimilar. Then, two states s1 1 s2 2(including initial ones) bisimilar, closed LA formulas ,s1 ()1iffs2 ()2 .Proof. proof analogous standard proof bisimulation invariance -calculus (Stirling, 2001), though taking account bisimulation, guarantees ECQs evaluatedidentically bisimilar states. Notice assumption two transition systems shareuniverse predicate alphabet makes easy compare answers queries.Making use notion bisimulation, can, example, redefine transition systemgenerated KAB K = (T, A0 , , ) maintaining bisimilarity, modifying definitionK = hU, T, , s0 , abox , given Section 4 follows.(i) modify DO() function term t0 introduced generated ABox A0current ABox4 already term (T, A) |= = t0 .(ii) ABox A0 = DO(T, abox (s), ) obtained current state logically equivalentABox abox (s00 ), already generate state s00 , generate new state,simply add s00 K .6. Verification KABsimmediate see verification KABs undecidable general easy representTuring machines using KAB. Actually using fragment capabilitiesKABs, shown next lemma.Lemma 5 Checking formulas form Z.(N (a) hiZ), N atomic conceptindividual occurring A0 , undecidable already KAB K = (T, A0 , , ) where:4. Note terms present current ABox preserved new ABox, together equalitiesterms.665fiBAGHERI H ARIRI , C ALVANESE , E G IACOMO , E ASELLIS , F ELLI , & ONTALI{First(c)}{value(c, x)}{value(c, av0 )}{cell(cr , aq0 )}{cell(n(c), aq0 ), next(c, n(c)), Last(n(c))}{Last(c)}{cell(c, #)}{cell(cr , #)}{Stop(0)}[First(c)][cell(c, #) value(c, x)][cell(c, aq ) value(c, av )][cell(c, aq ) value(c, av ) next(c, cr )][cell(c, aq ) value(c, av ) Last(c)][cell(c, #) Last(c)][cell(c, #) First(c)][cell(c, #) next(c, cr )][cell(c, aqf )]Figure 2: Effects action used encode transition (q, v, q 0 , v 0 , R) Turing Machineempty TBox,actions make use negation equality,trivial process poses restriction executability actions.Proof. Given Turing machine = hQ, , q0 , , qf , i, show construct correspondingKAB KM = (, A0 , , ) mimics behavior M. Specifically, encode haltingproblem verification problem KM . Roughly speaking, KM maintains tapestate information (current) ABox, encodes transitions actions.construction makes use tape initially contains unique cell, represented constant0, extended on-the-fly needed: cells right 0 represented function termsform n(n( (0) )), cells left 0 represented function terms formp(p( (0) )). Then, make use one constant aq state q Q, one constant avtape symbol value v , special constant #, following concepts roles:cell(c, h) models cell tape, c cell identifier, h corresponds currentstate M, head currently points c, # head currently pointc;next(cl , cr ) models relative position cells, stating cr cell immediately following cl ;value(c, v) models cell c currently contains value v, v ;First(c) Last(c) respectively denote current first cell last cell portiontape explored far.Stop(c) used detect halts.initial state KM contains unique cell definedA0 = { cell(0, aq0 ), value(0, ), First(0), Last(0) }.action component, contains action parameters transition ,process poses restriction executability actions, i.e., contains rule true 7 ()action .provide specification actions, detailing case right shift transition(q, v, q 0 , v 0 , R). corresponding action specification consists set effects shownFigure 2. first effect maintains first position tape unaltered. second third666fiD ESCRIPTION L OGIC K NOWLEDGE ACTION BASESeffects deal cell values. remain except current cell, updatedaccording transition. next three effects deal right shift Turing Machinestate. current cell next cell therefore last one, head movednext cell state change recorded there. case last cell remainssame. instead current cell last one, moving head tape must properlyextended. function n/1 used create identifier new successor cell, startingidentifier current one. Furthermore, since transition corresponds right shift onecell, first cell cells immediately following cell marked # marked #next state. Finally, last effect used identify case reached final state.marked inserting new state special assertion Stop(0).construction left shift transition done symmetrically, using function p/1 createnew predecessor cell. construction, KM satisfies conditions theorem. Observe that,transition system KM generated KM , every action corresponding every transitionexecuted ABox/state KM , since empty, actually generatesuccessor state s. However, state, (unique) action corresponds actuallyexecuted transition generate successor state containing ABox assertion formcell(c, aq ), state q M. Therefore, ABoxes/states properly correspondingconfigurations could eventually lead ABox/state KM Stop(0) holds.latter happen halts. precisely, one show induction lengthrespectively halting computation shortest path initial state KMstate Stop(0) holds, halts KM |= Z.([Stop(0)] hiZ),concludes proof.previous lemma, shows undecidability already special case, immediatelyobtain following result.Theorem 6 Verification LA formulae KABs undecidable.observe Lemma 5 uses KB constituted ABox containing conceptrole assertions, makes use conjunctive queries defining actions effects. Moreover, formula check makes use quantification all, simply seenpropositional CTL formula form EF p, expressing proposition p eventually holds alongone path.7. Verification Weakly Acyclic KABsspite Theorem 6, next introduce notable class KABs verification arbitraryLA properties decidable. so, rely syntactic restriction resembles notionweak acyclicity data exchange (Fagin et al., 2005)5 , guarantees boundedness ABoxesgenerated execution KAB and, turn, decidability verification.ready introduce notion weak acyclicity context. introduceedge-labeled directed dependency graph KAB K = (T, A0 , , ), defined follows. Nodes,called positions, obtained TBox T: node every concept name N ,two nodes every role name P , corresponding domain range P . Edges5. use original definition weak acyclicity. However, results applied also variants weakacyclicity (see discussion Section 9).667fiBAGHERI H ARIRI , C ALVANESE , E G IACOMO , E ASELLIS , F ELLI , & ONTALIVillainlivesIn,1livesIn,2Cityenemy,1CharacteralterEgo,2defeats,2*defeats,1**alterEgo,1*SuperHeroenemy,2Figure 3: Weakly acyclic dependency graph Example 1.drawn considering every effect specification [q + ] QA0 action contained, tracing values copied contribute generate new values system progresses.particular, let p position corresponding concept/role component rewriting rew (q + )q + variable x. every position p0 A0 variable x, include normaledge p p0 . every position p00 A0 function term f (~t) x ~t, includespecial edge pp00 . say K weakly-acyclic dependency graph cycle goingspecial edge.Example 3 KAB Example 1 weakly acyclic. dependency graph, shown Figure 3,contain cycle going special edges. readability, self-loops shownFigure (but present nodes), dashed edges used compactly representcontributions given rewriting queries. E.g., dashed edge form Villain Characterdenotes every outgoing edge Character, exists outgoing edge Villaintype target. Hence, w.r.t. weak acyclicity dashed edges simply replacednormal edges.ready state main result work, going prove remaindersection.Theorem 7 Verification LA properties weakly acyclic KAB decidable E XP IMEsize KAB.observe restriction imposed weak acyclicity (or variants) severe,many real cases KABs indeed weakly acyclic transformed weakly acyclic onescost redesign. Indeed, weakly acyclic KABs cannot indefinitely generate new valuesold ones, depend chain unboundedly many previous values. words,current values depend bounded number old values. unbounded systems existtheory, e.g., Turing machines, higher level processes, business process managementservice-oriented modeling, typically require boundedness practice. systematicallytransform systems weakly acyclic ones remains open issue.remainder section present proof Theorem 7. several steps:1. Normalized KAB. First introduce normalized form K KAB K, isolatescontribution equalities TBox actions effects KAB. important pointnormalizing KAB preserves weak acyclicity.668fiD ESCRIPTION L OGIC K NOWLEDGE ACTION BASES2. Normalized DO(). Then, introduce normalized version NORM () DO(), avoidsconsider equalities generating bulk set tuples used effects generatenext ABox. transition system K,NORM generated normalized versionNORM () () normalized KAB K bisimilar transition system K generated DO() K. Hence two transition systems satisfy LA formulae.3. Positive dominant. next step introduce call positive dominant K++normalized KAB K. obtained K essentially dropping equalities, negations,TBox. However K++ contains enough information positive part that,drop features, active domain transition system K++ generatedK++ overestimates active domain transition system K,NORM generatednormalized KAB K. Moreover, normalized (and hence original) KAB weaklyacyclic, positive dominant. Finally positive dominant weakly acyclicsize active domain transition system K++ polynomially bounded sizeinitial ABox, hence size active domain K,NORM . impliessize K,NORM finite exponential size initial ABox.4. Putting together. Tying results together, get claim.following, detail steps.7.1 Normalized KABGiven KAB K = (T, A0 , , ), build KAB K = (T, A0 , , ), called normalized formK, applying sequence transformations preserve semantics K producingKAB format easier study.1. view ABox partitioned part collecting concept role assertions,part collecting equality assertions. denote A6E Q former EQ(T, A)latter, closed w.r.t. (the functionality assertions in) TBox . Noticeclosure computed polynomial time size .2. K individuals appearing equality assertions ABox also occur special concept assertions form Dummy(t), concept Dummy unrelatedconcepts roles KAB. by:adding concept assertions Dummy(t) equality assertion A0appear elsewhere;adding right-hand part action effect ei concept assertion Dummy(t)equality assertion right-hand part ei ;adding action effect specification form[Dummy(x)]{Dummy(x)}.Notice that, result transformation, get ABoxes containing additionalconcept Dummy, however never queried actions effects rules formingprocess. impact transformation simply ADOM(A) ABoxes669fiBAGHERI H ARIRI , C ALVANESE , E G IACOMO , E ASELLIS , F ELLI , & ONTALIKAB transition system readily identified set terms occurringconcept role assertion (without looking equality assertions).Given ABox A, denote result two transformations, respectively add closure equalities extension Dummy.3. manipulate resulting effect specification[q + ] QA0follows:3.1. replace [q + ] Q [rew (q + )] rew (Q ) (Calvanese et al., 2007a), exploitingresults Calvanese et al. (2007b) Artale et al. (2009), guarantee that,every ECQ Q every ABox equalities closed functionalitycongruence,ANS (Q, T, A)= ANS (rew (Q), , A).3.2. replace effect specification [rew (q + )] rew (Q )A0 , resulting+Step 3.1, set effect specifications [qi ] rew (Q )A0 , one CQ qi+UCQ rew (q ).3.3. effect specification [qi+ ] rew (Q )A0 , re-express qi+ makeequalities used join terms explicit remove constants qi+ . Specifically,replace effect specification[qi++ ] q = rew (Q )A0 ,where:qi++ CQ without repeated variables obtained qi+ (i) replacingvariable x occurring qi+ , j-th occurrence x except first one, x[j] ;(ii) replacing constant c new variable xc ;VVq = = [x = x[j] ] [xc = c] (i) first conjunction contains one equality[x = x[j] ] variable x qi+ variable x[j] introduced stepabove, (ii) second conjunction contains one equality constant cqi+ .clarify latter consider following example:Example 4 Given query.[qi+ ] = [N (x) P1 (x, y) P2 (c, x)],Step 3.3 replaces [qi++ ] q = ,.qi++ = N (x) P1 (x[2] , y) P2 (xc , x[3] ),670.q = = [x = x[2] ] [x = x[3] ] [xc = c].fiD ESCRIPTION L OGIC K NOWLEDGE ACTION BASEScorrectnessStep 3.3, immediate notice [qi+ ] equivalent [qi++VV[j](x = x ) (xc = c)]. equivalence latter [qi++ ]q = consequenceconstruction Artale et al. (2009), shows query entailment presenceequalities reduced query evaluation saturating equalities w.r.t. transitivity,reflexivity, symmetry, functionality.Given action , denote action normalized above.Since transformations preserve logical equivalence (as long query Dummy),Lemma 8 DO(T, A, ) T,ALPH(K) DO(T, A, ).Also normalization KAB preserves weak acyclicity, crucial considerationlater results.Lemma 9 K weakly acyclic, also K weakly acyclic.Proof. Consider effect specification [q + ] QA0 belonging action K.contribution effect specification dependency graph G K limited CQ qiUCQ rew (q + ), set concept role assertions A0 . observeqi corresponds query qi++ K variable qi occurs exactly once. every freevariable x qi also appears A0 , every occurrence x qi itself, edge includedG. dependency graph G K, one edges appears, corresponding singleoccurrence variable x qi++ .Notice Dummy omitted dependency graph G since, definition K,Dummy occur left-hand side effects except trivial effect [Dummy(x)]{Dummy(x)}. true K, Dummy needed. Therefore, G indeed subgraphG, hence weak acyclicity G implies weak acyclicity G.7.2 Normalized DO()Next give simplified version DO(), call NORM (). start observingreformulate definition DO() given Section 3. that, first need definesuitable notion join two queries. Let q1 q2 two ECQs, may free variablescommon, let A1 A2 two ABoxes. define ANS (q1 , , A1 ) ./ ANS (q2 , , A2 )set substitutions free variables q1 q2 qi holds , Ai , i.e.,, Ai , |= qi , {1, 2}. Then, given action parameters substitution ABoxA,[(T, A, ) =APPLY (T, A, e, ),eeffect specification e : [q ++ ] q = QAPPLY (T, A, e, )[=A0 ,A0ANS (q ++ ,,A)./ANS ((q = Q ),,A)671EQ(T, A).fiBAGHERI H ARIRI , C ALVANESE , E G IACOMO , E ASELLIS , F ELLI , & ONTALIInstead, define NORM ()NORM (T, A, )=[APPLY NORM (T, A, e, ),ewhere, effect specification e : [q ++ ] q = QAPPLY NORM (T, A, e, )A0 ,[=A0EQ(T, A).ANS (q ++ ,,A6E Q )./ANS ((q = Q ),,A)Notice difference DO() NORM () latter use A6E Qinstead compute answers CQs q ++ .following lemma shows applications DO() NORM () give rise logicallyequivalent ABoxes.Lemma 10 DO(T, A, ) T,ALPH(K) NORM (T, A, ).Proof. order prove claim, enough show concept/role assertion 2 NORM (T, A, ) whose concept/role name belongs ALPH(K),(T, DO(T, A, )) |= 2 , concept/role assertion 1 DO(T, A, ) whose concept/role name belongs ALPH(K), (T, NORM (T, A, )) |= 1 . actually proveslightly stronger result:(1) ABox assertion 2 APPLY NORM (T, A, e, ), (T, APPLY(T, A, e, )) |=2 .(2) ABox assertion 1 APPLY(T, A, e, ), (T, APPLY NORM (T, A, e, )) |=1 .(1), monotonicity q ++ fact A6E Q A,[[A0containedA0 ,(ANS (q ++ ,,A6E Q )./ANS ((q = Q ),,A))(ANS (q ++ ,,A)./ANS ((q = Q ),,A))hence claim follows.(2), consider ABox assertion APPLY(T, A, e, ). definition APPLY(), knowexists effect e : [q ++ ] q = QA0 assignment free variables++=q(which include also free variables q Q ) (ANS (q ++ , , A) ./=ANS ((q Q ), , A)) A0 . Let {x1 , . . . , xn } free variables q ++ ,= {x1 t1 , . . . , xn tn , }. variable xi , let N (xi ) (unique) concept atomq ++ xi occurs (similar considerations hold xi occurs role atom). Then,either N (ti ) A6E Q , t0i , N (t0i ) A6E Q (ti = t0i ) EQ(T, A). formercase, let t00i denote ti , latter case let t00i denote t0i . Then, consider substitution0 = {x1 t001 , . . . , xn t00n , }. construction, 0 ANS (q ++ , , A6E Q ), sinceANS ((q = Q ), , A), (t00i = ti ) EQ(T, A) {1, . . . , n}, also0 ANS (q ++ , , A6E Q ) ./ ANS ((q = Q ), , A). SinceA0 ,672fiD ESCRIPTION L OGIC K NOWLEDGE ACTION BASES0 identical modulo EQ(T, A)EQ(T, A) APPLY NORM (T, A, e, ),infer (T, APPLY NORM (T, A, e, )) |= . Hence claim holds.combining Lemma 8 Lemma 10, get DO() K NORM () K behaveequivalently, starting equivalent ABoxes.Lemma 11 A1 T,ALPH(K) A2 DO(T, A1 , ) T,ALPH(K) NORM (T, A2 , ).Proof. claim direct consequence Lemma 8, Lemma 10, equivalence A1A2 , observation logical equivalence transitive.Given KAB K normalized version K, call transition system generatedway K , using NORM () K instead DO() K, normalized transition systemgenerated K, denote K,NORM .Lemma 12 Given KAB K, transition systems K K,NORM bisimilar.Proof. Let K = (U, T, , s0 , abox , ) K,NORM = (U, T, NORM , s0 , abox NORM , NORM ).define relation B NORM follows: (s1 , s2 ) B iff abox (s1 ) T,ALPH(K)abox NORM (s2 ) show B bisimulation. so, prove B closeddefinition bisimulation itself. Indeed, (s1 , s2 ) B, then:abox (s1 ) T,ALPH(K) abox (s2 ) definition.s1 s01 exists action substitution s01 =(T, abox (s1 ), ) (notice abox (s1 ) = s1 ) s01 consistent w.r.t. . letus consider s02 = NORM (T, abox (s2 ), ). Since abox (s1 ) T,ALPH(K) abox (s2 ),Lemma 11, s01 T,ALPH(K) s02 . Therefore, s02 consistent w.r.t. , hences2 NORM s02 , (s01 , s02 ) B.Similarly, s2 NORM s02 exists action substitutions02 = NORM (T, abox (s2 ), ) s02 consistent w.r.t. . let us consider s01 =(T, abox (s1 ), ). Since s2 T,ALPH(K) s1 , Lemma 11, s02 T,ALPH(K)s01 Therefore, s01 consistent w.r.t. , hence s1 s01 , and, considering equivalenceenjoys symmetry, (s01 , s02 ) B.proves claim.direct consequence lemma that, considering Bismulation InvarianceTheorem 4, faithfully check LA formulas K,NORM instead K .7.3 Positive Dominantnext step show weakly acyclic KAB K, normalized transition system K,NORMfinite. considering another transition system, behaviorally unrelatedK,NORM , hence K , whose active domain bounds active domain K,NORM .obtain transition system essentially ignoring negative information equalities.allows us refer back literature data exchange show boundedness. calltransition system positive dominant.Given normalized KAB K = (T, A0 , , ), define positive dominant K KAB6E QK+ = (, A0 , { + }, {true 7 + }).673fiBAGHERI H ARIRI , C ALVANESE , E G IACOMO , E ASELLIS , F ELLI , & ONTALIaction + without parameters effect specification constituted CopyAllone effect form6E Q[qi++ ]A0ieffect [qi++ ] qi= QA0i every action . Observe parametersactions become simply free variables + .Notice + applicable every step process trivially always allows it.resulting state always consistent, since K+ empty TBox. Moreover, equality assertionever generated. transition system K+ constituted single run, incrementallyaccumulates facts derived iterated application + increasingABox. behavior closely resembles chase tuple-generating dependencies (TGDs) dataexchange, application + corresponds parallel chase step (Deutsch, Nash, &Remmel, 2008).technical point view, notice K+ already normalized form (i.e., K+ = K+ ),DO() NORM () identical since neither equality negation considered. HenceK+ = K+ ,NORM .next lemma shows K+ preserves weak acyclicity K.Lemma 13 K weakly acyclic also positive dominant K+ weakly acyclic.Proof. claim follows fact that, construction, dependency graph G + K+equal G. Indeed, qi++ connection Ai preserved K+ . Hence, getclaim.Next show K+ weakly acyclic active domain ABoxes transitionsystem K+ polynomially bounded active domain initial ABox.Lemma 14 K+ weakly acyclic, exists polynomial P()6E Q|ADOM(K+ )| < P(|ADOM(A0 )|).Proof. observe exists strict connection execution K+ chaseset TGDs data exchange. Therefore, proof closely resembles one Fagin et al.(2005, Thm. 3.9), shown weakly acyclic TGDs, every chase sequence bounded.Let K+ = (U, , , A0 6E Q , abox , ), let G + = (V, E) dependency graph K+ ,let n = |ADOM(A0 6E Q )|. every node p V , consider incoming path (finiteinfinite) path ending p. Let rank (p) maximum number special edgesincoming path. Since K+ weakly acyclic hypothesis, G + contain cycles goingspecial edges, therefore rank (p) finite. Let r maximum among rank (pi )nodes. observe r |V |; indeed path lead node twice using specialedges, otherwise G + would contain cycle going special edges, thus breaking weakacyclicity hypothesis. Next observe partition nodes V according rank,obtaining set sets {V0 , V1 , . . . , Vr }, Vi set nodes rank i.Let us consider state obtained A0 6E Q applying action + containedK+ arbitrary number times. prove, induction i, following claim:every exists polynomial Pi total number distinct values c occurpositions Vi Pi (n).674fiD ESCRIPTION L OGIC K NOWLEDGE ACTION BASES(Base case) Consider p V0 . definition, p incoming path containing special edges.Therefore, new values stored p along run A0 6E Q A. Indeed p storevalues part initial ABox A0 6E Q . holds nodes V0 hence fixP0 (n) = n.(Inductive step) Consider p Vi , {1, . . . , r}. first kind values may storedinside p values stored inside p A0 6E Q . number valuesn. addition, value may stored p two reasons: either copiedposition p0 Vj 6= j, generated possibly new function term, built applyingeffects contain function head.first determine number fresh individuals generated function terms.possibility generating storing new value p result action reflectedpresence special edges. definition, special edge entering p must start node0p0 V0 Vi1 .P induction hypothesis, number distinct values exist pbounded H(n) = j{0,...,i1} Pj (n). Let ba maximum number special edgesenter position, positions TBox; ba bounds arity taken function termcontained . every choice ba values V0 Vi1 (one special edgeenter position), number new values generated position p bounded tf H(n)ba ,tf total number facts contained effects + . Note numberdepend data A0 6E Q . considering positions Vi , total number valuesgenerated bounded F(n) = |Vi | tf H(n)ba . Clearly, F() polynomial,tf ba determined + .count next number distinct values copied positions Vi positionsVj , j 6= i. copy represented graph normal edge going node Vjnode Vi , j 6= i. observe first normal edges start nodesV0 Vi1 , is, cannot start nodes Vj j > i. prove contradiction.Assume exists p0 p E, p Vi p0 Vj j > i. case,rank p would j > i, contradicts fact p Vi . consequence, numberdistinct values copied positions Vi bounded total number valuesV0 Vi1 , corresponds H(n) previous consideration.Putting together, define Pi (n) = n + F(n) + H(n). Since Pi () polynomial,claim proven.Notice that, claim, bounded r, constant. Hence, existsfixed polynomial P() number distinct values exist every statebounded P(n). K+ inflationary, + applied copies concept roleassertions current next state. Since K+ contains single run, P(n) boundADOM(K+ ) well.following lemma shows key feature positive dominant.Lemma 15 ADOM(K ) ADOM(K+ ).6E QProof. Let K = (T, A0 , , ) K+ = (, A0 , { + }, {true 7 + }).first observe that, every ABox K , ADOM(A) = ADOM(A6E Q ) definition K(this role special concept Dummy).675fiBAGHERI H ARIRI , C ALVANESE , E G IACOMO , E ASELLIS , F ELLI , & ONTALIshow induction construction K (U, T, 1 , A0 , abox , 1 ) K+ =6E Q(U, , 2 , A0 , abox , 2 ), state A1 1 exists state A2 2A1 6E Q A2 .6E Qbase case holds initial states A0 A0 two transition systems definition.inductive case, show that, given A1 1 A2 2 A1 6E Q A2 ,A01 1 A1 1 A01 , unique state A02 2 A2 2 A02 A01 A02 .show this, note A1 1 A01 exists action K substitution parametersA01 = NORM (T, A1 , ). Similarly, taking account + parametersalways executable K+ , A02 = DO(T, A2 , + ) = NORM (T, A2 , + ).construction K+ , effect e1 forme1 : [q ++ ] q = QA0e1 ,effect e2 + forme2 : [q ++ ]6E QA0e1 ,A0e1 6E Q obtained A0e1 removing equality assertions. induction hypothesis,A1 6E Q A2 . observing ANS ([q ++ ], , A1 6E Q ) ./ ANS ((q = Q ), , A1 )6E QANS ([q ++ ], , A2 ), obtain A0e1A0e2 , A0e1 = APPLY NORM (T, A1 , e1 , )A0e2 = APPLY(, A2 , e2 , ). Hence, get claim A01 6E Q A02 .since ABox K active domain ADOM(A) ADOM(A6E Q ) identical construction, since ADOM(K ) ADOM(K+ ) simply union activedomains generated ABoxes, get claim.7.4 Putting TogetherKAB K weakly acyclic, then, Lemma 9, normalized form K weakly acyclic welland, Lemma 13, positive dominant K+ . Hence, Lemma 14, size activedomain ADOM(K+ ) transition system K+ K+ polynomially related sizeinitial ABox.Now, Lemma15, implies also size active domain ADOM(K,NORM )transition system K K polynomially related size initial ABox. Hence, numberpossible states K finite, fact exponential size initial ABox.follows checking LA formulae K done E XP IME w.r.t. size K.Finally, Lemma 12, K K bisimilar, Bisimulation Invariance Theorem 4,K K satisfy exactly LA formulae. Hence, check LA formula Ksufficient check K , done E XP IME. concludes proofTheorem 7.8. Related Workprovide detailed review work related framework results presentedprevious sections.676fiD ESCRIPTION L OGIC K NOWLEDGE ACTION BASES8.1 Combining Description Logics Temporal Logicswork deeply related research studies combinations description logics temporal logics. Indeed, actions progress knowledge time and, although temporal logicsmention actions, easily used describing progression mechanisms, including transition systems (see, e.g., Clarke et al., 1999; Calvanese, De Giacomo, & Vardi, 2002).research mostly explored combination standard description logics standardtemporal logics level models, certainly natural form combinationlogical point view. Technically, form combination gives rise combined logictwo-dimensional semantics, one dimension time DL domain(Schild, 1993; Wolter & Zakharyaschev, 1999b, 1999a; Gabbay et al., 2003). Unfortunately,computational point view, form combination suffers key undecidability result,makes fragile many practical purposes: possibility specifying rolespreserve extension time (the called rigid roles) causes undecidability6 . Referringdomain interest Example 1, would result, example, undecidabilitytheories specify instance Character livesIn City forever. Moreover,undecidability result already holds concept satisfiability w.r.t. fixed TBox (i.e.,TBox axioms must hold time points), without ABoxes, single rigid role (Wolter& Zakharyaschev, 1999b, 1999a; Gabbay et al., 2003). is, holds reasoning servicemuch simpler conjunctive query answering (Calvanese, De Giacomo, & Lenzerini, 2008),even fixed TBox data (no ABox assertions, hence individual terms) onesimplest kinds temporal formulae, namely forever something true (safety) (Clarke et al.,1999).Decidability regained by: (i) dropping TBoxes altogether, decision problemstill hard non-elementary time (Gabbay et al., 2003); (ii) allowing temporal operatorsconcepts (Schild, 1993; Artale & Franconi, 1998, 2005; Gutierrez-Basulto, Jung, & Lutz, 2012;Jamroga, 2012), case complexity depends crucially description logic; (iii) allowing temporal operators TBox ABox assertions (Lutz, Wolter, & Zakharyaschev,2008; Baader et al., 2012). fact cases (ii) (iii) mixed (Baader & Laux, 1995; Wolter& Zakharyaschev, 1998).Allowing temporal operators assertions (case (iii) above), tightly relatedfunctional approach adopted paper: fact admit temporal operators frontassertions allows us consider temporal models whose time points actually sets modelsdescription logic assertions. Hence keeps temporal component distinct descriptionlogic one, exactly here. particular, results Baader et al. (2012) directlycompared ours. Apart obvious differences formalism used, one key point getdecidability individual terms mentioned ABox assertions fixed priori.possible that, adapting techniques presented here, results could extended allowfunctions denoting terms, hence allowing adding fresh individual terms temporalevolution.6. lose decidability, suffices able specify/verify persistence binary predicates/roles, allowsone build infinite grid hence encode Turing-machine computation (Robinson, 1971; van Emde Boas,1997).677fiBAGHERI H ARIRI , C ALVANESE , E G IACOMO , E ASELLIS , F ELLI , & ONTALI8.2 Combining Description Logics ActionsSomehow hampered undecidability results mentioned beginning section, alsocombinations description logics action theories studied years. particular,Liu, Lutz, Milicic, Wolter (2006b, 2006a) study combinations description logics actiontheories level models, w.r.t. two classical problems reasoning actions,namely projection executability. problems require explicitly give sequenceactions check property resulting final state (projection), check executabilitysequence actions, comes certain precondition (Reiter, 2001).sophisticated temporal properties (in particular, forever something true mentioned above) wouldlead undecidability. way, notice undecidability result also deeply questionscomputational point view possibility adding (sound complete) automated reasoningcapabilities proposals OWL-S (Semantic Markup Web Services) (Martin, Paolucci,McIlraith, Burstein, McDermott, McGuinness, Parsia, Payne, Sabou, Solanki, Srinivasan, & Sycara,2004).Possibly first proposal based implicitly functional view KB pioneeringwork De Giacomo, Iocchi, Nardi, Rosati (1999), adopts epistemic description logic(based certain answers) combined action formalism describe routines mobilerobot. Again, one important point individual terms bounded fixed priori.functional view approach first spelled Calvanese, De Giacomo, Lenzerini,Rosati (2007), Calvanese et al. (2011). work, projection executabilitystudied, however distinction KB states actions (there specifiedupdates), framework gives rise single transition system whose states labeledKBs (in fact TBox fixed ABox changes state state). However, again,individual terms considered fixed priori hence resulting transition system finite.So, although studied work, sophisticated forms temporal properties proposedreadily verifiable setting. Interestingly, apart KBs action, workalso Golog-like programs considered. programs whose atomic actions definedaction formalism, combined using (usual less usual) programming constructs,sequence, while-loop, if-then-else, nondeterministic pick value (Levesque, Reiter,Lesperance, Lin, & Scherl, 1997; De Giacomo, Lesperance, & Levesque, 2000). importantcharacteristic programs finite number control states (noticememory storage programs kept action theory, KB case). Althoughscope paper, finiteness allows easily extending results programwell.interesting alternative way combine description logics reasoning actionsone reported Gu Soutchanski (2010). There, description logics KB7 used specialFOL theory describing initial situation situation calculus basic action theory (Reiter, 2001).Notice result, TBox assertions act state constraints (Lin & Reiter, 1994),would lead undecidability discussed (Wolter & Zakharyaschev, 1999b, 1999a; Gabbayet al., 2003), fact essentially persist way actions.7. actually mainly focus concepts description logic includes universal role, allowsone express TBox assertions concepts (Baader et al., 2003).678fiD ESCRIPTION L OGIC K NOWLEDGE ACTION BASES8.3 Description Logics UpdateObserve effects action setting seen basic form update previousstate (Katsuno & Mendelzon, 1991). Although mechanism sidesteps semantic computational difficulties description logic KB update (Liu et al., 2006b; De Giacomo, Lenzerini, Poggi,& Rosati, 2009; Calvanese, Kharlamov, Nutt, & Zheleznyakov, 2010; Lenzerini & Savo, 2012)simply rejecting execution actions would lead inconsistent state. Adopting properforms update setting interesting issue future research.8.4 Artifacts Data-Aware Processeswork also closely related research verification artifact-centric business processes(Nigam & Caswell, 2003; Bhattacharya et al., 2007). Artifact-centric approaches model businessprocesses giving equal importance control-flow perspective data interest.artifact typically represented tuple schema, models artifact type, togetherset actions/services specify information maintained artifactmanipulated time. action usually represented terms pre- post-conditionsrespectively used determine action eligible execution, relatecurrent artifact state successor state obtained action execution. Pre- postconditions modeled first-order formulae, post-conditions employ existentially quantifiedvariables account external inputs environment. Differently KABs,approaches targeting artifact-centric processes assume complete information data, usingrelational database maintain artifacts information. paper, aim worksverify whether relational artifact-centric process meets temporal/dynamic property,formalized using first-order variants branching linear temporal logics.work Deutsch et al. (2009), infinite domain artifacts database equippeddense linear order, mentioned pre-conditions, post-conditions, properties.Runs receive unbounded external input infinite domain. Decidability verificationachieved avoiding branching time properties, restricting formulae used specifypre-, post-conditions properties. particular, approach refers read-only read-writedatabase relations differently, querying latter checking whether contain given tuple constants. authors show restriction tight, integrity constraints cannotadded framework, since even single functional dependency leads undecidabilityverification. Damaggio et al. (2011) extend approach disallowing read-write relations,allows extension decidability result integrity constraints expressed embeddeddependencies terminating chase, decidable arithmetic. major differenceapproach, concepts KAB considered read-write relations,arbitrarily queried determine progression system. Differently works,Belardinelli et al. (2011) consider first-order variant CTL quantification across statesverification formalism. framework supports incorporation new values external environment parameters actions; corresponding execution semantics considerspossible actual values, thus leading infinite-state transition systems. decidability verification, authors show that, assumption state system(constituted union artifacts relational instances) bounded active domain, possible construct faithful abstract transition system which, differently original one,finite number states. Belardinelli, Lomuscio, Patrizi (2012) improve results Belar679fiBAGHERI H ARIRI , C ALVANESE , E G IACOMO , E ASELLIS , F ELLI , & ONTALIdinelli et al. (2011) introducing semantic property uniformity which, roughly speaking,says transition system representing execution process study abledistinguish among states constants patterns data. assumptions uniformity state boundedness, decidability verification achieved richerlogic, namely CTL quantification across states, interpreted active domain semantics.notion state boundedness also adopted independently developed frameworkBagheri Hariri, Calvanese, De Giacomo, Deutsch, Montali (2012, 2013), first-ordervariants -calculus, similar one considered here, considered. There, beside differencesway data external information modeled, sufficient syntactic conditions guarantee state boundedness proposed. works developed within relational databasesetting, extend trivially systems actions change DL knowledge bases.connection data-/artifact-centric business processes data exchange exploit paper first established Cangialosi et al. (2010), De Giacomo, De Masellis,Rosati (2012). transition relation described terms TGDs, mapcurrent state, represented relational database instance, next one. Null values usedmodel incorporation new, unknown data system. process evolution essentiallyform chase. suitable weak acyclicity conditions chase terminates, guaranteeing,turn, system finite-state. Decidability shown first-order -calculus withoutfirst-order quantification across states. approach extended Bagheri Hariri et al. (2011),TGDs replaced actions rule-based process follow structureKAB action component. revised framework, values imported external environment represented uninterpreted function terms, play role nullswork Cangialosi et al. (2010), De Giacomo et al. (2012). Since Bagheri Hariri et al.(2011), Cangialosi et al. (2010), De Giacomo et al. (2012) rely purely relational setting, choice leads ad-hoc interpretation equality, null value/function termconsidered equal itself. Differently works, allow sophisticatedschema constraints, i.e., TBox itself, provide time fine-grained treatmentequality, individuals inferred equal due application schemaconstraints and/or execution action. treatment equality differentiates workalso one Bagheri Hariri, Calvanese, De Giacomo, De Masellis (2011), introduces preliminary version framework presented, UNA assumed equalityconsidered. specifically, Bagheri Hariri et al. (2011) propose semantic artifactsmeans represent artifacts corresponding processes higher level abstraction relational artifacts, representing artifact data semantically rich knowledge base operatingincomplete information. KABs constitute general framework, seamlesslycustomized account semantic artifacts. major difference work Bagheri Haririet al. (2011) also constituted verification formalism. particular, works focusform -calculus ECQs used query states system, Bagheri Hariri et al.(2011) support quantification across states, done here.Calvanese et al. (2012) investigate framework data-centric processes mixes approach proposed Bagheri Hariri et al. (2013) relational artifacts notion knowledgebases used here. particular, semantically-governed data-aware processes introducedmechanism model dynamic system working relational database, providingtime conceptual representation manipulated data terms DL-Lite knowledge base.relying ontology-based data access (Calvanese et al., 2009), declarative mappings used680fiD ESCRIPTION L OGIC K NOWLEDGE ACTION BASESconnect knowledge base underlying relational database. Differently KABs,system evolves relational layer, knowledge base used understand ultimatelygovern execution higher level abstraction.observe results presented fully subsume Bagheri Hariri et al. (2011),underlying description logic OWL 2 QL. one hand, remove possibilityasserting functionality roles knowledge component, equating individuals result action action component, precisely obtain setting presented Bagheri Haririet al. (2011). hand, frameworks established complexity upper boundssame.9. Conclusionspaper studied verification knowledge action bases, dynamic systemsconstituted knowledge base, expressed description logics, action specificationchanges knowledge base time. obtained interesting decidability resultrelying notion weak acyclicity, based connection theory chase TGDsrelational databases.work, used original notion weak acyclicity. However, easy adoptadvanced forms acyclicity, since results depend ability finding finitebound number distinct function terms generated (when applying chase).majority approaches adopt forms weak-acyclicity focus databases (Marnette &Geerts, 2010; Meier, Schmidt, Wei, & Lausen, 2010), Cuenca Grau, Horrocks, Krotzsch, Kupke,Magka, Motik, Wang (2012) investigate sophisticated forms acyclicity contextknowledge bases without UNA. results thus seamlessly applied KABs. Interestingly,manage impact equalities setting without UNA, resort singularization technique presented Marnette (2009), closely resembles normalization KABs introducedSection 7.Weak acyclicity allows us gain decidability bounding number distinct functionterms occur transition system. alternative approach gain decidability boundnumber distinct terms occurring ABox assertions state. Variants notion stateboundedness proposed recently contexts (Belardinelli et al., 2012; De Giacomo,Lesperance, & Patrizi, 2012; Bagheri Hariri et al., 2013). great interest exploreapproach setting presented actions acting description logic knowledge base.observe decidability result (as well ones commented Section 8),comes algorithm verification exponential size initial ABox. precludes direct application techniques large-scale systems, without careful analysismodularized small units verified (almost) separately. importantdirection investigation.Acknowledgmentsresearch partially supported EU ICT Collaborative Project ACSI(Artifact-Centric Service Interoperation), grant agreement n. FP7-257593, large-scaleintegrating project (IP) Optique (Scalable End-user Access Big Data), grant agreement n. FP7318338.681fiBAGHERI H ARIRI , C ALVANESE , E G IACOMO , E ASELLIS , F ELLI , & ONTALIReferencesArtale, A., Calvanese, D., Kontchakov, R., & Zakharyaschev, M. (2009). DL-Lite familyrelations. J. Artificial Intelligence Research, 36, 169.Artale, A., & Franconi, E. (1998). temporal description logic reasoning actionsplans. J. Artificial Intelligence Research, 9, 463506.Artale, A., & Franconi, E. (2005). Temporal description logics. Gabbay, D., Fisher, M., & Vila, L.(Eds.), Handbook Temporal Reasoning Artificial Intelligence, Foundations ArtificialIntelligence. Elsevier.Baader, F., Calvanese, D., McGuinness, D., Nardi, D., & Patel-Schneider, P. F. (Eds.). (2003).Description Logic Handbook: Theory, Implementation Applications. Cambridge University Press.Baader, F., Ghilardi, S., & Lutz, C. (2012). LTL description logic axioms. ACM Trans.Computational Logic, 13(3), 21:121:32.Baader, F., & Laux, A. (1995). Terminological logics modal operators. Proc. 14thInt. Joint Conf. Artificial Intelligence (IJCAI95), pp. 808814.Bagheri Hariri, B., Calvanese, D., De Giacomo, G., & De Masellis, R. (2011). Verificationconjunctive-query based semantic artifacts. Proc. 24th Int. Workshop Description Logic (DL 2011), Vol. 745 CEUR Electronic Workshop Proceedings, http://ceur-ws.org/.Bagheri Hariri, B., Calvanese, D., De Giacomo, G., De Masellis, R., & Felli, P. (2011). Foundationsrelational artifacts verification. Proc. 9th Int. Conference Business ProcessManagement (BPM 2011), Vol. 6896 Lecture Notes Computer Science, pp. 379395.Springer.Bagheri Hariri, B., Calvanese, D., De Giacomo, G., Deutsch, A., & Montali, M. (2012). Verification relational data-centric dynamic systems external services. Corr technical report arXiv:1203.0024, arXiv.org e-Print archive. Available http://arxiv.org/abs/1203.0024.Bagheri Hariri, B., Calvanese, D., De Giacomo, G., Deutsch, A., & Montali, M. (2013). Verificationrelational data-centric dynamic systems external services. Proc. 32nd ACMSIGACT SIGMOD SIGART Symp. Principles Database Systems (PODS 2013).Bao, J., et al. (2012). OWL 2 Web Ontology Language document overview (second edition). W3CRecommendation, World Wide Web Consortium. Available http://www.w3.org/TR/owl2-overview/.Belardinelli, F., Lomuscio, A., & Patrizi, F. (2011). Verification deployed artifact systems via dataabstraction. Proc. 9th Int. Joint Conf. Service Oriented Computing (ICSOC 2011),Vol. 7084 Lecture Notes Computer Science, pp. 142156. Springer.Belardinelli, F., Lomuscio, A., & Patrizi, F. (2012). abstraction technique verificationartifact-centric systems. Proc. 13th Int. Conf. Principles KnowledgeRepresentation Reasoning (KR 2012), pp. 319328.Berardi, D., Calvanese, D., & De Giacomo, G. (2005). Reasoning UML class diagrams. ArtificialIntelligence, 168(12), 70118.682fiD ESCRIPTION L OGIC K NOWLEDGE ACTION BASESBhattacharya, K., Gerede, C., Hull, R., Liu, R., & Su, J. (2007). Towards formal analysis artifactcentric business process models. Proc. 5th Int. Conference Business ProcessManagement (BPM 2007), Vol. 4714 Lecture Notes Computer Science, pp. 288234.Springer.Burkart, O., Caucal, D., Moller, F., & Steffen, B. (2001). Verification infinite structures..Handbook Process Algebra. Elsevier Science.Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., Poggi, A., Rodrguez-Muro, M., &Rosati, R. (2009). Ontologies databases: DL-Lite approach. Tessaris, S., & Franconi, E. (Eds.), Reasoning Web. Semantic Technologies Informations Systems 5th Int.Summer School Tutorial Lectures (RW 2009), Vol. 5689 Lecture Notes Computer Science, pp. 255356. Springer.Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., & Rosati, R. (2007a). EQL-Lite: Effective first-order query processing description logics. Proc. 20th Int. Joint Conf.Artificial Intelligence (IJCAI 2007), pp. 274279.Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., & Rosati, R. (2007b). Tractable reasoning efficient query answering description logics: DL-Lite family. J. AutomatedReasoning, 39(3), 385429.Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., & Rosati, R. (2013). Data complexityquery answering description logics. Artificial Intelligence, 195, 335360.Calvanese, D., De Giacomo, G., Lembo, D., Montali, M., & Santoso, A. (2012). Ontology-basedgovernance data-aware processes. Proc. 6th Int. Conf. Web Reasoning RuleSystems (RR 2012), Vol. 7497 Lecture Notes Computer Science, pp. 2541. Springer.Calvanese, D., De Giacomo, G., & Lenzerini, M. (2008). Conjunctive query containment answering description logics constraints. ACM Trans. Computational Logic, 9(3),22.122.31.Calvanese, D., De Giacomo, G., Lenzerini, M., & Rosati, R. (2007). Actions programs description logic ontologies. Proc. 20th Int. Workshop Description Logic (DL 2007),Vol. 250 CEUR Electronic Workshop Proceedings, http://ceur-ws.org/, pp. 2940.Calvanese, D., De Giacomo, G., Lenzerini, M., & Rosati, R. (2011). Actions programsdescription logic knowledge bases: functional approach. Lakemeyer, G., & McIlraith,S. A. (Eds.), Knowing, Reasoning, Acting: Essays Honour Hector Levesque. CollegePublications.Calvanese, D., De Giacomo, G., & Montali, M. (2013). Foundations data aware process analysis:database theory perspective. Proc. 32nd ACM SIGACT SIGMOD SIGART Symp.Principles Database Systems (PODS 2013).Calvanese, D., De Giacomo, G., & Vardi, M. Y. (2002). Reasoning actions planningLTL action theories. Proc. 8th Int. Conf. Principles Knowledge Representation Reasoning (KR 2002), pp. 593602.Calvanese, D., Kharlamov, E., Nutt, W., & Zheleznyakov, D. (2010). Updating ABoxes DL-Lite.Proc. 4th Alberto Mendelzon Int. Workshop Foundations Data Management683fiBAGHERI H ARIRI , C ALVANESE , E G IACOMO , E ASELLIS , F ELLI , & ONTALI(AMW 2010), Vol. 619 CEUR Electronic Workshop Proceedings, http://ceur-ws.org/, pp. 3.13.12.Cangialosi, P., De Giacomo, G., De Masellis, R., & Rosati, R. (2010). Conjunctive artifact-centricservices. Proc. 8th Int. Joint Conf. Service Oriented Computing (ICSOC 2010),Vol. 6470 Lecture Notes Computer Science, pp. 318333. Springer.Clarke, E. M., Grumberg, O., & Peled, D. A. (1999). Model checking. MIT Press, Cambridge,MA, USA.Cohn, D., & Hull, R. (2009). Business artifacts: data-centric approach modeling businessoperations processes. Bull. IEEE Computer Society Technical Committee DataEngineering, 32(3), 39.Cuenca Grau, B., Horrocks, I., Krotzsch, M., Kupke, C., Magka, D., Motik, B., & Wang, Z. (2012).Acyclicity conditions application query answering description logics.Proc. 13th Int. Conf. Principles Knowledge Representation Reasoning(KR 2012), pp. 243253.Damaggio, E., Deutsch, A., & Vianu, V. (2011). Artifact systems data dependenciesarithmetic. Proc. 14th Int. Conf. Database Theory (ICDT 2011), pp. 6677.De Giacomo, G., De Masellis, R., & Rosati, R. (2012). Verification conjunctive artifact-centricservices. Int. J. Cooperative Information Systems, 21(2), 111139.De Giacomo, G., Iocchi, L., Nardi, D., & Rosati, R. (1999). theory implementationcognitive mobile robots. J. Logic Computation, 9(5), 759785.De Giacomo, G., Lenzerini, M., Poggi, A., & Rosati, R. (2009). instance-level update erasure description logic ontologies. J. Logic Computation, Special Issue OntologyDynamics, 19(5), 745770.De Giacomo, G., Lesperance, Y., & Levesque, H. J. (2000). ConGolog, concurrent programminglanguage based situation calculus. Artificial Intelligence, 121(12), 109169.De Giacomo, G., Lesperance, Y., & Patrizi, F. (2012). Bounded situation calculus action theoriesdecidable verification. Proc. 13th Int. Conf. Principles KnowledgeRepresentation Reasoning (KR 2012), pp. 467477.Deutsch, A., Hull, R., Patrizi, F., & Vianu, V. (2009). Automatic verification data-centric businessprocesses. Proc. 12th Int. Conf. Database Theory (ICDT 2009), pp. 252267.Deutsch, A., Nash, A., & Remmel, J. B. (2008). chase revisited. Proc. 27th ACMSIGACT SIGMOD SIGART Symp. Principles Database Systems (PODS 2008), pp. 149158.Fagin, R., Kolaitis, P. G., Miller, R. J., & Popa, L. (2005). Data exchange: Semantics queryanswering. Theoretical Computer Science, 336(1), 89124.Gabbay, D., Kurusz, A., Wolter, F., & Zakharyaschev, M. (2003). Many-dimensional Modal Logics:Theory Applications. Elsevier Science Publishers.Gu, Y., & Soutchanski, M. (2010). description logic based situation calculus. Ann. MathematicsArtificial Intelligence, 58(1-2), 383.684fiD ESCRIPTION L OGIC K NOWLEDGE ACTION BASESGutierrez-Basulto, V., Jung, J. C., & Lutz, C. (2012). Complexity branching temporal descriptionlogics. Proc. 20th Eur. Conf. Artificial Intelligence (ECAI 2012), pp. 390395.Jamroga, W. (2012). Concepts, agents, coalitions alternating time. Proc. 20th Eur.Conf. Artificial Intelligence (ECAI 2012), pp. 438443.Katsuno, H., & Mendelzon, A. (1991). difference updating knowledge baserevising it. Proc. 2nd Int. Conf. Principles Knowledge RepresentationReasoning (KR91), pp. 387394.Kowalski, R. A., & Sadri, F. (2011). Abductive logic programming agents destructivedatabases. Ann. Mathematics Artificial Intelligence, 62(12), 129158.Lenzerini, M., & Savo, D. F. (2012). Updating inconsistent description logic knowledge bases.Proc. 20th Eur. Conf. Artificial Intelligence (ECAI 2012), pp. 516521.Levesque, H. J., Reiter, R., Lesperance, Y., Lin, F., & Scherl, R. (1997). GOLOG: logic programming language dynamic domains. J. Logic Programming, 31, 5984.Levesque, H. J. (1984). Foundations functional approach knowledge representation. ArtificialIntelligence, 23, 155212.Limonad, L., De Leenheer, P., Linehan, M., Hull, R., & Vaculin, R. (2012). Ontology dynamicentities. Proc. 31st Int. Conf. Conceptual Modeling (ER 2012).Lin, F., & Reiter, R. (1994). State constraints revisited. J. Logic Programming, 4(5), 655678.Liu, H., Lutz, C., Milicic, M., & Wolter, F. (2006a). Reasoning actions using descriptionlogics general TBoxes. Proc. 10th Eur. Conference Logics ArtificialIntelligence (JELIA 2006), Vol. 4160 Lecture Notes Computer Science. Springer.Liu, H., Lutz, C., Milicic, M., & Wolter, F. (2006b). Updating description logic ABoxes. Proc.10th Int. Conf. Principles Knowledge Representation Reasoning (KR 2006),pp. 4656.Lutz, C., Wolter, F., & Zakharyaschev, M. (2008). Temporal description logics: survey. Proc.15th Int. Symp. Temporal Representation Reasoning (TIME 2008), pp. 314.Marnette, B. (2009). Generalized schema-mappings: termination tractability. Proc.28th ACM SIGACT SIGMOD SIGART Symp. Principles Database Systems(PODS 2009), pp. 1322.Marnette, B., & Geerts, F. (2010). Static analysis schema-mappings ensuring oblivious termination. Proc. 13th Int. Conf. Database Theory (ICDT 2010), pp. 183195.Martin, D., Paolucci, M., McIlraith, S., Burstein, M., McDermott, D., McGuinness, D., Parsia, B.,Payne, T., Sabou, M., Solanki, Srinivasan, N., & Sycara, K. (2004). Bringing semanticsweb services: OWL-S approach. Proc. 1st Int. Workshop Semantic WebServices Web Process Composition (SWSWPC 2004).Meier, M., Schmidt, M., Wei, F., & Lausen, G. (2010). Semantic query optimization presencetypes. 111-122 (Ed.), Proc. 29th ACM SIGACT SIGMOD SIGART Symp.Principles Database Systems (PODS 2010).Meyer, A., Smirnov, S., & Weske, M. (2011). Data business processes. EMISA Forum, 31(3),531.685fiBAGHERI H ARIRI , C ALVANESE , E G IACOMO , E ASELLIS , F ELLI , & ONTALIMilner, R. (1971). algebraic definition simulation programs. Proc. 2nd Int.Joint Conf. Artificial Intelligence (IJCAI71), pp. 481489.Motik, B., Cuenca Grau, B., Horrocks, I., Wu, Z., Fokoue, A., & Lutz, C. (2012). OWL 2 Web Ontology Language profiles (second edition). W3C Recommendation, World Wide Web Consortium. Available http://www.w3.org/TR/owl2-profiles/.Nigam, A., & Caswell, N. S. (2003). Business artifacts: approach operational specification.IBM Systems Journal, 42(3), 428445.Park, D. M. R. (1976). Finiteness Mu-ineffable. Theoretical Computer Science, 3(2), 173181.Poggi, A., Lembo, D., Calvanese, D., De Giacomo, G., Lenzerini, M., & Rosati, R. (2008). Linkingdata ontologies. J. Data Semantics, X, 133173.Reiter, R. (2001). Knowledge Action: Logical Foundations Specifying ImplementingDynamical Systems. MIT Press.Robinson, R. (1971). Undecidability nonperiodicity tilings plane. Inventiones Math.,12, 177209.Rosati, R., & Franconi, E. (2012). Generalized ontology-based production systems. Proc.13th Int. Conf. Principles Knowledge Representation Reasoning (KR 2012), pp.435445. AAAI Press.Schild, K. (1993). Combining terminological logics tense logic. Proc. 6th PortugueseConf. Artificial Intelligence (EPIA93), Vol. 727 Lecture Notes Computer Science,pp. 105120. Springer.Stirling, C. (2001). Modal Temporal Properties Processes. Springer.van Ditmarsch, H., van der Hoek, W., & Kooi, B. (2007). Dynamic epistemic logic. Springer.van Emde Boas, P. (1997). convenience tilings. Sorbi, A. (Ed.), Complexity, Logic,Recursion Theory, Vol. 187 Lecture Notes Pure Applied Mathematics, pp. 331363.Marcel Dekker Inc.Vianu, V. (2009). Automatic verification database-driven systems: new frontier. Proc.12th Int. Conf. Database Theory (ICDT 2009), pp. 113.Wolter, F., & Zakharyaschev, M. (1998). Satisfiability problem description logics modaloperators. Proc. 6th Int. Conf. Principles Knowledge RepresentationReasoning (KR98), pp. 512523.Wolter, F., & Zakharyaschev, M. (1999a). Modal description logics: Modalizing roles. FundamentaInformaticae, 39(4), 411438.Wolter, F., & Zakharyaschev, M. (1999b). Temporalizing description logic. Gabbay, D., &de Rijke, M. (Eds.), Frontiers Combining Systems, pp. 379402. Studies Press/Wiley.686fiJournal Artificial Intelligence Research 46 (2013) 303341Submitted 09/12; published 03/13Boolean Equi-propagation Concise Efficient SATEncodings Combinatorial ProblemsAmit MetodiMichael Codishamit.metodi@gmail.commcodish@cs.bgu.ac.ilDepartment Computer ScienceBen-Gurion University Negev, IsraelPeter J. Stuckeypjs@csse.unimelb.edu.auDepartment Computer Science Software EngineeringNICTA Victoria LaboratoryUniversity Melbourne, AustraliaAbstractpresent approach propagation-based SAT encoding combinatorial problems, Boolean equi-propagation, constraints modeled Boolean functionspropagate information equalities Boolean literals. informationapplied simplify CNF encoding constraints. key factor consideringsmall fragment constraint model one time enables us apply stronger,even complete, reasoning detect equivalent literals fragment. detected,equivalences apply simplify entire constraint model facilitate reasoningfragments. Equi-propagation combination partial evaluation constraint simplification provide foundation powerful approach SAT-based finitedomain constraint solving. introduce tool called BEE (Ben-Gurion Equi-propagationEncoder) based ideas demonstrate variety benchmarks approach leads considerable reduction size CNF encodings subsequentspeed-ups SAT solving times.1. Introductionrecent years, Boolean SAT solving techniques improved dramatically. Todays SATsolvers considerably faster able manage larger instances yesterdays. Moreover, encoding modeling techniques better understood increasingly innovative.SAT currently applied solve wide variety hard practical combinatorial problems, often outperforming dedicated algorithms. general idea encode (typically,NP) hard problem instance, , Boolean formula, , satisfying assignments correspond solutions . Given encoding, SAT solverapplied solve .Tailgating success SAT technology variety tools appliedspecify compile problem instances corresponding SAT instances. generalobjective tools facilitate process providing high-level descriptions(constraint) problem hand solved. Typically, constraint-based modelinglanguage introduced used model instances. Drawing analogy programming languages, given description, compiler provide low-level executablec2013AI Access Foundation. rights reserved.fiMetodi, Codish, & Stuckeyunderlying machine. Namely, context, formula underlying SATSMT solver.example, Cadoli Schaerf (2005) introduce NP-SPEC, logic-based specificationlanguage allows specifying combinatorial problems declarative way. coresystem component translates specifications CNF formula. Similarly Sugar(Tamura, Taga, Kitagawa, & Banbara, 2009) SAT-based constraint solver. solvefinite domain constraint satisfaction problem first modeled constraint language(also called Sugar) encoded CNF formula solved using MiniSATsolver (Een & Sorensson, 2003). MiniZinc (Nethercote, Stuckey, Becket, Brand, Duck, &Tack, 2007) constraint modeling language compiled variety solverslow-level target language FlatZinc exist many solvers. particular,FlatZinc instances solved fzntini (Huang, 2008) encoding CNFfzn2smt encoding SMT-LIB (Barrett, Stump, & Tinelli, 2010).Simplifying CNF formulae prior application SAT solving utmostimportance wide range techniques applied achievegoal. See example work Li (2003), Een Biere (2005), Heule, Jarvisalo,Biere (2011), Manthey (2012), references therein work.techniques exhibit clear trade-off amount simplification obtainedtime requires. Moreover, stronger techniques become prohibitive SAT modelinvolves hundreds thousands variables millions clauses. CNF simplificationtools, time limits simplification techniques imposed and/or approximations used.paper takes new approach CNF simplification. Typically, CNF random collection clauses, rather structure derived application specificproblem domain. SAT solving applied encode solve finite domain constraintproblems, original constraint model manifest structure. Usually, constraints discarded encoded CNF. advocate maintaining constraintsprovides important structural information applied drive process CNFsimplification. specific, constraints model induce partitioning CNFencoding conjunction sub-formulae call portions.novelty approach CNF simplification instead considering CNFwhole, assume partitioned conjunction smaller portions.simplification repeatedly applied individual portions. facilitates propagationbased process simplification one portion propagates informationportions information may trigger simplification portions.portions typically much smaller entire CNF effectively applystronger simplification algorithms. introduce notion equi-propagation. Similarunit propagation inferring unit clauses applied simplifyCNF formulae, equi-propagation inferring equational consequences literals(and Boolean constants).wide body research CNF simplification applied implementequi-propagation sometimes called equivalent literal substitution, exampleGelder (2005). Techniques typically involve binary clause based simplifications using, amongothers, hyper binary resolution binary implication graphs. See example, workHeule et al. (2011) references therein. guiding principle works304fiBoolean Equi-propagationtechniques must simple efficient prohibitive size CNFmust apply.approach different focus far richer forms inference even relatedCNF structure formula. one extreme apply complete equi-propagationdetects equivalences implied formula. Clearly complete equi-propagationNP-hard. However, complete equi-propagators feasible apply smallportions formula. complete equi-propagation slow consider ad-hoctechniques. forms equi-propagation common drivenCNF structure (e.g. binary clauses) rather underlying constraint structureCNF was, being, generated.rest paper structured follows. Section 2 introduces modeling languagefinite domain constraints consists 5 constraint constructs sufficientillustrate contribution paper. argue constraints model inducenatural partition CNF encoding smaller portions partitionused drive simplification CNF encoding. Section 3 presents equi-propagationfirst ingredient contribution. Equi-propagation learning information apply simplify CNF encodings. Section 4 describes practical basisimplementing equi-propagation. Section 5 introduces second ingredient: partial evaluation. Given information derived using equi-propagation, partial evaluation appliessimplify constraints particular remove Boolean variables CNFencodings. Section 6 describes tool, called BEE (Metodi & Codish, 2012) (Ben-GurionEqui-propagation Encoder) based equi-propagation partial evaluation.introduce full constraint language similar Sugar subsetFlatZinc relevant finite domain constraint problems. also spell specialtreatment all-different constraint BEE. Section 7 demonstrates applicationBEE. Section 8 presents experimental evaluation. Finally Section 9 presentsconclusion.paper extends earlier work presented Metodi, Codish, Lagoon, Stuckey(2011), first introduced equi-propagation, also BEE tool paper (Metodi &Codish, 2012). BEE tool available download (Metodi, 2012).2. Constraint Based Boolean Modelingsection provides basis contribution: constraint-based modeling language,together Boolean interpretation constraint language. enables usview constraint model conjunction Boolean formulae provides structuredrives subsequent encoding CNF.first introduce simple small fragment typical finite domain constraint-basedmodeling language. serves illustrate approach. Later, Section 6, showfull language. discuss several options Boolean representation integers.paper adopt particular unary representation, called order encoding.contribution independent choice, although equi-propagation works well it.Finally finish section constraints language fragmentviewed Boolean formula, constraint model conjunction.305fiMetodi, Codish, & Stuckey(1)(2)(3)(4)(5)new int(I, c1 , c2 )int neq(I1 , I2 )allDiff([I1 , . . . , ])int plus(I1 , I2 , I)int array plus([I1 , . . . , ], I)0 c1 c2I1 6= I2Vi<j Ii 6= IjI1 + I2 =I1 + + =Figure 1: core constraint language2.1 Constraint Language Fragmentfocus small fragment typical constraint modeling language detailed Figure 1.serves present main ideas paper. Constraint (1) declaring finitedomain integer variables range [c1 ...c2 ]. simplicity presentationassume c1 0. Constraints (23) difference integer variables,constraints (45) sums integer variables. syntactic sugar also allowwriting integer constants constraints. example, int neq(I, 5) shortnew int(I0 , 5, 5), int neq(I, I0 ).2.2 Modeling Kakuro: ExampleKakuro puzzle n board black white cells. black cells contain hintswhite cells filled numbers 1 9 (the bound 9 oftengeneralized larger value r). hints specify constraints sums valuesblocks white cells right and/or hint. numbers assigned whitecells block required different. Figure 2 illustrates 4 4 Kakuropuzzle (left) solution (right).model Kakuro puzzle view set blocks (of white cells) blockB set integer variables associated corresponding integer value, hint(B).block B associated two constraints: integers B must sum hint(B)must all-different. Figure 3 illustrates constraints corresponding Kakuroinstance Figure 2.2.3 Representing Integersfundamental design choice encoding finite domain constraints concerns representation integer variables. Gavanelli (2007) surveys several possible choices (theFigure 2: 4 4 Kakuro puzzle (right) solution (left).306fiBoolean Equi-propagationnewnewnewnewnewnewnewint(I1 , 1, 9)int(I2 , 1, 9)int(I3 , 1, 9)int(I4 , 1, 9)int(I5 , 1, 9)int(I6 , 1, 9)int(I7 , 1, 9)intintintintintintarrayarrayarrayarrayarrayarrayplus([I1 , I2 ], 13)plus([I1 , I3 ], 5)plus([I3 , I4 , I5 ], 12)plus([I2 , I4 , I6 ], 19)plus([I6 , I7 ], 3)plus([I5 , I7 ], 4)allDiff([I1 , I2 ])allDiff([I1 , I3 ])allDiff([I3 , I4 , I5 ])allDiff([I2 , I4 , I6 ])allDiff([I6 , I7 ])allDiff([I5 , I7 ])Figure 3: Constraints Kakuro instance Figure 2.direct-, support- log- encodings) introduces log-support encoding. Given choicerepresentation constraints bit-blasted interpreted Boolean formulae. focususe unary representation, so-called, order-encoding (see, e.g. Crawford& Baker, 1994; Bailleux & Boufkhad, 2003) many nice properties appliedsmall finite domains.order-encoding, integer variable X domain [0, . . . , n] representedbit vector X = [x1 , . . . , xn ]. bit xi interpreted X particular bitsequence X constitutes monotonic non-increasing Boolean sequence. example,value 3 interval [0, 5] represented 5 bits [1, 1, 1, 0, 0].important property Boolean representation finite domain integersability represent changes set values variable take. well-knownorder-encoding facilitates propagation bounds. Consider integer variableX = [x1 , . . . , xn ] values interval [0, n]. restrict X take values range[a, b] (for 1 b n), sufficient assign xa = 1 xb+1 = 0 (if b < n).variables xa0 xb0 0 a0 > b < b0 n determined true false,respectively, unit propagation. example, given X = [x1 , . . . , x9 ], assigning x3 = 1x6 = 0 propagates give X = [1, 1, 1, x4 , x5 , 0, 0, 0, 0], signifying dom(X) = {3, 4, 5}.observe additional property order-encoding X = [x1 , . . . , xn ]: abilityspecify variable cannot take specific value 0 v n domain equatingtwo variables: xv = xv+1 . indicates order-encoding well-suitedpropagate lower upper bounds, also represent integer variables arbitrary,finite set, domain. example, given X = [x1 , . . . , x9 ], equating x2 = x3 imposesX 6= 2. Likewise x5 = x6 x7 = x8 impose X 6= 5 X 6= 7. Applyingequalities X gives, X = [x1 , x2 , x2 , x4 , x5 , x5 , x7 , x7 , x9 ] (note repeated literals),signifying dom(X) = {0, 1, 3, 4, 6, 8, 9}.order-encoding many additional nice features exploited simplifyconstraints encodings CNF. illustrate one, consider constraint form+ B = 5 B integer values range 0 5 representedorder-encoding. bit level (in order encoding) have: = [a1 , . . . , a5 ]B = [b1 , . . . , b5 ]. constraint satisfied precisely B = [a5 , . . . , a1 ]. Insteadencoding constraint CNF, substitute bits b1 , . . . , b5 literals a5 , . . . , a1 ,remove constraint. section 3 formalize process discovering equalitiesliterals implied constraint using simplify CNF encodings.307fiMetodi, Codish, & Stuckey2.4 Bit BlastingGiven constraint model decision represent finite domain integer variablesbit level (we chose order encoding), bit-blasting process instantiating integer variables corresponding bit vectors interpreting constraints Booleanformulae.integer variable, I, declared constraint form new int(I, c1 , c2 )0 c1 c2 represented bit-vector = [1, . . . , 1, Xc1 +1 , . . . , Xc2 ]. So, may viewconstraint model consisting Boolean variables constraint c correspondsBoolean formula denoted [[c]], bit-blasted version c. specific definition[[]] important. illustration, note one could define^[[new int(I, c1 , c2 )]] =(xi+1 xi )c1 i<c2= [1, . . . , 1, Xc1 +1 , . . . , Xc2 ] well[[int neq(I1 , I2 )]] =n_(xi xor yi )i=1simplify presentation assume I1 = [x1 , . . . , xn ] I2 = [y1 , . . . , yn ]represented number bits. mapping [[]] extends natural way applyconjunctions constraints. So, given constraint model one Figure 3,integer variables instantiated unary (order encoding) bit vectors constraintviewed Boolean formula. constraint model takes Boolean representationconjunction formulae.3. Boolean Equi-propagationsection present approach propagation-based SAT encoding, Boolean equipropagation, propagates information equalities Boolean literals (andconstants). prove Boolean equi-propagation stronger unit propagationdetermines least many fixed literals unit propagation. demonstrate,example, power equi-propagation show leads considerable reductionsize CNF encoding.3.1 Boolean Equi-propagationLet B set Boolean variables. literal Boolean variable b B negationb. negation literal `, denoted `, defined b ` = b b ` = b.Boolean constants 1 0 represent true false, respectively. set literalsdenoted L L0,1 = L {0, 1}. set (free) Boolean variables appearBoolean formula denoted vars(). extend vars function sets formulaenatural way.assignment, A, partial mappingBooleanconstants, oftenfifivariablesfifiviewed following set literals: b A(b) = 1 b A(b) = 0 . formulab B, denote [b] (likewise [b]) formula obtained substituting308fiBoolean Equi-propagationoccurrences b B true (false). notation extends natural waysets literals. say satisfies vars() vars(A) [A] evaluates true.Boolean Satisfiability (SAT) problem consists Boolean formula determinesexists assignment satisfies .Boolean equality constraint ` = `0 `, `0 L0,1 . equi-formula E setBoolean equalities understood conjunction. set Boolean equalities denotedLeq0,1 set equi-formulae denoted E.Example 1. Suppose B = {x, y, z}. L0,1 = {0, 1, x, x, y, y, z, x}. exampleassignment = {x, z}, B = {x, y, z, y} assignment (since includes{y, y}). Given formula = x (y z) [x] formula 0 (y z)equivalently z. formula [A] = 1 (y 1) equivalent true,satisfy since vars() = {x, y, z} 6 {x, z} = vars(A). example equi-formula B{x = 0, = z} equivalently x (y z).3.1.1 Equi-propagationprocess inferring equational consequences Boolean formula given equational information. equi-propagator formula extensive function : E Edefined E E,nfifiE (E) e Leq0,1 E |= eis, conjunction equalities, least strongtrue E. sayn E, madeeq fifiequi-propagator complete (E) = e L0,1 E |= e . denotecomplete equi-propagator . assume equi-propagators monotonic:E1 E2 (E1 ) (E2 ). particular, follows, definition, completeequi-propagators. Section 3.3 discuss several methods implement completeincomplete equi-propagators.Example 2. Consider constraintC = new int(X, 0, 4) new int(Y, 0, 4) int neq(X, Y)corresponding Boolean representation = [[C]] bit representationX = [x1 , x2 , x3 , x4 ] = [y1 , y2 , y3 , y4 ]Assume settingE=y1 = 1, y2 = 1, y3 = 0, y4 = 0signifying = 2. Then, (E) = E {x2 = x3 } indicating X 6= 2. occurssince E equivalent (x2 x1 ) (x3 x2 ) (x4 x3 ) (x1 x2 x3 x4 )E |= x2 = x3 .following theorem states complete equi-propagation least powerfulunit propagation.309fiMetodi, Codish, & StuckeyTheorem 3. Let complete equi-propagator Boolean formula . Then,literal made true unit propagation clausal representation usingequations E also determined true (E).Proof. Let Boolean formula, E equi-formula, let C CE clausalrepresentations E respectively. Clearly |= C E |= CE . Let bpositive literal determined unit propagation C CE . correctness unitpropagation, C CE |= b. Hence, E |= b thus (E) |= b = 1. casenegative literal b same, except infer b = 0.following example illustrates equi-propagation powerful unitpropagation.Example 4. Consider = (x1 x2 ) (x1 x2 ) (x1 x2 x3 ). clausalrepresentation (x1 x2 )(x1 x2 )(x1 x2 )(x1 x2 x3 ) unit propagationpossible, since unit clauses. Equi-propagation (with additional equationalinformation) gives: () = {x1 = 1, x2 = 1, x3 = 0}.3.1.2 Boolean Unifierssometimes convenient view equi-formula E generic solved-form Booleansubstitution, E , (most general) unifier equations E. Boolean substitutions generalize assignments variables bound also literals.fi Booleansubstitution idempotent mapping : B L0,1 dom() = b B fi (b) 6= bfinite. Note particular idempotence implies (b) 6= b every b B.Note also defined B domain, dom(), includes elementsnon-identity.Boolean substitution, , viewed set =fib 7 (b) fi b dom().apply another substitution 0 , obtain substifitution ( 0 ) = b 7 (0 (b)) fi b dom() dom(0 ) . unifier equi-formula Esubstitution |= (e), e E. most-general unifier Esubstitution unifier 0 E, exists substitution 0 = .Example 5. Consider equi-formula E {b1 = b2 , b3 = b4 , b5 = b6 , b6 = b4 , b7 =1, b8 = b7 } unifier E {b2 7 b1 , b4 7 b3 , b5 7 b3 , b6 7 b3 , b7 7 1, b8 7 0}.Note (E) trivially true equi-formula {b1 = b1 , b3 = b3 , b3 = b3 , b3 = b3 , 1 =1, 0 = 1}.Consider enumeration L0,1 = {0, 1, b1 , b1 , b2 , b2 , . . .} let total (strict)order L0,1 0 1 b1 b1 b2 b2 . define canonical most-generalunifier unifyE satisfiable equi-formula E where:fiunifyE (b) = min ` L0,1 fi E |= b = `is, substitution unifyE maps b smallest literal equivalent b givenE. compute unifyE almost linear (amortized) time using variationunion-find algorithm (Tarjan, 1975).Example 6. equi-formula E substitution Example 5unifyE = .310fiBoolean Equi-propagationfollowing proposition provides foundation equi-propagation based Booleansimplification. allows us apply equational information simplify given formula.particular, E equi-formula literals occurring unifyE () smallercontains fewer variables.Proposition 1. Let Boolean formula E E satisfiable equi-formula. Then,a. E unifyE () E;b. E satisfiable unifyE () satisfiable;c. satisfying assignment unifyE () unifyE satisfying assignmentE.Proof. (a) Let = unifyE assume satisfying assignment E,view substitution, unifier E. Hence, since general unifier,exists substitution = . Clearly (b) = (b) variables b range. Hence, agree variables () implies (()) = (())meaning (()) = (). So, satisfying assignment () Esatisfying assignment E. (b) () direction follows (a)() direction (c). (c) Assume satisfying assignment unifyE (). ClearlyunifyE satisfies construction. Also unifyE satisfies E since unifyE (E)trivial. Hence unifyE satisfying assignment E.3.1.3 Equi-propagation Processequi-propagation process presented central theme paper: Let =1 n partitioning Boolean formula n portions, let 1 , . . . , n corresponding equi-propagators, take initial E = . Satisfiability determinedfollows:1. long possible, select (E) ) E update E = (E).2. Finally, equi-propagators apply more, check unifyE () satisfiable.3. satisfying assignment unifyE () unifyE satisfying assignment.typically apply equi-propagation theme Boolean representation =1 n constraint model C = C1 Cn = [[Ci ]]. requireCi small conjunction constraints. Typically, integer variables referredCi also declared Ci (sometimes requires duplicating variabledeclarations). individual constraint c denote c+ conjunction constraintsincluding c declarations integer variables refers to. specificsdeclarations clear context.Example 7. Let C following constraint model:new int(X, 1, 3) new int(Y, 1, 3) new int(Z, 1, 3)C=int plus(X, Y, 3) int plus(Y, Z, 4) int neq(Y, Z)311fiMetodi, Codish, & Stuckey1. int plus+ (X, Y, 3) = int plus(X, Y, 3) new int(X, 1, 3) new int(Y, 1, 3),2. int plus+ (Y, Z, 4) = int plus(Y, Z, 4) new int(Y, 1, 3) new int(Z, 1, 3),3. int neq+ (Y, Z) = int neq(Y, Z) new int(Y, 1, 3) new int(Z, 1, 3).basis equi-propagation take = 1 2 3 1 = [[int plus+ (X, Y, 3)]],2 = [[int plus+ (Y, Z, 4)]], 3 = [[int neq+ (Y, Z)]]. Denoting X = [1, x2 , x3 ], =[1, y2 , y3 ], Z = [1, z2 , z3 ] applying corresponding complete equi-propagatorsstarting E0 = have:1. E1 = 1 (E0 ) = E0 {x3 = 0, y3 = 0, x2 = y2 };2. E2 = 2 (E1 ) = E1 {z2 = 1, y2 = z3 };3. E3 = 3 (E2 ) = E2 {y2 = 0}.point equi-propagation applies more, unifyE3 = {x2 7 1, x3 7 0, y2 70, y3 7 0, z2 7 1, z3 7 1} . Now, unifyE3 () tautology (all Boolean variablesdeterment equi-propagation).following theorem clarifies order equi-propagators appliedequi-propagation process influence final result.Theorem 8. equi-propagation process confluent.Proof. Let = 1 n Boolean formula 1 , . . . , n corresponding equipropagators. Let E1 = ir (ir1 (. . . i1 () . . .)) E2 = js (js1 (. . . j1 () . . .))two different applications equi-propagation process. construction,given equi-propagators, property (?): (E1 ) = E1 (E2 ) = E2 .assume, contradiction, E1 6= E2 . w.l.o.g. exists e E2 eE1 (swap roles E1 E2 E2 E1 ). E1 ( E2 . Let us focus firststep equi-propagation process leading E2 introduced equation e E2introduced E1 : So, exists ` < E = j` (j`1 (. . . j1 () . . .)) E1e `+1 (E) e 6 E1 . But, E E1 , monotonicity `+1 ,`+1 (E) `+1 (E1 ) hence e `+1 (E1 ) contradiction constructionproperty (?).following proposition provides alternative, efficient implement, definitioncomplete equi-propagation.Proposition 2. Let Boolean formula complete equi-propagator .Define E E,nfifi unify () |= e(E) = E e LeqE0,1Then, (E) = (E). is, implements complete equi-propagator .312fiBoolean Equi-propagationProof. Forthefi first direction, ():definition, (E) E. also(E) e fi unifyE () |= eProposition 1(a) E |= unifyE (). So,(E) (E). direction, (): Let e (E). e E proofstraightforward. Otherwise, let unifyE () |= e assume contrary e 6 (E),words E 6|= e. means exists assignmentsatisfies E satisfy e. Lemma 1(a), also satisfies unifyE () Eparticular satisfies unifyE (). assumption unifyE () |= esatisfies e. Contradiction.Computing considerably efficient since simply examineformula application unifyE determine new Boolean equality consequences.Finally comment: intention equi-propagation process appliedmake SAT instance smaller also obtain easier solve representation.However, decreasing size CNF main objective. fact, often explicitlyintroduce redundancies improve SAT encoding. example, consider if-thenelse construct, xITE(s,t,f), propositional variable: indicates selector,indicates true branch, f indicates false branch, x indicates result.corresponding CNF {{s, t, x}, {s, t, x}, {s, f, x}, {s, f, x}}. Een Sorensson(2006) propose add redundant clauses, {t, f, x} {t, f, x}. commentimproves encoding observe redundant clauses often introducedachieve arc-consistency SAT encoding. show given clausal encodingformula , application equi-propagation strengthen unit propagation.Theorem 9. Let C set clauses, suppose C |= E E equi-formula.unit propagation unifyE (C) least strong unit propagation C.Proof. Unit propagation C starting assignment A0 repeatedly chooses clausec {l} C {l0 | l0 c} Ai sets Ai+1 := Ai {l}. Unit propagation terminatesAk clauses occur. Note failure detected Ak containsliteral negation.show using order unit propagation unifyE (C) determinedoccurs C starting assignment B0 = unifyE (A0 ) always obtain assignmentBi Bi unifyE (Ai ). proof induction unit propagation steps C.base case holds construction.Assume c {l} C {l0 | l0 c} Ai . induction Bi unifyE (Ai ){unifyE (l0 ) | l0 c}. Either unifyE (l) Bi case set Bi+1 = Biinduction holds. unifyE (l) 6 Bi . since c {l} C {unifyE (l0 ) | l0c}{unifyE (l)} unifyE (C). Hence unit propagation unifyE (C) Bi obtainBi+1 := Bi {unifyE (l)}. Hence induction holds.Given unit propagation reaches unique fixpoint unit propagation orderunifyE (A0 ) end assignment B B Bk unify(Ak )3.2 Power Equi-propagationillustrate impact equi-propagation come back Kakuro exampleSection 2.2 (recall Figure 2). fact solving puzzles via SAT encodings quite easy,without equi-propagation. example viewed illustrating313fiMetodi, Codish, & Stuckeya. 1b. 2c. 3Figure 4: Applying complete equi-propagation Kakuro Instance using different modelsimpact equi-propagation size encoding. compare 3 different modelsproblem, give different equi-propagation.consider, baseline discussion, following Boolean representationderived constraint model declarations specified explicitlyform new int(I, 1, h) h smallest hint block includesnumber 9 smaller.^^1 =[[int neq+ (Ii , Ij )]][[int array sum+ (B, hint(B))]]{I1 , . . . , Ik } Blocks1i<j kB BlocksNotice one int neq conjunct pair white cells block,one int array sum conjunct block. Applying equi-propagation process1 complete equi-propagators determines six integer values depicted Figure 4(a).Figure 4(b) illustrates impact applying equi-propagation processequi-propagators allDiff constraints instead individual int neq constraints. determines seven integer variables formalized taking followingBoolean representation constraint model (and introducing equi-propagatorconjunct).^^2 =[[allDiff+ (B)]][[int array sum+ (B, hint(B))]]B BlocksB BlocksFigure 4(c) illustrates impact applying equi-propagation process equipropagators pairs, consisting allDiff constraint together corresponding sum constraint. form equi-propagation powerful. fixes integervalues white cells (in example). stress equi-propagation reasonsequalities Boolean literals constants. take model as:^3 =[[allDiff+ (B)]] [[int array sum+ (B, hint(B))]]B Blocksdemonstrate impact equi-propagation, Table 1 provides data 15additional instances,1 categorized as: easy, medium hard. first two columnstable indicate instance category ID. five columns headed Integer1. Instances available http://4c.ucc.ie/~hcambaza/page1/page7/page7.html (generated HelmutSimonis).314fihardmediumeasyBoolean Equi-propagationID168169170171172188189190191192183184185186187Integer Variablesinit 1 2 3 BEEinit484 439 2800 385 3872467 456 4400 440 3736494 485 4690 469 3952490 406 3930 422 3920506 495 4840 492 4048476 461 4550 461 3808472 437 42562 449 3776492 481 4800 480 3936478 452 448 161 448 3824499 481 478 136 478 3992490 365 3450 371 3920506 489 48423 486 4048482 482 455 206 467 3856472 466 4540 466 3776492 475 47369 473 3936Average compilation time sec.Boolean Variables123144084301823 168201961 179801280 114801676 157301939 191502017 1911811998 192001864 18211972455 24172141151 105901613 1495212181 21112202115 206201991 1959483.739 2.981 0.916BEE1170169218051341163419341976193618282420116815452144208619600.477Table 1: Applying SAT-based complete equi-propagation Kakuro encodingVariables, first four specify number unassigned white cells initial stagethree complete equi-propagation processes described above. fivecolumns headed Boolean variables, first four indicate corresponding informationregarding number Boolean variables bit representations integers. So,smaller number table, variables removed due equipropagation. particular, 3 model completely solves 9 15 instances. twocolumns titled BEE show corresponding information obtained using weaker formequi-propagation described Section 4 below. last row table indicatesaverage time takes perform equi-propagation (in seconds) using threeschemes, 1 , 2 , 3 , weaker scheme titled BEE. come back discusslater detailing equi-propagation performed. results table indicateclear benefit performing equi-propagation based coarser portions model.3.3 Implementing Equi-propagatorsimplement complete equi-propagators need infer Boolean equalities impliedgiven Boolean formula, , equi-formula, E. Based Proposition 2, sufficienttest conditionunifyE () |= (`1 `2 )(1)consider three techniques: using SAT solver, using BDDs, using ad-hoc rulesapplied Boolean representations individual constraints.straightforward implement complete equi-propagator using SAT solver.test Condition (1) consider formula = (`1 6 `2 ). satisfiable,Condition (1) holds. way, Condition (1) checked relevant equations315fiMetodi, Codish, & Stuckeyinvolving variables unifyE () (and constants 0,1). major obstacle SATbased approach testing single equivalence, `1 `2 , least hard testingsatisfiability . fact testing unsatisfiability typically expensive.Hence importance assumption small fragment CNFinterest. practice SAT-based equi-propagation surprisingly fast. illustration,last row Table 1 average times SAT-based complete equi-propagationdifferent models indicated columns 1 , 2 , 3 . interesting observestrongest technique, using 3 , fastest. fewer (butlarger) conjuncts hence fewer queries SAT solver.implement complete equi-propagator using binary decision diagrams (BDDs)follows. construct BDD formula beginning equi-propagation.new equational information E 0 added E simplify BDD conjoiningBDD BDD E 0 projecting variables longer appearunifyE (). Note simplification increase size BDD. practice,rather two steps, use Restrict operation Coudert Madre(1990) (bdd simplify Somenzi, 2009) create new BDD efficiently.Given BDD unifyE (), explicitly test Condition (1) using standardBDD containment test (e.g., bddLeq Somenzi, 2009). SAT-based approach, test performed relevant equations involving variables unifyE ()(and constants 0,1). Alternately use method Bagnara Schachte (1998)(extended extract literal equalities opposed variable equalities) extractfixed literals equivalent literal consequences BDD.Example 10. Consider BDD shown Figure 5(a) represents formula:new int(A, 0, 3) new int(B, 0, 3) int neq(A, B). Figure 5(b) depicts BDDunifyE () E = {B1 = 1, B2 = 1, B3 = 0 }. easy see equipropagation determines A2 = A3 . Let E 0 = E {A2 = A3 }. Figure 5(c) showssimplified BDD unifyE 0 ().major obstacle BDD-based approach concerns size formulaunifyE (). constraints, corresponding BDD guaranteed polynomial (in size constraint). following result holds arbitrary constraint, also holds unifyE ().Proposition 3. Let c constraint k integer variables represented n bitsorder encoding. Then, number nodes BDD representing [[c]] boundO(nk ).Proof. (Sketch) n + 1 legitimate states n bit unary variable,BDD cannot nodes possible states.Constraints like new int, int neq, int plus involve 3 integer variableshence BDD-based complete equi-propagators polynomially bounded. However,case global constraints allDiff int array plusarity fixed. Moreover, well known allDiff constraintpolynomial sized BDD (Bessiere, Katsirelos, Narodytska, & Walsh, 2009).316fiBoolean Equi-propagation76540123A1:::7654012376540123B1B1012376540123 765476540123r 2 rrr A2: A2rr:r rrrrrr rrrr:r0123 765401230123012376540123B2 L 7654B2B2: 7654B2: 7654, LLB 2::L:L:,LLL:::, L76540123765401230123,A3: A3 l 7654A3, : l -ll, l l : l7654012376540123B3B3;; - ;;S;-76540123A1.....0123765401237654A2A2.....012301237654A3.A3 > 7654>>>> .>> .>> .>>>.76540123A1+++++76540123A2(a) BDD int nequ (A, B)(b) Simpld wrt B=[1, 1, 0](c) Simpld wrt A2 =A3Figure 5: BDDs (a) new int3 (A, [0, 3]) new int3 (B, [0, 3]) int neq(A, B) (b)unifyE () E = {B1 =1, B2 =1, B3 =0} (c) unifyE 0 () E 0 =E {A2 =A3 }. Full (dashed) lines correspond true (false) edges. Edgesfalse node F omitted brevity.Given potential exponential run-time performing SAT-based equi-propagation,potential exponential size BDD-based equi-propagators, consider thirdapproach implement equi-propagation collection ad-hoc transition rulestype constraint. approach complete equationsimplied constraint detected implementation fast, works wellpractice. topic next section.4. Ad-hoc Equi-Propagationconsider rule-based approach define equi-propagators. definition givenset ad-hoc rules specified type constraint. novelty approachbased CNF, previous works, rather driven bit blasted constraintsencoded CNF. presentation focuses case finite domainintegers represented order encoding. integer X = [x1 , . . . , xn ], oftenwrite: X denote equation xi = 1, X < denote equation xi = 0, X 6=denote equation xi = xi+1 , X = denote pair equations xi = 1, xi+1 = 0.Moreover, simplify notation specifying rules below, view X = [x1 , . . . , xn ]larger vector padded sentinel cells cells left x1 take value 1cells right xn take value 0. Basically facilitates specificationend cases formalism. consider 5 constraintslanguage fragment presented Section 2.317fiMetodi, Codish, & Stuckeyc = new int([x1 , . . . , xn ], 0, n)Eadd c (E)xi = 1 x1 = 1, . . . , xi1 = 1xi = 0 xi+1 = 0, . . . , xn = 0(a)c = int neq(X, )X = [x1 , . . . , xn ] = [y1 , . . . , yn ]Eadd c (E)X=i6=xi = yi+1 , yi = xi+1X 6= i, 6=xi = yi+1 , yi = xi+1X 6= i, 6=(b)Figure 6: Ad-hoc rules (a) new int (b) int neqc = int plus(X, Y, Z) X = [x1 , . . . , xn ],= [y1 , . . . , ym ], Z = [z1 , . . . , zn+m ]c = allDiff([Z1 , Z2 , Z3 , . . . , Zn ])Eadd c (E)Z1 , Z2 {i, j}Z1 =6 Z2 , Zk 6=Zk =6 j (k > 2)EX i, jX < i, < jZ k, X <Z < k, XX=iZ=k(a)add c (E)Z i+jZ <i+j1ki<kizi+1 = y1 , . . . , zi+m = ymx1 = yk , . . . , xk = y1(b)Figure 7: Ad-hoc rules (a) allDiff (b) int plus(1) two rules Figure 6(a) derive monotonicity order encodingrepresentation. basically correspond unit propagation, constraint level.(2) first rule Figure 6(b) considers cases X constant (the symmetriccase handled exchanging X ). two rules capture templatescommonly arise equi-propagation process. illustrate justificationthird rule consider possible truth values variables xi xi+1 : (a) xi = 0xi+1 = 1 integers relation take form [. . . , 0, 1, . . .] violatingspecification ordered, possible. (b) xi = 1 xi+1 = 0numbers take form [1, . . . , 1, 0, . . . , 0] equal, violating neq constraint.possible bindings xi xi+1 xi = xi+1 .(3) Figure 7(a) illustrate single rule allDiff constraint considersHall sets size 2. Zi represents integer order encoding focuscase Z1 Z2 restricted equations E take two possiblevalues, j. expressed E [x1 , . . . , xn ] {i, j} (for < j) meansxk = 1 k < i, xk = xk+1 k < j, xk = 0 j < k n. Z1 6= Z2means adding single equation xi = yi (because Z1 Z2 take two values).addition rule, apply rules int neq(Zi , Zj ) pair integers ZiZj constraint.318fiBoolean Equi-propagationz1 =1, . . . , z4 =1,E0 =z5 =0, . . . , z18 =0X = [x1 , . . . , x4 , x5 , x6 , . . . , x9 ],= [y1 , . . . , y4 , y5 , y6 , . . . , y9 ],Z = [1, 1, 1, 1, 0, . . . , 0]E2 = E1 {y6 =0, . . . , y9 =0}X = [x1 , . . . , x4 , x5 , x6 , . . . , x9 ],= [y1 , . . . , y4 , 0, . . . , 0],Z = [1, 1, 1, 1, 0, . . . , 0]E4 = E3 {x6 =0, . . . , x9 =0}X = [x1 , . . . , x4 , 0, . . . , 0],= [y1 , . . . , y4 , 0, . . . , 0],Z = [1, 1, 1, 1, 0, . . . , 0]x2 =y3 ,y2 =x3int neqZ<5, X0int plusZ<5, 0int plusZ=4int plusE1 = E0 {y5 =0}X = [x1 , . . . , x4 , x5 , x6 , . . . , x9 ],= [y1 , . . . , y4 , 0, y6 , . . . , y9 ],Z = [1, 1, 1, 1, 0, . . . , 0]E3 = E2 {x5 =0}X = [x1 , . . . , x4 , 0, x6 , . . . , x9 ],= [y1 , . . . , y4 , 0, . . . , 0],Z = [1, 1, 1, 1, 0, . . . , 0]E5 = E4 {x1 =y4 , . . . , x4 =y1 }X = [x1 , x2 , x3 , x4 , 0, . . . , 0],= [x4 , x3 , x2 , x1 , 0, . . . , 0],Z = [1, 1, 1, 1, 0, . . . , 0]y5 =0new intx =05new intE6 = E5 {x2 =x3 }X = [x1 , x2 , x2 , x4 , 0, . . . , 0],= [x4 , x2 , x2 , x1 , 0, . . . , 0],Z = [1, 1, 1, 1, 0, . . . , 0]Figure 8: Ad-hoc equi-propagation described Example 11(4) first four rules Figure 7(b) capture standard propagation behaviorinterval arithmetics. last two rules apply one integers relationconstant. symmetric cases replacing role X .(5) special ad-hoc rules equi-propagation int array plus constraint. simply viewed decomposition set int plus constraints.simplification performed level using rules int plus. decompositionint array plus explained Section 6.Example 11 (ad-hoc equi-propagation). Consider following (partial) constraint model,context Kakuro example Section 2.2, represent variables X,Z X = [x1 , . . . , x9 ], = [y1 , . . . , y9 ] Z = [z1 , . . . , z18 ] assume previous equi-propagation (on constraints) determined current equi-formula E0specify integer variable Z = 4:C=new int(X, 0, 9) new int(Y, 0, 9) new int(Z, 0, 18)int plus(X, Y, Z) int neq(X, Y)Figure 8 illustrates, step-by-step, equi-propagation process C using ad-hoc rulesdefined above. step corresponds application one defined ad-hocequi-propagation rules indicated label transition. stage illustratederived equations (top part) application (as unifier) state variablesX, Z (lower part).319fiMetodi, Codish, & Stuckeyc = ordered([x1 , . . . , xn ]) (new int)replacen1truex1 = 1ordered([1, x2 . . . , xn ])xn = 0ordered([x1 , . . . , xn1 , 0]), . . . , xn ])xi = xi+1 ordered([x1 , . . . , xi ,xi+1Figure 9: Simplification rules new int (crossed elements removed).summarize, let us come back Table 1. numbers presented two columnsheaded BEE specify number variables remaining application ad-hoc equipropagation. also observe definition ad-hoc equi-propagation triviallymonotonic.5. Constraint Model Partial EvaluationPartial evaluation, together equi-propagation, second important componentapproach compile constraint models CNF. Partial evaluation simplifying given constraint model view information becomes available due equipropagation. Typically, constraint simplification process, apply alternating stepsequi-propagation partial evaluation. Examples partial evaluation include constantelimination removing constraints tautologies. section detailpartial evaluation rules apply five constraint types defined languagefragment presented Section 2.(1) new int(I, c1 , c2 ) constraint specifies integer = [x1 , . . . , xn ] representedorder encoding particular corresponding bit sequence sorted (notincreasing). denote ordered([x1 , . . . , xn ]). Partial evaluation focusesaspect constraint ignores bounds c1 , c2 specified constraint. tableFigure 9 specifies four simplification rules apply. first rule identifies tautologies,second third rules remove leading ones trailing zeros, fourth removes(one two) equated bits. figure, subsequent, crossed elementsequence, indicates removed sequence.(2) simplification rules int neq constraint shown Figure 10(a) symmetricexchanging role X . first two rules identify tautologies. thirdrule X equal bit position i. corresponding bitsremoved representation X , resulting shorter list bitsrepresentations. last two rules removing leading ones trailing zeroesillustrated following example.Example 12. Figure 10(b) shows two steps partial evaluation, int neq constraint,first removing leading ones, removing trailing zeroes.320fiBoolean Equi-propagationc = int neq(X, )X = [x1 , . . . , xn ] = [y1 , . . . , yn ]replaceX = i, 6=truexi = yitrueint neq(x[x1 , . . . ,xi = yi, . . . , xn ],yi , . . . , yn ])[y1 , . . . ,int neq([1, xi+1 , . . . , xn ],Xi2[yi , yi+1 , . . . , yn ])int neq([x1 , . . . , xi , 0],Xi[y1 , . . . , yi , yi+1 ])int neq([x1 , . . . , x4 , 0, 0, 0], P.Eint neq[1, 1, 1, y4 , . . . , y7 ])int neq([x3 , x4 , 0, 0, 0], P.Eint neq[1, y4 , . . . , y7 ])int neq([x3 , x4 , 0],[1, y4 , y5 ])(a)(b)Figure 10: (a) Simplification rules int neq (b) example application.c = allDiff([Z1 , . . . , Zn ])Zi = [zi,1 , . . . , zi,m ] (1 n)replacen1true^ dom(Z1 )allDiff([Z2 , . . . , Zn ])dom(Zk ) =k>1 [|dom(Zi )| = 2allDiff([Z3 , . . . , Zn ])i{1,2}^kZk 6=allDiff([z1,1 , . . . ,z1,i+1, . . . , z1,m ]...[zn,1 , . . . ,zn,i+1, . . . , zn,m ])Figure 11: Simplification rules allDiff(3) Four rules simplifying allDiff constraints illustrated Figure 11. first,detecting tautologies. second, identifies cases one integersconstraint (assume Z1 ) domain disjoint others. rule also capturescase Z1 constant. third rule removes Hall set size 2 (assume {Z1 , Z2 })constraint. Note corresponding equi-propagation rule detectsvalues Z3 , . . . , Zn different values {Z1 , Z2 } next fourth ruleapplies. fourth rule case none integers constraint takecertain value i. rule also captures case numbers leadingones trailing zeroes. last two rules illustrated Example 14.(4 & 5) simplification rules shown Figure 12 symmetric exchangingrole X . first two apply (at least) one X, Z constant.already applied equi-propagation constraint, tautology. SeeExample 13. last two rules apply remove leading ones trailing zeroes.321fiMetodi, Codish, & Stuckeyc = int plus(X, Y, Z) X = [x1 , . . . , xn ],= [y1 , . . . , ym ], Z = [z1 , . . . , zn+m ]replaceX=itrueZ=ktrueint plus([xi+1 , . . . , xn ], Y,X i, Z[zi+1 , . . . , zn+m ])int plus([x1 , . . . , xi ], Y,X i, Z +[z1 , . . . , zi+m ])Figure 12: Simplification rules int plus.(a) int plus(I1 , I2 , K)(b) allDiff([I1 , I2 , I3 , I4 , I5 , I6 , I7 , I8 ])(c) int array plus([I2 , I3 , I4 , I5 ], K)Figure 13: Constraint Model Examples 1315simplification rules int array plus constraint straightforward generalizationsones int plus. See Example 15.summarise rule based approach apply equi-propagation partial evaluationpresent following sequence three examples focus simplificationthree constraints given Figure 13 integer variables I1 , . . . , I8 definedrange 1 8 K = 14.Example 13. Consider equi-propagation constraint (a) Figure 13 E0 specifiesK = 14:k1 =1, . . . , k14 =1k15 = 0, k16 = 0I1 = [1, i1,2 , . . . , i1,8 ],I2 = [1, i2,2 , . . . , i2,8 ],K = [1, 1, . . . , 1, 0, 0]| {z }14i1,2 =1, . . . , i1,6 =1,i2,2 =1, . . . , i2,6 =1,i1,7 =i2,8 , i1,8 =i2,7I1 = [1, 1, 1, 1, 1, 1, i1,7 , i1,8 ],I2 = [1, 1, 1, 1, 1, 1, i1,8 , i1,7 ],K = [1, 1, . . . , 1, 0, 0]| {z }E1 = E0E0 =K=14int plus14Given E1 , constraint tautology removed partial evaluation:int plus(P.E[1, 1, 1, 1, 1, 1, i1,7 , i1,8 ],int plus[1, 1, 1, 1, 1, 1, i1,8 , i1,7 ], 14)Example 14. Consider equi-propagation constraint (b) Figure 13 given E1Example 13:E1I1 = [1, 1, 1, 1, 1, 1, i1,7 , i1,8 ],I2 = [1, 1, 1, 1, 1, 1, i1,8 , i1,7 ]i1,7 =i2,8 ,i2,7 =i1,8int neq322E2 = E1 {i1,7 =i1,8 }I1 = [1, 1, 1, 1, 1, 1, i1,7 , i1,7 ],I2 = [1, 1, 1, 1, 1, 1, i1,7 , i1,7 ]fiBoolean Equi-propagationGiven E2 , equi-propagation rule allDiff detects {I1 , I2 } Hall set (wheretwo variables take values 6 8). adds E2 set equations, E 0 , specifyI3 , I4 , I5 , I6 , I7 , I8 6= 6, 8. result E3 = E2 E 0 result step givesfollowing bindings (where impact E 0 underlined):I1I2I3I4= [1, 1, 1, 1, 1, 1, i1,7 , i1,7 ]= [1, 1, 1, 1, 1, 1, i1,7 , i1,7 ]= [1, i3,2 , i3,3 , i3,4 , i3,5 , i3,7 , i3,7 , 0]= [1, i4,2 , i4,3 , i4,4 , i4,5 , i4,7 , i4,7 , 0]I5I6I7I8= [1, i5,2 , i5,3 , i5,4 , i5,5 , i5,7 , i5,7 , 0]= [1, i6,2 , i6,3 , i6,4 , i6,5 , i6,7 , i6,7 , 0]= [1, i7,2 , i7,3 , i7,4 , i7,5 , i7,7 , i7,7 , 0]= [1, i8,2 , i8,3 , i8,4 , i8,5 , i8,7 , i8,7 , 0]Given E3 , partial evaluation constraint first removes Hall set:P.E[allDiff([I1 , I2 , I3 , I4 , I5 , I6 , I7 , I8 ])] [allDiff([I3 , I4 , I5 , I6 , I7 , I8 ])]allDiffapplies remove three redundant bits underlying representationremaining integer (which equal 0, 6, 8):allDiff([[1, i3,2 , i3,3 , i3,4 , i3,5 , i3,7 , i3,7 , 0],[1, i4,2 , i4,3 , i4,4 , i4,5 , i4,7 , i4,7 , 0],[1, i5,2 , i5,3 , i5,4 , i5,5 , i5,7 , i5,7 , 0],[1, i6,2 , i6,3 , i6,4 , i6,5 , i6,7 , i6,7 , 0],[1, i7,2 , i7,3 , i7,4 , i7,5 , i7,7 , i7,7 , 0],[1, i8,2 , i8,3 , i8,4 , i8,5 , i8,7 , i8,7 , 0]])P.EallDifferentallDiff([[i3,2 , i3,3 , i3,4 , i3,5 , i3,7 ],[i4,2 , i4,3 , i4,4 , i4,5 , i4,7 ],[i5,2 , i5,3 , i5,4 , i5,5 , i5,7 ],[i6,2 , i6,3 , i6,4 , i6,5 , i6,7 ],[i7,2 , i7,3 , i7,4 , i7,5 , i7,7 ],[i8,2 , i8,3 , i8,4 , i8,5 , i8,7 ]])Example 15. Consider equi-propagation constraint (c) Figure 13 given E3Example 14. rules apply derive decomposition int array plusconstraint int plus parts. dictate I3 , I4 , I5 5:E3I2I3I4I5= [1, 1, 1, 1, 1, 1, i1,7 , i1,7 ],= [1, i3,2 , i3,3 , i3,4 , i3,5 , i3,7 , i3,7 , 0],= [1, i4,2 , i4,3 , i4,4 , i4,5 , i4,7 , i4,7 , 0],= [1, i5,2 , i5,3 , i5,4 , i5,5 , i5,7 , i5,7 , 0]int arrayplusE4I2I3I4I5= E3 {i3,7 =0, i4,7 =0, i5,7 =0}= [1, 1, 1, 1, 1, 1, i1,7 , i1,7 ],= [1, i3,2 , i3,3 , i3,4 , i3,5 , 0, 0, 0],= [1, i4,2 , i4,3 , i4,4 , i4,5 , 0, 0, 0],= [1, i5,2 , i5,3 , i5,4 , i5,5 , 0, 0, 0]Applying partial evaluation simplifies constraint follows:int array plus([[1, 1, 1, 1, 1, 1, i1,7 , i1,7 ],[1, i3,2 , i3,3 , i3,4 , i3,5 , 0, 0, 0],[1, i4,2 , i4,3 , i4,4 , i4,5 , 0, 0, 0],[1, i5,2 , i5,3 , i5,4 , i5,5 , 0, 0, 0]], 14 )P.Eint arrayplusint array plus([[i1,7 , i1,7 ],[i3,2 , i3,3 , i3,4 , i3,5 ],[i4,2 , i4,3 , i4,4 , i4,5 ],[i5,2 , i5,3 , i5,4 , i5,5 ]], 5 )summarize Examples 1315 observe initial constraint model 3 constraints8 integers represented 56 bits. constraint simplification 2 constraintsremain 8 integers represented using 28 bits:I1I2I3I4= [1, 1, 1, 1, 1, 1, i1,7 , i1,7 ]= [1, 1, 1, 1, 1, 1, i1,7 , i1,7 ]= [1, i3,2 , i3,3 , i3,4 , i3,5 , 0, 0, 0]= [1, i4,2 , i4,3 , i4,4 , i4,5 , 0, 0, 0]I5I6I7I8323= [1, i5,2 , i5,3 , i5,4 , i5,5 , 0, 0, 0]= [1, i6,2 , i6,3 , i6,4 , i6,5 , i6,7 , i6,7 , 0]= [1, i7,2 , i7,3 , i7,4 , i7,5 , i7,7 , i7,7 , 0]= [1, i8,2 , i8,3 , i8,4 , i8,5 , i8,7 , i8,7 , 0]fiMetodi, Codish, & Stuckey6. Compiling Constraints BEEBEE (Ben-Gurion Equi-propagation Encoder) tool applies encode finite domainconstraint models CNF. BEE first introduced Metodi Codish (2012).encoding process, BEE performs optimizations based equi-propagation partialevaluation improve quality target CNF. BEE implemented (SWI) Prologapplied conjunction CryptoMiniSAT solver (Soos, 2010)Prolog interface (Codish, Lagoon, & Stuckey, 2008). CryptoMiniSAT offers direct supportxor clauses, BEE takes advantage feature. BEE downloaded (Metodi,2012) one also find examples paper others.source language BEE compiler also called BEE. constraint modelinglanguage similar FlatZinc (Nethercote et al., 2007), focus subsetlanguage relevant finite domain constraint problems. Five constraint constructsBEE language introduced Section 2.1. full language presentedTable 2.BEE Boolean constants true false viewed (integer) values 1 0.Constraints represented (a list of) Prolog terms. Boolean integer variablesrepresented Prolog variables, may instantiated simplifying constraints.Table 2, X Xs (possibly subscripts) denote literal (a Boolean variablenegation) vector literals, (possibly subscript) denotes integer variable,c (possibly subscript) denotes integer constant. right columntable brief explanations regarding constraints. table introduces 26 constrainttemplates.Constraints (1-2) variable declarations: Booleans integers. Constraint (3)expresses Boolean integer value. Constraints (4-8) Boolean (and reifiedBoolean) statements. special cases Constraint (5) bool array or([X1 , . . . , Xn ])bool array xor([X1 , . . . , Xn ]) facilitate specification clauses xor clauses(supported directly CryptoMiniSAT solver Soos, 2010). Constraint (8) specifiessorting bit pair [X1 , X2 ] (decreasing order) results pair [X3 , X4 ]. basicbuilding block construction sorting networks (Batcher, 1968) used encode cardinality (linear Boolean) constraints compilation described Asn, Nieuwenhuis,Oliveras, Rodrguez-Carbonell (2011) Codish Zazon-Ivry (2010). Constraints (9-14) integer relations operations. Constraints (15-20)linear (Boolean, Pseudo Boolean, integer) operations. Constraints (21-26)lexical orderings Boolean integer arrays.main design choice BEE integer variables represented orderencoding. So, BEE suitable problems integer variables take smallmedium sized values. compilation constraint model CNF using BEE goesthree phases.1. Unary bit-blasting: integer variables (and constants) represented bit vectorsorder-encoding.2. Constraint simplification: three types actions applied: equi-propagation, partialevaluation, decomposition constraints. Simplification applied repeatedlyrule applicable.324fiBoolean Equi-propagationDeclaring Variables(1)(2)(3)declare Boolean Xdeclare integer I, c1 c2(X = 1) (X = 0)new bool(X)new int(I, c1 , c2 )bool2int(X, I)op {or, and, xor, iff}Boolean (reified) Statements(4)(5)(6)(7)(8)bool eq(X1 , X2 ) bool eq(X1 , X2 )bool array op([X1 , . . . , Xn ])bool array op reif([X1 , . . . , Xn ], X)bool op reif(X1 , X2 , X)comparator(X1 , X2 , X3 , X4 )X1 = X2 X1 = X2X1 op X2 op XnX1 op X2 op Xn XX1 op X2 Xsort([X1 , X2 ]) = [X3 , X4 ]Integer relations (reified)rel {leq, geq, eq, lt, gt, neq}arithmeticop {plus, times, div, mod, max, min}, op0 {plus, times, max, min}(9)(10)(11)(12)(13)(14)intintintintintintrel(I1 , I2 )rel reif(I1 , I2 , X)array allDiff([I1 , . . . , ])abs(I1 , I)op(I1 , I2 , I)array op0 ([I1 , . . . , ], I)I1 rel I2V1 rel I2 Xi<j Ii 6= Ij|I1 | =I1 op I2 =I1 op0 op0 =Linear Constraints(15)(16)(17)(18)(19)(20)rel{leq, geq, eq, lt, gt}bool array sum rel([X1 , . . . , Xn ], I)bool array pb rel([c1 , . . . , cn ], [X1 , . . . , Xn ], I)bool array sum modK([X1 , . . . , Xn ], c, I)int array sum rel([I1 , . . . , ], I)int array lin rel([c1 , . . . , cn ], [I1 , . . . , ], I)int array sum modK([I1 , . . . , ], c, I)( Xi ) rel( ci Xi ) rel(( Xi ) mod c) =( Ii ) rel( ci Ii ) rel(( Ii ) mod c) =Lexical Order(21)(22)(23)(24)(25)(26)bool arrays lex(Xs1 , Xs2 )bool arrays lexLt(Xs1 , Xs2 )bool arrays lex reif(Xs1 , Xs2 , X)bool arrays lexLt reif(Xs1 , Xs2 , X)int arrays lex(Is1 , Is2 )int arrays lexLt(Is1 , Is2 )Xs1 precedes (leq) Xs2 lex orderXs1 precedes (lt) Xs2 lex orderX Xs1 precedes (leq) Xs2 lex orderX Xs1 precedes (lt) Xs2 lex orderIs1 precedes (leq) Is2 lex orderIs1 precedes (lt) Is2 lex orderTable 2: Syntax BEE Constraints.3. CNF encoding: best suited encoding technique applied simplified constraints.Bit-blasting equi-propagation BEE follow general descriptions Sections 2.4 3.1. Bit-blasting implemented Prolog unification. declaration form new int(I, c1 , c2 ) triggers unification = [1, . . . , 1, Xc1 +1 , . . . , Xc2 ] (toease presentation assume integer variables represented positive intervalstarting 0 limitation practice BEE also supports negativesintegers). BEE applies ad-hoc equi-propagators described Section 4. equalityform X = L (between variable literal constant) detected, equipropagation implemented unifying X L. unification applies occurrencesX sense propagates constraints involving X.Decomposition replacing complex constraints (for example arrays)simpler constraints (for example array elements). Consider, instance, constraint int array plus(As, Sum). decomposed list int plus constraints applyingstraightforward divide conquer recursive definition. base case, As=[A]325fiMetodi, Codish, & Stuckeyc = allDiff ([Z1 , Z2 , Z3 , . . . , Zn ])Eadd c (E)dom(Z1 )/ dom(Zk ) (k > 1)Z1 = authors{i, j} dom(Zk ) =(k > 2)dom(Z1 ) {i, j}dom(Z2 ) {i, j}Z1 6= Z2Zk 6= i, Zk 6= j(k > 2)Figure 14: Simplification rules allDiff .constraint replaced constraint form int eq(A,Sum) equates bitsSum, = [A1 , A2 ] replaced int plus(A1 , A2 , Sum). generalcase split two halves, constraints generated sum halves,additional int plus constraint introduced sum two sums.another example, consider int plus(A1 , A2 , A) constraint. One approach, supported BEE, decomposes constraint odd-even merger (from contextodd-even sorting networks) (Batcher, 1968). Here, sorted sequences bits A1 A2merged obtain sum A. results model O(n log n) comparatorconstraints (and later encoding O(n log n) clauses). Another approach, alsosupported BEE, decompose constraint encodes directly CNFsize O(n2 ), context so-called totalizers (Bailleux & Boufkhad, 2003). hybridapproach, leaves choice BEE, depending size domains variablesinvolved. Finally, note user configure BEE fix way compilesconstraint (and others).CNF encoding last phase compilation constraint model.remaining simplified (bit-blasted) constraints encoded directly CNF. encodingsstandard similar applied various tools. BEE encodings similarapplied Sugar (Tamura et al., 2009).6.1 All-Different Constraint BEEall-different constraint specifies set integer variables take different valuesspecified domains. constraint received much attention literature(see example survey van Hoeve, 2001). BEE provides special treatmentconstraint.many applications, all-different constraints applied model special caseconstraint permutation. Namely, [I1 , . . . , ] different maytake precisely n different values. BEE identifies special case applies two additionalad-hoc equi-propagation rules case. table Figure 14 illustrates rules.annotate constraint * emphasize detectedpermutation. first rule case one integer (assume Z1 )take value i. second rule case variables except two, assume Z1 ,Z2 , cannot take two values, assume i, j. Now, constraint permutation,326fiBoolean Equi-propagationdetermine Z1 Z2 must take two values j. illustrate secondrule consider following example.Example 16. Consider constraint allDiff(I1 , . . . , I5 ) 5 integer variables taking values interval [0, 4] (exactly 5 values) E0 specifies I3 , I4 I5 cannot takevalues 0 1. Therefore introduce equations restrict I1 I2 takevalues 0 1, corresponding ad-hoc rule permutation applies:E0I1I2I3I4I5x3,1 =1, x4,1 =1,x5,1 =1, x3,2 =1,=x4,2 =1, x5,2 =1= [x1,1 , . . . , x1,4 ],= [x2,1 , . . . , x2,4 ],= [1, 1, x3,3 , x3,4 ],= [1, 1, x4,3 , x4,4 ],= [1, 1, x5,3 , x5,4 ]dom(Ik ) {0, 1}=k>2allDiffE1I1I2I3I4I5x1,2 =0, . . . , x1,4 =0,x2,2 =0, . . . , x2,4 =0= E0x1,1 =x2,1= [x1,1 , 0, . . . , 0],= [x1,1 , 0, . . . , 0],= [1, 1, x3,3 , x3,4 ],= [1, 1, x4,3 , x4,4 ],= [1, 1, x5,3 , x5,4 ]facilitate implementation ad-hoc equi-propagation all-different constraints,BEE adopts dual representation integer variables occurring constraints combining order encoding the, so-called, direct encoding. essentiallyencoding proposed Gent Nightingale (2004). declaring integer variable I, bit-blast order encoding applies corresponding unification= [x1 , . . . , xn ]. encountering allDiff constraint, additional bit-blastintroduces I0 = [d0 , . . . , dn ] direct encoding, channeling formula channel(I, I0 )introduced.direct encoding unary representation I0 = [d0 , . . . , dn ] bit di trueI0 = i. So, exactly one bits takes value true. example, value3 interval [0, 5] represented 6 bits [0, 0, 0, 1, 0, 0]. dual representationfollowing channeling formula captures relation two representationsinteger variable = [x1 , . . . , xn ] I0 = [d0 , . . . , dn ].channel([x1 , . . . , xn ], [d0 , . . . , dn ]) =d0 = x1dn = xnn1^(di xi xi+1 )i=1Consider allDiff constraint integer variables take different values 0 n. constraint simplification, allDiff([I1 , . . . , Im ]) constraintviewed direct encoding bit matrix row consists bits[di0 , . . . , din ] Ii direct encoding. element dij true iff Ii takes value j.j th column specifies Ii take value j hence, one variablecolumn may take value true. representation one main advantage:direct encoding decompose allDiff([I1 , . . . , Im ]), conjunction n + 1 constraints, one column 0 j n, form bool array sum leq([d1j , . . . , dmj ], 1),arc-consistent. soon di,j = 1 (Ii = j) di,j 0 = 0 (Ii 6= j 0 )2j0 6= j. contrastfi order encoding alone decomposition O(m ) constraintsint neq(Ii , Ij ) fi < jarc-consistent. illustrate advantage dualencoding allDiff constraint Section 8.1.327fiMetodi, Codish, & Stuckey:- use module(bee compiler, [bCompile/2]).:- use module(sat solver, [sat/1]).solve(Instance, Solution) :encode(Instance, Map, Constraints),bCompile(Constraints, CNF),sat(CNF),decode(Map, Solution).Figure 15: generic application BEE.7. Using BEEtypical BEE application form depicted Figure 15 predicate solve/2takes problem Instance provides Solution. specifics applicationcall encode/3 given Instance generates Constraints solvetogether Map relating instance variables constraint variables. callsbCompile/2 sat/1 compile constraints CNF solve applying SAT solver.instance solution, SAT solver binds constraint variables accordingly.Then, call decode/2, using Map, provides Solution terms Instancevariables. definitions encode/3 decode/2 application dependent provided user. predicates bCompile/2 sat/1 part tool provideinterface BEE underlying SAT solver.7.1 Example BEE Application: Magic Graph Labelingillustrate application BEE using Prolog modeling language solve graphlabeling problem. Graph labeling finding assignment integers verticesedges graph subject certain conditions. Graph labellings introduced60s hundreds papers wide variety related problems publishedsince then. See example survey Gallian (2011) 1200 references.Graph labellings many applications. instance radars, X-ray crystallography,coding theory, etc.focus vertex-magic total labeling (VMTL) problem onefind graph G = (V, E) labeling one-to-one map V E {1, 2, . . . , |V | +|E|} property sum labels vertex incident edgesconstant K independent choice vertex. problem instance takes formvmtl(G, K) specifying graph G constant K. context Figure 15,query solve(vmtl(G, K), Solution) poses question: exist vmtl labelingG magic constant K? binds Solution indicate labeling one exists,unsat otherwise. Figure 16 illustrates example problem instance togethersolution.Figure 17 illustrates Prolog program implements encode/3 predicateVMTL problem. call predicate declareInts/4 introduces constraintsdeclare integer variables vertex edge graph, generatesmap. call predicate sumToK/5 introduces constraints require sumlabels vertex incident edges equals K. auxiliary predicate328fiBoolean Equi-propagationInstanceInstance = vmtl(G, K),G = (V, E),V = [1, 2, 3, 4],E = [(1, 2), (1, 3),(2, 3), (3, 4)],K = 14Graph4236666Solution1V1V2V3V4= 4,= 5,= 1,= 6,E(1,2)E(1,3)E(2,3)E(3,4)= 7,= 3,= 2,=8Figure 16: VMTL instance solution.encode(vmtl((Vs,Es),K),Map,Constraints):append(Vs,Es,VEs), length(VEs,N),declareInts(VEs,N,Map,Constraints-Cs2),sumToK(Vs,Es,Map,K,Cs2-Cs3),getVars(VEs,Map,Vars),Cs3=[int array allDiff(Vars)].declareInts([], , ,Cs-Cs).declareInts([ID|IDs],N,[(ID,X)|Map],[new int(X,1,N)|CsH]-CsT):declareInts(IDs,N,Map,CsH-CsT).sumToK([], , , ,Cs-Cs).sumToK([VID|Vs],Es,Map,K,[int array plus(Vars,K)|CsH]-CsT):findall((X,Y),(member((X,Y),Es),(X=VID ; Y=VID)),EsIDs),getVars([VID|EsIDs],Map,Vars),sumToK(Vs,Es,Map,K,CsH-CsT).getVars([], ,[]).getVars([ID|IDs],Map,[Var|Vars]):member((ID,Var),Map),getVars(IDs,Map,Vars).Figure 17: encode/3 predicate VMTL application BEEMap((1, 2), E1 ),(1, V1 ),((1, 3), E2 ),(2, V2 ),((2, 3), E3 ),(3, V3 ),((3, 4), E4 ),(4, V4 )Constraintsnew int(V1 , 1, 8), new int(E1 , 1, 8), int array plus([V1 , E1 , E2 ], K),new int(V2 , 1, 8), new int(E2 , 1, 8), int array plus([V2 , E1 , E3 ], K),new int(V3 , 1, 8), new int(E3 , 1, 8), int array plus([V3 , E2 , E3 , E4 ], K),new int(V4 , 1, 8), new int(E4 , 1, 8), int array plus([V4 , E4 ], K),new int(K, 14, 14), allDiff([V1 , V2 , V3 , V4 , E1 , E2 , E3 , E4 ])Figure 18: VMTL instance constraints map generated encode/3.getVars/3 receives list identifiers (vertices edges) extracts correspondinglist integer variables map.Given VMTL instance Figure 16, call predicate encode/3 Figure 17generates map constraints detailed Figure 18.329fiMetodi, Codish, & StuckeySolving constraints Figure 18 binds Map follows, indicating solution(in unary order encoding):(1,(2,=(3,(4,[1, 1, 1, 1, 0, 0, 0, 0]),[1, 1, 1, 1, 1, 0, 0, 0]),[1, 0, 0, 0, 0, 0, 0, 0]),[1, 1, 1, 1, 1, 1, 0, 0]),((1, 2),((1, 3),((2, 3),((3, 4),[1, 1, 1, 1, 1, 1, 1, 0]),[1, 1, 1, 0, 0, 0, 0, 0]),[1, 1, 0, 0, 0, 0, 0, 0]),[1, 1, 1, 1, 1, 1, 1, 1])Using BEE compile constraints Figure 18 generates CNF contains 301clauses 48 Boolean variables. Encoding set constraints without applyingsimplification rules generates larger CNF contains 642 clauses 97 Booleanvariables.Section 8.3 report using BEE enables us solve interesting instancesVMTL problem previously solvable techniques.7.2 BumbleBEEBEE distribution includes also command line solver, call BumbleBEE.BumbleBEE enables one specify BEE model input file line containssingle constraint model last line specifies type goal. BumbleBEEreads input file, compiles constraint model CNF, solves CNF usingembedded CryptoMiniSAT solver (Soos, 2010) outputs set bindings declaredvariables model (or message indicating constraints satisfiable).Figure 19 contains left BumbleBEE input file VMTL instanceFigure 16 right BumbleBEE output, solution constraintmodel. example, last line input file specifies goal solver.options are:1. solve satisfy: solve single satisfying assignment constraint model;2. solve satisfy(c): solve (at most) c satisfying assignments constraint modelc integer value. c 0 option solve solutions.3. solve minimize(I): solve solution minimizes value integervariable I. solver outputs intermediate solutions (with decreasing valuesI) encountered search minimum value I.4. solve maximize(I): similar minimize, maximizes.details examples found BEE distribution (Metodi & Codish,2012).8. Experimentsreport experience applying BEE. appreciate ease use readerencouraged view example encodings available tool (Metodi & Codish,2012). experiments run Intel Core 2 Duo E8400 3.00GHz CPU 4GB memoryLinux (Ubuntu lucid, kernel 2.6.32-24-generic). BEE written Prolog run330fiBoolean Equi-propagationContent BumbleBEE input filenew int(V1, 1, 8)new int(V2, 1, 8)new int(V3, 1, 8)new int(V4, 1, 8)new int(E1, 1, 8)new int(E2, 1, 8)new int(E3, 1, 8)new int(E4, 1, 8)int array plus([V1, E1, E2], 14)int array plus([V2, E1, E3], 14)int array plus([V3, E2, E3, E4], 14)int array plus([V4, E4], 14)int array allDiff([V1, V2, V3, V4, E1, E2, E3, E4])solve satisfyBumbleBEE outputV1 = 4V2 = 5V3 = 1V4 = 6E1 = 7E2 = 3E3 = 2E4 = 8==========Figure 19: Solving VMTL instance using BumbleBEE.using SWI Prolog v6.0.2 64-bits. Comparisons Sugar (v1.15.0) based useidentical constraint models, apply SAT solver (CryptoMiniSAT v2.5.1), runmachine. Times reported seconds.8.1 Quasigroup Completion ProblemsQuasigroup Completion Problem (QCP) proposed Gomes, Selman, Crato (1997)constraint satisfaction benchmark, given n n board integer variables (inrange [1, n]) assigned integer values. task assign valuesvariables, column row contains value twice. constraint modelconjunction allDiff constraints. Ansotegui, del Val, Dotu, Fernandez, Manya(2004) argue advantage direct encoding QCP.consider 15 instances 2008 CSP competition.2 Table 3 considers threesettings: BEE dual encoding allDiff constraints, BEE using orderencoding (equivalent using int neq constraints instead allDiff), Sugar.table shows: instance identifier (sat unsat), compilation time (comp) seconds,clauses encoding (clauses), variables encoding (vars), SAT solving time(SAT) seconds.results indicate that: (1) Application BEE using dual representationallDiff 38 times faster produces 20 times fewer clauses (in average)using order-encoding alone (despite need maintain two encodings); (2) Withoutdual representation, solving encodings generated BEE slightly fasterSugar BEE still generates CNF encodings 4 times smaller (on average)generated Sugar. Observe 3 instances found unsatisfiable BEE (indicated2. http://www.cril.univ-artois.fr/CPAI08/. competition instances specified using binary disequalities, use model allDiff.331fiMetodi, Codish, & Stuckeyinstance25-264-0 sat25-264-1 sat25-264-2 sat25-264-3 sat25-264-4 sat25-264-5 sat25-264-6 sat25-264-7 sat25-264-8 sat25-264-9 sat25-264-10 unsat25-264-11 unsat25-264-12 unsat25-264-13 unsat25-264-14 unsatTotalBEE (dualcomp clauses(sec)0.23 65090.20 74750.21 65310.21 68190.21 70820.21 70550.21 77120.21 74280.21 66030.21 67840.21 64910.1210.1610.1210.23 5984encoding)varsSAT(sec)13170.3315083.2913290.0713740.8314310.3414313.1215510.3414960.1313350.1813500.1912960.0400.0000.0000.0012100.078.93BEE (ordercomp clauses(sec)0.36 332240.30 343230.30 352380.29 324570.29 328250.30 335900.33 390150.30 365800.27 315610.27 354040.30 333210.28 379120.29 391350.29 350480.28 31093encoding)varsSAT(sec)8878.9591797.509052.4689918.5289719.0889746.1593269.8193719.9389610.3290334.0893010.929550.099840.089440.0988511.60349.58clausesSugarvars126733127222127062127757126777126973128354127106124153128423126999125373127539127026126628107701079810787108271077910784108501079410687108531078510744108151078610771SAT(sec)34.2013.938.0644.0385.9241.0412.677.019.6938.8057.750.470.570.5615.93370.63Table 3: QCP results 25 25 instances 264 holesCNF single clause variables). comment Sugar pre-processingtimes higher BEE indicated table.8.2 Word Design DNAProblem 033 CSPLib seeks largest parameter n, existsset n eight-letter words alphabet = {A, C, G, } followingproperties: (1) word exactly 4 symbols {C, G}; (2) pairdistinct words differ least 4 positions; (3) every x, S: xR (the reversex) C (the word obtained replacing , C G, vice versa)differ least 4 positions.Mancini, Micaletto, Patrizi, Cadoli (2008) provide comparison several stateof-the-art solvers applied DNA word problem variety encoding techniques.best reported result solution 87 DNA words, obtained 554 seconds, usingOPL (van Hentenryck, 1999) model lexicographic order break symmetry. Frutos,Liu, Thiel, Sanner, Condon, Smith, Corn (1997) present strategy solve problemfour letters modeled bit-pairs [t, m]. eight-letter wordviewed combination t-part, [t1 , . . . , t8 ], bit-vector, m-part,[m1 , . . . , m8 ], also bit-vector. authors report solution composed two pairs(t-part m-part) sets3 [T1 , M1 ] [T2 , M2 ] |T1 | = 6, |M1 | = 16, |T2 | = 2,|M2 | = 6. forms set (6 16) + (2 6) = 108 DNA words. Marc van Dongenreports larger solution 112 words.4Building approach described Frutos et al. (1997), pose conditions setst-parts m-parts, , Cartesian product =satisfy requirements original problem. three conditions below,required satisfy (10 ) (20 ), required satisfy (20 ) (30 ). set3. notions t-part m-part slightly different ours.4. See http://www.cs.st-andrews.ac.uk/~ianm/CSPLib/.332fiBoolean Equi-propagationbit-vectors V , conditions are: (10 ) bit-vector V sums 4; (20 ) pairdistinct bit-vectors V differ least 4 positions; (30 ) pair bit-vectors(not necessarily distinct) u, v V , uR (the reverse u) v C (the complement v)differ least 4 positions. equivalent requiring (uR )C differs vleast 4 positions.strategy model BEE encoding. instance takes formdna(n1 , n2 ) signifying numbers bit-vectors, n1 n2 sets . Withoutloss generality, impose, remove symmetries, lexicographicallyordered. solution Cartesian product = .Using BEE, find, fraction second, sets t-parts size 14 m-partssize 8. provides solution size 14 8 = 112 DNA word problem. RunningComet (v2.0.1) find 112 word solution 10 seconds using model HakanKjellerstrand.5 Using BEE, also prove exist set 15 t-parts (0.15seconds), set 9 m-parts (4.47 seconds). facts unknown prior BEE.Proving solution DNA word problem 112 words,without restriction two part t-m strategy, still open problem.8.3 Vertex Magic Total LabelingMacDougall, Miller, Slamin, Wallis (2002) conjecture n vertex complete graph,Kn , n 5 vertex magic total labeling magic constants specific rangevalues k, determined n. conjecture proved correct odd n verifiedbrute force n = 6. address cases n = 8 n = 10 involve 15 instances(different values k) n = 8, 23 (different values k) n = 10. Startingsimple constraint model (illustrated example Figure 16), add additionalconstraints exploit fact graphs symmetric: (1) assume edgesmallest label e1,2 ; (2) assume labels edges incident v1ordered hence introduce constraints e1,2 < e1,3 < < e1,n ; (3) assumelabel edge e1,3 smaller labels edges incident v2 (except e1,2 )introduce constraints accordingly. setting BEE solve except 2 instances4 hour timeout Sugar solve except 4.Table 4 gives results 10 hardest instances K8 20 hardest instances K104 hour time-out. BEE compilation times order 0.5 sec/instance K82.5 sec/instance K10 . Sugar encoding times slightly larger. instancesindicated magic constant, k; columns BEE Sugar indicate SAT solvingtimes (in seconds). bottom two lines indicate average encoding sizes (numbers clausesvariables).results indicate Sugar encodings (in average) 60% larger,average SAT solving time BEE encodings 2 times faster (average excluding instances Sugar times-out).address two VMTL instances solvable using BEE models described(K10 magic labels 259 258), partition problem fixing values e1,2e1,3 maintaining constraints. Analysis symmetry breakingconstraints indicates results 198 new instances two cases.5. See http://www.hakank.org/comet/word_design_dna1.co.333fiMetodi, Codish, & StuckeyinstanceBEEK8kSAT (sec)1431.2614210.141417.6414014.6813925.6013812.9913722.9113614.46135298.54134331.80Average CNF size:clauses248000vars5688SugarSAT (sec)2.871.622.946.466.672.80298.58251.82182.90instanceBEEK10kSAT (sec)2775.312767.1127513.572744.9327345.9427222.742717.352706.032695.2026894.4426788.51266229.802651335.31264486.09263236.682621843.702612771.602604873.99259258Average CNF size:clauses1229000vars155294020009370SugarSAT (sec)9.259.9119.639.249.0386.459.4955.9411.05424.89175.70247.56259.45513.61648.436429.257872.76196600025688Table 4: VMTL results K8 K10 (4 hour timeout)original VMTL instance solved one 198 instances solved. So, solveparallel. Fixing e1,2 e1,3 fuels compiler encodings considerablysmaller. instance k = 259 solved 1379.50 seconds e1,2 = 1 e1,3 = 6.compilation time 2.09 seconds encoding consists 1 million clauses15 thousand variables.best knowledge, hard instances suite beyond reachprevious approaches program search magic labels. SAT based approachpresented Jager (2010) cannot handle these.6 comparison Sugar indicatesimpact compiler.8.4 Balanced Incomplete Block DesignsProblem 028 CSPlib (BIBD) instance defined 5-tuple positiveintegers [v, b, r, k, ] requires partition v distinct objects b blocksblock contains k different objects, exactly r objects occur block, every twodistinct objects occur exactly blocks.6. Personal communication (Gerold Jager), March 2012.334fiBoolean Equi-propagationFigure 20: BIBD symmetry breaking.naive model BIBD instance [v, b, r, k, ] introduces following constraintsv b Boolean incidence matrix: (1) exactly r ones row, (2) exactly k onescolumn, (3) exactly ones scalar product two (different) rows.model contain sufficient degree information trigger equipropagation process. order take advantage BEE simplifications addedsymmetry breaking described Frisch, Jefferson, Miguel (2004) illustratedFigure 20: row viewed sequence four parts . . . sizes , (r ),(r ), (b 2r + ). first row fixed assigning parts B ones(marked black) parts C zeros (marked white). second rowfixed assign parts C ones (marked black) parts B zeros(marked white). third subsequent rows (marked gray), sum constraints decomposed summing part (A . . . D) summing resultsfollows: + B = , + C = , C + = r , B + = r . ensuresrow contains exactly r ones scalar product first (and second) row. denote constraint model SymB (for symmetry breaking).instance[v, b, r, k, ][7, 420, 180, 3, 60][7, 560, 240, 3, 80][12, 132, 33, 3, 6][15, 45, 24, 8, 12][15, 70, 14, 3, 2][16, 80, 15, 3, 2][19, 19, 9, 9, 4][19, 57, 9, 3, 1][21, 21, 5, 5, 1][25, 25, 9, 9, 3][25, 30, 6, 5, 1]Total (sec)comp(sec)1.653.730.950.510.560.810.230.340.020.640.10BEE (SymB)clausesSAT(sec)6985791.731211941 13.601802380.731160168.46815630.391094420.56399310.091130530.1700.00920591.33245940.0636.66Sugar (SymB)compclausesSAT(sec)(sec)12.01 2488136 13.2411.74 2753113 36.4383.37 13322417.094.2446608623.585400891.8764.816237732.262.271259760.4931.9137160.0142.655690078.5216.02933880.42> 722.93SatELite (SymB)compclauses SAT(sec)(sec)1.67802576 2.182.73 1397188 5.181.18184764 0.570.641341461.0279542 0.201.14105242 0.350.444714 0.0910.45111869 0.140.010 0.001.0197623 8.931.223828 0.05> 219.14Table 5: BIBD results (180 sec. timeout)Table 5 shows results comparing BEE (compilation time, clauses encoding, SATsolving time) Sugar using SymB model. also compare BEE SatELite (Een335fiMetodi, Codish, & Stuckey& Biere, 2005), CNF minimizer, input SatELite CNF encodingSymB model generated BEE without applying simplifications. compilationtime (comp) indicates SatELite pre-processing time. final row indicates totalcompilation SAT solving time entire suite approach. casestime measured seconds.experiment indicates BEE generates significantly smaller CNF Sugaraffects SAT solving time. Moreover, Sugar compilation time extremelylong. comparing BEE SatELite see output CNFsimilar size SatELite applied entire CNF, instancescompilation time significantly longer solving time.instance[v, b, r, k, ][7, 420, 180, 3, 60][7, 560, 240, 3, 80][12, 132, 33, 3, 6][15, 45, 24, 8, 12][15, 70, 14, 3, 2][16, 80, 15, 3, 2][19, 19, 9, 9, 4][19, 57, 9, 3, 1][21, 21, 5, 5, 1][25, 25, 9, 9, 3][25, 30, 6, 5, 1]TotalBEE (SymB)compSAT1.651.733.73 13.600.950.730.518.460.560.390.810.560.230.090.340.170.020.000.641.330.100.0636.66[M06]0.540.665.5112.22107.4353.231.26>900.00MinionSymB1.361.771.4213.4038.301.710.671.37>600.00SymB+0.420.521.7675.870.310.350.310.350.150.920.3181.24Table 6: BIBD results, comparison Minion (times seconds; 180 sec. timeout).Table 6 shows results comparing BEE using SymB model Minion constraintsolver (Gent, Jefferson, & Miguel, 2006). consider three different models Minion:[M06] indicates results using BIBD model described Gent et al. (2006), SymB usesmodel use SAT approach, SymB+ , enhanced symmetry breakingmodel tricks applied also [M06] model. columnstimeouts show total times (for BEE includes compile time SAT solving). Noteusing clever modeling problem improved also previous run-timesMinion.experiment indicates BEE significantly faster Minion BIBDmodels ([M06]). tailoring SymB model, Minion becomes competitiveours.8.5 Combining BEE SatELitedemonstrate impact combining BEE SatELite. describe experiments involving two benchmarks SatELite applied simplify outputBEE. idea first apply powerful, local, techniques, performed BEE.reduces size CNF fast. apply SatELite takes globalconsiderations CNF whole. wish determine smaller, simplified,336fiBoolean Equi-propagationCNF amenable simplification using SatELite. results indicatealthough CNF size slightly decreased, solving times often increased, sometimesdrastically.Tables 7 8 show results. tables four columns BEEheading indicate: BEE compilation time, size encoding (clauses variables),subsequent SAT solving time. Similarly, four columns SatELite headingindicate application SatELite output BEE: SatELite processing time,size resulting CNF (clauses variables), subsequent SAT solving time.Table 7 illustrates results BIBD benchmark Section 8.4 Table 8, results10 hardest VMTL instances K8 K10 described Section 8.3. Observeapplying SatELite output BEE decreases CNF size slightlyimprove SAT solving time. fact, contrary, cases renders CNFtakes time solve. several cases, SAT solving time increases drasticallyintroduce timeout.instance[v, b, r, k, ][7, 420, 180, 3, 60][7, 560, 240, 3, 80][12, 132, 33, 3, 6][15, 45, 24, 8, 12][15, 70, 14, 3, 2][16, 80, 15, 3, 2][19, 19, 9, 9, 4][19, 57, 9, 3, 1][21, 21, 5, 5, 1][25, 25, 9, 9, 3][25, 30, 6, 5, 1]comp(sec)1.653.730.950.510.560.810.230.340.020.640.10BEEclausesvars6985791211941180238116016815631094423993111305309205924594413995844531947195071969326223927365760220982160SAT(sec)1.7313.600.738.460.390.560.090.170.001.330.06comp(sec)1.883.141.200.660.981.130.3812.490.000.971.14SatELiteclausesvars6969141209788179700115938786301047603980511231409173624028387495404328351176421587721116798862300185401926SAT(sec)3.416.970.910.350.500.160.370.0010.340.09Table 7: BIBD results, BEE combined SatELite (180 sec. timeout)results demonstrate application SatELite remove redundanciesCNF often non-beneficial. Presumably difference see applicationSatELite CNF benchmarks results fact BEE produces highlyoptimized CNF output, many CNF benchmarks significant inefficiencyoriginal encoding. BEE removes variable CNF, also instantiatesvariable, either constant equivalent variable, removepotential propagations encoding, captured Theorem 9.9. Conclusionconsiderable body work CNF simplification techniques clear trade-offamount reduction achieved invested time. approaches determine binary clauses implied CNF, certainly enough determine Booleanequalities. problem determining binary clauses implied CNFprohibitive SAT model may involve many (hundreds of) thousands variables.337fiMetodi, Codish, & StuckeyinstanceK8K10143142141140139138137136135134267266265264263262261260259258comp(sec)0.510.270.200.190.180.180.180.180.180.180.650.650.650.650.650.650.650.650.650.65BEEclausesvars2485582484142482542480782478862476782474542472142469582466861228962122866012283381227996122763412272521226850122642812259861225524572457165708570056925684567656685660565215529155291552915529155291552915529155291552915529SAT(sec)1.2610.147.6414.6825.612.9922.9114.46298.54331.888.51229.81335.31486.09236.681843.72771.64873.99comp(sec)2.602.592.592.602.592.602.592.592.582.593.023.013.023.023.013.023.043.023.033.01SatELiteclausesvars2482502481072479472477712475792473712471472469072466512463791228368122806612277441227402122704012266581226256122583412253921224930545254455437542954215413540553975389538114990149901499014990149901499014990149901499014990SAT(sec)0.983.2232.813.506.1812.1877.1697.69705.48430.00259.55540.4863.741008.061916.73Table 8: VTML results, BEE combined SatELite (4 hour timeout)Typically implied binary clauses determined, visibleunit propagation. trade-off regulated choice techniques applied inferbinary clauses, considering power cost. See example work Een Biere(2005) references therein. also approaches (Li, 2003) detect useBoolean equalities run-time, complementary approach.approach, beast tamed introducing notion locality.consider full CNF. Instead, maintaining original representation, conjunctionconstraints, viewed Boolean formula, apply powerful reasoning techniquesseparate parts model maintain efficient pre-processing.end, introduce BEE, compiler follows approach encode finitedomain constraints CNF. Applying optimizations based ad-hoc equi-propagationpartial evaluation rules high level view problem allows us simplify problemaggressively possible CNF representation. resulting CNF modelssignificantly smaller resulting straight translation.well-understood making CNF smaller ultimate goal: often smallerCNFs harder solve. Indeed, one often introduces redundancies improve SATencodings: removing counterproductive. experience BEE reducessize encoding way productive subsequent SAT solving.particular, removing variables determined compile time definitelyequal (or definitely different) solution.338fiBoolean Equi-propagationBEE uses ad-hoc equi-propagation partial evaluation rules keeps compilationtimes typically small (measured seconds) even instances result several millionsCNF clauses. reduction SAT solving time larger orders magnitude.Hence, believe Boolean equi-propagation makes important contributionencoding CSPs SAT.BEE currently tuned represent integers order encoding. Ongoing workaims extend BEE binary additional number representations mixed radixbases considered Een Sorensson (2006) Codish, Fekete, Fuhs,Schneider-Kamp (2011).Acknowledgmentsthank Vitaly Lagoon many insightful discussions concerning research.NICTA funded Australian Government represented DepartmentBroadband, Communications Digital Economy Australian Research Council ICT Centre Excellence Program.ReferencesAnsotegui, C., del Val, A., Dotu, I., Fernandez, C., & Manya, F. (2004). Modeling choicesquasigroup completion: SAT vs. CSP. McGuinness, D. L., & Ferguson, G. (Eds.),AAAI, pp. 137142, San Jose, California, USA. AAAI Press / MIT Press.Asn, R., Nieuwenhuis, R., Oliveras, A., & Rodrguez-Carbonell, E. (2011). Cardinalitynetworks: theoretical empirical study. Constraints, 16 (2), 195221.Bagnara, R., & Schachte, P. (1998). Factorizing equivalent variable pairs ROBDDbased implementations Pos. Haeberer, A. M. (Ed.), Algebraic MethodologySoftware Technology, 7th International Conference, AMAST 98, Amazonia, Brasil,January 4-8, 1999, Proceedings, Vol. 1548 Lecture Notes Computer Science, pp.471485.Bailleux, O., & Boufkhad, Y. (2003). Efficient CNF encoding Boolean cardinality constraints. Rossi, F. (Ed.), CP, Vol. 2833 LNCS, pp. 108122, Kinsale, Ireland.Springer.Barrett, C., Stump, A., & Tinelli, C. (2010). Satisfiability Modulo Theories Library(SMT-LIB). www.SMT-LIB.org.Batcher, K. E. (1968). Sorting networks applications. AFIPS Spring JointComputing Conference, Vol. 32 AFIPS Conference Proceedings, pp. 307314, Atlantic City, NJ, USA. Thomson Book Company, Washington D.C.Bessiere, C., Katsirelos, G., Narodytska, N., & Walsh, T. (2009). Circuit complexitydecompositions global constraints. Proceedings IJCAI 2009, pp. 412418.Cadoli, M., & Schaerf, A. (2005). Compiling problem specifications SAT. ArtificialIntelligence, 162 (1-2), 89120.339fiMetodi, Codish, & StuckeyCodish, M., Fekete, Y., Fuhs, C., & Schneider-Kamp, P. (2011). Optimal base encodingspseudo-Boolean constraints. Abdulla, P. A., & Leino, K. R. M. (Eds.), TACAS,Vol. 6605 Lecture Notes Computer Science, pp. 189204. Springer.Codish, M., Lagoon, V., & Stuckey, P. J. (2008). Logic programming satisfiability.TPLP, 8 (1), 121128.Codish, M., & Zazon-Ivry, M. (2010). Pairwise cardinality networks. Clarke, E. M., &Voronkov, A. (Eds.), LPAR (Dakar), Vol. 6355 Lecture Notes Computer Science,pp. 154172. Springer.Coudert, O., & Madre, J. C. (1990). unified framework formal verificationsequential circuits. ICCAD, pp. 126129.Crawford, J. M., & Baker, A. B. (1994). Experimental results application satisfiability algorithms scheduling problems. Hayes-Roth, B., & Korf, R. E. (Eds.),AAAI, Vol. 2, pp. 10921097, Seattle, WA, USA. AAAI Press / MIT Press.Een, N., & Biere, A. (2005). Effective preprocessing SAT variable clauseelimination. Bacchus, F., & Walsh, T. (Eds.), SAT, Vol. 3569 Lecture NotesComputer Science, pp. 6175. Springer.Een, N., & Sorensson, N. (2003). extensible SAT-solver. Giunchiglia, E., & Tacchella, A. (Eds.), SAT, Vol. 2919 Lecture Notes Computer Science, pp. 502518.Springer.Een, N., & Sorensson, N. (2006). Translating pseudo-Boolean constraints SAT. JSAT,2 (1-4), 126.Frisch, A. M., Jefferson, C., & Miguel, I. (2004). Symmetry breaking prelude impliedconstraints: constraint modeling pattern. Proc. 16th Euro. Conf. AI, 171175,pp. 171175. Press.Frutos, A. G., Liu, Q., Thiel, A. J., Sanner, A. M. W., Condon, A. E., Smith, L. M., &Corn, R. M. (1997). Demonstration word design strategy DNA computingsurfaces. Journal Nucleic Acids Research, 25 (23), 47484757.Gallian, J. A. (2011). dynamic survey graph labeling. Electronic JournalCombinatorics, 18.Gavanelli, M. (2007). log-support encoding CSP SAT. Bessiere, C. (Ed.),CP, Vol. 4741 LNCS, pp. 815822, Providence, RI, USA. Springer.Gelder, A. V. (2005). Toward leaner binary-clause reasoning satisfiability solver. Ann.Math. Artif. Intell., 43 (1), 239253.Gent, I. P., Jefferson, C., & Miguel, I. (2006). Minion: fast scalable constraint solver.Brewka, G., Coradeschi, S., Perini, A., & Traverso, P. (Eds.), ECAI, Vol. 141Frontiers Artificial Intelligence Applications, pp. 98102. IOS Press.Gent, I. P., & Nightingale, P. (2004). new encoding alldifferent SAT. Proceedings3rd International Workshop Modeling Reformulating Constraint Satisfaction Problems, http://www-users.cs.york.ac.uk/frisch/Reformulation/04/proceedings.pdf.340fiBoolean Equi-propagationGomes, C. P., Selman, B., & Crato, N. (1997). Heavy-tailed distributions combinatorialsearch. Smolka, G. (Ed.), CP, Vol. 1330 LNCS, pp. 121135. Springer.Heule, M., Jarvisalo, M., & Biere, A. (2011). Efficient CNF simplification based binaryimplication graphs. Sakallah, K. A., & Simon, L. (Eds.), SAT, Vol. 6695 LectureNotes Computer Science, pp. 201215. Springer.Huang, J. (2008). Universal Booleanization constraint models. CP2008, Vol. 5202Lecture Notes Computer Science, pp. 144158.Jager, G. (2010). effective SAT encoding magic labeling. Faigle, U., Schrader, R.,& Herrmann, D. (Eds.), CTW, pp. 97100.Li, C. (2003). Equivalent literal propagation DLL procedure. Discrete AppliedMathematics, 130 (2), 251276.MacDougall, J., Miller, M., Slamin, M., & Wallis, W. (2002). Vertex-magic total labelingsgraphs. Utilitas Mathematica, 61, 321.Mancini, T., Micaletto, D., Patrizi, F., & Cadoli, M. (2008). Evaluating ASP commercialsolvers CSPLib. Constraints, 13 (4), 407436.Manthey, N. (2012). Coprocessor 2.0 - flexible CNF simplifier - (tool presentation).Cimatti, A., & Sebastiani, R. (Eds.), SAT, Vol. 7317 Lecture Notes ComputerScience, pp. 436441. Springer.Metodi, A. (2012). BEE. http://amit.metodi.me/research/bee/.Metodi, A., & Codish, M. (2012). Compiling finite domain constraints SAT BEE.TPLP, 12 (4-5), 465483.Metodi, A., Codish, M., Lagoon, V., & Stuckey, P. J. (2011). Boolean equi-propagationoptimized SAT encoding. Lee, J. H.-M. (Ed.), CP, Vol. 6876 LNCS, pp. 621636.Springer.Nethercote, N., Stuckey, P. J., Becket, R., Brand, S., Duck, G. J., & Tack, G. (2007).Minizinc: Towards standard CP modeling language. Bessiere, C. (Ed.), CP2007,Vol. 4741 Lecture Notes Computer Science, pp. 529543, Providence, RI, USA.Springer-Verlag.Somenzi, F. (2009). CUDD: Colorado University Decision Diagram package. (Online,accessed 13 April 2011). http://vlsi.colorado.edu/~fabio/CUDD/.Soos, M. (2010). CryptoMiniSAT, v2.5.1. http://www.msoos.org/cryptominisat2.Tamura, N., Taga, A., Kitagawa, S., & Banbara, M. (2009). Compiling finite linear CSPSAT. Constraints, 14 (2), 254272.Tarjan, R. (1975). Efficiency good linear set union algorithm. JACM, 22 (2),215225.van Hentenryck, P. (1999). OPL Optimization Programming Language. MIT Press.van Hoeve, W. J. (2001). alldifferent constraint: survey.. CoRR:http://arxiv.org/abs/cs.PL/0105015.341fiJournal Artificial Intelligence Research 46 (2013) 89-127Submitted 03/12; published 01/13Automatic Aggregation Joint ModelingAspects ValuesChristina SauperRegina Barzilaycsauper@csail.mit.eduregina@csail.mit.eduComputer Science Artificial Intelligence LaboratoryMassachusetts Institute Technology32 Vassar St.Cambridge, 02139 USAAbstractpresent model aggregation product review snippets joint aspect identification sentiment analysis. model simultaneously identifies underlying setratable aspects presented reviews product (e.g., sushi miso Japaneserestaurant) determines corresponding sentiment aspect. approachdirectly enables discovery highly-rated inconsistent aspects product. generative model admits efficient variational mean-field inference algorithm. also easilyextensible, describe several modifications effects model structureinference. test model two tasks, joint aspect identification sentiment analysis set Yelp reviews aspect identification alone set medical summaries.evaluate performance model aspect identification, sentiment analysis,per-word labeling accuracy. demonstrate model outperforms applicablebaselines considerable margin, yielding 32% relative error reduction aspectidentification 20% relative error reduction sentiment analysis.1. IntroductionOnline product reviews become increasingly valuable influential source information consumers. ability explore range opinions allows consumersform general opinion product gather information positive negativeaspects (e.g., packaging battery life). However, reviews added time,problem information overload gets progressively worse. example, hundredsreviews restaurant, consumers read handful making decision.work, goal summarize large number reviews discoveringinformational product aspects associated user sentiment.address need, online retailers often use simple aggregation mechanisms represent spectrum user sentiment. Many sites, Amazon, simply presentdistribution user-assigned star ratings, approach lacks reasoningproducts given rating. retailers use breakdowns specificpredefined domain-specific aspects, food, service, atmosphere restaurant.breakdowns continue assist effective aggregation; however, aspectspredefined, generic particular domain explanation one aspect rated well poorly. Instead, truly informative aggregation,c2013AI Access Foundation. rights reserved.fiSauper & Barzilayproduct needs assigned set fine-grained aspects specifically tailoredproduct.goal work provide mechanism effective unsupervised contentaggregation able discover specific, fine-grained aspects associated values. Specifically,represent data set collection entities; instance, representproducts domain online reviews. interested discovering fine-grainedaspects entity (e.g., sandwiches dessert restaurant). Additionally, wouldlike recover value associated aspect (e.g., sentiment product reviews).summary input output found Figure 1. input consists shorttext snippets multiple reviews several products. restaurant domain,Figure 1, restaurants. assume snippet opinion-bearingdiscusses one aspects relevant particular product. outputconsists set dynamic (i.e., pre-specified) aspects product, snippets labeledaspect discuss, sentiment values snippet individuallyaspect whole. Figure 1, aspects identified Tasca Spanish Tapas includechicken, dessert, drinks, snippets labeled aspects describecorrect polarity.One way approach problem treat multi-class classification problem.Given set predefined domain-specific aspects, would fairly straightforwardhumans identify aspect particular snippet describes. However, taskdiscovering fine-grained entity-specific aspects, way know prioriaspects may present across entire data set provide training data each; instead,must select aspects dynamically. Intuitively, one potential solution clusterinput snippets, grouping lexically similar without prior knowledgeaspects represent. However, without knowledge words representaspect given snippet, clusters may align ones useful cross-review analysis.Consider, example, two clusters restaurant review snippets shown Figure 2.clusters share many words among members, first describescoherent aspect cluster, namely drinks aspect. snippets second clusterdiscuss single product aspect, instead share expressions sentiment.successfully navigate challenge, must distinguish words indicate aspect, words indicate sentiment, extraneous words neither.aspect identification sentiment analysis, crucial know words withinsnippet relevant task. Distinguishing straightforward, however.work sentiment analysis relies predefined lexicon WordNet providehints, way anticipate every possible expression aspect sentiment,especially user-generated data (e.g., use slang deeeeeee-lish delicious).lieu explicit lexicon, attempt use information proxy,part speech; example, aspect words likely nouns, value wordslikely adjectives. However, show later paper, additional informationsufficient tasks hand.Instead, propose approach analyze collection product review snippetsjointly induce set learned aspects, respective value (e.g., sentiment).capture idea using generative Bayesian topic model set aspectscorresponding values represented hidden variables. model takes collection90fiAutomatic Aggregation Joint Modeling Aspects ValuesInputOutputTasca Spanish TapasTasca Spanish TapasReview 1chicken cooked perfectlydessert goodChicken+ chicken cooked perfectlychicken tough tasty+ Moist delicious chickenReview 2red wines cheapexcellent creme bruleeReview 3used frozen small shrimpchicken tough tastyPitcher sangria pretty goodDouzo Sushi BarReview 1sushi creative pretty goodponzu overpoweringReview 2Real wasabi thats fresh!torched roll tasted rather blandDessert+ dessert good+ excellent creme bruleeDrinksred wines cheap+ Pitcher sangria pretty goodDouzo Sushi BarSushi+ sushi creative pretty goodtorched roll tasted rather blandCondimentsponzu overpowering+ Real wasabi thats fresh!Figure 1: example desired input output system restaurantdomain. input consists collection review snippets several restaurants.output aggregation snippets aspect (e.g., chicken dessert) alongassociated sentiment snippet. Note input data completely unannotated;information given snippets describe restaurant.snippets input explains observed text arises latent variables,thereby connecting text fragments corresponding aspects values.Specifically, begin defining sets sentiment word distributions aspect worddistributions. expect types sentiment words consistent acrossproducts (e.g., product may labeled great terrible), allow positivenegative sentiment word distributions shared across products.hand, case restaurant reviews similar domains, aspect words expectedquite distinct products. Therefore, assign product set aspectword distributions. addition word distributions, model takes accountseveral factors. First, model idea particular aspect productunderlying quality; is, already 19 snippets praising particular aspect,likely 20th snippet positive well. Second, account commonpatterns language using transition distribution types words. example,common see pattern Value Aspect, phrases like great pasta.Third, model distributions parts speech type distribution.91fiSauper & BarzilayCoherent aspect cluster+martinisgood.:::::::::drinkswine :::::::::martinis - tasty.::::::::::::-winelist pricey.::::::::::::::wine:::::::::::selection horrible.Incoherent aspect cluster+sushi :::::best :::::Ive :::::ever:::::had.BestpaellaIdeverhad.:::::::::::::::::::fillet beststeak wed ever had.:::::::::::::::::::::::::::bestsoupIveever had.::::::::::::::::::::::::::Figure 2: Example clusters restaurant review snippets generated lexical clusteringalgorithm; words relevant clustering highlighted. first cluster represents coherent aspect underlying product, namely drinks aspect. latter cluster simplyshares common sentiment expression represent snippets discussingproduct aspect. work, aim produce first type aspect cluster alongcorresponding values.covers intuition aspect words frequently nouns, whereas value words oftenadjectives. describe factors model whole detail Section 4.formulation provides several advantages: First, model require setpredefined aspects. Instead, capable assigning latent variables discoverappropriate aspects based data. Second, joint analysis aspect valueallows us leverage several pieces information determine words relevantaspect identification used sentiment analysis, including partspeech global entity-specific distributions words. Third, Bayesian modeladmits efficient mean-field variational inference procedure parallelizedrun quickly even large numbers entities snippets.evaluate approach domain restaurant reviews. Specifically, use setsnippets automatically extracted restaurant reviews Yelp. collection consistsaverage 42 snippets 328 restaurants Boston area, representingwide spectrum opinions several aspects restaurant. demonstratemodel accurately identify clusters review fragments describeaspect, yielding 32.5% relative error reduction (9.9 absolute F1 ) standalone clusteringbaseline. also show model effectively identify snippet sentiment,19.7% relative error reduction (4.3% absolute accuracy) applicable baselines. Finally,test models ability correctly label aspect sentiment words, discoveringaspect identification high-precision, sentiment identification highrecall.Additionally, apply slimmed-down version model focuses exclusivelyaspect identification set lab- exam-related snippets medical summariesprovided Pediatric Environmental Health Clinic (PEHC) Childrens HospitalBoston. summaries represent concise overviews patient information par92fiAutomatic Aggregation Joint Modeling Aspects Valuesticular visit, relayed PEHC doctor childs referring physician. modelachieves 7.4% (0.7 absolute F1 ) standalone clustering baseline.remainder paper structured follows. Section 2 compares workprevious work aspect identification sentiment analysis. Section 3 describesspecific problem formulation task setup concretely. Section 4 presents detailsfull model various model extensions, Section 5 describes inference procedure necessary adjustments extension. details data sets,experimental formulation, results presented Section 6. summarize findingsconsider directions future work Section 7. code data used paperavailable online http://groups.csail.mit.edu/rbg/code/review-aggregation.2. Related Workwork falls area multi-aspect sentiment analysis. section, firstdescribe approaches toward document-level sentence-level sentiment analysis (Section2.1), provide foundation future work, including own. Then, describethree common directions multi-aspect sentiment analysis; specifically, usedata-mining fixed-aspect analysis (Section 2.2.1), incorporate sentimentanalysis multi-document summarization (Section 2.2.2), finally, focusedtopic modeling additional sentiment components (Section 2.2.3).2.1 Single-Aspect Sentiment AnalysisEarly sentiment analysis focused primarily identification coarse document-level sentiment (Pang, Lee, & Vaithyanathan, 2002; Turney, 2002; Pang & Lee, 2008). Specifically,approaches attempted determine overall polarity documents. approaches included rule-based machine learning approaches: Turney (2002) usedrule-based method extract potentially sentiment-bearing phrases comparedsentiment known-polarity words, Pang et al. (2002) used discriminativemethods features unigrams, bigrams, part-of-speech tags, word positioninformation.document-level sentiment analysis give us overall view opinion, looking individual sentences within document yields fine-grained analysis.work sentence-level sentiment analysis focuses first identifying sentiment-bearing sentences determining polarity (Yu & Hatzivassiloglou, 2003; Dave, Lawrence,& Pennock, 2003; Kim & Hovy, 2005, 2006; Pang & Lee, 2008). identificationsentiment-bearing sentences polarity analysis performed supervisedclassifiers (Yu & Hatzivassiloglou, 2003; Dave et al., 2003) similarity known text (Yu& Hatzivassiloglou, 2003; Kim & Hovy, 2005), measures based distributionalsimilarity using WordNet relationships.recognizing connections parts document, sentiment analysisimproved (Pang & Lee, 2004; McDonald, Hannan, Neylon, Wells, & Reynar, 2007;Pang & Lee, 2008). Pang Lee (2004) leverage relationship sentencesimprove document-level sentiment analysis. Specifically, utilize subjectivityindividual sentences information strength connection sentencesmin cut formulation provide better sentiment-focused summaries text. McDonald93fiSauper & Barzilayet al. (2007) examine different connection, instead constructing hierarchical modelsentiment sentences documents. model uses complete labelingsubset data learn generalized set parameters improve classification accuracydocument-level sentence-level.none approaches attempt identify aspects analyze sentimentaspect-based fashion, intuitions provide key insight approaches takework. example, importance distinguishing opinion sentences followsintuition necessity identifying sentiment-bearing words within snippet.2.2 Aspect-Based Sentiment AnalysisFollowing work single-aspect document-level sentence-level sentiment analysiscame intuition modeling aspect-based (also called feature-based) sentiment review analysis. divide approaches roughly three types systems basedtechniques: systems use fixed-aspect approaches data-mining techniquesaspect selection sentiment analysis, systems adapt techniques multi-documentsummarization, systems jointly model aspect sentiment probabilistictopic models. Here, examine avenue work relevant examples contrastwork.2.2.1 Data-Mining Fixed-Aspect Techniques Sentiment AnalysisOne set approaches toward aspect-based sentiment analysis follow traditional techniques data mining (Hu & Liu, 2004; Liu, Hu, & Cheng, 2005; Popescu, Nguyen, &Etzioni, 2005). systems may operate full documents snippets, generally require rule-based templates additional resources WordNet identifyaspects determine sentiment polarity. Another approach fix predeterminedrelevant set aspects, focus learning optimal opinion assignmentaspects (Snyder & Barzilay, 2007). Below, summarize approach comparecontrast work.One set work relies combination association mining rule-based extractionnouns noun phrases aspect identification. Hu Liu (2004) Liu et al. (2005)developed three-step system: First, initial aspects selected association minerpruned series rules. Second, related opinions aspect identifiedrule-based fashion using word positions, polarity determined WordNetsearch based set seed words. Third, additional aspects identified similarfashion based position selected polarity words. steps, part-ofspeech information provides key role extraction rules. later work,additional component identify implicit aspects deterministic fashion; e.g., heavymaps deterministically <weight> (Liu et al., 2005). task similarutilize part-of-speech information important feature well, additionallyleverage distributional information identify aspects sentiment. Furthermore,avoid reliance WordNet predefined rule mappings order preservegenerality system. Instead, joint modeling allows us recover relationshipswithout need additional information.94fiAutomatic Aggregation Joint Modeling Aspects Valuesapproaches also rely WordNet relationships identify sentimentpolarity, also aspects, using parts properties particular product class.Popescu et al. (2005) first use relations generate set aspects givenproduct class (e.g., camera). Following that, apply relaxation labeling sentimentanalysis. procedure gradually expands sentiment individual words aspectssentences, similar Cascade pattern mentioned work McDonald et al. (2007).Like system Liu et al. (2005), system requires set manual rules severaloutside resources. model require seed words, requiremanual rules additional resources due joint formulation.separate direction work relies predefined aspects focusing improvementsentiment analysis prediction. Snyder Barzilay (2007) define set aspects specificrestaurant domain. Specifically define individual rating model aspect,plus overall agreement model attempts determine whether resulting ratingsagree disagree. models jointly trained supervised fashion usingextension PRanking algorithm (Crammer & Singer, 2001) find best overallstar rating aspect. problem formulation differs significantly workseveral dimensions: First, desire refined analysis using fine-grained aspectsinstead coarse predefined features. Second, would like use little supervisedtraining data possible, rather supervised training required PRankingalgorithm.work, attempt capture intuitions approaches reducingneed outside resources rule-based components. example, rather supplyingrule-based patterns extraction aspect sentiment, instead leverage distributional patterns across corpus infer relationships words different types.Likewise, rather relying WordNet relationships synonymy, antonymy, hyponymy, hypernymy (Hu & Liu, 2004; Liu et al., 2005; Popescu et al., 2005), bootstrapmodel small set seed words.2.2.2 Multi-Document Summarization Application SentimentAnalysisMulti-document summarization techniques generally look repetition across documentssignal important information (Radev & McKeown, 1998; Barzilay, McKeown, & Elhadad,1999; Radev, Jing, & Budzikowska, 2000; Mani, 2001). aspect-based sentiment analysis,work focused augmenting techniques additional components sentimentanalysis (Seki, Eguchi, Kanodo, & Aono, 2005, 2006; Carenini, Ng, & Pauls, 2006; Kim &Zhai, 2009). general, end goal approaches task forming coherent textsummaries using either text extraction natural language generation. Unlike work,many approaches explicitly identify aspects; instead, extractedrepeated information. Additionally, model explicitly looks connectioncontent sentiment, rather treating secondary computationinformation selected.One technique incorporating sentiment analysis follows previous work identification opinion-bearing sentences. Seki et al. (2005, 2006) present DUC summarization95fiSauper & Barzilaysystems designed create opinion-focused summaries task topics.1 system,employ subjectivity component using supervised SVM lexical features, similarwork Yu Hatzivassiloglou (2003) Dave et al. (2003). componentused identify subjective sentences and, work Seki et al. (2006), polarity,task sentences selected response summary. However, likeprevious work unlike task, aspect-based analysis summarizationtask. also fully supervised, relying hand-annotated set 10,000 sentencestrain SVM.Another line work focuses augmenting summarization system aspectselection similar data-mining approaches Hu Liu (2004), rather usingsingle-aspect analysis. Carenini, Ng, Zwart (2005) Carenini et al. (2006) augmentprevious aspect selection user-defined hierarchical organization aspects; e.g.,digital zoom part lens. Polarity aspect assumed given previouswork. aspects incorporated existing summarization systems MEAD*sentence extraction (Radev et al., 2000) SEA natural language generation (Carenini &Moore, 2006) form final summaries. Like work Seki et al. (2005, 2006), workcreate new techniques aspect identification sentiment analysis; instead,focus process integrating sources information summarization systems.aspects produced comparable across reviews particular product,highly-supervised nature means approach feasible large set productscorpus reviews many types restaurants. Instead, must abledynamically identify relevant aspects.final line related work relies traditional summarization technique identifying contrastive contradictory sentences. Kim Zhai (2009) focus generatingcontrastive summaries identifying pairs sentences express differing opinionsparticular product feature. this, define metrics representativeness (coverage opinions) contrastiveness (alignment quality) using semantic similarityWordNet matches word overlap. comparison work, approach follows orthogonal goal, try find defining aspects insteadcontradictory ones. Additionally, selected pairs hint disagreements rating,identification many people agree side overall ratingparticular aspect. work, aim produce concrete set aspectsuser sentiment each, whether unanimous shows disagreement.Overall, methods designed produce output summaries focussubjective information, specifically targeted aspect-based analysis. Instead,aspects identified supervised fashion (Carenini et al., 2005, 2006) defined(Seki et al., 2005, 2006; Kim & Zhai, 2009). work, crucialdynamically-selected aspects feasible preselect aspects supervisedfashion.2.2.3 Probabilistic Topic Modeling Sentiment Analysiswork closest direction aspect-based analysis focuses useprobabilistic topic modeling techniques identification aspects. may aggre1. task examples, see work Dang (2005, 2006).96fiAutomatic Aggregation Joint Modeling Aspects Valuesgated without specific sentiment polarity (Lu & Zhai, 2008) combined additionalsentiment modeling either jointly (Mei, Ling, Wondra, Su, & Zhai, 2007; Blei & McAuliffe,2008; Titov & McDonald, 2008a) separate post-processing step (Titov & McDonald,2008b). Like work, approaches share intuition aspects may representedtopics.Several approaches focus extraction topics sentiment blog articles.one approach, used expert articles aspect extraction combinationlarger corpus user reviews. Lu Zhai (2008) introduce model semi-supervisedprobabilistic latent semantic analysis (PLSA) identifies sentiment-bearing aspectssegmentation expert review. Then, model extracts compatible supportingsupplementary text aspect set user reviews. Aspect selectionconstrained rule-based approaches; specifically, aspect words requirednouns. work differs work significantly. share common goalidentifying aggregating opinion-bearing aspects, additionally desire identifypolarity opinions, task addressed work. addition, obtaining aspectsexpert review unnecessarily constraining; practice, expert reviewers maymention key aspects, mention every aspect. crucial discoveraspects based entire set articles.work direction aspect identification blog posts. example,Mei et al. (2007) use variation latent Dirichlet allocation (LDA) similarexplicitly model topics sentiment, use hidden Markov model discoversentiment dynamics across topic life cycles. general sentiment polarity distributioncomputed combining distributions several separate labeled data sets (e.g., movies,cities, etc.). However, work, sentiment measured document-level, rathertopic-level. Additionally, topics discovered model broad; example, processing query Da Vinci Code, returned topics may labeledbook, movie, religion, rather fine-grained aspects desire model,representing major characters events. model expands work discovering fine-grained aspects associating particular sentiment individualaspect. addition, tying sentiment aspects, able identify sentiment-bearingwords associated polarities without additional annotation required trainexternal sentiment model.Sentiment may also combined LDA using additional latent variablesdocument order predict document-level sentiment. Blei McAuliffe (2008) proposeform supervised LDA (sLDA) incorporates additional response variable,used represent sentiment star rating movie. jointlymodel documents responses order find latent topics best predictresponse variables future unlabeled documents. work significantly differentwork, supervised predict multi-aspect framework.Building approaches comes work fine-grained aspect identification sentiment analysis. Titov McDonald (2008a, 2008b) introduce multi-grain unsupervisedtopic model, specifically built extension LDA. technique yields mixtureglobal local topics. Word distributions topics (both global local) drawnglobal level, however; unlike model. consequence topicseasy compare across products corpus; however, topics gen97fiSauper & Barzilayeral less dynamic hope achieve must shared among everyproduct. One consequence defining global topics difficulty finding relevant topicsevery product little overlap. example, case restaurant reviews,Italian restaurants completely different set aspects Indian restaurants.course, factors known, would possible run algorithm separatelysubset restaurants, distinctions immediately clear priori. Increasing number topics could assist recovering additional aspects; howeveraspects still global, still difficult identify restaurant-specific aspects.sentiment analysis, PRanking algorithm Snyder Barzilay (2007) incorporated two ways: First, PRanking algorithm trained pipeline fashiontopics generated (Titov & McDonald, 2008b); later, incorporated modelinference joint formulation (Titov & McDonald, 2008a). However, cases,original algorithm, set aspects fixed aspects correspondsfixed set topics found model. Additionally, learning problem supervised.fixed aspects, necessary additional supervision, global topic distribution, model formulation sufficient problem domain, requiresfine-grained aspects.approaches structural similarity work present,variations LDA. None, however, intent model. Mei et al. (2007)model aspect sentiment jointly; however aspects vague, treatsentiment document level rather aspect level. Likewise, Titov McDonald(2008b, 2008a) model fine-grained aspects, still coarser aspectsrequire, even increase number aspects, distributions sharedglobally. Finally, Lu Zhai (2008), Blei McAuliffe (2008), Titov McDonald(2008b, 2008a) require supervised annotation supervised expert reviewhave. attempt solve issues joint formulation order proceedminimal supervision discover truly fine-grained aspects.3. Problem Formulationexplaining model details, describe random variables abstractionsmodel, well intuitions assumptions.2 visual explanation modelcomponents shown Figure 3. present complete details generative storySection 4.3.1 Model Componentsmodel composed five component types: entities, snippets, aspects, values,word topics. Here, describe type provide examples.2. Here, explain complete model value selection sentiment restaurant domain.simplified case medical domain would like use aspects, may simply ignorevalue-related components model.98fiAutomatic Aggregation Joint Modeling Aspects ValuesTasca Spanish TapasEntityAspectsChicken++chicken cooked perfectlychicken tough tastyMoist delicious chickenSnippetsValuesDessert++dessert goodexcellent creme bruleeDouzo Sushi BarSushi+sushi creative pretty goodtorched roll tasted rather blandFigure 3: Labeled model components example Figure 1. Note aspectsnever given explicit labels, ones shown presented purely easeunderstanding; aspects exist simply groups snippets share common subject.Also, word topics pictured here; word topic (Aspect, Value, Background)assigned word snippet. model components described high levelSection 3.1 depth Section 4.3.1.1 Entityentity represents single object described review. restaurantdomain, represent individual restaurants, Tasca Spanish Tapas, Douzo SushiBar, Outback Steakhouse.3.1.2 Snippetsnippet user-generated short sequence words describing entity. snippetsprovided user (for example, quick reaction box) extractedcomplete reviews phrase extraction system one Sauper,Haghighi, Barzilay (2010). assume snippet contains one singleaspect (e.g., pizza) one single value type (e.g., positive). restaurant domain,corresponds giving opinion one particular dish category dishes. Examplesrestaurant domain include pasta dishes perfection ,fantastic drinks, lasagna rustica cooked perfectly.99fiSauper & Barzilay3.1.3 Aspectaspect corresponds one several properties entity. restaurant domainentities represent restaurants, aspects may correspond individual dishes categories dishes, pizza alcoholic drinks. domain, entityunique set aspects. allows us model aspects appropriate granularity.example, Italian restaurant may dessert aspect pertains information variety cakes, pies, gelato. However, bakerys menu wouldfall dessert aspect. Instead, present useful aspect-based summary,would require separate aspects cakes, pies, on. aspectsentity-specific rather shared, ties restaurants aspectscommon (e.g., sushi restaurants sashimi aspect); considerpoint potential future work. Note still possible compare aspects acrossentities (e.g., find best restaurant burger ) comparing respective worddistributions.3.1.4 ValueValues represent information associated aspect. review domain, twovalue types represent positive negative sentiment respectively. general, possibleuse value represent distinctions; example, domain aspectsassociated numeric value others associated text description,set value type. intended distinctions may encourageduse seed words (see Section 3.2), may left unspecified model assignwhatever finds best fit data. number value types must prespecified;however, possible use either many types.3.1.5 Word Topicwords snippet observed, word associated underlyinglatent topic. possible latent topics correspond aspect, value, backgroundtopic. example, review domain, latent topic words great terrible wouldValue, words represent entity aspects pizza would Aspect,stop words like in-domain white noise like food would Background.3.2 Problem Setupwork, assume snippet words always observed, correlationsnippets entities known (i.e., know entity given snippet describes).addition, assume part speech tags word snippet. final sourcesupervision, may optionally include small sets seed words lexical distribution,order bias distribution toward intended meaning. example, sentimentcase, add seed words order bias one value distribution toward positive onetoward negative. Seed words certainly required; simply tool constrainmodels use distributions fit prior expectations.Note formulation, relevant aspects restaurant observed;instead, represented lexical distributions induced inference time.100fiAutomatic Aggregation Joint Modeling Aspects Valuessystem output, aspects represented unlabeled clusters snippets.3 Givenformulation, goal work induce latent aspect value underlyingsnippet.4. Modelmodel generative formulation snippets corpus. section,first describe detail general formulation notation model, discussnovel changes enhancements particular corpora types. Inference modeldiscussed Section 5. mentioned previously, describe complete modelincluding aspect values.4.1 General Formulationmodel, assume collection snippet words entities, s. use si,j,wdenote wth word jth snippet ith entity. also assume fixed vocabularywords W .present summary notation Table 1, concise summary modelFigure 4, model diagram Figure 5. three levels model design:global distributions common snippets entities collection, entity-leveldistributions common snippets describing single entity, snippet- word-levelrandom variables. Here, describe turn.4.1.1 Global Distributionsglobal level, draw set distributions common entities corpus.include everything shared across domain, background stop-word distribution,value types, word topic transitions.Background Distribution global background word distribution B drawn represent stop-words in-domain white noise (e.g., food becomes white noise corpusrestaurant reviews). distribution drawn symmetric Dirichlet concentration parameter B ; experiments, set 0.2.Value Distributions value word distribution Vv drawn value type v.example, review domain positive negative sentiment types,distribution words positive type one negative type. Seed wordsWseedv given additional probability mass value priors type v; specifically,non-seed word receives hyperparameter, seed word receives + V ;experiments, set 0.15.Transition Distribution transition distribution drawn represent transitionprobabilities underlying word topics. example, may likelyValue Aspect transition review domain, fits phrases like great pizza.experiments, distribution given slight prior bias toward helpful transitions;3. label desired, automatically extract one selecting highest probability wordsparticular aspect. simplicity exactness, provide manual cluster labels examplespaper.101fiSauper & BarzilayData Setsi,j,wti,j,wWWseedvCollection snippet words entitieswth word jth snippet ith entityPart-of-speech tag corresponding si,j,wFixed vocabularySeed words value type vLexical DistributionsBi,a)(AvVBackground word distributionAspect word distribution aspect entityValue word distribution type vIgnored words distributionDistributionsi,a (a )( )Transition distribution word topicsAspect-value multinomial aspect entityAspect multinomial entityPart-of-speech tag distributionLatent Variablesi,jZAZVi,ji,j,wZWAspect selected si,jValue type selected si,jWord topic (A, V, B, ) selected si,j,wNotationKVBNumber aspectsIndicator correspondingIndicator correspondingIndicator correspondingIndicator correspondingaspect wordvalue wordbackground wordignored wordTable 1: Notation used paper. Items markedSection 4.2.relate extensions mentionedexample, encouraging sticky behavior providing small boost self-transitions.bias easily overridden data; however, provides useful starting point.4.1.2 Entity-Specific Distributionsnaturally variations aspects snippets describe many snippetsdescribe aspect. example, mobile device popular long battery life likelysnippets describing battery device known large screen.domains may enormous variation aspect vocabulary; example, restaurantreviews, two restaurants may serve food items compare. account102fiAutomatic Aggregation Joint Modeling Aspects ValuesGlobal Level:Draw background word distribution B Dirichlet(B W )value type v,Draw value word distribution Vv Dirichlet(W + V Wseedv )Entity Level:entity i,i,aDraw aspect word distributionsDirichlet(A W ) = 1, . . . , KDraw aspect value multinomial i,a Dirichlet(AV N ) = 1, . . . , KDraw aspect multinomial Dirichlet(M K)Snippet Level:snippet j describing ith entity,i,jDraw snippet aspect ZAi,jDraw snippet value ZVi,j i,ZAi,j,w1i,j,w|ZWDraw sequence word topic indicators ZWi,jvalue ZVi,jDraw snippet word given aspect ZAsi,j,wi,Z i,j,Z i,jV V ,B ,i,j,w=AZWi,j,w=VZWi,j,wZW = BFigure 4: summary generative model presented Section 4.1. use Dirichlet(W ) denote finite Dirichlet prior hyper-parameter counts scalartimes unit vector vocabulary items. global value word distribution, priorhyper-parameter counts vocabulary items V Wseedv , vectorvocabulary items set seed words value v.variations, define set entity-specific distributions generateaspect vocabulary popularity, well distribution value types aspect.i,aAspect Distributions aspect word distributiondrawn aspect a.represents distribution unigrams particular aspect. example,domain restaurant reviews, aspects may correspond menu items pizza,reviews cell phones, may correspond details battery life.103fiSauper & BarzilayValue vBackground worddistributionTransitiondistributionValue worddistributionsBVvEntityAspectAspectmultinomialAspect worddistributionsAspect-valuemultinomialAi,ai,aSnippet jSnippet aspectSnippet valueZAi,jZVi,jHMM snippet wordsi,j,w1ZWi,j,wZWi,j,w+1ZWsi,j,w1si,j,wsi,j,w+1ZAi,j , Ai,aZVi,j , VvBFigure 5: graphical description model presented Section 4.1. written description generative process located Figure 4. Curved arrows indicate additional linkspresent model drawn readability.104fiAutomatic Aggregation Joint Modeling Aspects Valuesaspect word distribution drawn symmetric Dirichlet prior hyperparameter; experiments, set 0.075.Aspect-Value Multinomials Aspect-value multinomials i,a determine likelihoodvalue type v corresponding aspect a. example, value types representpositive negative sentiment, corresponds agreement sentiment across snippets.Likewise, value types represent formatting integers, decimals, text, aspectgenerally prefers type value. multinomials drawn symmetricDirichlet prior using hyperparameter AV ; experiments, set 1.0.Aspect Multinomial aspect multinomial controls likelihood aspectdiscussed given snippet. encodes intuition certain aspectslikely discussed others given entity. example, particular Italianrestaurant famous pizza, likely pizza aspect frequentlydiscussed reviews, drinks aspect may mentioned occasionally.aspect multinomial encode higher likelihood choosing pizza snippetaspect drinks. multinomial drawn symmetric Dirichlet distributionhyperparameter ; experiments, set 1.0.4.1.3 Snippet- Word-Specific Random VariablesUsing distributions described above, draw random variables snippetdetermine aspect value type described, well sequenceunderlying word topics words.i,jsnippet describe drawn aspectAspect single aspect ZAmultinomial . aspect words snippet (e.g., pizza corpus restauranti,ji,ZAreviews) drawn corresponding aspect word distribution.Value Type single value type ZVi,j drawn conditioned selected aspecti,jcorresponding aspect-value multinomial i,ZA . value words snippet (e.g., greatZ i,jreview domain) drawn corresponding value word distribution V V .i,j,1i,j,mgenerWord Topic Indicators sequence word topic indicators ZW, . . . , ZWated using first-order Markov model parameterized transition matrix .indicators determine unigram distribution generates word snippet.i,j,wexample, ZW= B, wth word snippet generated background worddistribution B .4.2 Model Extensionsoptional components model may improve performancecases. briefly list here, present necessary modifications modeldetail case. Modifications inference procedure presented Section 5.2.First, corpora contain irrelevant snippets, may introduce additional worddistribution word topic Ignore allow model ignore certain snippetspieces snippets altogether. Second, possible acquire part speech tags105fiSauper & Barzilaysnippets, using extra piece information quite beneficial. Finally, corporaevery entity expected share aspects, model altered useset aspect distributions entities.4.2.1 Ignoring Snippetssnippet data automatically extracted, may noisy, snippets mayviolate initial assumptions one aspect one value. example, findsnippets mistakenly extracted neither aspect value.extraneous snippets may difficult identify priori. compensate this, modifymodel allow partial entire snippets ignored addition globalunigram distribution, namely Ignore distribution . distribution drawnsymmetric Dirichlet concentration parameter .Ignore distribution differs Background distribution includescommon uncommon words. intended select whole snippets large portionssnippets, words may overlap Background distribution distributions. order successfully incorporate distribution model, must allowi,j,wconsider Ignore topic I. Additionally, ensureword topic indicator ZWselects long segments text, give large boost prior Ignore Ignoresequence transition distribution , similar boost self-transitions.4.2.2 Part-of-Speech TagsPart-of-speech tags provide valuable evidence determining snippet wordsdrawn distribution. example, aspect words often nouns, representconcrete properties concepts domain. Likewise, domains, value wordsdescribe aspects therefore tend expressed numbers adjectives.intuition directly incorporated model form additionaloutputs. Specifically, modify HMM produce words tags. Additionally,, v , , similar corresponding unigramdefine distributions tagsVBdistributions.4.2.3 Shared Aspectsdomains regular, every entity expected express aspectsconsistent set, beneficial share aspect information across entities. example,medical domain, general set lab tests physical exam categories runpatients. Note quite unlike restaurant review case, restaurantsaspects completely different (e.g., pizza, curry, scones, on).Sharing aspects way accomplished modifying aspect distributionsi,a. Likewise, aspect-value multinomials i,a becomebecome global distributionsshared across entities . Treatment aspect multinomials dependdomain properties. distribution aspects expected acrossentities, also made global; however, individual entity expected exhibitvariation number snippets related aspect, kept entityspecific. example, reviews set cell phones may expected focus varying106fiAutomatic Aggregation Joint Modeling Aspects ValuesValue vBackground worddistributionTransitiondistributionValue worddistributionsBVvEntityAspectAspectmultinomialAspect worddistributionsAspect-valuemultinomialAaSnippet jSnippet aspectSnippet valueZAi,jZVi,jHMM snippet wordsi,j,w1ZWi,j,wZWi,j,w+1ZWsi,j,w1si,j,wsi,j,w+1ZAi,j , Ai,aZVi,j , VvBFigure 6: graphical description model shared aspects presented Section 4.2.Note similarities Figure 5; however version, aspects shared entirecorpus, rather entity-specific. would also possible share aspectmultinomial corpus-wide; case would indicate entities sharegeneral distribution aspects, version individual entities allowedcompletely different distributions.parts, depending unique problematic phones. graphicaldescription changes compared original model shown Figure 6.107fiSauper & BarzilayMean-field FactorizationQ (B , V , , , , , Z)= q (B ) q ()N!q (Vv )v=1qKi,aqq i,a!a=1!i,j i,j i,j,wq ZV q ZAq ZWwjSnippet Aspect Indicatori,jlog q(ZA= a) Eq(i ) log (a) +Xi,j,wi,a i,j,wq(ZW= A)Eq(i,a ) log(s)+wNXq(ZVi,j = v)Eq(i,a ) log i,a (v)v=1Snippet Value Type IndicatorXXi,ji,j,wlog q(ZVi,j = v)q(ZA= a)Eq(i,a ) log i,a (v) +q(ZW= V )Eq(Vv ) log Vv (si,j,w )wWord Topic IndicatorXi,j,w+1i,ji,j i,j,wi,j,w1i,j,w+= Eq(i,a ) log, A, ZWq ZAlog q ZW= log P ZW = + Eq() log ZWi,j,w+1i,j,w1i,j,w= V log P ZW = V + Eq() log ZW, V V, ZWlog q ZW+Xq ZVi,j = v Eq(Vv ) log Vv si,j,wvlog qi,j,wZW= B log P ZW = B + Eq() logi,j,w1,BZWi,j,w+1B, ZW+ Eq(B ) log B si,j,wFigure 7: mean-field variational algorithm used learning inference obtain posterior predictions snippet properties attributes, described Section 5.Mean-field inference consists updating latent variable factors wellstraightforward update latent parameters round robin fashion.5. Inferencegoal inference model predict aspect value snippetproduct j, given text observed snippets, marginalizing remaininghidden parameters:i,jP (ZA, ZVi,j |s)accomplish task using variational inference (Blei, Ng, & Jordan, 2003). Specifically, goal variational inference find tractable approximation Q() fullposterior model.P (B , V , , , , , Z|s) Q(B , V , , , , , Z)model, assume full mean-field factorization variational distribution,shown Figure 7. variational approximation defined product factors q(),assumed independent. approximation allows tractable inferencefactor individually. obtain closest possible approximation, attempt set108fiAutomatic Aggregation Joint Modeling Aspects Valuesq() factors minimize KL divergence true model posterior:arg min KL(Q(B , V , , , , , Z)kP (B , V , , , , , Z|s))Q()5.1 Optimizationoptimize objective using coordinate descent q() factors. Concretely,update factor optimizing criterion factors fixedcurrent values:q() EQ/q() log P (B , V , , , , , Z, s)summary variational update equations given Figure 7, graphicalrepresentation involved variables step presented Figure 8. Here,present update factor.5.1.1 Snippet Aspect Indicatori,jFirst, consider update snippet aspect indicator, ZA(Figure 8a):i,jlog q(ZA= a) Eq(i ) log (a)Xi,j,wi,a i,j,w+q(ZW= A)Eq(i,a ) log(s)+(1b)wNX(1a)q(ZVi,j = v)Eq(i,a ) log i,a (v)(1c)v=1optimal aspect particular snippet depends three factors. First, includelikelihood discussing aspect (Eqn. 1a). mentioned earlier, encodesprior probability aspects discussed frequently others. Second,examine likelihood particular aspect based words snippet (Eqn. 1b).word identified aspect word, add probability discussesaspect. Third, determine compatibility chosen aspect typecurrent aspect (Eqn. 1c). example, know value type likely integer,assigned aspect accept integers.5.1.2 Snippet Value Type IndicatorNext, consider update snippet value type indicator, ZVi,j (Figure 8b):Xi,jlog q(ZVi,j = v)q(ZA= a)Eq(i,a ) log i,a (v)(2a)+Xi,j,wq(ZW= V )Eq(Vv ) log Vv (si,j,w )(2b)wbest value type snippet depends two factors. First, like snippet aspectindicator, must take consideration compatibility snippet aspectvalue type (Eqn. 2a). Second, word identified value word, includelikelihood comes given value type.109fiSauper & BarzilayvBvVvi,ajZV , VZA ,BjZVi,jw1ZWwZWw+1ZWsw1swsw+1ZV , VZA ,BVvji,jZAw1ZWZAsw1ZjwZWw+1ZWsw1swsw+1Vvi,aw+1ZWw1ZWswsw+1ZZVsw1 VVvi,ai,aBi,jZAwZWwi. ZW=Aw1ZWvi,aZVi,jBi,aZVi,jvi,a(b) Inference procedure snippet value, ZVi,jvi,ai,jZAi,j(a) Inference procedure snippet aspect, ZABVvi,ai,jZABi,aZVi,jjwZWw+1ZWswsw+1Zi,jZAw1ZWZVi,jwZWw+1ZWswsw+1wii. ZW=Vsw1 Bwiii. ZW=Bi,j,w(c) Inference procedure word topic, ZWFigure 8: Variational inference update steps latent variable. latent variablecurrently updated shown double circle, variables relevantupdate highlighted black. variables impact updategrayed out. Note snippet aspect (a) snippet value type (b), update takesform possible aspect value type. However, word topic (c),update symmetric relevant variables different possible word topic.110fiAutomatic Aggregation Joint Modeling Aspects Values5.1.3 Word Topic Indicatori,j,wFinally, consider update word topic indicators, ZW(Figure 8c). Unlikeprevious indicators, possible topic slightly different equation, mustmarginalize possible aspects value types.i,j,wi,j,w1i,j,w+1log q ZW= log P ZW = + Eq() log ZW, A, ZWXi,ji,j i,j,w+q ZA= Eq(i,a ) log(3a)i,j,wi,j,w1i,j,w+1log q ZW= V log P ZW = V + Eq() log ZW, V V, ZWX+q ZVi,j = v Eq(Vv ) log Vv si,j,w(3b)vi,j,w+1i,j,w1i,j,w, B B, ZW= B log P ZW = B + Eq() log ZWlog q ZW+ Eq(B ) log B si,j,w(3c)update topic composed prior probability topic, transition probabilities using topic, probability word coming appropriate unigram distribution, marginalized possibilities snippet aspect valueindicators.5.1.4 Parameter FactorsUpdates parameter factors variational inference derived simplecounts latent variables ZA , ZV , ZW . Note include partial counts;i,j= a1 ) = 0.35, would contribute 0.35particular snippet aspect probability P (ZAcount (a1 ).5.1.5 Algorithm DetailsGiven set update equations, update procedure straightforward. First, iteratecorpus computing updated values random variable, batchupdate factors simultaneously. update algorithm run convergence.practice, convergence achieved 50th iteration, algorithm quite efficient.Note batch update means update computed using valuesprevious iteration, unlike Gibbs sampling uses updated values runscorpus. difference allows variational update algorithm parallelized, yieldingnice efficiency boost. Specifically, parallelize algorithm, simply split setentities evenly among processors. Updates entity-specific factors variablescomputed pass data, updates global factors collectedcombined end pass.111fiSauper & Barzilay5.2 Inference Model Extensionsdiscussed Section 4.2, add additional components model improveperformance data certain attributes. Here, briefly discuss modificationsinference equations extension.5.2.1 Ignoring Snippetsmain modifications model extension addition unigramdistribution word topic I, chosen ZW . update equation ZWmodified addition following:i,j,wlog q(ZW= I) log P (ZW = I) + Eq(I ) log (si,j,w )pieces equation (Eqn. 3), composed prior probabilityword topic likelihood word generated .addition, transition distribution must updated include transition probabilities I. mentioned earlier, II transition receives high weight,transitions receive low weight.5.2.2 Part-of-Speech Tagsadd part speech tags, model updated include part-of-speech distributionsi,a, V , B , one word topic. Note unlike unigram distributionsVv , corresponding tag distributions dependent snippet entity, aspect,value. included referenced updates ZW follows:i,j,w+1i,j,wi,j,w1, A, ZWlog q ZW= log P ZW = + Eq() log ZWXi,j i,j,wi,j+ Eq(A ) log ti,j,w += Eq(i,a ) logq ZAi,j,w+1i,j,w1i,j,w, V V, ZW= V log P ZW = V + Eq() log ZWlog q ZWXq ZVi,j = v Eq(Vv ) log Vv si,j,w+ Eq(V ) log V ti,j,w +vi,j,wi,j,w1i,j,w+1log q ZW= B log P ZW = B + Eq() log ZW, B B, ZW+ Eq(B ) log B ti,j,w + Eq(B ) log B si,j,wHere, define set tags ti,j,w tag corresponding word si,j,w .5.2.3 Shared Aspectsglobal set shared aspects simplification model reduces totalaspect-valuenumber parameters. model redefines aspect distributionsmultinomials . Depending domain, may also redefine aspect multinomial. resulting latent variable update equations same; parameter112fiAutomatic Aggregation Joint Modeling Aspects Valuesfactor updates changed. Rather collecting counts snippets describing singleentity, counts collected across corpus.6. Experimentsperform experiments two tasks. First, test full model joint predictionaspect sentiment corpus review data. Second, use simplified versionmodel designed identify aspects corpus medical summary data.domains structured quite differently, therefore present different challengesmodel.6.1 Joint Identification Aspect Sentimentfirst task test full model jointly predicting aspect sentimentcollection restaurant review data. Specifically, would like dynamically selectset relevant aspects restaurant, identify snippets correspondaspect, recover polarity snippet individually aspect whole.perform three experiments evaluate models effectiveness. First, testquality learned aspects evaluating predicted snippet clusters. Second, assessquality polarity classification. Third, examine per-word labeling accuracy.6.1.1 Data Setdata set task consists snippets selected Yelp restaurant reviewsprevious system (Sauper et al., 2010). system trained extract snippets containingshort descriptions user sentiment towards aspect restaurant.4 purposeexperiment, select snippets labeled system referencing food.order ensure enough data meaningful analysis, ignore restaurantsfewer 20 snippets across reviews. model easily operaterestaurants fewer snippets, want ensure cases select evaluationnontrivial; i.e., sufficient number snippets cluster makevalid comparison. 13,879 snippets total, taken 328 restaurantsaround Boston/Cambridge area. average snippet length 7.8 words,average 42.1 snippets per restaurant. use MXPOST tagger (Ratnaparkhi,1996) gather POS tags data. Figure 9 shows example snippets.domain, value distributions consist one positive one negative distribution. seeded using 42 33 seed words respectively. Seed words hand-selectedbased restaurant review domain; therefore, include domain-specific wordsdelicious gross. complete list seed words included Table 2.6.1.2 Domain Challenges Modeling Techniquesdomain presents two challenging characteristics model. First, widevariety restaurants within domain, including everything high-end Asian fusioncuisine greasy burger fast food places. try represent using single4. exact training procedures, please reference paper.113fiSauper & BarzilayPositiveamazingdelightfulextraordinaryflavorfulgenerousheaveninexpensiveperfectrecommendstimulatingwonderfulNegativeawesomedivinefantasticfreegoodhugelovephenomenalrichstrongyummybestenjoyfavfreshgreatincrediblenicepleasantsleektastydeliciousexcellentfavoritefunhappyinterestingoutstandingqualitystellartenderaverageblanddisappointedexpensivegrosslamemehpoortackytinyuninspiringawfulboringdisgustingfattyhorriblelessmushypriceytastelessunappetizingworsebadconfuseddrygreasyinediblemediocreovercookedsaltyterribleunderwhelmingworstTable 2: Seed words used model restaurant corpus, 42 positive words 33negative words total. words manually selected data set.shared set aspects, number aspects required would immense, wouldextremely difficult model make fine-grained distinctions them.defining aspects separately restaurant mentioned Section 4, achieveproper granularity aspects individual restaurant without overwhelmingoverlapping selection choices. example, model able distinguishItalian restaurant may need single dessert aspect, bakery requires separatepie, cake, cookie aspects.Second, usually fairly cohesive set words refer particularaspect (e.g., pizza aspect might commonly seen words slice, pepperoni,cheese), near-unlimited set potential sentiment words. especiallypronounced social media domain many novel words used expresssentiment (e.g., deeeeeeeelish substitute delicious). mentioned Section 4,part-of-speech transition components model helps identify unknownwords likely sentiment words; however, additionally need identify polarity sentiment. this, leverage aspect-value multinomial,represents likelihood positive negative sentiment particular aspect.snippets given aspect positive, likely word deeeeeeeelishrepresents positive sentiment well.6.1.3 Cluster Predictioni,jgoal task evaluate quality aspect clusters; specifically ZAvariableSection 4. ideal clustering, predicted clusters cohesive (i.e., snippetspredicted discuss given aspect related other) comprehensive (i.e.,snippets discuss aspect selected such). example, snippetassigned aspect pizza snippet mentions aspect pizza,crust, cheese, toppings.Annotation experiment, use set gold clusters complete setssnippets 20 restaurants, 1026 snippets total (an average 51.3 snippets perrestaurant). Cluster annotations provided graduate students fluent English.annotator provided complete set snippets particular restaurant,asked cluster naturally. 199 clusters total, yields average114fiAutomatic Aggregation Joint Modeling Aspects Valuesnoodles meat actually +:::::::pretty ::::::good.+:::::::::::::recommend chicken noodle pho.soggy.noodles ::::::chicken pho also + good.:::::though.spring rolls coffee + good,:::::spring roll wrappers littledry tasting.::::::::::::::::::crispyspringrolls.+ favorites:::::::::Crispy Tuna Spring Rolls + fantastic!:::::::::lobster roll mother ordered ::::dry scant.:::::portabella mushroom + go-tosandwich.:::::bread sandwich stale.:::::rather ::::::::measly.slice tomato :::::::shumai california maki sushi +::::::::decent.+spicy tuna roll eel roll perfect.::::::::::so::::::great.rolls spicy mayo ::::+love Thai rolls.:::::Figure 9: Example snippets data set, grouped according aspect. Aspect wordsunderlined colored blue, negative value words labeled - colored red,positive value words labeled + colored green. grouping labelinggiven data set must learned model.10.0 clusters per restaurant. annotations high-quality; average annotatoragreement 81.9 MUC evaluation metric (described detail below). coulddefine different number clusters restaurant varying number aspectdistributions, simplicity ask baseline systems full model produce10 aspect clusters per restaurant, matching average annotated number. Varyingnumber clusters simply cause existing clusters merge split; largesurprising changes clustering.Baseline use two baselines task, using clustering algorithm weightedTF*IDF implemented publicly available CLUTO package (Karypis, 2002),5using agglomerative clustering cosine similarity distance metric (Chen, Branavan,Barzilay, & Karger, 2009; Chen, Benson, Naseem, & Barzilay, 2011).first baseline, Cluster-All, clusters entire snippets data set.baseline put strong connection things lexically similar.model uses aspect words tie together clusters, baseline may capture correlationswords model correctly identify aspect words.5. Available http://glaros.dtc.umn.edu/gkhome/cluto/cluto/overview.115fiSauper & BarzilayCluster-AllCluster-NounmodelPrecisionRecall57.368.674.360.170.585.3F158.769.579.4Table 3: Results using MUC metric cluster prediction joint aspect valueidentification task. MUC deficiency putting everything singlecluster artificially inflate score, models set use number clusters.Note task, Cluster-Noun significantly outperforms Cluster-Allbaseline, indicating part speech crucial piece information task.second baseline, Cluster-Noun, works nouns snippets.snippet POS-tagged using MXPOST (Ratnaparkhi, 1996),6 non-noun (i.e.,NN, NNS, NNP, NNPS) words removed. expect aspects containleast one noun, acts proxy aspect identification model.Metric use MUC cluster evaluation metric task (Vilain, Burger, Aberdeen,Connolly, & Hirschman, 1995). metric measures number cluster mergessplits required recreate gold clusters given models output. Therefore,concisely show accurate clusters whole. would possibleartificially inflate score putting everything single cluster, parametersmodel likelihood objective model prefers use availableclusters, number baseline system.Results Results cluster prediction task Table 3. model shows strongperformance baseline, total error reduction 32% Cluster-Nounbaseline 50% Cluster-All baseline. common cause poor clusterchoices baseline systems inability distinguish words relevantaspect words. example, Cluster-All baseline, many snippets use worddelicious, may end cluster based alone. Cluster-Nounbaseline able avoid pitfalls thanks built-in filter. ableavoid common value words adjectives also focus seemsconcrete portion aspect (e.g., blackened chicken); however, still cannot makecorrect distinctions assumptions broken. model capabledistinguishing words aspect words (i.e., words relevant clustering),choose clusters make sense overall.6.1.4 Sentiment Analysisevaluate systems predictions snippet sentiment using predicted posteriori,jvalue distributions snippet (i.e., ZA). task, consider binaryi,jjudgment simply one higher value q(ZA) (see Section 5). goaltask evaluate whether model correctly distinguishes sentiment value words.6. Available http://www.inf.ed.ac.uk/resources/nlp/local_doc/MXPOST.html.116fiAutomatic Aggregation Joint Modeling Aspects ValuesMajorityDiscriminative-SmallSeedDiscriminative-LargemodelAccuracy60.774.178.280.482.5Table 4: Sentiment prediction accuracy model compared DiscriminativeSeed baselines, well Majority representing majority class (Positive) baseline.One advantage system ability distinguish aspect words sentiment wordsorder restrict judgment relevant terms; another leverage gainsbiasing unknown sentiment words follow polarity observed snippetsrelating aspect.Annotation task, use set 662 randomly selected snippets Yelpreviews express opinions. get clear result, set specifically excludes neutral,mixed, potentially ambiguous snippets fries salty tastyblackened chicken spicy, make 10% overall data. setsplit training set 550 snippets test set 112 snippets, snippetmanually labeled positive negative. one baseline, use set positivenegative seed words manually chosen model, shown Table 2. Notebefore, model access full corpus unlabeled data plus seed words,labeled examples.Baseline use two baselines task, one based standard discriminativeclassifier one based seed words model.Discriminative baseline task standard maximum entropy discriminative binary classifier7 unigrams. Given enough snippets enough unrelated aspects,classifier able identify words like great indicate positive sentimentlike bad indicate negative sentiment, words like chicken neutraleffect. illustrate effect training size, include results DiscriminativeSmall, uses 100 training examples, Discriminative-Large, uses 550training examples.Seed baseline simply counts number words positivenegative seed lists used model, Vseed+ Vseed , listed Table 2.words Vseed+ , snippet labeled positive, wordsVseed , snippet labeled negative. tie seed words, splitprediction. seed word lists manually selected specifically restaurantreviews (i.e., contain food-related sentiment words delicious), baselineperform well.Results overall sentiment classification accuracy system shown Table 4). model outperforms baselines. obvious flaw Seed baseline7. Available https://github.com/lzhang10/maxent.117fiSauper & Barzilay85Accuracy82.579.58080.478.2 77.776.878.675DiscriminativeSeedmodel74.1700100 200 300 400 500 600Number snippets training dataFigure 10: Discriminative baseline performance number training examples increases. performance generally increases, inconsistencies. mainissue baseline needs see examples words training dataimprove; phenomenon seen plateau graph.inability pre-specify every possible sentiment word. perform highly, duetailoring restaurant domain good coverage frequent words (e.g.,delicious, good, great), performance model indicates generalizebeyond seed words.Discriminative-Large outperforms Seed baseline test set; however,given smaller training set Discriminative-Small, performs worse. trainingcurve Discriminative baseline shown Figure 10. Discriminativebaseline system correctly identify polarity statements containing informationseen past, two main weaknesses. First, every sentiment word mustpresent training data. example, test data, rancid appears negativesentence; however, appear training data, model labels exampleincorrectly. problematic, way find training data every possiblesentiment word, especially social media data novel words typos frequentoccurrence. models ability generalize polarity snippets describingparticular aspect allows predict sentiment values words unknown polarity.example, already several positive snippets describing particular aspect,system guess snippet unknown polarity likely also positive.6.1.5 Per-Word Labeling Accuracygoal task evaluate whether word correctly identified aspectword, value word, background word. distinction crucial order achievecorrectness clustering sentiment analysis, errors may help us identifyweaknesses model.118fiAutomatic Aggregation Joint Modeling Aspects Valuesrolls also ntwell made .:::::::: :::: ::::::pita ::::::::beyond dry:::::::tasted likecardboard !::::::::::::::::::falafel !Falafel King best::::rolls spicy mayogood .::: :: :::::!Ordered spicy tuna california roll amazing:::::::::Table 5: Correct annotation set phrases containing elements may confusing,annotators tested allowed annotate actual test data. Aspect words colored blue underlined; value words colored orange underlinedwavy line. common mistakes include: annotating nt background (becauseattached background word was), annotating cardboard aspectnoun, annotating Falafel King aspect subject position.Annotation Per-word annotation acquired Mechanical Turk. per-word labeling task seems difficult Turk annotators, implement filtering procedureensure high-quality annotators allowed submit results. Specifically,ask annotators produce labels set difficult phrases known labels (shownTable 5). annotators successfully produced correct mostly-correct annotations allowed access annotation tasks containing new phrases.unknown tasks presented 3 annotators, majority label taken word.total, test 150 labeled phrases, total 7,401 labeled words.Baseline baseline task relies intuition part-of-speechuseful proxy aspect value identification. know aspects usually representconcrete entities, often nouns, value words descriptive counting,often adjectives adverbs. Therefore, use MXPOST tagger findPOS word snippet. main baseline, Tags-Full, assign noun(NN*) aspect label, numeral, adjective, adverb, verb participle (CD, RB*,JJ*, VBG, VBN) value label. comparison, also present results smaller tagset,Small-Tags, labeling nouns (NN*) aspect adjectives (JJ*) values. Notetags added Tags-Full baseline beneficial baselines score.Tree expansion full model baselines designed pickrelevant individual words rather phrases, may correspond well phraseshumans selected relevant. Therefore, also evaluate set expandedlabels identified parse trees Stanford Parser (Klein & Manning, 2003).8 Specifically, non-background word, identify largest containing noun phrase (foraspects values) adjective adverb phrase (for values only)also contain oppositely-labeled words. example, noun phrase blackened chicken,chicken labeled aspect word blackened labeled background word,labeled aspect words. However, noun phrase tasty chickentasty already labeled value, label changed expansionattempted. final heuristic step, punctuation, determiners, conjunctions8. Available http://nlp.stanford.edu/software/lex-parser.shtml.119fiSauper & BarzilayTree expansion procedure aspect words:1. Find noun phrases contain aspect word (pork).+ innovative::::::::::::appetizers pork apple glazeNP1NP2NP3+ highlights:::::::::::2. Select largest noun phrase contain value (sentiment) words.NP1 valid; contain value words. However, largest valid NP.NP2 valid; contain value words. largest valid NP, selected.NP3 contains value word (+ innovative),invalid.::::::::::::3. Convert background words within selected noun phrase aspect wordsexcept punctuation, determiners, conjunctions.+ innovative::::::::::::appetizers pork apple glaze+ highlights:::::::::::Figure 11: tree expansion procedure value words, example snippet.procedure similar aspect words, except adjective phrases adverb phrases alsoconsidered expansion.AspectPrecision RecallF1ValuePrecision RecallF1Tags-SmallTree79.974.079.583.079.778.278.579.245.057.457.266.5Tags-FullTree79.975.679.581.479.778.478.177.168.770.173.173.485.279.552.671.965.075.570.576.761.670.965.773.7modelTreeTable 6: Per-word labeling precision recall model compared Tags-SmallTags-Full baselines, without expansion trees. modelprecise aspect better recall value. Note general process expanding labels tree structure increases recall expense precision.would newly labeled aspect value words ignored kept backgroundwords. steps procedure illustrative example shown Figure 11.Results evaluate systems precision recall aspect value separately.Results systems shown Table 6. model without tree expansionhighly precise expense recall; however expansion performed, recallimproves tremendously, especially value words.result initially disappointing, possible adjust model parametersincrease performance task; example, aspect words could put additional120fiAutomatic Aggregation Joint Modeling Aspects Valuesmoqueca delicious:::::::perfect winter food , ::::::warm , filling hearty ::::::::::::heavy .:::::::::bacon wrapped almond dates ::::::::amazing plantains cheese boring.::::::artichoke homemade pasta appetizers :::::greatTable 7: High-precision, low-recall aspect word labeling full model. Notehuman would likely identify complete phrases bacon wrapped almond dateshomemade pasta appetizers; however, additional noise degrades performanceclustering task.startVB0.060.190.020.220.00V0.000.030.320.260.00B0.940.770.470.430.010.000.010.010.170.99end0.000.000.180.060.00Table 8: Learned transition distribution model. pattern high-precisionaspect words represented preference continuing string several aspectwords, causing model prefer single, precise aspect words. Likewise, better recallvalue words indicated higher value V V transition, encourageseveral words row marked value words.i,j,wmass prior ZW= increase Dirichlet hyperparameter . However,increases performance word labeling task, also decreases performancecorrespondingly clustering task. examination data, correlationperfectly reasonable. order succeed clustering task, model selectsrelevant portions snippet aspect words. entire aspect valueidentified, clustering becomes noisy. Table 7 shows examples high-precisionlabeling achieves high clustering performance, Table 8 shows examplelearned transition distribution creates labeling.6.2 Aspect Identification Shared Aspectssecond task uses simplified version model designed aspect identificationonly. task, use corpus medical visit summaries. domain,summary expected contain similar relevant information; therefore, set aspectsshared corpus-wide. evaluate model formulation, examine predictedclusters snippets, full model.6.2.1 Data Setdata set task consists phrases selected dictated patient summariesPediatric Environmental Health Clinic (PEHC) Childrens Hospital Boston, specializingtreatment children lead poisoning. Specifically, patients office visit labresults completed, PEHC doctor dictates letter referring physician containing121fiSauper & Barzilayinformation previous visits, current developmental family status, in-office examresults, lab results, current diagnosis, plan future.experiment, select phrases in-office exam lab results sectionssummaries. Phrases separated heuristically commas semicolons. domain contains significant amount extraneous information, restaurantdomain, must extract phrases believe bear relevance task hand.However, medical text dense nearly relevant, heuristic separationsufficient extract relevant phrases. 6198 snippets total, taken 271summaries. average snippet length 4.5 words, average 23 snippetsper summary. Yelp domain, use MXPOST tagger (Ratnaparkhi, 1996)gain POS tags. Figure 12 shows example snippets. domain,values; simply concentrate aspect-identification task. Unlike restaurantdomain, use seed words.6.2.2 Domain Challenges Modeling Techniquescontrast restaurant domain, medical domain uses single global set aspects.represent either individual lab tests (e.g., lead level, white blood cell count) particular body systems (e.g., lungs cardiovascular ). aspects far commonothers, uncommon summary include one two snippetsgiven aspect. Therefore, mentioned Section 4.2, model aspect worddistributions aspect multinomial shared entities corpus.Also contrast restaurant domain, aspects defined words takenentire snippet. Rather aspects associated names measurements(e.g., weight), units descriptions measurement (e.g., kilograms) alsorelevant aspect definition. property extends numeric written measurements; example, aspect lungs commonly described clear auscultationbilaterally. order achieve high performance, model must leverageclues provide proper aspect identification name measurement missing(e.g., patient 100 cm). part speech still important factor model,predict greater importance additional parts speechnouns.Finally, data set noisy contains irrelevant snippets, sectionheadings (e.g., Physical examination review systems) extraneous information.described Section 4.2, modify model ignore partial completesnippets.6.2.3 Cluster Predictionjoint aspect sentiment prediction, goal task evaluate qualityaspect identification. aspects shared across documents, clustersgenerally much larger, set annotated snippets represents fractioncluster.Annotation experiment, use set gold clusters gathered 1,200 snippets, annotated doctor expert domain Pediatric Environmental Health Clinic Childrens Hospital Boston. Note mentioned before, clusters122fiAutomatic Aggregation Joint Modeling Aspects Values113 cm heightPatients height 146.5 cmLungs: Clear bilaterally auscultationlungs normalHeart regular rate rhythm; murmursHeart normal S1 S2Figure 12: Example snippets medical data set, grouped according aspect.Aspect words underlined colored blue. grouping labeling givendata set must learned model.Cluster-AllCluster-NounmodelPrecisionRecall88.288.489.193.083.993.4F190.586.191.2Table 9: Results using MUC metric cluster prediction aspect identificationtask. Note Cluster-All baseline significantly outperforms Cluster-Noun,opposite observe joint aspect value prediction task. duedependence aspect identification name lab test,units description test results, mentioned Section 6.2.2.global domain (e.g., many patients snippets representing blood leadlevel, grouped one cluster). doctor asked cluster 100snippets time (spanning several patients), clustering entire set wouldinfeasible human annotator. 12 sets snippets clustered, resultingclusters manually combined match similar clusters set. example, blood lead level cluster first set 100 snippets combinedcorresponding blood lead level clusters set snippets. clusterfinal set fewer 5 members removed. total, yields gold set30 clusters. 1,053 snippets total, average 35.1 snippets per cluster.match this, baseline systems full model asked produce 30 clusters acrossfull data set.Baselines & Metric keep results consistent previous task,use baselines evaluation metric. baselines rely TF*IDFweighted clustering algorithm, specifically implemented CLUTO package (Karypis,2002) using agglomerative clustering cosine similarity distance metric. before,Cluster-All represents baseline using unigrams snippets entire data set,Cluster-Noun works nouns snippets. useMUC cluster evaluation metric task. details baselinesevaluation metric, please see Section 6.1.3.123fiSauper & BarzilayResults experiment, system demonstrates improvement 7%Cluster-All baseline. Absolute performance relatively high systemsmedical domain, indicating lexical clustering task less misleadingrestaurant domain. interesting note unlike restaurant domain,Cluster-All baseline outperforms Cluster-Noun baseline. mentioned Section 6.2.2, medical data notable relevance entire snippet clustering(e.g., weight kilograms useful identify weight aspect).property, using nouns cluster Cluster-Noun baseline hurts performancesignificantly.7. Conclusions Future Workpaper, presented approach fine-grained content aggregation usingprobabilistic topic modeling techniques discover structure individual text snippets.model able successfully identify clusters snippets data set discussaspect entity well associated values (e.g., sentiment). requiresannotation, small list seed vocabulary bias positive negativedistributions proper direction.results demonstrate delving structure snippet assistidentifying key words important unique domain hand.values learned, joint identification aspect value help improvequality results. word labeling analysis reveals model learns differenttype labeling task; specifically, strict, high-precision labeling clusteringtask high-recall labeling sentiment. follows intuition importantidentify specific main points clustering, sentiment analysis task,may often several descriptions conflicting opinions presented needweighed together determine overall sentiment.model admits fast, parallelized inference procedure. Specifically, entire inference procedure takes roughly 15 minutes run restaurant corpus less 5minutes medical corpus. Additionally, model neatly extensible adjustablefit particular characteristics given domain.limitations model improved future work:First, model makes attempt explicitly model negation word interactions,increasing difficulty aspect sentiment analysis model. performing error analysis, find negation common source error sentimentanalysis task. Likewise, aspect side, model make errors attemptingdifferentiate aspects ice cream cream cheese share common aspectword cream, despite phrases occurring bigrams. using connectionsstronger way, indicator variable negation higher-order HMM,model could make informed decisions.Second, defining aspects per-entity restaurant domain advantagespossible get fine-grained set applicable aspects, also fails leveragepotential information data set. Specifically, know restaurants sharingtype (e.g., Italian, Indian, Bakery, etc.) share common aspects;however, ties current model. Likewise, even global124fiAutomatic Aggregation Joint Modeling Aspects Valueslevel, may aspects tie across restaurants. hierarchical versionmodel would able tie together identify different types aspects:global (e.g., presentation), type-level (e.g., pasta Italian type), restaurant-level(e.g., restaurants special dish).Bibliographic NotePortions paper published previously conference publication (Sauper,Haghighi, & Barzilay, 2011); however paper significantly extends work. describe several model generalizations extensions (Section 4.2) effectsinference procedure (Section 5.2). present new experimental results, including additional baseline comparisons additional experiment (Section 6.1). also introducenew domain, medical summary text, quite different domain restaurantreviews therefore requires several fundamental changes model (Section 6.2).Acknowledgmentsauthors acknowledge support NSF (CAREER grant IIS-0448168), NIH (grant5-R01-LM009723-02), Nokia, DARPA Machine Reading Program (AFRL primecontract no. FA8750-09-C-0172). Thanks Peter Szolovits MIT NLP grouphelpful comments. opinions, findings, conclusions, recommendations expressedpaper authors, necessarily reflect views fundingorganizations.ReferencesBarzilay, R., McKeown, K. R., & Elhadad, M. (1999). Information fusion contextmulti-document summarization. Proceedings ACL, pp. 550557.Blei, D. M., & McAuliffe, J. (2008). Supervised topic models. Advances NIPS, pp.121128.Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent Dirichlet allocation. JournalMachine Learning Research, 3, 9931022.Carenini, G., & Moore, J. D. (2006). Generating evaluating evaluative arguments.Artificial Intelligence, 170, 925952.Carenini, G., Ng, R., & Pauls, A. (2006). Multi-document summarization evaluative text.Proceedings EACL, pp. 305312.Carenini, G., Ng, R. T., & Zwart, E. (2005). Extracting knowledge evaluative text.Proceedings K-CAP, pp. 1118.Chen, H., Benson, E., Naseem, T., & Barzilay, R. (2011). In-domain relation discoverymeta-constraints via posterior regularization. Proceedings ACL, pp. 530540.Chen, H., Branavan, S. R. K., Barzilay, R., & Karger, D. R. (2009). Global modelsdocument structure using latent permutations. Proceedings ACL/HLT, pp. 371379.125fiSauper & BarzilayCrammer, K., & Singer, Y. (2001). Pranking ranking. Advances NIPS, pp.641647. MIT Press.Dang, H. T. (2005). Overview DUC 2005. Proceedings DUC EMNLP/HLT.Dang, H. T. (2006). Overview DUC 2006. Proceedings DUC NAACL/HLT.Dave, K., Lawrence, S., & Pennock, D. M. (2003). Mining peanut gallery: opinionextraction semantic classification product reviews. Proceedings WWW,pp. 519528.Hu, M., & Liu, B. (2004). Mining summarizing customer reviews. ProceedingsSIGKDD, pp. 168177.Karypis, G. (2002). CLUTO clustering toolkit. Tech. rep. 02-017, Dept. ComputerScience, University Minnesota. Available http://www.cs.umn.educluto.Kim, H. D., & Zhai, C. (2009). Generating comparative summaries contradictory opinionstext. Proceedings CIKM, pp. 385394.Kim, S., & Hovy, E. (2005). Automatic detection opinion bearing words sentences.Proceedings IJCNLP, pp. 6166.Kim, S.-M., & Hovy, E. (2006). Automatic identification pro con reasons onlinereviews. Proceedings COLING ACL, pp. 483490.Klein, D., & Manning, C. D. (2003). Accurate unlexicalized parsing. Proceedings ACL,pp. 423430.Liu, B., Hu, M., & Cheng, J. (2005). Opinion observer: Analyzing comparing opinionsweb. Proceedings WWW, pp. 342351.Lu, Y., & Zhai, C. (2008). Opinion integration semi-supervised topic modeling.Proceedings WWW, pp. 121130.Mani, I. (2001). Automatic summarization, Vol. 3. John Benjamins Pub Co.McDonald, R., Hannan, K., Neylon, T., Wells, M., & Reynar, J. (2007). Structured modelsfine-to-coarse sentiment analysis. Proceedings ACL, pp. 432439.Mei, Q., Ling, X., Wondra, M., Su, H., & Zhai, C. (2007). Topic sentiment mixture: modelingfacets opinions weblogs. Proceedings WWW, pp. 171180.Pang, B., & Lee, L. (2004). sentimental education: Sentiment analysis using subjectivitysummarization based minimum cuts. Proceedings ACL, pp. 271278.Pang, B., & Lee, L. (2008). Opinion mining sentiment analysis. Foundations TrendsInformation Retrieval, 2, 1135.Pang, B., Lee, L., & Vaithyanathan, S. (2002). Thumbs up? Sentiment classification usingmachine learning techniques. Proceedings EMNLP, pp. 7986.Popescu, A.-M., Nguyen, B., & Etzioni, O. (2005). OPINE: Extracting product featuresopinions reviews. Proceedings EMNLP/HLT, pp. 339346.Radev, D., & McKeown, K. (1998). Generating natural language summaries multipleon-line sources. Computational Linguistics, 24 (3), 469500.126fiAutomatic Aggregation Joint Modeling Aspects ValuesRadev, D. R., Jing, H., & Budzikowska, M. (2000). Centroid-based summarization multiple documents: sentence extraction, utility-based evaluation, user studies.Proceedings NAACL-ANLP Workshop Automatic Summarization, pp. 2130.Ratnaparkhi, A. (1996). maximum entropy model part-of-speech tagging. Proceedings EMNLP, pp. 133142.Sauper, C., Haghighi, A., & Barzilay, R. (2010). Incorporating content structure textanalysis applications. Proceedings EMNLP, pp. 377387.Sauper, C., Haghighi, A., & Barzilay, R. (2011). Content models attitude. Proceedings ACL, pp. 350358.Seki, Y., Eguchi, K., Kanodo, N., & Aono, M. (2005). Multi-document summarizationsubjectivity analysis DUC 2005. Proceedings DUC EMNLP/HLT.Seki, Y., Eguchi, K., Kanodo, N., & Aono, M. (2006). Opinion-focused summarizationanalysis DUC 2006. Proceedings DUC NAACL/HLT, pp. 122130.Snyder, B., & Barzilay, R. (2007). Multiple aspect ranking using good grief algorithm.Proceedings NAACL/HLT, pp. 300307.Titov, I., & McDonald, R. (2008a). joint model text aspect ratings sentimentsummarization. Proceedings ACL, pp. 308316.Titov, I., & McDonald, R. (2008b). Modeling online reviews multi-grain topic models.Proceedings WWW, pp. 111120.Turney, P. D. (2002). Thumbs thumbs down?: semantic orientation applied unsupervised classification reviews. Proceedings ACL, pp. 417424.Vilain, M., Burger, J., Aberdeen, J., Connolly, D., & Hirschman, L. (1995). modeltheoretic coreference scoring scheme. Proceedings MUC, pp. 4552.Yu, H., & Hatzivassiloglou, V. (2003). Towards answering opinion questions: Separatingfacts opinions identifying polarity opinion sentences. ProceedingsEMNLP, pp. 129136. Association Computational Linguistics.127fiJournal Artificial Intelligence Research 46 (2013) 203-233Submitted 06/12; published 02/13Integrative Semantic Dependency Parsing via EcientLarge-scale Feature SelectionHai ZhaoXiaotian Zhangzhaohai@cs.sjtu.edu.cnxtian.zh@gmail.comShanghai Jiao Tong University,800 Dongchuan Road, Shanghai,ChinaChunyu Kitctckit@cityu.edu.hkCity University Hong Kong,Tat Chee Avenue, Kowloon, Hong Kong SAR, ChinaAbstractSemantic parsing, i.e., automatic derivation meaning representationinstantiated predicate-argument structure sentence, plays critical role deep processing natural language. Unlike top systems semantic dependency parsingrely pipeline framework chain series submodels specialized specic subtask, one presented article integrates everything onemodel, hopes achieving desirable integrity practicality real applicationsmaintaining competitive performance. integrative approach tackles semantic parsing word pair classication problem using maximum entropy classier. leverageadaptive pruning argument candidates large-scale feature selection engineeringallow largest feature space ever use far eld, achieves state-of-the-artperformance evaluation data set CoNLL-2008 shared task, top onetop pipeline system, conrming feasibility eectiveness.1. Introductionpurpose semantic parsing derive meaning representation sentence,usually taking syntactic parse input. popular formalism represent kindmeaning predicate-argument structure and, accordingly, parsing instantiatepredicate argument(s) structure properly actual words phrasesgiven sentence. context dependency parsing, becomes semantic dependencyparsing, takes syntactic dependency tree input outputs lled predicateargument structure predicate, argument word properly labeledsemantic role relation predicate.Semantic role labeling (SRL) one core tasks semantic dependency parsing,dependency constituent based. Conventionally, tackled mainly twosubtasks, namely, argument identication classication. Conceptually, former determines whether word true argument predicate, latter semanticrole plays relation predicate (or argument instantiates predicateargument structure). predicate given, two indispensable subtaskspredicate identication disambiguation, one identify word predicatesentence determine predicate-argument structure identiedpredicate particular context.c2013AI Access Foundation. rights reserved.fiZhao, Zhang & Kitpipeline framework adopted almost previous researches handle subtasks one another. main reason dividing whole task semantic dependencyparsing multiple stages way twofold: maintaining computational eciencyadopting dierent favorable features subtask. general, joint learning systemmultiple components slower pipeline system, especially training. alsoreported Xue Palmer (2004) dierent features favor dierent subtasksSRL, especially argument identication classication. results CoNLLshared tasks 2005 2008 (Carreras & Marquez, 2005; Koomen, Punyakanok, Roth, &Yih, 2005; Surdeanu, Johansson, Meyers, Marquez, & Nivre, 2008; Johansson & Nugues,2008) seem suggest pipeline strategy benchmark technologystate-of-the-art performance specic NLP task.SRL systems pipeline, integrated SRL system holds unique merits, e.g., integrity implementation, practicality real applications, single-stage featureselection beneting whole system, all-in-one model outputting expected semantic role information, on. particular, takes account interactive eectfeatures favoring dierent subtasks hence holds comprehensive viewfeatures working together whole. article intended present recent research explore feasibility constructing eective integrated system semanticdependency parsing melds subtasks together one, including predicate identication/disambiguation argument identication/classication, verbal nominalpredicates, uses feature set subtasks. core researchverify, practical implementation empirical evaluation, methodological soundness eectiveness approach. success, however, rootedsolid technical foundation, i.e., large-scale engineering procedure ecient miningeective feature templates huge set feature candidates, feature space far richerothers ever used before. piece engineering brings potentialsintegrative approach full play. Another focus article hence illustratetechnical essentials.Nevertheless, worth pointing term integrative, used oppositepipeline, misleading mean subtasks carried jointly single run.Instead, used highlight integrity model implementation usessingle representation feature set accommodate subtasks. Althoughapproach unique advantages simplifying system engineering feature selection,model implemented present joint one accomplishwhole semantic parsing synchronous determination predicatesarguments. two types indispensable objects semantic parse tree recognizedsuccession decoding using trained model.rest article organized follows. Section 2 gives brief overview relatedwork, providing background research. Section 4 presents approach adaptivepruning argument candidates generate head-dependent word pairs trainingdecoding, underlies whole process semantic parsing. two keyprocedures optimize parsing, namely, feature selection decoding, presentedSection 5 6, respectively. details evaluation, including evaluation data, experimental results comprehensive comparative analysis results, presented204fiSemantic Dependency ParsingSection 7. Finally, Section 8 concludes research, highlighting contributionspracticality competitiveness approach.2. Related WorkNote SRL almost become surrogate semantic dependency parsing literature recent years. recent research eorts eld, including CoNLL sharedtasks 2004 2005, focused verbal predicates, thanks availabilityPropBank (Palmer, Gildea, & Kingsbury, 2005). complement PropBank, NomBank(Meyers, Reeves, Macleod, Szekely, Zielinska, Young, & Grishman, 2004) annotates nominal predicates correspondent semantic roles using similar semantic framework.Although oering challenges, SRL nominal predicates drawn relatively littleattention (Jiang & Ng, 2006). issue merging various treebanks, including PropBank,NomBank others, discussed work Pustejovsky, Meyers, Palmer,Poesio (2005). idea merging two treebanks put practiceCoNLL-2008 shared task (Surdeanu et al., 2008). best system CoNLL-2008 used twodierent subsystems cope verbal nominal predicates, respectively (Johansson &Nugues, 2008). Unfortunately, however, integrative approachillustrate performance close system.fact, research eorts direction, except recent onejoint identication predicates, arguments senses Meza-Ruiz Riedel (2009).formulate problem Markov Logic Network, weights learnt via 1-bestMIRA (Crammer & Singer, 2003) Online Learning method, use Cutting Plane Inference(Riedel, 2008) Integer Linear Programming (ILP) base solver ecient jointinference best choice predicates, frame types, arguments role labelsmaximal posteriori probability. Using CoNLL-2008 data, system achieves bestsemantic F1 80.16% WSJ test set. 0.75 percentage point lower ours,reported below, whole WSJ+Brown test set. Note trained CoNLL2008 training corpus, subset WSJ corpus, SRL system performance least 10percentage points higher WSJ Brown test set (Surdeanu et al., 2008).CoNLL-2008 -2009 shared tasks1 devoted joint learning syntacticsemantic dependencies, aimed testing whether SRL well performed usingdependency syntax input. research reported article focuses semanticdependency parsing. conduct valid reliable evaluation, use data setevaluation settings CoNLL-2008 compare integrated system, bestSRL system CoNLL-2009 (Zhao, Chen, Kit, & Zhou, 2009), top systemsCoNLL shared tasks (Surdeanu et al., 2008; Hajic, Ciaramita, Johansson, Kawahara, Mart,Marquez, Meyers, Nivre, Pado, Stepanek, Stranak, Surdeanu, Xue, & Zhang, 2009).2 Notesystems achieved higher performance scores CoNLL-2008 CoNLL-2009.integrative approach dependency semantic parsing pros cons.deal main drawbacks, two key techniques need applied purpose1. Henceforth referred CoNLL-2008 -2009, respectively.2. CoNLL-2008 English-only task, CoNLL-2009 multilingual one. Although useEnglish corpus, except more-sophisticated structures former (Surdeanu et al., 2008),main dierence semantic predicate identication required latter.205fiZhao, Zhang & Kiteciency enhancement. One bring auxiliary argument labels enableimprovement argument candidate pruning. signicantly facilitates developmentfast lightweight SRL system. apply greedy feature selectionalgorithm perform task feature selection given set feature templates.helps nd many features possible benet overall processparsing. Many individual optimal feature template sets reported literatureachieved excellent performance specic subtasks SRL. rst timeintegrated SRL system reported produce result close stateart SRL achieved pipelines individual sub-systems highly specializedspecic subtask specic type predicate.3. System ArchitectureDependencies words sentence, syntactic semantic, formulatedindividual edges abstract graph structure. practice, dependency edgebuilt, type (usually referred label) identied, proper learningdecoding. conventional syntactic parsing makes use property projectiveness stipulated well-formedness syntactic tree. contrast, dependencyparsing, new dependencies built regard existing ones. However,case semantic parsing, semantic parsing results projective trees.Instead, actually directed acyclic graphs, word serveargument multiple predicates. Inevitably, learning model semantic parsingtake word pairs account exploring possible dependent relationships.SRL specic task semantic dependency parsing formulated word pairclassication problem tackled various machine learning models, e.g., MaximumEntropy (ME) model used Zhao Kit (2008). model also usedwork probability estimation support global decoding givenSection 6, extends model beyond sequential model. Without constraint,classier task deal word pairs input sequence thusinevitably prone poor computational eciency also unsatisfactory performance.straightforward strategy alleviate problems perform proper pruningtraining sample test data.word pair consists word semantic head another semantic dependent,conventionally denoted p (for predicate) (for argument), respectively.follow convention feature representation below. Since approach uniestwo tasks SRL, namely, predicate identication/disambiguation argument identication/classication, one classication framework, need dierentiateverbal non-verbal heads, handled way.one unique characteristics integrated system.overall architecture system depicted Figure 1. input sentencedata set use, training, development test set, parsed word pairsequence word pair generator using pruning algorithm, e.g., adaptive pruningdescribed below, eliminate useless pairs. Word pairs generated sentencetraining set used train word pair classier, supports decodingformulated Section 6 search optimal set word pairs test sentence206fiSemantic Dependency ParsingData setFeature template setWord pair generatorFeature selection procedureTrainingWord pair sequenceSelected feature setWord pair classifierPredicate decodingArgument decodingYespredicate?Next sentenceFigure 1: Illustration system architecture work ow training testingform semantic parse tree. decoding rst recognizes predicates sentencedetermines arguments predicate beam search argument rolelabels. features used classier selected predened feature spacegreedy selection procedure using training development set repeated trainingtesting rene candidate feature set performance gain achievable(see Section 5). classier obtained way selected features testedtest set.4. Adaptive Argument PruningWord pairs derived sentence classier following ways. (1)predicate identication/disambiguation, word pair consists virtual root (VR)semantic parse tree construction (whose root virtually preset), head,predicate candidate dependent. Theoretically, words sentence questionpredicate candidate. reduce number, opt simple POS tag pruningstrategy verbs nouns allowed predicate candidates. (2) argumentidentication/classication, word pair consists identied predicate, head,another word dependent (or argument, conventional term). Potentially,word sentence argument candidate. Pruning manyargument candidates possible thus particularly signicant improving eciencyperformance classier.two ways collect argument candidates given predicate, onesyntactic dependency tree linear path input sentence.former (referred synPth hereafter), use dependency version pruningalgorithm Xue Palmer (2004), given follows necessary modicationallow predicate also included argument candidate list,nominal predicate sometimes takes argument.207fiZhao, Zhang & KitID12345678a.b.c.d.e.FORMaInvestorfocusshiftedquickly,traderssaid.LEMMAinvestorfocusshiftquickly,tradersay.POSNNNNVBDRB,NNSVBD.HEADb23737707DEPRELcNMODSBJOBJMNRPSBJROOTPPREDdARG LabeleA0focus.01shift.01A1A1AM-MNRA0say.01Word form, token.Syntactic head current token, identied ID.Syntactic dependency relation current token HEAD.Roleset semantic predicate.Argument labels semantic predicates text order.Table 1: example input sentence CoNLL-2008 shared task data setInitialization: Given predicate current node syntactic dependency tree.1. Collect syntactic children argument candidates, traversing childrenleft right.2. Reset current node syntactic head repeat Step 1 till root tree.3. Collect root stop.algorithm eective collecting words path given predicateroot children argument candidates. However, ecient one stillneeded lend stronger support SRL system designed tackle argumentidentication/classication single stage. Following observation argumentsusually tend surround predicate close distance, auxiliary label noMoreArgintroduced signify pruning stops collecting argument candidates. trainingsample generation, label assigned next word soon argumentscurrent predicate saturated previously collected words, light originaltraining data illustrated Table 1. Accordingly, pruning process stops collectingcandidates. decoding, signals decoder stop searching, along similartraverse pruning, arguments identied predicate. adaptivetechnique improves pruning eciency signicantly, saving 1/3 training timememory cost missing true arguments pruning withoutlabel, according experiments. training sample generated waysentence Table 1, means POS pruning pruning algorithm,illustrated Table 2, class labels third column.collect argument candidates along linear path (referred linPth hereafter)instead syntactic tree sentence, classier search words aroundgiven predicate. way similar pruning along synPth improved, twoauxiliary labels, namely, noMoreLeftArg noMoreRightArg, introduced signifyadaptive pruning along linPth stops, skipping words far awaypredicate. Given example illustrate two labels used, e208fiSemantic Dependency ParsingHead-dependent word pairVRInvestorVRfocusVRshiftedVRtradersVRsaidfocusInvestorfocusfocusshifted focusshifted quicklyshifted saidsaidshiftedsaid,saidtraderssaid.LabelNONE PRED0101NONE PRED01A0noMoreArgA1AM-MNRnoMoreArgA1NONE ARGA0NONE ARGTable 2: example training sample generated via pruninginput sequence predicate two arguments, labeled A0 A1, respectively.two labels assigned next two words c g, respectively, indicatingarguments farther predicate. Accordingly, word sequence cg taken training sample.bce fgh .noMoreLeftArg A1A0 noMoreRightArgtotal list class labels model, including CoNLL-2008 data setauxiliary ones newly introduced purpose, provided Table 9 Appendix A.labels three categories, namely, 22 PropBank sense labels predicate classes,54 argument classes, 23 auxiliary labels extra classes, total 78-79. Pruningalong linPth needs one label along synPth. Note workassume whether sense label training test set meansdierent words. tendency particular word form associate sensesstatistically signicant way throughout data set allows classier predict senselabels using word form features.principle, auxiliary label assigned last item sample generatedpredicate via pruning along traversal order, syntactic linear. is,assigned rst item immediately last argument predicateseen pruning. auxiliary label treated exactly wayargument labels training decoding, except extra utility signalstop search.5. Feature Generation SelectionFollowing many previous works (Gildea & Jurafsky, 2002; Carreras & Marquez, 2005;Koomen et al., 2005; Marquez, Surdeanu, Comas, & Turmo, 2005; Dang & Palmer, 2005;Pradhan, Ward, Hacioglu, Martin, & Jurafsky, 2005; Toutanova, Haghighi, & Manning,209fiZhao, Zhang & Kit2005; Jiang & Ng, 2006; Liu & Ng, 2007; Surdeanu, Marquez, Carreras, & Comas, 2007;Johansson & Nugues, 2008; Che, Li, Hu, Li, Qin, Liu, & Li, 2008), carefully examinefactors involved wide range features used facilitateundertaking two SRL subtasks, verbal nominal predicates.endeavor decompose factors fundamental elements,largest possible space feature templates explored eectivenovel combinations features.5.1 Feature Elementfeatures adopted work intended make full use elements,mainly drawn word property syntactic connection node syntacticparse tree input sentence. sequences sets tree nodes, whose basic elementsdrawn form features via feature generation means many predened featuretemplates, identied path family relations stipulated below.Word Property type elements include word form (denoted formsplit form spForm),3 lemma (as lemma spLemma), part-of-speech tag (as posspPos), syntactic semantic dependency labels (as dprel semdprel).4Syntactic Connection includes syntactic head (as h), left/right farthest/nearestchild (as slm, ln, rm rn), high/low support verb noun. Note alongpath given word root syntactic tree, rst/last verb calledlow/high support verb, respectively. notion widely adopted eld (Toutanovaet al., 2005; Xue, 2006; Jiang & Ng, 2006).5 work, extend nounsprepositions. Besides, also introduce another syntactic head feature ppheadgiven word question, retain left sibling headed preposition,original head otherwise, aimed drawing utility fact preposition usuallycarries little semantic information. positive eect new feature conrmedexperiments.Path two basic types path argument candidate givenpredicate p, namely, linear path linePath sequence input words(inclusive) path dpPath (inclusive) syntacticdependency tree. Given two paths root r tree meetnode r , common part dpPathShare r r, dierent partsdpPathArgu dpPathPred p r , respectively, path dpPathp. Similarly, dpPath two nodes syntactic tree.Family Two child sets dierentiated given predicate argument candidate,one (as children) including syntactic children (as noFarChildren) excluding leftmost rightmost one. latter introduced featuredierentiate modiers (i.e., children) close head far away.3. Note CoNLL-2008, many treebank tokens split position hyphen (-) forwardslash (/), resulting two types form each, namely, non-split split.4. lemma pos, training test, directly pre-analyzed columns inputle, automatically generated organizer CoNLL shared tasks.5. Note notion term support verb slightly dierent works. used referverb introduces long-distance argument nominal predicate outside nounphrase headed nominal predicate.210fiSemantic Dependency ParsingOthers also number elements, besides categories,play signicant role feature generation. Many derived inter-wordrelationships. Listed number representative ones.dpTreeRelation returns relationship p input syntactic tree.possible values feature include parent, sibling, etc.isCurPred checks whether word question current predicate, returnspredicate yes, default value otherwise.existCross checks potential dependency relation given pair wordsmay cross existing relation semantic tree construction.distance returns distance two words along given path, dpPathlinePath, number words.existSemdprel checks whether given argument label predicate assigned word.voice returns either Active Passive verb default value noun.baseline small set simple rules6 used generate SRL output baselineCoNLL evaluation (Carreras & Marquez, 2005). baseline outputselectively used features, two categories: baseline Ax tags head rstNP predicate A0 A1, respectively, baseline Mod tagsmodal verb dependent predicate AM-MOD.number features existCross existSemdprel dependsemantic dependencies dependency labels existing part semantic parse tree(re)construction sentence, training decoding. Note trainingdecoding rst take candidate word pairs given sentence input, illustratedTable 2, undergo process selecting subset candidates (re)constructsemantic parse tree, consists root, predicate(s) child(ren),argument(s) predicate(s) grandchild(ren). decoding infers optimalsemantic tree sentence aid trained model (see Section 6).training reconstructs gold standard semantic tree input sentence scanningword pairs sequence dierentiating true ones treeothers. true ones rebuild tree part part. features (including existCrossexistSemdprel) extracted true ones, partially (re)built partstree, others current context fed model training.words, feature generation based gold standard argument labelstraining predicted ones decoding.5.2 Feature GenerationSequences syntactic tree nodes rst collected means paths and/or familyrelations dened above. Three strategies applied combine elementstype (e.g., form, spPos) nodes feature via string concatenation. threestrategies concatenation are: (1) sequencing (as seq), concatenates given elementstrings original order path, (2) unduplicating (as noDup), frees6. Developed Erik K Sang, University Antwerp, Belgium.211fiZhao, Zhang & Kitseq adjacent duplicates, (3) bagging (as bag), concatenates unique elementstrings alphabetical order.Given number typical feature templates illustrate individualfeatures derived ways described above, aid following operators:x+y (the concatenation x y), x.y (the attribute x), x:y (the path x y),x:y|z (the collection instances attribute z along path x y).a.lm.lemma lemma leftmost child argument candidate a.p.h.dprel dependency label syntactic head predicate candidate p.p-1 .pos + p.pos concatenation POS tags two consecutive predicates.a:p|dpPath.lemma.bag bag lemmas along dpPath p.a:p.highSupportNoun|linePath.dprel.seq seq dependency labels alonglinePath high support noun p.way, set 781 feature templates,7 henceforth referred F , generatedspecify allowable feature space feature selection. Many generatedanalogy existing feature templates literature. example, given featuretemplate like a.lm.lemma used previous works, analogous onesa.rm.lemma, a.rn.lemma a.ln.lemma included F .Predicate sense labels data set also utilized type element variousfeature templates F . However, worth noting sense label associateddierent words, e.g., 02 take.02 say.02, assumed anythingcommon anything other. predicate disambiguation, however,features always combine predicate sense word form, hence naturallydierentiate sense label dierent words. predict predicate senselabel always predict association word form. is, sense label neverused separation word form. way, model gives high precisionsense label prediction according empirical results.5.3 Feature Template Selectioncomplicated hence computationally expensive task extract optimal subsetfeature templates large feature space. sake eciency, greedy procedurefeature selection applied towards goal, illustrated many previousworks, e.g., Jiang Ng (2006), Ding Chang (2008). algorithmimplemented purpose presented Algorithm 1 below, imposes fewerassumptions previous works, aiming higher eciency. repeats twomain steps performance gain achievable given development set:1. Include template rest F current set candidate templatesinclusion would lead performance gain.2. Exclude template current set candidate templates exclusionwould lead deterioration performance. repeatedly adding/removing7. Available http://bcmi.sjtu.edu.cn/zhaohai/TSRLENAllT.txt, macro language usedimplementation, far readable notation illustrations given here.212fiSemantic Dependency Parsingmost/least useful template, algorithm aims return better smaller candidateset next round.Given n candidate feature templates, algorithm Ding Chang (2008) requiresO(n2 ) time execute training/test routine, whereas one Jiang Ng (2006)requires O(n) time, assuming initial set feature templates good enoughothers handled strictly incremental way. time complexityalgorithm also analyzed terms execution time training-and-test routinescr(M (.)), subroutines sorting negligible comparedexecution time. Algorithm 1, recruitMore rst calls routine |F S| ntimes loop, shakeOff calls |Smax | n times preparesorting, followed another |Smax | times inner loop. Assumingrst loop outer shakeOff iterate k1 k2 times, respectively,algorithm O(k1 (|F S| + k2 (|Smax | + |Smax |))) = O(k1 k2 n) time.Empirically, however, k1 , k2 << n, experiments seldom showk1 > 5 k2 > 10, especially running 1/10 F randomly chosen initial S.particular, rst loop often iterates 2-3 times, rst iteration k2drops rapidly. observation k1 k2 varies limited range suggestsmay O(k1 k2 n) = O(n) empirical estimation eciency algorithmparticular context. reasonable account rst loopcomprises two functions, namely, recruitMore recruit positive feature templatesshakeOff lter negative ones, improve model either case,likely positive/negative ones remain positive/negative consistently throughoutlooping. result, remain outside/inside candidate setrecruiting/ltering couple iterations loop.eciency allows large-scale engineering feature selection accomplishedreasonable cost time. experiments 1/10 F randomly selectedinitial S, greedy selection procedure performed along one two argumentcandidate traverse schemes (i.e., synPth linPth) NomBank, PropBank, Ss , Sslllcombination, output six feature template sets SNPN+P , SN , SP SN+P ,186, 87, 246, 120, 80 118 selected templates, respectively, performance evaluationcomparison. 5500 machine learning routines ran synPth schemenearly 7000 routines linPth. contrastive analysis template sets,focus top 100 important templates them, presentedAppendix Tables 9-17, rank columns present rankings featuretemplates terms importance respective feature template sets. importancefeature template template set measured terms performance changeadding removing template, performance model using template setmeasured labeled F1 score given test set, following conventional practiceSRL evaluation CoNLL shared tasks.interesting note six template sets tiny intersection 5templates, listed Table 10, manifesting notable variance importance rankingdierent sets. Excluding ve, rest overlap top 100 synPth,sets SNPN +P also small, 11 templates, contrastl , l llinPth sets SNPN +P , 4 times larger, 46 templates; listed213fiZhao, Zhang & KitAlgorithm 1 Greedy Feature SelectionInputtraining data set:development data set:set feature templates: FTDenotation(S) = (S, ), model using feature template set S, trained ;scr(M ) = scr(M, D), evaluation score model D;Since xed, let scr(M (S)) = scr(M (S, ), D) brevity.Algorithm1: = {f0 , f1 , ..., fk }, random subset F ;F : globally accessible constant2:3:Cr = recruitMore(S);4:Cr == {} return S;5:= shakeOff(S + Cr );6:scr(M (S)) scr(M (S )) return S;7:= S;8: end1: function recruitMore(S)Retrieve positive templates F2:Cr = {}, p = scr(M (S));3:f F4:p < scr(M (S + {f })) Cr = Cr + {f };5:end6:return Cr ;7: end function1: function shakeOff(Smax )Shake useless templates Smax2:3:= S0 = Smax ;4:sort descending ordera scr(M (S {f })) f S;5:(S = {f0 }) = {}6:Smax = argmaxx{Smax , S} scr(M (x));Drop f0 useless7:end8:S0 == Smax return S0 ;none dropped9:end10: end functiona. Namely ascending order importance f S, estimated scr(M (S)) scr(M (S {f })).214fiSemantic Dependency ParsingTables 11 12, respectively. Besides shared templates, six sets hold 84, 71,84, 69, 29 67 others top 100, listed Tables 13-18, respectively,negative/positive subscript denotes preceding/following word. example, a.lm -1 .lemmareturns lemma previous word left child.rather small overlap six sets suggests greedy feature selection algorithm maintains stable eciency working template sets huge divergence,lending evidence support empirical estimation above. Despite divergence,template sets enables SRL model achieve state-of-the-art performanceCoNLL-2008 data set,8 indicating eectiveness approach,details evaluation provided Section 7 below.6. DecodingFollowing exactly procedure generating training sample, classier,training, outputs series labels sequence word pairs generatedinput sentence, inferring predicates arguments one another. Dierentexisting SRL systems, instantiates integrative approach conductspredication trained model. However, following common practice incorporating task-specic constraints global inference (Roth & Yih, 2004; Punyakanok,Roth, Yih, & Zimak, 2004), opt developing decoding algorithm inferoptimal argument structure predicate identied way classier. main dierences work Punyakanok et al. (2004) (1)use ILP joint inference, exact, use beam search, greedyapproximate, (2) constraints (e.g., duplicate argument label allowed)impose arguments individual linear (in)equalities realizedconstraint fulllment features (e.g., existCross existSemdprel).Specically, decoding identify arguments among candidate words inferring best semantic role label candidate (cf. training sample Table2 one label per word). Let = {a0 , a1 , ..., an1 } candidates predicate,ai embodies available properties word, including candidate label,let Ai = a0 a1 ... ai1 partial argument structure (of target search)determined ready use context inferring next argument. Insteadcounting best-rst search, simply keeps picking next best argument accordingconditional probability p(ai |Ai ), resort beam search better approximationglobal optimization maximal probability= argmaxnp(ai |Ai ),(1)i=0Ai consists rst elements . Ideally, beam search returnsprobable subset arguments predicate question. rests conditionalmaximum entropy sequential model incorporating global features decoding inferarguments necessarily sequential order. previous practice,8. Note early version model also illustrated top-ranking performance CoNLL-2009multilingual data sets (Zhao, Chen, Kit, & Zhou, 2009).215fiZhao, Zhang & Kitmodel adopts tunable Gaussian prior (Chen & Rosenfeld, 1999) estimate p(ai |Ai )applies L-BFGS algorithm (Nocedal, 1980; Nash & Nocedal, 1991) parameteroptimization.7. Evaluationevaluation SRL approach conducted various feature template setsocial training/development/test corpora CoNLL-2008 (Surdeanu et al., 2008).data set derived merging dependency version Penn Treebank 3 (Marcus,Santorini, & Marcinkiewicz, 1993) PropBank NomBank. Note CoNLL-2008essentially joint learning task syntactic semantic dependencies. research presented article focused semantic dependencies, primaryevaluation measure semantic labeled F1 score (Sem-F1 ). scores, includingmacro labeled F1 score (Macro-F1 ), used rank participating systemsCoNLL-2008, Sem-F1 /LAS, ratio labeled F1 score semantic dependencies labeled attachment score (LAS) syntactic dependencies, also providedreference.7.1 Syntactic InputTwo types syntactic input used examine eectiveness integrative SRL approach. One gold standard syntactic input available ocial data setparsing results data set two state-of-the-art syntacticparsers, namely, MSTparser9 (McDonald, Pereira, Ribarov, & Hajic, 2005; McDonald& Pereira, 2006) parser Johansson Nugues (2008). However, instead usingoriginal MSTparser, substantially enriched additional features, followingChen, Kawahara, Uchimoto, Zhang, Isahara (2008), Koo, Carreras, Collins (2008),Nivre McDonald (2008). latter one, henceforth referred J&N short,second-order graph-based dependency parser takes advantage pseudo-projectivetechniques resorts syntactic-semantic reranking rening nal outputs.However, 1-best outputs reranking used evaluation, eventhought reranking slightly improve parsing performance. Note reward reranking joint-learning syntactic semantic parsing gainedhuge computational cost. contrary, approach intended show highlycomparable results achieved much lower cost.7.2 Experimental Resultseectiveness proposed adaptive approach pruning argument candidatesexamined three syntactic inputs, results presented Table 3,10coverage rate proportion true arguments pruning output. Noteusing auxiliary labels aect rate, accounted choicetraverse path quality syntactic input, suggested dierencesynPth rows. results show pruning reduces 50% candidates along9. Available http://mstparser.sourceforge.net.10. Decimal gures tables herein percentages unless otherwise specied.216fiSemantic Dependency ParsingSyntactic Input (LAS)MST (88.39%)J&N (89.28%)Gold (100.0%)PathlinP thsynP thlinP thsynP thlinP thsynP thOriginal5.29M2.15M5.28M2.15M5.29M2.13MPruning1.57M1.06M1.57M1.06M1.57M1.05MReduction-70.32-50.70-70.27-50.70-70.32-50.70Coverage100.095.0100.095.4100.098.4Table 3: Reduction argument candidates adaptive pruningPath xlinPthsynPthReductionxSN7,1035,609-21.03SPx7,2145,470-24.18xSN+P7,1465,572-22.03Table 4: Number executions training-and-test routine greedy feature selectionsynPth, cost losing 1.6-4.6% true ones, 70% along linPth withoutloss. Nevertheless, candidate set resulted synPth 1/3 smaller sizelinPth.number times training-and-test routine executed greedy selectionsix feature sets presented Table 4, showing synPth saves 21%-24% executiontimes. Given estimation time complexity selection algorithm O(k1 k2 n)executing routine, empirically 7 < k1 k2 < 10 feature space size n = 781experiments, verifying high eciency algorithm.pointed Pradhan, Ward, Hacioglu, Martin, Jurafsky (2004), argumentidentication (before classication) bottleneck problem way improving SRLperformance. Narrowing set predicate candidates much possible reliable way shown feasible means alleviate problem. eectivenessadaptive pruning purpose examined comparative experimentsterms time reduction performance enhancement. results seriesexperiments presented Table 5, showing adaptive pruning saves trainingtest time 30% 60%, respectively, enhancing performance (inSem-F1 score) 23.9%24.8%, nearly quarter. results also conrm signicantimprovement upon non-adaptive origin (Xue & Palmer, 2004) twofold benetpruning arguments far away predicates, follows assumption true arguments tend close predicates. straightforwardusing noMoreArg label reduces training samples using (see Section 4)hence leads greater reduction training time. Using label also decreasestest time remarkably. decoding, noMoreArg label, assigned probabilityhigher possible role labels current word pair, hints decoderstop working next word pair, resulting test time reduction 18.521.0percentage points upon non-adaptive pruning. particularly low performance without pruning also reects soundness motivation candidate pruning217fiZhao, Zhang & KitBankFeaturesPropBank87NomBank+PropBank246PruningaAdaptiveb+AdaptiveAdaptive+AdaptiveTraining122,469s109,094s83,208s432,544s392,216s305,325sRedu.-10.9-32.1-9.3-29.4Test747s372s234s2,795s1,615s1,029sRedu.-50.2-68.7-42.2-63.2Sem-F166.8580.5982.8064.8579.7780.91a. Syntactic input: MST; Traverse scheme: synPth; Machine conguration: Four six-coreRRXeonX5690 3.46GHz processors, 48G memory.Intelb. original pruning Xue Palmer (2004), using noMoreArg.Table 5: Time reduction performance enhancement adaptive pruningSyn. Input(LAS)MST(88.39%)J&N(89.28%)Gold(100%)FeatureSetInitialSelectedInitialSelectedInitialSelectedPath xlinPthsynPthlinPthsynPthlinPthsynPthlinPthsynPthlinPthsynPthlinPthsynPthNomiF1 xN44.5844.6777.9377.8944.8445.0177.7377.7045.5745.8980.4380.37VerbF1 xP58.8363.2482.7282.8058.8463.2683.2183.9061.7967.6389.4490.37NomiF1 xN+P41.1842.4276.7577.5242.1643.6476.4576.7942.4143.7679.4480.20VerbF1 xN+P56.3461.2882.3083.2456.4061.3682.7083.7159.0965.5189.0790.27SemF1 xN+P51.1454.7980.0580.9151.3655.1280.1580.8853.1257.7784.9986.02SemF1 xN+P /LAS57.8661.9990.5691.5457.5361.7489.7790.5953.1257.7784.9986.02Table 6: Performance random initial greedily selected feature setsmachine learning linguistic perspective. pruning provides balancedtraining dataset classier training without pruning. Note without pruning,word pairs generated training irrelevant far away currentpredicate, inevitably interfering informative features truly relevant onessmall minority and, hence, leading unsatisfactory performance. Althoughpruning, especially adaptive version, rooted linguistic insight gained empirical observations real data, previous works semantic parsing simply tookpruning indispensable step towards good parsing performance, seldom paying muchattention poor performance without pruning comparing performancedierent pruning strategies.Table 6 presents comprehensive results semantic dependency parsingthree syntactic inputs aforementioned dierent quality. number observationsmade results. (1) greedy feature selection, encoded Algorithm 1above, boosts SRL performance drastically, raising Sem-F1 scores synPthrows 54.79%57.77% initial feature sets, baseline, 80.88%86.02%218fiSemantic Dependency ParsingSyn. Input(LAS)Feature setPath xMST(88.39%)lSN+P - SenseSN+P - SenselSN+ SPlSN+ SPslinP thsynP thlinP thsynP thNomiF1 xN+P76.5176.7676.7876.60VerbF1 xN+P82.0982.7582.2082.76SemF1 xN+P79.8280.3079.9980.24SemF1 xN+P /LAS90.3090.8590.5090.78LossF1 xN+P-0.29-0.75-0.07-0.83Table 7: Experimental results feature ablation feature set combinationselected feature sets, increment 46.73%48.90%. rise corresponding linPthrows even larger. Among three inputs, largest increment gold standard,suggesting feature selection greater eect input better quality.(2) traverse scheme synPth leads better model linPth, reecteddierence Sem-F1 Sem-F1 /LAS scores them, indicating integrativeSRL approach sensitive path along argument candidates traversed.dierence Sem-F1 /LAS scores, instance, range 7.14%8.75%0.91%1.21% initial selected feature sets, respectively. signicantadvantage synPth conrmed consistently, even though optimized feature set narrowsperformance discrepancy two radically. (3) resultNomi-F1 xN Verb-F1 xP higher corresponding F1 xN +P consistently throughoutalmost experimental settings except one shows feature selection separatelyNombank PropBank (for verbal nominal predicates, respectively) gives betterperformance combination Nombank+PropBank both.explained interference two data sets due heterogeneousnature, namely, interference nominal verbal predicate samples. Hence,optimizing feature set specically particular type predicates eectiveboth. (4) overall comparison systems SRL performance three syntacticinputs dierent quality (as reected LAS) shows performance wholevaries accord quality input. exhibited contrast Sem-F1scores inputs, even though small LAS dierence may necessarily leadsignicant performance dierence (for instance, MST LAS 0.89 percentage pointlower J&N gives Sem-F1 score high one four experimental settings).table also shows LAS dierence 11.61 percentage points, 88.39% 100%,corresponds Sem-F1 score dierence 5.14 percentage points, 80.88%86.02%, best setting (i.e., using selected feature set taking synPth).However, Sem-F1 scores cannot trusted faithfully reect competencesemantic parser, quality syntactic input also decisive factor decidescores. reason, Sem-F1 /LAS ratio evaluation metric.Interestingly, parsers scores ratio two syntactic inputs LAS 10.8211.61 percentage points gold standard are, contrarily, 4.575.52 percentage pointshigher. certainly mean parser able rescue, sense, truesemantic parses erroneous syntactic input. Instead, explainedparsers high tolerance imperfections syntactic input.Table 7 presents experimental results feature ablation feature set combination. former examine eect sense features latter feature219fiZhao, Zhang & Kitoptimization. Along synPth, ablation sense feature mix two featuresets respectively optimized (through greedy selection) NomBank PropBanklead signicant performance loss 0.75%0.83%, comparison performancefeature set SN+P optimized combination two treebanks given Table6. Along linPth, lead much less signicant insignicant loss, respestively.results show sense features greedy selection featuressignicant joining adaptive pruning along synPth achieve performance gain.7.3 Comparison Analysisorder evaluate parser impartially comparative manner, performance alongsynPth compared state-of-the-art systems CoNLL-2008.chosen comparison ranked among top four among participantsshared task using sophisticated joint learning techniques. one Titov,Henderson, Merlo, Musillo (2009) adopts similar joint learning approachHenderson, Merlo, Musillo, Titov (2008) also included, signicantmethodological dierence others. particular, former attained bestperformance date direction genuine joint learning. reported performancesystems CoNLL-2008 test set terms series F1 scores presentedTable 8 comparison. signicantly better (t = 14.6, P < 0.025) othersexcept post-evaluation result Johansson Nugues (2008). Contrary bestthree systems CoNLL-2008 (Johansson & Nugues, 2008; Ciaramita, Attardi, DellOrletta,& Surdeanu, 2008; Che et al., 2008) use SRL pipelines, current work intendedintegrate one. Another baseline, namely, current model using featureset work Zhao Kit (2008), instead random set, also includedtable comparison, showing signicant performance enhancement top previousmodel and, then, enhancement greedy feature selection.Although work draws necessary support basic techniques (especiallytraverse along synP th) underlying previous systems CoNLL-2008 -2009(Zhao & Kit, 2008; Zhao, Chen, Kit, & Zhou, 2009; Zhao, Chen, Kazama, Uchimoto, &Torisawa, 2009), marks uniqueness SRL sub-tasks performed oneintegrative model one selected feature set. previous systems dealt predicatedisambiguation separate sub-task. rst attempt fully integrated SRLsystem.fact integrated system yet give performance par postevaluation result Johansson Nugues (2008) seems attributable number factors,including ad hoc features adopted work handle linguistic constructionsraising/control coordination. However, noticeable ones followingdiscrepancies two systems, addition pipeline vs. all-in-one integration.(1) n-best syntactic candidates input, without doubt provideuseful information 1-best use. (2) Then, exploit reranking jointlearning strategy make fuller use n-best candidates intermediate semanticresult available, resulting gain 0.5% increment Sem-F1 score. (3)use respective sub-systems deal verbal nominal predicates specicmanner, following observation adaptive optimization feature sets nominal220fiSemantic Dependency ParsingSystemsaLASOurs:GoldJohansson:2008*dOurs:MSTOurs:JohanssonJohansson:2008Ours:BaselineeCiaramita:2008*Che:2008Zhao:2008*Ciaramita:2008Titov:2009Zhao:2008Henderson:2008*Henderson:2008100.089.3288.3989.2889.3288.3987.3786.7587.6886.6087.5086.6687.6486.91SemF186.0281.6580.9180.8880.3779.4278.0078.5276.7577.5076.1076.1673.0970.97MacroF192.2785.4985.0985.1284.8684.3482.6982.6682.2482.0681.8081.4480.4879.11Sem-F1/LAS86.0291.4191.5490.5989.9889.8589.2890.5187.5389.4986.9787.8883.4081.66PredF1 b89.2587.2287.1586.4785.4086.6083.4685.3178.5283.4678.2681.4279.60ArguF1 c84.5479.0478.0178.2978.0276.0875.3575.2775.9374.5675.1869.1066.83VerbF190.2784.7883.2383.7184.4581.7180.9380.4678.8180.1577.6775.8473.80NomiF180.2077.1277.5276.7974.3276.0773.8075.1873.5973.1773.2868.9066.26a.b.c.d.Ranked according Sem-F1 , rst authors listed sake space limitation.Labeled F1 predicate identication classication.Labeled F1 argument identication classication.superscript * indicates post-evaluation results, available ocial website CoNLL2008 shared task http://www.yr-bcn.es/dokuwiki/doku.php?id=conll2008:start.e. Syntactic input traverse scheme: Ours:MST; Features: Zhao:2008Table 8: Performance comparison best existing SRL systemsverbal predicates respectively likely give better performancemix both. observation also conrmed evidence experimental results:F1 xN F1 xP scores consistently higher respective F1 xN +P ones Table 6 above.integrative nature approach, however, priority givenoptimizing whole feature set verbal nominal predicates. neverthelessunderstood point potential ways enhance system, e.g.,taking advantage specialized feature sets various kinds words and/or utilizingjoint learning techniques syntactic-semantic reranking, way integritysystem maintained properly.dierence joint learning work Johansson Nugues (2008)Titov et al. (2009) worth noting. former kind cascade-style jointlearning rst syntactic submodel provide n-best syntactic treessemantic submodel infer correspondent semantic structures, reranking model,log probabilities syntactic trees semantic structures features, ndbest joint syntactic-semantic analysis, resulting improvement top individualsubmodels. contrast former non-synchronous pipeline syntacticsemantic parsing, latter adopts stricter all-in-one strategy joint learning,syntactic semantic dependencies learnt decoded synchronously, basedaugmented version transition-based shift-reduce parsing strategy (Henderson et al.,2008). Regrettably, however, performance approach still far topranked list Table 8, indicating particular signicance current work.221fiZhao, Zhang & KitWhether worth integrating form joint-learning integrative systemdepends cost-eectiveness so. illustratedjoint learning lead certain performance improvement, CoNLL shared taskSRL successive works, e.g., Johansson Nugues (2008). However, great dealcomputational cost paid order enable reranking procedure handlemultiple syntactic inputs. certainly makes impractical real applications,mention integrative system born particularly strong demand integritypreclude accommodating stand-alone submodel.8. ConclusionSemantic parsing, aims derive instantiate semantic structure sentencevia identifying semantic relations words, plays critical role deep processingnatural language. article, presented integrative approach semanticdependency parsing form semantic role labeling, implementation all-inone word pair classier, comprehensive evaluation using three syntactic inputsdierent quality. evaluation results conrm eectiveness practicalityapproach. major contributions research following. exhibits signicantsuccess rst time integrative SRL system achieved performance nextbest pipeline system, indicating potentials integrative approachbesides practicality real applications. large-scale feature selection engineeringunderlying success work also demonstrates (1) largest feature space everuse eld formed allowing wide range exible (re)combinations basicelements extracted known features properties input words (2)speedy adaptive feature selection procedure formulated applied selecteective set features allowable feature space.core techniques contributed success developed based twotypes traverse path, along syntactic tree branches vs. linear input word sequence.argument candidate pruning feature selection performed along identical path.strategy using auxiliary labels facilitate argument candidate pruning, followingobservation true arguments tend close predicates, works welltraverse schemes. Interestingly, although feature selection procedure outputs twodierent feature sets NomBank, PropBank combination whilst workingalong two paths, feature sets lead SRL system close performancetest data, competitive performance top one best pipeline system,conrming robustness eectiveness feature selection procedure.Evidence also presented evaluation results reconrm nding previous works semantic parsing feature sets optimized specically verbal nominalpredicates outperform collective one both. However, competitive performancecollective one arrived also suggests harmonious rival feature settypes predicate whole reachable slight performance dierencespecic sets fairly acceptable unavoidable small cost exchangehigher integrity practicality integrative SRL system. competitivenessattributable least two main factors. One large feature space use,provides dozen times many feature templates previous222fiSemantic Dependency Parsingworks (e.g., see Xue & Palmer, 2004; Xue, 2006). classieraccommodate many features one model. According experience piecework, model vulnerable use many overlapping features,SVM margin-based learners usually suer lot.Acknowledgmentsresearch reported article partially supported Department Chinese, Translation Linguistics, City University Hong Kong, post-doctorateresearch fellowship rst author research grant (CTL UNFD-GRF-144611)third corresponding author, National Natural Science Foundation China(Grants 60903119 61170114), National Basic Research Program China (Grant2009CB320901), National High-Tech Research Program China (Grant 2008AA02Z315),Research Grants Council HKSAR, China (Grant CityU 144410), City University Hong Kong (Grant 7002796). Special thanks owed Richard Johanssonkindly providing syntactic output CoNLL-2008 shared task, three anonymousreviewers insightful comments John S. Y. Lee help.Appendix A. Feature Templates Importance RankingsTypePredicateArgumentA05AA,C-A04R-A04R-AAAM-PRDAM-PRTAM-RECAM-TMAM-TMPPropBank0121 (21)AM-ADV C-AM-ADVAM-CAU C-AM-CAUAM-DIR C-AM-DIRAM-DIS C-AM-DISAM-EXT C-AM-EXTAM-LOC C-AM-LOCAM-MNR C-AM-MNRAM-MOD C-AM-NEGAM-NEG C-AM-PNCAM-PNC C-AM-TMPR-AM-ADVR-AM-CAUR-AM-DIRR-AM-EXTR-AM-LOCR-AM-MNRR-AM-PNCR-AM-TMPC-R-AM-TMPSU (54)Extra/AuxiliaryNONE PREDNONE ARGTotal22noMoreArg(for synPth)56noMoreLeftArgnoMoreRighArg(for linPth)57Table 9: list class labels predicate argumentTemplateRank in:p.lm.dprela:p|dpPath.dprela.lemma + p.lemmaa.lemma + a.dprel + a.h.lemmaa.spLemma + p.spLemmaSN+P413510554SN3931444097SPs65244915lSN+P822511213Table 10: Overlap six resulted feature template sets223lSN11362366968SPl60264426fiZhao, Zhang & KitTemplateRank in:p1 .pos + p.posp1 .spLemmap.spForm + p.lm.spPos + p.noFarChildren.spPos.bag + p.rm.spPosa.isCurPred.lemmaa.isCurPred.spLemmaa:p|existCrossa:p|dpPath.dprel.baga:p|dpPathPred.spForm.baga:p|dpPath.spLemma.seqa:p|linePath.spForm.baga.semdprel = A0 ?SN+P22778336484797678550SN3713459438771424594886SPs795963758682855716140Table 11: Overlap SN, SPs SN+P besides Table 10TemplateRank in:p.spLemma + p.currentSensep.currentSense + a.spLemmap.voice + (a:p|direction)p.children.dprel.noDupp.rm.dprelp.rm.formp1 .spLemma + p.spLemmap.voicep.form + p.children.dprel.noDupp.lm.form + p.noFarChildren.spPos.bag + p.rm.formp.lemmap.lemma + p1 .lemmap.spFormp.spForm + p.children.dprel.bagp.spForm + p.lm.spForm + p.noFarChildren.spPos.bag + p.rm.spFormp.splemmap.spLemma + p.h.spFormp.spLemma + p1 .spLemmap1 .posa1 .isCurPred.lemmaa1 .isCurPred.lemma + a.isCurPred.lemmaa1 .isCurPred.spLemma + a.isCurPred.spLemmaa.isCurPred.Lemma + a1 .isCurPred.Lemmaa.isCurPred.spLemma + a1 .isCurPred.spLemmaa.spPos.baseline Ax + a.voice + (a:p|direction)a.spPos.baseline Moda.h.children.dprel.baga.lm1 .spPosa.lm1 .lemmaa.children.spPos.seq + p.children.spPos.seqa.rm.dprel + a.posa.rm.dprel + a.spPosa.rm1 .spPosa.rm.lemma224lSN+P1833651160113382696884739911049100727667421429502486974749192130636lSN28571205411411061481106265100610651111210410924894445980356330901777450SPl56172540380691065550343630146470332877436267594618454968762422154fiSemantic Dependency Parsing282737512533294167943110a.rn.dprel + a.spPosa1 .lemma + a.lemmaa:p|dpPathArgu.dprel.seqa:p|dpPathArgu.pos.seqa:p|dpPathPred.dprel.seqa.forma.form + a.posa.form + a1 .forma.spForm + a.spPosa.spForm + a1 .spForma.spLemma + a.dprela.spLemma + a.h.spForm334696796494933173381182723719357832384852751llTable 12: Overlap SN, SPl SN+P besides Table 10Templatep.lemma + p.currentSensep.currentSense + a.lemmaa.form + p.semdprel ctype ?a.form + p.semdprel rtype ?p.lm.formp1 .form + p.formp2 .formp2 .spForm + p1 .spFormp.form + p.dprelp.lemma + p.h.formp.spForm + p.dprelp.spLemma + p.children.dprel.noDupp.spLemma + p1 .spLemmaa.voice + (a:p|direction)leaf syntactic tree ?a.lm.dprel + a.spPosa.lm.pos + a.posa.pphead.spLemmaa.rm1 .forma.rm1 .spPosa.highSupportVerb.forma.lowSupportPorp.forma.lowSupportPorp.spLemmaa1 .posa1 .spForma:p|dpPath.distancea:p|dpPathArgu.spLemma.baga:p|dpPathPred.spPos.baga:p|linePath.dprel.baga.form + a.children.pos.seqa.form + a.posa.spForm + a.children.spPos.seqa.spForm + a.spPosa.spLemmaa.spLemma + a1 .spLemmaRank825735477178157410464349231667501981795651697085996938853176871160225Templatep.spLemma + p.currentSensep.currentSense + a.spLemmaa.form + p.ctypeSemdprela.form + p.rtypeSemdprelp.lm.spFormp1 .spLemma + p.spLemmap2 .spFormp.formp.lemmap.posp.spForm + p.children.dprel.bagp.spLemma + p.h.spFormp1 .posa.children.adv.baga.lm.dprel + a.forma.lm1 .spLemmaa.lm.spPosa.rm.dprel + a.spPosa.rm1 .spForma.rn.dprel + a.spForma.highSupportVerb.spForma.lowSupportPorp.lemmaa1 .lemma + a1 .lemmaa1 .pos + a.posa1 .spPos + a1 .spPosa:p|dpPath.spLemma.baga:p|dpPathPred.spLemma.baga:p|dpPathArgu.dprel.seqa.semdprel = A2 ?a.form + a.forma.pos + a.children.spPos.seqa.spForm + a.children.spPos.baga.spForm + a1 .spForma.spLemma + a.pphead.spForma.spPos + a.dprel + a.h.spPosRank801846792616863629027289575100826553299912084987322235581265526441fiZhao, Zhang & Kita1 .form54a1 .spForma1 .spPos33(a:p|dpTreeRelation) + p.form(a:p|dpTreeRelation) + p.spPos29(a:p|dpTreeRelation) + a.spPos(a:p|dpPath.dprel.seq) + p.spForm36a1 .isCurPred.spLemma + a.isCurPred.spLemmaa.noFarChildren.spPos.bag + a.rm.spPosa.children.spPos.seq + p.children.spPos.seqa.highSupportNoun:p|dpPath.dprel.seq(a.highSupportNoun:p|dpTreeRelation) + p.form(a.highSupportVerb:p|dpTreeRelation) + a.spForm(a.lowSupportVerb:p|dpTreeRelation) + a.spForm83253017213489667242Table 13: Feature templates SNbesides Tables 10 11TemplateRankTemplatep.rm.dprel47p.dprelp.children.dprel.bag66p.lm.spPosp.children.pos.seq70p.rm.dprelp2 .pos23p2 .spForm + p1 .spFormp.dprel = OBJ ?50p.lemma + p.h.formp.lemma+p1 .lemma3p.posp.spForm76p.spForm + p.children.dprel.noDupp.splemma9p.spLemma+p1 .spLemmap1 .spPos21a.lowSupportVerb:p|dpTreeRelationa.children.adv.bag20a.dprela.children.dprel.bag7a.h.lemmaa.h.spLemma72a.lm.dprel + a.spPosa.lm1 .spLemma54a.pphead.lemmaa.pphead.spLemma46a.rm.dprel + a.spPosa1 .lemma + a1 .lemma16a1 .posa1 .spLemma + a.spLemma29a:p|linePath.distancea:p|dpPath.distance22a:p|dpPathPred.dprel.baga:p|dpPath.spForm.seq12a:p|dpPathArgu.spForm.seqa:p|dpPathArgu.spLemma.bag84a:p|dpPathPred.spLemma.baga:p|dpPathArgu.spLemma.seq17a:p|dpPath.spPos.baga:p|dpPathPred.spPos.bag64a:p|dpPathArgu.dprel.seqa:p|linePath.spLemma.seq42a:p|linePath.spLemma.baga:p|dpPathPred.spPos62a.existSemdprel A0a.existSemdprel A156a.existSemdprel A2a.semdprel = A2 ?77a.dprel = OBJ ?a.form + a.children.pos.seq10a.pos + p.posa.spLemma + a.dprel87a.spLemma+a.dprel+a.h.spLemmaa1 .lemma14a1 .spPos(a:p|dpTreeRelation) + a.spPos81(a:p|dpPath.dprel.seq) + p.spPosa1 .isCurPred.spLemma + a.isCurPred.spLemmaa2 .isCurPred.lemma + a1 .isCurPred.lemmaa.isCurPred.spLemma + a1 .isCurPred.spLemmaa.lowSupportVerb:p|dpPath.dprel.seqa.lowSupportVerb:p|dpPathArgu.dprel.seqa.lowSupportVerb:p|dpPathArgu.spPos.seqa.lowSupportVerb:p|dpPathShared.dprel.seq226Rank254851436826601321383180782455531126528276757734519691841587433343536fiSemantic Dependency Parsinga.lowSupportVerb:p|dpPathShared.spPos.seqa.lowSupportVerb:p|dpPathPred.dprel.seqa.lowSupportVerb:p|dpPathPred.spPos.seqa.highSupportNoun:p|dpPath.dprel.seqa.lowSupportVerb:p|dpPath.dprel.seq(a.highSupportVerb:p|dpTreeRelation) + a.spPos373839833044Table 14: Feature templates SPs besides Tables 10 11TemplateRankTemplatep.lemma + p.currentSense100p.currentSense + a.lemmaa.form + p.semdprel ctype ?90a.form + p.ctypeSemdprela.form + p.semdprel rtype ?92a.form + p.rtypeSemdprelp.dprel8p.children.pos.seqp.rm.dprel46p.lowSupportProp:p|dpTreeRelationp1 .spForm + p.spForm54p.voicep.lemma+p1 .lemma18p.pos + p.dprelp.splemma88p.spLemma+p.h.spFormp.spPos + p.children.dprel.bag15p.spPos + p1 .spPosp1 .spForm26a1 .isCurPred.lemmaa.isCurPred.pos84a.isCurPred.spPosa1 .isCurPred.Lemma37a1 .isCurPred.spLemmaa:p|direction57(a:p|dpPath.dprel.seq) + a.spForma.form.baseline Mod73a.pos.baseline Moda.spForm.baseline Mod75a.baseline Moda.lm.Lemma59a.lm.spForma.lm.spPos65a.rm.lemmaa.highSupportNoun.pos62a.highSupportNoun.spPosa.highSupportVerb.spPos42a.lowSupportNoun.posa.lowSupportPorp.spLemma98a.lowSupportVerb.posa1 .spLemma+a.spLemma38a:p|dpPathPred.spLemma.seqa:p|linePath.spForm.seq80a:p|linePath.spLemma.seqa:p|linePath.spLemma.bag86a:p|linePath.spPos.seqa:p|linePath.spPos.bag66a:p|dpPathPred.spPosa.existSemdprel A049a.existSemdprel A1a.form94a.form = p.form ?a.form + a.form95a.lemmaa.lemma + a.dprel21a.lemma + a.h.forma.lemma + a.pphead.form44a.spForm = p.spForm ?a.spLemma + a.pphead.spForm24a.spPos + a.spPos(a:p|dpPath.dprel.seq) + p.form45(a:p|dpPath.dprel.seq) + p.spForm(a:p|dpPath.dprel.seq) + a.form13p.lm.form + p.noFarChildren.spPos.bag + p.rm.forma2 .isCurPred.lemma + a1 .isCurPred.lemmaa.isCurPred.pos + a1 .isCurPred.posa.isCurPred.spLemma + a1 .isCurPred.spLemmaa.form.baseline Ax + a.voice + (a:p|direction)a.spForm.baseline Ax+ a.voice + (a:p|direction)a.spPos.baseline Ax + a.voice + (a:p|direction)a.highSupportNoun:p|dpPathShared.dprel.seqa.highSupportVerb:p|dpPathShared.dprel.seq227Rank619193612953142896221174766081208756635351391404329898225526499237778793068fiZhao, Zhang & Kit163132333417697071725819a.lowSupportNoun:p|dpPath.dprel.seqa.lowSupportNoun:p|dpPathArgu.dprel.seqa.lowSupportNoun:p|dpPathArgu.spPos.seqa.lowSupportNoun:p|dpPathShared.dprel.seqa.lowSupportNoun:p|dpPathShared.spPos.seqa.lowSupportNoun:p|dpPathPred.dprel.seqa.lowSupportVerb:p|dpPathArgu.dprel.seqa.lowSupportVerb:p|dpPathArgu.spPos.seqa.lowSupportVerb:p|dpPathShared.dprel.seqa.lowSupportVerb:p|dpPathShared.spPos.seq(a.highSupportVerb:p|dpTreeRelation) + a.form(a.lowSupportNoun:p|dpTreeRelation) + p.spPosTable 15: Feature templates SN+P besides Tables 10 11TemplateRankTemplatep1 .spLemma74p2 .formp1 .spPos19a1 .isCurPred.Lemmaa1 .isCurPred.spLemma53a.children.dprel.baga.h.lemma23a.lm.dprel + a.posa.lm1 .lemma31a.lm.Lemmaa.pphead.lemma27a.pphead.spLemmaa.lowSupportNoun.spPos8a.lowSupportPorp.forma.lowSupportPorp.lemma47a.lowSupportPorp.spForma.lowSupportPorp.spLemma57a1 .spPosa1 .spPos + a1 .spPos54a.semdprel = A2 ?(a:p|dpTreeRelation) + p.pos41(a:p|dpTreeRelation) + p.spPosa2 .isCurPred.spLemma + a1 .isCurPred.spLemmaa.lowSupportPorp:p|dpPathShared.dprel.seqa.lowSupportPorp:p|dpPathShared.spPos.seqa.lowSupportVerb:p|dpPath.dprel.seq(a.highSupportVerb:p|dpTreeRelation) + a.form(a.lowSupportNoun:p|dpTreeRelation) + p.pos(a.lowSupportNoun:p|dpTreeRelation) + p.spPosRank557142632939737958202161121316117566Table 16: Feature templates SPl besides Tables 10 12Templatep.rm.dprelp.lowSupportNoun.spFormp1 .form + p.formp1 .pos+p.posp1 .spLemmap2 .posp.dprel = OBJ ?p.lemma + p.h.formp.spPos + p1 .spPosa.voice + (a:p|direction)a.isCurPred.spLemmaRank88161033213185942347529228Templatep.children.dprel.seqp.lowSupportProp:p|dpTreeRelationp1 .lemma + p.lemmap1 .spForm + p.spFormp2 .form + p1 .formp2 .spFormp.form + p.dprelp.pos + p.dprelp1 .spForma.isCurPred.lemmaa.lm.dprel + a.dprelRank277291409939951864398fiSemantic Dependency Parsinga.lm.dprel + a.pos76a.lm1 .spLemmaa.lm.pos + a.pos19a.lm.spForma.lm.spPos49a.ln.dprel + a.posa.rm1 .spPos21a.highSupportNoun.lemmaa.highSupportNoun.pos48a.highSupportNoun.spPosa.lowSupportVerb.pos97a.lowSupportVerb.spLemmaa.lowSupportVerb.spPos12a1 .lemmaa1 .spLemma+a.spLemma77a2 .posa:p|linePath.distance67a:p|dpTreeRelationa:p|dpPathPred.spPos115a.dprel = OBJ ?a.form + p.form83a.pos + p.posa.spForm + p.spForm87a.spForm + a.children.spPos.seqa.spForm + a.children.spPos.bag119a.spLemma+a.dprel+a.h.spLemmaa.spLemma + a.pphead.spForm66a.spLemma + a1 .spLemmaa1 .pos52a1 .spPos(a:p|dpTreeRelation) + p.form111(a:p|dpTreeRelation) + p.spForm(a:p|dpTreeRelation) + a.form84(a:p|dpTreeRelation) + a.spForm(a:p|dpTreeRelation) + a.spPos15(a:p|dpPath.dprel.seq) + p.form(a:p|dpPath.dprel.seq) + p.spForm108(a:p|dpPath.dprel.seq) + a.form(a:p|dpPath.dprel.seq) + a.spForm37p.spForm + p.lm.spPos + p.noFarChildren.spPos.bag + p.rm.spPosa2 .isCurPred.lemma + a1 .isCurPred.lemma(a1 :p|direction) + (a2 :p|direction)a.noFarChildren.spPos.bag + a.rm.spPosa.highSupportVerb:p|dpTreeRelation(a.highSupportVerb:p|dpTreeRelation) + a.form(a.lowSupportNoun:p|dpTreeRelation) + p.form(a.lowSupportNoun:p|dpTreeRelation) + p.spForm3107251451781011022011692536055238415670117581052285478271lTable 17: Feature templates SNbesides Tables 10 12Templatep.currentSense + a.spPosp.lm.formp.lowSupportNoun.spFormp1 .form + p.formp1 .spForm + p.spFormp2 .posp.form + p.dprelp.spPos + p1 .spPosp1 .spPosa.isCurPred.lemmaa1 .isCurPred.Lemmaa.children.dprel.baga.lm1 .lemmaa.lm.Lemmaa.lm.spForma.ln.dprel + a.posa.lowSupportNoun:p|dpTreeRelationa1 .lemmaa1 .spPosRank6910199106988711445102524148208434639381109229Templatep.rm.dprelp.lm.spFormp.lowSupportProp:p|dpTreeRelationp1 .pos+p.posp2 .form + p1 .formp2 .spFormp.spForm + p.dprelp1 .spForma.voice + (a:p|direction)a.isCurPred.spLemmaa1 .isCurPred.spLemmaa.lm.dprel + a.dprela.lm1 .spLemmaa.lm.pos + a.posa.lm.spPosa.rm1 .spPosa.lowSupportVerb.spLemmaa1 .spLemma+a.spLemmaa1 .spPos + a1 .spPosRank117517414054115371066647017859111153192fiZhao, Zhang & Kita:p|linePath.distance80a:p|dpTreeRelationa:p|dpPathPred.spPos85a.existSemdprel A2a.semdprel = A2 ?78a.spForm + a.children.spPos.seqa.spForm + a.children.spPos.bag61a.spLemma+a.dprel+a.h.spLemmaa.spLemma + a.pphead.spForm62a1 .lemmaa1 .spPos44(a:p|dpTreeRelation) + a.form(a:p|dpTreeRelation) + a.spForm73(a:p|dpTreeRelation) + a.spPos(a:p|dpPath.dprel.seq) + p.form22(a:p|dpPath.dprel.seq) + p.spForm(a:p|dpPath.dprel.seq) + a.form89(a:p|dpPath.dprel.seq) + a.spFormp.spForm + p.lm.spPos + p.noFarChildren.spPos.bag + p.rm.spPosa2 .isCurPred.lemma + a1 .isCurPred.lemmaa2 .isCurPred.spLemma + a1 .isCurPred.spLemmaa.noFarChildren.spPos.bag + a.rm.spPosa.highSupportNoun:p|dpPath.dprel.seqa.lowSupportVerb:p|dpPath.dprel.seq(a.highSupportNoun:p|dpTreeRelation) + p.form(a.highSupportNoun:p|dpTreeRelation) + p.spForm(a.lowSupportNoun:p|dpTreeRelation) + p.spPos(a.lowSupportVerb:p|dpTreeRelation) + a.form(a.lowSupportVerb:p|dpTreeRelation) + a.spForm5777719068255883103108234695553511611856105107lTable 18: Feature templates SN+P besides Tables 10 12ReferencesCarreras, X., & Marquez, L. (2005). Introduction CoNLL-2005 shared task: Semantic role labeling. Proceedings Ninth Conference Computational NaturalLanguage Learning, pp. 152164, Ann Arbor, Michigan.Che, W., Li, Z., Hu, Y., Li, Y., Qin, B., Liu, T., & Li, S. (2008). cascaded syntacticsemantic dependency parsing system. Proceedings Twelfth ConferenceComputational Natural Language Learning, pp. 238242, Manchester.Chen, S. F., & Rosenfeld, R. (1999). Gaussian prior smoothing maximum entropymodels. Technical report CMU-CS-99-108, School Computer Science, CarnegieMellon University.Chen, W., Kawahara, D., Uchimoto, K., Zhang, Y., & Isahara, H. (2008). Dependencyparsing short dependency relations unlabeled data. ProceedingsThird International Joint Conference Natural Language Processing, Vol. 1, pp.8894, Hyderabad, India.Ciaramita, M., Attardi, G., DellOrletta, F., & Surdeanu, M. (2008). DeSRL: lineartime semantic role labeling system. Proceedings Twelfth ConferenceComputational Natural Language Learning, pp. 258262, Manchester.Crammer, K., & Singer, Y. (2003). Ultraconservative online algorithms multiclass problems. Journal Machine Learning Research, 3 (Jan), 951991.230fiSemantic Dependency ParsingDang, H. T., & Palmer, M. (2005). role semantic roles disambiguating verb senses. Proceedings 43rd Annual Meeting Association ComputationalLinguistics, pp. 4249, Ann Arbor, Michigan.Ding, W., & Chang, B. (2008). Improving Chinese semantic role classication hierarchical feature selection strategy. Proceedings 2008 Conference EmpiricalMethods Natural Language Processing, pp. 324333, Honolulu, Hawaii.Gildea, D., & Jurafsky, D. (2002). Automatic labeling semantic roles. ComputationalLinguistics, 28 (3), 245288.Hajic, J., Ciaramita, M., Johansson, R., Kawahara, D., Mart, M. A., Marquez, L., Meyers,A., Nivre, J., Pado, S., Stepanek, J., Stranak, P., Surdeanu, M., Xue, N., & Zhang,Y. (2009). CoNLL-2009 shared task: Syntactic semantic dependenciesmultiple languages. Proceedings Thirteenth Conference ComputationalNatural Language Learning: Shared Task, pp. 118, Boulder, Colorado.Henderson, J., Merlo, P., Musillo, G., & Titov, I. (2008). latent variable modelsynchronous parsing syntactic semantic dependencies. ProceedingsTwelfth Conference Computational Natural Language Learning, pp. 178182,Manchester.Jiang, Z. P., & Ng, H. T. (2006). Semantic role labeling NomBank: maximum entropyapproach. Proceedings 2006 Conference Empirical Methods NaturalLanguage Processing, pp. 138145, Sydney.Johansson, R., & Nugues, P. (2008). Dependency-based syntacticsemantic analysisPropBank NomBank. Proceedings Twelfth Conference ComputationalNatural Language Learning, pp. 183187, Manchester.Koo, T., Carreras, X., & Collins, M. (2008). Simple semi-supervised dependency parsing. Proceedings 46th Annual Meeting Association ComputationalLinguistics: Human Language Technologies, pp. 595603, Columbus, Ohio.Koomen, P., Punyakanok, V., Roth, D., & Yih, W.-T. (2005). Generalized inferencemultiple semantic role labeling systems. Proceedings Ninth ConferenceComputational Natural Language Learning, pp. 181184, Ann Arbor, Michigan.Liu, C., & Ng, H. T. (2007). Learning predictive structures semantic role labelingNomBank. Proceedings 45th Annual Meeting Association Computational Linguistics, pp. 208215, Prague.Marcus, M. P., Santorini, B., & Marcinkiewicz, M. A. (1993). Building large annotatedcorpus English: Penn Treebank. Computational Linguistics, Special IssueUsing Large Corpora: II, 19 (2), 313330.Marquez, L., Surdeanu, M., Comas, P., & Turmo, J. (2005). robust combination strategysemantic role labeling. Proceedings Human Language Technology ConferenceConference Empirical Methods Natural Language Processing, pp. 644651,Vancouver.McDonald, R., & Pereira, F. (2006). Online learning approximate dependency parsingalgorithms. Proceedings Eleventh Conference European ChapterAssociation Computational Linguistics, pp. 8188, Trento, Italy.231fiZhao, Zhang & KitMcDonald, R., Pereira, F., Ribarov, K., & Hajic, J. (2005). Non-projective dependencyparsing using spanning tree algorithms. Proceedings Human Language TechnologyConference Conference Empirical Methods Natural Language Processing, pp.523530, Vancouver, British Columbia.Meyers, A., Reeves, R., Macleod, C., Szekely, R., Zielinska, V., Young, B., & Grishman, R.(2004). NomBank project: interim report. Meyers, A. (Ed.), HLT-NAACL2004 Workshop: Frontiers Corpus Annotation, pp. 2431, Boston.Meza-Ruiz, I., & Riedel, S. (2009). Jointly identifying predicates, arguments sensesusing Markov logic. Proceedings Human Language Technologies: 2009 Annual Conference North American Chapter Association ComputationalLinguistics, pp. 155163, Boulder, Colorado.Nash, S. G., & Nocedal, J. (1991). numerical study limited memory BFGS methodtruncated-Newton method large scale optimization. SIAM Journal Optimization, 1 (2), 358372.Nivre, J., & McDonald, R. (2008). Integrating graph-based transition-based dependency parsers. Proceedings 46th Annual Meeting AssociationComputational Linguistics: Human Language Technologies, pp. 950958, Columbus,Ohio.Nocedal, J. (1980). Updating quasi-Newton matrices limited storage. MathematicsComputation, 35 (151), 773782.Palmer, M., Gildea, D., & Kingsbury, P. (2005). Proposition Bank: annotatedcorpus semantic roles. Computational Linguistics, 31 (1), 71106.Pradhan, S., Ward, W., Hacioglu, K., Martin, J., & Jurafsky, D. (2005). Semantic rolelabeling using dierent syntactic views. Proceedings 43rd Annual MeetingAssociation Computational Linguistics, pp. 581588, Ann Arbor, Michigan.Pradhan, S. S., Ward, W. H., Hacioglu, K., Martin, J. H., & Jurafsky, D. (2004). Shallowsemantic parsing using support vector machines. Proceedings Human Language Technology Conference North American Chapter AssociationComputational Linguistics, pp. 233240, Boston.Punyakanok, V., Roth, D., Yih, W., & Zimak, D. (2004). Semantic role labeling via integerlinear programming inference. Proceedings 20th International ConferenceComputational Linguistics, pp. 13461352, Geneva.Pustejovsky, J., Meyers, A., Palmer, M., & Poesio, M. (2005). Merging PropBank, NomBank, TimeBank, Penn Discourse Treebank coreference. ProceedingsWorkshop Frontiers Corpus Annotations II: Pie Sky, pp. 512, AnnArbor, Michigan.Riedel, S. (2008). Improving accuracy eciency map inference markov logic.Proceedings Twenty-Fourth Conference Annual Conference UncertaintyArtificial Intelligence, pp. 468475, Corvallis, Oregon.Roth, D., & Yih, W. (2004). linear programming formulation global inferencenatural language tasks. Proceedings Eighth Conference ComputationalNatural Language Learning, pp. 18, Boston.232fiSemantic Dependency ParsingSurdeanu, M., Johansson, R., Meyers, A., Marquez, L., & Nivre, J. (2008). CoNLL 2008shared task joint parsing syntactic semantic dependencies. ProceedingsTwelfth Conference Computational Natural Language Learning, pp. 159177,Manchester.Surdeanu, M., Marquez, L., Carreras, X., & Comas, P. R. (2007). Combination strategiessemantic role labeling. Journal Artificial Intelligence Research, 29, 105151.Titov, I., Henderson, J., Merlo, P., & Musillo, G. (2009). Online graph planarisationsynchronous parsing semantic syntactic dependencies. Proceedings21st International Jont Conference Artifical Intelligence, pp. 15621567, Pasadena,California.Toutanova, K., Haghighi, A., & Manning, C. D. (2005). Joint learning improves semanticrole labeling. Proceedings 43rd Annual Meeting Association Computational Linguistics, pp. 589596, Ann Arbor, Michigan.Xue, N. (2006). Semantic role labeling nominalized predicates Chinese. ProceedingsHuman Language Technology Conference North American ChapterAssociation Computational Linguistics, Main Conference, pp. 431438, New York.Xue, N., & Palmer, M. (2004). Calibrating features semantic role labeling. Proceedings2004 Conference Empirical Methods Natural Language Processing, pp. 8894, Barcelona.Zhao, H., Chen, W., Kazama, J., Uchimoto, K., & Torisawa, K. (2009). Multilingual dependency learning: Exploiting rich features tagging syntactic semantic dependencies. Proceedings Thirteenth Conference Computational Natural LanguageLearning: Shared Task, pp. 6166, Boulder, Colorado.Zhao, H., Chen, W., Kit, C., & Zhou, G. (2009). Multilingual dependency learning: hugefeature engineering method semantic dependency parsing. ProceedingsThirteenth Conference Computational Natural Language Learning: Shared Task,pp. 5560, Boulder, Colorado.Zhao, H., & Kit, C. (2008). Parsing syntactic semantic dependencies two singlestage maximum entropy models. Proceedings Twelfth Conference Computational Natural Language Learning, pp. 203207, Manchester.233fiJournal Artificial Intelligence Research 46 (2013) 449509Submitted 9/12; published 03/13Incremental Clustering Expansion FasterOptimal Planning Decentralized POMDPsFrans A. Oliehoekfrans.oliehoek@maastrichtuniversity.nlMaastricht UniversityMaastricht, NetherlandsMatthijs T.J. Spaanm.t.j.spaan@tudelft.nlDelft University TechnologyDelft, NetherlandsChristopher Amatocamato@csail.mit.eduMassachusetts Institute TechnologyCambridge, MA, USAShimon Whitesons.a.whiteson@uva.nlUniversity AmsterdamAmsterdam, NetherlandsAbstractarticle presents state-of-the-art optimal solution methods decentralizedpartially observable Markov decision processes (Dec-POMDPs), general modelscollaborative multiagent planning uncertainty. Building generalized multiagent A* ( GMAA*) algorithm, reduces problem tree one-shot collaborativeBayesian games (CBGs), describe several advances greatly expand range DecPOMDPs solved optimally. First, introduce lossless incremental clusteringCBGs solved GMAA*, achieves exponential speedups without sacrificingoptimality. Second, introduce incremental expansion nodes GMAA* searchtree, avoids need expand children, number worst casedoubly exponential nodes depth. particularly beneficial little clusteringpossible. addition, introduce new hybrid heuristic representationscompact thereby enable solution larger Dec-POMDPs. provide theoreticalguarantees that, suitable heuristic used, incremental clustering incremental expansion yield algorithms complete search equivalent. Finally,present extensive empirical results demonstrating GMAA*-ICE, algorithmsynthesizes advances, optimally solve Dec-POMDPs unprecedented size.1. Introductionkey goal artificial intelligence development intelligent agents interactenvironment order solve problems, achieve goals, maximize utility.agents sometimes act alone, researchers increasingly interested collaborative multiagentsystems, teams agents work together perform manner tasks. Multiagentsystems appealing, tackle inherently distributed problems,facilitate decomposition problems complex tackled singlec2013AI Access Foundation. rights reserved.fiOliehoek, Spaan, Amato, & Whitesonagent (Huhns, 1987; Sycara, 1998; Panait & Luke, 2005; Vlassis, 2007; Busoniu, Babuska, &De Schutter, 2008).One primary challenges multiagent systems presence uncertainty. Evensingle-agent systems, outcome action may uncertain, e.g., action may failprobability. Furthermore, many problems state environment mayuncertain due limited noisy sensors. However, multiagent settings problemsoften greatly exacerbated. Since agents access sensors, typicallysmall fraction complete system, ability predict agentsact limited, complicating cooperation. uncertainties properly addressed,arbitrarily bad performance may result.principle, agents use communication synchronize beliefs coordinateactions. However, due bandwidth constraints, typically infeasible agentsbroadcast necessary information agents. addition, many realisticscenarios, communication may unreliable, precluding possibility eliminating uncertainty agents actions.Especially recent years, much research focused approaches (collaborative)multiagent systems deal uncertainty principled way, yielding wide varietymodels solution methods (Pynadath & Tambe, 2002; Goldman & Zilberstein, 2004;Seuken & Zilberstein, 2008). article focuses decentralized partially observableMarkov decision process (Dec-POMDP), general model collaborative multiagent planning uncertainty. Unfortunately, solving Dec-POMDP, i.e., computing optimalplan, generally intractable (NEXP-complete) (Bernstein, Givan, Immerman, & Zilberstein,2002) fact even computing solutions absolutely bounded error (i.e., -approximatesolutions) also NEXP-complete (Rabinovich, Goldman, & Rosenschein, 2003). particular,number joint policies grows exponentially number agents observationsdoubly exponentially respect horizon problem.1 Though complexity results preclude methods efficient problems, developing better optimalsolution methods Dec-POMDPs nonetheless important goal, several reasons.First, since complexity results describe worst case, still great potentialimprove performance optimal methods practice. fact, evidencemany problems solved much faster worst-case complexity bound indicates(Allen & Zilberstein, 2007). article, present experiments clearly demonstratepoint: many problems, methods propose scale vastly beyond wouldexpected doubly-exponential dependence horizon.Second, computer speed memory capacity increase, growing set smallmedium-sized problems solved optimally. problems arise naturallyothers result decomposition larger problems. instance, may possibleextrapolate optimal solutions problems shorter planning horizons, usingstarting point policy search longer-horizon problems work EkerAkn (2013), use shorter-horizon, no-communication solutions inside problemscommunication (Nair, Roth, & Yohoo, 2004; Goldman & Zilberstein, 2008). generally,optimal policies smaller problems potentially used find good solutions largerproblems. instance, transfer planning (Oliehoek, 2010; Oliehoek, Whiteson, & Spaan,1. Surprisingly, number states Dec-POMDP less important, e.g., brute-force search dependsnumber states via policy evaluation routine, scales linearly number states.450fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPs2013) employs optimal solutions problems agents better solve problemsmany agents. performing (approximate) influence-based abstraction influence search(Witwicki, 2011; Oliehoek, Witwicki, & Kaelbling, 2012), optimal solutions componentproblems potentially used find (near-)optimal solutions larger problems.Third, optimal methods offer important insights nature specific Dec-POMDPproblems solutions. instance, methods introduced article enableddiscovery certain properties BroadcastChannel benchmark problem makemuch easier solve.Fourth, optimal methods provide critical inspiration principled approximation methods. fact, almost successful approximate Dec-POMDP methods based optimalones (see, e.g., Seuken & Zilberstein, 2007b, 2007a; Dibangoye, Mouaddib, & Chai-draa, 2009;Amato, Dibangoye, & Zilberstein, 2009; Wu, Zilberstein, & Chen, 2010a; Oliehoek, 2010)locally optimal ones (Velagapudi, Varakantham, Scerri, & Sycara, 2011)2 , clustering technique presented article forms basis recently introduced approximateclustering technique (Wu, Zilberstein, & Chen, 2011).Finally, optimal methods essential benchmarking approximate methods. recentyears, huge advances approximate solution Dec-POMDPs, leadingdevelopment solution methods deal large horizons, hundreds agentsmany states (e.g., Seuken & Zilberstein, 2007b; Amato et al., 2009; Wu et al., 2010a;Oliehoek, 2010; Velagapudi et al., 2011).However, since computing even -approximatesolutions NEXP-complete, method whose complexity doubly exponential cannotguarantees absolute error solution (assuming EXP6=NEXP). such,existing effective approximate methods quality guarantees.3Consequently, difficult meaningfully interpret empirical performance withoutupper bounds optimal methods supply. approximate methods also benchmarked lower bounds (e.g., approximate methods), comparisons cannotdetect method fails find good solutions. requires benchmarkingupper bounds and, unfortunately, upper bounds easier compute, QMDPQPOMDP, loose helpful (Oliehoek, Spaan, & Vlassis, 2008). such,benchmarking respect optimal solutions important part verificationapproximate algorithm. Since existing optimal methods tackle small problems,scaling optimal solutions larger problems critical goal.1.1 Contributionsarticle presents state-of-the-art optimal solution methods Dec-POMDPs.particular, describes several advances greatly expand horizon many DecPOMDPs solved optimally. addition, proposes evaluates complete algorithmsynthesizes advances. approach based generalized multiagent A*(GMAA*) algorithm (Oliehoek, Spaan, & Vlassis, 2008), makes possible reduceproblem tree one-shot collaborative Bayesian games (CBGs). appeal2. method Velagapudi et al. (2011) repeatedly computes best responses way similar DP-JESP(Nair, Tambe, Yokoo, Pynadath, & Marsella, 2003). best response computation, however, exploitssparsity interactions.3. Note refer methods without quality guarantees approximate rather heuristic avoidconfusion heuristic search, used throughout article exact.451fiOliehoek, Spaan, Amato, & Whitesonapproach abstraction layer introduces, led various insights DecPOMDPs and, turn, improved solution methods describe.specific contributions article are:41. introduce lossless clustering CBGs, technique reduce size CBGsGMAA* enumerates possible solutions, preserving optimality.exponentially reduce number child nodes GMAA* search tree, leadinghuge increases efficiency. addition, applying incremental clustering (IC)GMAA*, GMAA*-IC method avoid clustering exponentially sized CBGs.2. introduce incremental expansion (IE) nodes GMAA* search tree. Althoughclustering may reduce number children search node, numberworst case still doubly exponential nodes depth. GMAA*-ICE, appliesIE GMAA*-IC, addresses problem creating next child nodecandidate expansion.3. provide theoretical guarantees GMAA*-IC GMAA*-ICE. particular, show that, using suitable heuristic, algorithms completesearch equivalent.4. introduce improved heuristic representation. Tight heuristics like basedunderlying POMDP solution (QPOMDP ) value function resultingassuming 1-step-delayed communication (QBG ) essential heuristic search methods like GMAA* (Oliehoek, Spaan, & Vlassis, 2008). However, space neededstore heuristics grows exponentially problem horizon. introduce hybrid representations compact thereby enable solution largerproblems.5. present extensive empirical results show substantial improvementscurrent state-of-the-art. Whereas Seuken Zilberstein (2008) argued GMAA*best optimally solve Dec-POMDPs one horizon brute-forcesearch, results demonstrate GMAA*-ICE much better. addition,provide comparative overview results competitive optimal solution methodsliterature.primary aim techniques introduced article improve scalabilityrespect horizon. empirical results confirm techniques highlysuccessful regard. added bonus, experiments also demonstrate improvementscalability respect number agents. particular, present first optimalresults general (non-special case) Dec-POMDPs three agents. Extensionstechniques achieve improvements respect number agents,well promising ways combine ideas behind methods state-of-the-artapproximate approaches, discussed future work Section 7.4. article synthesizes extends research already reported two conference papers (Oliehoek,Whiteson, & Spaan, 2009; Spaan, Oliehoek, & Amato, 2011).452fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPs1.2 Organizationarticle organized follows. Section 2 provides background Dec-POMDP model,GMAA* heuristic search solution method, well suitable heuristics. Section 3,introduce lossless clustering CBGs integration GMAA*. Section 4 introduces incremental expansion search nodes. empirical evaluation proposedtechniques reported Section 5. give treatment related work Section 6. Futurework discussed Section 7 conclusions drawn Section 8.2. BackgroundDec-POMDP, multiple agents must collaborate maximize sum commonrewards receive multiple timesteps. actions affect immediaterewards also state transition. current state knownagents, timestep agent receives private observation correlatedstate.ffDefinition 1. Dec-POMDP tuple D, S, A, T, O, O, R, b0 , h ,= {1, . . . ,n} finite set agents.= s1 , . . . ,s|S| finite set states.= Ai set joint actions = ha1 , . . . , i, Ai finite set actionsavailable agent i.transition function specifying state transition probabilities Pr(s |s,a).= Oi finite set joint observations. every stage one joint observation= ho1 ,...,on received. agent observes component oi .observation function, specifies observation probabilities Pr(o|a,s ).R(s,a) immediate reward function mapping (s,a)-pairs real numbers.b0 (S) initial state distribution time = 0, (S) denotes infiniteset probability distributions finite set S.h horizon, i.e., number stages. consider case h finite.stage = 0 . . . h 1, agent takes individual action receives individualobservation.Example 1 (Recycling Robots). illustrate Dec-POMDP model, consider team robots taskedremoving trash office building, depicted Fig. 1. robots sensors find markedtrash cans, motors move around order look cans, well gripper arms grasp carrycan. Small trash cans light compact enough single robot carry, large trash cansrequire multiple robots carry together. people use them, larger trashcans fill quickly. robot must also ensure battery remains charged movingcharging station expires. battery level robot degrades due distancerobot travels weight item carried. robot knows battery levelrobots location robots within sensor range. goalproblem remove much trash possible given time period.problem represented Dec-POMDP natural way. states, S, consistdifferent locations robot, battery levels different amounts trashcans. actions, Ai , robot consist movements different directions well decisions453fiOliehoek, Spaan, Amato, & WhitesonFigure 1: Illustration Recycling Robots example, two robots removetrash office environment three small (blue) trash cans two large (yellow) ones.situation, left robot might observe large trash next full,robot small trash empty. However, none sure trashcans state due limited sensing capabilities, see state trash cansaway. particular, one robot knowledge regarding observations robot.pick trash recharge battery (when range charging station).observations, Oi , robot consist battery level, location, locationsrobots sensor range amount trash cans within range. rewards, R, could consistlarge positive value pair robots emptying large (full) trash can, small positive valuesingle robot emptying small trash negative values robot depleting batterytrash overflowing. optimal solution joint policy leads expected behavior (givenrewards properly specified). is, ensures robots cooperate emptylarge trash cans appropriate small ones individually considering battery usage.explanatory purposes, also consider much simpler problem, so-called decentralized tiger problem (Nair et al., 2003).Example 2 (Dec-Tiger). Dec-Tiger problem concerns two agents find hallway two doors. Behind one door, treasure behind tiger. statedescribes door tiger behindleft (sl ) right (sr )each occurring 0.5 probability(i.e., initial state distribution b0 uniform). agent perform three actions: open leftdoor (aOL ), open right door (aOR ) listen (aLi ). Clearly, opening door treasureyield reward, opening door tiger result severe penalty. greater rewardgiven agents opening correct door time. such, good strategyprobably involve listening first. listen actions, however, also minor cost (negative reward).every stage agents get observation. agents either hear tiger behind left(oHL ) right (oHR ) door, agent 15% chance hearing incorrectly (getting wrongobservation). Moreover, observation informative agents listen; either agent opensdoor, agents receive uninformative (uniformly drawn) observation problem resetssl sr equal probability. point problem continues, agents mayable open door treasure multiple times. Also note that, since two observationsagents get oHL , oHR , agents way detecting problem reset:one agent opens door listens, agent able telldoor opened. complete specification, see discussion Nair et al. (2003).Given Dec-POMDP, agents common goal maximize expected cumulativereward return. planning task entails finding joint policy = h1 , . . . ,nspace joint policies , specifies individual policy agent i.454fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPsindividual policy general specifies individual action action-observation history(AOH) ~it = (a0i ,o1i , . . . ,ait1 ,oti ), e.g., (~it ) = ati . However, possible restrictattention deterministic pure policies, case maps observation history~ action, e.g., (~o ) = . number policies(OH) (o1i , . . . ,oti ) = ~oith 1)/(|O |1)(|O||Ai |number joint policies thereforen|O |h 1|A | |O |1 ,(2.1)denote largest individual action observation sets. qualityparticular joint policy expressed expected cumulative reward induces, also referredvalue.Definition 2. value V () joint policyV () , Eh1hXt=0fifiR(st ,at )fi,b0 ,(2.2)expectation sequences states, actions observations.planning problem Dec-POMDP find optimal joint policy , i.e., jointpolicy maximizes value: = arg max V ().individual policy depends local information ~oi availableagent, on-line execution phase truly decentralized: communication takes placemodeled via actions observations. planning however, may take placeoff-line phase centralized. scenario consider article.detailed introduction Dec-POMDPs see, e.g., work Seuken Zilberstein(2008) Oliehoek (2012).2.1 Heuristic Search Methodsrecent years, numerous Dec-POMDP solution methods proposed.methods fall one two categories: dynamic programming heuristic search methods.Dynamic programming methods take backwards bottom-up perspective first considering policies last time step = h 1 using construct policies stage= h 2, etc. contrast, heuristic search methods take forward top-down perspectivefirst constructing plans = 0 extending later stages.article, focus heuristic search approach shown state-of-the-artresults. make clear section, method interpreted searchingtree collaborative Bayesian games (CBGs). CBGs provide convenient abstractionlayer facilitates explanation techniques introduced article.section provides concise background heuristic search methods.detailed description, see work Oliehoek, Spaan, Vlassis (2008). description dynamic programming methods relationship heuristic search methods,see work Oliehoek (2012).2.1.1 Multiagent A*Szer, Charpillet, Zilberstein (2005) introduced heuristically guided policy search methodcalled multiagent A* (MAA*). performs A* search partially specified joint policies,455fiOliehoek, Spaan, Amato, & Whitesont=0t=1t=2i0aLi=22ioHRoHLaOLi1aOLoHLoHRoHLoHRaLiaLiaOLaLii2=1Figure 2: arbitrary policy Dec-Tiger problem. figure illustrates differenttypes partial policies used paper. shown past policy 2i consists two decisionrules i0 , i1 . Also shown two sub-tree policies =1 , =2 (introduced Section 3.1.2).pruning joint policies guaranteed worse best (fully specified) joint policyfound far. Oliehoek, Spaan, Vlassis (2008) generalized algorithm making explicitexpand selection operators performed heuristic search. resulting algorithm,generalized MAA* (GMAA*) offers unified perspective MAA* forward sweeppolicy computation method (Emery-Montemerlo, 2005), differ implementGMAA*s expand operator: forward sweep policy computation solves (i.e., finds bestpolicy for) collaborative Bayesian games, MAA* finds policies collaborativeBayesian games, describe Section 2.1.2.GMAA* algorithm considers joint policies partially specified respecttime. partially specified policies formalized follows.Definition 3. decision rule agent decision stage mapping action~ Ai .observation histories stage actions :article, consider deterministic policies. Since policies need conditionactions observation histories, made decision rules map length~ Ai . joint decision rule = h , . . . ,nt specifiesobservation histories actions: :1decision rule agent. Fig. 2 illustrates concept, well past policy,introduce shortly. discussed below, decision rules allow partial policiesdefined play crucial role GMAA* algorithms developed article.Definition 4. partial past policy stage t, ti , specifies part agent policyrelates stages < t. is, specifies decision rules first stages:ti = (i0 ,i1 , . . . ,it1 ). past policy stage h regular, fully specified, policyhi = . past joint policy = ( 0 , 1 , . . . , t1 ) specifies joint decision rules firststages.GMAA* performs heuristic search partial joint policies constructingsearch tree illustrated Fig. 3a. node q = ht , vi search tree specifiespast joint policy heuristic value v. heuristic value v node representsoptimistic estimate past joint policy Vb (t ), computed viaVb (t ) = V 0...t1 (t ) + H t...h1 (t ),456(2.3)fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPsB(0 )00000001112...B(1 )111...B(1 )B(2 )2(a) MAA* perspective....B(1 )1...B(2 )(b) CBG perspective.Figure 3: Generalized MAA*. Associated every node heuristic value. searchtrees two perspectives shown equivalent certain assumptions heuristic,explained Section 2.2.H t...h1 heuristic value remaining h stages V 0...t1 (t ) actualexpected reward achieves first stages (for definition, see Appendix A.3).Clearly, H t...h1 admissible heuristica guaranteed overestimationso Vb (t ).5Algorithm 1 illustrates GMAA*. starts creating node q 0 completely unspecified joint policy 0 placing open list L. Then, selects nodes (Algorithm 2)expands (Algorithm 3), repeating process certain foundoptimal joint policy.Select operator returns highest ranked node, defined following comparison operator.Definition 5. node comparison operator < defined two nodes q = ht ,vi, q =ht ,v follows:, v 6= vv < vq < q = depth(q) < depth(q ) , otherwise depth(q) 6= depth(q )(2.4)<, otherwise.is, comparison operator first compares heuristic values. equal,compares depth nodes. Finally, nodes equal value equal depth,lexically compares past joint policies. ranking leads A* behavior (i.e., selectingnode open list highest heuristic value) GMAA*, well guaranteeingselection order incremental expansion technique (introduced Section 4).Ranking nodes greater depth higher case equal heuristic value helps find tightlower bounds early first expanding deeper nodes (Szer et al., 2005) also usefulincremental expansion.5. formally, H underestimate value. Note that, unlike classical A* applicationspath planningin admissible heuristic overestimatein setting maximize reward,rather minimize cost.457fiOliehoek, Spaan, Amato, & WhitesonAlgorithm 1 Generalized multiagent A*.Input: Dec-POMDP, admissible heuristic H, empty open list LOutput: optimal joint policy1: vGM AA2: q 0 h0 = (), v = +i3: L.insert(q 0 )4: repeat5:q Select(L)6:QExpand Expand(q, H)7:depth(q) = h 18:{ QExpand contains fully specified joint policies, interested best one }9:h, vi BestJointPolicyAndValue(QExpand )10:v > vGM AA11:{found new best joint policy}12:vGM AA v13:L.Prune(vGM AA ){(optionally) prune open list}14:end15:else{add expanded children open list}16:L.insert( q QExpand | q .v > vGM AA )17:end18:PostProcessNode(q, L)19: L empty20: returnAlgorithm 2 Select(L): Return highest ranked node open list.Input: open list L, total order nodes <Output: highest ranked node q1: q q L s.t. q L (q 6= q = q < q)2: return qExpand operator constructs QExpand , set child nodes. is, given nodecontains partial joint policy = ( 0 , 1 , . . . , t1 ), constructs t+1 , sett+1 = ( 0 , 1 , . . . , t1 , ), appending possible joint decision rules next timestep t. t+1 , heuristic value computed node constructed.expansion, algorithm checks (line 7) expansion resulted fully specifiedjoint policies. not, children sufficient heuristic value placed open listAlgorithm 3 Expand(q, H). expand operator plain MAA*.Input: q = ht , vi search node expand, H admissible heuristic.Output: QExpand set containing expanded child nodes.1: QExpand {}2: t+1 {t+1 | t+1 = (t , )}3: t+1 t+14:Vb (t+1 ) V 0...t (t+1 ) + H(t+1 )5:q ht+1 , Vb (t+1 )i6:QExpand .Insert(q )7: end8: return QExpand458{create child node}fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPsAlgorithm 4 PostProcessNode(q,L)Input: q expanded parent node, L open list.Output: expanded node removed.1: L.Pop(q)(line 16). children fully specified, BestJointPolicyAndValue returns bestjoint policy (and value) QExpand (see Algorithm 12 Appendix A.1 detailsBestJointPolicyAndValue). GMAA* also maintains lower bound vGM AA corresponds actual value best fully-specified joint policy found far. newlyfound joint policy higher value lower bound updated (lines 11 12). Also,nodes partial joint policies t+1 upper bound lower best solutionfar, Vb (t+1 ) < vGM AA , pruned (line 13). pruning takes additional time,save memory. Finally, PostProcessNode simply removes parent node openlist (this procedure augmented incremental expansion Section 4). search endslist becomes empty, point optimal joint policy found.GMAA* complete, i.e., search finds solution. Therefore, theory,GMAA* guaranteed eventually produce optimal joint policy (Szer et al., 2005).However, practice, often infeasible larger problems. major source complexityfull expansion search node. number joint decision rules stageform children node depth search tree6|A |n(|O | ) ,(2.5)doubly exponential t. Comparing (2.1) (2.5), see worst casecomplexity expanding node deepest level tree = h 1 comparablebrute force search entire Dec-POMDP. Consequently, Seuken Zilberstein(2008) conclude MAA* best solve problems whose horizon 1 greateralready solved nave brute force search.2.1.2 Bayesian Game PerspectiveGMAA* makes possible interpret MAA* solution collection collaborativeBayesian games (CBGs). employ approach throughout article, facilitatesimprovements GMAA* introduce, results significant advancesstate-of-the-art Dec-POMDP solutions.Bayesian game (BG) models one-shot interaction number agents.extension well-known strategic game (also known normal form game)agent holds private information (Osborne & Rubinstein, 1994). CBG BGagents receive identical payoffs. Bayesian game perspective, node qGMAA* search tree, along corresponding partial joint policy , definesCBG (Oliehoek, Spaan, & Vlassis, 2008). is, given state distribution b0 , ,possible construct CBG B(b0 ,t ) represents decision-making problemstage given followed first stages starting b0 . clearb0 is, simply write B(t ).6. follow convention root depth 0.459fiOliehoek, Spaan, Amato, & WhitesonDefinition 6. collaborative Bayesian game (CBG) B(b0 ,t ) = hD, A, , Pr(), ui modelingstage Dec-POMDP, given initial state distribution b0 past joint policy , consistsof:D, set agents {1 . . . n},A, set joint actions,, set joint types, specifies type agent =h1 , . . . ,n i,Pr(), probability distribution joint types,u, (heuristic) payoff function mapping joint type action real number: u(,a).Bayesian game, type agent represents private information holds.instance, Bayesian game modeling job recruitment scenario, type agentmay indicate whether agent hard worker. CBG Dec-POMDP, agentsprivate information individual AOH. Therefore, type agent corresponds~it , history actions observations: ~it . Similarly, joint type correspondsjoint AOH: ~ .Consequently, u provide (heuristic) estimate long-term payoff(~ ,a)-pair. words, payoff function corresponds heuristic Q-value: u(,a)b ~ ,a). discuss compute heuristics Section 2.2. Given , b0 ,Q(correspondence joint types AOHs, probability distribution joint types is:Pr() , Pr(~ |b0 ,t ),(2.6)latter probability marginal Pr(s,~ |b0 ,t ) defined (A.2) usedcomputation value partial joint policy V 0...t1 (t ) Appendix A.3. Note duecorrespondence types AOHs, size CBG B(b0 ,t ) stageexponential t.CBG, agent uses Bayesian game policy maps individual types actions:(i ) = ai . correspondence types AOHs, (joint) policyCBG corresponds (joint) decision rule: . remainder article,assume deterministic past joint policies , implies one ~ non-zeroprobability given observation history ~o . Thus, effectively maps observation historiesactions. number B(b0 ,t ) given (2.5). value joint CBGpolicy CBG B(b0 ,t ) is:Xb ~ ,(~ )),Pr(~ |b0 ,t )Q((2.7)Vb () =~(~ ) = hi (~it )ii=1...n denotes joint action results applicationindividual CBG-policies individual AOH ~it specified ~ .Example 3. Consider CBG Dec-Tiger given past joint policy 2 specifies listen first two stages. stage = 2, agent four possible observation histories:~ 2 = {(oHL ,oHL ), (oHL ,oHR ), (oHR ,oHL ), (oHR ,oHR )} correspond directly possible types.probabilities joint types given 2 listed Fig. 4a. Since joint OHs together2 determine joint AOHs, also correspond so-called joint beliefs: probability distributionsstates (introduced formally Section 2.2). Fig. 4b shows joint beliefs, servebasis heuristic payoff function (as discussed Section 2.2).460fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPs~o12(oHL ,oHL )(oHL ,oHR )(oHR ,oHL )(oHR ,oHR )~o22(oHL ,oHL )0.2610.0470.0470.016(oHL ,oHR )0.0470.0160.0160.047(oHR ,oHL )0.0470.0160.0160.047(oHR ,oHR )0.0160.0470.0470.261(a) joint type probabilities.~o12(oHL ,oHL )(oHL ,oHR )(oHR ,oHL )(oHR ,oHR )~o22(oHL ,oHL )0.9990.9700.9700.5(oHL ,oHR )0.9700.50.50.030(oHR ,oHL )0.9700.50.50.030(oHR ,oHR )0.50.0300.0300.001(b) induced joint beliefs. Listed probability Pr(sl |~ 2 ,b0 )tiger behind left door.Figure 4: Illustration Dec-Tiger problem past joint policy 2 specifieslisten actions first two stages.Algorithm 5 Expand-CBG(q, H). expand operator GMAA* makes use CBGs.Input: q = ht , vi search node expand.b ~,a).Input: H admissible heuristic form Q(Output: QExpand set containing expanded child nodes.b1: B(b0 ,t ) ConstructBG(b0 ,t , Q)2: QExpand GenerateAllChildrenForCBG(B(b0 ,t ))3: return QExpand{as explained Section 2.1.2}solution CBG maximizes (2.7). CBG equivalent teamdecision process finding solution NP-complete (Tsitsiklis & Athans, 1985). However,Bayesian game perspective GMAA*, illustrated Fig. 3b, issue solvingCBG (i.e., finding highest payoff ) relevant need expand .is, Expand operator enumerates appends form setextended joint policiest+1 = (t , ) | joint CBG policy B(b0 ,t )uses set construct QExpand , set child nodes. heuristic valuechild node q QExpand specifies t+1 = (t , ) givenVb (t+1 ) = V 0...t1 (t ) + Vb ().(2.8)Expand operator makes use CBGs summarized Algorithm 5, usesGenerateAllChildrenForCBG subroutine (Algorithm 13 Appendix A.1). Fig. 3b illustratesBayesian game perspective GMAA*.461fiOliehoek, Spaan, Amato, & Whiteson2.2 Heuristicsperform heuristic search, GMAA* defines heuristic value Vb (t ) using (2.3). contrast, Bayesian game perspective uses (2.8). two formulations equivalentb faithfully represents expected immediate reward (Oliehoek, Spaan, & Vlasthe heuristic Qsis, 2008). consequence GMAA* via CBGs complete (and thus finds optimalsolutions) stated following theorem.Theorem 1. using heuristic formb ~ ,a) = Est [R(st ,a) | ~ ] + E~ t+1 [Vb (~ t+1 ) | ~ , a],Q((2.9)Vb (~ t+1 ) Q (~ t+1 , (~ t+1 )) overestimation value optimal jointpolicy , GMAA* via CBGs complete.Proof. See appendix.theorem, Q (~ ,a) Q-value, i.e., expected future cumulative rewardperforming ~ joint policy (Oliehoek, Spaan,P& Vlassis, 2008). expectation~immediate reward also written R( ,a) = sS R(s,a) Pr(s|~ ,b0 ).computed using Pr(s|~ ,b0 ), quantity refer joint belief resulting ~also denote b. joint belief computed via repeated applicationBayes rule (Kaelbling, Littman, & Cassandra, 1998), conditional (A.2).rest subsection reviews several heuristics used GMAA*.2.2.1 QMDPb ~,a) solve underlying MDP, i.e.,One way obtain admissible heuristic Q(assume joint action chosen single puppeteer agent observe truestate. approach, known QMDP (Littman, Cassandra, & Kaelbling, 1995), usesMDP value function Qt,(s ,a), computed using standard dynamic programmingb ~ttechniques (Puterman, 1994). order transform Qt,(s ,a)-values QM ( ,a)-values,compute:X t,b (~ ,a) =(2.10)QQ (s,a) Pr(s|~ ,b0 ).sSSolving underlying MDP time complexity linear h, makes it,especially compared Dec-POMDP, easy compute. addition, necessarystore value (s,a)-pair, stage t. However, bound providesoptimal Dec-POMDP Q -value function loose (Oliehoek & Vlassis, 2007).2.2.2 QPOMDPSimilar underlying MDP, one define underlying POMDP Dec-POMDP,i.e., assuming joint action chosen single agent access joint observation.77. Alternatively one view POMDP multiagent POMDP agents instantaneouslybroadcast private observations.462fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPsTreeVectort=0t=1t=2t=3Figure 5: Visual comparison tree vector-based Q representations.resulting solution used heuristic, called QPOMDP (Szer et al., 2005; Roth,Simmons, & Veloso, 2005). optimal QPOMDP value function satisfies:QP (bt , a) = R(bt ,a) +XP (ot+1 |bt ,a) max QP (bt+1 ,at+1 ),at+1ot+1(2.11)Pbt joint belief, R(bt ,a) = sS R(s,a)bt (s) immediate reward, bt+1joint belief resulting bt action joint observation ot+1 . use QPOMDP ,b P (~ ,a) , Qt (b~t ,a).~ , directly use value induced joint belief: QPtwo approaches computing QPOMDP . One construct belief MDPtree joint beliefs, illustrated Fig. 5 (left). Starting b0 (correspondingempty joint AOH ~ 0 ), compute resulting ~ 1 corresponding~1belief b continue recursively. Given tree, possible compute valuesnodes standard dynamic programming.Another possibility apply vector-based POMDP techniques (see Fig. 5 (right)).Q-value function stage QtP (b,a) represented using set vectors joint} (Kaelbling et al., 1998). Qt (b,a) defined maximumaction V = {V1t , . . . ,V|A|Pinner product:QtP (b,a) , max b vat .VvaGiven V h1 , vector representation last stage, compute V h2 , etc. orderlimit growth number vectors, dominated vectors pruned.Since QMDP upper bound POMDP value function (Hauskrecht, 2000), QPOMDPprovides tighter upper bound Q QMDP . However, also costly computestore: tree-based vector-based approach may need store numbervalues exponential h.463fiOliehoek, Spaan, Amato, & Whiteson2.2.3 QBGthird heuristic, called QBG , assumes agent team accessindividual observation communicate 1-step delay.8 define QBGXQB (~ ,a) = R(~ ,a) + maxPr(ot+1 |~ ,a)QB (~ t+1 ,(ot+1 )),(2.12)ot+1t+1= h1 (ot+11 ),...,n (on )i tuple individual policies : Oi Ai CBGconstructed ~ ,a. Like QPOMDP , QBG also represented using vectors (Varaiya &Walrand, 1978; Hsu & Marcus, 1982; Oliehoek, Spaan, & Vlassis, 2008) twomanners computation (tree vector based) apply. yields tighter heuristicQPOMDP , computation additional exponential dependence maximumnumber individual observations (Oliehoek, Spaan, & Vlassis, 2008), particularlytroubling vector-based computation, since precludes effective application incremental pruning (A. Cassandra, Littman, & Zhang, 1997). overcome problem, OliehoekSpaan (2012) introduce novel tree-based pruning methods.3. ClusteringGMAA* solves Dec-POMDPs repeatedly constructing CBGs expanding jointBG policies them. However, number equal number regularMAA* child nodes given (2.5) thus grows doubly exponentially horizon h.section, propose new approach improving scalability respect hclustering individual AOHs. reduces number therefore numberconstructed child nodes GMAA* search tree.9Previous research also investigated clustering: Emery-Montemerlo, Gordon,Schneider, Thrun (2005) propose clustering types based profiles payofffunctions CBGs. However, resulting method ad hoc. Even given boundserror clustering two types CBG, guarantees made qualityDec-POMDP solution, bound respect heuristic payoff function.contrast, propose cluster histories based probability histories inducehistories agents states. critical advantage criterion,call probabilistic equivalence (PE), resulting clustering lossless:solution clustered CBG used construct solution original CBGvalues two CBGs identical. Thus, criterion allows clustering AOHsCBGs represent Dec-POMDPs preserving optimality.10Section 3.1, describe histories Dec-POMDPs clustered usingnotions probabilistic best-response equivalence. allows histories clustered8. name QBG stems fact 1-step delayed communication scenario modeledCBG. Note, however, CBGs used compute QBG different form B(b0 ,t )discussed Section 2.1.2: latter, types correspond length-t (action-) observation histories;former, types correspond length-1 observation histories.9. CBGs essential clustering, provide convenient level abstraction simplifiesexposition techniques. Moreover, level abstraction makes possible employ resultsconcerning CBGs outside context Dec-POMDPs.10. probabilistic equivalence criterion lossless clustering introduced Oliehoek et al. (2009).article presents new, simpler proof optimality clustering based PE.464fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPsrational always choose action. Section 3.2, describe applicationresults GMAA*. Section 3.3 introduces improved heuristic representationsallow computation longer horizons.3.1 Lossless Clustering Dec-POMDPssection, discuss lossless clustering based notion probabilistic equivalence.show clustering lossless demonstrating probabilistic equivalence impliesbest response equivalence, describes conditions rational agent selectaction two types. prove implication, show best responsedepends multiagent belief (i.e., probability distribution states policiesagents), two probabilistically equivalent histories. Relationsequivalence notions discussed Section 6.3.1.1 Probabilistic Equivalence Criterionfirst introduce probabilistic equivalence criterion, used decide whethertwo individual histories ~ia ,~ib clustered without loss value.Criterion 1 (Probabilistic Equivalence). Two AOHs ~ia ,~ib agent probabilisticallyequivalent (PE), written P E(~ia ,~ib ), following holds:~6=iPr(s,~6=i |~ia ) = Pr(s,~6=i |~ib ).(3.1)probabilities computed conditional Pr(s,~ |b0 ,t ), defined (A.2).subsections 3.1.23.1.4, formally prove PE sufficient criterion guaranteeclustering lossless. remainder Section 3.1.1 discuss key propertiesPE criterion order build intuition.Note criterion decomposed following two criteria:~6=i~6=iPr(~6=i |~ia ) = Pr(~6=i |~ib ),(3.2)Pr(s|~6=i ,~ia ) = Pr(s|~6=i ,~ib ).(3.3)criteria give natural interpretation: first says probability distributionagents AOHs must identical ~ia ~ib . second demandsresulting joint beliefs identical.probabilities well defined without initial state distribution b0past joint policy . However, since consider clustering histories within particular CBG(for stage t) constructed particular b0 ,t , implicitly specified. Thereforedrop arguments, clarifying notation.Example 4. Example 3, types (oHL ,oHR ) (oHR ,oHL ) agent PE. see this, noterows (columns second agent) histories identical Fig. 4aFig. 4b. Thus, specify distribution histories agents (cf. equation (3.2))induced joint beliefs (cf. equation (3.3)).Probabilistic equivalence convenient property algorithms exploit: holdsparticular pair histories, also hold identical extensionshistories, i.e., propagates forwards regardless policies agents.465fiOliehoek, Spaan, Amato, & WhitesonDefinition 7 (Identical extensions). Given two AOHs ~ia,t ,~ib,t , respective extensions~ a,t+1 = (~ a,t ,ai ,oi ) ~ b,t+1 = (~ b,t ,a ,o ) called identical extensionsai = ai oi = oi .Lemma 1 (Propagation PE). Given ~ia,t ,~ib,t PE, regardless decision ruleagents use ( t6=i ), identical extensions also PE:ati ot+1 st+1 ~t+16=i6=it+1t+1 ~ t+1 ~ b,t t+1, 6=i |i ,ai ,oi , 6=i ) (3.4)Pr(st+1 ,~6=i |~ia,t ,ati ,ot+1, 6=i ) = Pr(sProof. proof listed appendix, holds intuitively probabilitiesdescribed before, also taking actionseeing observation.Note that, probabilities defined (3.1) superficially resemble beliefs usedPOMDPs, substantially different. POMDP, single agent computeindividual belief using AOH. use belief determine valuefuture policy, sufficient statistic history predict future rewards(Kaelbling et al., 1998; Bertsekas, 2005). Thus, trivial show equivalence AOHsinduce individual belief POMDP. Unfortunately, Dec-POMDPsproblematic. next section elaborates issue discussing relation multiagentbeliefs.3.1.2 Sub-Tree Policies, Multiagent Beliefs Expected Future Valuedescribe relationship multiagent beliefs probabilistic equivalence,must first discuss policies agent may follow resulting values. beginintroducing concept sub-tree policies. illustrated Fig. 2 (on page 456),(deterministic) policy represented tree nodes labeled using actionsedges labeled using observations: root node corresponds first action taken,nodes specify action observation history encoded path root node.such, possible define sub-tree policies, , correspond sub-trees agentpolicy (also illustrated Fig. 2). particular, writew~o = =ht(3.5)sub-tree policy corresponding wobservation history ~oit specifies actionslast = h stages. refer policy consumption operator, sincewconsumes part policy corresponding ~oit . Similarly write =k ~o l = =kl(note (3.5), = h-steps-to-go sub-tree policy) use similar notation,=k , joint sub-tree policies. extensive treatment different formspolicy, refer discussion Oliehoek (2012).Given concepts, define value = k-stages-to-go joint policy startingstate s:XXwPr(s ,o|s,a)V (s , =k ).(3.6)V (s, =k ) = R(s,a) +Here, joint action specified roots individual sub-tree policies specified=k stage = h k.466fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPsdefinition, follows directly probability distribution statessub-tree policies agents 6=i sufficient predict value sub-tree policy .fact, distribution known multiagent belief bi (s, 6=i ) (Hansen, Bernstein, &Zilberstein, 2004). value givenXXV (bi ) = maxbi (s, 6=i )V (s,hi , 6=i i),(3.7)6=irefer maximizing agent best response bi . illustratesmultiagent belief sufficient statistic: contains sufficient information predict valuesub-tree policy .possible connect action observation histories multiagent beliefs fixingpolicies agents. Given agents act according profilepolicies 6=i , agent multiagent belief first stage Dec-POMDP: bi (s, 6=i ) =b0 (s). Moreover, agent maintain multiagent belief execution. such,given 6=i , history ~i induces multiagent belief, write bi (s, 6=i |~i , 6=i )make dependence ~i , 6=i explicit. multiagent belief history definedbi (s, 6=i |~i , 6=i ) , Pr(s, 6=i |~i , b0 , 6=i ),(3.8)induces best response via (3.7):BR(~i | 6=i ) , arg maxXXbi (s, 6=i |~i , 6=i )V (s, 6=i ,i ).(3.9)6=iconclude two AOHs ~ia ,~ib clustered together inducemultiagent belief.However, notion multiagent belief clearly quite different distributionsused notion PE. particular, establish whether two AOHs inducemultiagent belief, need full specification 6=i . Nevertheless, show two AOHsPE also best response equivalent therefore cluster them.crux show that, Criterion 1 satisfied, AOHs always inducemultiagent beliefs 6=i (consistent current past joint policy 6=i ).3.1.3 Best-Response Equivalence Allows Lossless Clustering Historiesrelate probabilistic equivalence multiagent belief follows.Lemma 2 (PE implies multiagent belief equivalence). 6=i , probabilistic equivalenceimplies multiagent belief equivalence:P E(~ia ,~ib ) s,6=i bi (s, 6=i |~ia , 6=i ) = bi (s, 6=i |~ib , 6=i )(3.10)Proof. See appendix.lemma shows two AOHs PE, produce multiagent belief.Intuitively, gives us justification cluster AOHs together: since multiagentbelief sufficient statistic act multiagent belief,since Lemma 2 shows ~ia ,~ib induces multiagent beliefs 6=iPE, conclude always act histories. Formally,prove ~ia ,~ib best-response equivalent PE.467fiOliehoek, Spaan, Amato, & WhitesonTheorem 2 (PE implies best-response equivalence). Probabilistic equivalence implies bestresponse equivalence.P E(~ia ,~ib ) 6=i BR(~ia | 6=i ) = BR(~ib | 6=i )Proof. Assume arbitrary 6=i ,BR(~ia | 6=i ) = arg maxXXbi (s, 6=i |~ia )V (s, 6=i ,i )= arg maxXXbi (s, 6=i |~ib )V (s, 6=i ,i ) = BR(~ib | 6=i ),6=i6=iLemma 2 employed assert equality bi (|~ia ) bi (|~ib ).theorem key demonstrates two AOHs ~ia ,~ib agentPE, agent need discriminate future. Thus,searching space joint policies, restrict search assignsub-tree policy ~ia ~ib . such, directly provides intuition losslessclustering possible. Formally, define clustered joint policy space follows.Definition 8 (Clustered joint policy space). Let C subset joint policiesclustered: i.e., part C assigns sub-tree policy actionobservation histories probabilistically equivalent.Corollary 1 (Existence optimal clustered joint policy). exists optimal jointpolicy clustered joint policy space:max V () = max V ()C(3.11)Proof. clear left hand side (3.11) upper bounded right hand side,since C . suppose = arg max V () strictly higher valuebest clustered joint policy. least one agent one pair PE histories ~ia , ~ib , mustassign different sub-tree policies ia 6= ib (otherwise would clustered). Without lossgenerality assume one pair. follows directly Theorem 2policy construct clustered policy C C (by assigning either ia ib~ia , ~ib ) guaranteed value less , thereby contradictingassumption strictly higher value best clustered joint policy.formally proves restrict search C , space clustered jointpolicies, without sacrificing optimality.3.1.4 Clustering Commitment CBGsThough clear two AOHs PE clustered, making resultoperational requires additional step. end, use abstraction layer providedBayesian games. Recall CBG stage, AOHs correspond types.468fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPsTherefore, want cluster types CBG. accomplish clustering twotypes ia ,ib , introduce new type ic replace them, defining:6=i Pr(ic , 6=i ) , Pr(ia , 6=i ) + Pr(ib , 6=i )ju(hic , 6=i ,a)ffPr(ia , 6=i )u(hia , 6=i ,a) + Pr(ib , 6=i )u( ib , 6=i ,a).,Pr(ia , 6=i ) + Pr(ib , 6=i )(3.12)(3.13)Theorem 3 (Reduction commitment). Given agent collaborative Bayesiangame B committed selecting policy assigns action two typesia ,ib , i.e., selecting policy (ia ) = (ib ), CBG reduced withoutloss value agents. is, result new CBG B agent employspolicy reflects clustering whose expected payoff originalCBG: V B (i , 6=i ) = V B (i , 6=i ).Proof. See appendix.theorem shows that, given agent committed taking actiontypes ia ,ib , reduce collaborative Bayesian game B smaller one Btranslate joint CBG-policy foundB back joint CBG-policy B.necessarily mean = , 6=i also solution B, best-responseagent 6=i may select action ia ,ib . Rather best-response6=i given action needs taken ia ,ib .11Even though Theorem 3 gives conditional statement depends agentcommitted select action two types, previous subsection discussedrational agent make commitment. Combining results givesfollowing corollary.Corollary 2 (Lossless Clustering PE). Probabilistically equivalent histories ~ia ,~ibclustered without loss heuristic value merging single type CBG.Proof. Theorem 3 shows that, given agent committed take actiontwo types, types clustered without loss value. Since ~ia ,~ib PE,best-response equivalent, means agent committed usesub-tree policy hence action ai . Therefore directly apply clusteringwithout loss expected payoff, CBG stage Dec-POMDP means lossexpected heuristic value given (2.7).Intuitively, maximizing action ~ia ~ib regardless (future)joint policies 6=i agents use hence cluster without lossheuristic value. Note depend heuristic used hence alsoholds optimal heuristic (i.e., using optimal Q-value function givestrue value). directly relates probabilistic equivalence equivalence optimal value.1211. Although focus CBGs, results generalize BGs individual payoff functions. Thus,could potentially exploited algorithms general-payoff BGs. Developing methodsinteresting avenue future work.12. proof originally provided Oliehoek et al. (2009) based showing histories PEinduce identical Q-values.469fiOliehoek, Spaan, Amato, & WhitesonAlgorithm 6 ClusterCBG(B)Input: CBG BOutput: Losslessly clustered CBG B1: agent2:individual type B.i3:Pr(i ) = 04:B.i B.i \i5:continue6:end7:individual type B.i8:isProbabilisticallyEquivalent true9:hs, 6=i10:Pr(s, 6=i |i ) 6= Pr(s, 6=i |i )11:isProbabilisticallyEquivalent false12:break13:end14:end15:isProbabilisticallyEquivalent16:B.i B.i \i17:18:6=i19:u(i , 6=i ,a) min(u(i , 6=i ,a),u(i , 6=i ,a))20:Pr(i , 6=i ) Pr(i , 6=i ) + Pr(i , 6=i )21:Pr(i , 6=i ) 022:end23:end24:end25:end26:end27: end28: return B{Prune B:}{Prune B:}{ take lowest upper bound }Note result establishes sufficient, necessary condition lossless clustering.particular, given policies agents, many types best-response equivalentclustered. However, far know, criterion must hold order guaranteetwo histories best-response policy agents.3.2 GMAA* Incremental ClusteringKnowing individual histories clustered together without loss valuepotential speed many Dec-POMDP methods. article, focus applicationwithin GMAA* framework.Emery-Montemerlo et al. (2005) showed clustering incorporated every stagealgorithm: CBG stage constructed, clustering individualhistories (types) performed first afterwards (reduced) CBG solved.approach employed within GMAA* modifying Expand procedure (Algorithm 5)cluster CBG calling GenerateAllChildrenForCBG.Algorithm 6 shows clustering algorithm. takes input CBG returnsclustered CBG. performs clustering performing pairwise comparison types470fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPsbAlgorithm 7 ConstructExtendedBG(B, t1 , Q)Input: CBG B stage 1, joint BG policy followed t1 .b ~,a).Input: admissible heuristic form Q(Output: CBG B stage t.1: B B{make copy B subsequently alter}2: agent3:B .i = ConstructExtendedTypeSet(i){overwrite individual type sets}4: end5: B . iD{the new joint type set (does explicitly stored)}6: joint type = ( t1 ,at1 ,ot ) B .7:state st8:Compute Pr(st |){from Pr(st1 | t1 ) via Bayes rule }9:end10:Pr() Pr(ot | t1 ,at1 ) Pr( t1 )11:12:q13:history ~ representedb ~ ,a))b take lowest upper bound }14:q min(q,Q({ Q Q15:end16:B .u(,a) q17:end18: end19: return Bagent see satisfy criterion, yielding O(|i |2 ) comparisons agent i.comparison involves looping hs, 6=i (line 9). many states, efficiencycould gained first checking (3.2) checking (3.3). Rather takingaverage (3.13), line 19 take lowest payoff, done usingupper bound heuristic values.following theorem demonstrates that, incorporating clustering GMAA*,resulting algorithm still guaranteed find optimal solution.Theorem 4. using heuristic form (2.9) clustering CBGs GMAA*using PE criterion, resulting search method complete.Proof. Applying clustering alter computation lower bound values. Also,heuristic values computed expanded nodes admissible fact unalteredguaranteed Corollary 2. Therefore, difference regular GMAA*class considered joint policies restricted C , class clustered joint policies:possible child nodes expanded, clustering effectively prunes away policieswould specify different actions AOHs PE thus clustered. However, Corollary 1guarantees exists optimal joint policy restricted class.modification Expand proposed rather naive. construct B(b0 ,t )must first construct |Oi |t possible AOHs agent (given past policy ti ).subsequent clustering involves pairwise comparison exponentially many types.Clearly, tractable later stages.However, PE AOHs propagates forwards (i.e., identical extensions PE histories also PE), efficient approach possible. Instead clustering exponentially471fiOliehoek, Spaan, Amato, & WhitesonAlgorithm 8 Expand-IC(q, H). expand operator GMAA*-IC.Input: q = ht , vi search node expand.b ~,a).Input: H admissible heuristic form Q(Output: QExpand set containing expanded child nodes.1: B(t1 ) t1 .CBG{retrieve previous CBG, note = (t1 , t1 )}t1 bt12: B( ) ConstructExtendedBG(B(), , Q)3: B(t ) ClusterBG(B(t ))4: .CBG B(t ){store pointer CBG}5: QExpand GenerateAllChildrenForCBG(B(t ))6: return QExpandgrowing set types, simply extend already clustered types previous stagesCBG, shown Algorithm 7. is, given , set types agent previousstage 1, it1 policy agent took stage, set types stage t, ,constructed= = (i ,it1 (i ),oti ) | ,oti Oi .(3.14)means size newly constructed set |i | = |i | |Oi | . type setprevious stage 1 much smaller set histories |i | |Oi |t1 ,new type set also much smaller: |i | |Oi |t . way, bootstrap clusteringstage spend significantly less time clustering. refer algorithmimplements type clustering GMAA* Incremental Clustering (GMAA*-IC).approach possible perform exact, value-preserving clusteringLemma 1 guarantees identical extensions also clustered without lossvalue. performing procedure lossy clustering scheme (e.g., EmeryMontemerlo et al., 2005), errors might accumulate, better option might re-clusterscratch every stage.Expansion GMAA*-IC node takes exponential time respect numberagents types, O(|A |n| | ) joint CBG-policies thus child nodesGMAA*-IC search tree (A largest action set largest type set). Clusteringinvolves pairwise comparison types agent comparisons needscheck O(| |n1 |S|) numbers equality verify (3.1). total cost clusteringtherefore writtenO(n | |2 | |n1 |S|),polynomial number types. clustering decreases numbertypes | |, therefore significantly reduce number child nodes therebyoverall time needed. However, clustering possible, overhead incurred.3.3 Improved Heuristic RepresentationSince clustering reduce number types, GMAA*-IC potential scalelarger horizons. However, important consequences computationheuristics. Previous research shown upper bound provided QMDP oftenloose effective heuristic search (Oliehoek, Spaan, & Vlassis, 2008). However,space needed store tighter heuristics QPOMDP QBG grows exponentiallyhorizon. Recall Section 2.2.2 (see Fig. 5) two approaches computing472fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPsb minimum size.Algorithm 9 Compute Hybrid Q1:2:3:4:5:6:7:8:9:10:11:12:13:14:Qh1 {R1 , . . . ,R|A| }z |A| |S|= h 2 0~ | |A||z <V VectorBackup(Qt+1 )V Prune(V)Qt Vz |V | |S|endzQt TreeBackup(Qt+1 )endend{vector representation last stage}{the size |A| vectors}{size AOH representation}{From z y}QPOMDP QBG . first constructs tree joint AOHs heuristic values,simple implement requires storing value (~ , a)-pair, numbergrows exponentially t. second approach maintains vector-based representation,common POMDPs. Though pruning provide leverage, worst case, pruningpossible number maintained vectors grows doubly exponentially h t,number stages-to-go. Similarly, initial belief subsequently reachable beliefsused reduce number vectors retained stage, number reachablebeliefs exponential horizon exponential complexity remains.Oliehoek, Spaan, Vlassis (2008) used tree-based representation QPOMDP QBG heuristics. Sincecomputational cost solving Dec-POMDP bottleneck, inefficiencies representation could overlooked. However, approach longer feasiblelonger horizons made possible GMAA*-IC.Hybridt=0mitigate problem, propose hybrid represent=1tation heuristics, illustrated Fig. 6. maininsight exponential growth two existing representations occurs opposite directions. Therefore,t=2use low space-complexity side representations:later stages, fewer vectors, use vector-based representation, earlier stages, fewer histot=3ries, use history-based representation. similaridea utilizing reachable beliefs reduce size Figure 6: illustrationvector representation described but, rather stor- hybrid representation.ing vectors appropriate AOHs step,values needed using tree-based representation.Algorithm 9 shows how, mild assumptions, minimally-sized representationcomputed. Starting last stage, algorithm performs vector backups, switchingtree backups become smaller option. last time step h 1, represent473fiOliehoek, Spaan, Amato, & WhitesonQt set immediate reward vectors13 , variable z (initialized line 2) keeps tracknumber parameters needed represent Qt vectors time step hand.Note z depends effective vector pruning is, i.e., large parsimoniousrepresentation piecewise linear convex value function is. Since problemdependent, z updated pruning actually performed (line 9).contrast y, number parameters tree representation, computed directlyDec-POMDP (line 4). z > y, algorithm switches tree backups.144. Incremental Expansionclustering technique presented previous section potential significantlyspeed planning much clustering possible. However, little clustering possible,number children GMAA* search tree still grow super-exponentially. sectionpresents incremental expansion, complementary technique deal problem.Incremental expansion exploits recent improvements effectively solving CBGs. Firstnote expansion last stage = h 1 particular h1 ,interested best child (h1 , h1, ), corresponds optimal solutionBayesian game h1, . such, last stage, use new methods solvingCBGs (Kumar & Zilberstein, 2010b; Oliehoek, Spaan, Dibangoye, & Amato, 2010)provide speedups multiple orders magnitude brute force search (enumeration).15Unfortunately, improvements GMAA* afforded approach limited: orderguarantee optimality, still relies expansion (child nodes corresponding all)joint CBG-policies intermediate stages, thus necessitating brute-force approach.However, many expanded child nodes may low heuristic values Vb may thereforenever selected expansion.Incremental expansion overcomes problem exploits following key observation: generate children decreasing heuristic order using admissibleheuristic, expand children. before, A* search performedpartially specified policies new CBG constructed extending CBGparent node. However, rather fully expanding (i.e., enumerating CBG policiesthereby constructing children for) search node, instantiate incrementalCBG solver corresponding CBG. incremental solver returns one joint CBGpolicy time, used construct single child t+1 = (t , ). revisitingnodes, promising child nodes expanded incrementally.Below, describe GMAA*-ICE, algorithm combines GMAA*-IC incremental expansion. establish theoretical guarantees describe modificationsBaGaBaB, CBG solver GMAA*-ICE employs, necessary deliverchild nodes decreasing order.13. exceptional cases short horizon combined large state action spaces representing last time step vectors minimal. cases, algorithm trivially adapted.14. assumes vector representation shrink earlier stages. Although unlikelypractice, cases would prevent algorithm computing minimal representation.15. Kumar Zilberstein (2010b) tackle slightly different problem; introduce weighted constraint satisfaction approach solving point-based backup dynamic programming Dec-POMDPs. However,point-based backup interpreted collection CBGs (Oliehoek et al., 2010).474fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPs4.1 GMAA* Incremental Clustering Expansionbegin formalizing incremental expansion incorporating GMAA*-IC, yielding GMAA* incremental clustering expansion (GMAA*-ICE). coreincremental expansion lies following lemma:Lemma 3. Given two joint CBG policies , CBG B(b0 ,t ), Vb () Vb ( ),corresponding child nodes Vb (t+1 ) Vb (t+1 ).Proof. holds directly definition Vb (t ) given (2.8):Vb (t+1 ) = V 0...(t1) (t ) + Vb ()V 0...(t1) (t ) + Vb ( ) = Vb (t+1 ).follows directly that, B(b0 ,t ) use CBG solver generate sequencepolicies , , . . .Vb () Vb ( ) . . .then, sequence corresponding childrenVb (t+1 ) Vb (t+1 ) . . . .Exploiting knowledge, expand first child t+1 compute heuristicvalue Vb (t+1 ) using (2.8). Since unexpanded siblings heuristic values lessequal that, modify GMAA*-IC reinsert node q open list Lact placeholder non-expanded children.Definition 9. placeholder node least one child expanded.placeholder heuristic value equal last expanded child.Thus, expansion search node qs child, update q.v, heuristic valuenode, Vb (t+1 ), value expanded child, i.e., set q.v Vb (t+1 ). such,reinsert q L placeholder. mentioned above, correctunexpanded siblings (for parent node q placeholder) heuristic valueslower equal Vb (t+1 ). Therefore next sibling q represented placeholderalways expanded time: q always created nodes lower heuristic valueselected expansion. keep track whether node previously expandedplaceholder not.before, GMAA*-ICE performs A* search partially specified policies.GMAA*-IC, new CBG constructed extending CBG parent nodeapplying lossless clustering. However, rather expanding children, GMAA*-ICErequests next solution incremental CBG solver, single childt+1 = (t , ) constructed. principle GMAA*-ICE use CBG solver ableincrementally deliver descending order Vb (). propose modificationBaGaBaB algorithm (Oliehoek et al., 2010), briefly discussed Section 4.3.Fig. 7 illustrates process incremental expansion GMAA*-ICE, indexedletters. First, CBG solver root node ha, 7i created, optimal solutioncomputed, value 6. results child hb, 6i, root replaced placeholdernode ha, 6i. per Definition 5 (the node comparison operator), b appears475fiOliehoek, Spaan, Amato, & WhitesonLegend:v76Root node6b6t+1b4New B(a), Vb =6c4t+2ht , viopen listha, 7i5.5New B(b), Vb =4ha, 6ihc, 4ihb, 4ihb, 6iha, 6ib4c45.5Next solutionB(a), Vb =5.5hd, 5.5iha, 5.5ihc, 4ihb, 4iFigure 7: Illustration incremental expansion, nodes open list bottom.Past joint policies indexed letters. Placeholder nodes indicated dashes.open list hence selected expansion. best child hc, 4i added hb, 6i replacedplaceholder hb, 4i. search returns root node, second best solutionobtained CBG solver, leading child hd, 5.5i. Placeholder nodes retainedlong unexpanded children; values updated.using GMAA*-ICE, derive lower upper bounds CBG solution,exploited incremental CBG solver. incremental CBG solverB(t ) initialized lower boundvCBG = vGM AA V 0...(t1) (t ),(4.1)vGM AA value current best solution, V 0...(t1) (t ) true expectedvalue first stages. Therefore, vCBG minimum value candidatemust generate remaining h stages order beat current best solution. Notetime incremental CBG solver queried solution, vCBG re-evaluated(using (4.1)), vGM AA may changed.used heuristic faitfully represents immediate reward (i.e., form(2.9)), then, last stage = h 1, also specify upper bound solutionCBGvCBG = Vb (h1 ) V 0...(h2) (h1 ).(4.2)upper bound attained, solutions required CBG solver.upper bound holds since (2.8)Vb () , Vb (h ) V 0...(h2) (h1 )= V (h ) V 0...(h2) (h1 )Vb (h1 ) V 0...(h2) (h1 ).first step, Vb (h ) = V (h ), h fully specified policy heuristic valuegiven (2.8) equals actual value heuristic faithfully represents expected476fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPsAlgorithm 10 Expand-ICE(q, H). expand operator GMAA*-ICE.Input: q = ht , vi search node expand.b ~,a).Input: H admissible heuristic form Q(Output: QExpand set containing 0 1 expanded child nodes.1: IsPlaceholder(q)2:B(t ) .CBG{reuse stored CBG}3: else4:B(t1 ) t1 .CBG{retrieve previous CBG, note = (t1 , t1 )}t1b5:B(t ) ConstructExtendedBG(B(t1 ), , Q)6:B( ) ClusterBG(B( ))7:B(t ).Solver CreateSolver(B(t ))8:.CBG B(t ){store pointer CBG}9: end{set lower bound CBG solution}10: vCBG = vGM AA V 0...(t1) (t )11: = h 112:vCBG = Vb (h1 ) V 0...(h2) (h1 ){upper bound used last stage CBG}13: else14:vCBG = +15: endb ( )i B(t ).Solver.NextSolution(vCBG ,vCBG ){compute next CBG solution}16: h , V17:18:t+1 (t , ){create partial joint policy}t+10...t1bb19:V ( ) V( ) + V ( ){compute heuristic value}20:q ht+1 , Vb (t+1 )i{create child node}21:QExpand {q }22: else23:QExpand{fully expanded: exists solution s.t. V ( h1 ) vCBG }24: end25: return QExpandAlgorithm 11 PostProcessNode-ICE(q, L): Post processing node GMAA*-ICE.Input: q last expanded node, L open list.Output: q either removed updated.1: L.Pop(q)2: q fully expanded depth(q) = h 13:Cleanup q{delete node associated CBG Solver}4:return5: else6:c last expanded child q7:q.v c.v{update heuristic value parent node}8:IsPlaceholder(q) true{remember q placeholder}9:L.Insert(q){reinsert appropriate position}10: end477fiOliehoek, Spaan, Amato, & Whitesonimmediate reward used. implies Vb () lower bound. second stepV (h ) Vb (h1 ), Vb (h1 ) admissible. Therefore, stop expandingfind (lower bound) heuristic value equal upper bound vCBG . applieslast stage first step valid.GMAA*-ICE implemented replacing Expand PostProcessNodeprocedures Algorithms 8 4 Algorithms 10 11, respectively. Expand-ICE firstdetermines placeholder used either reuses previously constructed incremental CBG solver constructs new one. Then, new bounds calculated nextCBG solution obtained. Subsequently, single child node generated (ratherexpanding children Algorithm 13). PostProcessNode-ICE removes last nodereturned Select children expanded. Otherwise,updates nodes heuristic value reinserts open list. See Appendix A.2GMAA*-ICE shown single algorithm.4.2 Theoretical Guaranteessection, prove GMAA*-IC GMAA*-ICE search-equivalent. directresult establish GMAA*-ICE complete, means integrating incrementalexpansion preserves optimality guarantees GMAA*-IC.Definition 10. call two GMAA* variants search-equivalent select exactlysequence non-placeholder nodes corresponding past joint policies expandsearch tree using Select operator.GMAA*-IC GMAA*-ICE show set selected nodes same.However, set expanded nodes different; fact, precisely differencesincremental expansion exploits.Theorem 5. GMAA*-ICE GMAA*-IC search-equivalent.Proof. Proof listed Section A.4 appendix.Note Theorem 5 imply computational space requirementsGMAA*-ICE GMAA*-IC identical. contrary, expansion,GMAA*-ICE generates one child node stored open list. contrast,GMAA*-IC generates number child nodes is, worst case, doubly exponentialdepth selected node.16 However, GMAA*-ICE guaranteedefficient GMAA*-IC. example, case child nodes stillgenerated, GMAA*-ICE slower due overhead incurs.Corollary 3. using heuristic form (2.9) GMAA*-ICE complete.Proof. stated conditions, GMAA*-IC complete (see Theorem 4).GMAA*-ICE search equivalent GMAA*-IC, also complete.Since16. problem allows clustering, number child nodes grows less dramatically (see Section 3).478fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPs4.3 Incremental CBG SolversImplementing GMAA*-ICE requires CBG solver incrementally deliverdescending order Vb (). end, propose modify Bayesian game BranchBound (BaGaBaB) algorithm (Oliehoek et al., 2010). BaGaBaB performs A*-search(partially specified) CBG policies. Thus, applied within GMAA*-ICE, performssecond, nested A* search. expand node GMAA* search tree, nested A*search computes next CBG solution.17 section briefly summarizes main ideasbehind BaGaBaB (for information, see Oliehoek et al., 2010) modifications.BaGaBaB works creating search tree nodes correspond partiallyspecified joint CBG policies. particular, represents joint action vector, vectorh( 1 ), . . . ,( || )i joint actions specifies joint type. node gBaGaBaB search tree represents partially specified vector thus partially specifiedjoint CBG policy. example, completely unspecified vector h, , . . . ,i correspondsroot node, internal nodeg depth (root beingff depth 0) specifies jointactions first joint types g = ( 1 ), . . . , ( ), , , . . . , . value node V (g)value best joint CBG-policy consistent it. Since value knownadvance, BaGaBaB performs A* search guided optimistic heuristic.particular, compute upper bound value achievablepartially specified vector computing maximum value complete information jointpolicy consistent (i.e., non-admissible joint policy selects maximizingjoint actions remaining joint types). Since value guaranteed upper boundmaximum value achievable consistent joint CBG policy, admissible heuristic.propose modification BaGaBaB allow solutions incrementally delivered.main idea retain search tree first call BaGaBaB particular CBGB(t ) update subsequent calls, thereby saving computational effort.Standard A* search terminates single optimal solution found.behavior incremental BaGaBaB called first time B(t ).However, standard A*, nodes whose upper bound lower best known lowerbound safely deleted, never lead optimal solution. contrast,incremental setting nodes cannot pruned, could possibly result k-thbest solution therefore might need expanded subsequent calls BaGaBaB.nodes returned solutions pruned order avoid returning solutiontwice. modification requires memory affect A* search processotherwise.asked k-th solution, BaGaBaB resets internal lower bound valuenext-best solution previously found returned (or vCBG defined(4.1) solution found). starts A* search initialized using searchtree resulting (k 1)-th solution. essence, method similar searchingbest k solutions, k incremented demand. Recently shown that,fixed k, modification preserves theoretical guarantees (soundness, completeness,17. GMAA*-ICE could also use incremental CGB solver, avoid enumeratingproviding first result thus potential work incrementally. exception maymethod Kumar Zilberstein (2010b), employs AND/OR branch bound searchEDAC heuristic (and thus limited two-agent case). heuristic search method, mayamenable incremental implementation though knowledge attempted.479fiOliehoek, Spaan, Amato, & Whitesonoptimal efficiency) A* algorithm (Dechter, Flerova, & Marinescu, 2012), resultstrivially transfer setting k allowed increase.5. Experimentssection, empirically test validate proposed techniques: lossless clusteringjoint histories, incremental expansion search nodes, hybrid heuristic representations.introducing experimental setup, compare performance GMAA*-ICGMAA*-ICE GMAA* suite benchmark problems literature.Next, compare performance proposed methods state-of-the-art optimalapproximate Dec-POMDP methods, followed case study scaling behaviorrespect number agents. Finally, compare memory requirementshybrid heuristic representation tree vector representations.5.1 Experimental Setupwell-known Dec-POMDP benchmarks Dec-Tiger (Nair et al., 2003)BroadcastChannel (Hansen et al., 2004) problems. Dec-Tiger discussed extensivelySection 2. BroadcastChannel, two agents transmit messages communication channel, agents transmit time collision occursnoisily observed agents. FireFighting problem models team n firefightersextinguish fires row nh houses (Oliehoek, Spaan, & Vlassis, 2008).agent choose move houses fight fires location; two agentshouse, completely extinguish fire there. (negative) rewardteam firefighters depends intensity fire house; firesextinguished, reward zero received. Hotel 1 problem (Spaan & Melo, 2008),travel agents need assign customers hotels limited capacity. also sendcustomer resort yields lower reward. addition, also use following problems: Recycling Robots (Amato, Bernstein, & Zilberstein, 2007), scaled-down versionproblem described Section 2; GridSmall two observations (Amato, Bernstein, &Zilberstein, 2006) Cooperative Box Pushing (Seuken & Zilberstein, 2007a), largertwo-robot benchmark. Table 1 summarizes problems numerically, listing numberjoint policies different planning horizons.Experiments run Intel Core i5 CPU running Linux, GMAA*, GMAA*-IC,GMAA*-ICE implemented code-base using MADP Toolbox (C++)(Spaan & Oliehoek, 2008). vector-based QBG representation computed using variation Incremental Pruning (adapted computing Q-functions instead regular value functions), corresponding NaiveIP method described Oliehoek Spaan (2012).implement pruning, employ Cassandras POMDP-solve software (A. R. Cassandra,1998).results Sections 5.2 5.3, limited process 2Gb RAMmaximum CPU time 3,600s. Reported CPU times averaged 10 independent runsresolution 0.01s. Timings given MAA* search processes, since480fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPsproblem primitivesDec-Tigernum. hn|S||Ai ||Oi |24622327.29e22.06e141.31e60BroadcastChannel24226.40e11.07e98.51e37GridSmall216521.563e49.313e201.175e88Cooperative Box Pushing2100451.68e76.96e1871.96e4703Recycling Robots24327.29e22.06e141.31e60Hotel 1216345.90e41.29e813.48e1302FireFighting2432327.29e22.06e141.31e60Table 1: Benchmark problem sizes number joint policies different horizons.computation heuristic methods amortized multipleruns.18 problem definitions available via http://masplan.org.5.2 Comparing GMAA*, GMAA*-IC, GMAA*-ICEcompared GMAA*, GMAA*-IC, GMAA*-ICE using hybrid QBG representation. methods compute optimal policy, expect GMAA*-IC efficientGMAA* lossless clustering possible. Furthermore, expect GMAA*-ICEprovide improvements terms speedup scaling longer planning horizons.results shown Table 2. entries report results, QBG heuristicscould computed, thanks hybrid representation. Consequently, performanceGMAA*-IC much better previously reported results, including Oliehoeket al. (2009), often required resort QMDP larger problems and/or horizons.entries marked show limits using QMDP instead QBG :problems reach longer horizons QBG . FireFighting GMAA*-ICEQMDP compute solutions higher h possible QBG (hence missing ,showing GMAA*-ICE efficient using loose heuristic GMAA*-IC).Furthermore, entries indicate horizon solve problemtree-based QBG representation often much shorter.results clearly illustrate GMAA*-IC leads significant improvementperformance. problems, GMAA*-IC able produce solution quicklyincrease largest solvable horizon GMAA*. cases, GMAA*-IC abledrastically increase solvable horizon.Furthermore, results clearly demonstrate incremental expansion allows significant additional improvements. fact, table demonstrates GMAA*-ICE significantly outperforms GMAA*-IC, especially problems little clustering possible.results Table 2 also illustrate efficacy hybrid representation. problemslike GridSmall, Cooperative Box Pushing, FireFighting Hotel 1 neithertree vector representation able provide compact QBG heuristic longer hori18. heuristics computation time ranges less second hours (for high h difficultproblems). Table 4 presents heuristic computation time results.481fiOliehoek, Spaan, Amato, & Whitesonh234567V TGMAA* (s)Dec-Tiger4.0000000.015.1908120.014.802755563.097.02645110.381625TIC (s)TICE (s)h0.010.010.2721.030.010.010.010.0246.432345610151820304050607080FireFighting hnh = 3,nf = 3i2 4.3834960.090.010.013 5.7369693.050.110.104 6.5788341001.53 950.511.005 7.0698744.406 7.1755910.080.077##GridSmall20.9100000.010.010.0131.5504440.900.100.0142.241577*1.77 0.0152.9704960.0263.7171680.047##Hotel 12 10.0000000.00.010.013 16.875000*0.010.014 22.1875000.01 0.015 27.1875000.010.016 32.1875000.010.017 37.1875000.010.018 42.1875000.010.019 47.1875000.020.0110##Cooperative Box Pushing217.6000000.020.010.01366.0810000.11 0.01498.593613313.075##234567102025304050531002505006007008009001000V TGMAA* (s) TIC (s) TICE (s)Recycling Robots7.0000000.01 0.010.0110.6601250.01 0.010.0113.380000713.41 0.010.0116.4860000.01 0.0119.5542000.010.0131.8638890.010.0147.2485210.010.0156.4792900.01 0.0162.6331360.010.0193.4023670.080.05124.1715980.420.25154.9408282.021.27185.7100599.706.00216.47929028.66BroadcastChannel2.0000000.01 0.010.012.9900000.01 0.010.013.8900000.01 0.010.014.7900001.27 0.010.015.6900000.010.016.5900000.01 0.019.2900000.010.0118.3132280.010.0122.8815230.010.0127.4218500.010.0136.4597240.010.0145.5016040.010.0148.2264200.01 0.0190.7604230.010.01226.5005450.060.07452.7381190.810.94543.22807111.6313.84633.7242790.520.63814.7093939.5711.11Table 2: Experimental results comparing regular GMAA*, GMAA*-IC, GMAA*-ICE.Listed computation times GMAA* (TGMAA* ), GMAA*-IC (TIC ),GMAA*-ICE (TICE ), using hybrid QBG representation. use following symbols:memory limit violations, time limit overruns, # heuristic computation exceeded memory time limits, maximum planning horizon using QMDP , maximum planning horizonusing tree-based QBG . Bold entries indicate methods proposed articlecomputed results.zons. Apart Dec-Tiger FireFighting, computing storing QBG (or anothertight heuristic) longer horizons bottleneck scalability.Together, algorithmic improvements lead first optimal solutions manyproblem horizons. fact, vast majority problems tested, provide resultslonger horizons previous work (the bold entries). improvements quite sub482fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPsh2345234562342345678923|BGh1 ||cBGt |Dec-Tiger4 1.0, 4.016 1.0, 4.0, 9.064 1.0, 4.0, 9.0, 23.14256 1.0, 4.0, 9.0, 16.0, 40.43FireFighting hnh = 3,nf = 3i4 1.0, 4.016 1.0, 4.0, 16.064 1.0, 4.0, 16.0, 64.0256 1.0, 4.0, 16.0, 64.0, 256.01024 1.0, 1.0, 2.0, 3.0, 6.0, 10.0GridSmall4 1.0, 4.016 1.0, 4.0, 10.5064 1.0, 4.0, 10.50, 20.0Hotel 116 1.0, 4.0256 1.0, 4.0, 16.04096 1.0, 4.0, 8.0, 16.065536 1.0, 4.0, 4.0, 8.0, 16.01.05e6 1.0, 4.0, . . . , 4.0, 8.0, 16.01.68e7 1.0, 4.0, . . . , 4.0, 8.0, 16.02.68e8 1.0, 4.0, . . . , 4.0, 8.0, 16.04.29e9 1.0, 4.0, . . . , 4.0, 8.0, 16.0Cooperative Box Pushing25 1.0, 4.0625 1.0, 4.0, 25.0h23451015182030405060234567102025304050100900|BGh1 | |cBGt |Recycling Robots4 1.0, remaining stages16 1.0, remaining stages64 1.0, remaining stages256 1.0, remaining stages262144 1.0, remaining stages2.68e8 1.0, remaining stages1.72e10 1.0, remaining stages2.75e11 1.0, remaining stages2.88e17 1.0, remaining stages1.0, remaining stages1.0, remaining stages1.0, remaining stagesBroadcastChannel4 1.0 (for t)16 1.0 (for t)64 1.0 (for t)256 1.0 (for t)1024 1.0 (for t)4096 1.0 (for t)262144 1.0 (for t)2.75e11 1.0 (for t)2.81e14 1.0 (for t)2.88e17 1.0 (for t)1.0 (for t)1.0 (for t)1.0 (for t)1.0 (for t)4.04.04.04.04.04.04.04.04.04.04.04.0Table 3: Experimental results detailing effectiveness clustering. Listed sizeCBGs = h 1 without clustering (|BGh1 |), average CBG size stagesclustering (|cBGt |).stantial, especially given lengthening horizon one increases problem difficultyexponentially (cf. Table 1).5.2.1 Analysis Clustering HistoriesTable 3 provides additional details performance GMAA*-IC, listingnumber joint types GMAA*-IC search, |cBGt |, stage t. averagessince algorithm forms CBGs different past policies, leading clusterings differentsizes.19 see impact clustering, table also lists |BGh1 |, number joint typesCBGs constructed last stage without clustering, constant.Dec-Tiger, time needed GMAA*-IC 3 orders magnitude lessGMAA* horizon h = 4. h = 5, test problem 3.82e29 jointpolicies, method able optimally solve it. GMAA*-IC, however,able reasonable time. Dec-Tiger, clear symmetries19. Note problem domains report smaller clusterings Oliehoek et al. (2009). Dueimplementation mistake, clustering overly conservative, cases treat two historiesprobabilistically equivalent, fact were.483fiOliehoek, Spaan, Amato, & Whitesonobservations allow clustering, demonstrated Fig. 4. Another key propertyopening door resets problem, may also facilitate clustering.FireFighting, short planning horizons lossless clustering possiblestage, such, clustering incurs overhead. However, GMAA*-IC still fasterGMAA* constructing BGs using bootstrapping previous CBGtakes less time constructing CBG scratch. Interesting counterintuitive resultsoccur h = 6, solved within memory limits, contrast h = 5. fact, usingQMDP could compute optimal values V h > 6, turns equalh = 6. reason optimal joint policy guaranteed extinguishfires 6 stages. subsequent stages, rewards 0.influence clustering, analysis Table 3 reveals CBG instances encounteredh = 6 search happen cluster much better h = 5, possibleheuristics vary horizon. fact, h = 6 sends agentsmiddle house = 0, h = 5, agents dispatched different houses.agents fight fires house, fire extinguished completely, resulting jointobservations provide new information. result, different joint types leadjoint belief, means clustered. agents visit different houses,observations convey information, leading different possible joint beliefs (which cannotclustered).Hotel 1 allows large amount clustering, GMAA*-IC outperforms GMAA*large margin, former reaching h = 9 latter h = 2. problemtransition observation independent (Becker, Zilberstein, Lesser, & Goldman, 2003;Nair, Varakantham, Tambe, & Yokoo, 2005; Varakantham, Marecki, Yabu, Tambe, & Yokoo,2007), facilitates clustering, discuss Section 5.5. Unlike methodsspecifically designed exploit transition observation independence, GMAA*-IC exploitsstructure without requiring predefined explicit representation it. scalabilitylimited computation heuristic.BroadcastChannel, GMAA*-IC achieves even dramatic increase performance, allowing solution horizon h = 900. Analysis reveals CBGsconstructed stages fully clustered: contain one type agent.reason follows. constructing CBG = 1, one joint typeprevious CBG so, given 0 , solution previous CBG, uncertaintyrespect previous joint action a0 . crucial property BroadcastChannel(joint) observation reveals nothing new state, joint actiontaken (e.g., collision agents chose send). result, different individualhistories clustered. CBG constructed stage = 2, one jointtype previous game. Therefore, given past policy, actions agentsperfectly predicted. observation conveys information process repeats. Thus, problem special property could described non-observablegiven past joint policy. GMAA*-IC automatically exploits property. Consequently,time needed solve CBG grow horizon. solution time, however, still increases super-linearly increased amount backtracking.FireFighting, performance monotonic planning horizon. case however,clustering clearly responsible difference. Rather, explanationcertain horizons, many near-optimal joint policies, leading backtrackinghigher search cost.484fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPs10Nodes depth10DecTiger, h=6 Full Exp.DecTiger, h=6 Inc. Exp.GridSmall, h=6 Full Exp.GridSmall, h=6 Inc. Exp.FireFighting, h=5 Full Exp.FireFighting, h=5 Inc. Exp.51001001234Figure 8: Number expanded partial joint policies intermediate stages = 0, . . . ,h 2(in log scale).5.2.2 Analysis Incremental ExpansionDec-Tiger h = 5, GMAA*-ICE achieves speedup three orders magnitudecompute solution h = 6, unlike GMAA*-IC. GridSmall, achieves largespeedup h = 4 fast solutions h = 5 6, GMAA*-IC runs memory. Similar positive results obtained FireFighting, Cooperative Box PushingRecycling Robots. fact, using QMDP , GMAA*-ICE able computesolutions well beyond h = 1000 FireFighting problem, stands stark contrast GMAA*-IC computes solutions h = 3 heuristic. NoteBroadcastChannel problem GMAA*-IC (slightly) fasterGMAA*-ICE. problem exhibits clustering single joint type, overheadincremental expansion pay off.analyze incremental expansion, examined impact number nodesexpanded intermediate stages = 0, . . . ,h 2. Fig. 8 shows number nodes expandedGMAA*-ICE number would expanded GMAA*-IC (whicheasily computed since search-tree equivalent). clear relationshipresults Fig. 8 Table 2, illustrating, e.g., GMAA*-IC runs memoryGridSmall h = 6. plots confirm hypothesis that, practice, small numberchild nodes queried.5.2.3 Analysis Hybrid Heuristic RepresentationFig. 9 illustrates memory requirements terms number parameters (i.e., real numbers) tree, vector, hybrid representations QBG , latter computedfollowing Algorithm 9. Results vector representation omitted representations grew beyond limits. effectiveness vector pruning depends problemcomplexity value function, increase suddenly, instance happens Fig. 9c. results show that, several benchmark Dec-POMDPs, hybridrepresentation allows significant savings memory, allowing computation tightheuristics longer horizons.485fiOliehoek, Spaan, Amato, & WhitesonhMILPDP-LPCDP-IPGGMAA QBGICICEheurBroadcastChannel, ICE solvable h = 90020.380.010.090.0131.830.5056.660.01434.06*0.01548.940.010.010.010.010.010.010.010.010.01Dec-Tiger, ICE20.69323.99450.010.010.010.020.010.010.030.09solvable h = 60.050.3260.7355.462286.380.010.010.2721.03FireFighting (2 agents, 3 houses, 3 firelevels), ICE24.458.1310.340.013569.270.114950.51GridSmall, ICE solvable h = 626.6411.580.1834.09477.440.010.101.77Recycling Robots, ICE solvable h = 7021.180.050.300.013*2.791.070.0142136.1642.020.0151812.150.01Hotel2345910151, ICE solvable h = 91.926.140.22315.162913.420.540.731.118.4317.40283.76Cooperative Box Pushing23.5615.5132534.0840.010.010.010.010.02#solvable h 10000.01 0.010.100.071.000.650.010.010.010.010.4267.390.010.010.010.010.010.010.020.020.010.010.010.010.01#0.031.513.744.5420.26(QPOMDP ), ICE solvable h = 41.070.01 0.01 0.016.430.910.020.151138.61*328.97 0.63Table 4: Comparison runtimes methods. Total time GMAA* methodsgiven taking time method column (IC ICE) adding heuristiccomputation time (heur). use following symbols: memory limit violations, *time limit overruns, # heuristic computation exceeded memory time limits.486fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPs10Memory required51001051001234Horizon5106(a) Dec-Tiger.510010234Horizon5106(d) Recycling Robots.1001 2 3 4 5 6 7 8 9Horizon1510TreeVectorHybrid101051001 2 3 4 5 6 7 8 9 10HorizonTreeVectorHybrid(c) Hotel 1.15TreeVectorHybridMemory requiredMemory required101011020(b) FireFighting.151010TreeVectorHybrid1010Memory requiredMemory required10TreeVectorHybridMemory required1010TreeVectorHybrid101051001 2 3 4 5 6 7 8 9 10Horizon(e) BroadcastChannel.101234Horizon56(f) GridSmall.Figure 9: Hybrid heuristic representation. y-axis shows number real numbers storeddifferent representations QBG several benchmark problems (in log scale).5.3 Comparing Methodssection, compare GMAA*-IC GMAA*-ICE methods literature. begin comparing runtimes methods following state-ofthe-art optimal Dec-POMDP methods: MILP20 (Aras & Dutech, 2010) converts DecPOMDP mixed integer linear program, numerous solvers available.used MOSEK version 6.0. DP-LPC21 (Boularias & Chaib-draa, 2008) performs dynamic programming lossless policy compression, CPLEX 12.4 LP solver.DP-IPG (Amato et al., 2009) performs exact dynamic programing incremental policy20. results reported deviate reported Aras Dutech (2010). numberproblems, Aras et al. employed solution method solves MILP series (a tree) smallerMILPs branching continuous realization weight variables earlier stages. is, pastjoint policy stage t, solve different MILP involving subset consistent sequences.Additionally, FireFighting GridSmall, use benchmark versions standard literature(Oliehoek, Spaan, & Vlassis, 2008; Amato et al., 2006), whereas Aras Dutech (2010) use non-standardversions. explains difference results ones reported article (personalcommunication, Raghav Aras).21. goal Boularias Chaib-draa (2008) find non-dominated joint policies initial beliefs.previously reported results concerned run-time compute non-dominated joint policies, withoutperforming pruning full-length joint policies. contrast, report time needed computeactual optimal Dec-POMDP policy (given b0 ). additionally requires final round pruningsubsequently computing value remaining joint policies initial belief. additional overhead explains differences run time report previouslyreported (personal communication, Abdeslam Boularias).487fiOliehoek, Spaan, Amato, & WhitesonProblemDec-TigerCooperative Box PushingGridSmallh635733VMBDP9.9153.042.32V10.3866.082.97Table 5: Comparison optimal (V ) approximate (VMBDP ) values.generation exploits known start state knowledge states reachableDP backup.Table 4, shows results comparison, demonstrates that, almost cases,total time GMAA*-ICE (given sum heuristic computation time timeGMAA*-phase) significantly less state-of-the-art methods.Moreover, demonstrated Table 2, GMAA*-ICE compute solutions longer horizons problems, except Cooperative Box Pushing Hotel 1.22problems, possible compute QBG longer horizons. Overcomingproblem could enable GMAA*-ICE scale horizons well.DP-LPC algorithm proposed Boularias Chaib-draa (2008) also improvesefficiency optimal solutions form compression. performance algorithm,however, weaker GMAA*-IC. two main explanations performance difference. First, DP-LPC uses compression compactly represent valuessets useful sub-tree policies, using sequence form representation. policies themselves, however, compressed: still specify actions every possible observationhistory (for policy needs select exponential amount sequences makepolicy). Hence, cannot compute solutions long horizons. Second, GMAA*-ICexploit knowledge initial state distribution b0 .Overall, GMAA*-ICE substantially improves state-of-the-art optimally solvingDec-POMDPs. Previous methods typically improved feasible solution horizonone (or provided speed-ups horizons could already solved). contrast,GMAA*-ICE dramatically extends feasible solution horizon many problems.also consider MBDP-based approaches, leading family approximate algorithms.Table 5, reports VMBDP values produced PBIP-IPG (Amato et al., 2009) (withtypical maxTrees parameter setting m), demonstrates optimal solutions producedGMAA*-IC GMAA*-ICE higher quality. PBIP-IPG chosenMBDP algorithms parameters achieve value.exhaustive, comparison illustrates even best approximate Dec-POMDP methodspractice provide inferior joint policies problems. Conducting analysispossible optimal solutions computed. Clearly, data becomesavailable, thorough comparisons made. Therefore, scalable optimalsolution methods GMAA*-ICE critical improving analyses.488fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPsproblem primitivesnum. hn|S||A||O|24622744641.07e98.50e37381885123.51e137.84e56424316164.09e31.15e187.23e75572932323.27e43.77e226.67e946218764642.62e59.80e556.15e113Table 6: FireFightingGraph: number joint policies different numbers agentshorizons, 3 possible fire levels.5.4 Scaling Agentsbenchmark problems results presented far limited two agents. Here,present case study FireFightingGraph (Oliehoek, Spaan, Whiteson, & Vlassis,2008), variation FireFighting allowing agents, agentfight fires two houses, instead them. Table 6 highlights sizeproblems, including total number joint policies different horizons. comparedGMAA*, GMAA*-IC, GMAA*-ICE (all using QMDP heuristic), BruteForceSearch,DP-IPG, maximum run-time 12 hours running Intel Core i7 CPU,averaged 10 runs. BruteForceSearch simple optimal algorithm enumeratesevaluates joint policies, implemented codebase GMAA*variations. DP-IPG results use original implementation run Intel Xeoncomputer. Hence, timing results directly comparable, overall trendsapparent. Also, since DP-IPG implementation limited 2 agents, results shownagents.Fig. 10 shows computation times FireFightingGraph across different numbersagents planning horizons, Table 7 lists optimal values obtained. expected,baseline BruteForceSearch performs poorly, scaling beyond h = 2 2agents, DP-IPG reach h = 4. hand, regular GMAA* performsrelatively well, scaling maximum 5 agents. However, GMAA*-IC GMAA*-ICEimprove efficiency GMAA* 12 orders magnitude. such, substantiallyoutperform three methods, scale 6 agents. benefit incremental expansion clear n = 3,4, GMAA*-ICE reach higher horizon GMAA*-IC.Hence, although article focuses scalability horizon, results showmethods propose also improve scalability number agents.5.5 DiscussionOverall, empirical results demonstrate incremental clustering expansion offersdramatic performance gains diverse set problems. addition, results Broad22. Hotel 1, DP-IPG performs particularly well problem structure limited reachability.is, agent fully observe local state (but agent) local statesexcept one one action dominates others. result, DP-IPG generate small numberpossibly optimal policies.489fiOliehoek, Spaan, Amato, & Whiteson4410103210110010110210310654322345678910310computation time (s)computation time (s)computation time (s)410310210110010110210310654h#agents3223547689101021011001011021031065h#agents(a) GMAA* results.32234578910h#agents(b) GMAA*-IC results.4(c) GMAA*-ICE results.41010310210110010110210310654322345678910computation time (s)3computation time (s)46102101100101102103106h#agents54322345678910h#agents(d) BruteForceSearch results.(e) DP-IPG results.Figure 10: Comparison GMAA*, GMAA*-IC, GMAA*-ICE, BruteForceSearch,DP-IPG FireFightingGraph problem. Shown computation time (in logscale) various number agents horizons. Missing bars indicate methodexceeded time memory limits. However, DP-IPG implementation supports 2agents.h23456n=24.3942525.8063546.6265557.0939757.196444n=35.2136856.6545517.472568n=46.0273197.3914238.000277n=56.846752n=67.666185Table 7: Value V optimal solutions FireFightingGraph problem, differenthorizons numbers agents.castChannel illustrate key advantage approach: problem possesses property makes large amount clustering possible, clustering method exploitsproperty automatically, without requiring predefined explicit representation it.490fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPscourse, problems admit great reductions via clustering. One domain propertyallows clustering past joint policy encountered GMAA* makesobservations superfluous, BroadcastChannel FireFighting. Dec-Tiger,see certain symmetries lead clustering. However clustering occur evenwithout properties. fact, problems nearly horizons tested,size CBGs reduced. Moreover, accordance analysis Section 3.2,improvements planning efficiency huge, even modest reductions CBG size.One class problems say something priori amount clusteringpossible class Dec-POMDPs transition observation independence(Becker et al., 2003). problems, agents local states transitionsindependent, two agents expressedPr(s1 , s2 |s1 , s2 , a1 , a2 ) = Pr(s1 |s1 , a1 ) Pr(s2 |s2 , a2 ).(5.1)Similarly, observations assumed independent, means agentobservation probability depends action local state: Pr(oi |ai , si ).problems, probabilistic equivalence criterion (3.1) factors too. particular, duetransition observation independence23 , (3.2) holds true ~ia ,~ib . Moreover, (3.3)factors product Pr(s1 , s2 |~1 , ~2 ) = Pr(s1 |~1 ) Pr(s2 |~2 ) thus holds Pr(s1 |~1a ) =Pr(s1 |~1b ). is, two histories clustered induce local belief.such, size CBGs directly corresponds product number reachable localbeliefs. Since transition observation independent Hotel 1 problem also locallyfully observable, local state spaces consist four states, four possiblelocal beliefs (which consistent CBG size 16 Table 3). Moreover, seemaximum size typically reached end search. goodpolicies defer sending customers hotel thus visit local states hotelfilled earlier stages.general classes problems, even weakly coupled models (e.g., Becker,Zilberstein, & Lesser, 2004; Witwicki & Durfee, 2010), criterion (3.1) factor,hence direct correspondence number local beliefs. such,applying clustering algorithm determine well problem clusters.analogous to, e.g., state aggregation MDPs (e.g., discussed Givan, Dean, &Greig, 2003) known predict priori large minimized modelbe. Fortunately, empirical results demonstrate that, domains admit littleclustering, overhead small.expected, incremental expansion helpful problems allowmuch clustering. However, results for, e.g., Dec-Tiger illustrate limitamount scaling method currently provide. bottleneck solutionlarge CBGs later stages: CBG solver solve large CBGsreturning first solution order guarantee optimality, takes takes long time.expect improvements CBG solvers directly add efficacyincremental expansion.experiments also clearly demonstrate Dec-POMDP complexity results,important, worst-case results. fact, scalability demonstrated experimentsclearly show many problems successfully scale dramatically beyond would23. assumes external state variable s0 .491fiOliehoek, Spaan, Amato, & Whitesonexpected doubly-exponential dependence horizon. Even smallest problems,doubly-exponential scaling horizon implies impossible compute solutionsbeyond h = 4 all, indicated following simple calculation: let n = 2, |Ai | = 2actions, |Oi | = 2| observations,5|Ai |(n(|Oi |))4/|Ai |(n(|Oi |))= 4.2950e9.Thus, even simplest possible case, see increase factor 4.2950e09 h = 4h = 5. Similarly, next increment, h = 5 h = 6, increases size searchspace factor 1.8447e19. However, experiments clearly indicate almostcases, things dire. is, even though matters look bleak lightcomplexity results, many cases able perform substantially better worstcase.6. Related Worksection, discuss number methods related proposedarticle. methods already discussed earlier sections. Section 3,indicated clustering method closely related approach Emery-Montemerloet al. (2005) also fundamentally different method lossless. Section 5.3,discussed connections approach Boularias Chaib-draa (2008) clusterspolicy values. contrasts approach clusters histories thuspolicies themselves, leading greater scalability.Section 3.1.2, discussed relationship notion probabilistic equivalence (PE) multiagent belief. However, yet another notion belief, employedJESP solution method (Nair et al., 2003), superficially similar PEdistribution. JESP belief AOH ~i probability distribution Pr(s,~o6=i |~i , b0 , 6=i )states observation histories agents given (deterministic) full policyagents. sufficient statistic, since induces multiagent belief, thus alsoallows clustering histories. crucial difference with, utility of, PE liesfact PE criterion specified states AOHs given past joint policy.is, (3.1) induce multiagent belief.clustering approach also resembles number methods employ equivalencenotions. First, several approaches exploit notion behavioral equivalence (Pynadath &Marsella, 2007; Zeng et al., 2011; Zeng & Doshi, 2012). consider, perspectiveprotagonist agent i, possible models another agent j. Since j affectsactions, i.e., behavior, agent cluster together models agent j leadpolicy j agent. is, cluster models agent jbehaviorally equivalent. contrast, cluster models agents j, historiesagent agents, well environment, guaranteed behaveexpectation, thus leading best response agent i. is, methodcould seen clustering histories expected environmental behavior equivalent.notion utility equivalence (Pynadath & Marsella, 2007; Zeng et al., 2011) closerPE also takes account (value the) best-response agent (in particular,clusters two models mj mj using BR(mj )the best response mj achievesvalue mj ). However, remains form behavior equivalenceclusters models agents, histories protagonist agent.492fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPsalso connections PE work influence-based abstraction (Becker etal., 2003; Witwicki & Durfee, 2010; Witwicki, 2011; Oliehoek et al., 2012), since influence(or point parameter space, Becker et al., 2003) compact representationagents policies. Models agents clustered lead influenceagent i. However, though fine-grained, ultimately still form behavioralequivalence.final relation equivalence notion work Dekel, Fudenberg, Morris(2006), constructs distance measure topology space typesgoal approximating infinite universal type space (the space possible beliefsbeliefs beliefs, etc.) one-shot Bayesian games. setting, however, considerssimple finite type space types directly correspond private histories (inform AOHs) sequential problem. Thus, need approximate universaltype space; instead want know histories lead future dynamicsperspective agent. Dekel et al.s topology address question.incremental expansion technique related approaches extending deallarge branching factors context multiple sequence alignment (Ikeda & Imai, 1999;Yoshizumi, Miura, & Ishida, 2000). However, approach differentdiscard unpromising nodes rather provide mechanism generate necessaryones. Also, proposing MAA*, Szer et al. (2005) developed superficially similar approach could applied last stage. particular, proposed generatingchild nodes one one, time checking child found value equalparents heuristic value. Since value child specifies full policy, valuelower bound therefore expansion remaining child nodes skipped. Unfortunately, number issues prevent approach providing substantial leveragepractice. First, cannot applied intermediate stages 0 < h 1 since lower boundvalues expanded children available. Second, many problems unlikelychild node exists. Third, even does, Szer et al. specify efficient wayfinding it. Incremental expansion overcomes issues, yielding approach that,experiments demonstrate, significantly increases size Dec-POMDPssolved optimally.article focuses optimal solutions Dec-POMDPs finite horizon. partevaluation, compare MILP approach (Aras & Dutech, 2010), DPILP (Boularias & Chaib-draa, 2008) DP-IPG (Amato et al., 2009), extensionexact dynamic programming algorithm (Hansen et al., 2004). Research finite-horizon DecPOMDPs considered many approaches bounded approximations (Amato,Carlin, & Zilberstein, 2007), locally optimal solutions (Nair et al., 2003; Varakantham, Nair,Tambe, & Yokoo, 2006) approximate methods without guarantees (Seuken & Zilberstein,2007b, 2007a; Carlin & Zilberstein, 2008; Eker & Akn, 2010; Oliehoek, Kooi, & Vlassis, 2008;Dibangoye et al., 2009; Kumar & Zilberstein, 2010b; Wu et al., 2010a; Wu, Zilberstein, &Chen, 2010b).particular, much research considered optimal and/or approximate solutionsubclasses Dec-POMDPs. One subclass contains Dec-POMDPsagents local states agents cannot influence. resulting models,TOI-Dec-MDP (Becker et al., 2003; Dibangoye, Amato, Doniec, & Charpillet, 2013) NDPOMDP (Nair et al., 2005; Varakantham et al., 2007; Marecki, Gupta, Varakantham, Tambe,& Yokoo, 2008; Kumar & Zilberstein, 2009), interpreted independent (PO)MDPs493fiOliehoek, Spaan, Amato, & Whitesonagent coupled reward function (and possibly unaffectable statefeature). hand, event-driven interaction models (Becker et al., 2004) consideragents individual rewards influence others transitions.recently, models allow limited transition reward dependenceintroduced. Examples interaction-driven Markov games (Spaan & Melo, 2008), DecMDPs sparse interactions (Melo & Veloso, 2011), distributed POMDPs coordination locales (Varakantham et al., 2009; Velagapudi et al., 2011), event-driven interactionscomplex rewards (EDI-CR) (Mostafa & Lesser, 2011), transition decoupled Dec-POMDPs(Witwicki & Durfee, 2010; Witwicki, 2011). methods developed models often exhibit better scaling behavior methods standard Dec-(PO)MDPs, typicallysuitable agents extended interactions, e.g., collaborate transportingitem. Also, specialized models consider timing actions whoseordering already determined (Marecki & Tambe, 2007; Beynier & Mouaddib, 2011).Another body work addresses infinite-horizon problems (Amato, Bernstein, & Zilberstein, 2010; Amato, Bonet, & Zilberstein, 2010; Bernstein, Amato, Hansen, & Zilberstein,2009; Kumar & Zilberstein, 2010a; Pajarinen & Peltonen, 2011), possiblerepresent policy tree. approaches represent policies using finite-state controllersoptimized various ways. Also, since infinite-horizon case undecidable(Bernstein et al., 2002), approaches approximate optimal given particular controller size. exists boundedly optimal approach theoretically constructcontroller within optimal, feasible small problems large(Bernstein et al., 2009).also great interest Dec-POMDPs explicitly take account communication. approaches try optimize meaning communication actions withoutsemantics (Xuan, Lesser, & Zilberstein, 2001; Goldman & Zilberstein, 2003; Spaan, Gordon,& Vlassis, 2006; Goldman, Allen, & Zilberstein, 2007) others use fixed semantics (e.g.,broadcasting local observations) (Ooi & Wornell, 1996; Pynadath & Tambe, 2002; Nair etal., 2004; Roth et al., 2005; Oliehoek, Spaan, & Vlassis, 2007; Roth, Simmons, & Veloso, 2007;Spaan, Oliehoek, & Vlassis, 2008; Goldman & Zilberstein, 2008; Becker, Carlin, Lesser, & Zilberstein, 2009; Williamson, Gerding, & Jennings, 2009; Wu et al., 2011). Since models usedfirst category (e.g., Dec-POMDP-Com) converted normal Dec-POMDPs(Seuken & Zilberstein, 2008), contributions article applicable settings.Finally, numerous models closely related Dec-POMDPs, POSGs(Hansen et al., 2004), interactive POMDPs (I-POMDPs) (Gmytrasiewicz & Doshi, 2005),graphical counterparts (Doshi, Zeng, & Chen, 2008). models general sense consider self-interested settings agent individualreward function. I-POMDPs conjectured also require doubly exponential time (Seuken& Zilberstein, 2008). However, I-POMDP number recent advances(Doshi & Gmytrasiewicz, 2009). current paper makes clear link best-responseequivalence histories notion best-response equivalence beliefs I-POMDPs.particular, article demonstrates two PE action-observation histories (AOHs) induce, given past joint policy, distribution states AOHs agents,therefore induce multiagent belief future policies agents.induced multiagent beliefs, turn, interpreted special cases I-POMDP beliefsmodel agents sub-intentional models form fixed policytree. Rabinovich Rosenschein (2005) introduced method that, rather optimizing494fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPsexpected value joint policy, selects coordinated actions uncertainty trackingdynamics environment. approach, however, requires model ideal systemdynamics input many problems, considered article, identifyingdynamics difficult.7. Future WorkSeveral avenues future work made possible research presented article.Perhaps promising development new approximate Dec-POMDP algorithms.article focused optimal methods, GMAA*-ICE also seen framework approximate methods. methods could derived limiting amountbacktracking, employing approximate CBG solvers (Emery-Montemerlo, Gordon, Schneider,& Thrun, 2004; Kumar & Zilberstein, 2010b; Wu et al., 2010a), integrating GMAA* methods factored Dec-POMDPs (Oliehoek, Spaan, Whiteson, & Vlassis, 2008; Oliehoek, 2010;Oliehoek et al., 2013), performing lossy clustering (Emery-Montemerlo, 2005; Wu et al., 2011)using bounded approximations heuristics. particular, seems promising combine approximate clustering approximate factored GMAA* methods.Lossy clustering could achieved generalizing probabilistic equivalence criterion,currently strict little clustering may possible many problems.obvious approach cluster histories distributions states historiesagents merely similar, measured by, e.g., Kullback-Leibler divergence. Alternately,histories could clustered induce individual belief states:Pr(s|~i ) =XPr(s,~6=i |~i ).(7.1)~6=iindividual beliefs sufficient statistics history, hypothesizeconstitute effective metrics approximate clustering. Since individual belief simplymarginalizes agents histories probabilities used probabilisticequivalence criterion, intuitive heuristic metric approximate clustering.article focuses increasing scalability respect horizon, developingtechniques deal larger number agents important direction future work.plan explore performing GMAA* using factored representations (Oliehoek, Spaan,Whiteson, & Vlassis, 2008). previous work, could exploit factorizationlast stage, since earlier stages required full expansions guarantee optimality. However,larger problems, number joint BG policies (i.e., number child nodes)directly large (earlier stages tightly coupled); therefore incremental expansioncrucial improving scalability optimal solution methods respect numberagents.Another avenue future work generalize GMAA*-ICE. particular,may possible flatten two nested searches single search.could lead significant savings would obviate need solve entire CBGexpanding next one. work, employed plain algorithm basis,promising direction future work investigate enhancements literature(Edelkamp & Schrodl, 2012) benefit GMAA* most. particular, describedexperiments, different past joint policies lead CBGs different sizes. One idea495fiOliehoek, Spaan, Amato, & Whitesonfirst expand parts search tree lead small CBGs, biasing selectionoperator (but pruning operator, maintain optimality).Yet another important direction future work development tighter heuristics.Though researchers addressing topic, results presented article underscore important heuristics solving larger problems. Currently, heuristicbottleneck four seven problems considered. Moreover, twoproblems bottleneck already solved long (h > 50) horizons.Therefore, believe computing tight heuristics longer horizons singleimportant research direction improving scalability optimal Dec-POMDPsolution methods respect horizon.different direction employ theoretical results clustering beyond DecPOMDP setting develop new solution methods CBGs. instance, well-knownmethod computing local optimum alternating maximization (AM): startingarbitrary joint policy, compute best response agent given agents keeppolicies fixed select another agents policy improve, etc. One idea startcompletely clustered CBG, agents types clustered together thusrandom joint CBG policy simple form: agent selects single action.improving policy agent consider actual possible types computebest response. Subsequently, cluster together types agent selectsaction proceed next agent. addition, since clustering resultsrestricted collaborative setting, may also possible employ them, using similarapproach, develop new solution methods general-payoff BGs.Finally, two contributions significant impact beyond problemoptimally solving Dec-POMDPs. First, idea incrementally expanding nodes introducedGMAA*-ICE applied search methods. Incremental expansionuseful children generated order decreasing heuristic value without prohibitivecomputational effort, problems large branching factor multiple sequencealignment problems computational biology (Carrillo & Lipman, 1988; Ikeda & Imai, 1999).Second, representing PWLC value functions hybrid tree set vectorswider impact well, e.g., online search POMDPs (Ross, Pineau, Paquet, & Chaib-draa,2008).8. Conclusionsarticle presented set methods advance state-of-the-art optimal solutionmethods Dec-POMDPs. particular, presented several advances aim extendhorizon optimal solutions found. advances build GMAA*heuristic search approach include lossless incremental clustering CBGs solvedGMAA*, incremental expansion nodes GMAA* search tree, hybrid heuristicrepresentations. provided theoretical guarantees that, suitable heuristic used,incremental clustering incremental expansion yield algorithms complete search equivalent. Finally, presented extensive empirical results demonstratingGMAA*-ICE optimally solve Dec-POMDPs unprecedented size. significantyincrease planning horizons tackledin cases ordermagnitude. Given increase horizon one results exponentially largersearch space, constitutes large improvement. Moreover, techniques also im496fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPsprove scalability respect number agents, leading first ever solutionsgeneral Dec-POMDPs three agents. results also demonstratedoptimal techniques yield new insights particular Dec-POMDPs, incrementalclustering revealed properties BroadcastChannel make much easier solve.addition facilitating optimal solutions, hope advances inspire new principledapproximation methods, incremental clustering already done (Wu et al., 2011),enable meaningfully benchmarked.Acknowledgmentsthank Raghav Aras Abdeslam Boularias making code available us. Research supported part AFOSR MURI project #FA9550-09-1-0538 part NWOCATCH project #640.005.003. M.S. funded FP7 Marie Curie Actions IndividualFellowship #275217 (FP7-PEOPLE-2010-IEF).Appendix A. AppendixA.1 Auxiliary algorithmsAlgorithm 12 implements BestJointPolicyAndValue function, prunes childnodes fully specified. Algorithm 13 generates children particular CBG.Algorithm 12 BestJointPolicyAndValue(QExpand ): Prune fully expanded nodes setnodes QExpand returning best one value.Input: QExpand set nodes fully specified joint policies.Output: best full joint policy input set value.1: v =2: q QExpand3:QExpand .Remove(q)4:h, vi q5:v > v6:v v7:8:end9: end10: return h , vA.2 Detailed GMAA*-ICE algorithmcomplete GMAA*-ICE algorithm shown Algorithm 14.A.3 Computation V 0...t1 (t )quantity V 0...t1 (t ) defined recursively via:V 0...t1 (t ) = V 0...t2 (t1 ) + Est1 ,~t1 [R(st1 , t1 (~ t1 )) | b0 , ].497(A.1)fiOliehoek, Spaan, Amato, & WhitesonAlgorithm 13 GenerateAllChildrenForCBG(B(t )).Input: CBG B(t ).Output: QExpand set containing expanded child nodes CBG.1: QExpand {}2: jointPCBG policies B3:Vb () Pr()u(,())4:t+1 (t , ){create partial joint policy}t+10...t1bb5:V ( ) V( ) + V ( ){compute heuristic value}6:q ht+1 , Vb (t+1 )i{create child node}7:QExpand .Insert(q )8: end9: return QExpandexpectation taken respect joint probability distribution statesjoint AOHs induced :XPr(st ,~ |b0 ,t ) =Pr(ot |at1 ,st ) Pr(st |st1 ,at1 ) Pr(at1 |t ,~ t1 ) Pr(st1 ,~ t1 |b0 ,t ).st1(A.2)Here, ~ = (~ t1 ,at1 ,ot ) Pr(at1 |t ,~ t1 ) probability specifies at1AOH ~ t1 (which 0 1 case deterministic past joint policy ).A.4 ProofsProof Theorem 1Substituting (2.9) (2.7) yieldsXb ~ , (~ ))Vb () = Vb ( ) =Pr(~ |b0 ,t )Q(~=X~Pr(~ |b0 ,t ) Est [R(st , (~ )) | ~ ] + E~t+1 [Vb (~ t+1 ) | ~ , (~ )]= Est ,~t [R(st , (~ ) | b0 , ] + E~t+1 [Vb (~ t+1 ) | b0 , , ]Est ,~t [R(st , (~ ) | b0 , ] + E~t+1 [Q (~ t+1 , (~ t+1 )) | b0 , t+1 = (t , )]= Est ,~t [R(st , (~ ) | b0 , ] + H ,t+1...h1 (t+1 ),H optimal admissible heuristic. Substituting (2.8) obtainVb (t+1 = (t , )) = V 0...t1 (t ) + Est ,~t [R(st , (~ ) | b0 , ] + E~t+1 [Vb (~ t+1 ) | b0 , , ]V 0...t1 (t ) + Est ,~t [R(st , (~ )) | b0 , t+1 ] + H ,t+1...h1 (t+1 ){via (A.1)} = V 0...t (t+1 ) + H ,t+1...h1 (t+1 ),demonstrates heuristic value Vb (t ) used GMAA* via CBGs using heuristicform (2.9) admissible, lower bounded actual value first plusadmissible heuristic. Since performs heuristic search admissible heuristic,algorithm also complete.498fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPsAlgorithm 14 GMAA*-ICE1:2:3:4:5:6:7:8:9:10:11:12:13:14:15:16:17:18:19:20:21:22:23:24:25:26:27:28:29:30:31:32:33:34:35:36:37:38:39:40:41:42:43:44:45:46:47:vGM AA0 ()v +q 0 h0 , viLIE {q 0 }repeatq Select(LIE ){q = ht , vi}IEL .pop(q)IsPlaceholder(q)B(t ) .CBG{reuse stored CBG}else{Construct extended BG solver:}B(t1 ) t1 .CBG{note = (t1 , t1 )}t1t1B( ) ConstructExtendedBG(B( ), )B(t ) ClusterBG(B(t ))B(t ).Solver CreateSolver(B(t )).CBG B(t )end{Expand single child:}vCBG = vGM AA V 0...(t1) (t )vCBG = +last stage = h 1vCBG = Vb (h1 ) V 0...(h2) (h1 )endh , Vb ( )i B(t ).Solver.NextSolution(vCBG ,vCBG ){fully expanded: solution s.t. V ( h1 ) vCBG }delete q (and CBG + solver)continue{(i.e. goto line 8)}endt+1 (t , )Vb (t+1 ) V 0...t1 (t ) + Vb ( )last stage = h 1{Note = t+1 , V () = Vb (t+1 ) }V () > vGM AAvGM AA V (){found new lower bound}LIE .prune(vGM AA )enddelete q (and CBG + solver)elseq ht+1 , Vb (t+1 )iLIE .insert(q )q ht , Vb (t+1 )i{ Update parent node q, placeholder }LIE .insert(q)endLIE empty499fiOliehoek, Spaan, Amato, & WhitesonProof Lemma 1t+1t+1 ~Proof. Assume arbitrary ati ,ot+16=i ,at6=i ,ot+16=i = (~, 6=i ,s6=i )).t+1~ a,tPr(st+1 ,~6=i ,ot+1|i ,ai , 6=i )Xt+1t+16=i |~ia,t )Pr(ot+1) Pr(st+1 |st ,ati ,at6=i ) Pr(at6=i |~6=i , t6=i ) Pr(st ,~=,o6=i |ai ,a6=i ,sst=Xt+1t+16=i |~ib,t )) Pr(st+1 |st ,ati ,at6=i ) Pr(at6=i |~6=i , t6=i ) Pr(st ,~Pr(ot+1,o6=i |ai ,a6=i ,sstt+1~ b,t= Pr(st+1 ,~6=i ,ot+1|i ,ai , 6=i )t+1assumed arbitrary st+1 ,~6=i ,ot+1,st+1 ,~t+1 ,ot+16=it+1t+1 ~ t+1 t+1 ~ b,t~ a,t, 6=i ,oi |i ,ai , 6=i )Pr(st+1 ,~6=i ,ot+1|i ,ai , 6=i ) = Pr(s(A.3)generalt+1Pr(st+1t+1,~6=i |~it ,ati ,ot+1, 6=i ) =~tPr(st+1 ,~6=i ,ot+1|i ,ai , 6=i )Pr(ot+1 |~ ,at , )=6=it+1~tPr(st+1 ,~6=i ,ot+1|i ,ai , 6=i )Pt+1 t+1t+1 ,~~t+1 Pr(s6=i ,oi |i ,ai , 6=i )t+1~,6=iNow, (A.3), numerator denominator substituting~ia,t ,~ib,t equation. Consequently, concludet+1t+1 ~ t+1 ~ b,t t+1, 6=i |i ,ai ,oi , 6=i )Pr(st+1 ,~6=i |~ia,t ,ati ,ot+1, 6=i ) = Pr(st+1t+1 , ~Finally, ati , ot+16=i arbitrarily chosen, conclude, 6=i ,s(3.4) holds.Proof Lemma 2Proof. Assume arbitrary 6=i ,s 6=i ,bi (s, 6=i |~ia , 6=i ) , Pr(s, 6=i |~ia , 6=i ,b0 )X=Pr(s, 6=i ,~6=i |~ia , 6=i ,b0 )~6=i{factoring joint distribution}=XPr(s,~6=i |~ia , 6=i ,b0 ) Pr( 6=i |s,~6=i , ~ia , 6=i ,b0 )~6=i500fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPs{ 6=i depends ~6=i , 6=i } =XPr(s,~6=i |~ia , 6=i ,b0 ) Pr( 6=i |~6=i , 6=i ){ s,~6=i depend 6=i } =XPr(s,~6=i |~ia ,6=i ,b0 ) Pr( 6=i |~6=i , 6=i ){due PE} =XPr(s,~6=i |~ib ,6=i ,b0 ) Pr( 6=i |~6=i , 6=i )~6=i~6=i~6=i= [...] = Pr(s, 6=i |~ib , 6=i ,b0 ) = bi (s, 6=i |~ib , 6=i )conclude holds 6=i ,s 6=i .Proof Theorem 5 (Search Equivalence)prove search equivalence, explicitly write node tuple q = ht , v, PHi,past joint policy, v nodes heuristic value, PH boolean indicating whetherplaceholder. consider equivalence maintained open lists. open listL maintained GMAA*-IC contains non-expanded nodes q. contrast, openlist LIE GMAA*-ICE contains non-expanded nodes q placeholders (previouslyexpanded nodes), q. denote ordered subset LIE containing non-expanded nodesQ containing placeholders Q. treat open lists ordered setsheuristic values associated nodes.Definition 11. L LIE equivalent, L LIE if:1. Q L.2. qs ordering: L.remove(L \ Q) = Q.243. Nodes q L Q placeholder q parent higher rankedq:q=ht ,vq ,falsei(L\Q)q=ht1 ,vq ,trueiQ s.t. (t = (t1 , ) q < q).4. placeholders.Fig. 11 illustrates two equivalent lists past joint policies indexed letters.Note placeholders LIE ranked higher nodes L represent.Let us write IT-IC(L) IT-ICE(LIE ) one iteration (i.e., one loop main repeatAlgorithm 1) respective algorithms. Let IT-ICE* denote operation repeatsIT-ICE long placeholder selected (so ends q expanded).Lemma 4. L LIE , executing IT-IC(L) IT-ICE*(LIE ) leads new open listsequivalent: L LIE .Proof. IT-ICE* selects placeholder q, generates child q already presentL (due properties 3 4 Definition 11) inserts it. Insertion occursrelative location IT-IC algorithms use comparison operator(Definition 5). Together facts guarantee insertion preserves properties 1 2.24. A.remove(B) removes elements B without changing ordering.501fiOliehoek, Spaan, Amato, & WhitesonLIELQVb754.5ce332.510.5fghjQVb533fgVb84bplaceholder {c,e,j}nodes: positionplaceholder {h,i}consistent orderingequal valuesFigure 11: Illustration equivalent lists. Past joint policies indexed letters.example, b expanded earlier (but yet fully expanded ICE-case).remaining unexpanded children q, IT-ICE* reinserts q updated heuristicvalue q.v q .v guaranteed upper bound value unexpanded siblingsq since q .v = Vb (q .) Vb (q .) = q .v (preserving properties 3 4).IT-ICE* finally selects non-placeholder q, guaranteed qselected IT-IC (due properties 1 2). Expansion ICE generates one child q (againinserted relative location IC) inserts placeholder q = hq., q .v, trueisiblings q (again preserving properties 3 4).Proof Theorem 5. fact GMAA*-ICE GMAA*-IC search-equivalent follows directly Lemma 4. Search equivalence means algorithms selectnon-placeholders q expand. Since algorithms begin identical (and therefore trivially equivalent) open lists, maintain equivalent open lists throughout search. such,property 2 Definition 11 ensures every time IT-ICE* selects non-placeholder, IT-ICselects too.ReferencesAllen, M., & Zilberstein, S. (2007). Agent influence predictor difficulty decentralizedproblem-solving. Proceedings Twenty-Second AAAI Conference ArtificialIntelligence.Amato, C., Bernstein, D. S., & Zilberstein, S. (2006). Optimal fixed-size controllersdecentralized POMDPs. Proc. AAMAS Workshop Multi-Agent SequentialDecision Making Uncertain Domains.Amato, C., Bernstein, D. S., & Zilberstein, S. (2007). Optimizing memory-bounded controllersdecentralized POMDPs. Proc. Uncertainty Artificial Intelligence.Amato, C., Bernstein, D. S., & Zilberstein, S. (2010). Optimizing fixed-size stochastic controllers POMDPs decentralized POMDPs. Autonomous Agents Multi-AgentSystems, 21 (3), 293320.502fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPsAmato, C., Bonet, B., & Zilberstein, S. (2010). Finite-state controllers based Mealymachines centralized decentralized POMDPs. Proceedings TwentyFourth AAAI Conference Artificial Intelligence.Amato, C., Carlin, A., & Zilberstein, S. (2007). Bounded dynamic programming decentralized POMDPs. Proc. AAMAS Workshop Multi-Agent SequentialDecision Making Uncertain Domains.Amato, C., Dibangoye, J. S., & Zilberstein, S. (2009). Incremental policy generationfinite-horizon DEC-POMDPs. Proc. International Conference AutomatedPlanning Scheduling.Aras, R., & Dutech, A. (2010). investigation mathematical programming finitehorizon decentralized POMDPs. Journal Artificial Intelligence Research, 37 , 329396.Becker, R., Carlin, A., Lesser, V., & Zilberstein, S. (2009). Analyzing myopic approachesmulti-agent communication. Computational Intelligence, 25 (1), 3150.Becker, R., Zilberstein, S., & Lesser, V. (2004). Decentralized Markov decision processesevent-driven interactions. Proc. International Conference AutonomousAgents Multi Agent Systems.Becker, R., Zilberstein, S., Lesser, V., & Goldman, C. V. (2003). Transition-independentdecentralized Markov decision processes. Proc. International ConferenceAutonomous Agents Multi Agent Systems.Bernstein, D. S., Amato, C., Hansen, E. A., & Zilberstein, S. (2009). Policy iterationdecentralized control Markov decision processes. Journal Artificial IntelligenceResearch, 34 , 89132.Bernstein, D. S., Givan, R., Immerman, N., & Zilberstein, S. (2002). complexitydecentralized control Markov decision processes. Mathematics Operations Research,27 (4), 819840.Bertsekas, D. P. (2005). Dynamic Programming Optimal Control (3rd ed., Vol. I). AthenaScientific.Beynier, A., & Mouaddib, A.-I. (2011). Solving efficiently decentralized MDPs temporalresource constraints. Autonomous Agents Multi-Agent Systems, 23 (3), 486539.Boularias, A., & Chaib-draa, B. (2008). Exact dynamic programming decentralizedPOMDPs lossless policy compression. Proc. International ConferenceAutomated Planning Scheduling.Busoniu, L., Babuska, R., & De Schutter, B. (2008). comprehensive survey multi-agentreinforcement learning. IEEE Transactions Systems, Man, Cybernetics, Part C:Applications Reviews, 38 (2), 156172.Carlin, A., & Zilberstein, S. (2008). Value-based observation compression DEC-POMDPs.Proc. International Conference Autonomous Agents Multi Agent Systems.503fiOliehoek, Spaan, Amato, & WhitesonCarrillo, H., & Lipman, D. (1988). multiple sequence alignment problem biology.SIAM Journal Applied Mathematics, 48 (5), 10731082.Cassandra, A., Littman, M. L., & Zhang, N. L. (1997). Incremental pruning: simple, fast,exact method partially observable Markov decision processes. Proc. UncertaintyArtificial Intelligence.Cassandra, A. R. (1998). Exact Approximate Algorithms Partially Observable MarkovDecision Processes. Unpublished doctoral dissertation, Brown University.Dechter, R., Flerova, N., & Marinescu, R. (2012). Search algorithms best solutionsgraphical models. Proceedings Twenty-Sixth AAAI Conference ArtificialIntelligence.Dekel, E., Fudenberg, D., & Morris, S. (2006). Topologies types. Theoretical Economics,1 (3), 275309.Dibangoye, J. S., Amato, C., Doniec, A., & Charpillet, F. (2013). Producing efficient errorbounded solutions transition independent decentralized MDPs. Proc. International Conference Autonomous Agents Multi Agent Systems. (Submittedpublication)Dibangoye, J. S., Mouaddib, A.-I., & Chai-draa, B. (2009). Point-based incremental pruning heuristic solving finite-horizon DEC-POMDPs. Proc. InternationalConference Autonomous Agents Multi Agent Systems.Doshi, P., & Gmytrasiewicz, P. (2009). Monte Carlo sampling methods approximatinginteractive POMDPs. Journal Artificial Intelligence Research, 34 , 297337.Doshi, P., Zeng, Y., & Chen, Q. (2008). Graphical models interactive POMDPs: representations solutions. Autonomous Agents Multi-Agent Systems, 18 (3), 376416.Edelkamp, S., & Schrodl, S. (2012). Heuristic search: theory applications. MorganKaufmann.Eker, B., & Akn, H. L. (2010). Using evolution strategies solve DEC-POMDP problems.Soft ComputingA Fusion Foundations, Methodologies Applications, 14 (1), 3547.Eker, B., & Akn, H. L. (2013). Solving decentralized POMDP problems using geneticalgorithms. Autonomous Agents Multi-Agent Systems, 27 (1), 161196.Emery-Montemerlo, R. (2005). Game-Theoretic Control Robot Teams. Unpublisheddoctoral dissertation, Carnegie Mellon University.Emery-Montemerlo, R., Gordon, G., Schneider, J., & Thrun, S. (2004). Approximate solutions partially observable stochastic games common payoffs. Proc.International Conference Autonomous Agents Multi Agent Systems.Emery-Montemerlo, R., Gordon, G., Schneider, J., & Thrun, S. (2005). Game theoreticcontrol robot teams. Proc. IEEE International Conference RoboticsAutomation.Givan, R., Dean, T., & Greig, M. (2003). Equivalence notions model minimizationMarkov decision processes. Artificial Intelligence, 14 (12), 163223.504fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPsGmytrasiewicz, P. J., & Doshi, P. (2005). framework sequential planning multi-agentsettings. Journal Artificial Intelligence Research, 24 , 4979.Goldman, C. V., Allen, M., & Zilberstein, S. (2007). Learning communicate decentralized environment. Autonomous Agents Multi-Agent Systems, 15 (1), 4790.Goldman, C. V., & Zilberstein, S. (2003). Optimizing information exchange cooperativemulti-agent systems. Proc. International Conference Autonomous AgentsMulti Agent Systems.Goldman, C. V., & Zilberstein, S. (2004). Decentralized control cooperative systems:Categorization complexity analysis. Journal Artificial Intelligence Research, 22 ,143174.Goldman, C. V., & Zilberstein, S. (2008). Communication-based decomposition mechanismsdecentralized MDPs. Journal Artificial Intelligence Research, 32 , 169202.Hansen, E. A., Bernstein, D. S., & Zilberstein, S. (2004). Dynamic programming partially observable stochastic games. Proc. National Conference ArtificialIntelligence.Hauskrecht, M. (2000). Value-function approximations partially observable Markov decision processes. Journal Artificial Intelligence Research, 13 , 3394.Hsu, K., & Marcus, S. (1982). Decentralized control finite state Markov processes. IEEETransactions Automatic Control , 27 (2), 426431.Huhns, M. N. (Ed.). (1987). Distributed Artificial Intelligence. Pitman Publishing Ltd.Ikeda, T., & Imai, H. (1999). Enhanced A* algorithms multiple alignments: optimalalignments several sequences k-opt approximate alignments large cases. Theoretical Computer Science, 210 (2), 341374.Kaelbling, L. P., Littman, M. L., & Cassandra, A. R. (1998). Planning acting partiallyobservable stochastic domains. Artificial Intelligence, 101 (1-2), 99134.Kumar, A., & Zilberstein, S. (2009). Constraint-based dynamic programming decentralizedPOMDPs structured interactions. Proc. International ConferenceAutonomous Agents Multi Agent Systems.Kumar, A., & Zilberstein, S. (2010a). Anytime planning decentralized POMDPs usingexpectation maximization. Proc. Uncertainty Artificial Intelligence.Kumar, A., & Zilberstein, S. (2010b). Point-based backup decentralized POMDPs: Complexity new algorithms. Proc. International Conference AutonomousAgents Multi Agent Systems.Littman, M., Cassandra, A., & Kaelbling, L. (1995). Learning policies partially observable environments: Scaling up. Proc. International Conference MachineLearning.Marecki, J., Gupta, T., Varakantham, P., Tambe, M., & Yokoo, M. (2008). agentsequal: scaling distributed POMDPs agent networks. Proc. InternationalConference Autonomous Agents Multi Agent Systems.505fiOliehoek, Spaan, Amato, & WhitesonMarecki, J., & Tambe, M. (2007). opportunistic techniques solving decentralizedMarkov decision processes temporal constraints. Proc. InternationalConference Autonomous Agents Multi Agent Systems.Melo, F. S., & Veloso, M. (2011). Decentralized MDPs sparse interactions. ArtificialIntelligence, 175 (11), 17571789.Mostafa, H., & Lesser, V. (2011). compact mathematical formulation problemsstructured agent interactions. Proc. AAMAS Workshop Multi-Agent Sequential Decision Making Uncertain Domains.Nair, R., Roth, M., & Yohoo, M. (2004). Communication improving policy computationdistributed POMDPs. Proc. International Conference Autonomous AgentsMulti Agent Systems.Nair, R., Tambe, M., Yokoo, M., Pynadath, D. V., & Marsella, S. (2003). Taming decentralized POMDPs: Towards efficient policy computation multiagent settings. Proc.International Joint Conference Artificial Intelligence.Nair, R., Varakantham, P., Tambe, M., & Yokoo, M. (2005). Networked distributed POMDPs:synthesis distributed constraint optimization POMDPs. Proc. National Conference Artificial Intelligence.Oliehoek, F. A. (2010). Value-Based Planning Teams Agents Stochastic Partially Observable Environments. Amsterdam University Press. (Doctoral dissertation, UniversityAmsterdam)Oliehoek, F. A. (2012). Decentralized POMDPs. M. Wiering & M. van Otterlo (Eds.),Reinforcement learning: State art (Vol. 12). Springer Berlin Heidelberg.Oliehoek, F. A., Kooi, J. F., & Vlassis, N. (2008). cross-entropy method policy searchdecentralized POMDPs. Informatica, 32 , 341357.Oliehoek, F. A., & Spaan, M. T. J. (2012). Tree-based solution methods multiagentPOMDPs delayed communication. Proceedings Twenty-Sixth AAAI Conference Artificial Intelligence.Oliehoek, F. A., Spaan, M. T. J., Dibangoye, J., & Amato, C. (2010). Heuristic search identical payoff Bayesian games. Proc. International Conference AutonomousAgents Multi Agent Systems.Oliehoek, F. A., Spaan, M. T. J., & Vlassis, N. (2007). Dec-POMDPs delayed communication. Proc. AAMAS Workshop Multi-Agent Sequential Decision MakingUncertain Domains.Oliehoek, F. A., Spaan, M. T. J., & Vlassis, N. (2008). Optimal approximate Q-valuefunctions decentralized POMDPs. Journal Artificial Intelligence Research, 32 ,289353.Oliehoek, F. A., Spaan, M. T. J., Whiteson, S., & Vlassis, N. (2008). Exploiting localityinteraction factored Dec-POMDPs. Proc. International ConferenceAutonomous Agents Multi Agent Systems.Oliehoek, F. A., & Vlassis, N. (2007). Q-value functions decentralized POMDPs. Proc.International Conference Autonomous Agents Multi Agent Systems.506fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPsOliehoek, F. A., Whiteson, S., & Spaan, M. T. J. (2009). Lossless clustering historiesdecentralized POMDPs. Proc. International Conference AutonomousAgents Multi Agent Systems.Oliehoek, F. A., Whiteson, S., & Spaan, M. T. J. (2013). Approximate solutions factored Dec-POMDPs many agents. Proc. International ConferenceAutonomous Agents Multi Agent Systems. (Submitted publication)Oliehoek, F. A., Witwicki, S., & Kaelbling, L. P. (2012). Influence-based abstractionmultiagent systems. Proceedings Twenty-Sixth AAAI Conference ArtificialIntelligence.Ooi, J. M., & Wornell, G. W. (1996). Decentralized control multiple access broadcastchannel: Performance bounds. Proc. 35th conference decision control.Osborne, M. J., & Rubinstein, A. (1994). course game theory. MIT Press.Pajarinen, J., & Peltonen, J. (2011). Efficient planning factored infinite-horizon DECPOMDPs. Proc. International Joint Conference Artificial Intelligence.Panait, L., & Luke, S. (2005). Cooperative multi-agent learning: state art.Autonomous Agents Multi-Agent Systems, 11 (3), 387434.Puterman, M. L. (1994). Markov Decision ProcessesDiscrete Stochastic Dynamic Programming. John Wiley & Sons, Inc.Pynadath, D. V., & Marsella, S. C. (2007). Minimal mental models. ProceedingsTwenty-Second AAAI Conference Artificial Intelligence.Pynadath, D. V., & Tambe, M. (2002). communicative multiagent team decision problem:Analyzing teamwork theories models. Journal Artificial Intelligence Research,16 , 389423.Rabinovich, Z., Goldman, C. V., & Rosenschein, J. S. (2003). complexity multiagentsystems: price silence. Proc. International Conference AutonomousAgents Multi Agent Systems.Rabinovich, Z., & Rosenschein, J. S. (2005). Multiagent coordination extended Markovtracking. Proc. International Conference Autonomous Agents MultiAgent Systems.Ross, S., Pineau, J., Paquet, S., & Chaib-draa, B. (2008). Online planning algorithmsPOMDPs. Journal Artificial Intelligence Research, 32 , 664704.Roth, M., Simmons, R., & Veloso, M. (2005). Reasoning joint beliefs executiontime communication decisions. Proc. International Conference AutonomousAgents Multi Agent Systems.Roth, M., Simmons, R., & Veloso, M. (2007). Exploiting factored representations decentralized execution multi-agent teams. Proc. International ConferenceAutonomous Agents Multi Agent Systems.Seuken, S., & Zilberstein, S. (2007a). Improved memory-bounded dynamic programmingdecentralized POMDPs. Proc. Uncertainty Artificial Intelligence.507fiOliehoek, Spaan, Amato, & WhitesonSeuken, S., & Zilberstein, S. (2007b). Memory-bounded dynamic programming DECPOMDPs. Proc. International Joint Conference Artificial Intelligence.Seuken, S., & Zilberstein, S. (2008). Formal models algorithms decentralized decisionmaking uncertainty. Autonomous Agents Multi-Agent Systems, 17 (2), 190250.Spaan, M. T. J., Gordon, G. J., & Vlassis, N. (2006). Decentralized planning uncertainty teams communicating agents. Proc. International ConferenceAutonomous Agents Multi Agent Systems.Spaan, M. T. J., & Melo, F. S. (2008). Interaction-driven Markov games decentralizedmultiagent planning uncertainty. Proc. International ConferenceAutonomous Agents Multi Agent Systems.Spaan, M. T. J., & Oliehoek, F. A. (2008). MultiAgent Decision Process toolbox:software decision-theoretic planning multiagent systems. Proc. AAMASWorkshop Multi-Agent Sequential Decision Making Uncertain Domains.Spaan, M. T. J., Oliehoek, F. A., & Amato, C. (2011). Scaling optimal heuristic searchDec-POMDPs via incremental expansion. Proc. International Joint ConferenceArtificial Intelligence.Spaan, M. T. J., Oliehoek, F. A., & Vlassis, N. (2008). Multiagent planning uncertaintystochastic communication delays. Proc. International ConferenceAutomated Planning Scheduling.Sycara, K. P. (1998). Multiagent systems. AI Magazine, 19 (2), 7992.Szer, D., Charpillet, F., & Zilberstein, S. (2005). MAA*: heuristic search algorithmsolving decentralized POMDPs. Proc. Uncertainty Artificial Intelligence.Tsitsiklis, J., & Athans, M. (1985). complexity decentralized decision makingdetection problems. IEEE Transactions Automatic Control , 30 (5), 440446.Varaiya, P., & Walrand, J. (1978). delayed sharing patterns. IEEE TransactionsAutomatic Control , 23 (3), 443445.Varakantham, P., Kwak, J. young, Taylor, M. E., Marecki, J., Scerri, P., & Tambe, M. (2009).Exploiting coordination locales distributed POMDPs via social model shaping.Proc. International Conference Automated Planning Scheduling.Varakantham, P., Marecki, J., Yabu, Y., Tambe, M., & Yokoo, M. (2007). Letting looseSPIDER network POMDPs: Generating quality guaranteed policies. Proc.International Conference Autonomous Agents Multi Agent Systems.Varakantham, P., Nair, R., Tambe, M., & Yokoo, M. (2006). Winning back cup distributed POMDPs: planning continuous belief spaces. Proc. InternationalConference Autonomous Agents Multi Agent Systems.Velagapudi, P., Varakantham, P., Scerri, P., & Sycara, K. (2011). Distributed model shapingscaling decentralized POMDPs hundreds agents. Proc. International Conference Autonomous Agents Multi Agent Systems.Vlassis, N. (2007). Concise Introduction Multiagent Systems Distributed ArtificialIntelligence. Morgan & Claypool Publishers.508fiIncremental Clustering Expansion Faster Optimal Planning Dec-POMDPsWilliamson, S. A., Gerding, E. H., & Jennings, N. R. (2009). Reward shaping valuing communications multi-agent coordination. Proc. International ConferenceAutonomous Agents Multi Agent Systems.Witwicki, S. J. (2011). Abstracting Influences Efficient Multiagent CoordinationUncertainty. Unpublished doctoral dissertation, University Michigan, Ann Arbor,Michigan, USA.Witwicki, S. J., & Durfee, E. H. (2010). Influence-based policy abstraction weakly-coupledDec-POMDPs. Proc. International Conference Automated PlanningScheduling.Wu, F., Zilberstein, S., & Chen, X. (2010a). Point-based policy generation decentralizedPOMDPs. Proc. International Conference Autonomous Agents MultiAgent Systems.Wu, F., Zilberstein, S., & Chen, X. (2010b). Rollout sampling policy iteration decentralizedPOMDPs. Proc. Uncertainty Artificial Intelligence.Wu, F., Zilberstein, S., & Chen, X. (2011). Online planning multi-agent systemsbounded communication. Artificial Intelligence, 175 (2), 487511.Xuan, P., Lesser, V., & Zilberstein, S. (2001). Communication decisions multi-agent cooperation: Model experiments. Proc. International Conference AutonomousAgents.Yoshizumi, T., Miura, T., & Ishida, T. (2000). A* partial expansion large branchingfactor problems. Proc. National Conference Artificial Intelligence.Zeng, Y., & Doshi, P. (2012). Exploiting model equivalences solving interactive dynamicinfluence diagrams. Journal Artificial Intelligence Research, 43 , 211255.Zeng, Y., Doshi, P., Pan, Y., Mao, H., Chandrasekaran, M., & Luo, J. (2011). Utilizingpartial policies identifying equivalence behavioral models. ProceedingsTwenty-Fifth AAAI Conference Artificial Intelligence.509fiJournal Artificial Intelligence Research 46 (2013) 263302Submitted 07/12; published 03/13Parameterized Complexity ResultsExact Bayesian Network Structure LearningSebastian Ordyniakordyniak@fi.muni.czMasaryk University Brno, Czech RepublicStefan Szeiderstefan@szeider.netVienna University Technology, AustriaAbstractBayesian network structure learning notoriously difficult problem discovering Bayesian network optimally represents given set training data.paper study computational worst-case complexity exact Bayesian network structure learning graph theoretic restrictions (directed) super-structure.super-structure undirected graph contains subgraphs skeletons solutionnetworks. introduce directed super-structure natural generalization undirected counterpart. results apply several variants score-based Bayesian networkstructure learning score network decomposes local scores nodes.Results: show exact Bayesian network structure learning carriednon-uniform polynomial time super-structure bounded treewidth, lineartime addition super-structure bounded maximum degree. Furthermore,show directed super-structure acyclic, exact Bayesian network structure learning carried quadratic time. complement positive resultsnumber hardness results. show restrictions (treewidth degree)essential cannot dropped without loosing uniform polynomial time tractability(subject complexity-theoretic assumption). Similarly, exact Bayesian network structure learning remains NP-hard almost acyclic directed super-structures. Furthermore,show restrictions remain essential search globally optimalnetwork aim improve given network means k arc additions, arcdeletions, arc reversals (k-neighborhood local search).1. IntroductionBayesian Network Structure Learning (BNSL) important task discovering Bayesiannetwork represents given set training data. Unfortunately, solving problemoptimally (Exact BNSL) NP-complete (Chickering, 1996). common widely usedapproach overcome complexity barrier exploit structure problem.also popular direction BNSL two main kinds structural restrictions studied far, i.e., (1) restrictions probability distributiongenerating input (2) restrictions resulting Bayesian network. helprestrictions several tractable classes BNSL identified. BNSL solvable non-uniform polynomial time distribution generating input boundedtreewidth (Narasimhan & Bilmes, 2004) bounded degree (Pieter, Daphne, & Andrew,2006) solvable (non-uniform) polynomial time resulting BN branching (Chow & Liu, 1968) close branching (Gaspers, Koivisto, Liedloff, Ordyniak,c2013AI Access Foundation. rights reserved.fiOrdyniak & Szeider& Szeider, 2012). positive results contrasted series negative resultsmentioned restrictions, e.g., BNSL known NP-hard resulting BNpolytree (Dasgupta, 1999) directed path (Meek, 2001). Recently, novel approachrestrict structure BNSL introduced (Tsamardinos, Brown, & Aliferis,2006; Perrier, Imoto, & Miyano, 2008). so-called super-structure, undirectedgraph nodes resulting BN, used restrict search space BNSLadvance. super-structure obtained, usually using IT-based approach (Tsamardinos et al., 2006), one looks solution networks whose skeletonscontained super-structure. hence becomes important super-structuresound, i.e., contains least one optimal solution.two main questions concerning super-structure: First, suitable sound super-structure obtained efficiently? secondly, superstructure obtained, used guide search optimal solution?goal paper provide theoretical analysis latter question, consideringsuper-structures arise given local score function adapting model Parviainen Koivisto (2010). consider various combinations restrictions systematicway allows us draw broader picture complexity landscape BNSL. hopeanalysis help understand boundaries tractable intractablecases important problem. Furthermore, think results provide new insights help search efficient accurate heuristics. analysis,use theoretical framework parameterized complexity (Downey & Fellows, 1999)seems well suited investigating complexity BNSL allows takestructural properties (in terms parameters) account. best knowledgeparameterized complexity theory employed context before.1.1 Resultssection give brief overview results.1.1.1 Exact BNSL Using Super-structurefirst part paper study worst-case complexity Exact BNSLgraph-theoretic restrictions super-structure. One prominent restrictionssuper-structure consider treewidth. Treewidth important widelyused parameter measures similarity graph tree (Bodlaender, 1993, 1997,2005; Greco & Scarcello, 2010). Similarly trees, many otherwise intractable problemsbecome tractable graphs bounded treewidth. importantly, treewidth alreadysuccessfully applied context Bayesian Reasoning (Darwiche, 2001; Dechter,1999; Kwisthout, Bodlaender, & van der Gaag, 2010). hence seems natural applytreewidth (Exact) BNSL.results follows:(1) Exact BNSL solvable non-uniform polynomial time treewidth superstructure bounded arbitrary constant.(2) Exact BNSL solvable linear time treewidth maximum degreesuper-structure bounded arbitrary constants.264fiParameterized Complexity Exact BNSLnon-uniform mean order polynomial depends treewidth.obtain results (1) (2) means dynamic programming algorithm alongdecomposition tree super-structure.show thatin certain senseboth results optimal:(3) Exact BNSL instances super-structures maximum degree 4 (but unboundedtreewidth) solvable polynomial time unless P = NP. Thus, (1) (2)cannot drop bound treewidth.(4) Exact BNSL instances super-structures bounded treewidth (but unboundedmaximum degree) solvable uniform polynomial time unless FPT = W[1].Thus, (2) cannot drop bound degree.FPT 6= W[1] widely accepted complexity theoretic assumption (Downey & Fellows,1999) often considered parameterized analog P 6= NP. providenecessary background parameterized complexity fpt-reductions Section 2.2.1.1.2 Local Search BNSL Using Super-structureSince learning optimal Bayesian network computationally hard heuristic methodsused practice. popular heuristic BNSL so-called hill climbing procedure,local search. particular, highly competitive algorithm learning large Bayesian networks (MMHC) uses local search find optimal solution inside previously constructedsuper-structure (Tsamardinos et al., 2006). study worst case complexity wellknown generalization local search, k-Neighborhood Local Search (or k-Local Searchshort). variant local search one allowed modify one k arcsevery step search. Hence adjusting k, one able balance speed accuracy. However, k-local search space order nO(k) , applying k-local searchespecially desirable problems running time increase modestlyrespect k. Similarly result (2) able show following:(5) k-Local Search BNSL solvable linear time treewidth maximumdegree super-structure bounded arbitrary constants.Clearly, result minor interest already solve Exact BNSLproblem restrictions super-structure (see result (2)). However,contrast Exact BNSL problem one might able drop one restrictionswithout losing uniform polynomial-time tractability. show unlikely.(6) k-Local Search BNSL instances super-structures either boundedtreewidth bounded maximum degree solvable uniform polynomial timeunless FPT = W[1].1.1.3 Exact BNSL Using Directed Super-structurefar, one considered super-structure undirected graph. introducedirected super-structure expressive way restrict search space solutions,i.e., directed super-structure fixed restrict search solutions whose265fiOrdyniak & SzeiderProblem: Exact BNSLComplexityResultRestrictions Super-Structurebounded treewidthbounded treewidthbounded max degreelinear timeXP, W[1]-hardbounded max degreeNP-hardCorollary 1Corollary 1,Theorem 3Theorem 2poly-timeNP-hardCorollary 2Theorem 7ComplexityResultlinear timeXP, W[1]-hardXP, W[1]-hardProposition 6Theorem 5Theorem 6FPTFPTTheorem 4Theorem 4Restrictions Directed Super-Structureacyclicalmost acyclicbounded max degreeProblem: k-Local Search BNSLRestrictions Super-Structurebounded treewidthbounded treewidthbounded max degreebounded max degreeRestrictions Operationsarc deletionarc additionFigure 1: Overview complexity results. complexity classes FPT XP containproblems solvable uniform polynomial-time non-uniform polynomial-time,respectively.266fiParameterized Complexity Exact BNSLnetworks contained directed super-structure. Again, study complexityExact BNSL respect certain restrictions directed super-structure obtainfollowing result.(7) Exact BNSL solvable quadratic time directed super-structure acyclic.question arises whether extend result using general formacyclicity. show, however, case.(8) Exact BNSL NP-hard directed super-structures made acyclicdeleting one node. Hardness even holds maximum in-degree maximumout-degree directed super-structure bounded 3.systematic overview results (1)(8) given Figure 1.1.2 Related Worksection present relevant related work BNSL. related workfound respective sections, i.e., present related work parameterized complexitySection 2.2, related work treewidth tree decompositions Sections 2.3 3.1,Section 7 present related work k-neighborhood local search.1.2.1 Algorithms Exact BNSLdate handful exact algorithms BNSL proposed.split three groups: (A) exact algorithms employ restrictions (Parviainen& Koivisto, 2010; Koivisto, 2006; Yuan, Malone, & Wu, 2011; Ott, Imoto, & Miyano, 2004;Silander & Myllymaki, 2006; Yuan et al., 2011), (B) exact algorithms using restrictionsgenerating target distribution (Pieter et al., 2006; Chechetka & Guestrin, 2007;Friedman, Nachman, & Peer, 1999; Chow & Liu, 1968; Gaspers, Koivisto et al., 2012),(C) exact algorithms use restrictions undirected super-structure (Friedmanet al., 1999; Kojima, Perrier, Imoto, & Miyano, 2010). Algorithms falling group (A)suited small medium sized Bayesian networks restrictsearch space way. hand, restrictions used algorithmsgroup (B) general restrictions coming undirected super-structurenon-uniform polynomial-time algorithms could obtained usingrestrictions. best knowledge paper first employ in-depththeoretical analysis parameterized complexity BNSL using restrictionsundirected super-structure. obtain first algorithm exact BNSL uniformpolynomial running-time respect structural restriction undirected superstructure. similar approach taken Kojima et al. (2010), authorspropose algorithm exact BNSL uses cluster-decomposition undirectedsuper-structure. Even though practical results quite promising authors providetheoretical analysis worst-case complexity algorithm beyond trivialbound also applies exact algorithms group (A). Apart exact algorithmsalso exists variety approximation algorithms using tree decomposition degree-basedtechniques (Pieter et al., 2006; Elidan & Gould, 2008; Karger & Srebro, 2001).267fiOrdyniak & Szeider1.2.2 Hardness Results Exact BNSLalso number hardness results BNSL restrictions resultingBayesian network. particular, BNSL remains NP-hard in-degree resultingBayesian network bounded 2 (Chickering, 1996), resulting Bayesian networkpoly tree (Dasgupta, 1999), directed path (Meek, 2001). However, bestknowledge, negative results BNSL restrictions (directed) super-structureobtained.1.3 Organization Prior Workpaper organized follows: Section 2 introduce basic concepts notionsuse throughout paper. introduce main object study (BNSL)Section 3. Section 4 shows use dynamic programming algorithm treedecomposition order show results (1) (2). provide refined complexity analysisalgorithm Section 5. Section 6 show complexity boundaries ExactBNSL using super-structure, i.e., obtain results (3) (4). introduce k-LocalSearch BNSL Section 7 establish results (5) (6). introduce directedsuper-structure Section 8 show results (7) (8). conclude Section 9.appendix contains proofs technical claims.preliminary shortened version paper appeared proceedings UAI2010. Apart providing higher level detail readability giving examplesdetailed proofs, paper extends previous version four ways: addingsection related work (Section 1.2), providing refined complexity analysismain algorithmic result (Section 5), providing novel proof Theorem 6 allowsus decrease upper bound maximum degree super-structure 5 3,introducing directed super-structure (Section 8).2. Preliminariessection introduce basic concepts notions use throughoutpaper.2.1 Basic Graph Theoryassume reader familiar basic graph theory (see, e.g., Diestel,2000; Bang-Jensen & Gutin, 2009). consider undirected graphs directed graphs(digraphs). dag directed acyclic graph. write V (G) = V E(G) = Esets nodes edges (directed undirected) graph G = (V, E), respectively.denote undirected edge nodes u v {u, v} directed edge (or arc),directed u v (u, v). write NG (v) set neighbors node v V G,i.e., NG (v) = { u : (v, u) E (u, v) E } G directed NG (v) = { u : {u, v} E }G undirected. subset V 0 V write G[V 0 ] denote induced subgraph G0 =(V 0 , E 0 ) E 0 = { e V 0 : e E } G undirected E 0 = { e V 0 V 0 : e E }G directed. G digraph define PG (v) = { u V (G) : (u, v) E(G) } setparents v G. Furthermore, two directed graphs D1 D2 define D1 D2268fiParameterized Complexity Exact BNSLunion D1 D2 , i.e., V (D1 D2 ) = V (D1 ) V (D2 ) E(D1 D2 ) = E(D1 ) E(D2 ).Let G (directed undirected) graph e E(G) directed undirected edge G.denote G e (undirected directed) graph, G e = (V (G), E(G) \ {e}).Furthermore, subset X V (G) denote G X graph induced nodesV \ X. X contains one node v also write G v instead G {v}. callundirected graph G0 = (V 0 , E 0 ) skeleton directed graph G V 0 = V (G)E 0 = { {u, v} : (u, v) E(G) }.2.2 Parameterized ComplexityParameterized complexity provides theoretical framework distinguish uniformnon-uniform polynomial-time tractability respect parameter.introduced pioneered Downey Fellows (1999) receiving growing interestreflected recent publication two monographs (Flum & Grohe, 2006;Niedermeier, 2006) hundreds research papers (see references mentionedmonographs). 2008 Computer Journal devoted two special issues parameterized complexity order make key methods ideas known wide rangecomputer scientists (Downey, Fellows, & Langston, 2008).instance parameterized problem pair (I, k) main part kparameter ; latter usually non-negative integer. parameterized problemfixed-parameter tractable exist computable function f constant cinstances (I, k) size n solved time O(f (k)nc ). FPT classfixed-parameter tractable decision problems. Fixed-parameter tractable problems alsocalled uniform polynomial-time tractable k considered constant, instancesparameter k solved polynomial time order polynomialindependent k (in contrast non-uniform polynomial-time running times nk ).Parameterized complexity offers completeness theory similar theory NPcompleteness. One uses fpt-reductions many-one reductions parameterone problem maps parameter other. specifically, problem Lreduces problem L0 mapping R instances L instances L0(i) (I, k) yes-instance L (I 0 , k 0 ) = R(I, k) yes-instanceL0 , (ii) k 0 g(k) computable function g, (iii) R computed timeO(f (k)nc ) f computable function, c constant, n denotes size(I, k). parameterized complexity class W[1] considered parameterized analogNP. particular, FPT = W[1] implies (unlikely) existence 2o(n) algorithmn-variable 3SAT (Impagliazzo, Paturi, & Zane, 2001; Flum & Grohe, 2006). example,parameterized problem W[1]-complete fpt-reductions parameterizedMaximum Clique problem (given graph G parameter k 0, G containcomplete subgraph k nodes?). Note exists trivial non-uniform polynomialtime nk algorithm Maximum Clique problems checks sets k nodes.2.3 Tree DecompositionsTreewidth important graph parameter indicates certain sense treelikeness undirected graph (Bodlaender, 1993, 1997, 2005). graphs treewidthbounded constant many otherwise intractable problems become tractable. Bucket269fiOrdyniak & SzeiderElimination (Dechter, 1999) Recursive Conditioning (Darwiche, 2001) two important algorithmic concepts apply instances bounded treewidth.treewidth graph G = (V, E) defined via following notion decomposition:tree decomposition G pair (T, ) tree labeling function(t) V every tree node following conditions hold:1. Every node G occurs (t) tree node t.2. every edge {u, v} G tree node u, v (t).3. every node v G, let Tv subgraph induced nodesv (t). Tv (connected) subtree (Connectedness Condition).width tree decomposition (T, ) size largest set (t) minus 1 amongnodes . tree decomposition smallest width optimal. treewidthgraph G, denoted tw(G), width optimal tree decomposition G. known(T, ) tree decomposition graph G, every clique G contained(t) tree node V (T ) (Kloks, 1994, Lemma 2.2.2).following proposition useful retrieve upper bound treewidthgraph.Proposition 1. Let G undirected graph X V (G). graph G X containsedge, i.e., nodes G X isolated, tw(G) |X|.Proof. Let V (G) \ X contain nodes v1 , . . . , vn let tree node set{t, t1 , . . . , tn } edge set { {t, ti } : 1 n }. together function(t) = X (ti ) = X {vi } every 1 n tree decomposition Gwidth |X|.main property tree decompositions allows efficient bottom-up dynamicprogramming algorithms wide spectrum otherwise intractable problems wellknown separation property made precise following proposition.Proposition 2. Let G graph, (T, ) tree decomposition G, {t0 , t00 } edge, let 0 00 subtrees obtained deleting edge {t0 , t00 }000000000contains tSand contains . Furthermore, define = (t ) (t ), =tV (T 0 ) (t) B =tV (T 00 ) (t). following two statements hold:1. B = S;2. separates nodes nodes B, i.e., edge node\ node B \ S.Proof. first statement follows immediately Property (3) tree decomposition,since subtree containing node v (A B) make use edge {t0 , t00 } orderconnected hence v contained S.see second statement, suppose contradiction edgenode \ node b B \ S. follows first statement propositionTa Tb disjoint, i.e.,otherwise either b S. contradictsproperty (2) tree decomposition since {a, b} edge G.270fiParameterized Complexity Exact BNSLGiven graph G n nodes constant w, possible decide whether Gtreewidth w, so, compute optimal tree decomposition Gtime O(n) (Bodlaender, 1996). Furthermore exist powerful heuristics compute treedecompositions small width practically feasible way (Koster, Bodlaender, & vanHoesel, 2001; Gogate & Dechter, 2004; Dow & Korf, 2007). Recently, new randomizedheuristics studied context Bayesian reasoning (Kask, Gelfand, Otten, &Dechter, 2011; Gelfand, Kask, & Dechter, 2011).3. Bayesian Network Structure Learningsection define theoretical framework BNSL shall useconsiderations. closely follow abstract framework used Parviainen Koivisto(2010) encloses wide range score-based approaches structure learning.assume input data specifies set N nodes (representing random variables)local score function f assigns v N subset P N \ {v} nonnegative real number f (v, P ). Given local score function f set N nodes,problem find dag N score fXf (D) :=f (v, PD (v))vNlarge possible (the dag together certain local probability distributions formsBayesian network). setting accommodates several popular scores like BDe, BICAIC (Parviainen & Koivisto, 2010; Chickering, 1995).consider following decision problem:Exact Bayesian Network Structure LearningInput:local score function f defined set N nodes, realnumber > 0.Question: dag N f (D) s?following complexity results need fix score function representedproblem input. cannot list values f (v, P ) nodes v N subsetsP N \ {v}, requires (2|N | ) space. Therefore, assume f (v, P ) givendifferent 0; call non-zero representation. representationalso used fields, instance constraint satisfaction, allowed tuplesconstraints listed input (Tsang, 1993). alternative representation scorefunction would list input values f (v, P ) |P | c constantc. Let us call arity-c representation. representation requires polynomialspace every node c parents, small constant c, reasonableassumption. Given arity-c representation score function, clearly computelinear time corresponding non-zero representation. Hence non-zero representationgeneral, therefore base complexity results encoding.tractability results also carry arity-c representation.size f (given non-zero representation) number bits neededrepresent tuples f (v, P ) > 0 reasonable data structure, e.g., list271fiOrdyniak & Szeidertuples tuple stores node, set parents, value scorefunction. Clearly, size f exceeds total number tuples. definePf (v) := { P N : f (v, P ) > 0} {} set potential parent sets v.also definef := max |Pf (v)|;vNimportant measurement worst-case analysis running times.particular, assumption f represented implies following:(*) Let = (N, f, s) instance Exact Bayesian Network Structure Learning. f bounded size f .Let f local score function defined set N nodes. directed super-structuref directed graph Sf = (N, Ef ) Ef contains arc (u, v) upotential parent v, i.e., u P P Pf (v). (undirected) super-structuref , denoted Sf = (N, Ef ), skeleton directed super-structure.vPf (v, P ){d}1{b, c, d}0.5b {a, f }11c{e}1e{b}1f{d}1g {c, d}1fcbefSfgcbefgSfFigure 2: local score function f together directed super-structure Sfundirected super-structure Sf .Example 1. Figure 2 shows example local-score function f defined setN = {a, b, c, d, e, f, g} nodes. function f given table containing tuples(v, P, f (v, P )) v N P N \ {v} f (v, P ) > 0. Note since fnon-negative holds f (v, P ) = 0 remaining pairs (v, P ), i.e., pairscontained table. figure also shows directed super-structure Sfunique super-structure Sf f .say dag set N nodes admissible f skeletonsubgraph super-structure Sf . Furthermore, say dag N strictlyadmissible f every node v N PD (v) Pf (v). Note every strictlyadmissible dag also admissible. Furthermore, always exists (strictly) admissibledag highest score shown following lemma.272fiParameterized Complexity Exact BNSLLemma 1. Let f local score function defined set N nodes let dagN . strictly admissible dag D0 N score D.Proof. (strictly) admissible, i.e., v N f (v, PD (v)) = 0,delete arcs (w, v) w PD (v). decrease score sincef (v, ) f (v, PD (v)) = 0 every v.becfgbDaecfgbeDbDccfgbecfgDdFigure 3: Various dags correspondence local-score function f Example 1.Example 2. Figure 3 shows four examples dags nodes a, b, c, d, e, f, g. Usinglocal-score function f defined Example 1, make following observations:a) Da admissible, super-structure Sf contain edge {c, b}.score Da f (Da ) = 4.b) Db admissible strictly admissible, node parents cf (a, {c, d}) = 0. score Db f (Db ) = 5.c) Dc strictly admissible dag obtained Db described proof Lemma 1.f (Dc ) = f (Db ) = 5.d) Dd strictly admissible also optimal dag f . score Dd f (Dd ) =7.3.1 Tree Decompositionspresenting algorithms graphs bounded treewidth convenient considertree decompositions following normal form (Kloks, 1994): triple (T, , r) nicetree decomposition graph G (T, ) tree decomposition G, tree rootednode r, node one following four types:1. leaf node: node children;2. join node: node exactly two children t1 , t2 , (t) = (t1 ) = (t2 );3. introduce node: node exactly one child t0 , (t) = (t0 ) {v}node v G;4. forget node: node exactly one child t0 , (t) = (t0 ) \ {v} nodev G.273fiOrdyniak & Szeiderconvenience also assume (r) = root r . alwaysachieved adding forget nodes top root (see Figure 4 example). nicetree decomposition (T, , r) define (t) union sets (t0 ) t0contained subtree rooted t. Furthermore, denote Ft set nodesalready forgotten node t, i.e.,Ft = (t) \ (t).stated Section 2.3 one main properties tree decompositions allowsefficient algorithms well-known separator property made precise Proposition 2.following propositions provide different versions separator property tree decompositions node types nice tree decomposition. propositionswell-known (Kloks, 1994) immediate consequences separator propertytree decompositions state without proofs. propositions summarizealgorithmic properties nice tree decompositions use designalgorithm Section 4.Proposition 3. Let join node children t1 t2 . Ft1 Ft2 =edge node u Ft1 node v Ft2 G.Proposition 4. Let introduce node child t0 (t) = (t0 ) {v0 }.edge v0 node v Ft G. Furthermore, v0/ Ft0 .Proposition 5. Let forget node child t0 (t) = (t0 ) \ {v0 }.edge v0 node v V (G) \ (t) G.Given tree decomposition graph G width w, one effectively obtaintime O(|V (G)|) nice tree decomposition G O(|V (G)|) nodes widthw (Kloks, 1994).Example 3. Figure 4 shows tree decomposition corresponding nice tree decomposition super-structure Example 1.4. Dynamic Programming Algorithm Exact Bayesian NetworkStructure Learningsection present dynamic programming algorithm establish tractabilityresults. remainder section w denotes arbitrary fixed constant. Recallprevious section f maximum number potential parent sets node.Theorem 1. Given set N nodes local score function f N whose superstructure Sf = (N, Ef ) treewidth bounded arbitrary constant w. find2(w+1)time O(f|N |) dag N maximal score f (D).devise algorithm show Theorem 1 state prove direct consequence theorem.Corollary 1. Exact Bayesian Network Structure Learning decided polynomial time instances super-structure bounded treewidth. problemdecided linear time additionally super-structure bounded maximum degree.274fiParameterized Complexity Exact BNSLa, ba, b, ca, b, c,a, b, c,b, c,e, b, cf, b,g, c,b, c,b, c,b, c,b, c,b, cb,c,e, b, cf, b,g, c,Figure 4: tree decomposition (left) corresponding nice tree decomposition (right)super-structure Example 1.Proof. first statement follows immediately theorem since f boundedtotal input size instance w constant. Recall Section 3local score function f given list tuples f non-zero hence fbounded total input size instance. second statement follows since fbounded whenever maximum degree super-structure bounded clearlyf 2d .following assume given set nodes N local scorefunction f N together nice tree decomposition (T, , r) Sf width w.going establish Theorem 1 means dynamic programming algorithm alongnice tree decomposition Sf , computing local information nodes treedecomposition put together form optimal dag. algorithmclosely follows general approach used algorithms graphs (or structures) boundedtreewidth (Bodlaender & Koster, 2008).partial solution tree node V (T ) dag obtained inducedsubdigraph D[ (t)] strictly admissible dag f . tree node let D(t) denoteset partial solutions t. partial solution D(t) setft (D) =Xf (v, PD (v)),vFt275fiOrdyniak & Szeideri.e., ft (D) sum scores nodes Ft . Recall previous sectionFt set forgotten nodes t.main idea underlying algorithm reduce space required store partialsolution help so-called record. becomes possible propertiestree decomposition manifested Propositions 3, 4 5.record tree node V (T ) triple R = (a, p, s) that:1. mapping (t) Pf (v), i.e., every v (t) a(v) Pf (v);2. p transitive binary relation (t);3. non-negative real number.Informally, tree node V (T ) record R = (a, p, s), mapping fixesparent set every node (t), p compact representation reachability relationnodes (t) (using directed paths nodes (t)), sumscores nodes forgotten t, i.e., nodes Ft .say record represents partial solution D(t) satisfies followingconditions:1. a(v) V (D) = PD (v) every v (t).2. every pair nodes v1 , v2 (t) holds (v1 , v2 ) p containsdirected path v1 v2 .say record R = (a, p, s) tree node V (T ) valid represents dagD(t) maximum score ft (D) dags D(t) represented R.say partial solution represented R maximal respect R R validft (D) = s. tree node V (T ) associate set R(t) valid recordsrepresenting partial solutions D(t).certain sense, R(t) succinct representation optimal elements D(t),using space depends w f , |N |.becfb(t)gceDaf(t)gDbFigure 5: Two partial solutions node nice tree decomposition Example 3.Example 4. Figure 5 shows two partial solutions Da Db node nicetree decomposition Example 3. Da Db represented records R = (a, p, s)276fiParameterized Complexity Exact BNSLa(b) = {a, f }, a(c) = {e}, a(d) = p = {(b, c), (d, b), (d, c)}. ft (Da ) = 3 >ft (Db ) = 2 easy see 3 maximum score partial solutionsrepresented R. Hence record R = (a, p, 3) valid record Da maximalrespect R, thus R R(t).dynamic programming algorithm computes set valid records bottommanner, i.e., starting leave nodes nice tree decomposition algorithmproceeds root node. next three lemmas show compute set validrecords introduce, forget join nodes nice tree decompositionvalid records children. Informally, introduce node child t0(t) = (t0 ) {v0 } compute set R(t) valid records checkingpotential parent set P P(v0 ) v0 valid record R R(t0 ) t0 whethercombination P R constitutes valid record t.Lemma 2 (introduce node). Let introduce node child t0 . R(t)computed R(t0 ) time O(fw+1 ).Proof. following denote v0 node introduced t, i.e., (t) = (t0 ) {v0 }.going establish lemma help following claim whose prooffound appendix.Claim 1. R(t) set records R = (a, p, s) set P Pf (v0 )record R0 = (a0 , p0 , s0 ) R(t0 ) with:1. a(v0 ) = P .2. every v (t0 ) holds a(v) = a0 (v).3. = s0 .4. p transitive closure relation p0 { (u, v0 ) : u P } { (v0 , u) : u(t0 ) v0 a0 (u) }.5. p irreflexive.follows R(t) computed checking every pair (P, R0 ), P Pf (v0 )R0 R(t0 ), whether satisfies conditions (1)(5). Since fpossible sets P O(fw ) possible valid records t0 (observe |(t0 )| w)lemma follows fact every pair (P ,R0 ) conditions checkedtime depends w.Informally, forget node child t0 (t) = (t0 )\{v0 } computeset R(t) valid records projecting set R(t0 ) valid records t0(t).Lemma 3 (forget node). Let forget node child t0 . R(t) computedR(t0 ) time O(fw+1 ).277fiOrdyniak & SzeiderProof. following denote v0 forgotten node, i.e. (t) = (t0 ) \ {v0 }.show R(t) obtained projection R(t0 ) (t).need additional notation. Let R0 = (a0 , p0 , s0 ) R(t0 ). define projection R0t, denoted R0 [t] record R = (a, p, s) that:1. restriction a0 (t).2. p = { (v, w) p0 : v, w (t) }.3. = s0 + f (v0 , a0 (v0 )).Furthermore, define projection R(t0 ) t, denoted R(t0 )[t], set recordsR0 [t] R0 R(t0 ). say record R = (a, p, s) R(t0 )[t] maximals0 s0 > (a, p, s0 ) R(t0 )[t].Again, going establish lemma help following claim whoseproof found appendix.Claim 2. R(t) set maximal records R(t0 )[t].Since R(t0 ) contains O(fw+1 ) records easy see R(t) computedtime O(fw+1 ).Informally, join node children t1 t2 compute set R(t)valid records checking record R1 R(t1 ) record R2 R(t2 )whether combination R1 R2 constitutes valid record t.Lemma 4 (join node). Let t1 , t2 children . R(t) computed2(w+1)R(t1 ) R(t2 ) time O(f).Proof. going establish lemma help following claim (the rathertechnical proof claim found appendix).Claim 3. R(t) set records R = (a, p, s) records R1 =(a1 , p1 , s1 ) R(t1 ) R2 = (a2 , p2 , s2 ) R(t2 ) with:1. = a1 = a2 .2. = s1 + s2 .3. p transitive closure p1 p2 .4. p irreflexive, i.e., v (t) (v, v) p.follows R(t) computed considering pairs records R1 R(t1 )R2 R(t2 ) checking conditions (1)(4). Since O(fw+1 ) validrecords every V (T ) every pair records time required checkconditions (1)(4) depend w, follows running time procedure2(w+1)O(f).ready establish Theorem 1.278fiParameterized Complexity Exact BNSLProof Theorem 1. Let N set nodes, f local score function Nsuper-structure Sf treewidth w (a constant) |N | = n. compute nice treedecomposition (T, , r) Sf width w O(n) nodes. accomplishedtime O(n) (see discussion Section 2.3).Next compute sets R(t) via bottom-up traversal . leaf nodecompute R(t) considering strictly admissible dags w + 1 nodes(t). clearly done time O(fw+1 ) use Lemmas 4, 2 32(w+1)compute sets R(t) O(n) tree nodes time O(fn).Since (r) = , partial solutions root r exactly strictly admissibledags f , fr (D) = f (D) dag D. computationsets R(t) tree nodes t, set R(r) contains exactly one record R = (, , s).considerations, follows largest score strictly admissible dagsf , and, noted Section 3, also largest score dag whose nodes belongN . easy compute dag score f (D) = via top-down traversalstarting r using information previously stored node .2(w+1)also accomplished time O(fn).close section remark concerning relationship treewidthBayesian network treewidth super-structure. Bayesian reasoning oneusually associates dag Bayesian network moral graph (D)skeleton plus edges joining nodes common child D. treewidthBayesian network treewidth moral graph (Darwiche, 2001; Dechter, 1999).observe every Bayesian network bounded treewidth super-structurebounded treewidth contains Bayesian network. Hence, Bayesian networklearned considering super-structures bounded treewidth. handBayesian network contained super-structure bounded treewidth Bayesiannetwork bounded treewidth reasonable assumption nodesbounded number parents. Consequently, Bayesian network learnedsuper-structure bounded treewidth reasonable assume Bayesian networkbounded treewidth well.5. Refined Complexity Analysispresenting algorithm Section 4 focused broad evaluation complexity. Section provide fine-grained analysis running time.following assume N set nodes, f local score function N ,(T, , r) nice tree decomposition Sf width w. improverunning time algorithm using following five ideas.I1 keeping records node tree decomposition fixed order2(w+1))improve time needed join node tree decomposition O(fw+1O(f ) without additional cost sorting. achieve generatingnew records ordered manner keeping track order recordsstored.279fiOrdyniak & SzeiderI2 Parent sets supersets parent sets higher scoredisregarded (de Campos, Zeng, & Ji, 2009). simple preprocessing rule usuallyallows us consider fewer potentially 2d potential parent sets. sequeldenote preprocessed score function f 0 . Clearly, |Pf (v)| |Pf 0 (v)|every v N .I3 maximum in-degree resulting BN bounded advance one usefact preprocess score function natural way. followingdenote resulting score function f 00 . Clearly, |Pf 00 (v)| |Pf 0 (v)| everyv N.I4 Every node network might different number potential parent sets.Consequently, instead using one upper bound number f 00 potential parentsets realistic consider actual number |Pf 00 (v)| potential parent setsnode v N .I5 valid records need stored algorithm, i.e., records representacyclic networks.Considering ideas I1I4 worst-case complexity algorithm refinedfollows:XO(w2|Pf 00 (v)|)tV (T )w2v(t)QObservev(t) |Pf 00 (v)| number potential records tree node t.idea I5 general records need stored algorithm.refined analysis suggests running time algorithm dominatedmaximum number records need stored tree node V (T ).6. Hardness Results Exact Bayesian Network Structure Learningfollowing result follows reduction due Chickering (1996).Theorem 2. Exact Bayesian Network Structure Learning NP-hard instances super-structures maximum degree 4.Proof. Since use well-known reduction Chickering, sketch argument. reduction Feedback Arc Set (FAS). problem asks whetherdigraph = (V, E) made acyclic deleting k arcs (the deleted arcs formfeedback arc set D). problem NP-hard digraphs skeletons maximumdegree 4 (Karp, 1972). Given instance (D, k) FAS, skeleton maximum degree 4, construct set V 0 = V (D)E(D) nodes local score function fV 0 setting f ((u, v), {u}) = 1 (u, v) E(D), f (v, { (u, v) : u PD (v) }) = |PD (v)|v V (D), f (v, P ) = 0 cases. Clearly, super-structure Sf (recalldefinition Sf Section 3) isomorphic undirected graph obtainedskeleton subdividing every edge once, hence maximum degree Sfmaximum degree 4. easy see feedback arc setsize k exists dag D0 whose skeleton spanning subgraph Sff (D0 ) 2 |E| k.280fiParameterized Complexity Exact BNSLTheorem 3. Exact Bayesian Network Structure Learning parameterizedtreewidth super-structure W[1]-hard.Proof. devise fpt-reduction following problem, well-knownW[1]-complete (Pietrzak, 2003).Partitioned CliqueInput:Parameter:Question:k-partite graph G = (V, E) partition V1 , . . . , Vk|Vi | = |Vj | = n 1 < j k.integer k.nodes v1 , . . . , vk vi Vi 1 k{vi , vj } E 1 < j k? (The graph K =({v1 , . . . , vk }, { {vi , vj } : 1 < j k }) k-clique G.)Let G = (V, E) instance problem partition V1 , . . . , Vk , |V1 | = = |Vk | =n parameter k. Informally, encode given instance Partitioned Cliqueinstance (N, f, s) Exact Bayesian Structure Learning Gk-clique Bayesian network f (D) s. achieveintroduce node nv every node v G one node aij every 1 6= j k.node corresponds node Vi achieves maximum score parent setcontains nodes aij 1 j k. node aij achieves maximum score parent setcorresponds edge G node Vi node Vj . Hence, edges Gencoded local score function nodes aij 1 6= j k. choosingproper scores nodes right threshold assure Bayesiannetwork whose score higher every node aij attain maximum score,k nodes correspond nodes G achieve maximum score.follows parent sets chosen nodes aij correspond edges k-cliqueG.order make later calculations easier assume k > 2 remainderproof. Let = k 2 k 1, = 2k = nk + 1. construct set N nodeslocal score function f N satisfying following claims.Claim 4. tw(Sf ) k(k 1)/2Claim 5. G k-clique dag f (D) s.shown theorem establishing two claims.set = { aij : 1 < j k }, N = V (G) A, Ai = { alk : l =k = } every 1 k. ready define f . set f (v, Ai ) = everyv Vi , f (aij , {u, w}) = every 1 < j k, u Vi , w Vj , {u, w} E(G).Furthermore, set f (v, P ) = 0 remaining combinations v P . See Figure 6illustration. Now, Claim 4 follows Proposition 1 X = A. Hence, remainsshow Claim 5.go show Claim 5 give notation explanation. Let00E0E(G). denote 0V (E ) set nodes incident edges E , i.e.,set eE 0 e. say E representable every 1 < j k containsone edge node Vi node Vj . define eij (E 0 ) = {vi , vj } E 0contains edge vi Vi vj Vj eij = otherwise. define D(E 0 )281fiOrdyniak & Szeiderv11v12v13v11v12a13v33v21v327a12v33v22v31v13v21v32v23v22a23v31v23Figure 6: example graph G super-structure Sf according construction used proof Theorem 3.v11v12v13v11v12a13v33v21v327a12v33v22v31v13v21v32v23v22a23v31v23D(E 0 )GFigure 7: example graph G Figure 6 together edge set E 0 , givenbold edges illustration G, resulting dag D(E 0 ).directed graph node set N arc set { (v, aij ) : v eij (E 0 ) 1 < jk } { (aij , v) : v/ eij (E 0 ) 1 < j k }. Figure 7 shows D(E 0 ) representableedge set example graph G Figure 6.main idea show Claim 5 f (D) form D(E 0 )representable edge set E 0 corresponds k-clique G. formally expressedfollowing claim whose proof found appendix.Claim 6. following statements equivalent:1. G k-clique.2. dag f (D) s.3. representable edge set E 0 E(G) f (D(E 0 )) s.282fiParameterized Complexity Exact BNSLNote contrast Theorem 2, essential Theorem 3 super-structureunbounded degree: degree treewidth bounded problemfixed-parameter tractable Corollary 1 unlikely W[1]-hard.7. k-Neighborhood Local SearchImportant widely used algorithms BNSL based local search methods (Heckerman, Geiger, & Chickering, 1995). Usually local search algorithm tries improvescore given dag transforming new dag adding, deleting, reversingone arc time (in symbols: add, del, rev, respectively). main obstaclelocal search methods danger getting stuck poor local optimum. possibilitydecreasing danger perform k > 1 elementary changes one step, known kNeighborhood Local Search k-Local Search short. BNSL, try improvescore dag n nodes, k-local search space order nO(k) . Therefore, carriedbrute-force, k-local search costly even small values k. thereforesurprising practical local search algorithms BNSL consider 1-neighborhoodsonly.study parameterized complexity k-local search initiated Fellows(2003). date collection positive negative results parameterized complexityk-local search various combinatorial optimization problems known. instance,k-local search already investigated combinatorial problems graphs (Khuller,Bhatia, & Pless, 2003; Marx, 2008; Fellows, Rosamond, Fomin, Lokshtanov, Saurabh, & Villanger, 2009; Gaspers, Kim, Ordyniak, Saurabh, & Szeider, 2012), problem findingminimum weight assignment Boolean constraint satisfaction instance (Krokhin &Marx, 2012), stable marriage problem ties (Marx & Schlotter, 2011),satisfiability problem (Szeider, 2011).section show k-local search BNSL solved linear timesuper-structure bounded treewidth bounded maximum degree. resultgood agreement Theorem 1. However, contrast Exact BNSL mightstill possible drop one restrictions without losing uniform polynomial-timetractability, show case. also investigate k-Local Search BNSLdifferent combinations allowed operations reversal, addition deletionarc. results mostly negative. fact, somewhat surprisingly, k-Local SearchBNSL remains hard even edge reversal allowed operation.state show results define k-Local Search BNSL formally.Let k 0 {add, del, rev}. Consider dag = (V, E). directed graphD0 = (V 0 , E 0 ) k-O-neighbor1. D0 dag,2. V = V 0 ,3. E 0 obtained E performing k operations set O.{add, del, rev} consider following parameterized decision problem.283fiOrdyniak & Szeiderk-O-Local Search Bayesian Network Structure LearningInput:Question:local score function f , dag admissible f ,integer k.k-O-neighbor D0 higher score D?Note problem change require D0 admissible, alwaysavoid addition inadmissible arc.cbef7gcbefgD0Figure 8: Two dags illustrating notion k-O-neighbor Example 5.Example 5. Figure 8 shows two dags D0 D0 obtainedeither reversing deleting adding reverse bold arcs D. followsD0 3-{rev}-neighbor 6-{add, del}-neighbor D. directed graph obtainedreversing arc (a, d) contains cycle (on nodes {a, b, f, d}) D0not. score D0 larger score D, since f (D) = 3 f (D0 ) = 7(using score function f depicted Figure 2).Proposition 6. k-Local Search Bayesian Network Structure Learningdecided linear time instances super-structure bounded treewidthbounded maximum degree.Proof. proof uses arguments proof Theorem 1 sketchproof proposition.following assume given instance = (D, f, k) k-OLocal Search Bayesian Network Structure Learning together nice treedecomposition (T, , r) Sf width w let maximum degree Sf .assume w constants. set denote [S] setsubsets S.main difference proof Theorem 1 interestedsolutions k-O-neighbors D. take account need slightly extendconcept record tree node V (T ). include integer c reflect costpartial solution smallest number operations needed obtain D.technical reasons also include mapping b assigns set forgotten childrenevery node contained (t). allows us compute value c forget node.record tree node V (T ) quintuple R = (a, b, p, c, s) that:284fiParameterized Complexity Exact BNSL1. mapping (t) Pf (v);2. b mapping (t) [Ft ];3. p transitive binary relation (t);4. c non-negative integer;5. non-negative real number.say record represents partial solution Dp D(t) satisfies followingconditions:1. a(v) V (Dp ) = PDp (v) every v (t).2. b(v) = { u Ft : (v, u) E(Dp ) } every v (t).3. every pair nodes v1 , v2 (t) holds (v1 , v2 ) p Dp containsdirected path v1 v2 .4. c k smallest integer Dp [Ft ] c-O-neighbor D[Ft ].say record R = (a, b, p, c, s) tree node V (T ) valid representsdag Dp D(t) maximum score ft (D) dags D(t) represented R.say partial solution represented R maximal respect R Rvalid ft (D) = s. tree node V (T ) associate set R(t) validrecords representing partial solutions D(t).straightforward adapt dynamic programming algorithm Section 4new setting. Observe k + 1 possible values c. Furthermore,every considered partial solution Dp admissible, number possible valuesb(v) every v (t) bounded 2d . follows space requirement storerecord (k + 1) 2d(w+1) times space requirement needed store recorddefined Section 4. Using argumentation Section 4 leads overallrunning time O((fw+1 (k + 1) 2d(w+1) )2 |V (D)|) O((dw+1 (k + 1) 2d(w+1) )2 |V (D)|).Since w constants, constitutes linear running time.Theorem 4. = {add} = {del}, k-O-Local Search Bayesian NetworkStructure Learning solvable polynomial time.Proof. consider = {add} proof = {del} analogous. Let= (D, f, k) given instance k-{add}-Local Search Bayesian NetworkStructure Learning. Since allowed add arcs every step must leaveacyclic. follows k-{add}-neighbor D0 f (D0 ) > f (D)node v V (D) addition k incoming arcs increasesscore v resulting digraph remains acyclic. Now, every P V (D) \ {v}one easily check whether f (v, P ) > f (v, PD (v)) whether P obtainedPD (v) via addition k incoming arcs whether resulting digraphacyclic.285fiOrdyniak & Szeiderview Theorem 4 let us define set {add, del, rev} non-trivial/ {,{add}, {del}}.Theorem 5. Let {add, del, rev} non-trivial. k-O-Local Search BayesianNetwork Structure Learning W[1]-hard parameter tw(Sf ) + k.Proof. slightly modify reduction given proof Theorem 3. Let = (G, k)given instance Partioned Clique let N , f defined correspondenceproof Theorem 3. distinguish two cases depending whetherallowed reverse arc not, i.e., depending whether rev O.case rev claim 0 = (f, D, k 0 ) = D() k 0 = k2instance k 0 -O-Local Search Bayesian Network Structure LearningG contains k-clique k 0 -O-neighbor D0 f (D0 ) > f (D).see let K k-clique G. follows Claim 6 representableedge set E 0 f (D(E 0 )) > f (D) = 1 since E 0 representable easysee D(E 0 ) obtained reversal k 0 arcs D. HenceD0 = D(E 0 ) k 0 -O-neighbor f (D0 ) > f (D). see reverse let D0k 0 -O-neighbor f (D0 ) > f (D) = 1. Hence D0 dag since f (D0 ) integerfollows f (D0 ) s. Again, using Claim 6, k-clique G.Now, remaining case, i.e., case = {add, del} claim00 = (f, D, k 00 ) k 00 = 2k 0 instance k 0 -O-Local Search Bayesian NetworkStructure Learning G contains k-clique k 00 -Oneighbor D0 f (D0 ) > f (D). proof uses arguments caserev need twice many operations. is, replacereversal arc (u, v) deletion arc (u, v) followed additionarc (v, u).preliminary version paper (Ordyniak & Szeider, 2010) showed following theorem parametrized reduction Red/Blue Non-Blocker,claimed W[1]-complete graphs bounded degree (Downey & Fellows, 1995).However, recently found problem fact fixed-parameter tractableproof published work Downey Fellows (1995) contained mistake (Fellows, 2012). therefore use reduction Independent Set requireoriginal instance bounded degree. even allows us strengthen resultdecreasing upper bound maximum degree super-structure 5 3.Theorem 6. Let {add, del, rev} non-trivial. k-O-Local Search BayesianNetwork Structure Learning W[1]-hard parameter k. Hardness even holdssuper-structure Sf maximum degree 3.Proof. devise fpt-reduction following problem known W[1]complete (Downey & Fellows, 1999).286fiParameterized Complexity Exact BNSLIndependent SetInput:undirected graph G = (V, E) integer k.Parameter: integer k.Question:G independent set size least k, i.e.,set V |S| k {u, v}/ E every pairnodes u, v S.simplify initial construction first prove theorem case maximumdegree super-structure 5. later show refine proof superstructures maximum degree 3.Let (G = (V, E), k) instance problem k 0 = 2k + 1 rev0k = 2(2k + 1) otherwise. construct dag local score function f Gindependent set size least k k 0 -O-neighbor D0higher score D.dag obtained G applying following steps (see Figure 9illustration):1. replace every node v V two nodes v 1 v 2 arc (v 1 , v 2 ).2. every node v V add binary tree Tv1 exactly |NG (v)| leaves. rootTv1 v 1 arcs Tv1 directed away v 1 . Furthermore, define lv1bijective mapping NG (v) leaves Tv1 .3. every node v V add binary tree Tv2 exactly |NG (v)| leaves. rootTv2 v 2 arcs Tv2 directed towards v 2 . Furthermore, define lv2bijective mapping NG (v) leaves Tv2 .4. every edge {u, v} E, add arcs (lu1 (v), lv2 (u)) (lv1 (u), lu2 (v)) D.5. add binary tree T1 root r1 exactly |V | leaves, whose edges directedaway r1 . define lT1 bijective mapping V leaves T1 .6. add binary tree T2 root r2 exactly |V | leaves, whose edges directedtowards r2 . define lT2 bijective mapping V leaves T2 .7. every v V (G), add arcs (v 1 , lT1 (v)) (v 1 , lT2 (v)) D.8. add arc (r2 , r1 ) D.completes construction D. Next define local score function f V (D).Let = k 1, = |V (G)| = 1.1. every n V (D) \ { v 1 : v V (G) } {r1 } set f (n, PD (n)) = .2. every v V (G) set f (v 1 , {v 2 , lT1 (v)}) = , f (v 2 , PD (v 2 ) \ {v 1 }) = ,f (lT1 (v), PD (lT1 (v)) \ {v 1 }) = .3. set f (r1 , PD (r1 )) = f (r2 , PD (r2 ) {r1 }) = .4. remaining combinations n V (D) P V (D) set f (n, P ) = 0.287fiOrdyniak & Szeiderbc7r1lT1 (a)lT1 (b)lT2 (a)lT1 (c)lT2 (b)lT2 (c)r2a1b1la1 (b)la1 (c)la2 (b)la2 (c)a2lb1 (a)lb2 (a)c1lb1 (c)lc1 (b)lc1 (a)lb2 (c)lc2 (b)lc2 (a)b2c2Figure 9: Top: example graph G. Bottom: dag resulting G usingconstruction proof Theorem 6.Evidently acyclic f constructed G polynomial time.Observe super-structure Sf exactly skeleton D. Hence, construction,nodes v 1 v V (D) degree 5 nodes Sf degree3 showing maximum degree Sf 5. Consequently,establish theorem help following claim whose proof foundappendix.288fiParameterized Complexity Exact BNSL0700000D0Figure 10: dag Figure 9 together k 0 -{rev}-neighbor D0 f (D0 ) >f (D). k = 1, k 0 = 2k + 1 = 5 {a} independent set size kgraph G Figure 9. score every node given label.Claim 7. G independent set size least k k 0 -O-neighborD0 higher score k 0 = 2k + 1 rev k 0 = 2(2k + 1) otherwise.show alter construction obtain result maximumdegree 3. nodes Sf whose degree may exceed 3 nodes { v 1 : v V (G) }.main idea reduce degree nodes split sets neighborsusing binary trees. new construction define DAG D0 local score functionf 0 follows. DAG D0 obtained applying following actions:1. every v V (G), delete nodes Tv1 arcs incident nodes.2. every v V (G), add nodes v 1a v 1b arcs (v 1a , v 1b ) (v 1b , v 2 ).3. every v V (G) add binary tree Tv1a exactly |NG (v)| + 1 leaves.root Tv1a v 1a arcs Tv1a directed away v 1a . Furthermore,define lv1a bijective mapping NG (v) {lT2 (v)} leaves Tv1a .4. every v V (G) add arcs (v 1b , lT1 (v)) (lv1a (lT2 (v)), lT2 (v)).completes construction D0 . Next define local score function f 0 follows:1. every n V (D0 ) \ { v 1a : v V (G) } {r1 } set f 0 (n, PD0 (n)) = .2. every v V (G) set f 0 (v 1a , {v 1b }) = , f 0 (v 1b , {v 2 , lT1 (v)}) = , f 0 (v 2 , PD0 (v 2 ) \{v 1b }) = , f 0 (lT1 (v), PD0 (lT1 (v)) \ {v 1b }) = .289fiOrdyniak & Szeider3. set f 0 (r1 , PD0 (r1 )) = f 0 (r2 , PD0 (r2 ) {r1 }) = .4. remaining combinations n V (D0 ) P V (D0 ) set f 0 (n, P ) = 0.easy see Sf 0 maximum degree 3. Furthermore, using argumentsproof Claim 7 one show graph G independent set size kDAG D0 k 0 -O-neighbor D00 higher score D0 (with respectf 0 ) k 0 = 3k + 1 rev k 0 = 2(3k + 1) otherwise.Theorem 6 provides surprising contrast similar study k-local search MAX-SATproblem fixed-parameter tractable instances bounded degree (Szeider,2011). possible explanation surprising hardness k-O-Local Search BayesianStructure Learning could that, contrast MAX-SAT, global propertyentire instance (acyclicity) must checked.8. Directed Super-structureprevious sections considered problem Exact BNSL k-Local SearchBNSL certain restrictions undirected super-structure. However, every strictlyadmissible solution Exact BNSL actually contained restrictive directedsuper-structure. natural question whether additional information entaileddirected super-structure used find new structural restrictions ExactBNSL becomes tractable. well-known Exact BNSL becomes significantly easierordering variables given advance. instance, given orderingvariables BN, Exact BNSL becomes solvable polynomial time input givenarity-c representation (Teyssier & Koller, 2005). Fixing ordering variablesBN corresponds restricting search space acyclic directed super-structures.first observation section Exact BNSL solvable polynomial timedirected super-structure dag input problem given generalnon-zero representation. important note corresponding restrictionundirected super-structure, restricting directed super-structureacyclic impose restrictions undirected super-structure. Consideringpromising result becomes natural ask whether possible gradually generalizeclass acyclic directed super-structures. natural approach would considerdirected super-structures made acyclic deleting small number k nodes.approach looks promising known every fixed k directed superstructures made acyclic deleting k nodes recognized efficiently(Chen, Liu, Lu, OSullivan, & Razgon, 2008). However, show approachunlikely work Exact BNSL. Furthermore, correspondence resultsprevious sections, NP-hardness even holds additionally bound maximum in-degreeout-degree Sf .Theorem 7. Let N set nodes f local score function N Sfacyclic. find time O(|N |f ) dag maximal score f (D).Proof. Sf acyclic, follows every strictly admissible directed graphalso acyclic. Hence order compute dag highest score, sufficient290fiParameterized Complexity Exact BNSLcompute every n N parent set highest score. clearly donetime O(|N |f ) result follows.Corollary 2. Exact Bayesian Network Structure Learning solvable quadratictime acyclic directed super-structures.Proof. follows immediately Theorem 7 N f boundedtotal input size problem. Recall Section 3 local score function fgiven list tuples f non-zero hence f bounded totalinput size instance.Theorem 8. Exact Bayesian Network Structure Learning NP-hard instances Sf made acyclic deleting one node. Hardness even holdsadditionally bound maximum in-degree maximum out-degree Sf 3.Proof. devise polynomial reduction restricted version 3-SAT everyliteral contained two clauses. version 3-SAT still NP-complete(Garey & Johnson, 1979). Let 3-CNF formula variables x1 , . . . , xn clausesC1 , . . . , Cm Cj = lj,1 lj,2 lj,3 , every 1 j m. construct set N nodes,local score function f real number > 0.N contains nodes d0 , . . . , dn t0 , . . . , tn+m additionally:every 1 n nodes xi , xi , ai , bi .every 1 j nodes lj,1 , lj,2 , lj,3 , Cj .Let = n + 1 = 1. define f follows:set f (d0 , {t0 }) = f (t0 , t1 ) = .every 1 n set:f (di , {di1 }) = .f (ai , {di }) = f (xi , {ai }) = f (xi , {ai }) = .f (bi , {xi }) = f (bi , {xi }) = .f (ti , {ti+1 , bi }) = .every 1 j set:f (Cj , {lj,1 }) = f (Cj , {lj,2 }) = f (Cj , {lj,3 }) = .f (tn+j , {tn+j+1 , Cj }) = j < f (tn+j , {Cj }) = j = m.every 1 n, 1 j 1 l 3 set f (lj,l , {xi }) = lj,l = xif (lj,l , {xi }) = lj,l = xi .combinations v N P N set f (v, P ) = 0. Furthermore, set= (4n + 5m + 2) + n. example directed super-structure constructed3-CNF formula shown Figure 11. establish theorem showing followingclaims.291fiOrdyniak & SzeiderClaim 8. Sf made acyclic deleting one node.Claim 9. Sf maximum in-degree maximum out-degree 3.Claim 10. satisfiable dag score f (D) s.d0d1d2a1a2x1x2b1l1,2x3l1,3l2,1b3l2,2l2,3l3,1C2t1x3b2C1t0a3x2x1l1,1d3t2t3l3,2l3,3C3t4t5t6Figure 11: directed super-structure formula = (x1 x2 x3 ) (x1 x2 x3 )(x1 x2 x3 ) proof Theorem 8.easy see Sf d0 acyclic hence Claim 8 holds. Since every literaloccurs two clauses also easy verify Claim 9. proof Claim 10straightforward found appendix.9. Conclusionstudied computational complexity Bayesian Structure Learning (BNSL) various restrictions (directed) super-structure, considering Exact BNSLk-Local Search BNSL. obtained positive negative results theoreticalworst-case complexities problems. main positive result states Exact BNSLlinear-time tractable super-structure bounded treewidth bounded maximum degree. contrasted positive results negative results, using techniquesconcepts Parameterized Complexity. theoretical framework particularlywell-suited investigation allows fined-grained investigation takesstructural aspects problem instances account. results point combinations structural restrictions make problems tractable restrictions cannotdropped without loosing tractability. Considering various combinations restrictionssystematic way allows us draw broader picture complexity landscape (see tableSection 1). hope results provide better understanding principles292fiParameterized Complexity Exact BNSLBNSL contribute foundations. hope understanding alsouseful development heuristic methods practical BNSL systems.Acknowledgmentsshorter preliminary version paper appeared UAI 2010. Research supportedEuropean Research Council, grant reference 239962.AppendixProof Claim 1 (Lemma 2). Let us first assume R = (a, p, s) valid recordlet P = a(v0 ). Since R valid follows represents partial solution D(t)maximal respect R. Now, D0 = D[ (t0 )] partial solution t0follows Proposition 4 D0 = v0 furthermore ft0 (D0 ) = s. Hence,D0 represented record R0 = (a0 , p0 , s0 ) R, R0 P satisfyconditions claim. Furthermore, since R valid ft (D) = ft0 (D0 ) maximalityft0 (D0 ) respect R0 follows maximality ft (D) respect Rhence R0 valid record t0 .see converse let P Pf (v0 ) valid record R0 = (a0 , p0 , s0 ) R(t0 ) given.Let R = (a, p, s) triple defined via conditions (1)-(4). Since R0 valid recordrepresents partial solution D0 D0 maximal respect R0 . easysee digraph node set V (D0 ) {v0 } arc set E(D0 ) { (u, v0 ) : uP } { (v0 , u) : u (t0 ) v0 a0 (u) } acyclic p satisfies condition(5), i.e., p irreflexive. follows R represents p satisfies condition (5)furthermore maximality respect R follows maximality D0respect R0 . Hence, R valid record satisfies conditions(1)(5).Proof Claim 2 (Lemma 3). Let us first assume R = (a, p, s) valid record t.Since R valid record represents solution D(t) maximalrespect R. Now, let R0 = (a0 , p0 , s0 ) a0 (v0 ) = PD (v0 ), a0 (v) = PD (v) everyv (t), p0 union p tuples (v0 , v) (v, v0 ) v (t)directed path v0 v respectively v v0 s0 = f (v0 , PD (v0 )).Note Proposition 5 assume PD (v0 ) Pf (v0 ) hencealso represented R0 . follows maximality respect R R0maximal element R(t0 )[t].see converse let R = (a, p, s) maximal element R(t0 )[t]. Since R R(t0 )[t]follows record R0 = (a0 , p0 , s0 ) R(t0 ) R = R0 [t]. Hence,partial solution represented R0 ft0 (D) = s0 maximal respectpartial solutions represented R0 . Clearly, also represented R maximalityrespect R follows fact R maximal element R(t0 )[t].Proof Claim 3 (Lemma 4). Let us first assume R = (a, p, s) valid record t.Since R valid follows represents partial solution D(t)maximal respect R. Let D1 = D[ (t1 )] D2 = D[ (t2 )], i.e., D1 D2two subdigraphs induced nodes contained nodes subtrees293fiOrdyniak & Szeiderrooted t1 t2 , respectively. follows Proposition 3 = D1 D2V (D1 ) V (D2 ) = (t). Hence, D1 D2 partial solutions t1 t2 , respectively.{1, 2}, let Ri = (ai , pi , si ) = ai , (v, w) pidirected path v w Di si = fti (Di ). follows directly definitionR1 R2 represent D1 D2 , respectively, since ft (D) = ft1 (D1 ) + ft2 (D2 )maximality respect R implies maximality D1 D2 respectR1 R2 , respectively. Hence, R1 R2 valid records t1 t2 , respectively,easy see R, R1 R2 satisfy conditions claim.see converse let us assume given R1 = (a1 , p1 , s1 ) R(t1 )R2 = (a2 , p2 , s2 ) R(t2 ) triple R = (a, p, s) defined conditions (1)(3) satisfies condition (4). Since R1 R2 valid follows representpartial solutions D1 D2 D1 D2 maximal respect R1 R2 ,respectively. Furthermore, using Proposition 3 follows V (D1 ) V (D2 ) = (t)hence follows condition (4) = D1 D2 partial solution represented R.Now, ft (D) = ft1 (D1 ) + ft2 (D2 ) = s1 + s2 = maximality respectR follows maximality D1 D2 respect R1 R2 , respectively.follows R valid record t.Proof Claim 6 (Theorem 3). (1)(2) Suppose G k-clique K. f (D(E(K))) =nk |V (K)| + |E(K)| = nk + k remains show D(E(K)) acyclic.see note every cycle D(E(K)) use least one node V ,D(E(K)) contain arc two nodes A. since K clique, everynode v V either sink, i.e., v incoming arcs, source, i.e., voutgoing arcs hence cycle use node V .(2)(3) Suppose dag f (D) s. Let A0 set nodesf (a, PD (a)) = every A0 . follows definition f every aij A0unique edge e node Vi node Vj G PD (aij ) = e.Let E 0 E(G) set edges G correspond node A0 . followsE 0 representable claim every node v N least local scoreD(E 0 ) D. construction D(E 0 ) claim trivially satisfied everynode A. Similarly, every v V \ V (E 0 ) holds f (v, PD(E 0 ) (v)) = hencef (v, PD(E 0 ) (v)) f (v, PD (v)). Furthermore, every v V (E 0 ) holds f (v, PD (v)) =0 hence f (v, PD(E 0 ) (v)) f (v, PD (v)). follows f (D(E 0 )) f (D) s.(3)(1) Suppose E 0 E(G) representableedge set G f (D(E 0 )) s.show |V (E 0 )| = k |E 0 | = k2 implies E 0 edge setk-clique G.294fiParameterized Complexity Exact BNSLE 0 edge set, holds |E 0 ||V (E 0 )|2hence:f (D(E 0 )) nk = |V (E 0 )| + |E 0 ||V (E 0 )|0|V (E )| +2|V (E 0 )|02= |V (E )|(k k 1) +2k2= |V (E 0 )|(k 2 k 1) + (|V (E 0 )|2 |V (E 0 )|)k= |V (E 0 )|k 2 + |V (E 0 )|2 k + |V (E 0 )|k |V (E 0 )|k + |V (E 0 )|= |V (E 0 )|k 2 + |V (E 0 )|2 k + |V (E 0 )|f (D(E 0 )) s, follows |V (E 0 )|k 2 + |V (E 0 )|2 k + |V (E 0 )| 1 hence:|V (E 0 )|k 2 + |V (E 0 )|2 k + |V (E 0 )| 1k 2 + |V (E 0 )|k + 1|V (E 0 )|1|V (E 0 )|12|V (E 0 )| + k 1k11|V (E 0 )| k +k |V (E 0 )|kSince |V (E 0 )| integer k > 2, |V (E 0 )| k.Furthermore, since E 0 representable contain k2 edges hencef (D(E 0 ))nk |V (E 0 )| + k2 . Again, follows f (D(E 0 )) |V (E 0 )| +k2 1 and:k0|V (E )| +12|V (E 0 )|(k 2 k 1) + k 3 k 2 1|V (E 0 )|(k 2 k 1) k 3 + k 2 + 1k 3 + k 2 + 1|V (E 0 )|k2 k 1k+1|V (E 0 )| k + 2k k100Since |V (E )| integer k > 2, follows |V (E )| k hence |V (E 0 )| = k.Using f (D(E 0 )) nk 1 get:k + |E 0 | 1(k 3 k 2 k) + |E 0 |2k 1k3 k2 k + 1|E 0 |2k2k k 1 + k1|E 0 |21 k1k|E 0 |22295fiOrdyniak & Szeider|E 0 | integer k > 2, follows |E 0 |f (D(E 0 )) implies |V (E 0 )| = k |E 0 | =k-clique G.k. Putting everything together2k02 . Hence E edge setProof Claim 7 (Theorem 6). first show claim case revk 0 = 2k + 1.Let us first assume G independent set V (G) size least k.obtain D0 reversing k 0 arcs { (v 1 , v 2 ), (v 1 , lT1 (v)) : v } {(r2 , r1 )}.decreases score r1 increases score nodes { v 1 : v }score nodes remains unchanged. Hence, f (D0 ) =f (D) + |S| + k = f (D) + 1 > f (D) remains show D0 acyclic.see assume D0 contains cycle C. acyclic C must containleast one newly created arcs D0 , i.e., C must contain either arc (v 2 , v 1 ), arc(lT1 (v) , v 1 ) v arc (r1 , r2 ). r2 sink D0 , i.e., r2outgoing arcs, cycle C cannot contain arc (r1 , r2 ). Similarly, D0contain directed path v 1 lT1 (v) cycle C cannot contain arc (lT1 (v), v 1 )v S. Hence cycle C must contain arc (v 2 , v 1 ) v S. supposeC contains arc (v 2 , v 1 ) v S. D0 contains directed pathsnode T2 node T1 follows C cannot leave node v 1 using arc(v 1 , lT2 (v)). Consequently, cycle C must leave node v 1 towards w2 neighborw v G. independent set G follows w/ hence nodew2 sink D0 contradicting existence cycle C.see reverse direction assume D0 dag obtained reversingk 0 arcs. Note nodes { v 1 : v V (G) } nodes whose scoresyet maximum. Hence, order D0 higher score scoreleast one nodes increased. Now, score node v 1v V (G) increased reversing arcs (v 1 , v 2 ) (v 1 , lT1 (v)). easysee reversing arc (v 1 , lT1 (v)) introduces cycle C uses nodesV (T1 ) V (T2 ) {v 1 }. However, every cycle C contains arc (r2 , r1 )destroy cycles additionally reversing arc (r2 , r1 ). reversing(r2 , r1 ) decrease score r1 also cheapest way destroycycles. Now, = (k 1) follows order increase score scoresleast k nodes { v 1 : v V (G) } increased . Let set nodesV (G) score nodes { v 1 : v } increased manner.mentioned |S| k remains show independent set G.Suppose independent set let u, v {u, v} E(G).arcs (v 2 , v 1 ) (u2 , u1 ) together directed path v 1 u2 (using arcsTv1 Tu2 ) directed path u1 v 2 (using arcs Tu1 Tv2 ) formcycle D0 contradicting acyclicity D0 . follows independent set Gsize least k.Hence, shown theorem case rev O. remains showtheorem remaining non-trivial set rev/ O, i.e., set = {add, del}.k 0 = 2(2k + 1) idea replace every reversal arc (u, v) deletion(of (u, v)) addition (of (v, u)).296fiParameterized Complexity Exact BNSLProof Claim 10 (Theorem 8). prove Claim 10 help followingclaim.Claim 11. f (D) acyclic satisfies following conditions:1 contains least following arcs:arc (t0 , d0 ).every 1 n, arcs (di1 , di ), (di , ai ), (bi , ti ) (ti , ti1 ).every 1 n, 1 j 1 l 3 arc (xi , lj,l ) lj,l = xisimilarly arc (xi , lj,l ) lj,l = xi .every 1 j m, arcs (Cj , tn+j ) (tn+j , tn+j1 ) exactly onearcs (lj,1 , Cj ), (lj,2 , Cj ) (lj,3 , Cj ).2 every 1 n digraph contains arcs (ai , xi ) (xi , bi )arcs (ai , xi ) (xi , bi ) contains arcs (ai , xi ) (xi , bi ) arcs(ai , xi ) (xi , bi ).3 1 n, 1 j 1 l 3, following holds:lj,l = xi contains arc (lj,l , Cj ) contain arc(ai , xi ).lj,l = xi contains arc (lj,l , Cj ) contain arc(ai , xi ).first show previous claim used prove Claim 10. Supposesatisfiable let satisfying assignment . Let digraph satisfiescondition 1 additionally:every 1 n (xi ) = true contains arcs (ai , xi ) (xi , bi ),otherwise contains arcs (ai , xi ) (xi , bi ).every 1 j let lj,l literal clause Cj satisfied ; sincesatisfying assignment every clause Cj contains literal. containsarc (lj,l , Cj ).follows satisfies conditions 2 3 hence (using Claim 11) f (D)acyclic.see reverse let dag f (D) s. follows Claim 11satisfies conditions 1 3. claim assignment (xi ) = truecontain arc (ai , xi ) satisfying assignment . followscondition 1 every 1 j digraph contains arc (lj,l , Cj )1 l 3. W.l.o.g., assume lj,l = xi 1 n (the case lj,l = xianalog). using condition 1 follows contains arc (xi , lj,l ).condition 3 digraph contain arc (ai , xi ) hence (lj,l ) = true.Hence remains show Claim 11. Let us first show every dag f (D)satisfies conditions 1 3. see observe every node V = {x1 , x1 , . . . , xn , xn }297fiOrdyniak & Szeidereither score 0 . Similarly, every node V 0 = N \ V either score 0 .follows every directed graph 4n + 5m + 2 nodes score2n nodes score . Hence, maximum score every directed graph(4n + 5m + 2) + 2n. > n f (D) follows every nodeV 0 must score similarly least n 2n nodes V must score .Hence satisfies condition 1.show condition 2 observe every 1 n node bi mustscore holds exactly one arcs (xi , bi ) (xi , bi ) D. Now,contains arc (xi , bi ) 1 n cannot contain arc (ai , xi )otherwise would contain cycle (d, ai , xi , bi , t, d). Similarly, contains arc (xi , bi )1 n cannot contain arc (ai , xi ). follows every 1 nleast one arcs (ai , xi ) (ai , xi ) missing D. Since n nodesV score 0 follows every 1 n exactly one arcs (ai , xi )(ai , xi ) must D. follows satisfies condition 2.see condition 3 suppose 1 n, 1 j 1 l 3 lj,l = xidigraph contains arcs (lj,l , Cj ) (ai , xi ). follows would containcycle (d, ai , xi , lj,l , Cj , t, d), contradiction. case lj,l = xi analog hencesatisfies condition 3.see reverse implication claim suppose digraph satisfiesconditions 1 3. easy see f (D) = hence remains showacyclic. Sf (t0 , d0 ) acyclic follows every cycle usearc (t0 , d0 ). Hence contains cycle directed path P d0 t0D. follows condition 2 directed path d0 bi D,1 3, hence P cannot contain node bi . Since nodesarcs {t0 , . . . , tn+m } nodes C1 , . . . , Cm follows P use node Cj1 j m. condition 1 node Cj exactly one incoming neighbor(one lj,1 , lj,2 , lj,3 ) say lj,l . using condition 1 node lj,l exactly one incomingneighbor xi xi lj,l = xi lj,l = xi , respectively. W.l.o.g. let us assume lj,l = xi .follows condition 3 xi incoming neighbor hence contains directedpath P d0 t0 .ReferencesBang-Jensen, J., & Gutin, G. (2009). Digraphs (Second edition). Springer MonographsMathematics. Springer-Verlag London Ltd., London.Bodlaender, H. L. (1993). tourist guide treewidth. Acta Cybernetica, 11, 121.Bodlaender, H. L. (2005). Discovering treewidth. Proceedings 31st ConferenceCurrent Trends Theory Practice Computer Science (SOFSEM05), Vol.3381 Lecture Notes Computer Science, pp. 116. Springer Verlag.Bodlaender, H. L. (1996). linear-time algorithm finding tree-decompositions smalltreewidth. SIAM J. Comput., 25 (6), 13051317.Bodlaender, H. L. (1997). Treewidth: algorithmic techniques results. Mathematical foundations computer science 1997 (Bratislava), Vol. 1295 Lecture NotesComputer Science, pp. 1936. Springer, Berlin.298fiParameterized Complexity Exact BNSLBodlaender, H. L., & Koster, A. M. C. A. (2008). Combinatorial optimization graphsbounded treewidth. Comput. J., 51 (3), 255269.Chechetka, A., & Guestrin, C. (2007). Efficient principled learning thin junction trees.Platt, J. C., Koller, D., Singer, Y., & Roweis, S. T. (Eds.), Advances Neural Information Processing Systems 20, Proceedings Twenty-First Annual ConferenceNeural Information Processing Systems, Vancouver, British Columbia, Canada,December 3-6, 2007. MIT Press.Chen, J., Liu, Y., Lu, S., OSullivan, B., & Razgon, I. (2008). fixed-parameter algorithmdirected feedback vertex set problem. J. ACM, 55 (5), Art. 21, 19.Chickering, D. M. (1995). transformational characterization equivalent Bayesian network structures. Uncertainty artificial intelligence (Montreal, PQ, 1995), pp.8798. Morgan Kaufmann, San Francisco, CA.Chickering, D. M. (1996). Learning Bayesian networks NP-complete. Learningdata (Fort Lauderdale, FL, 1995), Vol. 112 Lecture Notes Statist., pp. 121130.Springer Verlag.Chow, C. I., & Liu, C. N. (1968). Approximating discrete probability distributionsdependence trees. IEEE Transactions Information Theory, 14, 462467.Darwiche, A. (2001). Recursive conditioning. Artificial Intelligence, 126 (1-2), 541.Dasgupta, S. (1999). Learning polytrees. Laskey, K. B., & Prade, H. (Eds.), UAI99: Proceedings Fifteenth Conference Uncertainty Artificial Intelligence,Stockholm, Sweden, July 30-August 1, 1999, pp. 134141. Morgan Kaufmann.de Campos, C. P., Zeng, Z., & Ji, Q. (2009). Structure learning Bayesian networks usingconstraints. Danyluk, A. P., Bottou, L., & Littman, M. L. (Eds.), Proceedings26th Annual International Conference Machine Learning, ICML 2009, Montreal, Quebec, Canada, June 14-18, 2009, Vol. 382 ACM International ConferenceProceeding Series, p. 15. ACM.Dechter, R. (1999). Bucket elimination: unifying framework reasoning. ArtificialIntelligence, 113 (1-2), 4185.Diestel, R. (2000). Graph Theory (2nd edition)., Vol. 173 Graduate Texts Mathematics.Springer Verlag, New York.Dow, P. A., & Korf, R. E. (2007). Best-first search treewidth. ProceedingsTwenty-Second AAAI Conference Artificial Intelligence, July 22-26, 2007, Vancouver, British Columbia, Canada, pp. 11461151. AAAI Press.Downey, R. G., & Fellows, M. R. (1999). Parameterized Complexity. Monographs Computer Science. Springer Verlag, New York.Downey, R. G., & Fellows, M. R. (1995). Fixed-parameter tractability completeness.II. completeness W [1]. Theoret. Comput. Sci., 141 (1-2), 109131.Downey, R. G., Fellows, M. R., & Langston, M. A. (2008). computer journal specialissue parameterized complexity: Foreword guest editors. ComputerJournal, 51 (1), 16.299fiOrdyniak & SzeiderElidan, G., & Gould, S. (2008). Learning bounded treewidth Bayesian networks. Koller,D., Schuurmans, D., Bengio, Y., & Bottou, L. (Eds.), Advances Neural InformationProcessing Systems 21, Proceedings Twenty-Second Annual Conference Neural Information Processing Systems, Vancouver, British Columbia, Canada, December8-11, 2008, pp. 417424. MIT Press.Fellows, M. R. (2003). Blow-ups, win/wins, crown rules: new directions FPT.Bodlaender, H. L. (Ed.), Graph-Theoretic Concepts Computer Science (WG2003), Vol. 2880 Lecture Notes Computer Science, pp. 112. Springer Verlag.Fellows, M. R., Rosamond, F. A., Fomin, F. V., Lokshtanov, D., Saurabh, S., & Villanger,Y. (2009). Local search: brute-force avoidable?. Boutilier, C. (Ed.), IJCAI2009, Proceedings 21st International Joint Conference Artificial Intelligence,Pasadena, California, USA, July 11-17, 2009, pp. 486491.Fellows, M. R. (2012) Personal Communication.Flum, J., & Grohe, M. (2006). Parameterized Complexity Theory, Vol. XIV TextsTheoretical Computer Science. EATCS Series. Springer Verlag, Berlin.Friedman, N., Nachman, I., & Peer, D. (1999). Learning Bayesian network structuremassive datasets: sparse candidate algorithm. Laskey, K. B., & Prade, H.(Eds.), UAI 99: Proceedings Fifteenth Conference Uncertainty Artificial Intelligence, Stockholm, Sweden, July 30-August 1, 1999, pp. 206215. MorganKaufmann.Garey, M. R., & Johnson, D. R. (1979). Computers Intractability. W. H. FreemanCompany, New York, San Francisco.Gaspers, S., Kim, E. J., Ordyniak, S., Saurabh, S., & Szeider, S. (2012). Dont strictlocal search!. Proceedings Twenty-Sixth AAAI Conference ArtificialIntelligence, AAAI 2012, Toronto, Ontaria, Canada, July 22-26, 2012. AAAI Press.appear.Gaspers, S., Koivisto, M., Liedloff, M., Ordyniak, S., & Szeider, S. (2012). findingoptimal polytrees. appear AAAI 2012.Gelfand, A., Kask, K., & Dechter, R. (2011). Stopping rules randomized greedy triangulation schemes. Burgard, W., & Roth, D. (Eds.), Proceedings Twenty-FifthAAAI Conference Artificial Intelligence, AAAI 2011, San Francisco, California,USA, August 7-11, 2011. AAAI Press.Gogate, V., & Dechter, R. (2004). complete anytime algorithm treewidth. Proceedings Proceedings Twentieth Conference Annual Conference UncertaintyArtificial Intelligence (UAI-04), pp. 201208, Arlington, Virginia. AUAI Press.Greco, G., & Scarcello, F. (2010). power structural decompositions graph-basedrepresentations constraint problems. Artificial Intelligence, 174 (56), 382409.Heckerman, D., Geiger, D., & Chickering, D. M. (1995). Learning Bayesian networks:combination knowledge statistical data. Machine Learning, 20 (3), 197243.Impagliazzo, R., Paturi, R., & Zane, F. (2001). problems strongly exponentialcomplexity?. J. Computer System Sciences, 63 (4), 512530.300fiParameterized Complexity Exact BNSLKarger, D. R., & Srebro, N. (2001). Learning markov networks: maximum bounded treewidth graphs. SODA, pp. 392401.Karp, R. M. (1972). Reducibility among combinatorial problems. Complexity computer computations (Proc. Sympos., IBM Thomas J. Watson Res. Center, YorktownHeights, N.Y., 1972), pp. 85103. Plenum, New York.Kask, K., Gelfand, A., Otten, L., & Dechter, R. (2011). Pushing power stochasticgreedy ordering schemes inference graphical models. Burgard, W., & Roth, D.(Eds.), Proceedings Twenty-Fifth AAAI Conference Artificial Intelligence,AAAI 2011, San Francisco, California, USA, August 7-11, 2011. AAAI Press.Khuller, S., Bhatia, R., & Pless, R. (2003). local search placement metersnetworks. SIAM J. Comput., 32 (2), 470487.Kloks, T. (1994). Treewidth: Computations Approximations. Springer Verlag, Berlin.Koivisto, M. (2006). Advances exact Bayesian structure discovery Bayesian networks.UAI 06, Proceedings 22nd Conference Uncertainty Artificial Intelligence, July 13-16 2006, Cambridge, MA, USA. AUAI Press.Kojima, K., Perrier, E., Imoto, S., & Miyano, S. (2010). Optimal search clusteredstructural constraint learning Bayesian network structure. J. Mach. Learn. Res.,11, 285310.Koster, A. M. C. A., Bodlaender, H. L., & van Hoesel, S. P. M. (2001). Treewidth: Computational experiments. Electronic Notes Discrete Mathematics, 8, 5457.Krokhin, A. A., & Marx, D. (2012). hardness losing weight. ACM TransactionsAlgorithms, 8 (2), 19.Kwisthout, J., Bodlaender, H. L., & van der Gaag, L. C. (2010). necessity boundedtreewidth efficient inference Bayesian networks. ECAI, pp. 237242.Marx, D. (2008). Searching k-change neighborhood TSP W[1]-hard. Oper. Res.Lett., 36 (1), 3136.Marx, D., & Schlotter, I. (2011). Stable assignment couples: Parameterized complexitylocal search. Discrete Optimization, 8 (1), 2540.Meek, C. (2001). Finding path harder finding tree. J. Artif. Intell. Res., 15,383389.Narasimhan, M., & Bilmes, J. A. (2004). PAC-learning bounded tree-width graphical models. Chickering, D. M., & Halpern, J. Y. (Eds.), UAI 04, Proceedings 20thConference Uncertainty Artificial Intelligence, July 7-11 2004, Banff, Canada,pp. 410417. AUAI Press.Niedermeier, R. (2006). Invitation Fixed-Parameter Algorithms. Oxford Lecture SeriesMathematics Applications. Oxford University Press, Oxford.Ordyniak, S., & Szeider, S. (2010). Algorithms complexity results exact Bayesianstructure learning. Grunwald, P., & Spirtes, P. (Eds.), Proceedings UAI 2010,26th Conference Uncertainty Artificial Intelligence, Catalina Island, California,USA, July 8-11, 2010. AUAI Press, Corvallis, Oregon.301fiOrdyniak & SzeiderOtt, S., Imoto, S., & Miyano, S. (2004). Finding optimal models small gene networks.Altman, R. B., Dunker, A. K., Hunter, L., Jung, T. A., & Klein, T. E. (Eds.), Biocomputing 2004, Proceedings Pacific Symposium, Hawaii, USA, 6-10 January2004, pp. 557567. World Scientific.Parviainen, P., & Koivisto, M. (2010). Bayesian structure discovery Bayesian networksless space. J. Mach. Learn. Res., 9, 589596.Perrier, E., Imoto, S., & Miyano, S. (2008). Finding optimal Bayesian network givensuper-structure. J. Mach. Learn. Res., 9, 22512286.Pieter, A., Daphne, K., & Andrew, Y. N. (2006). Learning factor graphs polynomial timesample complexity. J. Mach. Learn. Res., 7, 17431788.Pietrzak, K. (2003). parameterized complexity fixed alphabet shortest common supersequence longest common subsequence problems. J. ComputerSystem Sciences, 67 (4), 757771.Silander, T., & Myllymaki, P. (2006). simple approach finding globally optimal Bayesian network structure. UAI 06, Proceedings 22nd ConferenceUncertainty Artificial Intelligence, July 13-16 2006, Cambridge, MA, USA. AUAIPress.Szeider, S. (2011). parameterized complexity k-flip local search SAT MAXSAT. Discrete Optimization, 8 (1), 139145.Teyssier, M., & Koller, D. (2005). Ordering-based search: simple effective algorithmlearning bayesian networks. UAI 05, Proceedings 21st ConferenceUncertainty Artificial Intelligence, Edinburgh, Scotland, July 26-29, 2005, pp. 548549. AUAI Press.Tsamardinos, I., Brown, L., & Aliferis, C. (2006). max-min hill-climbing Bayesiannetwork structure learning algorithm. Machine Learning, 65, 3178.Tsang, E. P. K. (1993). Foundations Constraint Satisfaction. Academic Press.Yuan, C., Malone, B., & Wu, X. (2011). Learning optimal Bayesian networks using A*search. Walsh, T. (Ed.), IJCAI 2011, Proceedings 22nd International JointConference Artificial Intelligence, Barcelona, Catalonia, Spain, July 16-22, 2011,pp. 21862191. IJCAI/AAAI.302fiJournal Artificial Intelligence Research 46 (2013) 129-163Submitted 09/12; published 01/13Undominated Groves MechanismsMingyu GuoMingyu.Guo@liverpool.ac.ukUniversity Liverpool, UKEvangelos Markakismarkakis@gmail.comAthens University Economics Business, GreeceKrzysztof R. Aptapt@cwi.nlCWI, NetherlandsVincent Conitzerconitzer@cs.duke.eduDuke University, USAAbstractfamily Groves mechanisms, includes well-known VCG mechanism (alsoknown Clarke mechanism), family efficient strategy-proof mechanisms.Unfortunately, Groves mechanisms generally budget balanced. is,mechanisms, payments may flow system agents, resultingdeficits reduced utilities agents. consider following problem: withinfamily Groves mechanisms, want identify mechanisms give agentshighest utilities, constraint mechanisms must never incur deficits.adopt prior-free approach. introduce two general measures comparingmechanisms prior-free settings. say non-deficit Groves mechanism individually dominates another non-deficit Groves mechanism every type profile,every agents utility less , holds strictinequality least one type profile one agent. say non-deficit Grovesmechanism collectively dominates another non-deficit Groves mechanism everytype profile, agents total utility less , holdsstrict inequality least one type profile. definitions induce two partialorders non-deficit Groves mechanisms. study maximal elements correspondingtwo partial orders, call individually undominated mechanismscollectively undominated mechanisms, respectively.1. IntroductionMechanism design often employed coordinating group decision making among agents.Often, mechanisms impose payments agents pay central authority.Although maximizing revenue desirable objective many settings (for example,mechanism auction designed seller), desirable situationsentity profiting payments. examples include public project problems wellcertain resource allocation problems without seller (e.g., right use shared goodgiven time slot, assignment take-off slots among airline companies). cases,would like mechanisms minimize payments (or, even better, achieve budgetbalance), maintaining desirable properties, efficient, strategy-proofnon-deficit (i.e., mechanism need funded external source).c2013AI Access Foundation. rights reserved.fiGuo, Markakis, Apt, & Conitzerfamily Groves mechanisms, includes well-known VCG mechanism (alsoknown Clarke mechanism), family efficient strategy-proof mechanisms.many sufficiently general settings, including settings study paper,Groves mechanisms efficient strategy-proof mechanisms (Holmstrom,1979). Unfortunately though, Groves mechanisms generally budget balanced.is, mechanisms, payments may flow system agents,resulting deficits reduced utilities agents. Motivated considerpaper following problem: within family Groves mechanisms, wantidentify mechanisms give agents highest utilities, constraintmechanisms never incur deficits.1adopt prior-free approach, agent knows valuation vi ,prior probability distribution agents values. introducetwo natural measures comparing mechanisms prior-free settings. Given performanceindicator, say mechanism individually dominates mechanism every typeprofile agents, performs worse perspective individualagent, holds strict inequality least one type profile one agent.say mechanism collectively dominates mechanism every type profile,performs worse perspective set agents whole,holds strict inequality least one type profile. paper, focusmaximizing agents utilities. Given specific performance indicator, individualcollective dominance determined comparing either individual utilities sumagents utilities, respectively.definitions induce two partial orders non-deficit Groves mechanisms.goal work identify study maximal elements corresponding twopartial orders, call individually undominated (non-deficit Groves) mechanismscollectively undominated (non-deficit Groves) mechanisms, respectively.noted partial orders focus may different partial orders inducedperformance indicators, e.g., criterion revenue extracted agents.1.1 Structure Paperpresentation results structured follows: Sections 2 3, formallydefine notions individual collective dominance, well family Grovesmechanisms, provide basic observations. also establish generalproperties anonymous Groves mechanisms use later on, mayindependent interest. begin study individual dominance Section 4,give characterization individually undominated mechanisms. also proposetwo techniques transforming given non-deficit Groves mechanism oneindividually undominated.Sections 5 6 study question finding collectively undominated mechanisms two settings. first (Section 5) auctions multiple identical unitsunit-demand bidders. setting, VCG mechanism collectively dominated1. agents utilities may increased also consider mechanisms outside Grovesfamily (Guo & Conitzer, 2008a; de Clippel, Naroditskiy, & Greenwald, 2009; Faltings, 2005; Guo,Naroditskiy, Conitzer, Greenwald, & Jennings, 2011), paper take efficiency hardconstraint.130fiUndominated Groves Mechanismsnon-deficit Groves mechanisms, Bailey-Cavallo mechanism (Bailey, 1997;Cavallo, 2006). obtain complete characterization collectively undominated mechanisms anonymous linear (meaning redistribution linear functionordered type profile; see Section 5 definition). particular, showcollectively undominated mechanisms anonymous linear exactlyOptimal-in-Expectation Linear (OEL) redistribution mechanisms, include BaileyCavallo mechanism introduced Guo Conitzer (2010). second setting(Section 6) public project problems, agents must decide whetherfinance project. show case agents identical participation costs, VCG mechanism collectively undominated. hand,participation costs different across agents, exist mechanisms collectivelydominate VCG. finally show participation costs different across agents,VCG mechanism remains collectively undominated among pay-only mechanisms.1.2 Related Workefficiently allocate resources among group competing agents well-studiedtopic economics literature. example, famous Myerson-Satterthwaite Theorem (Myerson & Satterthwaite, 1983) rules existence efficient, Bayes-Nash incentive compatible, budget-balanced, individually rational mechanisms. Cramton, Gibbons,Klemperer (1987) characterized Bayes-Nash incentive compatible individually rational mechanisms dissolving partnership, gave necessary sufficient conditionpossibility dissolving partnership efficiently.main difference papers adopt prior-free approach. is, assume know prior distribution agentsvaluations. result this, notion truthfulness strategy-proofness,stronger Bayes-Nash incentive compatibility. many sufficiently general settings,including settings study paper, Groves mechanismsefficient strategy-proof mechanisms (Holmstrom, 1979). is, search undominated Groves mechanisms is, many settings, search efficient, strategy-proof,non-deficit mechanisms closest budget-balance.Recently, series works VCG redistribution mechanisms,mechanisms make social decisions according efficient strategy-proofVCG mechanism, redistribute VCG payments back agents,certain constraints, agents redistribution independenttype (therefore ensuring strategy-proofness), total redistributionnever exceed total VCG payment (therefore ensuring non-deficit). Actually,non-deficit Groves mechanism interpreted VCG-based redistributionmechanism, (non-deficit) VCG redistribution mechanism corresponds nondeficit Groves mechanism (more details provided Section 2).One example redistribution mechanism Bailey-Cavallo (BC) mechanism (Cavallo, 2006).2 BC mechanism, every agent, besides participating VCG2. settings revenue monotonic, Cavallo (2006) mechanism coincides mechanismdiscovered earlier Bailey (1997). Bailey-Cavallo mechanism single-item auction alsoindependently discovered Porter, Shoham, Tennenholtz (2004).131fiGuo, Markakis, Apt, & Conitzermechanism, also receives n1 times minimal VCG revenue could obtainedchanging agents bid. settings (e.g., single-item auction), BCmechanism successfully redistribute large portion VCG payments backagents. is, settings, BC mechanism individually collectivelydominates VCG mechanism.Guo Conitzer (2009) proposed another VCG redistribution mechanism calledworst-case optimal (WCO) redistribution mechanism, setting multi-unit auctionsnonincreasing marginal values. WCO optimal terms fraction total VCGpayment redistributed worst case.3 Moulin (2009) independently derived WCOslightly different worst-case optimality notion (in restrictive setting multi-unitauctions unit demand only). Guo Conitzer (2010) also proposed family VCGredistribution mechanisms aim maximize expected amount VCG paymentredistributed, setting multi-unit auctions unit demand. membersfamily called Optimal-in-Expectation Linear (OEL) redistribution mechanisms.Finally, paper closest study early work Moulincollectively undominated non-deficit Groves mechanisms (Moulin, 1986). dealsproblem selecting efficient public decision finitely many costless alternatives.4agent submits central authority utility alternative. Subsequently,central authority makes decision maximizes social welfare. Moulin (1986,Lemma 2) showed VCG mechanism collectively undominated setting.result generalizes earlier result case two public decisions LaffontMaskin (1997).2. Preliminariesfirst briefly review payment-based mechanisms (see, e.g., Mas-Colell, Whinston, &Green, 1995).2.1 Payment-Based MechanismsAssume set possible outcomes decisions D, set {1, . . ., n} agentsn 2, agent i, set types (initial ) utility functionvi : R. Let := 1 n .(direct revelation) mechanism, agent reports type based this,mechanism selects outcome payment made every agent. Hencemechanism given pair functions (f, t), f decision functionpayment function determines agents payments, i.e., f : D, : Rn .put ti () := (t())i , i.e., function ti computes payment agent i.vector announced types, ti () 0, agent pays ti (), ti () < 0, receives|ti ()|. true type agent announced type , final utilityfunction definedui ((f, t)(i , ), ) := vi (f (i , ), ) ti (i , ),3. notion worst-case optimality also studied general settings (Gujar & Narahari, 2011;Guo, 2011, 2012).4. public project model, cost associated building project.132fiUndominated Groves Mechanismsvector types announced agents.2.2 Properties Payment-Based Mechanismssay payment-based mechanism (f, t)PPefficient D, ni=1 vi (f (), ) ni=1 vi (d, ),Pbudget-balanced ni=1 ti () = 0 ,Pnon-deficit ni=1 ti () 0 , i.e., mechanism need fundedexternal source,pay-only ti () 0 {1, . . ., n},strategy-proof , {1, . . ., n}, ,ui ((f, t)(i , ), ) ui ((f, t)(i , ), ),i.e., agent i, reporting false type, , profitable.2.3 Individual Collective Dominanceconsider prior-free settings, agent knows function vi ,belief prior probability distribution regarding agents initial utilities.Payment-based mechanisms naturally compared terms either effectindividual agent global effect whole set agents. therefore introducetwo measures comparing mechanisms. Given performance indicator5 , saymechanism (f , ) individually dominates mechanism (f, t) every type profile,(f , ) performs worse (f, t) perspective every agent, holdsstrict inequality least one type profile one agent. say mechanism(f , ) collectively dominates mechanism (f, t) every type profile, (f , ) performsworse (f, t) perspective whole agent system, holds strictinequality least one type profile. paper, focus maximizing agentsutilities. Given specific performance indicator, individual collective dominancecaptured following definitions:Definition 2.1 Given two payment-based mechanisms (f, t) (f , ), say (f , )individually dominates (f, t){1, . . ., n}, ui ((f, t)(), ) ui ((f , )(), ),{1, . . ., n}, ui ((f, t)(), ) < ui ((f , )(), ).Definition 2.2 Given two payment-based mechanisms (f, t) (f , ), say (f , )collectively dominates (f, t)5. performance indicator mean function mechanisms outcome serves measurecomparing mechanisms. E.g., final utility agent, arbitrary function it,function agents payment function depends decision rulepayment rule mechanism.133fiGuo, Markakis, Apt, & Conitzer,Pn,i=1 ui ((f, t)(), )Pni=1 ui ((f, t)(), )Pni=1 ui ((f<Pn, )(),i=1 ui ((f),, )(),).two payment-based mechanisms (f, t) (f , ), clearly (f , ) individually dominates (f, t), also collectively dominates (f, t). Theorem 3.4 shows reverseimplication however need hold, even limit special typesmechanisms. is, fact (f , ) collectively dominates (f, t) imply(f , ) individually dominates (f, t).Given set Z payment-based mechanisms, individual collective dominance inducetwo partial orders Z, interested studying maximal elementsrespect partial orders. maximal element respect first partial ordercalled individually undominated mechanism, i.e., mechanismindividually dominated mechanism Z. maximal element secondpartial order called collectively undominated mechanism, i.e., mechanismcollectively dominated mechanism Z. maximal elementsrespect two partial orders may differ particular, notion collectivelyundominated mechanisms generally stronger notion. Clearly, (f , ) Z collectivelyundominated, also individually undominated. reverse may true,examples provided Section 4.2.focus decision function f , individual collective dominancestrictly due difference payment functions. Hence, (f, ) individually dominates(f, t) (or simply individually dominates t){1, . . ., n}, ti () ti (),{1, . . ., n}, ti () > ti (),collectively dominatesPP, ni=1 ti () ni=1 ti (),PP, ni=1 ti () > ni=1 ti ().define two transformations payment-based mechanisms originatingdecision function. transformations build upon surplus-guarantee concept(Cavallo, 2006) specific case VCG mechanism6 .Consider payment-based mechanismPn (f, t). Given = (1 , . . ., n ), let ()total amount payments, i.e., () := i=1 ti (). {1, . . ., n} letSiBCGC (i ) := inf (i , ).words, SiBCGC (i ) surplus guarantee independent report agent i.define payment-based mechanism (f, tBCGC ) setting {1, . . ., n}tBCGC() := ti ()SiBCGC (i ).n6. first transformation originally defined Bailey (1997) Cavallo (2006) specific caseVCG mechanism Guo Conitzer (2008b) non-deficit Groves mechanisms. callBCGC transformation authors papers (Bailey, Cavallo, Guo, Conitzer).134fiUndominated Groves MechanismsAlso, fixed agent j, define payment-based mechanism (f, tBCGC(j) )setting {1, . . ., n}ti () SiBCGC (i )= jBCGC(j)() :=titi ()6= jfirst transformation (from (f, t) (f, tBCGC )), every agent receives additional7 amount n1 times surplus guarantee independent type.second transformation (from (f, t) (f, tBCGC(j) ), agent j chosen agentreceives additional amount. additional amount equals entirety surplus guarantee independent js type. transformations agents additionalpayments independent types, thus strategy-proofness maintained:(f, t) strategy-proof, (f, tBCGC ) (f, tBCGC(j) ) j.following observations generalize earlier results Bailey (1997)Cavallo (2006).Proposition 2.3(i) payment-based mechanism form tBCGC non-deficit.(ii) non-deficit, either tBCGC coincide tBCGC individually (and hencealso collectively) dominates t.Proof. (i) {1, . . ., n} () SiBCGC (i ),BCGC() ==nXi=1nXi=1tBCGC()= ()nXBCGC (i )i=1n() SiBCGC (i )0.n(ii) non-deficit, {1, . . ., n} SiBCGC (i ) 0,hence tBCGC() ti ().2claims hold tBCGC(j) j {1, . . ., n}, equally simple proofs.3. Groves Mechanismsfirst briefly review Groves mechanisms.3.1 PreliminariesRecall Groves (1973) Mechanism payment-based mechanism (f, t)following hold8 :7. Receiving additional positive amount means paying less receiving additional negative amountmeans paying more.P8. j6=i shorthand summation j {1, . . ., n}, j 6= i.135fiGuo, Markakis, Apt, & ConitzerPf () arg maxdD ni=1 vi (d, ), i.e., chosen outcome maximizes allocationwelfare (the agents total valuation),ti : R defined ti () := hi (i ) gi (),Pgi () := j6=i vj (f (), j ),hi : R arbitrary function.gi () represents allocation welfare decision f () agent ignored.Recall following crucial result (see, e.g., Mas-Colell et al., 1995).Groves Theorem (Groves, 1973) Every Groves mechanism efficient strategyproof.several decision problems, efficient strategy-proof payment-based mechanisms Groves mechanisms. implied general result Holmstrom (1979),covers two domains consider Sections 5 6, explains focusGroves mechanisms. Hence on, use term mechanism referGroves mechanism.Focusing set non-deficit Groves mechanisms, individually (respectively, collectively) undominated mechanisms mechanisms set individually(respectively, collectively) dominated non-deficit Groves mechanism. mentioned earlier, matter domain set mechanisms consider, collectiveundominance always implies individual undominance. Section 4.2 show two examples single-item auction scenarios, collective undominance strictly strongerindividual undominance, non-deficit Groves mechanisms. is, exists individually undominated non-deficit Groves mechanism collectively dominated.Recall special Groves mechanism, called VCG Clarke (1971) mechanism,obtained using9Xvj (d, j ).hi (i ) := maxdDj6=icaseti () := maxdDXvj (d, j )Xvj (f (), j ),j6=ij6=ishows VCG mechanism pay-only.follows introduce slightly different notation describe Groves mechanisms,makes rest presentation convenient. First, denote paymentfunction ti VCG mechanism V CGi . Note Groves mechanism (f, t)defined terms VCG mechanism setting ti () := V CGi ()ri (i ),ri : R function . refer r := (r1 , . . ., rn ) redistributionfunction. Hence Groves mechanism identified redistribution function rviewed VCG mechanism combined redistribution. is,r agents first participate VCG mechanism. Then, top that, agent also9. below, whenever finite set, order ensure considered maximum exists,assume f continuous, vi i, also set compactsubsets Rk .136fiUndominated Groves Mechanismsreceives redistributionamountPequal ri (i ). definition, Groves mechanism rPnon-deficit iff ni=1 V CGi () ni=1 ri (i ) .3.2 Dominance RelationsUsing new notation above, individual collective dominance (among non-deficitGroves mechanisms) described follows:Definition 3.1 non-deficit Groves mechanism r individually dominates another nondeficit Groves mechanism r, ri (i ) ri (i ),, ri (i ) > ri (i ).Definition 3.2 non-deficit Groves mechanism r collectively dominates another nondeficit Groves mechanism rPP, ri (i ) ri (i ),PP, ri (i ) > ri (i ).consider mechanism results applying BCGC transformationVCG mechanism. refer Bailey-Cavallo mechanism simplyBC mechanism (Bailey, 1997; Cavallo, 2006). VCG mechanism characterizedconstant redistribution function rVCG = (0, 0, . . ., 0). BCGC transformation,every agent receives additional amount n1 times surplus guarantee SiBCGC (i ),independent type. is, BC mechanism also Groves mechanism,redistribution function111rBC = ( S1BCGC , S2BCGC , . . ., SnBCGC ).nnnLet := (1 , . . ., i1 , , i+1 , . . ., n ). starting VCG mechanism,SiBCGC (i ) = infnXk=1maxdDXj6=kvj (d, j )Xj6=kvj (f ( ), j ) ,is,SiBCGC (i ) = infnXk=1maxdDXvj (d, j ) (n 1)j6=knXk=1vk (f ( ), k )(1)many settings, i, SiBCGC (i ) = 0, consequentlyVCG BC mechanisms coincide (e.g., see Proposition 6.1). Whenever not,Proposition 2.3(ii), BC individually collectively dominates VCG. casesingle-item auction, seen SiBCGC (i ) = [i ]2 , [i ]2second-highest bid among bids agent bid.137fiGuo, Markakis, Apt, & Conitzer3.3 Anonymous Groves Mechanismsproofs main results obtained arguing first special classGroves mechanisms, called anonymous Groves mechanisms. provide resultsclass utilize later sections. call function f : Bpermutation independent permutations {1, . . ., n}, f = f . FollowingMoulin (1986), call Groves mechanism r = (r1 , . . ., rn ) anonymous 10type sets equal,functions ri coincide permutation independent.Hence, anonymous Groves mechanism uniquely determined single function r :n1 R.general, VCG mechanism anonymous. anonymoustype sets equal initial utility functions vi coincide. casetwo domains consider later sections.permutation {1, . . ., n} define letting:= 1 (i) .Denote (k) set permutations set {1, . . ., k}. Given Groves mechanism r := (r1 , . . ., rn ) type set every agent (and equalset 0 ), construct function r : 0n1 R, following Moulin (1986),settingPnX(n1) rj (x )r (x) :=,n!j=1x.defined analogouslyNote r permutation independent, r anonymous Groves mechanism.following lemma, independent interest, shows propertiesr transfer r .Lemma 3.3 Consider Groves Pmechanism r corresponding anonymous Grovesnmechanism r . Let V CG() :=i=1 V CGi (), suppose V CG functionpermutation independent. Then:(i) r non-deficit, r .(ii) anonymous Groves mechanism r0 collectively dominated r, collectively dominated r .Proof.nXi=1ri (i )=Pn Pi=1(n1)Pnn!j=1 rj ((i ))=10. definition slightly different one introduced Moulin (1986) conditionsput utility functions permutation independence refers redistribution function.138fiUndominated Groves MechanismsPn Pi=1(n) ri (i )n!last equality holds since terms aggregate applications rifunctions permutations n 1 elements .Let payment functions mechanisms r r , respectively.nXti () = V CG()nXri (i )i=1i=1(n)nXti ( ) = V CG( )i=1nXri (i).i=1Hence assumption V CG() followsPPnnX(n)i=1 ti ( )ti () =n!(2)i=1(i) immediate consequence (2).prove (ii) let t0 payment function r0 . r collectively dominates r0 ,(n)nnXXt0i ( )ti ( )i=1i=1least one inequality strict. HencePPnPPn 0(n)i=1 ti ( )(n)i=1 ti ( )n!n!least one inequality strict.fact r0 anonymous assumption V CG() implypermutations {1, . . ., n}nXnXt0i ( ) =t0i (),i=1i=1(2) inequality,nXi=1ti ()nXt0i (),i=1least one inequality strict.2assumption Lemma 3.3 permutation independence V CG() satisfieddomains consider Sections 5 6. item (ii) states Grovesmechanism considered sequel collectively undominated, collectivelydominated anonymous Groves mechanism.prove large class Groves mechanisms includes onesstudy sequel introduced relations dominance differ.139fiGuo, Markakis, Apt, & ConitzerTheorem 3.4 Suppose n 3. Assume sets types equal set 0contains least n1 elements. two non-deficit anonymous Groves mechanismsr r exist r collectively dominates r r individually dominate r .Proof. Fix non-deficit anonymous Groves mechanism determined permutationindependent function r : 0n1 R.Let a1 , . . ., an1 arbitrary different elements 0 . Define permutation independentfunction q : 0n1 R putting1 x permutation (a1 , . . ., an1 )q(x) :=2 otherwisen0 two subsequencesmay form permutationPn(a1 , . . ., an1 ). n 3, ,q()0. impliesi=1anonymous Groves mechanism determined function r := r q non-deficit.Trivially, sum payments r less equal sum paymentsr , since r redistributesPn money r . Moreover , instance= (a1 , . . ., a1 ), i=1 q(i ) > 0. Finally, definition, q(a1 , . . ., an1 ) = 1.imply r collectively dominated r individually dominatedr.24. Individually Undominated Mechanisms: CharacterizationAlgorithmic Techniques General Domainssection, focus individually undominated non-deficit Groves mechanisms.4.1 Non-deficit Groves Mechanismsstart characterization non-deficit Groves mechanisms.Recall firstPtype profile , denote V CG() total VCG payment, ni=1 V CGi ().Proposition 4.1 Groves mechanism r non-deficit ,Xrj (j)}(3)ri (i ) inf {V CG(i , )j6=ireported types agents j replaced .Here, jProof. first provedirection. , Equation 3 impliesP ri (i )Prj (j ) . let = , obtainV CG(i , )j rj (j )j6=iPV CG(i , ) = V CGi (). Thus, non-deficit property holds.prove direction. ToPensure non-deficit property, i,) V CG( , ), equivalentlyrj (j, , must ri (i ) +j6=P). Since arbitrary, Equation 3 follows.rj (j2ri (i ) V CG(i , )j6=ireplacing Equation 3 =, get characterization individuallyundominated non-deficit Groves mechanisms.140fiUndominated Groves MechanismsTheorem 4.2 Groves mechanism r non-deficit individually undominated,Xri (i ) = inf {V CG(i , )rj (j)}(4)j6=iHere, jreported types agents j replaced .Proof. prove direction first. Groves mechanism r satisfies Equation 4non-deficit Proposition 4.1. suppose r individually dominated, is,exists another non-deficit Groves mechanism r ,ri (i ) ri (i ), , ri (i ) > ri (i ).make inequality strict,X)}rj (jri (i ) > ri (i ) = inf {V CG(i , )j6=iinf {V CG(i , )Xrj (j)},j6=icontradicts fact r must satisfy Equation 3. follows r individually undominated.prove direction. Suppose Equation 4 Psatisfied. Then,)}. Letrj (jexists ri (i ) < inf {V CG(i , )j6=P)} r ( ) (so > 0), let r r,rj (j= inf {V CG(i , )j6=iexcept aforementioned , ri (i ) = ri (i ) + a. showbreak non-deficit constraint, consider type vector (i , )(that is, type profile affected). Then,Xrj (j)}ri (i ) = + ri (i ) = inf {V CG(i , )= inf {V CG(i , )j6=iXrj (j)}.j6=irThus, Proposition 4.1, non-deficit. contradicts r individually undominated. Hence, Equation 4 must hold.2give example individually undominated mechanism.Example 4.3 Consider single-item auction n 3 agents. Agent bids [0, ).Let []j jth highest type type profile . Let us consider anonymousGroves mechanism characterized r(i ) = n1 [i ]2 . is, mechanism,besides paying VCG payment, every agent receives n1 times second highestbid. fact, mechanism BC mechanism single-item auctions. showr individually undominated, suffices show Equation 4 satisfied. first observeevery agent, second highest bid second highest bid,141fiGuo, Markakis, Apt, & Conitzerequals total VCG payment. is, r non-deficit. Hence, Equation 3 holdsagents type profiles. Moreover, every type profile , setting = [i ]2 ,verify Equation 4 holds. follows BC mechanism individuallyundominated single-item auctions.follows, first show two examples single-item auction scenarios, collective undominance strictly stronger individual undominance non-deficit Grovesmechanisms. propose two techniques generating individually undominatedmechanisms starting known individually dominated mechanisms (if initial mechanism already individually undominated, techniques return mechanism). One technique immediately produces individually undominated mechanism.However, preserve anonymity. second technique preserves anonymity,repeated applications result converges individually undominated mechanism.emphasize start non-deficit Groves mechanism, including BCmechanism, Worst-Case Optimal mechanism (Guo & Conitzer, 2009), Optimal-inExpectation Linear mechanisms (Guo & Conitzer, 2010), VCG mechanism.4.2 Collective Undominance Strictly Stronger Individual Undominanceuse two examples show collective undominance is, general, strictly strongerindividual undominance.Example 4.4 Consider single-item auction 4 agents. assumeagent, set allowed types same, namely, integers 0 3. Here, VCGmechanism second-price auction.Let us consider following two anonymous non-deficit Groves mechanisms,computer-generated differentiating collective undominance individual undominance.Mechanism 1: r(i ) = r([i ]1 , [i ]2 , [i ]3 ), function r given Table 1.([i ]j jth highest type among types type.)Mechanism 2: r (i ) = r ([i ]1 , [i ]2 , [i ]3 ), function r given Table 1.characterization, mechanism2 collectively dominatesP mechPanism 1: example, type profile (3, 2, 2, 2), r(i ) = 1/2 < 1 = r (i ).hand, mechanism 2 individually dominate mechanism 1: example, r(3, 3, 2) = 1 > 5/6 = r (3, 3, 2). fact, based characterization individuallyundominated non-deficit Groves mechanisms (Theorem 4.2), able show mechanism 1 individually undominated.Example 4.5 Consider single-item auction 5 agents. assumeagent, set allowed types [0, ). Here, VCG mechanism second-priceauction.Let us consider following two anonymous non-deficit Groves mechanisms:Mechanism 1:142fiUndominated Groves Mechanismsr(0, 0, 0)r(1, 0, 0)r(1, 1, 0)r(1, 1, 1)r(2, 0, 0)r(2, 1, 0)r(2, 1, 1)r(2, 2, 0)r(2, 2, 1)r(2, 2, 2)001/41/401/1201/201/2r (0, 0, 0)r (1, 0, 0)r (1, 1, 0)r (1, 1, 1)r (2, 0, 0)r (2, 1, 0)r (2, 1, 1)r (2, 2, 0)r (2, 2, 1)r (2, 2, 2)001/41/407/241/61/21/41/2r(3, 0, 0)r(3, 1, 0)r(3, 1, 1)r(3, 2, 0)r(3, 2, 1)r(3, 2, 2)r(3, 3, 0)r(3, 3, 1)r(3, 3, 2)r(3, 3, 3)01/402/3102/3010r (3, 0, 0)r (3, 1, 0)r (3, 1, 1)r (3, 2, 0)r (3, 2, 1)r (3, 2, 2)r (3, 3, 0)r (3, 3, 1)r (3, 3, 2)r (3, 3, 3)01/41/42/319/241/65/67/125/61/2Table 1: Computer-generated example mechanisms differentiating collective undominance individual undominance.r(i ) = 0 four types identical.r(i ) = [i ]1 /4 highest three types identical, strictlyhigher lowest type .r(i ) = [i ]1 /6 highest two types identical, strictlyhigher third highest type .r(i ) = 3[i ]2 /16 highest type strictly higher second highesttype , second highest type identical third highest type .r(i ) = [i ]2 /5 highest three types different.Mechanism 2 (BC):r (i ) = [i ]2 /5.characterization, mechanismP 2 collectively dominatesmechanism 1: example,typeprofile(3,2,2,2,2),r(i ) = 4r(3, 2, 2, 2) +Pr(2, 2, 2, 2) = 3/2 + 0 = 3/2 < r (i ) = 4r (3, 2, 2, 2) + r (2, 2, 2, 2) = 8/5 + 2/5 = 2.hand, mechanism 2 individually dominate mechanism 1: example,r(4, 4, 4, 1) = 1 > 4/5 = r (4, 4, 4, 1). fact, based characterization individually undominated non-deficit Groves mechanisms (Theorem 4.2), able showmechanism 1 individually undominated.4.3 Priority-Based TechniqueGiven non-deficit Groves mechanism r priority order agents , improver individually undominated mechanism follows:1) Let : {1, . . . , n} {1, . . . , n} permutation representing priority order.is, (i) agent priority value (the lower value, higher priority). 1 (k)agent kth highest priority. high-level idea priority-basedtechnique go agents one one. first agent (the agenthighest priority), maximize redistribution function subject constraint143fiGuo, Markakis, Apt, & ConitzerProposition 4.1. later agents, same, take consideration earlieragents redistribution functions updated. priority order arbitrary.Generally, agents high priorities benefit technique, since earlieragents, room improvement.2) Let = 1 (1), update riXri (i ) = inf {V CG(i , )rj (j)}.(j)>1is, update ensures point r satisfies Equation 4 = 1 (1).noted update, payment agent = 1 (1)changed, changedXrj (j)} ri (i )ri (i ) ri (i ) = inf {V CG(i , )= inf {V CG(i , )(j)>1Xrj (j)} = SiBCGC (i ).jis, essentially, update amounts applying BCGC(i) transformationr, = 1 (1).3) consider remaining agents turn, according order .kth step, update ri (i = 1 (k))Xri (i ) = inf {V CG(i , ))rj (jX)}.rj (j(j)<k(j)>kis, update ensures point r satisfies Equation 4 = 1 (k).avoid breaking non-deficit property, make update, take previousk 1 updates account. update, essentially applyingBCGC( 1 (k)) transformation resulting mechanism previous update.Overall, every agent i,ri (i ) = inf {V CG(i , )X)rj (j(j)>(i)Xrj (j)}.(j)<(i)new mechanism r satisfies following properties:Proposition 4.6 , ri (i ) ri (i ).Proof. First consider = 1 (1),highest priority. ,Pthe agentrj (j )}. Since r non-deficit, Equation 3,ri (i ) = inf {V CG(i , )j6=iP)}. Hence r ( ) r ( ).rj (jri (i ) inf {V CG(i , )j6=i144fiUndominated Groves Mechanisms6= 1 (1), ri (i ) equalsXri (i ) + inf {V CG(i , ) ri (i )Xrj (j)(j)>(i)rj (j)}.(j)<(i)must showinf {V CG(i , ) ri (i )XXrj (j)(j)>(i)rj (j)} 0.(5)(j)<(i)Consider p = 1 ((i) 1) (the agent immediately terms priority)., ,XXV CG(i , ) ri (i )rj (j )rj (j )(j)>(i)= V CG(i , )Xrj (j )Xrj (j)(j)>(p)inf {V CG(p , p )p p(j)<(i)Xrj (j ) rp (p )Xrj (j)} rp (p ) = 0.(j)<(p)(j)>(p)(j)<(p)set types reported agents j,inequality, jpreplaced p . arbitrary, Equation 5 follows. Therefore, ri (i ) ri (i ).2Proposition 4.7 r individually undominated.Proof. Let = 1 (n). ,XXrj (j)} ri (i ) = 0.V CG()rj (j ) inf {V CG(i , )j=1,...,nj6=iHence r never incurs deficit. So, r non-deficit.Using Proposition 4.6, ,Xrj (j)ri (i ) = inf {V CG(i , )(j)>(i)inf {V CG(i , )XXrj (j)}(j)<(i)rj (j)}.j6=ir non-deficit, opposite inequality must also satisfied (Equation 3)hencemust equality, is, Equation 4 must hold. follows r individuallyundominated.2noted technique, updates, need keeptrack value ri (i ) . is, due space complexity,technique suitable cases agents possible types. reduce145fiGuo, Markakis, Apt, & Conitzerspace complexity, update, could also recompute earlier updates recursivefashion. so, later updates much difficult compute comparedearlier updates. Fortunately, earlier updates tend important,generally room improvement earlier updates. Therefore,reasonable approximation would update high-priority agents ignoreremaining agents low priorities.4.4 Iterative Technique Preserves Anonymityprevious technique will, general, produce anonymous mechanism, eveninput mechanism anonymous. agents higher priority order tendbenefit technique. Here, introduce another technique preservesanonymity.Given anonymous mechanism r, let r0 = r. , letrk+1 (i ) =Xn1 k1rk (j)}.r (i ) +inf{VCG(,)nnj6=ieasily seen induction rk mechanisms anonymous. rk anonymous, (n 1), rk ((i ) ) = rk (i ) i. alsoV CG(i , (i ) ) = V CG(i , ) , , i. Finally, let ((i ) , )type profilepermuted according , replacedP types P) , i, .. j6=i rk (((i ) , )j ) = j6=i rk (jimplies rk+1 also permutation independent, thus anonymous.noted rk rk+1 , agent payment changed===n1 kn r (i )+1n1nrk+1 (i ) rk (i )P k, )r (j )} rk (i )inf{VCG(inf {Vj6=iCG(i , )Pj1 BCGC(i ).n Si)}rk (jis, essentially, rk+1 resulting mechanism applying BCGC transformrk .next propositions immediately follow Proposition 2.3:Proposition 4.8 r0 non-deficit, rk non-deficit k.Proposition 4.9 , rk (i ) nondecreasing k.Proposition 4.10 rk+1 = rk , rk individually undominated.Proposition 4.11 rk individually undominated, rk+1 individually dominatesrk .Finally, following proposition establishes convergence.146fiUndominated Groves MechanismsProposition 4.12 k , rk converges (pointwise) individually undominatedmechanism.Proof. Proposition 4.9, rk (i ) nondecreasing k, since every rk nondeficit Proposition 4.8, must bounded; hence must converge (pointwise)., letXrk (j)} rk (i ).dk = inf {V CG(i , )j6=iUsing Proposition 4.9, derive following inequality:Xrk+1 (j)} rk+1 (i )dk+1 = inf {V CG(i , )j6=iinf {V CG(i , )Xrk (j)} rk+1 (i )j6=i= inf {V CG(i , )=Xrk (j)}j6=iXn1 k1r (i )inf{VCG(,)rk (j)}nnj6=i=Xn1 kn1n1{VCG(,)rk (j)}infr (i ) =dk .nnnj6=ik , dk = inf {V CG(i , )Pj6=i)} r k ( ) 0. limit, Equark (jtion 4 satisfied. Thus, rk converges (pointwise, linearly) individually undominatedmechanism.2Similar priority-based technique, iterative process, computingrk , need value rk1 (i ), . is, due space complexity,technique suitable cases agents possible types.reduce space complexity could also recompute rk1 recursive fashion.so, rk becomes much difficult compute large values k. Fortunately, earlieriterative steps crucial, generally room improvementearlier steps. Therefore reasonable approximation would computeiterative steps.5. Multi-Unit Auctions Unit Demandsection, consider auctions multiple identical units singlegood agents unit demand, i.e., agent wants one unit (ifsingle unit good, simply standard single-item auction). focusnotion collectively undominated mechanisms relates individuallyundominated mechanisms. particular, first obtain analytical characterization147fiGuo, Markakis, Apt, & Conitzercollectively undominated Groves mechanisms non-deficit, anonymous,linear payment functions, proving Optimal-in-Expectation Linear redistributionmechanisms (OEL) (Guo & Conitzer, 2010), include BC mechanism,collectively undominated Groves mechanisms anonymous linear. showindividual undominance collective undominance equivalent restrictconsideration Groves mechanisms anonymous linear setting multiunit auctions unit demand. Note even single-item auctions, examplesgiven Section 4.2 show equivalence hold restrictlinear anonymous mechanisms.one mechanism collectively dominates another mechanism, first mechanism, agents expected total utility, prior distribution agentsvaluations, must less second mechanism, strictly higher minimal conditions prior distribution. Therefore, good directionlook collectively undominated mechanisms start mechanismsoptimal-in-expectation.Optimal-in-Expectation Linear (OEL) redistribution mechanisms (Guo & Conitzer,2010), described below, special cases non-deficit Groves mechanisms anonymous linear. OEL mechanisms defined multi-unit auctions unitdemand. unit demand multi-unit auction, indistinguishable units sale,agent interested one unit. agent i, type valuationwinning one unit. assume bids (announced types) bounded LU , i.e., = [L, U ] (note L 0).linear anonymous Groves mechanism characterized function r foln1Pcj [i ]j (where [i ]j jth highest bid among ).lowing form: r(i ) = c0 +j=1OEL mechanisms, cj chosen according one following options (indexedinteger parameter k, k ranges 0 n, k odd):k = 0:ni1m1ci = (1)/= 1, . . . , m,nm1i1Xm1mi n 1(1)c0 = U m/n U/,nm1i1mii=1ci = 0 values i.k = 1, 2, . . . , m:ni1m1ci = (1)/= k + 1, . . . , m,nm1i1Xm1mi n 1(1)ck = m/n/,nm1i1mii=k+1ci = 0 values i.148fiUndominated Groves Mechanismsk = + 1, + 2, . . . , n 1:ci = (1)mi1nm1i1= + 1, . . . , k 1,/ni1m1ck = m/nk1X(1)mi1i=m+1i1nm1/,m1ni1ci = 0 values i.k = n:ci = (1)mi1i1nm1/= + 1, . . . , n 1,m1ni1c0 = Lm/n Ln1X(1)mi1i=m+1nm1i1,/ni1m1ci = 0 values i.example, k = + 1, cm+1 = m/n ci = 0 i.specific OEL mechanism, r(i ) =n [i ]m+1 . is, besides participating VCGmechanism, every agent also receives amount equal m/n times (m + 1)thhighest bid agents. Actually, exactly BC mechanism multi-unitauctions unit demand.Besides non-deficit, one property OEL mechanisms alwaysbudget balanced following scenarios.[]1 = U k = 0[]k+1 = []k k {1, . . ., n 1}[]n = L k = nUsing property, prove OEL mechanisms collectivelyundominated non-deficit Groves mechanisms anonymous linear.first show OEL mechanisms collectively undominated.Theorem 5.1 multi-unit auctions unit demand, non-deficit Grovesmechanism collectively dominates OEL mechanism.using Lemma 3.3, need prove case anonymous Grovesmechanisms.Lemma 5.2 multi-unit auctions unit demand, non-deficit anonymousGroves mechanism collectively dominates OEL mechanism.149fiGuo, Markakis, Apt, & ConitzerProof. first prove: OEL mechanism index k {1, . . ., n 1} collectivelydominated non-deficit anonymous Groves mechanism.Suppose non-deficit anonymous Groves mechanism r collectively dominates OELmechanism index k {1, . . ., n 1}. use rOEL denote OEL mechanism., define following function:(i ) = r(i ) rOEL (i ).PSince r collectively dominates rOEL , , ni=1 (i ) 0.also that, whenever []k+1 = []k , OEL mechanism budget balanced.OEL , agents total payment 0; case, since r non-deficit,is,Pn rmust i=1 (i ) = 0.claim (i ) = 0 . Let C(i ) number bids amongequal [i ]k . Hence, must show C(i ) 1,(i ) = 0.prove induction value C(i ) (backwards, n 1 1).Base case: C(i ) = n 1.Suppose C(i ) = n 1. is, bids identical.also equal bids , allPbids []k+1 = []k .Hence, earlier observation, nj=1 (j ) = 0. know j,j set bids. Hence (i ) = 0 C(i ) = n 1.Induction step.Let us assume , C(i ) p (where p {2, . . ., n 1}), (i ) = 0.considerP C(i ) = p 1. equal [i ]k , []k =[]k+1 , implies nj=1 (j ) = 0. j j = [i ]k , (Pn j ) = (i ),j, C(j ) = p. Therefore, induction assumption, j=1 (j )positive multiple (i ), implies (i ) = 0.induction, shown (i ) = 0 . implies rrOEL identical. Hence, non-deficit anonymous Groves mechanism collectivelydominates OEL mechanism index k {1, . . ., n 1}.prove: OEL mechanism index k = 0 collectively dominateddifferent non-deficit anonymous Groves mechanism.Suppose non-deficit anonymous Groves mechanism r collectively dominates OELmechanism index k = 0. use rOEL denote OEL mechanism., define following function:(i ) = r(i ) rOEL (i ).PSince r collectively dominates rOEL , , ni=1 (i ) 0. alsoOEL , agents total payment 0; case,that, whenever []1 = U , rPr non-deficit, must ni=1 (i ) = 0.claim (i ) = 0 . Let C(i ) number bids amongequal U . Hence, must show C(i ) 0, (i ) = 0.prove induction value C(i ) (backwards, n 1 0).150fiUndominated Groves MechanismsBase case: C(i = n 1.Suppose C(i ) = n 1. is, bidsPnare equal U .also equal bids U , earlier observation, j=1 (j ) = 0.know j, (j ) value. Hence (i ) = 0C(i ) = n 1.Induction step.Let us assume , C(i ) p (where p {2, . . ., n 1}), (i ) = 0.considerPi C(i ) = p 1. equal U , []1 = U ,implies nj=1 (j ) = 0. j j = U , (Pnj ) = (i ),j, C(j ) = p. Therefore, induction assumption, j=1 (j ) positivemultiple (i ), implies (i ) = 0.induction, shown (i ) = 0 . implies rrOEL identical. Hence, non-deficit anonymous Groves mechanism collectivelydominates OEL mechanism index k = 0.remains prove: OEL mechanism index k = n collectively dominated different non-deficit anonymous Groves mechanism.case similar case k = 0 omit here.2proceed show within family anonymous linear non-deficitGroves mechanisms, OEL mechanisms ones collectively undominated. Actually, also ones individually undominated,stronger claim since individually undominated weaker property.Theorem 5.3 multi-unit auctions unit demand, anonymous linear nondeficit Groves mechanism individually undominated, must OEL mechanism.proving theorem, let us introduce following lemma.Lemma 5.4 Let set points (s1 , s2 , . . . , sk ) (U s1 s2 . . . sk L)satisfy Q0 + Q1 s1 + Q2 s2 + . . . + Qk sk = 0 (the Qi constants). measurepositive (Lebesgue measure Rk ), Qi = 0 i.Proof. Qi 6= 0 i, U s1 s2 . . . si1 si+1 . . . sk L,make Q0 + Q1 s1 + Q2 s2 + . . . + Qk sk = 0, si take one value. resultmeasure must 0.2ready prove Theorem 5.3.Proof. Let r non-deficit anonymous linear Groves mechanism. recallGroves mechanism anonymous linear r linear function defined r(i ) =n1Paj [i ]j (where [i ]j jth highest type among , aj constants).a0 +j=1multi-unit auctions unit demand, total VCG payment equals m[]m+1(m times (m + 1)th bid). r, agents total payment equalsm[]m+1nXr(i ) = m[]m+1 na0n n1XXi=1 j=1i=1151aj [i ]j .fiGuo, Markakis, Apt, & Conitzertotal payment linear function terms types among . simplicity,rewrite total payment C0 + C1 []1 + C2 []2 + . . . + Cn []n . Ci constantsdetermined ai .C0 = na0C1 = (n 1)a1C2 = a1 (n 2)a2C3 = 2a2 (n 3)a3...Cm = (m 1)am1 (n m)amCm+1 = mam (n 1)am+1 +Cm+2 = (m + 1)am+1 (n 2)am+2...Cn1 = (n 2)an2 an1Cn = (n 1)an1Given , possible value , mustis, , infnPi=1nPti () 0 (non-deficit).i=1ti () 0. , infnPi=1ti () >( > 0), reduce payment agent without violating non-deficitconstraint, agents types . Therefore, mechanism individuallynPti () = 0.undominated, , infi=1denote [i ]j sj (j = 1, . . . , n 1). is, s1 s2 . . . sn1 .nPexpression infti () equals minimum following expressions:i=1infLi sn1nXti ()i=1infsn1 sn2nXti ()i=1...infs2 s1infs1 UnXti ()nXti ()i=1i=1152fiUndominated Groves Mechanismstake closer lookinfnPLi sn1 i=1ti (). L sn1 , jth highest type[]j = sj j = 1, . . . , n 1, nth highest type []n = (this case correspondsagent agent lowest type).infLi sn1nXti () =i=1infLi sn1(C0 + C1 s1 + C2 s2 + . . . + Cn1 sn1 + Cn )= min{C0 + C1 s1 + . . . + Cn1 sn1 + Cn L, C0 + C1 s1 + . . . + Cn1 sn1 + Cn sn1 }.is, expression linear, minimum reached set eitherlower bound L upper bound sn1 .Similarly,infsn1 sn2nXti () = min{C0 + C1 s1 + . . . + Cn2 sn2 + Cn1 sn1 + Cn sn1 ,i=1C0 + C1 s1 + C2 s2 + . . . + Cn2 sn2 + Cn1 sn2 + Cn sn1 },...infs2 s1nXti () = min{C0 + C1 s1 + C2 s1 + C3 s2 + . . . + Cn sn1 ,i=1C0 + C1 s1 + C2 s2 + C3 s2 + . . . + Cn sn1 },infs1 UnXti () = min{C0 + C1 U + C2 s1 + . . . + Cn sn1 ,i=1C0 + C1 s1 + C2 s1 + . . . + Cn sn1 }.Putting together, U s1 s2 . . . sn1 L,minimum following expressions 0.(n): C0 + C1 s1 + C2 s2 + . . . + Cn1 sn1 + Cn L(n 1): C0 + C1 s1 + C2 s2 + . . . + Cn1 sn1 + Cn sn1(n 2): C0 + C1 s1 + C2 s2 + . . . + Cn2 sn2 + Cn1 sn2 + Cn sn1...(2): C0 + C1 s1 + C2 s2 + C3 s2 + . . . + Cn sn1(1): C0 + C1 s1 + C2 s1 + C3 s2 + . . . + Cn sn1(0): C0 + C1 U + C2 s1 + C3 s2 + . . . + Cn sn1153fiGuo, Markakis, Apt, & Conitzerexpressions numbered 0 n. Let I(i) set points (s1 , . . . , sn1 )(U s1 s2 . . . sn1 L) make expression (i) equal 0. must existleast one measure I(i) positive. According Lemma 5.4, expression(i) must constant 0.expression (0) constant 0, total payment r 0 whenever highesttype equal upper bound U . is, , total payment C0 + C1 []1 +C2 []2 + . . . + Cn []n must constant multiple U []1 (the total payment linearfunction). C0 = U C1 Cj = 0 j 2. turns equalitiesCj completely determine values aj (the values aj solvedbased Cj pure algebraic manipulations), corresponding mechanismOEL mechanism index k = 0. expression (i) constant values i,corresponding mechanism OEL mechanism another index.2Hence, following complete characterization context.Corollary 5.5 multi-unit auctions unit demand, non-deficit anonymous linearGroves mechanism individually / collectively undominated OELmechanism.Proof. corollary proved combining Theorem 5.1 Theorem 5.3, wellfact collectively undominated mechanism also individually undominated. 2corollary also shows consider Groves mechanismsnon-deficit, anonymous, linear setting multi-unit auctions unit demand,individual undominance collective undominance equivalent. Thus,characterized individually/collectively undominated Groves mechanisms nondeficit, anonymous, linear multi-unit auctions unit demand.6. Public Project Problemstudy well known class decision problems, namely public project problems (see,e.g., Mas-Colell et al., 1995; Moulin, 1988; Moore, 2006). setting set n agentsneeds decide financing project cost c. agents type private valuationproject takes place. consider two versions problem.6.1 Equal Participation Costscase project takes place, agent contributes share, c/n,cover total cost. Hence participation costs agents same.problem defined follows.Public project problemConsider (D, 1 , . . ., n , v1 , . . ., vn ),= {0, 1} (reflecting whether project canceled takes place),{1, . . ., n}, = [0, c], c > 0,154fiUndominated Groves Mechanisms{1, . . ., n}, vi (d, ) := d(i nc ),agents employ payment-based mechanism decide project,addition c/n, agent also pay receive payment, ti (), imposedmechanism. result Holmstrom (1979), efficient strategyproof payment-based mechanisms domain PGroves mechanisms.PnTo determineefficient outcome given type vector , note ni=1 vi (d, P) = d( i=1 c). Henceefficiency mechanism (f, t) means f () = 1 ni=1 c f () = 0otherwise, i.e., project takes place declared total value agentsproject exceeds cost.first observe following result.Proposition 6.1 public project problem equal participation costs, BC mechanism coincides VCG.Proof. suffices check equation (1) holds SiBCGC (i ) = 0. Since VCG non-deficit mechanism, SiBCGC (i ) 0, termSiBCGC (i ) sum payments type vector. Hence need showvaluemakes expression (1) equal 0. Checking quitePsimple. j6=i j < n1n c, take := 0 otherwise := c. former caseefficient outcome implement project whereas latter case, oppositeoccurs. easy check cases SiBCGC (i ) = 0.2show fact VCG cannot improved upon. stating result,wouldPlike note one ideally would like mechanism budget-balanced,i.e., ti () = 0 , total agents pay cost projectmore. However possible, since public project problem, mechanismexists efficient, strategy-proof, budget balanced (Mas-Colell et al., 1995).theorem considerably strengthens result, showing VCG optimalrespect minimizing total payment agents.Theorem 6.2 public project problem exists non-deficit Groves mechanismcollectively dominates VCG mechanism.case unit-demand auctions, first establish desired conclusionanonymous Groves mechanisms extend arbitrary ones Lemma 3.3. NoticeVCG anonymous setting hence apply Lemma 3.3(ii).Lemma 6.3 public project problem exists anonymous non-deficit Grovesmechanism collectively dominates VCG mechanism.Proof. Suppose anonymous non-deficit Groves mechanism (r1 , ..., rn ) existscollectively dominates VCG. anonymity, {1, . . ., n} ri = r, functionr : [0, c]n1 R. HencenX[0, c]nr(i ) 0(6)i=1155fiGuo, Markakis, Apt, & Conitzershow x [0, c]n1 , r(x) = 0 thus r coincides VCG.divide proof two cases.n1Xn1xiCase 1: vector x satisfiesc.ni=1Given x, define C(x) = |{i : xi = c}|, i.e., given vector x n 1 types, C(x)number agents submitted c. Define following predicate:P (k) : x [0, c]n1 ((C(x) = kn1Xi=1xin1c) r(x) = 0)nprove P (k) holds k {0, . . ., n 1}, using induction (going backwards n 1). Let ti () = V CGi () r(i ) payment function agentmechanism r.Base case.Let x C(x) = n 1. Consider := (c, . . ., c) [0, c]n .{1, . . ., n}, = x. Clearly f () = 1 agent paying anything VCGmechanism instance, i.e., V CGi () = 0.Since r non-deficit mechanismnnnnXXXXr(i ) = nr(x),r(i ) =V CGi ()ti () =0i=1i=1i=1i=1(6) r(x) = 0.Induction step.Assume P (k) holds k 1. prove P (k 1). Let xC(x) = k 1 (note x may zero cs). Since r permutation independent,assume without loss generality elements x sorted descending order (i.e.,r(x) change reordering). Consider type vector = (c, x),concatenation (c) x. Hence starts k cs rest like rest x. Note{1, . . ., k}, = x C(i ) = k 1. {k1, . . ., n}, C(i ) = k,P+ntherefore induction hypothesis, r(i ) = 0. means i=1 r(i ) = kr(x).Furthermore, f () = 1 since least one c, agent paying paymentVCG mechanism. see this, k 2, every agent , anotheragent submitted c hence agentpivotal. k = 1, agent alterPdecision outcome factxi n1n c, hence agent pivotal casewell. Thus, {1, . . ., n}, V CGi () = 0, r non-deficit0nXti () =nXr(i ) = kr(x)i=1i=1(6) r(x) = 0. concludes induction step consequentlyr(x) = 0 vectors x belong Case 1.n1Xn1c. proof case uses completelyni=0symmetric argument Case 1. include sake completeness.Case 2: vector x satisfiesxi <156fiUndominated Groves MechanismsDefine C (x) = |{i : xi = 0}|. analogy predicate P (k) Case 1, definefollowing predicate:P (k) : x [0, c]n1 ((C (x) = kn1Xxi <i=0provewards n 1).P (k)n1c) r(x) = 0)nholds k {0, ..., n 1}, using induction (going back-Base case.Let x C (x) = n 1, i.e., zero vector. Consider := (0, ..., 0) [0, c]n .{1, . . ., n}, = x. Clearly f () = 0 agent payingPanythingti () =P VCG mechanism. Hence ti () payment paid agent i,r(i ) = nr(x).Since r non-deficit mechanism,0nXti () = nr(x)i=1(6) implies r(x) = 0.Induction step.Suppose P (k) holds k 1. prove P (k 1). Let xC (x) = k 1. Since r permutation independent, assume without loss generalityelements x sorted increasing order 0s left sidex (note may also x 0s, since k 1 maybe equal 0).Consider type vector = (0, x). starts k 0s rest like rest x.Note {1, . . ., k}, = x C (i ) = kP1. {k + 1, . . ., n}, C (i ) = kinduction hypothesis, r(i ) = 0 hencer(i ) = kr(x).note f () = 0 also agent paying payment VCGmechanism. see this, enough Pverify agent pivotal, followsn1fact case i=0xi < n1n c. Since = (0, x), agentpivotal.P Therefore V CGi () = 0 every {1, . . ., n}. Since r non-deficit0 r(i ) = kr(x). (6) r(x) = 0.completes proof induction step hence Case 2. Since Cases 1 2cover vectors x [0, c]n1 , proof Lemma complete.2using Lemma 6.3 Lemma 3.3(ii), proof Theorem 6.2 complete.interesting open question whether mechanisms share properties VCG mechanism also collectively undominated. particular,exhibited VCG pay-only anonymous mechanism. anonymouspay-only mechanisms collectively undominated public project problemequal participation costs?start pay-only mechanisms. provide general observation holdsmany domains public project problems, showing VCG mechanism dominates pay-only mechanisms.157fiGuo, Markakis, Apt, & ConitzerLemma 6.4 Let r Groves mechanism. Suppose following condition{1, . . ., n}:11holdsbi V CGi (bi , ) ri (i ) = 0.r individually dominates pay-only Groves mechanisms.condition essentially says every agent always able make payment equal0 type vector agents.Proof. Suppose exists pay-only mechanism r = (r1 , . . ., rn ) differentr = (r1 , . . ., rn ) dominated r. Then, {1, . . .n}, ri (i ) >ri (i ). Let bi type agent satisfies condition theorem. Consider= (bi , ). V CGi ( ) = ri (i ).payment agent mechanism r profileti ( ) = V CGi ( ) ri (i ) < V CGi ( ) ri (i ) = 0,contradiction, r pay-only mechanism.2Theorem 6.5 Consider public project problem equal participation costs.pay-only Groves mechanism r, following equivalent:1. r individually undominated,2. r VCG mechanism,3. r collectively undominated.Proof. 1 2. Consider pay-only individually undominated Groves mechanism r.claim r VCG mechanism.considered domain every agent i, given , force VCG payment 0declaring bi = c/n. Indeed, would V CGi (c/n, ) = 0. Hence Lemma6.4 VCG mechanism individually dominates pay-only mechanisms. meansindividually undominated mechanism VCG.2 3 holds Theorem 6.2 3 1 holds definition.2theorem shows public project problem equal participationcosts, VCG pay-only Groves mechanism individually/collectively undominated. Appendix A, show similar result anonymous Groves mechanisms,case two agents. is, exactly two agents, VCGanonymous Groves mechanism individually/collectively undominated. Further,n 3, Herve Moulin (private communication) observed public project problems equal participation costs, VCG mechanism non-deficit Grovesmechanism collectively undominated.11. slight generalization Potential Universal Relevance Nullification (PURN) conditionintroduced Cavallo (2006). agent satisfies PURN make payment VCGmechanism equal 0 type vector agents. Here, differenceconsider Groves mechanisms instead VCG.158fiUndominated Groves Mechanisms6.2 General Caseassumption made far public project problem agentscost share may always realistic. Indeed, may argued richeragents (such larger enterprises) contribute more. matter modifyformulation problem appropriately? answer yes. First, let us formalizeversion problem. assume initial utility function formvi (d, ) := d(i ci ),P{1, . . ., n}, ci > 0 ni=1 ci = c.setting, ci share project cost financed agent i. callresulting problem general public project problem. taken Moore (2006).problem two results, concerning individual dominance relation.Theorem 6.6 general public project problem VCG mechanism individually dominates pay-only Groves mechanisms.Proof. Note , agent force VCG payment 0declaring ci , since ti (ci , ) = 0. Lemma 6.4 proof complete.2theorem cannot extended non-deficit Groves mechanisms, illustrated following theorem. theorem also showsindividually undominated mechanism setting, cannot pay-only mechanism.Theorem 6.7 n 3, instance general public project problem nagents exists BC mechanism individually dominates VCG mechanism.Proof. show n = 3. n > 3, fairly simple extend proof.omit details. VCG mechanism non-deficit, hence suffices showProposition 2.3(ii) VCG BC mechanisms coincide, choicec, c1 , c2 , c3 , c1 + c2 + c3 = c.end need find 2 3 S1BCGC (2 , 3 ) > 0.((R1 + R2 + R3 ) L),S1BCGC (2 , 3 ) := min1 1:= (1 , 2 , 3 )L := (n 1)nXvk (f ( ), k ),k=1R1 = maxXvj (d, j ) = max{0, 2 + 3 (c2 + c3 )},R2 = maxXvj (d, j ) = max{0, 1 + 3 (c1 + c3 )},R3 = maxXvj (d, j ) = max{0, 1 + 2 (c1 + c2 )}.dDdDdDj6=1j6=2j6=3159fiGuo, Markakis, Apt, & ConitzerNow, take c = 100, c1 = 10, c2 = 40, c3 = 50 2 := 10, 3 := 70. R1 +R2 +R3 =1 + 10 + max{0, 1 40}. Two cases arise.Case 1 f ( ) = 0.L = 0, (R1 + R2 + R3 ) L 10.Case 2 f ( ) = 1.L = 2(1 + 2 + 3 100) = 21 40,(R1 + R2 + R3 ) L = 50 1 + max{0, 1 40}(50 1 ) + (1 40) 10.proves S1BCGC (2 , 3 ) 10. taking 1 [40, 100] see fact= 10.2S1BCGC (2 , 3 )virtue Theorem 6.6 BC mechanism proof pay-only.7. Conclusions Future Workfamily Groves mechanisms, includes well-known VCG mechanism (alsoknown Clarke mechanism), family efficient strategy-proof mechanisms.Unfortunately, Groves mechanisms generally budget balanced. is,mechanisms, payments may flow system agents, resultingdeficits reduced utilities agents. identify non-deficit Groves mechanismsgive agents highest utilities, introduced two general measures comparingmechanisms prior-free settings. Specifically, say non-deficit Groves mechanismindividually dominates another non-deficit Groves mechanism every type profile, every agents utility less , holds strictinequality least one type profile one agent. say non-deficit Grovesmechanism collectively dominates another non-deficit Groves mechanism everytype profile, agents total utility (social welfare) less, holds strict inequality least one type profile. definitionsinduce two partial orders non-deficit Groves mechanisms. paper mainly focusedstudying maximal elements corresponding two partial orders.number interesting open problems remain. Specifically,provided Section 4.2 two examples showing collective undominancestrictly stronger individual undominance. One example involves discrete typespace, example involves discontinuous redistribution functions.remains seen whether two definitions undominance coincidetype space smoothly connected redistribution functions continuous.know Guo Conitzer (2010) OEL mechanismscollectively undominated mechanisms multi-unit auctions unit demand, exist prior distributions mechanisms achieve strictlyhigher expected social welfare. is, multi-unit auctions unit demand,exist unknown collectively undominated mechanisms (based nonlinearredistribution functions). However, remains seen whether also existcollectively undominated mechanisms (other VCG) public project problems.160fiUndominated Groves Mechanismsproposed two techniques generating individually undominated mechanisms.also derive techniques generating collectively undominated mechanisms?Acknowledgmentsauthors would like thank three reviewers useful comments. alsothank Herve Moulin valuable discussions. work supported projectDIACODEM Dutch organization scientific research (NWO), projectAGT research funding program THALIS (co-financed European Social FundESF Greek national funds). also thank National Science FoundationAlfred P. Sloan Foundation support Awards IIS-0812113, IIS-0953756, CCF1101659, Sloan Fellowship.Appendix A. Uniqueness VCG Case Two AgentsTheorem A.1 Consider public project problem equal participation costs.number agents n = 2, non-deficit, anonymous Groves mechanismr, following equivalent:1. r individually undominated,2. r VCG mechanism,3. r collectively undominated.Proof. proof Theorem 6.5 suffices show 1 2. take non-deficit,anonymous, individually undominated Groves mechanism, determined functionr.x [0, c], take := (x, x). x c/2, efficient outcome f () = 1agent pivotal, hence total VCG payment 0. x < c/2, projectbuilt agent pivotal. Hence cases VCG payment 0.payment function corresponding r, t1 () + t2 () = 2r(x). Sincer non-deficit, every x [0, c], r(x) 0. since r individuallyundominated, cannot case r(x) < 0 x, VCGmechanism would dominate r. Hence r coincides VCG mechanism.2ReferencesApt, K., Conitzer, V., Guo, M., & Markakis, E. (2008). Welfare undominated Groves mechanisms. Proceedings Fourth Workshop Internet Network Economics(WINE), pp. 426437, Shanghai, China.Bailey, M. J. (1997). demand revealing process: distribute surplus. Public Choice,91, 107126.Cavallo, R. (2006). Optimal decision-making minimal waste: Strategyproof redistribution VCG payments. Proceedings International Conference AutonomousAgents Multi-Agent Systems (AAMAS), pp. 882889, Hakodate, Japan.161fiGuo, Markakis, Apt, & ConitzerClarke, E. H. (1971). Multipart pricing public goods. Public Choice, 11, 1733.Cramton, P., Gibbons, R., & Klemperer, P. (1987). Dissolving partnership efficiently.Econometrica, 55 (3), 615632.de Clippel, G., Naroditskiy, V., & Greenwald, A. (2009). Destroy save. ProceedingsACM Conference Electronic Commerce (EC), pp. 207214, Stanford, CA,USA.Faltings, B. (2005). budget-balanced, incentive-compatible scheme social choice.Agent-Mediated Electronic Commerce (AMEC), LNAI, 3435, pp. 3043.Groves, T. (1973). Incentives teams. Econometrica, 41, 617631.Gujar, S., & Narahari, Y. (2011). Redistribution mechanisms assignment heterogeneous objects. J. Artif. Intell. Res. (JAIR), 41, 131154.Guo, M. (2011). VCG redistribution gross substitutes. Proceedings NationalConference Artificial Intelligence (AAAI), San Francisco, CA, USA.Guo, M. (2012). Worst-case optimal redistribution VCG payments heterogeneousitem auctions unit demand. Proceedings Eleventh International JointConference Autonomous Agents Multi-Agent Systems (AAMAS), Valencia,Spain.Guo, M., & Conitzer, V. (2008a). Better redistribution inefficient allocation multiunit auctions unit demand. Proceedings ACM Conference ElectronicCommerce (EC), pp. 210219, Chicago, IL, USA.Guo, M., & Conitzer, V. (2008b). Undominated VCG redistribution mechanisms. Proceedings Seventh International Joint Conference Autonomous AgentsMulti-Agent Systems (AAMAS), pp. 10391046, Estoril, Portugal.Guo, M., & Conitzer, V. (2009). Worst-case optimal redistribution VCG paymentsmulti-unit auctions. Games Economic Behavior, 67 (1), 6998.Guo, M., & Conitzer, V. (2010). Optimal-in-expectation redistribution mechanisms. Artificial Intelligence, 174 (5-6), 363381.Guo, M., Naroditskiy, V., Conitzer, V., Greenwald, A., & Jennings, N. R. (2011). Budgetbalanced nearly efficient randomized mechanisms: Public goods beyond.Proceedings Seventh Workshop Internet Network Economics (WINE),Singapore.Holmstrom, B. (1979). Groves scheme restricted domains. Econometrica, 47 (5), 11371144.Laffont, J., & Maskin, E. (1997). theory incentives: overview, in: W. Hildenbrand,ed., Advances economics theory, Econometric Society Monograph QuantitativeEconomics. Cambridge University Press.Mas-Colell, A., Whinston, M., & Green, J. R. (1995). Microeconomic Theory. OxfordUniversity Press.Moore, J. (2006). General Equilibrium Welfare Economics: Introduction. Springer.Moulin, H. (1988). Axioms Cooperative Decision Making. Cambridge University Press.162fiUndominated Groves MechanismsMoulin, H. (1986). Characterizations pivotal mechanism. Journal Public Economics, 31 (1), 5378.Moulin, H. (2009). Almost budget-balanced VCG mechanisms assign multiple objects.Journal Economic Theory, 144 (1), 96119.Myerson, R., & Satterthwaite, M. (1983). Efficient mechanisms bilateral trading. JournalEconomic Theory, 28, 265281.Porter, R., Shoham, Y., & Tennenholtz, M. (2004). Fair imposition. Journal EconomicTheory, 118, 209228.163fiJournal Artificial Intelligence Research 46 (2012) 47-87Submitted 07/12; published 01/13Optimal Rectangle Packing:Absolute Placement ApproachEric Huangehuang@parc.comPalo Alto Research Center3333 Coyote Hill RoadPalo Alto, CA 94304 USARichard E. Korfkorf@cs.ucla.eduUCLA Computer Science Department4532E Boelter HallUniversity California, Los AngelesLos Angeles, CA 90095-1596 USAAbstractconsider problem finding enclosing rectangles minimum areacontain given set rectangles without overlap. rectangle packer chooses xcoordinates rectangles y-coordinates. transformproblem perfect-packing problem empty space adding additional rectangles. determine y-coordinates, branch different rectanglesplaced empty position. packer allows us extend known solutionsconsecutive-square benchmark 27 32 squares. also introduce three new benchmarks, avoiding properties make benchmark easy, rectangles shareddimensions. third benchmark consists rectangles increasingly high precision.pack efficiently, limit rectangles coordinates bounding box dimensionsset subset sums rectangles dimensions. Overall, algorithms representcurrent state-of-the-art problem, outperforming algorithms ordersmagnitude, depending benchmark.1. IntroductionGiven set rectangles, problem find enclosing rectangles minimum areacontain without overlap. refer enclosing rectangle bounding box,avoid confusion. optimization problem NP-hard, problem decidingwhether set rectangles packed given bounding box NP-complete, viareduction bin-packing (Korf, 2003). consecutive-square benchmark simple setincreasingly difficult benchmarks problem, task find boundingboxes minimum area contain set squares dimensions 1 1, 2 2, ...,N N (Korf, 2003). example, Figure 1 optimal solution N =32.use benchmark explain many ideas paper, techniqueslimited packing squares, apply rectangles.Rectangle packing many practical applications, including modeling scheduling problems tasks require resources allocated contiguous chunks.example, consider task scheduling allocating contiguous memory addressesprograms. width rectangle represents length time program runs,c2012AI Access Foundation. rights reserved.fiHuang & KorfFigure 1: optimal solution N =32 consecutive-square benchmark, packingsquares dimensions 1 1, 2 2, ..., 31 31, 32 32 bounding box minimumarea, 85 135.48fiOptimal Rectangle Packing: Absolute Placement Approachheight represents amount contiguous memory needs. rectangle packing solutiontells us programs run, well memory addressesassigned. Similar problems include scheduling ships different lengthberthed along single, long wharf (Li, Leong, & Quek, 2004), well allocation scheduling radio frequency spectra usage (Mitola & Maguire, 1999). Rectanglepacking also appears loading set rectangular objects pallet without stackingthem. cutting stock layout problems also contain rectangle packing subproblems.1.1 Overviewremainder article organized follows. first introduce various benchmarks Section 2 specifically define rectangle packing instances solve.Section 3, review state-of-the-art rectangle packers techniques,provides foundation upon present new work. follow Section 4data collected compare work previous state-of-the-art using previousbenchmarks. also compare difficulty previous benchmarks new ones.Section 5, present benchmark rectangles successively higher precisiondimensions, new solution techniques handle this, follow experimental results.compare methods competing search spaces used packing highprecision rectangles, show methods remain competitive.Sections 6 7 explain various avenues future work, concluding articlesummarizing contributions results. previously published muchwork several conference papers (Huang & Korf, 2009, 2010, 2011).2. Benchmarksseveral reasons motivating benchmarks. First, benchmarks describeinstances single parameter N , allowing researchers easily reproduce instances.Second, instances unique, optimal solutions reported easilyvalidated others. advantages many real-world instance librariesrandomly generated ones. Third, benchmarks define infinite set instancessuccessive instance harder previous. solver superior another solversolve instance faster, larger instance amount time.contrast, comparison using library instances may require counting numberinstances completed within given time limit. Furthermore, instance libraries,often one solver performs well one subset instances competing solver performswell different subset, making comparisons inconclusive.believe benchmarks capture difficult instances rectanglepacker may face investigate modeling generation random problems.Although Clautiaux et al. (2007) others used random instances, non-randombenchmarks used Korf (2003) Simonis OSullivan (2008) better facilitatedcomparison state-of-the-art packers. However, comprehensive overviews,refer reader numerous surveys available (Lodi, Martello, & Vigo, 2002; Lodi,Martello, & Monaci, 2002; Dowsland & Dowsland, 1992; Sweeney & Paternoster, 1992).49fiHuang & Korf2.1 Previous BenchmarksSeveral previous benchmarks used literature shown easierbenchmarks propose. Part due fact benchmarks, like solvers,may also improved research, ensure cover various propertiesrectangles, addition providing easy way compare performance among differentpackers measure progress.consecutive-square benchmark (Korf, 2003), simple set increasingly difficultinstances, task find bounding boxes minimum area contain setsquares sizes 1 1, 2 2, ..., N N . Prior work, many recent stateof-the-art packers used popular benchmark measure performance, includingMoffitt Pollack (2006), Korf, Moffitt, Pollack (2010), Simonis OSullivan(2008). date, largest instance solved problem N =32, shown Figure 1,using packer (Huang & Korf, 2009). consider problem packing squaressquare benchmark gets much easier problem size increases, duelarge differences areas consecutive square bounding boxes.unoriented consecutive-rectangle benchmark (Korf et al., 2010), instanceset rectangles sizes 1 2, 2 3, ..., N (N + 1), rectangles mayrotated 90-degrees. subsequently explain, fact many pairsrectangles instance share equal dimensions causes optimal solutionsleave empty space, making benchmark easy solve. include benchmarkcompleteness, note effective measure comparing different packers.Finding first optimal solution another benchmark Simonis OSullivan(2011) used conjunction problem instances unoriented consecutiverectangle benchmark. contrast problem finding optimal solutions,measure time takes find first optimal solution, makes muchdifficult reliably compare solvers unless focus research valueordering tie-breaking among bounding boxes equal area.example, Simonis OSullivan (2011) report find first solutionN =26 takes 3:28:20 (3 hours, 28 minutes, 20 seconds). shown Table 8 page72, six solutions N =26: 42 156, 52 126, 56 117, 63 104, 72 91, 78 84,requiring solver CPU times 0:32, 41:40, 53:19, 1:55:04, 1:33:22, 8:53:01,respectively. smaller bounding boxes needed test optimalsolution empty space, used Simonis OSullivans termination criteriareturned first optimal solution, would need 32 seconds. Therefore, findingminimum bounding boxes instead first one benchmark producesharder problems larger N , better facilitates program comparisons.2.2 Properties Easy Benchmarks Avoidmotivate new benchmarks, explain previous benchmarks tendedmuch easier comparison, constructed new benchmarksdescribe instances consisting rectangles unique dimensions, without duplicates,without area occupied rectangles.50fiOptimal Rectangle Packing: Absolute Placement Approach(a) Solution 21 35 boundingbox unoriented instance 1 2,2 3, ..., 11 12, 12 13.(b) Solution 14 26 boundingbox unoriented instance 112, 2 11, ..., 11 2, 12 1.Figure 2: Examples solutions instances rectangles equal dimensions.2.2.1 Rectangles Equal Dimensionsunoriented consecutive-rectangle benchmark, rectangles share dimensionanother rectangle. example, Figure 2a optimal solution N =12. optimalsolutions, rectangles equal dimensions tend line next other, forming largerrectangles leaving little empty space. Figure 2a, 8 9 7 8 line up,5 6 4 5, 3 4 2 3. fact, solutionsbenchmark much smaller percentage empty space similar-sized instancesconsecutive-square benchmark, rectangles unique dimensions.also notice benchmarks duplicate rectangles, Figure 2b, solvedquickly.2.2.2 Rectangles Small Area Small DimensionsFigure 2b also example perfect packing, empty spacesolution. Problems perfect packings tend easy two reasons. Onetest bounding boxes increasing order area, test fewer boxes, since never testbox minimum area required. second problems,rather deciding rectangle go bounding box,efficient algorithm decide cell empty space rectangle occupy51fiHuang & Korfit. soon small region empty space created cant accomodate remainingrectangles, algorithm backtrack.consecutive-square unoriented rectangle benchmarks, largerectangles capture much total area instance. Thus, packer searchdeeply using allowable empty space. little empty space, earlybacktracking likely since cannot find place next rectangle. Therefore,small rectangles benchmarks insignificant impact search effort.previous benchmarks, consecutive-square benchmark, retangleslargest area also largest dimensions, making obvious rectanglesplace first, largest rectangles constrained, imposeconstraints remaining rectangles.contrast, new benchmarks trade-off rectangles largedimensions large area. widest rectangle oriented equal-perimeter benchmark, described below, smallest branching factor search xcoordinates. However, also least area, search wont constrainplacement remaining rectangles much. raises non-trivial question bestvariable ordering non-square rectangles.2.3 New Benchmarkspropose several new benchmarks difficult comparing instancesnumber rectangles. experimental results make use followingbenchmarks, addition consecutive-square unoriented consecutive-rectanglebenchmarks described above.2.3.1 Equal-Perimeter RectanglesFirst, present oriented equal-perimeter rectangle benchmark, instanceset rectangles sizes 1 N , 2 (N 1), ..., (N 1) 2, N 1, rectangles mayrotated (see Figure 3). Given N , rectangles unique perimeter 2N +2.experiments, benchmark much difficult either consecutive-squarebenchmark unoriented consecutive-rectangle benchmark (Korf et al., 2010)number rectangles. tested state-of-the-art packer (Huang & Korf, 2010)old new benchmarks. N =22 oriented equal-perimeter benchmarktook nine hours solve, N =22 consecutive-square unorientedconsecutive-rectangle benchmarks took one second six seconds, respectively.Second, present unoriented double-perimeter rectangle benchmark, instances described set rectangles 1 (2N 1), 2 (2N 2), ..., (N 1) (N + 1),N N , rectangles may rotated 90-degrees. rectangles uniqueperimeter 4N . benchmark difficult benchmarksused previously literature, benchmark also difficult orientedone introduced previous paragraph. experiments using techniques,N =18 took two days solve.far, benchmarks discussed low-precision integer dimensions.property poses problem packer, enumerates various integer coordinate locations rectangle may placed. high-precision values, however,52fiOptimal Rectangle Packing: Absolute Placement ApproachFigure 3: optimal solution N =23 oriented equal-perimeter benchmark, packingoriented rectangles dimensions 1 23, 2 22, ..., 22 2, 23 1 bounding boxminimum area, 38 61.53fiHuang & Korfnumber distinct positions increases dramatically. motivates study packing rectangles high-precision dimensions. particular, propose unorientedhigh-precision rectangle benchmark, instances described set rectangles11 11111 2 , 2 3 , ..., N N +1 . methods used solve benchmark quitedifferent used low-precision case.3. Solution Techniquessection describe previous solution strategies well various new techniquesuse rectangle packer. first describe techniques apply consecutive-square benchmark, oriented equal-perimeter benchmark, unorienteddouble-perimeter benchmark. work unoriented high-precision rectangle benchmark included methods significantly different, deferredSection 5.3.1 Previous Workearlier work focused optimal methods packing set rectanglesgiven bounding box motivated problem pallet loading. Dowsland (1987)used depth-first search abstract graph representation search space solveproblem optimally problem sets modeled real-world pallet box dimensions.Although problem instances contained average 30 rectangles 50,benchmarks far easier consider here, rectanglessize, significant amount empty space solutions. Bhattacharyaet al. (1998) extended work additional lower bounds pruning techniques baseddominance conditions demonstrated work benchmarks.examining rectangle packing instances rectangles different dimensions,Onodera et al. (1991) used depth-first search, branching point searchspace commitment particular non-overlap constraint two rectangles.Lower bound graph reduction techniques applied prune search space, allowing optimally solve problems six rectangles.Chan Markovs BloBB (2004) packer used branch-and-bound order findminimum area bounding box contain set rectangles. solver could handleeleven rectangles, observed instances duplicate rectanglesmuch easier, causing packer cluster rectangles together optimal solution.Lesh et al.s solver (2004) used depth-first search, placing rectangle first bottommost left-most position fit (the bottom-left heuristic, see Chazelle, 1983),determine whether set rectangles packed given enclosing rectangle.able handle twenty-nine rectangles ten minutes average,testbed consisted instances whose optimal solutions empty space.Clautiaux et al. (2007) presented branch-and-bound method x-coordinates rectangles computed prior y-coordinates. assigningx-coordinates, method uses relaxation similar cumulative constraint (Aggoun& Beldiceanu, 1993) requires sum heights rectangles overlappingparticular x-coordinate cannot exceed height bounding box. y-coordinatesdetermined using search space derived bottom-left heuristic (Chazelle,54fiOptimal Rectangle Packing: Absolute Placement Approach1983), using optimized data structures Martello Vigo (1998). BeldiceanuCarlsson (2001) applied plane sweep algorithm used computational geometry detect violations non-overlap constraints, later adapted technique geometricconstraint kernel (Beldiceanu, Carlsson, Poder, Sadek, & Truchet, 2007). Lipovetskii (2008)proposed branch-and-bound algorithm placed rectangles lower-left hand positions.prior state-of-the-art, due Korf (2003, 2004) Simonis OSullivan (2008),divide rectangle packing problem containment problem minimalbounding box problem. former tries pack given set rectangles given boundingbox, latter finds bounding box least area contain given setrectangles. packers algorithm minimal bounding box problem callsalgorithm containment problem subroutine.3.2 Overall Search StrategyLike Korf et al.s (2010) algorithm, minimum bounding box solver callscontainment problem solver, like Simonis OSullivan (2008), assign x-coordinatesprior y-coordinates.Although use Simonis OSullivans (2008) ideas, takeconstraint programming approach constraints specified general-purposesolver like Prolog. Instead, implemented program scratch C++, allowing usflexibly choose constraints use time naturally encodesearch space use y-coordinates. implemented chronological backtrackingalgorithm dynamic variable ordering. algorithm works five stages goesroot search tree leaves:1. minimum bounding box algorithm generates initial candidate set boundingboxes various widths heights.2. containment solver called bounding box order increasing area,infeasible bounding box, insert another back candidate setbounding boxes height one unit greater. packing found, continuetesting boxes equal area find optimal solutions terminating.3. containment solver first works x-coordinates model variablesrectangles values x-coordinate locations, using dynamic variable orderingconstraint detects infeasible subtrees.4. x-coordinate solution found, problem transformed perfect packing instance.5. searches set y-coordinates model variables empty cornersvalues rectangles.describe detail steps.55fiHuang & Korf3.3 Minimum Bounding Box ProblemOne way solve minimum bounding box problem find minimum maximumareas describing set candidate potentially optimal bounding boxes. Boxessizes generated areas within range, tested non-decreasing orderarea solutions smallest area found. lower bound areasum areas given rectangles. upper bound area determinedbounding box greedy solution found setting bounding box heighttallest rectangle, placing rectangles first available positionscanning left right, column scanning bottom top.several techniques (Korf, 2003, 2004) use prune set boundingboxes, review here. first generate set widths bounding boxes,starting width widest rectangle width greedy solutiondescribed above. width, generate feasible height using lower boundssubsequently describe. resulting bounding boxes used initializemin-heap sorted non-decreasing order area. search proceeds callingcontainment solver bounding box minimal area heap. box infeasible,increase height box one, insert new box back min-heap.given bounding box width, initialize height maximum followinglower bounds. First, height must least height tallest rectangleinstance. Second, height must large enough accommodate total arearectangles instance. Third, every pair rectangles, sum widthsexceed width bounding box, bounding box height must leastsum heights, since cant appear side-by-side, one must topother. Fourth, set rectangles whose widths greater half widthbounding box must stacked vertically, including rectangle smallest heightwhose width exactly half width bounding box. Finally, certain propertiesexist given rectangle packing instance, force height greater equalwidth break symmetry. example, one sufficient property instanceconsisting squares, since solution W H bounding box easily transformsanother one H W bounding box. Another sufficient property every rectangledimensions w h correspond another one dimensions h w.unoriented instances, given bounding box width, certain rectangles may forcedone orientation, improving lower bound bounding box height. Notealso break symmetry bounding box dimensions every unoriented instance.3.3.1 Anytime Algorithmproblem instance many rectangles, immediate solution required,Korf (2003) provides anytime algorithm bounding box problem, replacing onedescribed above, also calls containment problem solver. first find greedysolution bounding box whose height equal tallest rectangle, describedprevious section. repeatedly call containment problem solver followingway. previous attempt given bounding box resulted packing areagreater area best solution seen far, decrease widthone unit attempt solve resulting bounding box problem. instead previous56fiOptimal Rectangle Packing: Absolute Placement Approachattempt infeasible, increase height bounding box one unit.algorithm terminates width current bounding box less widthwidest rectangle.3.4 Containment ProblemKorfs (2003) absolute placement approach modeled rectangles variables positionsbounding box values. Rectangles placed turn depth-first search,possible locations tested rectangle. contrast, Simonis OSullivans(2008) packer assigned x-coordinates rectangles y-coordinates,suggested Clautiaux et al. (2007), well using cumulative constraint (Aggoun& Beldiceanu, 1993), improving performance orders magnitude. cumulativeconstraint adds height rectangles overlap given x-coordinate location,pruning values exceed height bounding box. constraintchecked exploring x-coordinates also exploring y-coordinates later on.improved exploring y-coordinates differently, modeling candidate locationsvariables, rectangles values (Huang & Korf, 2009), made packerorder magnitude faster Simonis OSullivans.Simonis OSullivan (2008) furthermore applied least-commitment principle (Yap,2004) constraint processing, first committing placement rectanglesinterval x-coordinates instead single x-coordinate value. x-intervalsexplored turn, constrain candidate individual x-coordinates explored later.works committing x-interval induce pruning via cumulative constraint.example, picking x-interval [a, b] size smaller widthrectangle wr , implies regardless x-coordinate rectangle eventually takes,must contribute height x-coordinate within interval [b, + wr ]. Finally,height bounding box constrains cumulative heights rectanglesgiven x-coordinate, similar ideas Beldiceanu et al. (2008). Larger intervals resultweaker constraint propagation (less pruning) smaller branching factor, smallerintervals result stronger constraint propagation larger branching factor. sizeintervals experimentally determined.example, 4 2 rectangle x-coordinates restricted interval [0,2] contributes height 2 x-coordinates 2 3 even prior deciding exact x-coordinatevalue. compulsory part (Lahrichi, 1982) constrains cumulative height rectangles may overlap x-coordinates 2 3 solution. interval assignmentsinfeasible, searching individual x-values futile. However, findset interval assignments, still search set single x-coordinatevalues. Simonis OSullivan (2008) assigned x-intervals, single x-coordinates, y-intervals,single y-coordinates, order.3.5 Assigning X-Intervals X-Coordinatesx-coordinates, propose pruning constraint adapted Korfs (2003) wastedspace pruning heuristic, dynamic variable order replace Beldiceanus (2008) fixed ordering, method optimize values assigned x-interval variables.57fiHuang & KorfFigure 4: test violations cumulative constraint, remaining spaceplacing 3 2 rectangle x=2 represented vector h3, 3, 1, 1, 1, 3i.3.5.1 Pruning Infeasible Subtreespresent constraint-based formulation Korfs (2003) two-dimensional wasted spacepruning algorithm, adapted one-dimensional case. Given partial solution, Korfsalgorithm computed lower bound amount wasted space, usedprune upper bound. contrast, compute numerical boundsinstead detect infeasibility single constraint.rectangles placed bounding box, remaining empty space gets choppedsmall irregular regions. Eventually empty space segmented small enoughchunks cannot accommodate remaining unplaced rectangles,point backtrack. assigning x-coordinates bounding box height H,keep histogram hv1 , v2 , . . . , vH i, vi number empty cells (units emptyspace) empty columns height i. example, assume Figure 4assigned x-coordinates 3 2 rectangle 6 3 bounding box. resultinghistogram would h3, 0, 9i, since 3 cells empty columns height 1, emptycells columns height 2, 9 cells empty columns height 3.Assume left place 2 3 2 2 rectangle. assignsix cells 2 3 rectangle empty cells v3 =9, leaving us remainingempty cells h3, 0, 3i. point, cannot assign area 2 2,3 empty cells accommodate height need 4, prune.general, set unplaced rectangles R bounding box height H,h,Xwr hrrR,hr hHXvi ,(1)i=hrectangle r R dimensions wr hr . is, every given height h,amount space accommodate rectangles height h greater must leastcumulative area rectangles height h greater. check constraintx-coordinate assignment.58fiOptimal Rectangle Packing: Absolute Placement Approach(a) x=2 dominated position4 4 square.(b) x=0 undominated position4 4 square.Figure 5: Example dominance conditions.3.5.2 Pruning Dominance ConditionsKorf (2003) introduced set dominance conditions prune positions large rectangles close sides bounding box. example, imagine must packsquares 44, 33, 22, 11. Figure 5a, placement 44 square leaves2 4 gap left side bounding box 3 3 square cannot fit.2 2 1 1 squares fit within gap, fact placedentirely within gap. Notice solution arrangement Figure 5a,always rearrange Figure 5b without disturbing squares. Thus,need try placing 4 4 square x=2 long tried placingx=0. general, rectangle placement dominated leaves gap rectanglesindividually fit also packed together gap without protrudingit. Although Korf hard-coded dominance rules consecutive-square benchmark,dynamically generate every instance insignificant preprocessing overhead.3.5.3 Variable Orderingfollowing subsections consider two variable orders work together packer.use fixed ordering governs rectangle assigned next. ordering usedx-intervals independently use single x-coordinate variables.point time, also must choose whether assign next x-interval nextsingle x-coordinate variable. Since ordering x-intervals single x-coordinatevariables simpler, present technique first.Ordering X-Intervals X-Coordinates Area variable orderbased observation placing rectangles larger area constrainingplacing smaller area. times either choose assign singlex-coordinate rectangle previously assigned x-interval,assign x-interval rectangle yet made assignments for. shownFigure 4, either assignments decrease amount empty space representedcumulative constraint vector. always pick next variable resultsleast remaining space.59fiHuang & KorfOrdering Among Rectangles Branching Factor natural variable orderarises consecutive-square unoriented consecutive-rectangle benchmarks using strategy picking constrained variable next. example,consecutive-square benchmark, largest rectangle clearly largest height,width, area. However, new benchmarks rectangle largest widthsmallest height, largest area, making good variable ordering non-obvious.propose variable order rectangles various aspect ratios pickingvariable fewest number values first, favor smaller branching factor closerroot search tree. oriented equal-perimeter benchmark, recallassign intervals x-coordinates individual x-coordinates, like SimonisSullivan (2008) use constant factor times rectangle width define intervalsize. branching factor x-interval variables given rectangleBw rw1Bw 1b=,(2)=CrwC rwCBw bounding box width, rw rectangle width, C constant chosenexperimentally. numerator Bw rw number x-coordinate valuesrectangle still fitting bounding box, denominator Crwsize interval assigning given rectangle. example, C=0.75would assign intervals size three 4 2 rectangle.may drop translational constant 1/C well positive scalar Bw /C sinceinterested relative ordering rectangles, leaving us 1/rwmeans oriented benchmark place rectangles order decreasingwidth. unoriented double-perimeter benchmark, packer first tries valuesparticular x-interval, rotates rectangle 90-degrees trying another setx-interval values. case branching factorBw rwBw 1Bw rh12b==++.(3)CrwCrhC rwrhCmentioned before, drop scalar translational constant, giving us1rw + rh1=+.(4)rwrhrw rhrectangles given instance perimeter definition,numerator result Equation 4 constant. Therefore unoriented benchmark,place rectangles order decreasing area.3.5.4 Determining Sizes X-Intervalsconsecutive-square benchmark, packer used interval size 0.35 timeswidth given rectangle. found larger interval sizes improve performancepacker new equal-perimeter benchmarks, use value C=0.55 instead.assign larger intervals short wide rectangles, x-interval variablesrectangles tend branching factors three less. balancesizes intervals values assigned equally constraining subtrees.example, consider C=0.55, rectangle width 20, set possible x-coordinate60fiOptimal Rectangle Packing: Absolute Placement Approachvalues [0,23]. Without balancing sizes intervals, packer would explore intervalsizes 20C = 11, x=[0,10], x=[11,21], finally remaining domain valuessmall interval x=[22,23]. results small compulsory parts thereforelarge search subtrees first two branches, large compulsory part thussmall search subtree third.Since must explore three branches anyway, balance sizes intervalassignments exploring x=[0,7], x=[8,15], x=[16,23]. eventual effect betterbalance size search subtrees amongst branches. packer first computesbranching factor induced global interval parameter C=0.55 rectangle,balances number values interval assignment.Interactions Interval Assignment Dominance Conditions consecutive-square instances, squares several positions following x=0dominated. Therefore, packer first branches assigning degenerate interval x=[0,0] exploring interval assignments undominated positions. Althoughtechnique increased performance packer fivefold compared leaving out,strategy slowed performance fivefold oriented unoriented doubleperimeter benchmark. reason degradation performance follows.equal-perimeter benchmarks, 1 N rectangle always partially fit gapsleft rectangles, must always protrude gaps, thereby eliminatingdominance conditions previously described. Without dominated positionsaccount for, simply applying strategy used consecutive-squares newbenchmarks results packer committing single x-coordinate values situationsdesirable include positions larger interval assignment.avoid this, packer detects dominated positions dynamicallychooses whether assign degenerate interval x-coordinate assignment,immediately begin interval assignments.3.6 Perfect Packing Transformationevery complete x-coordinate solution, transform problem instance perfectpacking problem instance working y-coordinates. perfect packing instancerectangle packing problem property solution empty space.transformation done adding original set rectangles number 1 1rectangles necessary increase total area rectangles bounding box.Although new 1 1 rectangles increase problem size, hope easesolving perfect packing instances offset difficulty packing rectangles. Nextdescribe search space perfect packing. show, methods relyperfect packing property empty space.3.7 Assigning Y-Coordinatesalternative asking rectangle go? ask rectanglego here? former model, rectangles variables empty locationsvalues, whereas latter, empty locations variables rectangles values.y-coordinates, search latter model. use 2D bitmap draw placed rectangles61fiHuang & Korftest overlap, backtrack positions cannot accommodate remainingrectangles, required Korfs (2003) wasted space pruning rule.3.7.1 Empty Corner Modelperfect packing solutions, every rectangles lower-left corner fits lower-leftempty corner formed rectangles, sides bounding box, combinationboth. model, one variable per empty corner. final solution, sincerectangle goes exactly one empty corner, number empty corner variablesequal number rectangles perfect packing instance. set valuesset unplaced rectangles.search space interesting property variables dynamically createdsearch x- y-coordinates empty corner knownrectangles create placed. Furthermore, placing rectangle empty cornerassigns x- y-coordinates.Note empty corner model describe perfect packing solutions. Givenperfect packing solution, list unique sequence rectangles scanningleft right, bottom top lower-left corners rectangles. sequencecorresponds sequence assignments root search space leaftree. property also bounds maximum size search space N 0 ! N 0number rectangles performed perfect packing transformation.3.7.2 Duplicate RectanglesDue additional 1 1 rectangles perfect packing transformation,introduced additional redundancy problem. simple way handlefollows. particular empty corner, never place rectangle duplicate onealready tried position. method handling duplicates also appliesduplicate rectangles original problem instance.4. Experimental Resultsbenchmarked packers Linux 2GHz AMD Opteron 246 2GB RAM.packer call KMP10 (Korf et al., 2010) benchmarked machine,quote published results. include data relative placementpacker competitive. Results Simonis OSullivans packer (2008),call SS08, also quoted, obtained SICStus Prolog 4.0.2 Windows3GHz Intel Xeon 5450 3.25GB RAM. Since machine faster ours,comparisons conservative estimate relative performance.4.1 Previous Benchmarksconsecutive-square benchmark unoriented consecutive-rectanglebenchmarks (Korf et al., 2010) used literature measure performance,include data collected using two benchmarks.62fiOptimal Rectangle Packing: Absolute Placement ApproachSizeNKMP10TimeSS08TimeFixedOrderTime202122232425262728293031321:329:5437:033:15:2310:17:022:02:58:368:20:14:5134:04:01:03:02:07:513:585:5640:383:41:4311:30:02:00:03:02:14:402:2710:251:08:552:18:12:13HK09Time:00:03:02:12:372:149:3935:124:39:318:06:032:17:32:524:16:03:4233:11:36:23Table 1: CPU times required various packers consecutive-square benchmark,task pack squares 1 1 N N .4.1.1 Consecutive SquaresTable 1 compares CPU runtimes four packers consecutive-square benchmark.first column specifies instance size, number squaressize largest one. remaining columns specify CPU times required variousalgorithms find optimal solutions format days, hours, minutes,seconds. multiple boxes minimum area, N =27 listed Table8 Appendix 4.4, report total time required find optimal bounding boxes.two reasons. First, finding minimum area bounding boxes removesquestion bounding box test first one area. Second,providing optimal solutions, researchers working rectangle packing useinformation verify correctness programs.HK09 includes wasted space pruning rule x-coordinates, dynamic variableordering x-intervals x-coordinates, perfect packing transformation,related search space inference rules. named packer consistentprevious work (Huang & Korf, 2009). SS08 refers previous state-of-the-artpacker (Simonis & OSullivan, 2008). largest problem previously solved N =27took SS08 11 hours. solved problem 35 minutes solved fiveopen problems N =32. KMP10 refers Korf et al.s (2010) absolute placementpacker. FixedOrder assigns x-intervals single x-coordinates, includesideas. HK09s dynamic variable ordering x-coordinates ordermagnitude faster FixedOrder N =28. order magnitude improvementFixedOrder SS08 likely due use perfect packing assigning ycoordinates. include timing packer perfect packing disabledcompetitive (e.g., N =20 took 2.5 hours).63fiHuang & KorfSizeN212223242526272829303132X-CoordinateSolutions6652833918701931,0262442,71511,12910,24473,61437,742SecondsX0.350.956.5419.4173.38313.811,181.538,987.3615,677.20124,399.74214,575.081,916,312.67Seconds1.040.180.311.080.141.390.6023.4028.8217.97254.42102.59RatioX:Y0.35.321.118.0524.1225.81,969.2384.1544.06,922.6843.418,679.3Table 2: CPU times spent searching x- y-coordinates consecutive-squarebenchmarkTable 2 second column number complete x-coordinate assignmentspacker found entire run particular problem instance. third columntotal time spent searching x-coordinates. fourth column total timespent performing perfect packing transformation searching y-coordinates.columns represent total CPU time entire run given problem instance.last column ratio time third column fourth. Interestingly,almost time spent x-coordinates opposed y-coordinates,suggests could efficiently enumerate x-coordinate solutions, could alsoefficiently solve rectangle packing. confirmed relatively x-coordinatesolutions exist even large instances. data Table 2 obtained Linux2.93GHz Intel Core 2 Duo E7500 machine separate experiment Table 1,total time spent given instance different.4.1.2 Unoriented Consecutive RectanglesTable 3 compares CPU times packer unoriented consecutive-rectanglesbenchmark Korf et al. (2010). Although techniques due SimonisOSullivan (2008) outperform Korf et al. consecutive-square benchmark,previously published results benchmark besides Korf et al.benchmark easier consecutive-square benchmark, breakcontributions techniques, differences delineatedclearly previous section. primary differentiating feature benchmarkrectangles unoriented.first column gives size problem instance. second column givesperformance previous state-of-the-art packer benchmark, using Korf et al.scode (2010). third column gives performance packer benchmark.64fiOptimal Rectangle Packing: Absolute Placement ApproachSizeNKMP10TimeHK10Time1617181920212223242526272829:01:05:17:078:1115:001:09:458:51:4611:53:177:17:00:03:00:00:00:00:05:06:17:4713:382:21:106:31:514:07:37:081:16:43:026:04:47:06Table 3: CPU times required two packers unoriented consecutive-rectangle benchmark, task pack unoriented rectangles sizes 12, 23, ..., N (N +1).data table collected Linux 2.93GHz Intel Core 2 Duo E7500 machine,except N =28 N =29, collected Linux 2.53GHz Intel Xeon E563012GB RAM, experiments revealed 20% faster formermachine.benchmark techniques allowed us extend known solutionsN =25 N =29 allowed us solve N =25 80 times faster previousstate-of-the-art benchmark.4.2 Oriented Equal-Perimeter Unoriented Double-Perimeter Rectanglessection uses new benchmarks compare techniques developednon-square instances. techniques discuss here, including dynamic adjustmentinterval sizes generalized variable order based branching factor, largelyaffect performance packer consecutive-square benchmark. fact,tested packer benchmark see effects extra overhead addedimprovements. new packer resulted five percent speedup comparedpacker without changes consecutive-square benchmark, likely due minorimprovements data structures, balancing interval sizes. Therefore, compareeffects techniques new benchmarks. techniquesdeveloped new benchmarks improve performance oriented unorientedcases, discuss together.Table 4 compares performance packers oriented equal-perimeter benchmark Table 5 compares packers using unoriented double-perimeterbenchmark. first column refers problem size instance, number65fiHuang & KorfSizeNBoxesTestedHK09TimeOptDomTimeBrFactorTimeC=0.55TimeHK10Time131415161718192021222377109812121191516:01:02:16:575:561:06:326:35:481:18:51:343:21:31:46:00:01:05:161:2114:471:26:167:36:0913:33:16:00:00:01:02:276:1531:231:51:104:22:49:00:00:00:00:03:323:3413:0620:4914:22:03:00:00:00:00:02:222:157:5111:209:12:373:22:50:38Table 4: CPU times required various packers oriented equal-perimeter rectanglebenchmark, task pack oriented rectangles sizes 1 N , 2 (N 1), ...,(N 1) 2, N 1.SizeNBoxesTestedHK09TimeOptDomTimeBrFactorTimeC=0.55TimeHK10Time11121314151617181217131721352735:01:201:4528:481:43:011:16:46:44:00:04:214:5311:364:13:341:12:40:14:00:04:214:5311:364:13:341:12:40:14:00:01:061:193:331:16:029:44:14:00:01:061:152:341:01:547:53:502:02:10:38Table 5: CPU times required various packers unoriented double-perimeter rectangle benchmark, task pack unoriented rectangles sizes 1 (2N 1),2 (2N 2), ..., (N 1) (N + 1), N N .66fiOptimal Rectangle Packing: Absolute Placement Approachrectangles. second column gives number bounding boxes tested orderfind optimal solutions. remaining columns represent CPU times differentversions packer format days, hours, minutes, seconds. wrotepacker C++ collected data Linux 2.93GHz Intel Core 2 Duo E7500 machine.left right, successive packer improves previous one includingadditional technique. column called HK09 data collected using techniquesdeveloped specifically consecutive-square packing, include perfect packingtransformation related inference rules, dynamic variable ordering singlex-coordinates x-intervals, wasted space pruning rule x-coordinates(Huang & Korf, 2009). compare new variable ordering rectanglesvarious aspect ratios, used order decreasing area default HK09.OptDom improves upon HK09 dynamically detecting dominance rules applyinapplicable, optimizes x-interval assignments knowledge. BrFactorimproves upon OptDom orders oriented equal-perimeter benchmark decreasing width unoriented double-perimeter benchmark decreasing area. C=0.55improves upon BrFactor use interval size 0.55 instead C=0.35consecutive-square benchmark. Finally, HK10 improves upon C=0.55 usingknowledge branching factor rebalance sizes interval assignmentsx-coordinates.Notice OptDom, BrFactor, C=0.55 introduce techniques reducebranching factor, greater effect performance HK10, whosenew technique seeks make intervals assigned equally constraining. experimentsreveal techniques interact one another, note without includingdominated positions intervals, performance gained techniquesappears muted. interaction also tune global interval parameter Cincluding techniques affect branching factor.Ordering branching factor improved performance oriented equal-perimeterbenchmark unoriented benchmark. latter case, seen Table 5,technique ordering branching factor prescribes ordering decreasing area,gave packer reasonable default. Therefore, differencealgorithm performance OptDom BrFactor columns Table 5.Note unoriented double-perimeter benchmark requires packer trytwice many bounding boxes given parameter N required oriented benchmark. due 2N -1 largest dimension unorientedbenchmark N largest dimension oriented benchmark. largerrectangles introduce higher precision problem, must try boundingboxes. containment problem unoriented instance problem spacefactor 2N larger oriented instance due two orientationsrectangle. Thus, instance N rectangles benchmark incomparableinstance N squares consecutive-square benchmark evaluating benchmarkdifficulty.summary, using techniques together, solve N =21 orientedequal-perimeter benchmark 500 times faster N =16 unoriented doubleperimeter benchmark 40 times faster techniques presented optimizedconsecutive squares.67fiHuang & KorfSizeBoxes TestedCPU TimeNSquaresPerimeterSquaresPerimeter16171819202122232425262728293031321051412142017191917212230272130369812121191516:00:00:00:00:00:01:01:07:201:145:1519:422:30:114:21:461:10:33:382:11:40:2922:04:20:15:00:02:222:157:5111:209:12:373:22:50:38Table 6: Number bounding boxes tested CPU time required solve given instanceconsecutive-square oriented equal-perimeter benchmarks.68fiOptimal Rectangle Packing: Absolute Placement Approach4.3 Comparing Easy Hard Benchmarksfollowing tables compare difficulty various benchmarks using packer (Huang& Korf, 2010) optimizations enabled.4.3.1 Consecutive Squares vs. Equal-Perimeter RectanglesTable 6, first column indicates number rectangles instance. secondthird columns labeled Boxes Tested give number bounding boxestested finding optimal solutions consecutive-square benchmarkoriented equal-perimeter benchmark, respectively. fourth fifth columns giveperformance rectangle packer benchmarks well. data pointtable collected using Linux 2.93GHz Intel Core 2 Duo E7500 using one process, onethread, one core.Notice given instance number rectangles, oriented equalperimeter benchmark significantly harder consecutive-square benchmark.due fact given problem size, consecutive-square benchmark containsmany little squares typically easy place property missing equalperimeter benchmark. fact, N =23 packer requires four orders magnitudetime find optimal solutions new benchmark compared instancenumber items consecutive-square benchmark.4.3.2 Unoriented Consecutive-Rectangles vs. Unoriented Double-PerimeterRectanglesTable 7 shows removing certain properties results successively difficult benchmarks. start unoriented consecutive-rectangle benchmark (Korf et al., 2010)contains many easy properties. Doubly Scaled column pack 2 4, 4 6,68, ..., (2N )(2N +2) rectangles, simply scales unoriented consecutive-rectangle benchmark factor two. benchmark difficult integershigher magnitude lead x-coordinates search, turn increases branching factor problem. Unique Dimensions column pack rectanglessizes 1 2, 3 4, 5 6, ..., (2N 1) (2N ), differs previous benchmarkdimensions unique. last column distributes area among rectanglesuniformly avoid consolidating area first rectangles.column also culmination difficult properties identifiedrectangle packing benchmark, call unoriented double-perimeter benchmark.data points table collected using Linux 2.93GHz Intel Core 2 Duo E7500machine without parallelization, except N =28 N =29, collectedLinux 2.53GHz Intel Xeon E5630 machine 12GB RAM, estimatethirty percent faster.4.4 Bounding Boxes Minimum Areasection list optimal bounding boxes various benchmarks foundprogram optimizations enabled. Notice duplicate data69fiHuang & KorfSizeN121314151617181920212223242526272829UnorientedConsecutiveRectangles:00:00:00:00:00:00:00:00:05:06:17:4713:382:21:106:31:514:07:37:081:16:43:026:04:47:06DoublyScaled:00:00:00:00:00:00:01:01:09:10:291:1327:376:41:201:02:12:06UniqueDimensions:00:00:01:00:01:01:03:11:503:0015:343:21:3612:23:37UnorientedDoublePerimeter:01:061:152:341:01:547:53:502:02:10:38Table 7: CPU time required optimized packer various benchmarks increasingdifficulty.70fiOptimal Rectangle Packing: Absolute Placement Approachunoriented high-precision rectangle benchmark, leave Table 10, Section 5.5.2,since discussion refers data.first column tables 8 9 refer size problem instancerespective benchmarks. columns called Optimal Solution give dimensionsoptimal bounding boxes given instance. next column called Empty Space givespercent empty space optimal solution. next column gives numberbounding boxes tested order find optimal solutions given instance.5. Absolute Placement High-Precision InstancesMeir Moser (1968) first proposed problem finding smallest squarecontain infinite series rectangles sizes 11 12 , 12 31 , 13 14 , ..., etc. rectanglescannot overlap unoriented. unit square exactly enough area since totalarea rectangles infinite series one. hand, spacewasted, suggesting task infeasible. Inspired problem, propose lastbenchmark developed several new techniques.introduce unoriented high-precision rectangle benchmark, task findbounding boxes minimum area contain finite set unoriented rectanglessizes 11 12 , 12 13 , ..., N1 N 1+1 . example, N =4 one must pack rectanglessizes 11 12 , 21 13 , 13 41 , 14 51 . Alternatively, one may try pack rectangles sizes60 30, 30 20, 20 15, 15 12 60 60 square, original instancescaled factor 60, least common multiple rectangle denominators.strategy required broad class recent rectangle-packers explore domaininteger x- y-coordinates rectangles quickly break higher N .example, optimal solution N =15 400 billion unique coordinate pairsrectangles assigned to. benchmark complements rather replacescurrent low-precision benchmarks, neglected high-precision instances.remainder section organized follows. first review previouswork proposing solution techniques may unaffected precision rectangledimensions. describe several adaptations low-precision techniqueshigh-precision case, along new techniques developed specifically high-precisionrectangle instances, finally follow experimental results.5.1 Previous Workrelative placement approach Moffitt Pollack (2006) rectangle packing,similar types search spaces used resource-constrained scheduling (Weglarz, 1999),promises immune problem high-precision rectangle instances. However, sincemany techniques described previous sections cannotextended packer working relative placement search space, decidedstay within absolute placement framework attempt mitigate problemsintroduced high-precision numbers.71fiHuang & KorfConsecutive SquaresConsecutive RectanglesSizeNOptimalSolutionsEmptySpaceBoxesTestedOptimalSolutionsEmptySpaceBoxesTested1234567891011121314151617181920212223242526112335575129111114, 722141515201527192723292238234523552854, 2756394631694753348538883998646856884312970890.00%16.7%6.67%14.3%8.33%8.08%9.09%2.86%5.00%4.94%1.36%2.55%2.03%1.93%1.98%1.06%0.50%1.40%0.84%0.69%0.99%0.71%0.64%0.58%0.40%0.47%11111132453658131051412142017191917210.00%0.00%0.00%0.00%0.00%1.75%0.00%0.00%1.79%0.45%0.00%0.95%0.00%0.00%0.00%0.00%0.00%0.00%0.00%0.00%0.20%0.00%0.00%0.00%0.00%0.00%1112222155241212232422345727282930313247148, 749463123811065118691110851350.37%0.45%0.36%0.33%0.33%0.31%22302721303612244558, 410514619121415161621, 142417262226213526353235, 284034403251345730763576, 38703588, 4470, 55563991449240115, 4610040130, 52100, 658045130, 6590, 757842156, 52126, 56117,63104, 7291, 78846311656145, 70116621450.00%0.00%0.00%332Table 8: optimal solutions consecutive-square benchmark, taskpack squares sizes 11, 22, ..., N N , unoriented consecutive-rectanglebenchmark, task pack unoriented rectangles sizes 1 2, 2 3, ...,N (N + 1).72fiOptimal Rectangle Packing: Absolute Placement ApproachOriented Equal PerimeterUnoriented Double PerimeterSizeNOptimalSolutionsEmptySpaceBoxesTestedOptimalSolutionsEmptySpaceBoxesTested123456789101112131415161123344667610811816111611211421132916291930, 1538242923360.00%33.3%16.7%16.7%16.7%6.67%4.55%6.25%6.25%4.76%2.72%3.45%1.94%1.75%2.30%1.45%111142256867771090.00%22.2%8.33%7.41%6.86%5.85%3.08%1.59%1.50%0.69%1.15%1.37%0.71%0.67%0.67%0.83%11228911813812171317213517181920212223244124483242, 245637423551346038611.52%1.04%1.04%0.90%0.78%0.78%0.78%8121211915161133386961791913201821134124302933215938413851, 1711444544564, 3096,4072, 48603988, 526655740.44%0.57%2735Table 9: optimal solutions oriented equal-perimeter rectangle benchmark,task pack oriented rectangles sizes 1 N , 2 (N 1), ..., (N 1) 2,N 1, unoriented double-perimeter rectangle benchmark, taskpack unoriented rectangles sizes 1 (2N 1), 2 (2N 2), ..., (N 1) (N + 1),N N.73fiHuang & Korf(a)(b)Figure 6: Examples mapping solutions one rectangles left-most,bottom-most positions.5.2 Overall StrategyGiven instance high-precision benchmark described rational numbers,multiply values least common multiple denominators get instanceinteger dimensions. apply absolute placement solution techniques,improvements subsequently explain, order find optimal solutions.found, divide x- y-coordinates describing optimal solutions initialscaling constant order obtain optimal solutions original problem.Note map every solution one rectangles slidleft bottom much possible (Chazelle, 1983). example, solutionFigure 6a transformed Figure 6b. Since rectangles proppedleft rectangles, rectangles x-coordinate sumsubset widths rectangles rectangles y-coordinatesum subset heights rectangles. Similarly, width heightbounding box must sum subset widths heights rectangles,respectively.following subsections first explain techniques respect orientedinstances, follow handle unoriented case.5.3 Minimum Bounding Box ProblemSince build initial set bounding boxes pairwise combinations widthsheights within given ranges, space pruned considering bounding boxwidths heights equal subset sums rectangle widths, subset sumsrectangle heights, respectively. Recall Section 4.4 every bounding boxwidth, compute lower bound height. modify roundingresulting bound next subset sum rectangle heights.74fiOptimal Rectangle Packing: Absolute Placement Approach5.3.1 Precomputing Subset Sumscompute set subset sums prior searching. oriented rectanglescannot rotated compute two sets: one based heights rectanglesrepresenting candidate y-coordinates, one based widths representingcandidate x-coordinates. distinction generates fewer subset sums comparedsingle set subset sums generated widths heights.5.3.2 Pruning Combinations Widths Heightsreject bounding boxes certain width height combinationsinfeasible. pruning technique relies observation certain cases, mayone unique set rectangles generate specific width (height) boundingbox.example, consider bounding box width generated uniqueset rectangles. assume heights set rectangles also uniquelydetermine subset sum specific bounding box height. say combinationbounding box width height incompatible. reason set rectanglesway bounding box given width, implies setrectangles must appear solution laid horizontally one another. Thus,set rectangles cannot appear stacked vertically solution. contradictsimplications bounding box given height. Note particular example,compatible height maximum height rectangles.5.3.3 Learning Infeasible AttemptsRecall algorithm solving minimal bounding box problem repeatedly callsalgorithm solve containment problem. Bounding boxes tested ordernon-decreasing area first boxes solutions found. learninfeasible attempts.example, consider pack N rectangles {r1 , r2 , ..., rN }. Note usepre-computed variable order rectangles. Let rd , < N rectangle correspondingdeepest search tree depth-first search able go, entiresearch effort given bounding box. containmnet solver says bounding boxinfeasible, next bounding box height consider nextgreatest subset sum based smaller set {r1 , r2 , ..., rd+1 } instead considering Nrectangles. intuition behind since containment solver failed evenfind arrangement first + 1 rectangles, doesnt make sense involveremaining rectangles {rd+2 , ...rN } next largest subset sum bounding boxheight.method resembles conflict-directed backtracking. implementation, alsoconsider effect pruning using wasted space heuristic well.5.4 Containment ProblemSimilar low-precision methods, first assign x-coordinates rectangles,conduct perfect packing transformation, finally work y-coordinates (Huang75fiHuang & Korf& Korf, 2010). main difference high-precision methods lowprecision methods instead considering possible integers domainx- y-coordinates, consider smaller set subset sums widths heightsrectangles. methods using x-intervals remain unchangeddescribe search individual x-coordinates here.5.4.1 Assigning X-Coordinatesoriented rectangles, choose x-coordinates set subset sums rectanglewidths. Instead precomputing set minimal bounding box problem,generate dynamically every node search prior branchingvarious x-coordinate value assignments. set computed follows:1. Initialize set value 0, represents placing rectangleleft side bounding box.2. every rectangle r already assigned x-coordinate point search,insert set sum x-coordinate width. represents placingrectangle right side r.3. every rectangle x-coordinate still unassigned, add width everyelement set, insert new sums back set.5.4.2 Perfect Packing Transformationassigning x-coordinates, create number 1 1 rectangles accountempty space original instance. transformation results new instance,empty space, consists original rectangles plus new 1 1 rectangles.given empty corner partial solution, ask original unplaced rectanglesmight fit there, 1 1 rectangle, essentially modeling empty corners variablesrectangles values.high-precision benchmark, solving N =15 requires creating 1.5 billion 1 1rectangles scaled problem large number. packersimply requires much memory time. avoid problem creating fewermuch larger rectangles account empty space.Widening Existing Rectangles Assume Figure 7a task pack threerectangles. 10 20, 20 10, 40 10 rectangle 60 50 boundingbox, assume assigned x-coordinates y-coordinates. Givenx-coordinates already assigned, resulting packing solution space right40 10 rectangle must always empty. Thus, replace 40 10 rectangle60 10 rectangle, effectively widening original rectangle. Likewise, replace20 10 rectangle 30 10 rectangle, 10 20 rectangle 30 20 rectangle,Figure 7b. packer greedily attempts widen rectangles towards rightwidening towards left. solving problem returnrectangles back original widths. avoids creating many 1 1 rectanglesperfect packing transformation represent empty space.76fiOptimal Rectangle Packing: Absolute Placement Approach(a) partial solution xcoordinates known.(b) result widening rectangles.Figure 7: Widening existing rectangles.(b) solution without 60 1 rectangles empty space.(a) partial solution xcoordinates known.Figure 8: Consolidating empty space horizontal strips.Turning Empty Space Large Rectangles partial solution Figure 8a,assigned x-coordinates rectangles 60 40 bounding box. Insteadcreating three hundred 1 1 rectangles represent empty space indicatedsingle hash marks, use ten 30 1 rectangles without losing packing solutions.Similarly, represent doubly-hashed empty space twenty 301 rectangles insteadsix hundred 1 1 rectangles. Note cannot use 60 1 rectangles emptyspace since would inadvertently prune potential solution Figure 8b.5.4.3 Assigning Y-Coordinatesperfect packing transformation, assign y-coordinates asking rectangleplaced given empty corner. before, enforce constraint ycoordinate rectangle must subset sum rectangle heights. Noterectangles create via perfect packing transformation included subsetsum calculations, since represent empty space.5.4.4 Handling Unoriented Instancesunoriented instances, computing initial bounding box widths heights,generate single set subset sums using widths heights rectanglesinstance instead keeping widths separated heights. Likewise,generating set candidate x- y-coordinates, must add fourth step77fiHuang & KorfSizeNOptimalSolutionLCMBitsPrecisionHK10BoxesSubsetsBoxesMutexBoxesHK11Boxes1234567891011121314151/211/24/31/219/125/61, 1/25/31/217/101/2107/601/2107/601/2163/901/2163/901/21817/9901/27367/39601/267/361/2185/991/2169/901/279/42261260604208402,5202,52027,72027,720360,360360,360360,360720,72026812121820232330303737373911230201,9794,03339,35713,5712,682,94811257591516931,0837,48931,19666,425289,217549,1351,171,76511247441074657554,90122,82238,827162,507382,059651,0411124729461241925851,6412,3665,0279,54815,334Table 10: minimum-area bounding boxes number bounding boxes testedpacking unoriented rectangles 11 21 , 12 13 , 31 41 , ..., N1 N 1+1 .bulleted list subsection 5.4.1 add height every rectangleyet placed, every element set subset sums, represents possibilityrotating rectangle.5.5 Experimental Resultspresent two different data tables, one relating improvements minimal boundingbox problem measured number bounding boxes tested, another oneoverall CPU time solving entire rectangle-packing problem. separateexperiments way solution schema decouples minimal bounding boxproblem containment problem.5.5.1 Minimum Bounding Box ProblemTable 10 shows optimal solutions unoriented high-precision rectangle benchmarkalong various properties corresponding instances. first two columns giveproblem size dimensions optimal solutions, respectively. third givesleast common multiple first N +1 integers. fourth number bits requiredrepresent area minimal bounding box. Note one optimalsolutions width 12 , since first rectangle much larger others.N =12 larger, required precision exceeds 32-bit integer.fifth eighth columns compare number bounding boxes variouspackers test find optimal solutions unoriented high-precision rectangle bench78fiOptimal Rectangle Packing: Absolute Placement ApproachSizeN6789101112131415HK10TimeEmpty SpaceTimeDynamicTimeHK11Time:00:021:111:51:00:00:00:031:5741:407:30:26:00:00:00:00:02:576:382:20:121:05:56:14:00:00:00:00:01:18:3316:4146:564:28:20Table 11: CPU times various packers find minimum-area bounding boxes containingunoriented rectangles 11 12 , 21 13 , 13 41 , ..., N1 N 1+1 .mark. column going left right, add one new technique minimalbounding box problem.HK10 number bounding boxes required simply scaling probleminstance described completely integers. column called Subsets improves uponsecond testing bounding boxes whose dimensions constrainedsubset sums technique. column called Mutex improves upon third rejectingbounding boxes subset sum corresponding width mutually exclusivesubset sum corresponding height. HK11 improves upon previous packer usinginformation learned infeasible attempt reject future bounding boxes.Using improvements, N =10 test 4,500 times fewer bounding boxes comparedprevious state-of-the-art. instance HK10 ran memory lastbounding box sheer number 1 1 rectangles created perfectpacking transformation. introduction prime number 11 denominatorproblem instance responsible increased difficulty N =9 N =10.5.5.2 Containment ProblemTable 11 compares performance various packers using techniques.decoupled minimal bounding box problem containment problem,table use optimizations minimal bounding box problem,compare individual techniques applied containment problem. Therefore,performance data reported required solve overall problem using variouscontainment problem packers.first column gives size problem instance high-precision rectanglebenchmark. previous tables, successive column left right improves uponprevious column additional technique. column called HK10 correspondsusing previous state-of-the-art improved minimal bounding box algorithm.column called Empty Space improves upon HK10 precomputing subset79fiHuang & Korfsums prior searching x-coordinates, uses techniques consolidate emptyspace y-coordinates. column called Dynamic improves upon previous onedynamically computing subset sums. Finally, last column called HK11 addsability learn unplaced rectangles exclude subset sums computationexploring infeasible subtree. data collected using Linux eight core 3GHzIntel Xeon X5460 without parallelization.N =10, problem scaled 27,720 times dimensions, requiring HK10create 6,597,361 1 1 units empty space perfect packing transformationcausing run memory. Empty Space could complete N =13 withinday sheer number subset sums must explored x-y-coordinates, problem avoided Dynamic.5.5.3 Comparison Relative Placementinteresting note number bounding boxes appears increasing exponentially, mostly likely due exponential growth number subset sumsintroduced successive rectangle high-precision benchmark. difficultyunoriented high-precision rectangle benchmark compounded factprecision increases, branching factor single x- y-coordinate valuescontainment problem also increases.contrast absolute placement technique, Moffitt Pollacks (2006) relativeplacement techniques enumerate different exact locations rectangles,therefore promise immune problem high-precision rectangles. usedvariable every pair rectangles represent relations above, below, left, right.search algorithm required least one non-overlapping constraintstrue every pair rectangles. meta-CSP approach modeled workDechter, Meiri, Pearl (1991) solving binary constraint satisfaction problems,included various pruning techniques model reduction, symmetry breaking,graph-based pruning heuristics (Korf et al., 2010). solve minimum bounding boxproblem branch-and-bound algorithm, evaluating size bounding boxnon-overlapping relationships determined, keeping track boundingbox smallest area seen far.Note contrast, solver tests bounding boxes order non-decreasing area.Also, size formulation uses N 2 variables use N . Finally,packer returns one optimal solution opposed ours, workreturning optimal solutions.able benchmark code machine order providekind comparison methods ours. crude comparison,cannot run packer unoriented high-precision rectangle benchmark sincehard-coded packer unoriented consecutive-rectangle benchmark, mucheasier benchmark shown Table 7.first column Table 12 refers problem size. second column called MP06gives CPU time required Moffitt Pollacks code problem instancesunoriented consecutive-rectangle benchmark, uses low-precision rectangles. thirdcolumn called HK11 gives CPU time required packer problem instances80fiOptimal Rectangle Packing: Absolute Placement ApproachSizeNMP06TimeHK11Time101112131415:03:132:2617:401:48:097:27:42:01:18:3316:4146:564:28:20Table 12: CPU times required Moffitt Pollacks packer unoriented consecutiverectangle packer unoriented high-precision rectangle benchmarks.unoriented high-precision rectangle benchmark. data point tablecollected using eight core 3GHz Intel Xeon X5460 Linux without parallelization. Notealgorithm packs number rectangles somewhat faster MoffitPollacks.5.6 Summary High-Precision Rectanglessection proposed new benchmark consisting instances rectangleshigh-precision dimensions well techniques using subset sums limit numberpositions must considered, rules filter subset sums minimalbounding box containment problems, methods learn infeasible subtrees,ways reduce number rectangles created perfect packing transformation.techniques exploit special properties benchmark, usefulrectangles high-precision dimensions.Using methods, solved six problems N =15 new benchmark compared using low-precision packer scaled instance. packertwo orders magnitude faster N =9 previous state-of-the-art, tests4,500 times fewer bounding boxes. cursory comparison state-of-the-art usingrelative placement search space shows perform slightly fasterMoffitt Pollacks packer, benchmark previously shown Section4.3.2 significantly difficult unoriented consecutive-rectangle benchmarkMoffitt Pollacks program run on.6. Future WorkHumans solve jigsaw puzzles asking particular piece go, wellasking piece go empty region. packer makes usemodels, former x-coordinates latter y-coordinates. wouldinteresting see applicable dual formulation packing, layout,scheduling problems. Currently, work x-coordinates askinggo?, work y-coordinates asking goes location?method reduced time spent y-coordinates much time spent81fiHuang & Korfworking x-coordinates orders magnitude greater time spent workingy-coordinates. suggests performance might improved consideringmodels simultaneously.another direction continued work, data indicates number boundingboxes explored minimum bounding boxes solver main bottleneck solvinglarger instances unoriented high-precision rectangle benchmark. observationmake across many bounding boxes, partial solutionsexplored, resulting much redundant computation. Consequently, branch-and-boundmethod starts large bounding box, gradually reduces dimensionsvarious packings explored would promising avenue research.7. Conclusionspresented several new improvements previous state-of-the-art optimalrectangle packing. Within schema assigning x-coordinates prior y-coordinates,introduced dynamic variable order x-coordinates, constraint adaptsKorfs (2003) wasted space pruning heuristic one-dimensional case. ycoordinates work perfect packing transformation original problem,using model assigns rectangles empty corners, inference rules reducemodels variables.improvements search y-coordinates helped us solve N =27 consecutive-square benchmark order magnitude faster previous state-ofthe-art, improvements search x-coordinates also gave us another ordermagnitude speedup N =28, compared leaving optimizations out.techniques, 19 times faster previous state-of-the-art largestproblem solved date, allowing us extend known solutions consecutive-squarebenchmark N =27 N =32. Furthermore, data show little time spentsearching y-coordinates, suggesting rectangle packing may largely reducedproblem determining x-coordinates.techniques presented pick y-coordinates tightly coupled dualview asking must go empty location. Furthermore, searching xcoordinates, pruning rule based analysis irregular regions empty space,dynamic variable order also rests observation less empty space leadsconstrained problem. success techniques rectangle packing makeworth exploring many packing, layout, scheduling problems.also introduced two new benchmarks, one oriented one unoriented,include rectangles various aspect ratios. new benchmarks avoid various propertieseasy instances, identified, shown much harderside-by-side comparison various benchmarks using state-of-the-art packer.also proposed several search strategies improve performance new benchmarks. improved upon strategies used handle dominance conditions, proposedvariable ordering heuristic based increasing branching factor generalizes previousstrategies, tuned global interval parameter, introduced method balance sizesintervals assigned x-coordinate variables.82fiOptimal Rectangle Packing: Absolute Placement Approachexperiments revealed takes orders magnitude time solve newbenchmarks compared instances consecutive-square benchmarknumber rectangles. therefore advocate inclusion new, difficultbenchmarks suite benchmarks used research optimal rectangle packing. Finally, using techniques together, solved N =21 oriented equal-perimeterbenchmark 500 times faster, N =16 unoriented double-perimeter benchmark 40 times faster simply using methods tuned consecutive-squares.order test limits rectangle packer, presented new high-precision benchmark specifically capturing pathological case successive rectanglequickly increases precision required represent coordinate locations. presentedvarious techniques adapt absolute placement approach handle types instances, including dynamically using subset sums limit number coordinate valuesmust tested, mutex reasoning allows us reject certain combinations subsetsums used bounding boxs width height, general method rejecting future subset sums based previously infeasible search, finally memory-efficient adaptationperfect packing transformation high-precision rectangle instances.solved N =12 high-precision benchmark half minute, 800 times fasterbasic version packer augmented high-precision version perfectpacking inference rules run memory. also first instancerequiring precision exceeding capacity 32-bit integer. techniques allowed ussolve N =15 compared N =9, largest instance low-precision techniquesalone could solve. methods also reduced number bounding boxes generatedfactor 4,500. point solving problems require minimum 39 bitsprecision, meet requirements many real-world problems.provided comparison state-of-the-art relative placement packer showingabsolute-placement packer remains competitive even rectangles high-precision, reported promising avenues research may potentially give absoluteplacement approach clear competitive edge relative-placement methods.Although mainly focused obtaining optimal solutions benchmarks,work may easily adapted applications requiring quick suboptimal solutionssimply replacing algorithm minimum bounding box problem alternativesanytime algorithm described Section 3.3.1.7.1 Comparison Constraint Programming Methodologiesclearly tradeoffs taking ground-up programming approach C++taking constraint programming approach. latter provides quick prototypingreuse constraint libraries researchers already implemented, alsoforces problem expressed abstract constraint language. abstractlayer turns add unnecessary overhead algorithms data structuresone naturally uses solve problem optimal rectangle packing.example, previously described, cumulative constraint, simply addconstant consecutive range integer array assign x-coordinaterectangle. backtrack, scan array subtract constant.Scanning manipulating arrays, iteration, fast pushing popping program83fiHuang & Korfstack recursive algorithms precisely operations modern computer hardwareoptimized for. significant explore two trillion search nodesN =32 square-packing benchmark, fact solver spends 75%time array manipulation operations alone. explainorders magnitude speedup processing x-coordinate solutions 1D arrayinstead 2D bitmap Korf (2003). move 1D arrays, 2D bitmaps,abstract representations variables values constraint programming, patternscomputation data structures simply become distant underlyinghardware optimized for.optimal rectangle packing, happens algorithms data structuresnaturally solve problem map nicely form function hardware moderncomputers. Note one may always port code constraint module maycalled constraint solver, still computational indirectionmodule backtracking control logic constraint solver. sacrifice makeapproach, however, fact solver tailored specifically rectanglepacking problem defined it, would require implementation effortreconfigure algorithms heuristics slightly different rectangle packing problem.hope, however, latter problem ameliorated disciplined object-oriented,modular software design.8. Broader LessonsBeyond specific problem rectangle packing, broader lessons learnwork? believe several.One main applications rectangle packing scheduling. describedintroduction, rectangle packing problem abstraction scheduling problemdifferent tasks take different amounts time, require different amounts onedimensional resource must allocated contiguously, memory computer.width bounding box becomes total time, height total amountresource available, job becomes rectangle width equal time duration,height equal amount resource required.found, however, vast majority time used rectangle packerassigning x-coordinates rectangles, subject cumulative constraint,every x-coordinate bounding box, sum heights rectangles overlap x-coordinate cannot exceed height bounding box.important subpart rectangle-packing problem models much general problemknown resource-constrained scheduling problem. schedulingproblem described above, without constraint resource allocated contiguously. example, scheduling tasks planetary rover limited power budget,sum power requirements tasks active given time cannotexceed total power budget rover. Thus, subpart rectangle packerused tackle general scheduling problem.Another general lesson learned work absolute placementapproach various packing problems two, three, dimensions may effectiveeven problems high precision dimensions. One might expect absolute placement84fiOptimal Rectangle Packing: Absolute Placement Approachwould competitive relative placement approaches problems,key success area instead considering possible placements,consider placements correspond subset sums relevant dimensions.guarantee approach work high-precision packing problems,shown least worth considering, may effective.Perhaps largest lesson learned encouraging discouraging.problem rectangle packing extremely simple, understood playedgame children. Yet research last decade described showsefficient algorithms quite complex. best algorithms simpleproblem complex, likely best algorithms complex problemseven complex, discouraging part. encouraging parthistory research shown new idea result order magnitudeimprovement previous state art larger problems, suggestingstill significant progress made problem, extension others like it.Acknowledgmentswish thank Reza Ahmadi, Adnan Darwiche, Adam Meyerson advicework. also thank Michael Moffitt making packer available. research funded part National Science Foundation grant number IIS0713178. source code optimal rectangle packer open sourced availablehttp://code.google.com/p/rectpack.ReferencesAggoun, A., & Beldiceanu, N. (1993). Extending chip order solve complex schedulingplacement problems. Mathematical Computer Modelling, 17 (7), 5773.Beldiceanu, N., & Carlsson, M. (2001). Sweep generic pruning technique appliednon-overlapping rectangles constraint. CP 01: Proceedings 7th InternationalConference Principles Practice Constraint Programming, pp. 377391, London, UK. Springer-Verlag.Beldiceanu, N., Carlsson, M., & Poder, E. (2008). New filtering cumulative constraintcontext non-overlapping rectangles. Perron, L., & Trick, M. A. (Eds.),CPAIOR, Vol. 5015 Lecture Notes Computer Science, pp. 2135. Springer.Beldiceanu, N., Carlsson, M., Poder, E., Sadek, R., & Truchet, C. (2007). generic geometrical constraint kernel space time handling polymorphic k-dimensionalobjects. Bessiere, C. (Ed.), CP, Vol. 4741 Lecture Notes Computer Science,pp. 180194. Springer.Bhattacharya, S., Roy, R., & Bhattacharya, S. (1998). exact depth-first algorithmpallet loading problem. European Journal Operational Research, 110 (3), 610625.Chan, H. H., & Markov, I. L. (2004). Practical slicing non-slicing block-packing withoutsimulated annealing. GLSVLSI 04: Proceedings 14th ACM Great Lakessymposium VLSI, pp. 282287, New York, NY, USA. ACM.85fiHuang & KorfChazelle, B. (1983). bottomn-left bin-packing heuristic: efficient implementation.IEEE Transactions Computers, C-32 (8), 697707.Clautiaux, F., Carlier, J., & Moukrim, A. (2007). new exact method twodimensional orthogonal packing problem. European Journal Operational Research,183 (3), 11961211.Dechter, R., Meiri, I., & Pearl, J. (1991). Temporal constraint networks. Artificial Intelligence, 49 (1-3), 6195.Dowsland, K. A. (1987). exact algorithm pallet loading problem. EuropeanJournal Operational Research, 31 (1), 7884.Dowsland, K. A., & Dowsland, W. B. (1992). Packing problems. European JournalOperational Research, 56 (1), 214.Huang, E., & Korf, R. E. (2009). New improvements optimal rectangle packing.Boutilier, C. (Ed.), IJCAI, pp. 511516.Huang, E., & Korf, R. E. (2010). Optimal rectangle packing non-square benchmarks.AAAI10: Proceedings 24th National Conference Artificial intelligence, pp.317324. AAAI Press.Huang, E., & Korf, R. E. (2011). Optimal packing high-precision rectangles. Burgard,W., & Roth, D. (Eds.), AAAI. AAAI Press.Korf, R. E. (2003). Optimal rectangle packing: Initial results. Giunchiglia, E., Muscettola,N., & Nau, D. S. (Eds.), ICAPS, pp. 287295. AAAI.Korf, R. E. (2004). Optimal rectangle packing: New results. Zilberstein, S., Koehler, J.,& Koenig, S. (Eds.), ICAPS, pp. 142149. AAAI.Korf, R. E., Moffitt, M. D., & Pollack, M. E. (2010). Optimal rectangle packing. AnnalsOperations Research, 179 (1), 261295.Lahrichi, A. (1982). Scheduling: notions hump, compulsory parts usecumulative problems. Comptes Rendus de Academie des Sciences, Paris, 294, 209211.Lesh, N., Marks, J., McMahon, A., & Mitzenmacher, M. (2004). Exhaustive approaches2d rectangular perfect packings. Information Processing Letters, 90 (1), 714.Li, S. C., Leong, H. W., & Quek, S. K. (2004). New approximation algorithms dynamic storage allocation problems. Chwa, K.-Y., & Munro, J. I. (Eds.), COCOON,Vol. 3106 Lecture Notes Computer Science, pp. 339348. Springer.Lipovetskii, A. I. (2008). geometrical approach computation optimal solutionrectangle packing problem. American Mathematical Society Translations, 222,93110.Lodi, A., Martello, S., & Monaci, M. (2002). Two-dimensional packing problems: survey.European Journal Operational Research, 141 (2), 241252.Lodi, A., Martello, S., & Vigo, D. (2002). Recent advances two-dimensional bin packingproblems. Discrete Applied Mathematics, 123 (1-3), 379396.Martello, S., & Vigo, D. (1998). Exact solution two-dimensional finite bin packingproblem. Management Science, 44 (3), 388399.86fiOptimal Rectangle Packing: Absolute Placement ApproachMeir, A., & Moser, L. (1968). packing squares cubes. Journal CombinatorialTheory, 5 (2), 126134.Mitola, J., & Maguire, G. (1999). Cognitive radio: making software radios personal.IEEE Personal Communications Magazine, 6 (4), 1318.Moffitt, M. D., & Pollack, M. E. (2006). Optimal rectangle packing: meta-csp approach.Long, D., Smith, S. F., Borrajo, D., & McCluskey, L. (Eds.), ICAPS, pp. 93102.AAAI.Onodera, H., Taniguchi, Y., & Tamaru, K. (1991). Branch-and-bound placement buildingblock layout. DAC 91: Proceedings 28th ACM/IEEE Design AutomationConference, pp. 433439, New York, NY, USA. ACM.Simonis, H., & OSullivan, B. (2008). Search strategies rectangle packing. Stuckey,P. J. (Ed.), CP, Vol. 5202 Lecture Notes Computer Science, pp. 5266. Springer.Simonis, H., & OSullivan, B. (2011). Almost square packing. Achterberg, T., & Beck,J. C. (Eds.), CPAIOR, Vol. 6697 Lecture Notes Computer Science, pp. 196209.Springer.Sweeney, P. E., & Paternoster, E. R. (1992). Cutting packing problems: categorized,application-orientated research bibliography. Journal Operational ResearchSociety, 43 (7), 691706.Weglarz, J. (1999). Project scheduling: recent models, algorithms applications. Springer,Kluwer.Yap, R. H. C. (2004). Constraint processing rina dechter, morgan kaufmann publishers,2003, hard cover: Isbn 1-55860-890-7, xx + 481 pages. Theory Pract. Log. Program.,4 (5-6), 755757.87fi