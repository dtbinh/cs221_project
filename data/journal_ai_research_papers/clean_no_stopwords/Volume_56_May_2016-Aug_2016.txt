Journal Artificial Intelligence Research 56 (2016) 517-545Submitted 4/16; published 7/16Time-Sensitive Bayesian Information AggregationCrowdsourcing SystemsMatteo Venanzimavena@microsoft.comMicrosoft, 2 Waterhouse Square,London EC1N 2ST UKJohn Guiverjoguiver@microsoft.comMicrosoft Research, 21 Station Road,Cambridge CB1 2FB UKPushmeet Kohlipkohli@microsoft.comMicrosoft Research, One Microsoft Way,Redmond WA 98052-6399 USNicholas R. Jenningsn.jennings@imperial.ac.ukImperial College, South Kensington,London SW7 2AZ UKAbstractMany aspects design efficient crowdsourcing processes, defining workersbonuses, fair prices time limits tasks, involve knowledge likely durationtask hand. work introduce new timesensitive Bayesian aggregationmethod simultaneously estimates tasks duration obtains reliable aggregationscrowdsourced judgments. method, called BCCTime, uses latent variables representuncertainty workers completion time, tasks duration workersaccuracy. relate quality judgment time worker spends task,model assumes task completed within latent time window withinworkers propensity genuinely attempt labelling task (i.e., spammers)expected submit judgments. contrast, workers lower propensityvalid labelling, spammers, bots lazy labellers, assumed perform tasksconsiderably faster slower time required normal workers. Specifically, useefficient message-passing Bayesian inference learn approximate posterior probabilities(i) confusion matrix worker, (ii) propensity valid labelling worker,(iii) unbiased duration task (iv) true label task. Using two realworld public datasets entity linking tasks, show BCCTime produces11% accurate classifications 100% informative estimates tasksduration compared stateoftheart methods.1. IntroductionCrowdsourcing emerged effective way acquire large amounts data enablesdevelopment variety applications driven machine learning, human computation participatory sensing systems (Kamar, Hacker, & Horvitz, 2012; Bernstein, Little,Miller, Hartmann, Ackerman, Karger, Crowell, & Panovich, 2010; Zilli, Parson, Merrett,c2016AI Access Foundation. rights reserved.fiVenanzi, Guiver, Kohli & Jennings& Rogers, 2014). Services Amazon Mechanical Turk1 (AMT), oDesk2 CrowdFlower3 enabled number applications hire pools human workers providedata serve training image annotation (Whitehill, Ruvolo, Wu, Bergsma, & Movellan,2009; Welinder, Branson, Belongie, & Perona, 2010), galaxy classification4 (Kamar et al.,2012) information retrieval systems (Alonso, Rose, & Stewart, 2008). applications, central problem deal diversity accuracy speed workersexhibit performing crowdsourcing tasks. result, due uncertaintyreliability individual crowd responses, many systems collect many judgments different workers achieve high confidence quality labels. However, incurhigh cost either time money, particularly workers paid per judgment,delay completion entire crowdsourcing project introducedworkers intentionally delay submissions follow work schedule. example, typical crowdsourcing scenario, requester must specify number requestedassignments (i.e., individual responses different workers), well time limitcompletion assignment. must also set price paid response5 ,usually includes participation fee bonus based quality submissionactual effort required task. However, nontrivial problem set timelimit gives workers sufficient time perform task correctly without leadingtask starvation (i.e., one working task assigned). Generally speaking,knowledge actual duration assignment (task instance) usefulrequesters various reasons. First, tasks duration used proxy estimatedifficulty, difficult tasks usually take longer complete (Faradani, Hartmann,& Ipeirotis, 2011). Second, information useful set time limit taskreduce overall time task completion. Third, task requestor use taskduration pay fair bonuses workers based difficulty task complete.seeking estimate information, however, important considerworkers might perform task immediately might delay submissionsaccepting task or, extreme, might submit poor annotation rapidtime (Kazai, 2011). result, common heuristic estimates tasks duration (suchworkers average median completion time) account aspectslikely inaccurate.Given above, number challenges addressed various stepsdesigning efficient crowdsourcing workflows. First, judgmentscollected, uncertainty unknown reliability individual workers musttaken account compute final labels. aggregated labels often estimatedsettings true answer task never revealed, quantitycrowdsourcing process trying discover (Kamar et al., 2012). Second,estimating tasks duration, uncertainty completion time derivingprivate work schedule worker must taken account (Huff & Tingley, 2015).1.2.3.4.5.www.mturk.comwww.odesk.comwww.crowdflower.comwww.galaxyzoo.orgcommon guideline task requesters consider $0.10 per minute minimum wageethical crowdsourcing experiments (www.wearedynamo.org).518fiTime-Sensitive Bayesian Information Aggregation Crowdsourcing SystemsThird, two challenges must addressed simultaneously due interdependenciesworkers reliability, time required complete task, final labelsestimated tasks.attempt address challenges, growing interest developingalgorithms techniques compute accurate labels minimising set of, possiblyunreliable, crowd judgements (Sheng, Provost, & Ipeirotis, 2008). detail, simple solutions typically use heuristic methods majority voting weighted majority voting(Tran-Thanh, Venanzi, Rogers, & Jennings, 2013). However, methods considerreliability different workers treat judgments equally reliable.sophisticated methods onecoin model (Karger, Oh, & Shah, 2011), GLAD(Whitehill et al., 2009), CUBAM (Welinder et al., 2010), DS (Dawid & Skene, 1979)Bayesian Classifier Combination (BCC) (Kim & Ghahramani, 2012) use probabilisticmodels take reliabilities account, potential labelling biases workers, e.g., tendency worker consistently underrate items. particularDS represents workers skills based confusion matrix expressing reliabilityworker possible class objects. BCC works similarly DS, also considers uncertainty confusion matrices aggregated labels using principledBayesian learning framework. representational power enabled BCC successfully applied number crowdsourcing applications including galaxy classification(Simpson, Roberts, Psorakis, & Smith, 2013), disaster response (Ramchurn, Huynh, Ikuno,Flann, Wu, Moreau, Jennings, Fischer, Jiang, Rodden, et al., 2015) sentiment analysis(Simpson, Venanzi, Reece, Kohli, Guiver, Roberts, & Jennings, 2015). recently, Venanzi, Guiver, Kazai, Kohli, Shokouhi (2014) proposed communitybased extensionBCC (i.e., CBCC) improve predictions leveraging groups workers similar confusion matrices. Similarly, Simpson et al. combined BCC language modellingtechniques automated text sentiment analysis using crowd judgments. degreeapplicability performance BCC-based methods promising point departuredeveloping new data aggregation methods crowdsourcing systems. However, noneexisting methods reason workers completion time learn durationtask outsourced crowd. Moreover, methods learn probabilistic models information contained judgment set. Unfortunately,strategy challenged datasets arbitrarily sparse, i.e., workers provide judgments small sub-set tasks, therefore judgments provide weakevidence accuracy worker. contexts, hypothesis wider setfeatures must leveraged learn reliable crowdsourcing models. work,focus time takes worker complete task considered key indicatorquality work. Importantly, information workers completion timemade available popular crowdsourcing platforms including AMT, Microsoft Universal Human Relevance System (UHRS) CrowdFlower. Therefore, seekefficiently combine features data aggregation algorithm naturallyintegrated output data produced platforms. detail, presentnovel timesensitive data aggregation method simultaneously estimates tasksduration obtains reliable aggregations crowdsourced judgments. characteristictimesensitivity method relates ability jointly reason workerscompletion time together judgments data aggregation process. detail,519fiVenanzi, Guiver, Kohli & Jenningsmethod extension BCC, term BCCTime. Specifically, incorporatesnewly developed time model enables method leverage observations timespent worker task best inform inference final labels. BCC,use confusion matrices represent labelling accuracy individual workers.model granularity workers time profiles, use latent variables representpropensity worker submit valid judgments. Further, model uncertaintyduration task, use latent thresholds define time interval withintask expected completed workers high propensity valid labelling. Then, using Bayesian message-passing inference, method simultaneously infersposterior probabilities (i) confusion matrix worker, (ii) propensityvalid labelling worker, (iii) true label task (iv) upperlower bound duration task. particular, latter represents reliableestimate likely duration task obtained automatically filteringcontributions workers low propensity valid labelling. demonstrateefficacy method using two commonlyused public datasets relate important Natural Language Processing (NLP) application crowdsourcing entity linking tasks.datasets, method achieves 11% accurate classifications comparedseven state-of-the-art methods. Further, show tasks duration estimates100% informative common heuristics consider workerscompletion time correlated quality judgments.background, make following contributions state art.analysis two real-world datasets crowdsourcing entity-linking tasks,show existence different types taskspecific qualitytime trends, e.g.,increasing, decreasing invariant trends, quality judgmentstime spent workers produce them. also re-confirm existing resultsshowing workers submit judgments quickly slowlyentire task set typically provide lower quality judgments.develop BCCTime: first time-sensitive Bayesian aggregation model leverages observations workers completion time simultaneously aggregate crowdjudgments infer duration task well reliability worker.show BCCTime outperforms seven competitive stateoftheartdata aggregation methods crowdsourcing, including BCC, CBCC, one coinmajority voting, providing 11% accurate classifications 100%informative estimates tasks duration.rest paper unfolds follows. Section 2 describes notation preliminaries Bayesian aggregation crowd judgments. Section 3 details time analysisreal-world datasets. Then, Section 4 formally introduces BCCTime details probabilistic inference. Section 5 presents evaluation state art. Section 6summarises rest related work areas data aggregation time analysiscrowd generated content Section 7 concludes.520fiTime-Sensitive Bayesian Information Aggregation Crowdsourcing SystemsTable 1: List symbols.SymbolNKCJti(k)ci(k)(k)pkvik000000s0p0(k)0DefinitionNumber tasksNumber workersNumber true label valuesSet observed workers completion timeSet observed judgmentsTrue label taskVector tiJudgment k taskTime spent k judging taskConfusion matrix kClass proportions tasksPropensity k making valid labelling attemptsLabelling probabilities general low-propensity workerVector k , k = 1, . . . , K(k)Boolean variable signalling ci valid labelling attemptLower-bound threshold duration taskUpper-bound threshold duration taskMean Gaussian priorPrecision hyperparameter Gaussian priorMean hyperparameter Gaussian priorPrecision hyperparameter Gaussian priorTrue count hyperparameter Beta prior kFalse count hyperparameter Beta prior kHyperparameter Dirichlet priorHyperparameter Dirichlet prior pHyperparameter Dirichlet prior (k)2. PreliminariesConsider crowd K workers labelling N objects C possible classes symbols(k)listed Table 1. Assume k submits judgment ci {1, . . . , C} classifying(k)object i. Let ti unobserved true label i. Then, suppose R+(k)(k)time taken k produce ci . Let J = {ci |i = 1, . . . , N, k = 1, . . . , K}(k)= {i |i = 1, . . . , N, k = 1, . . . , K} set containing judgmentstime spent workers, respectively.introduce key features BCC model relevant method.First introduced Kim Ghahramani (2012), BCC method combines multiplejudgments produced independent classifiers (i.e., crowd workers) unknown accuracy. Specifically, model assumes that, task i, ti drawn categoricaldistribution parameters p:ti |p Cat(ti |p)521(1)fiVenanzi, Guiver, Kohli & Jenningsp denotes class proportions objects. Then, workers accuracyrepresented confusion matrix (k) comprising labelling probabilities k(k)(k)(k)possible true label value. Specifically, row matrix c = {c,1 , . . . , c,C }(k)vector c,j probability k producing judgment j object classc. Importantly, confusion matrix expresses accuracy (diagonal values)biases (off-diagonal values) worker. recognise workers particularly accurate (inaccurate) bias specific class objects. fact, accurate (inaccurate)workers represented high (low) probabilities diagonal confusionmatrix, whilst workers bias towards particular class high probabilitiescorresponding column matrix. example, galaxy zoo domainworkers classify images celestial galaxies, confusion matrices detect workerslow accuracy classifying spiral galaxies systematically classifyevery object elliptical galaxies (Simpson et al., 2013).relate workers confusion matrix quality judgment, BCC assumes(k)ci drawn categorical distribution parameters corresponding ti -th row(k) :(k)(k)(k)ci | (k) , ti Cat ci | ti(2)(k)equivalent categorical mixture model ci ti mixtureparameter c parameter c-th categorical component. Then, assumingjudgments independent identically distributed (i.i.d.), joint likelihoodexpressed as:p(C, t|, p) =NCat(ti |p)i=1K(k)(k)Cat ci | tik=1Using conjugate Dirichlet prior distributions parameters p applyingBayes rule, joint posterior distribution derived as:p(, p|C, t) Dir(p|p0 )N nCat(ti |p)i=1K(k)(k)Cat ci | ti(k) (k)Dir( ti | ti ,0 )(3)k=1expression, possible derive predictive posterior distributionsunobserved (latent) variable using standard integration rules Bayesian inference (Bishop,2006). Unfortunately, exact derivation posterior distributions intractableBCC due non-conjugate form model (Kim & Ghahramani, 2012). However,shown that, particularly BCC models, possible compute efficient approximations distributions using standard techniques Gibbs sampling (KimGhahramani), variational Bayes (Simpson, 2014) Expectation-Propagation (Venanziet al., 2014). Building this, several extensions BCC proposed various522fiTime-Sensitive Bayesian Information Aggregation Crowdsourcing Systemscrowdsourcing domains (Venanzi et al., 2014; Simpson et al., 2015, 2013). particular,CBCC applies communitybased techniques represent groups workers similarconfusion matrices classifier combination process (Venanzi et al.). mechanismenables model transfer learning workers reliability communitiesimprove quality inference.However, drawback BCC based models learn tasksduration consider extra features workers judgments.result, perform full learning confusion matrices task labels usingjudgments produced workers. But, mentioned earlier, strategy challengedsparse datasets worker labels tasks. case, instance,Crowdflower dataset used 2013 CrowdScale Shared Task challenge6sentiment 98,980 tweets classified 1,960 workers five sentiment classes.dataset, 30% workers judged 15 tweets, i.e., 0.015% total samples,long tail workers less 3 judgments.3. Analysis Workers Time Spent Judgmentsdiscussed basic concepts non-time based data aggregation, turnanalysis relationship time workers spend taskquality judgments produce. contrast previous works area (Demartini, Difallah, & Cudre-Mauroux, 2012; Wang, Faridani, & Ipeirotis, 2011), extendanalysis qualitytime responses specific task instances, well entiretask set. doing, provide key insights inform design timesensitiveaggregation model. end, consider two public datasets generated widelyused NLP application crowdsourcing entity linking tasks.3.1 DatasetsZenCrowd - India (ZC-IN): contains set links names entitiesextracted news articles uniform resource identifiers (URIs) describing entityFreebase7 DBpedia8 (Demartini et al., 2012). dataset collected using AMT,worker asked classify whether single URI either irrelevant (0)relevant (1) single entity. contains timestamps acceptancesubmission judgment. Moreover, gold standard labels collected experteditors tasks. information released regarding restrictionsworker pool, although workers known living India, workerpaid $0.01 per judgment. total 11,205 judgements collected small pool25 workers, giving dataset moderately high number judgements per worker,detailed Table 2. particular, Figure 1a shows vast majority tasks receive5 judgements, Figure 1c shows skewed distribution gold labels, 78%links entities URIs classified workers irrelevant (0). such,worth noting binary classifiers bias towards unrelated classificationcorrectly classify majority tasks thus receive high accuracy. Therefore,6. www.crowdscale.org/shared-task7. www.freebase.com8. www.dbpedia.org523fiVenanzi, Guiver, Kohli & JenningsTable 2: Crowdsourcing datasets entity linking tasks.Dataset:JudgementsWorkersTasksLabels11205121906000257411020402040300225ZC-INZC-USWS-AMTJudgementaccuracy0.6780.7700.704180010001400800# tasks# tasks1200100080060060040040020020023457910# judgements1415190202 3 4 5 6 7 8 9 101112131516192225# judgements(a) ZC -(b) ZC - US160010014008012001000# tasks# tasksJudgementsper worker448.200164.73054.545120016000Judgementsper task5.4935.97520.000800600400604020200000Gold label101234Gold label(c) ZC(d) WS - AMTFigure 1: Histograms number judgments per task ZC-IN (a) ZC-US (b)WS - AMT shown tasks received exactly 20 judgments numbertasks per gold label ZC (c) WS - AMT (d).detail Section 5, important select accuracy metrics evaluate classifieracross whole spectrum possible discriminant thresholds.ZenCrowd - USA (ZC-US): dataset also provided Demartini et al. (2012)contains judgements set tasks ZC-IN, although judgementscollected AMT workers US. payment $0.01 per judgementused. However, larger pool 74 workers involved, lower numberjudgements collected worker, shown Table 2. Furthermore, Figure 1bshows similar distribution judgements per task India dataset, although slightlyfewer tasks received 5 judgements, remaining tasks receiving 3-4 judgements524fiTime-Sensitive Bayesian Information Aggregation Crowdsourcing Systems9-11 judgements. judgement accuracy US dataset higher Indiadataset despite identical crowdsourcing system reward mechanism used.Weather Sentiment - AMT (WS-AMT): Weather Sentiment dataset provided CrowdFlower 2013 Crowdsourcing Scale shared task challenge.9includes 300 tweets 1,720 judgements 461 workers used severalexperimental evaluations crowdsourcing models (Simpson et al., 2015; Venanzi et al.,2014; Venanzi, Teacy, Rogers, & Jennings, 2015b). detail, workers askedclassify sentiment tweets respect weather following categories:negative (0), neutral (1), positive (2), tweet related weather (3) cant tell (4).result, dataset pertains multi-class classification problem. However, originaldataset used Share task challenge contain time informationcollected judgments. Therefore, new dataset (WS-AMT), recollectedtasks CrowdFlower shared task dataset using AMT platform, acquiring exactly 20 judgements recording elapsed time judgment (Venanzi, Rogers,& Jennings, 2015a). result, WS-AMT contains 6,000 judgements 110 workers,shown Table 2. restrictions placed worker pool workerpaid $0.03 per judgement. Furthermore, Figure 1d shows that, per original dataset,common gold label unrelated, five tasks assigned gold labelcant tell.3.2 Time Spent Task versus Judgment Accuracywish analyse distribution workers completion time judgmentsaccuracy. so, focus two datasets, ZC-US ZC-IN binary labels.fact, binary nature two datasets allow us analyse accuracy higher leveldetail, i.e., terms precision recall workers judgments time spentproduce them. Specifically, Figure 2 shows cumulative distribution precisionrecall set judgments selected specific time threshold (x-axis) respectgold standard labels. Here, precision fraction true positive classificationsreturned positive classifications (true positives + false positives) recallnumber true positive classifications divided number positive samples.Similarly Demartini et al. (2012), find accuracy lower extremestime distributions. ZC-US, precision recall higher sub-setjudgments produced 80 seconds less 1500 seconds.ZC-IN, precision recall higher judgments produced 80 secondsless 600 seconds.addition, Figure 3 shows distribution recall execution time sampleset six positive task instances (i.e., entities positive gold standard labels)least ten judgments. example, Figure 3b shows time distribution judgmentsURI: freebase.com/united states associated entity American.graphs, samples increasing quality-time curve, i.e., workers spendingtime produce better judgments, (Figure 3a Figure 3b). samples decreasingquality-time curve, i.e., workers spending time produce worse judgments (Figure 3cFigure 3d). Finally, last two samples approximately constant quality-time9. www.kaggle.com/c/crowdflower-weather-twitter525fi0.90.80.80.70.70.60.6RecallPrecisionVenanzi, Guiver, Kohli & Jennings0.50.40.50.40.30.30.20.20.10.10580620320015205806203201520Time spent(sec)Time spent (sec)(b) ZC - US(a) ZC - US0.50.70.60.4RecallPrecision0.50.30.20.40.30.20.100.1585350Time spent (sec)60001400585350Time spent (sec)6001400(d) ZC -(c) ZC -Figure 2: Histograms precision recall binned time spent US workers(a, b) Indian workers (c, d) ZenCrowd datasets.curve, i.e., workers quality invariant time spent (Figure 3e Figure 3f).also seen trends naturally correlate difficulty task instance.instance, URI: freebase.com/m/03hkhgs linked entity Southern Avenuedifficult judge URI: dbpedia.org/page/Switzerland linked entitySwitzerland. fact, Southern Avenue ambiguous entity name,may lead worker open URI check content able issue correctjudgment. Instead, relevance second entity Switzerland judgedeasily visual inspection URI. addition, task specific time intervalincludes sub-set judgments highest precision. example, ZC-IN,judgments highest precision URI: dbpedia.org/page/Switzerlandentity Switzerland submitted 5 sec. 20 sec. (Figure 2d).Instead, ZC-US, best judgments URI: dbpedia.org/page/European linkedentity European submitted interval 2 sec. 16 sec. (Figure 2c).result, clear task instance specific qualitytime profile relatesdifficulty labelling instance.better analyse trends, Figure 4 shows Pearsons correlation coefficient ()(i.e., standard measure degree linear correlation two variables)13 entities positive links ten judgments across two datasets.time spent worker always (linearly) correlated quality judgment526fi110.80.8RecallRecallTime-Sensitive Bayesian Information Aggregation Crowdsourcing Systems0.60.40.231018041Time spent (sec)112348Time spent (sec)(a) Time-increasing task(b) Time-increasing taskZC - USEntity: SouthernAvenue - Link:freebase.com/m/03hkhgsZC -Entity: American - Link: freebase.com/united_states110.80.80.60.400.60.40.20.2212Time spent (sec)0251936Execution time (sec)45(d) Time-decreasing taskZC - USEntity: European - Link: dbpedia.org/page/EuropeanZC -Entity: Swiss - Link: dbpedia.org/page/Switzerland110.80.80.60.60.40.205(c) Time-decreasing taskRecallRecall0.40.2RecallRecall00.60.40.22814Time spent (sec)0712710Time spent(sec)2060(e) Time-decreasing task(f) Time-constant taskZC - USEntity: Switzerland - Link: dbpedia.org/page/SwitzerlandZC - USEntity: GMT - Link: dbpedia.org/page/Greenwich_Mean_TimeFigure 3: Histograms recall six entity linking tasks positive gold standardlabels least ten judgments ZenCrowd datasets. show different trendsrecall-time curves various tasks.across task instances. tasks significantly positive correlation (i.e., taskindex = 6, 8, 13 > 0.7, p < 0.05), others significantly negative correlation(i.e., task index = 9, 12 < 0.7, p < 0.05), whilst tasks less significantcorrelation accuracy judgments time spent workers.confirms different task instances substantially different quality-time responsesbased difficulty sample. Thus, insight significantly extends previousfindings reported Demartini et al. (2013) qualitytime trendobserved across entire task set. Moreover, empirically supports theory severalexisting data aggregation models (Kamar, Kapoor, & Horvitz, 2015; Whitehill et al., 2009;Bachrach, Graepel, Minka, & Guiver, 2012) make use taskspecific features527fiVenanzi, Guiver, Kohli & Jennings0.4510.8Strong positivecorrelation0.40.60.350.30.2p valuePearson's0.40-0.20.20.15-0.40.1-0.6-0.8-10.25Significant correlation0.05Strong negativecorrelation01 2 3 4 5 6 7 8 9 10 11 12 13Task index(a)1 2 3 4 5 6 7 8 9 10 11 12 13Task index(b)Figure 4: Pearsons correlation coefficient (a) p-value (b) linear correlation workers completion time judgments accuracy 13 entitylinking tasks positive gold standard labels judgments ZenCrowd datasets.achieve accurate classifications number crowdsourcing applications concerning,among others, galaxy classification (Kamar et al.), image labelling (Whitehill et al., 2009)problem solving (Bachrach et al., 2012).4. BCCTime ModelBased results time analysis workers judgments, observeddifferent types qualitytime trends occur specific task instances. However, standard BCC, well existing aggregation models considerinformation, unable perform inference likely duration task. rectifythis, need extend BCC able include trends aggregationcrowd judgments. end, model must flexible enough identify workerswho, addition imperfect skills, may also intention make validattempt complete task. increases uncertainty data reliability.section, describe Bayesian Classifier Combination model Time (BCCTime). particular, describe three components model concerning (i)representation unknown workers propensity valid labelling, (ii) reliabilityworkers judgments (iii) uncertainty workers completion time, followeddetails probabilistic inference.4.1 Modelling Workers Propensity Valid LabellingGiven uncertainty intention worker submit valid judgments, introduce latent variable k [0, 1] representing propensity k towards makingvalid labelling attempt given task. way, model able naturallyexplain unreliability worker based imperfect skills alsoattitude towards approaching task correctly. particular, k close one means528fiTime-Sensitive Bayesian Information Aggregation Crowdsourcing Systemsworker tendency exert best effort provide valid judgments, even thoughjudgments might still noisy consequence imperfect skills possesses.contrast, k close zero means worker tends provide valid judgmentstasks, means behaves similarly spammer. Specifically,workers high propensity valid labelling provide inputs meaningfultasks true label tasks duration. capture this, define per-judgment(k)(k)boolean variable vi {0, 1} vi = 1 meaning k made valid labelling(k)(k)(k)attempt submitting ci vi = 0 meaning ci invalid annotation.setting, number valid labelling attempts made worker derives(k)propensity valid labelling. Thus, model assuming vi randomdraw Bernoulli distribution parametrised k :(k)viBernoulli(k )(4)is, workers high propensity valid labelling likely make validlabelling attempts, whilst workers low propensity likely submit spamannotations.4.2 Modelling Workers Judgmentsdescribe part model concerned generative process crowdjudgments confusion matrix propensity workers. Intuitively,judgments associated valid labelling attempts considered estimatefinal labels. means judgment may generated two differentprocesses depending whether comes valid labelling attempt. capturegenerative model BCCTime, mixture model used switch(k)(k)two cases conditioned vi . first case valid labelling attempt, i.e., vi = 1,judgment generated workers confusion matrix per standard BCC(k)model. Therefore, assume ci generated model described BCC(k)(Eq. 2), including vi conditional variables. Formally:(k)(k)ci | (k) , ti , vi(k)(k)= 1 Cat ci | ti(5)(k)second case judgment produced invalid labelling attempt, i.e., vi = 0,natural assume judgment contribute estimation truelabel. Formally, assumption represented general random vote model(k)ci drawn categorical distribution vector parameter s:(k)(k)ci |s, vi(k)= 0 Cat ci |s(6)vector labelling probabilities general worker low propensitymake valid labelling attempts. Notice equation depend ti ,means judgments coming invalid labelling attempts treatednoisy responses uncorrelated ti .529fiVenanzi, Guiver, Kohli & Jennings4.3 Modelling Workers Completion Timeshown Section 3, duration task may defined intervalworkers likely submit high-quality judgments. However, due dependencyduration tasks characteristics, requirement interval mustnon-constant across tasks. model this, define lower-bound threshold, ,upper-bound threshold, , time interval representing duration i.pertask thresholds latent variables must learnt training time. Then,tasks lower higher variability duration represented basedvalues time thresholds. setting, valid labelling attempts madeworkers expected completed within tasks duration interval detailed(k)thresholds. Formally, represent probability greater usingstandard greaterThan probabilistic factor introduced Herbrich, Minka, Graepel(2007) TrueSkill Bayesian ranking model:(k)I(i(k)> |vi= 1)(7)factor defines non-conjugate relationship posterior distribution(k)form prior distribution . Therefore posterior(k)distribution p(i ) needs approximated. via moment matching(k)Gaussian distribution p(i ) matching precision precision adjusted mean(k)(i.e., mean multiplied precision) posterior distribution p(i ), shown(k)Table 1 Herbrich et al. (2007). similar way, model probabilitygreater as:(k)(k)I(i > |vi= 1)(8)Drawing together, upon observing set i.i.d. pairs judgments workerscompletion times contained J respectively, express joint likelihoodBCCTime as:p(J , , t|, p, s, ) =Ni=1Cat(ti |p)K(k)I(i(k)(k)(k) k> )I(i > )Cat ci | tik=1(k) (1k )Cat ci |s(9)factor graph BCCTime illustrated Figure 5. Specifically, two shaded vari(k)(k)ables ci observed inputs, unobserved random variablesunshaded. graph uses gate notation (dashed box) introduced Minka Winn(2008) represent two mixture models BCCTime. Specifically, outer gate represents workers judgments (see Section 4.2) completion times (see Section 4.3)(k)generated either BCC random vote model using vi gating variable.inner gate mixture model generating workers judgments rowsconfusion matrix using ti gating variable.530fiTime-Sensitive Bayesian Information Aggregation Crowdsourcing SystemsDir.Dir.pK workersBetaGaussianGaussianCat.ktiBernoulliC true label valuesCat.Dir.(k)greatersmaller(k)(k)N tasksviCat.(k)cciFigure 5: factor graph BCCTime.4.4 Probabilistic Inferenceperform Bayesian inference unknown quantities, must provide priordistributions latent parameters BCCTime. Following structure model,select conjugate distributions parameters enable tractableinference posterior probabilities. Therefore, prior p Dirichlet distributedhyperparameter p0 :(true label prior)(k)priors crespectively:p Dir(p|p0 )(10)(k)also Dirichlet distributed hyperparameter s0 c,0(spammer label prior)(confusion matrix prior)Dir(s|s0 )(k)c(k)Dir( (k)c | c,0 )(11)(12)Then, k Beta prior true count 0 false count 0 :(workers propensity prior) k Beta(k |0 , 0 )(13)two time thresholds Gaussian priors mean 0 0 precision0 0 respectively:(lower-bound tasks duration threshold prior) N (i |0 , 0 )(14)(upper-bound tasks duration threshold prior) N (i |0 , 0 )(15)531fiVenanzi, Guiver, Kohli & JenningsCollecting hyperparameters set = {p0 , s0 , 0 , 0 , 0 , 0 , 0 , 0 }, findapplying Bayes theorem joint posterior distribution proportional to:p(, p, s, t, |J , , ) Dir(s|s0 )Dir(p|p0 )NCat(ti |p)N (i |0 , 0 )N (i |0 , 0 )i=1K(k)I(i(k)(k)(k)> )I(i < )Cat ci | ti(k)(k)Dir( ti | ti ,0 )kk=1Cat(k) (1k )ci |sBeta(k |0 , 0 )(16)expression, compute marginal posterior distributions latentvariable integrating remaining variables. Unfortunately, integrationsintractable due nonconjugate form model. However, still computeapproximations posterior distributions using standard techniques family approximate Bayesian inference methods (Minka, 2001). particular, usewell-known EP algorithm (Minka, 2001) shown provide good quality approximations BCC models (Venanzi et al., 2014)10 . method leverages factoriseddistribution joint probability approximate marginal posterior distributionsiterative message passing scheme implemented factor graph. Specifically,use EP implementation provided Infer.NET (Minka, Winn, Guiver, & Knowles,2014), standard framework running Bayesian inference probabilistic models.Using Infer.NET, able train BCCTime largest dataset 12,190 judgmentswithin seconds using approximately 80MB RAM standard laptop.5. Experimental Evaluationdescribed model, test performance terms classification accuracyability learn tasks duration real crowdsourcing experiments. Using datasetsdescribed Section 3, conduct experiments following experimental setup.5.1 Benchmarksconsider set benchmarks consisting three popular baselines (Majority voting,Vote distribution Random) three stateoftheart aggregation methods (One coin,BCC CBCC) commonly employed crowdsourcing applications.detail:One coin: method represents accuracy worker single reliabilityparameter (or workers coin) assuming worker return correct answerprobability specified coin, incorrect answer inverse probability. result, method applicable binary datasets. Crucially,model represents core mechanism several existing methods including (Whitehill10. Alternative inference methods Gibbs sampling Variational Bayes trivially appliedmodel Infer.NET framework.532fiTime-Sensitive Bayesian Information Aggregation Crowdsourcing Systemset al., 2009; Demartini et al., 2012; Liu, Peng, & Ihler, 2012; Karger et al., 2011; Li,Zhao, & Fuxman, 2014)11 .BCC: closest benchmark method described Section 2.learns confusion matrices aggregated labels without consideringworkers completion time input feature. used several crowdsourcing contexts including galaxy classification (Simpson et al., 2013), image annotation(Kim & Ghahramani, 2012) disaster response (Ramchurn et al., 2015).BCCPropensity: equivalent BCCTime workers propensitylearnt. benchmark used assess contribution inferringworkers propensity, versus joint learning tasks time thresholds,quality final labels. Note BCCPropensity easy obtain BCCTimesetting time thresholds static observations = 0.0 = max.value.CBCC: extension BCC learns communities workers similarconfusion matrices described Section 2. Given judgment set, CBCC ablelearn confusion matrix community worker, well tasklabel. method also used number crowdsourcing applicationsincluding web search evaluation sentiment analysis (Venanzi et al., 2014).experiments, ran CBCC number worker types set two communitiesorder infer two groups reliable workers less reliable workerssimilar results observed higher number communities.Majority Voting: simple yet popular algorithm estimatesaggregated label one receives votes (Littlestone & Warmuth,1989; Tran-Thanh et al., 2013). assigns point mass label highestconsensus among set judgments. Thus, algorithm representuncertainty around classification considers judgments comingreliable workers.Vote Distribution: method estimates true label based empirical probabilities class observed judgment set (Simpson et al., 2015). Specifically,assigns probability label fraction judgments correspondinglabel.Random: baseline method assigns random class labels tasks,i.e., assigns uniform probabilities labels.Note alternative variant BCCTime captures time spent redundant. fact, workers propensity modelled together time spent,workers accuracy captured confusion matrices. meansmodel equivalent BCC, already included benchmarks. benchmarks also implemented Infer.NET trained using EP algorithm.11. particular, refer One coin unconstrained version ZenCrowd (Demartini et al., 2012)without two unicity SameAs constraints defined original method. suggestsversion suitable fair comparison methods.533fiVenanzi, Guiver, Kohli & Jenningsexperiments, set hyperparameters BCCTime reproduce typical situationtask requester prior knowledge true labels labelling probabilities workers, basic prior knowledge accuracy workersrepresenting that, priori, assumed better random annotators (Kim& Ghahramani, 2012). Therefore, workers confusion matrices initialisedslightly higher value diagonal (0.6) lower values rest matrix. Then,Dirichlet priors p set uninformatively uniform counts12 . priorsconfusion matrices initialised higher diagonal value (0.7) meaningpriori workers assumed better random. Gaussian priors taskstime durations set means 0 = 10 0 = 50 precisions 0 = 0 = 101 ,meaning priori entity linking task expected completed within 10 50seconds. Furthermore, initialise Beta prior k function number tasks0 = 0.7N 0 = 0.3N represent fact priori worker consideredreliable makes valid labelling attempts 70% tasks. Importantly, givenshape distribution workers time completion data observed datasets (see(k)Figure 2), apply logarithmic transformation order obtain uniformdistribution workers completion time training data. Finally, priorsbenchmarks set equivalently BCCTime.5.2 Accuracy Metricsevaluate classification accuracy tested methods measured AreaROC Curve (AUC) ZC-US ZC-IN average recall WS-AMT.particular, former standard accuracy metric evaluate performancebinary classifiers range discriminant thresholds applied predictive classprobabilities (Hanley & McNeil, 1982), well suited two ZenCrowd binarydatasets. latter recall averaged class categories (Rosenberg, 2012),main metric used score probabilistic methods competed 2013CrowdFlower shared task challenge dataset equivalent WS-AMT (see Section 3.1).5.3 ResultsTable 3 reports AUC seven algorithms ZenCrowd datasets. Specifically,shows BCCTime BCCPropensity highest accuracy datasets:AUC 11% higher ZC-IN 8% higher ZC-US, respectively, comparedmethods. Among two, BCCTime best method improvement 13%ZC-IN 1% ZC-US. Similarly, Table 4 reports average recall methodsWS-AMT showing BCCTime highest average recall, 2% highersecond best benchmark (Vote distribution) 4% higher BCCPropensity13 .means inference time thresholds, already provides valuable informationtasks extracted judgments, also adds extra quality improvementaggregated labels addition modelling workers propensities.12. noted cases different type knowledge available workers,information plugged method selecting appropriate prior distributions.13. dataset, majority vote performs similarly BCCTime due higher qualityworkers534fiTime-Sensitive Bayesian Information Aggregation Crowdsourcing SystemsTable 3: AUC tested methods measured ZenCrowd datasets.highest AUC dataset highlighted bold.Dataset:Majority voteVote distributionOne coinRandomBCCCBCCBCCPropensityBCCTimeZC-US0.38200.21010.72040.50000.64180.67300.77400.7800Table 4: average recall testedmethods measured WS-AMT.highest average recall highlightedbold.ZC-IN0.38620.30800.62630.50000.54070.55440.61770.6925Dataset:Majority voteVote distributionOne coinRandomBCCCBCCBCCPropensityBCCTimeWS-AMT0.7270.728N/A0.1830.7050.7110.7030.730important observation proves information workers completion timeeffectively data aggregation. Altogether, information allows modelcorrectly filter unreliable judgments consequently provide accurate classifications.Figure 6 shows ROC curve methods ZenCrowd (binary) datasets,namely plot false positive rate true positive rate obtained differentdiscriminant thresholds. graph shows true positive rate BCCTime generally higher benchmarks false positive rate. detail, Majorityvote, Vote distribution perform worse Random datasets methodsclearly penalised presence less reliable workers treat workersequally reliable. Interestingly, One coin performs better BCC CBCC meaningconfusion matrix better approximated single (one coin) parametertwo datasets. Also, looking percentages workers propensities inferred BCCTime reported Table 5, found 93.2% workers ZC-US, 60% workersZC-IN 97.3% workers WS-AMT propensity greater 0.5.means that, ZC-US WS-AMT, workers identified suspected spammers majority estimated reliable different propensityvalues. ZC-IN, percentage suspected spammers higher also reflectedlower accuracy judgments respect gold standard labels.Figure 7 shows mean value inferred upper-bound time threshold (bluecross points) workers maximum completion time (green asterisked points)Table 5: propensity workers learnt BCCTime dataset.Dataset:ZC-USZC-INWS-AMT% high propensityworkers (p(k ) > 0.5)93.2%60%97.3%535% low propensityworkers (p(k ) 0.5)6.8%30%2.7%fiVenanzi, Guiver, Kohli & JenningsMajority voteVote distributionOne coinRandomBCCBCCPropensityCBCCBCCTime110.90.80.7True positive rateTrue positive rate0.80.60.50.40.30.60.40.20.20.1000.20.40.6False positive rate0.801(a) ZC - US00.20.40.6False positive rate0.81(b) ZC -Figure 6: ROC curve aggregation methods ZC-US (a) ZC-IN (b).task three datasets. Looking raw data ZenCrowd datasets, averagemaximum time spent US workers higher (approx. 1.7 minutes)Indian workers (approx. 1 minute). also seen datasetssignificant portion outliers reach 50 minutes. However, discussed Section3, know many entity linking tasks fairly simpleeasily solved visual inspection candidate URI. implynormal worker completes task single session (i.e., interrupts)take long time issue judgment. Interestingly, BCCTime efficiently removesoutliers recovers realistic estimates maximum duration entitylinking task. fact, estimated upper-bound time thresholds lie within smaller timeband, i.e., around 10 seconds ZC-US 40 seconds ZC-IN. Similar results alsoobserved WS-AMT average observed maximum time significantly higheraverage inferred maximum time, thus suggesting BCCTime estimatesalso realistic dataset. addition, Figure 7 shows plotaverage duration estimated BCCTime (i.e, (E[i ] E[i ])/2 i) averageworkers completion time task. graphs show BCCTime estimatessimilar micro-tasks three datasets, i.e., 3 5,estimates obtained workers completion time data much higher: 53 secondsZC-US, 45 seconds ZC-IN 80 seconds WS-AMT. Again, duepresence outliers original data significantly bias empirical average timestowards high values. Moreover, measuring variability two sets estimates,BCCTime estimates much smaller standard deviation 100% lowerempirical averages. means estimates informativecompared normal average times obtained raw workers completion timedata.536fiTime-Sensitive Bayesian Information Aggregation Crowdsourcing Systems10104Inferred max. timeObserved max. time3Time spent (sec)Time spent (sec)104102101105001000 1500Task index10210000Inferred avg. timeObserved avg. time20000500(a) ZC - USTime spent (sec)Time spent (sec)10Inferred max. timeObserved max. time3102104Inferred avg. timeObserved avg. time102100105001000 1500Task index200005001000 1500Task index4104Inferred avg. timeObserved avg. timeTime spent (sec)Time spent (sec)Inferred max. timeObserved max. time103102102000(d) ZC -(c) ZC -102000(b) ZC - US104101000 1500Task index10210010100200300Task index0100200300Task index(e) WS - AMT(f) WS - AMTFigure 7: plot inferred (+) observed (*) maximum time spent tasksZC-US (a), ZC-IN (c) WS-AMT (e), average time spent tasksZC-US (b), ZC-IN (d) WS-AMT (f).537fiVenanzi, Guiver, Kohli & Jennings0010101AUCAUC1021011030200040006000800010100000200040006000Num. judgmentsNum. judgments(a) ZC US(b) ZC0.88000Majority voteAverage recallVote distribution0.6BCCOne coin0.4CBCCBCCPropensity0.2BCCTime0Random1000 2000 3000 4000 5000 6000Num. judgments(c) WS AMTFigure 8: AUC ZC-US (a) ZC-IN (b) average recall WS-AMT (c)methods trained increasingly large sub-sets judgments.evaluate performance methods data sparsity, Figure 8 showsaccuracy measured sub-samples judgments dataset. detail, one coinaccurate sparse judgments ZC-IN ZC-US, WS-AMTclear winner since methods except Random similar average recall trainedsparse judgments. shows BCCTime current form necessarilyoutperform methods sparse data. explained factextra latent variables (i.e., workers propensity time thresholds) used improvequality final labels also require larger set judgments accurately learnt.However, address issue, possible draw community-based models (e.g.,CBCC) design hierarchical extension BCCTime over, example, workersconfusion matrices improve robustness sparse data. Here, simplicity, BCCTime presented based simpler instance Bayesian classifier combination framework(i.e., BCC model), community-based version considered trivial extension.538fiTime-Sensitive Bayesian Information Aggregation Crowdsourcing SystemsTable 6: Comparison 21 existing methods computing aggregated labels crowdsourced judgments classified according classification models (binary class multiclass) learning features (worker accuracy, worker confusion matrix, task difficulty, taskduration workers type).Majority votingDS - Dawid & Skene (1979)GLAD - Whitehill et al. (2009)RY - Raykar et al. (2010)CUBAM - Welinder et al. (2010)YU - Yan et al. (2010)LDA - Wang et al. (2011)KJ - Kajino et al. (2012)ZenCrowd - Demartini et al. (2012)DARE - Bachrach et al. (2012)MinMaxEntropy - Zhou et al. (2012)BCC - Kim & Ghahramani (2012)MSS - Qi et al. (2013)MLNB - Bragg et al. (2013)BM - Bi et al. (2014)GP - Rodriguez et al. (2014)LU - Liu et al. (2014)WM -Li et al. (2014)CBCC - Venanzi et al. (2014)APM - Nushi et al. (2015)BCCTime - Proposed methodbinaryclassXXXXXXXXXXXXXXXXXXXXmulticlassXXXXXXXXXXXXworkeracc.XXXXXXXXXXXXXXXXXXXworkerCFXXXXXtaskdiff.XXXXtaskdurationXXworkertypeXXXXXX6. Related Workreview rest previous work relating aggregation models time analysiscrowdsourcing contexts extending background methods already consideredexperimental evaluation. recent years, large body literature focusseddevelopment smart data aggregation methods aid requesters combining judgmentsmultiple workers. general, existing methods vary assumptions complexitymodelling different aspects labelling noise. interested reader may refersurvey Sheshadri Lease (2013), well summary Table 6 listspopular methods comparison approach.particular, methods able handle binary classification problems, i.e., workers vote objects two possible classes, multi-classclassification problems, i.e., workers vote objects twoclasses. Among these, many approaches use one coin model introduced benchmarks. detail, model represents workers reliability single parameterdefined within range [0, 1] (0 = unreliable worker, 1 = reliable worker) (Karger et al.,2011; Liu et al., 2012; Demartini et al., 2012; Li et al., 2014; Nushi, Singla, Gruenheid, Zamanian, Krause, & Kossmann, 2015). Specifically, Karger et al. combines modelbudgetlimited task allocation framework provides strong theoretical guarantees539fiVenanzi, Guiver, Kohli & Jenningsasymptotical optimality inference workers reliability worker-taskmatching. Liu et al. uses general variational inference model reduces Kargeret al.s method, well algorithms special conditions. methods usetwo coin model represents bias worker towards positive labelling class(specificity) towards negative class (sensitivity) (Raykar, Yu, Zhao, Valadez, Florin,Bogoni, & Moy, 2010; Rodrigues, Pereira, & Ribeiro, 2014; Bragg, Mausam, & Weld, 2013).Then, quantities may inferred using logistic regression work Raykaret al. maximumaposteriori approaches work Bragg et al. Alternatively,Rodrigues et al. uses two coin model embedded Gaussian process classificationframework compute predictive probabilities aggregated labels workersreliability using EP. Along lines, models reason difficulty taskaffects quality judgment improve reliability aggregated labels (Whitehill et al., 2009; Bachrach et al., 2012; Kajino & Kashima, 2012). area, Whitehillet al. use logistic regression model incorporate tasks difficulty, togetherexpertise worker labelling images. contrast, Bachrach et al. use differencetwo quantities quantify advantage worker may classifying object within joint difficulty-ability-response model. similar setting, KajinoKashima exploit convex problem formulation model improve efficiencyinferring quantities numerical optimisation method. Additional factors,workers motivation propensity particular task, taken accountsophisticated models (Welinder et al., 2010; Yan, Rosales, Fung, Schmidt, Valadez,Bogoni, Moy, & Dy, 2010; Bi, Wang, Kwok, & Tu, 2014). recently, Nushi et al. (2015)devised method leverage fact error rates workers directly affected access path follow, access path represents several contextualfeatures task (e.g., task design, information sources task composition). However,unlike work, none methods learn confusion matrix worker.result, represent reliability considering accuracy potential biasesworker single data structure.Alternative models learn confusion matrices workers presented, among others, works Dawid Skene (1979), Zhou, Basu, Mao, Platt(2012), Kim Ghahramani (2012) Venanzi et al. (2014). particular, DawidSkene introduced first confusion matrix-based model confusion matricesinferred using expectation-maximisation unsupervised manner. Then, Zhou et al.extended work include taskspecific latent matrix representing confusabilitytask perceived workers. However, neither methods consider uncertainty workers reliability parameters models. example,one label obtained worker, methods may infer workerperfectly reliable totally incompetent when, reality, worker neither. overcome limitation, methods BCC CBCC capture uncertaintyworkers expertise true labels using Bayesian learning framework. twomethods extensively discussed earlier (see Sections 2 5) included benchmarks experiments. Similarly CBCC, methods leverage groups workersequivalent reliability improve quality aggregated labels limited data(Li et al., 2014; Bi et al., 2014; Kajino & Kashima, 2012; Yan et al., 2010). However,already noted, methods use extra information workers540fiTime-Sensitive Bayesian Information Aggregation Crowdsourcing Systemsjudgments learn probabilistic models. result, unlike approach, cannot take full advantage time information provided crowdsourcing platformimprove quality inference results.turn problem time analysis crowd generated content. Recentlyintroduced metric measuring effort required complete crowdsourced taskbased area error-time curve (ETA). such, metric supportsidea considering time important factor crowdsourcing effort. regard,closely related work analysis ZenCrowd datasets (see Section 3) presentedwork Difallah, Demartini, Cudre-Mauroux (2012). work showedworkers complete tasks fast slow typically less accurateothers. findings also confirmed work. However, addition, extendedanalysis showing judgments quality correlated time spentworkers different ways specific task instances. intuition methodexploits efficiently combine workers completion time features data aggregationprocess. Furthermore, earlier work introducing method predicts durationtask based number available features (including tasks price, creation timenumber assignments) using survival analysis model presented paperWang et al. (2011). However, method deal aggregating labels,learning accuracy workers, approach.7. Conclusionspresented evaluated BCCTime, new timesensitive aggregation method simultaneously merges crowd labels estimates duration individual task instancesusing principled Bayesian inference. key innovation method leverage extended set features comprising workers completion time judgment set.appropriately correlated together, features become important indicators reliability worker that, turn, allow us estimate final labels, tasks durationworkers reliability accurately. Specifically, introduced new representationaccuracy profile worker consisting workers confusion matrix,accounts workers labelling probabilities class, workers propensityvalid labelling, represents workers intention meaningfully participatelabelling process. Furthermore, used latent variables represent durationtask using pairs latent thresholds capture time interval best judgments task likely submitted honest workers. way, modeldeal differences time length task instance relating differenttype correlation quality received judgments time spentworkers. fact, taskspecific correlations observed experimentalanalysis crowdsourced datasets various task instances showed different typesqualitytime trends. Thus, main idea behind BCCTime model trendsaggregation crowd judgments make reliable inference quantitiesinterest. extensive experimental validation real-world datasets, showedBCCTime produces significantly accurate classifications estimatestasks duration considerably informative common heuristics obtainedraw workers completion time data.541fiVenanzi, Guiver, Kohli & Jenningsbackground, several implications work concerning variousaspects reliable crowdsourcing systems. Firstly, process designing tasktake exploit unbiased tasks duration estimated BCCTime. shown,information valid proxy assess difficulty task therefore supports numberdecisionmaking problems fair pricing difficult tasks defining fairbonuses honest workers. Secondly, workers propensity valid labelling uncoversadditional dimension workers reliability enables us score attitudetowards correctly approaching given task. information useful select different taskdesigns engaging tasks workers systematically approach task incorrectly.Thirdly, method uses features readily available common crowdsourcingsystems, allows faster take technology real applications.Building advances, several aspects current model indicatepromising directions improvements. example, consider timedependencies accuracy profile worker capture fact workers typicallyimprove skills time performing sequence tasks. doing, possibletake advantage temporal dynamics potentially improve quality finallabels. addition, crowdsourcing settings involve continuous-valued judgmentscurrently supported method. deal cases, number nontrivial extensions generative model and, turn, new treatment probabilisticinference required.8. Acknowledgmentsauthors gratefully acknowledge funding bodies, Microsoft UK ResearchCouncil ORCHID project, grant EP/I011587/1ORCHID, Bhaskar Mitra (Microsoft) proofreading manuscript.ReferencesAlonso, O., Rose, D. E., & Stewart, B. (2008). Crowdsourcing relevance evaluation.ACM SigIR Forum, Vol. 42, pp. 915, New York, NY, USA. ACM.Bachrach, Y., Graepel, T., Minka, T., & Guiver, J. (2012). grade test withoutknowing answersa Bayesian graphical model adaptive crowdsourcingaptitude testing. Proceedings 29th International Conference MachineLearning (ICML), pp. 11831190.Bernstein, M., Little, G., Miller, R., Hartmann, B., Ackerman, M., Karger, D., Crowell, D.,& Panovich, K. (2010). Soylent: word processor crowd inside. Proceedings23nd annual ACM symposium User interface software technology, pp.313322. ACM.Bi, W., Wang, L., Kwok, J. T., & Tu, Z. (2014). Learning predict crowdsourceddata. Proceedings 30th International Conference Uncertainty ArtificialIntelligence (UAI).Bishop, C. (2006). Pattern recognition machine learning, Vol. 4. Springer New York.542fiTime-Sensitive Bayesian Information Aggregation Crowdsourcing SystemsBragg, J., Mausam, & Weld, D. (2013). Crowdsourcing multi-label classification taxonomy creation. First AAAI Conference Human Computation Crowdsourcing,pp. 2533.Dawid, A., & Skene, A. (1979). Maximum likelihood estimation observer error-rates usingem algorithm. Applied statistics, 2028.Demartini, G., Difallah, D. E., & Cudre-Mauroux, P. (2012). Zencrowd: Leveraging probabilistic reasoning crowdsourcing techniques large-scale entity linking.Proceedings 21st international conference World Wide Web (WWW), pp.469478.Difallah, D. E., Demartini, G., & Cudre-Mauroux, P. (2012). Mechanical cheat: Spammingschemes adversarial techniques crowdsourcing platforms. CrowdSearch, pp.2630.Faradani, S., Hartmann, B., & Ipeirotis, P. G. (2011). Whats right price? pricing tasksfinishing time. Human Computation, Vol. WS-11-11 AAAI Workshops,pp. 2631. AAAI.Hanley, J. A., & McNeil, B. J. (1982). meaning use area receiveroperating characteristic (roc) curve. Radiology, 143 (1), 2936.Herbrich, R., Minka, T., & Graepel, T. (2007). Trueskill(tm): Bayesian skill ratingsystem. Advances Neural Information Processing Systems (NIPS), pp. 569576.MIT Press.Huff, C., & Tingley, D. (2015). people? evaluating demographic characteristics political preferences mturk survey respondents. Research & Politics,2 (3), 2053168015604648.Kajino, H., & Kashima, H. (2012). Convex formulations learning crowds. Transactions Japanese Society Artificial Intelligence, 27, 133142.Kamar, E., Hacker, S., & Horvitz, E. (2012). Combining human machine intelligencelarge-scale crowdsourcing. Proceedings 11th International ConferenceAutonomous Agents Multiagent Systems (AAMAS), pp. 467474.Kamar, E., Kapoor, A., & Horvitz, E. (2015). Identifying accounting task-dependentbias crowdsourcing. Third AAAI Conference Human ComputationCrowdsourcing.Karger, D., Oh, S., & Shah, D. (2011). Iterative learning reliable crowdsourcing systems.Advances Neural Information Processing Systems (NIPS), pp. 19531961. MITPress.Kazai, G. (2011). search quality crowdsourcing search engine evaluation.Advances information retrieval, pp. 165176. Springer.Kim, H., & Ghahramani, Z. (2012). Bayesian classifier combination. InternationalConference Artificial Intelligence Statistics, pp. 619627.Li, H., Zhao, B., & Fuxman, A. (2014). wisdom minority: discovering targetingright group workers crowdsourcing. Proceedings 23rd InternationalConference World Wide Web (WWW), pp. 165176.543fiVenanzi, Guiver, Kohli & JenningsLittlestone, N., & Warmuth, M. K. (1989). weighted majority algorithm. 30thAnnual Symposium Foundations Computer Science, pp. 256261. IEEE.Liu, Q., Peng, J., & Ihler, A. (2012). Variational inference crowdsourcing. AdvancesNeural Information Processing Systems (NIPS), pp. 692700. MIT Press.Minka, T. (2001). Expectation propagation approximate Bayesian inference. Proceedings 17th Conference Uncertainty Artificial Intelligence (UAI), pp.362369.Minka, T., & Winn, J. (2008). Gates. Advances Neural Information Processing Systems(NIPS), pp. 10731080. MIT Press.Minka, T., Winn, J., Guiver, J., & Knowles, D. (2014). Infer.NET 2.6. Microsoft ResearchCambridge.Minka, T. P. (2001). family algorithms approximate Bayesian inference. Ph.D.thesis, Massachusetts Institute Technology.Nushi, B., Singla, A., Gruenheid, A., Zamanian, E., Krause, A., & Kossmann, D. (2015).Crowd access path optimization: Diversity matters. Third AAAI ConferenceHuman Computation Crowdsourcing, pp. 130139.Ramchurn, S. D., Huynh, T. D., Ikuno, Y., Flann, J., Wu, F., Moreau, L., Jennings, N. R.,Fischer, J. E., Jiang, W., Rodden, T., et al. (2015). Hac-er: disaster response systembased human-agent collectives. 2015 International Conference AutonomousAgents Multiagent Systems, pp. 533541.Raykar, V., Yu, S., Zhao, L., Valadez, G., Florin, C., Bogoni, L., & Moy, L. (2010). Learningcrowds. Journal Machine Learning Research, 11, 12971322.Rodrigues, F., Pereira, F., & Ribeiro, B. (2014). Gaussian process classification activelearning multiple annotators. Proceedings 31st International ConferenceMachine Learning (ICML), pp. 433441.Rosenberg, A. (2012). Classifying skewed data: Importance weighting optimize averagerecall. INTERSPEECH, pp. 22422245.Sheng, V., Provost, F., & Ipeirotis, P. (2008). Get another label? Improving data qualitydata mining using multiple, noisy labelers. Proceedings 14th InternationalConference Knowledge Discovery Data Mining (SIGKDD), pp. 614622. ACM.Sheshadri, A., & Lease, M. (2013). Square: benchmark research computing crowdconsensus. Proceedings 1st AAAI Conference Human ComputationCrowdsourcing (HCOMP), pp. 156164.Simpson, E., Roberts, S., Psorakis, I., & Smith, A. (2013). Dynamic bayesian combinationmultiple imperfect classifiers. Decision Making Imperfection, pp. 135.Springer.Simpson, E., Venanzi, M., Reece, S., Kohli, P., Guiver, J., Roberts, S., & Jennings, N. R.(2015). Language understanding wild: Combining crowdsourcing machinelearning. 24th International World Wide Web Conference (WWW), pp. 9921002.ACM.544fiTime-Sensitive Bayesian Information Aggregation Crowdsourcing SystemsSimpson, E. (2014). Combined Decision Making Multiple Agents. Ph.D. thesis, University Oxford.Tran-Thanh, L., Venanzi, M., Rogers, A., & Jennings, N. R. (2013). Efficient Budget Allocation Accuracy Guarantees Crowdsourcing Classification Tasks.12th International Conference Autonomous Agents Multi-Agent Systems (AAMAS), pp. 901908.Venanzi, M., Guiver, J., Kazai, G., Kohli, P., & Shokouhi, M. (2014). Community-basedbayesian aggregation models crowdsourcing. 23rd International ConferenceWorld Wide Web (WWW), pp. 155164. ACM.Venanzi, M., Rogers, A., & Jennings, N. R. (2015a). Weather Sentiment - Amazon Mechanical Turk dataset. University Southampton.Venanzi, M., Teacy, W., Rogers, A., & Jennings, N. R. (2015b). Bayesian modellingcommunity-based multidimensional trust participatory sensing data sparsity.Twenty-Fourth International Joint Conference Artificial Intelligence (IJCAI15), pp. 717724.Wang, J., Faridani, S., & Ipeirotis, P. (2011). Estimating completion time crowdsourced tasks using survival analysis models. Crowdsourcing Search DataMining (CSDM), Vol. 31, pp. 3134.Welinder, P., Branson, S., Belongie, S., & Perona, P. (2010). multidimensional wisdomcrowds. Advances Neural Information Processing Systems (NIPS), Vol. 10,pp. 24242432. MIT Press.Whitehill, J., Ruvolo, P., Wu, T., Bergsma, J., & Movellan, J. R. (2009). Whose votecount more: Optimal integration labels labelers unknown expertise.Advances Neural Information Processing Systems (NIPS), Vol. 22, pp. 20352043. MIT Press.Yan, Y., Rosales, R., Fung, G., Schmidt, M., Valadez, G. H., Bogoni, L., Moy, L., & Dy,J. (2010). Modeling annotator expertise: Learning everybody knows bitsomething. International Conference Artificial Intelligence Statistics, pp.932939.Zhou, D., Basu, S., Mao, Y., & Platt, J. (2012). Learning wisdom crowdsminimax entropy. Advances Neural Information Processing Systems (NIPS), pp.21952203. MIT Press.Zilli, D., Parson, O., Merrett, G. V., & Rogers, A. (2014). hidden markov model-basedacoustic cicada detector crowdsourced smartphone biodiversity monitoring. Journal Artificial Intelligence Research, 805827.545fiJournal Artificial Intelligence Research 56 (2016) 613-656Submitted 03/16; published 08/16Datalog Ontology ConsolidationCristhian Ariel D. DeagustiniMara Vanina MartnezMarcelo A. FalappaGuillermo R. Simaricadd@cs.uns.edu.armvm@cs.uns.edu.armfalappa@cs.uns.edu.argrs@cs.uns.edu.arAI R&D Lab., Institute Computer Science Engineering (ICIC)Consejo Nacional de Investigaciones Cientficas Tecnicas (CONICET)Universidad Nacional del Sur (UNS), Alem 1253,(B8000CPB) Baha Blanca, Argentina.AbstractKnowledge bases form ontologies receiving increasing attentionallow clearly represent available knowledge, includes knowledge constraints imposed domain users. particular, Datalogontologies attractive property decidability possibilitydealing massive amounts data real world environments; however,case many ontological languages, application collaborative environmentsoften lead inconsistency related issues. paper introduce notion incoherence regarding Datalog ontologies, terms satisfiability sets constraints,show specific conditions incoherence leads inconsistent Datalog ontologies.main contribution work novel approach restore consistencycoherence Datalog ontologies. proposed approach based kernel contractionrestoration performed application incision functions select formulasdelete. Nevertheless, instead working minimal incoherent/inconsistent sets encountered ontologies, operators produce incisions non-minimal structures calledclusters. present construction consolidation operators, along propertiesexpected satisfied them. Finally, establish relation construction properties means representation theorem. Although proposalpresented Datalog ontologies consolidation, operators appliedtypes ontological languages, Description Logics, making apt usedcollaborative environments like Semantic Web.1. Introductionintegration different systems, interaction resulting integration, ledhost pervasive practical problems challenging research opportunities;interesting ones occurs Webs collaborative environments, e.g., e-commerce,arrival Semantic Web, ontology engineering. However,collaboration among systems brings along problem conflicting pieces informationlikely appear knowledge repositories evolve. Admittedly, managementconflicting information important challenging issue faced (Gomez,Chesnevar, & Simari, 2010; Haase, van Harmelen, Huang, Stuckenschmidt, & Sure, 2005;Huang, van Harmelen, & ten Teije, 2005; Bell, Qi, & Liu, 2007), specially integratingc2016AI Access Foundation. rights reserved.fiDeagustini, Martinez, Falappa & Simariknowledge coming different sources (Black, Hunter, & Pan, 2009; Baral, Kraus, &Minker, 1991; Amgoud & Kaci, 2005), knowledge expected exploitedreasoning process. context, knowledge bases form ontologies becoming useful device provide convenient way represent intensionalextensional knowledge application domain. Moreover, expressive power ontologies allows perform important tasks data integration (Lenzerini, 2002), also playsrole great importance aforementioned Semantic Web (Berners-Lee, Hendler, &Lassila, 2001). work adopt Datalog ontologies, family rule-based ontologylanguages (Cal, Gottlob, & Lukasiewicz, 2012). Datalog enables modular rule-basedstyle knowledge representation, represent syntactical fragments first-orderlogic (FOL) answering Boolean Conjunctive Query (BCQs) Q setDatalog rules input database equivalent classical entailment check|= Q. Tractable fragments Datalog guarantee termination query answeringprocedures polynomial time data complexity first-order rewritability. Moreover, ontologies described using existential rules generalize several well-known DescriptionLogics (DLs); particular, linear guarded Datalog (two basic tractable fragmentsfamily) strictly expressive whole DL-Lite family (Calvanese, De Giacomo, Lembo, Lenzerini, & Rosati, 2005), guarded Datalog strictly expressiveEL (Brandt, 2004; Baader, Brandt, & Lutz, 2005). Therefore, results presentedpaper extend directly DLs well. properties Datalog togetherexpressive power, fact keeps syntax closer used relationaldatabases greater readability, make useful modeling real applications,ontology querying, Web data extraction, data exchange, ontology-based data access,data integration.focus two particular problems arise integration and/or evolutioninformation systems: inconsistency incoherence. Inconsistency refers lackmodels theory. hand, ontological settings, incoherence refers setontological rules cannot applied without leading violations constraintsimposed knowledge, making unsatisfiable. Incoherence inconsistency,arise automated procedures data integration ontology matching, mayserious issues real world applications. Since standard ontology languages adhereclassical FOL semantics, classical inference semantics fails presence kindproblems. Thus, important focus formalization methods addressinconsistency incoherence ontologies able cope users expectationsterms effectiveness procedures query answering meaninganswers potential conflict exists.paper addresses problem handling inconsistencies incoherencesmay appear Datalog ontologies. regard, propose general frameworkaims consolidation Datalog ontologies (i.e., solving every conflict coherenceconsistency them). is, consolidation operator takes input (possibly)incoherent inconsistent Datalog ontology returns another Datalog ontologyconflicts amended, thus ensuring coherent consistent.usual setting, assumption minimal change made, say,expected consolidation process changes original ontology little possible.approach presented based use incision functions (Hansson, 1993, 1994, 1997,614fiDatalog Ontology Consolidation2001) Belief Revision literature. Instead operators accountinformation included conflicts knowledge base, work aim captureconsolidation operators consider information included KB solvingconflicts. main contributions work following:introduce notion incoherence tailored Datalog . achieve adaptsetting similar notions Description Logics. Also, look relationship incoherence inconsistency impacts consolidation process.provide set properties expected satisfied consolidation operatorsDatalog ontologies means postulates. postulates provide formalcharacterization consolidation operator without focusing consolidationprocess actually performed, thus providing formal comparison frameworkconsolidation operators. postulates consider intuitions classic BeliefRevision; nevertheless, adapted Datalog ontological setting (andcould also adapted suit ontological languages), meaningtwo versions (one addressing incoherence another one inconsistency).present complete construction consolidation operators take (possibly)incoherent inconsistent Datalog ontology gives result consistentcoherent one. noteworthy characteristic operators involves twosteps approach, first considering incoherence conflicts, solving inconsistencyconflicts latter step, helping improve final result terms informationneeds deleted solve conflicts.study relationship formal properties operatorconstruction propose, demonstrating equivalent; thus, showsconsolidation operator satisfying properties corresponds constructionintroduced work.paper organized follows: Section 2 introduce necessary notionsDatalog Belief Revision. Next, though inconsistency incoherence related,also two distinct problems setting ontological knowledge basesparticular, clear separation intensional extensional knowledge.Therefore, Section 3, discuss two notions Datalog ontologies, relateother, reasons need treated combination separately.Then, Section 4 present properties ontology consolidation operator mustsatisfy, Section 5 introduce process used restore consistency coherenceDatalog ontologies, relate presented process given properties meansrepresentation theorem. Next, present complete example depicting entireconsolidation process. Finally, Sections 7 8 discuss related work differentareas Artificial Intelligence Database Theory, provide conclusions futurelines research, respectively.2. Preliminaries Backgroundfacilitate reading, begin introducing notions Datalog BeliefRevision needed rest paper.615fiDeagustini, Martinez, Falappa & Simari2.1 Preliminaries DatalogFirst, recall basic notions Datalog ontologies used paper(see Cal et al., 2012 details). Datalog extends Datalog allowing existentialquantification rule heads, together extensions enumerate below,limiting interaction elements order achieve tractability.assume domain discourse Datalog ontology consistscountable set data constants , countable set nulls N (as place holders unknownvalues), countable set variables V. also assume different constants representdifferent values (unique names assumption). distinguish constants variables,adopt standard notation logic programming, variable names beginuppercase letters, constants predicate symbols begin lowercase letters.assume relational schema R finite set predicate symbols (or simply predicates). term constant, null, variable. atom form p(t1 , . . . , tn ),p n-ary predicate t1 , . . . , tn terms; atom ground iff termsconstants. Let L first-order language R L; LR denotessublanguage generated R. database (instance) R finite set atoms predicates R terms N . homomorphism constants, nulls variablesmapping h : N V N V (i) c implies h(c) = c, (ii)c N implies h(c) N , (iii) h naturally extended atoms, sets atoms,conjunctions atoms.Given relational schema R, tuple-generating dependency (TGD) first-orderformula form XY(X, Y) Z(X, Z) (X, Y) (X, Z) conjunctions atoms R called body (denoted body()) head (denoted head()),respectively. Consider database relational schema R, TGD Rform (X, Y) Z (X, Z). Then, applicable exists homomorphismh maps atoms (X, Y) atoms D. Let applicable D, h0homomorphism extends h follows: Xi X, h0 (Xi ) = h(Xi );Zj Z, h0 (Zj ) = zj , zj fresh null, i.e., zj N , zj occur D, zjlexicographically follows nulls already introduced. application addsatom h0 ((X, Z)) already D. application saysatisfied D. Chase database set TGDs , denoted chase(D, ),exhaustive application TGDs (Cal et al., 2012) breadth-first (level-saturating) fashion, leads (possibly infinite) chase . importantremark BCQs Q evaluated chase , i.e.,|= Q equivalent chase(D, ) |= Q (Cal et al., 2012).Negative constraints (NCs) first-order formulas form X(X) ,(X) conjunction atoms (without nulls) head truth constant false,denoted . NC satisfied database set TGDs iffexist homomorphism h maps atoms (X) D, everyTGD satisfied, i.e., atoms body cannot true together.Equality-generating dependencies (EGDs) first-order formulas formX(X) Xi = Xj , (X) conjunction atoms, Xi Xj variables X. EGD satisfied database R iff, whenever existshomomorphism h h((X)) D, holds h(Xi ) = h(Xj ). work616fiDatalog Ontology Consolidationfocus particular class EGDs, called separable (Cal et al., 2012); intuitively,separability EGDs w.r.t. set TGDs states that, EGD violated, atomscontained reason violation (and application TGDs); i.e.,EGD E violated apply TGDs database D, EGDalso violated D. Separability standard assumption Datalog ontology, oneimportant features family languages focus decidable (Cal,Lembo, & Rosati, 2003) (actually tractable) fragments Datalog .NCs EGDs play important role matter conflicts Datalog ontologies.fact, approach present work ensure neither NCs EGDsviolated resulting ontology. Also, important remark, note restrictionusing separable EGDs makes certain cases conflicts consideredproposal. treatment cases, though interesting technical point view,outside scope work since focus tractable fragments Datalog .usual case literature, general universal quantifiers TGDs,negative constraints EGDs omitted, sets dependencies constraintsassumed finite. presented different ways expressingknowledge Datalog , ready formally define Datalog ontologies.Definition 1 (Datalog Ontology) Datalog ontology KB = (D, ), =E NC , consists database instance finite set ground atoms (withoutnulls), set TGDs , set separable EGDs E set NCs NC .Otherwise explicitly said, paper clear context refercomponent KB set constraints ontology, without distinguishingdependencies constraints. Given database R set constraints= E NC , set models , denoted mods(D, ), setdatabases B B every formula satisfied. followingexample shows simple Datalog ontology; ontology describes knowledgetherapy/psychology domain.Example 1 (Datalog Ontology)D: {a1 : therapy(charlie), a2 : dating(kate, charlie),a3 : therapist(kate), a4 : belongs to(g1 , charlie),5 : therapy(patrick ), a6 : belongs to(g2 , ed ),7 : belongs to(g1 , kate)}NC : {1 : treating(T , P ) dating(T , P ) }KB =E : {1 : treating(T , P ) treating(T 0 , P ) = 0 }: {1 : therapy(P ) patient(P ),2 : therapist(T ) belongs to(G, ) leads(T , G),3 : leads(T , G) belongs to(G, P ) treating(T , P ),4 : treating(T , P ) therapist(T )}set TGDs expresses dependencies as: TGD 1 states person Ptherapy P patient, 2 establishes therapist belongs group617fiDeagustini, Martinez, Falappa & SimariG leader group. NC 1 states patient cannot datingtherapist, EGD 1 states every patient treatment one therapist.Following classical notion consistency, say consistent Datalog ontologynon-empty set models.Definition 2 (Consistency) Datalog ontology KB = (D, ) consistent iffmods(D, ) 6= . say KB inconsistent otherwise.Example 2 Consider Datalog ontology example above; ontology clearlyinconsistent. Database instance clearly model since least TGD 2applicable D, superset satisfies TGDs constraints time. instance TGDs 2 applicable creating atomleads(kate, g1 ) making 3 applicable resulting new atom treating(kate, charlie),together dating(kate, charlie) (that already D) violate NC 1 ,therapist dating one patients.rest paper, otherwise explicitly stated KB = (D, ) denote Datalogontology = E NC , database instance, setTGDs, E set separable EGDs NC set NCs .2.2 Background Belief RevisionEstablishing origins scientific ideas difficult task sometimes controversial; nevertheless, could argued origins belief change theory go backwork Isaac Levi (1977), discussed problems concerning field research,William Harpers proposal rational way interrelate belief change operators (Harper, 1975). However, main advances belief change theory came1980s Carlos Alchourron David Makinson studied changes legal codes (Alchourron & Makinson, 1981), Peter Gardenforss introduced rational postulateschange operators (Gardenfors, 1982). that, three authors produced foundational paper containing became known AGM model (Alchourron, Gardenfors,& Makinson, 1985). core contribution AGM model presentationnew general formal framework study belief change; today, workconsidered cornerstone belief change theory evolved.Since introduction AGM model, different frameworks belief dynamicsrespective epistemic models proposed. epistemic model correspondsformalism beliefs represented, providing framework differentkinds operators defined. AGM model conceived idealistic theoryrational change epistemic states represented belief sets (sets sentencesclosed logical consequence, commonly denoted boldface), epistemic inputrepresented sentence. AGM model, three basic change operators defined:expansion, contraction, revision. rest section, whenever use termconsistent inconsistent, refer traditional notion inconsistency knowledgebase models. Let K belief set, change operations follows:618fiDatalog Ontology ConsolidationExpansions: result expanding K sentence possibly larger setinfers ; intuitively, belief , hopefully consistent given epistemic state,directly added K.Contractions: result contracting K possibly smaller setinfer , unless tautology;Revisions: result revising K set neither extends partset K. general, fallacy consistently inferredrevision K .great importance AGM comes providing axiomatic characterizations contraction revision terms rationality postulates. rationality postulates regardoperators black boxes, characterizing do, explainingit. words, behavior constrained regard inputs basic cases,without describing internal mechanisms used achieving behavior, crucialsay contraction revision operators also obtained via constructiveapproaches. AGM contractions realized partial meet contractions,based selection among (maximal) subsets K imply . Via Levisidentity (Gardenfors, 1988), associated revision operations called partial meet revisionsobtained. Another possible approach contraction based selection among(minimal) subsets K contribute make K imply , safe contraction (Alchourron & Makinson, 1985). general variant approach, knownkernel contraction, introduced later (Hansson, 1994). shown safecontractions kernel contractions equivalent partial meet contractions, henceAGM approach contraction (Hansson, 1994, 2001).particularly interesting characteristic kernel contraction may concernedchanges symbolic level since suitable applied belief bases (setsentences closed consequence relation) well belief sets. Thus, mattersbeliefs actually represented. happen AGM approach,studies changes knowledge level since uses belief sets. distinctionknowledge symbolic level proposed Allen Newell (1982). According Newell,knowledge level lies symbolic level, latter used somehow representformer. this, belief bases different symbolic content may representknowledge. importance that, although statically equivalent(they represent beliefs), equivalent belief bases could dynamically differentchoose use approach working directly them, kernel contraction.Besides three basic operations mentioned, years additional operationsdeveloped Belief Revision achieve different behaviors. instance,belief base inconsistent, removal enough sentences lead consistentstate. additional operation called consolidation, consolidation belief baseK denoted K ! (see Hansson, 1991, 2001). focus last operation,inherently different contraction revision, since ultimate goal obtainconsistent belief base possibly inconsistent one (without given epistemicinput), rather revising knowledge base specific formula removingparticular formula it. consolidation K obtained natural way belief619fiDeagustini, Martinez, Falappa & Simaribases contracting falsum, i.e., K ! = K , represents contractionoperator; process restores consistency attending every conflict K (Hansson, 1991).3. Incoherence Inconsistency Problems Related DatalogOntology Consolidationproblem obtaining consistent knowledge inconsistent knowledge basenatural many computer science fields. knowledge evolves, contradictions likelyappear, inconsistencies handled way affectquality information obtained database.setting Consistent Query Answering (CQA), repairing relational databases,inconsistency-tolerant query answering ontological languages (Arenas, Bertossi, &Chomicki, 1999; Lembo, Lenzerini, Rosati, Ruzzi, & Savo, 2010; Lukasiewicz, Martinez, &Simari, 2012), often assumption made set expresses semanticsdata component D, internal conflict set constraintsconstraints subject changes time. means first, setalways satisfiable, sense application inevitably yield consistencyproblem. Second, result assumption, must case conflicts comedata contained database instance, part ontologymust modified order restore consistency. Although reasonable assumptionmake, specially case single ontology, work focusgeneral setting, consider data constraints change timebecome conflicting. general scenario, knowledge evolves (and ontologyrepresents it) data related issues appear, also constraint related ones.argue also important identify separate sources conflictsDatalog ontologies. previous section defined inconsistency Datalogontology based lack models. operational point view, conflicts appearDatalog ontology whenever NC EGD violated, is, whenever bodyone constraint mapped either atoms atoms obtainedapplication TGDs . Beside conflicts, alsofocus relationship set TGDs set NCs EGDs,could happen (a subset of) TGDs cannot applied without leadingalways violation NCs EGDs. Note case clearly datadatabase instance problem, database TGDs applicableinevitable produce inconsistent ontology. issue related unsatisfiabilityproblem concept ontology, known Description Logics communityincoherence (Flouris, Huang, Pan, Plexousakis, & Wache, 2006; Schlobach & Cornet, 2003;Borgida, 1995; Beneventano & Bergamaschi, 1997; Kalyanpur, Parsia, Sirin, & Hendler,2005; Schlobach, Huang, Cornet, & van Harmelen, 2007; Qi & Hunter, 2007). Incoherenceparticularly important combining multiple ontologies since constraintsimposed one data could (possibly) represent conflicting modelsapplication hand. Clearly, notions incoherence inconsistency highlyrelated; fact, Flouris et al.s (2006) work establish relation incoherenceinconsistency, considering incoherence particular form inconsistency.620fiDatalog Ontology ConsolidationLater section present complete definition incoherence Datalog , basedconcept unsatisfiability sets TGDs. Nevertheless, sufficientknow proposed notion incoherence states given set unsatisfiableconstraints possible find set atoms KB = (D, ) consistentontology time TGDs applicable D. meansDatalog ontology consistent even set constraints incoherent, longdatabase instance make dependencies applicable. hand,Datalog ontology inconsistent even set constraints satisfiable,e.g., KB = ({tall(peter), small(peter)}, {tall(X) small(X) }), (empty)set dependencies trivially satisfiable thus ontology coherent; ontology is,nevertheless, inconsistent.formalizing notion incoherence use Datalog settingneed identify set atoms relevant given set TGDs. Intuitively, sayset atoms relevant set TGDs atoms setapplication generates atoms needed apply TGDs , i.e.,triggers application every TGD .Definition 3 (Relevant Set Atoms Set TGDs) Let R relationalschema, set TGDs, (possibly existentially closed) non-empty setatoms, R. say relevant iff formXY(X, Y) Z(X, Z) holds chase(A, ) |= XY(X, Y).clear context, singleton set = {a} relevantsay atom relevant .Example 3 (Relevant Set Atoms) Consider following constraints:= {1 : supervises(X , ) supervisor (X ),2 : supervisor (X ) makes decisions(X ) leads department(X , D),3 : employee(X ) works in(X , D)}Consider set A1 = {supervises(walter , jesse), makes decisions(walter ), employee(jesse)}.set relevant set atoms set constraints = {1 , 2 , 3 }, since 13 directly applicable A1 2 becomes applicable apply 1 (i.e.,chase entails atom supervisor (walter ), together makes decisions(walter )triggers 2 ).However, set A2 = {supervises(walter , jesse), makes decisions(gus)} relevant. Note even though 1 applicable A2 , TGDs 2 3 never appliedchase(A2 , ), since atoms bodies never generated chase(A2 , ).instance, consider TGD 2 . chase create atomsupervisor(walter), nevertheless still cannot trigger 2 sincecannot generate atom makes decisions(walter ), atom makes decisions(gus)already A2 match constant value.present notion coherence Datalog , adapts efforts madeDLs Schlobach Cornets (2003) Flouris et al.s (2006). conception621fiDeagustini, Martinez, Falappa & Simari(in)coherence based notion satisfiability set TGDs w.r.t. setconstraints. Intuitively, set dependencies satisfiable relevant setatoms triggers application dependencies set produceviolation constraint NC E , i.e., TGDs satisfied along NCsEGDs KB .Definition 4 (Satisfiability Set TGDs w.r.t. Set Constraints) Let Rrelational schema, set TGDs, N NC E , R. setsatisfiable w.r.t. N iff set (possibly existentially closed) atoms Rrelevant mods(A, N ) 6= . say unsatisfiable w.r.t.N iff satisfiable w.r.t. N . Furthermore, satisfiable w.r.t. NC E iffunsatisfiable w.r.t. N N NC E .rest paper sometimes write set TGDs (un)satisfiable omittingset constraints, context particular ontologyfixed set constraints NC E since set TGDs satisfiable w.r.t. NC Esatisfiable w.r.t. subset and, hand, set TGDs unsatisfiablew.r.t. subset NC E also unsatisfiable w.r.t. whole set constraints.Example 4 (Unsatisfiable Sets Dependencies) Consider following constraints.1NC = { : risky job(P ) unstable(P ) }1T = {1 : dangerous work (W ) works in(W, P ) risky job(P ),2 : therapy(P ) unstable(P )}set 1T satisfiable set TGDs, even though simultaneous application1 2 may violate formula 1NC 1E , hold every relevantset atoms. Consider example relevant set D1 = {dangerous work (police),works in(police, marty), therapy(rust)}; D1 relevant set 1T , however,mods(D1 , 1T 1NC 1E ) 6= 1T satisfiable.hand, example unsatisfiability consider following constraints:2NC = {1 : sore throat(X) sing(X) }2T = {1 : rock singer (X) sing loud (X), 2 : sing loud (X) sore throat(X),3 : rock singer (X) sing(X)}set 2T unsatisfiable set dependencies, application TGDs {1 , 2 , 3 }relevant set atoms cause violation 1 . instance, considerrelevant atom rock singer (axl): application 2T {rock singer (axl)}causes violation 1 considered together 2T , thereforemods({rock singer (axl)}, 2T 2NC 2E ) = . Note set relevant atomscause violation 1 .ready formally define coherence Datalog ontology. Intuitively,ontology coherent subset TGDs unsatisfiable w.r.t.constraints ontology.622fiDatalog Ontology ConsolidationDefinition 5 (Coherence) ontology KB coherent satisfiablew.r.t. NC E . Also, KB said incoherent iff coherent.Example 5 (Coherence) Consider sets dependencies constraints defined Ex-ample 4 arbitrary database instance D. see Datalog ontologyKB 1 = (D, 1T 1NC 1E ) coherent, KB 2 = (D, 2T 2NC 2E ) incoherent.Considering incoherence set TGDs important consolidation processDatalog ontologies, since treated appropriately within consolidation process,incoherent set TGDs may lead trivial solution removing every single relevantatom (in worst case, entire database instance). may adequateparticular domains, seem desirable outcome general case.Looking Definitions 4 5 see close relationshipconcepts incoherence inconsistency. fact, inferred definitionsincoherent KB induce inconsistent KB database instance containsset atoms relevant unsatisfiable sets TGDs. result capturedfollowing proposition (proofs results presented Appendix A).Proposition 1 KB incoherent exists relevantunsatisfiable set U KB = (D, ) inconsistent.instance relationship, consider following representative example.Example 6 (Relating Incoherence Inconsistency) Consider following ontology.KB =: {a1 : sing(simone), a2 : rock singer (axl ), a3 : sing loud (ronnie),a4 : fans(ronnie), a5 : rock singer (ronnie), a6 : rock singer (roy),a7 : manage(band1 , richard )}NC : {1 : sore throat(X) sing(X) ,2 : private life(X) famous(X) }E ::{1 : manage(X, ) manage(X, Z) = Z}{12345: rock singer (X) sing loud (X),: sing loud (X) sore throat(X),: fans(X) famous(X),: rock singer (X) sing(X),: fans(X) private life(X)}hinted previously Example 4, set = {rock singer (axl)}unsatisfiable set TGDs U = {1 : rock singer (X) sing loud (X), 2 :sing loud (X) sore throat(X), 4 : rock singer (X) sing(X)}. Since relevantU conditions Proposition 1 fulfilled, indeed ontology KB = (D, )inconsistent since 1 violated.set constraints one presented Example 6 may appear consider scenarios components ontology evolve (perhaps collaboratively623fiDeagustini, Martinez, Falappa & Simarimaintained pool users). long new constraints added, incoherence problemsmay arise. particular scenario would seem sensible identify modify,somehow, set incoherent constraints make satisfiable, instead deletinginformation ontology; proceed solve remaining inconsistencies, any. is, could beneficial define consolidation processeschanges performed achieve coherence given higher priority changes neededconsistency possible. address present twofold proposal consolidation Datalog ontologies: is, obtain new KB 0 begin addressing issuescomponent w.r.t. components E NC original ontology obtainnew coherent set constraints, giving TGDs necessary. Then, addressproblems arising component D, obtaining new one D0 consistent0T E NC . next section characterize, means set postulates,consolidation operator takes account considerations.4. Characterizing Consolidation: Postulates Datalog OntologyConsolidation OperatorsBelief Revision one main areas deals defined principled methods solveincoherences inconsistencies; explained Section 2, common characterizechange operators means postulates, properties operators must satisfy. section introduce set postulates objective characterizingconsolidation operators Datalog ontologies. start briefly defining scenariounderlying consolidation process introducing characteristics sets formulas focus (Friedman & Halpern, 2001).4.1 Defining Consolidation EnvironmentDepending type knowledge base, find two main streams work BeliefRevision. one hand, works based sets formulas closedconsequence relation, called belief sets (Alchourron et al., 1985). knownBelief Revision literature coherence model. hand, optionchoose belief bases (Katsuno & Mendelzon, 1991, 1992; Fuhrmann, 1991; Hansson, 1994,1997, 2001; Falappa, Kern-Isberner, & Simari, 2002), i.e., non-closed sets formulas;referred foundational model.Opposite traditional closed world assumption found established areaslike relational databases, one important characteristic Datalog open worldassumption, unknown data represented means null values. consequence,generation new information language application rules susceptibleinfinite (Cal, Gottlob, & Kifer, 2008, 2013), seems make foundationalmodel appealing choice working setting. Therefore, consolidationDatalog ontologies chosen follow foundational model. model,epistemic state (possibly incoherent inconsistent) Datalog ontology.624fiDatalog Ontology Consolidation4.2 Expected Properties Consolidation Operator: Postulatespresent set properties consolidation operator Datalog ontologiesmust satisfy. use following notation rest paper. Let KB = (D, )original Datalog ontology consolidated, = E NC . Also,KB ! denotes Datalog ontology KB ! = (D!, !) resulting consolidation KB ,D! ! consolidated components KB !, respectively.necessary differentiate KBs using subscripts. cases, given KB denoteconsolidation KB ! = (Di !, !).ready introduce Ontology Consolidation Postulates (OCP) expectedsatisfied consolidation operators. Let set Datalog ontologies.Then, Datalog ontology consolidation operator ! : function mustsatisfy following properties:OCP 1. (Inclusion) ! D! D.consolidation process includes resulting ontology formulas belonging original ontology.OCP 2. (Consistency) KB ! consistent.ontology obtained consolidation process must consistent, i.e.,negative constraints equality-generating dependenciesviolated apply TGDs ! atoms D!, thereforemods({D!, !}) 6= .OCP 3. (Coherence) KB ! coherent.ontology obtained consolidation process must coherent, i.e.,! must satisfiable respect NC E !.OCP 4. (Minimality): KB 0 KB coherent consistent, holdsKB ! 6 KB 0 .coherent consistent ontology obtained original ontology strictly contains consolidated ontology.postulates presented inspired properties proposed Hansson (1994) Konieczny Pino-Perez (2002). Nevertheless, adaptedsuit particularities ontological setting Datalog ; particular, takeaccount distinction incoherence inconsistency. instance, Inclusiondirect adaptation Hanssons homonymous postulate (Hansson, 1994), statescontraction knowledge base (not necessarily proper) subsetoriginal one. Consistency Coherence, hand, result adaptingsetting Konieczny Pino-Perezs postulate IC1 (2002), intuitively askresulting merging must consistent; ask resulting consolidationconsistent also coherent. Minimality postulate added ensurequality consolidation (w.r.t. loss information aspect), adaptedparticular work, rather general notion Belief Revision, noted625fiDeagustini, Martinez, Falappa & SimariHansson (2001) given many names conservatism (Harman, 2008), conservativity (Gardenfors, 1988), minimum mutilation (Quine, 1986) minimal change (Rott,1992).proposed postulates capture notion changes made respectoriginal ontology necessary, resulting ontology is, expected,coherent consistent. is, given original ontology consolidation processremoves constraints (TGDs) atoms somehow involved makingoriginal ontology incoherent/inconsistent, makes way unnecessaryremoval made.5. Datalog Ontology Consolidation Operatorprevious sections presented examples incoherences inconsistenciesarise Datalog ontologies. Additionally, stated properties consolidationoperator satisfy order make adequate changes original ontology regainingcoherence consistency. Now, propose construction consolidation operatoraddresses incoherence inconsistency problems Datalog ontologies.5.1 Possible Construction Consolidation Operatorliterature Belief Revision several constructions revision contraction operators studied. Hansson (1994) presents contraction operation beliefbases modeled means application incision functions. functionscontract belief base formula taking minimal sets entail (called -kernels)producing incisions sets longer entail . resulting belief baseconformed union formulas removed function.approach known kernel contraction; task restoring consistency also knownbelief revision literature contraction falsum (Hansson, 1991). work,define consolidation process application incision functions. Nevertheless,instead directly considering minimal inconsistent subsets formulas differentcomponents ontology (which equivalent -kernels), work perform incisions structures called clusters (Martinez, Pugliese, Simari, Subrahmanian, & Prade,2007; Lukasiewicz et al., 2012) groups together related kernels. specifically,solve incoherence begin establishing dependency kernels; analogous way,define data kernels solve inconsistencies w.r.t. , then, based them,obtain dependency clusters data clusters exploiting overlapping relation.5.1.1 Identifying Relation among Conflictsfirst step towards conflict resolution framework calculate minimal coherence consistency conflicts, identify possible relations among conflicts, any.Dependency kernels sets TGDs unsatisfiable w.r.t. set NCs EGDsDatalog ontology minimal set inclusion. sets known Minimal unsatisfiability-preserving sub-TBoxes (MUPS) Minimal incoherence-preservingsub-TBoxes (MIPS) (Schlobach & Cornet, 2003) DL community.626fiDatalog Ontology ConsolidationDefinition6 (Dependency Kernels) set dependency kernels KB , denotedQKB , set X X unsatisfiable set dependenciesw.r.t. NC E every proper subset X 0 X (X 0 ( X) satisfiable w.r.t. NC E .Example 7 (Dependency Kernels) Consider following sets constraintsDatalog ontology KB :NC : {1 : counselor (X ) regent(X ) ,2 : cannot rule(X ) heir (X ) }E : {1 : advise(X , K ) advise(X , K 0 ) K = K 0 }: {1 : advise(X , K ) counselor (X ),KB =2 : propose law (X , K ) regent(X ),3 : prince(P ) heir (P ),4 : son(P , K ) king(K ) prince(P ),5 : counselor (C ) regent(C ),6 : bastard son(X , ) son(X , ),7 : bastard son(X , K ) king(K ) cannot rule(X )}KB exist two dependency kernels, i.e.,QKB= {{3 , 4 , 6 , 7 }, {5 }}.easy show dependency kernels Datalog ontology independentparticular component ontology, thus obtained lookingcomponent . is, even replace component ontologyempty set atoms, dependency kernels ontology empty databaseoriginal one.Lemma 1 Let KBQ 1 = (DQ1 , 1 ) KB 2 = (, 2 ) two Datalog ontologies1 = 2 . Then, KB 1 = KB 2 .addition removal TGDs make set unsatisfiable (thus makingontology incoherent), solve inconsistencies may need remove atoms components order address data inconsistency well. Analogously definitiondependency kernels, define data kernels minimal subset atomsmakes KB = (D, ) inconsistent.`Definition 7 (Data Kernels) set data kernels KB , denoted KB , setX mods(X, ) = every X 0 ( X holds mods(X 0 , ) 6= .627fiDeagustini, Martinez, Falappa & SimariExample 8 (Data Kernels) Consider following coherent inconsistent KB , pro-posed Lukasiewicz et al. (2012).KB =: {directs(john, d1 ), directs(tom, d1 ), directs(tom, d2 ),supervises(tom, john), works in(john, d1 ), works in(tom, d1 )}NC : {supervises(X , ) manager (Y ) ,supervises(X , ) works in(X , D) directs(Y , D) }E ::{directs(X , D) directs(X , 0 ) = 0 }{1 : works in(X , D) employee(X ),2 : directs(X , D) employee(X ),3 : directs(X , D) works in(X , D) manager (X )}KB , set data kernels`KB{supervises(tom, john), directs(john, d1 ), works in(john, d1 )},{supervises(tom, john), directs(john, d1 ), works in(tom, d1 )},={directs(tom, d1 ), directs(tom, d2 )}know minimal conflicts ontology identify relations among them,relation exists. this, group related kernels together new structure calledcluster, makes possible achieve optimal solution related kernels. Clustersobtained overlapping relation defined follows.Definition 8 (Overlapping, Equivalence) Let L first order language, R Lrelational schema, LR sublanguagegenerated R. Given LR B LR ,say overlap, denoted B, iff B 6= . Furthermore, given multi-set firstequivalence relation obtainedorder formulas 2LR denotereflexive transitive closure .exploiting overlapping among dependency kernels data kernels definedependency clusters data clusters, respectively.QDefinition 9 (Dependency Clusters) Let KBset Dependency KernelsQKB . Let overlapping relation, K = KB /Qquotient set equivKBQalence relation obtainedQQover KB . Constraint Cluster set C = [] ,[] K. denote KB set Constraint Clusters KB .`Definition 10 (Data Clusters) `Let KB set Data Kernels KB . Letoverlapping relation, K = KB /`quotient set equivalence relationKB`obtainedKB . Data Cluster set C = [] , [] K. denote``set Data Clusters KB .KBIntuitively, dependency cluster groups dependency kernels TGDcommon, transitive fashion; data clusters groups data kernels analogous way.628fiDatalog Ontology ConsolidationExample 9 (Dependency Clusters Data Clusters) Assume KBQ`= {{1 , 2 }, {1 , 3 }, {4 , 5 }} KB = {{a1 , a2 }, {a1 , a3 }, {a2 , a4 , a5 }}. Then,two dependency clusters based kernels, grouping first two kernels (due1 ) remaining kernel another cluster; i.e.,KBQQKB= {{1 , 2 , 3 }, {4 , 5 }}.hand, case data clusters``KB= {{a1 , a2 , a3 , a4 , a5 }}.following proposition states that, since clusters based equivalence classes, everykernel included one one cluster.QQQPropositionQ2 KB X some`X KB * X 0Q0X 0 . Analogously,KB X````X KB X 6=00X KB * X X KB X 6= X 0 .corollary Proposition 2 formula kernel includedone cluster.QCorollary 1 (CorollaryProposition2)LetKB`QQ00forQX X KB/ X 0``allQsome KB . Then,000XXX KB X 6= X . Analogously,KB``/ X 0 X 0 KB X 6= X 0 .following lemma shall use paper shows example how,ontological setting Datalog , Leibnizs indiscernibility identicals (von Leibniz,1976) holds w.r.t. clusters Datalog ontologies, two KBs equivalentset clusters.ontologies KB = KB . Then,Lemma2 Let KB 1QKBQ2 two Datalog12````QQ==.KB 1KB 2KB 1KB 25.1.2 Solving Conflicts: Incision Functionsidentified clusters, establish incoherences inconsistencies solved. incision function selects formulas deleteddata dependency clusters.Definition 11 (General Incision Function) General Incision Function KBfunction : (2LR , 2LR ) 2LR following conditions holds:SQQS``1. (KB ) ( KB ) ( KB ).QQQ2. X KB KB X holds (Y (KB )) 6= .```3. X KB KB X holds (Y (KB )) 6= .629fiDeagustini, Martinez, Falappa & SimariQQ4. X KB holds = (X (KB )) exists R XR satisfies conditions 1 2, R ( .``5. X KB holds = (X (KB )) exists R XR satisfies conditions 1 3, R ( .Definition 11 states general incision function selects dependency (data,respectively) cluster TGDs (atoms, respectively) deletion order restore coherence(consistency). incision function complies Definition 11 used baseconsolidation operator. However, note operator may differentiaterestoring coherence consistency. problem classic literatureBelief Revision since notion incoherence, distinctionrules facts languages like propositional logic; thus, consistency conflictsappear, avoiding need treat incoherences. Nevertheless, ontological settingDatalog opportunity exploiting fact two different althoughrelated kinds conflicts address separately goal finding solutionbetter suits needs applications rely kind knowledge bases.point paper trying make that, knowledge bases formDatalog ontologies important differentiate, adequately handle, incoherenceinconsistency quality consolidated ontology heavily dependsassuming strive minimal loss process. This, complex setting, needscareful definition constitutes kernel. see could happen doneproperly, consider following example.Example 10 (Influence Incoherence Consolidation) Consider KB Exam-ple 6. = 2T 2E 2NC 2T unsatisfiable. explained Example 6, singleton set {rock singer (axl)} NC 1 : sore throat(X)sing(X) violated, making {rock singer (axl)} inconsistent . Then,{rock singer (axl)} data kernel (and data cluster, since cannot overlapkernel) verifiable every singleton set relevant dependency cluster. Thus,``KB{rock singer (axl)},{rock singer (ronnie)},={rock singer (roy)},{has fans(ronnie)}Consider cluster {rock singer (axl)}; cluster({rock singer (axl)}) = rock singer (axl).````S``situation holds every cluster KB , thus ( KB ) = ( KB ).problem example data kernels (and hence data clusters)computed w.r.t. original component, which, case, contain unsatisfiable setsconstraints. seen Example 10, becomes utter importanceatoms relevant unsatisfiable sets: case, general incision function (andinconsistency management technique based deletion treat incoherenceconflicts) necessarily delete atoms.630fiDatalog Ontology ConsolidationProposition 3 Let general incision function. relevant X(KB ).QKBClearly, corollary Proposition 3 every atom relevantunsatisfiable set remove every atom restore consistency.Corollary 2 (Corollary Proposition 3) LetQ general incision function.holds relevant X KB (KB ).seen, incoherence great influence consolidation treated properly(that is, previously consistency restoration). would seem better computedata clusters based retained satisfiable part components. Lemma 1show dependency kernels obtained independently componentoriginal ontology, unsatisfiable sets violate negativeconstraint equality-generatingdependency relevant set atoms. Therefore,QQfirst obtain,useincision functionKB`` dependency clusters selectTGDs deleted.QQThen, calculate KB based result applicationincision function KB , way paying attention constraintsprevail consolidation process.Next, define constraint incision functions data incision functionsused select candidates deletion (from original ontology) restore coherenceconsistency, respectively. First, define incision function dependency clustershelps solve incoherence constraints.Definition 12 (Constraints Incision Function) Constraint Incision Function KBfunction : (2LR , 2LR ) 2LR following conditions hold:SQQ1. (KB ) ( KB ).QQQ2. X KB KB X holds (Y (KB )) 6= .QQ3. X KB holds = (X (KB )) exists R XR satisfies conditions 1 2, R ( .Intuitively, constraint incision function takes dependency clusters removes TGDsway resulting KB coherent. Analogouslyconstraintsincision functions, define data incision functions solve inconsistencies``.KBDefinition 13 (Data Incision Function) Data Incision Function function% : (2LR , 2LR ) 2LR following conditions hold:S``%(KB ) ( KB ).```X KB KB X holds (Y %(KB )) 6= .``X KB holds = (X %(KB )) exists R XR satisfies conditions 13 13, R ( .631fiDeagustini, Martinez, Falappa & SimariFinally, necessary make significant remark regarding usage incisionfunctions. that, let us first consider following excerpt quoted Hanssons (2001,cf. p. 122) regarding possible parameters passed selection functions (whichcase incision functions) choice affects possible outcomes.[. . . ] proof uniformity makes essential use fact selection functions defined remainder sets form A, pairsform hA, i. instead defined selection functions follows:(A, ) non-empty subset (A, ) non-empty.(A, ) = {A} empty.(A, ) would operation similar partial meet contraction respects, would possible (A, ) 6= (A, )hold = A, standard definition allow [. . . ]Thus, extending Hanssons observation incision functions use consolidation,take sets conflicts arguments incisions formulasremoved two different ontologies set conflicts operatorusing incision function identical. reason operator couldtell difference ontologies since parameter conflicts,exactly same. However, chosen restrict family operatorsbehaviors; instead, model operators whose behavior could select removalformula equal conflicts, restricted choice. achievethis, chosen take ontologies parameters; so, fits application domainoperators exploited, formulas conflict could affectoutcome consolidation.approach presented here, incision function consider TGDseffect cluster, global effect whole knowledge base. reasonrequirement unlike classic models belief revision, language usedgreater expressivity fact TGD generates multiple inferences. instance,framework TGD form XY(X, Y) Z(X, Z) possibleinfer multiple instances (X, Z).see reason behind choice clearly consider following example.Example 11 Consider following ontologies.: {p(a), q(a)}NC : {p(X ) r (X ) }KB 1 =: {1 : q(X ) r (X )}KB 2 =: {p(a), q(a)}NC : {p(X ) r (X ) }{1 : q(X ) r (X ),2 : p(X ) s(X ),3 : p(X ) t(X )}:KB , set data clusters equal,632fiDatalog Ontology Consolidation``KB 1=``KB 2={p(a), q(a)}use standard approach take clusters arguments incisions, mustremove formula ontologies, explained incisionfunction therefore cannot choose differently argument.Nevertheless, suppose particular scenario want remove atoms basedinformation help infer. case, KB 1 removep(a), KB 2 take q(a), since KB 2 formula p(a) triggersTGDs, thus inferring atoms. achieve type behavior, necessary passontologies parameters, since provides adequate context.5.1.3 Cluster Contraction-Based Consolidation OperatorLastly, define consolidation operator Datalog ontologies represents twodifferent parts consolidation. First, coherence restoration componentobtained based dependency clusters component original ontology.Second, restoration consistency component obtained based dataclusters w.r.t. ! component obtained applying constraint incision functionoriginal . way achieve behavior stated earlier paper; sense,give incoherence resolution higher priority, since retain atoms addressingunsatisfiable sets TGDs instead, choose follow path. cluster contractionbased consolidation operator formally defined follows:Definition 14 (Cluster Contraction-based Consolidation Operator)Let KB Datalog ontology, Constraint Incision Function % Data IncisionFunction. Also, let KB ? = (D, \ (KB )) Datalog ontology resulting deletingKB TGDs selected . Cluster contraction-based consolidation operatorKB !, defined follows:KB ! = (D \ %(KB ? ), \ (KB ))result KBQ! Datalog ontology obtained removing,first, TGDs``Q(selected) atoms (selected %) originalKB ?KBontology KB . important note that, one hand TGDs removed, dependency clusters contain EGDs NCs. hand, DataIncision Function uses KB ? instead KB atoms conflicts\ (KB ) removed; data clusters calculated based constrainsobtained consolidation .5.2 Relation Postulates Construction: Representation TheoremSection 4 introduced properties Datalog consolidation operatormust satisfy. means following representation theorem establishrelationship set postulates Datalog ontology consolidation operatorcluster contraction-based consolidation operator proposed previoussection. follows denote ! consolidation operator defined Definition 14% correspond arbitrary constraint data incision functions, respectively.633fiDeagustini, Martinez, Falappa & SimariTheorem 1 (Representation Theorem) operator consolidation ! ClusterContraction-based Datalog Ontology Consolidation Operator Datalog ontology KBiff satisfies Inclusion, Coherence, Consistency, Minimality.6. Complete Example Datalog Ontologies Consolidationintroduced operator allows us consolidate Datalog ontologiessatisfies set expected properties expressed postulates Section 4.section, complete process consolidation Datalog ontologies depictedfollowing example.Example 12 (Consolidation Datalog Ontologies) Suppose (in-coherent inconsistent) ontology KB shown Figure 1, expresses informationcollected certain company.D:NC :E :KB =:{a1a3a5a7a8a9: boss(walter ), a2 : supervises(walter , jesse),: makes decisions(walter ),a4 : makes decisions(jesse),: supervises(skyler , walter ), a6 : employee(walter ),: charge (jesse, distribution),: charge (walter , cooking),: strike(mike)}{1 : follows orders(X ) makes decisions(X ) ,2 : supervises(Y , X ) supervisor (X ) ,3 : absent(X ) strike(X ) }{1 : charge (X , ) charge (X , 0 ) = 0 }{1 : employee(X ) supervised (X ),2 : supervised (X ) follows orders(X ),3 : boss(X ) makes profit(X ),4 : supervises(Y , X ) supervisor (Y ),5 : supervises(Y , X ) employee(X ),6 : supervised (X ) makes decisions(X ),7 : supervised (X ) work (X ),8 : work (X ) get paid (X ),9 : work (X ) charge (X , ),10 : strike(X ) absent(X )}Figure 1: original ontology consolidated.Now, begin first part consolidation process (i.e., solving incoherences making set satisfiable) obtain, first step towards obtainingdependency clusters, dependency kernels KB :QKB= {{2 , 6 }, {10 }},based kernels, calculate set dependency clusters KBQQ= {{2 , 6 }, {10 }}.KB634fiDatalog Ontology ConsolidationQQQNote that, overlap among dependency kernels, KB = KB . Next,use cluster incision function solve incoherency problems. sake exampleassume guide contraction process means quantitative criterion, i.e.,choosing among possible incisions ones removes fewer formulas, usingplausibility among formulas cardinality incisions same.following show possible incisions, i.e., satisfying conditions Definition 12.setscluster {2 , 6 } could either remove 2 6 . Since two incisions removenumber atoms assume example 2 plausible 6 ,thus prefer retain former.cluster {10 } remove 10 .Then, particular incision example follows:({2 , 6 }) = {6 }({10 }) = {10 }Now, move next part consolidation process: consistency recovery.explained before, part operator considers TGDs effectivelyincluded consolidation. particular example ! = \ {6 , 10 }.let KB ? = (D, !); based KB ? calculate data kernels.Q= {{a2 , a4 }, {a3 , a5 }, {a3 , a6 }, {a2 , a5 }}KB ?Then, obtain data clusters, are:QQ= {{a2 , a3 , a4 , a5 , a6 }}KB ?Now, solve inconsistencies need consider sets intersectionkernels included clusters empty, using ! insteadthis. again, analyze possible incisions (the sets respecting conditionsDefinition 13) light number atoms deleted plausibility formulasthem. different possible incisions cluster are:- remove {a2 , a3 }.- remove {a2 , a3 , a6 }.- remove {a2 , a5 , a6 }.- remove {a4 , a5 , a6 }.again, sets presented removal induce operatorsatisfies postulates, thus captured framework. Nonetheless, explainedexample choose remove atoms possible. is,choose remove {a2 , a3 }),%({{a2 , a3 , a4 , a5 , a6 }}) = {a2 , a3 })Then, using Datalog ontology consolidation operator based contractionclusters like one introduced Definition 14 obtain coherent consistentontology shown Figure 2.635fiDeagustini, Martinez, Falappa & SimariKB ! =D! : {boss(walter ), makes decisions(jesse),supervises(skyler , walter ), employee(walter ),charge (jesse, distribution),charge (walter , cooking),strike(mike)}NC ! : {follows orders(X ) makes decisions(X ) ,supervises(Y , X ) supervisor (X ) ,absent(X ) strike(X ) }E ! :! :{in charge (X , ) charge (X , 0 ) = 0 }{employee(X ) supervised (X ),supervised (X ) follows orders(X ),boss(X ) makes profit(X ),supervises(Y , X ) supervisor (Y ),supervises(Y , X ) employee(X ),supervised (X ) work (X ),work (X ) get paid (X ),work (X ) charge (X , )}Figure 2: ontology resulting consolidation.7. Related Workclosely related work work Croitoru Rodriguez (2015).work authors present consolidation operators used basis definitionsemantics inconsistency tolerant ontology query answering Datalog+ (aexpressive language Datalog , Cal et al., 2012). case work,work Croitoru Rodriguez (2015) based use Hanssons incision functions(Hansson, 1994) solve conflicts. Nevertheless, remarkable differencesworks well. Among important ones operators presentedCroitoru Rodriguez deal inconsistent ontologies, acknowledgmentincoherence problem made. shown work,significant impact quality consolidation analysed respect minimalloss information. Moreover, fact makes that, even though set postulatesworks similar spirit, family operators characterized CroitoruRodriguez subset ones characterized here. due fact settingconsider (i.e., inconsistent incoherent ontologies) general,since instance operators remove facts also TGDs, CroitoruRodriguezs operators since focus inconsistency.Another closely related work one Lukasiewicz et al. (2012). There, authors define general framework inconsistency-tolerant query answering Datalogontologies based notion incision functions. Nevertheless, work focusedenforcing consistency query time obtaining (lazy) consistent answers inconsistent ontology instead consolidating one. Clearly, process must carriedevery query posed system, approach obtain new knowledge base636fiDatalog Ontology Consolidationoffline manner, knowledge base queried without considering inconsistency issues; approaches prove useful, depending application domain.Additionally, one KB used rational assumption conflictsconstraints also made, therefore notions unsatisfiabilityincoherence. stated before, order gain generality chosen dropassumption, treat incoherence problems well inconsistency ones. additionworks Croitoru Rodriguez Lukasiewicz et al., several workssolve inconsistency incoherence means adapting approaches based BeliefRevision techniques knowledge representation formalism.7.1 Propositional Knowledge Basesnumerous works revision merging propositional knowledge bases (see,instance, Konieczny & Perez, 2002; Katsuno & Mendelzon, 1992; Lin & Mendelzon, 1999;Liberatore & Schaerf, 1998; Everaere, Konieczny, & Marquis, 2008; Konieczny & Perez,2011; Delgrande, Dubois, & Lang, 2006; Booth, Meyer, Varzinczak, & Wassermann, 2010;Delgrande, 2011; Delgrande & Jin, 2012; Falappa, Kern-Isberner, Reis, & Simari, 2012),provided foundations work (fragments of) first order logics.expected, works deep connections ours, also remarkabledifferences, shall see.mentioned throughout paper, work Sven Ove Hansson (1994)provides inspiration foundations work: follow approach akin KernelContraction several intuitions it, adapted ontological language, Datalog .consequence, besides treating incoherence also provide complete inconsistencyresolution process takes advantage ontological setting, exploiting relationcomponents ontology define coherence consistencyrestored. Also, classic incision functions introduced Hansson produce incisionminimal conflicts. approach, however, work clusters, groupingskernels, thus always minimal. Then, propose particularizationHanssons incision functions, focusing incision functions successfully workclusters.Konieczny Pino-Perez (2002) made one main contributions mergingconflicting information. work follow intuitions proposed them.Nevertheless, main difference approach (besides obvious oneaims works, merging vs. consolidation) state final mergingconsistent presence consistent (or, terminology, coherent) setintegrity constraints, analyze alternative case.respect work Lin Mendelzon (1999), besides differencefocus (once merging vs. consolidation), main difference inconsistencymanagement strategy chosen work conflict solving strategy reliesvotes majority establish formulas retained merging. Instead,chosen introduce particular strategy. Nevertheless, possible adaptframework use preference relations choose possible incisions (insimilar way shown Example 12). relations indeed designedcomply majority intuition (providing votes,637fiDeagustini, Martinez, Falappa & Simariapply ontology consolidation environment since one ontology), thusobtaining similar strategy.work Katsuno Mendelzon (1992) problem knowledge base revisionpropositional case addressed. approach, language usedexpress facts world constraints imposed KB .Nevertheless, difference (in case) update KBconsolidation KB arises treatment integrity constraints: workintegrity constraints considered invariant updates restore consistencyrestricted facts.works Delgrande (2011), Delgrande Jin (2012) authors presentapproach revising propositional knowledge base set sentences, everysentence set independently accepted inconsistenciesconsidering whole set. main idea follows AGM theory, differsthat, necessary alter Success postulate suits intuition everysentence set final revision (since set inconsistent).Guided principle informational economy, characterize revisionplausible worlds among various maximally consistent subsets set sentences.parallel Datalog ontology environment, revising componentontology solve inconsistencies. set sentences inconsistent, unionoriginal KB inconsistent. Nonetheless, important differenceworks ours. works authors first solve inconsistenciesset sentences, decide subset characterize revision.approach different, directly consider inconsistent KB . Then, order solveproblem setting, necessary consider union KBentire set sentences, apply consolidation operator.7.2 Knowledge Expressed Description Logics Ontologies, Logic ProgramsRelational Databasesfocus knowledge representation formalisms closely relatedDatalog , mainly family Description Logics (Baader, Calvanese, McGuinness,Nardi, & Patel-Schneider, 2003) Logic Programming (Lloyd, 1987; Nilsson & Maluszynski, 1995; Gelfond, 2008). remarkable work using belief revision solve conflicts DLsone Qi, Liu, Bell (2006), based AGM theory (Alchourronet al., 1985; Gardenfors, 1988). makes work standintroduce generalizations AGM postulates case DLs, also define twooperators knowledge bases, based formulas weakening, satisfy postulates.main difference approach take account consistency problems ontologies, incoherence treatment provided. pointed earlier,incoherence lead extreme weakening information, maytake every individual name general concept inclusion.previously mentioned, notion incoherence inspired SchlobachCornets work (2003), among others. paper authors focus definitionprocesses capable detecting unsatisfiabilities incoherences DLs ontologies, introducing complete algorithms along empirical analysis approach. Nevertheless,638fiDatalog Ontology Consolidationmain focus work, authors set aside issue recovercoherence conflict detected, also consider inconsistencies.work presented consolidation process treats incoherence inconsistency,based use Belief Revision techniques. Thus, approach presented SchlobachCornet could potentially useful regarding implementation operators presented work, providing effective way obtaining set kernelsset clusters based.Black et al. (2009) propose approach capable using information comingseveral DL ontologies order answer queries, taking care process incoherence inconsistency. approach based agents argumentative capabilities,one personal knowledge base form DL ontology. agents usedialogue games interchange arguments reach agreement answercertain query. Thus, agents use (possible incoherent/inconsistent) unionontologies without merging them, still obtain answer influenced everyontology play. Moreover, approach advantage information lost,formula deleted ontologies, result inferences obtainedapproach superset obtained ontology resultingconsolidation union DL ontologies. Even though authors argue oneadvantage proposed approach need waste time effortperforming consolidation KB , one disadvantage computational complexityassociated argumentative reasoning (Parsons, Wooldridge, & Amgoud, 2003; Dunne &Wooldridge, 2009; Cecchi, Fillottrani, & Simari, 2006) process conductedquery issued online manner. Even though consolidation processalso computationally expensive, necessary performdone offline query answering system becomes available. choice one approach depends highly environment used, i.e.,size ontologies used, often updates issued KBcritical time consumption system, among considerations; courseset inferences obtained every approach may differalso taken account. consolidation-based approach could suitabletime-dependant systems like real-time systems query intensive systems datatractability associated (a consolidated) Datalog ontology may proven handy.Another work worth mentioning Kalyanpur, Parsia, Horridge, Sirins(2007). work verses find justifications entailments DescriptionLogics ontology. justification simply precise set axioms ontology responsibleparticular entailment (Kalyanpur et al., 2007). words, minimal setaxioms sufficient produce entailment, related use kernels meanobtain clusters part consolidation strategy used. Moreover, Horridge, Parsia,Sattler (2009) state justifications important repairing inconsistent ontologies.Thus, could important definition consolidation processes similarcluster-based consolidation, least one axioms justificationsentailment removed ontology, corresponding entailment longerholds.(Kalyanpur et al., 2007, p. 269). One main contributions workdefinition practical black-box (i.e.,, reasoner independent) techniques allows usfind justifications entailments ontology efficient way. such, evident639fiDeagustini, Martinez, Falappa & Simariwork verses different direction still benefit findings.particular, may possible use developed algorithms part implementationstrategy consolidation operators, adapting used Datalogdual incoherence/inconsistency setting.Regarding Logic Programming, also several works address problemmerging knowledge bases expressed logic programs, solving inconsistency issuesprocess. instance, Hue, Papini, Wurbel (2009) introduce merging processbased stable model semantics, using logic Here-and-There (Turner, 2003). Hueet al. consider merging strategy based pre-orders among deletion candidates calledpotential removed sets establish particular way obtain preorders. Instead, assume strategy P given pre-order definesP . case Lin Mendelzons work (1999), although falls scopepresent work certainly adapt framework use similar techniqueschoosing incision prevails.Another notorious work Logic Programming field one Delgrande, Schaub,Tompits, Woltran (2009). work two different approaches proposed. firstone follows arbitration approach, selecting models program differs leastw.r.t. models programs. work case unsatisfiable programsstudied, similar way consider incoherence leaded unsatisfiable sets TGDs.Nevertheless, consider unsatisfiability certain program, conceptunion programs. Furthermore, strategy solve unsatisfiability simplyleaving unsatisfiable program consideration merging, instead tryingsolve conflict somehow. second approach based selection modelsspecial program P0 , thought constraints guiding merging process,least variations w.r.t. programs merging. approachseen particular instance approach proposed Konieczny Perez (2002).area databases, one influential works one Arenas et al.(1999) Consistent Query Answering, authors propose model theoretic definition consistent answers query relational database potentially inconsistentset integrity constraints. Intuitively, consistent answers query set atoms(classical) answers query every repair inconsistent database;repair set atoms satisfy set constraints close possibleoriginal database. Different notions repairs studied literature, welldifferent notions means set atoms close possible originaldatabase. proposals based repairing inserting and/or deleting tuplesto/from database (actually, possible actions depend form integrityconstraints expressiveness) notion closeness defined via set inclusioncardinality. work Arieli, Denecker, Bruynooghe (2007), however, proposesuniform framework representing implementing different approaches databaserepairing based minimizing domain dependent distances. main idea workshow thinking terms (different) distances express preferences among repairsleads different preferences applied different scenarios. authors showset repairs obtained using proposed distance functions deviateobtained using set-inclusion. Furthermore, besides insertion deletion entire tuples several domain independent approaches, e.g., based cardinality640fiDatalog Ontology Consolidationcomplex objective functions. approach proposed Wijsen (2005) updatesconsidered primitive theoretical framework; Bohannon et al. (2005) presentcost-based framework allows finding good repairs databases exhibit inconsistencies form violations either functional inclusion dependencies, allowingalso updates attribute values. work, two heuristics defining constructing repairs based equivalence classes attribute values; algorithms presentedbased greedy selection least repair cost, number performance optimizations also explored. quite different semantics repairing proposed Caroprese,Greco, Zumpano (2009), Caroprese Truszczynski (2011) Active IntegrityConstraints (AICs short); AIC production rule body conjunctionliterals, false database consistent, whereas headdisjunction update atoms performed body true (that constraint violated). Repairs defined minimal sets (under set inclusion) updateactions (tuple deletions/insertions) AICs specify set update actions usedrestore data consistency. Hence, among set possible repairs, subsetfounded repairs consisting update actions supported AICs considered.works area propose different semantics repairing either explicitly implicitlyconsidering preference relation among set repairs (cf. Andritsos, Fuxman, & Miller,2006; Staworko, Chomicki, & Marcinkowski, 2012; Greco & Molinaro, 2012).recently, area ontology-based data access (OBDA), Lembo et al. (2010)study adaptation CQA DL-Lite ontologies, called AR (ABox semantics).work, also intersection (IAR) semantics presented sound approximation consistent answers; semantics consists computing intersection repairs answersobtained there, though (possibly many) AR answers cannot obtained IAR semantics, latter computationally easy obtain DL-Lite family,i.e., necessary compute whole set repairs order compute intersection. data combined complexity semantics studied (Rosati,2011) wider spectrum DLs. Also, Rosati (2011) presents intractability resultsquery answering EL intersection semantics, non-recursive segmentlanguage proved computable polynomial time. recently, BienvenuRosati (2013) propose another family approximations CQA, also DL-Litefamily. k-support semantics allows (soundly) approximate set queries entailedCQA semantics, based k subsets database consistently entail q;hand, k-defeater semantics approximates complete approximations seekingsets contradict supporters q. semantics FO-rewritable ontological language standard CQ answering FO-rewritable well, usedconjunction over- under-approximate consistent answers.Much like Black et al. (2009), treatment inconsistencies proposedsemantics related particular queries instead inconsistency whole database.Thus, attempt obtain final consistent database queried withoutconsidering restrictions. Furthermore, address issues incoherenceinconsistency together; instead approaches either assume set integrityconstraint correctly defines semantics database instance, roomincoherence, treat constraints data alike moment removing ignoringinformation, leads type problems discuss Example 10.641fiDeagustini, Martinez, Falappa & Simaritechniques may suitable case one single database, presenceincoherence set ICs, case consider several databases together,approach would lead meaningless empty answers, since subset databasecould satisfy constraints would also case approach Lukasiewiczet al. (2012).Also related databases field work Lin Mendelzon (1998). There,database viewed first-order theory without rules, ICs used ensureconsistency final result work Konieczny Perez (2002), presenting wayssolve known database merging problems like synonyms homonyms. Nonetheless, likeKonieczny Pino-Perezs work, consider problems related set ICs.Instead, set ICs used merging process unique, choice setexpected performed merge designer. Unlike Lin Mendelzon, madeassumption consolidation environment set ICs conflict-free.Cholvy (1998) introduces another approach used reason contradictory information. framework represented set axioms inferencerules. Additionally, paper several applications framework introduced,e.g., solving conflicts among beliefs represented first order databases, factsground literals rules integrity constraints deduction rules.scenario, contradiction obtained application constraints considering several databases together. establishes certain parallel caseinconsistency Datalog ontology. However, main difference work liesstrategy inconsistency management process defined. work,preference order databases assumed. Instead, chosen restrictachieve consolidation, thus presenting general approach. Nevertheless, statedadapt incision functions suit intuition every formula equallydesirable, choosing instance preferences ontologies guideline (if usingapproach tasks rather consolidation single ontology), obtaininginconsistency management strategy akin one introduced Cholvy.Finally, Meyer, Lee, Booth (2005) use two well-known techniques knowledgeintegration propositional case, adapted refined expressiveness DLs.proposed approach takes knowledge bases produces disjunctive knowledgebase (DKB) result integration. One disadvantage DKBs statepossible options take conflicting knowledge expected exploitedreasoning process rather choosing one them. Thus, contrary approachfinal consolidated ontology given, definitive final merging; moreover,set aside research problems related incoherence integration process.8. Conclusions Future WorkCollaborative work information exchange becoming key aspects almost system; thus, uttermost importance automatic adequate ways solveconflicts: knowledge evolves collaborative environment incoherence inconsistency prone arise. knowledge often represented ontologiescollaboratively built, often shared among entities use modify them. One particular way deal conflicts appear application environments642fiDatalog Ontology Consolidationtry modify information contained ontology order regain coherenceconsistency. paper shown achieve consolidation Datalogontologies. introduced concept incoherence Datalog ontologies termsunsatisfiability sets TGDs, showed relationship classical notioninconsistency logical theory lacks models.also proposed construction consolidation operators. construction inspired kernel contraction, uses incision functions groupings minimal unsatisfiable/inconsistent sets called clusters solve conflicts. Finally, stated propertiesDatalog ontology consolidation operator expected satisfy. showedoperators satisfy respective properties, obtaining result consolidationnew Datalog ontology always coherent consistent minimizing changesmade conflict resolution.final remark, notice operators take care incoherences ontology. However, rare cases ontology designer introduce unsatisfiableconcepts ontology purpose, model particular feature application domain. case remove incoherence, ratherdelete atoms triggering it, any. Clearly, since defined settingmind behavior cannot achieved operators presented here. Nevertheless,modify present approach suit setting almost straightforward, provideidentify whether unsatisfiable set TGDs made purpose not.future work, intend study new constructions Datalog consolidationoperators. this, first plan change general approach, i.e., operators basedformalisms kernel contraction, mainly AGM theory (Alchourron& Makinson, 1985; Alchourron et al., 1985); then, proposed frameworkcluster contraction based consolidation operators fully constructive, dependingapplication domain may certainly difficult asses effect incisions, i.e.,may hard decide among family possible incisions one select.design point view, may easier select perform consolidationadditional information formulas knowledge base,preference relation can, example, elicited domain experts. general,could easier expert provide guidelines information applicationdomain hand could modeled preference relation formulasontology rather trying single desired incisions. direction wantexplore constructions based exploiting preference relations among formulasontologies define different strategies choose formulas delete, possibly tailoredparticular scenarios. Mainly, plan analyze two different aspects: relationoperators based preference relations respect ones presentedwork, different strategies affect behavior.Also, work make point differentiating concept inconsistencyincoherence; therefore, need focus languages separate extensionalintensional knowledge, otherwise two notions indistinguishable (ascase propositional logic). sense, choice Datalog due desirableproperty generalizing several popular languages classical Datalog, DL-Lite, ELH,F-Logic-Lite, etc. Even though paper perform particular analysiseffects nulls proposed solutions consolidation, Datalog family languages643fiDeagustini, Martinez, Falappa & Simarichosen offers wide variety languages high computational tractability(some FO rewritable others PTIME inference algorithms). resultswork pave way continue research line next natural step, show(or whether) different syntactic semantic properties yield tractabilityquery answering allow us obtain tractability results also consolidation problem,much way happened already area consistent query answering(where repairs extensional part KB considered). is, example,rewriting algorithms capability value invention plays important role:value invention process controlled (in general syntactic restrictions) orderkeep low complexity reasoning tasks. mind, futurelook role processes like value invention consolidation Datalogontologies, impact conflicts solved computationalefficiency.currently working implementation operators; plan study different techniques used order produce efficient implementation, possiblytailored specific fragments Datalog . explained before, algorithms introducedSchlobach Cornet (2003) proven useful regarding aspect since mayprovide way calculate kernels Datalog ontology, thus providing first steptowards incoherence resolution. Another important work regarding implementationconsolidation operators one Wassermann (2000), author showsminimal incision functions knowledge base obtained kernels KBusing algorithm finding minimal hitting sets (Reiter, 1987). Several worksarea ontology debugging repairs, (e.g., Halaschek-Wiener & Katz, 2006, Horridgeet al., 2009 way find justifications inconsistency) exploited Reitersalgorithms order implement frameworks. Among others, plan studyadequate techniques operators, almost direct relationminimal incision functions Reiters minimal hitting sets; way, may possible adapt Reiter techniques attend incoherences inconsistencies, moreover,already discussed, plan analyze relation cluster incision functionspreference relations. Regarding implementation, hold conjecture relationsexploited refine implementation operators: Reiters algorithmbased expansion directed acyclic graph, expansion madebreadth first fashion, end generates possible values minimal incisionfunctions. acknowledged Wassermann, kind ordering among formulaspresent, ordering used choose branch expand; words,may possible implement construction operators proposed workmeans exploiting Reiters hitting sets algorithm, also use preferencerelation equivalent incision (if any) guide consolidation process. is,may possible adapt algorithm chooses expand branch lesspreferred set formulas, thus guiding graph expansion process.Appendix A. ProofsProof Proposition 1644fiDatalog Ontology ConsolidationProof Consider U U unsatisfiable set dependencies w.r.t.NC E , set atoms relevant U .follows definition satisfiability set dependencies w.r.t. setconstraints U unsatisfiable exist relevant set atoms A0makes mods(A0 , U E NC ) 6= , otherwise U satisfiable.Then, mods(A, U E NC ) = . Moreover, since Uchase(A, U ) chase(D, ), thus NC EGD violated chase(A, U )also violated chase(D, ). Thus, mods(D, E NC ) = , i.e., KB inconsistent.Proof Lemma 1Proof Let KB 1 = (D1 , 1 ) 1 = 1T 1E 1NC , KB 2 = Q(, 2 )2 = 2T 2E 2NC twoQDatalog ontologies 1 = 2 , KB 1dependency kernels KB 1 , KB 2 dependency kernels KB 2 , respectively.QConsider X KB 1 . Then, Definition 6 X 1T unsatisfiableset dependencies w.r.t. 1E 1NC every X 0 ( X satisfiable w.r.t. 1E 1NC .Since 1 = 2 , 1T = 2T , 1E = 2E 1NC = 2NC , thus holdsX 2T unsatisfiable set dependencies w.r.t. 2E 2NC every X 0 ( Xsatisfiable w.r.t. 2E 2NC .QThen,QDefinition 6 QQ X KB 2 , since holds arbitrarykernel KB 1 KB 1 = KB 2 .Proof Proposition 2Proof focus case dependency clusters, omitting theQproof dataclusters, analogous other. Consider arbitrary KB .) begin showing kernelQQ partQQis part cluster00cluster, i.e., X X KB * X X KBX 6= X 0 .obtained directly definition clusters: X Q= [][] equivalence class equivalence relation obtained KB . Then,clearly X []. Therefore, since definition twoequivalenceclasses either equal disjoint holds/ [0 ] [0 ]. Let0000X = 0 [0 ] . holds X 6= X * X . Since holdsQQarbitrary equivalenceclass [0 ] holds X X KB * X 0QQX 0 KB X 6= X 0 .) showcluster, i.e.,QQ exist any0 kernel belongQQ0* X X KB X 6= X X X KB . Again,QariseQuse equivalence classes Definitions 9 10. * X 0 X 0 KBX 6= X 0 , holds/ [0 ] [0 ] 6= []. So,Ssince equivalence classes form0partition must holds []. Therefore, X = [] * XQQX 0 KB X 6= X 0 X.Proof Corollary 1645fiDeagustini, Martinez, Falappa & SimariQProof ConsiderKB . QProposition2 XQQQ0 X 0X KB onlyQ*XX 6= X 0 . Thus,KBQQQX X KB/ X 0 X 0 KB X 6= X 0 .`00Analogously,showXKB````00X KB/ X X KB X 6= X 0 .Proof Lemma 2QProof Consider X KB 1 . Then, X minimal unsatisfiable set TGDs w.r.t.1NC 1E . Since KB 1 = KB 2 , holds X KB 2 , 1E = 2E , 1NC = 2NCX unsatisfiable set TGDs w.r.t. 2NC 2E . Also, exist X 0 ( XX 0 unsatisfiable setcontradictQ TGDs w.r.t. 2NC 2E , since otherwise wouldQhypothesis X KB 1 , 1NCE . Then,Q = 2NC 1E = 2QQX KB 2 ;since holds arbitrary X KB 1 , KB 1 = KB 2 .QQQConsiderarbitrary X, KB 1 XY . Since= KB 2 ,KB1QQQQQX, KB 2 . Thus, Qequivalent Q, KB 1 = KB 2 .KB 1KB 2```0 0 . Since0, 0=XLikewise, considerarbitraryXKBKB1 `1` KB 2 ,```00``, thus KB 1 = KB 2 .equivalentX , KB 2 . Therefore,KB 2KB 1Proof Proposition 3QProof Consider X KB relevant X. Definition 6X unsatisfiable w.r.t. N E NC , Definition 4fact relevant X mods({}, X N ) = (1). Also, since {}singleton ( {} = , clearlymods(, X N ) 6= (2). Then,`(1), (2)``and Definition 7 follows {} KB . Also, Definition 9{} KB , since {} cannot overlap kernel, singleton.Consider incision {}. Definition 11 follows (KB ) {} 6= .Then, (KB ) {} = , thus (KB ).Proof Corollary 2QProof Consider arbitrary D. Since relevant X KB ,Proposition 3 holds (KB ). Thus, since holds arbitrary(KB ).Proof Theorem 1Proof Let KB 1 = (D1 , 1 ) KB 2 = (D2 , 2 ) two Datalog ontologiesKB 1 = KB 2 .) Construction postulatesConsider operator ! defined Definition 14; prove ! satisfies everypostulate Theorem 1. Let KB 1 ! = (D1 !, 1 !) KB 2 ! = (D2 !, 2 !) two Datalogontologies resulting consolidation KB 1 KB 2 means !, respectively.ontologyFurthermore, let KB ?1 = (D1 , 1 \ (KB 1 )) KB ?2 = (D2 , 2 \ (KB 2 )) Q`resulting removing TGDs selected KB 1 KB 2 . Let KB 1 KB ?1646fiDatalog Ontology ConsolidationQ`set dependency data kernels KB 1 KB ?1 respectively, KB 2 KB ?QQ`` 2sets dependency data kernels KB 2 KB ?2 . Finally, let KB 1 KB ?QQ`` 1set dependency data clusters KB 1 KB ?1 respectively, KB 2 KB ?2sets dependency data clusters KB 2 KB ?2 .Inclusion: 1 ! 1 D1 ! D1 .definition KB 1 ! D1 ! = D1 \ %(KB ?1 ), thus D1 ! D1 .similar way, definition KB 1 ! 1 ! = 1 \ (KB 1 ), thus1 ! 1 .Coherence: KB 1 ! coherent.prove KB 1 ! coherent show 1 ! satisfiableE NC 1 !. sufficient show minimal conflictsattended operator, i.e., dependency kernel included 1 !,QexistsConsiderQQ arbitrary X KB 1 . Proposition 2 QQX . definition holdsKBQKB 11X KB 1 X holds ((KB 1 ) X) 6= . Then, existsX ((KB 1 ) X), thus/ 1 !. Therefore,X * 1 !,Qi.e., conflict solved. Since holds arbitrary X KB 1 everyunsatisfiable set 1 included 1 !, thus 1 ! satisfiableE NC 1 !, i.e., KB 1 ! coherent.Consistency: Proof analogous Coherence.Minimality: KB 0 KB 1 coherent consistent, holds KB 1 ! 6KB 0 .0LetQKB 0 coherent consistent, let CF 1 = 1 \S``QKB KB 1( KB ) CF D1 = D1 \ ( KB ) set formulas belongkernel 1 D1 , respectively.0Supposereductio KB 1 ! SKB`` . definition KB 1 ! (KB 1 )QQ( KB ) , %(KB 1 ) ( KB ). Then, CF 1 KB 1 ! CF D1 KB 1 !.Therefore, CF 1 KB 0 CF D1 KB 0 .QQ``Then, since KB 1 ! KB 0 must exist KB KB KB 0/ KB 1 !, KB 0 coherent consistent same. is, existsdependency cluster data cluster removal optimal, since couldincluded consolidation. rest proof simplicity reasons,consider case belongs dependency cluster. made withoutloss generality, since proof case included data clusteranalogous one presented here.QQLet us considerKB 0 . Corollary 1KBQQX X KB . Let = (X (KB )) incision performedcluster, let R = (X {KB \ KB 0 }) formulas removed XQwhenobtaining KB 0 . Clearly, since KB 0 coherent X KB647fiDeagustini, Martinez, Falappa & Simariholds R 6= , otherwiseKB 0 , make KB 0 incoherent.QQBesides, since R R KB , thus R satisfies first two conditionsDefinition 12.Definition 12 exists set TGDssatisfies first two conditions definition time holds(1) R.Since/ KB 1 ! X (KB ), thus . However, knowX KB 0 , thus/ R. Therefore (2) 6 R.(1) (2) R 6 R, absurd comingoriginal assumption KB 1 ! KB 0 , holds KB 0 KB 1 coherentconsistent KB 1 ! 6 KB 0 .) Postulates Constructionsecond part proof, consider operator ! satisfies postulatesTheorem 1. Let (!) function based ! defined follows:QQ(!) (KB 1 ) = {x | x X X KB 1 x/ {1 KB 1 !}}Let KB ?1 = (D1 , 1 \ (!) (KB 1 )) ontology resulting removing KB 1TGDs selected (!) . Then, let %(!)D another function based ! defined follows:``/ {D1 KB 1 !}}%(!)D (KB ?1 ) = {x | x X X KB ? x1Based %(!)D (!) define new operator follows:KB 1 !0 = (D1 \ %(!)D (KB ?1 ), 1 \ (!) (KB 1 ))show !0 Datalog ontology consolidation operator based ClusterContraction. this, first prove %(!)D well-defined data incision function(!) well-defined constraint incision function. is, given (!)prove that:- (!) well-defined, i.e., KB 1 = KB 2 , (!) (KB 1 ) = (!) (KB 2 ).QQdefinition (!) (!) (KB 1 ) = {x | x X XKB 1x/ 1 KB 1 !}.Considerarbitraryx (!) (KB 1 ). Since KB 1 = KBQ, Lemma 22QQQ QQKBQ=.Sincex(KB),xX, thus holds1(!)KB 2KB 11Qx X KB 2 (1).QQBesides, since x X KB 1 x 1 . Thus, since x/ 1 KB 1 !, x/ KB 1 !.Since KB 1 = KB 2 , fact ! function KB 1 ! = KB 2 !,also holds x/ KB 2 !. Thus, x/ 2 KB 2 !(2).(1) (2)QQit follows x (!) (KB 1 ) holds x {y |/ 2 KB 2 !}. definition (!) (!) (KB 2 ),KB 2thus KB 1 = KB 2 , (!) (KB 1 ) = (!) (KB 2 ).648fiDatalog Ontology Consolidation- (!) (KB 1 )SQQ( KB 1 ).follows directly Qdefinition (!) , since every x (!) (KB 1 ) holdsQx X X KB 1 first condition definition.- XQQeYQ6= X, (Y (!) (KB 1 )) 6= .QQQSuppose reductioexists X KB 1 KB 1 6= ,X (Y (!) (KB 1 )) = .QQThen, holds/ (!) (KB),i.e.,/X{1 KB 1 !}.1KB 1Qhypothesis KB 1 X. Thus X, thereforemust hold {1 KB 1 !}, extension KB 1 !.KB 1KB 1Since holds arbitrary 1 !. Definition 6holds minimal unsatisfiable set TGDs w.r.t. E NC 1 . Then,relevant set atoms holds mods(A, E NC ) = . Then, since1 ! relevant set A0 holds mods(A0 , 1 ! E NC ) = ,TGDs triggered A0 . Then, 1 ! unsatisfiable set TGDs w.r.t.E NC 1 .However, Coherence KB 1 ! coherent, thus 1 ! satisfiablew.r.t. E NC 1 .1 ! satisfiable w.r.t. E NC 1 1 ! unsatisfiablew.r.t. E QNC 1 , absurdcoming initial suppositionQQexistsX KB 1 KB=6,X(Y(!) (KB 1 )) = ,1QQQandTit holds X KB 1 KB 1 X, 6=(Y (!) (KB 1 )) 6= .QQholds = (X (!) (KB 1 )) exists- XKB 1R X R satisfies two previous conditions R ( .prove sufficient show that, clusters disjoint sets, electioncluster optimal, otherwise exists cluster incisionfunction choose optimal wayQQ Minimality would satisfied.So, suppose reductio exists X KB 1 = (X (!) (KB 1 ))exist R X R satisfies two previous conditions R ( .QQLet us consider KB 0 = (0 , D0 ) KB 1 6= X holds0 = (Y (!) (KB 1 )) R0 = (Y {KB \ KB 0 }) (those formulas removedobtaining KB 0 ) 0 = R0 . Since 0 = R0 R0 QQtwo conditionsDefinition12satisfied.Besides,letCF=\11KB``CF D1 = D1 \ KB set formulas belong kernel 1D1 , respectively; let KB 0 CF 1 0 CF D1 D0 .fact every formula conflict belongs KB 0 KB 0built way election cluster different XKB 0 (!) (KB 1 ) makes KB 0 \ (X {KB \ KB 0 }) = KB 1 ! \ (X (!) (KB 1 )).is, difference KB 0 KB 1 ! difference ariseelection formulas remove X.649fiDeagustini, Martinez, Falappa & SimariFinally, supposition exists R X R satisfies twoprevious conditions R ( . Let KB 0 R ( R = (X {KB \KB 0 })set formulas removed X obtaining KB 0 . Then, KB 0coherent consistent, since every conflict clusters KB 1 solved, whetherremoving R (for cluster X) sets R0 (for every cluster different X). Besides,since KB 0 \ (X {KB \ KB 0 }) = KB 1 ! \ (X S(!) (KB 1 )) R ( ,QKB 1 ! = KB 1 \ (!) (KB 1 ) KB 0 = KB 1 \ { {QR0 R}\X}KB 1(R0 = {KB \ KB 0 }) R = (X {KB \ KB 0 }) holds KB 1 ! KB 0 (1).is, formulas involved conflicts belong KB 1 ! KB 0 ,cluster different X formulas removed, set formulasremoved X obtain KB 0 strict subset removed (!) (KB 1 )obtain KB 1 !, KB 1 ! strict subset KB 0 , i.e., removed formulasdeleting deleting R.hand, since KB 0 coherent consistent, MinimalityKB 1 ! 6 KB 0 (2).0Therefore, (1) (2) KB 1 ! KB 0 KBQQ 1 ! 6 KB , absurd= (Xcoming initial supposition exists XKB 1(!) (KB 1 )) exists R XRsatisfiestwopreviousconditionsQQR ( , Xholds=(X(!) (KB 1 ))KB 1exists R X R satisfies two previous conditionsR ( T.omit proof %(!)D well-defined data incision function using ConsistencyMinimality since analogous proof (!) well-defined constraintincision function using Coherence Minimality.shown %(!)D (!) well-defined data incision functionsconstraint incision functions, respectively, conclude second part proofshow !0 coincides !. Inclusion follows D1 ! D11 ! 1 (1). Also, definition %(!)D follows %(!)D (KB ?1 ) = D1 \ D1 !,definition (!) follows (!) (KB 1 ) = 1 \ 1 ! (2). Then,(1) (2) D1 ! = D1 \ %(!)D (KB ?1 ) 1 ! = 1 \ (!) (KB 1 ). Thus, ! =(D1 \ %(!)D (KB ?1 ), 1 \ (!) (KB 1 )), therefore !0 coincides !.ReferencesAlchourron, C., Gardenfors, P., & Makinson, D. (1985). logic theory change:Partial meet contraction revision functions. Journal Symbolic Logic, 50 (2),510530.Alchourron, C., & Makinson, D. (1981). Hierarchies Regulation Logic. NewStudies Deontic Logic, 125148.Alchourron, C., & Makinson, D. (1985). Logic Theory Change: Safe Contraction.Studia Logica, 44, 405422.Amgoud, L., & Kaci, S. (2005). argumentation framework merging conflicting knowledge bases: prioritized case. Proc. 8th European Conferences Symbolic650fiDatalog Ontology ConsolidationQuantitative Approaches Reasoning Uncertainty (ECSQUARU 05), pp.527538.Andritsos, P., Fuxman, A., & Miller, R. J. (2006). Clean answers dirty databases:probabilistic approach. Proc. 22nd International Conference Data Engineering (ICDE 06), p. 30.Arenas, M., Bertossi, L. E., & Chomicki, J. (1999). Consistent query answers inconsistent databases. Proc. 18th ACM SIGACT-SIGMOD-SIGART SymposiumPrinciples Database Systems (PODS 99), pp. 6879.Arieli, O., Denecker, M., & Bruynooghe, M. (2007). Distance semantics database repair.Annals Mathematics Artificial Intelligence, 50 (3-4), 389415.Baader, F., Brandt, S., & Lutz, C. (2005). Pushing EL envelope. Proc. 19thInternational Joint Conference Artificial Intelligence (IJCAI 05), pp. 364369.Baader, F., Calvanese, D., McGuinness, D. L., Nardi, D., & Patel-Schneider, P. F. (Eds.).(2003). Description Logic Handbook: Theory, Implementation, Applications.Cambridge University Press.Baral, C., Kraus, S., & Minker, J. (1991). Combining multiple knowledge bases. Transactions Knowledge Data Engineering, 3 (2), 208220.Bell, D. A., Qi, G., & Liu, W. (2007). Approaches inconsistency handling descriptionlogic based ontologies. Proc. Move Meaningful Internet Systems(OTM) Workshops (2), pp. 13031311.Beneventano, D., & Bergamaschi, S. (1997). Incoherence subsumption recursiveviews queries object-oriented data models. Data Knowledge Engineering,21 (3), 217252.Berners-Lee, T., Hendler, J., & Lassila, O. (2001). semantic web. Scientific American,284(5):3443.Bienvenu, M., & Rosati, R. (2013). Tractable approximations consistent query answeringrobust ontology-based data access. Proc. 23rd International Joint ConferenceArtificial Intelligence (IJCAI 13), pp. 775781.Black, E., Hunter, A., & Pan, J. Z. (2009). argument-based approach using multiple ontologies. Proc. 3rd International Conference Scalable UncertaintyManagement (SUM 09), pp. 6879.Bohannon, P., Flaster, M., Fan, W., & Rastogi, R. (2005). cost-based model effective heuristic repairing constraints value modification. Proc. 24th ACMSIGMOD International Conference Management Data / Principles DatabaseSystems (PODS 05), pp. 143154.Booth, R., Meyer, T. A., Varzinczak, I. J., & Wassermann, R. (2010). Horn belief change:contraction core. Proc. 19th European Conference Artificial Intelligence(ECAI 10), pp. 10651066.Borgida, A. (1995). Description logics data management. Transactions KnowledgeData Engineering, 7 (5), 671682.651fiDeagustini, Martinez, Falappa & SimariBrandt, S. (2004). Polynomial time reasoning description logic existential restrictions, GCI axioms, - else?. Proc. 16th European ConferenceArtificial Intelligence (ECAI 04), pp. 298302.Cal, A., Gottlob, G., & Kifer, M. (2008). Taming infinite chase: Query answeringexpressive relational constraints. Brewka, G., & Lang, J. (Eds.), Proc. 11thInternational Conference Principles Knowledge Representation Reasoning(KR 08), pp. 7080. AAAI Press.Cal, A., Gottlob, G., & Kifer, M. (2013). Taming infinite chase: Query answeringexpressive relational constraints. Journal Artificial Intelligence Research, 48,115174.Cal, A., Gottlob, G., & Lukasiewicz, T. (2012). general Datalog-based frameworktractable query answering ontologies. Journal Web Semantic, 14, 5783.Cal, A., Lembo, D., & Rosati, R. (2003). decidability complexity query answering inconsistent incomplete databases. Proc. 22nd ACM SIGMODSymposium Principles database systems (PODS 03), pp. 260271. ACM.Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., & Rosati, R. (2005). DL-Lite:Tractable description logics ontologies. AAAI, pp. 602607.Caroprese, L., Greco, S., & Zumpano, E. (2009). Active integrity constraints databaseconsistency maintenance. Transactions Knowledge Data Engineering, 21 (7),10421058.Caroprese, L., & Truszczynski, M. (2011). Active integrity constraints revision programming. Theory Practice Logic Programming, 11 (6), 905952.Cecchi, L., Fillottrani, P., & Simari, G. R. (2006). Complexity DeLPGame Semantics. Dix, J., & Hunter, A. (Eds.), Proc. 11th International Workshop Non-Monotonic Reasoning (NMR 06), pp. 386394.Cholvy, L. (1998). Reasoning merged information. Belief Change, Vol. 3, pp.233263. Springer Netherlands.Croitoru, M., & Rodriguez, R. O. (2015). Using kernel consolidation query answeringinconsistent OBDA. Proc. Joint Ontology Workshops 2015 Episode 1:Argentine Winter Ontology.Delgrande, J. P. (2011). Revising inconsistent set formulas. Proc. 22ndInternational Joint Conference Artificial Intelligence (IJCAI 11), pp. 833838.Delgrande, J. P., Dubois, D., & Lang, J. (2006). Iterated revision prioritized merging.Proc. 10th International Conference Principles Knowledge RepresentationReasoning (KR 06), pp. 210220.Delgrande, J. P., & Jin, Y. (2012). Parallel belief revision: Revising sets formulas.Artificial Intelligence, 176 (1), 22232245.Delgrande, J. P., Schaub, T., Tompits, H., & Woltran, S. (2009). Merging logic programsanswer set semantics. Proc. 25th International Conference Logic Programming (ICLP 09), pp. 160174.652fiDatalog Ontology ConsolidationDunne, P., & Wooldridge, M. (2009). Argumentation Artificial Intelligence, chap. Complexity Abstract Argumentation, pp. 85104. Springer.Everaere, P., Konieczny, S., & Marquis, P. (2008). Conflict-based merging operators.Proc. 11th International Conference Principles Knowledge RepresentationReasoning (KR 08), pp. 348357.Falappa, M. A., Kern-Isberner, G., Reis, M. D. L., & Simari, G. R. (2012). Prioritizednon-prioritized multiple change belief bases. Journal Philosophical Logic, 41 (1),77113.Falappa, M. A., Kern-Isberner, G., & Simari, G. R. (2002). Belief Revision, ExplanationsDefeasible Reasoning. Artificial Intelligence, 141, 128.Flouris, G., Huang, Z., Pan, J. Z., Plexousakis, D., & Wache, H. (2006). Inconsistencies,negations changes ontologies. Proc. 21st National Conference ArtificialIntelligence (AAAI 06), pp. 12951300.Friedman, N., & Halpern, J. Y. (2001). Belief revision: critique. Computer ResearchRepository (CoRR), cs.AI/0103020.Fuhrmann, A. (1991). Theory contraction base contraction. Journal Philosophical Logic, 20, 175203.Gardenfors, P. (1982). Rule rational changes belief. Philosophical Essay DediccatedLennart Aqvist Fiftieth Birthday, 88101.Gardenfors, P. (1988). Knowledge Flux: Modeling dynamics epistemic states. MITPress.Gelfond, M. (2008). Answer sets. Handbook Knowledge Representation, chap. 7, pp.285316. Elsevier.Gomez, S. A., Chesnevar, C. I., & Simari, G. R. (2010). Reasoning inconsistentontologies argumentation. Applied Artificial Intelligence, 24 (1&2), 102148.Greco, S., & Molinaro, C. (2012). Probabilistic query answering inconsistent databases.Annals Mathematics Artificial Intelligence (AMAI), 64 (2-3), 185207.Haase, P., van Harmelen, F., Huang, Z., Stuckenschmidt, H., & Sure, Y. (2005). framework handling inconsistency changing ontologies. Proc. 4th InternationalSemantic Web Conference (ISWC 05), pp. 353367.Halaschek-Wiener, C., & Katz, Y. (2006). Belief base revision expressive descriptionlogics. Proc. International Workshop OWL: Experiences Directions(OWLED 06).Hansson, S. O. (1991). Belief Base Dynamics. Ph.D. thesis, Uppsala University, DepartmentPhilosophy, Uppsala, Sweden.Hansson, S. O. (1993). Theory contraction base contraction unified. Journal SymbolicLogic, 58 (2), 602625.Hansson, S. O. (1994). Kernel contraction. Journal Symbolic Logic, 59 (3), 845859.Hansson, S. O. (1997). Semi-revision. Journal Applied Non-Classical Logics, 7 (1-2),151175.653fiDeagustini, Martinez, Falappa & SimariHansson, S. O. (2001). Textbook Belief Dynamics: Solutions Exercises. KluwerAcademic Publishers, Norwell, MA, USA.Harman, G. (2008). Change view: Principles reasoning. Cambridge University Press.Harper, W. (1975). Rational Belief Change, Popper Functions Counterfactuals. Synthese, 30, 221262.Horridge, M., Parsia, B., & Sattler, U. (2009). Explaining inconsistencies OWL ontologies.Scalable Uncertainty Management, pp. 124137. Springer.Huang, Z., van Harmelen, F., & ten Teije, A. (2005). Reasoning inconsistent ontologies.Proc. 19th International Joint Conference Artificial Intelligence (IJCAI 05),pp. 454459.Hue, J., Papini, O., & Wurbel, E. (2009). Merging belief bases represented logic programs. Proc. 10th European Conference Symbolic Quantitative Approaches Reasoning Uncertainty (ECSQARU 09), pp. 371382.Kalyanpur, A., Parsia, B., Horridge, M., & Sirin, E. (2007). Finding justificationsOWL DL entailments. Springer.Kalyanpur, A., Parsia, B., Sirin, E., & Hendler, J. A. (2005). Debugging unsatisfiable classesowl ontologies. Web Semantics: Science, Services Agents World WideWeb, 3 (4), 268293.Katsuno, H., & Mendelzon, A. O. (1991). difference updating knowledge base revising it. Proc. 2nd International Conference PrinciplesKnowledge Representation Reasoning (KR91), pp. 387394.Katsuno, H., & Mendelzon, A. O. (1992). Propositional knowledge base revision minimal change. Artificial Intelligence, 52 (3), 263294.Konieczny, S., & Perez, R. P. (2002). Merging information constraints: logicalframework. Journal Logic Computation, 12 (5), 773808.Konieczny, S., & Perez, R. P. (2011). Logic based merging. Journal Philosophical Logic,40 (2), 239270.Lembo, D., Lenzerini, M., Rosati, R., Ruzzi, M., & Savo, D. F. (2010). Inconsistencytolerant semantics description logics. Proc. 4th International ConferenceWeb Reasoning Rule Systems (RR 10), pp. 103117.Lenzerini, M. (2002). Data integration: theoretical perspective. Proc, 21st ACMSIGMOD Symposium Principles Database Systems (PODS 02), pp. 233246.Levi, I. (1977). Subjunctives, Dispositions Chances. Synthese, 34 (4), 423455.Liberatore, P., & Schaerf, M. (1998). Arbitration (or merge knowledge bases).Knowledge Data Engineering, 10 (1), 7690.Lin, J., & Mendelzon, A. O. (1998). Merging databases constraints. InternationalJournal Cooperative Information Systems, 7 (1), 5576.Lin, J., & Mendelzon, A. O. (1999). Knowledge base merging majority. Applied LogicSeries, 18, 195218.654fiDatalog Ontology ConsolidationLloyd, J. W. (1987). Foundations Logic Programmming. Springer-Verlag.Lukasiewicz, T., Martinez, M. V., & Simari, G. I. (2012). Inconsistency handlingdatalog+/- ontologies. Proc. 20th European Conference Artificial Intelligence (ECAI 12), pp. 558563.Martinez, M., Pugliese, A., Simari, G., Subrahmanian, V., & Prade, H. (2007). dirtyrelational database? axiomatic approach. Mellouli, K. (Ed.), Proc.9th European Conference Symbolic Quantitative Approaches ReasoningUncertainty (ECSQARU 07), Vol. 4724 Lecture Notes Computer Science, pp.103114. Springer.Meyer, T., Lee, K., & Booth, R. (2005). Knowledge integration description logics.Veloso, M., & Kambhampati, S. (Eds.), Proceedings AAAI05, Twentieth NationalConference Artificial Intelligence, pp. 645650. AAAI Press.Newell, A. (1982). Knowledge Level. Artificial Intelligence, 18, 87127.Nilsson, U., & Maluszynski, J. (1995). Logic, Programming Prolog (2ed). John Wiley& Sons Ltd.Parsons, S., Wooldridge, M., & Amgoud, L. (2003). Properties complexityformal inter-agent dialogues. Journal Logic Computation, 13 (3), 347376.Qi, G., & Hunter, A. (2007). Measuring incoherence description logic-based ontologies.Proc. 6th International Semantic Web Conference 2nd Asian SemanticWeb Conference (ISWC/ASWC 07), pp. 381394.Qi, G., Liu, W., & Bell, D. A. (2006). Knowledge base revision description logics.Proc. 10th European Conference Logics Artificial Intelligence (JELIA 06),pp. 386398.Quine, W. V. O. (1986). Philosophy logic. Harvard University Press.Reiter, R. (1987). theory diagnosis first principles. Artificial Intelligence, 32(1),5795.Rosati, R. (2011). complexity dealing inconsistency description logicontologies. Proc. International Joint Conference Artificial Intelligence (IJCAI11), pp. 10571062.Rott, H. (1992). Modellings belief change: Prioritization entrenchment. Theoria,58 (1), 2157.Schlobach, S., & Cornet, R. (2003). Non-standard reasoning services debuggingdescription logic terminologies.. Proceedings Eighteenth International JointConference Artificial Intelligence (IJCAI 03), pp. 355362.Schlobach, S., Huang, Z., Cornet, R., & van Harmelen, F. (2007). Debugging incoherentterminologies. Journal Automated Reasoning, 39 (3), 317349.Staworko, S., Chomicki, J., & Marcinkowski, J. (2012). Prioritized repairing consistent query answering relational databases. Annals Mathematics ArtificialIntelligence, 64 (2-3), 209246.655fiDeagustini, Martinez, Falappa & SimariTurner, H. (2003). Strong equivalence made easy: nested expressions weight constraints.Theory Practice Logic Programming, 3 (4-5), 609622.von Leibniz, G. W. F. (1976). Philosophical Papers Letters: selection, Vol. 1. Springer.Wassermann, R. (2000). algorithm belief revision. Proc. International Conference Principles Knowledge Representation Reasoning (KR 00), pp. 345352.Wijsen, J. (2005). Database repairing using updates. ACM Transaction Database Systems, 30 (3), 722768.656fiJournal Artificial Intelligence Research 56 (2016) 153-195Submitted 03/15; published 06/16Global Continuous Optimization Error BoundFast ConvergenceKenji Kawaguchikawaguch@mit.eduMassachusetts Institute TechnologyCambridge, MA, USAYu Maruyamamaruyama.yu@jaea.go.jpNuclear Safety Research CenterJapan Atomic Energy AgencyTokai, JapanXiaoyu Zhengzheng.xiaoyu@jaea.go.jpNuclear Safety Research CenterJapan Atomic Energy AgencyTokai, JapanAbstractpaper considers global optimization black-box unknown objective functionnon-convex non-differentiable. difficult optimization problem arisesmany real-world applications, parameter tuning machine learning, engineeringdesign problem, planning complex physics simulator. paper proposes newglobal optimization algorithm, called Locally Oriented Global Optimization (LOGO), aimfast convergence practice finite-time error bound theory. advantageusage new algorithm illustrated via theoretical analysis experimentconducted 11 benchmark test functions. Further, modify LOGO algorithmspecifically solve planning problem via policy search continuous state/actionspace long time horizon maintaining finite-time error bound. applyproposed planning method accident management nuclear power plant. resultapplication study demonstrates practical utility method.1. IntroductionOptimization problems prevalent held great importance throughout historyengineering applications scientific endeavors. instance, many problemsfield artificial intelligence (AI) viewed optimization problems. Accordingly,generic local optimization methods, hill climbing gradient method,successfully adopted solve AI problems since early research topic (Kirk, 1970;Gullapalli, Franklin, & Benbrahim, 1994; Deisenroth & Rasmussen, 2011).hand, application global optimization AI problems studied much lessdespite practical importance. mainly due lack necessary computationalpower past absence practical global optimization method strongtheoretical basis. two obstacles, former becoming less serious today,evidenced number studies global optimization past two decades (Horst& Tuy, 1990; Ryoo & Sahinidis, 1996; He, Verstak, Watson, Stinson, et al., 2004; Rios &Sahinidis, 2013). aim paper partially address latter obstacle.c2016AI Access Foundation. rights reserved.fiKawaguchi, Maruyama, & Zhenginherent difficulty global optimization problem led two distinct researchdirections: development heuristics without theoretically guaranteed performanceadvancement theoretically supported methods regardless difficulty. degreepractical success resulted heuristic approaches simulated annealing, geneticalgorithms (for brief introduction context AI, see Russell & Norvig, 2009),swarm-based optimization (for interesting example recent study, see Daly & Shen,2009). Although methods heuristics without strong theoretical supports,became popular partly optimization mechanisms aesthetically mimicnatures physical biological optimization mechanism.hand, Lipschitzian approach global optimization aims accomplish global optimization task theoretically supported manner. Despite earlysuccesses theoretical viewpoints (Shubert, 1972; Mladineo, 1986; Pinter, 1986; Hansen,Jaumard, & Lu, 1991), early studies based assumption impracticalapplications: Lipschitz constant, bound slope objectivefunction, known. relaxation crucial assumption resulted well-knownDIRECT algorithm (Jones, Perttunen, & Stuckman, 1993) worked well practice,yet guarantees consistency property. Recently, Simultaneous Optimistic Optimization (SOO) algorithm (Munos, 2011) achieved guarantee finite-time error boundwithout knowledge Lipschitz constant. However, practical performancealgorithm unclear.paper, propose generic global optimization algorithm aimed achievesatisfactory performance practice finite-loss bound theoretical basiswithout strong additional assumption1 (Section 2), apply AI planning problem(Section 6). AI planning problem, aim solving real-world engineering problemlong planning horizon continuous state/action space. illustrationadvantage method, present preliminary results application studyconducted accident management nuclear power plant well. Note optimization problems discussed paper practically relevant yet inherently difficultscale higher dimensions, i.e., NP-complete (Murty & Kabadi, 1987). Accordingly,discuss possible extensions algorithm higher dimensional problems,experimental illustration 1000-dimensional problem.2. Global Optimization Black-Box Functiongoal global optimization solve following general problem:maxx f (x)subject xf objective function defined domain RD . Since performanceproposed algorithm independent scale , consider problemrescaled domain 0 = [0, 1]D . Further, paper, focus deterministic function f .1. paper, use term strong additional assumption indicate assumption tightLipschitz constant known and/or main assumption many Bayesian optimization methodsobjective function sample Gaussian process known kernel hyperparameters.154fiGlobal Continuous Optimization Error Bound Fast Convergenceglobal optimization, performance algorithm assessed loss rn ,givenrn = max0 f (x) f (x+ (n)).xx+ (n)Here,best input vector found algorithm n trials (more precisely,define n denote total number divisions next section).minimal assumption allows us solve problem objective function f evaluated points 0 arbitrary order. applications,assumption easily satisfied, example, simulator world dynamicsexperimental procedure defines f itself. former case, x correspondsinput vector simulator f , ability arbitrarily change inputrun simulator satisfies assumption. possible additional assumptiongradient function f evaluated. Although assumption may produceeffective methods, limits applicability terms real-world applications. Therefore,assume existence simulator method evaluate f , accessgradient f . methods scope often said derivative-freeobjective function said black-box function.However, assumption made, general problem provenintractable. specifically, number function evaluations cannot guarantee gettingclose optimal (maximum) value f (Dixon, 1978). solution mayexist arbitrary high narrow peak, makes impossible relate optimalsolution evaluations f points.One simplest additional assumptions restore tractability wouldslope f bounded. form assumption studied Lipschitz continuityf :|f (x1 ) f (x2 )| bkx1 x2 k, x1 , x2 0 ,(1)b > 0 constant, called Lipschitz constant, k k denotes Euclideannorm. global optimization assumption referred Lipschitz optimization, studied long time. best-known algorithm early dayshistory Shubert algorithm (Shubert, 1972), equivalently Piyavskii algorithm (Piyavskii, 1967) algorithm independently developed. Basedassumption Lipschitz constant known, creates upper bound functionobjective function chooses point 0 highest upper bounditeration. problems higher dimension 2, finding point highest upper bound becomes difficult many algorithms proposed tackleproblem (Mayne & Polak, 1984; Mladineo, 1986). algorithms successfully providedfinite-loss bounds.However appealing theoretical point view, practical concern soon raisedregarding assumption Lipschitz constant known. many applications,complex physics simulator objective function f , Lipschitz constantindeed unknown. researchers aimed relax somewhat impractical assumptionproposing procedures estimate Lipschitz constant optimization process(Strongin, 1973; Kvasov, Pizzuti, & Sergeyev, 2003). Similarly, Bayesian optimizationmethod upper confidence bounds (Brochu, Cora, & de Freitas, 2009) estimates objective function upper confidence bounds certain model assumption, avoiding155fiKawaguchi, Maruyama, & Zhengprior knowledge Lipschitz constant. Unfortunately, approach, includingBayesian optimization method, results mere heuristics unless several additionalassumptions hold. notable assumptions algorithm maintain overestimate upper bound finding point highest upperbound done timely manner. noted Hansen Jaumard (1995),unclear approach provides advantage, considering successful heuristicsalready available. argument still applies day relatively recent algorithmsKvasov et al. (2003) Bubeck, Stoltz, Yu (2011).Instead trying estimate unknown Lipschitz constant, well-known DIRECTalgorithm (Jones et al., 1993) deals unknowns simultaneously consideringpossible Lipschitz constants, b: 0 < b < . past decade, manysuccessful applications DIRECT algorithm, even large-scale engineering problems(Carter, Gablonsky, Patrick, Kelly, & Eslinger, 2001; et al., 2004; Zwolak, Tyson, &Watson, 2005). Although works well many practical problems, DIRECT algorithmguarantees consistency property, limn rn = 0 (Jones et al., 1993; Munos, 2013).SOO algorithm (Munos, 2011) expands DIRECT algorithm solves majorissues, including weak theoretical basis. is, SOO algorithm guaranteesfinite-time loss bound without knowledge slopes bound, also employsweaker assumption. contrast Lipschitz continuity assumption used DIRECTalgorithm (Equation (1)), SOO algorithm requires local smoothness assumptiondescribed below.Assumption 1 (Local smoothness). decreasing rate objective function f aroundleast one global optimal solution {x 0 : f (x ) = supx0 f (x)} bounded semimetric `, x 0f (x ) f (x) `(x, x ).Here, semi-metric generalization metric satisfytriangle inequality. instance, `(x, x ) = bkx xk metric semi-metric.hand, whenever > 1 p < 1, `(x, x ) = bkx xkp metricsemi-metric since satisfy triangle inequality. assumption muchweaker assumption described Equation (1) two reasons. First, Assumption 1requires smoothness (or continuity) global optima, Equation (1)points whole input domain, 0 . Second, Lipschitz continuity assumptionEquation (1) requires smoothness defined metric, Assumption 1 allowssemi-metric used. best knowledge, SOO algorithmalgorithm provides finite-loss bound weak assumption.Summarizing above, DIRECT algorithm successful practice,concern weak theoretical basis led recent development generalizedversion, SOO algorithm. generalize SOO algorithm increasepracticality strengthen theoretical basis time. paper adoptsweak assumption, Assumption 1, maintain generality wide applicability.156fiGlobal Continuous Optimization Error Bound Fast ConvergenceIteration 1Iteration 2Iteration 4(N = 9)w=1(N = 3)Iteration 3w=4(N = 9)Figure 1: Illustration SOO (w = 1) LOGO (w = 1 4) end iteration3. Locally Oriented Global Optimization (LOGO) Algorithmsection, modify SOO algorithm (Munos, 2011) accelerate convergenceguaranteeing theoretical loss bounds. new algorithm modification,LOGO (Locally Oriented Global Optimization) algorithm, requires additional assumption. use LOGO algorithm, one needs prior knowledge objectivefunction f ; may leverage prior knowledge available. algorithm uses two parameters, hmax (n) w, inputs hmax (n) [1, ) w Z+ . hmax (n) wact part balance local global search. hmax (n) biases search towardsglobal search whereas w orients search toward local area.case w = 1 4 (top bottom diagrams) Figure 1 illustrates functionality LOGO algorithm simple 2-dimensional objective function. view,LOGO algorithm generalization SOO algorithm local orientationparameter w SOO special case LOGO fixed parameter w = 1.3.1 Predecessor: SOO Algorithmdiscuss algorithm detail, briefly describe direct predecessor,SOO algorithm2 (Munos, 2011). top diagrams Figure 1 (the scenario w =1) illustrates functionality SOO algorithm simple 2-dimensional objectivefunction. illustrated Figure 1, SOO algorithm employs hierarchical partitioningmaintain hyperintervals, center evaluation point objectivefunction f . is, Figure 1, rectangle represents hyperintervals enditeration algorithm. Let h set rectangles sizedivided h times. algorithm uses parameter, hmax (n), limit size rectangleoverly small (and hence restrict greediness search). order selectrefine intervals likely contain global optimizer, algorithm executesfollowing procedure:2. describe SOO simple division procedure LOGO uses. SOO specifydivision procedure.157fiKawaguchi, Maruyama, & Zheng(i)(ii)(iii)(iv)Initialize 0 = {0 } > 0, = {}Set h = 0Select interval maximum center value among intervals set hinterval selected (iii) center value greater largerinterval (i.e., intervals l l < h), divide adds new intervalsh+1 . Otherwise, reject interval skip step.(v) Set h = h + 1(vi) Repeat (iii)(v) smaller interval exists (i.e., l = {} l h)h > hmax (n)(vii) Delete intervals already divided (iv) repeat (ii)(vi)explain procedure using example Figure 1. brevity, useterm, iteration, refer iteration step (ii)(vii). Figure 1, center pointshown (black) dot rectangle rectangle (red) circle around(black) dot one divided (into three smaller rectangles) iteration.beginning first iteration, one rectangle 0 , entiresearch domain 0 . Thus, step (iii) selects rectangle step (iv) divides it, resultingleftmost diagram N = 3 (the rectangle center point red circleone divided first iteration two created result).beginning second iteration, three rectangles 1 (i.e., three rectanglesleftmost diagram N = 3) none 0 (because step (vii) previousiteration deleted interval 0 ). Hence, steps (iii)(iv) executed 0begin 1 . Step (iii) selects top rectangle three rectanglesmaximum center point among these. Step (iv) divideslarger interval, resulting second diagram top (labeled w = 1). Iteration2 continues conducting steps (iii)(iv) 2 three smaller rectangles2 . Step (iii) selects center rectangle top (in second diagram toplabeled w = 1). However, step (iv) rejects center value greaterlarger rectangle l l < h = 2. smaller rectangleiteration 2 ends. beginning iteration 3, two rectangles 1three rectangles 2 (as shown second diagram top labeled w = 1).Iteration 3 begins conducting steps (iii)(iv) 1 . Steps (iii)(iv) select dividetop rectangle. rectangles 2 , steps (iii)(iv) select divides middle rectangle.Here, middle rectangle rejected iteration 2 larger rectanglelarger center value existed iteration 2. However, larger rectangle longerexists iteration 3 due step (vii) end iteration 2, hence rejected.result third diagram (on top labeled w = 1). Iteration 3 continuesnewly created rectangles 3 . halts, however, reason iteration 2.3.2 Description LOGOLet k superset union w sets k = kw kw+1 kw+w1k = 0, 1, 2, . . . . Then, similar SOO algorithm, LOGO algorithm conductsfollowing procedure select refine intervals likely contain globaloptimizer:158fiGlobal Continuous Optimization Error Bound Fast Convergence(i) Initialize 0 = {0 } > 0, = {}(ii) Set k = 0(iii) Select interval maximum center value among intervals superset k(iv) interval selected (iii) center value greater larger interval(i.e., intervals l l < k), divide adds new intervals . Otherwise,reject interval skip step.(v) Set k = k + 1(vi) Repeat (iii)(v) smaller interval exists (i.e., l = {} l k)k > bhmax (n)/wc.(vii) Delete intervals already divided (iv) repeat (ii)(vi)compared SOO algorithm, steps identical exceptLOGO processes superset k instead set h . superset k reduced hk = h w = 1 thus LOGO reduced SOO.explain procedure using example Figure 1. w = 1, LOGOalgorithm functions fashion SOO algorithm. See last paragraphprevious section explanation SOO LOGO algorithms functionexample. case w = 4, difference arises iteration 3compared case w = 1. beginning iteration 3, two sets 12 (i.e., two sizes rectangles second diagram bottom w = 4).However, one superset consisting two sets 0 = 0 0+1 0+41 .Therefore, step (iii)(iv) conducted k = 0 LOGO algorithm dividesone rectangle highest center value among 0 . Consequently,algorithm one additional iteration (iteration 4) using number functionevaluations (N = 9) case w = 1. seen w increases,algorithm biased local search and, example, strategy turnsbeneficial algorithm divides rectangle near global optimaw = 4 w = 1.pseudocode LOGO algorithm provided Algorithm 1. Steps (ii), (v),(vi) correspond for-loop lines 1019. Steps (iii)(iv) correspond line 11 line1214, respectively. use following notation. hyperrectangle, 0 , coupledfunction value center point f (c ), c indicates center pointrectangle. explained earlier, use h denote number divisions indexset h . define h,i ith element set h (i.e., h,i h ). Letxh,i ch,i arbitrary point center point rectangle h,i , respectively.denote val[h,i ] indicate stored function value center point rectangleh,i . seen line 14, paper considers simple division procedurerescaled domain 0 . prior knowledge domain function,leverage information. example, could map original input spaceanother obtain better ` based theoretical results Section 4,could employ elaborate division procedure based prior knowledge.discussed LOGO algorithm functions, consider reasonalgorithm might work well. key mechanism DIRECT SOO algorithms159fiKawaguchi, Maruyama, & ZhengAlgorithm 1: LOGO algorithm0:1:2:3:4:5:6:7:8:9:10:11:12:13:14:15:16:17:18:19:20:Inputs (problem): objective function f : x RD R, search domain : xInputs (parameter): search depth function hmax : Z+ [1, ), local weightw Z+ , stopping conditionDefine set h set hyperrectangles divided h timesDefine superset k union w sets: k = kw kw+1 kw+w1Normalize domain 0 = [0, 1]DInitialize variables: set hyperrectangles: h = {}, h = 0, 1, 2, . . . ,current maximum index set: hupper = 0number total divisions: n = 1Adds initial hyperrectangle 0 set: 0 0 {0 } (i.e., 0,0 = 0 )Evaluate function f center point 0 , c0,0 : val [0,0 ] f (c0,0 )iteration = 1, 2, 3, . . .val max , hplus hupperk = 0, 1, 2, . . . , max(bmin(hmax (n), hupper )/wc, hplus )Select hyperrectangle divided: (h, i) arg maxh,i val [h,i ] h, : h,i kval [h,i ] > val maxval max val [h,i ], hplus 0, hupper max(hupper , h + 1), n n + 1Divide hyperrectangle h,i along longest coordinate direction- three smaller hyperrectangles created left , center , right- val [center ] val [h,i ]Evaluate function f center points two new hyperrectangles:val [left ] f (cleft ), val [right ] f (cright )Group new hyperrectangles set h+1 remove original rectangle:h+1 h+1 {center , left , right }, h h \ h,iendstopping condition met Return (h, i) = arg maxh,i val [h,i ]endenddivide hyperintervals potentially highest upper bounds w.r.t. unknownsmoothness iteration. idea behind LOGO algorithm reduce numberdivisions per iteration biasing search toward local area conceptsupersets. Intuitively, beneficial two reasons. First, reducingnumber divisions per iteration, information utilized selecting intervalsdivide. example, one may simultaneously divide five ten intervals per iteration.former, selecting sixth tenth interval divide, one leverageinformation gathered previous five divisions (evaluations), whereas latter makesimpossible. selection intervals depends information, turnprovides new information next selection, minor difference availabilityinformation may make two sequences search different long run.Second, biasing search toward local area, algorithm likely converges fastercertain type problem. many practical problems, aim find position160fiGlobal Continuous Optimization Error Bound Fast Convergenceglobal optima, position function value close global optima. case,local bias likely beneficial unless many local optima, value farglobal optima. Even though local bias strategy motivated solveproblem impractically slow convergence rate global optimization methods,algorithm maintains guaranteed loss bounds w.r.t. global optima, discussed below.4. Theoretical Results: Finite-Time Loss Analysisfirst derive loss bounds LOGO algorithm uses division strategysatisfies certain assumptions. Then, provide loss bounds algorithmconcrete division strategy provided Algorithm 1 parameter valuesuse rest paper. motivation first part extend existingframework theoretical analysis thus produce basis future work.second part prove LOGO algorithm maintains finite-time loss boundsparameter settings actually use experiments.4.1 Analysis General Division Methodsection, generalize result obtained Munos (2013) previous resultseen special case new result w = 1. previous work providedloss bound SOO algorithm division process satisfied followingtwo assumptions, adopt section.Assumption A1 (Decreasing diameter). exists function (h) > 0hyperinterval h,i 0 , (h) supxh,i `(xh,i , ch,i ), (h 1) (h) holdsh 1.Assumption A2 (Well-shaped cell). exists constant > 0 hyperinterval h,i contains `-ball radius (h) centered h,i .Intuitively, Assumption A1 states unknown local smoothness ` upper-boundedmonotonically decreasing function h. assumption ensures divisionincrease upper bound, (h). Assumption A2 ensures every interval coversleast certain amount space order relate number intervals unknownsmoothness ` (because ` defined terms space). present analysis, needdefine relevant terms variables. define -optimal set XX := {x 0 : f (x) + f (x )}.is, set -optimal set X set input vectors whose function value least-close value global optima. order bound number hyperintervalsrelevant -optimal set X , define near-optimality dimension follows.Definition 1 (Near-optimality dimension). near-optimality dimension smallest0 exists C > 0, > 0, maximum number disjoint `-ballsradius centered -optimal set X less equal Cd .near-optimality dimension introduced Munos (2011) closely relatedprevious measure used Kleinberg, Slivkins, Upfal (2008). value161fiKawaguchi, Maruyama, & Zhengnear-optimality dimension depends objective function f , semi-metric `division strategy (i.e., constant Assumption A2). consider semi-metric `satisfies Assumptions 1, A1, A2, value depends semimetric ` division strategy. Theorem 2, show division strategyLOGO algorithm let = 0 general class semi-metric `.defined relevant terms variables used previous work,introduce new concepts advance analysis. First, define set -optimalhyperinterval h (w)h (w) := {h,i 0 : f (ch,i ) + (h w + 1) f (x )}.-optimal hyperinterval h (w) used relate hyperintervals -optimal set X .Indeed, -optimal hyperinterval h (w) almost identical (h w + 1)-optimalset X(hw+1) (-optimal set X (hw+1)), except h (w) considershyperintervals values center points X(hw+1) wholeinput vector space. order relate h (w) h (1) , define `-ball ratio follows.Definition 2 (`-ball ratio). every h w, `-ball ratio smallest h (w) > 0volume `-ball radius (h w + 1) volume h (w)disjoint `-balls radius (h).following lemma, bound maximum cardinality h (w) . use |h (w) |denote cardinality.Lemma 1. Let near-optimality dimension C denote corresponding constant Definition 1. Let h (w) `-ball ratio Definition 2. Then, -optimalhyperinterval bounded|h (w) | Ch (w)(h w + 1)d .Proof. proof follows definition -optimal set X , Definition 1, Definition 2,Assumption A2. definition -optimal space X , write (hw+1)-optimalsetX(hw+1) = {x 0 : f (x) + (h w + 1) f (x )}.definition near-optimality dimension (Definition 1) implies C(hw + 1)d centers disjoint `-balls radius (h w + 1) exist within space X(hw+1) .Then, definition `-ball ratio (Definition 2), space C(h w + 1)ddisjoint `-balls radius (h w + 1) covered Ch (w)(h w + 1)d disjoint`-balls radius (h). Notice set space covered C(h w + 1)d disjoint`-balls radius (h w + 1) superset X(hw+1) . Therefore, deduceCh (w)(h w + 1)d centers disjoint `-balls radius (h) withinX(hw+1) . Now, recall definition h-w-optimal interval,h (w) := {h,i 0 : f (ch,i ) + (h w + 1) f (x )}notice number intervals equal number centers ch,i satisfycondition f (ch,i ) + (h w + 1) f (x ). Assumption A2 causes numberequivalent number centers disjoint `-balls radius (h), showedupper bounded Ch (w)(h w + 1)d .162fiGlobal Continuous Optimization Error Bound Fast ConvergenceNext, bound maximum size optimal hyperinterval, contains globaloptimizer x . following analysis, use concept set supersethyperintervals. Recall set h contains hyperintervals divided htimes thus far, superset k union w sets, given k = kw kw+1kw+w1 k = 0, 1, 2, . . . . say hyperinterval dominated intervalshyperinterval divided center valuehyperintervals set.Lemma 2. Let kn highest integer optimal hyperinterval, containsglobal optimizer x , belongs superset kn n total divisions (i.e., kn ndetermines size optimal hyperinterval, hence loss algorithm). Then,kn lower bounded kn K K satisfies 0 K bhmax (n)/wcKw1Xhmax (n) + w Xn|kw (1) | +|kw+l (l + 1) | .wk=0l=1Proof. Let (k ) number divisions, algorithm dividesoptimal hyperinterval superset k places k+1 . example Figure 1w = 1, optimal hyperinterval initially whole domain 0 0 . dividedfirst division optimal hyperinterval placed 1 . Therefore, (0 ) = 1.Similarly, (1 ) = 2. division non-optimal interval occurs optimalone (2 ) hence (2 ) = 4. words, (k ) time optimalhyperinterval superset k divided escapes superset k , enteringk+1 . Let ckw+l,i center point optimal hyperinterval set kw+l k .prove statement showing quantity (k ) (k1 ) boundednumber -optimal hyperintervals h (w) . so, let us consider possiblehyperintervals divided time [ (k1 ), (k ) 1]. hyperintervalsset kw , ones possibly divided time must satisfy f (ckw,i ) f (ckw,i ) f (x ) (kw). first inequality due factalgorithm divide interval center value less maximum center value existing interval set, exists f (ckw,i ) time[ (k1 ), (k ) 1]. second inequality follows Assumption 1 definitionoptimal interval. Then, definition h (w) , hyperintervals possiblydivided time belong kw (1) k .addition set kw , superset k , sets kw+l l : w 1 l 1.sets, f (ckw+l,i ) f (clw+l,i ) f (x ) (kw) similar deductions.Here, notice time [ (k1 ), (k ) 1], sure center valuesuperset lower bounded f (ckw,i ) instead f (ckw+l,i ). addition,(kw) = (kw + l l). Thus, conclude hyperintervals set kw+ldivided time [ (k1 ), (k ) 1] belongs kw+l (l + 1) (w 1) l 1.hyperinterval superset k may divided iteration since intervalsdominated supersets. case, f (cjw+l,i ) f (ckw,i )f (x ) (kw) j < k l 0. similar deductions, easy seef (x ) (kw) f (x ) (jw + l). Thus, hyperintervals superset j j < kdominate superset k [ (k1 ), (k ) 1] belongs jw+l (1) .163fiKawaguchi, Maruyama, & ZhengPutting results together noting algorithm dividesbhmax (n)/wc + 1 intervals iteration (hplus plays role algorithm divides one interval),(k ) (k1 )w1k1 w1XXXhmax (n)+ 1 |kw (1) | +|kw+l (l + 1) | +|jw+l (1) | .wj=1 l=0l=1Then,knXk=1knw1Xhmax (n) + w X(k ) (k1 )|kw (1) | +|kw+l (l + 1) |wk=1l=1since last term superset j j k 1 previous inequality containsoptimalPkn intervals subsets optimal intervals covered newsummation k=1 .kn bhmax (n)/wc, statement always holds true 0 K bhmax (n)/wcsince kn bhmax (n)/wc K. Accordingly, assume kn < bhmax (n)/wc following.Since (0 ) upper bounded term previous summation right handinequality k = 0,hmax (n) + w(kn +1 )wkXw1n +1X|kw+l (l + 1) | .|kw (1) | +l=1k=0definition kn , n < (kn +1 ). Therefore, K bhmax (n)/wcKw1Xhmax (n) + w X|kw (1) | +|kw+l (l + 1) |wk=0l=1hmax (n) + wn<wkXn +1|kw (1) | +w1Xk=0|kw+l (l + 1) | ,l=1kn K.Lemmas 1 2, present main result section providesfinite-time loss bound LOGO algorithm.Theorem 1. Let ` semi-metric Assumptions 1, A1, A2 satisfied.Let h(n) smallest integer hhmax (n) + wnCwbh/wcw1XX(kw) +kw+l (l + 1)(kw).k=0l=1Then, loss LOGO algorithm boundedrn min(wbh(n)/wc w, wbhmax (n)/wc) .164fiGlobal Continuous Optimization Error Bound Fast ConvergenceProof. Lemma 1 definition h(n),hmax (n) + wn>Cwhmax (n) + wwbh(n)/wc1w1XX(kw) +kw+l (l + 1)(kw + l l)k=0l=1bh(n)/wc1X|kw (1) | +k=0w1X|kw+l (l + 1) | .l=1Therefore, set K K = bh(n)/wc 1 following apply result Lemma 2.Then, follows kn K(n) K < bhmax (n)/wc. Here, number divisionsinterval superset K least Kw = wbh(n)/wc w. Therefore,Assumptions 1, A1, A2, deduce rn (wbh(n)/wc w).K bhmax (n)/wc, b(hupper )/wc kn bhmax (n)/wc. Thus,case, kn equal least bhmax (n)/wc. Assumptions 1, A1, A2,similarly deduce rn (wbhmax (n)/wc).loss bound stated Theorem 1 applies LOGO algorithm divisionstrategy satisfies Assumptions A1 A2. add following assumptiondivision process derive concrete forms loss bound.Assumption A3 (Decreasing diameter revisit). decreasing diameter defined Assumption 1 written (h) = c h/D c > 0 < 1, accordinglycorresponding `-ball ratio h (w) = ((h w + 1)/(h))D .Assumption A3 similar assumption made Munos (2013),(h) = c h . contrast previous assumption, assumption explicitly reflectsfact size hyperinterval decreases slower rate higher dimensionalproblems. LOGO algorithm, validity Assumptions A1, A2, A3 confirmed next section.present finite-loss bound LOGO algorithm case generaldivision strategy additional assumption = 0.Corollary 1. Let ` semi-metric Assumptions 1, A1, A2, A3 satisfied.near-optimality dimension = 0 hmax (n) set n w, lossLOGO algorithm bounded n!w w 1 1w1rn c exp minn2, n wln.C 1 1Proof. Based definition h(n) Theorem 1, first relate h(n) nhmax (n) + wnCwhmax (n) + w=Cwbh(n)/wcX(kw)+k=0k=0kw+l (l + 1)(kw + l l)l=1bh(n)/wcXw1X1+w1Xl=1whmax (n) + wCw165w1Xh(n)+1w .wl=0fiKawaguchi, Maruyama, & Zheng0.90.80.70.60.50.40.30.20.112345wFigure 2: Effect local bias w loss bound case = 0 := w2 ( 1 1)/( w 1)first line follows definition h(n), second line due = 0Assumption A3. algebraic manipulation,Here, use hmax (n) =wh(n)nw1 11.wC hmax (n) + w 1 1n w, hencew w 1 1h(n)n1.wC 1 1substituting results statement Theorem 1,!w2 w 1 12n2w, w n w.rn minC 1 1Assumption A3, (h) = c h/D . using (h) = c h/D inequality,statement corollary.Corollary 1 shows LOGO algorithm guarantees exponential boundloss terms n (a stretched exponential bound terms n). loss boundCorollary 1 becomes almost identical SOO algorithm w = 1. Accordingly,illustrate effect w, n large enough let us focus coefficient n,Figure 2. (red) bold line label 1 indicates area w effectbound. area lines labels greater one w improves bound,area labels less one w diminishes bound. concretely,figure, consider ratio coefficient n loss bound variousvalue w w = 1. ratio w2 ( 1 1)/( w 1) w, dependingelement min bound smaller. Since w2 ( 1 1)/( w 1) w (indomain consider), plotted w2 ( 1 1)/( w 1) avoid overestimating benefitw. Thus, rather pessimistic illustration advantage generalization166fiGlobal Continuous Optimization Error Bound Fast Convergenceregarding w. instance, second element min bound smaller nlarge enough, increasing w always improves bound, regardless values Figure 2.next corollary presents finite-loss bound LOGO algorithm case6= 0.Corollary 2. Let ` semi-metric Assumptions 1, A1, A2, A3 satisfied.near-optimality dimension > 0 hmax (n) set ((ln n)c1 ) wc1 > 1, loss LOGO algorithm bounded!w1 1 1/d2 wd/D2wd/D1/dw ()rn n.1 1Proof. way first step proof Corollary 1, except > 0,n Ccdhmax (n) + wwbh(n)/wc w1XXk=0l=0lkwd/D .reason couldPbound loss similar rate case = 0last summation term w1l=0 longer independent k. Sincew1Xlkwd/D=w1,11kwd/Dl=0bh(n)/wcXk=0kwd/D =(bh(n)/wc+1)wd/D 1,wd/D 1algebraic manipulation,c((bh(n)/wc+1)wd/Dww1 1 wd/Dn(1).1)C hmax (n) + w 1 1Therefore,c (wbh(n)/wcw)/D!1/dwnw1 1 wd/D.(1) 2wd/DC hmax (n) + w 1 1Theorem 1 Assumption A3,rn max!w1/dnw1 1 wd/D(2wd/D ), c (wbhmax (n)/wcw)/D .C hmax (n) + w 1 1hmax (n) = ((ln n)c1 ) w sufficiently large n, first element previousmax becomes larger second one, order equivalent onestatement.derived loss bound SOO algorithm Assumption A3 case6= 0 well. SOO version loss bound rn O(n1/d ( d/D 2d/D )1/d ),equivalent loss bound LOGO algorithm w = 1 Corollary 2.Figure 3, thereby illustrate effect w loss bound form.figure, plotted ratio elements inside loss bounds. Figure 2167fiKawaguchi, Maruyama, & Zheng1110.80.80.80.60.60.60.40.40.40.20.20.212345w(a) = 0.01123w(b) = 0.545412345w(c) = 1.0Figure 3: Effect local bias w loss bound case 6= 0 := (w2 ( wd/Dw 1)1 )1/d /( d/D 2d/D )1/d2wd/D ( 1 1Figure 3, infer loss bound improved w > 1 largesmall (when n sufficiently large). Intuitively, makes sense, sincedifferent yet similar sizes hyperintervals w.r.t. ` larger smaller.case, dividing hyperintervals marginally different sizes would redundantwaste computational resources. Note discussion limited lossbound now, may tightened future work. would seedifferent effects w tightened bounds.4.2 Basis Practical Usagesection, derive loss bound LOGO algorithm concrete divisionstrategy presented Section 3.1. purpose section analyze LOGOalgorithm division process parameter settings actually usedrest paper. results section directly applicable experiments.section, discard Assumptions A1, A2, A3. consider following assumptionpresent loss bound concrete form.Assumption B1. exists semi-metric ` satisfies Assumption 1following conditions hold:exist b > 0, > 0 p 1 x, 0 , `(x, y) = bkx ykpexist (0, 1) x 0 , f (x ) f (x) + ` (x, x ).First, state loss bound algorithm practical division processparameter settings decreases stretched exponential rate.Theorem 2 (worst-case analysis). Let ` semi-metric Assumptions 1 B1satisfied. loss LOGO algorithm bounded!ww1 1w10rn c exp minn 02, w n wlnw C 1 1168fiGlobal Continuous Optimization Error Bound Fast Convergence= 3 c = b3 D/p . Here, w0 = 1 set parameter hmax (n) =hand, w0 = w set parameter hmax (n) = w n w.nw.Proof. Assumption B1 division strategy,supxh,i `(xh,i , ch,i ) b(3bh/Dc D1/p ) = bD/p 3bh/Dccorresponds diagonal length rectangle, 3bh/Dc correspondslength longest side. quantity upper bounded 3h/D+ . Thus,consider case (h) = b3 D/p 3h/D , satisfies Assumption A1. Also,Assumption A3 satisfied (h) = 3 c = b3 D/p .Every rectangle contains least `-ball radius corresponding lengthshortest side rectangle. Consequently, least `-ball radius (h) =b3 3h/D rectangle = 32 D/p , satisfies Assumption A2.Assumption B1, volume V `-ball radius (h) proportional ((h))Dfollowing: VDp ((h)) = (2(h)(1 + 1/p))D /(1 + D/p). Therefore, Assumption A3satisfied `-ball ratio h (w). addition, (h)-optimal set X(h) covered`-ball radius (h) Assumption B1, thereby contains ((h)/(h))D =disjoint `-balls radius (h). Hence, number `-balls depend (h),means = 0.satisfied Assumptions A1, A2, A3 = 3 , c = b3 D/p ,= 0, obtain statement following proof Corollary 1.Regarding effect local orientation w, Theorem 2 presents worst-case analysis.Recall w introduced paper restore practicality global optimizationmethods. Thus, focusing worst case likely pessimistic. mitigateproblem, present following optimistic analysis.Theorem 3 (best-case analysis terms w). Let ` semi-metric Assumptions 1 B1 satisfied. 1 l w, let h+l1,i0 hyperintervalmay dominate intervals set h algorithms execution. Assumeh+l1,i0 h (1) . Then, loss LOGO algorithm bounded1w10rn c exp minn 0 2, w n wlnwC= 3 c = b3 D/p . Here, w0 = 1 set parameter hmax (n) = nw.hand, w0 = w set parameter hmax (n) = w n w.Proof. statement Lemma 2 modifiedKw1Xhmax (n) + w Xn|kw (1) | +|kw (1) | .wk=0l=1statement Theorem 1 modifiedbh/wchmax (n) + w XnC(w(kw)d ),wk=0rn min(wbh(n)/wc w, wbhmax (n)/wc) .169fiKawaguchi, Maruyama, & Zheng2525451w(a) Pessimistic w2 ( 1 1)/( w 1)5312250.52.51200.5201151151.5101.510210252.552.513132w345(b) Optimistic w2Figure 4: Effect local bias w loss bound practical setting. real effect wouldexist somewhere in-between.Then, follow proof Theorem 2 Corollary 1, obtainingn1h(n)1.wC hmax (n) + whmax (n) =theorem.nw, modified statement Theorem 1, obtain statementTheorem 3 makes strong assumption eliminate negative effect localorientation bound, increasing w always improves loss bound theoremn sufficiently large. may seem overly optimistic, show instancecase experiment.realistically, effect w large n would exist somewhere leftright diagrams Figure 4. previous figures, (red) bold linelabel 1 w effect bound, area labels greater onew improves bound, area labels less one w diminishesbound. left diagram shows effect w worst case Theorem 2 plottingw2 ( 1 1)/( w 1) = 3 . reason plotting w2 ( 1 1)/( w 1)represents worst case discussed previous section. right diagram presentseffect w best case Theorem 2 Theorem 3 simply plotting w2 . NoticeTheorem 2 Theorem 3, best scenario effect w usehmax (n) = w n w second element min dominates bound. case,coefficient n w2 , effect w bound n large enoughignore term.conclusion, showed LOGO algorithm provides stretched exponentialbound loss algorithms division strategy, likely practicalone used analysis SOO algorithm, parameter settinghmax (n) = n w hmax (n) = w n w. also discussed local bias w170fiGlobal Continuous Optimization Error Bound Fast Convergenceaffects loss bound. Based results, use LOGO algorithm followingexperiments.5. Experimental Resultssection, test LOGO algorithm series experiments. mainpart experiments, compared LOGO algorithm direct predecessor,SOO algorithm (Munos, 2011) latest powerful variant, Bayesian Multi-ScaleOptimistic Optimization (BaMSOO) algorithm (Wang, Shakibi, Jin, & de Freitas, 2014).BaMSOO algorithm combines SOO algorithm Gaussian Process (GP)leverage GPs estimation upper confidence bound. shown outperformtraditional Bayesian Optimization method uses GP DIRECT algorithm(Wang et al., 2014). Accordingly, omitted comparison traditional BayesianOptimization method. also compare LOGO popular heuristics, simulated annealing(SA) genetic algorithm (GA) (see Russell & Norvig, 2009 brief introduction).experiments, rescaled domains [0, 1]D hypercube. useddivision process SOO, BaMSOO LOGO, one presented Section 3.2proven provide stretched exponential bounds loss Section 4.2. Previousalgorithms also used division process experiments (Jones et al., 1993;Gablonsky, 2001; Munos, 2013; Wang et al., 2014). SOO LOGO algorithms,set hmax (n) = w n w. setting guarantees stretched exponential bound LOGO,proven Section 4.2, SOO (Munos, 2013). LOGO algorithm, usedsimple adaptive procedure set parameter w. Let f (x+) best value foundthus far end iteration i. Let W = {3, 4, 5, 6, 8, 30}. algorithm beginsw = W1 = 3. end iteration i, algorithm set w = Wk k = min(j + 1, 6)+f (x+) f (xi1 ), k = max(j 1, 1) otherwise, Wj previous parametervalue w adjustment occurs. Intuitively, adaptive procedure encouragealgorithm locally biased seems making progress, forcing exploreglobal region seem case. Although valuesset W = {3, 4, 5, 6, 8, 30} arbitrary, simple setting used experimentspaper, including real-world application Section 6.4. results demonstraterobustness setting. discussed later, future work would replacesimple adaptive mechanism improve performance proposed algorithm.BaMSOO algorithm, previous work Wang et al. (2014) used pair good kernelhyperparameters handpicked test function. experiments,assumed handpicking procedure unavailable, typically casepractice. tested several pairs kernel hyperparameters; however, nonepairs performed robustly well test functions (e.g., one pair performed wellone test function, although others). Thus, used empirical Bayes method3adaptively update hyperparametersp . selected isotropic Matern kernel0= 5/2, given (x, x ) = g( 5kx x0 k2 /l), function g definedg(z) = 2 (1 + z + z 3 /3). hyperparameters initialized = 1 l = 0.25.updated hyperparameters every iteration 1,000 function evaluations executed3. implemented BaMSOO use empirical Bayes method, doneoriginal implementation. original implementation BaMSOO available us well.171fiKawaguchi, Maruyama, & ZhengfSOONTime (s)BaMSOOErrorNTime (s)LOGOErrorNTime (s)ErrorSin 11[0, 1]575.3 E02 2.3 E06302.0 E+00 2.3 E06 17 3.9 E02 2.3 E06Sin 22[0, 1]22711.7 E01 4.6 E061817.5 E+00 4.6 E06 45 5.4 E02 4.6 E06Peaks2[3, 3]21411.0 E01 9.0 E05373.5 E+00 9.0 E05 35 6.1 E02 9.0 E05Branin2 [5, 10] [0, 15]3392.1 E01 9.0 E051218.1 E+00 9.0 E05 85 7.0 E02 8.7 E05Rosenbrock 22[5, 10]24913.1 E01 9.7 E06 >4000 5.8 E+04 5.5 E03 137 1.3 E01 9.7 E06Hartman 33[0, 1]33592.3 E01 7.91 E051268.9 E+00 7.9 E05 65 7.1 E02 5.1 E05Shekel 54[0, 10]41101 6.6 E01 8.4 E053163.1 E+01 8.4 E05 157 1.2 E01 8.4 E05Shekel 74[0, 10]41117 7.1 E01 9.4 E05951.2 E+01 9.4 E05 157 1.2 E01 9.4 E05Shekel 104[0, 10]41117 6.4 E0.1 9.68 E05 >4000 4.5 E+04 8.1 E+00 197 1.5 E01 9.7 E05Hartman 66[0, 1]61759 1.2 E+00 7.51 E05 >4000 4.0 E+04 2.3 E03 161 1.3 E01 6.8 E05[5, 10]10>8000 7.8 E+00 3.83 E03 >8000 5.8 E+04 9.6 E+00 1793 1.7 E+00 4.8 E05Rosenbrock 10 10Table 1: Performance comparison terms number evaluations (N ) CPU time(Time) achieve Error < 104 . grayed cells indicate experiments couldachieve Error < 104 even large number function evaluations (4000 8000).per 1,000 iterations afterward (to reduce computational cost). SA GA,used settings Matlab standard subroutines simulannealbndga, except specified domain bounds.Table 1 shows results comparison 11 test functions termsnumber evaluations CPU time achieve small error. first two test functions,Sin 1 Sin 2, used test SOO algorithm (Munos, 2013), formf (x) = (sin(13x) sin(27x) + 1)/2 f (x1 , x2 ) = f (x1 )f (x2 ) respectively. formthird function, Peaks, given Equation (16) illustrated Figure 2 McDonald,Grantham, Tabor, Murphys paper (2007). rest test functions commonbenchmarks global optimization literature; Surjanovic Bingham present detailedinformation functions (2013). table, Time (s) indicates CPU timesecond Error defined(|(f (x ) f (x+ ))/f (x )| f (x ) 6= 0,Error =|f (x ) f (x+ )|otherwise.table, N = 2n number function evaluations needed achieve Error < 104 ,n total number divisions one used main measure analyses previous sections. Here, N equal 2n adopted division process.Thus, lower value N becomes, better algorithms performance is.continued iterations 4000 function evaluations functions dimensionalityless 10, 8000 function dimensionality equal 10.seen Table 1, LOGO algorithm outperformed algorithms.superior performance LOGO algorithm small number function evaluations attributable focusing promising area discovered search.Conversely, SOO algorithm continues search global domain tends similar uniform grid search. BaMSOO algorithm also follows tendency toward172fiGlobal Continuous Optimization Error Bound Fast Convergencegrid search based SOO algorithm. BaMSOO algorithm choosesdivide based SOO algorithm; however, omits function evaluations uppercoincidence bound estimated GP indicates evaluation likelybeneficial. Although mechanism BaMSOO algorithm seems beneficialreduce number function evaluations cases, two serious disadvantages.first disadvantage computational time due use GP. Notice requiresO(N 3 ) every time re-compute upper confidence bound.4 serious disadvantage possibility determining solution all. Table 1, seeBaMSOO improves performance SOO 7/11 cases; however, severely degradesperformance 4/11 cases. Moreover, may BaMSOO reduce performancealso may guarantee convergence even limit practice. BaMSOOalgorithm reduces number function evaluations relying estimationupper confidence bound. However, estimation wrong, wrong, maynever explore region global optimizer exists. Notice limitationsunique BaMSOO also apply many GP-based optimization methods.terms first limitation (computational time), BaMSOO significant improvementcompared traditional GP-based optimization methods (Wang et al., 2014).Although LOGO algorithm bias toward local search, maintains strongtheoretical guarantee, similar SOO algorithm, proven previous sections.terms theoretical guarantee, SOO algorithm LOGO algorithm sharesimilar rate loss bound base analyses set assumptionshold practice. hand, BaMSOO algorithm worse rate lossbound (an asymptotic loss order n(1)/d ) bound applies restrictedclass metric ` (the Euclidean norm power = {1, 2}). also requires severaladditional assumptions guarantee bound. additional assumptions wouldimpractical, particularly assumption objective function always wellcaptured GP chosen kernel hyperparameters. discussed above,assumption would cause BaMSOO lose loss bound also consistencyguarantee (i.e., convergence limit) practice.Figure 5 presents performance comparison number function evaluationsFigure 6 plots corresponding computational time. figures, lower plottedvalue along vertical axis indicates improved algorithm performance. SA GA,figure shows mean 10 runs. report mean standard deviationtime following. SA, 1.19 (Sin 1), 1.32 (Sin 2), 0.854 (Peaks), 0.077(Branin), 1.06 (Rosenbrock 2), 0.956 (Hartman 3), 0.412 (Shekel 5), 0.721 (Shekel 7), 1.38(Shekel 10), 0.520 (Hartman 6), 0.489 (Rosenbrock 10). GA, 0.921 (Sin 1),0.399 (Sin 2), 0.526 (Peaks), 0.045 (Branin), 1.27 (Rosenbrock 2), 0.493 (Hartman 3), 0.216(Shekel 5), 0.242 (Shekel 7), 1.19 (Shekel 10), 0.994 (Hartman 6), 0.181 (Rosenbrock10).illustrated Figure 5, LOGO algorithm generally delivered improved performance compared algorithms. particularly impressive result LOGOalgorithm robustness challenging functions, Shekel 10 Rosenbrok 10.4. Although several methods mitigate computational burden approximation, effectapproximation performance BaMSOO algorithm unknown left futurework.173fiKawaguchi, Maruyama, & Zheng046SAGASOOBaMSOOLOGO8101214161246SAGASOOBaMSOOLOGO8101214161N1011079N100N5SAGASOOBaMSOOLOGO152025110100N56110100N3SAGASOOBaMSOOLOGO5791113LogDistancetoOptima6420246810123SAGASOOBaMSOOLOGO791113110N100(j) Hartman 679100010N100100002SAGASOOBaMSOOLOGO4681012110100N1000(h) Shekel 71552(g) Shekel 511000(f) Hartman 311000100SAGASOOBaMSOOLOGO31157N11000LogDistancetoOptimaLogDistancetoOptimaSAGASOOBaMSOOLOGO10111413(e) Rosenbrock 203111101000129(c) Peaks0(d) Branin1SAGASOOBaMSOOLOGO7130111051000LogDistancetoOptimaLogDistancetoOptimaLogDistancetoOptimaSAGASOOBaMSOOLOGO1LogDistancetoOptima1005153(b) Sin 213115100(a) Sin 1LogDistancetoOptimaLogDistancetoOptimaLogDistancetoOptimaLogDistancetoOptima02110N1001000(i) Shekel 10SAGASOOBaMSOOLOGO110N1001000(k) Rosenbrock 10Figure 5: Performance comparison: number evaluations N vs. log error computedlog10 |f (x ) f (x+ )|. f (x ) indicates true optimal value objective functionf (x+ ) best value determined algorithm.function Shekel local optimizers slope surface generally becomeslarger increases. Therefore, Shekel 10 Rosenbrok 10, 10-dimensionality,generally difficult functions compared others experiment.Indeed, LOGO algorithm achieved acceptable performance these. Figure 6, see LOGO algorithm SOO algorithm fast. LOGOalgorithm often marginally slower SOO algorithm owing additionalcomputation required maintain supersets. reason BaMSOO algorithmrequired large computational cost horizontal axis points continued skipping conduct function evaluations (because evaluations judged174fiGlobal Continuous Optimization Error Bound Fast Convergence10001000100CPUtime(s)10.10.01SOOSA0.001020BaMSOOGA4060NCPUtime(s)1001010.1SABaMSOO0.01LOGO0.001800100200(a) Sin 1400600NSOO10.1800010000.010.0010200SASOOLOGO400 N 600GABaMSOO10010GABaMSOO11000N20003000400001000SASOOLOGO0.0010200400600NGABaMSOO10.1SASOOLOGO0.010.001800(g) Shekel 5200400600N8001000(h) Shekel 7400600N8001000SASOOLOGO10010GABaMSOO10.101000N200030004000(i) Shekel 10100000100000100001000SASOOLOGO10010GABaMSOO10.11000CPUtime(s)10000CPUtime(s)200GABaMSOO0.0101000GABaMSOOCPUtime(s)1000010CPUtime(s)100000100CPUtime(s)100010011000(f) Hartman 310000.1SASOOLOGO(e) Rosenbrock 21080010.10.00101000600N0.010.01800400100.1(d) Branin0.01200100SASOOLOGO1000CPUtime(s)1SOO10001000010GALOGO(c) Peaks1000001000.1SABaMSOO0.010.001(b) Sin 21000CPUtime(s)GALOGO10CPUtime(s)CPUtime(s)10SASOOLOGO10010GABaMSOO10.10.010.0101000N 2000(j) Hartman 63000400002000N 400060008000(k) Rosenbrock 10Figure 6: CPU time comparison: CPU time required achieve performance indicatedFigure 5beneficial based GP). effective mechanism BaMSOO avoid wasteful function evaluations; however, one must careful make sure function evaluationscostly, relative mechanism.summary, compared BaMSOO algorithm, LOGO algorithm fasterconsiderably simpler (in implementation parameter selection) strongertheoretical bases delivering superior performance experiments. comparedSOO algorithm, LOGO algorithm decreased theoretical convergence rateworst case analysis, exhibited significant improvements experiments.confirmed advantages LOGO algorithm, discuss possible limitations: scalability parameter sensitivity. scalability high dimensions175fiKawaguchi, Maruyama, & ZhengLog Distance OptimalLogDistancetoOptima0123SAGASOOBaMSOOREMBOLOGO456781-1-3-5w=1w=2w = 20adaptive w-7-9-11110N1001000110N1001000(b) Sensitivity local bias parameter w(a) Scalability: 1000-dimensional functionFigure 7: current possible limitations LOGOchallenge non-convex optimization general search space grows exponentiallyspace. However, may achieve scalability leveraging additional structuresobjective function present applications. example, Kawaguchi (2016b)showed instance deep learning models, objective functionadditional structure: nonexistence poor local minima. illustration, combineLOGO random embedding method, REMBO (Wang, Zoghi, Hutter, Matheson, &De Freitas, 2013), account another structure: low effective dimensionality. Figure 7 (a), report algorithms performances 1000 dimensional function: Sin 2embedded 1000 dimensions manner described Section 4.1 previousstudy (Wang et al., 2013).Another possible limitation LOGO sensitivity performance freeparameter w. Even though provided theoretical analysis insight effectparameter value previous section, yet unclear set w principlemanner. illustrate current limitation Figure 7 (b). result labeledadaptive w indicates result fixed adaptive mechanisms w useexperiments except ones Figure 7 (b) 8. illustration, useBranin function experiment conducted clearly illustrated limitation.seen figure, performance early stage always improved wincreases algorithm finds local optimum faster higher w. However, wlarge, w = 20 figure, algorithm gets stuck local optimumlong time. Thus, best value (or sweet spot) exists large smallvalues w. results experiment, seen choice w = 2best, finds global optima high precision within 200 function evaluations.However, limitation would serious problem practice following fourreasons. First, similar limitation exists, best knowledge, algorithmssuccessfully used practice (e.g., simulated annealing, genetic algorithm, swarmbased optimization, DIRECT algorithm, Bayesian optimization). Second, unlikeprevious algorithm, finite-time loss bound always applies even bad choicew. Third, demonstrated previous experiments simple adaptiverule may suffice produce good result. Also, future work may mitigatelimitation developing different methods adaptively determine value w. Also,176fiGlobal Continuous Optimization Error Bound Fast Convergenceanother possibility would conduct optimization w cheaper surrogate model.Finally, limitation may apply target objective functions all.fourth final reason, recall speculated algorithms analysisincreasing w would always beneficial effects problems, illustratedFigure 4. Clearly, problems within scope local optimization fall category.Figure 8, show rather unobvious instance problems, thus example,limitation parameter sensitivity apply. seendiagram left Figure 8, test function many local optima, oneglobal optimum. Nevertheless, diagram right, performanceLOGO algorithm improves w increases, harmful effect.Log Distance Optimal0fx1-2N = 50-4N = 100-6-8-10-12-14-16x2110100Local Bias Parameter: wFigure 8: example problems increasing w always better. diagramleft shows objective function, diagram right presents performanceN = 50 100 w.6. Planning LOGO via Policy Searchapply LOGO algorithm planning, important area fieldAI. goal planning problem find action sequence maximizestotal return infinite discounted horizon finite horizon (unlike classical planningproblem, consider constraints specify goal state). paper, discussformulations case infinite discounted horizon, argumentsapplicable case finite horizon straightforward modifications. considercase state/action space continuous, planning horizon long,transition reward functions known deterministic.planning problem formulated follows. Let RDS set states,RDA set actions, : RDS RDS transition function, R : RDS RDA Rreturn reward function, 1 discount factor. planner considerstake action state S, triggers transition another state basedtransition function , receiving return based reward function R.discount factor discounts future rewards fulfill either followingtwo roles: accounting relative importance immediate rewards compared future177fiKawaguchi, Maruyama, & Zhengrewards, obviating need think ahead toward infinite horizon. actionsequence represented policy maps state space action space:: RDS RDA .value action sequence policy , V , sum rewardsinfinite discounted horizon,V (s0 ) =XR(sj , (sj )).j=0value policy also written recursive formV (s) = R(s, (s)) + V (s, (s)) .(2)Here, interested finding optimal policy . dynamic programing approach, compute optimal policy, solving following Bellmans optimalityequation:V (s) = max R(s, a) + V (T (s, a))(3)V value optimal policy. Equation (3), optimal policy setactions defined max. major problem approach efficiencycomputation depends size state space. real-world application,state space usually large continuous, often makes impractical solveEquation (3).successful approach avoid state size dependency focus statespace reachable current state within planning time horizon.way, even infinitely large state space, planner needs consider finitelysized subset space. approach called local planning. Unlike local optimizationvs. global optimization, optimal solution local planning indeed globally optimal,given initial state. called local planning cover statessolution changes different initial states. Accordingly, initial state changes,planner may need conduct re-planning.natural way solve local planning use tree search methods, constructtree rooted initial state toward future possible states depth planninghorizon. tree search conducted using traditional search method, includinguninformed search (e.g., breadth-first depth-first search) informed (heuristic)search (e.g., search). Also, recent studies developed several tree-based algorithmsspecialized local planning. Among those, SOO algorithm, direct predecessor LOGO algorithm, applied local planning tree search approach(Busoniu, Daniels, Munos, & Babuska, 2013). new algorithms, example,HOLOP (Bubeck & Munos, 2010; Weinstein & Littman, 2012), operate stochastictransition functions.However efficient proposed algorithms are, search space tree searchapproach grows exponentially planning time horizon, H. Therefore, local planningtree search approach would work well long time horizon.applications, small H justified, applications, not. applicationproblem requires long time tradeoff immediate future rewards, tree178fiGlobal Continuous Optimization Error Bound Fast Convergencesearch approach would impractical. Here, motivated solve real-worldapplication, therefore need another approach.paper, consider policy search (Deisenroth, Neumann, & Peters, 2013)effective alternative solve planning problem continuous state/action spacelong time horizon. Policy search form local planning. Thus, like tree searchapproach, operates even infinitely large continuous state space. addition, unliketree search approach, policy search significantly reduces search space naturallyintegrating domain-specific expert knowledge structure policy.concretely, search space policy search set policies {x : x },parameterized vector x RD . Therefore, search space longer dependentplanning time horizon H, state space S, action space A,parameter space . Here, parameter space determined expert knowledge,significantly reduce search space.use regret rm measure policy search algorithms performance:+rm = V x (s0 ) V x (m) (s0 )x optimal policy given set policies {x : x }, x+ (m)best policy found algorithm steps planning. evaluationpolicy takes mH steps consider fixed planning horizon H. Here, x may differoptimal policy covered set {x : x }.policy search approach usually adopted gradient methods (Baxter & Bartlett,2001; Kober, Bagnell, & Peters, 2013; Weinstein, 2014). gradient method fast,converges local optima (Sutton, McAllester, Singh, & Mansour, 1999). Further,observed may result mere random walk large plateaus existsurface policy space (Heidrich-Meisner & Igel, 2008). Clearly, problemsresolved using global optimization methods cost scalability (Brochu et al.,2009; Azar, Lazaric, & Brunskill, 2014). Unlike previous policy search methods, methodguarantees finite-time regret bounds w.r.t. global optima {x : x } without strongadditional assumption, provides practically useful convergence speed.6.1 LOGO-OP Algorithm: Leverage (Unknown) Smoothness PolicySpace Planning Horizonsection, present simple modification LOGO algorithm leverageunknown smoothness policy space also known smoothness planninghorizon. former accomplished direct application LOGO algorithmpolicy search, latter modification section aims withoutlosing advantage original LOGO algorithm. call modified version, LocallyOriented Global Optimization Optimism Planning horizon (LOGO-OP). resultmodification, add new free parameter L.pseudocode LOGO-OP algorithm provided Algorithm 2. comparingAlgorithms 1 2, seen LOGO-OP algorithm functionsmanner LOGO algorithm, except line 15 (the function evaluation or, equivalently,policy evaluation policy search) line 20. Notice LOGO algorithmdirectly applicable policy search considering V f Algorithm 1.179fiKawaguchi, Maruyama, & ZhengLOGO algorithm assume structure function f , LOGO-OP algorithmfunctions exploits given structure value function V (i.e., MDP model).algorithm functions follows. policy evaluation performed policyx parameter x specified two new hyperrectangles (from line 15-115-11). Given initial condition s0 S, transition function , reward function R,discount factor 1, policy x , algorithm computes value policyEquation (2) (from line 15-2 line 15-10, except line 15-6).main modification appears line 15-6 algorithm leverages knownsmoothness planning horizon. Remember unknown smoothness policyspace (or input space x) specified f (x )f (x) `(x, x ) (from Assumption 1) thusinfers upper bound value policy yet evaluated similar (closepolicy space w.r.t. `) already evaluated polices. Conversely, known smoothnessplanning horizon renders upper bound value policy particularpolicy evaluated. is, known smoothness planning horizonwrittenXXt+1j R(sj , x (sj ))j R(sj , x (sj ))Rmax1j=0j=00 arbitrary point planning horizon line 15-3 Rmaxmaximum reward. known smoothness due definition Rmax sumgeometric series. case finite horizon H, formula( /(1 ))Rmax replaced (H t)Rmax . line 15-6, unlike originalLOGO algorithm, LOGO-OP algorithm terminates evaluation policycontinuation evaluating policy judged misuse computational resourcesbased known smoothness planning horizon. Concretely, terminatesevaluation policy upper bound value policy becomes less(V + L), V + value best policy found thus far L algorithmsparameter.upper bound value policy becomes less V + , plannerknow policy best policy. Thus, tempting simply terminatepolicy evaluation criterion. However, essence LOGO algorithmutilization unknown smoothness embedded surface value functionpolicy space. words, algorithm makes use result policy evaluation,whether policy best one not. interruption policy evaluation changesshape surface value function, interferes mechanismLOGO algorithm. Nevertheless, degree interruption likely beneficialsince goal find optimal policy instead surface analysis.LOGO-OP algorithm uses L determine degree interruption.+V monotonically increasing along execution, value policy fullyevaluated owing line 15-6 early iterations tends greater value policyfully evaluated later iterations. algorithm resolves problemline 20 biased divide interval evaluated early iteration.smaller L, LOGO-OP algorithm stop evaluation non-optimalpolicy earlier, cost accuracy evaluation value functions surface.larger L, algorithm needs spend time evaluation non-optimal policy,180fiGlobal Continuous Optimization Error Bound Fast ConvergenceAlgorithm 2: LOGO-OP algorithmInputs (problem): initial condition s0 S, transition function , rewardfunction R, discount factor 1 convergence criteria (or finite horizon H),policy space x : x RD .1: Inputs (parameter): search depth function hmax : Z+ [1, ), local weightw Z+ , stopping condition, maximum reward Rmax , parameter L.25: lines 25 exactly lines 25 Algorithm 16: Adds initial hyperrectangle 0 set: 0 0 {0 } (i.e., 0,0 = 0 )7: Evaluate value function V center point 0 , c0,0 : val [0,0 ] V (c0,0 ),V + val [0,0 ]8: iteration = 1, 2, 3, . . .9: val max , hplus hupper10: k = 0, 1, 2, . . . , max(bmin(hmax (n), hupper )/wc, hplus )11:Select hyperrectangle divided: (h, i) arg maxh,i val [h,i ] h, : h,i k12:val [h,i ] > val max13:val max val [h,i ], hplus 0, hupper max(hupper , h + 1), n n + 114:Divide hyperrectangle h,i along longest coordinate direction- three smaller hyperrectangles created left , center , right- val [center ] val [h,i ]15:Evaluate value function V center points two new hyperrectangles:151:policy x corresponding cleft cright152:z1 0, z2 1, s0153:= 0, 1, 2, . . . ,154:z1 z1 + z2 R(s, x (s))155:z2 z2 , (s, x (s)) ,156:z1 + ( t+1 /(1 ))Rmax < (V + L) Exit loop157:convergence criteria met Exit loop158:end159:save z1 value corresponding rectangle1510:val [left ] z1 val [right ] z11511:end1512:V + max(V + , val [left ], val [center ], val [right ])16:Group new hyperrectangles set h+1 remove original rectangle:h+1 h+1 {center , left , right }, h h \ h,i17:end18:stopping condition met Return (h, i) = arg maxh,i val [h,i ]19: end20: intervals val [] < (V + L) val [] (V + L)21: end0:obtain accurate estimate value functions surface. regretanalysis, show certain choice L ensures tighter regret bound compareddirect application LOGO algorithm.181fiKawaguchi, Maruyama, & Zheng6.2 Parallel Version LOGO-OP AlgorithmLOGO-OP algorithm presented Algorithm 2 four main procedures: Select (line11), Divide (line 14), Evaluate (line 15), Group (line 16). natural way parallelize algorithm decouple Select three procedures. is, letalgorithm first Select z hyperrectangles divided, allocate z numberDivide, Evaluate, Group z parallel workers. However, natural parallelizationdata dependency one Select another Select. words, procedurenext Select cannot start Divide, Evaluate, Group previous Selectfinalized. result, parallel overhead tends non-negligible. addition,Select chooses less hyperrectangles parallel workers, available resourcesparallel workers wasted. Indeed, latter problem tackled creating multipleinitial rectangles recent parallelization study DIRECT algorithm (He, Verstak,Sosonkina, & Watson, 2009). use multiple initial rectangles certainlymitigate problem, still allows occasional occurrence resource wastage,addition requiring user specify arrangement initial rectangles.solve problems, instead decouple Evaluate procedurethree procedures allocate Evaluate task parallel worker. callparallel version, pLOGO-OP algorithm. algorithm uses one master processconduct Select, Divide, Group operations arbitrary number parallel workersexecute Evaluate. main idea temporarily use artificial value assignmentcenter point hyperrectangle master process, overwrittentrue value parallel worker finishes evaluating center point. strategy,data dependency parallel workers occupied tasks almosttime. paper, use center value original hyperrectangledivision temporary artificial value, artificial value may computed usingadvanced method (e.g., methods surface analysis) future work. centerpoint initial hyperrectangle, simply assign worst possible value (ifknowledge regarding worst value, use ).master process keeps selecting new hyperrectangles unless parallel workersoccupied tasks. logic ensures parallel workers always tasksassigned master process, master process select many hyperrectangles based artificial information. Note parallelization makes senseEvaluate time consuming procedure, likely true policyevaluation.6.3 Regret Analysiscertain condition, finite-loss bounds LOGO algorithm directlytranslated regret bound LOGO-OP algorithm. condition mustmet (V + L) less center value optimal hyperintervalalgorithms execution. state regret bound concretely below. simplicity,use notion planning horizon H, effective (non-negligible) planninghorizon LOGO accordance discount factor, . Let H 0 effectiveplanning horizon LOGO-OP algorithm. Then, planning horizon LOGO-OP,H 0 , becomes smaller LOGO, H, algorithm finds improved function182fiGlobal Continuous Optimization Error Bound Fast Convergencevalues. LOGO-OP algorithm terminates policy evaluation line15-6 upper bound policy value determined lower (V + L).Corollary 3. Let H 0 H planning horizon used LOGO-OP algorithmpolicy evaluation. Let V + value best policy found algorithmiteration. Assume value function policy satisfies Assumptions 1 B1.(V + L) maintained less center value optimal hyperinterval,algorithm holds finite-time loss bound Theorem 2n.2H 0Proof. policy search special case optimization problem, trivialloss bound Theorem 2 holds LOGO algorithm applied policysearch. every function evaluation takes H steps planning horizon,n bm/2Hc case. LOGO-OP algorithm, effect new parameterL loss analysis takes place proof Lemma 2. (V + L) maintainedless center value optimal interval, statements proofhold true LOGO-OP algorithm well. Here, due effect L, function evaluationmay take less H steps planning horizon. Therefore, statementcorollary.tighten regret bound LOGO-OP algorithm decreasing L, sincealgorithm terminate evaluations unpromising policies earlier, meansvalue H 0 bound reduced. However, using small value Lviolates condition Corollary 3 leads us discard theoretical guarantee. Evencase, small value L results global search,consistency property, limn rn = 0, still trivially maintained. hand,set L = , LOGO-OP algorithm becomes equivalent direct applicationLOGO algorithm policy search, thus, regret bound Corollary 3H 0 = H.pLOGO-OP algorithm also maintains regret bound n = npnp counts number total divisions devoted set -optimal hyperinterval kw+l (l + 1) , (w 1) l 0. non-parallel versions ensuredevotion kw+l (l + 1) , parallelization makes possible conduct divisionhyperintervals. Thus, considering worst case, pLOGO-OP may improvebound proof procedure, although parallelization likely beneficial practice.6.4 Application Study Nuclear Accident Managementmanagement risk potentially hazardous complex systems, nuclearpower plants, major challenge modern society. section, apply proposedmethod accident management nuclear power plants demonstrate potentialutility usage method real-world application. focus assessingefficiency containment venting accident management measure obtainingknowledge effective operational procedure (i.e., policy ). problem requiresplanning continuous state space long planning horizon (H 86400),183fiKawaguchi, Maruyama, & Zhengdynamic programming (e.g., value iteration), tree-based planning (e.g., searchvariants) would work well (dynamic programming suffers cursedimensionality state space, search space tree-based methods grows exponentially planning horizon).Containment venting operation used maintain integrity containment vessel mitigate accident consequences releasing gases containment vessel atmosphere. accident Fukushima Daiichi nuclear powerplant 2011, containment venting activated essential accident managementmeasure. result, 2012, United States Nuclear Regulatory Commission (USNRC) issued order 31 nuclear power plants install containment vent system(USNRC, 2013). Currently, many countries considering improvement containment venting system operational procedures (OECD/NEA/CSNI, 2014).difficulty determining actual benefit effective operation comes factcontainment venting also releases fission products (radioactive materials) atmosphere. words, effective containment venting must trade future riskcontainment failure immediate release fission products (radioactive materials). experiments, use release amount main fission product compound,cesium iodide (CsI), measure effectiveness containment venting.nuclear accident management literature, integrated physics simulator usedmodel world dynamics transition function state space S.simulator adopt paper THALES2 (Thermal Hydraulics radionuclidebehavior Analysis Light water reactor Estimate Source terms severe accidentconditions) (Ishikawa, Muramatsu, & Sakamoto, 2002). Thus, transition functionstate space fully specified THALES2. initial condition s0 designedapproximately simulate accident Fukushima Daiichi nuclear power plant.experiment, focus single initial condition deterministic simulator,relaxation discussed next section. reward function R negativeamount CsI released atmosphere result state-action pair.use finite-time horizon H = 86400 seconds (24 hours), traditional first phasetime-window considered risk analysis nuclear power plant simulations (owingassumption 24 hours, many highly uncertain human operations expected).use following policy structure based engineering judgment.(1 ((FP x1 ) (Press x2 )) (Press > 100490),x =0 otherwise,x = 1 indicates implementation containment venting, FP (g) representsamount CsI gas phase suppression chamber, Press (kgf/m2 )pressure suppression chamber. Here, suppression chamber volumecontainment vessel connected atmosphere via containment ventingsystem. policy structure reflects engineering knowledge ventingdone fission products exist certain amount suppression chamber, operated pressure gets larger specific value.consider x1 = [0, 3000] x2 = [10810, 100490]. let x = 1 whenever pressureexceeds 100490 kgf/m2 , since containment failure considered probably occur af184fiCsI release Computed Policy (g)Global Continuous Optimization Error Bound Fast Convergence100001000100SOOLOGOLOGO-OPpLOGO-OP101010000200003000040000Wall time (s)Figure 9: Performance computed policy (CsI release) vs. Wall Time.ter pressure exceeds point. detail experimental setting outlinedAppendix A.first compare performance various algorithms problem.algorithms, used parameter settings benchmark tests Section 5.is, used hmax (n) = w n w simple adaptive procedure parameter wW = {3, 4, 5, 6, 8, 30}. LOGO-OP algorithm pLOGO-OP algorithm,blindly set L = 1000 (i.e., likely better parameter setting L). usedeight parallel workers pLOGO-OP algorithm.Figure 9 shows result comparison wall time 12 hours. vertical axistotal amount CsI released atmosphere (g), want minimize.Since conducted containment venting whenever pressure exceeded 100490 kgf/m2 ,containment failure prevented simulation experiments. Thus, lowervalue along vertical axis gets, better algorithms performances is. seen,new algorithms performed well compared SOO algorithm. also cleartwo modified versions LOGO algorithm improved performance original.LOGO-OP algorithm, effect L computational efficiency becomes greaterfound best policy improves. Indeed, LOGO algorithm required 10798 secondsten policy evaluations 52329 seconds 48 evaluations. LOGO-OP algorithmrequired 9297 seconds ten policy evaluations, 44678 seconds 48 evaluations.data conjunction Figure 9 illustrates property LOGO-OP algorithmpolicy evaluation becomes faster found best policy improves. pLOGOOP algorithm, number function evaluations performed algorithm increasedfactor approximately eight (the number parallel workers) compared nonparallel versions. Notice parallel version tends allocate extra resourcesglobal search (as opposed local search). focus local searchutilizing previous results policy evaluations; however, parallel version mustinitiate several policy evaluations without waiting previous evaluations, resultingtendency global search. tendency forced improvement, terms reducing185fiKawaguchi, Maruyama, & Zheng(1)(2)(4)(3)(5)(6)Venting (-) / CsI (g)Venting (-) / CsI (g)1Venting = 1: yes, 0:0.8CsI atmosphere0.60.40.20010000200003000040000500006000070000Time along Accident Progression (s)80000Time along Accident Progression (s)Figure 10: Action sequence generated found policy CsI releaseFigure 10: Action sequence generated found policy CsI releaseamount CsI, moderate relative number policy evaluationsparticular experiment. However, tendency may positive effect differentproblems increased global search beneficial. CPU time per policy evaluationvaried significantly different policies owing different phenomenon computedsimulator. average, LOGO-OP algorithm, took approximately 930 secondsper policy evaluation.partially confirmed validity pLOGO-OP algorithm, attemptuse provide meaningful information application field. Based examination results comparison, narrowed range parameter valuesx1 = [0, 1.2] x2 = [10330, 10910]. computation CPU time 86400 (s)eight workers parallelization, pLOGO-OP algorithm found policyx1 0.195 (g) x2 10880 (kgf/m2 ). policy determined, containmentfailure prevented total amount CsI released atmosphere limitedapproximately 0.5 (g) (approximately 0.002% total CsI) 24 hoursinitiation accident. major improvement scenarioexperimental setting considered result containment failure best, largeamount CsI release, 2000 (g) (about 10% total CsI) setting.computational cost CPU time 86400 (s) likely acceptable application field.terms computational cost, must consider two factors: offline computationvariation scenarios. computational cost CPU time 86400 (s) phenomenon requires 86400 (s) acceptable online computation (i.e., determiningsatisfactory policy accident progressing). However, computational costlikely acceptable consider preparing acceptable policies various scenariosoffline manner (i.e., determining satisfactory polices accident). informationregarding polices utilized accident first identifying accidentscenario heuristics machine learning methods (Park & Ahn, 2010). offline preparation, must determine policies various major scenarios thus computationtakes, example, one month, may acceptable.Note policy found method novel nontrivial literature,yet worked well. Accordingly, explain policy performed welldid. Figure 10 shows action sequence generated policy found amount186fiGlobal Continuous Optimization Error Bound Fast ConvergenceCsI (g) released versus accident progression time (s). analyze action sequencedividing six phases, indicated Figure 10, six numbers insideparentheses. first phase (1), venting conducted intermittently orderkeep pressure around x2 10880 (kgf/m2 ). phase, fission product yetreleased nuclear fuels. Reducing pressure heat donepreferably without releasing fission products, actions phase accomplish this.One may wonder venting done intermittently, instead continuingconduct venting reduce pressure much possible, done withoutrelease fission products phase. reducing pressure muchleads large difference pressures suppression chamber reactorpressure vessel, turn results large mass flow fission product transportationreactor pressure vessel suppression chamber (see Figure 11 Appendixinformation mass flow paths). increase amount fission productssuppression chamber likely result large release fission productsatmosphere venting conducted. Therefore, specific value x2 generatesintermittent venting works well first phase. second phase (2), containmentventing executed time since pressure suppression chamber increasesrapidly phase (due operation depressurizing reactor pressure vessel viaSRV line), thus, criterion (Press x2 ) policy satisfied timepoint. beginning third phase (3), amount CsI suppressionchamber exceeds x1 0.195 (g) thereby venting conducted. fourth phase(4), pressure reaches 100490 (kgf/m2 ) containment venting intermittently doneorder keep pressure point avoid catastrophic containment failure.fifth phase (5), containment vent kept open amount CsI gasphase suppression chamber decreases x1 (due phenomenon illustratedFigure 12 Appendix A). continuous containment venting decreases pressureventing required terms pressure final phase (6), ventingconducted also amount CsI becomes larger x1 .Thus, clear policy found AI-related method also basisterms physical phenomenon. addition, generated action sequence likelysimple enough engineer discover several sensitivity analyses. particular,method solve known tradeoff immediate CsI releaserisk future containment failure, method also discovered existence new tradeoffimmediate reduction pressure without CsI release future increasemass flow. Although consensus operate containment ventingsystem moment, tendency use pressure exceeds certainpoint order prevent immediate sever damage containment vessel, correspondsfourth phase (4) Figure 10. experiment, myopic operationresulted containment failure, significantly large amount CsI releasedatmosphere (at least 4800 (g)).summary, successfully applied proposed method investigate containmentventing policy nuclear power plant accidents. preliminary application study, severaltopics left future work. theoretical viewpoint, future work considerway mitigate simulation bias due model error model uncertainty.model error, robotics community already cognizant small error simulator187fiKawaguchi, Maruyama, & Zhengresult poor performance derived policy (i.e., simulation bias) (Kober et al.,2013). mitigate problem adding small noise model, sincenoise works regularization prevent over-fitting demonstrated Atkeson (1998).model uncertainty, recent studies field nuclear accident analysis providepossible directions treatment uncertainty accident phenomena (Zheng, Itoh,Kawaguchi, Tamaki, & Maruyama, 2015) accident scenarios (Kawaguchi, Uchiyama, &Muramatsu, 2012). result either countermeasures, objectivefunction becomes stochastic, thereby may first expand pLOGO-OP algorithmstochastic case. hand, phenomenological point view, future workconsider fission products well CsI. fission products include,limited to, Xe, Cs, I, Te, Sr, Ru. particular, noble gas element, Xe,major concern accident (it tends released lot easily diffusedatmosphere), property different CsI (its half-life much smaller). Thus,Xe identified major concern, one may consider significantly different policy(considering half-life, one may delay conducting containment venting).7. Conclusionspaper, proposed LOGO algorithm, global optimization algorithmdesigned operate well practice maintaining finite-loss bound strongadditional assumption. analysis LOGO algorithm generalized previous finite-lossbound analysis. Importantly, analysis also provided several insights regarding practicalusage type algorithm showing relationship among loss bound,division strategy, algorithms parameters.applied LOGO algorithm AI planning problem policy searchframework, showed performance algorithm improved leveraging unknown smoothness policy space, also known smoothnessplanning horizon. study motivated solve real-world engineering applications, also discussed parallelization design utilizes property AI planningorder minimize overhead. resulting algorithm, pLOGO-OP algorithm,successfully applied complex engineering problem, namely, policy derivation nuclearaccident management.Aside planning problem considered, LOGO algorithm alsoused, example, optimize parameters algorithms (i.e., algorithm configuration). AI community, algorithm configuration problem addressedseveral methods, including genetic algorithm (Ansotegui, Sellmann, & Tierney, 2009), discrete optimization convergence guarantee limit (Hutter, Hoos, Leyton-Brown, &Stutzle, 2009), racing approach originated machine learning community (Hoeffding Races) (Birattari, Yuan, Balaprakash, & Stutzle, 2010), model-based optimizationconvergence guarantee limit (Hutter, Hoos, & Leyton-Brown, 2011), simultaneous use several randomized local optimization methods (Gyorgy & Kocsis, 2011),Bayesian optimization (Snoek, Larochelle, & Adams, 2012). Compared previousparameter tuning methods, LOGO algorithm limited optimizing continuousdeterministic functions. apply stochastic functions, future work would modifyLOGO algorithm done SOO algorithm previous study (Valko, Car188fiGlobal Continuous Optimization Error Bound Fast Convergencepentier, & Munos, 2013). consider categorical and/or discrete parameters additioncontinuous parameters, possibility could use LOGO algorithm subroutinedeal continuous variables one previous methods.promising results presented paper suggest several interesting directionsfuture research. important direction leverage additional assumptions. Since LOGObased weak set assumptions, would natural use LOGO main subroutine add mechanisms account additional assumptions. example,illustrated LOGO would able scale higher dimension additionalassumptions Section 5. Another possibility add GP assumption basedidea presented recent paper (Kawaguchi, Kaelbling, & Lozano-Perez, 2015). Futurework also would design autonomous agent integrating planning algorithmlearning/exploration algorithm (Kawaguchi, 2016a). One remaining challenge LOGOderive series methods adaptively determine algorithms free parameterw. illustrated experiment, achievement topic mitigatesproblem parameter sensitivity, also would improve algorithms performance.Acknowledgmentswork carried first author Japan Atomic Energy Agency.authors would like thank Dr. Hiroto Itoh Mr. Jun Ishikawa JAEA severaldiscussions related topics. authors would like thank Mr. Lawson WongMIT insightful comments. authors would like thank anonymous reviewersinsightful constructive comments.Appendix A. Experimental Design Application Study NuclearAccident Managementappendix, present experimental setting Application Study NuclearAccident Management Section 6.4. THALES2, consider volume nodalization shown Figure 11. reactor pressure vessel divided seven volumes,consisting core, upper plenum, lower plenum, steam dome, downcomer, recirculationloops B. containment vessel consists drywell, suppression chamber, pedestalvent pipes. atmosphere suppression chamber connected via containment venting system (S/C venting). plant data initial conditions determinedbased data Unit 1 Browns Ferry nuclear power plant (BWR4/Mark-I)construction permit application forms BWR plants Japan. failurecontainment vessel assumed occur pressure vessel becomes 2.5times greater design pressure. Here, design pressure 3.92 (kgf/cm2 g)criterion containment failure 108330 kgf/m2 . degree openingcontainment venting fixed 25% filtering considered.consider TQUV sequence accident scenario. TQUV sequence,Emergency Core Cooling Systems (ECCSs) functions, similar case accidentFukushima Daiichi nuclear power plant. TQUV sequence one majorscenarios considered Probabilistic Risk Assessment (PRA) nuclear plants. Therefore,189fiFigure 11: Nodalization physical spaceKawaguchi, Maruyama, & ZhengReactor buildingContainmentCSTSteamdomeRCICHPCIPlenumSRVDCCoreLPCILowerplenumLoopLoop BVacuumbreakerPedestalVentpipeSuppression chamberContainment VentFigure 11: Nodalization physical spaceFP: Fission ProductsFP: Fission Products12: PhenomenonconsideredfissionproducttransportationtransportationFigure 1:FigurePhenomenonconsideredfissionproduct190fiGlobal Continuous Optimization Error Bound Fast Convergenceresults show promising benefit containment venting, long usegood policy.simulator developed Japan Atomic Energy Agency adoptedexperiment (THALES2) computes transportation fission products well thermalhydraulics volume Figure 11 core melt progression Core volume.transportation fission products considered experiment shown Figure 12.details computation THALES2 code found paper Ishikawa et al.(2002). Figure 11 Figure 12 modified versions graphs used previouspresentation ongoing development THALES2 code, givenUSNRCs 25th Regulatory Information Conference (Maruyama, 2013).ReferencesAnsotegui, C., Sellmann, M., & Tierney, K. (2009). gender-based genetic algorithmautomatic configuration algorithms. Proceedings 15th InternationalConference Principles Practice Constraint Programing (CP 2009).Atkeson, C. G. (1998). Nonparametric model-based reinforcement learning. AdvancesNeural Information Processing Systems (NIPS), pp. 10081014.Azar, M. G., Lazaric, A., & Brunskill, E. (2014). Stochastic optimization locallysmooth function correlated bandit feedback. 31st International ConferenceMachine Learning (ICML).Baxter, J., & Bartlett, P. L. (2001). Infinite-horizon policy-gradient estimation. JournalArtificial Intelligence Research (JAIR), 15, 319350.Birattari, M., Yuan, Z., Balaprakash, P., & Stutzle, T. (2010). F-race iterated F-race:overview. Experimental Methods Analysis Optimization Algorithms,pp. 311336. Springer-Verlag.Brochu, E., Cora, V. M., & de Freitas, N. (2009). tutorial Bayesian optimizationexpensive cost functions, application active user modeling hierarchical reinforcement learning. Technical report No. UBC TR-2009-23 arXiv:1012.2599v1,Dept. Computer Science, University British Columbia.Bubeck, S., & Munos, R. (2010). Open loop optimistic planning. Conference LearningTheory.Bubeck, S., Stoltz, G., & Yu, J. Y. (2011). Lipschitz bandits without Lipschitz constant.Proceedings 22nd International Conference Algorithmic Learning Theory.Busoniu, L., Daniels, A., Munos, R., & Babuska, R. (2013). Optimistic planningcontinuous-action deterministic systems. 2013 Symposium Adaptive DynamicProgramming Reinforcement Learning.Carter, R. G., Gablonsky, J. M., Patrick, A., Kelly, C. T., & Eslinger, O. J. (2001). Algorithms noisy problems gas transmission pipeline optimization. OptimizationEngineering, 2 (2), 139157.Daly, R., & Shen, Q. (2009). Learning Bayesian network equivalence classes ant colonyoptimization. Journal Articial Intelligence Research, 35 (1), 391447.191fiKawaguchi, Maruyama, & ZhengDeisenroth, M. P., Neumann, G., & Peters, J. (2013). survey policy search robotics.Foundations Trends Robotics, 2, 1142.Deisenroth, M. P., & Rasmussen, C. E. (2011). PILCO: model-based data-efficientapproach policy search. 28th International Conference Machine Learning(ICML).Dixon, L. C. W. (1978). Global Optima Without Convexity. Hatfield, England: NumericalOptimization Centre, Hatfield Polytechnic.Gablonsky, J. M. (2001). Modifications direct algorithm. Ph.D. thesis, North CarolinaState University, Raleigh, North Carolina.Gullapalli, V., Franklin, J., & Benbrahim, H. (1994). Acquiring robot skills via reinforcementlearning. Control Systems Magazine, IEEE, 14 (1), 1324.Gyorgy, A., & Kocsis, L. (2011). Efficient multi-start strategies local search algorithms.Journal Artificial Intelligence Research (JAIR), 41, 407444.Hansen, P., & Jaumard, B. (1995). Lipschitz optimization. Horst, R., & Pardalos,P. M. (Eds.), Handbook Global Optimization, pp. 407493. Netherlands: KluwerAcademic Publishers.Hansen, P., Jaumard, B., & Lu, S. H. (1991). number iterations Piyavskiisglobal optimization algorithm. Mathematics Operations Research, 16, 334350.He, J., Verstak, A., Sosonkina, M., & Watson, L. (2009). Performance modeling analysismassively parallel DIRECT, Part 1. Journal High Performance ComputingApplications, 23, 1428.He, J., Verstak, A., Watson, L. T., Stinson, C. A., et al. (2004). Globally optimal transmitterplacement indoor wireless communication systems. IEEE Transactions WirelessCommunications, 3 (6), 19061911.Heidrich-Meisner, V., & Igel, C. (2008). Evolution strategies direct policy search.Proceedings 10th International Conference Parallel Problem SolvingNature: PPSN X, pp. 428437. Springer-Verlag.Horst, R., & Tuy, H. (1990). Global Optimization: Deterministic Approaches. Berlin:Springer.Hutter, F., Hoos, H. H., & Leyton-Brown, K. (2011). Sequential model-based optimizationgeneral algorithm configuration. Learning Intelligent Optimization, 5, 507523.Hutter, F., Hoos, H. H., Leyton-Brown, K., & Stutzle, T. (2009). ParamILS: automaticalgorithm configuration framework. Journal Artificial Intelligence Research (JAIR),36, 267306.Ishikawa, J., Muramatsu, K., & Sakamoto, T. (2002). Systematic source term analysislevel 3 PSA BWR Mark-II containment THALES-2 code. Proceedings10th International Conference Nuclear Engineering, ICONE-10-22080.Jones, D. R., Perttunen, C. D., & Stuckman, B. E. (1993). Lipschitzian optimization withoutLipschitz constant. Journal Optimization Theory Applications, 79 (1), 157181.192fiGlobal Continuous Optimization Error Bound Fast ConvergenceKawaguchi, K., Uchiyama, T., & Muramatsu, K. (2012). Efficiency analytical methodologies uncertainty analysis seismic core damage frequency. Journal PowerEnergy Systems, 6 (3), 378393.Kawaguchi, K. (2016a). Bounded optimal exploration MDP. Proceedings 30thAAAI Conference Artificial Intelligence (AAAI).Kawaguchi, K. (2016b). Deep learning without poor local minima. Massachusetts InstituteTechnology, Technical Report, MIT-CSAIL-TR-2016-005.Kawaguchi, K., Kaelbling, L. P., & Lozano-Perez, T. (2015). Bayesian optimizationexponential convergence. Advances Neural Information Processing (NIPS).Kirk, D. E. (1970). Optimal Control Theory. Englewood Cliffs, NJ: Prentice-Hall.Kleinberg, R. D., Slivkins, A., & Upfal, E. (2008). Multi-armed bandit problems metricspaces. Proceedings 40th ACM Symposium Theory Computing, pp.681690.Kober, J., Bagnell, J. A. D., & Peters, J. (2013). Reinforcement learning robotics:survey. International Journal Robotics Research, 32.Kvasov, D. E., Pizzuti, C., & Sergeyev, Y. D. (2003). Local tuning partition strategiesdiagonal GO methods. Numerische Mathematik, 94 (1), 93106.Maruyama, Y. (2013). Development THALES2 code application analysisaccident Fukushima Daiichi Nuclear Power Plant. NRCs 25th RegulatoryInformation Conference.Mayne, D. Q., & Polak, E. (1984). Outer approximation algorithm nondifferentiableoptimization problems. Journal Optimization Theory Applications, 42 (1), 1930.McDonald, D. B., Grantham, W. J., Tabor, W. L., & Murphy, M. J. (2007). Globallocal optimization using radial basis function response surface models. Applied Mathematical Modelling, 31 (10), 20952110.Mladineo, R. H. (1986). algorithm finding global maximum multimodal,multivariate function. Mathematical Programming, 34, 188200.Munos, R. (2011). Optimistic optimization deterministic functions without knowledgesmoothness. Advances Neural Information Processing Systems (NIPS), pp.783791.Munos, R. (2013). bandits Monte-Carlo tree search: optimistic principle appliedoptimization planning. Foundations Trends Machine Learning, 7 (1),1130.Murty, K. G., & Kabadi, S. N. (1987). np-complete problems quadraticnonlinear programming. Mathematical programming, 39 (2), 117129.OECD/NEA/CSNI (2014). Status report filtered containment venting. Technical reportNEA/CSNI/R(2014)7, JT03360082.Park, Y., & Ahn, I. (2010). SAMEX: severe accident management support expert. AnnalsNuclear Energy, 37 (8), 10671075.193fiKawaguchi, Maruyama, & ZhengPinter, J. (1986). Globally convergent methods n-dimensional multiextremal optimization. Optimization, 17, 187202.Piyavskii, S. A. (1967). algorithm finding absolute minimum function.Theory Optimal Solutions, 2, 1324. Kiev, IK USSR.Rios, L. M., & Sahinidis, N. V. (2013). Derivative-free optimization: review algorithmscomparison software implementations. Journal Global Optimization, 56,12471293.Russell, S. J., & Norvig, P. (2009). Articial intelligence: modern approach (3rd edition).Prentice-Hall.Ryoo, H. S., & Sahinidis, N. V. (1996). branch-and-reduce approach global optimization. Journal Global Optimization, 8 (2), 107138.Shubert, B. O. (1972). sequential method seeking global maximum function.SIAM Journal Numerical Analysis, 9, 379388.Snoek, J., Larochelle, H., & Adams, R. P. (2012). Practical Bayesian optimization machine learning algorithms. Advances Neural Information Processing Systems(NIPS).Strongin, R. G. (1973). convergence algorithm finding global extremum.Engineering Cybernetics, 11, 549555.Surjanovic, S., & Bingham, D. (2013). Virtual library simulation experiments: Testfunctions datasets. Retrieved July 2, 2014, http://www.sfu.ca/~ssurjano.Sutton, R. S., McAllester, D., Singh, S., & Mansour, Y. (1999). Policy gradient methods reinforcement learning function approximation. Advances NeuralInformation Processing Systems, 12, 10571063.USNRC (2013). Hardened Vents Filtration Boiling Water Reactors MarkMark II containment designs . Retrieved August 2015 http://www.nrc.gov/reactors/operating/ops-experience/japan-dashboard/hardened-vents.html .Valko, M., Carpentier, A., & Munos, R. (2013). Stochastic simultaneous optimistic optimization. Proceedings 30th International Conference Machine Learning(ICML).Wang, Z., Shakibi, B., Jin, L., & de Freitas, N. (2014). Bayesian multi-scale optimisticoptimization. AI Statistics, pp. 10051014.Wang, Z., Zoghi, M., Hutter, F., Matheson, D., & De Freitas, N. (2013). Bayesian optimization high dimensions via random embeddings. Proceedings Twenty-Thirdinternational joint conference Artificial Intelligence, pp. 17781784. AAAI Press.Weinstein, A. (2014). Local planning continuous Markov decision processes. Ph.D.thesis, Rutgers, State University New Jersey.Weinstein, A., & Littman, M. L. (2012). Bandit-based planning learning continuousaction Markov decision processes. International Conference Automated PlanningScheduling, pp. 306314.194fiGlobal Continuous Optimization Error Bound Fast ConvergenceZheng, X., Itoh, H., Kawaguchi, K., Tamaki, H., & Maruyama, Y. (2015). ApplicationBayesian nonparametric models uncertainty sensitivity analysis sourceterm BWR severe accident. Journal Reliability Engineering & System Safety,138, 253262.Zwolak, J. W., Tyson, J. J., & Watson, L. T. (2005). Globally optimized parametersmodel mitotic control frog egg extracts. IEE Systems Biology, 152 (2), 8192.195fiJournal Artificial Intelligence Research 56 (2016) 403428Submitted 11/15; published 07/16Satisfiability Problem SPARQL PatternsXiaowang Zhangxiaowangzhang@tju.edu.cnSchool Computer Science Technology,Tianjin University, ChinaTianjin Key LaboratoryCognitive Computing Application,Tianjin, ChinaJan Van den Busschejan.vandenbussche@uhasselt.beHasselt University, BelgiumFrancois Picalausafpicalausa@gmail.comAbstractsatisfiability problem SPARQL 1.0 patterns undecidable general, sincerelational algebra emulated using patterns. goal paper delineateboundary decidability satisfiability terms constraints allowed filterconditions. classes constraints considered bound-constraints, negated boundconstraints, equalities, nonequalities, constant-equalities, constant-nonequalities.main result paper summarized saying that, soon inconsistent filterconditions formed, satisfiability undecidable. key insight case findway emulate set difference operation. Undecidability obtainedknown undecidability result algebra binary relations union, composition,set difference. inconsistent filter conditions formed, satisfiabilitydecidable syntactic checks bound variables use literals. Althoughproblem shown NP-complete, experimentally shown checksimplemented efficiently practice. paper also points satisfiabilityso-called well-designed patterns decided check bound variables checkinconsistent filter conditions.1. IntroductionResource Description Framework popular data model information Web.RDF represents information form directed, labeled graphs. standard querylanguage RDF data SPARQL (Harris & Seaborne, 2013). current version 1.1SPARQL extends SPARQL 1.0 (Prudhommeaux & Seaborne, 2008) important features aggregation regular path expressions (Arenas, Conca, & Perez, 2012).features, negation subqueries, also added, mainly efficiency reasons, already expressible, involved manner, version 1.0.Hence, still relevant study fundamental properties SPARQL 1.0. paper,follow elegant formalization SPARQL 1.0 Arenas, Gutierrez, & Perez (2009)eminently suited theoretical investigations.fundamental problem investigate satisfiability SPARQL patterns. pattern called satisfiable exists RDF graph patternevaluates nonempty set mappings. query language, satisfiability clearly onec2016AI Access Foundation. rights reserved.fiZhang, Van den Bussche, & Picalausaessential properties one needs understand one wants automated reasoning.Since SPARQL patterns emulate relational algebra expressions (Angles & Gutierrez,2008; Polleres, 2007; Arenas & Perez, 2011), satisfiability relational algebra undecidable (Abiteboul, Hull, & Vianu, 1995), general satisfiability problem SPARQLundecidable well.Whether pattern satisfiable depends mainly filter operations appearingpattern; without filter operations, pattern always satisfiable except trivial casesliteral occurs wrong place. goal paper precisely delineatedecidability SPARQL fragments defined terms constraintsused filter conditions. six basic classes constraints consider boundconstraints; equalities; constant-equalities; negations. way, fragmentsSPARQL constructed specifying kinds constraints allowed filterconditions. example, fragment SPARQL(bound, 6=, 6=c ), filter conditionsbound constraints, nonequalities, constant-nonequalities.main result states fragments satisfiability decidabletwo fragments SPARQL(bound, =, 6=c ) SPARQL(bound, 6=, 6=c ) subfragments. Consequently, soon either negated bound-constraints, constant-equalities,combinations equalities nonequalities allowed, satisfiability problem becomesundecidable. undecidable case established showing set difference operation emulated. already known using negated bound-constraints (Angles& Gutierrez, 2008; Arenas & Perez, 2011); show also possible using constantequalities, using combinations equalities nonequalities, way.Undecidability obtained known undecidability result algebrabinary relations union, composition, set difference (Tan, Van den Bussche, &Zhang, 2014).decidable cases, satisfiability decided syntactic checks bound variables use literals. Although problem shown NP-complete,experimentally shown checks implemented efficiently practice.end paper look well-behaved class patterns knownwell-designed patterns (Perez et al., 2009). observe satisfiability well-designedpatterns decided combining check bound variables check inconsistent filter conditions.paper organized follows. next section, introduce syntaxsemantics SPARQL patterns introduce different fragments consideration. Section 3 introduces satisfiability problem shows satisfiability checkingfragments SPARQL(bound, =, 6=c ) SPARQL(bound, 6=, 6=c ). Section 4 shows undecidability fragments SPARQL(bound), SPARQL(=c ), SPARQL(=, 6=). Section 5considers well-designed patterns.Section 6 reports experiments test decision methods practice. Section 7briefly discuss results extend new operators addedSPARQL 1.1. conclude Section 8.404fiSatisfiability Problem SPARQL2. SPARQL Fragmentssection recall syntax semantics SPARQL patterns, closely followingcore SPARQL formalization given Arenas, Gutierrez, & Perez (2009).1 semanticsuse set-based, whereas semantics real SPARQL bag-based. However,satisfiability (the main topic paper), makes difference whether use setbag semantics (Schmidt, Meier, & Lausen, 2010, Lemma 1).section also define language fragments defined terms allowedfilter conditions, form object paper.2.1 RDF GraphsLet I, B, L infinite sets IRIs, blank nodes literals, respectively. threesets pairwise disjoint. denote union B L U , elements Lreferred constants. Note blank nodes constants.triple (s, p, o) (I B) U called RDF triple. RDF graph finite setRDF triples.2.2 Syntax SPARQL PatternsAssume furthermore infinite set V variables, disjoint U . conventionSPARQL variables written beginning question mark, distinguishconstants. follow convention paper.SPARQL patterns inductively defined follows.triple (I L V ) (I V ) (I L V ) pattern (called triple pattern).P1 P2 patterns, following:P1 UNION P2 ;P1 P2 ;P1 OPT P2 .P pattern C constraint (defined next), P FILTER C pattern;call C filter condition.Here, constraint one six following forms:1. bound-constraint: bound(?x)2. negated bound-constraint: bound(?x)3. equality: ?x = ?y4. nonequality: ?x 6= ?y ?x ?y distinct variables5. constant-equality: ?x = c c constant6. constant-nonequality: ?x 6= c1. Arenas, Perez, Guttierez (2009) discuss minor deviations formalization realSPARQL, differences inessential purpose formal investigation.405fiZhang, Van den Bussche, & Picalausaneed consider conjunctions disjunctions filter conditions, sinceconjunctions expressed repeated application filter, disjunctionsexpressed using UNION. Hence, going disjunctive normal form, predicate builtusing negation, conjunction, disjunction indirectly supported language.Moreover, real SPARQL also allows blank nodes triple patterns. featureomitted formalization blank nodes triple patterns equivalentlyreplaced variables.2.3 Semantics SPARQL Patternssemantics patterns defined terms sets so-called solution mappings, hereinafter simply called mappings. solution mapping total function : Ufinite set variables. denote domain dom().make use following convention.Convention. mapping constant c L, agree (c) equals citself.words, mappings default extended constants according identitymapping.given graph G pattern P , define semantics P G, denotedJP KG , set mappings, following manner.P triple pattern (u, v, w),JP KG := { : {u, v, w} V U | ((u), (v), (w)) G}.definition relies Convention 2.3 formulated above.P form P1 UNION P2 ,JP KG := JP1 KG JP2 KG .P form P1 P2 ,JP KG := JP1 KGn JP2 KG ,where, two sets mappings 1 2 , define1n 2 = {1 2 | 1 1 2 2 1 2 }.Here, two mappings 1 2 called compatible, denoted 1 2 , agreeintersection domains, i.e., every variable ?x dom(1 )dom(2 ),1 (?x) = 2 (?x). Note 1 2 compatible, union 1 2well-defined mapping; property used formal definition above.P form P1 OPT P2 ,JP KG := (JP1 KGn JP2 KG ) (JP1 KG r JP2 KG ),where, two sets mappings 1 2 , define1 r 2 = {1 1 | 2 2 : 1 2 }.406fiSatisfiability Problem SPARQLFinally, P form P1 FILTER C,JP KG := { JP1 KG | |= C}satisfaction constraint C mapping , denoted |= C, definedfollows:1. |= bound(?x) ?x dom();2. |= bound(?x) ?x/ dom();3. |= ?x = ?y ?x, ?y dom() (?x) = (?y);4. |= ?x 6= ?y ?x, ?y dom() (?x) 6= (?y);5. |= ?x = c ?x dom() (?x) = c;6. |= ?x 6= c ?x dom() (?x) 6= c.Note |= ?x 6= ?y 6|= ?x = ?y, similarly |= ?x 6= c.line three-valued logic semantics filter conditions used officialsemantics (Arenas et al., 2009). example, ?x/ dom(), three-valued logic?x = c evaluates error ; consequently, also ?x = c evaluates error .Accordingly, semantics above, 6|= ?x = c 6|= ?x 6= c.2.4 SPARQL Fragmentsform fragments SPARQL specifying six classes constraintsallowed filter conditions. denote class bound-constraints bound, negatedbound-constraints bound, equalities =, nonequalities 6=, constant-equalities=c , constant-nonequalities 6=c . subset F {bound, bound, =, 6=, =c , 6=c } form fragment SPARQL(F ). example, SPARQL(bound, =,6=c ), filter conditions bound constraints, equalities, constant-nonequalities.3. Satisfiability: Decidable Fragmentspattern P called satisfiable exists graph G JP KG nonempty.general, checking satisfiability complicated, indeed undecidable, problem.two fragments SPARQL(bound, =, 6=c ) SPARQL(bound, 6=, 6=c ), turnessentially two possible reasons unsatisfiability.first possible reason pattern specifies literal value first positionRDF triple, whereas RDF triples literals third position.example, using literal 42, triple pattern (42, ?x, ?y) unsatisfiable. Note literalsmiddle position triple pattern already disallowed definition triplepattern, need worry first position.discrepancy triple patterns RDF triples easy sidestep, however.Appendix show how, without loss generality, may assumepatterns contain triple pattern (u, v, w) u literal.second main possible reason unsatisfiability filter conditions requirevariables bound together way cannot satisfied subpattern407fiZhang, Van den Bussche, & Picalausafilter applies. example, pattern((?x, a, ?y) UNION (?x, b, ?z)) FILTER (bound(?y) bound(?z))unsatisfiable. Note bound constraints strictly necessary illustratephenomenon: example replace filter condition ?y = ?z resultingpattern still unsatisfiable.next prove formally satisfiability patterns SPARQL(bound, =, 6=c )SPARQL(bound, 6=, 6=c ) effectively decidable, catching reason unsatisfiabilitydescribed above. Note also two fragments combined, since satisfiabilitySPARQL(=, 6=) undecidable see next Section.3.1 Checking Bound Variablesperform bound checks variables, associate every pattern P set (P ) schemes,scheme simply set variables, following way.2P triple pattern (u, v, w), (P ) := {{u, v, w} V }.(P1 UNION P2 ) := (P1 ) (P2 ).(P1 P2 ) := {S1 S2 | S1 (P1 ) S2 (P2 )}.(P1 OPT P2 ) := (P1 P2 ) (P1 ).(P1 FILTER C) := {S (P1 ) | ` C}, ` C defined follows:C form bound(?x) ?x = c ?x 6= c, ` C ?x S;C form ?x = ?y ?x 6= ?y, ` C ?x, ?y S;` bound(?x) ?x/ S.Example 1. Consider patternP = (?x, p, ?y) OPT ((?x, q, ?z) UNION (?x, r, ?u)).subpattern P1 = (?x, q, ?z) UNION (?x, r, ?u) (P1 ) = {{?x, ?z}, {?x, ?u}}.Hence, ((?x, p, ?y) P1 ) = {{?x, ?y, ?z}, {?x, ?y, ?u}}. conclude (P ) ={{?x, ?y}, {?x, ?y, ?z}, {?x, ?y, ?u}}.Example 2. another example, consider patternP = ((?x, p, ?y) OPT ((?x, q, ?z) FILTER ?y = ?z)) FILTER ?x 6= c.(?x, q, ?z) = {{?x, ?z}}. Note {?x, ?z} 6` ?y = ?z, ?y/ {?x, ?z}.Hence, subpattern P1 = (?x, q, ?z) FILTER ?y = ?z (P1 ) = .subpattern P2 = (?x, p, ?y) OPT P1 (P2 ) = (?x, p, ?y) = {{?x, ?y}}. Since{?x, ?y} ` ?x 6= c, conclude (p) = {{?x, ?y}}.2. define (P ) general patterns, belonging fragments consideredSection, make another use (P ) Section 5.408fiSatisfiability Problem SPARQLestablish main result Section.Theorem 3. Let P SPARQL(bound, =, 6=c ) SPARQL(bound, 6=, 6=c ) pattern.P satisfiable (P ) nonempty.only-if direction Theorem 3 easy direction given followingLemma 4. Note lemma holds general patterns; straightforwardlyproven induction structure P .Lemma 4. Let P pattern G graph. JP KG exists (P )dom() = S.direction Theorem 3 SPARQL(bound, =, 6=c ) given followingLemma 5.following use var(P ) denote set variables occurring patternP .3Lemma 5. Let P pattern SPARQL(bound, =, 6=c ). Let c constantappear constant-nonequality filter condition P . constant mapping: var(P ) {c}, let G RDF graph consisting possible triples ((u), (v), (w))(u, v, w) triple pattern P .every (P ) exists 0 |S 0 belongs JP KG .Proof. induction structure P . P triple pattern (u, v, w) ={u, v, w} V . Since (|S (u), |S (v), |S (w)) = ((u), (v), (w)) G, |S JP KGtake 0 = S.P form P1 UNION P2 , claim follows readily induction.P form P1 P2 , = S1 S2 Si (Pi ) = 1, 2.induction, exists Si0 Si |Si0 JPi KG . Clearly |S10 |S20 sincerestrictions mapping. Hence |S10 |S20 = S10 S20 JP KG take0 = S10 S20 .P form P1 OPT P2 , two possibilities.(P1 P2 ) reason previous case.(P1 ) induction exists S10 |S10 JP1 KG .two possibilities:(P2 ) nonempty induction exists S20 |S20 JP2 KG .reason case P1 P2 .Otherwise, Lemma 4 know JP2 KG empty. JP KG = JP1 KGtake 0 = S10 .Finally, P form P1 FILTER C, know (P1 ) ` C.induction, exists 0 |S 0 JP1 KG . show |S 0 JP KGshowing |S 0 |= C. three possibilities C.3. also use following standard notion restriction mapping. f : X total functionZ X, restriction f |Z f Z total function Z defined f |Z (z) = f (z)every z Z. is, f |Z f defined subdomain Z.409fiZhang, Van den Bussche, & PicalausaC form bound(?x), know ` C ?x 0 . Hence |S 0 |= C.C form ?x = ?y, know ?x, ?y 0 , certainly |S 0 |= Csince maps everything c.C form ?x 6= d, 6= c choice c, |S 0 |= C since(?x) = c.Example 6. illustrate Lemma, consider patternP = ((?x, p, ?y) FILTER ?x 6= a) OPT ((?x, q, ?z) UNION (?x, r, ?u))variant pattern Example 1. example, (P ) ={{?x, ?y}, {?x, ?y, ?z}, {?x, ?y, ?u}}. case, mapping Lemma maps ?x,?y, ?z ?u c. graph G Lemma equals {(c, p, c), (c, q, c), (c, r, c)},JP KG = {1 , 2 } 1 = |{?x,?y,?z} 2 = |{?x,?y,?u} . consider = {?x, ?y}(P ). 0 = {?x, ?y, ?z} indeed 0 |S 0 = 1 JP KG . Noteexample could also chosen {?x, ?y, ?u} 0 .counterpart Lemma 5 fragment SPARQL(bound, 6=, 6=c ) givenfollowing Lemma, thus settling Theorem 3 fragment.Lemma 7. Let P pattern SPARQL(bound, 6=, 6=c ). Let W set constantsappearing constant-nonequality filter condition P . Let Z finite setconstants cardinality var(P ), disjoint W . : var(P ) Zarbitrary fixed injective mapping, let G RDF graph consisting possibletriples ((u), (v), (w)) (u, v, w) triple pattern P .every (P ) exists 0 |S 0 belongs JP KG .Proof. prove every subpattern Q P every (Q) exists 0|S 0 JQKG . proof induction height Q. reasoninglargely proof Lemma 5. difference case Qform Q1 FILTER C. showing 0 |= C, argue follows lasttwo cases:C form ?x 6= ?y, |S 0 |= C since injective.C form ?x 6= c, |S 0 |= C since Z W disjoint.3.2 Computational Complexitysection show satisfiability decidable fragments NP-complete. Noteimmediately follow NP-completeness SAT, since booleanformulas part syntax decidable fragments.Theorem 3 implies following complexity upper bound:Corollary 8. satisfiability problem SPARQL(bound, =, 6=c ) patterns, wellSPARQL(bound, 6=, 6=c ) patterns, belongs complexity class NP.410fiSatisfiability Problem SPARQLProof. Theorem 3, SPARQL(bound, =, 6=c ) SPARQL(bound, 6=, 6=c ) pattern Psatisfiable exists scheme (P ). Following definition (P ),clear polynomial-time nondeterministic algorithm that, inputP , accepting possible run computes scheme (P ), every scheme(P ) computed accepting possible run.Specifically, algorithm works bottom-up syntax tree P computesscheme every subpattern. every leaf Q, corresponding triple pattern P ,compute unique scheme (Q). every UNION operator nondeterministicallychoose continuing scheme left right child. everyoperator continue union left right child schemes. everyOPT operator, nondeterministically choose treating AND, simplycontinuing scheme left. every FILTER operation constraint Ccheck child scheme whether ` C. check succeeds, continueS; check fails, run rejected. computation reached rootsyntax tree compute scheme root, run acceptingcomputed scheme output.Remark 9. presentation syntax SPARQL, consider conjunctiondisjunction filter conditions. Extending syntax allow would ruinNP upper bound. Allowing conjunctions disjunctions, would need extenddefinition (P ) obvious manner, defining ` C1 C2 ` C1 ` C2 ,similarly definition ` C1 C2 . results would carry through.next show satisfiability actually NP-hard, even patterns usingOPT operators using bound constraints filter conditions.Proposition 10. satisfiability problem OPT-free patterns SPARQL(bound)NP-hard.Proof. define problem Nested Set Cover follows:Input: finite set finite set E sets subsets . (So, every element Eset subsets .)Decide: Whether element e E choose subset Se e, eE Se =T.show later problem NP-hard; let us first describereduced polynomial time satisfiability problem hand. Consider input (T, E)Nested Set Cover. Without loss generality may assume set variables{?x1 , ?x2 , . . . , ?xn }. Fix constant c. subset , make pattern PStaking (x, c, c) x S. set e subsets , formpattern Pe taking UNION PS e. Finally, form pattern PEtaking Pe e E.consider following pattern denote P(T,E) :PE FILTER bound(?x1 ) FILTER bound(?x2 ) . . . FILTER bound(?xn )411fiZhang, Van den Bussche, & Picalausaclaim P(T,E) satisfiable (T, E) yes-instance Nested SetCover. see only-if direction, let G graph JP(T,E) KG nonempty, i.e.,element solution mapping .particular JPE KG . Hence, everye E exists e JPe KG = eE e . Since Pe UNION PSe, e E exists Se e e JPSe KG . Since PSeSis(x, c, c) x SSe , follows dom(e ) = Se . Hence, since dom() = eE dom(e ),dom() = eE Se . However, bound constraints filtersapplied P(T,E) ,also dom() = {?x1 , . . . , ?xn } = . conclude = eE Se desired.if-direction, assume e E exists Se e =eE Se . Consider singleton graph G = {(c, c, c)}. subset , let :{c} constant solution mapping domain S. Clearly, JPS KG , Se JPe KGeverySe E. map constant,Hence,compatible.= eE Se , JPE KG . Since dom() = eE dom(Se ) = eE Se = ={?x1 , . . . , ?xn }, mapping satisfies every constraint bound(?xi ) = 1, . . . , n.conclude JP(E,T ) KG desired.remains show Nested Set Cover NP-hard. Thereto reduce classicalCNF-SAT problem. Assume given boolean formula CNF, conjunctionclauses, clauses disjunction literals (variables negated variables).construct input (T, E) Nested Set Cover follows. Denote set variables usedW .take set clauses . variable x W , consider set Posxconsisting clauses contain positive occurrence x, set Negx consistingclauses contain negative occurrence x. define ex pair{Posx , Negx }.E defined set {ex | x W }. clear satisfiableconstructed input yes-instance Nested Set Cover. Indeed, truth assignmentsvariables correspond selecting either Posx Negx ex x W .4. Undecidable FragmentsSection show two decidable fragments SPARQL(bound, =, 6=c )SPARQL(bound, 6=, 6=c ) are, sense, maximal. Specifically, three minimal fragmentssubsumed one two fragments SPARQL(bound), SPARQL(=, 6=),SPARQL(=c ). main result Section is:Theorem 11. Satisfiability undecidable SPARQL(bound) patterns, SPARQL(=,6=) patterns, SPARQL(=c ) patterns.prove theorem reducing satisfiability problem algebrafinite binary relations union, composition, difference (Tan et al., 2014).algebra also called Downward Algebra denoted DA. expressions DAdefined follows. Let R arbitrary fixed binary relation symbol.symbol R DA-expression.e1 e2 DA-expressions, e1 e2 , e1 e2 , e1 e2 .412fiSatisfiability Problem SPARQLSemantically, DA-expressions represent binary queries binary relations, i.e., mappingsbinary relations binary relations. Let J binary relation. DA-expression e,define binary relation e(J) inductively follows:R(J) = J;(e1 e2 )(J) = e1 (J) e2 (J);(e1 e2 )(J) = e1 (J) e2 (J) (set difference);(e1 e2 )(J) = {(x, z) | : (x, y) e1 (J) (y, z) e2 (J)}.DA-expression called satisfiable exists finite binary relation Je(J) nonempty.Example 12. example DA-expression e = (R R) R. J binary relation{(a, b), (b, c), (a, c), (c, d)} e(J) = {(b, d), (a, d)}. example unsatisfiable DAexpression ((R R R) R) (R R R).recall following result. actually well known (Andreka, Givant, & Nemeti,1997) relational composition together union complementation leadsundecidable algebra; following result simplifies matters showing undecidabilityalready holds expressions single relation symbol using set difference insteadcomplementation. following result proven reduction universalityproblem context-free grammars.Theorem 13 (Tan et al., 2014). satisfiability problem DA-expressions undecidable.4.1 Expressing MINUSmain problem face reducing DA SPARQL fragments stated Theorem 11, emulate difference operator. review generally emulateMINUS operator, meaningful counterpart relational differenceoperator SPARQL context.MINUS operator defined follows. two patterns P1 P2 graph G,defineJP1 MINUS P2 KG = JP1 KG r JP2 KG ,reuse r operation sets mappings, already seen definition OPTSection 2.3.fragment SPARQL(bound), expressibility MINUS already known:Lemma 14 (Arenas & Perez, 2011). MINUS expressible SPARQL(bound).precisely, two patterns P1 P2 graph G, JP1 MINUSP2 KG = JP KGP patternP1 OPT (P2 (?u, ?v, ?w)) FILTER bound(?u).Here, ?u, ?v ?w fresh variables used P1 P2 .413fiZhang, Van den Bussche, & Picalausatask find similar expressions two fragments SPARQL(=, 6=)SPARQL(=c ). actually able express MINUS projection,mild assumptions graph G.projection, counterpart SPARQL operation SELECT, definedfollows. Let P pattern let finite set variables. SELECTS Prestricts solution mappings coming P variables listed S. Formally,graph G, defineJSELECTS P KG = {|Sdom() | JP KG }.assumptions graph G need make active domain.Intuitively, active domain graph set entries triples graph.Formally, defineadom(G) = {s | p, : (s, p, o) G} {p | s, : (s, p, o) G} {o | s, p : (s, p, o) G}.easily express active domain SPARQL, following sense. Using threevariables ?u, ?v, ?w, consider patternadom = (?u, ?v, ?w) UNION (?w, ?u, ?v) UNION (?v, ?w, ?u).graph,adom(G) = {(?u) | JadomKG }= {(?v) | JadomKG }= {(?w) | JadomKG }.ready state counterpart Lemma 14 SPARQL(=, 6=).Lemma 15. MINUS expressible SPARQL(=, 6=), projection graphsleast two distinct elements. precisely, two patterns P1 P2graph G adom(G) least two distinct elements, equalityJP1 MINUS P2 KG = JSELECTvar(P1 ) P KG , P patternP1 OPT ((P2 adom adom 0 ) FILTER ?u 6= ?u0 )adom adom 0 FILTER ?u = ?u0 .Here, adom 0 copy adom pattern different variables ?u0 , ?v 0 ?w0 .variables, variables ?u, ?v ?w used adom, fresh variables used P1P2 .Proof. prove equality stated Theorem going consider inclusions.easy reference name subpatterns P follows.P20 denotes (P2 adom adom 0 ) FILTER ?u 6= ?u0 ;P3 denotes P1 OPT P20 .414fiSatisfiability Problem SPARQLThus, P (P3 adom adom 0 ) FILTER ?u = ?u0 .prove inclusion right left, let JP KG . = 3 ,3 JP3 KG mapping defined {?u, ?v, ?w, ?u0 , ?v 0 , ?w0 } (?u) = (?u0 ).particular, 3 . Since P3 = P1 OPT P20 , two possibilities 3 :3 JP1 KG 02 JP20 KG 3 02 . 3 = |var(P1 ) ,remains show exist 2 JP2 KG 3 2 . Assumecontrary. Since adom(G) least two distinct elements, 2 extendedmapping 02 JP20 KG . 2 02 3 , contradiction.3 = 1 02 1 JP1 KG 02 JP20 KG . particular, 3 defined ?u?u0 3 (?u) 6= 3 (?u0 ). hand, since 3 , (?u) = (?u0 ),also 3 (?u) = 3 (?u0 ). contradiction, possibility considerationcannot happen.prove inclusion left right, let 1 JP1 MINUS P2 KG . Assume, sakeargument, would exist 02 JP20 KG 1 02 . Mapping 02 containsmapping 2 JP2 KG , definition P20 . Since 1 02 , also 1 2 possible.So, know exist 02 JP20 KG 1 02 . Hence,1 JP3 KG . Note six variables ?u, ?u0 , ?v, ?v 0 , ?w, ?w0 belongvar(P1 ). Since G nonempty, 1 thus extended mapping JP KG .conclude 1 JSELECTvar(P1 ) P KG desired.analogous result fragment SPARQL(=c ) follows. Fix two distinctconstants b arbitrarily.Lemma 16. MINUS expressible SPARQL(=c ), projection graphsb appear. precisely, two patterns P1 P2 graph Gb belong adom(G), equality JP1 MINUS P2 KG = JSELECTvar(P1 ) P KG ,P patternPe1 OPT ((Pe2 adom ?u ) FILTER ?u = a) adom ?u FILTER ?u = b.always, expression, variables ?u, ?v ?w used adom takenfresh variables used P1 P2 .correctness proof Lemma analogous proof given Lemma 15;instead exploiting inconsistency ?u 6= ?u0 ?u = ?u0 done proof,exploit inconsistency ?u = ?u = b.4.2 Reduction Downward Algebraready formulate reduction satisfiability problem DAsatisfiability problem three fragments mentioned Theorem 11. preciselyformulate reduction prove Theorem fragment SPARQL(bound) first.discuss reduction must adapted two fragments.say RDF graph G represents binary relation J J = {(s, o) | p : (s, p, o)G}. Intuitively, view RDF graph binary relation ignoring middle column.415fiZhang, Van den Bussche, & PicalausaLemma 17. every DA-expression e exists SPARQL(bound) pattern Pefollowing properties:1. exist two distinct fixed variables ?x ?y every RDF graph Gevery JPe KG , ?x ?y belong dom();2. every binary relation J RDF graph G represents J,e(J) = {((?x), (?y)) | JPe KG };Proof. induction structure e. e R Pe triple pattern (?x, ?z, ?y).e form e1 e2 , Pe Pe1 UNION Pe2 .e form e1 e2 , Pe Pe01 Pe02 , Pe01 Pe02 obtainedfollows. First, renaming variables, may assume without loss generality Pe1Pe2 variables common ?x ?y. Let ?z fresh variable.Pe1 , rename ?y ?z, yielding Pe01 , Pe2 , rename ?x ?z, yielding Pe02 .Finally, e form e1 e2 , use expression P Lemma 14 appliedPe1 Pe2 . may assume without loss generality Pe1 Pe2variables common ?x ?y.lemma clearly e satisfiable Pe satisfiable.thus reduction satisfiability DA satisfiability SPARQL(bound),showing undecidability latter problem.discuss two remaining fragments.4.3 SPARQL(=, 6=)fragment consider minor variant satisfiability DA-expressionsrestrict attention binary relations least two elements. Formally, active domainbinary relation J set entries pairs belonging J, adom(J) := {x |: (x, y) J (y, x) J}. DA-expression e called two-satisfiable e(J)nonempty J adom(J) least two distinct elements.Clearly, two-satisfiability undecidable well, decidable, satisfiabilitywould decidable too. Indeed, e satisfiable two-satisfiable, satisfiablebinary relation J single element. isomorphism one J(the singleton {(x, x)}), case could checked separately.Lemma 17 adapted claiming second property binary relationsJ least two distinct elements. proof case e e1 e2 ,use Lemma 15.Using adapted lemma, reduce two-satisfiability DA satisfiabilitySPARQL(=, 6=). need extra test whether graph represents binaryrelation least two distinct elements. use following pattern test (usingfresh variables ?u, . . . , ?w0 usual):(((?u, ?v, ?w) (?u0 , ?v 0 , ?w0 )) FILTER ?u 6= ?u0 )UNION (((?u, ?v, ?w) (?u0 , ?v 0 , ?w0 )) FILTER ?w 6= ?w0 )UNION (((?u, ?v, ?w) (?u0 , ?v 0 , ?w0 )) FILTER ?u 6= ?w0 )Then, e two-satisfiable Pe test satisfiable.416fiSatisfiability Problem SPARQL4.4 SPARQL(=c )fragment consider variant two-satisfiability, called ab-satisfiability,two arbitrary fixed constants a, b I. DA-expression called ab-satisfiable e(J)nonempty binary relation J a, b adom(J).DA-expressions distinguish isomorphic binary relations. Hence, absatisfiability equivalent two-satisfiability, thus still undecidable.adapt Lemma 17, follows. second property claimedbinary relations J a, b adom(J). proof case e = e1 e2 ,use Lemma 16.obtain e ab-satisfiable Pe test ab satisfiable,test ab following pattern tests whether graph represents binary relationb active domain:(((?u, ?v, ?w) UNION (?w, ?v, ?u)) ((?u0 , ?v 0 , ?w0 ) UNION (?w0 , ?v 0 , ?u0 )))FILTER ?u = FILTER ?u0 = bRemark 18. Recall literals cannot appear first second position RDF triple.Patterns using constant-equality predicates unsatisfiable reason.example, using literal 42, pattern (?x, ?y, ?z) FILTER ?y = 42 unsatisfiable. However, seen use constant-equality predicates leads undecidabilitysatisfiability much fundamental reason, nothing literals,namely, ability emulate set difference.5. Satisfiability Well-Designed Patternswell-designed patterns (Perez et al., 2009) identified well-behaved classSPARQL patterns, properties similar conjunctive queries relational databases(Abiteboul et al., 1995). Standard conjunctive queries always satisfiable, conjunctivequeries extended equality nonequality constraints, possibly involving constants,unsatisfiable constraints inconsistent. analogous behavior presentcall AF-patterns: patterns use FILTER operators.formalize Proposition 19. show Theorem 21 well-designedpattern satisfiable reduction AF-pattern satisfiable.words, far satisfiability concerned, well-designed patterns treated like AFpatterns.5.1 Satisfiability AF-PatternsSection 3.1 associated set schemes (P ) every pattern P . (P )empty, P unsatisfiable (Lemma 4).P AF-pattern (P ) nonempty, satisfiability P turndepend solely equalities, nonequalities, constant-equalities, constantnonequalities occurring filter conditions P . denote set constraintsC(P ).set constraints called consistent exists mapping satisfies everyconstraint .417fiZhang, Van den Bussche, & Picalausaestablish:Proposition 19. AF-pattern P satisfiable (P ) non-emptyC(P ) consistent.Proof. only-if direction proposition given Lemma 4 togetherobservation JP KG , satisfies every constraint C(P ). Since P satisfiable,G exist, C(P ) consistent.direction, since P UNION OPT operators, (P )singleton {S}. Since C(P ) consistent, exists mapping : U satisfying everyconstraint C(P ). Let G graph consisting triples ((u), (v), (w))(u, v, w) triple pattern P . straightforward show induction heightQ every subpattern Q P , |S 0 JQKG , (Q) = {S 0 }. HenceJP KG P satisfiable.Note (P ) blow possible UNION OPT operators,missing AF-pattern. Hence, AF-pattern P , efficiently compute(P ) single bottom-up pass P . Morever, C(P ) conjunction possibly negatedequalities constant equalities. well known consistency conjunctionsdecided polynomial time (Kroening & Strichman, 2008). Hence, conclude:Corollary 20. Satisfiability AF-patterns checked polynomial time.5.2 AF-Reduction Well-Designed Patternswell-designed pattern defined union union-free well-designed patterns. Sinceunion satisfiable one terms is, focus union-free patternsfollows. Formally, union-free pattern P called well-designed (Perez et al., 2009)1. every subpattern P form Q FILTER C, variables mentioned C alsooccur Q;2. every subpattern Q P form Q1 OPT Q2 , every ?x var(Q2 ), ?xalso occurs P outside Q, ?x var(Q1 ).associate every union-free pattern P AF-pattern (P ) obtained removingapplications OPT right operands; left operand remains place. Formally,define following:P triple pattern, (P ) equals P .P form P1 P2 , (P ) = (P1 ) (P2 ).P form P1 FILTER C, (P ) = (P1 ) FILTER C.P form P1 OPT P2 , (P ) = (P1 ).announced result given following theorem, proved directlyresults Perez et al. (2009).44. thank anonymous referee offering given proof only-if direction.418fiSatisfiability Problem SPARQLTheorem 21. Let P union-free well-designed pattern. P satisfiable(P ) is.Proof. going refer Lemma 4.3 Proposition 4.5 Perez et al. (2009).Indeed, Lemma 4.3 gives us if-direction Theorem 21. cited paper introducednotion reduction P 0 E P . Whenever P 0 E P , also (P ) E P 0 (P ) = (P 0 ).only-if direction, assume P satisfiable, exists GJP KG . exists P 0 E P Jand(P 0 )KG (Proposition 4.5). Here,and(P 0 ) denotes pattern obtained P 0 replacing every OPT AND.(P ) = (P 0 ).following claim easy verify every union-free pattern P 0 :Jand(P 0 )KG |var((P 0 )) J(P 0 )KG . claim, obtain J(P 0 )KG nonempty(P 0 ) = (P ) satisfiable, desired.Since (P ) efficiently computed P , Theorem Corollary 20imply:Corollary 22. Satisfiability union-free well-designed patterns tested polynomialtime.6. Experimental Evaluationwant evaluate experimentally positive results presented far:1. Wrong literal reduction (Proposition 24);2. Satisfiability checking SPARQL(bound, =, 6=c ) SPARQL(bound, 6=, 6=c ) computing (P ) (Theorem 3);3. Satisifiability checking well-designed patterns, reduction AF-patterns (Proposition 19 Theorem 21).experiments follow reported earlier Picalausa Vansummeren(2011). test datasets real-life SPARQL queries, use logs SPARQL endpointDBpedia.5 data source contains query dumps year 2012, divided14 logfiles. chose three logs 20120913, 20120929 20121031obtain span roughly three months; took sample 100 000 queriesthem. typical query log size 75 125 (size measured numbernodes syntax tree). 10% queries log usablesyntax errors use features covered analysis.implementation tests done Java 7 Windows 7, Intel Core 2Duo SU94000 processor (1.40GHz, 800MHz, 3MB) 3GB memory (SDRAM DDR31067MHz).tests measure time needed perform analyses SPARQL queries presentedabove. timings averaged queries log, experiment repeatedfive times smooth accidental quirks operating system. Although give5. ftp://download.openlinksw.com/support/dbpedia/419fiZhang, Van den Bussche, & PicalausaTable 1: Timings experiments (averaged five repeats). Times ms. Baselinetime read parse 1000 000 queries; WL stands baseline plus timewrong-literal reduction. (P ) stands WL plus time computing (P ). AFstands WL, plus testing well-designedness, plus AF-reduction testingsatisfiability (Proposition 19). percentages show increases relativebaseline.logfile201209132012092920121031baseline39 42234 28132 286WL41 25435 86833 1865%5%3%(P )44 39538 10234 4198%7%4%AF48 32941 08736 99310%9%8%absolute timings, main emphasis percentage time needed analysequery, respect time needed simply read parse query.percentage small demonstrates efficient, linear time complexity practice.turn indeed achieved experiments, shown Table 1.following subsections discuss results detail.6.1 Wrong Literal ReductionTesting removing triple patterns wrong literals pattern P performedreduction (P ) defined Appendix. definition (P ) clearcomputed single bottom-up traversal P indeed borneexperiments. Table 1 shows average, wrong-literal reduction takes 35% time needed read parse input.Interestingly, real-life queries literals wrong position indeed found;one example following:SELECT DISTINCT *{ 49 dbpedia-owl:wikiPageRedirects?redirectLink .}6.2 Computing (P )Section 3 seen satisfiability decidable fragments testedcomputing (P ), problem NP-complete. Intuitively, problemintractable (P ) may size exponential size P . actually occursreal life; common SPARQL query pattern use many nested OPTIONAL operatorsgather additional information strictly required query may maypresent. found experiments queries 50 nested OPT operators,naively would lead (P ) size 250 . shortened example queryshown Figure 1.practice, however, blowup (P ) avoided follows. Recall Theorem 3 states P satisfiable (P ) nonempty. elements (P )sets variables. Looking definition (P ), set may removed (P )420fiSatisfiability Problem SPARQLSELECT DISTINCT *{?s <http://dbpedia.org/ontology/EducationalInstitution>,<http://dbpedia.org/ontology/University> .?s <http://dbpedia.org/ontology/country> <http://dbpedia.org/resource/Brazil> .OPTIONAL {?s <http://dbpedia.org/ontology/affiliation> ?ontology_affiliation .}OPTIONAL {?s <http://dbpedia.org/ontology/abstract> ?ontology_abstract .}OPTIONAL {?s <http://dbpedia.org/ontology/campus> ?ontology_campus .}OPTIONAL {?s <http://dbpedia.org/ontology/chairman> ?ontology_chairman .}OPTIONAL {?s <http://dbpedia.org/ontology/city> ?ontology_city .}OPTIONAL {?s <http://dbpedia.org/ontology/country> ?ontology_country .}OPTIONAL {?s <http://dbpedia.org/ontology/dean> ?ontology_dean .}OPTIONAL {?s <http://dbpedia.org/ontology/endowment> ?ontology_endowment .}OPTIONAL {?s <http://dbpedia.org/ontology/facultySize> ?ontology_facultySize .}OPTIONAL {?s <http://dbpedia.org/ontology/formerName> ?ontology_formerName .}OPTIONAL {?s <http://dbpedia.org/ontology/head> ?ontology_head .}OPTIONAL {?s <http://dbpedia.org/ontology/mascot> ?ontology_mascot .}OPTIONAL {?s <http://dbpedia.org/ontology/motto> ?ontology_motto .}OPTIONAL {?s <http://dbpedia.org/ontology/president> ?ontology_president .}OPTIONAL {?s <http://dbpedia.org/ontology/principal> ?ontology_principal .}OPTIONAL {?s <http://dbpedia.org/ontology/province> ?ontology_province .}OPTIONAL {?s <http://dbpedia.org/ontology/rector> ?ontology_rector .}OPTIONAL {?s <http://dbpedia.org/ontology/sport> ?ontology_sport .}OPTIONAL {?s <http://dbpedia.org/ontology/state> ?ontology_state .}OPTIONAL {?s <http://dbpedia.org/property/acronym> ?property_acronym .}OPTIONAL {?s <http://dbpedia.org/property/address> ?property_address .}OPTIONAL {?s <http://www.w3.org/2003/01/geo/wgs84_pos#lat> ?property_lat .}OPTIONAL {?s <http://www.w3.org/2003/01/geo/wgs84_pos#long> ?property_long .}OPTIONAL {?s <http://dbpedia.org/property/established> ?property_established .}OPTIONAL {?s <http://dbpedia.org/ontology/logo> ?ontology_logo .}OPTIONAL {?s <http://dbpedia.org/property/website> ?property_website .}OPTIONAL {?s <http://dbpedia.org/property/location> ?property_location .}FILTER ( langMatches(lang(?ontology_abstract), "es") ||langMatches(lang(?ontology_abstract), "en") )FILTER ( langMatches(lang(?ontology_motto), "es") ||langMatches(lang(?ontology_motto), "en") )}Figure 1: real-life query many nested OPTIONAL operators, retrieving muchinformation possible universities Brazil.421fiZhang, Van den Bussche, & Picalausaapplication FILTER. Hence, variables mentioned FILTER conditions influence emptiness (P ); variables ignored. example,query Figure 1, two variables appear filter, namely ?ontology abstract?ontology motto, maximal size (P ) reduced 22 .experiments, turns typically variables involved filter conditions. Hence, strategy works well practice.Another practical issue that, paper, considered filter conditions bound checks, equalities, constant-equalities, possibly negated. practice, filter conditions typically apply built-in SPARQL predicates predicatelangMatches Figure 1. experimental purpose testing practicality computing (P ), however, predicates simply treated bound checks. wayapply experiments 70% queries testfiles.practical adaptations, experiments show computing (P )efficient: Table 1 shows requires, average, 4 8% time neededread parse input, timings even include wrong-literal reductionall, experiments encountered unsatisfiable queries. observationcorrobated findings recent new statistical analysis practical SPARQL usage(Han, Feng, Zhang, Wang, Rao, & Jiang, 2016). course, users practicewrite unsatisfiable expressions good news. Satisfiability remains basic problemneed understand, many problems reduced it.6.3 Satisfiability Testing Well-Designed PatternsSection 5 seen testing satisfiability well-designed pattern donetesting satisfiability AF-reduction (Theorem 21). latter done testingnonemptiness (P ) testing consistency filter conditions (Proposition 19).Computing AF-reduction done simple bottom-up traversal pattern.Moreover, AF-pattern P , computing (P ) poses problems since either emptysingleton. far testing consistency filter conditions concerned, experimentsyield rather baffling observation: almost well-designed patterns test setsfilters all. cannot explain phenomenon, impliesable test performance consistency checks real-life SPARQL queries.Anyhow, Table 1 shows entire analysis wrong-literal reduction, testingwell-designedness, AF-reduction, computing (P ), consistency checking (incases latter necessary), incurs 10% increase relative readingparsing input.6.4 Scalabilityexperiments described run sets 100 000 queries each. alsomodest scaling experiment varied number queries 5 000 200 000.Table 2 shows performance scales linearly.422fiSatisfiability Problem SPARQLTable 2: Scalability experiment (times ms). Timings clearly scale linearly increasinginput size.input sizebaselineWL(P )AF200 00074 16877 80081 73091 470100 00039 42241 25344 39548 32950 00021 31521 87623 55226 02310 0003 5963 7624 0164 4635 0001 8511 9422 0362 254Pearson coeficient0.9999240050.9999894540.9999009480.9990445427. Extension SPARQL 1.1already mentioned Introduction, SPARQL 1.0 extended SPARQL 1.1number new operators building patterns. main new features propertypaths; grouping aggregates; BIND; VALUES; MINUS; EXISTS EXISTSsubqueries; SELECT. complete analysis SPARQL 1.1 goes beyond scopepresent paper. Nevertheless, section, briefly discuss results mayextended new setting.Property paths provide form regular path querying graphs. aspect graphquerying already extensively investigated, including questions satisfiabilitykinds static analysis query containment (Kostylev, Reutter, & Vrgoc, 2014;Kostylev, Reutter, Romero, & Vrgoc, 2015). Therefore discuss property pathshere.SPARQL 1.1 features discuss grouped two categories:cause undecidability, harmless far satisfiability concerned.begin harmless category.7.1 SELECT Operator EXISTS-SubqueriesSPARQL 1.1 allows patterns form SELECTS P , finite set variablesP pattern. novelty compared 1.0 applied subexpressions.semantics projection; already seen Section 4.1.feature influence satisfiability patterns. Indeed, patternsextended SELECT operators reduced patterns without said operators.reduction amounts simply rename variables projected fresh variablesused anywhere else pattern; SELECT operatorsremoved. resulting, SELECT-free, pattern equivalent original oneomit fresly introduced variables solution mappings final result.particular, two patterns equisatisfiable.Example 23. Rather giving formal definition SELECT-reduction formallystating proving equivalence, give example. Consider pattern P :(c, p, ?x) OPT ((?x, p, ?y) SELECT?y (?y, q, ?z) SELECT?y (?y, r, ?z))423fiZhang, Van den Bussche, & PicalausaRenaming projected-out variables fresh variables omitting SELECT operatorsyields following pattern P 0 :(c, p, ?x) OPT ((?x, p, ?y) (?y, q, ?z1 ) (?y, r, ?z2 ))Pattern P 0 equivalent P sense graph G, JP KG = { |JP 0 KG }, denotes mapping obtained omitting values ?z1?z2 (if present dom()).know handle SELECT operators, also handle EXISTSsubqueries. Indeed, pattern P FILTER EXISTS(Q) (with obvious SQL-like semantics)equivalent SELECTvar(P ) (P Q).7.2 Features Leading UndecidabilitySection 4 seen soon one express union, compositiondifference binary relations, satisfiability problem becomes undecidable. Since unioncomposition readily expressed basic SPARQL (UNION AND), key liesexpressibility difference operator. subsection see variousnew features SPARQL 1.1 indeed allow expressing difference.7.2.1 MINUS Operator EXISTS Subqueriestwo features quite obviously used express difference,dwell further.7.2.2 Grouping Aggregatesknown trick expressing difference using grouping counting (Celko, 2005)emulated extension SPARQL 1.0 grouping. illustrate techniqueexample.Consider query (?x, p, ?y) MINUS (?x, q, ?y) asking pairs (a, b)(a, p, b) holds (a, q, b) not. express query (with obvious SQL-likesemantics) follows:SELECT?x,?y (?x, p, ?y) OPT ((?x, q, ?y) (?xx, p, ?yy))GROUP ?x, ?ycount(?xx) = 0Note technique looking (?x, ?y) groups zero count ?xxsimilar technique used express difference using negated bound constraint (seenproof Lemma 17).7.2.3 BIND VALUESseen Section 4.4 allowing constant equalities filter constraints allows usemulate difference operator. Two mechanisms introduced SPARQL 1.1, BINDVALUES, allow introduction constants solution mappings. Together equalityconstraints allows us express constant equalities, hence, difference.424fiSatisfiability Problem SPARQLSpecifically, using VALUES, express P FILTER ?x = cSELECTvar(P ) (P VALUES?x (c)).Using BIND, expressedSELECTvar(P ) ((P BIND?x0 (c)) FILTER ?x = ?x0 )?x0 fresh variable. Note use SELECT, which, however, influencesatisfiability discussed above. conclude SPARQL(=) extended BIND,SPARQL(=) extended VALUES, undecidable satisfiability problem.8. Conclusionresults paper may summarized saying that, long kindsconstraints allowed filter conditions cannot combined yield inconsistent setsconstraints, satisfiability SPARQL patterns decidable; otherwise, problem undecidable. Moreover, well-designed patterns, satisfiability decidable well.positive results yield straightforward bottom-up syntactic checks implementedpractice.thus attempted paint rather complete picture satisfiability problemSPARQL 1.0. course, satisfiability basic automated reasoningtask. One may move complex tasks equivalence, implication,containment, query answering ontologies. Indeed, investigations along linelimited fragments SPARQL already happening (Letelier, Perez, Pichler, & Skritek,2013; Wudage, Euzenat, Geneves, & Layada, 2012; Kollia & Glimm, 2013; Cuenca Grau,Motik, Stoilos, & Horrocks, 2012) hope work may serve provideadditional grounding investigations.also note query optimization standard check satisfiabilitysubexpressions, avoid executing useless code. specific works SPARQL queryoptimization (Sequeda & Miranker, 2013; Groppe, Groppe, & Kolbaum, 2009) mentioninconsistent constraints cause unsatisfiability, provided soundcomplete characterizations satisfiability, like offered paper. Thus,results useful direction well.Acknowledgmentthank anonymous referees, original submission revisedsubmission, critical comments, encouraged us significantly improvepaper. work funded grant G.0489.10 Research Foundation Flanders(FWO).Appendix A.Literals wrong place triple patterns easily dealt following manner.define wrong-literal reduction pattern P , denoted (P ), set eitherempty singleton containing single pattern P 0 :425fiZhang, Van den Bussche, & PicalausaP triple pattern (u, v, w) u literal, (P ) := ; else (P ) := {P }.(P1 UNION P2 ) := (P1 ) (P2 ) (P1 ) (P2 ) empty;(P1 UNION P2 ) := {P10 UNION P20 | P10 (P1 ) P20 (P2 )} otherwise.(P1 P2 ) := {P10 P20 | P10 (P1 ) P20 (P2 )}.(P1 OPT P2 ) := (P1 ) empty;(P1 OPT P2 ) := (P1 ) (P2 ) empty (P1 ) nonempty;(P1 OPT P2 ) := {P10 OPT P20 | P10 (P1 ) P20 (P2 )} otherwise.(P1 FILTER C) := {P10 FILTER C | P10 (P1 )}.Note wrong-literal reduction never literal subject position triplepattern. next proposition shows that, far satisfiability checking concerned,may always perform wrong-literal reduction.Proposition 24. Let P pattern. (P ) empty P unsatisfiable; (P ) ={P 0 } P P 0 equivalent, i.e., JP KG = JP 0 KG every RDF graph G. Moreover,(P ) = {P 0 } P 0 contain triple pattern (u, v, w) u literal.Proof. Assume P triple pattern (u, v, w) u literal, (P ) = . Since uconstant, (u) equals literal u every solution mapping . Since tripleRDF graph literal first position, JP KG empty every RDF graph G,i.e., P unsatisfiable. u literal, (P ) = {P } claims Propositiontrivial.P form P1 UNION P2 , P1 P2 , P1 FILTER C, claimsProposition follow straightforwardly induction.P form P1 OPT P2 , three cases consider.(P1 ) empty (P ). case, induction, P1 unsatisfiable,whence P .(P1 ) = {P10 } nonempty (P2 ) empty, (P ) = {P10 }. induction,P2 unsatisfiable. Hence, P equivalent P1 , turn equivalent P10induction. P10 contain triple pattern literal first positionfollows induction.(P1 ) = {P10 } (P2 ) = {P20 } nonempty, (P ) = P10 OPT P20 .induction, P1 equivalent P10 P2 P20 . Hence, P equivalentP10 OPT P20 desired. induction, neither P10 P20 contain triple patternliteral first position, neither P10 OPT P20 .426fiSatisfiability Problem SPARQLReferencesAbiteboul, S., Hull, R., & Vianu, V. (1995). Foundations Databases. Addison-Wesley.Andreka, H., Givant, S., & Nemeti, I. (1997). Decision problems equational theoriesrelational algebras, Vol. 126 Memoirs. AMS.Angles, R., & Gutierrez, C. (2008). expressive power SPARQL. Sheth, A., Staab,S., et al. (Eds.), Proceedings 7th International Semantic Web Conference, Vol. 5318Lecture Notes Computer Science, pp. 114129. Springer.Arenas, M., Conca, S., & Perez, J. (2012). Counting beyond Yottabyte, SPARQL1.1 property paths prevent adoption standard. Mille, A., et al. (Eds.),Proceedings 21st World Wide Web Conference, pp. 629638. ACM.Arenas, M., & Perez, J. (2011). Querying semantic web data SPARQL. Proceedings30st ACM Symposium Principles Databases, pp. 305316. ACM.Arenas, M., Perez, J., & Gutierrez, C. (2009). semantics SPARQL. De Virgilio,R., Giunchiglia, F., & Tanca, L. (Eds.), Semantic Web Information ManagementAModel-Based Perspective, pp. 281307. Springer.Celko, J. (2005). SQL Smarties: Advanced SQL Programming (Third edition). Elsevier.Cuenca Grau, B., Motik, B., Stoilos, G., & Horrocks, I. (2012). Completeness guaranteesincomplete ontology reasoners: Theory practice. Journal Artificial IntelligenceResearch, 43, 419476.Groppe, J., Groppe, S., & Kolbaum, J. (2009). Optimization SPARQL using coreSPARQL. Cordeiro, J., & Filipe, J. (Eds.), Proceedings 11th International ConferenceEnterprise Information Systems, pp. 107112.Han, X., Feng, Z., Zhang, X., Wang, X., Rao, G., & Jiang, S. (2016). statisticalanalysis practical SPARQL patterns. Proceedings 19th International WorkshopWeb Databases.Harris, S., & Seaborne, A. (2013). SPARQL 1.1 query language. W3C Recommendation.Kollia, I., & Glimm, B. (2013). Optimizing SPARQL query answering OWL ontologies.Journal Artificial Intelligence Research, 48, 253303.Kostylev, E., Reutter, J., Romero, M., & Vrgoc, D. (2015). SPARQL property paths.Arenas, M., Corcho, O., Simperl, E., Strohmaier, M., et al. (Eds.), Proceedings14th International Semantic Web Conference, Vol. 9366 Lecture Notes ComputerScience, pp. 318. Springer.Kostylev, E., Reutter, J., & Vrgoc, D. (2014). Containment data graph queries.Proceedings 17th International Conference Database Theory. ACM.Kroening, D., & Strichman, O. (2008). Decision Procedures. Springer.Letelier, A., Perez, J., Pichler, R., & Skritek, S. (2013). Static analysis optimizationsemantic web queries. ACM Transactions Database Systems, 38 (4), article 25.Perez, J., Arenas, M., & Gutierrez, C. (2009). Semantics complexity SPARQL. ACMTransactions Database Systems, 34 (3), article 16.427fiZhang, Van den Bussche, & PicalausaPicalausa, F., & Vansummeren, S. (2011). real SPARQL queries like?. De Virgilio, R., Giunchiglia, F., & Tanca, L. (Eds.), Proceedings International WorkshopSemantic Web Information Management, No. 7. ACM Press.Polleres, A. (2007). SPARQL rules (and back). Williamson, C., Zurko, M., et al.(Eds.), Proceedings 16th World Wide Web Conference, pp. 787796. ACM.Prudhommeaux, E., & Seaborne, A. (2008). SPARQL query language RDF. W3CRecommendation.Schmidt, M., Meier, M., & Lausen, G. (2010). Foundations SPARQL query optimization.Proceedings 13th International Conference Database Theory, pp. 433. ACM.Sequeda, J., & Miranker, D. (2013). Ultrawrap: SPARQL execution relational data. WebSemantics, 22, 1939.Tan, T., Van den Bussche, J., & Zhang, X. (2014). Undecidability satisfiability algebra finite binary relations union, composition, difference. arXiv:1406.0349.Wudage, M., Euzenat, J., Geneves, P., & Layada, N. (2012). SPARQL query containmentSHI axioms. Proceedings 26th AAAI Conference Artificial Intelligence,pp. 1016.428fiJournal Artificial Intelligence Research 56 (2016) 547-571Submitted 01/16; published 08/16Research NoteTime-Bounded Best-First Search Reversible Non-reversibleSearch GraphsCarlos HernandezCARLOS . HERNANDEZ . U @ UNAB . CLDepartamento de Ciencias de la Ingeniera,Universidad Andres Bello,Santiago, ChileJorge A. BaierJABAIER @ ING . PUC . CLDepartamento de Ciencia de la ComputacionPontificia Universidad Catolica de ChileSantiago, ChileRoberto AsnRASIN @ UCSC . CLDepartamento de Ingeniera InformaticaUniversidad Catolica de la Santsima ConcepcionConcepcion, ChileAbstractTime-Bounded A* real-time, single-agent, deterministic search algorithm expandsstates graph order A* does, unlike A* interleaves search action execution. Known outperform state-of-the-art real-time search algorithms based Korfs LearningReal-Time A* (LRTA*) benchmarks, studied detail sometimesconsidered true real-time search algorithm since fails non-reversible problems evengoal still reachable current state. paper propose study Time-BoundedBest-First Search (TB(BFS)) straightforward generalization time-bounded approachbest-first search algorithm. Furthermore, propose Restarting Time-Bounded Weighted A* (TBR(WA*)), algorithm deals adequately non-reversible search graphs, eliminatingbacktracking moves incorporating search restarts heuristic learning. non-reversibleproblems prove TB(BFS) terminates deduce cost bounds solutions returnedTime-Bounded Weighted A* (TB(WA*)), instance TB(BFS). Furthermore, prove TBR(WA*), reasonable conditions, terminates. evaluate TB(WA) grid pathfinding15-puzzle. addition, evaluate TBR (WA*) racetrack problem. comparealgorithms LSS-LRTWA*, variant LRTA* exploit lookahead search weightedheuristic. general observation performance TB(WA*) TBR (WA*) improves weight parameter increased. addition, time-bounded algorithms almostalways outperform LSS-LRTWA* significant margin.1. Introductionmany search applications, time scarce resource. Examples range video game pathfinding, handful milliseconds given search algorithm controlling automatedcharacters (Bulitko, Bjornsson, Sturtevant, & Lawrence, 2011), highly dynamic robotics (Schmid,Tomic, Ruess, Hirschmuller, & Suppa, 2013). settings, usually assumed standardsearch algorithm able compute complete solution action required,thus execution search must interleaved.c2016AI Access Foundation. rights reserved.fiH ERN ANDEZ , BAIER , &Time-Bounded A* (Bjornsson, Bulitko, & Sturtevant, 2009) algorithm suitable searching tight time constraints. nutshell, given parameter k, runs standard A* searchtowards goal rooted initial state, k expansions completed, move performed search, still needed, resumed. move computed follows. agentpath found A* root node best node b search frontieragent moved towards b following path . Otherwise, performs backtracking move, returning agent previous state. algorithm always terminates agent goalstate, problem solution.Time-Bounded A* algorithm relevant real-time search community.significantly superior well-known real-time heuristic search algorithms applications.Indeed Hernandez, Baier, Uras, Koenig (2012) showed significantly outperforms state-of-theart real-time heuristic search algorithms RTAA* (Koenig & Likhachev, 2006) daRTAA*(Hernandez & Baier, 2012) pathfinding.relatively new algorithm, Time-Bounded A* studied deeply literature. One reasons perhaps inability adequately deal non-reversibleproblems. Indeed, non-reversible problems real-time search algorithm fail soonalgorithm led agent dead-end state; i.e., one goal unreachable. TimeBounded A*, however, additional failure condition: always fail soon backtrackmove required unreversible action. Thus class problems cannot solve limited compared real-time search algorithms, like, example, well-known LRTA* (Korf,1990). reason, Time-Bounded A* sometimes excluded experimental comparisonsreal-time search algorithms (see e.g. Burns, Ruml, & Do, 2013, p. 725).paper extend time-bounded search approach two directions. already notedauthors (Bjornsson et al., 2009), time-bounded approach limited A*. firstcontribution paper study implications using search algorithmsinstead A*. Specifically, generalize Time-Bounded A* Time-Bounded Best-First Search.general, instance Best-First Search, call TB(A) algorithm resultsapplying time-bounded approach A. second contribution paper extensiontime-bounded search approach allows algorithm deal adequately non-reversibleproblems. algorithm propose here, Restarting Time-Bounded Weighted A*which callTBR (WA*), seen lying middle ground time-bounded algorithmslearning-based real-time search algorithms like Korfs Learning Real-Time A* (LRTA*) (1990).fact, TBR (WA*) restarts search current state backtracking move availableupdates heuristic function.carry theoretical analysis Time-Bounded Weighted A* (TB(WA*)), instanceTB(BFS), TBR (WA*). TB(WA*) establish upper lower boundssolution cost. cost bound establishes that, domains, solution cost may reducedsignificantly increasing w without increasing search time; hence, contrast wellknown Weighted A* solving offline search problems, might obtain better solutionsincreasing weight. result important since suggests TB(WA*) (with w > 1)preferred TB(A*) domains WA* runs faster A*. WA*always run faster A* (see e.g., Wilt & Ruml, 2012), known many situations.Experimentally, evaluate TB(WA*) pathfinding benchmarks 15-puzzle,TBR (WA*) racetrack problem. three benchmarks observe performance improvement w increased. addition, observe TB(WA*) significantly superior TB(A*)548fiT IME -B OUNDED B EST-F IRST EARCH R EVERSIBLE N - REVERSIBLE EARCH G RAPHSLSS-LRTWA* (Rivera, Baier, & Hernandez, 2015), real-time search algorithm useweighted heuristics.paper extends work appears conference proceedings (Hernandez, Asn, & Baier,2014), including empirical analysis new benchmarks (Counter Strike Maps, racetrack,15-puzzle), extending pathfinding experiments 16-neighbor connectivity, providing lower bound cost solution returned TB(WA*) (Theorem 2, below),introducing, analyzing, evaluating TBR (WA*).rest paper organized follows. start describing background neededrest paper. describe TB(BFS) TBR (BFS), including formal analysisproperties. describe experimental results, finish summaryperspectives future research.2. Backgrounddescribe background rest paper.2.1 Search Reversible Non-reversible Environmentssearch graph tuple G = (S, A), finite set states, set edgesrepresent actions available agent state. path graph (S, A)sequence states = s0 s1 sn , (si , si+1 ) A, {0, . . . , n 1}, s0 = s,sn = t. say successor (s, t) edge A. Moreover, everydefine Succ(s) = {t | (s, t) A}.cost function c search graph (S, A) c : PR+ ; i.e., associates actionpositive cost. cost path = s0 s1 sn c() = n1i=0 c(si , si+1 ), i.e. sumcosts edge considered path. cost-optimal path onelowest cost among paths t; denote cost c (s, t). addition, denotecT (s, t) cost cost-optimal path visits states , is,cost-optimal path = s1 s2 . . . sn = s1 , sn = t, si , {2, . . . , n 1}.search problem tuple (S, A, c, sstart , sgoal ) G = (S, A) search graph, sstartsgoal states S, c cost function G. search graph G = (S, A) reversiblesymmetric; is, whenever (s, t) (t, s) A. search problem reversiblesearch graph reversible. Consequently, problem non-reversible search graphcontains action (s, t) contain action (t, s).solution search problem path sstart sgoal .2.2 Best-First SearchBest-First Search (BFS) (Pearl, 1984) encompasses family search algorithms static environments associate evaluation function f (s) every state s. priorityf (s) < f (t) viewed promising node t. BFS starts initializingpriority states search space infinity, except sstart , priority setf (sstart ). priority queue Open initialized containing sstart . iteration, algorithmextracts Open state lowest priority, s. successor computes evaluation fs (t), considering path found s. fs (t) lower f (t),549fiH ERN ANDEZ , BAIER , &added Open f (t) set fs (t). algorithm repeats process sgoal Openlowest priority.pseudo code presented Algorithm 1. f -value state usually implementedattribute s, Open list implemented priority list. Furthermore, assumecost fs (t) computed Line 13 function path via s. Thus fs (t) take finitenumber values execution BFS, depends (finite) number simplepaths connect initial state s.Algorithm 1: Best-First Search1617sroot scurrentOpenforeachf (s)f (sroot ) evaluation srootInsert sroot OpenOpen 6=Let state minimum f -value Open= sgoalreturnRemove Openforeach Succ(s)fs (t) evaluation function considering discoveredfs (t) < f (t)f (t) fs (t)parent(t)Insert Open18return solution123456789101112131415instance Best-First Search Weighted A* (WA*) (Pohl, 1970). WA* computes evaluation function terms two functions, g h. g-value corresponds costlowest-cost path found far towards s, implemented attribute s. WA*s evaluation function defined f (s) = g(s) + wh(s), g(s) cost lowest-cost pathfound sstart s. addition, h non-negative, user-given heuristic function h(s)estimates cost path sgoal . Finally, w real number greater equal 1.pseudo-code WA* obtained Algorithm 1 storing g-value attribute state, h value computed external function. resulting pseudo-codeappears Algorithm 2.heuristic function h admissible h(s) c (s, sgoal ), S. Functionh consistent h(sgoal ) = 0, h(s) c(s, t) + h(t) every edge (s, t) search graph.Consistency implies path h(s) c() + h(t), which, turn, impliesadmissibility.BFSs closed listdenoted henceforth Closed defined set statesOpen g(s) infinity.1 words, contains statespath known considered re-expansion.1. BFS initially sets f (s) infinity every start node. WA* translates setting g(s) infinityexcept sstart .550fiT IME -B OUNDED B EST-F IRST EARCH R EVERSIBLE N - REVERSIBLE EARCH G RAPHSAlgorithm 2: Weighted A*1920sroot scurrentOpenforeachg(s)f (s)g(sroot ) 0f (sroot ) wh(sroot )Insert sroot OpenOpen 6=Let state minimum f -value Open= sgoalreturnRemove Openforeach Succ(s)gs,t = min{g(t), g(s) + c(s, t)}gs,t < g(t)g(t) gs,tf (t) g(t) + wh(t)parent(t)Insert Open21return solution123456789101112131415161718h admissible, WA* known find solution whose cost cannot exceed wc (sstart , sgoal ).such, WA* may return increasingly worse solutions w increased. advantage increasing w search time usually decreased fewer states expanded. w = 1,WA* equivalent A* (Hart, Nilsson, & Raphael, 1968). Another interesting result generalizeswell-known property consistent heuristics A* algorithm. formally stated follows:Lemma 1 (Ebendt & Drechsler, 2009) every moment execution Weighted A*state sroot , h consistent, upon expansion state (Line 14 Algorithm 2), holdsg(s) wc (sroot , s).Another instance Best-First Search Greedy Best-First Search (GBFS). f equaluser-given heuristic function h. WA* used sufficiently large value w,WA* GBFS rank nodes similar way. Indeed, let fGBFS fWA* denote, respectively, ffunction GBFS WA*. w exceeds g-value every node ever generatedtwo nodes s1 s2 generated g-value algorithmsfGBFS (s1 ) = h(s1 ) > h(s2 ) = fGBFS (s2 ), hold fWA* (s1 ) > fWA* (s2 ). However,even w sufficiently large, reverse always true since fWA* (s1 ) > fWA* (s2 ) holdtrue h(s1 ) = h(s2 ), g-value fWA* acts practice tie breaker.2.3 Real-Time Heuristic Searchreal-time search objective solve search problem subject additional real-timeconstraint. constraint, constant amount time (independent problem size) givensearch algorithm, end expected perform one actionssequence. constant small relation time would required offline551fiH ERN ANDEZ , BAIER , &search algorithm solve search problem. performing actions agent reachedgoal, process repeats. iteration algorithm understood two consecutiveepisodes: (1) search episode, path computed, (2) execution episode,actions path performed.Rather receiving time limit seconds, real-time search algorithms receive parameter, say k, guarantee computational time taken search episode boundednon-decreasing function k. example real-time search algorithm Local Search-Space,Learning Real-Time A* (LSS-LRTA*; Algorithm 3) (Koenig & Sun, 2009). receives searchproblem P parameter k. search episode, runs bounded execution A* rootedcurrent state expands k states. Following, updates heuristic valuesstates closed list A* run. update, usually referred learning step, makes hinformed, guarantees following holds every A*s closed list:h(s) = min {cClosed (s, t) + h(t)}.tOpen(1)execution episode performs actions appear path found A* currentstate towards state lowest f -value open list. reversible search spaces hAlgorithm 3: LSS-LRTA*12345678Input: search problem P natural number ksstartgoal staterun A* k states expanded goal node best state Openbest state A*s closed list lowest f -valueClosedupdate h-value Equation 1 holdsmove along path found A* bestbest,initially consistent shown LSS-LRTA* terminates search problemsolution (Koenig & Sun, 2009). search space non-reversible, however, termination cannotguaranteed. see later, time-bounded algorithms (without restarts) prove solutionexist well. property hold algorithms whose search expands nodeswhose distance current state bounded, like LSS-LRTA*.2.4 Comparing Two Real-Time Search AlgorithmsOne way frequently used literature compare two real-time search algorithms Bcomparing cost returned paths algorithms configured waysearch episodes approximately duration. Assume real-time search algorithmrequires n search episodes solve search problem runtime . sayaverage time per search episode run /n.evaluate relative performance two algorithms B use set benchmarkproblems P set algorithm parameters. parameter algorithm A, obtainrecord average solution cost problems P average time per episode.likewise B plot average solution cost versus average time per episodealgorithm. curve algorithm always top curve algorithm B clearly552fiT IME -B OUNDED B EST-F IRST EARCH R EVERSIBLE N - REVERSIBLE EARCH G RAPHSstate B superior A, B returns better-quality solutions comparable search timeper episode.Another approach used compare real-time search algorithms Game TimeModel (Hernandez et al., 2012). model, time partitioned uniform time intervals.agent execute one movement time interval, search movements doneparallel. objective move agent start location goal locationtime intervals possible. game time model motivated video games. Video games oftenpartition time game cycles, couple milliseconds long (Bulitko et al.,2011). using Game Time Model, implementation real-time search algorithmmodified stop search soon units timewhere parameterhave passed.3. Time-Bounded Best-First SearchTime-Bounded A* (TB(A*), Bjornsson et al., 2009) real-time search algorithm based A*.Intuitively, TB(A*) understood algorithm runs A* search sstart sgoalalternates search phase execution phase goal reached. search phasebounded number states expanded using A*. execution phase two cases.agent path sstart best state Open, forward movementpath performed. Otherwise, algorithm performs backtracking moves agentmoved state came from. search phase execute path connectingsstart sgoal already found. algorithm terminates agent reachedgoal.generalization TB(A*) Time-Bounded Best-First Search, simply replaces A*TB(A*) Best-First Search. pseudo code shown Algorithm 4. parameterssearch problem (S, A, c, sstart , sgoal ), integer k refer lookaheadparameter.TB(BFS) uses variable scurrent store current state agent. MoveToGoal procedure (called Main) implements loop alternates search execution. initialization(Lines 2728) scurrent initialized sstart , and, among things, BFSs Open list setcontain sstart only. goal state reached (represented fact variablegoalF ound false), bounded version BFS called (Line 31) expands k states,computes path sstart state Open minimizes evaluation function f .path built quickly following parent pointers, stored variable path. execution phase (Lines 3236), current position agent, scurrent , path, agentperforms action determined state immediately following scurrent path. Otherwise,backtracking move implemented moving agent parent search tree BFS,parent(scurrent ). use backtracking moves mechanism guarantees agenteventually reach state variable path because, worst case, agent eventually reachsstart . soon state reached agent start moving towards state believedclosest goal.Algorithm 4 equivalent TB(A*) BFS replaced A*. Finally, call TimeBounded Greedy Best-First Search (TB(GBFS)) algorithm results use GreedyBest-First Search instead BFS.Note length path cannot general bounded constant sizeproblem. bound computation search episode use technique described553fiH ERN ANDEZ , BAIER , &Algorithm 4: Time-Bounded Best-First Search123456789101112131415161718192021222324252627282930313233343536373839404142procedure InitializeSearch()sroot scurrentOpenforeachf (s)f (sroot ) evaluation srootInsert sroot OpengoalFound falsefunction Bounded-Best-First-Search()expansions 0Open 6= expansions < k f (sgoal ) > mintOpen f (t)Let state minimum f -value OpenRemove Openforeach Succ(s)Compute fs (t) considering discovered s.fs (t) < f (t)f (t) fs (t)parent(t)Insert Openexpansions expansions + 1Open = return falseLet sbest state minimum priority Open.sbest = sgoal goalFound truepath path sroot sbestreturn truefunction MoveToGoal()scurrent sstartInitializeSearch()scurrent 6= sgoalgoalFound = falseBounded-Best-First-Search() = false return falsescurrent pathscurrent state scurrent pathelsescurrent parent(scurrent );Execute movement scurrentreturn trueprocedure MainMoveToGoal() = trueprint(the agent goal state)elseprint(no solution)554fiT IME -B OUNDED B EST-F IRST EARCH R EVERSIBLE N - REVERSIBLE EARCH G RAPHSBjornsson et al. (2009), whereby additional counter (analogous k used measureeffort path extraction). omitted pseudocode clarity.3.1 Propertiesanalyze interesting properties algorithms proposed. First,like TB(A*), TB(BFS) always terminates finds solution one exists. importantproperty since many real-time heuristic search algorithms (e.g., LSS-LRTA*) enter infinite loopunsolvable problems. Second, prove upper lower bound cost solutionsreturned TB(WA*). bound interesting since suggests increasing w one mightobtain better solutions rather worse.Theorem 1 TB(BFS) move agent goal state given reversible search problem Psolution P exists. Otherwise, eventually print solution.Proof: Follows fact Best-First Search eventually finds path towards goal.fact search space finite state inserted Openfinite number times. addition, moves carried algorithm (including movingparent(s)) executable reversible search space.important note reason TB(BFS) eventually print solutionunsolvable problem dependent fact Open list used. LSS-LRTA* cannot alwaysdetect unsolvable problems search expand locality around current state.characteristic agent-centered search algorithms (Koenig, 2001), class algorithmsTB(BFS) member of.following two lemmas intermediate results allow us prove upper boundcost solutions obtained TB(WA*). results apply TB(A*) knowledgeLemma 2 Theorem 2 proven TB(A*).results below, assume P = (S, A, c, sstart , sgoal ) reversible search problem,TB(WA*) run parameter w 1 h admissible heuristic. Furthermore,assume c+ = max(u,v)A c(u, v), c = min(u,v)A c(u, v), N (w) numberexpansions needed WA* solve P . Finally, assume k N (w) reasonableassumption given real-time setting.Lemma 2 cost moves incurred agent controlled TB(WA*) goalFoundbecomes true bounded b N (w)1cc bounded b N (w)1cc+ .kkProof: N (w) 1 states expanded goalFound becomes true. k states expandedper call search procedure, clearly b N (w)1c number calls Best-FirstkSearch terminates without setting goalFound true. move costs least c c+ ,result follows.focus cost incurred complete path found. following Lemmarelated property enjoyed TB(A*) stated Theorem 2 Hernandez et al. (2012).Lemma 3 cost moves incurred agent controlled TB(WA*) goalFoundbecome true cannot exceed 2wc (sstart , sgoal ).555fiH ERN ANDEZ , BAIER , &Proof: Assume goalFound become true. Let path starts sstart , endsscurrent defined following parent pointers back sstart . Path prefixpath lowest f -value state previous run WA* therefore, Lemma 1,c() < wc (sstart , sgoal ). worst case terms number movements necessary reachgoal path coincide sstart . case, agent backtrackway back sstart . sstart reached, agent move goal path costwc (sstart , sgoal ). Thus agent may incur cost higher 2wc (sstart , sgoal ) reachgoal.obtain lower bound upper bound solution cost TB(WA*)follows straightforwardly two previous lemmas.Theorem 2 Let C solution cost obtained TB(WA*). Then,bN (w) 1N (w) 1 +cc C bcc + 2wc (sstart , sgoal ).kkProof: put together inequalities implied Lemmas 2 3.first observation result shown empirically domains,w increased, N (w) may decrease substantially. Gaschnig (1977), example, reports8-puzzle N (1) exponential depth solution whereas N (w), large wsubexponential d. domains like grid pathfinding, well known using high valuesw results substantial reductions expanded nodes (see e.g., Likhachev, Gordon, & Thrun,2003). Thus, increasing w, lower bound first term upper bound maydecrease substantially. second term upper bound, 2wc (sstart , sgoal ), increasingw, may increase linearly w. suggests situations better- ratherworse-quality solutions may found w increased. see later, confirmedexperimental evaluation.second observation bounds factor b(N (w) 1)/kc decreases k increases. suggests k large (i.e., close N (w)), increasing w may actually leaddecreased performance.Putting observations together, Theorem 2 suggests TB(WA*) produce better solutions TBA* k relatively small problems WA* expands fewer nodesA* offline mode. Problems WA* expand fewer nodes A* exist (Wilt &Ruml, 2012).Finally, hard see Theorem 2 generalized algorithms provideoptimality guarantees. Given two search algorithms B provide bounds whoserelative performance known, theorem used predictor relative performanceTB(A) versus TB(B).3.2 Non-reversible Search Problems via Restartingnon-reversible problems, well-known real-time heuristic search algorithms LSS-LRTA*fail when, execution episode, state path goal visited.Time-bounded algorithms like TB(BFS) fail condition alsofail soon physical backtrack required non-reversible action. second condition556fiT IME -B OUNDED B EST-F IRST EARCH R EVERSIBLE N - REVERSIBLE EARCH G RAPHSfailure reason sometimes time-bounded algorithms discarded use nonreversible domains. objective section propose time-bounded algorithm that,used non-reversible problems, fail due latter condition, due former.modification TB(WA*) non-reversible problems comes incorporatingtwo key characteristics real-time search algorithms like LSS-LRTA*: search restarts heuristicupdates. Indeed, whenever physical backtracking available, or, generally,predefined restart condition holds, algorithm restarts search. addition, avoid gettingtrapped infinite loops, algorithm updates heuristic using update rule LSSLRTA*. call resulting algorithm Restarting Time-Bounded Weighted A* (TBR (WA*)).Algorithm 5 shows details TBR (WA*). Lines 1012 relevant differenceprevious algorithm. algorithm restarts search agent path certainrestart condition, must become true action leading current state(scurrent ) parent (parent(scurrent )).Algorithm 5: Restarting Time-Bounded Weighted A*123456789101112131415161718192021function MoveToGoal()scurrent sstartInitializeSearch()scurrent 6= sgoalgoalFound = falseBounded-WA*() = false return false;scurrent pathscurrent state scurrent pathExecute movement scurrentelse restart condition holdsUpdate heuristic function h using LSS-LRTA* update rule (Equation 1)InitializeSearch()elsescurrent parent(scurrent );Execute movement scurrentreturn trueprocedure MainMoveToGoal() = trueprint(the agent goal state)elseprint(no solution)Note prior restarting algorithm updates heuristic LSS-LRTA* would.implemented version Dijkstras algorithm. Note number states mayneed updated may bounded constant. needed, compute updateincremental manner, across several episodes. refer reader analysis KoenigSun (2009), Hernandez Baier (2012) details implementation proofscorrectness.3.2.1 ERMINATION TBR (WA*)TBR (WA*) used reversible non-reversible domains. heuristic function hinitially consistent search graph strongly connected, algorithm terminates.557fiH ERN ANDEZ , BAIER , &Theorem 3 Let P search problem strongly connected search graph. TBR (WA*),run consistent heuristic h, finds solution P .proof Theorem 3 depends intermediate results, proofsappear elsewhere. following result establishes h consistent, remains consistentupdated.Lemma 4 (Koenig & Sun, 2009) h consistent remains consistent h updatedEquation 1.Another intermediate results says h cannot decrease update following Equation 1.Lemma 5 (Koenig & Sun, 2009) h initially consistent h(s), every s, cannot decreaseh updated following Equation 1.Another intermediate result says h(s) finitely converges, intuitively means evenwanted apply infinite number updates h, point on, h changeanymore.Definition 1 (Finite Convergence) series functions {fi }i0 finitely converges function fexists n every n, holds fm = f . addition, say seriesfunctions {fi }i0 finitely converges exists function f finitely converges.Lemma 6 Let h0 consistent heuristic function P strongly connected graph. Let= {hi }i0 hk+1 function results (1) assigning hk hk+1 (2)updating hk+1 using Equation 1, set Closed Open generated bounded WeightedA* run rooted arbitrary state. finitely converges.Proof: first observation hk (s) bounded positive number everyevery k. Indeed, Lemma 4, hk consistent, thus admissible, every k.addition, problem solution, hk (s) c (s, sgoal ), every every k.second observation set h-values state take finite, eveninfinite. Formally prove H(s) = {hk (s) | k 0} finite set. Indeed, hard verifyinduction (we leave exercise reader) using Equation 1, every k 0,holds hk (s) = c(ks ) + h0 (s0 ) some, possibly empty path ks originating finishings0 . recall hk (s) bounded observe finitely many pathsgraph whose cost bounded. conclude H(s) finite set, every s.proof follows contradiction, assuming finitely converge.non-decreasing (Lemma 5), possibility increases infinitely often. impliesleast one state H(s) infinite: contradiction. conclude finitelyconverges.Note previous lemma saying anything function converges to;need know function rest proof. last intermediate resultrelated result Ebendt Drechsler (2009) stated Section 2.2 (Lemma 1).Lemma 7 every moment execution Weighted A* state sroot , h consistent,every state open list, holds g(s) wcClosed (sroot , s).558fiT IME -B OUNDED B EST-F IRST EARCH R EVERSIBLE N - REVERSIBLE EARCH G RAPHSProof: Let cost-optimal path sroot visits states Closed. Let s0state precedes . s0 part optimal path have:cClosed (sroot , s0 ) + c(s0 , s) = cClosed (sroot , s).(2)successor s0 , holds that:g(s) g(s0 ) + c(s0 , s).(3)g(s0 ) wc (sroot , s0 ),(4)g(s) wc (sroot , s0 ) + c(s0 , s).(5)g(s) wcClosed (sroot , s0 ) + wc(s0 , s) = w(cClosed (sroot , s0 ) + c(s0 , s)).(6)Lemma 1, that:Inequalities 3 4 imply:w > 0 cClosed c :Substituting Equation 2 that:g(s) wcClosed (sroot , s),(7)finishes proof.provide proof main result section.Proof (of Theorem 3) : Let us assume algorithm terminate thus enters infiniteloop. Note means algorithm restarts infinite number times (otherwise, Weighted A*would eventually find goal state, allowing agent reach goal). Assume momentinfinite execution h converged (we know Lemma 6), let s1 s2 . . .infinite sequence states si state search restarted. proveevery i, h(si ) > h(si+1 ).Let denote contents open list exactly algorithm expanded si+1 , Closeddenote contents closed list immediately heuristic updated. Equation 1,following holds:h(si ) = cClosed (si , ) + h(sO ),(8)rewrite Equation 8 as:wh(si ) = wcClosed (si , ) + wh(sO ),(9)Let g(sO ) denote g-value exactly si+1 preferred expansion . Now,prove wcClosed (si , ) g(sO ). Indeed, Closed follows Lemma 1fact cClosed c w 1. hand, Open, obtainwcClosed (si , ) g(sO ) Lemma 7. use fact write:wh(si ) g(sO ) + wh(sO ).559(10)fiH ERN ANDEZ , BAIER , &algorithm preferred expand si+1 instead , g(sO ) + wh(sO ) g(si+1 ) +wh(si+1 ), hence:wh(si ) g(si+1 ) + wh(si+1 ).(11)Finally, w > 0 g(si+1 ) > 0 obtain h(si ) > h(si+1 ).implies sequence states s1 s2 . . . strictly decreasing h-values.state space finite, must case si = sj , j 6= j, wouldlead conclude h(si ) > h(si ), contradiction.4. Experimental Resultssection presents experimental results. objective experimental evaluationunderstand effect weight configuration performance TB(WA*) TBR(WA*). end, evaluate TB(WA*) reversible search problems (grid pathfinding15-puzzle), TBR (WA*) non-reversible problem (the racetrack). reference, compareLSS-LRTWA* (Rivera et al., 2015), version LSS-LRTA* uses Weighted A* ratherA* search phase. used algorithm since among real-time searchalgorithms able exploit weights search. LSS-LRTWA* configured performsingle action execution phase.decided include results WLSS-LRTA* (Rivera et al., 2015), another real-timesearch algorithm exploits weights, two reasons. First, new results focused relatively large lookahead values (over 128). lookahead values, Rivera et al. (2015),grid-like terrain, observe improvements significant. Second, observed that,15-puzzle, WLSS-LRTA* yields worse performance w increased.Section 4.1 report results 8- 16-neighbor grids similar manner reportedearlier publication (Hernandez et al., 2014). Section 4.2 reports results 8- 16-neighborgrids using Game Time Model (cf. Section 2.4). Section 4.3 reports results non-reversiblemaps deterministic version setting used evaluate algorithms Stochastic ShortestPath problem (Bonet & Geffner, 2003). Finally, Subsection 4.4 reports results 15-puzzle.path-finding tasks Section 4.1 Section 4.2 evaluated using 8-neighbor (Bulitkoet al., 2011; Koenig & Likhachev, 2005) and16-neighborgrids (Aine & Likhachev, 2013) (seeFigure 5). costs movements 1, 2, 5 for, respectively, orthogonal, diagonal,chess-knight movements. implementation agent cannot jump obstacles. addition, diagonal movement (d, d) (for {1, 1}) illegal (x, y) either (x+d, y) (x, +d)obstacle. 8-neighbor 16-neighbor grids use octile distance Euclideandistance heuristic values, respectively. experiments run Intel(R) Core(TM) i72600 @ 3.4Ghz machine, 8Gbytes RAM running Linux. algorithms commoncode base use standard binary heap Open. Ties Open broken favor largerg-values; rule breaking ties.4.1 Results 8-Neighbor 16-Neighbor Grid Mapsevaluated algorithms considering solution cost runtime, measures solution qualityefficiency, respectively, several lookahead weight values.used 512 512 maps video game Baldurs Gate (BG), Room maps(ROOMS), maps different size Starcraft (SC) available N. Sturtevants560fiT IME -B OUNDED B EST-F IRST EARCH R EVERSIBLE N - REVERSIBLE EARCH G RAPHS8-neighbor BG Maps600040002000AlgorithmCost (log scale)Cost (log scale)))(3.0)TB(WA*)(2.6)(WA*8-neighbor Counter Strike MapsLookahead 1Lookahead 32Lookahead 128Lookahead 256Lookahead 512Lookahead 10242000010000TBAlgorithm8-neighbor Starcraft Maps100000)(2.2)TB(WA*)(1.8)(WA*TBTB(WA*)(1.0))(3.0)TB(WA*)(2.6))(2.2(WA*TB)(1.8(WA*TBTB(WA*)(1.4(WA*)(1.0TB(WA*TB)300)500300)500)1000(WA*100010000)(1.42000Lookahead 1Lookahead 4Lookahead 16Lookahead 64Lookahead 128Lookahead 256TB60004000Cost (log scale)Lookahead 1Lookahead 4Lookahead 16Lookahead 64Lookahead 128Lookahead 25610000Cost (log scale)8-neighbor Room Maps3000020001000500Lookahead 1Lookahead 32Lookahead 128Lookahead 256Lookahead 512Lookahead 10241e+061000002000010000Algorithm0)A*)(3.6)TB(WA*)(2.2)TB(WA*)(2.8)(WTB(WA*)(1.4)TB(WA*)(1.0)TBTB(WA*)(1.0)A*)(3.6)TB(WA*)(2.2)TB(WA*)(2.8)TB(WA*)(1.4)(1.TB(WA*)(WTBTB(WA*)(1.0)2000AlgorithmFigure 1: 8-neighbor results, solution cost tends decrease w lookahead parameterincreased.path-finding repository (Sturtevant, 2012). addition, used 7 large maps Counter Strike(CS), whose sizes range 4259 4097 4096 5462.evaluated six lookahead values (1, 4, 16, 64, 128, 256) 512 512 maps sixlookahead values (1, 32, 128, 256, 512, 1024) SC CS maps. used six weight values(1.0, 1.4, 1.8, 2.2, 2.6, 3.0). map generated 50 random solvable search problems, resulting 1800 problems BG, 2000 problems ROOMS, 3250 problems SC, 350problems CS.Figures 1 2 show performance measures 8-neighbor grid maps. Noteaverage search time per episode across algorithms using lookaheadparameter. search time per episode proportional lookahead parameterdepends variable (in particular, depend weight). Thus fair conclusionsdrawn comparing two configurations lookahead parameter setvalue.561fiH ERN ANDEZ , BAIER , &8-neighbor BG Maps))(3.0)TB(WA*)(2.6)(WA*8-neighbor Counter Strike MapsLookahead 1Lookahead 32Lookahead 128Lookahead 256Lookahead 512Lookahead 1024Runtime (ms)Runtime (ms))(2.2))(1.8Algorithm8-neighbor Starcraft Maps50(WA*TBTB(WA*)(1.0(WA*A*)(AlgorithmTB)0)3.6)TB(WA*)(2.2)TB(WA*)(2.8)WTB(TB(WA*)(1.4)1.A*)(WTB(TB(WA*)(1.0)0)11086420(WA*2)(1.4320TBRuntime (ms)4Lookahead 1Lookahead 4Lookahead 16Lookahead 64Lookahead 128Lookahead 25625TBLookahead 1Lookahead 4Lookahead 16Lookahead 64Lookahead 128Lookahead 2565Runtime (ms)8-neighbor Room Maps531Lookahead 1Lookahead 32Lookahead 128Lookahead 256Lookahead 512Lookahead 102420000100001000200Algorithm))(3.0A*)TB(WTB(WA*)(2.6)(2.2A*)TB(W)(1.8A*)TB(W)(1.4(1.0A*)TB(W(WTB(WA*)0)A*)(3.6)TBTB(WA*)(2.2)(WA*)(2.8)TBTB(WA*)(1.4)(1.A*)(WTBTB(WA*)(1.0))60AlgorithmFigure 2: 8-neighbor results, search time typically decreases w lookahead parameterincreased.observe following relations hold maps regarding solution cost search time.Solution Cost lookahead values, solution cost decreases w increased. significant improvements observed lower lookahead values. surprising lightcost bound (Theorem 2) . large lookahead parameters ( 256), value waffect solution cost significantly. lookahead parameter increases, fewer searchepisodes needed less physical backtracks (back moves) needed (Hernandez et al.,2014). Back moves strongly influence performance algorithms. TB(WA*),w increased number back moves decreases, explains improvement solution quality. example, BG maps, using lookahead 1, average reductionback moves 1,960.5, comparing w = 1 w = 3, whereas lookahead 512reduction 2.4, comparing w = 1 w = 3.562fiT IME -B OUNDED B EST-F IRST EARCH R EVERSIBLE N - REVERSIBLE EARCH G RAPHS16-neighbor BG MapsLookahead 1Lookahead 4Lookahead 16Lookahead 64Lookahead 128Lookahead 2566000400020001000100006000400020001000500AlgorithmCost (log scale)Cost (log scale)))(3.0)TB(WA*)(2.6)(WA*16-neighbor Counter Strike MapsLookahead 1Lookahead 32Lookahead 128Lookahead 256Lookahead 512Lookahead 10242000010000TBAlgorithm16-neighbor Starcraft Maps100000)(2.2)TB(WA*)(1.8(WA*)(1.4TB)(WA*TBTB(WA*)(1.0))(3.0)TB(WA*)(2.6)TB(WA*)(2.2))(1.8(WA*TB)(WA*TB))(1.4(WA*)(1.0TB(WA*)500300300TBLookahead 1Lookahead 4Lookahead 16Lookahead 64Lookahead 128Lookahead 25630000Cost (log scale)10000Cost (log scale)16-neighbor Room Maps20001000500Lookahead 1Lookahead 32Lookahead 128Lookahead 256Lookahead 512Lookahead 10241e+061000002000010000Algorithm0)A*)(3.6)TB(WA*)(2.2)TB(WA*)(2.8)(WTB(WA*)(1.4)TB(WA*)(1.0)TBTB(WA*)(1.0)A*)(3.6)TB(WA*)(2.2)TB(WA*)(2.8)TB(WA*)(1.4)(1.TB(WA*)(WTBTB(WA*)(1.0)2000AlgorithmFigure 3: 16-neighbor results, solution cost tends decrease w lookahead parameterincreased.Search Time w increased, search time decreases significantly lower lookahead valuesdecreases moderately higher lookahead values. ROOMS observe largestimprovements w increased. behavior ROOMS explained WA*performs well type map w > 1.Figures 3 4 show performance measures 16-neighbor grid maps. observerelations observed 8-neighbor grid maps regarding solution cost search time.4.1.1 8-N EIGHBOR VERSUS 16-N EIGHBOR G RID APSLower cost solutions obtained 8-neighbor grids 16-neighbor grids lookahead values 1, 4, 16 BG. Note exist 16-neighbor movementsexpensive 8-neighbor moves, small lookaheads, 16-neighbor solutions maysimilar number moves, worse quality 8-neighbor solutions. hand,563fiH ERN ANDEZ , BAIER , &16-neighbor BG Maps))(3.0)TB(WA*)(2.6)16-neighbor Counter Strike MapsLookahead 1Lookahead 32Lookahead 128Lookahead 256Lookahead 512Lookahead 1024Runtime (ms)Runtime (ms)(WA*Algorithm16-neighbor Starcraft Maps50)(2.2)Algorithm(WA*TBTB(WA*)(1.0(WA*A*)()(1.8)0)3.6)TB(WA*)(2.2)TB(WA*)(2.8)TB(WA*)(1.4)1.TB(WA*)(TB(WTB(WA*)(1.0)01086420TB1)2(WA*320)(1.4425TBRuntime (ms)5Lookahead 1Lookahead 4Lookahead 16Lookahead 64Lookahead 128Lookahead 25630TBLookahead 1Lookahead 4Lookahead 16Lookahead 64Lookahead 128Lookahead 2567Runtime (ms)16-neighbor Room Maps531Lookahead 1Lookahead 32Lookahead 128Lookahead 256Lookahead 512Lookahead 102420000100001000200Algorithm))(3.0A*)TB(WTB(WA*)(2.6)(2.2A*)TB(W)(1.8A*)TB(W)(1.4(1.0A*)TB(W(WTB(WA*)0)A*)(3.6)TBTB(WA*)(2.2)(WA*)(2.8)TBTB(WA*)(1.4)(1.A*)(WTBTB(WA*)(1.0))60AlgorithmFigure 4: 16-neighbor results, search time typically decreases w lookahead parameter increased.(a)(b)Figure 5: 8-neighborhoods (a) 16-neighborhoods (b).564fiT IME -B OUNDED B EST-F IRST EARCH R EVERSIBLE N - REVERSIBLE EARCH G RAPHS8-neighbor Counter Strike Maps10000TB(WA*)(1.0)TB(WA*)(1.4)TB(WA*)(1.8)TB(WA*)(2.2)TB(WA*)(2.6)TB(WA*)(3.0)900080007000600050004000300020001000TB(WA*)(1.0)TB(WA*)(1.4)TB(WA*)(1.8)TB(WA*)(2.2)TB(WA*)(2.6)TB(WA*)(3.0)9000Number Time IntervalsNumber Time Intervals1000016-neighbor Counter Strike Maps80007000600050004000300020000.10.30.50.70.91.11000Duration Time Interval (ms)0.10.30.50.70.91.1Duration Time Interval (ms)Figure 6: Results Game Time Model.similar quality observed lookahead values. TB(WA*), almost values wlookahead configurations, 16-neighbor grids performs fewer moves 8-neighbor grids.example, SC w = 2.6 lookahead parameter 1024, 8-neighbor grids needfactor 1.6 moves 16-neighbor grids. Note however 16-neighbor moveshigher cost 8-neighbor moves. Regarding runtime, TB(WA*) 8-neighbor connectivity runs faster TB(WA*) 16-neighbor connectivity. happens expansionstate 16-neighbor connectivity takes time expanding state 8-neighborconnectivity.4.2 Results Game Time Modelreport results TB(WA*) using Game Time Model Counter Strike maps 8and 16-neighbor grids. use 0.1, 0.3, 0.5, 0.7, 0.9, 1.1 milliseconds duration timeintervals. setting, quality solution measured number time intervalsrequired solve problem, fewer intervals used, better solution qualityis.Figure 6 shows average performance. observe length time interval increases, TB(WA*) yields solutions better quality. hand, w increased, TB(WA*)obtains better solutions. observed clearly duration intervalssmall (e.g., 0.1ms). also observe better-quality solutions 16- rather 8-neighborconnectivity. 16-neighbor connectivity agent perform knight movesingle interval.4.3 Results Non-reversible Search Graphs: Racetracksection compare TBR(WA*) LSS-LRTWA* deterministic version racetrack problem (Barto, Bradtke, & Singh, 1995; Bonet & Geffner, 2003). problem race565fiH ERN ANDEZ , BAIER , &Extended Hansen Racetrack400TBR(WA*)(1.0)TBR(WA*)(3.0)TBR(WA*)(5.0)TBR(WA*)(7.0)LSS-LRT(WA*)(1.0)LSS-LRT(WA*)(3.0)LSS-LRT(WA*)(5.0)LSS-LRT(WA*)(7.0)300250TBR(WA*)(1.0)TBR(WA*)(3.0)TBR(WA*)(5.0)TBR(WA*)(7.0)LSS-LRT(WA*)(1.0)LSS-LRT(WA*)(3.0)LSS-LRT(WA*)(5.0)LSS-LRT(WA*)(7.0)450400Number Actions350Number ActionsGame Map Racetrack500200150350300250200150100100Average Time per Search (ms)2861.41.1.121.860.40.0.20.222.861.41.121.1.860.0.4500.0.250Average Time per Search (ms)Figure 7: Results Racetrack Grids.track represented grid cells marked obstacles. Similar grid pathfinding,problem move agent set initial positions cells marked finalposition. Nevertheless, problem agent associated velocity, set actionsinvolve accelerating (vertically horizontally), performing no-op action maintainscurrent velocity.state racetrack tuple (x, y, vx , vy ), (x, y) position vehicle,(vx , vy ) velocity vector. actions represented tuples form (ax , ay ),ax , ay {1, 0, 1}, correspond acceleration vector. Unlike original version (Bartoet al., 1995), actions deterministic one initial one destination cell.actions deterministic, (ax , ay ) performed (x, y, vx , vy ), new state given(x0 , 0 , vx0 , vy0 ), vx0 = vx +ax vy0 = vy +ay , (x0 , 0 ) computed consideringvehicle changes velocity (vx0 , vy0 ) moving. movement towards (x0 , 0 )would lead crashing obstacle, like Bonet Geffner (2003) do, leave vehicle nextobstacle velocity (0, 0).experiments, used two racetracks. firstwhich refer HRTis 33207 grid corresponds extended version racetrack used Hansen Zilberstein (2001) (which 33 69 grid). also use game map AR0205SR Baldurs Gate,whose size 214x212. refer map GRT.generated 50 random test cases HRT GRT Manhattan distanceinitial state goal state greater half width map. absolutevalue components velocity vector restricted 3. heuristicuse Euclidean distance divided maximum speed.evaluated TBR (WA*) LSS-LRTAWA* four weight values (1.0, 3.0, 5.0, 7.0). Figure 7 shows plot number actions versus average time per search episode. TBR (WA*)number actions corresponds sum number moves plus number times566fiT IME -B OUNDED B EST-F IRST EARCH R EVERSIBLE N - REVERSIBLE EARCH G RAPHSTBWA*(2)TBWA*(3)TBWA*(4)TBWA*(5)LSS-LRTWA*(2)LSS-LRTWA*(3)LSS-LRTWA*(4)LSS-LRTWA*(5)050100150200250300350400450500550100Lookahead100000TBWA*(2)TBWA*(3)TBWA*(4)TBWA*(5)LSS-LRTWA*(2)LSS-LRTWA*(3)LSS-LRTWA*(4)LSS-LRTWA*(5)10000050100150200250300350400450500550100015-puzzleNumber Expansions (log scale)Cost (log scale)15-puzzleLookaheadFigure 8: Cost time comparison TB-WA LSS-LRTWA*vehicle move. TBR (WA*) make movements searchrestarted.important note time spent updating heuristic proportional numberstates updated. update TBR (WA*) may take time updateLSS-LRTWA* Closed list may contain states former algorithm.reason use comparison average time per search, considers searchupdate time.HRT (Figure 7) observe worst behavior one obtained TBR (WA*)(1.0).algorithms improve performance increasing w, TBR (WA*), used weightgreater 1.0, algorithm clearly yields best performance. GRT, worst algorithms TBR (WA*)(1.0) LSS-LRTA(1.0). Here, algorithms improve increasingweight.benchmark used fewer problems game maps, carried95% confidence analysis cost solutions. HRT, showed costsbest configuration TBR (WA*)(5.0) could 10% away true mean,LSS-LRTA*(3.0) costs could 11% away true mean. GRT, hand,difference performance two best configurations TBR (7) LSS-LRTWA*(7)statistically significant.Finally, experiments showed computational cost learning phase TB(WA*)higher LSS-LRTA(WA*). Indeed, number updates carried TB(WA*)3.4 times less number updates carried LSS-LRTA(WA*) HRT 1.6 timeless GRT. explains better performance terms runtime.567fiH ERN ANDEZ , BAIER , &4.4 Results 15-Puzzlechose 15-puzzle another domain evaluating time-bounded algorithms.build 15-puzzle implementation extending Richard Korfs implementation available Carlos Linaress homepage.2 present results TB(WA*), LSS-LRT(WA*) algorithms.use 100 test cases presented Korf (1993), uses Manhattan distance heuristic.domain report results slightly different way. First, omit results TB(A*)(TB(WA*) w = 1) terminate reasonable time. due factA* needs many expansions solving hardest test cases. Second, use numberexpansions instead runtime efficiency measure. domain, found measurestable since, general, solving 100 problems take much timew > 1 (0.3s w = 2; 0.08s w = 3), thus time prone affected external factorscontrolled operating system.Figure 8 shows performance TB(WA*) LSS-LRT(WA*). use lookahead values{16, 32, 64, 128, 256, 512} weights {2, 3, 4, 5}. observe following relations.Solution Cost solution cost TB(WA*) decreases w increased almost lookaheadvalues. TB(WA*) obtains better results LSS-LRTWA* lookahead valuesw > 2. w < 2 performance TB(WA*) worse performence LSSLRTA*. hand, TB(WA*) w = 5 obtains solution 2.0 times betteraverage solution obtained LSS-LRTA* (LSS-LRTWA* w = 1).Number Expansions number expansions TB(WA*) decreases w increased.TB(WA*) efficient LSS-LRTWA* lookahead values w > 2.worst performing configuration TB(WA*) w = 1.Note curve remains flat several configurations. smallnumber expansions needed solve problem.conclusion, considering solution cost number expansions, 15-puzzle TB(WA*)better algorithm. instance, average solution cost TB(WA*) 1.6 times betteraverage average solution cost LSS-LRTA*.compare greedy algorithm (Parberry, 2015), real-time domainspecific, unlike our.5. Summary Conclusionspaper introduced Time-Bounded Best-First Search, generalization real-time searchalgorithm Time-Bounded A*. addition, introduced restarting version time-boundedapproach, TBR (WA*), unlike TB(BFS), better coverage non-reversible domains.carried theoretical analysis TB(WA*) TBR (WA*), including terminationresults cost bound TB(WA*). Given weight w, bound suggests TB(WA*)significantly superior TB(A*) precisely search problems WA* expands significantlyfewer states A*. addition, bound suggests TB(WA*) may yield benefitsdomains WA*, run offline, yield improvements A*. theoreticalbounds easily adapted instances Best-First Search offer guarantees solution2. http://scalab.uc3m.es/clinares/download/source/ida/ida.html568fiT IME -B OUNDED B EST-F IRST EARCH R EVERSIBLE N - REVERSIBLE EARCH G RAPHSquality. TBR (WA*), proved termination strongly connected graphs, even containnon-reversible actions. property also enjoyed real-time search algorithms LRTA*family enjoyed TB(BFS).experimental evaluation, focused pathfinding, 15-puzzle, racetrackproblem, found TB(WA*) TBR (WA*) significantly superior real-timesearch algorithms LRTA* family. addition, found performance tends improveweight parameter increased, without increasing time per search episode. findinginteresting although quality also improved increasing lookahead parameter,increases time spent search episode.well known many search benchmarks, WA* may expand significantly fewer nodesA*. Consistent this, experiments, time-bounded versions suboptimal algorithmslike Weighted A* produce significantly better solutions obtained TB(A*). Improvements less noticeable lookahead parameter large, also predicted theory.first observe performance gains using weights real-time setting.Indeed, findings consistent Rivera et al. (2015), also obtain better solutionsusing weighted heuristics. work adds another piece evidence justifies studyingincorporation weights real-time algorithms (e.g., RIBS EDA;* Sturtevant, Bulitko,& Bjornsson, 2010; Sharon, Felner, & Sturtevant, 2014). Finally, SLA* (Shue & Zamani, 1993)LRTS (Bulitko & Lee, 2006) two algorithms also perform backtracking moves.investigation whether restarts could provide benefits algorithms left futurework.Acknowledgementsthank Vadim Bulitko providing Counter Strike maps. research partly fundedFondecyt grant number 1150328.ReferencesAine, S., & Likhachev, M. (2013). Truncated incremental search: Faster replanning exploitingsuboptimality. Proceedings 27th AAAI Conference Artificial Intelligence (AAAI),Bellvue, Washington, USA.Barto, A. G., Bradtke, S. J., & Singh, S. P. (1995). Learning act using real-time dynamic programming. Artificial Intelligence, 72(1-2), 81138.Bjornsson, Y., Bulitko, V., & Sturtevant, N. R. (2009). TBA*: Time-bounded A*. Proceedings21st International Joint Conference Artificial Intelligence (IJCAI), pp. 431436.Bonet, B., & Geffner, H. (2003). Labeled rtdp: Improving convergence real-time dynamicprogramming.. ICAPS, Vol. 3, pp. 1221.Bulitko, V., & Lee, G. (2006). Learning real time search: unifying framework. JournalArtificial Intelligence Research, 25, 119157.Bulitko, V., Bjornsson, Y., Sturtevant, N., & Lawrence, R. (2011). Real-time Heuristic SearchGame Pathfinding. Applied Research Artificial Intelligence Computer Games. Springer.Burns, E., Ruml, W., & Do, M. B. (2013). Heuristic search time matters. Journal ArtificialIntelligence Research, 47, 697740.569fiH ERN ANDEZ , BAIER , &Ebendt, R., & Drechsler, R. (2009). Weighted A* search - unifying view application. ArtificialIntelligence, 173(14), 13101342.Gaschnig, J. (1977). Exactly good heuristics?: Toward realistic predictive theory bestfirst search. Reddy, R. (Ed.), Proceedings 5th International Joint ConferenceArtificial Intelligence (IJCAI), pp. 434441. William Kaufmann.Hansen, E. A., & Zilberstein, S. (2001). Lao: heuristic search algorithm finds solutionsloops. Artificial Intelligence, 129(1), 3562.Hart, P. E., Nilsson, N., & Raphael, B. (1968). formal basis heuristic determinationminimal cost paths. IEEE Transactions Systems Science Cybernetics, 4(2).Hernandez, C., Asn, R., & Baier, J. A. (2014). Time-bounded best-first search. Proceedings7th Symposium Combinatorial Search (SoCS).Hernandez, C., & Baier, J. A. (2012). Avoiding escaping depressions real-time heuristicsearch. Journal Artificial Intelligence Research, 43, 523570.Hernandez, C., Baier, J. A., Uras, T., & Koenig, S. (2012). TBAA*: Time-Bounded Adaptive A*.Proceedings 10th International Joint Conference Autonomous Agents MultiAgent Systems (AAMAS), pp. 9971006, Valencia, Spain.Koenig, S. (2001). Agent-centered search. Artificial Intelligence Magazine, 22(4), 109131.Koenig, S., & Likhachev, M. (2005). Fast replanning navigation unknown terrain. IEEETransactions Robotics, 21(3), 354363.Koenig, S., & Likhachev, M. (2006). Real-time adaptive A*. Proceedings 5th InternationalJoint Conference Autonomous Agents Multi Agent Systems (AAMAS), pp. 281288.Koenig, S., & Sun, X. (2009). Comparing real-time incremental heuristic search real-timesituated agents. Autonomous Agents Multi-Agent Systems, 18(3), 313341.Korf, R. E. (1990). Real-time heuristic search. Artificial Intelligence, 42(2-3), 189211.Korf, R. E. (1993). Linear-space best-first search. Artificial Intelligence, 62(1), 4178.Likhachev, M., Gordon, G. J., & Thrun, S. (2003). ARA*: Anytime A* Provable BoundsSub-Optimality. Proceedings 16th Conference Advances Neural InformationProcessing Systems (NIPS), Vancouver, Canada.Parberry, I. (2015). Memory-efficient method fast computation short 15-puzzle solutions.IEEE Trans. Comput. Intellig. AI Games, 7(2), 200203.Pearl, J. (1984). Heuristics: Preintelligent Search Strategies Computer Problem Solving.Addison-Wesley Longman Publishing Co., Inc., Boston, MA, USA.Pohl, I. (1970). Heuristic search viewed path finding graph. Artificial Intelligence, 1(3),193204.Rivera, N., Baier, J. A., & Hernandez, C. (2015). Incorporating weights real-time heuristicsearch. Artificial Intelligence, 225, 123.Schmid, K., Tomic, T., Ruess, F., Hirschmuller, H., & Suppa, M. (2013). Stereo vision based indoor/outdoor navigation flying robots. IEEE/RSJ International Conference Intelligent Robots Systems (IROS), pp. 39553962.570fiT IME -B OUNDED B EST-F IRST EARCH R EVERSIBLE N - REVERSIBLE EARCH G RAPHSSharon, G., Felner, A., & Sturtevant, N. R. (2014). Exponential deepening a* real-time agentcentered search. Proceedings 7th Symposium Combinatorial Search (SoCS), pp.871877.Shue, L., & Zamani, R. (1993). admissible heuristic search algorithm. Komorowski, H. J.,& Ras, Z. W. (Eds.), Proceedings 7th International Symposium MethodologiesIntelligent Systems (ISMIS), Vol. 689 LNCS, pp. 6975. Springer.Sturtevant, N. (2012). Benchmarks grid-based pathfinding. Transactions ComputationalIntelligence AI Games, 4(2), 144 148.Sturtevant, N. R., Bulitko, V., & Bjornsson, Y. (2010). learning agent-centered search.Proceedings 9th International Joint Conference Autonomous Agents Multi AgentSystems (AAMAS), pp. 333340, Toronto, Ontario.Wilt, C. M., & Ruml, W. (2012). weighted A* fail?. Proceedings 5th Symposium Combinatorial Search (SoCS), Niagara Falls, Ontario, Canada.571fiJournal Artificial Intelligence Research 56 (2016) 379-402Submitted 09/15; published 06/16Generating Models Matched FormulaPolynomial DelayPetr Savickysavicky@cs.cas.czInstitute Computer Science, Czech Academy SciencesPod Vodarenskou Vez 2, 182 07 Praha 8, Czech RepublicPetr Kucerakucerap@ktiml.mff.cuni.czDepartment Theoretical Computer Science Mathematical LogicFaculty Mathematics Physics, Charles University Prague,Malostranske nam. 25, 118 00 Praha 1, Czech RepublicAbstractmatched formula CNF formula whose incidence graph admits matchingmatches distinct variable every clause. formula always satisfiable. Matchedformulas used, example, area parametrized complexity. proveproblem counting number models (satisfying assignments) matchedformula #P-complete. hand, define class formulas generalizingmatched formulas prove formula class one choose polynomialtime variable suitable splitting tree search models formula.consequence, models formula class, particular matchedformula, generated sequentially delay polynomial size input.hand, prove task cannot performed efficiently linearlysatisfiable formulas, generalization matched formulas containing classconsidered above.1. Introductionpaper, consider problem counting models (satisfying assignments)generating subsets models given formula conjunctive normal form (CNF).well known problem counting models general CNF #P-complete (Sipser,2006). problem generating models general CNF formula clearly also hard,checking whether least one satisfying assignment formula,SAT problem, NP-complete (Garey & Johnson, 1979).paper, mostly deal problem enumerating models formula.problem important areas research applications, unbounded modelchecking (Kang & Park, 2005; McMillan, 2002) data mining (Coquery, Jabbour, Sais,Salhi, et al., 2012). success modern SAT solvers inspired design model countingenumeration algorithms well (see e.g. Jabbour, Lonlac, Sais, & Salhi, 2014; Morgado& Marques-Silva, 2005a, 2005b). addition basic enumeration problemrequire models generated prescribed order, versionsconsidered, e.g. generating models non-decreasing weight (Creignou, Olive, & Schmidt,2011).Another line research concentrated studying special classes boolean formulasenumeration algorithm guaranteed complexity could devised. Onec2016AI Access Foundation. rights reserved.fiSavicky & Kuceraeasily find example formula set models exponentially largersize formula itself. case reasonable include size outputbound running time enumeration algorithm. specifically sayalgorithm enumerates models formula runs output polynomial timerunning time bounded polynomial two variables, size input(i.e. input formula ) size output (i.e. number models ).paper, consider restrictive setting follows. algorithm receives inputformula generates sequence models way time neededgenerating first model time generating two consecutive modelssequence polynomial length formula. type complexity boundcalled polynomial delay. clear enumerate models formulapolynomial delay, construct output polynomial algorithmtask well. hand, much harder get enumeration algorithmpolynomial delay output polynomial algorithm. overview variousnotions enumeration complexity (see Johnson, Yannakakis, & Papadimitriou, 1988).special classes formulas polynomial delay enumeration algorithmsdescribed, includes 2-CNF formulas, Horn formulas, generalized satisfiabilityproblems others (see e.g. Aceto, Monica, Ingolfsdottir, Montanari, & Sciavicco, 2013;Creignou & Hebrard, 1997; Dechter & Itai, 1992; Kavvadias, Sideri, & Stavropoulos, 2000).paper, describe another class formulas polynomial delay enumeration algorithm based backtrack-free search described. contraryalgorithms known 2-CNF Horn formulas, splitting variable step cannotchosen arbitrarily, however, existence suitable variable guaranteedefficiently identified.particular consider class matched formulas introduced FrancoVan Gelder (2003). Given CNF formula , consider incidence graph I() defined follows. I() bipartite graph one part consisting clausespart containing variables . edge {x, C} variable x clause CI() x x appears C. observed Aharoni Linial (1986) Tovey(1984) I() admits matching (i.e. set pairwise disjoint edges) size (wherenumber clauses ), satisfiable. Later formulas satisfyingcondition called matched formulas Franco Van Gelder. Since matchingmaximum size bipartite graph found polynomial time (see e.g. Lovasz &Plummer, 1986), one check efficiently whether given formula matched.Given general formula , measure far matched considering maximum deficiency (), number clauses remain unmatchedmaximum matching I(). formula thus matched iff () = 0. weaker notiondeficiency () = n, number clauses n numbervariables , also often considered.Matched formulas play significant role theory satisfiability solving. Sinceintroduction matched formulas considered base class parameterizedalgorithms satisfiability, see e.g. book Flum Grohe (2006) overviewparameterized algorithms theory. particular, Fleischner, Kullmann, Szeider (2002)show satisfiability formulas maximum deficiency bounded constant kdecided time O(kknO(k) ) kk length input formula n380fiGenerating Models Matched Formuladenotes number variables. result later improved Szeider (2003)algorithm satisfiability parameterized maximum deficiency formulacomplexity O(2k n3 ). Parameterization based backdoor sets respect matchedformulas considered Szeider (2007).Since matched formulas trivially satisfiable, ask stronger question:hard count enumerate models matched formula? prove countingmodels matched formula #P-complete problem, turn attentiongenerating models matched formula. main result paper algorithmgenerates models matched formula polynomial delay. algorithmconstructs splitting tree whose nodes correspond either matched unsatisfiableformula. However, cases strategy sufficient since nodes treecannot split way. prove node corresponds formulasatisfied iterated elimination pure literals. Formulas propertycalled pure literal satisfiable. formulas studied Kullmann (2000)subclass linearly satisfiable formulas. node pure literal satisfiable formulareached, algorithm switches simpler strategy. prove models pureliteral satisfiable formula generated delay linear length formula.hand, #SAT problem pure literal satisfiable formulas #P-complete,problem #P-complete monotone 2CNFs (Valiant, 1979a, 1979b),pure literal satisfiable.Several generalizations matched formulas also considered literature. Kullmann (2000) generalized matched formulas class linearly satisfiableformulas. Autarkies based matchings studied Kullmann (2003). Szeider (2005)considered another generalization matched formulas, classes biclique satisfiablevar-satisfiable formulas. Unfortunately, biclique satisfiable var-satisfiableformulas hard check formula falls one classes (Szeider, 2005).show paper result transfer class linearly satisfiableformulas demonstrating possible generate models linearly satisfiableformula polynomial delay unless P=NP.paper organized follows. giving basic definitions Section 2, describeSection 3 specific simple splitting property class formulas, allowsgenerate models formula class efficiently. Section 4, consider pureliteral satisfiable formulas prove class required splitting property.Section 5, consider matched formulas prove required splitting propertyclass formulas, generalizes matched pure literal satisfiable formulasnatural way. implies algorithm generating models matched formulaformula general class polynomial delay. Section 6, presentcomplexity bounds efficient versions algorithms previous sections.Section 7, show negative result concerning linearly satisfiable formulas. Section 8contains concluding remarks directions research.2. Definitionssection, give necessary definitions summarize results usepaper.381fiSavicky & Kucera2.1 Boolean FunctionsBoolean function n variables mapping f : {0, 1}n {0, 1}. literal eithervariable, called positive literal, negation, called negative literal. negationvariable x denoted x x. clause disjunction set literals,contains one literal variable. Formula conjunctive normal form(CNF) or, equivalently, CNF formula, conjuction clauses. often treatclause set literals CNF formula set clauses. well knownfact every Boolean function represented CNF formula (see e.g. Genesereth& Nilsson, 1987). size formula number clausesdenoted ||. length formula total number occurrences literals ,i.e. sum sizes clauses , denoted kk. Given variable xvalue {0, 1}, [x = a] denotes formula originating substituting xvalue obvious simplifications consisting removing falsified literals satisfiedclauses. extend notation negative literals well setting [x = a] = [x = a].formula obtained assigning values a1 , . . . , ak {0, 1} variablesx1 , . . . , xk denoted [x1 = a1 , x2 = a2 , . . . , xk = ak ]. say literal l pureCNF formula, occurs formula negated literal l not. literalirrelevant formula, neither literal negation occurs formula.variable pure, appears positively, negatively , i.e. appearsliteral, pure .Let formula defining Boolean function f n variables. assignmentvalues v {0, 1}n model (also satisfying assignment, true point ),satisfies f , i.e. f (v) = 1. set models denoted (). models() defined variables occurrence . set variablesfunction defined formula larger, however, introduce specialnotation general case. algorithmic purposes, also necessary,since adding irrelevant variable formula changes set models addingvariable possible values element original set models.partial assignment assigns values subset variables. formulavariables x1 , . . . , xn , represented ternary vector v {0, 1, }n ,vi = denotes fact xi assigned value v.Note empty clause admit satisfying assignment empty CNFsatisfied assignment.2.2 Matched Formulaspaper use standard graph terminology, (see e.g. Bollobas, 1998). Givenundirected graph G = (V, E), subset edges E matching G edgespairwise disjoint. bipartite graph G = (A, B, E) undirected graphdisjoint sets vertices B, set edges E satisfying E B. setW vertices G, let (W ) denote neighborhood W G, i.e. set verticesadjacent element W . shall use following well-known result matchingsbipartite graphs:Theorem 2.1 (Halls Theorem). Let G = (A, B, E) bipartite graph. matchingsize |M | = |A| exists every subset |S| |(S)|.382fiGenerating Models Matched FormulaLet = C1 . . . Cm CNF formula n variables X = {x1 , . . . , xn }. associatebipartite graph I() = (, X, E) (also called incidence graph ),vertices correspond clauses variables X. clause Ci connectedvariable xj (i.e. {Ci , xj } E) Ci contains xj xj . CNF formula matched I()matching size m, i.e. matching pairs clause uniquevariable, shall call matching clause saturated matching. Note matchedCNF trivially satisfiable, since clause satisfied literal containingvariable matched given clause. variable, matched clausegiven matching , called matched , free otherwise.2.3 Generating Models Polynomial Delaymain goal paper describe algorithm which, given matched formula ,generates set () models polynomial delay. Let us state formallyrequire algorithm.say algorithm generates models Boolean formula polynomialdelay, polynomial p, algorithm, given formula input,satisfies following properties.1. works steps, takes time O(p(kk)).2. step, either finds model different models obtainedprevious steps (in particular, model first step) determinesmodel, previous steps already found models .algorithm properties exists, follows constructset () models time O((|T ()| + 1) p(kk)), means algorithmoutput polynomial. Note since () may exponential size respect kk,efficiency respect size input output best hopeconstructing ().3. Efficient Splitting Tree Algorithmidea algorithm construct decision tree function representedgiven satisfiable CNF, every subtree larger single leaf contains 1-leaf.depth tree number variables. tree searchedDFS order, time needed arbitrary moment reach 1-leaf ntimes time needed split node. following, show classesformulas including matched formulas possible find splitting procedureyields tree described above.decision tree Boolean function f labeled binary tree, inner nodelabeled variable, leaves edges labels 0 1. decision tree computesf (x) given assignment x process starts root visitednode follows edge labeled value variable, label node.output label leaf reached process. computation path testsvariable, tested previous part path, test redundant.consider trees without redundant tests.383fiSavicky & Kuceradecision tree representing function given CNF formula constructed top follows. root tree assigned . non-leaf nodetree assigned formula , choose arbitrary split variable xoccurrence assign restricted formulas [x = 0] [x = 1] successors.node assigned empty formula becomes 1-leaf node assigned formula,contains empty clause, becomes 0-leaf. resulting decision tree representsfunction given , although large practical purposes. pathroot inner node u tree corresponds partial assignment changesformula representing function computed subtree whose root u.depth tree function n variables n.leaf node labeled 1 represents set models , precisely, leafdepth represents 2nd models . Moreover, different leaves tree representdisjoint sets models. Given decision tree function represented , can,traversing it, generate models time proportional size. process leadslarge delay generating successive models, tree contains large subtrees0-leaves. following condition class formulas describes situationavoided.Definition 3.1. Let U class formulas, let U let x variableoccurrence . say x splitting variable relative U , every{0, 1}, [x = a] satisfiable, [x = a] U .class formulas U splitting property, every formula U containingvariable contains splitting variable relative U .shall associate splitting problem class formulas U splitting property.Definition 3.2. Let U class formulas splitting property. splitting problemrelative U following problem: Given formula U , find splitting variablerelative U results satisfiability tests formulas [x = 0] [x = 1].Note complexity splitting problem relative U also upper boundtime satisfiability test formulas U . formula satisfiable,variable x least one formulas [x = 0][x = 1] satisfiable. result satisfiability checks splitting variable xrequired part solution splitting problem.Theorem 3.3. class formulas U splitting property splitting problemrelative U solved time c(), c() kk formula U ,models formula U n variables generated delay O(n c()).Proof. Construct tree DFS order using splitting variable every formulaassigned non-leaf node. non-leaf node labeled x splittingvariable, successors labeled [x = 0] [x = 1]. formulasunsatisfiable, corresponding successor becomes 0 leaf. formulasempty, corresponding successor becomes 1 leaf. root tree split evenunsatisfiable, however, nodes labeled unsatisfiable formula split.384fiGenerating Models Matched FormulaHence, except possibly root, node two 0-leaves successors.Since length every formula tree kk, node, time O(c())sufficient choose splitting variable, determine successors leaf,construct formulas successors node.Let us assume u non-leaf node constructed tree different root.One successors u labeled unsatisfiable formula. recognizedsplitting algorithm successor 0-leaf. Consequently, timeO(c()), construction tree continues satisfiable successor u. Hence,n splitting steps time O(n c()), 1-leaf reached. 2Remark 3.4. contains unit clause U closed unit propagation,variable x contained unit clause splitting variable identified efficiently.reason known satisfiable, one formulas [x = a] containsempty clause and, hence, satisfiable.Remark 3.5. class U satisfies1. satisfiability formulas U tested polynomial time,2. U closed partial assignments,splitting problem relative U polynomial complexity. Indeed, casevariable formula U splitting variable satisfiability testscorresponding restrictions obtained polynomial time. Class U propertysometimes also conservative. also say property particular formself-reducibility (in sense considered e.g. Khuller & Vazirani, 1991). classesgeneralized satisfiability problem described Creignou Hebrard (1997)property addition classes, consider, instance, Horn formulas, SLUR formulas,2CNFs, q-Horn formulas, etc. immediate corollary Theorem 3.3, possiblegenerate models formulas classes polynomial delay.main result paper splitting problem relative slight generalization matched formulas also polynomial complexity although class matchedformulas closed partial assignments.4. Pure Literal Satisfiable Formulasconsidering matched formulas, let us make small detour class formulassatisfiable iterated elimination pure literals, call pure literalsatisfiable. formulas already considered Kullmann (2000) specialcase linearly satisfiable formulas.set literals called consistent, contain contradictory literals. lliteral, let assign(l) assignment variable contained l, satisfiesl. consistent set sequence literals L, let assign(L) partial assignmentvariables satisfying literals L. formula , [L] abbreviation[assign(L)].385fiSavicky & KuceraDefinition 4.1. pure literal sequence formula consistent sequence literals(l1 , . . . , lk ), every = 1, . . . , k, literal li either pure irrelevantformula [l1 , . . . , li1 ]. particular, l1 pure irrelevant . pure literal sequencecalled strict, literals li pure [l1 , . . . , li1 ].L pure literal sequence , formula [L] called reduced formulacorresponding L. [L] contain pure literal, L called maximalpure literal sequence .Definition 4.2. formula pure literal satisfiable, pure literal sequence L, reduced formula [L] empty or, equivalently, assign(L) satisfyingassignment .autarky formula partial assignment v variables, everyclause either satisfied unchanged v. Autarkies studied e.g. Kullmann(2000). Note every initial segment pure literal sequence defines assignmentvariables, autarky. Moreover, one easily verify propertycharacterizes pure literal sequences.Let us note pure literal satisfiable formulas closed partial assignments.Consider formula , contain pure literal. Let formula obtainedadding new variable x positive literal every clause. Formulapure literal satisfiable, [x = 0] = pure literal satisfiable. followspure literal satisfiable formulas satisfy second property required Remark 3.5put effort showing pure literal satisfiable formulassplitting property splitting problem relative pure literal satisfiable formulaspolynomial complexity.every CNF formula, may tested polynomial time, whether pure literalsatisfiable. order find pure literal sequence witnessing fact, procedureFindPLS Algorithm 1 uses greedy approach, step chooses satisfiespure literal current formula. approach meaningful, since literalpure stage procedure, either remains pure becomes irrelevantfollowing stages. pure literal sequence obtained procedure dependsnondeterministic choices made procedure, however, Corollary 4.4, resultingreduced formula uniquely determined input.Lemma 4.3. clause C CNF removed run FindPLS,removed every run FindPLS input .Proof. Let L K pure literal sequences produced different runs FindPLS. formulas [L] [K] corresponding reduced formulas let Cclause contained [L]. Hence, L contains literals C. Since [K]subset , L pure literal sequence [K]. literal L contained[K], first literals pure [K]. Since [K] contain pureliteral, literal L contained [K]. particular, C contained [K]. 2following immediate corollary.386fiGenerating Models Matched FormulaAlgorithm 1 Constructing pure literal sequenceRequire: CNF formula .Ensure: maximal strict pure literal sequence L corresponding reducedformula.1: procedure FindPLS()2:3:Initialize new empty list literals L.4:Initialize Pure() set pure literals .5:Pure() 6=6:Choose literal l Pure().7:Add l L.8:[l].9:Update Pure() consist pure literals .10:end11: end procedureCorollary 4.4. Let CNF formula let L pure literal sequence obtainedFindPLS .1. formula [L] uniquely determined .2. formula pure literal satisfiable, [L] empty.Since running time procedure FindPLS polynomial length inputformula, maximal pure literal sequence formula constructed polynomialtime. complexity constructing maximal pure literal sequence formula is,fact, O(kk) Lemma 6.1.Lemma 4.5. Let L = (l1 , . . . , ln ) pure literal sequence formula , containsliteral variable . = 1, . . . , n, denote xi variable contained li .xi variable largest index among variables, occurence, xi splitting variable relative pure literal satisfiable formulasformulas [xi = 0] [xi = 1] satisfiable, containempty clause.Proof. Let one formulas [xi = 0] [xi = 1] let L = (l1 , . . . , li1 ).Clearly, L pure literal sequence . Moreover, contain empty clause,L assigns value literals every clause hence, satisfies it. 2sufficient show splitting problem relative class pure literalsatisfiable formulas polynomial complexity. Later Theorem 6.2 shall showsplitting problem case solved time O(kk).Lemma 4.6. splitting problem relative class pure literal satisfiable formulaspolynomial complexity.387fiSavicky & KuceraProof. pure literal satisfiable, pure literal sequence, satisfies it,obtained FindPLS polynomial time. sequence contain literalsvariables, extended polynomial time appending arbitrary literals missingvariables obtain pure literal sequence satisfying assumption Lemma 4.5. Then,lemma implies method select splitting variable obtain resultssatisfiability test corresponding restrictions polynomial time. 2pure literal sequence satisfies assumption Lemma 4.5 formula ,sequence used find splitting variable formulas splitting tree. Using this, models pure literal satisfiable formula generateddelay smaller general bound Theorem 3.3, see Corollary 6.2.Remark 4.7. sign literal given variable, occurs strict pure literalsequence, uniquely determined. variables y1 y2 occurpositively negatively strict pure literal sequence formula(x1 y1 ) (x2 y1 ) (x3 y2 ) (x4 y2 ) (y1 y2 ) .example, (x2 , y1 , x3 , y2 ) (x4 , y2 , x1 , y1 ) strict pure literal sequences formula.5. Matched Formulassection concentrate matched formulas. Let us start showingproblem determining number models matched formula , i.e. size |T ()|,hard general #SAT problem.Theorem 5.1. problem determining |T ()| given matched formula #Pcomplete.Proof. Let = C1 C2 . . . Cm arbitrary CNF formula n variables. Lety1 , . . . , ym new variables appearing let = (y1 y2 . . . ym ) clause.Let us define CNF formula n + variables equivalent= (C1 D) (C2 D) . . . (Cm D) .Clearly, matched formula one also observe |T ()| = |T ()| 2n (2m 1).thus reduced problem counting models general CNF formula(i.e. general #SAT problem) problem counting models matched CNFformula (i.e. #SAT problem restricted matched formulas). 2goal show generate models matched formulapolynomial delay. Theorem 3.3 cannot used directly, since classmatched formulas splitting property seen followingexample. Consider formula(x1 x2 ) (x1 x3 ) (x2 x3 ) .388fiGenerating Models Matched Formulaformula matched, splitting variable. Indeed, setting x1 0 leadssatisfiable, yet matched formula (x2 )(x3 )(x2 x3 ) symmetry truevariables x2 x3 well. order achieve objective, consider richerclass formulas. class consider generalizes matched pure literal satisfiableformulas follows. Note empty formula matched, since correspondsempty graph formally assume empty graph possesses requiredmatching.Definition 5.2. formula called pure literal matched, reduced formula obtainedprocedure FindPLS matched.Elimination pure literal preserves property matched, since pure literalautarky. Hence, matched formula pure literal matched. Clearly, every pure literalsatisfiable formula pure literal matched, since reduced formula empty and, hence,matched.basic idea efficient splitting algorithm matched formulas presentedfollowing theorem. Later shall show Corollary 6.4 splitting problemrelative pure literal matched formulas solved time O(n2 kk).Theorem 5.3. class pure literal matched formulas splitting propertysplitting problem relative pure literal matched formulas polynomial complexity.order prove Theorem 5.3, show several statements concerningstructure matched formula. V set variables, say clause limitedV , contains literals variables V .Definition 5.4. Let V subset variables matched formula let C denoteset clauses limited V . set V called critical block ,|C| = |V |. Formally, V empty, also critical block.Note matched formula, V subset variables, C setclauses limited V , Halls theorem (Theorem 2.1 above)|C| (C) |V |. Critical blocks achieving equality. blocksfollowing property.Lemma 5.5. Let V critical block matched formula . Then, every clausesaturated matching I(), variables V matched clauses limited V .Proof. Let matched formula fixed clause saturated matchingvariables clauses . V critical block, |V | clauses limitedV clauses matched variables V . Since variables matchedclauses different, variables V matched one clauses. 2Another useful property set critical blocks follows.Lemma 5.6. set critical blocks matched formula closed intersection.Proof. Let matched formula let V1 , V2 critical blocks. intersectionV1 V2 empty, conclusion lemma satisfied. variable x V1 V2 ,389fiSavicky & KuceraLemma 5.5, every clause saturated matching I(), variable matchedclause, limited V1 also V2 . Hence, number clauses,limited V1 V2 , least |V1 V2 |. Since matched, number clausesequal |V1 V2 | Halls theorem. Hence, V1 V2 critical block required. 2formula x variable contained least one critical block,Lemma 5.6 implies unique inclusion minimal critical block containingx, equal intersection critical blocks containing x. matchedformula number clauses variables, every variable containedcritical block, since set variables formula critical block.Definition 5.7. matched formula number clauses variables x one variables, let Bx denote inclusion minimal critical blockcontaining x.notation Bx specify formula, since always clearcontext. aim show formula matched, either findsplitting variable relative matched formulas, actually pure literal satisfiable.order show property basis algorithm, shall first investigatestructure critical blocks respect matchings.Lemma 5.8. Let matched formula number clauses variables.Let l literal containing variable x let us assume formula [l]matched.1. literal l pure irrelevant clauses limited Bx ,2. clause C contains l, every matching , C matchedvariable y, Bx (where denotes strict inclusion).Proof. symmetry, shall consider case l = x. Hence, assumptions,[x = 0] matched formula.1. critical block Bx subset every critical block containing x. Hence, orderprove first part lemma, sufficient show least onecritical block B containing x, x occur negatively clauseslimited B. Let C set clauses Halls conditionformula [x = 0] satisfied |(C)| < |C|. Let V = (C) setvariables, occurrence clauses C, let k = |V |.least k + 1 clauses C. Since every clause C limited V ,least k + 1 clauses [x = 0] limited V . clauses eitherclause obtained clause removing literal x. Considerset clauses limited V {x}. Since matched, Halls conditionsatisfied set. Hence, contains k + 1 clauses limited V {x}.Setting x = 0 leads least k + 1 clauses limited V . Hence, contains preciselyk + 1 clauses limited V {x} none contains literal x. Hence,V {x} critical block required property proof first partlemma finished.390fiGenerating Models Matched Formula2. Let us fix clause saturated matching clauses variables I() letclause matched x. Since [x = 0] matched, followscontains positive literal x, otherwise matching would work[x = 0] well. Let C clause containing x let variable Cmatched . Since C different D, 6= x. assumptions,set variables critical block and, hence, critical blockwell-defined. Since C matched y, C limited Lemma 5.5.implies x , x C. Since critical block containing xBx inclusion minimal critical block containing x, Bx . first partlemma, clause limited Bx contains x implies C limitedBx thus Bx 6= . Together get Bx .2structure critical blocks used show following proposition neededprove Theorem 5.3.Theorem 5.9. Let matched formula. every variable x, occurence, {0, 1}, [x = a] matched, pure literal satisfiable.Proof. Let matched formula satisfying assumptions let us fix clausesaturated matching I(). variable x matchedclause, assigning value x yields matched formula. assumptiontherefore suppose variable exist variable matchedclause. case, numbers clauses variables equalvariable x , Bx well-defined.Let n number variables clauses . = 1, . . . , n, let liliteral containing variable xi clause matched variable. every= 1, . . . , n, formula [li ] matched formula [li ] matched. Considerstrict partial order variables definedx < Bx(1)means strict inclusion. Lemma 5.8, variables maximalpartial order pure . Let us consider total ordering variables,consistent strict partial order (1). Using appropriate renaming variables,may assume ordering x1 , . . . , xn , every i, j, xi < xj , < j.Let us verify using ordering, sequence ln , ln1 , . . . , l1 satisfying pure literalsequence . Let us show induction = n, . . . , 1 xi pure irrelevantformula [ln , . . . , li+1 ]. true = n Lemma 5.8, xn maximalorder variables induced inclusion critical blocks. Let us fixconsider partial assignment assign(ln , . . . , li+1 ). Lemma 5.8, clause containingli matched variable xj satisfying xi < xj . Hence, clauses eliminatedconsidered partial assignment variable xi pure irrelevant formula[ln , . . . , li+1 ]. 2Proof Theorem 5.3. Assume, pure literal matched formula. Let L pureliteral sequence obtained FindPLS procedure let = [L], is,391fiSavicky & Kuceraassumption, matched formula. Since L maximal, contain pure literal.empty, pure literal satisfiable formula find splitting variablemethod Lemma 4.6. empty, matched pureliteral satisfiable. Hence, Theorem 5.9, variable x , [x = 0][x = 1] matched. Since L contain literal variable x,application assign(L) x = commute {0, 1}. Hence, L pure literalsequence formula [x = a] application assign(L) [x = a] leads[x = a], matched. Hence, {0, 1}, formula [x = a] pure literalmatched variable x splitting variable formula .time polynomial length formula sufficient select splitting variablex proof above. nonempty, satisfiability [x = 0] [x = 1]guaranteed choice x. empty, pure literal satisfiable methodLemma 4.6 used. Hence, splitting variable results required satisfiabilitytests obtained polynomial time. 2Similarly class matched formulas, also class pure literal matched formulasclosed unit propagation. implies unit propagation used partconstruction splitting tree, particular Remark 3.4 always selectvariable unit clause splitting variable.Proposition 5.10. class pure literal matched formulas closed unit propagation.Proof. Assume, pure literal matched formula containing unit clause C = (l)l literal. Let us prove [l] pure literal matched formula.Let L pure literal sequence . Observe l cannot contained L,[l] unsatisfiable. rest proof, distinguish, whether l contained Lnot.l contained L, let L1 denote sequence literals L l let L2sequence literals L l. simplicity, written L = (L1 , l, L2 ).clauses missing [l] changed removing l. Since lcontained L1 , sequence L1 pure literal sequence [l]. Since assignmentsdisjoint sets variables commute, [L1 , l] = [l, L1 ] and, hence, sequenceL2 pure literal sequence formulas. Hence, sequence L = (L1 , L2 )pure literal sequence [l]. Since, moreover, [L1 , l, L2 ] = [l, L1 , L2 ], applicationL [l] leads matched formula. Consequently, [l] pure literal matched.Let us consider case l contained L. case [L] matchedformula contains unit clause C = (l), since clause cannot eliminatedsatisfying literals L. every maximum matching [L], clause C matchedl. Thus satisfying l gives matched formula [L, l]. Since [L, l] = [l, L] Lpure literal sequence [l], formula pure literal matched. 26. Algorithms Complexitysection, prove specific complexity bounds algorithms presentedprevious sections. complexity bounds derived RAM model unit cost392fiGenerating Models Matched Formulameasure word size O(log kk), input formula. data structuresused algorithms similar described Minoux (1988) MurakamiUno (2014). Let us first concentrate pure literal satisfiable formulas.Lemma 6.1. maximal pure literal sequence L CNF formula constructedtime O(kk).Proof. use approach presented linear time algorithm unit propagationMinoux (1988) obtain efficient version procedure FindPLS Algorithm 1.addition initializations Algorithm 1, initialize auxiliary data structures.data structures similar described Murakami Uno (2014).particular, occurences literals formula represented nodes arrangedsparse matrix, whose rows correspond literals columns correspond clauses.node contains identification clause literal, whose occurence represents.auxiliary data structures names follows:literal l denote cl(l) row matrix, doubly-linked listnodes representing occurences l .clause C denote lit(C) column matrix, doublylinked list nodes corresponding occurences literals C.literal l denote cnt(l) counter, contains size list lit(C)number clauses l appears.initialize set Pure() queue always contains pure literals .literals l cnt(l) > 0 cnt(l) = 0.data structures initialized traversing linear time. importantnote node represents occurence literal l clause C.structure representing node contains four pointers, two doubly-linked list lit(C)two double-linked list cl(l). Thus removing node listsperformed constant time.procedure FindPLS repeat following steps find pure literal l , add lL apply assign(l) . Finding pure literal amounts dequeueing Pure().applying assign(l) remove clauses containing l (these satisfied)remove l remaining clauses. Let 1 consist clauses contain llet 0 consist clauses contain l. claim assign(l) appliedtime O(k1 k + |0 |).1. Removing clauses 1 means going list cl(l) clause Clist literal l lit(C) (including l), remove corresponding nodecl(l ) make list lit(C) inaccessible. requires time O(1) literal l .operation also decrement counters cnt(l ) literals lit(C)negated counterparts becomes pure, add queue Pure().2. Removing occurrences l means going list cl(l) clauseC list, remove corresponding node cl(l) lit(C).done time O(1) occurrence l.393fiSavicky & KuceraRepeating steps literals included L requires constant numberoperations occurrence literal input formula implies totaltime O(kk). 2Theorem 6.2. splitting problem relative pure literal satisfiable formulassolved time O(kk) input pure literal satisfiable formula. Moreover,set () models pure literal satisfiable formula generated delayO(kk).Proof. Using efficient version FindPLS guaranteed Lemma 6.1, operationsused proof Lemma 4.6 done time O(kk). implies first statementtheorem. procedure used preprocessing step algorithmproving second statement. time O(kk), preprocessing produces pure literalsequence L = (l1 , . . . , ln ), contains literal variable . auxiliary datastructures cl(l), lit(C) cnt(l) used preprocessing used also later,stored reconstructed needed.construction L, assumption Lemma 4.5 satisfied L. methodLemma 4.5 used find splitting variable formula,corresponding restrictions either contains empty clause also satisfies assumptionLemma 4.5 L. Hence, sequence L used selecting splitting variablenodes splitting tree .DFS search controlled stack postponed nodes, initializedroot search starts. search split sequence descending branches.descending branches starts removing node stack resumingsearch node. visited node two satisfiable successors, DFS continuesone put onto stack. node single satisfiable successor,stack modified. descending branch ends 1-leaf found.estimate delay, estimate total time needed construct nodes onedescending branches follows.indices L splitting variables chosen descending branch monotonically decreasing. Hence, total time needed search splitting variablesone descending branch O(n) and, hence, O(kk).time needed manipulations formula one descending branchfollows. node removed stack, auxiliary data structures cl(l), lit(C)cnt(l) computed original formula modified accordingsequence settings variables along path root current node.done time O(kk). Then, node descending branch, assignmentschosen variable computed satisfiable successor selected. one node,done time O(k), k number occurrences chosen variable. satisfiable successor selected, auxiliary structures updated accordingit. total time needed operations one descending branch O(kk) usingsimilar argument proof Corollary 6.1.combining estimates, total time constructing descending branchand, hence, delay generating two consecutive models, O(kk). 2394fiGenerating Models Matched Formulalet us concentrate time complexity selecting splitting variable pureliteral matched formula.Lemma 6.3. splitting problem relative pure literal matched formulas solvedtime O(n kk) input formula n variables.Proof. Following proof Theorem 5.3, first find pure literal sequence Ldone time O(kk) Lemma 6.1. = [L] empty formula,last variable L splitting variable. Otherwise matched find maximummatching . step performed time O(kk n) (see Hopcroft & Karp,1973). Then, search variable x , [x = 0] [x = 1]matched. variable exists Theorem 5.9. number clauses lessn, variable used matching property. Otherwise, checkevery variable, whether [x = a] matched {0, 1}. assignment x = satisfiesmatched literal containing x, [x = a] matched. rest proof,estimate complexity n checks assignments falsifyingmatched literal.Partial assignment performed time O(kk) = O(kk). partial assignment satisfied clauses removed occurences variable x removedremaining clauses. modify matching matching N [x = a] accordingly, remove pairs containing satisfied clause pair containing x.|N | = (where number clauses [x = a]), done. Otherwise know|N | = 1, since one pair containing clause [x = a], specifically,pair containing literal x, removed forming N . remains checkwhether N already maximum matching whether better matching.tested looking single augmentating path I([x = a]) matching N .augmentating path found using breadth first search time linear sizegraph I([x = a]) (see e.g. Hopcroft & Karp, 1973; Lovasz & Plummer, 1986). Hence,test, whether [x = a] matched done time O(kk) = O(kk). 2corollary Lemma 6.3 general bound Theorem 3.3, getfollowing.Corollary 6.4. Models pure literal matched formula n variables generateddelay O(n2 kk).Proof. Lemma 6.3 find splitting variable pure literal matched formulatime O(n kk), time determine satisfiability formulas [x = 0][x = 1] well. Theorem 3.3 thus get delay O(n2 kk). 27. Linearly Satisfiable Formulassection consider class linearly satisfiable formulas. results Kullmann(2000), class generalizes matched formulas pure literal satisfiableformulas and, combining proofs, also class pure literal matched formulas.section, show possible generate models linearly satisfiable formulaspolynomial delay unless P=NP.395fiSavicky & Kuceraconsequence, splitting problem relative linearly satisfiable formulaspolynomial complexity unless P=NP. consequence follows also unconditionallyExample 7.10, presents linearly satisfiable formula 4 variables,splitting variable respect class linearly satisfiable formulas.Let us recall notation introduced Kullmann, used presentdefinition basic facts concerning linearly satisfiable formulas. l literal,var(l) variable literal. v partial assignment, v(l) valueassignment literal l.Definition 7.1 (Kullmann, 2000). Let CNF formula let v non-emptypartial assignment variables . say v simple linear autarky,associated weight function w assigns variable x evaluated v positivereal number w(x) clauses CXXw(var(l))w(var(l)) .(2)lC,v(l)=1lC,v(l)=0Clearly, literal C falsified v, must literal satisfied vwell. Therefore simple linear autarky autarky. Kullmann showed checkwhether simple linear autarky v CNF formula find one, exists,solving several linear programs.literal l pure formula, partial assignment v(l) = 1 weightw(var(l)) = 1 simple linear autarky formula. another example, considersatisfying assignment satisfiable 2-CNFs. assignment weightvariables forms simple linear autarky. Similarly, pure Horn CNFs without unitclauses satisfiable simple linear autarky assigns value 0 equal weightvariables. hand, pure Horn CNF formula contains unit clause,satisfiable simple linear autarky. example formula(x1 ) (x1 x2 ) (x1 x3 ) (x1 x2 x3 ) ,simple linear autarky Theorem 7.8 Lemma 7.9 below.considering iterative application simple linear autarkies formula getclass linearly satisfiable formulas defined follows.Definition 7.2 (Kullmann, 2000). class linearly satisfiable formulas definedsmallest class satisfying following two properties:1. empty CNF linearly satisfiable.2. Let CNF, simple linear autarky v [v] linearlysatisfiable. .words, CNF formula linearly satisfiable subsequent applicationslinear autarkies obtain empty formula. composition simple linear autarkiescalled linear autarky Kullmann (2000) class linearly satisfiable formulastherefore consists formulas satisfiable linear autarky. Kullmann showedmatched formulas linearly satisfiable. Since pure literal simple linear396fiGenerating Models Matched Formulaautarky, pure literal satisfiable formula linearly satisfiable. Similarly, pure literalmatched formula defined Section 5 linearly satisfiable simple linear autarkiespure literals concatenated linear autarky resulting matched formula.matched pure literal satisfiable formulas presented algorithmsgenerate models formulas polynomial delay, possible extendresult linearly satisfiable formulas unless P=NP. Let us first present construction,used reduction argument.Let arbitrary 3-CNF formula variables x1 , . . . , xn clauses c1 , . . . , cm .Consider new variables y1 , y2 , y3 let formula consisting clauses(y1 y2 ), (y2 y3 ), (y3 y1 ),(cj y1 y2 y3 ),j = 1, . . . ,(xi y1 )= 1, . . . , n .Recall number models formula number satisfying assignmentsvariables, occurrence it. Hence, next lemma, () ()defined different sets variables.Lemma 7.3. Formula linearly satisfiable number models |T ()| =|T ()| + 1.Proof. clause ci contains three literals. Hence, clause , numberpositive literals least number negative literals. followsassignment variables 1 equal weight variables defines simple linearautarky, satisfies . Hence, formula linearly satisfiable.model satisfies y1 = y2 = y3 . assignment containing y1 = y2 = y3 = 1model xi = 1 = 1, . . . , n. assignment containing y1 = y2 = y3 = 0model assignment variables xi model . impliessecond part statement lemma. 2Since formula constructed every 3-CNF formula , lemma impliesfollowing immediate corollary.Corollary 7.4. NP-complete problem determine, whether general linearlysatisfiable formula least 2 models.Note implies NP-hardness #SAT problem restricted linearly satisfiable formulas. problem is, fact, also #P-complete, since #P-completecount models monotone formulas, pure literal satisfiable and, hence, linearlysatisfiable.Example 7.10 below, present linearly satisfiable formula, splittingvariable relative class linearly satisfiable formulas. analysis example,use characterization simple linear autarkies obtained using clause-variable matrix.Definition 7.5. Let CNF formula clauses c1 , . . . , cm variables x1 , . . . , xn .clause-variable matrix formula matrix = {aj,i } dimension ndefined1 xi cj1 xi cjaj,i =0 otherwise .397fiSavicky & Kucerau Rm , u 0 means uj 0 j = 1, . . . , m. Kullmann showed followingproposition.Lemma 7.6 (Kullmann, 2000). formula clause-variable matrix simplelinear autarky, nonzero z Rn , Az 0. Moreover,linear autarky obtained vector z using assignment1 zi > 0v(xi ) =0 zi < 0zi = 0weight function w(xi ) = |zi |.Let us present well-known Farkas lemma form used proof Theorem7.8.Theorem 7.7 (Farkas lemma). Let n real matrix b Rn . Then, exactlyone following statements true.1. vector Rm , 0 = bt .2. vector z Rn , Az 0 bt z < 0.linear combination real vectors non-negative coefficients called,simplicity, non-negative combination.Theorem 7.8. Assume, formula n variables clauses clausevariable matrix dimension n. Then, exactly one following statementstrue:(a) linear autarky,(b) every vector Rn non-negative combination rows A.Proof. First, assume, (a) (b) satisfied. Lemma 7.6 impliesnon-zero z Rn , Az 0. (b), non-negative vector Rm ,= z . Multiplying z right, getAz = z z < 0 .contradiction, since Az non-negative.Assume, (b) satisfied. Hence, vector b Rn , non-negativecombination rows A. Farkas lemma, vector z Rn , Az 0bt z < 0. Since latter condition implies z non-zero, simple linearautarky Lemma 7.6 means (a) satisfied. 2Lemma 7.9. Assume, matrix dimension n, rank(A) = nvector u Rm components positive, ut = 0. Then, every vectorRn non-negative combination rows A.398fiGenerating Models Matched FormulaProof. assumption, linear space generated rows Rn . Hence,every z Rn , v Rm , v = z. sufficiently large real number s,vector v + su components non-negative (v + su)t = z. 2Note every linearly satisfiable CNF formula 3 variables splittingvariable relative class linearly satisfiable CNF formulas, since setting variableconstant leads formula 2 variables, quadratic and,hence, satisfiable linearly satisfiable.Example 7.10. Denote E = {a {0, 1}4 | 2 a1 + a2 + a3 + a4 3} every Booleanvariable x, let x1 = x x0 = x. formula(x1 , x2 , x3 , x4 ) =4^ _xaiaE i=1linearly satisfiable, splitting variable relative class linearly satisfiableformulas.Proof. every clause, number positive literals least number negativeliterals. Hence, formula linearly satisfiable Lemma 7.6 z = (1, 1, 1, 1).Since invariant permutation variables, sufficient provex4 splitting variable. Since every clause contains negative literal,(0, 0, 0, 0) = 1. follows formula [x4 = 0] satisfiable. One verify[x4 = 0] =3^ _xai ,aE i=1E = {a {0, 1}3 | 1 a1 + a2 + a3 2}. order prove [x4 = 0]linearly satisfiable, consider clause-variable matrix columns correspondingx1 , x2 , x3 ,11 11 111111 1 1 .11 11 11matrix rank 3, since vectors (2, 0, 0), (0, 2, 0), (0, 0, 2) sum tworows first three. Moreover, sum rows matrix zerovector. Hence, formula [x4 = 0] linear autarky Lemma 7.9Theorem 7.8. 28. Conclusion Directions Researchpaper, shown possible generate models matched formulan variables delay O(n2 kk). byproduct shown models399fiSavicky & Kucerapure literal satisfiable formula (i.e. formula satisfiable iterated pure literalelimination) generated delay O(kk). also shown resultcannot generalized class linearly satisfiable formulas since possiblegenerate models linearly satisfiable formulas polynomial delay unless P=NP.Let us mention procedure generating models bounded delayextended formulas small strong backdoor set respect classmatched formulas empty clause detection found. Let us assume Bbackdoor set formula , i.e. B set variables satisfying partialassignment variables B leads matched formula, formula containingempty clause. generate decision tree (and thus generate models)time O(2|B| kk + (f ) n2 kk). Unfortunately, searching strong backdoor setsrespect class matched formulas hard (Szeider, 2007).algorithms described paper cases pure literal satisfiable pureliteral matched formulas used general algorithm model enumerationbased splitting tree. This, turn, DPLL based enumeration algorithm.end, similar approach one described Stefan Szeider (2003) used.Together formula would keep maximum matching I(). maximummatching maintained reduction assignment steps performedenumeration algorithm. algorithm arrives matched formula,select splitting variables way described paper guaranteedpolynomial delay.interesting question whether approach could used parameterizedsatisfiability algorithm based maximum deficiency (see Szeider, 2003) order getparameterized algorithm generating models general formula.AcknowledgmentsPetr Savicky supported CE-ITI GACR grant number GBP202/12/G061institutional research plan RVO:67985807. Petr Kucera supportedCzech Science Foundation (grant GA15-15511S).ReferencesAceto, L., Monica, D., Ingolfsdottir, A., Montanari, A., & Sciavicco, G. (2013). LogicProgramming, Artificial Intelligence, Reasoning: 19th International Conference,LPAR-19, Stellenbosch, South Africa, December 14-19, 2013. Proceedings, chap.Algorithm Enumerating Maximal Models Horn Theories ApplicationModal Logics, pp. 117. Springer Berlin Heidelberg, Berlin, Heidelberg.Aharoni, R., & Linial, N. (1986). Minimal non-two-colorable hypergraphs minimalunsatisfiable formulas. Journal Combinatorial Theory, Series A, 43 (2), 196 204.Bollobas, B. (1998). Modern Graph Theory, Vol. 184 Graduate Texts Mathematics.Springer.400fiGenerating Models Matched FormulaCoquery, E., Jabbour, S., Sais, L., Salhi, Y., et al. (2012). SAT-based approachdiscovering frequent, closed maximal patterns sequence. ProceedingsECAI.Creignou, N., & Hebrard, J.-J. (1997). generating solutions generalized satisfiabilityproblems. Informatique theorique et applications, 31 (6), 499511.Creignou, N., Olive, F., & Schmidt, J. (2011). Theory Applications SatisfiabilityTesting - SAT 2011: 14th International Conference, SAT 2011, Ann Arbor, MI, USA,June 19-22, 2011. Proceedings, chap. Enumerating Solutions Boolean CSPNon-decreasing Weight, pp. 120133. Springer Berlin Heidelberg, Berlin, Heidelberg.Dechter, R., & Itai, A. (1992). Finding solutions find one. AAAI-92Workshop Tractable Reasoning, pp. 3539.Fleischner, H., Kullmann, O., & Szeider, S. (2002). Polynomial-time recognition minimal unsatisfiable formulas fixed clause-variable difference. Theoretical ComputerScience, 289 (1), 503 516.Flum, J., & Grohe, M. (2006). Parameterized complexity theory (1st edition)., Vol. 3Texts Theoretical Computer Science. EATCS Series. Springer-Verlag BerlinHeidelberg.Franco, J., & Van Gelder, A. (2003). perspective certain polynomial-time solvableclasses satisfiability. Discrete Appl. Math., 125 (2-3), 177214.Garey, M., & Johnson, D. (1979). Computers Intractability: Guide TheoryNP-Completeness. W.H. Freeman Company, San Francisco.Genesereth, M., & Nilsson, N. (1987). Logical Foundations Artificial Intelligence. MorganKaufmann, Los Altos, CA.Hopcroft, J. E., & Karp, R. M. (1973). n5/2 algorithm maximum matchingsbipartite graphs. SIAM Journal computing, 2 (4), 225231.Jabbour, S., Lonlac, J., Sais, L., & Salhi, Y. (2014). Extending modern sat solversmodels enumeration. IEEE 15th International Conference Information ReuseIntegration (IRI), 2014, pp. 803810. IEEE.Johnson, D. S., Yannakakis, M., & Papadimitriou, C. H. (1988). generating maximalindependent sets. Information Processing Letters, 27 (3), 119 123.Kang, H.-J., & Park, I.-C. (2005). Sat-based unbounded symbolic model checking.Computer-Aided Design Integrated Circuits Systems, IEEE Transactions on,24 (2), 129140.Kavvadias, D. J., Sideri, M., & Stavropoulos, E. C. (2000). Generating maximal modelsBoolean expression. Information Processing Letters, 74 (34), 157162.Khuller, S., & Vazirani, V. V. (1991). Planar graph coloring self-reducible, assumingP 6= N P . Theoretical Computer Science, 88 (1), 183 189.Kullmann, O. (2000). Investigations autark assignments. Discrete Applied Mathematics,107 (13), 99 137.401fiSavicky & KuceraKullmann, O. (2003). Lean clause-sets: generalizations minimally unsatisfiable clausesets. Discrete Applied Mathematics, 130 (2), 209 249. Renesse Issue Satisfiability.Lovasz, L., & Plummer, M. D. (1986). Matching Theory. North-Holland.McMillan, K. L. (2002). Computer Aided Verification: 14th International Conference,CAV 2002 Copenhagen, Denmark, July 2731, 2002 Proceedings, chap. Applying SATMethods Unbounded Symbolic Model Checking, pp. 250264. Springer Berlin Heidelberg, Berlin, Heidelberg.Minoux, M. (1988). LTUR: simplified linear time unit resolution algorithm Hornformulae computer implementation. Information Processing Letters, 29, 1 12.Morgado, A., & Marques-Silva, J. (2005a). Algorithms propositional model enumeration counting. Tech. rep., Instituto de Engenharia de Sistemas e Computadores,Investigacao e Desenvolvimento, Lisboa.Morgado, A., & Marques-Silva, J. (2005b). Good learning implicit model enumeration. Tools Artificial Intelligence, 2005. ICTAI 05. 17th IEEE InternationalConference on, pp. 6 pp.136.Murakami, K., & Uno, T. (2014). Efficient algorithms dualizing large-scale hypergraphs.Discrete Applied Mathematics, 170, 8394.Sipser, M. (2006). Introduction Theory Computation, Vol. 2. Thomson CourseTechnology Boston.Szeider, S. (2003). Minimal unsatisfiable formulas bounded clause-variable differencefixed-parameter tractable. Warnow, T., & Zhu, B. (Eds.), ComputingCombinatorics, Vol. 2697 Lecture Notes Computer Science, pp. 548558. SpringerBerlin Heidelberg.Szeider, S. (2005). Generalizations matched CNF formulas. Annals MathematicsArtificial Intelligence, 43 (1-4), 223238.Szeider, S. (2007). Matched formulas backdoor sets. Marques-Silva, J., & Sakallah,K. A. (Eds.), Theory Applications Satisfiability Testing SAT 2007, Vol. 4501Lecture Notes Computer Science, pp. 9499. Springer Berlin Heidelberg.Tovey, C. A. (1984). simplified NP-complete satisfiability problem. Discrete AppliedMathematics, 8 (1), 85 89.Valiant, L. (1979a). complexity computing permanent. Theoretical ComputerScience, 8 (2), 189 201.Valiant, L. (1979b). complexity enumeration reliability problems. SIAM JournalComputing, 8 (3), 410421.402fiJournal Artificial Intelligence Research 56 (2016) 247-268Submitted 12/15; published 06/16Association Discovery Diagnosis Alzheimers DiseaseBayesian Multiview LearningZenglin Xuzlxu@uestc.edu.cnBig Data Research CenterSchool Computer Science & EngineeringUniversity Electronic Science & Technology ChinaChengdu, Sichuan, 611731 ChinaShandian Zheszhe@purdue.eduDepartment Computer Science, Purdue UniversityWest Lafayette, 47906 USAYuan(Alan) Qialanqi@cs.purdue.eduDepartment Computer Science & Department StatisticsPurdue UniversityWest Lafayette, 47906 USAPeng Yuyu peng py@lilly.comEli Lilly Company, Indianapolis, 46225, USAAbstractanalysis diagnosis Alzheimers disease (AD) based genetic variations, e.g., single nucleotide polymorphisms (SNPs) phenotypic traits, e.g., MagneticResonance Imaging (MRI) features. consider two important related tasks: i)select genetic phenotypical markers AD diagnosis ii) identify associationsgenetic phenotypical data. previous studies treat two tasks separately, tightly coupled underlying associations genetic variationsphenotypical features contain biological basis disease. present newsparse Bayesian approach joint association study disease diagnosis. approach, common latent features extracted different data sources based sparseprojection matrices used predict multiple disease severity levels; return,disease status guide discovery relationships data sources. sparseprojection matrices reveal interactions data sources also select groupsbiomarkers related disease. Moreover, take advantage linkage disequilibrium (LD) measuring non-random association alleles, incorporate graphLaplacian type prior model. learn model data, develop efficientvariational inference algorithm. Analysis imaging genetics dataset studyAlzheimers Disease (AD) indicates model identifies biologically meaningful associations genetic variations MRI features, achieves significantly higheraccuracy predicting ordinal AD stages competing methods.1. IntroductionAlzheimers disease (AD) common neurodegenerative disorder (Khachaturian,1985). order predict onset progression AD, NIH funded AlzheimersDisease Neuroimaging Initiative (ADNI) facilitate evaluation genetic variations,e.g., Single Nucleotide Polymorphisms (SNPs) phenotypical traits, e.g., Magnetic Resoc2016AI Access Foundation. rights reserved.fiXu, Zhe, Qi, & Yunance Imaging (MRI). addition progression study, becoming important medicalstudies identify relevant pathological genotypes phenotypic traits, discoverassociations. Although found many bioinformatics applications (Consoli, Lefevre,Zivy, de Vienne, & Damerval, 2002; Hunter, 2012; Gandhi & Wood, 2010; Liu, Pearlson,Windemuth, Ruano, Perrone-Bizzozero, & Calhoun, 2009), association studies scarceespecially need AD study.Many statistical approaches developed discover associations select features (or variables) prediction high dimensional problem. association studies, representative approaches canonical correlation analysis (CCA) extensions (Harold, 1936; Bach & Jordan, 2005). approaches widely usedexpression quantitative trait locus (eQTL) analysis (Parkhomenko, Tritchler, & Beyene,2007; Daniela & Tibshirani, 2009; Chen, Liu, & Carbonell, 2012). disease diagnosis based high dimensional biomarkers, popular approaches include lasso (Tibshirani,1994), elastic net (Zou & Hastie, 2005), group lasso (Yuan & Lin, 2007), Bayesianautomatic relevance determination (MacKay, 1991; Neal, 1996). Despite wide successmany applications, approaches limited following reasons:association studies neglect supervision disease status.many diseases, AD, direct result genetic variations often highlycorrelated clinical traits, disease status provides useful yet currently unutilizedinformation finding relationships genetic variations clinical traits.disease diagnosis, sparse approaches use classification modelsconsider order disease severity. subjects AD studies, naturalseverity order normal mild cognitive impairment (MCI)MCI AD. Classification models cannot capture order ADs severity levels.previous methods designed handle heterogeneous data types.SNPs values discrete (and ordinal based additive genetic model),imaging features continuous. Popular CCA lasso-type methods simply treatcontinuous data overlook heterogeneous nature data.previous methods ignore cannot utilize valuable prior knowledge.example, occurrence combinations alleles genetic markers population often less often would expected randomformation haplotypes alleles based frequencies, knownLinkage Disequilibrium (LD) (Falconer & Mackay, 1996). knowledge,structure utilized association discovery.address problems, propose new Bayesian approach unifies multiviewlearning sparse ordinal regression joint association study disease diagnosis.also conduct nonlinear classification latent variables (Zhe, Xu, Qi, & Yu, 2014)find associations incorporating LD information additional prior SNPsdata (Zhe, Xu, Qi, & Yu, 2015) . detail, genetic variations phenotypical traitsgenerated common latent features based separate sparse projection matricessuitable link functions, common latent features used predict disease status(See Section 2). enforce sparsity projection matrices, assign spike slab priors248fiSimultaneous Association Discovery Diagnosis(George & McCulloch, 1997) them; priors shown effectivel1 penalty learn sparse projection matrices (Goodfellow, Couville, & Bengio, 2012;Mohamed et al., 2012). order take advantage linkage disequilibrium,describes non-random association alleles different loci, employ additionalgraph Laplacian type prior SNPs view. sparse projection matricesreveal critical interactions different data sources also identify biomarkersdata relevant disease status. Meanwhile, via direct connection latent features,disease status influences estimation projection matrices guidediscovery associations heterogeneous data sources relevant disease.learn model data, develop variational inference approach (See Section3). iteratively minimizes Kullback-Leibler divergence tractable approximation exact Bayesian posterior distributions. extend proposed sparse multiviewlearning model incorporating linkage disequilibrium information SNPs Section 4. employ model real study AD Section 5. results showmodel achieves highest prediction accuracy among competing methods. Furthermore, model finds biologically meaningful predictive relationshipsSNPs, MRI features, AD status.2. Sparse Heterogeneous Multiview Learning Modelssection, first present notations assumptions, present sparseheterogeneous multiview learning model.2.1 Notations AssumptionsFirst, let us describe data. assume two heterogeneous data sources: onecontains continuous data example, MRI features contains discreteordinal data instance, SNPs. Note easily generalize modelhandle views data types adopting suitable link functions (e.g., Poissonmodel count data). Given data n subjects, p continuous features q discretefeatures, denote continuous data p n matrix X = [x1 , . . . , xn ], discreteordinal data q n matrix Z = [z1 , . . . , zn ] labels (i.e., disease status)n 1 vector = [y1 , . . . , yn ]> . AD study, let yi = 0, 1, 2 i-th subjectnormal, MCI AD condition, respectively.2.2 Spare Heterogeneous Multiview Learning Modellink two data sources X Z together, introduce common latent featuresU = [u1 , . . . , un ] assume X Z generated U sparse projection.common latent feature assumption sensible association studies SNPsMRI features biological measurements subjects. Note ui latentfeature i-th subject dimension k. denote proposed Spare HeterogeneousMultiview LearningQ Model SHML. Bayesian framework, assign Gaussian priorU, p(U) = N (ui |0, I), specify rest model (see Figure 1) follows.249fiXu, Zhe, Qi, & YuShHwSwUGSgXZFigure 1: graphical representation SHML, X continuous view, Z ordinalview, labels.2.2.1 Continuous Data DistributionGiven U, X generatedp(X|U, G, ) =nN (xi |Gui , 1 I)i=1G = [g1 , g2 , ...gp ]> p k projection matrix, identity matrix, 1precision matrix Gaussian distribution. precision parameter ,assign conjugate prior Gamma prior, p(|r1 , r2 ) = Gamma(|r1 , r2 ) r1 r2hyperparameters set 103 experiments.2.2.2 Ordinal Data Distributionordinal variable z {0, 1, . . . , R1}, value decided region auxiliaryvariable c falls= b0 < b1 < . . . < bR = .c falls [br , br+1 ), z set r. AD study, SNPs Z take values {0, 1, 2}therefore R = 3. Given q k projection matrix H = [h1 , h2 , ...hq ]> , auxiliaryvariables C = {cij } ordinal data Z generatedp(Z, C|U, H) =qnp(cij |hi , uj )p(zij |cij )i=1 j=1p(cij |hi , uj ) = N (cij |h>uj , 1)p(zij |cij ) =2X(zij = r)(br cij < br+1 ).r=0(a) = 1 true (a) = 0 otherwise.250fiSimultaneous Association Discovery Diagnosis2.2.3 Label Distributiondisease status labels ordinal variables too. generate y, use ordinalregression model based latent representation U,p(y, f |U, w) = p(y|f )p(f |U, w),f latent continuous values corresponding y, w weight vectorlatent featuresp(fi |ui , w) = N (fi |u>w, 1),p(yi |fi ) =2X(yi = r)(br fi < br+1 ).r=0Note labels linked data X Z via latent features Uprojection matrices H G. Due sparsity H G, groupsvariables X Z selected predict y.2.2.4 Sparse Priors Projection Matrices Weights Vectorwant identify critical interactions different data sources,use spike slab prior (George & McCulloch, 1997) sparsify projection matrices GH. spike slab priors continuous bimodal priors model hypervariance parameters, controls selection variable effective scale choosingvariable. apply spike slab prior weight vector w. Specifically,use p k matrix Sg represent selection elements G: sgij = 1, gij selectedfollows Gaussian prior distribution variance 12 ; sgij = 0, gij selectedforced almost zero (i.e., sampled Gaussian small variance 22 ).following prior G:p(G|Sg , g ) =pkij ijp(gij |sijg )p(sg |g )i=1 j=1ij2ij2p(gij |sijg ) = sg N (gij |0, 1 ) + (1 sg )N (gij |0, 2 ),ijijp(sijg |g ) = gsijgij(1 gij )1sg ,22gij g probability sijg = 1, 1 2 (in experiment, set2261 = 1 2 = 1o ). reflect uncertainty g , assign Beta hyperpriordistribution:pkp(g |l1 , l2 ) =Beta(gij |l1 , l2 ),i=1 j=1l1 l2 hyperparameters. set diffuse non-informative hyperprior, i.e.,l1 = l2 = 1 experiments. Similarly, H sampledp(H|Sh , h ) =qki=1 j=1251ij ijp(hij |sijh )p(sh |h ),fiXu, Zhe, Qi, & Yusijijijijij ijij h22p(hij |sij(1hij )1sh .h ) = sh N (hij |0, 1 )+(1sh )N (hij |0, 2 ) p(sh |h ) = hijijSh binary selection variables h h probability sh = 1. assign Betahyperpriors h :qkp(h |d1 , d2 ) =Beta(hij |d1 , d2 ),i=1 j=1d1 d2 hyperparameters. set d1 = d2 = 1 experiments sincefound sensitive final performance. Similarly weights vector w,p(w|sw , w ) =kjp(wj |sjw )p(sjw |w)j=1sjwjjjj 1swp(wj |sjw ) = sjw N (wj |0, 12 ) + (1 sjw )N (wj |0, 22 ) p(sjw |w) = w(1 w).jjsw binary selection variables w w probability sw = 1. assignBeta hyperpriors w :p( w ) =kBeta(w|e1 , e2 ),i=1e1 e2 hyperparameters. similarly set e1 = e2 = 1 experiments.2.2.5 Joint DistributionBased specifications, joint distribution modelp(X, Z, y, U, G, Sg , g , , C, H, H, Sh , h , Sw , w , f )= p(X|U, G, )p(G|Sg )p(Sg |g )p(g |l1 , l2 )p(|r1 , r2 )p(Z, C|U, H)p(H|Sh )p(Sh |h )p(h |d1 , d2 )p(y|f )p(f |U, w)p(w|Sw )p(Sw |w )p(U).(1)Different Figure 1, put conjugate prior Sw Sg joint distribution. next step estimate distributions latent variableshyperparemeters.3. Model InferenceGiven model specified previous section, present efficient methodestimate latent features U, projection matrices H G, selection indicators SgSh , selection probabilities g h , variance , auxiliary variables Cgenerating ordinal data Z, auxiliary variables f generating labels y, weightsvector w generating f corresponding selection indicators probabilities sww . Bayesian framework, estimation task amounts computing posteriordistributions.However, computing exact posteriors turns infeasible since cannotcalculate normalization constant posteriors based Equation (1). Thus,252fiSimultaneous Association Discovery Diagnosisresort mean-field variational approach. Specifically, approximate posteriordistributions U, H, G, Sg , Sh , g , h , , w, C f factorized distributionQ() = Q(U)Q(H)Q(G)Q(Sg )Q(Sh )Q(g )Q(h )Q()Q(w)Q(C)Q(f )(2)denotes latent variables.Variational inference minimizes Kullback-Leibler (KL) divergence approximate exact posteriorsmin KL (Q()kp(|X, Z, y))(3)Q()specifically, using coordinate descent algorithm, variational approach updatesone approximate distribution, e.g, q(H), Equation (2) timeothers fixed. detailed updates given following paragraphs.3.1 Updating Variational Distributions Continuous Datacontinuous data X, approximate distributions projection matrix G,noise variance , selection indicators Sg selection probabilities gQ(G) =Q(Sg ) =pN (gi ; , ),i=1pksij(4)ijijg (1 ij )1sg ,(5)Beta(gij |l1ij , l2ij ),(6)i=1 j=1Q(g ) =pki=1 j=1Q() = Gamma(|r1 , r2 ).(7)mean covariance gi calculated follows:= hihUU> +111diag(hsig i) + 2 diag(1 hsig i) ,212= (hihUixi ),hi means expectation distribution, xi sig transpose i-th2 j-th diagonal element .rows X Sg , hsig = [i1 , . . . , ik ]> , hgijcomputation parameters ij Q(gij ) found Appendices A.3.2 Updating Variational Distributions Ordinal Dataordinal data Z, update approximate distributions projection matrix H,auxiliary variables C, sparse selection indicators Sh selection probabilitiesh . make variational distributions tractable, update Q(H) column-wise253fiXu, Zhe, Qi, & Yuway re-denote H = [h1 , h2 , ...hk ], Sh = [s1h , s2h , ...skh ] U = [u1 , u2 , ...uk ]> .variational distributions C HQ(C) =qkQ(cij ),(8)i=1 j=1Q(cij ) (bzij cij < bzij +1 )N (cij |cij , 1),Q(H) =kN (hi ; , ),(9)(10)i=11cij = (hHihUi)ij , = hui > ui iI+ 12 diag(hsih i)+ 12 diag(h1sih i) , = Ci hui12PCi = C j6=i j huj i> . computation parameters distributions Shh given Appendices B.3.3 Updating Variational Distributions Labelsordinal labels y, update approximation distributions auxiliary variablesf , weights vector w, sparse selection indicators sw selection probabilitiesw . variational distributions f wQ(f ) =nQ(fi ),(11)i=1Q(fi ) (byi fi < byi +1 )N (fi |fi , f2i ),(12)Q(w) = N (w; m, w ),(13)1= w hUihf i.fi = (hUi> m)i , w = hUU> i+ 12 diag(sw )+ 12 diag(1sw )12computation parameters variational distributions sw w foundAppendices C.3.4 Updating Variational Distributions Latent Representation Uvariational distribution U givenQ(U) =N (ui |i , )(14)1(15)= (hwihfi + hihGi xi + hHi hci i).(16)= hww> + hihG> Gi + hH> Hi +>>required moments given Appendices D.3.5 Label PredictionLet us denote training data Dtrain = {Xtrain , Ztrain , ytrain } test dataDtest = {Xtest , Ztest }. prediction task needs latent representation Utest Dtest .254fiSimultaneous Association Discovery Diagnosiscarry variational inference simultaneously Dtrain Dtest . Q(Utest )Q(Utrain ) obtained, predict labels test data follows:ftest = hUtest i> m,ytest=R1Xr (br ftest< br+1 ),(17)(18)r=0ytestprediction i-th test sample.4. Sparse Heterogeneous Multiview Learning Model LinkageDisequilibrium Priorspopulation genetics, lLinkage Disequilibrium (LD) refers non-random associationalleles different loci, i.e., presence statistical associations allelesdifferent loci different would expected alleles independently,randomly sampled based individual allele frequencies (Slatkin, 2008).linkage disequilibrium alleles different loci said linkageequilibrium.Linkage Disequilibrium also appears SNPs, measure pairsSNPs regarded natural indicator correlation SNPs.information publicly retrieved www.ncbi.nlm.nih.gov/books/NBK44495/.incorporate correlation prior model, first introduce latent q k matrixH, tightly linked H explained later. column hj H regularizedgraph Laplacian LD structure, i.e.,p(H|L) =N (hj |0, L1 )j=N (0|hj , L1 )j= p(0|H, L),L graph Laplacian matrix LD structure. shown above, priorp(H|L) form p(0|H, L), viewed generative modelwords, observation 0 sampled H. view enables us combinegenerative model graph Laplacian regularization sparse projection model viaprincipled hybrid Bayesian framework (Lasserre et al., 2006).link two models together, introduce prior H:p(H|H) =N (hj |hj , I)jvariance controls similar H H model. simplicity,set = 0 p(H|H) = Dirac(H H) Dirac(a) = 1 = 1 Dirac(a) = 0= 0.Adopting additional information, new graphical model designed shownFig. 2.255fiXu, Zhe, Qi, & YuLShwSwhjHUGSg0ZXFigure 2: graphical representation model, X continuous view, Zordinal view, labels L graph laplacian generated LD structure.Based specifications, joint distribution modelp(X, Z, y, U, G, Sg , g , , C, H, H, Sh , h , Sw , w , f )= p(X|U, G, )p(G|Sg )p(Sg |g )p(g |l1 , l2 )p(|r1 , r2 )p(Z, C|U, H)p(H|Sh )p(Sh |h )p(h |d1 , d2 )p(H|H)p(0|H, L)p(y|f )p(f |U, w)p(w|Sw )p(Sw |w )p(U).(19)inference almost original model described Section 2, exceptupdating sparse projection matrix H. Given ordinal data Z updatesvariables, update approximate distributions projection matrix H,auxiliary variables C, sparse selection indicators Sh selection probabilities h .variational distributions C HQ(C) =qkQ(cij ),(20)i=1 j=1Q(cij ) (bzij cij < bzij +1 )N (cij |cij , 1),Q(H) =kN (hi ; , ),(21)(22)i=11cij = (hHihUi)ij , = hui > ui iI + L + 12 diag(hsih i) + 12 diag(h1 sih i) , =12PCi hui Ci = C j6=i j huj i> . updating variables remains same.5. Experimental Results Discussionorder examine performance proposed method , design simulation studyrealworld study Alzheimers Disease.256fiSimultaneous Association Discovery Diagnosis5.1 Simulation Studyfirst design simulation study examine basic model, i.e., model, terms(i) estimation accuracy finding associations two views (ii) predictionaccuracy ordinal labels. Note similar study conducted modelLD priors.5.1.1 Simulation Datagenerate ground truth, set n = 200 (200 instances), p = q = 40, k = 5.designed G, 40 5 projection matrix continuous data X, block diagonalmatrix; column G 8 elements ones rest zeros,ensuring row one nonzero element. designed H, 40 5 projectionmatrix ordinal data Z, block diagonal matrix; first four columnsH 10 elements ones rest zeros, fifth columncontained zeros. randomly generated latent representations U Rkncolumn ui N (0, I). generate Z, first sampled auxiliary variables Ccolumn ci N (Hui , 1), Pdecided value element zij regioncij fell inin words, zij = 2r=0 r(br cij < br+1 ). Similarly, generate y,sampled auxiliary variables f N (0, U> U + I) yi generatedp(yi |fi ) = (yi = 0)(fi 0) + (yi = 1)(fi > 0).5.1.2 Comparative Methodscompared model several state-of-the-art methods including (1) CCA (Bach &Jordan, 2005), finds projection direction maximizes correlationtwo views, (2) sparse CCA (Sun, Ji, & Ye, 2011; Daniela & Tibshirani, 2009),sparse priors put CCA directions, (3) multiple-response regression lasso(MRLasso) (Kim, Sohn, & Xing, 2009) column second view (Z) regardedoutput first view (X). include results sparse probabilisticprojection approach (Archambeau & Bach, 2009) performed unstablyexperiments. Regarding software implementation, used built-in Matlab routineCCA code (Sun et al., 2011) sparse CCA. implemented MRLassobased Glmnet package (cran.r-project.org/web/packages/glmnet/index.html).test prediction accuracy, compared proposed SHML model basedGaussian process prior following ordinal multinomial regression methods: (1)lasso multinomial regression (Tibshirani, 1994), (2) elastic net multinomial regression(Zou & Hastie, 2005), (3) sparse ordinal regression spike slab prior, (4) CCA+ lasso, first ran CCA obtain latent features H applied lassopredict y, (5) CCA + elastic net, first ran CCA obtain projectionmatrices applied elastic net projected data, (6) Gaussian Process OrdinalRegression (GPOR) (Chu & Ghahramani, 2005), (7) Laplacian Support Vector Machine(LapSVM) (Melacci & Mikhail, 2011), semi-supervised SVM classification method.used published code lasso, elastic net, GPOR LapSVM. methods,used 10-fold cross validation training data run choose kernel form(Gaussian linear Polynomials) parameters (the kernel width polynomialorders) model, GPOR, LapSVM.257fiXu, Zhe, Qi, & Yualternative methods cannot learn dimension automatically simple comparison, provided dimension latent representation methods testedsimulations. partitioned data 10 subsets used 9 training1 subset testing; repeated procedure 10 times generate averaged testresults.5.1.3 Resultsestimate linkage (i.e., interactions) X Z, calculated cross covariancematrix GH> . computed precision recall based ground truth.precision-recall curves shown Figure 3. Clearly, method successfully recov1SHML0.90.8Precision0.7Sparse CCA0.6MRLasso0.50.40.3CCA0.20.1000.20.40.60.81RecallFigure 3: precision-recall curves association discovery.ered almost links significantly outperformed competing methods.improvement may come i) use spike slab priors, removeirrelevant elements projection matrices also avoid over-penalizing active association structures (the Laplace prior used sparse CCA penalize relevantones) ii) importantly, supervision labels y, probablybiggest difference methods association study. failingCCA sparse CCA may due insufficient representation sources datacaused using one projection direction. prediction accuracies unknownstandard errors shown Figure 4a AUC standard errorsshown Figure 4b. proposed SHML model achieves significant improvementmethods. reduces prediction error elastic net (which ranks secondbest) 25%, reduces error LapSVM 48%.5.2 Real-World Study Alzheimers DiseaseAlzheimers Disease common form dementia 30 million patientsworldwide payments care estimated $200 billion 2012 (Alzheimers258fiSimultaneous Association Discovery Diagnosis0.95Area Curve0.9Precision0.850.8LapSVMLassoElasticNetSparseORGPORCCA + LassoCCA + ElasticNetSHML0.90.850.750.8(a) Precision simulation(b) AUC simulationFigure 4: prediction results simulated real datasets. results averaged 10runs. error bars represent standard errors.Association, 2012). conducted association analysis diagnosis AD baseddataset Alzheimers Disease Neuroimaging Initiative(ADNI) 1 . ADNI studylongitudinal multisite observational study elderly individuals normal cognition, mildcognitive impairment, AD. applied proposed method study associationsgenotypes brain atrophy measured MRI predict subject status(normal vs MCI vs AD). Note statuses ordinal since represent increasingseverity levels.removing missing values, data set consists 625 subjects including183 normal,308 MCI 134 AD cases, subject contains 924 SNPs 328 MRI features.selected SNPs top SNPs separating normal subjects AD ADNI.MRI features measure brain atrophies different brain regions based corticalthickness, surface areas volumes, obtained FreeSurfer software 2 . testdiagnosis accuracy, compared method previously mentioned ordinalmultinomial regression methods. employ extended model linkage disequilibriumpriors, denoted SHML-LD, discover associations.compare SHML SHML-LD state-of-the-art classification methods.used 10-fold cross validation run tune free parameters trainingdata. determine dimension k latent features U method, computedvariational lower bounds approximation model marginal likelihood (i.e.,evidence), various k values {10, 20, 40, 60}. chose value largest approximate evidence, led k = 20 (see Figure 5). experiments confirmedk = 20, model achieved highest prediction accuracy, demonstrating benefitevidence maximization.shown Figure 6, method achieved highest prediction accuracy, highersecond best method, GP ordinal Regression, 10% worstmethod, CCA+lasso, 22%. two-sample test shows model outperformsalternative methods significantly (p < 0.05).1. http://adni.loni.ucla.edu/2. http://surfer.nmr.mgh.harvard.edu259fiXu, Zhe, Qi, & Yu5x 10Evidence Lower Bound7.47.427.447.467.48102040Dimensions60Figure 5: variational lower bound model marginal likelihood.0.64Precision0.60.550.5LSHCCC+SHetNicElLaR+CRPOGseicNetSparLaElLapSVM0.46Figure 6: prediction accuracy standard errors real data.also examined strongest associations discovered model. Firstly, ranking MRI features terms prediction power three different disease populations(normal, MCI AD) demonstrate top ranked features basedcortical thickness measurement. hand, features based volume260fiSimultaneous Association Discovery DiagnosisCT std R. CaudalAnteriorCingulateCT std L. SuperiorParietalCT std R. PostcentralCT std R. SuperiorParietalCT std L. PrecentralVol (WMP) CorpusCallosumMidPosteriorVol (WMP) CorpusCallosumCentralVol (WMP) CorpusCallosumPosteriorVol (WMP) CorpusCallosumMidAnteriorVol (WMP) L. CerebellumWMVol (WMP) R. CerebellumWMVol (WMP) FifthVentricleVol (WMP) NonWMHypoIntensitiesSurf Area L. UnknownVol (WMP) ThirdVentricleVol (WMP) R. LateralVentricleVol (WMP) L. LateralVentricleVol (WMP) R. Caudate2101CAPCAPZB(rs7CA ZB(rs 04415CAPPZB(rs43692 0)528ZCAP B(rs1605023 )2CAPZB(rs1936880)CAPZB(rs732472 )7BCAZB(rs914550 )9BCAR3(rs188785 )9)5R5P3K3(rs38 3833)NCO 1(rs1 5803NCO A2(r 1318 8)s8077)NC 2(rs12 14818TRAOA2(rs588339)TRA F3(rs 76130 )78FTR 3(rs1252178 )TRAAF3(rs 896382)F3( 2533 1)rs130260 59)060)2(a)CAPZCAP B(rs7044ZCAP B(rs80 150)5CAP ZB(rs4 0232)369ZBCAP (rs169 252)368ZB(80)rs7CAPZB( 14550rs139)CAP24ZTRA B(rs98 727)878F3(TRA rs1326 59)0FTRA 3(rs25 060)330F3(TRA rs1289 59)6FBCA 3(rs75 381)21RBCA 3(rs15 782)5R3(rs3 3833)P3K858NCO 1(rs11 038)3NCO A2(rs8 1877)014A2(8rNCO s1258 18)83A2(rs76 39)1308)Vol (WMP) R. HippocampusVol (WMP) L. HippocampusVol (CP) R. ParahippocampalCT std L. UnknownCT std R. UnknownVol (CP) L. EntorhinalVol (CP) R. EntorhinalCT Avg R. EntorhinalCT Avg L. EntorhinalVol (CP) L. ParahippocampalCT Avg R. ParahippocampalCT Avg L. ParahippocampalCT Avg R. UnknownCT Avg L. UnknownVol (WMP) L. AmygdalaVol (CP) R. UnknownVol (WMP) R. AmygdalaVol (CP) L. Unknown(b)Figure 7: estimated associations MRI features SNPs. sub-figure,MRI features listed right SNP names givenbottom.surface area estimation less predictive. Particularly, thickness measurements middletemporal lobe, precuneus, fusiform found predictive comparedbrain regions. findings consistent memory-related functionregions findings literature prediction power AD. also foundmeasurements structure left right sides similar weights,indicating algorithm automatically select correlated features groups, sinceasymmetrical relationship found brain regions involved AD.Secondly, analysis associating genotype AD prediction also generated interesting results. Similar MRI features, SNPs vicinity oftenselected together, indicating group selection characteristics algorithm. example, top ranked SNPs associated genes including CAPZB (F-actin-capping261fiXu, Zhe, Qi, & Yuprotein subunit beta), NCOA2 (The nuclear receptor coactivator 2) BCAR3(Breastcancer anti-estrogen resistance protein 3).last, biclustering gene-MRI associations, shown Figure 7, reveals interesting patterns terms relationship genetic variations brain atrophymeasured structural MRI. example, top ranked SNPs associatedgenes including BCAR3 (Breast cancer anti-estrogen resistance protein 3) NCOA2,MAP3K1 (mitogen-activated protein kinase kinase kinase 1) studiedcarefully cancer research. set SNPs associated cingulate negativedirections, part limbic system involves emotion formation processing. Compared structures temporal lobe, plays importantrole formation long-term memory. example, association MAP3K1caudate anterior cingulate cortex identified. Literature shownMAP3K1 associated biological processes apoptosis, cell cycle, chromatinbinding DNA binding3 , cingulate cortex shown severely affectedAD (Jones et al., 2006). strong association discovered work might indicatepotential genetic effects atrophy pattern observed cingulate subregion.6. Related Workproposed model model related broad family probabilistic latent variablemodels, including probabilistic principle component analysis (Tipping & Bishop, 1999),probabilistic canonical correlation analysis (Bach & Jordan, 2005) extensions (Yu,Yu, Tresp, Kriegel, & Wu, 2006; Archambeau & Bach, 2009; Guan & Dy, 2009; Virtanen,Klami, & Kaski, 2011). learn latent representation whose projection leadsobserved data. Recent studies probabilistic factor analysis methods put focussparsity-inducing priors projection matrix. Among them, Guan Dy (2009)used Laplace prior, Jeffreys prior, inverse-Gaussian prior; ArchambeauBach (2009) employed inverse-Gamma prior; Virtanen et al. (2011) used Automatic Relevance Determination(ARD) prior. Despite success, sparsity-inducingpriors disadvantages confound degree sparsity degreeregularization relevant irrelevant variables, practical settingslittle reason two types complexity control tightly boundedtogether. Although inverse-Gaussian prior inverse-Gamma prior provideflexibility controlling sparsity, suffer highly sensitive controlling parameters thus lead unstable solutions. contrast, model adopts spikeslab prior, recently used multi-task multiple kernel learning (Titsias& Lazaro-Gredilla, 2011), sparse coding (Goodfellow et al., 2012), latent factor analysis (Carvalho, Chang, Lucas, Nevins, Wang, & West, 2008). Note Beta priorsselection indicators lead simple yet effective variational updates, hierarchical prior work Carvalho et al.(2008) better handle selection uncertainty.Regardless priors assigned spike slab models, generally avoidconfounding issue separately controlling projection sparsity regularizationeffect selected elements.3. https://portal.genego.com/262fiSimultaneous Association Discovery DiagnosisSHML also connected many methods learning multiple sourcesviews (Hardoon, Leen, Kaski, & Shawe-Taylor, 2008). Multiview learning methodsoften used learn better classifier multi-label classification usually text miningimage classification domains based correlation structures among training datalabels (Yu et al., 2006; Virtanen et al., 2011; Rish, Grabarnik, Cecchi, Pereira, &Gordon, 2008). However, medical analysis diagnosis, meet two separate tasksassociation discovery genetic variations clinical traits, diagnosispatients. proposed SHML conducts two tasks simultaneously: employsdiagnosis labels guide association discovery, leveraging association structuresimprove diagnosis. particular, diagnosis procedure SHML leads ordinalregression model based latent Gaussian process models. latent Gaussian processtreatment differentiates multiview CCA models (Rupnik & Shawe-Taylor, 2010).Moreover, multiview learning methods model heterogeneous data typesdifferent views, simply treat continuous data. simplificationdegrate predictive performance. Instead, based probabilistic framework , SHMLuses suitable link functions fit different types data.7. Conclusionspresented new Bayesian multiview learning framework simultaneously findkey associations data sources (i.e., genetic variations phenotypic traits)predict unknown ordinal labels. shown model also employ backgroundinformation, e.g., Linkage Disequilibrium information, via additional graph Laplaciantype prior. proposed approach follows generative model: extracts commonlatent representation encodes structural information within data views,generates data via sparse projections. encoding knowledge multipleviews via latent representation makes possible effectively detect associationshigh sensitivity specificity.Experimental results ADNI data indicate model found biologically meaningful associations SNPs MRI features led significant improvementpredicting ordinal AD stages alternative classification ordinal regressionmethods. Despite drawbacks proposed framework slow training speed requirement careful tuning parameters, strong modeling power due Bayesiannature. Although focused AD study, expect model, powerful extension CCA, applied wide range applications biomedical researchexample, eQTL analysis supervised additional labeling information.AcknowledgmentsData used preparation article obtained Alzheimers Disease Neuroimaging Initiative (ADNI) database (adni.loni.ucla.edu). such, investigators withinADNI contributed design implementation ADNI and/or provided dataparticipate analysis writing report. complete listing ADNI investi263fiXu, Zhe, Qi, & Yugators found at: http://adni.loni.ucla.edu/wp-content/uploads/how apply/ADNIAcknowledgement List.pdf.work supported NSF IIS-0916443, IIS-1054903, CCF-0939370, NSF China(Nos. 61572111, 61433014, 61440036), 973 project china (No.2014CB340401), 985Project UESTC (No.A1098531023601041) Basic Research Project China CentralUniversity ( No. ZYGX2014J058).Zenglin Xu Shandian Zhe equal contributions article. Yuan QiPrinciple corresponding author.Appendix A. Parameter Update Continuous Dataparameter ij Q(sijg ) introduced Section 3.1 calculated ij = 1/ 1 +2ijij2 i( 1 1 )) . parameters Betaexp(hlog(1 g )i hlog(g )i + 21 log( 12 ) + 12 hgij22212distribution Q(gij ) given lij = ij + l1 lij = 1 ij + l2 . parameters121>Gamma distribution Q() updated r1 = r1 + np2 r2 = r2 + 2 tr(XX )1>>>tr(hGihUiX ) + 2 tr(hUU ihG Gi).moments required distributions calculated hi = rr12hlog(gij )i = (l1ij ) (l1ij + l2ij ),hlog(1 gij )i = (l2ij ) (l1ij + l2ij ),>hG Gi =pX+ >,i=1hGi = [1 , . . . , p ]> ,(x) =dx(23)ln (x).Appendix B. Parameter Update Ordinal Datavariational distributions Sh h introduced Section 3.2 givenQ(Sh ) =qksijijijh (1 ij )1sh ,(24)Beta(hij |dij1 , dij2 ),(25)i=1 j=1Q(h ) =qki=1 j=12ij = 1/ 1+exp(hlog(1hij )ihlog(hij )i+ 12 log( 12 )+ 12 hh2ij i( 12 12 )) , dij1 = ij +d1 ,212ij>2d2 = 1 ij + d2 , hsh = [1i , . . . , qi ] , hhij i-th diagonal element j .required moments updating distributions calculated follows:hlog( ij )i = (dij ) (dij + dij ),hhij )i112ij ijhlog(1= (dij2 ) (d1 + d2 ),N (bzij +1 |cij , 1) N (bzij |cij , 1),hcij = cij(bzij +1 cij ) (bzij cij )264fiSimultaneous Association Discovery Diagnosis() cumulative distribution function standard Gaussian distribution. NoteEquation (26), Q(cij ) truncated Gaussian truncation controlledobserved ordinal data zij .Appendix C. Parameter Update Labelsvariational distributions sw w Section 3.3 givenQ(sw ) =Q( w ) =ki=1ksiw (1 )1sw ,(26)Beta(w; ei1 , ei2 ),(27)i=1)i hlog( )i + 1 hw 2 i( 1= 1/ 1 + exp(hlog(1 ww2211))22, ei1 = + e1ei2 = 1 + e2 .required moments updating distributions calculated follows:hlog(w)i = (ei1 ) (ei1 + ei2 ),hlog(1 w)i = (ei2 ) (ei1 + ei2 ),N (byi +1 |fi , 1) N (byi |fi , 1)hfi = fi.(by +1 fi ) (by fi )Note Q(fi ) also truncated Gaussian truncated region decided ordinal label yi . way, supervised information incorporated estimationf estimation quantities recursive updates.Appendix D. Parameter Update Latent Representation U>required moments hww> i, hG> GiSection 3.4 calculatedPpand hH Hi introduced>>>hww = w + mm , hG Gi = i=1 + >(trace(i + >) i=j(hH> Hi)ij =.>j6= jrequired moments already listed previous sections. moments regarding U requiredvariational distributionsPare hUi =Pn the> updatesn>2[1 , 2 , ...n ], hUU = i=1 +i , hui = [1 , 2 , ...in ]> hu>ui =j=1 (j ) +(j )ii .ReferencesAlzheimers Association (2012). 2012 facts figures alzheimers disease facts figures.Tech. rep..Archambeau, C., & Bach, F. (2009). Sparse probabilistic projections. Advances NeuralInformation Processing Systems 21, pp. 7380.265fiXu, Zhe, Qi, & YuBach, F., & Jordan, M. (2005). probabilistic interpretation canonical correlationanalysis. Tech. rep., UC Berkeley.Carvalho, C., Chang, J., Lucas, J., Nevins, J., Wang, Q., & West, M. (2008). Highdimensional sparse factor modeling: applications gene expression genomics. JournalAmerican Statistical Association, 103 (484), 14381456.Chen, X., Liu, H., & Carbonell, J. (2012). Structured sparse canonical correlation analysis..AISTATS12, Vol. 22, pp. 199207.Chu, W., & Ghahramani, Z. (2005). Gaussian processes ordinal regression. JournalMachine Learning Research, 6, 10191041.Consoli, L., Lefevre, A., Zivy, M., de Vienne, D., & Damerval, C. (2002). QTL analysisproteome transcriptome variations dissecting genetic architecturecomplex traits maize. Plant Mol Biol., 48 (5), 575581.Daniela, M., & Tibshirani, R. (2009). Extensions sparse canonical correlation analysis,applications genomic data. Stat Appl Genet Mol Biol., 383 (1).Falconer, D., & Mackay, T. (1996). Introduction Quantitative Genetics (4th ed.). AddisonWesley Longman.Gandhi, S., & Wood, N. (2010). Genome-wide association studies: key unlockingneurodegeneration?. Nature Neuroscience, 13, 789794.George, E., & McCulloch, R. (1997). Approaches bayesian variable selection.. StatisticaSinica, 7 (2), 339373.Goodfellow, I., Couville, A., & Bengio, Y. (2012). Large-scale feature learning spikeand-slab sparse coding. Proceedings International Conference Machine Learning.Guan, Y., & Dy, J. (2009). Sparse probabilistic principal component analysis. JournalMachine Learning Research - Proceedings Track, 5, 185192.Hardoon, D., Leen, G., Kaski, S., & Shawe-Taylor, J. (Eds.). (2008). NIPS WorkshopLearning Multiple Sources.Harold, H. (1936). Relations two sets variates. Biometrika, 28, 321377.Hunter, D. (2012). Lessons genome-wide association studies epidemiology. Epidemiology, 23 (3), 363367.Jones, B. F., et al. (2006). Differential regional atrophy cingulate gyrus Alzheimerdisease: volumetric MRI study. Cereb. Cortex, 16 (12), 17011708.Khachaturian, S. (1985). Diagnosis Alzheimers disease. Archives Neurology, 42 (11),10971105.Kim, S., Sohn, K., & Xing, E. (2009). multivariate regression approach associationanalysis quantitative trait network. Bioinformaics, 25 (12), 204212.Lasserre, J., et al. (2006). Principled hybrids generative discriminative models.IEEE Computer Society Conference Computer Vision Pattern Recognition,Vol. 1, pp. 8794.266fiSimultaneous Association Discovery DiagnosisLiu, J., Pearlson, G., Windemuth, A., Ruano, G., Perrone-Bizzozero, N., & Calhoun, V.(2009). Combining fMRI SNP data investigate connections brainfunction genetics using parallel ICA. Hum Brain Mapp, 30 (1), 241255.MacKay, D. (1991). Bayesian interpolation. Neural Computation, 4, 415447.Melacci, S., & Mikhail, B. (2011). Laplacian support vector machines trained primal.Journal Machine Learning Research, 12, 11491184.Mohamed, S., et al. (2012). Bayesian L1 approaches sparse unsupervised learning.Proceedings International Conference Machine Learning.Neal, R. M. (1996). Bayesian Learning Neural Networks. Springer-Verlag New York,Inc.Parkhomenko, E., Tritchler, D., & Beyene, J. (2007). Genome-wide sparse canonical correlation gene expression genotypes. BMC Proc.Rish, I., Grabarnik, G., Cecchi, G., Pereira, F., & Gordon, G. (2008). Closed-form supervised dimensionality reduction generalized linear models. ProceedingsInternational Conference Machine Learning08, pp. 832839.Rupnik, J., & Shawe-Taylor, J. (2010). Multi-view canonical correlation analysis. Proceedings SIG Conference Knowledge Discovery Mining10.Slatkin, M. (2008). Linkage disequilibrium understanding evolutionary pastmapping medical future. Nature Reviews Genetics, Vol. 6, pp. 477485.Sun, L., Ji, S., & Ye, J. (2011). Canonical correlation analysis multi-label classification:least squares formulation, extensions analysis. IEEE Transactions PatternAnalysis Machine Intelligence, 33 (1), 194200.Tibshirani, R. (1994). Regression shrinkage selection via lasso. Journal RoyalStatistical Society, Series B, 58, 267288.Tipping, M., & Bishop, C. (1999). Probabilistic principal component analysis. JournalRoyal Statistical Society Series B-statistical Methodology, 61, 611622.Titsias, M., & Lazaro-Gredilla, M. (2011). Spike slab variational inference multitask multiple kernel learning. Advances Neural Information ProcessingSystems11, pp. 23392347.Virtanen, S., Klami, A., & Kaski, S. (2011). Bayesian CCA via group sparsity. ProceedingsInternational Conference Machine Learning11, pp. 457464.Yu, S., Yu, K., Tresp, V., Kriegel, H., & Wu, M. (2006). Supervised probabilistic principalcomponent analysis. Proceedings SIG Conference Knowledge DiscoveryMining06, pp. 464473.Yuan, M., & Lin, Y. (2007). Model selection estimation regression groupedvariables.. Journal Royal Statistical Society, Series B, 68 (1), 4967.Zhe, S., Xu, Z., Qi, Y., & Yu, P. (2014). Supervised heterogeneous multiview learningjoint association study disease diagnosis. Pacific Symposium Biocomputing,19.267fiXu, Zhe, Qi, & YuZhe, S., Xu, Z., Qi, Y., & Yu, P. (2015). Sparse bayesian multiview learning simultaneous association discovery diagnosis alzheimers disease. ProceedingsTwenty-Ninth AAAI Conference Artificial Intelligence, January 25-30, 2015,Austin, Texas, USA., pp. 19661972.Zou, H., & Hastie, T. (2005). Regularization variable selection via elastic net.Journal Royal Statistical Society, Series B, 67, 301320.268fiJournal Artificial Intelligence Research 56 (2016) 463-515Submitted 12/15; published 07/16Computing Repairs Inconsistent DL-Programs EL OntologiesEITER @ KR . TUWIEN . AC .Thomas EiterMichael FinkDaria StepanovaFINK @ KR . TUWIEN . AC .DASHA @ KR . TUWIEN . AC .Institut fr Informationssysteme, TU Wien,Favoritenstrae 9-11, 1040 Vienna, AustriaAbstractDescription Logic (DL) ontologies non-monotonic rules two prominent KnowledgeRepresentation (KR) formalisms complementary features essential various applications. Nonmonotonic Description Logic (DL) programs combine formalisms thus providing support rule-based reasoning top DL ontologies using well-defined query interfacerepresented so-called DL-atoms. Unfortunately, interaction rules ontology mayincur inconsistencies DL-program lacks answer sets (i.e., models), thus yieldsinformation. issue addressed recently defined repair answer sets, computingeffective practical algorithm proposed DL-Lite ontologies reduces repair computation constraint matching based so-called support sets. However, algorithm exploitsparticular features DL-Lite readily applied repairing DL-programsprominent DLs like EL. Compared DL-Lite , EL support sets may neither smallsupport sets might exist, completeness algorithm may need givensupport information bounded. thus provide approach computing repairsDL-programs EL ontologies based partial (incomplete) support families. latterconstructed using datalog query rewriting techniques well ontology approximation basedlogical difference EL-terminologies. show maximal size number support sets given DL-atom estimated analyzing properties support hypergraph,characterizes relevant set TBox axioms needed query derivation. present declarative implementation repair approach experimentally evaluate set benchmarkproblems; promising results witness practical feasibility repair approach.1. IntroductionDescription Logics (DLs) powerful formalism Knowledge Representation (KR)used formalize domains interest describing meaning terms relationshipsthem. well-suited terminological modelling contexts as, Semantic Web, dataintegration ontology-based data access (Calvanese, De Giacomo, Lenzerini, Lembo, Poggi, &Rosati, 2007b; Calvanese, De Giacomo, Lembo, Lenzerini, Poggi, & Rosati, 2007a), reasoningactions (Baader, Lutz, Milicic, Sattler, & Wolter, 2005), spatial reasoning (zccep & Mller,2012), runtime verification program analysis (Baader, Bauer, & Lippmann, 2009; Kotek,Simkus, Veith, & Zuleger, 2014), mention few.DLs fragments classical first-order logic, shortcomings modelling application settings, nonmonotonicity closed-world reasoning needs expressed.Rules nonmonotonic logic programming offer features. addition, serve welltool declaring knowledge reasoning individuals, modelling nondeterminism model generation possible Answer Set Programming. get best twoc2016AI Access Foundation. rights reserved.fiE ITER , F INK & TEPANOVA(1) Blacklisted Staff(2) StaffRequest hasAction.Action hasSubject.Staff hasTarget.Project= (3) BlacklistedStaffRequest StaffRequest hasSubject.Blacklisted(4) StaffRequest(r1 ) (5) hasSubject(r1 , john) (6) Blacklisted (john)(7) hasTarget(r1 , p1 ) (8) hasAction(r1 , read ) (9) Action(read )(10) projfile(p1 ); (11) hasowner (p1 , john);(12) chief (Y ) hasowner (Z , ), projfile(Z );(13) grant(X) DL[Project projfile; StaffRequest](X), deny(X);P=(14) deny(X) DL[Staff chief ; BlacklistedStaffRequest](X);(15)hasowner(Y,Z),grant(X),DL[; hasTarget](X, ), DL[; hasSubject](X, Z).Figure 1: DL-program policy ontologyworlds DLs nonmonotonic rules, natural idea combining led numberapproaches combination, often called hybrid knowledge bases; see workMotik Rosati (2010) references therein. Among them, Nonmonotonic Description Logic(DL-)programs (Eiter, Ianni, Lukasiewicz, Schindlauer, & Tompits, 2008) prominent approachso-called DL-atoms serve query interfaces ontology loose coupling enable bidirectional information flow rules ontology. possibility addinformation rules part prior query evaluation allows adaptive combinations. However, loose interaction rules ontology easily lead inconsistency,lack models answer sets.Example 1 Consider DL-program = hO, Pi Figure 1 formalizing access policyontology = hT , Ai (Bonatti, Faella, & Sauro, 2010), whose taxonomy (TBox) given(1)-(3), (4)-(9) sample data part (ABox) A. Besides facts (10), (11) simplerule (12), rule part P contains defaults (13), (14) expressing staff members grantedaccess project files unless blacklisted, constraint (15), forbids owners project information lack access it. parts, P O, interact via DL-atomsDL[Project projfile; StaffRequest](X). latter specifies temporary update via operator , prior querying it; i.e. additional assertions Project(c) considered individual c, projfile(c) true interpretation P, instances X StaffRequestretrieved O. Inconsistency arises john, chief project p1 owner files,access them.Inconsistency well-known problem logic-based data intensive systems, problem treating logically contradicting information studied various fields, e.g. beliefrevision (Alchourrn, Grdenfors, & Makinson, 1985; Grdenfors & Rott, 1995), knowledge baseupdates (Eiter, Erdem, Fink, & Senko, 2005), diagnosis (Reiter, 1987), ontology based data access (Lembo, Lanzerini, Rosati, Ruzzi, & Savo, 2015), nonmonotonic reasoning (Brewka, 1989;Sakama & Inoue, 2003), many others; (cf. Bertossi, Hunter, & Schaub, 2005; Nguyen, 2008;Martinez, Molinaro, Subrahmanian, & Amgoud, 2013; Bertossi, 2011). hybrid formalismsfar inconsistency management concentrated mostly inconsistency tolerance. instance,464fiC OMPUTING R EPAIRS NCONSISTENT DL-P ROGRAMS EL NTOLOGIESMKNF knowledge bases paraconsistent semantics developed Knorr, Alferes, Hitzler(2008), Huang, Li, Hitzler (2013) Kaminski, Knorr, Leite (2015). DL-programs inconsistency tolerance issues targeted Fink (2012), paraconsistent semantics basedLogic introduced. Furthermore, Phrer, Heymans, Eiter (2010)considered suppressing certain problematic DL-atoms. approaches aimed reasoninginconsistent system rather making required changes system arrive consistentstate. contrast repair techniques recently developed Eiter, Fink,Stepanova (2013, 2014d).theoretical framework repairing inconsistent DL-programs proposed Eiter et al.(2013), ontology ABox (a likely source errors) changed modified DL-programanswer sets, called repair answer sets. Different repair options including deletion ABox formulas various restricted forms addition considered together naive algorithmcomputing repair answer sets lacked practicality.effective repair algorithm DL-atoms decided without dynamic ontologyaccess presented Eiter, Fink, Stepanova (2015). based support sets (Eiter, Fink,Redl, & Stepanova, 2014b) DL-atoms, portions input together ABoxdetermine truth value DL-atom. algorithm exploits complete support families, i.e.stocks support sets value DL-atom every interpretation determined, (repeated) ontology access avoided. approach works wellDL-Lite , prominent tractable DL, since complete support families small easycompute.However, unfortunately, DLs approach readily usable, generallarge infinite support families. applies even EL, another wellknown important DL offers tractable reasoning widely applied many domains, including biology, (cf. e.g., Schulz, Cornet, & Spackman, 2011; Aranguren, Bechhofer, Lord, Sattler, &Stevens, 2007), medicine (Steve, Gangemi, & Mori, 1995), chemistry, policy management, etc. Duefeatures EL include range restrictions concept conjunctions left-hand sideinclusion axioms, DL-atom accessing EL ontology arbitrarily large infinitelymany support sets general. latter excluded acyclic TBoxes, often occurringpractice (Gardiner, Tsarkov, & Horrocks, 2006), complete support families still large,constructing well managing might impractical. obstructs deploymentapproach proposed Eiter et al. (2014d) EL ontologies. paper tackle issuedevelop repair computation techniques DL-programs ontologies EL. focus EL,since apart simple widely used, DL well-researched, available effectivealgorithms query rewriting important reasoning readily used.specifically, introduce general algorithm repair answer set computation operates partial (incomplete) support families along techniques familieseffectively computed. problem computing repair answer sets DL-programsEL ontologies P2 -complete (in formulation decision problem; refer workStepanova (2015) details complexity).contributions advances previous works Eiter et al. (2014b, 2014d, 2015)summarized follows:effective computation repair answer sets exploit support sets Eiter et al.(2014d). contrast approaches Eiter et al. (2014d, 2015), however, TBoxclassification invoked, use datalog rewritings queries computing support sets465fiE ITER , F INK & TEPANOVA(see also Hansen, Lutz, Seylan, & Wolter, 2014). introduce notion partial supportfamilies, ontology reasoning access completely eliminated.general constructing complete support families always feasible EL ontologies, provide novel methods computing partial support families exploiting ontologyapproximation techniques based logical difference EL-terminologies considered Konev, Ludwig, Walther, Wolter (2012) Ludwig Walther (2014).capture restricted classes TBoxes, complete support families stilleffectively computed, consider support hypergraph DL-atoms, inspiredontology hypergraphs (Nortje, Britz, & Meyer, 2013; Ecke, Ludwig, & Walther, 2013).support hypergraph serves characterize TBox parts relevant derivingquery. analysis support hypergraphs allows us estimate maximal sizenumber support sets needed form complete support family.generalize algorithm repair answer set computation proposed Eiter et al. (2014d)EL ontologies handled. novel algorithm operates partial supportfamilies, principle applied ontologies DLs beyond EL. useshitting sets disable known support sets negative DL-atoms performs evaluationpostchecks needed compensate incompleteness support families. Moreover, tradesanswer completeness scalability using minimal hitting sets; however completeness mayensured simple extension.provide system prototype declarative realization novel algorithm repairanswer set computation. repair approach evaluated using novel benchmarks; results show promising potential proposed approach.Organization. rest paper organized follows. Section 2, recall basic notionspreliminary results. Section 3 deals support sets computation, Section 4discusses partial support family construction based TBox approximation techniques. Section 5analyze properties support hypergraph estimating maximal size number support sets complete support family DL-atom. Section 6, algorithm repair answerset computation declarative implementation presented. Experiments presented Section 7, followed discussion related work Section 8 concluding remarks Section 9.2. Preliminariessection, recall basic notions Description Logics, focus EL (Baader,Brandt, & Lutz, 2005), DL-programs (Eiter et al., 2008). background DescriptionLogics, (see Baader, Calvanese, McGuinness, Nardi, & Patel-Schneider, 2003).2.1 Description Logic Knowledge Basesconsider Description Logic (DL) knowledge bases (KBs) signature = hI, C, Riset individuals (constants), set C concept names (unary predicates), set R rolenames (binary predicates) usual. DL knowledge base (or ontology) pair = hT , AiTBox ABox A, finite sets formulas capturing taxonomic resp. factual466fiC OMPUTING R EPAIRS NCONSISTENT DL-P ROGRAMS EL NTOLOGIESTnorm(1) StaffRequest hasAction.Action(2)StaffRequest hasSubject.Staff(3)StaffRequesthasTarget.Project(4) hasAction.Action ChasA.A=(5) hasSubject.Staff ChasS .St(6) hasTarget.Project ChasT .P(7) ChasA.A ChasS .St ChasA.AhasS .St(8) ChasA.AhasS .St ChasT .P StaffRequestFigure 2: Normalized TBoxknowledge, whose form depends underlying DL. abuse notation, also write =viewing set formulas.Syntax. EL, concepts C, denoting sets objects, roles R, denoting binary relationsobjects, obey following syntax, C atomic concept R Ratomic role:C | | C C | R.CEL, TBox axioms form C1 C2 (also called generalized concept inclusion axioms,GCIs), C1 , C2 EL-concepts. ABox formulas form A(c) R(c, d),C, R R, c, I. sequel, use P generic predicate C R (ifdistinction immaterial).example EL ontology given Figure 1.Definition 2 (normalized TBox) TBox normalized, axioms one following forms:A1 A2A1 A2 A3R.A1 A2A1 R.A2 ,A1 , A2 , A3 atomic concepts.E.g., axiom (1) Example 1 normal form, axioms (2) (3) not.EL TBox, equivalent TBox normal form constructible linear time (Stuckenschmidt,Parent, & Spaccapietra, 2009) (over extended signature)1 (Baader et al., 2005).special class TBoxes widely studied literature EL-terminologies, defined follows:Definition 3 (EL-terminology) EL-terminology EL TBox , satisfying following conditions:(1) consists axioms forms C C, atomic Carbitrary EL concept;(2) concept name occurs left hand side axioms .example, TBox ontology Figure 1 EL-terminology.Semantics. semantics DL ontologies based first-order interpretations (Baader et al.,2005). interpretation pair = hI , non-empty domain interpretation1. Linear complexity results obtained standard assumption DLs atomic conceptsconstant size, i.e., length binary string representing atomic concept depend particularknowledge base.467fiE ITER , F INK & TEPANOVAfunction assigns individual c object cI , concept name C subsetC , role name R binary relation RI . interpretation extendsinductively non-atomic concepts C roles R according concept resp. role constructors;EL, (R.C)I = {o1 | ho1 , o2 RI , o2 C } (C D)I = {o1 | o1 C , o1 DI }.Satisfaction axiom resp. assertion w.r.t. interpretation I, i.e. |= , follows:(i) |= C D, C DI ; (ii) |= C(a), aI C ; (iii) |= R(a, b), (aI , bI ) RI .Furthermore, satisfies set formulas , denoted |= , |= .TBox (respectively ABox A, ontology O) satisfiable (or consistent),interpretation satisfies it. call ABox consistent TBox , consistent.Since negation neither available expressible EL, EL ontologies consistent.Example 4 ontology Figure 1 consistent; satisfying interpretation = hI ,exists, = {john, read , p1 , r1 }, Action = {read }, Blacklisted = Staff = {john},hasSubject = {r1 , john}, StaffRequest = BlacklistedStaffRequest = {r1 }, hasAction ={r1 , read }, hasTarget = {r1 , p1 }.Throughout paper, consider ontologies EL unique name assumption (UNA),i.e., o1 6= o2 whenever o1 6= o2 holds interpretation. However, results carryontologies without UNA, hard see UNA EL effect queryanswering, (cf. Lutz, Toman, & Wolter, 2009).2.2 DL-ProgramsDL-program = hO, Pi pair DL ontology set P DL-rules, extendrules non-monotonic logic programs special DL-atoms. formed signature= hC, P, I, C, Ri, P = hC, Pi signature rule part P set C constantsymbols (finite) set P predicate symbols (called lp predicates) non-negative arities,= hI, C, Ri DL signature. set P disjoint C, R. simplicity, assumeC = I.Syntax. (disjunctive) DL-program = hO, Pi consists DL ontology finite set PDL-rules r forma1 . . . b1 , . . . , bk , bk+1 , . . . , bm(1)negation failure (NAF)2 ai , 0 n, first-order atom p(~t)predicate p P (called ordinary lp-atom) bi , 1 m, either lp-atom DLatom. rule constraint, n = 0, normal, n 1. call H(r) = {a1 , . . . , }head r, B(r) = {b1 , . . . , bk , bk+1 , . . . , bm } body r. B + (r) = {b1 , . . . , bk }B (r) = {bk+1 , . . . , bm } denote positive negative parts B(r) respectively.DL-atom d(~t) formDL[; Q](~t),(2)(a) = S1 op 1 p1 , . . . , Sm op pm , 0 input list i, 1 m, SiC R, op {} update operator, pi P input predicate aritySi ; intuitively, op = increases Si extension pi ;2. Strong negation added resp. emulated usual (Eiter et al., 2008).468fiC OMPUTING R EPAIRS NCONSISTENT DL-P ROGRAMS EL NTOLOGIES(b) Q(~t) DL-query, one forms (i) C(t), C conceptterm; (ii) R(t1 , t2 ), R role t1 , t2 terms; (iii) C1 C2 ~t = .Note inclusion DL-queries form C1 C2 easily reduced instance queries.3Thus simplicity, consider work instance DL-queries.Example 5 Consider DL-atom DL[Project projfile; StaffRequest](X ) rule (13)Figure 1 X = r1 . DL-query StaffRequest(r1 ); list = Project projfilecontains input predicate projfile extends ontology predicate Project via updateoperator .Semantics. semantics DL-program= hO, Pi given terms groundinggr() = hO, gr(P)i C, i.e., gr(P) = rP gr(r) contains possible ground instancesrules r P C. remainder, default assume ground.(Herbrand) interpretation set HB ground atoms, HB Herbrand base P =hC, Pi, i.e. set ground atoms P ; satisfies lp- DL-atom a,(i) I, lp-atom,(ii) (a) |= Q(~t) = hT , Ai, DL-atom form (2),(d) =[Ai (I) Ai (I) = {Si (~t) | pi (~t) I}, 1 m.(3)i=1Satisfaction DL-rule r (resp. set P rules) Herbrand interpretation = hP, Oiusual, satisfies bj , satisfy bj ; satisfies , satisfiesr P. |=O denote satisfies (is model of) object , (DL)atom, rule set rules; superscript |= specifies ontology DL-atomsevaluated. model minimal, model exists I.Example 6 DL-atom = DL[Project projfile; StaffRequest](r1 ) satisfied interpretation = {projfile(p1 ), hasowner (p1 , john)}, since |= StaffRequest(r1 ). =O\{StaffReqeust(r1 )} still holds |=O d, (d) |= StaffRequest(r1 ).Repair Answer Sets. Various semantics DL-programs extend answer set semantics logicprograms (Gelfond & Lifschitz, 1991) DL-programs, (e.g., Eiter et al., 2008; Lukasiewicz, 2010;Wang, You, Yuan, & Shen, 2010; Shen, 2011). concentrate weak answer sets (Eiteret al., 2008), treat DL-atoms like atoms NAF, flp-answer sets (Eiter, Ianni, Schindlauer, & Tompits, 2005), obey stronger foundedness condition. like answer setsordinary logic program interpretations minimal models program reduct,intuitively captures assumption-based application rules reconstruct interpretation.I,OP relative HB results gr(P) deletingweak -reduct Pweak(i) rules r either 6|=O DL-atom B + (r), |=O l l B (r);(ii) DL-atoms B + (r) literals B (r).3. Evaluating = DL[; C1 C2 ]() = reduces evaluating = DL[; AC2 ](a) ={AC1 C1 , C2 AC2 } {AC1 (a)}, fresh constant AC1 , AC2 fresh concepts (similarTBox normalization).469fiE ITER , F INK & TEPANOVAI,Oflp-reduct PflpP results gr(P) deleting rules r, whose bodiessatisfied I, i.e. 6|= bi , bi , 1 k |=O bj , bj , k < j m.illustrate notions example.Example 7 Let Figure 1, let rule set P contain facts (10), (11) rules(12), (13) X, Y, Z instantiated r1 , john, p1 respectively. Consider interpretation =I,O{projfile(p1 ), hasowner (p1 , john), chief (john), grant(r1 )}. flp-reduct PflpcontainsI,Orules P, weak -reduct Pweakrule (13) replaced fact grant(r1 ).Definition 8 (x-deletion repair answer set) interpretation x-deletion repair answer set= hT A, Pi x {flp, weak }, minimal model PxI,T , A;called x-deletion repair . = A, standard x-answer set.Example 9 = {projfile(p1 ), chief (john), hasowner (p1 , john), grant(john)} weakflp-repair answer set Example 1 repair = A\{Blacklisted (john)}.Notation. denote normal logic program P (P) set answer sets P,DL-program x () (resp. RAS x ()) set x-answer sets (resp. x-repairanswer sets) .general flp-answer set weak -answer set, vice versa, i.e. flp-answer setsrestrictive notion; however, many cases weak flp answer sets coincide.information reducts, see works Eiter et al. (2008) Wang et al. (2010).Shifting Lemma. simplify matters avoid dealing logic program predicates separately, shall shift Eiter et al. (2014d) lp-input DL-atoms ontology. GivenDL-atom = DL[; Q](~t) P p , call Pp (c) input assertion d, Ppfresh ontology predicate c C; Ad set assertions. TBoxDL-atom d, let Td = {Pp P | P p }, interpretation I, letOdI = Td {Pp (~t) Ad | p(~t) I}. have:Proposition 10 (Eiter et al., 2014d) every = A, DL-atom = DL[; Q](~t) interIpretation I, holds |=O iff |=Od DL[; Q](~t) iff OdI |= Q(~t).Unlike OI (d), OdI clear distinction native assertions input assertionsw.r.t. (via facts Pp axioms Pp P ), mirroring lp-input. Note normalform, also Td normal form.3. Support Sets DL-Atomssection, recall support sets DL-atoms Eiter et al. (2014b), effectiveoptimization means (repair) answer set computation (Eiter et al., 2014d). Intuitively, supportset DL-atom = DL[; Q](~t) portion input that, together ABox assertions,sufficient conclude query Q(~t) evaluates true; i.e., given subsetinterpretation set ABox assertions ontology O, conclude|=O Q(~t). Basically, method suggests precomputing support sets DL-atomnonground level. DL-program evaluation, candidate interpretation ground instancessupport sets computed, help prune search space (repair) answer sets.470fiC OMPUTING R EPAIRS NCONSISTENT DL-P ROGRAMS EL NTOLOGIESExploiting Proposition 10 following definition support sets using ontologypredicates.Definition 11 (ground support sets) Given ground DL-atom = DL[; Q](~t), set AAdsupport set w.r.t. ontology = hT , Ai, Td |= Q(~t). Supp (d) denoteset support sets w.r.t. O.Support sets grouped families support sets simply support families. formally,Definition 12 (support family) collection Supp (d) support sets DL-atomw.r.t. ontology support family w.r.t. O.general EL particular, even -minimal support sets arbitrarily largeinfinitely many support sets may exist (not acyclic TBoxes , still exponentially manysupport sets possible). However, nonetheless exploit repair answer setcomputation algorithms Section 6.Support sets linked interpretations following notion.Definition 13 (coherence) support set DL-atom coherent interpretation I,Pp (~c) holds p(c) I.Example 14 DL-atom = DL[Project Projfile; StaffRequest](r1 ) Figure 1 twosupport sets: S1 = {StaffRequest(r1 )} S2 = {hasSubject(r1 , john),Projectprojfile (p1 ),Staff (john),hasAction(r1 , read ), Action(read )}. S1 coherent interpretation,S2 coherent interpretations {projfile(p1 )}.evaluation w.r.t. reduces search coherent support sets.Proposition 15 Let = DL[; Q](~t) ground DL-atom, let = hT , Ai ontology,let interpretation. Then, |=O iff Supp (d) exists s.t. coherent I.Using sufficient portion support sets, completely eliminate ontology accessevaluation DL-atoms. naive approach, one precomputes support sets ground DLatoms respect relevant ABoxes, uses repair answer set computation.scale practice, since support sets may computed incoherentcandidate repair answer sets.alternative fully interleave support set computation search repair answersets. construct coherent ground support sets DL-atom interpretationfly. input DL-atom may change different interpretations, support sets mustrecomputed, however, since reuse may possible; effective optimizations immediate.better solution precompute support sets nonground level, is, schematic supportsets, prior repair computation. Furthermore, may leave concrete ABox open;support sets DL-atom instance easily obtained syntactic matching.~ = DL[; Q](X)~Definition 16 (nonground support sets) Let TBox, let d(X)~nonground DL-atom. Suppose V X set distinct variables C set constants.nonground support set w.r.t. set = {P1 (Y~1 ), . . . , Pk (Y~k )} atoms471fiE ITER , F INK & TEPANOVA(i) Y~1 , . . . , Y~k V~k )} support set(ii) substitution : V C, instance = {P1 (Y~1 ), . . . , Pk (Y~d(X) w.r.t. OC = AC , AC set possible ABox assertions C.ontology = AC , denote SuppO (d) set nonground support setsw.r.t. .AC takes care possible ABox, considering largest ABox (sinceimplies Supp (d) Supp (d)).Example 17 = DL[Project projfile; StaffRequest](X ) set S1 = {StaffRequest(X )}nonground support set, likewise set S2 = {Action(W ), Staff (Y ), hasSubject(X , ),hasTarget(X , Z ), Projectprojfile (Z ), hasAction(X , W )}.sufficiently large portion nonground support sets precomputed, ontology accessfully avoided. call portion complete support family.Definition 18 (complete support family) family SuppO (d) nonground support sets~ w.r.t. ontology complete, every support set(non-ground) DL-atom d(X)~~~ existSupp (d(X)), : X C, extension : V C V X= .Example 19 Consider DL-atom d(X) = DL[Project projfile; StaffRequest](X) Figure 1. family = {S1 , S2 , S3 , S4 , S5 , S6 } complete w.r.t. O, hT = hasTarget,hS = hasSubject hA = hasAction:S1S2S3S4S5S6= {StaffRequest(X )};= {Project(Y ), hT (X , ), hS (X , Z ), Staff (Z ), hA(X , Z ), Action(Z )};= {Projectprojfile (Y ), hT (X , ), hS (X , Z ), Staff (Z ), hA(X , Z ), Action(Z )};= {Project(Y ), hT (X , ), hS (X , Z ), Blacklisted (Z ), hA(X , Z ), Action(Z )};= {Projectprojfile (Y ), hT (X , ), hS (X , Z ), Blacklisted (Z ), hA(X , Z ), Action(Z )};= {BlacklistedStaffRequest(X )}.say two nonground support sets (resp. support families) ground-identical,groundings coincide. E.g., support sets S1 = {P (X), r(X, )} S2 = {P (X), r(X, Z)}ground-identical DL-atom d(X) = DL[; Q](X), respective support families{S1 } {S2 }.Definition 20 (subsumption) nonground support set subsumed , denoted S,every ground instance ground instance exists S.nonground support families, say S1 subsumed S2 , denoted S2 S1 ,instance S1 instance S2 exists holds.Example 21 = {BlacklistedStaffRequest(X ),hasSubject(X , ),Blacklisted (Y )} supportset DL-atom d(X) = DL[Staff chief ; BlacklistedStaffRequest](X) w.r.t. Figure 1, subsumed = {BlacklistedStaffRequest(X )}, i.e. S. Moreover,S, = {S } ={S}, support families = {S, } = {S,{BlacklistedStaffRequest(X ),hasSubject(X , Z ),Blacklisted (Z )}} mutually subsume other.472fiC OMPUTING R EPAIRS NCONSISTENT DL-P ROGRAMS EL NTOLOGIESAxiomDatalog ruleA1 A2A2 (X) A1 (X)A1 A2 A3 A3 (X) A1 (X), A2 (X)R.A2 A1 A1 (X) R(X, ), A2 (Y )A1 R.A2R(X, oA2 ) A1 (X)A2 (oA2 ) A1 (X)Table 1: EL TBox RewritingDefinition 22 (maximal support set size, maxsup) maximal support set size DL-atomw.r.t. , denoted maxsup(d ), smallest integer n 0 every complete nonground support family w.r.t. support set |S| > n, support setexists w.r.t. Suppd (O) |S | n.instance, DL-atom TBox Example 19, maximal support setsize 6, i.e., maxsup(d) = 6.3.1 Computing Support Setssection, provide methods constructing nonground support sets. natural approachcomputation nonground support sets exploit (conjunctive) query answering methodsEL (e.g., Rosati, 2007; Lutz et al., 2009; Kontchakov, Lutz, Toman, Wolter, & Zakharyaschev,2010; Stefanoni, Motik, & Horrocks, 2012).Suppose given DL-program = hO, Pi, = hT , Ai EL ontology,~ = DL[; Q](X).~ method construct nonground support sets d(X)~DL-atom d(X)following three steps.Step 1. DL-query Rewriting TBox. first step exploits rewriting DL~ TBox Td = {Pp P | P p } set datalog rules.query Q d(X)preprocessing stage, TBox Td normalized. technique restricts syntactic formTBoxes decomposing complex simpler axioms. purpose, set fresh conceptsymbols introduced. normalized form Td norm Td computed, rewrite partTBox relevant query Q datalog program Prog Q,Tdnorm using translationgiven Table 1, variant translation Prez-Urbina, Motik, Horrocks (2010)Zhao, Pan, Ren (2009). rewriting axioms form A1 R.A2 (fourth axiomTable 1), introduce fresh constants (oA2 ) represent unknown objects. similar rewritingexploited R EQUIEM system (Prez-Urbina et al., 2010), function symbols usedinstead fresh constants. result obtain:Lemma 23 every data part, i.e., ABox A, every ground assertion Q(~c), deciding whetherProg Q,Tdnorm |= Q(~c) equivalent checking Td norm |= Q(~c).Step 2. Query Unfolding. second step proceeds standard unfolding rulesProg Q,Td norm w.r.t. target DL-query Q. start rule Q head expandbody using rules program Prog Q,Tdnorm . applying procedure exhaustively,get number rules correspond rewritings query Q Td norm . Notealways possible obtain rewritings effectively, since general might473fiE ITER , F INK & TEPANOVAProg Q,Td norm(4 ) ChasA.A (X ) hasAction(X , ), Action(Y ).(5 ) ChasS .St (X ) hasSubject(X , ), Staff (Y ).(6 ) ChasT .P (X ) hasTarget(X , ), Project(Y ).=(7) ChasA.AhasS .St (X ) ChasA.A (X ), ChasS .St (X ).(8)StaffRequest(X ) ChasA.AhasS .St (X ), ChasT .P (X ).(9) Project(X ) Projectprojfile (X ).Figure 3: DL-query Rewriting DL[Project projfile; StaffRequest](X) Td norminfinitely many cyclic, still exponentially many acyclic ; discusstechniques computing partial support families next section.Step 3. Support Set Extraction. last step extracts nonground support sets rewritingsStep 2. select containing predicates Td obtain rules r form~ P1 (Y~1 ), . . . , Pk (Y~k ), Pk+1~k+1 ), . . . , Pnp (Y~n ),Q(X)(Y(4)pk+1nPi native ontology predicate 1 k, predicate mirroring lp-inputotherwise. bodies rules correspond support sets given DL-atom, i.e.~k+1 ), . . . , Pnp (Y~n )}= {P1 (Y~1 ), . . . , Pk (Y~k ), Pk+1(Y(5)pk+1nfollowing holds.~ = DL[; Q](X)~ DL-atom program = hO, Pi ELProposition 24 Let d(X)~ontology = hT , Ai. Every set constructed Steps 1-3 nonground support set d(X).Shifting Lemma, working support sets focus ontology predicatesoperate them. specifically, rules form (4) k n fully reflect nongroundsupport sets Definition 16, ground instantiations rule constants Cimplicitly correspond ground support sets.illustrate computation nonground support sets DL-atoms EL ontologies.Example 25 Consider DL-atom DL[Project projfile; StaffRequest](X) accessing EL ontology = hT , Ai Figure 1. datalog rewriting computed Step 1 givenFigure 3. Step 2 obtain following query unfoldings StaffRequest:(1) StaffRequest(X) StaffRequest(X);(2) StaffRequest(X) hasAction(X, ), Action(Y ), hasSubject(X, ),Staff (Y ), hasTarget(X, ), Projectprojfile (Y );(3) StaffRequest(X) hasAction(X, ), Action(Y ), hasSubject(X, ),Staff (Y ), hasTarget(X, ), Project(Y );(4) StaffRequest(X) hasAction(X, ), Action(Y ), hasSubject(X, ),Blacklisted (Y ), hasTarget(X, ), Project(Y );(5) StaffRequest(X) hasAction(X, ), Action(Y ), hasSubject(X, ),Blacklisted (Y ), hasTarget(X, ), Projectprojfile (Y ).Step 3 thus get rule (2) S2 = {hasAction(X, ), Action(Y ), Staff (Y ),hasSubject(X, ), hasTarget(X, ), Projectprojfile (Y )} rule (3) S3 ={Action(Y ),hasAction(X, ),Staff (Y ),hasSubject(X, ), Project(Y ),hasTarget(X, )}. (1), (4)(5) remaining support sets similarly obtained.474fiC OMPUTING R EPAIRS NCONSISTENT DL-P ROGRAMS EL NTOLOGIES3.2 Partial Support FamiliesFinding support sets DL-atom tightly related computing solutions logic-basedabduction problem. Abduction important mode reasoning widely applied different areasAI including planning, diagnosis, natural language understanding many others (Console,Sapino, & Dupr, 1995). Various variants problem actively studied, e.g. Eiter,Gottlob, Leone (1997) Bienvenu (2008). Unfortunately, practically importantproblems context abduction intractable even restricted propositional theories (Eiter& Makino, 2007). abduction problem EL TBoxes considered Bienvenu (2008),represented tuple hT , H, Oi, TBox , set atomic concepts H atomic conceptO. explanation set {A1 , . . . , } H, |= A1 . . . O.ABox Ad contains atomic concepts, computing nonground support sets =DL[; Q](X) accessing = hT , Ai corresponds abduction problem hTd , sig(A Ad ), Qi.roles occur Ad , one introduce new fresh concepts construct complexconcepts hypothesis, e.g., R.A inclusion CR.A R.A added Td , CR.AH, CR.A fresh concept.Unlike DL-Lite , support families DL-atoms EL ontologies particularstructure; large, maximal support set size exponential size .Example 26 Consider following acyclic TBox , contains axioms:(1) r.B0 s.B0 B1(2) r.B1 s.B1 B2...(n) r.Bn1 s.Bn1 Bnd1 = DL[; B1 ](X1 ), maximal support set size 4, witnessedS1 = {r(X1 , X2 ), B0 (X2 ), s(X1 , X3 ), B0 (X3 )}.DL-atom d2 = DL[; B2 ](X1 ), maxsup(d2 ) = 10, due S2 = {r(X1 , X2 ),r(X2 , X3 ), B0 (X3 ), s(X2 , X4 ), B0 (X4 ), s(X1 , X5 ), r(X5 , X6 ), B0 (X6 ), s(X5 , X7 ), B0 (X7 )}.Moreover, di = DL[; Bi ](X), maxsup(di ) = maxsup(di1 ) 2 + 2, 1 n.Note maximal support set dn involves n + 3 predicates. Therefore, TBoxform, |sig(T )|= k, lower bound worst case support set size2k1 + 2 = (2k ), single exponential size .general many unfoldings produced Step 2, according recent resultsHansen et al. (2014), complete support families EL computed large classes ontologies. Therefore, still exploit support families, unlike Eiter et al. (2014d) requirecomplete, develop techniques computing partial (i.e. incomplete) support families DL-atoms. natural approach context aim finding support sets boundedsize. general, due cyclic dependencies r.C C, possible ELDL-Lite , support sets arbitrary large. analysis vast number ontologiesrevealed many realistic cases ontologies contain (nor imply) cyclic axioms (Gardineret al., 2006); thus assume practical considerations TBox ontology givenDL-program acyclic, i.e., entail inclusion axioms form r.C C. However, evenrestriction support sets large Example 26 shows.475fiE ITER , F INK & TEPANOVAcomputing complete support families computationally expensive, natural approachproduce support sets certain size k using e.g. limited program unfolding.unfolding branch reaches depth k, stop expand different branch. Similarly, compute limited number k support sets stopping rule unfolding program Prog Q,Tdnormk-th support set produced. alternative approach, based TBox approximationtechniques, pursued next section.4. Partial Support Family Construction via TBox Approximationprovide practical methods construct partial support families using TBox approximation.4.1 TBox Approximationapproximation DL ontologies source language L different target language Lwell-known important technique ontology management. Existing approachesapproximation roughly divided syntactic approaches semantic approaches. former,e.g. Tserendorj, Rudolph, Krtzsch, Hitzler (2008) Wache, Groot, Stuckenschmidt (2005), focus syntactic form axioms original ontology appropriatelyrewrite axioms comply syntax target language. rather effective general produce unsound answers (Pan & Thomas, 2007). Semantic approachesfocus model-based entailment original ontology, rather syntactic structure.aim preserving entailments much possible transforming ontologytarget language; general sound, might computationally expensive(Console, Mora, Rosati, Santarelli, & Savo, 2014).task computing partial support families, sound ontology approximation techniquesrelevant. choose DL-Lite core target approximation language, lies intersection EL DL-Lite , complete support families effectively identified (Eiteret al., 2014d). approach approximating TBox EL DL-Lite core exploits logicaldifference EL TBoxes considered Konev et al. (2012). idea behind decidewhether two ontologies give answers queries given vocabulary (called signature), compute succinct representation difference empty. Typical queries includesubsumption concepts, instance queries conjunctive queries. setting subsumption queries particular interest, based nonground support families constructed.~ ontology = hT , Ai,approach follows. Given DL-atom = DL[; Q](X)eliminate TBox Td axioms outside DL-Lite core language, obtain simplifiedTBox Td . compute succinct representation logical difference Td Tdw.r.t. = {sig(Ad A) Q}; axioms logical difference fall DL-Lite coreadded Td . restricting predicates potentially appear support sets avoidredundant computations approximate relevant part TBox. approachparticularly attractive, logical difference EL intensively studied, e.g. Lutz,Walther, Wolter (2007) Konev et al. (2012), polynomial algorithms availableEL-terminologies; thus confine latter.present approximation approach formally, first recall notions introducedKonev et al. (2012).476fiC OMPUTING R EPAIRS NCONSISTENT DL-P ROGRAMS EL NTOLOGIESDefinition 27 (-concept difference) -concept difference EL-terminologies T1T2 set cDiff (T1 , T2 ) EL-inclusions T1 |= T2 6|= .Example 28 terminologies T1 = {B E, E r., C B} T2 ={C A, B, C} holds cDiff (T1 , T2 ) = = {A, B, C},cDiff (T1 , T2 ) = {B r.} = {B, r}.two EL-terminologies entail concept subsumptions signature , i.e. holdscDiff (T1 , T2 ) = cDiff (T2 , T1 ) = , called -concept inseparable,CCdenoted T1 CT2 . E.g. Example 28 T1 T2 T1 6 T2 .logical difference terms instance queries defined follows.Definition 29 (-instance difference) -instance difference terminologies T1 T2set iDiff (T1 , T2 ) pairs form (A, ), -ABox -instanceassertion, T1 |= T2 6|= . say T1 T2 -instance inseparable,symbols T1 T2 iDiff (T1 , T2 ) = iDiff (T2 , T1 ) = .easily seen, T1 T2 implies T1 CT2 . converse obvious also holds.Theorem 30 (cf. Lutz & Wolter, 2010) EL-terminologies T1 T2 signature , T1 CT2 iff T1 T2 .4.2 Partial Support Family Constructionshow DL-atom set support sets -concept inseparable terminologies. Prior that, establish following lemma.Lemma 31 Let = DL[; Q](~t) DL-atom, let = hT1 , Ai EL ontology, let T2CTBox. T1 CT2 , =sig(A) sig(Q) {P | P p }, T1 T2 ,= sig(Ad ).Armed this, obtain following result equivalence nonground support families.~ DL-atom let T1 , T2 EL-terminologiesProposition 32 Let = DL[; Q](X)CT1 T2 = sig(A Ad Q) {P | P p }. S1 S2 complete nongroundsupport families w.r.t. O1 = hT1 , Ai O2 = hT2 , Ai, respectively, S1 S2ground-identical.Given two EL-terminologies T1 T2 , inclusions C cDiff (T1 , T2 ) (resp.C cDiff (T1 , T2 )) following Konev et al. (2012) called left (resp. right) witnesses denotedlhscWTnrhs(T1 , T2 ) (resp. cWTn (T1 , T2 )). shown every inclusion C concept difference T1 T2 contains either left right witness.Theorem 33 (cf. Konev et al., 2012) Let T1 T2 EL-terminologies signature.cDiff (T1 , T2 ), either C member cDiff (T1 , T2 ), sig()concept name C EL-concepts occurring .477fiE ITER , F INK & TEPANOVAAlgorithm 1: PartSupFam: compute partial support family~ ontology = hT , AiInput: DL-atom = DL[; Q](X),Output: Partial nonground support family SuppO (d)(a) {sig(A Ad ) Q}(b) Td {Pp P | P p }(c) Td Td \{C | C 6 {A, r.} 6 {A, r.}}rhslhs(d) lrw cWTn (Td , Td ) cWTn (Td , Td )(e) Td Td {C lrw | C, {A, r.}}(f) {ComplSupF am(d, Td )}returnlogical difference two EL-terminologies compact representation consistsinclusions atomic concept name either left right hand side. mayinclusions atomic concepts sides role restrictions form r., falltarget language DL-Lite core DL, therefore reintroduced.ready describe algorithm P artSupF (see Algorithm 1) compute partial~ ontologyfamilies support sets. input given DL-atom = DL[; Q](X)= hT , Ai, EL-terminology. first set signature (a) predicatesrelevant support set computation d. construct TBox Td (b) simplifiedversion Td (c) removing Td axioms form C D, C complexconcept, i.e. axioms DL-Lite core fragment. (d) compute right-hand sideleft-hand side witnesses Td Td store lrw . that, (e)construct TBox Td extending Td axioms lrw , concepts formr sides inclusions. Based support set construction method DL-Lite Eiteret al. (2014d), obtain complete support family Td (f), partial supportfamily .Proposition 34 family computed Algorithm 1 fulfills SuppO (d), i.e., partialsupport family given DL-atom w.r.t. = A.lwr = (d) cDiff (Td , Td ) = (e), guaranteed complete Proposition 32. general Algorithm 1 used computing support families DL-atomsaccessing arbitrary TBoxes4 , practically efficient procedures (d) available acyclicEL-terminologies (Konev et al., 2012).5. Bounded Support Setssection, analyze size number support sets given DL-atom have.bounds quantities hand, one limit search space support sets.precisely, aim support set families sufficient evaluating DL-atom. supportsets (properly) subsumed another support set (i.e., ) dropped,consider non-ground support families subsume (in particular, complete) supportfamily. formally,4. computing logical difference arbitrary TBoxes recent results Feng, Ludwig, Walther (2015) mightexploited.478fiC OMPUTING R EPAIRS NCONSISTENT DL-P ROGRAMS EL NTOLOGIESDefinition 35 (-complete support family) say nonground support family DL-atom-complete w.r.t. ontology O, SuppO (d).Thus question bounds size support sets cardinality smallest S.Throughout section, tacitly assume TBoxes acyclic, i.e. entail inclusions form R.C C.5.1 Estimation Support Set Size Boundsfirst consider estimate maximal size support sets smallest -complete supportfamily analyzing syntactic properties given TBox. start with, recall workKonev et al. (2012) atomic concept primitive terminology , occursaxiom left-hand side, pseudo-primitive, either primitive occursleft-hand side axioms C, C arbitrary EL concept.EL-terminology every pseudo-primitive |= A, =A1 . . . r1 .C1 . . . rm .Cm , (atomic) conjunct Ai exists |= Ai(Konev et al., 2012, Lemma 15). obtain:Proposition 36 Let = DL[; Q](~t) DL-atom, let EL-terminology. Qpseudo-primitive , maxsup(d) = 1.Proposition 36 exploits specific case, support set size bound 1. providingliberal syntactic conditions ensure bounded size support sets, use ontology hypergraphs (Nortje et al., 2013; Ecke et al., 2013). latter widely studied extractingmodules ontologies (Nortje et al., 2013), determining concept difference EL terminologies (Ecke et al., 2013), efficient reasoning OWL 2 QL (Lembo, Santarelli, & Savo, 2013),important tasks.First let us recall notion directed hypergraph, natural generalizationdirected graph, proposed Ausiello, DAtri, Sacc (1983) context databases represent functional dependencies.Definition 37 (directed hypergraph) directed hypergraph pair G = (V, E), Vset nodes graph E set directed hyperedges form e = (H, H ),H, H V nonempty sets called hypernodes.Given hyperedge e = (H, H ), call H tail e H head e, denotedtail (e) head (e), respectively. hypernode singleton, |H| = 1, binary hypernode,|H| = 2; abuse notation, singleton {v}, also simply write v. notionontology hypergraph DL EL introduced Ecke et al. (2013) follows.Definition 38 (ontology hypergraph) Let EL TBox normal form, let C R.ontology hypergraph GT directed hypergraph GT = (V, E),V = {xA | C ( sig(T ))} {xr | r R ( sig(T ))} {x },E = {({xA }, {xB }) | B }{({xA }, {xr , xY }) | r.Y , C {}}{({xr , xY }, {xA }) | r.Y , C {}}{({xB1 , xB2 }, {xA }) | B1 B2 }.479fiE ITER , F INK & TEPANOVAxr1xr3xA3xA1x C2xr2xA2xA4x C1xDxr4Figure 4: Hypergraph GT Example 39Example 39 Consider following TBox normal form:(4) C1 C2(1) r1 .A1 C1(2) r2 .A2 C2(5) A3 A2=(3) r .A(6)r4 .A4331ontology hypergraph GT =sig(T ) depicted Figure 4..define notions directed path two nodes incoming path singletonnode ontology hypergraph; natural generalizations path standard graph.Definition 40 (directed path, incoming path) Suppose EL TBox normal form,GT = (V, E) ontology hypergraph, x, V singleton nodes occurring GT .directed path x GT sequence = e1 , e2 , . . . , en (hyper) edges, that:(i) tail (e1 ) x;(ii) head (en ) y;(iii) every ei , < n, successor s(ei ) = ej ei exists GT j > i, head (ei )tail (ej ), s(ei ) = s(ei ) implies head (ei ) 6= head (ei ) 6= .incoming path singleton node x V GT = (V, E) directed path = e1 , . . . , ennode V x, head (en ) = x. set incoming paths node xhypergraph G denoted Paths(x , G).Intuitively, hyperedges ontology hypergraph GT model inclusion relations (complex)concepts . Consequently, incoming path singleton node xC GT models chaininclusions logically follow , C rightmost element chain.Example 41 Let us look ontology hypergraph GT Figure 4. sequence edges1 = ({xr3 , xA3 }, xA1 ), ({xr1 , xA1 }, xC1 )480fiC OMPUTING R EPAIRS NCONSISTENT DL-P ROGRAMS EL NTOLOGIESx 3pxr13xr3xA3xA1x C2xr2xA2xDxCxAx C1xBxDxQ(a) Gsupp(d),TExample 43(b) Gsupp(d),TExample 45Figure 5: Examples support hypergraphsincoming path xC1 GT reflects inclusions r1 .A1 C1 r1 .(r3 .A3 ) C1 ;sequence2 = ({xr3 , xA3 }, xA1 ), ({xr1 , xA1 }, xC1 ), ({xr2 , xA2 }, xC2 ), ({xC1 , xC2 }, xD )incoming path singleton xD , following set inclusions extracted:(1) C1 C2 D, (2) r2 .A2 C1 D, (3) r2 .A2 r1 .A1 D, (4) r2 .A2 r1 .(r3 .A3 ) D.introduce notion support hypergraph DL-atom.Definition 42 (support hypergraph) support hypergraph DL-atom d=DL[; Q](~t)constructed follows:normal ontology = hT , Ai hypergraph Gsupp(d),T1. build ontology hypergraph GTd = (V, E), = sig(A Ad ) {Q};2. leave nodes edges Paths(xQ , GTd ) remove nodes edges;3. xC GTd C 6 , Paths(xC , GTd ) (hyper) node N exists {P | xPN } leave xC , otherwise remove corresponding edges;4. xr GTd , r 6 , leave e = ({xr , y}, xC ) (xC , {xr , y}) exists GTd ,{xD , }, otherwise remove e.Let us illustrate notion support hypergraph following example:~Example 43 Let Example 39 accessed DL-atom = DL[A3 p3 ; D](X),Td = {A3p3 A3 }. support hypergraph Gsupp(d),T = sig(Td ) shown Figure 5a. node xD colored blue corresponds DL-query d. edge ({xD }, {xr4 , xA4 }), lie incoming path xD .Gsupp(d),T481fiE ITER , F INK & TEPANOVAdescribing approach extracting support sets DL-atom hypergraph,introduce notion tree-acyclicity. alternative definitions refer reader works,e.g. Ausiello, DAtri, Sacc (1986), Gallo, Longo, Pallottino (1993) ThakurTripathi (2009).Definition 44 (tree-acyclicity) hypergraph G = (V, E) called tree-acyclic, (i) onedirected path exists G singleton nodes x, V, (ii) G paths =e1 , . . . , ek tail (e1 ) head (ek ) 6= .refer hypergraphs tree-acyclic tree-cyclic.= {BExample 45 Gsupp(d),TFigure 5a tree-acyclic, G = Gsupp(d),T= {AA3 , B A2 } = {B} not, neither G = Gsupp(d),T,1C2 }.hypergraph Gsupp(d),T= DL[; Q](X), = {D C; C A; C B; B Q}= sig(T ) given Figure 5b tree-cyclic, since contains two paths xD xQ ,namely 1 = xD , xC , xA , {xA , xB }, xQ 2 = xD , xC , xB , {xA , xB }, xQ .support hypergraph Gsupp(d),T= (V, E) DL-atom = DL[; Q](X) containsincoming paths xQ start nodes corresponding predicates Ad construction,i.e. reflects inclusions Q right-hand side predicates Ad lefthand-side entailed Td . Hence, traversing edges incoming paths xQ ,construct sufficiently many query rewritings Q TBox Td corresponding nongroundsupport sets allow subsume every nonground support family w.r.t. O.support hypergraph given DL-atom tree-acyclic, support sets conveniently constructed annotating nodes variables Xi , N way describedhXbelow. use subscripts annotations, e.g. xC means node xC annotatedhX ,Xvariable Xi , xr j states xr annotated ordered pair variables Xi , Xj .approach proceeds follows. start node xQ , annotate X0 ,hXi.e. xQ 0 ; traverse hypergraph backwards, going head edge tail.every edge e encounter annotate tail (e) based form annotationhead (e), variable names occur annotation head (e) and/or fresh variable names Xi ,N, following way:(1) |tail (e)| = 1,hX(1.1) head (e) = {xC1 }, tail (e) annotated hXi i;hXi1 ,Xi2(1.2) head (e) = {xr1hXihXi, xC1 3 }, tail (e) = xC2 annotated hXi1 i, i.e.obtain xC2 1 ;hXi(2) |tail (e)| = 2 head (e) = {xC},hXhX(2.1) tail (e)={xC1 ,xC2 }, xC1 xC2 annotated Xi , i.e. {xC1 ,xC2 };hXi ,Xi1(2.2) tail (e)={xr1 , xC1 }, get {xr1482hXi, xC1 1 },fiC OMPUTING R EPAIRS NCONSISTENT DL-P ROGRAMS EL NTOLOGIESevery annotated hypernode N , one create set nonground atoms predicate namesextracted labels hypernodes variable names annotations. nongroundsupport sets = DL[; Q](X0 ) constructed incoming paths xQ .pick incoming path 1 xQ containing n edges, start traversingedge en head (en ) = {xQ }. first immediate support set S1 = {Q(X0 )}; nextone, S2 , extracted annotated tail en taking nonground predicates labelsvariables. pick edge ek head (ek ) tail (en ), obtain supportsets substituting nonground atoms correspond head (ek ) tail (en ) S2 atomsextracted tail (ek ); repeated. One fact construct incoming path backwardsalong support set extraction, maximal path obtained.Example 46 Consider maximal incoming path xD Gsupp(d),TFigure 5a:= (xA3 p3 , xA3 ), ({xr3 , xA3 }, xA1 ), ({xr1 , xA1 }, xC1 ), ({xr2 , xA2 }, xC2 ), ({xC1 , xC2 }, xD ).{z} |{z} |{z} |{z}|{z} |e1e2e3e4e5hX33iTraversing path backwards, i.e. edges order e5 , e4 , e3 , e2 , e1 , obtain: (xhXA3 p ,xA3 )e ,{z}1hXhXhXhXhX hXhXhXhX0 ,X20 ,X12 ,X3,xA1 2 },xC1 0 ) ,({xhX,xA2 1 },xC2 0 ) ,({xC1 0 ,xC2 0 },xD 0 ).({xhX,xA3 3 },{xA1 2 }) ,({xhXr1r2r3{z} |{z} |{z} |{z}||e2e3e43e5nonground support sets extracted resulting annotated path follows:S0 = {D(X0 )} immediately obtained head (xD );first incoming path consider 1 = e5 , get S1 = {C1 (X0 ), C2 (X0 )};next path 2 = e4 , e5 head (e4 ) tail (e5 ), yielding support set S2 = {C1 (X0 ),r2 (X0 , X1 ), A2 (X0 , X1 )};then, 3 = e3 , e5 get S3 = {C1 (X0 ), r1 (X0 , X2 ), A1 (X2 )};4 = e3 , e4 , e5 yields S4 = {r2 (X0 , X1 ), A2 (X1 ), r1 (X0 , X2 ), A1 (X2 )};5 = e2 , e3 , e5 , extract S5 = {r1 (X0 , X2 ), r3 (X2 , X3 ), A3 (X3 ), C2 (X0 )};6 = e2 , e3 , e4 , e5 yields S6 = {r1 (X0 , X2 ), r3 (X2 , X3 ), A3 (X3 ), r2 (X0 , X1 ), A2 (X1 )};7 = e1 , e2 , e3 , e5 , extract S7 = {r1 (X0 , X2 ), r3 (X2 , X3 ), A3p3 (X3 ), C2 (X0 )};finally, 8 = e1 , e2 , e3 , e4 , e5 get S8 = {r1 (X0 , X2 ), r3 (X2 , X3 ), A3p3 (X3 ),r2 (X0 , X1 ), A2 (X1 )}.following lemma formally asserts correctness procedure.Lemma 47 Let SG support family constructed tree-acyclic hypergraph G=Gsupp(d),T~ SG -complete w.r.t. O, i.e., SG every SuppO (d).= DL[; Q](X).particular, Lemma 47 holds complete w.r.t. ontology = hT , Ai. Thusdetermine sufficiently many nonground support sets looking support hypergraph. Note restriction tree-acyclic TBoxes crucial correctness procedureabove, ensures every node hypergraph annotated once.Lemma 47 allows us reason structure size support sets analyzingparameters support hypergraph. One parameter, instance, maximal numbern(, G) hyperedges singleton head node excluding ({xr , }, xA ), occurringincoming path xQ hypergraph G.483fiE ITER , F INK & TEPANOVAxQxLxExFxDxMxBxAxKxCFigure 6: Support hypergraph Gsupp(d),TExample 49Proposition 48 Let = hT , Ai EL ontology normal form, let =~ DL-atom tree-acyclic support hypergraph GDL[; Q](X).supp(d),Tmaxsup(d) maxGsupp(d),T(n(, Gsupp(d),T)) + 1.(6)tree-cyclic hypergraphs, bound tight, illustrate next.Example 49 Consider DL-atom d(X) = DL[; Q](X) accessing TBox Td :(4) E F L(1) F(2) C K(5) E K.Td =(3) B E(6) L Qsupport hypergraph depicted Figure 6, = sig(Td ). six hyperedges singleton head nodes, maximal support set size d(X) 4, e.g. ={A(X), B(X), D(X), K(X)}.next define out- in-degrees nodes hypergraph.Definition 50 (hyper-outdegree -indegree) Given directed hypergraph G = (V, E),hyper-outdegree denoted hd+ (x) (resp., hyper-indegree hd (x)) singleton node x Vnumber hyperedges e E tail (e) x (resp., head (e) x) either |tail (e)| = 2|head (e)| = 2. Similarly, outdegree d+ (x) (resp., indegree (x)) x numberedges e E tail (e) = {x} (resp., head (e) = {x}) |head (e)| = |tail (e)| = 1.Example 51 nodes X V\{xA3p , xD } hypergraph Gsupp(d),TFigure 5a hyper++outdegree 1, xAp3 xD hd (xAp3 ) = hd (xD ) = 0, moreover, d+ (xAp3 ) = 1.hyper-indegrees hd (xA3 ) = hd (xA1 ) = hd (xC1 ) = hd (xC2 ) = 1. graph({xC2 , xA2 }, xD ), holds hd+ (xC2 ) = hd+ (xA2 ) = hd (xD ) = 2,G = Gsupp(d),Tmoreover, (xA3 ) = 1.484fiC OMPUTING R EPAIRS NCONSISTENT DL-P ROGRAMS EL NTOLOGIESlet us definesmax (x, G) = maxPaths(x,G) (n(, G) m(, G) + 1),(7)Pm(, G) = xA (hdc+ (xA ) 1), hdc+ (xA ) number hyperedges form({xA , xB }, xC ) .Example 52 Consider Gsupp(d),TFigure 5a, Paths(xD , Gsupp(d),T) contains single maximal path xD , viz. = (xA3p3 , xA3 ), ({xr3 , xA3 }, xA1 ), ({xr2 , xA2 }, xC2 ), ({xr1 , xA1 }, xC1 ),({xC1 , xC2 }, xD ). n(, G) = 4, four hyperedges singleton head node,m(, G) = 0, nodes hyper-outdegree 1; hence smax (xQ , G) = 4 0 + 1 =5. hypergraph Figure 6 single maximal incoming path xQ , n(, G) = 6,m(, G) = (hdc+ (xA ) 1) + (hdc+ (xE ) 1) = 3; thus smax (xQ , G) = 6 3 + 1 = 4.generalize bound maximal support set size Proposition 48 usingparameter smax (xQ , G) node corresponding DL-query Q DL-atom d, obtainfollowing result hypergraphs possibly tree-cyclic:Proposition 53 Let = hT , Ai EL ontology normal form, let =~ DL-atom support hypergraph GDL[; Q](X)supp(d),T , role predicates. maxsup(d ) smax (xQ , Gsupp(d),T ).) = 4,Example 54 tree-cyclic hypergraph Figure 6 smax (xQ , Gsupp(d),T4 indeed maximal support set size = DL[; Q](X). hypergraph Figure 5a3 hyperedges, every node x V, hd+ (x) 1. Thus, smax (xQ , Gsupp(d),T) = 4,coincides maxsup(d ), = DL[A3 p3 ; Q](X).Note Proposition 53, take computing m(, G) outgoing hyperedgesform ({xC , xD }, xE ) account, C, D, E concepts, moreover, roles occur .Multiple outgoing hyperedges involving roles r r influence support set size.Example 55 Let support hypergraph = DL[; Q](X) hyperedges ({xr , xC }, xD ),({xC , xs }, xM ), ({xD , xM }, xQ ) r , reflecting axioms r.C D, s.CDQ. largest minimal support set S={r(X, ), C(Y ), s(X, Z), C(Z)}; sizen + 1, n number hyperedges singleton head node, hd+ (xC ) = 2.5.2 Number Support SetsOrthogonal question considered previous section conditions givennumber n support sets sufficient obtain -complete support family. problem tightlyrelated counting minimal solutions abduction problem, analyzed HermannPichler (2010) propositional theories various restrictions. particular, counting minimal explanations shown # coNP-complete general propositional theories#P -complete Horn propositional theories; EL subsumes propositional Horn logic, determining size smallest -complete support family least #P -hard thus intractable.Like size support sets, support hypergraph fruitfully exploited estimatingmaximal number support sets given DL-atom. provide estimate, traversesupport hypergraph forward starting leaves label every node xP numberrewritings P . conveniently compute labels, introduce support weight functions.485fiE ITER , F INK & TEPANOVADefinition 56 (support weight function) Let Gsupp(d),T= (V, E) support hypergraphDL-atom d. support weight function ws : V N assigns every node xA V numberws(xA ) rewritings w.r.t. .every node tree-acyclic support hypergraph, value ws conveniently computed recursive manner.Proposition 57 Let Gsupp(d),Ttree-acyclic support hypergraph DL-atom (normalized) ontology = hT , Ai. ws given follows, VC V set nodesconcepts:1, PQws(x) = 1 + (x) x ws(x )PP+ (x),T 6VC ({x },T )E ws(x ),hd (x) = 0 (x) = 0 x/ VC ,otherwise.(8)(x) = {T | (T, {x}) E}.demonstrate usage Proposition 57 following examples.Example 58 compute ws(x) nodes Gsupp(d),TFigure 5a, traverse graphleaves root, x {xr1 , xA2 , xC2 , xr2 , xA3p3 , xr3 } obtain ws(x) = 1; furthermore,ws(xA3 ) = ws(xC2 ) = 2, ws(xA1 ) = 3, ws(xC1 ) = 4. Finally, ws(xD ) = 1 + ws(xC1 )ws(xC2 ) = 1 + 4 2 = 9, number rewritings D(X) (and hence support setsd(X) = DL[A3 p3 ; D](X)) identified Example 46.Example 59 Consider TBox = {A B Q; C A; A; E A; F B; G B; HB; L} DL-atom = DL[; Q](X), whose support hypergraph = sig(T )Figure 7. ws(xQ ) = 1 + ws(xB ) ws(xA ) = 1 + 4 4 = 17, indeed 17rewritings Q(X), namely S1 = {A(X), B(X)}, S2 = {C(X), B(X)}, S3 = {D(X), B(X)},S4 = {E(X), B(X)}, S5 = {A(X), F (X)}, S6 = {A(X), G(X)}, S7 = {A(X), H(X)}, S8 ={C(X), F (X)}, S9 = {C(X), G(X)}, S10 = {C(X), H(X)}, S11 = {D(X), F (X)}, S12 ={D(X), G(X)}, S13 = {D(X), H(X)}, S14 = {E(X), F (X)}, S15 = {E(X), G(X)}, S16 ={E(X), H(X)}, S17 = {Q(X)}.immediate corollary Proposition 57, obtain= (V, E) tree-acyclic support hypergraph DL-atom =Corollary 60 Let Gsupp(d),T~ EL ontology = hT , Ai. edge e E satisfies |tail (e)|=|head (e)|=1,DL[; Q](X)Xws(tail (e)) + 1.(9)ws(v) =eE | head(e)=vThus query node xQ , get ws(xQ ) = |E| + 1. fact, Proposition 57 leadssimple bound size -minimal complete support families general cases.486fiC OMPUTING R EPAIRS NCONSISTENT DL-P ROGRAMS EL NTOLOGIESxCxDxExFxAxGxHxBxQFigure 7: Hypergraph Gsupp(d),T= (V, E) tree-acyclic support hypergraph DL-atomProposition 61 Let Gsupp(d),T~ EL ontology, every edge e = ({x, y}, z) E edges= DL[; Q](X)e1 , e2 E head (ei ) {x, y}, {1, 2}, holds head (e1 ) = head (e2 ).|SG| = |E| + 1.supp(d),TExample 62 hypergraph Gsupp(d),TFigure 5a single maximal path length 5,hyperedges satisfy condition Corollary 61. 6 support sets, |S| = |E| + 1 holds.condition Proposition 61 e e1 , e2 violated, maximal size minimal complete support family assessed easily. instance, support hyperFigure 7 contains 7 edges, 17 support sets. shown kgraph Gsupp(d),Tnodes Gsupp(d),T violate condition, SGcontains |E|k+1 + 1 support sets;supp(d),Tconsidered example, yields bound 72 + 1 = 50, far tight.note Proposition 57 applied tree-cyclic support hypergraphs.= DL[; Q](X), =Example 63 Consider tree-cyclic support hypergraph Gsupp(d),T{D C; C A; C B; B Q} = sig(T ), shown Figure 5b. UsingProposition 57 get ws(xD ) = 1, ws(xC ) = 2, ws(xA ) = 3, ws(xB ) = 3, ws(xQ ) =33+1 = 10. However, Q(X) 4 rewritings: (1) S1 = {Q(X)}, (2) S2 = {A(X), B(X)},(3) S3 = {C(X)}, (4) S4 = {D(X)}.Intuitively, tree-cyclic hypergraphs support weight function ws may also account nonminimal rewritings {B(X), C(X)}, {A(X), C(X)}, {A(X), D(X)}, {B(X), D(X)},rewritings counted multiple times. Thus general, ws(x) provides upper boundnumber rewriting. Likewise, bound Proposition 61 tight even simple treecyclic support hypergraphs; e.g., one DL-atom = DL[; Q](X) w.r.t. TBox Bi ,Bi Q, 1 n, contains 2 n edges, n + 2 support sets.6. Repair Computation Based Partial Support Familiessection, present algorithm SoundRAnsSet computing deletion repair answersets. shown Stepanova (2015), deciding whether given DL-program = hT A, PiEL ontology deletion repair answer set P2 -complete general case,membership part established guessing candidate repair ABox along candidate487fiE ITER , F INK & TEPANOVAanswer set = hT , Pi, suitability guess checked using NP oracle.Clearly efficient, |2n | candidate repair ABoxes n = |A|, even findinganswer set would cheap.restrict search space repairs approach work Eiter et al. (2014d)exploiting support families DL-atoms; however, contrast results Eiter et al. (2014d),support families required complete. families complete (which mayknown asserted construction), SoundRAnsSet guaranteed complete;otherwise, may miss repair answer set, easy extension ensures completeness.algorithm repair answer set computation, shown Algorithm 2, proceeds follows.start (a) computing family nonground support sets DL-atom.Next (b) so-called replacement program constructed.replacement program obtained simple rewriting gr(), DL-atomreplaced ordinary atom ed (called replacement atom), disjunctive choice ruleed ned added informally guesses truth value d, ed (respectively ned )stands value true (respectively false). repair answer set augmentedproper choice ed resp. ned answer set (Eiter et al., 2013, Proposition 13); thussearch confined answer sets , found using standard ASPsolver.(c) answer sets computed one one.determine (d) sets Dp (resp. Dn ) DL-atoms guessed true (resp.I,A) instantiates DL-atomsfalse) use function Gr(S, I,Dp Dn relevant ground support sets, i.e., compatible I.(e) loop minimal hitting sets H support sets DL-atoms Dnconsist ABox assertions, (f) construct H set Dp atomsDp least one support set disjoint H (thus removing Haffect values atoms Dp ).(g) evaluate postcheck atoms Dn Dp \Dp A\H w.r.t. I.Boolean flag rep stores evaluation result function eval n (resp. eval p ). specifically, given Dn (resp. Dp ), A\H, function eval n (resp. eval p ) returns true,atoms Dn (resp. Dp ) evaluate false (resp. true).\ H, P) succeeds, (h)rep true foundedness check flpFND(I,restriction I| original language output repair answer set.remark many cases, foundedness check might trivial superfluous (Eiter,Fink, Krennwallner, Redl, & Schller, 2014a), e.g., loops DL-atoms;consider weak answer sets (Eiter et al., 2013), entirely skipped.Example 64 Let DL-program Example 1 equivalence () axioms (2)(3) weakened , assertions Project(p1 ) BlacklistedStaffRequest(r1 )added ABox A. Moreover, assume d1 (r1 ) = DL[Project projfile; Staffrequest](r1 ),d2 (r1 )=DL[Staff chief ; BlacklistedStaffRequest](r1 ), d3 (r1 ,p1 )=DL[; hasTarget](r1 ,p1 )488fiC OMPUTING R EPAIRS NCONSISTENT DL-P ROGRAMS EL NTOLOGIESAlgorithm 2: SoundRAnsSet: compute deletion repair answer setsInput: =hT A, PiOutput: set repair answer sets(a) compute set nonground support families DL-atoms(b) construct replacement program(c) ()Dn {d | ned I};SIgrA);(d)Dp {d | ed I};Gr(S, I,(e)minimal hitting sets H Dn SIgr (d )(f)Dp {d Dp | SIgr (d) s.t. H = }A\H) evalp (Dp \Dp , I,A\H)(g)rep evaln (Dn , I,hT A\H, Pi) output I|(h)rep flpFND(I,endendd4 (r1 ,john) = DL[; hasSubject](r1 ,john). (b) following replacement programconstructed:(1) ed1 (r1 ) ned1 (r1 ); (2) ed2 (r1 ) ned2 (r1 ); (3) ed3 (r1 , p1 ) ned3 (r1 , p1 );(4)e(r1,john)ne(r1,john);(5)projfile(p1);(6)hasowner(p1,john);d4d4(7) chief (john) hasowner (p1 , john), projfile(p1 );=.(8) grant(r1 ) ed1 (r1 ), deny(r1 );(9) deny(r1 ) ed2 (r1 );(10) hasowner (p1 , john), grant(r1 ), ed3 (r1 , p1 ), ed4 (r1 , john).Suppose = {ed1 , ned2 , ed3 , ed4 , hasowner (p1 , john), projfile(p1 ), chief (john)} returned(c) following partial support families obtained (d):SIgr (d1 ) = {S1 , S2 }, S1 = {hasAction(r1 , read ), hasSubject(r1 , john), Action(read ),Staff (john), hasTarget(r1 , p1 ), Projectprojfile (p1 )} S2 = {StaffRequest(r1 )};SIgr (d2 ) = {S1 ,S2 }, S1 = {StaffRequest(r1 ),hasSubject(r1 , john),Blacklisted (john)}S2 = {BlacklistedStaffRequest(r1 )}.SIgr (d3 ) = {S1 }, S1 = {hasTarget(r1 , p1 )};SIgr (d4 ) = {S1 }, S1 = {hasSubject(r1 , john)}.(e) get hitting set H = {StaffRequest(r1 ), BlacklistedStaffRequest(r1 )}, disjointS1 , S1 S1 . Thus (f) obtain Dp = {d1 , d3 , d4 } (g) check whetherd2 false A\H. true, rep = false pick different hitting set H , e.g.{Blacklisted (john), BlacklistedStaffRequest(r1 )}. Proceeding H , get (g),H) = true flp-check succeeds (f), interpretation I|output.eval n (d2 , I,following results state algorithm works properly.489fiE ITER , F INK & TEPANOVATheorem 65 Algorithm SoundRAnsSet sound, i.e., given program = hT A, Pi, everyoutput deletion repair answer set .know addition support families complete, postchecks (g)redundant. Dp = Dp , set rep = true, otherwise rep = false.Theorem 66 Suppose input program = hT A, Pi Algorithm SoundRAnsSet,holds DL-atom support family computed Step (a) SoundRAnsSet-complete. Algorithm SoundRAnsSet complete, i.e., outputs every deletion repairanswer set .easily turn SoundRAnsSet complete algorithm, modifying (e) considerhitting sets, minimal ones. worst case, means fallback almost naivealgorithm (note hitting sets enumerated efficiently relative number).6.1 Optimizations ExtensionsResearch repairing databases (see work Bertossi, 2011, overview) suggests severaltechniques, potential interest DL-programs, could exploited optimizingextending repair approach. Localization repairs proposed Eiter, Fink, Greco,Lembo (2008) one technique, cautiously part data affected inconsistency identified search repairs narrowed part. Using localization,setting ontology ABox split safe set facts, touchedrepair, set facts (probably) affected. affected part repaired, resultcombined safe ABox part get final solution. find suitable ABox split, metaknowledge ontology (e.g. modules, additional domain information) used.Another common approach tackling inconsistency problem, proved effectivedatabases, decomposition (Eiter et al., 2008). Here, available knowledge decomposedparts, reasons inconsistency identified part separately,respective repairs conveniently merged. databases decomposition natural,general unclear inconsistent DL-program effectively decomposed. One wayapproach problem determining DL-atoms whose replacement atoms guessed true(resp. false) answer sets . Given set DL-atoms, one aim first searchingrepair every DL-atom desired value, extend solutionget final result. Modules DL-programs (as identified DLVHEX solver) alsoexploited program decomposition.repairs equally useful certain setting, various filterings repairs applied get plausible candidates. Here, qualitative domain-specific aspects repairscrucial importance practicability. formulated terms additional local constraints express instance facts involving certain predicates constants mustpreserved (resp. checked removal). Furthermore, number facts/predicates/constants allowed deletion bounded. filterings incorporated repair approach.Yet several extensions possible like conditional predicate dependence. example, user might willing express condition StaffRequest(r ) eliminatedhasAction(r , read ) holds data part, Blacklisted staff members removed,files, modifying separate StaffRequest issued non-blacklisted staffmember.490fiC OMPUTING R EPAIRS NCONSISTENT DL-P ROGRAMS EL NTOLOGIES~ P (Y~)(r1 ) Supd (X)A,P~~)(Y(r2 ) Supd (X)~ P (Y~)(r5 ) ned (X),~ ) . . . Pnd (Y~ ) ned (X),~ A,P (Y~)(r6 ) P1d (Y~ ) rb(S p (Y~ ))(r3 ) SdP (YA,P ~A,P ~~ ))(r4 ) Sd (Y ) rb(Sd (Y )), nd(SdA,P (Y~ ed (X),~ Cd , Supd (X)~(r7 ) eval (X)~~(r8 ) eval (X) ned (X), Cd~ Cd , Supd (X)~(r9 ) ed (X),Figure 8: Rules Rd declarative implementation6.2 Implementationimplemented repair approach C++ system prototype (dlliteplugin DLVHEXsystem, 2015).discussed, support sets EL ontologies rich structure, thus computation, TBox classification work Eiter et al. (2014d) insufficient. Indeed, needidentify inclusions atomic concepts, also inclusions form C B,C arbitrarily complex concept B atomic. constructing support sets thus exploit R EQUIEM tool (Prez-Urbina et al., 2010), rewrites target query TBoxusing datalog rewriting techniques. limiting number (resp. size) rewritings, partialsupport families computed.principle support sets may subsumed smaller support sets (e.g., {R(c, d),A(c)}{A(c)}). support sets redundant thus eliminate implementation.support families constructed, use declarative approach determining repairanswer sets, minimal hitting set computation accomplished rules. end,~ three fresh predicates (i) Supd (X),~ (ii) P (Y~ ), (iii) A,P (Y~ )DL-atom d(X)~ , intuitively say d(X)~ = XX~ (i) support set, (ii)introduced,support set involving rule predicates, (iii) support set involving ABox predicates (andpossibly rule predicates), called mixed support set. Furthermore, every DL-atom d(X), rulesRd Figure 8 added replacement program .~ knownrules, atom Cd informally says support family d(X)complete. Information completeness support families certain DL-atoms addeddeclarative program form facts Cd . rules (r1 )-(r4 ) reflect information~ potential repair; rb(S) stands rule body representing supportsupport sets d(X)~ ), . . . , pPnd (Y~ ),set S, i.e. rb(S) = A1 , . . . Ak = {A1 , . . . , Ak }; nd(S) = pP1d (Y~ ), . . . , pPnd (Y~ )}, encodes ontology part pP (Y~ ) states assertion{pP1d (Yid~~Pi (Y ) marked deletion. constraint (r5 ) forbids d(X), guessed false matching~ matchingsupport set consists input assertions; (r6 ) means instead d(X)mixed support set, assertion ontology part must eliminated. rule (r7 )~ guessed true, completeness support family unknown matchingsays d(X)~support set available, evaluation postcheck necessary (eval (X));rule (r8 ) similar~d(X) guessed false. rule (r9 ) states DL-atom guessed true must supportset, support family known complete.set facts f acts(A) = {pP (~c) | P (~c) A}, encoding ABox assertions COMP{Cd | Sd complete support family d} added program , answer sets491fiE ITER , F INK & TEPANOVA(1) projfile(p1 ); (2) hasowner (p1 , john); (3) issued (john, r1 );(4) chief (john) hasowner (p1 , john), projfile(p1 );(5) deny(r1 ) ed (r1 );(6) hasowner (p1 , john), issued (john, r1 ), deny(r1 );(7) ed (r1 ) ned (r1 );(8) supd (X ) pBlacklistedStaffRequest (X ), pBlacklistedStaffRequest (X);(9) pBlacklistedStaffRequest (X ) ned (X ), pBlacklistedStaffRequest (X );R = (10) supd (X ) pStaffRequest (X ), pStaffRequest (X ), phasSubject (X , ),phasSubject (X, ), pBlacklisted (Y ), pBlacklisted (Y );(11)p(X) phasSubject (X , ) pBlacklisted (Y ) ned (X), pBlacklisted (Y ),StaffRequestpStaffRequest (X ),phasSubject (X , );(12)eval(X)e(X),C,sup(X);(13) eval (X) ned (X), Cd ;(14) ed (X), Cd , supd (X).Figure 9: Program R Example 68proceed evaluation postcheck atomscomputed. answer set I,d(~c) fact eval (~c) answer set. evaluation postchecks succeed,original program I.way one identifiesextract repair answer set = I|weak repair answer sets; flp-repair answer sets, additional minimality check needed.many cases, however, flp weak answer sets coincide (cf. Eiter et al., 2014a); particular,holds example benchmark programs consider.formally show described approach indeed correctly computes weak repair answer sets.Proposition 67 Let = hO, Pi ground DL-program, EL ontology, letDL-atom Sd SuppO (d), let Rd set rules (r1 )-(r9 ) d. Define1 = R f acts(A) COMP,R = Rd , f acts(A) = {pP (~c) | P (~c) A} COMP {Cd | Sd -completew.r.t. O}. Suppose (1 ) evaluation postcheck succeeds every DL-atomRAS weak (). Moreover, Cd COMP every DL-atom d,Cd 6 COMP. I|RAS weak () = {I| | (1 )}.Let us demonstrate usage declarative implementation example.Example 68 Consider Figure 9 replacement program rules R = hP, Oi,Example 1, P follows:(1) projfile(p1 ); (2) hasowner (p1 , john); (3) issued (john, r1 );(4) chief (john) hasowner (p1 , john), projfile(p1 );P=(5) deny(r1 ) DL[Staff chief ; BlacklistedStaffRequest](r1 );(6) hasowner (p1 , john), issued (john, r1 ), deny(r1 ).492.fiC OMPUTING R EPAIRS NCONSISTENT DL-P ROGRAMS EL NTOLOGIESAssume d(X) = DL[Staff chief ; BlacklistedStaffRequest](X) givenincomplete support family Sd = {S1 , S2 }, S1 = {BlackListedStaffRequest(X )} S2 ={StaffRequest(X ), hasSubject(X , ), Blacklisted (Y )}. interpretation {ned (r1 ),pStaffRequest (r1 ), pBlacklisted (john), evald } among answer sets R facts(A).post-check needed d(r1 ); test succeeds, thus I|repair answer set.eval I,7. Evaluationrepair answer set computation approach implemented within DLVHEX system; detailsfound work Stepanova (2015), software freely online available (dlliteplugin, 2015). approach evaluated multi-core Linux server running DLVHEX 2.4.0HTCondor load distribution system (HTCondor, 2012), specialized workload management system compute-intensive tasks, using two cores (AMD 6176 SE CPUs) 8GBRAM.best knowledge, similar system repairing inconsistent DL-programs exists. list systems evaluating DL-programs includes DR E W system (DReW, 2012;Xiao, 2014) dlplugin DLVHEX system (dlplugin, 2007). DR E W system exploitsdatalog rewritings evaluating DL-programs EL ontologies; however, handle inconsistencies, focus work. Thus DR E W per se could used baselineexperiments. facilitate comparison, thus extended DR E W naive repairtechnique, guess repair ABox followed check suitability. However,immediate implementation turned infeasible even small instances, generalsearch space repairs ways large full exploitation; guided search needed ensurescalability. dlplugin DLVHEX system invokes R ACER P RO reasoner (RacerPro, 2007)back-end evaluating calls ontology. However, lightweight ontologies evenstandard evaluation mode without repair extensions, scales worse dlliteplugin (Eiteret al., 2014b); thus focus latter experiments.7.1 Evaluation Workflowgeneral workflow experimental evaluation follows. first step, constructed benchmarks building rules constraints top existing ontologiesdata parts constructed programs become inconsistent. instances generated usingshell scripts (DL-program benchmark generation scripts, 2015) size conflicting datapart parameter. benchmarks run using HTCondor system, timesextracted log files runs. run, measured time computing firstrepair answer set, including support set computation, timeout 300 seconds.benchmark, present experimental results tables. first column p specifiessize instance (varied according certain parameters specific benchmark),parentheses number generated instances. E.g., value 10(20) first column statesset 20 instances size 10 tested. columns represent particular repairconfigurations, grouped three sets.first set refers settings -complete support families exploited,second third refer settings size, respectively number computed sup493fiE ITER , F INK & TEPANOVAport sets restricted. -complete setting, addition limit number facts (lim_f ),predicates (lim_p) constants (lim_c) involved facts removed; e.g., lim_p = 2states set removed facts involve two predicates. parameter del _p storespredicates deleted; e.g., del _p = StaffRequest means repairs obtainedremoving facts StaffRequest.restricted configurations, column size = n (resp. num = n) statescomputed partial support families size (resp. number) support sets n; n = ,fact support sets computed, system aware -completeness.exploit partial -completeness number size restriction cases, i.e. support setsatom computed number/size limits yet reached, support familyconsidered atom -complete.entry t(m)[n], total average running time (including support set generationtimeouts), number timeouts n number found repair answer sets.7.2 Benchmarksevaluation developed algorithms, considered following benchmarks.(1) policy benchmark variant Example 1, rule (14) P changeddeny(X ) DL[Staff chief ; UnauthorizedStaffRequest](X), two axioms,namely UnauthorizedStaffRequest StaffRequest hasSubject.UnauthorizedBlacklisted Unauthorized added .(2) OpenStreetMap benchmark contains set rules ontology enhanced personalized route planning semantic information (MyITS ontology, 2012) extendedABox containing data OpenStreetMap project (OSM, 2012).(3) LUBM benchmark comprises rules top well-known LUBM ontology (LUBM,2005) EL.describe benchmark results details. experimental data online available(Experimental data, 2015).7.2.1 ACCESS P OLICY C ONTROLconsidered ABoxes n staff members, n {10, 250, 500}. data set 5projects 3 possible actions; furthermore 20% staff members unauthorized 40%blacklisted. generating instances, used probability p/100 (with p column 1)fact hasowner (pi , si ) added rules part P si , pi , Staff (si ), Project(pi )(i.e., instances vary facts hasowner (pi , si ) P.) parameter. Here, p ranges 20,30, etc. 90 A10 5, 10 etc. 40 A250 A500 . total average running timessettings shown Tables 24, SR stands StaffRequest. experimentsperformed ABoxes chosen size (i.e., A10 , A250 , A500 ) demonstrateapproach works small, medium large data.regards A10 , limiting -complete setting number predicates removal slightlyincreases running times. Restricting repairs removing facts StaffRequestslow repair computation compared unrestricted case, many actualrepairs indeed satisfy condition. results bounded number size support sets494fiC OMPUTING R EPAIRS NCONSISTENT DL-P ROGRAMS EL NTOLOGIESp20 (20)30 (20)40 (20)50 (20)60 (20)70 (20)80 (20)90 (19)-complete support familiesrestr .lim_p = 2del_p = SR1.92 (0)[20] 2.70 (0)[20]1.91 (0)[20]1.94 (0)[20] 2.72 (0)[20]1.94 (0)[20]1.93 (0)[20] 2.71 (0)[20]1.93 (0)[20]1.92 (0)[20] 2.70 (0)[20]1.92 (0)[20]1.94 (0)[20] 2.72 (0)[20]1.95 (0)[20]1.95 (0)[20] 2.73 (0)[20]1.95 (0)[20]1.94 (0)[20] 2.72 (0)[20]1.95 (0)[20]1.96 (0)[19] 2.74 (0)[19]1.96 (0)[19]Incomplete support familiessize = 3size = 5num = 338.51 (0)[20]33.86 (0)[20] 1.93 (0)[20]86.35 (1)[19]80.52 (1)[19] 1.95 (0)[20]98.69 (1)[19]96.45 (1)[19] 1.94 (0)[20]100.46 (2)[18]98.06 (2)[18] 1.93 (0)[20]182.16 (3)[17] 186.20 (3)[17] 1.96 (0)[20]153.66 (2)[18] 152.66 (2)[18] 1.96 (0)[20]227.81 (6)[14] 223.24 (6)[14] 1.96 (0)[20]267.52 (11)[8] 267.89 (12)[8] 1.96 (0)[19]num =1.92 (0)[20]1.93 (0)[20]1.93 (0)[20]1.91 (0)[20]1.94 (0)[20]1.94 (0)[20]1.95 (0)[20]1.95 (0)[19]Table 2: Policy benchmark, A10p5(20)10(20)15(20)20(20)25(20)30(20)35(20)40(20)-complete support familiesrestr .lim_p = 2del_p = SR6.06(0)[20]8.28 (0)[20]6.05 (0)[20]6.68(0)[20]8.90 (0)[20]6.68 (0)[20]8.37(0)[20] 10.56 (0)[20]8.35 (0)[20]9.39(0)[20] 11.61 (0)[20]9.40 (0)[20]11.41(0)[20] 13.62 (0)[20]11.41 (0)[20]14.04(0)[20] 16.24 (0)[20]14.09 (0)[20]15.17(0)[20] 17.32 (0)[20]15.19 (0)[20]17.49(0)[20] 19.64 (0)[20]17.47 (0)[20]Incomplete support familiessize = 6num = 36.06 (0)[20]6.07 (0)[20]6.67 (0)[20]6.69 (0)[20]8.33 (0)[20]8.34 (0)[20]9.40 (0)[20]9.43 (0)[20]11.46 (0)[20] 11.40 (0)[20]14.10 (0)[20] 14.05 (0)[20]15.12 (0)[20] 15.16 (0)[20]17.46 (0)[20] 17.45 (0)[20]num =6.05 (0)[20]6.67 (0)[20]8.34 (0)[20]9.41 (0)[20]11.40 (0)[20]14.04 (0)[20]15.17 (0)[20]17.43 (0)[20]Table 3: Policy benchmark, A250p5 (20)10 (20)15 (20)20 (20)25 (20)30 (20)35 (20)40 (20)-complete support familiesrestr .lim_p = 2del_p = SR14.99 (0)[20]18.71 (0)[20]14.98 (0)[20]23.57 (0)[20]27.14 (0)[20]23.52 (0)[20]35.07 (0)[20]38.85 (0)[20]35.09 (0)[20]73.43 (2)[18]53.27 (0)[20]73.29 (2)[18]152.29 (8)[12]64.91 (0)[20] 152.33 (8)[12]288.06 (19)[1]97.32 (1)[19] 288.08 (19)[1]300.00 (20)[0]153.03 (5)[15] 300.00 (20)[0]300.00 (20)[0] 206.96 (10)[10] 300.00 (20)[0]Incomplete support familiessize = 6num = 315.00 (0)[20]14.97 (0)[20]23.50 (0)[20]23.51 (0)[20]35.02 (0)[20]35.12 (0)[20]73.50 (2)[18]73.32 (2)[18]164.34 (9)[11] 152.25 (8)[12]276.11 (18)[2] 288.05 (19)[1]300.00 (20)[0] 300.00 (20)[0]300.00 (20)[0] 300.00 (20)[0]num =14.97 (0)[20]23.43 (0)[20]35.13 (0)[20]85.33 (3)[17]164.32 (9)[11]300.00 (20)[0]300.00 (20)[0]300.00 (20)[0]Table 4: Policy benchmark, A500almost constant, except size limited 5 smaller (just size 3 size 5 shown).support sets exceed bound post-evaluation checks often fail, visibly impactsrunning times. support sets large, them; seeninsignificant difference times num = 3 num = .significantly larger ABox A250 , get value p considered settingsperform almost identical except lim_p = 2 bit slower. Moreover, running times increasegracefully value p. bounding support set size 5 produces timeouts (thuscolumn omitted), computing support sets size 6 always sufficient identify repairs.largest setting A500 , -complete case finding arbitrary repair fasterrestriction lim_p = 2 , p = 15. p = 20 results lim_p = 2495fiE ITER , F INK & TEPANOVAp10 (20)20 (20)30 (20)40 (20)50 (20)60 (20)70 (20)80 (20)90 (20)-complete support familiesrestr .lim_f = 5lim_c = 1013.01 (0)[20]13.04 (0)[20]13.05 (0)[20]13.10 (0)[20]13.04 (0)[20]13.08 (0)[20]13.11 (0)[20]13.07 (0)[20]13.12 (0)[20]16.50 (0)[20]16.49 (0)[20]16.54 (0)[20]16.58 (0)[20]16.60 (0)[20]16.61 (0)[20]16.68 (0)[20]16.70 (0)[20]16.81 (0)[20]16.46 (0)[20]16.48 (0)[20]16.49 (0)[20]16.47 (0)[20]16.51 (0)[20]16.55 (0)[20]16.58 (0)[20]16.53 (0)[20]16.59 (0)[20]size = 116.39 (0)[11]20.98 (0)[5]24.56 (0)[0]59.26 (0)[1]123.80 (0)[0]106.63 (1)[0]139.08 (2)[0]211.33 (5)[0]260.36 (11)[0]Incomplete support familiessize = 3num = 113.03 (0)[20]13.04 (0)[20]13.06 (0)[20]13.07 (0)[20]13.10 (0)[20]13.06 (0)[20]13.07 (0)[20]13.06 (0)[20]13.10 (0)[20]13.23 (0)[20]13.35 (0)[20]13.51 (0)[20]13.55 (0)[20]13.56 (0)[20]13.60 (0)[20]13.61 (0)[20]13.61 (0)[20]13.67 (0)[20]num = 313.06 (0)[20]13.01 (0)[20]13.02 (0)[20]13.09 (0)[20]13.04 (0)[20]13.08 (0)[20]13.07 (0)[20]13.06 (0)[20]13.10 (0)[20]num =12.99 (0)[20]13.02 (0)[20]13.05 (0)[20]13.05 (0)[20]13.06 (0)[20]13.08 (0)[20]13.13 (0)[20]13.08 (0)[20]13.08 (0)[20]Table 5: Open Street Map benchmark resultsoutperform unrestricted setting, posed limitation restricts search space repairs effectively. Removing facts StaffRequest longer always sufficient, witnesseddecreased number identified repairs del _p = StaffRequest compared lim_p = 2 .time increases rather gracefully p long repair answer sets found.7.2.2 PEN TREET APsecond benchmark, added rules top ontology developed MyITS project.fixed ontology contains 4601 axioms, 406 axioms TBox 4195ABox. fragment relevant scenario rules P shown Figure 10.Intuitively, states building features located inside private areas publicly accessiblecovered bus stop bus stop roof. rules P check public stations lackpublic access, using CWA private areas.used method introduced Eiter, Schneider, imkus, Xiao (2014) extract dataOpenStreetMap repository (OSM, 2012). constructed ABox extractingsets bus stops (285) leisure areas (682) Irish city Cork, well isLocatedInsiderelations (9) (i.e., bus stops located leisure areas). data gatheredmany volunteers, chances inaccuracies may high (e.g. imprecise GPS data). Sincedata roofed bus stops private areas yet unavailable, randomly made 80%bus stops roofed 60% leisure areas private. Finally, added bsiisLocatedInside(bsi , laj ) fact busstop(bsi ) P probability p/100. instancesinconsistent since data set roofed bus stops located inside private areas.results shown Table 5. -complete setting arbitrary repairs computed3.5 seconds faster repairs bounded changes. restricted configurationtimes vary much except size = 1, significant time increase observed,repairs found smaller instances. Like previous benchmark computing smallnumber support sets often sufficient, configuration num = 1 expected slightlyslower num = 3 (computing support sets cheap, postchecks take time).496fiC OMPUTING R EPAIRS NCONSISTENT DL-P ROGRAMS EL NTOLOGIES=(1) BuildingFeature isLocatedInside.Private NoPublicAccess(2) BusStop Roofed CoveredBusStop(9) publicstation(X) DL[BusStop busstop; CoveredBusStop](X),DL[; Private](X);P=(10)DL[BuildingFeaturepublicstation; NoPublicAccess](X),publicstation(X ).Figure 10: DL-program OpenStreetMap ontologyp5 (20)15 (20)25 (20)35 (20)45 (20)55 (20)65 (20)75 (20)85 (20)95 (20)restr .37.14 (0)[20]35.74 (0)[20]35.71 (0)[20]36.07 (0)[20]35.98 (0)[20]35.92 (0)[20]36.13 (0)[20]36.07 (0)[20]36.11 (0)[20]36.38 (0)[20]-complete support familieslim_f = 5lim_p = 247.77 (0)[20] 43.74 (0)[20]34.93 (0)[11] 42.74 (0)[20]26.94 (0)[5] 42.80 (0)[20]20.53 (0)[0] 43.04 (0)[20]20.50 (0)[0] 43.11 (0)[20]20.51 (0)[0] 43.11 (0)[20]20.43 (0)[0] 43.44 (0)[20]20.63 (0)[0] 43.45 (0)[20]20.30 (0)[0] 43.35 (0)[20]20.55 (0)[0] 43.24 (0)[20]lim_c = 2043.88 (0)[20]41.51 (0)[19]41.71 (0)[19]26.91 (0)[7]19.54 (0)[1]18.47 (0)[0]18.33 (0)[0]18.28 (0)[0]18.04 (0)[0]18.20 (0)[0]Incomplete support familiessize = 1size = 342.57 (0)[20] 36.52 (0)[20]42.02 (0)[20] 35.96 (0)[20]41.91 (0)[20] 35.80 (0)[20]42.22 (0)[20] 36.00 (0)[20]41.94 (0)[20] 36.40 (0)[20]42.31 (0)[20] 35.98 (0)[20]41.81 (0)[20] 36.02 (0)[20]42.09 (0)[20] 36.21 (0)[20]42.22 (0)[20] 36.15 (0)[20]42.52 (0)[20] 36.17 (0)[20]num =36.26 (0)[20]35.49 (0)[20]35.49 (0)[20]35.65 (0)[20]35.66 (0)[20]35.60 (0)[20]35.92 (0)[20]35.85 (0)[20]35.83 (0)[20]35.62 (0)[20]Table 6: LUBM benchmark results7.2.3 LUBMalso tested approach DL-programs = hP, Oi built EL versionLUBM ontology, whose TBox extended following axioms:(1) GraduateStudent assists.Lecturer TA(2) GraduateStudent teaches.UndergraduateStudent TArules follows:(3) stud (X ) DL[; Employee](X ), DL[; TA](X );;P=(4) DL[Student stud ; TAof ](X , ), takesexam(X , )(3) states unless teaching assistant (TA) known employee, he/she student,(4) forbids teaching assistants take exams courses teach.ABox contains information one university 600 students, 29 teachingassistants, constructed dedicated ABox generator (LUBM data generator, 2013). pairsconstants t, c, teachingAssistantOf (t, c) A, facts takesexam(t, c) randomlyadded rules part probability p/100, thus contradicting part DL-programgrowing respect p.results benchmark provided Table 6. Bounding -complete settingnumber removed facts 5 slows computation, repairs satisfying conditionexist. instances p 35 (i.e., inconsistency entrenched), 5 facts mustdropped obtain repair; moreover, often involve 20 constants according497fiE ITER , F INK & TEPANOVAcolumn 5. absence repairs lim_f = 5 lim_c = 20 found faster repairunrestricted mode.Limiting support set size 1 allows one find repairs instances delay less10 seconds compared -complete setting. However, many support setsbenchmark, thus bounding number less effective.7.3 General Results DiscussionOne observe -complete settings settings post-evaluation checks fast,running times vary slightly growing p. due declarative implementation,computing repairs reduced finding answer sets program 1 = R facts(A)COMP followed possible evaluation postchecks. benchmarks differenceinstances size pi pi+1 data part logic program, small comparedpart facts(A) 1 constant p. Thus long postchecks needed, timesrequired repairing differ much even though programs become inconsistent.expected, using -complete support families works well practice. Naturally, takestime compute restricted repairs rather arbitrary repairs; however, imposed restrictions strong repair satisfy them, solver may recognize faster.reported Hansen et al. (2014), EL-TBoxes originate real-world applicationsadmit FO-rewritings (of reasonable size) almost cases. provides evidence realworld EL-TBoxes hardly contain involving constraints conceptual level, hence eithersize number support sets DL-atoms often turn limited. novel algorithmsdeletion repair answer set computation demonstrated applicability DL-programsreal world data (Open Street Map benchmark results Table 5).benchmarks run synthetic, still vary w.r.t. TBoxABox sizes. capability algorithms handling diverse DL-programs confirmspotential approach.8. Related WorkInconsistencies DL-programs studied several works (Phrer et al., 2010; Fink, 2012;Eiter et al., 2013, 2014d). Phrer et al. proposed inconsistency tolerant semantics. Keepingontology untouched, DL-atoms introduce inconsistency well rules involvingdeactivated. repair problem, outlined open issue Phrer et al., formalized Eiteret al. (2013), notions repair repair answer sets together naive algorithmcomputation proposed. latter optimized Eiter et al. (2014d, 2015)DL-Lite effectively exploiting complete support families DL-atoms. approachgeneral, differs one Eiter et al. (2014d, 2015) uses partial (notnecessarily complete) support families applied ontologies DL, thoughpossible impact complexity.hybrid formalisms, inconsistency management concentrated inconsistency tolerance rather repair. instance, Huang et al. (2013) presented four-valued paraconsistentsemantics based Belnaps logic (Belnap, 1977) hybrid MKNF knowledge bases (Motik &Rosati, 2010), prominent tightly coupled combination rules ontologies.Inspired paracoherent stable semantics Sakama Inoue (1995), work Huanget al. (2013) extended Huang, Hao, Luo (2014) handle also incoherent MKNF KBs,498fiC OMPUTING R EPAIRS NCONSISTENT DL-P ROGRAMS EL NTOLOGIESi.e. programs inconsistency arises result dependency atom defaultnegation analogy work Fink (2012). Another direction inconsistency handlinghybrid MKNF KBs using three-valued (well-founded) semantics Knorr, Alferes, Hitzler (2011), avoids incoherence disjunction-free stratified programs. recently,extended Kaminski et al. (2015) additional truth values evaluate contradictorypieces knowledge. works aim inconsistency tolerance rather repair, gearedspirit query answering inherent well-founded semantics; such, limitednormal logic programs, DL-programs allow disjunctive rule heads.context Description Logics, repairing ontologies studied intensively, foremosthandle inconsistency. DL-program repair related ABox cleaning (Masotti, Rosati, &Ruzzi, 2011; Rosati, Ruzzi, Graziosi, & Masotti, 2012). However, latter differs variousrespects: aims restoring consistency inconsistent ontology deleting -minimal setsassertions (i.e., computing -maximal deletion repairs); deal inconsistency incurredtop consistent ontology, arbitrary (non-monotonic) rules access queryinterface. Furthermore, must consider multiple ABoxes (via updates), use EL insteadDL-Lite. Refining algorithm compute -maximal deletion repairs possible.problem computing support families tightly related finding solutions abductionproblem, considered Bienvenu (2008) theories expressed EL-terminologies.hypothesis H = {A1 , . . . , } set atomic concepts, observation another atomicconcept. solution abduction problem set H, |= Ai Ai O.setting general involves also roles along atomic concepts. Abductionstudied various related areas e.g., DL-Lite ontologies Calvanese, Ortiz, Simkus,Stefanoni (2013), propositional logic Eiter Makino (2007), datalog Eiter et al.(1997) Gottlob, Pichler, Wei (2007), etc. Using incomplete support families DL-atomsrelated spirit approximate inconsistency-tolerant reasoning DLs using restricted supportsets considered Bienvenu Rosati (2013); however, focus repair computationmodel generation Bienvenu Rosati target inference repairs.methods constructing partial support families exploit results logical differenceEL terminologies presented Konev et al. (2012) Ecke et al. (2013); recentlyextended ELHR Ludwig Walther (2014) general TBoxes Feng et al.(2015).Repairing inconsistent non-monotonic logic programs investigated workSakama Inoue (2003), approach deleting rules based extended abductionstudied; however, restore consistency addition rules also possible. latter consideredBalduccini Gelfond (2003), Occams razor consistency-restoring rules mayadded. Methods explaining inconsistency arises logic program studied, e.g.,Syrjnen (2006), exploited model-based diagnosis Reiter (1987) debug logic program. Generalized debugging logic programs investigated e.g., Gebser, Phrer, Schaub,Tompits (2008). recently, Schulz, Satoh, Toni (2015) considered characterizationreasons inconsistency extended logic programs (i.e., disjunction-free logic programsstrong (classical) negation weak negation) terms culprit sets literals, basedwell-founded maximal partial stable model semantics, derivation-based method explain culprits described; however, remains open debugging logic programsbased culprit sets could done whether could fruitfully extended debugging DLprograms. latter addressed Oetsch, Phrer, Tompits (2012) relatedF499fiE ITER , F INK & TEPANOVAchallenging but, best knowledge, unexplored problem repairing rule partDL-program.9. Conclusionconsidered computing repair answer sets DL-programs EL ontologies,generalized support set approach Eiter et al. (2014d, 2014b) DL-Lite workincomplete families supports sets; advance needed since EL complete support families large even infinite. discussed generate support sets, exploiting queryrewriting ontologies datalog (Lutz et al., 2009; Rosati, 2007; Stefanoni et al., 2012),contrast work Eiter et al. (2014d), TBox classification invoked. Moreover,developed alternative techniques effective computation partial support families.approach approximate relevant part TBox DL-Lite core exploiting notion logicaldifference EL-terminologies, compute complete support families approximated TBox using methods Eiter et al. (2014d). obtained support family complete,approximated TBox logically equivalent original one.estimate maximal size support sets, analyzed properties novel support hypergraph, corresponds subgraph ontology hypergraph (Nortje et al., 2013;Ecke et al., 2013), nodes encode ontology predicates (or pairs them), (hyper) edgesreflect TBox inclusions. shown traversing support hypergraph one convenientlycompute upper bound number support sets given DL-atom. If, addition,support hypergraph satisfies certain conditions (e.g. tree-acyclicity), exact estimateobtained.developed sound algorithm computing deletion repair answer sets DL-programsEL ontologies, complete case support families also known complete.algorithm trades answer completeness scalability (a simple variant ensures completeness).implemented novel algorithm using declarative means within system prototype,invokes R EQUIEM reasoner partial support family computation. experimental assessment repair approach, set novel benchmarks constructed including realworld data. availability complete support families adds scalability repaircomputation, partial support families work surprisingly well practice due structurebenchmark instances: support sets either small them, thus postevaluation checks cause much overhead. Overall, experimental evaluation revealedpromising potential novel repair methodology practical applications.9.1 Outlookdirections future work considered area manifold. cover theoreticalpractical aspects inconsistency handling approach. theoretical side, relevant openissue sufficient conditions computing nonground support sets DL-atomaccessing EL ontology becomes tractable. Like work Gebser et al. (2008) boundedtree-width might considered, also parameters like density support hypergraphvarious acyclicity properties. Analyzing complexity counting support sets completesupport family might give hints possible restricted settings, support family computationefficient, complexity analysis also interesting problem such. practical500fiC OMPUTING R EPAIRS NCONSISTENT DL-P ROGRAMS EL NTOLOGIESside, optimization current implementation extending range applications real usecases another issue.Repair may intermingled stepping techniques used debugging DL-programs (Oetschet al., 2012). considered DL-programs monolithic structures applying repairtechniques, repair computation performed DL-program taken whole.interesting relevant quest extend approach dealing modular DL-programs.Splitting program separate components individually evaluated well-knownprogramming technique, studied context DL-programs (Eiter et al., 2008).clear, however, extent program classes repair methodsadapted modular setting.considered EL paper, basic algorithm approach applicable alsoDLs. Extensions work EL+ EL++ easily possible. main differencenegation, expressible via concept; ontology get inconsistentupdates DL-atoms, leading increased number support sets need effectivelycomputed appropriately handled. extension expressive DLs SHIQ, SHOINeven SROIQ challenging, efficient methods support set construction remaindeveloped; relatively high complexity DLs, comes computational cost.hand, computation may done (even offline) reused; fortunately,support families need complete, may expect return investment time supportset construction overall running time.Orthogonal DLs, one study various additional repair possibilities, e.g. boundedaddition; overview repair possibilities see work Eiter et al. (2013).concentrated repairing data part ontology, also natural allow changesrules interfaces. repairing rules, works ASP debugging Frhstck, Phrer,Friedrich (2013), Gebser et al. (2008), Syrjnen (2006) used starting point,problem challenging search space possible changes large. Priorities rulesatoms involving might applied ensure high quality rule repairs. interfacessimilarly admit numerous modifications, makes type repair difficult; user interactionprobably required.Last least one could develop methods repairing hybrid formalisms includingtightly-coupled hybrid KBs even general representations like HEX-programs (Eiter et al.,2005), instead ontology arbitrary sources computation accessed logicprogram. Heterogeneity external sources HEX-programs makes repair paraconsistentreasoning challenging task.Acknowledgmentsthank anonymous reviewers detailed constructive suggestions helpedimprove work. article significantly extends preliminary work Eiter, Fink, Stepanova(2014c). research supported Austrian Science Fund (FWF) projects P24090P27730.501fiE ITER , F INK & TEPANOVAAppendix A. Proofs Section 3A.1 Proof Proposition 15() Proposition 10, |=O iff Td AI |= Q(~t), AI = {Pp (~t) Ad | p(~t) I}.Thus, = AI support set w.r.t. O, coherent construction.() Supp (d) coherent I, form = AIAI AI , thus AI . Td |= Q(~t), monotonicity Td AI |= Q(~t),hence Proposition 10 |=O d.A.2 Proof Proposition 24~Consider instance = {P1 (Y1 ), . . . , Pk (Yk )} set form (5) d(X),: V C. show support set w.r.t. OC = hT , AC (recall AC set~possible ABox assertions C), i.e., AC Ad (which clearly holds) Td |= Q(X).~latter equivalent Tdnorm |= Q(X),turn Lemma 23 equivalent0~Prog Q,Td norm |= Q(X). Let Prog = Prog Q,Td norm , let Prog i+1 , 0,~program results Prog unfolding rule w.r.t. target query Q(X).i+1~~Prog|= Q(X) iff Prog |= Q(X) holds. construction S, rule~ thus Prog |= Q(X).~r form (4) Prog . Clearly {r} |= Q(X)0~~~follows Prog |= Q(X) hence Td norm |= Q(X) Td |= Q(X).Appendix B. Proofs Section 4B.1 Proof Lemma 31Towards contradiction, assume T1d 6CT2d . w.l.o.g. T1d |= P1 P2 T2d 6|= P1 P2 ,P1 , P2 . Observe differ predicates Pp , P p occurs, = T1d \T1 = T2d \T2 consists axioms Pp P Pp occurT1 T2 . first show P2 must hold. Indeed, otherwise P2 \ thus P2 =Pp sig(Ad ) P p . let = {P1 (c)} P1 , = {P1 (c), P1p (c)}otherwise (i.e., P1 \ ), arbitrary c I. T1d model cI P1I(resp. cI P1I cI P1 Ip ) Pp = (thus P1 6= P2 ), EL negation-free Pp occursaxioms left. 6|= P1 Pp , follows T1d 6|= P1 P2 , contradiction.proves P2 \ . two cases.(i) P1 : T1 CT2 implies T2 |= P1 P2 ; monotonicity T2d |= P1 P2 , contradiction.(ii) P1 \ : P1 = Pp , P p occurs , P . claim T1 |= P P2 .Indeed, otherwise T1 model P 6 P2 . easily seen interpretationcoincides Pp = P \ P2 Pp = Pp \ modelT1 ; however, 6|= Pp P2 , would contradiction. proves claim.claim T1 CT2 , follows T2 |= P P2 monotonicity T2d |= P P2 .Pp P T2 , follows T2 |= P1 P2 ; contradiction.B.2 Proof Proposition 32Suppose S1 complete nonground support family w.r.t. O1 let instanceS1 ; = Ad AC Ad . Lemma 31, T1d CT2d ; thus Theorem 30, T1d502fiC OMPUTING R EPAIRS NCONSISTENT DL-P ROGRAMS EL NTOLOGIEST2 well. definition -instance inseparability, -ABoxes -assertionsT1d |= , holds T2d |= ; hence T2d Ad |= Q(~c). Consequently,= Ad (ground) support set w.r.t. O2 . S2 complete nonground support familyw.r.t. O2 , follows instance S2 . converse membership symmetric.Hence, S1 S2 ground-identical.B.3 Proof Proposition 34Towards contradiction, assume \ SuppO (d) exists. grounding existsTd 6|= d(X). However, Td |= d(X), according (f), nongroundsupport set w.r.t. Td = Td lrw . Consequently, Td 6|= Td , contradiction,Td Td construction (c) lrw = {C | Td |= C , Td 6|= C } Tdlhs(d) definition cWTnrhscWTn .Appendix C. Proofs Section 5C.1 Proof Lemma 47w.r.t. ontology =construction support sets given hypergraph Gsupp(d),ThT , Ai presented mimics DL-query unfolding TBox Td . formallyshow (i) set extracted described way indeed nonground support sets d,(ii) ground instance nonground support set d, (nonground) supportset constructed following procedure suitable groundsubstitution . proves SG holds.first prove (i) induction length n incoming paths, support setsextracted.Base: n=1. Consider path hypergraph Gsupp(d),T. Assume single(hyper-) edge e . construction, hyperedge must xQ head node, i.e. head (e) =xQ . four possibilities: (1) tail (e) = {xC }, (2) tail (e) = {xr , xC }, (3) tail (e) ={xC , xD } (4) tail (e) = {xr , }. annotate nodes path variables describedabove, extract nonground atoms labels annotations nodes. resultcase (1) obtain {C(X0 )}, (2): {r(X0 , X1 ), C(X1 )}, (3): {C(X0 ), D(X0 )},(4): {r(X0 , X1 )}, X1 fresh variable. construction hypergraph edgesforms (1)-(4) correspond TBox axioms C Q, r.C Q, C Q r. Qrespectively. Therefore, sets constructed considered cases reflectDL-query unfoldings d, hence represent nonground support sets Proposition 24.Induction step: Suppose statement true n, i.e. path n edges sets extracted way described nonground support sets d. Consider path = e0 , . . . , enn+1 edges, let e = e0 first edge . induction hypothesis, sets extractedpath \ e = e1 , . . . , en following approach support sets d. severalpossibilities form e: (1) tail (e) = {xC } head (e) = {xD }, (2) tail (e) = {xr , xC }head (e) = {xD }, (3) tail (e) = {xC , xD } head (e) = {xB }, (4) tail (e) = {xr , }head (e) = {xC }, (5) tail (e) = {xC } head (e) = {xr , xD }.(1), construction xC xD annotated Xi . Let family setsextracted \e. pick set C(Xi ) occurs. substitute C(Xi ) D(Xi ),obtain set . induction hypothesis must support set d. However,503fiE ITER , F INK & TEPANOVAclearly also support set, mimics additional unfolding step accounts ruleC(X) D(X) datalog rewriting Td .Let us look (2). Assume set D(Xi ) nonground atoms constructed usingprocedure. Xi must annotation xD . According construction {xr , xD }annotated {hXi , Xj i, hXj i}, Xj fresh variable. sets get resultsubstituting D(Xi ) {r(Xi , Xj ), C(Xj )}. latter mimics unfolding stepQ accounts rule D(Xi ) r(Xi , Xj ), C(Xj ) rewriting Td . supportset induction hypothesis, must support set well. cases (3)-(5)analyzed analogously. Thus sets size n + 1 extracted support sets d.remains prove (ii). Towards contradiction, assume ground instanceSuppO (d) exists, ground instance every SuppO (d)constructed procedure 6 S. support set, definition Td norm|= Q(~c), thus Lemma 23 Prog Q,Tdnorm |= Q(~c). turn means Q(~c)~ 0 Sm = ,backchaining proof S0 , S1 , . . . , Sm Prog Q,Td norm form S0 = Q(X)~ 7 ~c, Si = (Si1 Hi + Bi )i , 1, Hi Bi0 substitution Xrule resp. fact Prog Q,Td norm general unifier Hi atom Si1 .Without loss generality, Hi = A2 (oA2 ) Hi1 = R2 (X, oA2 ) Biempty end, i.e. positions k, k + 1, . . . , m. Sj resp. Sj+1 , 0 j kamounts instance support set Sj resp. Sj+1generated Gsupp(d),T. particular,Sk1 instance Sk1 consequently {Hk , Hk+1 , . . . , Hm } ( S) instance Sk1well. means instance = Sk1 , contradiction.C.2 Proof Proposition 48prove statement induction number n hyperedges singleton head nodeG = Gsupp(d),TDL-atom DL[; Q](X).Base: n = 0. show maxsup(d ) = 1 hyperedges required form exist G.Several cases possible: (i) G contains hyperedges form (xC , {xr , xD }); (ii) Ghyperedges form ({xr , }, xC ) (xC , {xr , }); (iii) G hyperedges.(i) Consider hyperedge . ej must exist , head (ei ) tail (ej ).latter implies ej form ({xr , xD }, xD ) n 6= 0, i.e. contradiction.(ii) (iii), construction contains GCIs C C, either atomicform r.. axioms fall DL-Lite core fragment, -minimalsupport sets size 2; moreover, |S| = 2 reflects DL-Lite core inconsistency arisingupdated ontology (Eiter et al., 2014d). negation available expressible EL,exists thus maximal support set size 1.Induction Step: Suppose statement true n; prove n+1. Let = e1 , . . . , ekmaximal number n+1 hyperedges singletonincoming path xQ Gsupp(d),Thead node. Assume ei first hyperedge required form occurring . Let ussplit two parts: e1 , . . . , ei ei+1 , . . . , ek . Consider hypergraph G = (V, E ),E = E \ {e1 , . . . , ei }, TBox reconstructed it. induction hypothesis,maxsup(d ) w.r.t. = hT , Ai bounded n + 1. let hypergraph G = (V, E )E = E {ei } correspond TBox . assumption head (ei ) = xA , i.e. ei eitherreflects B C r.B A. Two cases possible: either = Q 6= Q.504fiC OMPUTING R EPAIRS NCONSISTENT DL-P ROGRAMS EL NTOLOGIESx C n1x Cnx C nkxCn...xCn1...k......xQFigure 11: Fragment hypergraph used illustration proof Proposition 53former case, ei single hyperedge , i.e. n = 1. Support sets obtained rewriting QB C Q r.B Q size 2. support sets constructed combiningquery rewritings predicates occurring left hand side GCIs reflected ei ;rewritings size 1 shown base case. Thus overall support set size w.r.t.bounded 2 n + 1.Suppose 6= Q, i.e. ei reflects either B C r.B A. definitionincoming path (hyper) edge ej must exist, head (ei ) tail (ej ). Moreover, note ejunique (hyper) edge connected ei , otherwise given hypergraph tree-cyclic, i.e.contradiction. distinguish two cases: (1) head (ei ) = tail (ej ) ej corresponds . . . ;(2) head (ei ) tail (ej ) ej reflects B . . . .1. Consider maximal support set w.r.t. , suppose A(Y ) holds. inductionhypothesis |S| n. G = Gsupp(d),Ttree-acyclic, single atom mightoccur S. Adding edge ei G obtain support set atom A(Y )substituted atoms B(Y ) C(Y ), r(Z, ) B(Z) result additionalquery unfolding step. Hence support set size bounded n + 2.2. ej reflects B . . . , support set {A(Y ), B(Y )} must exist. unfoldingrespective datalog rule, get bound n + 2 support set w.r.t. .C.3 Proof Sketch Proposition 53Observe tree-acyclic hypergraphs nodes hyper out-degree 1, hencem(, G) = 0. Thus, G tree-acyclic, Proposition 48 support set size givenDL-atom bounded n(, G) 0 + 1, equals smax . show claimed boundalso correct tree-cyclic hypergraphs. Intuitively, m(, G) must subtracted n(, G)avoid certain atoms support set counted multiple times. Regarding structuresupport hypergraph distinguish two cases: (i) roles appear hypergraph; (ii) xr G,holds r 6 .505fiE ITER , F INK & TEPANOVAFirst consider (i). Since concepts appear support hypergraph assumption, support sets contain atoms single variable X0 occurs. Considernode xCn hdc+ (xCn ) = k, k > 1, i.e., k outgoing hyperedgesxCn containing nodes corresponding concepts: ({xCn1 , xCn }, xCn ). . .({xCnk ,xCn }, xCn )1k(see Figure 11). support sets {Cn 1 (X0 ), . . . , Cn k (X0 )} get support sets{Cn1 (X0 ), . . . , Cnk (X0 ), Cn (X0 )}. Estimating maximal support set size number hyperedges hypergraph, Cn (X0 ) counted k times, appears (as variableguaranteed X0 ). avoid multiple countings, m(, G) must subtracted n(, G).Consider (ii). construction G, every hypernode {xr , xC } edges e1 =(xA , {xr , xC }) e2 = ({xr , xC }, xB ) exist G. Thus xr occurs , considersupport set {B(X)}. Rewriting TBox axiom reflected e2 , get datalog ruleB(X) r(X, ), C(Y ). axiom r.C reflected e1 rewritten datalog rulesr(X, oC ) A(X); C(oC ) A(X). Unifying oC obtain unfolding A(X).essentially shows role occurring support hypergraph , support setsinvolve single variable; case, shown (i), provided bound correct.C.4 Proof Proposition 57proof induction number n (hyper) edges G = Gsupp(d),T. Base: n=0. G(hyper) edges, node one support set.Induction step: Suppose statement holds n; show holds G n + 1 (hyper)edges. Obviously, holds x VR . G tree-acyclic normal form, Gnode x hd+ (x) = d+ (x) = 0, i.e., outgoing (hyper) edges, hd (x) 6= 0(x) 6= 0, i.e., incoming (hyper) edge. G tree-acyclic, rewritingset Qx = {A(X)}, x = xA consists Qx rewritings sets Qtail(e)(hyper) nodes tail (e) head (e) = x. tail (e) {xB } (resp., {xB , xC }, {xr , xC })rewritings {B(X)} (resp. {B(X), C(X)}, {R(X, ), C(Y )}). is, ws(xA )sum number rewritings Qtail(e) denoted Qtail(e) , plus 1. Considerarbitrary e head (e) = xA let G = G\e. G n edges tree-acyclic,induction hypothesis node x V G , value ws(x), denoted wsG (x), (8).Furthermore, ws(Qtail(e) ) ws(x ), x 6= xA G G. thus get x = xA :wsG (x) = wsG (x) + ws(Qtail(e) )XwsG (x ) += 1+= 1+(x)xXwsG (x ) +XwsG (x ) +(x) xXws(x ) + ws(Qtail(e) )XXws(x ) + ws(Qtail(e) )(x),T 6VC ({x },T )E(x),T 6VC ({x },T )E(x) x= 1+XXXws(x )(x),T 6VC ({x },T )E(x) = {T | (T, {x}) E } E = E \ {e}, (x) above. obtainws(Qtail (e)), simply need count combinations rewritings node tail (e),case tail (e) = {xr , xB } (where ws(xr ) = 1), need add number rewritingstail hyperedge (T, {xr , xB }) (as normal form, must form {xC }).506fiC OMPUTING R EPAIRS NCONSISTENT DL-P ROGRAMS EL NTOLOGIESC.5 Proof Corollary 60Qimmediate Proposition 57: hypothesis, (8) form {y} VC ;thus x ws(x ) = ws(y), i.e., ws(tail (e)) rightmost term 0.C.6 Proof Sketch Proposition 61condition e e1 , e2 , every set (x) Equation (8)Q |T| = {x, y} > 1contains (at least) one element, say x, ws(x) = 1, thus x wsG (x ) equals ws(y). inductive argument, obtain every node xA VC ,G = Gsupp(d),Tws(xA ) 1 number distinct edges G occur incoming paths xAxB VC edge ({xB }, {xr , xA }) E, plus number edges.turn implies query node xQ , ws(xq ) = |E| + 1 holds, construction edge e Eamong respective edges xQ . result follows immediately.Appendix D. Proofs Section 6D.1 Proof Theorem 65. get (h) answer set ,Suppose SupRAnsSet outputs = I|foundedness check w.r.t. ontology , = A\H succeeded. thusremains show compatible set , i.e., DL-atom , Dpiff |=O Dn iff 6|=O d. Towards contradiction, suppose case.(d) partitioned DL-atoms two sets: Dp Dn , corresponding DL-atoms guessedrespectively, set SIgrA). Since assumetrue false I,Gr(S, I,compatible, one following must hold:(1) DL-atom Dn , |=O d. two possibilities: (i) eithersupport set SIgr (d) (ii) support sets identified. case (i), guaranteedsupport sets 6= , since otherwise hitting sets H found(e). Hence must exist support set =6 . According (e) H 6=thus 6 Suppd (O ). rep = true (h), post-check must succeeded (g),i.e. 6|=O must hold. contradiction. case (ii), likewise post-evaluation mustsucceeded (h), raises contradiction.(2) DL-atom Dp , 6|=O d. Hence SIgr (d) = , 6 Dp , post-evaluationperformed (g). latter, however, must succeeded, rep = true (h);contradiction. Hence compatible set , thus deletion repair answer set .D.2 Proof Theorem 66following lemmas useful prove Theorem 66.Lemma 69 Let ASx () x {flp, weak } = hT , A, Pi ground DL-program.= {ed | DL , |=T d} {ned | DL , 6|=T d} answer set ,DL set DL-atoms occurring .lemma follows general result compatible sets basis evaluationapproach HEX-programs DLVHEX-solver (cf. (Eiter et al., 2014a)).507fiE ITER , F INK & TEPANOVALemma 70 Let = hT , A, Pi ground DL-program let () = I|ASx (), x {flp, weak }. Suppose DL-atom occurring P,holds |=T iff |=T d. ASx ( ) = hT , , Pi., PxI,T PxI,T coincide; ASx (), minimalProof. note = I|model PxI,T . Consequently, also model PxI,T . Moreover, minimal,J satisfies PxI,T , J |= PxI,T ; hence answer set PxI,T , contradiction.Suppose RAS x (). implies ASx ( ) = hT , Pi,A. Lemma 69 answer set thus considered (c). (d), DpDn set (correct) guess |=O DL-atom d, = .)(d) 6=Proposition 15 -completeness S, obtain Dp Gr(S, I,)(d) = . Gr(S, I,)(d) Gr(S, I,A)(d) holdsDn Gr(S, I,DL-atom d, follows Dn Sgr (d) (A \ ) 6= ; meansH = \ hitting set Dn SIgr (d ), hence minimal hitting set H Hconsidered (e). (f), Dp set Dp Dp SIgr (d) existsH = , hence H = . Thus (g) call eval p ( ) yields true,\ H)(d) = ; thus rep true. Eventually, (h)likewise call eval n ( ) Gr(S, I,test flpFND(I, hT A\H, Pi) succeed, x-answer set = hT , Pi,output.Lemma 70 also = hT \ H, Pi, \ H. Thus step (h) = I|D.3 Proof Proposition 67RAS weak (). Towards contradiction,first show every (1 ), holds I|suppose (1 ) exists I| 6 RAS weak (). every6 weak ( ) = hT , , Pi. particular, = A\{P (~c) | pP (~c) I}I|6 weak ( ) = hT , , Pi several possibilities: (i)holds I|c ; (ii)guess replacement atoms ed , ned modelextension I|,Ocompatible set ; (iii) interpretation J I|model P I|.extension I|weakc hence follows I|c .|=case (i) impossible: =. Towards contradiction, assumeAssume (ii) true. Consider interpretation I|,|=O ned I|compatible . DL-atom either (1) I|holds. case (1), I||=d, ed I|d, support set(2) 6|=exists. consider whether Sd 6 Sd . former case, mustcoherent I|contain ABox assertions SdA , otherwise constraint form (r5 ) violated. Duerule (r6 ) least one assertion Pid SdA must marked deletion. Note Pidpresent , relevant support set w.r.t. . Sd known complete,immediately arrive contradiction. Otherwise, rule form (r8 ) applied,evaluation postcheck succeeded assumption, get contradiction. 6 Sd ,Sd known complete, rule form (r8 ) applied; due successful6|=O d,evaluation postcheck, contradiction obtained. suppose (2) true. I|. Sd known complete,support set exists w.r.t. coherent I|508fiC OMPUTING R EPAIRS NCONSISTENT DL-P ROGRAMS EL NTOLOGIESconstraint (r9 ) violated; contradicts |= 1 . Thus, body rule (r7 ) satisfied,evaluation postcheck issued fails; hence get contradiction.,Omodel P I|Finally, assume (iii) holds, i.e. interpretation J I|weak .\J contains atoms signature . Let us consider IM = \ .set = I|know (1 ); Hence rule rI must exist 1 IM |= B(rI ),glglglIM 6|=Recall 1 = ( R f acts(A) COMP). rgl,O,O,OI|P I|P I|( f acts(A) COMP)Igl , rgliff rglweakweak J 6|= Pweak constructionmust RI . However,GL weak reducts, contradiction. Therefore, rglgllatter also raises contradiction: rule Rgl atoms signature head IMcoincide rule head; thus follows 6|= B(RIgl ), contradiction. Therefore,( ) holds, global contradiction, i.e. I|RAS weak () follows.I|).H(rglconsider case support family Sd known complete, proveAS| (1 ) = RAS weak (). shown above, remains checkAS| (1 ) RAS weak (). Towards contradiction, assume RAS weak () exists/ (1 ) every extension I. RAS weak (), ABox exists( ) = hT , , Pi. construct extension follows:= {ed | |=O d} {ned | 6|=O d}{pP (~c) | P (~c) A\A } f acts(A) COMP{Supd (~c) | d(~c) support set Sd coherent I}{S P (~c) | |= rb(S A,P (~c))} {S A,P (~c) | |= rb(S A,P (~c)), nd(S A,P (~c))}.Since assumption 6 (1 ), one following must hold:(i) |6 = ( R f acts(A) COMP)Igl(ii) J exists, J |= ( R f acts(A) COMP)Igl .satisfies rules forms (r )-(r ).First assume (i) true. construction I,14Moreover, constraints form (r5 ) violated, DL-atom d(~c) 6|=O d(~c)support set consists input assertions. rules (r7 ) (r8 ) presentreduct 1 Igl , |= Cd DL-atom d(~c).Thus rule r 1 6|= rI could form (r ) (r ). casegl69form (r6 ), DL-atom d(~c) would exist 6|=O d(~c). Proposition 15 support setHence, r mustd(~c) would exist coherent I, construction SdA,P (~c)/ I.form (r9 ); however, |=O d(~c) completeness Sd Proposition 15, constructionimplies r violated.Supd (~c) I,let (ii) hold, i.e. J exists s.t. J |= 1 Igl . J contains DL-atomd(~c) exactly one ed (~c) ned (~c) 1 contains ed (~c) ned (~c), interpretations Jcoincide replacement atoms ed (~c) ned (~c). Suppose \ J contains atoms6|= P I,O ; hence rule rI,O P I,O , existslanguage . J|weakweakweak6|= H(rI,O ). Consider respective rule rI . J 6|= H(rJ ),|= B(rI,O ), J|J|glglglweakweak509fiE ITER , F INK & TEPANOVA). construction weak GL reduct, respectively, positivemust J 6|= B(rglJ ) B(r I,O ) same. Hence, replacement atom e (~normal atoms B(rglc) (resp.weakned (~c)) must occur positively B(rgl ), ed (~c) \ J (resp. ned (~c) \ J).already argued, latter possible, leading contradiction.Consequently, \ J must contain atoms language R. every rule rglform (r ) (r ) J |= B(rI ) iff |= B(rI ), thus J agree atoms P (~c)3A,PSd (~c).4glglSimilarly, via (r1 ) (r2 ) must J agree atoms Supd (~c). Finally,conclusion,holds pP (~c) pP (~c) rules (r6 ) construction I.J = holds, violates (ii).Thus, follows (1 ). Consequently, (1 ) RAS weak (1 ) holds; provesresult.ReferencesAlchourrn, C. E., Grdenfors, P., & Makinson, D. (1985). logic theory change: Partialmeet contraction revision functions. J. Symbolic Logic, 50(2), 510530.Aranguren, M. E., Bechhofer, S., Lord, P. W., Sattler, U., & Stevens, R. D. (2007). Understandingusing meaning statements bio-ontology: recasting gene ontology OWL.BMC Bioinformatics, 8(1), 113.Ausiello, G., DAtri, A., & Sacc, D. (1983). Graph algorithms functional dependency manipulation. J. ACM, 30(4), 752766.Ausiello, G., DAtri, A., & Sacc, D. (1986). Minimal representation directed hypergraphs. SIAMJ. Computing, 15(2), 418431.Baader, F., Bauer, A., & Lippmann, M. (2009). Runtime verification using temporal descriptionlogic. Proc. 7th Intl Symp. Frontiers Combining Systems, FroCoS 2009, pp. 149164.Baader, F., Brandt, S., & Lutz, C. (2005). Pushing EL envelope. Proc. 19th Intl Joint Conf.Artificial Intelligence, IJCAI 2005, pp. 364369.Baader, F., Calvanese, D., McGuinness, D., Nardi, D., & Patel-Schneider, P. F. (Eds.). (2003).Description Logic Handbook: Theory, Implementation Applications. Cambridge University Press, 2003.Baader, F., Lutz, C., Milicic, M., Sattler, U., & Wolter, F. (2005). Integrating description logicsaction formalisms: First results. Proc. 20th National Conf. Artificial Intelligence 17thConf. Innovative Applications Artificial Intelligence, pp. 572577.Balduccini, M., & Gelfond, M. (2003). Logic programs consistency-restoring rules. IntlSymp. Logical Formalization Commonsense Reasoning, AAAI 2003 Spring Symposium Series, pp. 918.Belnap, N. (1977). useful four-valued logic. Modern Uses Multiple-Valued Logic, pp. 737.Reidel Publishing Company, Boston.Bertossi, L. E. (2011). Database Repairing Consistent Query Answering. Morgan & ClaypoolPublishers, Ottawa, Canada.510fiC OMPUTING R EPAIRS NCONSISTENT DL-P ROGRAMS EL NTOLOGIESBertossi, L. E., Hunter, A., & Schaub, T. (2005). Introduction inconsistency tolerance. Inconsistency Tolerance [result Dagstuhl seminar], pp. 114.Bienvenu, M. (2008). Complexity abduction EL family lightweight description logics.Proc. 11th Intl Conf. Principles Knowledge Representation Reasoning, KR 2008,pp. 220230.Bienvenu, M., & Rosati, R. (2013). New inconsistency-tolerant semantics robust ontology-baseddata access. Proc. 26th Intl Workshop Description Logics, pp. 5364.Bonatti, P. A., Faella, M., & Sauro, L. (2010). EL default attributes overriding. Proceedings 9th Intl Semantic Web Conf., ISWC 2010, pp. 6479.Brewka, G. (1989). Preferred subtheories: extended logical framework default reasoning.Proc. 11th Intl Joint Conf. Artificial Intelligence, IJCAI 1989, pp. 10431048.Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., Poggi, A., & Rosati, R. (2007a).Ontology-based database access. Proc. 15th Italian Symposium Advanced DatabaseSystems, SEBD 2007, pp. 324331.Calvanese, D., De Giacomo, G., Lenzerini, M., Lembo, D., Poggi, A., & Rosati, R. (2007b).MASTRO-I: efficient integration relational data DL ontologies. Proc. 20thIntl Workshop Description Logics.Calvanese, D., Ortiz, M., Simkus, M., & Stefanoni, G. (2013). Reasoning explanationsnegative query answers DL-Lite. J. Artificial Intelligence Research, 48, 635669.Console, L., Sapino, M. L., & Dupr, D. T. (1995). role abduction database view updating.J. Intelligent Information Systems, 4(3), 261280.Console, M., Mora, J., Rosati, R., Santarelli, V., & Savo, D. F. (2014). Effective computationmaximal sound approximations description logic ontologies. Proc. 13th Intl SemanticWeb Conf., ISWC 2014, Part II, pp. 164179.dlliteplugin DLVHEX system (2015). https://github.com/hexhex/dlliteplugin.Scripts DL-program benchmark generation (2015).dlliteplugin/benchmarks.https://github.com/hexhex/dlplugin DLVHEX system (2015). https://github.com/hexhex/dlplugin.DR E W reasoner DL-Programs Datalog-rewritable Description Logics (2012). http://www.kr.tuwien.ac.at/research/systems/drew/.Ecke, A., Ludwig, M., & Walther, D. (2013). concept difference EL-terminologies usinghypergraphs. Proc. Intl Workshop Document Changes: Modeling, Detection, StorageVisualization.Eiter, T., Erdem, E., Fink, M., & Senko, J. (2005). Updating action domain descriptions. Proc.19th Intl Joint Conf. Artificial Intelligence, IJCAI 2005, pp. 418423.Eiter, T., Fink, M., Greco, G., & Lembo, D. (2008). Repair localization query answeringinconsistent databases. ACM Transactions Database Systems, 33(2).Eiter, T., Fink, M., Krennwallner, T., Redl, C., & Schller, P. (2014a). Efficient HEX-programevaluation based unfounded sets. J. Artificial Intelligence Research, 49, 269321.511fiE ITER , F INK & TEPANOVAEiter, T., Fink, M., Redl, C., & Stepanova, D. (2014b). Exploiting support sets answer setprograms external evaluations. Proc. 28th Conf. Artificial Intelligence, AAAI 2014,pp. 10411048.Eiter, T., Fink, M., & Stepanova, D. (2013). Data repair inconsistent DL-programs. Proc. 23rdIntl Joint Conf. Artificial Intelligence, IJCAI 2013, pp. 869876.Eiter, T., Fink, M., & Stepanova, D. (2014c). Computing repairs inconsistent DL-programsEL ontologies. Proc. 14th Joint European Conf. Logics Artificial Intelligence, JELIA2014, pp. 426441.Eiter, T., Fink, M., & Stepanova, D. (2014d). Towards practical deletion repair inconsistentDL-programs. Proc. 21st European Conf. Artificial Intelligence, ECAI 2014, pp. 285290.Eiter, T., Fink, M., & Stepanova, D. (2014d). Data repair inconsistent DL-programs. Tech. rep.INFSYS RR-1843-15-03, Institut f. Informationssysteme, TU Wien, A-1040 Vienna, Austria.Eiter, T., Gottlob, G., & Leone, N. (1997). Abduction logic programs: Semantics complexity. Theoretical Computer Science, 189(1-2), 129177.Eiter, T., Ianni, G., Lukasiewicz, T., Schindlauer, R., & Tompits, H. (2008). Combining answerset programming description logics Semantic Web. J. Artificial Intelligence,172(12-13), 14951539.Eiter, T., Ianni, G., Schindlauer, R., & Tompits, H. (2005). uniform integration higher-orderreasoning external evaluations answer-set programming. Proc. 19th Intl Joint Conf.Artificial Intelligence, IJCAI 2005, pp. 9096.Eiter, T., & Makino, K. (2007). computing abductive explanations propositional Horntheory. J. ACM, 54(5).Eiter, T., Schneider, P., imkus, M., & Xiao, G. (2014). Using OpenStreetMap data create benchmarks description logic reasoners. Proc. 2nd Intl Workshop OWL Reasoner Evaluation, ORE 2014, Vol. 1207, pp. 5157.Experimental data inconsistent DL-programs (2015). http://www.kr.tuwien.ac.at/staff/dasha/jair_el/benchmark_instances.zip.Feng, S., Ludwig, M., & Walther, D. (2015). logical difference EL: terminologiestowards tboxes. Proc. 1st Intl Workshop Sem. Technologies, IWOST 2015, pp. 3141.Fink, M. (2012). Paraconsistent hybrid theories. Proc. 13th Intl Conf. Principles Knowledge Representation Reasoning, KR 2012, pp. 141151.Frhstck, M., Phrer, J., & Friedrich, G. (2013). Debugging answer-set programs ouroboros extending sealion plugin. Proc. 12th Intl Conf. Logic Programming NonmonotonicReasoning, LPNMR 2013, pp. 323328.Gallo, G., Longo, G., & Pallottino, S. (1993). Directed hypergraphs applications. DiscreteApplied Mathematics, 42(2), 177201.Grdenfors, P., & Rott, H. (1995). Belief revision. Handbook Logic Artificial IntelligenceLogic Programming, 4, 35132.Gardiner, T., Tsarkov, D., & Horrocks, I. (2006). Framework automated comparisondescription logic reasoners. Proc. 5th Intl Semantic Web Conf., ISWC 2006, pp. 654667.512fiC OMPUTING R EPAIRS NCONSISTENT DL-P ROGRAMS EL NTOLOGIESGebser, M., Phrer, J., Schaub, T., & Tompits, H. (2008). meta-programming techniquedebugging answer-set programs. Proc. 23rd Conf. Artificial Intelligence, AAAI 2008, pp.448453.Gelfond, M., & Lifschitz, V. (1991). Classical negation logic programs disjunctive databases.New Generation Computing, 9, 365385.Gottlob, G., Pichler, R., & Wei, F. (2007). Efficient datalog abduction bounded treewidth.Proc. 22nd Intl Conf. Artificial Intelligence, AAAI 2007, pp. 16261631.Grau, B. C., Horrocks, I., Kazakov, Y., & Sattler, U. (2007). right amount: extractingmodules ontologies. Proc. 16th Intl Conf. World Wide Web, WWW 2007, pp. 717726.Hansen, P., Lutz, C., Seylan, I., & Wolter, F. (2014). Query rewriting EL TBoxes: Efficientalgorithms. Proc. 27th Intl Workshop Description Logics, pp. 197208.Hermann, M., & Pichler, R. (2010). Counting complexity propositional abduction. J. ComputerSystem Sciences, 76(7), 634649.HTCondor load distribution system, version 7.8.7 (2012). http://research.cs.wisc.edu/htcondor/.Huang, S., Hao, J., & Luo, D. (2014). Incoherency problems combination description logicsrules. J. Applied Mathematics, 604753:16.Huang, S., Li, Q., & Hitzler, P. (2013). Reasoning inconsistencies hybrid MKNF knowledgebases. Logic J. IGPL, 21(2), 263290.Kaminski, T., Knorr, M., & Leite, J. (2015). Efficient paraconsistent reasoning ontologiesrules. Proc. 24th Intl Joint Conf. Artificial Intelligence, IJCAI 2015, pp. 30983105.Knorr, M., Alferes, J. J., & Hitzler, P. (2008). coherent well-founded model hybrid MKNFknowledge bases. Proc. 18th European Conf. Artificial Intelligence, ECAI 2008, pp.99103.Knorr, M., Alferes, J. J., & Hitzler, P. (2011). Local closed world reasoning description logicswell-founded semantics. Artificial Intelligence, 175(9-10), 15281554.Konev, B., Ludwig, M., Walther, D., & Wolter, F. (2012). logical difference lightweightdescription logic EL. J. Artificial Intelligence Research, 44, 633708.Kontchakov, R., Lutz, C., Toman, D., Wolter, F., & Zakharyaschev, M. (2010). combinedapproach query answering DL-Lite. Proc. 12th Intl Conf. Principles KnowledgeRepresentation, KR 2010, pp. 247257.Kotek, T., Simkus, M., Veith, H., & Zuleger, F. (2014). Towards description logic programanalysis: Extending ALCQIO reachability. Proc. 27th Intl Workshop DescriptionLogics, pp. 591594.Lembo, D., Lenzerini, M., Rosati, R., Ruzzi, M., Savo, D. F. (2015). Inconsistency-tolerant queryanswering ontology-based data access. J. Web Sem., 33, 329.Lembo, D., Santarelli, V., & Savo, D. F. (2013). graph-based approach classifying OWL 2 QLontologies. Proc. 26th Intl Workshop Description Logics, pp. 747759.LUBM benchmark (2005). http://swat.cse.lehigh.edu/projects/lubm/.513fiE ITER , F INK & TEPANOVALUBM data generator (2013). http://code.google.com/p/combo-obda/.Ludwig, M., & Walther, D. (2014). logical difference ELHr-terminologies using hypergraphs. Proc. 21st European Conf. Artifical Intelligence, ECAI 2014, pp. 555560.Lukasiewicz, T. (2010). novel combination answer set programming description logicssemantic web. IEEE Trans. Knowledge Data Engineering, 22(11), 15771592.Lutz, C., Toman, D., & Wolter, F. (2009). Conjunctive query answering description logic ELusing relational database system. Boutilier, C. (Ed.), Proc. 21st Joint Intl Conf. ArtificialIntelligence, IJCAI 2009, pp. 20702075.Lutz, C., Walther, D., & Wolter, F. (2007). Conservative extensions expressive description logics.Proc. 20th Intl Joint Conf. Artificial Intelligence, IJCAI 2007, pp. 453458.Lutz, C., & Wolter, F. (2010). Deciding inseparability conservative extensions descriptionlogic EL. J. Symbolic Computation, 45(2), 194228.Martinez, M. V., Molinaro, C., Subrahmanian, V. S., & Amgoud, L. (2013). General FrameworkReasoning Inconsistency. Springer Briefs Computer Science. Springer, 2013.Masotti, G., Rosati, R., & Ruzzi, M. (2011). Practical abox cleaning DL-Lite (progress report).Proc. Description Logics Workshop.Motik, B., & Rosati, R. (2010). Reconciling Description Logics Rules. J. ACM, 57(5),162.MyITS - Personalized Intelligent Mobility Service (2012). http://www.kr.tuwien.ac.at/research/projects/myits/GeoConceptsMyITS-v0.9-Lite.owl/.Nguyen, N. T. (2008). Advanced Methods Inconsistent Knowledge Management. AdvancedInformation Knowledge Processing. Springer.Nortje, R., Britz, A., & Meyer, T. (2013). Module-theoretic properties reachability modulesSRIQ. Proc. 26th Intl Workshop Description Logics, pp. 868884.Oetsch, J., Phrer, J., & Tompits, H. (2012). Stepwise debugging description-logic programs.J. Correct Reasoning, pp. 492508.Open Street Map project (2012). http://www.openstreetmap.org/.zccep, . L., & Mller, R. (2012). Combining DL Lite spatial calculi feasible geothematic query answering. Proc. 25th Intl Workshop Description Logics.Pan, J. Z., & Thomas, E. (2007). Approximating OWL-DL ontologies. Proc. 22nd Intl Conf.Artificial Intelligence, AAAI 2007, pp. 14341439.Prez-Urbina, H., Motik, B., & Horrocks, I. (2010). Tractable query answering rewritingdescription logic constraints. J. Applied Logic, 8(2), 186209.Phrer, J., Heymans, S., & Eiter, T. (2010). Dealing inconsistency combining ontologiesrules using DL-programs . Proc. 7th Extended Semantic Web Conf., ESWC 2010, partI, pp. 183197.R ACER P RO reasoner OWL ontologies (2007). http://franz.com/agraph/racer/.Reiter, R. (1987). theory diagnosis first principles. J. Artificial Intelligence, 32(1), 5795.514fiC OMPUTING R EPAIRS NCONSISTENT DL-P ROGRAMS EL NTOLOGIESRosati, R. (2007). conjunctive query answering EL. proceedings 20th Intl WorkshopDescription Logics.Rosati, R., Ruzzi, M., Graziosi, M., & Masotti, G. (2012). Evaluation techniques inconsistency handling OWL 2 QL ontologies. Proc. 11th Intl Semantic Web Conf., ISWC 2012,pp. 337349.Sakama, C., & Inoue, K. (1995). Paraconsistent stable semantics extended disjunctive programs.J. Logic Computation, 5(3), 265285.Sakama, C., & Inoue, K. (2003). abductive framework computing knowledge base updates.Theory Practice Logic Programming, 3(6), 671713.Schulz, C., Satoh, K., & Toni, F. (2015). Characterising explaining inconsistency logicprograms. Proc. 13th Intl Conf., LPNMR 2015, pp. 467479.Schulz, S., Cornet, R., & Spackman, K. A. (2011). Consolidating SNOMED CTs ontologicalcommitment. Applied Ontology, 6(1), 111.Shen, Y.-D. (2011). Well-supported semantics description logic programs. Proc. 22nd IntlJoint Conf. Artificial Intelligence, IJCAI 2011, pp. 10811086.Stefanoni, G., Motik, B., & Horrocks, I. (2012). Small datalog query rewritings EL. Proc.25th Intl Workshop Description Logics.Stepanova, D. (2015). Inconsistencies Hybrid Knowledge Bases. PhD thesis, Vienna UniversityTechnology.Steve, G., Gangemi, A., & Mori, A. R. (1995). Modelling sharable medical concept system:Ontological foundation galen. AIME, pp. 411412.Stuckenschmidt, H., Parent, C., & Spaccapietra, S. (Eds.). (2009). Modular Ontologies: Concepts,Theories Techniques Knowledge Modularization, Vol. 5445 Lecture Notes Computer Science. Springer.Syrjnen, T. (2006). Debugging Inconsistent Answer-Set Programs. Proc. 11th Intl WorkshopNonmonotonic Reasoning, NMR 2006, pp. 7783.Thakur, M., & Tripathi, R. (2009). Linear connectivity problems directed hypergraphs. Theoretical Computer Science, 410(27-29), 25922618.Tserendorj, T., Rudolph, S., Krtzsch, M., & Hitzler, P. (2008). Approximate OWL-reasoningScreech. Proc. 2nd Intl Conf. Web Reasoning Rule Systems, RR 2008, pp. 165180.Wache, H., Groot, P., & Stuckenschmidt, H. (2005). Scalable instance retrieval semantic webapproximation. Proc. 1st Intl Workshops Web Information Systems Engineering,WISE 2005, pp. 245254.Wang, Y., You, J.-H., Yuan, L.-Y., & Shen, Y.-D. (2010). Loop formulas description logicprograms. Theory Practice Logic Programming, 10(4-6), 531545.Xiao, G. (2014). Inline Evaluation Hybrid Knowledge Bases. Ph.D. thesis, Vienna UniversityTechnology, Austria.Zhao, Y., Pan, J. Z., & Ren, Y. (2009). Implementing evaluating rule-based approachquerying regular EL+ ontologies. Proc. 9th Intl Conf. Hybrid Intelligent Systems,2009, pp. 493498.515fiJournal Artificial Intelligence Research 56 (2016) 1-59Submitted 06/15; published 05/16Query Predicate Emptiness Ontology-Based Data AccessFranz BaaderFRANZ . BAADER @ TU - DRESDEN . DETU Dresden, GermanyMeghyn BienvenuMEGHYN @ LIRMM . FRCNRS, Universite de Montpellier& INRIA, FranceCarsten LutzCLU @ UNI - BREMEN . DEUniversity Bremen, GermanyFrank WolterWOLTER @ LIVERPOOL . AC . UKDepartment Computer ScienceUniversity Liverpool, UKAbstractontology-based data access (OBDA), database querying enriched ontologyprovides domain knowledge additional vocabulary query formulation. identify queryemptiness predicate emptiness two central reasoning services context. Query emptiness asks whether given query empty answer databases formulated givenvocabulary. Predicate emptiness defined analogously, quantifies universally queriescontain given predicate. paper, determine computational complexity queryemptiness predicate emptiness EL, DL-Lite, ALC-families description logics,investigate connection ontology modules, perform practical case study evaluatenew reasoning services.1. Introductionrecent years, paradigm ontology-based data access (OBDA) gained increased popularity. general idea add ontology database querying provide domain knowledgeenrich vocabulary available formulation queries. particularlyuseful data queried highly incomplete multiple data sources diverging vocabularies integrated (Poggi, Lembo, Calvanese, De Giacomo, Lenzerini, & Rosati,2008). OBDA taken particular verve area description logic (DL)studied intensively lightweight DLs members DL-Lite ELfamilies, tractable regarding data complexity querying, expressiveDLs ALC SHIQ families querying intractable data complexity. use OBDA former, see example work Calvanese, De Giacomo,Lembo, Lenzerini, Poggi, Rodriguez-Muro, Rosati (2009), Lutz, Toman, Wolter (2009),Perez-Urbina, Motik, Horrocks (2009), Chortaras, Trivela, Stamou (2011) Eiter, Ortiz,Simkus, Tran, Xiao (2012), surveys Krotzsch (2012) Kontchakov, RodriguezMuro, Zakharyaschev (2013); latter, see work Glimm, Lutz, Horrocks, Sattler(2008), Ortiz, Calvanese, Eiter (2008), Bienvenu, ten Cate, Lutz, Wolter (2014) wellreferences given work Ortiz Simkus (2012).c2016AI Access Foundation. rights reserved.fiBAADER , B IENVENU , L UTZ , & W OLTEROBDA approach fueled availability ontologies aim providing standardvocabulary targeted application domain. particular, many ontologiesbio-medical domain SNOMED CT (IHTSDO, 2016), NCI (Golbeck, Fragoso, Hartel, Hendler, Oberthaler, & Parsia, 2003), GO (Gene Ontology Consortium, 2016),formulated DL allow comparably inexpensive adoption OBDA bio-medical applications querying electronic medical records (Patel, Cimino, Dolby, Fokoue, Kalyanpur,Kershenbaum, Ma, Schonberg, & Srinivas, 2007). Ontologies kind typicallybroad coverage vocabulary often contain tens even hundreds thousands predicatesembrace various subject areas anatomy, diseases, medication, even social contextgeographic location. given application, small fragment ontologys vocabulary actually occur data. Still, remaining predicates potentially usefulformulating queries linked data vocabulary ontologythis preciselyOBDA enriches vocabulary available query formulation.Due size complexity involved ontologies vocabularies, however,difficult understand additional predicates useful query formulationwrite meaningful queries extended vocabulary. Static analysis tools analyzingqueries would thus useful. paper, consider fundamental static analysis problemquery emptiness well natural variation called predicate emptiness. former,problem decide whether given query q provides empty answer databasesformulated given data vocabulary . Query emptiness thus helps identify queriesuseless due wrong use ontology vocabulary. standard static analysis problem manysubareas database theory XML, see e.g. work Benedikt, Fan, Geerts (2008)references therein.example, consider following simple ontology O:DiabetesPatient Patient u disease.DiabetesDiabetesPatientwithoutMedication Patient u disease.Diabetes umedication for.DiabetesAssume used support querying medical patient database unary tableconcept names Patient Diabetes binary tables role names diseasemedication for, distinguishing particular diabetes type 1 type 2. example,database could given following set assertions:Patient(a),disease(a, type1),Patient(b),medication for(a, type1),Diabetes(type1),disease(b, type2),Diabetes(type2).Thus, patient diabetes type 1 medication b patientdiabetes type 2. OBDA, queries interpreted open world assumption thus oneinterested certain answers query q w.r.t. A, is, answers qextension satisfy ontology agreement. concrete queryq1 (x) = DiabetesPatient(x),b certain answers q1 (x) w.r.t. Adespite fact predicate DiabetesPatient used q1 (x) occur A. Since formulated using data vocabulary= {Patient, disease, medication for, Diabetes},2fiQ UERY P REDICATE E MPTINESSQ UERY EVALUATIONE MPTINESSDLIQCQIQ-query / CQ-predicateCQ-queryELPT IME-c.NP-c.PT IME-c.PT IME-cELPT IME-c.NP-c.E XP IME-c.E XP IME-c.ELIE XP IME-c.E XP IME-c.E XP IME-c.E XP IME-c.Horn-ALCIFE XP IME-c.E XP IME-c.E XP IME-c.E XP IME-c.NL OG PACE-c.NP-c.NL OG PACE-c.coNP-c.PT IME-c.NP-c.coNP-c.coNP-c.ALCE XP IME-c.E XP IME-c.NE XP IME-c.NE XP IME-c.ALCIE XP IME-c.2E XP IME-c.NE XP IME-c.2E XP IME-c.ALCFE XP IME-c.E XP IME-c.undecidableundecidableDL-Litecore|F |RDL-LitehornFigure 1: Known complexity results query evaluation new complexity results emptinesssay q1 (x) non-empty given O. regard evidence q1 (x) potentiallyuseful query databases formulated vocabulary . another example, consider queryq2 (x) = DiabetesPatientwithoutMedication(x)certain answer q2 (x) w.r.t. since, open world assumption, mereabsence information b medication diabetes type 1 type 2 implynegation true. One even show that, whatever database A0 formulated vocabularyuse, never certain answer q2 (x) w.r.t. A0 . case, sayq2 (x) empty given O. contrast q1 (x), query thus useless -databases.also consider predicate emptiness, problem decide whether given predicate pdata vocabulary , case queries q involve p yield empty answer-databases. example above, predicate DiabetesPatientwithoutMedication emptyw.r.t. important class conjunctive queries (queries constructed atomic formulas using conjunction existential quantification). Predicate emptiness used identifypredicates ontology useless query formulation, even starting constructconcrete query. graphical user interface, example, predicates would offeredusers query formulation. notion predicate emptiness loosely related predicateemptiness datalog queries studied e.g. Vardi (1989) Levy (1993).aim paper perform detailed study query emptiness predicate emptinessvarious DLs including members EL, DL-Lite, ALC families, concentratingtwo common query languages DL-based OBDA: instance queries (IQs) conjunctivequeries (CQs). resulting combinations DLs query languages, determine(un)decidability exact computational complexity query emptiness predicate emptiness.results summarized right side Figure 1 range PT IME basic membersEL DL-Lite families via NE XP IME basic members ALC family undecidable ALC extended functional roles (ALCF). adopt standard notion combined3fiBAADER , B IENVENU , L UTZ , & W OLTERcomplexity, measured terms size whole input (TBox, data vocabulary,query predicate symbol).restricted data vocabulary quantification -databasesdefinition, query emptiness predicate emptiness reduce standard reasoning problemsquery evaluation query containment. Formally, demonstrated undecidability result ALCF, contrasted decidability query entailmentcontainment DL, shown Calvanese, De Giacomo, Lenzerini (1998). emptiness decidable, complexity still often differs query evaluation. simplifycomparison, display Figure 1 known complexity results query evaluation considered DLs; please consult work Baader, Brandt, Lutz (2005, 2008), Krotzsch, Rudolph,Hitzler (2007), Eiter, Gottlob, Ortiz, Simkus (2008) results concerning ELHorn extensions, work Calvanese, De Giacomo, Lembo, Lenzerini, Rosati (2007)Artale, Calvanese, Kontchakov, Zakharyaschev (2009) results DL-Lite, workTobies (2001), Hustadt, Motik, Sattler (2004), Lutz (2008), Ortiz, Simkus, Eiter(2008) results DLs ALC family. comparing two sides Figure 1,observe clear relationship complexity emptiness checkingcomplexity query evaluation. Indeed, problems often similar complexity,several cases emptiness checking difficult corresponding query evaluation problem. also way around, complexities also incomparable. Noteextension EL EL bottom concept (used express class disjointness),observe particularly significant difference tractability evaluating instance queriesE XP IME-completeness checking IQ-query emptiness.key ingredient developing algorithms establishing upper complexity bounds emptiness show that, searching databases witness non-emptiness (such databasenon-emptiness q1 given example), one often focus singledatabase constructed specifically purpose class databases easier handleclass databases. single database / class databases consider dependsDL question. reason, secondary theme paper analyze shape witnessdatabases. turns ALC extension ALCI inverse roles, considersingle exponential-size database whose construction reminiscent type elimination filtrationconstructions known modal logic literature. EL extension ELI, may alsoconcentrate single witness candidate, much simpler one: consists factsconstructed using data vocabulary single constant. extensions EL,use class databases witness candidates, namely tree forest structure.DL-Lite, may restrict attention class databases whose size bounded polynomiallyw.r.t. input query ontology.demonstrate predicate emptiness useful reasoning service static analysis,perform experiments using well-known large-scale medical ontology SNOMED CT coupledreal-world data vocabulary (corresponding terms obtained analyzing clinical noteshospital) randomly generated vocabularies. real world vocabulary,contains 8,858 370,000 concept names 16 62 role names SNOMED CT,16,212 predicates turned non-empty IQs 17,339 non-empty CQs. Thus,SNOMED CT provides substantial number additional predicates query formulationlarge number predicates cannot meaningfully used queries -databases;thus, identifying relevant predicates via predicate emptiness potentially helpful.4fiQ UERY P REDICATE E MPTINESSalso consider use query predicate emptiness extraction modulesontology. Thus, instead using emptiness directly support query formulation, showused simplify ontology. -substitute ontology subset ontologygives certain answers conjunctive queries -databases. Replacinglarge ontology (potentially quite small) -substitute supports comprehension ontologythereby formulation meaningful queries. show that, ELI, one use predicate emptiness extract particularly natural -substitute ontology, called CQ -core,containing exactly axioms original ontology contain predicatesnon-empty -databases. Thus, predicates CQ -core ontology meaningfully used queries posed -databases. example, CQ -core ontologyO0 = {DiabetesPatient Patient u disease.Diabetes}.second axiom removed CQ contains DiabetesPatientwithoutMedicationempty given O. analyze practical interest CQ -cores, carry case studycompute CQ -cores ontology SNOMED CT coupled various signatures,showing tend drastically smaller original ontology also smaller-modules, popular way extracting modules ontologies (Grau, Horrocks, Kazakov, &Sattler, 2008).article structured follows. begin Section 2 recalling syntax semantics description logics considered work. Section 3, introduce four notionsemptiness (IQ-query, IQ-predicate, CQ-predicate, CQ-query) investigate formal relationships them. first observe IQ-query IQ-predicate emptiness coincide(so three problems consider) CQ-predicate emptiness corresponds CQquery emptiness CQs restricted simple form. also exhibit two polynomialreductions predicate query emptiness: DLs considered paper exceptDL-Lite family, CQ-predicate emptiness polynomially reducible IQ-query emptiness, Horn-DLs considered paper, IQ-query emptiness polynomially reducibleCQ-predicate emptiness.Section 4, investigate computational complexity decidability query predicate emptiness ALC family expressive DLs. ALC ALCI, provide tightcomplexity bounds, showing NE XP IME-completeness three emptiness problems ALCIQ-query emptiness CQ-predicate emptiness ALCI, 2E XP IME-completenessCQ-query emptiness ALCI. situation dramatically (and surprisingly) differentALCF, emptiness problems proven undecidable. previously mentioned,complexity upper bounds ALC ALCI rely characterization non-emptiness termsspecial witness database. complexity lower bounds undecidability results provenmeans reductions tiling problems.Section 5, continue investigation query predicate emptiness consideringDL EL Horn extensions. plain EL, provide simple characterization nonemptiness terms maximal singleton database, allows us show three emptinessproblems decided polynomial time. Using characterization factstandard reasoning ELI E XP IME-complete, obtain E XP IME-completeness emptinesschecking ELI. extensions EL allow contradictions, singleton database mayconsistent ontology, requiring another approach. handle extensions, show5fiBAADER , B IENVENU , L UTZ , & W OLTERsufficient consider tree-shaped databases witnesses non-emptiness, devisedecision procedure emptiness checking based upon tree automata. obtain mannerE XP IME upper bound Horn-ALCIF, sharply contrasts undecidability result(non-Horn) ALCF. Interestingly, show matching E XP IME lower boundconsiderably simpler DL EL , standard reasoning tasks tractable.Section 6, turn attention DL-Lite family lightweight DLs,commonly considered DLs ontology-based data access. show CQ-query emptinesscoNP-complete considered DL-Lite dialects. IQ-query emptiness CQ-predicateemptiness, show complexity depends whether considered dialect allows conjunctions left-hand side axioms. standard dialects like DL-Litecore , DL-LiteR ,DL-LiteF , allow conjunction, show IQ-query emptiness CQ-predicateemptiness NL OG PACE-complete. dialects like DL-Litehorn admits conjunctions,IQ-query emptiness CQ-predicate emptiness coNP-complete. difference complexity due fact dialects allowing conjunction, need consider witnesses nonemptiness polynomial size, whereas absence conjunction, sufficientconsider databases consist single assertion.Section 7, apply query predicate emptiness extract modules ontology.introduce notion -substitute CQ -core ontology show ELICQ -core ontology -substitute. also relate -substitutes notions moduleproposed literature. particular, observe semantic syntactic -modules (Grauet al., 2008) examples -substitutes, thus, algorithms computing modulesalso used compute (possibly non-minimal) -substitutes. demonstrate potentialutility -substitutes emptiness checking experiments based SNOMED CT.Finally, Sections 8 9, conclude paper discussing related future work. Pleasenote improve readability text, technical proofs deferred appendix.2. PreliminariesDLs, concepts inductively defined help set constructors, starting countably infinite sets NC concept names NR role names. constructors important paper summarized Figure 2. inverse role form r r role namerole role name inverse role. uniformity, define double inverse identity,is, (r ) := r role names r. Throughout paper, use A, B denote conceptnames, C, denote (possibly compound) concepts, r denote roles.shall concerned variety different DLs well-known literature.least expressive ones EL DL-Lite, logical underpinnings OWL2profiles OWL2 EL OWL2 QL, respectively (Motik, Grau, Horrocks, Wu, Fokoue, & Lutz,2009). EL, concepts constructed according following grammar using constructorstop concept, conjunction, existential restriction:C,::=>|| C uD|r.Cranging concept names r role names. DL-Lite concepts TBoxesintroduced Section 6. basic expressive DL consider paper ALCextension EL constructors bottom concept, negation, disjunction value restriction:C,::=>||| C uD6|C tD|r.C|r.CfiQ UERY P REDICATE E MPTINESSNameSyntaxconcept namerole namerSemanticsAIrItop conceptbottom conceptnegationconjunctiondisjunctionexistential restrictionvalue restrictionrole inverse>CC uDC tDr.Cr.Cr>I ==\ CC DIC DI{d | e C (d, e) rI }{d | e : (d, e) rI e C }{(d, e) | (e, d) rI }concept inclusionconcept assertionrole assertionCvDA(a)r(a, b)C DIaI AI(aI , bI ) rIFigure 2: Syntax semantics DL constructors, TBox axioms, ABox assertions.availability additional constructors indicated concatenation letters subscripts:letter stands addition inverse roles (inside existential value restrictions, present)subscript stands adding . gives, example, extension ALCI ALCinverse roles, whose constructors exactly ones shown Figure 2. also definesextension ELI EL inverse roles existential restrictions bottom concept.concept inclusion (CI) DL L takes form C v D, C L-concepts.use C abbreviation CIs C v v C. description logic, ontologiesformalized TBoxes. Given DLs L introduced above, L-TBox finite setCIs L. use letter F write LF denote description logic TBoxes consistCIs L, also functionality statements funct(r), r role nameinverse role (if inverse roles admitted L). example, ALCF thus extension ALCTBoxes contain CIs ALC functionality statements role names. useterm axioms refer concept inclusions functionality statements uniform way.addition DLs introduced above, also consider DLs impose restrictionsconstructors used side concept inclusions. Horn-ALCI conceptinclusion (CI) form L v R, L R concepts defined syntax rulesR, R0 ::= > | | | | R u R0 | L R | r.R | r.RL, L0 ::= > | | | L u L0 | L L0 | r.Lranging concept names r (potentially inverse) roles. Horn-ALCIF-TBoxfinite set Horn-ALCI CIs functionality statements funct(r). Note differentdefinitions Horn-DLs found work Hustadt, Motik, Sattler (2007), Eiter et al.(2008), Kazakov (2009). original definition Hustadt, Motik, Sattler basedpolarity rather technical, prefer (equivalent) definition.size |T | TBox obtained taking sum lengths axioms,length axiom number symbols needed write word.7fiBAADER , B IENVENU , L UTZ , & W OLTERDatabases represented using ABox, finite set concept assertions A(a)role assertions r(a, b), a, b drawn countably infinite set NI individual names,concept name, r role name. Note role assertions cannot use inverse roles.shortcut, though, sometimes write r (a, b) r(b, a) A. use Ind(A) denoteset individual names used ABox A. knowledge base pair K = (T , A)TBox ABox.semantics description logics defined terms interpretation = (I , ).domain non-empty set interpretation function maps concept name NCsubset AI , role name r NR binary relation rI , individualname element aI . extension compound concepts inductively definedshown third column Figure 2. interpretation satisfies (i) CI C v C DI ,statement funct(r) rI functional, (iii) assertion A(a) aI AI , (vi) assertionr(a, b) (aI , bI ) rI . Then, model TBox satisfies axioms , modelABox satisfies assertions A. TBox satisfiable model ABoxsatisfiable w.r.t. TBox common model. write |= C vmodels satisfy CI C v D.consider two types queries. First, instance queries (IQs) take form A(v),concept name v individual variable taken set NV . Note instance queriesused query concept names, role names. traditional definition, duefact role assertions implied ABox explicitly contained it,thus querying trivial.1 general conjunctive queries (CQs) take form ~u (~v , ~u)conjunction atoms form A(v) r(v, v 0 ) v, v 0 individual variables~v ~u NV . Variables existentially quantified called answer variables,arity q defined number answer variables. Queries arity 0 called Boolean.use var(q), avar(q), qvar(q) denote set variables, answer variables, quantifiedvariables respectively query q. on, use IQ refer set IQs CQrefer set CQs.Let interpretation q (instance conjunctive) query q arity k answervariables v1 , . . . , vk . match q mapping : var(q) (v) AIA(v) q, ((v), (v 0 )) rI r(v, v 0 ) q, every answer variable v var(q),individual name (v) = aI . write |= q[a1 , . . . , ak ] matchq (vi ) = aIi every 1 k. knowledge base (T , A), write, |= q[a1 , . . . , ak ] |= q[a1 , . . . , ak ] models A. case, (a1 , . . . , ak )certain answer q w.r.t. A. use certT ,A (q) denote set certain answersq w.r.t. A. Note q Boolean query, () certT ,A (q)match q every model , A, otherwise certT ,A (q) = . query evaluation problemCQs DL L problem decide L-TBox , ABox A, CQ q arity k, tuple~a Ind(A)k , whether ~a certT ,A (q).use term predicate refer concept name role name signature referset predicates (in introduction, informally called signature vocabulary). sig(q)denotes set predicates used query q, similarly sig(T ) sig(A) refersignature TBox ABox A. -ABox ABox uses predicatessignature , likewise -concept.1. longer true presence role hierarchy statements which, however, consider paper.8fiQ UERY P REDICATE E MPTINESScontext query answering DLs, sometimes useful adopt unique nameassumption (UNA), requires aI 6= bI interpretations a, b NI6= b. results obtained paper depend UNA. following well-knownlemma shows UNA make difference ALCI (and fragments ELALC) certain answers queries change.Lemma 1 Let ALCI-TBox, ABox, q CQ. certT ,A (q) identicalwithout UNA.analogous statement fails ALCF, e.g. ABox = {f (a, b), f (a, b0 )} satisfiable w.r.t. TBox = {funct(r)} without UNA (and thus certT ,A (A(v)) = ),unsatisfiable UNA (and thus certT ,A (A(v)) = Ind(A)).3. Query Predicate Emptinessintroduce central notions reasoning problems studied paper, showinterrelated, make basic observations used throughout paper. followingdefinition introduces different notions emptiness studied paper.Definition 2 Let TBox, signature, Q {IQ, CQ} query language. callQ-query q empty given -ABoxes satisfiable w.r.t. ,certT ,A (q) = .predicate Q-empty given every Q-query q sig(q) emptygiven .follows, signatures used ABoxes called ABox signatures. quantifyABoxes formulated ABox signature address typical database applicationsdata changes frequently, thus deciding emptiness based concrete ABoxmuch interest. example, assume ABoxes formulated signature= {Person, hasDisease, DiseaseA, DiseaseB}following, upper-case words concept names lower-case onesrole names. signature typically fixed application design phase, similar schemadesign databases. TBox, take= {Person v hasFather.(Person u Male), DiseaseA v InfectiousDisease}.IQ InfectiousDisease(v) CQ v hasFather(u, v) non-empty givendespite using predicates cannot occur data, witnessed -ABoxes {DiseaseA(a)}{Person(a)}, respectively. illustrates TBox enriches vocabularyavailable query formulation. contrast, CQvv 0 (hasFather(u, v) hasDisease(v, v 0 ) InfectiousDisease(v 0 )),uses predicates plus additional one ABox signature, emptygiven .9fiBAADER , B IENVENU , L UTZ , & W OLTERRegarding predicate emptiness, interesting observe choice query languageimportant. example, predicate Male IQ-empty given , CQ-emptywitnessed -ABox {Person(a)} CQ v Male(v). thus makes sense use Maleinstance queries -ABoxes given , whereas meaningfully used conjunctivequeries.every IQ also CQ, predicate CQ-empty must also IQ-empty. illustratedexample, converse hold. Also note role names IQ-emptygiven since role name cannot occur instance query. contrast, hasFather clearlyCQ-empty example.follows Lemma 1 that, ALCI fragments, query emptiness predicate emptiness oblivious whether UNA made, IQ CQ. establishedfollowing lemma, also true ALCIF despite fact certain answers queriesdiffer without UNA.Lemma 3 Let ALCIF-TBox. CQ q empty given UNA iffempty given without UNA.proof Lemma 3 given appendix. direction left right one assumesq non-empty given without UNA takes witness -ABox A. Using modelsatisfying without UNA identifying a, b Ind(A) aI = bI onedefine -ABox A0 shows q non-empty given UNA.Conversely, one assumes q non-empty given UNA takes witness-ABox A. One use show q non-empty given without UNA.exception DL-Lite dialect (containing role inclusions) DLs consideredpaper fragments ALCIF. Thus, free adopt UNA not. remainderpaper, choose whatever convenient, careful always point explicitlywhether UNA made not. DL-Lite dialect covered formulation Lemma 3observe discussion DL-Lite even Lemma 1 holds free adoptUNA case well.also relevant note decision disallow individual names query atoms withoutloss generality. Indeed, easily verified predicate emptiness whetheradmit individuals queries not. Moreover, immediate reduction query emptinessgeneralized CQs (which may contain individual names) query emptiness CQs definedpaper: suffices replace every individual query fresh answer variable xa ,test whether resulting query (without individuals) empty given .Definition 2 gives rise four natural decision problems.Definition 4 Let Q {IQ, CQ}.Q-query emptiness problem deciding, given TBox , signature , Qquery q, whether q empty given ;Q-predicate emptiness means decide, given TBox , signature , predicate S,whether Q-empty given .10fiQ UERY P REDICATE E MPTINESSIQ-query = IQ-predicateemptinessemptinessTrivialCQ-queryemptinessTheorem 7(materializable DLs)Lemma 5Theorem 6CQ-predicateemptinessFigure 3: Polytime reductions emptiness notions.Clearly, four problems intimately related. particular, IQ-query emptiness IQpredicate emptiness effectively problem since instance query consistssingle predicate. reason, disregard IQ-predicate emptinessspeak IQ-query emptiness. CQ case, things different. Indeed, following lemmashows CQ-predicate emptiness corresponds CQ-query emptiness CQs restrictedsimple form. easy consequence fact that, since composite concepts queriesdisallowed, CQs purely positive, existential, conjunctive.Lemma 5 NC (resp. r NR ) CQ-predicate empty given iff conjunctive queryv A(v) (resp. vv 0 r(v, v 0 )) empty given .Lemma 5 allows us consider queries form v A(v) vv 0 r(v, v 0 ) dealingCQ-predicate emptiness. on, without notice.Trivially, IQ-query emptiness special case CQ-query emptiness. following observation less obvious applies DLs considered paper except DL-Litefamily.Theorem 6 DL contained ALCIF admits CIs r.B v B r.> v B Bconcept name, CQ-predicate emptiness polynomially reduced IQ-query emptiness.Proof. Let TBox, signature, B concept name occur ,role name occur . prove1. CQ-predicate empty given iff IQ B(v) empty {s} given TBox0 = TB {A v B}, TB = {r.B v B | r = r occurs };2. r CQ-predicate empty given iff IQ B(v) empty {s} given TBox0 = TB {r.> v B}, TB above.proofs Points 1 2 similar, concentrate Point 1. First suppose CQpredicate non-empty given . -ABox , |= v A(v). Choosea0 Ind(A) set A0 := {s(a0 , b) | b Ind(A)}. Using fact , |= v A(v)definition A0 0 , shown 0 , A0 |= B(a0 ). converse direction,suppose B IQ-query non-empty {s} given 0 . {s}-ABox A00 , A0 |= B(a) Ind(A0 ). Let obtained A0 removing assertionss(a, b). Using fact 0 , A0 |= B(a) definition A0 0 , shown, |= v A(v).11fiBAADER , B IENVENU , L UTZ , & W OLTERFigure 3 gives overview available polytime reductions four (rather: three)problems. terms computational complexity, CQ-query emptiness thus (potentially) hardest problem, CQ-predicate emptiness simplest. precisely, CQ-query emptinessDL L belongs complexity class C (larger equal PT IME), IQ-query emptinessCQ-predicate emptiness L also C. Moreover, DLs L satisfying conditionsTheorem 6, C-hardness CQ-predicate emptiness L implies C-hardness CQ-query emptinessIQ-query emptiness L.certain conditions, also prove converse Theorem 6. Following workLutz Wolter (2012), call model TBox ABox materializationevery CQ q arity k tuple ~a Ind(A)k , |= q[~a] iff , |= q[~a]. DL Lcalled materializable every ABox satisfiable w.r.t. exists materializationA. Typical DL-Lite dialects, DL EL, Horn-extensions EL ELIFmaterializable (Lutz & Wolter, 2012).Theorem 7 Let L materializable DL admits CIs form A1 u A2 v A3 ,A1 , A2 , A3 NC . Then, L, IQ-query emptiness polynomially reduced CQ-predicateemptiness.Proof. claim IQ A(v) empty given iff B CQ-empty {X} givenTBox 0 = {A u X v B}, B X concept names occur .direction, assume A(v) IQ non-empty given , let ABox , |= A(a) Ind(A). Set A0 := {X(a)}. easy see0 , A0 |= v B(v) thus B CQ-predicate non-empty {X} given 0 .direction, assume B CQ non-empty {X} given 0 , let A0{X}-ABox satisfiable 0 0 , A0 |= v B(v). may assumeX(a) A0 Ind(A0 ) adding assertions neither result unsatisfiabilityw.r.t. 0 invalidate 0 , A0 |= v B(v). assumption materializability, existsmaterialization 0 0 A0 0 |= v B(v). definition 0 , may assume0000X = Ind(A0 ) B = AI X (if case, take modified version,00000000000 , 0 defined setting X := Ind(A0 ), B := AI X , :=0remaining concept role names ; 00 still satisfies 0 A0 since X Ind(A0 )000u X v B inclusion containing X B still materialization sinceconcept role names ). 0 |= v B(v) implies Ind(A0 )0 |= B(a). Since 0 materialization 0 A0 , implies 0 , A0 |= B(a). definition0 , implies , |= A(a), obtained A0 dropping assertions formX(b). Since -ABox satisfiable w.r.t. (since A0 satisfiable w.r.t. 0 ), witnessesA(v) non-empty given .final observation section, note deciding query predicate emptinessessentially ABox satisfiability whenever contains symbols used TBox.described reductions, suffices consider CQ-query emptiness. CQ q = ~u (~v , ~u)associate every individual variable v q individual name av setAq = {A(av ) | A(v) conjunct } {r(av , av0 ) | r(v, v 0 ) conjunct }.Theorem 8 Let ALCIF-TBox, signature sig(T ) , q CQ. qempty given iff sig(q) 6 Aq unsatisfiable w.r.t. .12fiQ UERY P REDICATE E MPTINESSProof. (If) Assume q non-empty given . -ABoxsatisfiable w.r.t. certT ,A (q) 6= . clearly implies sig(q) since otherwisepredicate sig(q) \ find model predicateinterpreted empty set, would mean certT ,A (q) = . thus remains show Aqsatisfiable w.r.t. . end, let model A. Since certT ,A (q) 6= , existsmatch q I. Modify setting aIv = (v) variables v used q. readily checkedmodified model Aq , thus Aq satisfiable w.r.t. required.(Only if) Assume sig(q) Aq satisfiable w.r.t. . sig(Aq ) . Sinceclearly certT ,Aq (q) 6= , means q non-empty given .4. Expressive Description Logicsconsider query predicate emptiness ALC family expressive DLs, establishing tightcomplexity results ALC ALCI, undecidability ALCF. start upper boundproofs, showing IQ-query emptiness CQ-predicate emptiness ALCI NE XP IME, CQ-query emptiness ALC. Moreover, establish CQ-query emptiness2E XP IME. move corresponding lower bound proofs also establishundecidability considered emptiness problems ALCF.4.1 Upper Boundsfirst main step proofs show that, deciding emptiness problems ALCALCI, suffices consider single special -ABox. Specifically, show constructgiven satisfiable TBox ABox signature canonical -ABox ,every CQ q, certT ,AT , (q) 6= exists -ABox satisfiablew.r.t. certT ,A (q) 6= . prove NE XP IME upper bounds IQ-query emptiness ALCI computing , (in exponential time) guessing model , (ofexponential size |T | ) falsifies query; 2E XP IME upper bound CQ-queryemptiness ALCI obtained even simpler way computing , checkingwhether certT ,AT , (q) = using known algorithms. Significantly work required obtainNE XP IME upper bound CQ-query emptiness ALC. construct , , needexercise lot care check whether certT ,AT , (q) = without leaving NE XP IME.Let satisfiable ALCI-TBox ABox signature. define canonical -ABox, introduce well-known notion types (or Hintikka sets) (Pratt, 1979; Kaminski,Schneider, & Smolka, 2011). closure cl(T , ) smallest set containsNC well concepts occur (potentially subconcept) closedsingle negations. type set cl(T , ) model, = tI (d), tI (d) = {C cl(T , ) | C } typerealized I. Let TT , denote set types . role name rt, t0 TT , , say pair (t, t0 ) r-coherent write ;r t0C t0 whenever r.C t,C whenever r .C t0 .seen implies also corresponding conditions existential restrictions,C t0 r.C cl(C, ) implies r.C t.13fiBAADER , B IENVENU , L UTZ , & W OLTERDefinition 9 (Canonical -ABox) Let satisfiable ALCI-TBox ABox signature.Fix (distinct) individual name TT , . canonical -ABox , definedfollows:, = {A(at ) | TT , , NC }{r(at , at0 ) | ;r t0 t, t0 TT , , r NR }.cardinality TT , exponential size cardinality ,set TT , computed exponential time making use well-known E XP IME proceduresconcept satisfiability w.r.t. TBoxes ALCI (Gabbay, Kurucz, Wolter, & Zakharyaschev, 2003,p. 72) Thus, , exponential size computed exponential time. interestingnote ABox , finitary version canonical model basic modal logicessentially identical model constructed Pratts type elimination procedure (Pratt, 1979);fact, exactly identical sig(T ). show , satisfiable w.r.t. .Lemma 10 Let satisfiable ALCI-TBox ABox signature. , satisfiablew.r.t. .Proof. Let interpretation , defined setting,= TT ,AIT ,= {t TT , | t}rIT ,= {(t, t0 ) TT , TT , | ;r t0 }NC r NR . One prove induction structure C Ccl(T , ), C iff C , . definition types, C v C imply t.Thus, , model . immediate consequence definition , ,also model , ; fact, , regarded reduct , signature .crucial tool analyzing properties canonical ABoxes, introduce homomorphismABoxes. Let A0 ABoxes. ABox homomorphism A0 totalfunction h : Ind(A) Ind(A0 ) following conditions satisfied:A(a) implies A(h(a)) A0 ;r(a, b) implies r(h(a), h(b)) A0 .next lemma identifies central property ABox homomorphisms regarding query answering.Lemma 11 ALCI-TBox, q CQ , |= q[a1 , . . . , ], h ABoxhomomorphism A0 , , A0 |= q[h(a1 ), . . . , h(an )].Proof. prove contrapositive. Thus assume , A0 6|= q[h(a1 ), . . . , h(an )].model 0 A0 0 6|= q[h(a1 ), . . . , h(an )]. Define model starting 00reinterpreting individual names Ind(A) setting aI = h(a)I Ind(A). Sinceindividual names occur , model . also model A: A(a) A,A(h(a)) A0 definition ABox homomorphisms. Since 0 model A0 definitionI, follows aI AI . case r(a, b) analogous. Finally, 0 6|= q[h(a1 ), . . . , h(an )]definition yield 6|= q[a1 , . . . , ]. thus shown , 6|= q[a1 , . . . , ].14fiQ UERY P REDICATE E MPTINESSfollowing lemma characterizes satisfiability -ABoxes w.r.t. existence ABoxhomomorphism , .Lemma 12 Let satisfiable ALCI-TBox ABox signature. -ABox satisfiablew.r.t. iff ABox homomorphism , .Proof. Assume satisfiable w.r.t. . Let model A. Define homomorphism h, setting h(a) = , type realized aI I. Usingdefinition , , one see h indeed ABox homomorphism. Conversely, let hABox homomorphism , . Lemma 10, , satisfiable w.r.t. . proofLemma 11 shows one construct model model , usinghomomorphism h. Thus satisfiable w.r.t. .ready prove main property , regarding emptiness, discussedbeginning section.Theorem 13 Let satisfiable ALCI-TBox ABox signature. CQ q emptygiven iff certT ,AT , (q) = .Proof. direction follows directly fact , satisfiable w.r.t. (byLemma 10). direction, let certT ,AT , (q) = . show q empty given ,take -ABox satisfiable w.r.t. . Lemmas 11 12, certT ,AT , (q) = impliescertT ,A (q) = , required.employ Theorem 13 prove NE XP IME upper bounds IQ-query emptiness.Theorem 14 ALCI, IQ-query emptiness NE XP IME.Proof. Let satisfiable TBox, ABox signature, A(v) IQ emptinessgiven decided. employ following:Fact. ABox A, , 6|= A(a), exists model aI 6 AI|I | |Ind(A)| + 2|T | .Proof Fact. , 6|= A(a), exists model J aJ 6 AJ .may assume {aJ | Ind(A)} disjoint domain TT ,0 interpretation ,0defined proof Lemma 10 (where assume 0 := ). define unionrestriction J {aJ | Ind(A)} interpretation ,0 expanded adding rIpairs(aJ , t) tJ (aJ ) ;r t, Ind(A), TT ,0 ;(t, aJ ) ;r tJ (aJ ), Ind(A), TT ,0 .model aI 6 AI required size. finishes proof fact.NE XP IME algorithm computes canonical ABox , (in exponential time) guessesevery Ind(AT , ) model Ia |Ia | |Ind(AT , )| + 2|T | . algorithm returns yesInd(AT , ):1. Ia model , ,15fiBAADER , B IENVENU , L UTZ , & W OLTER2. aIa 6 AIa .conditions checked exponential time. Thus, Theorem 13 fact above,algorithm returns yes iff A(v) empty given .Note Theorem 6 CQ-predicate emptiness ALCI NE XP IME well. CQ-queryemptiness ALCI, easily derive 2E XP IME upper bound using , resultswork Calvanese et al. (1998) complexity query answering DLs.Theorem 15 ALCI, CQ-query emptiness 2E XP IME.Proof. 2E XP IME algorithm obtained first computing canonical ABox ,certT ,AT , (q), checking whether latter empty. done 2E XP IME sinceshown work Calvanese et al. (1998) , A, q ALCI-TBox,p(n)set certT ,A (q) computed time 2p(m)2p polynomial, size A,n size q.provide improved NE XP IME upper bound CQ-query emptiness ALC,allow us show ALC three emptiness problems complexity.Theorem 16 ALC, CQ-query emptiness NE XP IME.proof somewhat technical reuses machinery fork rewritings spoilers introducedLutz (2008), proves combined complexity CQ-answering DL SHQE XP IME. concretely, show one decide emptiness CQ q ABoxsignature given ALC-TBox guessing extension AeT , canonical ABox ,assertions prevent possible match q checking AeT , satisfiable w.r.t.. example, q A(x), obviously suffices add A(a) every individual ,(we allow also complex concepts used ABox). general case requires carefulanalysis assertions considered additions, mentionedfork rewritings spoilers enter picture. fact, used prove that, sinceinverse roles TBox, suffices consider extensions , contain additionalindividual names additional assertions taken candidate set whose sizepolynomial size , q. remains show satisfiability (T , AeT , )decided (non-deterministically) time single exponential size q. Full detailsgiven appendix.4.2 Lower Bounds Undecidabilityprove matching lower bounds upper complexity bounds presented show undecidability IQ-query emptiness, CQ-predicate emptiness, CQ-query emptiness ALCF.undecidability proof NE XP IME-lower bound proof reduction two differenttiling problems, first asks tiling finite rectangle (unbounded) sizesecond asks tiling 2n 2n -square. 2E XP IME lower bound CQ-query emptiness ALCI straightforward reduction query entailment ALCI. beginNE XP IME lower bound.Theorem 17 ALC, CQ-predicate emptiness NE XP IME-hard.16fiQ UERY P REDICATE E MPTINESSProof. proof reduction NE XP IME-hard 2n 2n -tiling problem. instancetiling problem given natural number n > 0 (coded unary) triple (T, H, V )non-empty, finite set tile types including initial tile Tinit placed lower leftcorner, H horizontal matching relation, V vertical matching relation.tiling (T, H, V ) map f : {0, . . . , 2n 1} {0, . . . , 2n 1} f (0, 0) = Tinit ,(f (i, j), f (i + 1, j)) H < 2n 1, (f (i, j), f (i, j + 1)) V j < 2n 1.NE XP IME-complete decide whether instance 2n 2n -tiling problem tiling.reduction, let n > 0 (T, H, V ) instance 2n 2n -tiling problem= {T1 , . . . , Tp }. construct signature TBox ALC (T, H, V )solution selected concept name CQ-predicate empty given .proof, convenient impose UNA.formulating reduction TBox, use role names x represent 2n 2n grid two binary counters X counting 0 2n 1. counters use conceptnames X0 , . . . , Xn1 Y0 , . . . , Yn1 bits, respectively. contains following wellknown inclusions stating value counter X0 , . . . , Xn1 incremented goingx-successors value counter Y0 , . . . , Yn1 incremented going y-successors:k = 1, . . . , n 1,u0j<k0j<kXj v (Xk x.Xk ) u (Xk x.Xk )Xj v (Xk x.Xk ) u (Xk x.Xk )similarly Y0 , . . . , Yn1 y. also states value counter X changegoing y-successors value counter change going xsuccessors: = 0, . . . , n 1,Xi v y.Xi ,Xi v y.XiYi v x.Yi ,Yi v x.Yi .addition, states counter X 2n 1, x-successorcounter 2n 1, y-successor:X0 u u Xn1 v x.,Y0 u u Yn1 v y..states Tinit holds (0, 0) tiling complete:X0 u u Xn1 u Y0 u u Yn1 v Tinit ,states tiling condition violated, true:0 < j p: Ti u Tj v A,0 i, j p (Ti , Tj ) 6 H: Ti u x.Tj v A,0 i, j p (Ti , Tj ) 6 V : Ti u y.Tj v A.17>v1ipTi ,fiBAADER , B IENVENU , L UTZ , & W OLTERFinally, since cannot use negation ABoxes, states concept names X 0 , . . . , X n10 , . . . , n1 equivalent X0 , . . . , Xn1 Y0 , . . . , Yn1 , respectively: =1, . . . , n 1:Xi v X , Xi v X , Yi v , Yi v X .set = {x, y, X0 , . . . , Xn1 , Y0 , . . . , Yn1 , X 0 , . . . , X n1 , 0 , . . . , n1 } showClaim. (T, H, V ) 2n 2n -tiling iff exists -ABox satisfiable w.r.t., |= v A(v).Proof claim. Assume first (T, H, V ) 2n 2n -tiling. construct A, regardpairs (i, j) < 2n j < 2n individual names let x((i, j), (i + 1, j))< 2n 1 y((i, j), (i, j + 1)) j < 2n 1. also set Xk (i, j) kth bit1, X k (i, j) kth bit 0, Yk (i, j) kth bit j 1, k (i, j)kth bit j 0. readily checked satisfiable w.r.t. , |= v A(v).Conversely, assume (T, H, V ) 2n 2n -tiling given f : {0, . . . , 2n 1}{0, . . . , 2n 1} T. Let -ABox satisfiable w.r.t. . show , 6|= v A(v).Let model A. AI = , done. Otherwise re-define interpretationT1 , . . . , Tp follows. Associate every uniquely determined pair (id , jd )given values counters X I. set TkI iff f (id , jd ) = Tk letAI = . readily checked resulting interpretation still model A.follows preceding result IQ-query emptiness CQ-query emptinessALC ALCI NE XP IME-hard. CQ-query emptiness ALCI, easily derive2E XP IME lower bound results complexity query entailment ALCI.Theorem 18 ALCI, CQ-query emptiness 2E XP IME-hard.Proof. shown Lutz (2008) CQ entailment ALCI 2E XP IME-hard alreadyABoxes form {A(a)} Boolean CQs. clearly strengthened emptyABoxes: replace A(a) empty ABox compensate adding TBox > vr.A r fresh role name. thus remains observe Boolean CQ q entailedempty ABox iff q non-empty = .show simple addition functional roles ALC leads undecidability CQpredicate emptiness, thus also IQ-query emptiness CQ-query emptiness. proofreduction tiling problem asks tiling rectangle finite size (which neitherfixed bounded). reduction involves couple technical tricks using conceptnames universally quantified second-order variables. allows us enforcegrid structure using standard frame axioms modal logic (which second-order nature).reduction requires role names functional inverse functional. Since inversefunctionality cannot expressed ALCF, also use modal logic frame axiom enforcedifferent, (forwards) functional role name interpreted inverse role nameinterested in. course, undecidability carries variants ALCF use conceptconstructor ( 1 r) instead functional roles additional sort, DLs qualifiedunqualified number restrictions.Theorem 19 ALCF, CQ-predicate emptiness undecidable.18fiQ UERY P REDICATE E MPTINESSinstance aforementioned tiling problem given triple (T, H, V ) non-empty,finite set tile types including initial tile Tinit placed lower left corner final tileTfinal placed upper right corner, H horizontal matching relation, VTT vertical matching relation. tiling (T, H, V ) map f : {0, . . . , n}{0, . . . , m}n, 0, f (0, 0) = Tinit , f (n, m) = Tfinal , (f (i, j), f (i + 1, j)) H < n,(f (i, j), f (i, j + 1)) v < m. undecidable whether instance tiling problemtiling.reduction, let (T, H, V ) instance tiling problem = {T1 , . . . , Tp }.construct signature TBox (T, H, V ) solution selectedconcept name CQ-predicate non-empty given .ABox signature = {T1 , . . . , Tp , x, y, x , } T1 , . . . , Tp used conceptnames, x, y, x , functional role names. use role names x represent horizontal vertical adjacency points rectangle, role names xsimulate inverses x y. , use additional auxiliary concept names. particular UR mark upper right border rectangle, Zc,1 , Zc,2 , Zx,1 , Zx,2 , Zy,1 , Zy,2 servesecond-order variables, C serves flag indicates grid cells closed positionset, Ix Iy similar flags intended behavior role names x, xy, . concept name propagated grid upper right corner lowerleft one, ensuring flags set everywhere, every position grid labeledleast one tile type, horizontal vertical matching conditions satisfied.lower left corner grid reached, set flag, query v A(v) asksfor.TBox defined set following CIs, (Ti , Tj , T` ) range triples(Ti , Tj ) H (Ti , T` ) V , e {c, x, y}, rangesBoolean combinations concept names Ze,1 Ze,2 , i.e., concepts L1 u L2 LiZe,i Ze,i :Tfinal v u U u Rx.(U u u Tj ) u Ix u Ti v U uy.(R u u T` ) u Iy u Ti v R ux.(Tj u u y.Y ) u y.(T` u u x.Y ) u Ix u Iy u C u Ti vu Tinit vBx u x.x .Bx v Ixu y.y .By v Iyx.y.Bc u y.x.Bc v CU v y. R v x. U v x.UR v y.R1s<tpTs u Tt vCIs Ix Iy responsible enforcing x inverse x inversey, least ABox individuals interested in. fact, ABox containsassertions x(a, b) x (b, c) thus violates intended interpretation x x ,interpret Zx,1 Zx,2 left-hand sides possible instantiations CIIx violated, e.g. making Zx,1 Zx,2 true a, false c. ABox containsx(a, b), x (b, a), possible. Since x functional, thus enforce x19fiBAADER , B IENVENU , L UTZ , & W OLTERinverse functional. CIs C achieve similar way closing grid cells, i.e.,x-y-successor y-x-successor every relevant ABox individual coincide. However,seen proofs, works x inverse functional.establish Theorem 19, suffices prove following lemma (see appendix details).Lemma 20 (T, H, V ) admits tiling iff -ABox satisfiable, |= v A(v).5. EL Horn Extensionsstudy query predicate emptiness DL EL several Horn extensions. First,show that, plain EL, three emptiness problems decided polynomial time. reasoncase, exponential-size canonical ABox , Section 4 replacedtotal -ABox contains single individual instance -predicates.Note satisfiable w.r.t. EL-TBox EL cannot express unsatisfiability.approach works ELI, case one obtains E XP IME upper boundoptimal since subsumption ELI already E XP IME-hard (Baader et al., 2005, 2008).soon unsatisfiability expressed, situation changes drastically. fact, showeven EL subsumption standard reasoning tasks still tractable, (all versionsof) emptiness E XP IME-hard. Nevertheless, emptiness Horn extensions EL turnseasier emptiness expressive DLs. contrast undecidability result ALCFNE XP IME-hardness result ALC, emptiness E XP IME even Horn-ALCIF.reason unraveling tolerance Horn description logics observed work LutzWolter (2012), implies looking ABoxes witness non-emptiness,restrict tree-shaped ones. enables use automata-theoretic techniquesdecide emptiness.5.1 EL ELIbegin showing EL, CQ-query emptiness, CQ-predicate emptiness, IQ-queryemptiness PT IME. proofs transparent simple. signature , total-ABox := {A(a ) | } {r(a , ) | r }.Lemma 21 Let EL-TBox signature. CQ q empty given iffcertT ,A (q) = .Proof. proof simplified version proof Theorem 13. (contrapositive the)direction follows fact satisfiable w.r.t. . direction, letcertT ,A (q) = . show q empty given , take -ABox A. Define ABoxhomomorphism setting h(a) := Ind(A ). Lemmas 11 12,certT ,A (q) = implies certT ,A (q) = , required.Lemma 21 provides polytime reduction CQ-query emptiness (and, therefore, IQ-queryemptiness CQ-predicate emptiness) query evaluation problem CQs .appendix, show due simple shape , checking whether certT ,A (q) =done polynomial time. fact, give polytime procedure either returns certT ,A (q) =succeeds constructing Boolean forest-shaped query qb empty given iff q is.20fiQ UERY P REDICATE E MPTINESSconstruction relies fact that, immediate consequence results proved LutzWolter (2010), emptiness q given implies existence modelcertT ,A (q) = shape tree extended reflexive loops root.Checking , 6|= qb requires answer concept queries extension ELu ELuniversal role, possible PT IME (Lutz & Wolter, 2010). obtain followingresult.Theorem 22 EL, IQ-query emptiness CQ-query emptiness decided PT IME.matching PT IME lower bound Theorem 22 shown reduction subsumptionEL, PT IME-hard (Haase, 2007). Consider EL-TBox EL-concepts C D.CI C v follows if, if, IQ B(v) non-empty signature {A}given TBox {A v C, v B}, A, B concept names appear C,. Thus, obtainTheorem 23 EL, IQ-query emptiness CQ-query emptiness PT IME-hard.Observe Lemma 7 materializability EL obtain CQ-predicate emptinessPT IME-complete well EL.Note need little proof Lemma 21 go through: suffices total-ABox satisfiable every TBox. thus reduce emptiness query answeringtotal -ABox extension EL unable express contradictions. anotherimportant example, consider ELI. Since CQ evaluation ELI E XP IME-complete,obtain E XP IME upper bound case. matching lower bound obtainedE XP IME-hardness subsumption ELI simple reduction subsumption IQ-queryemptiness given above.Theorem 24 ELI, IQ-query emptiness CQ-query emptiness E XP IME-complete.follows Lemma 7 materializability ELI CQ-predicate emptiness E XP IMEcomplete ELI.5.2 Horn Extensions Involving Negation Functionalitysimplest extension EL express unsatisfiability EL . begin showingIQ-emptiness EL E XP IME-hard, thus significantly harder subsumption instance checking (both decided polynomial time). end, first showdecide IQ-query emptiness EL sufficient consider emptiness w.r.t. directed treeshaped ABoxes, ABox called directed tree-shaped following conditions hold:1. directed graph GdA = (Ind(A), {(a, b) | r(a, b) A}) tree;2. a, b Ind(A), one role name r r(a, b) r(b, a)(and one case).Proposition 25 instance query B(v) non-empty signature given EL -TBox iffexists directed tree-shaped -ABox satisfiable w.r.t. , |= B(a)root A.21fiBAADER , B IENVENU , L UTZ , & W OLTERProof. provide sketch since result also follows general Proposition 30 proved below. Assume B(v) non-empty given . find -ABoxsatisfiable w.r.t. , |= B(a). Let potentially infinite ABox obtainedunfolding follows: individuals words a0 r0 rn1 a0 =ri (ai , ai+1 ) 0 < n; include A(a0 r0 rn1 ) iff A(an )include r(a0 r0 , a0 r0 rn an+1 ) rn (an , an+1 ) A. One show satisfiable w.r.t. since is, , |= B(a) iff , |= B(a). compactness first-orderconsequence, obtain finite ABox A0 , A0 |= B(a). A0 required.Theorem 26 EL , IQ-query emptiness E XP IME-hard.Proof. Let , , B(v) given. Proposition 25, B(v) non-empty given iffexists directed tree-shaped -ABox witness non-emptiness B(v)given . Directed tree-shaped -ABoxes viewed EL-concepts using symbolsonly, vice versa. Thus, witness -ABox exists iff exists EL-concept C usingsymbols C satisfiable w.r.t. |= C v B. followingestablished carefully analyzing reduction underlying Theorem 36 work LutzWolter (2010): given EL -TBox , signature , concept name B, E XP IME-harddecide exists EL-concept C using symbols C satisfiablew.r.t. |= C v B. establishes E XP IME-hardness non-emptiness. Using factE XP IME = coE XP IME, hardness result transfers IQ-query emptiness.Observe Lemma 7 materializability EL obtain CQ-predicate emptinessE XP IME-hard well EL .Instead proving matching E XP IME upper bound emptiness EL ,expressive Horn DL Horn-ALCIF, EL fragment. fact, restsection devoted proof following theorem. interesting contrast resultundecidability emptiness ALCF.Theorem 27 Horn-ALCIF, CQ-query emptiness E XP IME.strategy proof Theorem 27 follows. first exhibit polynomial-time reductionCQ-query emptiness Horn-ALCIF CQ-query emptiness ELIF . Then, shownon-emptiness CQ q ELIF -TBox always witnessed ABoxes certain,forest-like shape. consider canonical models forest-shaped ABoxes (and TBoxconsideration), constructed chase-like procedure special kindmaterialization (cf. Section 3), is, answers returned model precisely certainanswers. central observation matches q canonical models forest-shaped ABoxesgrouped equivalence classes induced certain splittings q. finallyshow construct, equivalence class, tree automaton decides existenceforest-shaped witness ABox whose canonical model admits match q falls class.Throughout proof, generally impose UNA.begin reduction CQ-query emptiness ELIF . fact, reduction evenshows suffices consider ELIF -TBoxes normal form, meanCIs take one formsA1 u u v B,v r.B,22r.A v B,fiQ UERY P REDICATE E MPTINESSA, A1 , . . . , , B NC {>, } r role name inverse role.Proposition 28 every Horn-ALCIF TBox , ABox signature , CQ q, one constructpolynomial time ELIF -TBox 0 normal form q empty given iff qempty given 0 .proof Proposition 28 standard given appendix. follows, assumeELIF TBoxes normal form.next define canonical models. Let (T , A) ELIF KB satisfiablew.r.t. . construct (typically infinite) canonical model ,A (T , A), start viewedinterpretation, is: ,A = Ind(A), AIT ,A = {a | A(a) A}, rIT ,A = {(a, b) |r(a, b) A}. exhaustively apply following completion rules:1. A1 u u v Ai ,A 1 n,/ AIT ,A , addAIT ,A .2. r.A v B , (d, e) rIT ,A , e AIT ,A ,/ B ,A , add B ,A ;3. v r.B , AIT ,A , either/ (r.B)IT ,A funct(r) 6/,A,A,A,A,A(r.>), add (d, e) reB, e freshelement./ B ,A , add e4. v r.B , funct(r) , AIT ,A , (d, e) rIT ,A , e,AB.construction rendered deterministic using ordering inclusions domainelements decide among different possible rule applications. reason, may speakcanonical model. call model U universal homomorphism Umodel A, is, function h : U AU implies h(d) AI ,(d, e) rU implies (h(d), h(e)) rI , h(aU ) = aI Ind(A). importantproperty ,A universal.2 fact, following standard prove omitdetails, see example work Lutz Wolter (2012).Lemma 29 Let ELIF -TBox ABox satisfiable w.r.t. . ,Auniversal model (T , A).Let ELIF TBox -ABox satisfiable w.r.t. . easy consequenceLemma 29 -ABox witness CQ q non-empty givensatisfiable w.r.t. match q ,A .next step proof Theorem 27 establish proposition constrains shapeABoxes considered deciding emptiness ELIF . follows,ABox called tree-shaped1. undirected graph GA = (Ind(A), {{a, b} | r(a, b) A}) tree;2. readers wondering relationship universal models materializations defined Section 3,remark every universal model TBox ABox materialization A. Conversely,materialization A, exists also universal model (Lutz & Wolter, 2012).23fiBAADER , B IENVENU , L UTZ , & W OLTER2. a, b Ind(A), one role name r r(a, b) r(b, a) A,one case.working tree-shaped ABoxes, often designate one individuals root.root tree-shaped ABox fixed, use A|a denote restrictionindividuals b whose unique path root GA contains a, call b Ind(A)r-successor (resp. r -successor) Ind(A) r(a, b) A|a (resp. r(b, a) A|a ).also consider (rooted) tree-shaped interpretations tree-shaped queries, defined analogouslytree-shaped ABoxes.-ABox forest-shaped ABoxes A0 , A1 , . . . , Ak followingconditions satisfied:1. union A0 , A1 , . . . , Ak ;2. k |Ind(A0 )|;3. 1 < j k: Ind(Ai ) Ind(Aj ) = |Ind(Ai ) Ind(A0 )| = 1;4. 1 k: Ai tree-shaped ABox rooted individual Ind(A0 ).call A0 root component A1 , . . . , Ak tree components. width k.degree smallest number n every tree component Ai every Ind(Ai ),number assertions r(a, b) r(b, a) Ai bounded n. following propositionclarifies role forest-shaped ABoxes witnesses non-emptiness.Proposition 30 Let ELIF -TBox, ABox signature, q CQ. q non-emptygiven , witnessed -ABox forest-shaped, width |q|,degree |T |.Proposition 30 proved appendix taking witness -ABox A, selecting partsize |q| identified match q serves root component forest-shapedABox, unraveling infinite ABox starting selected part, afterwards removingunnecessary individual names obtain desired degree, finally applying compactnessmake resulting witness finite.Clearly, assume w.l.o.g. forest-shaped witness ABoxes according Proposition 30, individual names used root component taken fixed set Ind cardinality|q|. make assumption without notice follows.next analyze matches forest-shaped ABoxes, using splitting query components. similar splittings queries used Appendix B, simpler. forestsplitting CQ q tuple F = (q 0 , q0 , q1 , . . . , qn , ) q 0 obtained q identifying variables, q0 , q1 , . . . , qn partition atoms q 0 , : var(q0 ) Indfollowing conditions satisfied1. q1 , . . . , qn tree-shaped;2. var(qi ) var(q0 ) 1 1 n;3. var(qi ) var(qj ) = 1 < j n.24fiQ UERY P REDICATE E MPTINESSLet ELIF -TBox, forest-shaped ABox root component A0 , matchq ,A . Note ,A consists extended (potentially infinite) trees attachedABox individuals generated completion rules. type F =(q 0 , q0 , q1 , . . . , qn , ) q 0 obtained q identifying variables sendselement, q0 consists atoms q 0 matches A0 -part ,A , q0 , . . . , qnmaximal connected components q 0 \ q0 , restriction range Ind. Notethat, matter match choose, maximal connected components q 0 \ q0 musttree-shaped match tree-shaped part ,A , consists tree componentplus attached trees generated completion rules. Thus every match typefollowing immediate, WT ,q,F denotes set forest-shaped -ABoxes width|q| degree |T | admit match q type F .Lemma 31 Let ELIF -TBox, ABox signature, q CQ. q emptygiven WT ,q,F empty every forest splitting F q.on, let ELIF -TBox normal form, ABox signature, q CQ,assume want decide whether q empty given . Lemma 31, suffices checkwhether WT ,q,F empty every forest splitting F q.Note defining set WT ,q,F possible definition forest splittingrefer particular ABox, turn due use fixed set individualnames Ind root components forest ABoxes. fact, first quantifying forest splittingsLemma 31 quantifying forest-shaped ABoxes (when testing emptinessWT ,q,F ) essential obtaining single exponential time upper bound. Since numberforest splittings single exponential |q|, obtain bound test emptinessWT ,q,F time single exponential |T | + |q|. achieve constructing,forest splitting F q, two-way alternating parity automaton infinite trees (TWAPA) AFaccepts non-empty language WT ,q,F 6= . Note infinite trees neededautomata take trees input represent (finite) forest-shaped -ABox A, also(potentially infinite) model .start introducing necessary background TWAPAs. Letdenote positiveintegers. tree non-empty (and potentially infinite) set closed prefixes.node root . use standard concatenation words (nodes trees) and,convention, take x 0 = x (x i) 1 = x x . Note 1undefined. 1, node x said child node x, x called parentx i. slightly depart Vardis original definition TWAPAs (Vardi, 1998) workingtrees full, is, define m-ary tree tree whose nodes(rather exactly) children. W.l.o.g., assume nodes m-ary tree{1, . . . , m} . infinite path P prefix-closed set P every n 0,unique x P |x| = n.set X, use B + (X) denote set positive Boolean formulas X, i.e.,formulas built using conjunction disjunction elements X used propositional variables, special formulas true false allowed well. alphabet ,-labeled tree pair (T, V ) tree V : node labeling function.NNNNNDefinition 32 (TWAPA) two-way alternating parity automaton (TWAPA) m-ary treestuple = (S, , , s0 , F ) finite set states, finite alphabet, :25fiBAADER , B IENVENU , L UTZ , & W OLTERB + (tran(A)) transition function tran(A) = {hii, [i] | {1, 0, . . . m}} settransitions A, s0 initial state, F = (G1 , . . . , Gk ) sequence subsetssatisfying G1 G2 . . . Gh = S, called parity condition.Intuitively, transition (hii, s) > 0 means copy automaton state senti-th successor current node, required exist; contrast, transition ([i], s)sends copy i-th successor exists. transitions (hii, s) ([i], s) {1, 0}interpreted similarly 1 indicates sending copy predecessor 0 indicates sendingcopy current node. Note transition (h1i, s) cannot applied root.Definition 33 (Run, Acceptance) run TWAPA = (S, , , s0 , F ) -labeled tree(T, V ) S-labeled tree (T , ) () = (, s0 ) , (y) = (x, s)(s, V (x)) = implies (possibly empty) set {(d1 , s1 ), . . . , (dn , sn )} tran(A)satisfies 1 n:1. di = hji, x j defined, x j , , (y i) = (x j, si ).2. di = [j] x j defined belongs , (y i) = (x j, si ).Given infinite path P , denote inf(P ) set states q infinitelymany P (y) form (d, q). say run (T , ) acceptinginfinite paths P , exists even k inf(P ) Gk 6= inf(P ) Gk1 = .-labeled tree (T, V ) accepted accepting run (T, V ). useL(A) denote set -labeled trees accepted A.note original definition TWAPAs (Vardi, 1998) uses transitions form(hii, q) {1, . . . , m}, since (hii, q) ([i], q) coincide full m-ary trees. easysee emptiness version TWAPAs reduced polynomial time emptinessTWAPAs original definition since encode m-ary trees full m-ary trees. Vardi (1998)shown emptiness problem TWAPAs E XP IME-complete. precisely,algorithm that, given TWAPA = (S, , , s0 , F ), decides whether L(A) = runstime exponential cardinality polynomial cardinality size .also remind reader given two TWAPAs A1 A2 Ai = (Si , , , s0,i , Fi ),easy construct (in polynomial time) TWAPA L(A) = L(A1 ) L(A2 )state set S1 S2 .make accessible TWAPAs, encode forest-shaped -ABoxes width |q|degree |T | m-ary trees, = |q| |T |. already mentioned,tree additionally encodes model encoded ABox. explain alphabet usedshape trees detail. root node labeled element alphabetR consists sig(T )-ABoxes (i) Ind(A) Ind, (ii) r(a, b) implies r ,(iii) satisfies functionality statements . Let sub(T ) denote set conceptsoccur subconcepts. Non-root nodes labeled elements alphabet Nconsists subsets(NC sub(T )) ] {M } ] {r, r | r NR occurs } ] Ind ] {A | B v r.A }26fiQ UERY P REDICATE E MPTINESScontains (i) exactly one role name inverse role, (ii) one element Ind,(iii) either role name inverse role exactly one element form and,latter case, also A.tree hT, `i ` labeling described supposed represent forest-shaped ABox AhT,`i together model IhT,`i ABox . individuals AhT,`iABox labels root , plus non-root nodes whose label containsmarker . nodes denote domain elements IhT,`i identifiedABox individual. assertions AhT,`i concept role memberships IhT,`irepresented labels hT, `i. Note ABox sig(T )-ABox whereas AhT,`i usessignature . fact, -assertions part AhT,`i assertionspart IhT,`i .need impose additional conditions make sure R N -labeled tree hT, `iindeed represents ABox model intended. call hT, `i proper satisfies followingconditions x :1. root labeled element R nodes element N ;2. `(x) N contains element Ind(A) x child root , elementInd otherwise;3. take path remove root node (because carries special label),nodes whose label contains form finite (possibly empty) prefix resulting path;4. child x `(y) N , B v r.A onefollowing true:(a) x root, B `(x) r `(y);(b) x root Ind, B(a) `(x) {a, r} `(y).roles individual names element labels describe elements connectedelements via roles AhT,`i IhT,`i . particular, successor root containsrole r individual name Ind, node represents r-successor a. labelelements form serve special marking purpose: `(x), meanselement x (which part IhT,`i AhT,`i since `(x) cannot contain )satisfy concept inclusion B v r.A. later need special markers makesure IhT,`i model AhT,`i , materialization AhT,`i .explanations subsequent definitions, three conditions imposed elementsfour conditions used define properness make sense reader.Let hT, `i proper R N -labeled tree. define AhT,`i IhT,`i formally. LetABox labels root , let restriction signature .-ABox AhT,`i described hT, `iAhT,`i = {A(x) | `(x) NC `(x)}{r(b, x) | {b, r, } `(x)} {r(x, b) | {b, r , } `(x)}{r(x, y) | child x `(x) `(y) r `(y)}{r(y, x) | child x `(x) `(y) r `(y)}27fiBAADER , B IENVENU , L UTZ , & W OLTERinterpretation IhT,`i follows:IhT,`i= (T \ {}) Ind(A)IhT,`i= {a | A(a) A} {x | `(x) NC }rIhT,`i= {(a, b) | r(a, b) A} {(a, x) | {a, r} `(x)} {(x, a) | {a, r } `(x)}{(x, y) | child x r `(y)} {(x, y) | x child r `(x)}cIhT,`i= cc Ind(AhT,`i )Apart represented ABox instead interpretation, AhT,`i identicalrestriction IhT,`i individuals AhT,`i symbols , particular meansIhT,`i model AhT,`i . Note ABox AhT,`i forest-shaped -ABox. Conversely,forest-shaped -ABox width |q| degree |T |, define properm-ary R N -labeled trees hT, `i AhT,`i = IhT,`i = ,A .Let F = (q 0 , q0 , q1 , . . . , qn , ) splitting q. build TWAPA AF m-aryR N -labeled trees accepts exactly trees hT, `i AhT,`i WT ,q,F .number states AF polynomial |T | + |q| since checked time singleexponential number states whether L(AF ) = , obtain desired E XP IME upperbound deciding whether WT ,q,F = . construct AF intersection followingTWAPAs:1. Aprop , makes sure input tree proper;2. , ensures input tree hT, `i IhT,`i model ;3. Awf ensures hT, `i satisfies certain well-foundedness condition;4. Amatch guarantees, exploiting conditions ensured previous automata,input tree hT, `i AhT,`i WT ,q,F .construction first automaton Aprop straightforward, details left reader.Note that, enforce Condition 3 proper trees, automaton needs make use parityacceptance condition (a co-Buchi condition would actually sufficient). second TWAPAensures following conditions satisfied non-root nodes x, x0 input tree:r(a, b) `() funct(r) , {a, r} 6 `(x);funct(r) {a, r} `(x) `(x0 ), x = x0 ;funct(r) , x one child r `(y), additionally r `(x),child y;A1 u u v {A1 (a), . . . , (a)} `(), A(a) `();A1 u u v {A1 , . . . , } `(x), `(x);v r.B A(a) `(), (i) b {r(a, b), B(b)} `(),(ii) child x root {a, r, B} `(x);28fiQ UERY P REDICATE E MPTINESSv r.B `(x), (i) {a, r } `(x) B(a) `(), (ii) r `(x)x non-root parent B `(y), (iii) x child {r, B} `(y);r.A v B (i) {r(a, b), A(b)} `() (ii) child root{a, r, A} `(y), B(a) `();r.A v B (i) {a, r } `(x) A(a) label root, (ii) r `(x)x parent `(y), (iii) x child {r, A} `(y), B `(x).Working exact details Aprop left reader.Ideally, would like third automaton Awf ensure IhT,`i canonical modelAhT,`i . However, seem easily possible model constructedapplying completion rules certain order difficult simulate automatonnoteapplying rules different order might result construction interpretationisomorphic one obtained following prescribed application order. thus defineAwf achieve crucial property canonical models positive information (conceptrole memberships domain elements) reason, namely containedAhT,`i logically implied AhT,`i together . formalize termsderivations.Let hT, `i proper R N -labeled tree, A0 (NC sub(T )) {>}, x0 A0 hT,`i .derivation A0 x0 hT, `i finite L-labeled tree hT 0 , `0 i, L set pairs (A, x)(NC sub(T )) {>} x AIhT,`i . require root 0 labeled(A0 , x0 ) 0 minimal nodes z 0 `0 (z) = (A, x), onefollowing holds:1. {>} x Ind(AhT,`i );2. 6 `(x) CI A1 u u v children z1 , . . . , zn z 0`0 (zi ) = (Ai , x) 1 n;3. 6 `(x) CI r.A0 v child z 0 z 0 `0 (z 0 ) = (A0 , x0 )(x, x0 ) rIhT,`i . Moreover, B `(x), child z 00 z 0`0 (z 00 ) = (B, x).4. 6 `(x) CI A0 v r.A funct(r) child z 0 z 0`0 (z 0 ) = (A0 , x0 ) (x0 , x) rIhT,`i . Moreover, B `(x), child z 00z 0 `0 (z 00 ) = (B, x).5. = >, > 6 `(x), B `(x), child z 0 z 0 `0 (z 0 ) = (B, x).6. `(x), CI A0 v r.A , child z 0 z 0 `0 (z 0 ) =(A0 , x0 ) (x0 , x) rIhT,`i either (i) x child x0 , (ii) x childroot, x0 Ind, {r, x0 } `(x).say hT, `i well-founded whenever x AIhT,`i , NC {>},derivation x hT, `i. hard construct TWAPA Awf accepts preciselywell-founded proper R N -labeled trees; essentially, automaton verify existencerequired derivations implementing Conditions 1 6 transitions, additionallyusing co-Buchi condition ensure finiteness derivation.Next let F = (q 0 , q0 , q1 , . . . , qn , ). automaton Amatch checks29fiBAADER , B IENVENU , L UTZ , & W OLTER1. match q0 IhT,`i2. match qi IhT,`i v var(q0 ) var(qi ), (v) = (v).Amatch easy construct omit details. announced, define AFaccepts intersection languages accepted Aprop , , Awf , Amatch . remainsshow WT ,q,F empty iff L(AF ) empty.this, first clarify relation well-foundedness, canonical models, universal models. call proper R N -labeled tree hT, `i canonical (i) IhT,`i canonicalmodel AhT,`i , (ii) every x \{} 6 `(x), concept `(x)element x created due application third completion rule inclusionform B v r.A parent x .Lemma 341. proper R N -labeled tree canonical, well-founded;2. hT, `i proper R N -labeled tree well-founded IhT,`i model ,IhT,`i universal model AhT,`i .proof Lemma 34 found appendix. Point 1 established tracing applicationscompletion rules applied construct canonical model AhT,`i showingaddition make gives rise derivation. Point 2, first show onemake certain uniformity assumption derivations show define homomorphismIhT,`i model AhT,`i starting part IhT,`i corresponds rootcomponent AhT,`i moving downwards along tree-shaped parts IhT,`i .Lemma 35 WT ,q,F = iff L(AF ) = .Proof. First assume WT ,q,F 6= . forest-shaped -ABox width|T | degree |q| satisfiable w.r.t. match q ,A type F .Let hT, `i m-ary proper R N -labeled tree satisfies AhT,`i = canonical.hT, `i L(Aprop ). Since satisfiable w.r.t. , IhT,`i = ,A model thushT, `i L(Aprop ). Point 1 Lemma 34, hT, `i L(Awf ). Finally, match witnessesConditions 1 2 definition Amatch satisfied thus hT, `i L(Amatch )done.Conversely, assume tree hT, `i L(AF ). Since hT, `i L(Aprop ), = AhT,`idefined; definition, forest-shaped -ABox width |T | degree |q|.remains show match q ,A type F . Since hT, `i L(AT )L(Awf ), IhT,`i model hT, `i well-founded. Point 2 Lemma 34, IhT,`i thusuniversal model A. hT, `i L(Amatch ), Conditions 1 2 definitionAmatch satisfied. verified that, consequently, match q IhT,`itype F . Composing homomorphism IhT,`i ,A , exists since IhT,`iuniversal, yields match q ,A type F .30fiQ UERY P REDICATE E MPTINESS6. DL-Lite Familystudy query predicate emptiness DL-Lite family description logics (Calvaneseet al., 2007; Artale et al., 2009). begin with, introduce dialects DL-Lite consider.Basic concepts B definedB::=>||r.>ranges NC r (possibly inverse) roles. DL-Litecore TBox finiteset CIs form B1 v B2 B1 u B2 v , B1 B2 basic concepts. Thus,DL-Litecore included ELI but, includes inverse roles, included EL. DL-LiteFextension DL-Litecore functionality statements. DL-LiteR extension DLLitecore role inclusions r v s, r, roles. DL-LiteR logical underpinningOWL profile OWL2 QL (Motik et al., 2009). Finally, DL-Litehorn extension DL-Litecoreconjunctions basic concepts left hand side CIs. Alternatively, definedfragment ELI qualified existential restrictions r.C replaced unqualified existentialrestrictions r.>. details, refer readers work Calvanese et al. (2007), Artaleet al. (2009), Calvanese, De Giacomo, Lembo, Lenzerini, Rosati (2013).briefly discuss UNA DL-Lite dialects introduced above. First observe DLLitehorn DL-LiteF fragments ALCIF. Thus, Lemma 3, query emptiness predicate emptiness DL-Litehorn DL-LiteF oblivious whether UNA made not.DL-LiteR fragment ALCIF. is, however, straightforward show DL-LiteRcertain answers CQs depend whether one adopts UNA not. Thus, alsoDL-LiteR query emptiness predicate emptiness oblivious whether UNA madenot. following proofs make UNA.main results follows: CQ-query emptiness coNP-complete DL-Lite dialects.coNP-lower bound holds already fragment DL-Litecore without role names.contrast, complexity deciding IQ-query emptiness CQ-predicate emptiness dependswhether conjunctions admitted left-hand side concept inclusions not.conjunctions admitted (as DL-Litecore , DL-LiteR , DL-LiteF ), IQ-query emptinessCQ-predicate emptiness NL OG PACE-complete. conjunctions admitted (as DLLitehorn ), IQ-query emptiness CQ-predicate emptiness coNP-complete. Again,lower bound holds already fragments DLs without role names.note follows use Theorem 6 gives polynomial reductionCQ-predicate emptiness IQ-query emptiness certain DLs apply DL-Litedialects. Instead give direct proofs. results presented Figure 1 DL-Lite dialectsstraightforward consequences results established section.begin proving coNP lower bounds. Let Lcore DL admits CIs v Bu B v , let Lhorn DL admits CIs u A0 v B u B v ,A, A0 , B concept names.Theorem 36 Lhorn , IQ-query emptiness, CQ-query emptiness, CQ-predicate emptinesscoNP-hard. Lcore , CQ-query emptiness coNP-hard.Proof. proofs reduction well-known coNP-complete problem testing whetherpropositional formula conjunctive normal form (CNF) unsatisfiable. Let = 1 k31fiBAADER , B IENVENU , L UTZ , & W OLTERCNF formula, v1 , . . . , vn variables used , A1 , . . . , Ak concept names representing clauses, Av1 , Av1 , . . . , Avn , Avn concept names representing literals. Letadditional concept name, set = {Av1 , Av1 , . . . , Avn , Avn }. Consider Lhorn -TBoxconsisting following CIs:Avj u Avj v 1 j n;A`j v Ai 1 k `j = ()vj disjunct ;A1 u u Ak v .straightforward show A(u) empty given iff u A(u) empty giveniff unsatisfiable. Thus, deciding IQ-query emptiness, CQ-predicate emptiness, CQ-queryemptiness Lhorn coNP-hard. coNP-hardness result CQ-query emptiness Lcore ,drop last CI use CQ A1 (u) Ak (u) instead.prove matching upper complexity bounds, considering logics DL-Litecore , DLLiteR , DL-LiteF , DL-Litehorn . end, formulate general sufficient conditionsdeciding emptiness PT IME coNP. say DL L polysize emptiness witness property whenever CQ q empty given , exists -ABoxpolynomial size size q satisfiable w.r.t. certT ,A (q) 6= .Lemma 37 Let L description logic polysize emptiness witness propertyquery evaluation problem CQs L NP. Moreover, assume satisfiability ABoxesw.r.t. L-TBoxes decided polynomial time. CQ-query emptiness L coNP.Proof. NP-algorithm deciding whether CQ q empty w.r.t. guesses (i) -ABoxpolynomial size q, (ii) tuple ~a individual names Ind(A) appropriatelength, (iii) polysize certificate ~a certT ,A (q); verifies polynomial timesatisfiable w.r.t. guessed certificate valid.Theorem 38 DL-Litecore , DL-LiteR , DL-LiteF , DL-Litehorn , deciding CQ-query emptinesscoNP.Proof. conditions stated Lemma 37 shown Calvanese et al. (2007) Artaleet al. (2009). sketch proof polysize emptiness witness property. Assume ~a certT ,A (q)CQ q = ~u (~v , ~u) TBox DLs listed theorem statementassume satisfiable w.r.t. . use canonical model ,A Lemma 29(for DL-Litecore , DL-LiteF , DL-Litehorn used without modification sincefragments ELIF ; DL-LiteR , one add following completion ruleconstruction ,A : (x, y) rIT ,A r v , add (x, y) sIT ,A ). Letmatch q ,A . recall ,A consists restriction individuals aIT ,AInd(A) tree-shaped interpretations Ia attached aIT ,A . Let A00 set assertionsuse individual names Ind(A) exists v var(q) (v) = aIT ,A(v) Ia . Moreover, individual Ind(A00 ) selected existsrole r b r(a, b) A, select one r(a, b ) include A01 . Let A0 = A00 A01 .Clearly ABox A0 required: polynomial size, satisfiable w.r.t. (being subsetA), construction, satisfies ~a certT ,A0 (q).32fiQ UERY P REDICATE E MPTINESSsay DL L singleton emptiness witness property whenever CQ q formA(v) v A(v) empty given , exists -ABox containing oneassertion satisfiable w.r.t. certT ,A (q) 6= .Lemma 39 Let L description logic singleton emptiness witness propertyquery evaluation problem CQs form A(v) v A(v) L NL OG PACE. Moreover, assume satisfiability singleton ABoxes w.r.t. L-TBoxes decided NL OG PACE.IQ-query emptiness CQ-predicate emptiness L NL OG PACE.Proof. non-deterministic logarithmic space algorithm deciding whether CQ form A(v)v A(v) empty w.r.t. iterates -ABoxes containing one assertionchecks whether least one ABoxes satisfiable w.r.t. satisfies , |= v A(v)or, respectively, , |= A(a), individual Ind(A).Theorem 40 DL-Litecore , DL-LiteR , DL-LiteF , deciding IQ-query emptiness CQpredicate emptiness NL OG PACE-complete.Proof. NL OG PACE-upper bound, conditions stated Lemma 39 shownCalvanese et al. (2007) Artale et al. (2009). sketch proof singleton emptinesswitness property. Assume ~a certT ,A (q) CQ q form A(v) v A(v) TBoxDLs listed theorem statement. assume satisfiable w.r.t. .consider case q = v A(v); case q = A(v) similar. proof Theorem 38,use canonical model ,A . Let mapping ,A (v) AIT ,Aconsider uniquely determined Ind(A) (v) = aIT ,A (v) Ia . Usingfact conjunctions occur left-hand side CIs , one show existssingle assertion form B(a) r(a, b) ABox A0 consisting assertion,() certT ,A0 (q). follows A0 desired witness ABox.matching lower bound follows directly fact deciding whether |= v BNL OG PACE-hard TBoxes DL-Litecore (Artale et al., 2009).7. Case Study Application Modularitydemonstrate usefulness emptiness two ways. First, carry case studypredicate emptiness medical domain, find use realistic ontology addssignificant number non-empty predicates ABox signature also largenumber predicates empty. static analysis, thus potentially non-trivial usermanually distinguish non-empty empty predicates. Second, show (predicate)emptiness used produce smaller version TBox tailor-made queryinggiven ABox signature (in sense: module TBox). Replacing potentiallymuch smaller module facilitates comprehension TBox, thus helping query formulation.support claims experiments.case study, use comprehensive medical ontology SNOMED CT, providessystematic vocabulary used medical information interchange enable interoperable electronic health records. covers diverse medical areas clinical findings, symptoms, diagnoses, procedures, body structures, organisms, substances, pharmaceuticals, devices specimens.33fiBAADER , B IENVENU , L UTZ , & W OLTERconcepts rolesIQCQ axiomsaxiomsnon-empty non-empty -mod. CQ -core50016355746318910459750031365447348911469610001658277385141107349100031624277621414777315000161833021451334692142750003118469215573361621532100001629519334934704433489100003130643346454725634637Figure 4: Experimental ResultsSNOMED CT formulated EL extended role inclusions (which removed experiments). contains 370,000 concept names 62 role names. use SNOMED CTtogether ABox signature real-world application randomly generated ABoxsignatures. real-world signature obtained analyzing clinical notes emergencydepartment intensive care unit two Australian hospitals, using natural language processing methods detect SNOMED CT concepts roles.3 contains 8,858 concepts 16 roles.signature, 16,212 IQ-non-empty predicates 17,339 CQ-non-empty predicatescomputed. Thus, SNOMED CT provides substantial number additional predicates queryformulation, roughly identical number predicates ABox signature. However,numbers also show majority predicates SNOMED CT cannot meaningfully usedqueries -ABoxes, thus identifying relevant ones via predicate emptiness potentiallyhelpful. Somewhat surprisingly, number CQ-non-empty predicates 10%higher number IQ-non-empty symbols.analyzed randomly generated signatures contain 500, 1,000, 5,000, 10,000concept names 16 31 role names (1/2 1/4 role names ontology). Everysignature contains special role name role-group, used SNOMED CT implementcertain modeling pattern present also ABoxes allow pattern there.number concept role names, generated 10 signatures. columns IQ nonempty CQ non-empty Figure 4 show results, numbers averages10 experiments size. additional experiments confirm findings real-worldsignature: case, substantial number additional predicates becomes available queryformulation, also large number predicates empty.come application modularity. Recall main motivation studyingemptiness support query formulation: TBoxes large complex, difficultunderstand whether TBox contains sufficient background knowledge given query qnon-empty answer -ABox. case, clearly make sense3. See Current Collaborative Projects Health Information Technologies Research Laboratory UniversitySydney (HITRL, 2016).34fiQ UERY P REDICATE E MPTINESSpose q -ABox TBox used background ontology. Similarly,hard find whether TBox sufficiently powerful entail given predicate occurquery non-empty answer -ABox. Again, case,predicate used formulating queries. Here, go one step further: insteadusing emptiness directly support query formulation, use simplify TBox.precisely, consider problem extracting (hopefully small!) subset given TBoxgives exactly answers CQs (or IQs) -ABox. subset called-substitute w.r.t. CQ (or IQ, respectively) original TBox replace originalTBox answering CQs (or IQs, respectively). Working small -substitute insteadoriginal TBox supports comprehension TBox thereby formulation meaningfulqueries.beyond scope paper investigate -substitutes depth. Instead, show that,description logic ELI, predicate emptiness gives rise particularly natural kind substitute call CQ -core. CQ -core obtained removing concept inclusionscontain predicate CQ-predicate empty w.r.t. TBox. Thus,CQ -core give answers CQs original TBox -ABoxes, alsoappealing property predicates occur used meaningfully CQquerying -ABoxes.also show widely known semantic -modules introduced Grau et al. (2008)-substitutes CQ -cores cannot larger semantic -modules (unless originalontology contains tautological concept inclusions). evaluate method practice comparesize CQ -cores -modules, also extend case study based SNOMED CTextraction CQ -cores comparison -modules. start defining -substitutesformal way.Definition 41 Let 0 Q {IQ, CQ}. 0 -substitute w.r.t. Q-ABoxes q Q, certT 0 ,A (q) = certT ,A (q).aware -substitutes according Definition 41 studied before,closely related types modules. example, -modules give answersCQs formulated signature -ABoxes studied work Lutz Wolter (2010),Kontchakov, Wolter, Zakharyaschev (2010), Konev, Ludwig, Walther, Wolter (2012), Botoeva, Kontchakov, Ryzhikov, Wolter, Zakharyaschev (2014, 2016), Romero, Kaminski,Grau, Horrocks (2015). stronger version module provided -modules require original TBox model-conservative extension module regarding signature, studied Konev, Lutz, Walther, Wolter (2013) Gatens, Konev, Wolter (2014).However, important difference -modules -substituteslatter restrict signature ABox, queries. contrast, mentioned-modules guarantee answers CQs formulated signature (and -ABoxes).particular, follows minimal modules, defined work Kontchakov et al. (2010)Konev et al. (2013), general used -substitute.show ELI (and, therefore, also fragment EL) one use CQ-predicateemptiness straightforward way compute -substitute w.r.t. CQ. Let TBoxABox signature. CQ -core , denoted TCQ , set concept inclusionsX sig() CQ-predicate empty given .35fiBAADER , B IENVENU , L UTZ , & W OLTERTheorem 42 Let TBox ELI. CQ -core -substitute w.r.t. CQ(and thus also w.r.t. IQ).Proof. Let 0 CQ -core assume 0 , 6|= q[~a] -ABox A. Considercanonical model 0 ,A , introduced Section 5.2. 0 ,A model 0 A,0 ,A 6|= q[~a]. sufficient show 0 ,A model . Let C v \ 0assume 0 ,A 6|= C v D. C 0 ,A 6= . Let qC (v) tree-shaped conjunctivequery corresponding C, constructed standard way (see Appendix B formal definitionsimilar construction). 0 ,A |= v qC (v) 0 , |= v qC (v). Hence , |=v qC (v) X sig(C) CQ-empty given . Since C v , alsoobtain , |= v qD (v), qD (v) tree-shaped conjunctive query corresponding D.Thus, X sig(D) CQ-empty given . means C v 0 ,contradiction.Note Theorem 22, CQ -core computed polynomial time EL-TBox.make simple observations regarding CQ -cores:1. Theorem 42 fails DLs admit negation. example, = {A v B, B v E}= {A}, -substitute w.r.t. CQ coincides , CQ -coreempty.2. CQ -core always minimal -substitute w.r.t. CQ. Consider, example, ={A v B1 , v B2 , B1 v B2 } let = {A}. 0 = {A v B1 , v B2 }-substitute w.r.t. CQ CQ -core coincides .3. let IQ -core TBox defined analogy CQ -core , based IQemptiness instead CQ-emptiness. IQ -core cannot serve -substitutew.r.t. IQ even EL-TBox. example, let = {A v r.B, r.B v E}= {A}. B IQ-empty given IQ -core empty. However,empty TBox -substitute w.r.t. IQ since , |= E(a) = {A(a)}.Interestingly, contrast -modules discussed above, -modules introduced Grauet al. (2008) turn examples -substitutes. define -modules, let signature.00Two interpretations 0 coincide w.r.t. = X = X X .subset 0 TBox called semantic -module w.r.t. every interpretation0interpretation 0 coincides w.r.t. sig(T 0 ) X = X 6sig(T 0 ) model \ 0 . shown work Grau et al. (2008) extracting minimalsemantic -module complexity standard reasoning (that is, subsumption).addition, shown syntactic approximation called syntactic -module computedpolynomial time (every syntactic -module semantic -module, necessarilyway around). following lemma establishes relationship -modules substitutes. concept inclusion C v tautological |= C v D.Proposition 43 Let TBox formulated DLs introduced paper, let 0semantic -module w.r.t. .1. 0 -substitute w.r.t. CQ;36fiQ UERY P REDICATE E MPTINESS2. sig(T 0 ) contains predicates CQ-empty given ;3. ELI-TBox contain tautological CIs, CQ -corecontained 0 .Proof. Point 1, suppose 0 , 6|= q[~a], 0 semantic -module w.r.t.-ABox. Let model 0 6|= q[~a], consider interpretation 0coincides 0 sig(T 0 ) X = remaining predicates X.0 model since 0 semantic -module w.r.t. . 0 model since-ABox. Since shrank extension predicates transitioning 06|= q[~a], 0 6|= q[~a]. Hence , 6|= q[~a], required.Point 2, assume X CQ-empty given , X 6 sig(T 0 ). Suppose X =concept name (the case X = r role name r similar left reader). Take-ABox satisfiable w.r.t. , |= v A(v). Let model (T , A),0let 0 interpretation coincides sig(T 0 ) =0remaining (in particular AI = ). definition semantic -modules, 0 model(T , A). derived contradiction 0 shows , 6|= v A(v).Point 3, assume formulated ELI contains tautological inclusions. Let C v\ 0 . Then, definition semantic -modules, sig(C v D) contains predicateX 6 sig(T 0 ) (because otherwise C v tautology). Thus, Point 2, sig(C v D)contains predicate CQ-empty given . C v CQ -core ,required.Point 1, use algorithms computing syntactic semantic -modulesones provided work Grau et al. (2008) find -substitutes large variety DLs.Point 2, modules also provide over-approximation set predicatesCQ-empty. Finally, Point 3 means that, ELI, -modules cannot smaller CQ -coreunless tautological concept inclusions. general, however, -modules largerCQ -core TBox. following example shows case already acyclicEL-TBoxes: let= {A v s1 .r1 .> u s2 .r2 .>, B r1 .> u r2 .>}= {A}. predicates CQ-empty given A, s1 , s2 , r1 , r2 . HenceCQ -core contains first CI . However, non-trivial semantic -modulesw.r.t. (and thus syntactic ones either).demonstrate potential usefulness -substitutes CQ -core extendingcase study medical domain. use ontology SNOMED CT ABoxsignatures described beginning section, analyzing size CQ -core comparing size original ontology syntactic -module. real-worldsignature, CQ -core contains 17,322 370,000 concept inclusions SNOMED CT. Thus,5% size original ontology. -module w.r.t. turns significantlylarger CQ -core, containing 27,383 axioms. random signatures, sizes CQ cores -modules shown two right-most columns Figure 4. confirmfindings real-world signature: CQ -core much smaller original ontology-module.37fiBAADER , B IENVENU , L UTZ , & W OLTERQ UERY C ONTAINMENTQ UERY E MPTINESSDLIQCQIQCQELE XP IME-c.E XP IME-c.PT IME-c.PT IME-cELE XP IME-c.E XP IME-c.E XP IME-c.E XP IME-cELI, Horn-ALCIFE XP IME-c.2E XP IME-c.E XP IME-c.E XP IME-c.PT IMEcoNP-c.NL OG PACE-c.coNP-c.coNP-c.p2 -c.coNP-c.coNP-c.NE XP IME-c.NE XP IME-h.,NE XP IME-c.NE XP IME-c.NE XP IME-c.2E XP IME-c.DL-LitecoreDL-LitehornALC2NE XP IMEALCINE XP IME-c.2NE XP IME-c.Figure 5: Query Containment vs Query Emptiness8. Related WorkQuery emptiness fundamental problem static analysis database queries. alsocalled query satisfiability problem. XML, example, takes following form: givenXPath query p DTD D, exist XML document conformsanswer p non-empty. complexity problem ranges tractableundecidable depending XPath fragment, see e.g. work Benedikt et al. (2008)references therein. DL context, query emptiness first considered work LubyteTessaris (2008), use step guide enrichment ontologies.query emptiness problem studied paper special case following querycontainment problem, first considered work Bienvenu, Lutz, Wolter (2012).regard pair (T , q) consists TBox query q compound query Q, calledontology-mediated query (OMQ), answers Q certain answers q w.r.t.(Bienvenu et al., 2014). take two OMQs Qi = (Ti , qi ), {1, 2}, q1 q2 IQsCQs arity. Q1 -contained Q2 , ABox signature , ABoxes satisfiable w.r.t. T1 T2 , certT1 ,A (q1 ) certT2 ,A (q2 ). case,write Q1 Q2 . notion containment generalizes traditional query containmentproblem DLs (Calvanese et al., 2007) relativizing ABox signature admittingdistinct TBoxes T1 T2 . Query emptiness IQ q given clearly polynomiallyreduced -containment setting T1 = , q1 = q, T2 = , q2 = A(x) fresh conceptname A, similarly CQs. Deciding -containment, however, often computationally harderdeciding query emptiness. Table 5 summarizes known results; results EL DLLite work Bienvenu et al. (2012), results ELI work Bienvenu,Hansen, Lutz, Wolter (2016), results ALC ALCI work Bienvenuet al. (2014) Bourhis Lutz (2016).Query emptiness also closely related explaining negative answers queries. problemstudied, example, Calvanese, Ortiz, Simkus, Stefanoni (2013). Adopting abductive reasoning approach, described follows. Assume , 6|= q(~a) TBox , ABoxA, query q. explain ~a answer q, one wants find minimal ABoxes E38fiQ UERY P REDICATE E MPTINESScertain signature interest E satisfiable w.r.t. , E |= q(~a).-ABoxes E regarded explanation missing answer used debugging purposes. shown work Calvanese et al. (2013) query emptiness IQsBoolean CQs reduces (under many-one logarithmic space reductions) problem decidingexistence explanation , 6|= q(~a) = . DL-LiteA , reduction even worksunions conjunctive queries arity. Calvanese et al. (2013) use observation obtainlower complexity bounds explaining negative query answers, exploiting results publishedconference predecessor paper (Baader, Bienvenu, Lutz, & Wolter, 2010). alsoconjecture that, conversely, techniques proving upper complexity bounds query emptiness(such ones paper) used obtain upper bounds explaining negative answers.9. Conclusioninvestigated computational complexity query predicate emptiness EL,DL-Lite, ALC families DLs, concentrating instance queries conjunctive queriesshowing complexities range NL OG PACE undecidable. also highlighted that,different DLs query languages, different kinds witness ABoxes sufficient establishnon-emptiness. DLs queries considered paper, would interestinginvestigate future work, include following:DLs include transitive roles, role inclusions, symmetric roles, role inclusion axioms(Horrocks, Kutz, & Sattler, 2006; Kazakov, 2010).cases, straightforward reductions results presented paper possible.example, IQ-query emptiness Horn-SHIF decidable E XP IME since everyHorn-SHIF TBox , IQ A(x), ABox signature , one construct polynomialtime Horn-ALCIF TBox 0 , |= A(a) iff 0 , A, |= A(a) -ABoxes(Hustadt et al., 2007; Kazakov, 2009). cases, CQ-query emptinessHorn-SHIF, seems reduction.DLs include nominals.important classes queries unions conjunctive queries (UCQs).materializable DLs Horn-ALCIF,W UCQ query emptiness reduced CQquery emptiness since every UCQ q = iI qi (~x), , |= q(~a) iff, |= qi (~a). simple reduction work non-Horn DLsALC.would also interesting develop practical algorithms emptiness evaluate algorithms real-world ontologies queries. Note algorithms EL DL-Liteeasily implementable efficient presented paper. actually confirmedcase study Section 7. work required design efficient algorithms expressive DLs. Finally, would relevant investigate notion -substitute introducedapplication modularity detail. example, open question compute minimal -substitutes expressive DLs ALC practice, involvedcomplexities.39fiBAADER , B IENVENU , L UTZ , & W OLTERAcknowledgementsfirst author partially supported cfaed (Center Advancing Electronics Dresden)second author partially supported ANR project PAGODA (ANR-12-JS02-007-01).grateful Julian Mendez Dirk Walther supporting us case study. wouldlike thank anonymous reviewers provided excellent comments helped us improvepaper.Appendix A. Proofs Section 3formulate result proved again.Lemma 3 Let ALCIF-TBox. CQ q empty given UNA iffempty given without UNA.Proof. Consider CQ q answer variables v1 , . . . , vn .(Only if) Assume q non-empty given without UNA. ABox satisfiable w.r.t. without UNA certT ,A (q) 6= without UNA.Take model suppose without loss generality infinite. Defineequivalence relation Ind(A) setting b whenever aI = bI . Choose singlerepresentative equivalence class, denote representative equivalenceclass containing a. Let A0 ABox obtained replacing individual .show A0 satisfiable w.r.t. UNA certT ,A0 (q) 6= UNA.Regarding satisfiability, easy see model (T and) A0 satisfiesUNA individuals appearing A0 . Moreover, since infinite, reinterpretindividual names NI \ Ind(A0 ) obtain interpretation model A0 satisfiesUNA.showing certT ,A0 (q) 6= UNA. Take (a1 , . . . , ) certT ,A (q) withoutUNA. aim show (a1 , . . . , ) certT ,A0 (q). Let J 0 model A00satisfies UNA. show match q J 0 (vi ) = (ai )J every01 n. Consider interpretation J obtained J 0 setting aJ = (a )J everyInd(A). easy see J model without UNA, J |= q[a1 , . . . , ]match q J (vi ) = aJevery 1 n. alsodesired match q J 0 , finishes proof.(If) Assume q non-empty given UNA. -ABoxsatisfiable w.r.t. UNA certT ,A (q) 6= UNA. Clearly,also satisfiable w.r.t. without UNA remains show certT ,A (q) 6= withoutUNA. Let (a1 , . . . , ) certT ,A (q) UNA, let model withoutUNA. show |= q[a1 , . . . , ]. Ind(A), let Ia followingunfolding aI :domain Ia Ia consists words d0 r0 d1 rk1 dk d0 , . . . , dkr0 , . . . , rk1 (potentially inverse) roles d0 = aI , (di , di+1 ) riI < k,ri 6= ri+1 functional ri+1 < k, r0 (a, b) 6 b Ind(A) r0functional.AIa = {d0 r0 d1 dk | dk AI } NC ;40fiQ UERY P REDICATE E MPTINESSrIa = {(d0 r0 d1 dk , d0 r0 d1 dk rk+1 dk+1 ) | r = rk+1 }{(d0 r0 d1 dk rk+1 dk+1 , d0 r0 d1 dk ) | r = rk+1 } r NR .Assume Ia mutually disjoint let individual name root Ia .obtain interpretation J taking disjoint union Ia , Ind(A), adding (a, b) rJwhenever r(a, b) A, setting aJ = Ind(A). One show J modelUNA. Thus J |= q[a1 , . . . , ] match q J (vi ) = (ai )Jevery 1 n. verified 0 defined setting 0 (vi ) = aI (vi ) = Ind(A)0 (vi ) = dk (v) = d0 , . . . , dk k 1 match q I, thus |= q[a1 , . . . , ],required.Appendix B. Proofs Section 4restate first result proved.Theorem 16 ALC, CQ-query emptiness NE XP IME.general idea proving Theorem 16 follows. Given ALC-TBox , signature ,CQ q, Theorem 13 suffices test whether , , 6|= q. thus start computing , . check whether , , 6|= q, guess extension set 0concept inclusions extension , set A0 ABox assertions 0 A0satisfied models , q match; subsequently, remainstest satisfiability , A0 w.r.t. 0 . subtlety lies selecting class extensionsguessed careful enough way final satisfiability check carriedNE XP IME.reuse technical definitions results work Lutz (2008) provescombined complexity CQ-answering DL SHQ E XP IME. definitionsslightly modified since Lutz considers CQs without answer variables uses DL SHQ,ALC proper fragment. However, straightforward verify proofs givenLutz also work modified definitions.CQ q viewed directed graph Gdq = (Vqd , Eqd ) Vqd = var(q) Eqd ={(v, v 0 ) | r(v, v 0 ) q r NR }. call q directed tree-shaped Gdq directed treer(v, v 0 ), s(v, v 0 ) q implies r = s. q directed tree-shaped v0 root Gdq , call v0root q. U var(q), write q|U denote restriction q atoms containvariables U . set DTrees(q) directed tree-shaped subqueries q defined follows:DTrees(q) = {q|U | U = Reachq (v), v var(q), q|U directed tree-shaped}Reachq (v) set variables reachable v Gdq . sayq 0 obtained q performing fork elimination q 0 obtained q selecting twoatoms r(v 0 , v) s(v 00 , v) v 0 , v 00 , v qvar(q) v 0 6= v 00 , identifying v 0v 00 ;q 0 fork rewriting q q 0 obtained q repeatedly (but necessarily exhaustively) performing fork elimination;41fiBAADER , B IENVENU , L UTZ , & W OLTERq 0 maximal fork rewriting q q 0 fork rewriting fork eliminationpossible q 0 .following shown work Lutz (2008), plays central role subsequentdefinitions.Lemma 44 variable renaming, every CQ unique maximal fork rewriting.following definitions splittings spoilers also taken work Lutz. understand splitting CQ q intuitive level, useful consider matches q modelTBox ABox special shape: consists core part whose elements exactly(the interpretations of) ABox individuals tree-shaped parts attachedelement core part disjoint other. fact, proved Lutz , 6|= q,model described shape 6|= q. match q modeldescribed shape partitions variables q several sets: set R contains variables matched ABox individual; sets S1 , . . . , Sn represent disjoint tree-shapedsubqueries q matched tree part whose root connected variableR via role atom q; set represents collection tree-shaped subqueriesq disconnected variables R Si . addition partitioning, splittingsrecord variable R set S1 , . . . , Sn connected ABox elementsvariables R mapped to. define splittings formally.Let K = (T , A) ALC-knowledge base q CQ. splitting q w.r.t. K tuple =hR, T, S1 , . . . , Sn , , i, R, T, S1 , . . . , Sn partitioning var(q), : {1, . . . , n} Rassigns set Si variable (i) R, : R Ind(A) assigns variable Rindividual A. splitting satisfy following conditions:1. CQ q|T variable-disjoint union directed tree-shaped queries;2. queries q|Si , 1 n, directed tree-shaped;3. r(v, v 0 ) q, one following holds: (i) v, v 0 belong set R, T, S1 , . . . , Sn(ii) v R, (i) = v, v 0 Si root q|Si ;4. 1 n, unique r NR r((i), v0 ) q, v0 root q|Si ;5. avar(q) R.Let q directed tree-shaped CQ. define ALC-concept Cq,v v var(q):v leaf Gdq , Cq,v =otherwise, Cq,v =uA(v)qAuu C;u r.CA(v)qr(v,v 0 )qq,v 0 .v root q, use Cq abbreviate Cq,v .following, allow compound concepts negated roles used ABox assertions. semantics assertions corresponding KBs defined expected way:interpretation satisfies C(a) aI C satisfies r(a, b) satisfy r(a, b).Let = hR, T, S1 , . . . , Sn , , splitting q w.r.t. K q1 , . . . , qk (directedtree-shaped) disconnected components q|T . ALC-knowledge base (T 0 , A0 ) spoiler q,K, one following conditions hold:42fiQ UERY P REDICATE E MPTINESS1. > v Cqi 0 , 1 k;2. atom A(v) q v R A((v)) A0 ;3. atom r(v, v 0 ) q v, v 0 R r((v), (v 0 )) A0 ;4. D(((i))) A0 1 n, = r.Cq|S v0 root q|Sir((i), v0 ) q.call K0 spoiler q K (i) every fork rewriting q 0 q, every splitting q 0w.r.t. K, K0 spoiler q 0 , K, ; (ii) K0 minimal Property (i). followingresult proved work Lutz (2008).Theorem 45 Let K = (T , A) ALC-knowledge base q CQ. K 6|= q iffspoiler (T 0 , A0 ) q K A0 satisfiable w.r.t. 0 .following lemma, observed Lutz, plays central role obtaining NE XP IMEdecision procedure.Lemma 46 Let K = (T , A) ALC-knowledge base, q CQ, q maximal fork rewriting,K0 = (T 0 , A0 ) spoiler q K. K0 contains concept inclusions ABoxassertions following form:1. > v Cq0 q 0 DTrees(q );2. A(a) Ind(A) occurring q;3. r(a, b) a, b Ind(A) r occurring q;4. D(a) Ind(A) = r.Cq0 , r occurs q q 0 DTrees(q ).Note definition spoiler q K refers fork rewritings q,exponentially many, Lemma 46 refers unique maximal form rewriting q .fact, since cardinality DTrees(q ) clearly bounded size q, number conceptinclusions assertions listed Lemma 46 polynomial size q.set proof Theorem 16. Theorems 13 45, CQ q emptysignature given TBox iff spoiler (T 0 , A0 ) q (T , , ) , A0satisfiable w.r.t. 0 . Given CQ q, signature TBox , thus decide emptinessq given follows:1. compute , ;2. guess TBox 0 ABox A0 satisfy Conditions 1 4 Lemma 46 KBK = (T , , ) role assertion r(a, b) , r(a, b) A0 ;3. verify (T 0 , A0 ) spoiler q (T , , );4. verify , A0 satisfiable w.r.t. 0 .43fiBAADER , B IENVENU , L UTZ , & W OLTERremains argue yields NE XP IME algorithm. already noted, Step 1 carried(deterministic) exponential time. Due Conditions 1 4 Lemma 46 since ,size exponential , TBox 0 ABox A0 guessed Step 2size exponential , size polynomial q. Step 3 implementedstraightforward iteration fork rewritings q 0 q splittings q 0 w.r.t. (T , , ),requires exponential time.thus remains deal Step 4. Let closure single negations unionfollowing sets concepts:NC ;concepts occur (possibly subconcepts);concept names occur q;concepts Cq r.Cq subconcepts, q DTrees(q ), q maximal forkrewriting q, r occurs q.Based remark Lemma 46, easy verify (and crucial argument) ||polynomial , , q. -type set model 0, = {C | C }. Section 4, introduce notioncoherence types: say pair -types (t, t0 ) r-coherent, denoted ;r t0 ,r.C , C t0 implies r.C t. set -types computed E XP IME.verify satisfiability , A0 w.r.t. 0 , guess map : Ind(AT , ) , acceptfollowing two conditions satisfied reject otherwise:(i) C(c) , A0 implies C (c)(ii) r(b, c) , A0 , C (c), r.C imply r.C (b).Clearly, checking whether two conditions satisfied done single exponential time.thus remains argue , A0 satisfiable w.r.t. 0 case existsmap verifying conditions. First note given model KB (T 0 , , A0 ),define desired map setting (c) = {C | cI C }. Conversely, given mapsatisfying conditions (i) (ii), define interpretation follows:=AI = {t | t}rI = {(t, t0 ) | ;r t0 }cI = (c)readily verified C , C iff C . this,show, using similar argument given Lemma 10, model (T 0 , , A0 ).complete proof undecidability result (Theorem 19) proving Lemma 20.Lemma 20 (T, H, V ) admits tiling iff -ABox satisfiable w.r.t., |= v A(v).Proof. (Only if) Straightforward. Consider tiling f : {0, . . . , n} {0, . . . , m}(T, H, V ). Create individuals ai,j 0 n 0 j m, consider ABoxcomposed following assertions:44fiQ UERY P REDICATE E MPTINESSx(ai,j , ai+1,j ) 0 < n 0 jx (ai+1,j , ai,j ) 0 < n 0 jy(ai,j , ai,j+1 ) 0 j < 0 n(ai,j+1 , ai,j ) 0 j < 0 nTh (ai,j ) f (i, j) = Th .easily verified satisfiable w.r.t. satisfies , |= v A(v).(If) Let -ABox satisfiable w.r.t. , |= v A(v). first showIx Iy enforce x inverse x inverse y, respectively, Cforces relevant grid cells closed. r {x, y} call Ind(A) r-defect existsb Ind(A) r(a, b) r (b, a) 6 A. call inv-defect x-defecty-defect. call Ind(A) cl-defect exist x(a, b), y(a, c), y(b, d), x(c, e)6= e inv-defect, b y-defect c x-defect.Claim 1. exists model Ind(A):(d1) aI 6 IrI , r-defects Ind(A) r {x, y};(d2) aI 6 C , cl-defects Ind(A).Moreover, satisfies following conditions Ind(A), role names r, h {1, . . . , p}:1. = Ind(A);2. aI = a;3. (a, a0 ) rI implies r(a, a0 ) A;4. ThI implies Th (a) A.Proof Claim 1. Let r {x, y}. Call two-element set {a, b} r-defect witness existsc Ind(A) r(a, c), r (c, b) A. Consider undirected graph G nodes Ind(A)set r-defect witnesses edges. Note G degree two (since r rfunctional). Hence G three-colorable. Choose three coloring G colors Br,1 = Zr,1 uZr,2 ,Br,2 = Zr,1 u Zr,2 Br,3 = Zr,1 u Zr,2concept namesSand choose interpretationZr,1 , Zr,2 correspondingly. set Ir = i=1,2,3 (Br,i u r.r .Br,i ) .Call two-element set {d, e} cl-defect witness exist x(a, b), y(a, c), y(b, d), x(c, e)inv-defect, b y-defect c x-defect. Consider undirectedgraph G nodes Ind(A) set cl-defect witnesses edges. Note G degreetwo (again since x, x , y, functional). Hence G three-colorable colorsC1 = Zc,1 u Zc,2 , C2 = Zc,1 u Zc,2 C3 = Zc,1 u Zc,2choose interpretationconcept names Zc,1 , Zc,2 correspondingly. set C = i=1,2,3 (x.y.Ci u y.x.Ci )I .Since neither existential restrictions concept names Th occur right-hand sideCIs , hard verify interpret remaining concept namesway additional conditions satisfied. (End proof claim)45fiBAADER , B IENVENU , L UTZ , & W OLTERLet model satisfying conditions Claim 1. additionally assume w.l.o.g.A, -minimal: model J satisfying conditions Claim 1AJ AI J least one inclusions proper.Let aA AI . exhibit grid structure gives rise tiling (T, H, V ).start identifying diagonal starts aA ends instance Tfinal .Claim 2. set G = {r1 (ai0 ,j0 , ai1 ,j1 ), . . . , rk1 (aik1 ,jk1 aik ,jk ), Tfinal (aik ,jk )}i0 = 0, j0 = 0, a0,0 = aA ;1 ` < k, either (i) r` = x, i`+1 = i` + 1, j`+1 = j` (ii) r` = y,j`+1 = j` + 1, i`+1 = i` .Proof claim. sequence, convert new model Jinterpreting false points reachable (equivalently: A) aA settingAJ = AI \ {aA }, contradicts A, -minimality I. (End proof claim)Let n number occurrences role x ABox G Claim 1 numberoccurrences y. next showClaim 3..(a) a0,0 Tinit(b) ai,j RI implies = n;(c) ai,j U implies j = m;(d) ai,j ai,j Ind(G);(e) ai,j Ind(G), (unique) Th ai,j ThI , henceforth denoted Ti,j ;(f) (Ti,j , Ti+1,j ) H ai,j , ai+1,j Ind(G) (Ti,j , Ti,j+1 ) V ai,j , ai,j+1Ind(G).Proof claim. Point (a) easy consequence fact a0,0 = aA , aA AI ,A, -minimal. (b), first note unique ` k = n {`, . . . , k}< n {0, . . . , ` 1}. Due CI R v x., ai`1 ,j`1/ RI . showais ,js/ R < ` 1, suffices use CIs R v x. R v y.R. proof(c) similar. prove (d)-(f) together, showing induction ` (d)-(f) satisfiedinitial partsG` := {r1 (ai0 ,j0 , ai1 ,j1 ), . . . , r`1 (ai`1 ,j`1 ai` ,j` )}G, ` k. base case, ai0 ,j0 = aA AI clearly implies ai0 ,j0 , thus (d)satisfied. Point (e) follows (a) disjointness tiles expressed . Point (f)vacuously true since single individual G0 . induction step, assume G`1satisfies (d)-(f). distinguish four cases:ai`1 ,j`1 (U u R)I .46fiQ UERY P REDICATE E MPTINESSSince G`1 satisfies (d), ai`1 ,j`1 , definition A, minimality together fact ai`1 ,j`1 (U u R)I ensureai`1 ,j`1 (x.(Tg u u y.Y ) u y.(Th u u x.Y ) u Ix u Iy u C u Tf )I(Tf , Tg ) H (Tf , Th ) V . Using functionality x y, easyshow G` satisfies (d)-(f).ai`1 ,j`1 (U u R)I .Since ai`1 ,j`1 RI , ensures x-successor ai`1 ,j`1 I. Moreover,ai`1 ,j`1 . Together definition , getai`1 ,j`1 (y.(Tg u u R) u Iy u Tf )I(Tf , Tg ) V . must i` = i`1 , j` = j`1 + 1, r`1 = y. Usingfunctionality y, easy show G` satisfies (d)-(f).ai`1 ,j`1 (U u R)I .Analogous previous case.ai`1 ,j`1 (U u R)I .neither x-successor y-successor ai`1 ,j`1 (U u R)I . follows` 1 = k, contradiction ` k.(End proof claim)Next, extend G full grid Conditions (a)-(e) Claim 3 still satisfied.achieved, trivial read solution tiling problem. construction gridconsists exhaustive application following two steps:1. x(ai,j , ai+1,j ), y(ai+1,j , ai+1,j+1 ) G ai,j+1 Ind(G) y(ai,j , ai,j+1 )G x(ai,j+1 , ai+1,j+1 ) G, identify ai,j+1 Ind(A) y(ai,j , ai,j+1 )x(ai,j+1 , ai+1,j+1 ) add latter two assertions G.2. y(ai,j , ai,j+1 ), x(ai,j+1 , ai+1,j+1 ) G ai+1,j Ind(G) x(ai,j , ai+1,j )G y(ai+1,j , ai+1,j+1 ) G, identify ai+1,j Ind(A) x(ai,j , ai+1,j )y(ai+1,j , ai+1,j+1 ) add latter two assertions G.hard see exhaustive application rules yields full grid, i.e., final G(i) Ind(G) = {ai,j | n, j m}, (ii) x(ai,j , ai0 ,j 0 ) G iff i0 = + 1 j = j 0 ,(iii) y(ai,j , ai0 ,j 0 ) G iff = i0 j 0 = j + 1.Since two steps construction completely analogous, deal Case 1detail. Thus let x(ai,j , ai+1,j ), y(ai+1,j , ai+1,j+1 ) G ai,j+1/ Ind(G). Clearly, < nj < m. (b) (c), thus ai,j/ (R U )I . Since ai,j (d)A, -minimal, getai,j (x.(Tg u u y.Y ) u y.(Th u u x.Y ) u Ix u Iy u C u Tf )I(Tf , Tg ) H (Tf , Th ) V . together minimality meansselect ai,j+1 , b Ind(A) y(ai,j , ai,j+1 ), x(ai,j+1 , b) A, ai,j+1 , b , Ti,j+1 =47fiBAADER , B IENVENU , L UTZ , & W OLTERTh . choice, (a), (d), (e), second half (f) clearly satisfied. get propertiesrequired Step 1 above, show b = ai+1,j+1 . show this,satisfaction (b) (c) apply construction step, CIsR v x. R v y.RU v y. U v x.Uensure (b) (c) still satisfied construction step. Showing b = ai+1,j+1 alsogive us first half (f). Finally, prove b = ai+1,j+1 sufficient show ai,jcl-defect Ind(A). follows Claim 1 since ai,j C , ai,j IxI IyI , ai+1,j IyI ,ai,j+1 IxI .use completed grid build solution tiling problem: tile point(i, j) unique tile satisfied ai,j Ind(A). Property (f) Claim 2correctness grid construction ensure adjacent tiles satisfy vertical horizontalconstraints.Appendix C. Proofs Section 5Theorem 22. EL, CQ-query emptiness decided PT IME.Proof. Lemma 21, suffices show n-ary CQ q alphabet , decidedPT IME whether , |= q[a , . . . , ] total -ABox. First note, |= q[a , . . . , ] iff , Ab |= qb,Ab obtained adding assertion X(a ), X concept nameoccur , , q;qb Boolean CQ obtained q adding conjunct X(v) answer variable vquantifying away answer variables.Recall discussion Lemma 44 every CQ q viewed directed graphGdq . say Boolean CQ q directed forest-shaped disjoint union directed treeshaped Boolean CQs. Every Boolean CQ q directed forest-shaped corresponds conceptCq description logic ELu extends EL universal role u , |= q iff, |= C(a) Ind(A) (Lutz & Wolter, 2010). Checking latter condition possiblePT IME (Lutz & Wolter, 2010). Thus, sufficient convert qb polynomial time directedforest-shaped CQ qb0 , Ab |= qb iff , Ab |= qb0 .construct qb0 qb, exhaustively apply following rewriting rules:1. r(v, v 00 ) r(v 0 , v 00 ) query, identify v v 0 replacing occurrencesv 0 v;2. r(v 0 , v) s(v 00 , v) query (with r 6= s), identify v, v 0 , v 00 replacingoccurrences v 0 v 00 v;3. cycle r0 (v0 , v1 ), . . . , rn1 (vn1 , vn ), vn = v0 query {v0 , . . . , vn1 } containsleast two variables, identify variables v0 , . . . , vn1 replacing occurrencesv1 , . . . , vn1 v0 .48fiQ UERY P REDICATE E MPTINESSresulting query contains reflexive loop r(v, v) r/ , immediately returnno. Otherwise, replace final step reflexive loop r(v, v) r X(v).query resulting last step qb0 . easy see query obtained pointdirected forest-shaped since every variable one predecessor cyclescorresponding directed graph.prove correctness algorithm, first establish following claim:Claim. qb0 defined, , Ab |= qb iff , Ab |= qb0 .suffices prove rule application preserves (non)entailment query Ab .preliminary, recall that, shown Lutz Wolter (2010), exists materializationJT ,Ab (T , Ab ) directed tree-shaped interpretation individual root(potentially) additional reflexive loops added root (an interpretation directed treeshaped corresponding CQ domain elements interpretation regardedvariables directed tree-shaped). Assume rewriting rule 1 applied query p resultingquery p0 . clear , Ab |= pb0 implies , Ab |= pb. converse, assume , Ab |= pblet JT ,Ab materialization Ab introduced above. match pJT ,Ab . Since JT ,Ab contain domain elements d, d0 , d00 6= d0JJrole name r, (d, d00 ) r ,Ab (d0 , d00 ) r ,Ab , match p JT ,Ab must mapidentified variables v v 0 domain element thus also match p0 .two rules replacement r(v, v), r , X(v) dealt similar way.claim, substitute qb qb0 intended. Moreover, easy see, Ab 6|= qb algorithm returns due reflexive loop r(v, v) r/ : simply useinterpretation JT ,Ab proof claim.Proposition 28. every Horn-ALCIF TBox , ABox signature , CQ q, one constructpolynomial time ELIF -TBox 0 normal form q empty given iff qempty given 0 .Proof. proof similar reductions provided work Hustadt et al. (2007)Kazakov (2009). Nevertheless, Kazakov considers reductions preserving subsumption only,Hustadt, Motik, Sattler also Kazakov reduce ELIF TBoxes,give detailed proof.following rules used rewrite ELIF -TBox normal form (all freshlyintroduced concept names sig(T ) sig(q). Assume L v R given.L form L1 u L2 R concept name, take fresh concept namereplace L v R L v v R. R concept name, either L1 L2concept names, take fresh concept names A1 , A2 replace L v R L1 v A1 ,L2 v A2 A1 u A2 v R;L form L1 L2 R concept name, replace L v R L1 v RL2 v R. Otherwise take fresh concept name replace L v R L v v R;L form r.L0 L0 concept name, take fresh concept name A0replace L v R L0 v A0 r.A0 v R;R form A, replace L v R L u v ;49fiBAADER , B IENVENU , L UTZ , & W OLTERR form R1 u R2 L concept name, take fresh concept namereplace L v R L v v R. Otherwise take fresh concept names A1 , A2replace L v R L v A1 , L v A2 , A1 v R1 , A2 v R2 ;R form L0 R0 , replace L v R L u L0 v R0 ;R form r.R0 R0 concept name, take fresh concept name A0replace L v R L v r.A0 A0 v R0 ;R form r.R0 , replace L v R r .L v R.resulting TBox 0 required. particular, every -ABox model 0 ,also model ; conversely, every model extendedmodel appropriately interpreting fresh concept names. Consequently,certT (q, A) = certT 0 (q, A) thus q empty given iff q empty given 0 .Proposition 30. Let ELIF -TBox, ABox signature, q CQ. q non-emptygiven , witnessed -ABox forest-shaped, width |q|,degree |T |.Proof. Assume q answer variables v1 , . . . , vn non-empty given .find -ABox satisfiable w.r.t. certT ,A (q) 6= . identify forestshaped witness non-emptiness q given , consider canonical model ,A(T , A). construction, ,A consists ABox part I0 , restriction ,AInd(A), tree-shaped interpretations Ia , Ind(A), rooted containing ABoxindividuals. Since ,A universal, match q ,A . Let consist individualsInd(A) v var(q) (v) Ia (possibly (v) = a). Let A0ABox obtained restricting individuals ; going root componentforest-shaped witness seeking define (observe |Ind(A0 )| |q|). addtree components, consider, , (typically infinite) tree-shaped ABox Auaobtained unraveling starting a, work Lutz Wolter (2012):Ind(Aua ) set sequences = c0 r0 c1 . . . rm1 cm c0 , . . . , cm Ind(A)r0 , . . . , rm1 (possibly inverse) roles (i) c0 = a, (ii) c1 6 , (iii) rj (cj1 , cj )0 j < m, (iv) (cj1 , rj1) 6= (cj+1 , rj ) j > 0; say copycm ;A(c) Ind(Aua ) copy c, A() Aua ;Ind(Aua ) copy c, = rc0 Ind(Aua ), r(, ) Aua ;Ind(Aua ) copy c, = r c0 Ind(Aua ), r(, ) Aua .let Ab union A0 tree-shaped ABoxes {Aua | }. Observe Conditionsb Note(ii) (iv) first item since satisfies functionality statements , A.bforest-shaped, need neither finite degree |T |; going fixlater.next aim show Ab satisfiable w.r.t. certT ,Ab(q) 6= . end,construct universal model J Ab . Start Ab viewed interpretation J0 ,50fiQ UERY P REDICATE E MPTINESSconstruction canonical models. take, Ind(A) copiesb copy tree interpretation Ia (i) root , (ii) J0 = {},Ind(A),(iii) 6= implies disjointness , (iv) = identical originaltree interpretation Ia (and copy). d0 result renaming Ia , d0called copy d. desired interpretation J obtained taking union J0 .Note every element J copy element ,A , that, construction, Jbmodel A.straightforward show induction structure C every ELI-concept Cevery element e J copy ,A , e C J iff C ,A . Since ,Amodel , follows J model thus Ab satisfiable w.r.t. . sketchproof J universal. Let model Ab . start define homomorphism h0b remains extend h0 componentsJ setting h0 (a) = aI Ind(A).J . copy tree interpretation Ia ,A copy a. shownwork Lutz Wolter (2012) that4b copy Ind(A), AIT ,A implies , Ab |= A() concept() Ind(A)names A.Recall ,A generated derivation rules building canonical models. Usingstraightforward induction number rule applications exploiting () factnormal form, one construct homomorphism ha Ia h(a) = .renaming, obtain homomorphism h h () = . desiredhomomorphism h union h0 h . thus established J universal. Goingconstruction J (and particular using Point (iv)), verified matchq ,A also match q J . Since J universal, yields certT ,Ab(q) 6= desired.want remove individuals Ab resulting ABox degree |T |still witnesses non-emptiness q given . Since J universal, homomorphism h J canonical model ,Ab. Composing match h, obtain matchq ,Ab sends every variable individual A0 element treeindividual. inductively mark individuals Ab relevant match , startingindividuals A0 proceeding follows: whenever Rule 2 4 adds marked individualx AIT ,Ab construction ,Ab presence (x, y) rIT ,A (please seeformulation mentioned rules), mark y. verified every individual outsideA0 one marked neighbor existential restriction . (potentially infinite) forest-shaped ABox Abd obtained Ab dropping assertions involve least oneunmarked individual thus degree |T |. Moreover, marking construction ensurescanonical model ,Abd contains A0 interpretation Ia , Ind(A0 ), hence matchq ,Abd .point, ABox Abd almost required forest witness, except may infinite.remains invoke compactness obtain finite subset Abf Abd certT ,Abf (q) 6= .Clearly, Abf contains forest witness non-emptiness q given .4. Lutz Wolter (2012) actually show case root component A0 Ab startunravel consists individual names A, contains concept role assertions; proof also goescase.51fiBAADER , B IENVENU , L UTZ , & W OLTERfollowing lemmas establish two statements Lemma 34.Lemma 47 Every canonical proper R N -labeled tree well-founded.Proof. Let hT, `i canonical proper R N -labeled tree, let I0 , I1 , . . . interpretations encountered construction canonical model AhT,`i . Since hT, `icanonical, IhT,`i canonical model AhT,`i .slightly abuse terminology using term concept atom refer statementsform B(e) B concept name (or >) e domain element. role atom takeform r(e, e0 ) r role e, e0 domain elements. say concept atom B(e) (resp.role atom r(e, e0 )) interpretation J e B J (resp. (e, e0 ) rJ ). atomIhT,`i , rank smallest Ii . show induction rankevery concept atom IhT,`i derivation, thus hT, `i well-founded.induction start straightforward concept atoms I0 involve concept {>}element x either x Ind(A) = `() x \ {} `(x),every atom derivation depth 0. induction step, let B(x) concept atomIi+1 \ Ii . consider rule application resulted addition B(x):1. Assume B(x) Ii+1 application Rule 1, is, A1 u u vB x AIj 1 j n.every 1 j n, atom Aj (x) rank i, IH, derivationhTj0 , `0j Aj x. obtain derivation hT 0 , `0 B x setting 0 = {} {jw | wTj0 }, `0 () = (B, x), `0 (jw) = `0j (w).2. Assume B(x) Ii+1 application Rule 2, is, r.A v Bx (r.A)Ii .x (r.A)Ii , must exist Ii (x, y) rIi AIi .atom A(y) rank i, IH, derivation hT 00 , `00 y.x I0 , x Ind(AhT,`i ), define derivation hT 0 , `0 B x setting0 = {} {1w | w 00 }, `0 () = (B, x), `0 (1w) = `00 (w).Next consider case x 6 I0 . x 6 Ind(AhT,`i ), properness ,concept E NC {>} E `(x). Since x 6 I0 x 6 Ii ,0 < j < x Ij \ Ij1 . Since hT, `i canonical, element x createddue application Rule 3 using concept inclusion form F v s.E, x E Ij .Applying IH, obtain derivation hT 000 , `000 E x. thus define derivationhT 0 , `0 B x setting 0 = {} {1w | w 00 } {2w | w 000 }, `0 () = (B, x),`0 (1w) = `00 (w), `0 (2w) = `000 (w).3. Assume B(x) Ii+1 application Rule 3 involving v r.B ,is, Ii AIi (y, x) rIi+1 \ rIi .atom A(y) rank i, IH, exists derivation hT 00 , `00 y.Moreover, since x created applying inclusion v r.B y, secondcondition canonicity ensures B `(x). thus define derivation B xtaking tree hT 0 , `0 0 = {} {1w | w 00 }, `0 () = (B, x), `0 (1w) = `00 (w).52fiQ UERY P REDICATE E MPTINESS4. Assume B(x) = >(x) Ii+1 application Rule 3 involving vr.E (E 6= >), is, Ii AIi (y, x) rIi+1 \ rIi .atom A(y) rank i, IH, exists derivation hT 00 , `00 y.Moreover, since x created applying inclusion v r.E y, secondcondition canonicity ensures E `(x). thus define derivation > xtaking tree hT 0 , `0 0 = {, 1} {11w | w 00 }, `0 () = (>, x), `0 (1) = (E, x),`0 (11w) = `00 (w).5. Assume B(x) Ii+1 application Rule 4, is, v r.B ,funct(r) , AIi , (y, x) rIi .atom A(y) rank i, IH, exists derivation hT 00 , `00 y.x I0 , obtain derivation B x taking tree hT 0 , `0 0 = {} {1w |w 00 }, `0 () = (B, x), `0 (1w) = `00 (w). x 6 I0 , useargument Point 2 find derivation hT 000 , `000 E x. obtain derivationhT 0 , `0 B x setting 0 = {} {1w | w 00 } {2w | w 000 }, `0 () = (B, x),`0 (1w) = `00 (w), `0 (2w) = `000 (w).Lemma 48 Let hT, `i proper R N -labeled tree well-founded IhT,`imodel . IhT,`i universal model AhT,`i .Proof. Assume hT, `i well-founded proper R N -labeled tree IhT,`i model. obligation pair (A, x) x `(x). every obligation(A, x), choose derivation hTA,x , `A,x x hT, `i minimal depth. obligations (A1 , x1 ),(A2 , x2 ), write (A1 , x1 ) (A2 , x2 ) (A1 , x1 ) occurs node labelhTA2 ,x2 , `A2 ,x2 i.Claim. relation acyclic.Proof claim. Assume contrary obligations (A0 , x0 ), . . . , (An , xn )(Ai , xi ) (Ai+1 , xi+1 ) n (An+1 , xn+1 ) := (A0 , x0 ). may assume without loss generality 0 < j n, (Ai , xi ) 6= (Aj , xj ), i.e., obligations(A0 , x0 ), . . . , (An , xn ) pairwise distinct. Let ki depth hTAi ,xi , `Ai ,xi `idepth shallow derivation (Ai , xi ) contained hTAi+1 ,xi+1 , `Ai+1 ,xi+1 i.hTAi ,xi , `Ai ,xi minimal depth, ki `i . Moreover, clearly also `i ki+1 .thus shown k0 = `0 = = kn = `n . Consequently, derivation (A0 , x0 )hTA1 ,x1 , `A1 ,x1 must start root hTAi ,xi , `Ai ,xi i, implies (A0 , x0 ) = (A1 , x1 )contradiction fact obligations distinct. finishes proof claim.claim, assume w.l.o.g. chosen derivation hTA,x , `A,x i, nodelabeled (B, y), subtree hTA,x , `A,x rooted node chosen derivationhTB,y , `B,y (uniformity assumption).prove IhT,`i universal model AhT,`i , take model AhT,`i .show homomorphism h IhT,`i I, constructing h step-by-step fashion.start, set h(a) = aI individual names occur AhT,`i . extensionh, argue53fiBAADER , B IENVENU , L UTZ , & W OLTER1. x AIhT,`i concept name, h(x) defined hTA,x , `A,x uses elementsdomain h, h(x) AI ;2. (x, y) rIhT,`i r role h(x), h(y) defined, (h(x), h(y)) rI ;3. (x, y) rIhT,`i , child x , h(y) defined, h(x) also defined.start observing initial mapping h, Point 2 trivial since model AhT,`irole edges restriction IhT,`i domain h AhT,`i . Point 3, usefact individual AhT,`i x parent , properness impliesx also individual AhT,`i (and hence x belongs domain h).Point 1 proved induction depth hTA,x , `A,x i. induction start, considerdepth zero. {>}, x Ind(AhT,`i ), A(x) AhT,`i . Since model AhT,`idefinition h, h(x) AI .induction step. Assume hTA,x , `A,x uses elements domain h.definition derivations gives rise following cases:6 `(x), CI A1 u u v 1 n, childz 0 z TA,x `A,x (z 0 ) = (Ai , x).1 n, let zi child z `A,x (zi ) = (Ai , x). subderivationhTA,x , `A,x rooted zi chosen derivation hTAi ,x , `Ai ,x Ai x. followshTAi ,x , `Ai ,x uses elements domain h depth strictly smallerhTA,x , `A,x i. therefore apply induction hypothesis get h(x) AIi . Sincemodel A1 u u v , obtain h(x) AI .6 `(x), CI r.A0 v child z 0 z TA,x `A,x (z 0 ) = (A0 , x0 )(x, x0 ) rIhT,`i .subderivation hTA,x , `A,x rooted z 0 chosen derivation hTA0 ,x0 , `A0 ,x0 A0x0 , thus contains elements domain h strictly smaller depthhTA,x , `A,x i. thus use IH infer h(x0 ) B , use Point 2get (h(x), h(x0 )) rI . Since model r.A0 v , h(x) AI .6 `(x), CI A0 v r.A funct(r) child z 0 z TA,x`A,x (z 0 ) = (A0 , x0 ) (x0 , x) rIhT,`i .previous item, use IH Point 2 get h(x0 ) B (h(x), h(x0 ))rI . Since model contains A0 v r.A funct(r), followsh(x) AI .= >, >/ `(x), B `(x),and child z TA,x `A,x (z) = (B, x).case applicable since B `(x), 6 `(x), hence x domainh.`(x), CI A0 v r.A , child z 0 z TA,x `A,x (z 0 ) =(A0 , x0 ) (x0 , x) rIhT,`i either (i) x child x0 , (ii) x childroot, x0 Ind, {r, x0 } `(x).case applicable since `(x), 6 `(x), hence x domainh.54fiQ UERY P REDICATE E MPTINESSextend h, first show h yet total, exists edge (bx, yb) rIhT,`iconcept name h(bx) defined, h(b) undefined, `(b) (and consequently`(b)), hTA,by , `A,by elements except root node domainh.Assume contrary h total edge, i.e., every edge (bx, yb)rIhT,`i h(bx) defined, h(b) undefined, `(b), derivation hTA,by , `A,bycontains non-root node domain h. Pick one edge (bx, yb) rIhT,`iassociated derivation hTA,by , `A,by minimal depth. Since h(bx) defined h(b)undefined, follows Point 3 either xb parent yb , yb child root node{bx, r} `(b). Since derivation rule 6 applicable rule node label containsformulation rule, must thus CI A0 v r.A uniquechild z TA,by satisfies `A,by (z) = (A0 , xb). Since xb domain h, non-root nodedomain h must somewhere z. Consequently, find nodes z1 , z2 TA,byz2 successor z1 domain element xb0 `A,by (z1 ) domain h,0domain element yb `A,by (z2 ) domain h. definition derivation rules,must (bx0 , yb0 ) sIhT,`i role s, Point 3, either yb0 child xb0 , yb000child root node label contains {bx , s}. follows xb related yb0 onederivation rules 4 5. Consequently, B `(b0 ) hTA,by , `A,by containsIhT,`i000obligation (B, yb ). Thus, edge (bx , yb )satisfies conditions associatedderivation hTB,by0 , `B,by0 strictly smaller depth hTA,by , `A,by i, contradicting minimalityhTA,by , `A,by i.extend h using edge (bx, yb) rIhT,`i whose existence established.definition derivations, CI A0 v r.A child z TA,by`A,by (z) = (A0 , xb). Since elements hTA,by , `A,by except root node domain h,subderivation hTA,by , `A,by rooted z uses elements domain h.uniformity assumption, derivation hTA0 ,bx , `A0 ,bx i, thus IH yields h(bx) A0 IhT,`i .IhT,`iIhT,`i0Since v r.A , (bx, d) r. Set h(b) = d.remains show Points 1 2 satisfied extended h (Point 3 obviously is).start Point 2. Assume (x, y) sIhT,`i . h(x) h(y) defined alreadyextension h, done. Otherwise, construction h must (x, y, s) = (bx, yb, r)(or (x, y, s) = (by, xb, r ), equivalent). choice h(b), (h(bx), h(b)) rI ,hence (x, y) sI . Point 1 proved induction depth A(x) initial versionh. induction start exactly same, induction step, cases differfollowing ones:/ `(x), B `(x), = >, child z TA,x `A,x (z) = (B, x).Immediate since = >.`(x), CI A0 v r.A , child z 0 z TA,x `A,x (z 0 ) =(A0 , x0 ) (x0 , x) rIhT,`i either (i) x child x0 , (ii) x childroot, x0 Ind, {r, x0 } `(x).Since `(x), x 6 Ind(AhT,`i ), x must introduced domainh examination edge (x0 , x) rIhT,`i . Since child z 0 labeled (A0 , x0 ),use CI A0 v r.A choose h(x) h(x) AIhT,`i .55fiBAADER , B IENVENU , L UTZ , & W OLTERReferencesArtale, A., Calvanese, D., Kontchakov, R., & Zakharyaschev, M. (2009). DL-Lite familyrelations. Journal Artifical Intelligence Research (JAIR), 36, 169.Baader, F., Bienvenu, M., Lutz, C., & Wolter, F. (2010). Query predicate emptiness description logics. Proceedings 12th International Conference Principles KnowledgeRepresentation Reasoning (KR).Baader, F., Brandt, S., & Lutz, C. (2005). Pushing EL envelope. Proceedings 19thInternational Joint Conference Artificial Intelligence (IJCAI), pp. 364369.Baader, F., Brandt, S., & Lutz, C. (2008). Pushing EL envelope further. ProceedingsWorkshop OWL: Experiences Directions (OWLED).Benedikt, M., Fan, W., & Geerts, F. (2008). XPath satisfiability presence DTDs. JournalACM, 55(2), 179.Bienvenu, M., Hansen, P., Lutz, C., & Wolter, F. (2016). First-order rewritability conjunctivequeries Horn description logics. Proceedings 25th International Joint ConferenceArtificial Intelligence (IJCAI).Bienvenu, M., Lutz, C., & Wolter, F. (2012). Query containment description logics reconsidered.Proceedings 13th International Conference Principles Knowledge Representation Reasoning (KR).Bienvenu, M., ten Cate, B., Lutz, C., & Wolter, F. (2014). Ontology-based data access: studydisjunctive datalog, CSP, MMSNP. ACM Transactions Database System(TODS), 39(4), 33.Botoeva, E., Kontchakov, R., Ryzhikov, V., Wolter, F., & Zakharyaschev, M. (2014). Query inseparability description logic knowledge bases. Proceedings 14th InternationalConference Principles Knowledge Representation Reasoning (KR).Botoeva, E., Kontchakov, R., Ryzhikov, V., Wolter, F., & Zakharyaschev, M. (2016). Gamesquery inseparability description logic knowledge bases. Artificial Intelligence Journal(AIJ), 234, 78119.Bourhis, P., & Lutz, C. (2016). Containment monadic disjunctive datalog, mmsnp, expressivedescription logics. Proceedings 15th International Conference PrinciplesKnowledge Representation Reasoning (KR).Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., Poggi, A., Rodriguez-Muro, M., &Rosati, R. (2009). Ontologies databases: DL-Lite approach. Tutorial Lectures5th International Reasoning Web Summer School, Vol. 5689 Lecture Notes ComputerScience, pp. 255356. Springer.Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., & Rosati, R. (2007). Tractable reasoningefficient query answering description logics: DL-Lite family. Journal AutomatedReasoning (JAR), 39(3), 385429.Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., & Rosati, R. (2013). Data complexityquery answering description logics. Artificial Intelligence Journal (AIJ), 195, 335360.56fiQ UERY P REDICATE E MPTINESSCalvanese, D., De Giacomo, G., & Lenzerini, M. (1998). decidability query containmentconstraints. Proceedings 17th ACM SIGACT-SIGMOD-SIGART SymposiumPrinciples Database Systems (PODS), pp. 149158.Calvanese, D., Ortiz, M., Simkus, M., & Stefanoni, G. (2013). Reasoning explanationsnegative query answers DL-Lite. Journal Artificial Intelligence Research (JAIR), 48,635669.Chortaras, A., Trivela, D., & Stamou, G. B. (2011). Optimized query rewriting OWL 2 QL.Proceedings 23rd International Conference Automated Deduction (CADE), pp.192206.Eiter, T., Gottlob, G., Ortiz, M., & Simkus, M. (2008). Query answering description logicHorn-SHIQ. Proceedings 11th European Conference Logics Artificial Intelligence (JELIA), pp. 166179.Eiter, T., Ortiz, M., Simkus, M., Tran, T., & Xiao, G. (2012). Query rewriting Horn-SHIQ plusrules. Proceedings 26th AAAI Conference Artificial Intelligence (AAAI).Gabbay, D., Kurucz, A., Wolter, F., & Zakharyaschev, M. (2003). Many-Dimensional Modal Logics:Theory Applications. Elsevier.Gatens, W., Konev, B., & Wolter, F. (2014). Lower upper approximations depleting modulesdescription logic ontologies. Proceedings 21st European Conference ArtificialIntelligence (ECAI), pp. 345350.Gene Ontology Consortium (2016). gene ontology. http://geneontology.org/. [Online; accessed16-April-2016].Glimm, B., Lutz, C., Horrocks, I., & Sattler, U. (2008). Answering conjunctive queriesSHIQ description logic. Journal Artificial Intelligence Research (JAIR), 31, 150197.Golbeck, J., Fragoso, G., Hartel, F., Hendler, J., Oberthaler, J., & Parsia, B. (2003). nationalcancer institutes thesaurus ontology. Journal Web Semantics: Science, ServicesAgents World Wide Web, 1(1), 7580.Grau, B. C., Horrocks, I., Kazakov, Y., & Sattler, U. (2008). Modular reuse ontologies: Theorypractice. Journal Artifical Intelligence Research (JAIR), 31, 273318.Haase, C. (2007). Complexity subsumption extensions EL. Masters thesis, Dresden University Technology.HITRL (2016). Health Information Technologies Research Laboratory. University Sydney.http://sydney.edu.au/engineering/it/hitru. [Online; accessed 16-April-2016].Horrocks, I., Kutz, O., & Sattler, U. (2006). even irresistible SROIQ. Proceedings10th International Conference Principles Knowledge Representation Reasoning(KR), pp. 5767.Hustadt, U., Motik, B., & Sattler, U. (2004). decomposition rule decision proceduresresolution-based calculi. Proceedings 11th International Conference LogicProgramming Artificial Intelligence Reasoning (LPAR), pp. 2135.Hustadt, U., Motik, B., & Sattler, U. (2007). Reasoning description logics reductiondisjunctive datalog. Journal Automated Reasoning (JAR), 39(3), 351384.57fiBAADER , B IENVENU , L UTZ , & W OLTERIHTSDO (2016). SNOMED CT: global language healthcare. http://www.ihtsdo.org/snomedct. [Online; accessed 16-April-2016].Kaminski, M., Schneider, T., & Smolka, G. (2011). Correctness worst-case optimality Prattstyle decision procedures modal hybrid logics. Proceedings 20th International Conference Automated Reasoning Analytic Tableaux Related Methods(TABLEAUX), pp. 196210.Kazakov, Y. (2009). Consequence-driven reasoning Horn-SHIQ ontologies. Proceedings21st International Joint Conference Artificial Intelligence (IJCAI), pp. 20402045.Kazakov, Y. (2010). extension complex role inclusion axioms description logicSROIQ. Proceedings 5th International Joint Conference Automated Reasoning(IJCAR), pp. 472486.Konev, B., Ludwig, M., Walther, D., & Wolter, F. (2012). logical difference lightweightdescription logic EL. Journal Artificial Intelligence Research (JAIR), 44, 633708.Konev, B., Lutz, C., Walther, D., & Wolter, F. (2013). Model-theoretic inseparability modularitydescription logic ontologies. Artificial Intelligence Journal (AIJ), 203, 66103.Kontchakov, R., Rodriguez-Muro, M., & Zakharyaschev, M. (2013). Ontology-based data accessdatabases: short course. Proceedings International Reasoning Web SummerSchool, pp. 194229.Kontchakov, R., Wolter, F., & Zakharyaschev, M. (2010). Logic-based ontology comparisonmodule extraction, application DL-Lite. Artificial Intelligence, 174(15), 10931141.Krotzsch, M. (2012). OWL 2 profiles: introduction lightweight ontology languages. Tutorial Lectures 8th International Reasoning Web Summer School, Vol. 7487 LectureNotes Computer Science, pp. 112183. Springer.Krotzsch, M., Rudolph, S., & Hitzler, P. (2007). Complexity boundaries Horn description logics.Proceedings 22nd AAAI Conference Artificial Intelligence (AAAI), pp. 452457.Levy, A. (1993). Irrelevance Reasoning Knowledge Based Systems. Ph.D. thesis, Stanford University.Lubyte, L., & Tessaris, S. (2008). Supporting design ontologies data access. Proceedings 21st International Description Logic Workshop (DL).Lutz, C. (2008). complexity CQ answering expressive description logics. Proceedings4th International Joint Conference Automated Reasoning (IJCAR), pp. 179193.Lutz, C., Toman, D., & Wolter, F. (2009). Conjunctive query answering description logic ELusing relational database system. Proceedings 21st International Joint ConferenceArtificial Intelligence (IJCAI), pp. 20702075.Lutz, C., & Wolter, F. (2010). Deciding inseparability conservative extensions descriptionlogic EL. Journal Symbolic Computation, 45(2), 194228.Lutz, C., & Wolter, F. (2012). Non-uniform data complexity query answering descriptionlogics. Proceedings 13th International Conference Principles KnowledgeRepresentation Reasoning (KR).58fiQ UERY P REDICATE E MPTINESSMotik, B., Grau, B. C., Horrocks, I., Wu, Z., Fokoue, A., & Lutz, C. (2009). OWL 2 Web Ontology Language: Profiles. W3C Recommendation. Available http://www.w3.org/TR/owl2profiles/.Ortiz, M., Calvanese, D., & Eiter, T. (2008). Data complexity query answering expressivedescription logics via tableaux. Journal Automated Reasoning (JAR), 41(1), 6198.Ortiz, M., & Simkus, M. (2012). Reasoning query answering description logics. Proceedings 8th International Reasoning Web Summer School, Vol. 7487 Lecture NotesComputer Science, pp. 153. Springer.Ortiz, M., Simkus, M., & Eiter, T. (2008). Worst-case optimal conjunctive query answeringexpressive description logic without inverses. Proceedings 23rd AAAI ConferenceArtificial Intelligence (AAAI), pp. 504510.Patel, C., Cimino, J. J., Dolby, J., Fokoue, A., Kalyanpur, A., Kershenbaum, A., Ma, L., Schonberg,E., & Srinivas, K. (2007). Matching patient records clinical trials using ontologies.Proceedings 6th International Semantic Web Conference (ISWC), pp. 816829.Perez-Urbina, H., Motik, B., & Horrocks, I. (2009). comparison query rewriting techniquesdl-lite. Proceedings 22nd International Description Logic Workshop (DL).Poggi, A., Lembo, D., Calvanese, D., De Giacomo, G., Lenzerini, M., & Rosati, R. (2008). Linkingdata ontologies. Journal Data Semantics, 10, 133173.Pratt, V. R. (1979). Models program logics. Proceedings IEEE Annual SymposiumFoundations Computer Science (FOCS), pp. 115122.Romero, A. A., Kaminski, M., Grau, B. C., & Horrocks, I. (2015). Ontology module extractionvia datalog reasoning. Proceedings 29th AAAI Conference Artificial Intelligence(AAAI), pp. 14101416.Tobies, S. (2001). Complexity Results Practical Algorithms Logics Knowledge Representation. Ph.D. thesis, RWTH Aachen.Vardi, M. Y. (1989). Automata theory database theoreticans. Proceedings 7th ACMSIGACT-SIGMOD-SIGART Symposium Principles Database Systems (PODS), pp. 8392.Vardi, M. Y. (1998). Reasoning past two-way automata. Proceedings 25thInternational Colloquium Automata, Languages Programming (ICALP), pp. 628641.59fiJournal Artificial Intelligence Research 56 (2016) 693-745Submitted 03/16; published 08/16Qualitative Spatial Logics Buffered GeometriesHeshan DuH.Du@leeds.ac.ukUniversity Leeds, UKNatasha AlechinaNatasha.Alechina@nottingham.ac.ukUniversity Nottingham, UKAbstractpaper describes series new qualitative spatial logics checking consistencysameAs partOf matches spatial objects different geospatial datasets,especially crowd-sourced datasets. Since geometries crowd-sourced data usually accurate precise, buffer geometries margin error leveltolerance R0 , define spatial relations buffered geometries. spatial logicsformalize notions buffered equal (intuitively corresponding possibly sameAs),buffered part (possibly partOf), near (possibly connected) far (definitely disconnected). sound complete axiomatisation logic provided respectmodels based metric spaces. logics, satisfiability problem shownNP-complete. Finally, briefly describe logics used system generating debugging matches spatial objects, report positive experimentalevaluation results system.1. Introductionmotivation work qualitative spatial logics comes needs integratingdisparate geospatial datasets, especially crowd-sourced geospatial datasets. Crowd-sourceddata involves non-specialists data collection, sharing maintenance. Comparedauthoritative geospatial data, collected surveyors geodata professionals, crowd-sourced data less accurate less well structured, often provides richeruser-based information reflects real world changes quickly much lower cost(Jackson, Rahemtulla, & Morley, 2010). interests national mapping agencies,government organisations, users geospatial data able integrateuse different geospatial data synergistically.Geospatial data matching refers problem establishing correspondences (matches)spatial objects represented different geospatial datasets. essential stepdata comparison, data integration enrichment, change detection data update.Matching authoritative geospatial data crowd-sourced geospatial data non-trivialtask. Geometry representations location place different datasets usually exactly same. Objects also sometimes represented different levelsgranularity. example, consider geometries objects Nottingham city centre givenOrdnance Survey Great Britain (OSGB) (2012) OpenStreetMap (OSM) (2012)Figure 1. position shape Prezzo Ristorante represented differentlyOSGB data (dotted) OSM data (solid). Victoria Shopping Centre representedwhole OSM, several shops OSGB.c2016AI Access Foundation. rights reserved.fiDu & AlechinaFigure 1: Prezzo Ristorante Victoria Shopping Centre represented OSGB (dotted)OSM (solid)order integrate datasets, need determine objects(represent entity) sometimes objects one dataset parts objectsdataset (as example Victoria Shopping Centre). statementsrepresenting two types relations referred sameAs matches partOfmatches respectively. One way produce matches use locations geometriesobjects, although course also use lexical labels associated objects,names restaurants etc. previous work (Du, Alechina, Jackson, & Hart, 2016),present method generates matches using location lexical informationspatial objects. generated matches may contain errors, seen retractableassumptions require validation checking. One way use logical reasoningcheck consistency matches respect statements input datasets.using description logic reasoning, correctness matches checked respectclassification information. example, wrong state spatial objectsb same, Bank b Clinic, concepts Bank Clinicdisjoint, containing common elements. However, sufficient validatingmatches spatial objects1 . example, two spatial objects closeone dataset cannot matched two spatial objects far away apartdataset, matter whether type not. Therefore, spatialreasoning required validate matches regard location information, additiondescription logic reasoning.Spatial logic studies relations geometrical structures spatial languages describing (Aiello, Pratt-Hartmann, & van Benthem, 2007). varietyspatial relations, topological connectedness regions, relations based distances,relations expressing orientations directions, etc. spatial logic, spatial relationsrepresented formal language, first order logic fragments, inter1. works (Lutz & Milicic, 2007) extending description logics concrete domains constraint systems, region connection calculus (RCC) (Randell, Cui, & Cohn, 1992) AllensInterval Algebra (Allen, 1983). description logic reasoner Pellet (Sirin, Parsia, Grau, Kalyanpur,& Katz, 2007) extended PelletSpatial (Stocker & Sirin, 2009), supports qualitative spatialreasoning RCC. However, later show appropriate use RCC application.694fiQualitative Spatial Logics Buffered Geometriespreted structures based geometrical spaces, topological spaces, metricspaces Euclidean spaces. field qualitative spatial reasoning, several spatialformalisms developed representing reasoning topological relations,Region Connection Calculus (RCC) (Randell et al., 1992), 9-intersectionmodel (Egenhofer & Franzosa, 1991) extensions (Clementini & Felice, 1997; Roy& Stell, 2001; Schockaert, Cock, Cornelis, & Kerre, 2008b, 2008a; Schockaert, Cock, &Kerre, 2009). addition, formalisms representing reasoning directional relations (Frank, 1991, 1996; Ligozat, 1998; Balbiani, Condotta, & del Cerro,1999; Goyal & Egenhofer, 2001; Skiadopoulos & Koubarakis, 2004), well relativeabsolute distances (Zimmermann, 1995; Clementini, Felice, & Hernandez, 1997; Wolter &Zakharyaschev, 2003, 2005). Recent comprehensive surveys qualitative spatial representations reasoning provided Cohn Renz (2008) Chen, Cohn, Liu, Wang,OuYang, Yu (2015).Qualitative spatial reasoning shown applicable geospatial data (Bennett, 1996; Bennett, Cohn, & Isli, 1997; Guesgen & Albrecht, 2000; Mallenby, 2007; Mallenby & Bennett, 2007; Li, Liu, & Wang, 2013), location information spatialobjects comes single data source. application described paper different, location representations spatial object come different sourcesusually exactly same. Rather treating differences geometricrepresentations logical contradictions, would tolerate slight geometric differencestreat qualitatively defined large differences logical contradictions used detectingwrong matches. specifically, establishing matches two sets spatialobjects, set matches gives rise contradiction, match must wrongretracted. addition, would provide explanations help users understand contradiction exists matches wrong. following,assess appropriateness several existing spatial formalisms purposes.Region Connection Calculus (RCC) (Randell et al., 1992) first order formalismbased regions connection relation C, axiomatised reflexivesymmetric. Two regions x, connected (i.e. C(x, y) holds), closures sharepoint. Based connection relation, several spatial relations defined regions.Among them, eight jointly exhaustive pairwise disjoint (JEPD) relations identified:DC (Disconnected), EC (Externally Connected), P (Partially Overlap), P P (TangentialProper Part), N P P (Non-Tangential Proper Part), P P (Inverse Tangential ProperPart), N P P (Inverse Non-Tangential Proper Part) EQ (Equal). referredRCC8, well-known field qualitative spatial reasoning.9-intersection model developed Egenhofer Franzosa (1991) EgenhoferHerring (1991) based point-set interpretation geometries. comparingnine intersections interiors, boundaries exteriors point-sets, identifies 29mutually exclusive topological relations. 9-intersection model provides comprehensiveformal categorization binary topological relations points, lines regions.small number 29 relations realisable particular space (Egenhofer &Herring, 1991). Restricting point-sets simple regions (regions homeomorphic disks),512 relations collapse RCC8 relations.described application, found difficult use spatial formalismsRegion Connection Calculus (Randell et al., 1992) 9-intersection model (Egenhofer695fiDu & Alechina& Franzosa, 1991), since presuppose accurate geometries regions sharp boundaries define spatial relations based connection relation. strictcrowd-sourced geospatial data. shown Figure 2, a1 sameAs a2 , representingPrezzo Ristorante; b1 sameAs b2 , referring Blue Bell Inn. Though sameAsmatches correct, topological inconsistency still exists, since a1 b1 disconnected(DC), a2 b2 externally connected (EC), spatial relations DC ECdisjoint. Therefore, relations based connection strict crowd-sourced geospatialdata possibly inaccurate may contain errors.Figure 2: OSGB data, Prezzo Ristorante (a1 ) Blue Bell Inn (b1 ) disconnected, whilst OSM data, (a2 b2 ) externally connected.egg-yolk theory independently developed Lehmann Cohn (1994), CohnGotts (1996b, 1996a), Roy Stell (2001) extending RCC theoryClementini Felice (1996, 1997) extending 9-intersection model, order represent reason regions indeterminate boundaries. theory, regionindeterminate boundary (an indeterminate region) represented pair regions,egg yolk, maximum extension minimum extensionindeterminate region respectively (similar upper approximation lower approximation rough set theory, Pawlak, Polkowski, & Skowron, 2007). yolkempty always proper part egg. egg-yolk theory presupposesexistence core part region vague part. described application,location represented using two disconnected polygons authoritative geospatial dataset crowd-sourced geospatial dataset respectively. case,could define certain inner region disconnected polygons, otherwise,inconsistent treat different representations location.aware several approaches (Fine, 1975; Zadeh, 1975; Smith, 2008)representing vague concepts relations, adopted extend classicaltheories RCC 9-intersection model. main approach assign degreetruth degree membership concepts relations. example, fuzzy regionconnection calculus (fuzzy RCC) (Schockaert et al., 2008b, 2008a, 2009), connectionrelation C defined reflexive symmetric fuzzy relation. regions a, b, C(a, b)denotes degree b connected. Using C primitive relation,every RCC relation R redefined calculate degree R holds.fuzzy RCC similar formalisms may applied case shown Figure 2.example, appropriate membership function relation EC, EC(a1 , b1 ) = 0.8696fiQualitative Spatial Logics Buffered GeometriesFigure 3: Buffering geometry X distance ; three dashed circles buffered part(BPT) solid circle; dashed circle solid circle buffered equal(BEQ)EC(a2 , b2 ) = 1, contradiction arise. adopt approachmatching problem, mainly good way define degree membership,difficult generate user-friendly explanations matches wrongunderlying reasoning numerical relatively obscure.logic S(M ) proposed developed Sturm, Suzuki, Wolter, Zakharyaschev (2000), Kutz, Sturm, Suzuki, Wolter, Zakharyaschev (2002), Kutz, Wolter,Sturm, Suzuki, Zakharyaschev (2003), Wolter Zakharyaschev (2003, 2005),Kutz (2007) reasoning distances. logic S(M ) makes possible defineconcepts object within distance 100 meters School. S(M )parameter set. typical example Q0 . satisfiability problem finite setS(Q0 ) formulas metric space EXPTIME-complete (Wolter & Zakharyaschev,2003). S(M ) developed problem geospatial data matching. However, designed logics introduced paper, discovered formproper fragment S(Q0 ). detect problematic matches, also reason distances objects, reasoning restricted qualitative kind.complexity satisfiability problem logics NP-complete, makessomewhat suitable automatic debugging matches full S(Q0 ).syntax semantics S(M ) proofs proper fragment relationsprovided later paper (see Section 3).paper, present series new qualitative spatial logics developed validatingmatches spatial objects: logic NEAR FAR buffered points (LNF) (Du,Alechina, Stock, & Jackson, 2013), logic NEAR FAR buffered geometries(LNFS) logic part whole buffered geometries (LBPT) (Du & Alechina,2014a, 2014b). notion buffer (ISO Technical Committee 211, 2003) used modeluncertainty geometry representations, tolerating slight differences marginerror level tolerance R0 . shown Figure 3, buffer geometry Xgeometry contains exactly points within distance X. bufferX denoted buffer (X , ). geometry X possibly represented inaccuratelywithin margin error one dataset, corresponding representationdataset assumed somewhere within buffer (X , ).spatial logics involve four spatial relations BufferedPartOf (BPT), BufferedEqual(BEQ), NEAR FAR. formalize notions possibly partOf, possibly sameAs,possibly connected (given possible displacement ) definitely disconnected(even displaced ) respectively. geometry X BufferedPartOf geometry X 0 ,X within buffer (X 0 , ); two geometries BufferedEqual, BufferedPartOf697fiDu & AlechinaFigure 4: NEAR FAR(Figure 3). assume two geometries X X 0 two diferentdatasets may correspond object BufferedEqual. parametercaptures margin error representation geometries. Two geometries X,NEAR, corresponding geometries X 0 , 0 dataset could connected,i.e. distance(X, ) [0, 2] (Figure 4). Clearly, FAR(X , ) holds, NEAR(X , )false X dataset. addition, want excludepossibility NEAR(X 0 , 0 ) may hold X 0 , 0 (corresponding X, respectively)dataset. Therefore define FAR(X , ) distance(X, ) (4, +) (Figure 4). possible two geometries X, NEAR FAR, is,distance(X, ) (2, 4].way defining BEQ, N EAR F AR similar defining distancerelations points Moratz Wallgrun (2012), point assigned onereference distances. distance relations two points X, definedcomparing distance X, reference distances X .different points different reference distances indicating nearness, distancerelations may symmetric. Differing work Moratz Wallgrun (2012),relations defined points also general geometries, everygeometry reference distances (, 2 4), leads symmetricdefinitions BEQ, N EAR F AR. provide sound complete sets axiomssupport reasoning BEQ, BP , N EAR F AR relations (see Section 4). reasoning useful verifying matches spatial representations different sources.explained previous work (Du et al., 2013), though relations namedN EAR F AR, attempt model human notions nearness proximity,influenced several factors, absolute distance, relative distance, framereference, object size, travelling costs reachability, travelling distance attractivenessobjects (Guesgen & Albrecht, 2000). work, provide strict mathematicaldefinition calculation whether two objects considered N EARF AR, based margin error . makes approach less likelysuitable simulation human notions nearness, provides useful tool verifyingconsistency matches. following arguments formalized checking consistencysameAs partOf matches: spatial objects a1 , b1 sameAs partOf spatial objectsa2 , b2 respectively, a1 , b1 N EAR, a2 , b2 F AR, contradiction exists.rest paper structured follows. Section 2, Section 3 Section 4 provideintroduction new spatial logics: syntax semantics, relationshipslogic S(M ), axioms theorems. Section 5 Section 6 presentproofs soundness, completeness, decidability complexity theorems LBPT,proofs LNF LNFS similar LBPT expressive LNFLNFS. Section 7 describes LBPT used debugging matches objects698fiQualitative Spatial Logics Buffered Geometriesdifferent geospatial datasets. Section 8 discusses generality limitations spatiallogics. Section 9 concludes paper.2. Syntax Semanticslanguage L(LN F ) defined, := BEQ(a, b) | N EAR(a, b) | F AR(a, b) | |a, b individual names. def ( ). language L(LN F S)exactly L(LN F ). language L(LBP ) almost L(LN F )L(LN F S), except BP instead BEQ predicate. L(LBP ) defined, := BP (a, b) | N EAR(a, b) | F AR(a, b) | | .L(LN F ), L(LN F S) L(LBP ) interpreted models based metricspace. Every individual name involved LNF formula mapped point, whilstinvolved LNFS/LBPT formula mapped arbitrary geometrynon-empty set points.Definition 1 (Metric Space) metric space pair (, d), non-emptyset (of points) metric , i.e. function : R0 ,x, y, z , following axioms satisfied:1. identity indiscernibles: d(x, y) = 0 iff x = y;2. symmetry: d(x, y) = d(y, x);3. triangle inequality: d(x, z) d(x, y) + d(y, z).Definition 2 (Metric Model LNF) metric model LNF tuple (, d, I, ),(, d) metric space, interpretation function maps individualname element , R0 margin error. notion |= (true model ) defined follows:|= BEQ(a, b) iff (I (a), (b)) [0 , ];|= N EAR(a, b) iff (I (a), (b)) [0 , 2 ];|= F AR(a, b) iff (I (a), (b)) (4 , );|= iff 6|= ;|= iff |= |= ,a, b individual names, , formulas L(LN F ).Definition 3 (Metric Model LNFS/LBPT) metric model LNFS/LBPTtuple (, d, I, ), (, d) metric space, interpretation functionmaps individual name non-empty set elements , R0 marginerror. notion |= ( true model ) defined follows:699fiDu & Alechina|= BP (a, b) iff pa (a) pb (b) : (pa , pb ) [0 , ];|= N EAR(a, b) iff pa (a) pb (b) : (pa , pb ) [0 , 2 ];|= F AR(a, b) iff pa (a) pb (b) : (pa , pb ) (4 , );|= iff 6|= ;|= iff |= |= ,a, b individual names, , formulas L(LN F S)/L(LBP ). BEQ(a, b)defined BP (a, b) BP (b, a).notions validity satisfiability metric models standard. formulasatisfiable true metric model. formula valid (|= ) truemetric models (hence negation satisfiable). logic LNF/LNFS/LBPTset valid formulas language L(LN F )/L(LN F S)/L(LBP ) respectively.3. Relationship logic S(M )logic S(M ), well variations, developed Sturm et al. (2000), Kutzet al. (2002, 2003), Wolter Zakharyaschev (2003, 2005), Kutz (2007) reasoningdistances.S(M ) family logics defined relative parameter set Q0 .subject following two conditions: a, b + b r, + b ,r = sup bounded, otherwise r = ; a, b b > 0, b .alphabet S(M ) consistsinfinite list region variables X1 , X2 ,...;infinite list location constants c1 , c2 ,...;set constant {ci } every location constant ci ;.binary distance (), equality (=) membership () predicates;boolean operators u, (and derivatives t, > );two distance quantifiers <a , duals <a , , every ;two universal quantifiers .S(M ) terms defined as:s, := Xi | {ci } | > | | | u | <a | | s.addition standard description logic concept constructions, S(M ) defineconcept objects distance less instances concepts: <a s, similarly distance a. <a defined <a (s)(s) respectively.700fiQualitative Spatial Logics Buffered GeometriesS(M ) formulas defined., := c | = | (c1 , c2 ) < | (c1 , c2 ) | | ....Further, v abbreviation (s u t) = =6 abbreviation (s = t).(c1 , c2 ) > (c1 , c2 ) defined ((c1 , c2 ) a) ((c1 , c2 ) < a) respectively.S(M )-model B structure form:BB = hW, d, X1B , X2B , ..., cB1 , c2 , ...ihW, di metric space (Definition 1), XiB subset W , cBelement W . value S(M )-term B computed inductivelyfollows:>B = W , B = ;{ci }B = {cB};(s)B = W sB ;B(s1 u s2 )B = sB1 s2 ;(<a s)B = {x W | sB : d(x, y) < a};(a s)B = {x W | sB : d(x, y) a};(s)B = {x W | sB }.<a , dual <a , respectively. instance,(<a s)B = {x W | W : (d(x, y) < sB )}.truth condition B |= , S(M )-formula, defined follows:B |= c iff cB sB ;.BB |= s1 = s2 iff sB1 = s2 ;B |= (k, l) < iff d(k B , lB ) < a;B |= (k, l) iff d(k B , lB ) a;B |= iff B 6|= ;B |= iff B |= B |= .set S(M ) formulas satisfiable, exists S(M )-model BB |= every . denoted B |= .proved LNF/LNFS/LBPT proper fragments logic S(Q0 ).Strictly speaking, holds Q0 , later show finite setLNF/LNFS/LBPT formulas satisfiable R0 , satisfiable = 1.words, acts scaling factor (see proof Lemma 43).701fiDu & AlechinaLemma 1 individual names a, b, S(M ) formula {a} v {b} expressibleLNF.Proof. Let M1 , M2 metric models2 . M1 = (1 , d, I1 , ), M2 = (2 , d, I2 , ).M1 , 1 = {o1 , o2 }, d(o1 , o2 ) = . I1 (a) = o1 , I1 (b) = o2 . individual name xdiffering a, b, I1 (x) = o1 .M2 , 2 = {o}. I2 (a) = o, I2 (b) = o. individual name x differing a, b,I2 (x) = o. individual name y, Ii ({y}) = {Ii (y)}, {1, 2}.definitions M1 , M2 , individual names x, y, d(I1 (x), I1 (y)) [0, ],d(I2 (x), I2 (y)) = 0. atomic LNF formula x, y, Definition 2, M1 |=iff M2 |= . easy induction logical connectives, LNF formula , M1 |=iff M2 |= .Since I1 ({a}) = {o1 }, I1 ({b}) = {o2 } I2 ({a}) = I2 ({b}) = {o}, truth definition S(M ) formulas, M1 |= ({a} v {b}), M2 6|= ({a} v {b}). Hence, {a} v {b}equivalent LNF formula.Lemma 2 logic LNF proper fragment logic S(Q0 ).Proof. Every atomic LNF formula expressible S(Q0 ):BEQ(a, b) ((a, b) 0) ((a, b) );N EAR(a, b) ((a, b) 0) ((a, b) 2);F AR(a, b) ((a, b) > 4).means LNF formulas expressed fragment S(Q0 ) (the imageLNF translation above) contains location constants, binary distance predicate boolean connectives , . Lemma 1, LNF proper fragmentS(M ).Lemma 3 individual names a, b, S(M ) formula v b expressibleLNFS/LBPT.Proof. Let M1 , M2 metric models3 . M1 = (1 , d, I1 , ), M2 = (2 , d, I2 , ).M1 , 1 = {o1 , o2 }, d(o1 , o2 ) = . I1 (a) = {o1 }, I1 (b) = {o2 }. individualname x differing a, b, I1 (x) = {o1 }.M2 , 2 = {o}. I2 (a) = {o}, I2 (b) = {o}. individual name x differinga, b, I2 (x) = {o}.atomic LNFS/LBPT formula x, y, Definition 3, M1 |= iffM2 |= . easy induction logical connectives, LNFS/LBPT formula ,M1 |= iff M2 |= .2. Note construct models one-dimensional two-dimensional Euclidean space similarway prove lemma.3. Note construct models one-dimensional two-dimensional Euclidean space similarway prove lemma.702fiQualitative Spatial Logics Buffered Geometriestruth definition S(M ) formulas, M1 |= (a v b) M2 6|= (a v b).Hence, v b equivalent LNFS/LBPT formula.Lemma 4 logic LNFS/LBPT proper fragment S(Q0 ).Proof. Every atomic LNFS/LBPT formula expressible S(Q0 ):(For LNFS) BEQ(a, b) iff (a v ( b)) (b v ( a));(For LBPT) BP (a, b) iff (a v ( b));.N EAR(a, b) iff (a u (2 b) =6 );.F AR(a, b) iff (a u (4 b) = ).Note formulas right belong fragment S(Q0 ) imageLNFS/LBPT translation above.correctness translation BEQ(a, b) BP (a, b) S(Q0 ) follows directly truth definition BEQ BP (Definition 3). show translation N EAR F AR correct, consider truth definition N EAR(a, b)equivalent 0 dmin (a, b) 2 F AR(a, b) dmin (a, b) > 4, dmin (a, b) =inf{d(pa , pb ) | pa I(a), pb I(b)}. shown Wolter Zakharyaschev (2005).dmin (a, b) iff u (m b) =6 . makes translation formulastruth conditions defined Definition 3. Lemma 3, LNFS/LBPT properfragment S(Q0 ).Wolter Zakharyaschev (2003) proved satisfiability problem finite setS(Q0 ) formulas metric space EXPTIME-complete, provides upperbound complexity satisfiability problems LNF, LNFS LBPT metricspace.Kutz et al. (2002) Kutz (2007) gave axioms inference rules connecting S(M )terms (e.g. 0 s) S(M ) variants. However, axiomatisationgoing present LNF, LNFS LBPT formulas (corresponding S(M ) formulasrather S(M ) terms).4. Axioms Theoremssection presents sound complete axiomatisation logic LNF/LNFS/LBPTrespectively. axiomatic systems used basis rule-based reasonerdescribed later Section 7 4 .following calculus (which also refer LNF) sound completeLNF:Axiom 0 tautologies classical propositional logic4. important complete axiomatisation. Otherwise, reasoner detectLNF/LNFS/LBPT inconsistencies caused problematic matches.703fiDu & AlechinaAxiom 1 BEQ(a, a);Axiom 2 BEQ(a, b) BEQ(b, a);Axiom 3 N EAR(a, b) N EAR(b, a);Axiom 4 F AR(a, b) F AR(b, a);Axiom 5 BEQ(a, b) BEQ(b, c) N EAR(c, a);Axiom 6 BEQ(a, b) N EAR(b, c) BEQ(c, d) F AR(d, a);Axiom 7 N EAR(a, b) N EAR(b, c) F AR(c, a);MP Modus ponens: , ` .following calculus (which also refer LNFS) sound completeLNFS:Axiom 0 tautologies classical propositional logicAxiom 1 BEQ(a, a);Axiom 2 BEQ(a, b) BEQ(b, a);Axiom 3 N EAR(a, b) N EAR(b, a);Axiom 4 F AR(a, b) F AR(b, a);Axiom 5 BEQ(a, b) BEQ(b, c) N EAR(c, a);Axiom 6 BEQ(a, b) N EAR(b, c) BEQ(c, d) F AR(d, a);Axiom 8 N EAR(a, b) BEQ(b, c) BEQ(c, d) F AR(d, a);MP Modus ponens: , ` .Axiom 7 calculus LNF holds points, arbitrary geometries,two points within line polygon far other. Axiom 7replaced Axiom 8 LNFS. axioms LNFS LNF.following calculus (which also refer LBPT) sound completeLBPT:Axiom 0 tautologies classical propositional logicAxiom 3 NEAR(a, b) NEAR(b, a);Axiom 4 FAR(a, b) FAR(b, a);Axiom 9 BPT (a, a);Axiom 10 BPT (a, b) BPT (b, c) NEAR(c, a);704fiQualitative Spatial Logics Buffered GeometriesAxiom 11 BPT (b, a) BPT (b, c) NEAR(c, a);Axiom 12 BPT (b, a) NEAR(b, c) BPT (c, ) FAR(d , a);Axiom 13 NEAR(a, b) BPT (b, c) BPT (c, ) FAR(d , a);MP Modus ponens: , ` .calculus LBPT similar calculus LNFS. Changing predicates BEQBP , LNFS Axioms 1, 6, 8 replaced Axioms 9, 12, 13 respectively LBPT. SinceBP symmetric, LNFS Axiom 2 corresponding axiom LBPT,LNFS Axiom 5 replaced two LBPT axioms, Axiom 10 Axiom 11.notion derivability ` LNF/LNFS/LBPT calculus standard. formuladerivable ` . set LNF/LNFS/LBPT-inconsistent formuladerives .proved following theorems LNF, LNFS LBPT.Theorem 1 (Soundness Completeness) LNF/LNFS/LBPT calculus soundcomplete metric models, namely` |=(every derivable formula valid every valid formula derivable).Theorem 2 (Decidability Complexity) satisfiability problem finite setLNF/LNFS/LBPT formulas metric space NP-complete.following sections, give proofs results case LBPT.proofs LNF LNFS similar. LBPT, following derivable formulas,refer facts completeness proof:Fact 14 BP (a, b) N EAR(a, b);Fact 15 N EAR(a, b) F AR(a, b);Fact 16 N EAR(a, b) BP (b, c) F AR(c, a);Fact 17 BP (a, b) F AR(a, b);Fact 18 BP (a, b) BP (b, c) F AR(c, a);Fact 19 BP (b, a) BP (b, c) F AR(c, a);Fact 20 BP (a, b) BP (b, c) BP (c, d) F AR(d, a);Fact 21 BP (b, a) BP (b, c) BP (c, d) F AR(d, a);Fact 22 BP (a, b) BP (b, c) BP (c, d) BP (d, e) F AR(e, a);Fact 23 BP (b, a) BP (b, c) BP (c, d) BP (d, e) F AR(e, a);Fact 24 BP (b, a) BP (c, b) BP (c, d) BP (d, e) F AR(e, a).shown Facts 17-24, chain four BP implies negation F AR,F AR defined > 4 distance away Definition 3.705fiDu & Alechina5. Soundness Completeness LBPTsection shows LBPT calculus sound complete metric models.Though several definitions lemmas presented previous work (Du et al.,2013; Du & Alechina, 2014b), proofs presented complete, structured, accurate (small errors corrected) simplified.proof soundness (every LBPT derivable formula valid: ` |= )easy induction length derivation . Axioms 3, 4, 9-13 valid (bytruth definition BP , N EAR F AR) modus ponens preserves validity.rest section, prove completeness (every LBPT valid formula derivable):|= `actually prove finite set LBPT formulas consistent,metric model satisfying it. finite set formulas rewritten formulaconjunction formulas . consistent, iff consistent (6` ).metric model satisfying , satisfies , thus 6|= . Therefore,show consistent, exists metric model satisfying it,show 6` , 6|= . shows 6` 6|= contraposition getcompleteness.completeness theorem proved constructing metric model maximalconsistent set (Definition 4) finite consistent set LBPT formulas (Lemma 5).Definition 4 (MCS) set formulas language L(LBP ) maximal consistent, consistent, set LBPT formulas set individual namesproperly containing inconsistent. maximal consistent set formulas,call CS.Proposition 1 (Properties MCSs) CS, then,closed modus ponens: , , ;derivable, ;formulas : ;formulas , : iff ;formulas , : iff .Lemma 5 (Lindenbaums Lemma) consistent set formulas languageL(LBP ), CS + set individual names+ .Let 0 , 1 , 2 , ... enumeration LBPT formulas set individual names. + defined follows:0 = ;n+1 = n {n }, consistent, otherwise, n+1 = n {n };706fiQualitative Spatial Logics Buffered Geometries+ =n0 n .finite consistent set formulas , construct metric model satisfyingmaximal consistent set + , contains set individual names, follows. Firstly, equivalently transform + B(+ ), set basicquantified formulas. construct set distance constraints D(+ ) B(+ ).key concept path-consistency set distance constraints.Definition 5 (Non-negative Interval) interval h non-negative, h [0, +).Definition 6 (Distance Constraint, Distance Range) distance constraint statement form d(p, q) g, p, q constants representing points, d(p, q) standsdistance p, q, g non-negative interval, stands distancerange p, q.Definition 7 (Composition) d1 , d2 non-negative real numbers, composition {d1 } {d2 } defined as: {d1 } {d2 } = [|d1 d2 |, d1 + d2 ] 5 . g1 , g2 nonnegative intervals, composition interval union {d1 }{d2 },d1 g1 , d2 g2 , is,g1 g2 = d1 g1 ,d2 g2 {d1 } {d2 }.assumed set distance constraints contains one distance rangepair constants p, q involved D, closed symmetry, i.e. d(p, q) gD, d(q, p) g D.Definition 8 (Path-Consistency) set distance constraints D, every pairdifferent constants p, q involved D, distance range strengthened successivelyapplying following operation fixed point reached:: g(p, q) g(p, q) (g(p, s) g(s, q))constant D, 6= p, 6= q, g(p, q) denotes distance range p, q.process called enforcing path-consistency D. fixed point, every pairconstants p, q, g(p, q) 6= , called path-consistent.paper, say interval referred process enforcing path-consistencyD, occurs involved enforcement operation g(p, q) g(p, q)(g(p, s)g(s, q)). words, used g(p, q), g(p, s) g(s, q). distance constraintappears process enforcing path-consistency D, distance range (an interval)referred process enforcing path-consistency D.way enforcing path-consistency set distance constraints definedalmost enforcing path-consistency binary constraint satisfactionproblem (CSP) (Renz & Nebel, 2007; van Beek, 1992), except operation :g(p, q) g(p, q) (g(p, s) g(s, q)) ( composition operator non-negative intervals,Definition 7) applied instead k : Rij Rij (Rik Rkj ) ( composition operator5. Based d(x, z) d(x, y) + d(y, z) (Property 3 Definition 1).707fiDu & Alechinarelations). time complexity path-consistency algorithm CSP O(n3 ) (vanBeek, 1992; Mackworth & Freuder, 1985), n number variables involvedinput set binary constraints. path-consistency algorithm CSP adaptedeasily enforcing path-consistency set distance constraints. time complexityresulting path-consistency algorithm also O(n3 ), n number constantsinvolved input set distance constraints. Later paper, showprocess enforcing path-consistency D(+ ) terminates, fixed pointreached O(n3 ) (see Lemma 33).constructing set distance constraints D(+ ) + , prove MetricModel Lemma, Metric Space Lemma Path-Consistency Lemma stated below.notion path-consistency acts bridge lemmas.Lemma 6 (Metric Model Lemma) Let finite consistent set formulas, +CS contains set individual names .metric space satisfies D(+ ), extended metric model satisfying + .Lemma 7 (Metric Space Lemma) Let finite consistent set formulas, +CS contains set individual names . D(+ )path-consistent, metric space (, d) distance constraintsD(+ ) satisfied.Lemma 8 (Path-Consistency Lemma) Let finite consistent set formulas,+ CS contains set individual names . Then,D(+ ) path-consistent.Using three lemmas, prove completeness LBPT: finite set formulasLBPT-consistent, exists metric model satisfying it.Proof. LBPT-consistent, Lindenbaums Lemma (Lemma 5), CS+ set individual names + . Path-ConsistencyLemma (Lemma 8) Metric Space Lemma (Lemma 7), metric space (, d)distance constraints D(+ ) satisfied. Metric Model Lemma(Lemma 6), metric space extended metric model satisfying + . Since+ , satisfies .detailed proofs Metric Model Lemma, Metric Space Lemma PathConsistency Lemma provided Section 5.1, Section 5.2 Section 5.3 respectively.Note that, paper, + denotes CS contains given finite consistent setformulas set individual names .5.1 Metric Model Lemmasection shows construct set distance constraints D(+ ) + ,presents proof Metric Model Lemma.definition properties MCSs (Definition 4 Proposition 1), followinglemma holds.708fiQualitative Spatial Logics Buffered GeometriesLemma 9 + CS, pair individual names a, b occurring ,exactly one following cases holds + :1. case(a, b) = BP (a, b) BP (b, a);2. case(a, b) = BP (a, b) BP (b, a);3. case(a, b) = BP (a, b) BP (b, a);4. case(a, b) = BP (a, b) BP (b, a) N EAR(a, b);5. case(a, b) = N EAR(a, b) F AR(a, b);6. case(a, b) = F AR(a, b),case(a, b) denotes formula holds a, b case.Lemma 9 proved using LBPT axioms facts (such Axiom 3, Facts 14, 15)way proving lemma LNF (see Du et al., 2013). full proof Lemma 9provided Appendix A.construction set distance constraints D(+ ) + two main steps:Step 1 every pair individual names a, b occurring , translate case(a, b)set first order formulas equi-satisfiable case(a, b). unionsets first order formulas B(+ ) (hence, B(+ ) equi-satisfiable.).step described Definition 9 Definition 10.Step 2 construct set distance constraints D(+ ) B(+ ). step described Definitions 11-13.LBPT formulas, first order formulas corresponding truth definitionDefinition 3. use formulas form d(p, q) g abbreviations equivalentfirst order formulas. example, d(p, q) [0, ] abbreviates d(p, q) 0 d(p, q) .Observe that6BP (a, b) pa pb b : (pa , pb ) [0 , ] equi-satisfiable ;N EAR(a, b) pa pb b : (pa , pb ) [0 , 2 ] equi-satisfiable;F AR(a, b) pa pb b : (pa , pb ) (4 , ) equi-satisfiable.Definition 9 (Basic Quantified Formula) refer first order formulasfollowing forms basic quantified formulas:pa pb b : (pa , pb ) g;pa pb b : (pa , pb ) g;6. Note pa pb b : (pa , pb ) [0 , ], actually quantifying metric space.sense, precise say, example, BP (a, b) satisfiable metric model, iffpa pb b : (pa , pb ) [0 , ] satisfiable metric space.709fiDu & Alechinapa pb b : (pa , pb ) g;pa pb b : (pa , pb ) g,g non-negative interval. abbreviations four forms defined(a, b, g), (a, b, g), (a, b, g) (a, b, g) respectively. words, example,(a, b, g) (pa pb b : (pa , pb ) g).translate formula case listed Lemma 9 basic quantifiedformulas, used count number points needed interpreting individualnames occurring later.Definition 10 (B(+ )) CS + set individual names ,corresponding set basic quantified formulas B(+ ) constructed follows. everypair individual names a, b, translate case(a, b) basic quantified formulas:translate(BP (a, b) BP (b, a)) = {(a, b, [0, ]), (b, a, [0, ])};translate(BP (a, b) BP (b, a)) = {(a, b, [0, ]), (b, a, (, ))};translate(BP (a, b) BP (b, a)) = {(a, b, (, )), (b, a, [0, ])};translate(BP (a, b) BP (b, a) N EAR(a, b)) = {(a, b, (, )),(b, a, (, )), (a, b, [0, 2]), (b, a, [0, 2])};translate(N EAR(a, b) F AR(a, b)) = {(a, b, (2, )), (b, a, (2, )),(a, b, [0, 4]), (b, a, [0, 4])};translate(F AR(a, b)) = {(a, b, (4, )), (b, a, (4, ))},R0 fixed margin error. Let names() set individual namesoccurring . Then,B(+ ) = anames(),bnames() translate(case(a, b)).following, set basic quantified formulas B(+ ), construct setdistance constraints D(+ ), show metric space satisfying D(+ ),extended model + . words, constructing metricset points used interpret individual names.number points needed interpreting individual name dependsnumbers different forms formulas B(+ ). individual name a, let us predictmany particular constants points(a) (points assigned individual name a)specified finite set formulas B(+ ). points(a) contains leastone constant. formula B(+ ) says exists constant points(a),constant particular constant within points(a). pair different individual namesa, b, (a, b, g) (b, a, g) B(+ ), count one them; (a, b, g)B(+ ), map constants points(a) constant points(b).Lemma 9 Definition 10, B(+ ), pair different individual names a, bR {, , } never R(a, b, g1 ) R(a, b, g2 ), g1 6= g2 , time.cardinality points(a) specified follows.710fiQualitative Spatial Logics Buffered GeometriesDefinition 11 (num(a, B(+ )) points(a)) Let names() set individual namesoccurring 7 . individual name names(),num(a, B ( + )) = |{b names( ) | g : (a, b, g) B ( + )}|num(a, B ( + )) = |{b names( ) | g : (a, b, g) B ( + )}|num(a, B ( + )) = |{b names( ) | g : (b, a, g) B ( + )}|num(a, B ( + )) = num(a, B ( + )) + num(a, B ( + )) + num(a, B ( + )).points(a) set constants {p1a , . . . , pna }, n = num(a, B(+ )).Definition 12 (Witness formula) witness formula (a, b, g) pairconstants pa points(a), pb points(b) d(pa , pb ) g. witness formula(a, b, g) (b, a, g) constant pa points(a), constant pb points(b),d(pa , pb ) g. constant clean formula, witness formula.Definition 13 (D(+ )) Let B(+ ) corresponding set basic quantified formulasCS + . every individual name , assign fixed set new constants points(a) it. construct set distance constraints D(+ ) follows,iterating basic quantified formulas B(+ ) eliminating quantifiers newconstants. Initially, D(+ ) = {}. every individual name , every constantpa points(a), add d(pa , pa ) {0} D(+ ). every pair different individualnames a, b,(a, b, g) B ( + ), take clean constants pa points(a), pb points(b),add (pa , pb ) = (pb , pa ) g D(+ ), pa , pb become witness (a, b, g);(a, b, g) B ( + ), take clean constant pa points(a), every pb points(b),add (pa , pb ) = (pb , pa ) g D(+ ), pa becomes witness (a, b, g);(b, a, g) B ( + ), take clean constant pb points(b), every pa points(a),add (pa , pb ) = (pb , pa ) g D(+ ), pb becomes witness (b, a, g);(a, b, g) B ( + ), take clean constant pb points(b), every pa points(a),add (pa , pb ) = (pb , pa ) g D(+ ), pb becomes witness (a, b, g);(b, a, g) B ( + ), take clean constant pa points(a), every pb points(b),add (pa , pb ) = (pb , pa ) g D(+ ), pa becomes witness (b, a, g);(a, b, g) B ( + ), every pair constants pa points(a), pb points(b),add (pa , pb ) = (pb , pa ) g D(+ ).every pair different constants p, q D(+ ), add (p, q) = (q, p) [0 , )D(+ ), repeatedly replace (p, q) = (q, p) g1 (p, q) = (q, p) g2(p, q) = (q, p) (g1 g2 ), one distance range pair p, qD(+ ).7. definition + , + contains set individual names .711fiDu & AlechinaDefinition 13, every pair different individual names a, b, check whether(a, b, g) B ( + ) holds check whether (b, a, g) B ( + ) holds, possible one holds. reason, check (a, b, g) B ( + )(b, a, g) B ( + ) separately. Definition 10, (a, b, g) B ( + ) iff (b, a, g) B ( + ).Hence need check one them. check whether (a, b, g) B ( + )holds, (a, b, g) B ( + ) iff (b, a, g) B ( + ).Lemma 10 constructing D(+ ), individual name a, number cleanconstants needed points(a) larger num(a, B(+ )).Proof. Definition 10, individual name a, (a, a, [0, ]) B(+ ). Definition 11, num(a, B(+ )) 1.involved formula form (a, b, g), (a, b, g) (b, a, g),individual name b, Definition 11, num(a, B(+ )) = 1. Definition 13,need clean constants points(a).Otherwise, Lemma 9 Definition 10, B(+ ), pair different individualnames a, b R {, , }, never R(a, b, g1 ) R(a, b, g2 ), g1 6= g2 ,time. Definition 13, (a, b, g) B(+ ), take one clean constantpoints(a), num(a, B(+ )) clean constants needed total formulasform. Similarly, num(a, B(+ )) (num(a, B(+ )) 1) clean constants neededformulas forms (a, b, g) (b, a, g) respectively, a, b different individualnames. need clean constant points(a) formulas forms.Definition 11, num(a, B(+ )) enough.D(+ ) B(+ ) equi-satisfiable way assign witnessesformulas. specifically, pair different individual names a, b, (a, b, g)B(+ ), map constants points(a) constant points(b).words, B 0 (+ ) set formulas resulting replacing every (a, b, g) B(+ )(b, a, g), D(+ ) B 0 (+ ) equi-satisfiable. Since every individual nameinterpreted non-empty set constants, model satisfies (b, a, g), satisfies(a, b, g), vice versa. Hence, constructing D(+ ) B 0 (+ ) rather B(+ )imposes stronger restrictions (i.e. (a, b, g) B(+ ) replaced (b, a, g) B 0 (+ ))metric space compared required + . However, later show+ consistent, D(+ ) satisfied metric space proving MetricSpace Lemma Path-Consistency Lemma following sections.proving Metric Model Lemma, let us look important propertiesD(+ ), shown Lemmas 11-13. proof Lemma 11 provided Appendix A.Lemma 12 follows proof Lemma 11.Lemma 11 distance range g occurring D(+ ),g {{0}, [0, ], (, ), [0, 2], (2, ), (2, 4], (4, ), [0, )}.Lemma 12 p points(a), q points(b), 6= b, d(p, q) {0} D(+ ).Lemma 13 number constants D(+ ) finite.712fiQualitative Spatial Logics Buffered GeometriesProof. assumed finite consistent set formulas n (a finite number) individual names. Lemma 9 Definition 10, B(+ ) contains f =(n + 2n(n 1)) formulas n individual names. Definition 11, individualname a, num(a, B(+ )) f . Definition 13, number constants D(+ )nf .Metric Model Lemma proved follows.Lemma 14 metric model satisfies B(+ ), satisfies + .Proof. lemma follows two observations. First, Lemma 9, + entailedset C(+ ) = {case(a, b) : names(+ ), b names(+ )}. Second, Definition 10,B(+ ) translation truth conditions C(+ ) first order logic. metric modelsatisfies B(+ ), satisfies C(+ ), hence satisfies + .Lemma 6 (Metric Model Lemma) Let finite consistent set formulas,+ CS contains set individual names .metric space satisfies D(+ ), extended metric model satisfying + .Proof. Suppose metric space satisfies D(+ ). extend metric modelinterpreting every individual name occurring + points(a), corresponding setconstants size num(a, B(+ )) (Definition 11 Definition 13). Definition 13,formula form (a, a, [0, ]) satisfied . pair different individual names, every , formula witness, formulas also satisfied. Therefore, metric model B(+ ). Lemma 14, metric model + .5.2 Metric Space Lemmaprocess enforcing path-consistency (Definition 8) involves applicationcomposition operator (Definition 7), present several lemmas Section 5.2.1 demonstrate main calculation rules properties intervals obtained composition. Section 5.2.2, characterize distance constraints D(+ ) appearingprocess enforcing path-consistency D(+ ). Using definitions lemmas introduced Section 5.2.1 Section 5.2.2, Metric Space Lemma provedSection 5.2.3.5.2.1 Composition Operatorsection, present several lemmas show main calculation rules composition operator properties intervals obtained composition. lemmasimportant understanding several proofs later sections.Lemmas 15-16 follow Definition 7.Lemma 15 Let g1 , g2 non-negative intervals. d3 g1 g2 , exist d1 g1 ,d2 g2 d3 [|d1 d2 |, d1 + d2 ].713fiDu & AlechinaLemma 16 (Calculation Composition) (m, n), (s, t), (m, ), (s, ), {l}, {r}non-negative non-empty intervals, H1 , H2 , H non-negative intervals, followingcalculation rules hold:1. {l} {r} = [l r, l + r], l r;2. {l} (s, t) = (s l, + l), l;3. {l} (s, t) = [0, + l), l (s, t);4. {l} (s, t) = (l t, + l), l;5. {l} (s, +) = (s l, +), l;6. {l} (s, +) = [0, +), < l;7. (m, n) (s, t) = (s n, + n), n;8. (m, n) (s, t) = [0, + n), (m, n) (s, t) 6= ;9. (m, n) (s, +) = (s n, +), n;10. (m, n) (s, +) = [0, +), < n;11. (m, +) (s, +) = [0, +);12. H1 = ;13. H1 H2 = H2 H1 ;14. (H1 H2 ) H = (H1 H) (H2 H);15. ( k Hk ) H = k (Hk H), k N>0 ;16. (H1 H2 ) H = (H1 H) (H2 H), (H1 H2 ) 6= ;17. (H1 H2 ) H = H1 (H2 H).Lemma 16, Rule 14 special case Rule 15, k = 2. Rule 16 statescomposition operation distributive non-empty intersections intervals.necessary require H1 H2 6= , otherwise property may hold. example,H1 = [0, 1], H2 = [2, 3], H = [1, 2], (H1 H2 ) H = whilst (H1 H) (H2 H) =[0, 3] [0, 5] 6= . similar property defined Li, Long, Liu, Duckham,(2015) RCC relations. proofs last three calculation rules providedAppendix A, whilst others obvious.interval h form (l, u), [l, u), (l, u] [l, u], call l greatest lowerbound h, represented glb(h), u least upper bound h, represented lub(h).show interesting properties regarding composition intervalsgreatest lower/least upper bounds.Lemma 17 non-negative non-empty intervals g, h, following properties hold:714fiQualitative Spatial Logics Buffered Geometries1. lub(g h) = lub(g) + lub(h);2. glb(g h) max(glb(g), glb(h)).Proof. Follows Lemma 16.non-empty interval h right-closed, iff h = [x, y] h = (x, y]. h right-open, iffh = [x, y) h = (x, y). h right-infinite, iff h = [x, ) h = (x, ). h left-closed, iffh = [x, y] h = [x, y). h left-open, iff h = (x, y] h = (x, y).Lemma 18 Let g1 , g2 , g3 non-negative non-empty right-closed intervals, g1 g2 g3 ,lub(g1 ) lub(g2 ) + lub(g3 ).Proof. Suppose g1 g2 g3 . Since lub(g1 ) g1 , lub(g1 ) g2 g3 . Lemma 15,exist d2 g2 , d3 g3 , lub(g1 ) d2 + d3 . Since d2 lub(g2 ), d3 lub(g3 ),lub(g1 ) lub(g2 ) + lub(g3 ).Lemma 19 Let g1 , g2 , g3 non-negative non-empty intervals, g1 g2 g3 . g1 rightinfinite, g2 g3 right-infinite.Proof. Suppose g1 right-infinite. Since g1 g2 g3 , g2 g3 right-infinite. Definition 7 Lemma 16, g2 g3 right-infinite.5.2.2 Distance Constraints D(+ ) DS(+ )section, characterize distance constraints appear processenforcing path-consistency D(+ ) two main steps:Step 1 characterize intervals involved D(+ ), well compositionintervals. step described Definition 14 Lemmas 20-24.Step 2 introduce notion DS(+ ) set containing distance constraintsappearing process enforcing path-consistency D(+ ), characterizedistance constraints DS(+ ). step described Definitions 15-17Lemmas 25-31.Definition 14 (Primitive, Composite, Definable Intervals) Let h non-negativeinterval. h primitive, h one [0, ], (, ), [0, 2], (2, ), (2, 4], (4, ),[0, ). h composite, obtained composition least two primitiveintervals. h definable, primitive composite.Lemma 20 non-negative interval h, h {0} = h.Proof. Follows Definition 7.Since Lemma 20 holds, call {0} identity interval.715fiDu & AlechinaLemma 21 interval occurs D(+ ), identity interval primitiveinterval.Proof. Follows Definition 14 Lemma 11.Lemma 22 h definable interval, h 6= .Proof. Follows Definition 14 Definition 7.Lemma 23 interval h definable, following properties hold:1. glb(h) = n, n {0, 1, 2, 3, 4};2. lub(h) = + lub(h) = m, N>0 .Proof. Let us prove induction structure h.Base case: h primitive. Definition 14, n {0, 1, 2, 4}, lub(h) = + {1, 2, 4}.Inductive step: Suppose Properties 1, 2 hold interval ht obtainedcomposition primitive intervals, N>0 (induction hypothesis).show Properties 1, 2 hold interval ht+1 obtained composition(t + 1) primitive intervals.ht+1 , exist ht primitive interval hp ht+1 = ht hp .induction hypothesis, glb(ht ) = nt , nt {0, 1, 2, 3, 4}; lub(ht ) = + lub(ht ) = mt ,mt N>0 . base case, glb(hp ) = np , np {0, 1, 2, 4}; lub(hp ) = + lub(hp ) =mp , mp {1, 2, 4}. Lemma 17, lub(ht+1 ) = lub(ht ) + lub(hp ). Thus, Property 2 holds.Lemma 16,lub(ht ) < glb(hp ), glb(ht+1 ) = glb(hp ) lub(ht );lub(hp ) < glb(ht ), glb(ht+1 ) = glb(ht ) lub(hp );otherwise, glb(ht+1 ) = 0.Since mt > 0 mp > 0, glb(ht+1 ) = nt+1 , nt+1 < 4. case, nt+1 {0, 1, 2, 3}(Property 1 holds).Lemma 24 h identity definable interval, then:1. lub(h) = 0, iff h = {0};2. lub(h) = , iff h = [0, ];3. glb(h) = 4, iff h = (4, ).Proof. Follows Lemma 17, Lemma 23 proof.start characterize distance constraints appear processenforcing path-consistency D(+ ).716fiQualitative Spatial Logics Buffered GeometriesDefinition 15 (DS(+ )) DS(+ ) minimal set distance constraintsfollowing holds:distance constraint D(+ ) DS(+ );distance constraints d(p, q) h d(q, s) g DS(+ ), d(p, s) h gDS(+ );distance constraints d(p, q) h d(p, q) g DS(+ ), d(p, q) h gDS(+ ),p, q, constants D(+ ).definition above, DS(+ ) required minimal, intervalinvolved DS(+ ) either D(+ ) obtained applying composition intersectionoperations intervals D(+ ). generality, restrict p, q, differentconstants. example, possible p = q.Lemma 25 distance constraint appears process enforcing path-consistencyD(+ ), DS(+ ).Proof. Follows Definition 8 (path-consistency) Definition 15.DS(+ ) covers distance constraints appearing process enforcing pathconsistency D(+ ). However, every distance constraint DS(+ ) necessarilyappears process enforcing path-consistency D(+ ). example, D(+ )contains exactly one distance constraint d(p, p) [0, ], Definition 15, d(p, p)[0, 2] DS(+ ) (so d(p, p) [0, n], n N>0 ), Definition 8, d(p, p)[0, 2] appear process enforcing path-consistency. easy seeDS(+ ) infinite set.concept DS(+ ) similar concept distributive subalgebra definedLi et al. (2015), composition operation distributes non-empty intersectionsintervals involved DS(+ ) (Rule 16 Lemma 16). However, work, compositionoperation defined intervals rather relations.Lemma 26 distance constraint d(p, q) h DS(+ ), h non-negativeinterval.Proof. distance constraint d(p, q) h D(+ ), Lemma 11, h non-negativeinterval. Definitions 5, 7 definition intersection, applying compositionintersection non-negative intervals, obtain non-negative intervals. Definition 15,h non-negative interval.Differing previous version (Du & Alechina, 2014b), following definitionslemmas restricted non-empty intervals.Recall non-empty interval h right-closed, iff h = [x, y] h = (x, y]. hright-open, iff h = [x, y) h = (x, y). h right-infinite, iff h = [x, ) h = (x, ). hleft-closed, iff h = [x, y] h = [x, y). h left-open, iff h = (x, y] h = (x, y).717fiDu & AlechinaLemma 27 distance constraint d(p, q) h DS(+ ) h 6= , h eitherright-infinite right-closed.Proof. Let n denote total number times applying composition intersectionobtain h, n 0. prove induction n.Base case: n = 0, d(p, q) h D(+ ). Lemma 11, h either right-infiniteright-closed. Inductive step: Suppose statement holds non-empty hobtained applying composition intersection n times (inductionhypothesis). show also holds non-empty h obtainedapplying composition intersection (n + 1) times.last step obtain h intersection, Definition 15, exist non-emptyh1 , h2 h = h1 h2 . induction hypothesis, hi , {1, 2}, hieither right-infinite right-closed. intersection rules, h either right-infiniteright-closed.last step obtain h composition, Definition 15, exist nonempty h1 , h2 h = h1 h2 . induction hypothesis, hi , {1, 2}, hieither right-infinite right-closed. composition rules (Lemma 16), h eitherright-infinite right-closed.Lemma 28 distance constraint d(p, q) h DS(+ ) h 6= , glb(h) 6= 0,h left-open.Proof. Let n denote total number times applying composition intersectionobtain h, n 0. prove induction n.Base case: n = 0, d(p, q) h D(+ ). Lemma 11, glb(h) 6= 0,h left-open. Inductive step: Suppose statement holds non-empty hobtained applying composition intersection n times (inductionhypothesis). show also holds non-empty h obtainedapplying composition intersection (n + 1) times.last step obtain h intersection, Definition 15, exist nonempty h1 , h2 h = h1 h2 . induction hypothesis, hi , {1, 2},glb(hi ) 6= 0, hi left-open. intersection rules, glb(h) 6= 0, hleft-open.last step obtain h composition, Definition 15, exist non-emptyh1 , h2 h = h1 h2 . glb(h) 6= 0, composition rules (Lemma 16),h1 h2 = . Suppose lub(h1 ) glb(h2 ), glb(h) = glb(h2 )lub(h1 ). Lemma 26glb(h) 6= 0, glb(h) > 0, thus glb(h2 ) > lub(h1 ). Lemma 26, lub(h1 )0, thus glb(h2 ) > 0. induction hypothesis, h2 left-open. composition rules(Lemma 16), h left-open. Similarly, also holds lub(h2 ) glb(h1 ).718fiQualitative Spatial Logics Buffered Geometriesdistance constraint d(p, q) h DS(+ ), Definition 15, h obtainedapplying composition and/or intersection operations n 0 times intervals occurringD(+ ). applying intersection operation generate new bound,greatest lower/least upper bound (and openness) h mustinterval D(+ ) composition intervals D(+ ). formalize rationaleconcepts Left-Definable Right-Definable characterize distance constraints DS(+ ). Later, show every distance constraint d(p, q) h (h 6= )DS(+ ) left-definable right-definable. Left-Definable Right-Definable keyconcepts proving Path-Consistency Lemma, establish correspondencesdistance constraint DS(+ ) sequence distance constraints D(+ ).non-empty interval h left-open, greatest lower bound representedglb (h). h left-closed, greatest lower bound represented glb+ (h). hright-open, least upper bound represented lub (h). h right-closed,least upper bound represented lub+ (h).Definition 16 (Left-Definable) distance constraint d(p1 , pn ) hs (n > 1) leftdefinable, iff hs 6= exists sequence distance constraints d(pi , pi+1 ) hi(0 < < n) D(+ ), = h1 ... hn1 , following holds:1. hs left-open, left-open glb (m) = glb (hs );2. hs left-closed, left-closed glb + (m) = glb + (hs );3. hs m.Definition 17 (Right-Definable) distance constraint d(p1 , pn ) hs (n > 1) rightdefinable, iff hs 6= exists sequence distance constraints d(pi , pi+1 ) hi(0 < < n) D(+ ), = h1 ... hn1 , following holds:1. hs right-open, right-open lub (m) = lub (hs );2. hs right-closed, right-closed lub + (m) = lub + (hs );3. hs m.important distinguish definition left-definable/right-definable distanceconstraints (Definitions 16 17) Definition 14 (Definable Intervals). example,distance constraints d(p1 , p2 ) {0} d(p2 , p3 ) {0} D(+ ), d(p1 , p3 ) {0}left-definable right-definable, {0} definable interval. distance constraintsd(p1 , p2 ) [0, ] d(p2 , p3 ) (4, ) D(+ ), d(p1 , p3 ) (3, 5] leftdefinable, (3, 5] definable interval.Lemma 29 Let h, g non-negative intervals. distance constraints d(p, q) hd(q, s) g left-definable right-definable, d(p, s) h g left-definableright-definable.719fiDu & AlechinaProof. Since d(p, q) h d(q, s) g right-definable, Definition 17, h 6= ,g 6= . Definition 7, h g 6= . Definition 17, D(+ ), exist sequencedistance constraints d(p, x2 ) h1 , ..., d(xn1 , q) hn1 d(p, q) h sequencedistance constraints d(q, y2 ) g1 , ..., d(yt1 , s) gt1 d(q, s) g respectively satisfyingthree properties. Let us take union two sequences new one, is,d(p, x2 ) h1 , ..., d(xn1 , q) hn1 , d(q, y2 ) g1 , ..., d(yt1 , s) gt1 . compositionrules (Lemma 16), new sequence satisfies properties Definition 17 d(p, s) hg.Hence, d(p, s) h g right-definable.composition rules (Lemma 16), h g 6= , glb+ (h g) = 0. usenew sequence above. Let m1 = (h1 ... hn1 ), m2 = (g1 ... gt1 ). Definition 17,h m1 , g m2 . m1 m2 6= , therefore, glb+ (m1 m2 ) = 0. Definition 7,h g m1 m2 . Definition 16, d(p, s) h g left-definable.h g = , let us suppose glb(h) lub(g). Since d(p, q) h left-definabled(q, s) g right-definable, Definitions 16 17 respectively, D(+ ), existsequence distance constraints d(p, q) h sequence distance constraintsd(q, s) g, satisfying corresponding properties. composition rules (Lemma16), union two sequences satisfies properties Definition 16 d(p, s) hg.Hence, d(p, s) h g left-definable. Similarly, show d(p, s) h g left-definable,glb(g) lub(h).Lemma 30 Let h, g non-negative intervals. distance constraints d(p, q) hd(p, q) g left-definable right-definable, h g 6= , d(p, q) h g leftdefinable right-definable.Proof. applying intersections generate new bound h g 6= ,left/right bound h g h g. left bound h gh, Definition 16, sequence used showing d(p, q) hleft-definable used show d(p, q) h g left-definable. cases similar.Lemma 31 distance constraint d(p, q) h DS(+ ) h 6= , leftdefinable right-definable.Proof. Let n denote total number times applying composition intersectionobtain h, n 0. prove induction n.Base case: n = 0, d(p, q) h D(+ ). Definitions 16 17, d(p, q) hleft-definable right-definable.Inductive step: Suppose statement holds non-empty h obtainedapplying composition intersection n times (induction hypothesis).show also holds non-empty h obtained applying compositionintersection (n + 1) times. Definition 15, last operation obtain h eithercomposition intersection. former case, exist d(p, s) g1 d(s, q) g2DS(+ ), g1 g2 = h. h 6= , Definition 7, gi 6= , {1, 2}. Since g1g2 obtained applying composition intersection n times,induction hypothesis, d(p, s) g1 d(s, q) g2 left-definable right-definable.720fiQualitative Spatial Logics Buffered GeometriesLemma 29, d(p, q) h left-definable right-definable. latter case, existd(p, q) g1 d(p, q) g2 DS(+ ), g1 g2 = h. h 6= , intersectionrules, gi 6= , {1, 2}. induction hypothesis, d(p, q) g1 d(p, q) g2 leftdefinable right-definable. Lemma 30, d(p, q) h left-definable right-definable.generality, exclude possibility d(p, q) DS(+ ). However,follows proof Path-Consistency Lemma Section 5.3 d(p, q)DS(+ ). Alternatively, direct proof, see Lemmas 45 46 Appendix C.5.2.3 Proving Metric Space Lemmafollowing, show metric space satisfying D(+ ), D(+ ) pathconsistent (Metric Space Lemma). Firstly, show process enforcing patchconsistency D(+ ) terminates. Lemma 13, number constants D(+ )finite. Let us suppose number constants D(+ ) N>0 .Lemma 32 Let N>0 number constants D(+ ). non-emptyright-closed interval h referred process enforcing path-consistency D(+ ),lub+ (h) 4t.Proof. non-empty right-closed interval h occurs D(+ ), Lemma 11,lub+ (h) 4 4t.Otherwise, generated application composition and/or intersection operators Definition 8. Composition creates larger least upper bounds (Lemma 17), whilstintersection not. Since h right-closed, lub+ (h) obtained composing right-closedintervals (Lemma 16). constants, longest chain involves (t 1) intervals.lub+ (h) maximal use (t 1) intervals least upper boundinterval 4. Thus, lub+ (h) 4t.Lemma 33 Let N>0 number constants D(+ ). Enforcing path-consistencyD(+ ), fixed point reached O(t3 ).Proof. Definition 8, Lemmas 23 fact intersection generate newbounds, interval appearing process enforcing path-consistency D(+ ),following properties hold:1. glb(s) = n, n {0, 1, 2, 3, 4};2. lub(s) = + lub(s) = m, N>0 .interval h appearing D(+ ), enforcing path-consistency (Definition 8), hbecome h0 h. Lemma 11, h 6= . Lemma 27, h either right-closedright-infinite, h0 , right-closed right-infinite.h right-closed, h0 = h0 right-closed. h0 right-closed,Lemma 11, lub(h0 ) lub(h) 4. Properties 1, 2, finitely manypossibilities h0 .721fiDu & Alechinah right-infinite, h0 , right-closed right-infinite.h0 right-closed, Lemma 32, lub(h0 ) 4t. Properties 1, 2,finitely many possibilities h0 .h0 right-infinite, Property 1, finitely many possibilitiesgreatest lower bound, thus h0 .Since case, finitely many possibilities h0 , fixed point always reached.Suppose widest non-negative interval [0, ) appears process enforcingpath-consistency D(+ ). worst case, firstly, [0, ) strengthened [0, u],u 4t (by Lemma 32), [0, u] strengthened time. Hence, [0, )strengthened (4t + 1) times. constants, Definition 13, O(t2 )distance constraints D(+ ). interval h appearing D(+ ), h [0, ), hence hstrengthened (4t + 1) times. Therefore, total time strengtheningdistance constraints O(t3 ).following lemma shows construct metric space D(+ ). usedprove Metric Space Lemma.Lemma 34 Let N>0 number constants D(+ ), Df (+ ) fixed pointenforcing path consistency D(+ ). D(+ ) path-consistent, Ds (+ ) obtainedDf (+ ) replacing every right-infinite interval {5t}, every right-closed interval h{lub(h)}, Ds (+ ) path-consistent.Proof. Suppose D(+ ) path-consistent. Lemma 25, Df (+ ) DS(+ ). Definition 8, interval h appearing Df (+ ), h 6= . Lemma 27, h either right-infiniteright-closed. prove Ds (+ ) path-consistent, need show threedistance ranges, {npq }, {nqs }, {nps } Ds (+ ) three constants p, q, s,1. npq nqs + nps ;2. nqs npq + nps ;3. nps npq + nqs .Let hpq , hqs , hps denote corresponding distance ranges {npq }, {nqs }, {nps } respectivelyDf (+ ), Definition 8,hpq hqs hps ;hqs hpq hps ;hps hpq hqs .prove Ds (+ ) path-consistent cases:every hi (i {pq, qs, ps}) right-closed, ni = lub(hi ). Lemma 18, 1-3hold.722fiQualitative Spatial Logics Buffered GeometriesOtherwise, right-closed. Lemma 19, least tworight-infinite.right-infinite, ni = 5t. Since 5t 5t + 5t, 1-3 hold.Otherwise, one right-closed. Let hpq right-closed. Then,npq = lub(hpq ), nqs = 5t, nps = 5t. Lemma 32 R0 , lub(hpq )4t < 5t. Lemma 26, lub(hpq ) 0. Since lub(hpq ) < 5t + 5t5t 5t + lub(hpq ), 1-3 hold.Lemma 7 (Metric Space Lemma) Let finite consistent set formulas, +CS contains set individual names . D(+ )path-consistent, metric space (, d) distance constraintsD(+ ) satisfied.Proof. Suppose D(+ ) path-consistent. Let set constants D(+ ),used interpret individual names occurring , shown Definition 13. = ,trivial. Let us assume 6= . number constants denoted N>0 .Lemma 33, fixed point Df (+ ) reached enforcing path-consistency D(+ ).Let Ds (+ ) set distance constraints obtained Df (+ ) replacing everyright-infinite interval {5t}, every right-closed interval h {lub(h)}. Since everydistance constraint Ds (+ ) form d(p, q) {r}, r R0 , d(p, q) {r}equivalent d(p, q) = r, metric (distance function) defined . Definition 13Lemma 34, pair constants x, y, x = y, d(x, y) = 0 holds Ds (+ );x 6= y, d(x, y) > 0 holds Ds (+ ). Thus, d(x, y) = 0 iff x =Ds (+ ). Definitions 13 8, pair constants x, y, d(x, y) = d(y, x) holdsDs (+ ). Lemma 34, Ds (+ ) path-consistent. Thus, constants x, y, z,d(x, z) d(x, y) + d(y, z) holds Ds (+ ). Definition 1, (, d) Ds (+ )metric space distance constraints D(+ ) satisfied.5.3 Path-Consistency Lemmasection proves Path-Consistency Lemma contradiction, supposing D(+ )path-consistent. examine every case first interval obtainedenforcing path-consistency. case, show derivable correspondingLBPT formulas + using LBPT axioms. contradicts assumption +consistent. Lemma 35 used generate possible cases make sure duplicated onesgenerated. using Lemma 35, proof Path-Consistency Lemma largelysimplified, compared previous version (Du & Alechina, 2014b).Lemma 35 Let g, h non-negative intervals. g h = iff (g h) {0} = .Proof. g h 6= , Definition 7, 0 (g h).0 (gh), Lemma 15, exist d1 g, d2 h 0 [|d1 d2 |, d1 +d2 ].Thus, d1 = d2 . Therefore, g h 6= .723fiDu & AlechinaSince g h 6= iff 0 (g h), contraposition get g h = iff (g h) {0} = .Knowing least upper bound greatest lower bound definable interval h, Lemmas 36-42 show possible ways h obtained composition primitiveintervals. Lemma 36 Lemma 39 proved below. Proofs lemmas similar omitted.Lemma 36 interval h definable, lub(h) = 2, h primitive interval [0, 2]h obtained composition two [0, ].Proof. h primitive, Definition 14, h = [0, 2].h composite, Definition 14, exist two definable intervals g1 , g2g1 g2 = h. Lemma 17, lub(g1 ) + lub(g2 ) = 2. Lemma 23, lub(g1 ) , lub(g2 ) ,thus lub(g1 ) = , lub(g2 ) = . Lemma 24, h = [0, ] [0, ].Lemma 37 interval h definable, lub(h) = 3, h obtained composition[0, ] [0, 2] composition three [0, ].Lemma 38 interval h definable, lub(h) = 4, h primitive interval (2, 4],h obtained composition two [0, 2], composition two [0, ] one[0, 2] composition four [0, ].Lemma 39 interval h definable, glb(h) = 3, h obtained composition[0, ] (4, ).Proof. Definition 14, h cannot primitive.Since h composite, Definition 14, exist two definable intervals g1 , g2g1 g2 = h. g1 g2 = , otherwise, Lemma 16, glb(h) = 0.Without loss generality, let us suppose lub(g1 ) glb(g2 ). Lemma 16, glb(g2 )lub(g1 ) = 3. Lemma 23, glb(g2 ) 4, lub(g1 ) , thus glb(g2 ) = 4, lub(g1 ) = .Lemma 24, h obtained composition [0, ] (4, ).Lemma 40 interval h definable, glb(h) = 2, h primitive interval (2, )(2, 4], h obtained composition [0, 2] (4, ) compositiontwo [0, ] one (4, ).Lemma 41 interval h definable, glb(h) = , h primitive interval (, ),h obtained composition [0, ] (2, ), composition [0, ](2, 4], composition one [0, ], one [0, 2] one (4, ), compositionthree [0, ] one (4, ).Lemma 42 interval h definable left-open, glb(h) = 0, h obtainedexactly following ways:composition [0, ] (, );724fiQualitative Spatial Logics Buffered Geometriescomposition [0, 2] (2, );composition two [0, ] one (2, );composition [0, 2] (2, 4];composition two [0, ] one (2, 4];composition (2, 4] (4, );composition two [0, 2] one (4, );composition two [0, ], one [0, 2] one (4, );composition four [0, ] one (4, ).previous work (Du et al., 2013; Du & Alechina, 2014b), presented slightlydifferent way prove Path-Consistency Lemma LNF LBPT respectively:first empty interval obtained using strengthening operator, is, g1 (g2 g3 ) =gi 6= , {1, 2, 3}. gi may {0} primitive interval, writtenxi (yi zi ), xi , yi , zi may identity primitive internal also.latter case, since gi = xi (yi zi ) 6= , xi , yi , zi empty. Since compositionoperation distributive non-empty intersections intervals (Rule 16 Lemma 16),use Rule 16 repeatedly rewrite g1 (g2 g3 ) every interval identityprimitive interval. final form h1 ... hn = , n > 1, hx (0 < x n){0} definable interval. Thus exist two intervals hi , hj (0 < n, 0 < j n,6= j) hi hj = . look different combinationslub(hi ) glb(hj ). exactly 15 combinations. paper, proofPath-Consistency Lemma largely simplified. shows sufficient examine 5rather 15 combinations.Lemma 8 (Path-Consistency Lemma) Let finite consistent set formulas,+ CS contains set individual names . Then,D(+ ) path-consistent.Proof. Suppose D(+ ) path-consistent. Definitions 8, 15 Lemma 25,d(p, q) DS(+ ), constants p, q. Lemma 11, distance range goccurring D(+ ), g 6= . Definitions 15, 7, intersection rules, last operationobtain first interval intersection. Definition 15, exist d(p, q) g1d(p, q) g2 DS(+ ), g1 6= , g2 6= , g1 g2 = . Lemma 26, g1 , g2non-negative intervals. Lemma 35, g1 g2 = iff (g1 g2 ) {0} = .Definition 13 Definition 15, d(q, p) g2 DS(+ ). Since d(p, q) g1DS(+ ), Definition 15, d(p, p) (g1 g2 ) DS(+ ). Definition 7, g1 g2 6= .Lemma 31, d(p, p) (g1 g2 ) left-definable right-definable. Let h = g1 g2 .Since d(p, p) h left-definable, Definition 16, exists sequence distanceconstraints d(pi , pi+1 ) hi (0 < < n) D(+ ), p = p1 = pn h0 =h1 ... hn1 , h h0 greatest lower bound (including value725fiDu & Alechinaopenness) h h0 . Definition 14, Lemmas 21 20, h0 identity definableinterval. Lemma 23, glb(h0 ) {0, , 2, 3, 4}. Therefore, (g1 g2 ) {0} = iff onefollowing holds:glb(h) {, 2, 3, 4};h left-open glb (h) = 0.check whether derived every case using axioms (or derivable facts).Axiom 3 Axiom 4, N EAR F AR symmetric.1. glb(h) = : look different ways h0 obtained sequencedistance constraints d(pi , pi+1 ) hi (0 < < n) D(+ ) p = p1 = pnh0 = h1 ... hn1 (see Definition 16). every hi {0} primitive interval (byLemma 21), Lemma 41 specifies different ways obtain h0 :(a) h0 primitive interval (, +): Definition 16, d(p1 , pn ) (, +)D(+ ) n = 2. p = p1 = pn , d(p, p) (, +) D(+ ). Supposep points(a) individual name . proof Lemma 11, (, +)come formulas form (x, y, (, )), x, individual names. Definition 10, (x, y, (, )) come BP (x, y).Since d(p, p) (, +) D(+ ) p points(a), BP (a, a) + .Axiom 9, BP (a, a) .(b) h0 obtained composition [0, ] (2, ) composition[0, ] (2, 4]:proof Lemma 11 Definition 10, BP (a, b) + BP (b, a) + ,N EAR(a, b) + N EAR(b, a) + .Fact 14, BP (x1 , x2 ) N EAR(x1 , x2 ) , {x1 , x2 } = {a, b}.(c) h0 obtained composition one [0, ], one [0, 2] one (4, ):proof Lemma 11 Definition 10, BP (a, b) + BP (b, a) + ,N EAR(b, c) + , N EAR(c, b) + , F AR(c, a) + , F AR(a, c) + .Fact 16, BP (x2 , x1 )N EAR(x2 , x3 )F AR(x3 , x1 ) , {x1 , x2 , x3 } = {a, b, c}.(d) h0 obtained composition three [0, ] one (4, +):proof Lemma 11 Definition 10, three BP oneF AR four individual names a, b, c, d. BP refers either BP (x, y)BP (y, x). cases (for example, + , BP (a, b), BP (c, b),BP (d, c) F AR(a, d)) valid, different constantstaken points(b), individual name b (by Definition 13).consequence, invalid case, sequence consisting distance constraintsd(pi , pi+1 ) hi (0 < < n, p = p1 = pn ) cannot exist D(+ ). needconsider valid cases, listed below.i. BP (x1 , x2 ), BP (x2 , x3 ), BP (x3 , x4 ), F AR(x4 , x1 ),{x1 , x2 , x3 , x4 } = {a, b, c, d}. Fact 20, BP (x1 , x2 ) BP (x2 , x3 )BP (x3 , x4 ) F AR(x4 , x1 ) .ii. BP (x2 , x1 ), BP (x2 , x3 ), BP (x3 , x4 ), F AR(x4 , x1 ),{x1 , x2 , x3 , x4 } = {a, b, c, d}. Fact 21, BP (x2 , x1 ) BP (x2 , x3 )BP (x3 , x4 ) F AR(x4 , x1 ) .726fiQualitative Spatial Logics Buffered GeometriesCases 2-5 use similar arguments. following proof, BP refers eitherBP (x, y) BP (y, x) (whichever makes corresponding case valid). N EARF AR symmetric, thus order x, matter.2. glb(h) = 2: Definition 16 Lemma 21, Lemma 40 specifies differentways obtain h0 sequence distance constraints d(pi , pi+1 ) hi (0 < < n)D(+ ):(a) h0 primitive interval (2, ) (2, 4]:N EAR(a, a), using Axiom 9 Fact 14.(b) h0 obtained composition [0, 2] (4, +) :one N EAR one F AR, using Fact 15.(c) h0 obtained composition two [0, ] one (4, +):two BP one F AR, using Facts 18 19.3. glb(h) = 3: Definition 16 Lemma 21, Lemma 39 specifies waysobtain h0 sequence distance constraints d(pi , pi+1 ) hi (0 < < n)D(+ ). Lemma 39, h0 obtained composition [0, ] (4, +).one BP one F AR, using Fact 17.4. glb(h) = 4: Definition 16 Lemma 21, Lemma 24 specifies waysobtain h0 sequence distance constraints d(pi , pi+1 ) hi (0 < < n)D(+ ). Lemma 24, h0 = (4, +). F AR(a, a), using Axiom 9 Fact 17.5. glb (h) = 0: Definition 16 Lemma 21, Lemma 42 specifies waysobtain h0 sequence distance constraints d(pi , pi+1 ) hi (0 < < n)D(+ ):(a) h0 obtained composition [0, ] (, ): Definition 13, ensuringdifferent constants taken points(x),BP (x1 , x2 ) + BP (x1 , x2 ) + , {x1 , x2 } = {a, b}.BP (x1 , x2 ) BP (x1 , x2 ) .(b) h0 obtained composition [0, 2] (2, ) composition[0, 2] (2, 4]:one N EAR one N EAR, using Axiom 3.(c) h0 obtained composition two [0, ] one (2, ) composition two [0, ] one (2, 4]:two BP one N EAR, using Axioms 10 11.(d) h0 obtained composition (2, 4] (4, ):one F AR one F AR, using Axiom 4.(e) h0 obtained composition two [0, 2] one (4, ):two N EAR one F AR.case invalid. Definition 16, D(+ ) contains d(pa , pb ) [0, 2], d(pb , pc )[0, 2] d(pa , pc ) (4, ), pa points(a), pb points(b), pcpoints(c), individual names a, b, c. Definitions 10 13, d(pa , pb ) [0, 2]d(pb , pc ) [0, 2] cannot come N EAR(a, b) N EAR(b, c) + (by727fiDu & Alechinaproof Lemma 11, clear cannot come formulaswell), two different constants taken points(b) witnesses(a, b, [0, 2]) (b, c, [0, 2]) respectively.(f) h0 obtained composition two [0, ], one [0, 2] one (4, ):two BP , one N EAR one F AR, using Axioms 12 13.(g) h0 obtained composition four [0, ] one (4, ):four BP one F AR, using Facts 22-24.valid case, derivable using corresponding axioms facts, contradictsassumption + consistent. Therefore, D(+ ) path-consistent.alternative way prove Path-Consistency Lemma, believelonger complicated one presented paper, since may provideadditional intuitions reader, sketch Appendix B.6. Decidability Complexity LBPTsection, establish complexity LBPT satisfiability problem. complexity LNF/LNFS satisfiability problem established similar way.complexity satisfiability problems important, related complexityproblem finding inconsistencies, basis approach debuggingmatches geospatial datasets.Definition 18 (Size Formula) size LBPT formula s() defined follows:s(BP (a, b)) = 3, s(N EAR(a, b)) = 3, s(F AR(a, b)) = 3;s() = 1 + s();s( ) = 1 + s() + s(),a, b individual names, , formulas L(LBP ).set LBPT formulas conjunction formulas equi-satisfiable,combined size LBPT formulas set defined size conjunctionformulas S.Next prove Theorem 2 LBPT: satisfiability problem finite set LBPTformulas metric space NP-complete.Proof. NP-hardness LBPT satisfiability problem follows NP-hardnesssatisfiability problem propositional logic, included LBPT.prove LBPT satisfiability problem NP, show finite setLBPT formulas satisfiable, guess metric model verifymodel satisfies , time polynomial combined size formulas .Suppose finite set LBPT formulas, number individual namesn. completeness proof shows that, satisfiable, satisfiable metric model728fiQualitative Spatial Logics Buffered Geometriessize polynomially bounded number individual names . recapconstruction metric model , first construct B(+ ), corresponding setbasic quantified formulas MCS + containing , construct modelB(+ ). Definition 10, number formulas B(+ ) f = (n + 2n(n 1)).Definitions 11 13, every individual name , assign fixed set newconstants, points(a) = {p1a , . . . , pxa }, x = num(a, B(+ )). Since x f , numberconstants = nf . Lemma 34 proofs Metric Space Lemma,model , every value assigned distance function form m, N0 ,5t.guess metric model like this. Let combined size formulas .n < s. every individual name , assign {p1a , . . . , pxa }, x < 2s2 . resultsset constants , size < 2s3 . every pair constants p, q ,assign d(p, q), N0 , < 10s3 . verify (, d) metric space,Definition 1, O(s9 ).verify satisfies , need verify satisfies conjunction formulas . R(a, b), R {BP T, N EAR, F AR}, a, b individual names,verify R(a, b) satisfied, takes time polynomial |points(a)| |points(b)|,thus O(s4 ). Hence, verifying satisfies done O(s5 ).Section 3, mentioned acts scaling factor metric model.stated Lemma 43 follows proofs completeness theorem Theorem 2.proof LBPT provided below. proof LNF/LNFS similar.Lemma 43 finite set LNF/LNFS/LBPT formulas satisfiable metric modelR0 , iff satisfiable metric model = 1.Proof.[for LBPT] Suppose finite set LBPT formulas, number individualnames n combined size formulas s. Definition 18, n < s.completeness proof shows that, satisfiable, satisfiable metric model= (, d, I, ) constructed shown Section 5.1 Section 5.2. Definition 13,every individual name , assign {p1a , . . . , pxa }, x = num(a, B(+ )) < 2s2 .results set constants , size < 2s3 . every constant p ,assign d(p, p) = 0. Definition 13 Lemma 34, every pair different constantsp, q , assign d(p, q), N>0 , < 10s3 . metric model,Definition 1 proof Metric Space Lemma, x, y, z ,1. d(x, y) = 0, iff x = y;2. d(x, y) = mxy , iff d(y, x) = mxy ;3. d(x, z) = mxz , d(x, y) = mxy , d(y, z) = myz , mxz mxy + myz .satisfies , Definition 3, following holds:|= BP (a, b) iff pa (a) pb (b) : (pa , pb ) [0 , ];|= N EAR(a, b) iff pa (a) pb (b) : (pa , pb ) [0 , 2 ];729fiDu & Alechina|= F AR(a, b) iff pa (a) pb (b) : (pa , pb ) (4 , );|= iff 6|= ;|= iff |= |= ,a, b individual names, , formulas L(LBP ).setting = 1, (, d) still metric space, following holds x, y, z :1. d(x, y) = 0 iff x = y;2. d(x, y) = mxy , iff d(y, x) = mxy ;3. d(x, z) = mxz , d(x, y) = mxy , d(y, z) = myz , mxz mxy + myz .setting = 1, definitions BP , N EAR F AR change accordingly well.One easily see still satisfies replacing every 1.Similar, metric model = 1, obtain metric modelR0 multiplying every distance value . One easily see still satisfies multiplying distance values , multiplying greatest lowerbounds least upper bounds intervals truth definitions BP , N EARF AR .7. Validating Matches using Spatial Logicspatial logics LN F , LN F LBP used verify consistency sameAspartOf matches spatial objects different geospatial datasets. everyspatial object point geometry, apply LN F , otherwise, apply LN FLBP . LBP reasoning used together description logic reasoninggeospatial data matching system MatchMaps (Du, Nguyen, Alechina, Logan, Jackson, &Goodwin, 2015; Du, 2015). LBP reasoning description logic reasoning complementsense LBP reasoning verifies matches regarding spatial informationwhilst description logic reasoning verifies matches regarding classification information,unique name assumption stronger version it. following, describeLBPT used debugging matches.dedicated LBPT reasoner integrated assumption-based truth maintenancesystem (ATMS) (de Kleer, 1986) developed part MatchMaps. implementsLBPT axioms definition BEQ(a, b) BP (a, b) BP (b, a) set inferencerules. efficiency reasons, one-to-one correspondence rules axioms. speed matching avoid cycles, facts N EAR(a, b) storedone order b, symmetry axioms removed. remaining axiomsinvolving symmetric relation gives rise several rules, compensate removalsymmetry. example, axiomBPT (b, a) NEAR(b, c) BPT (c, ) FAR(d , a)also gives rise rule corresponding following implication:BPT (b, a) NEAR(c, b) BPT (c, ) FAR(d , a)730fiQualitative Spatial Logics Buffered Geometries(with N EAR(c, b) instead N EAR(b, c)). However set rules trivially equivalentset axioms.Possible matches form sameAs(a, b) partOf (a, b) (a partOf b) generated assumptions, withdrawn involved derivation contradictiondescription logic LBPT. order apply LBPT reasoning, sameAs(a, b)replaced BEQ(a, b), partOf (a, b) replaced BP (a, b). substitutionsaffect correctness matching results MatchMaps, MatchMaps adopts definitions sameAs partOf sameAs(a, b) entails BEQ(a, b) partOf (a, b)entails BP (a, b). N EAR(a, b) F AR(a, b) facts generated objects a, bdataset involved matches across two datasets (thereobject c dataset BEQ(a, c), BP (a, c) BP (c, a) holds,similarly b).LBPT reasoner derives new formulas applying inference rules previouslyderived formulas, ATMS maintains dependencies derived consequencesset assumptions (corresponding possible matches). particular, maintainsminimal sets assumptions responsible derivation (false), referred nogoodsATMS terminology. minimal sets assumptions responsible contradictionused decide matches wrong withdrawn.experiments, LBPT reasoner ATMS used validate matchesspatial objects OSM data (building layer) OSGB MasterMap data (Address LayerTopology Layer) (Du et al., 2015). study areas city centres Nottingham UKSouthampton UK. Nottingham data obtained 2012, Southamptondata 2013. numbers spatial objects case studies shown Table 1.NottinghamSouthamptonOSM spatial objects2812130OSGB spatial objects132047678Table 1: Data used Evaluationinitial matches generated matching method implemented MatchMaps.detailed matching method provided Du et al. (2016). method consiststwo main steps: matching geometries matching spatial objects. spatial objectgeospatial dataset ID, location information (coordinates geometry)meaningful labels, names types, represents object real world.geometry refers point, line polygon, used represent locationinformation geospatial datasets.geometry matching requires level tolerance, difference geometryrepresentation spatial object expected different datasets. discussingdomain experts geospatial science, decided apply level tolerancematching method spatial logic used MatchMaps. experimentsdescribed Du et al. (2015), level tolerance geometry matching set20 meters, based published estimate positional accuracy OSM data.OSM positional accuracy estimated 20 meters UK (Haklay, 2010).recent work (Du et al., 2016), analysed level tolerance affects731fiDu & Alechinaprecision recall matching results geographic area Nottingham(the data shown first row Table 1) using 12 different levels tolerancewithin range 1 80 meters. shows that, Nottingham case, 20 metersgood estimate, though optimal value.Following first step matching method, first aggregate adjacent single geometries, shops within shopping center, establish correspondencesaggregated geometries using geometry matching. second step, matchspatial objects located corresponding aggregated geometries comparingsimilarity names types spatial objects several different cases (one-to-one, manyto-one many-to-many). difficult case matchtwo aggregated geometries contain objects {a1 , . . . , } one dataset objects{b1 , . . . , bk } dataset (many-to-many matching case). cannot decideexact matches automatically using names types objects, generate matchespossibly correct objects two sets: pair ai , bj similar labels, generate sameAs(ai , bj ), partOf (ai , bj ), partOf (bj , ai ). apply reasoningLBPT description logic verify consistency matches. use descriptionlogic reasoning described Du (2015).Figure 5: Examples using LNFS LBPT validating matchesminimal set statements involved contradiction contains oneretractable assumption, domain expert needed decide correctness retractable assumptions remove wrong one(s) restore consistency. Location information visualized provided help domain experts make decisions.shown Figure 5, a1 , b1 , c1 , d1 (dotted) OSGB data a2 , b2 , c2 , d2 (solid)OSM data. left example, LNFS Axiom 6 (or LBPT Axiom 12BEQ(a, b) BP (a, b) BP (b, a)), minimal set statements deriving inconsistency consists BEQ(a1 , a2 ), BEQ(b1 , b2 ), N EAR(a1 , b1 ), F AR(a2 , b2 ). clearBEQ(b1 , b2 ) wrong, N EAR(a1 , b1 ) F AR(a2 , b2 ) facts. right example,BP (d2 , d1 ) wrong, contradicts BP (c2 , c1 ), N EAR(c2 , d2 ), F AR(c1 , d1 )LBPT Axiom 12. consequence, sameAs partOf matches correspondingBEQ(b1 , b2 ) BP (d2 , d1 ) respectively also incorrect.Table 2 shows numbers nogoods generated LBPT reasoner ATMS.mentioned earlier, nogoods justifications false: minimal sets statementscontradiction derivable. number interactions numbertimes users asked take actions use strategies resolve problems (a strategyheuristic allows users retract similar statements time, example,732fiQualitative Spatial Logics Buffered GeometriesNottinghamSouthamptonnogoods172268retracted BEQ/BPT31114retracted sameAs/partOf1325488interactions37Table 2: LBPT Verification Matchesretracting partOf (o, x) x differing object o). result LBPT reasoningremoval BEQ BPT assumptions, withdraw 1325 sameAs/partOf assumptionsNottingham case 488 sameAs/partOf assumptions Southampton case.LBPT validation matches, MatchMaps achieved high precision ( 90%)recall ( 84%) Nottingham Southampton cases.described previous work (Du, Alechina, Hart, & Jackson, 2015), MatchMapsused 12 experts University Nottingham Ordnance Survey GreatBritain match 100 buildings places Southampton. graphical user interface MatchMaps provided allowing users take different types actions removewrong matches. number actions decision time users recorded.precision recall matching results compared obtained without using user-involved verification. Experimental results showed using reasoningLBPT description logic, precision recall matches generated MatchMapsimproved average 9% 8% respectively. human effort also reduced,sense decision time required much less fully manual matchingprocess.8. Discussionspatial logics LNF, LNFS LBPT generally applicable reason spatial objects whose locations represented different levels accuracy granularity differentdatasets. Locations spatial objects represented using vector data (coordinates)raster data (images). Sometimes, spatial objects different datasets, measuringwhether locations buffered equal directly difficult impossible, example,locations represented images without knowing coordinates. cases,spatial objects may matched comparing shapes images using lexical information.matter matches spatial objects generated, LNF/LNFS/LBPTreasoning could used verify consistency matches regard relative locations(N EAR/F AR facts) spatial objects dataset, often reliableeasy capture.Another potential application logics matching non-georeferenced volunteered spatial information sketch data (Egenhofer, 1997; Kopczynski, 2006; Wallgrun,Wolter, & Richter, 2010). Instead created surveying mapping techniques, sketch data often created person memory schematizing authoritative geospatial data. sketch map cannot provide precise metric informationexact distance size spatial object, roughly shows several kinds qualitativerelations (e.g. nearness directions) spatial objects. work Wallgrunet al. (2010), qualitative spatial reasoning (based dipole relation algebra presentedMoratz, Renz, & Wolter, 2000 checking connectivity cardinal direction calculus733fiDu & Alechinapresented Ligozat, 1998) used task matching sketch map road network larger geo-referenced data set, example, OpenStreetMap. Endpointsjunctions roads extracted relative directions represented checkedspatial reasoning. spatial logic LNF applied similarly check relativedistances endpoints junctions roads. N EAR/F AR relationspoints indicate length roads. task matching sketch map polygonalobjects (e.g. buildings places), logic LNFS/LBPT applied. Suppose usersdraw sketch map buildings estimate distances buildingsN EAR F AR regarding agreed level tolerance . N EAR F AR relationsbuildings geo-referenced map calculated automatically. mappingsketch map geo-referenced map checked reasoning logicLNFS/LBPT. example, two buildings specified F AR sketchmap cannot matched two buildings N EAR geo-referenced map.main limitation new spatial logics require level toleranceusing logics, value spatial objects different sizestypes (such buildings, roads, rivers lakes). example, margin errorused cities larger buildings. Ideally, value varysize type spatial object checked. motivates developmentnew spatial logics reason sizes types spatial objects, additionrelative locations.paper, theorems proved respect metric space. However,models based metric space may realisable 2D Euclidean space,realistic geospatial data. Suppose four points pi , {1, 2, 3, 4}.point pi , d(pi , pi ) = 0. pair them, d(pi , pj ) = d(pj , pi ) = 1. clearmetric space satisfying distance constraints,2D Euclidean space. Wolter Zakharyaschev (2003, 2005) proved satisfiabilityproblem finite set S(Q0 ) formulas 2D Euclidean space R2 undecidable,whilst proper fragments may decidable. proved satisfiability problemfinite set LNF formulas 2D Euclidean space decidable PSPACE (Du,2015), whether satisfiability problem finite set LNFS/LBPT formulas2D Euclidean space decidable still unknown. also remains open whetherLNF/LNFS/LBPT calculus complete models based 2D Euclidean space. not,theoretical challenge design logics complete 2D Euclidean spaces,hence provide accurate debugging matches logics metric spaces.Finally, use description logic new spatial logics may able detectwrong matches. example, spatial objects X, one dataset X 0 , 0dataset, sameAs(X, X 0 ) correct, N EAR south X, 0N EAR north X 0 , sameAs(Y, 0 ) wrong cannot detected.deal this, could extend logics existing spatial formalisms reasoningdirectional relations (Frank, 1991, 1996; Ligozat, 1998; Balbiani et al., 1999; Goyal& Egenhofer, 2001; Skiadopoulos & Koubarakis, 2004).734fiQualitative Spatial Logics Buffered Geometries9. Conclusion Future Workpresented series new qualitative spatial logics LNF, LNFS LBPT validatingmatches spatial objects, especially crowd-sourced geospatial data. modelsbased metric space, sound complete axiomatisation provided correspondingtheorems proved logic. LNF, LNFS LBPT satisfiability problemsmetric space NP-complete. LBPT reasoner ATMS implementedused part MatchMaps. Experimental results show LBPT reasonerused verify consistency matches respect location informationdetect obvious logical errors effectively. future work, investigate whetherLNF/LNFS/LBPT calculus complete models based 2D Euclidean spacedevelop new spatial logics (e.g. reasoning directional relations object sizesaddition distances) provide accurate debugging matches.Acknowledgmentswould like thank anonymous reviewers provided excellent commentshelped us improve paper.Appendix A. ProofsLemma 9 + CS, pair individual names a, b occurring ,exactly one following cases holds + :1. case(a, b) = BP (a, b) BP (b, a);2. case(a, b) = BP (a, b) BP (b, a);3. case(a, b) = BP (a, b) BP (b, a);4. case(a, b) = BP (a, b) BP (b, a) N EAR(a, b);5. case(a, b) = N EAR(a, b) F AR(a, b);6. case(a, b) = F AR(a, b),case(a, b) denotes formula holds a, b case.Proof. pair individual names a, b occurring + , have:` (B B 1 N F ) (B B 1 N F ) (B B 1 N F ) (B B 1 N F )(B B 1 N F ) (B B 1 N F ) (B B 1 N F ) (B B 1 N F )(B B 1 N F ) (B B 1 N F ) (B B 1 N F ) (B B 1 N F )(BB 1 N F )(BB 1 N F )(BB 1 N F )(BB 1 N F )B, B 1 , N, F stand BP (a, b), BP (b, a), N EAR(a, b), F AR(a, b) respectively.Table 3,` (B B 1 ) (B B 1 ) (B B 1 ) (B B 1 N ) (N F ) F .735fiDu & AlechinaB1111111100000000B 11111000011110000N1100110011001100F1010101010101010Prime ImplicantB B 1B B 1B B 1B B 1 NFN FAxiom/Fact usedFact 15Facts 14, 15, AxiomFact 14, Axiom 3Fact 14, Axiom 3Fact 15Facts 14, 15, AxiomFact 14, Axiom 3Fact 14, Axiom 3Fact 15Facts 14, 15, AxiomFact 14, Axiom 3Fact 14, Axiom 3Fact 15Fact 15Facts 14, 15, AxiomFact 14, Axiom 33333Table 3: truth table, 1 stands true, 0 stands falseLemma 11 distance range g occurring D(+ ),g {{0}, [0, ], (, ), [0, 2], (2, ), (2, 4], (4, ), [0, )}.Proof. Suppose p, q constants d(p, q) g D(+ ). Let us look gdifferent cases:p = q: Definition 13, g = {0}.p 6= q:p points(a), q points(a), individual name a:Definition 13, g = [0, ).p points(a), q points(b), different individual names a, b:Lemma 9 Definition 10, exactly one following cases holds:C1C2C3C4C5C6{(a, b, [0, ]), (b, a, [0, ])} B(+ ){(a, b, [0, ]), (b, a, (, ))} B(+ ){(a, b, (, )), (b, a, [0, ])} B(+ ){(a, b, (, )), (b, a, (, )), (a, b, [0, 2]), (b, a, [0, 2])} B(+ ){(a, b, (2, )), (b, a, (2, )), (a, b, [0, 4]), (b, a, [0, 4])} B(+ ){(a, b, (4, )), (b, a, (4, ))} B(+ )C1:exactly one p, q witness (a, b, [0, ]) (b, a, [0, ]),Definition 13, construction process, d(p, q) [0, ] added736fiQualitative Spatial Logics Buffered GeometriesD(+ ), d(p, q) [0, +) added D(+ ). Since [0, ][0, +) = [0, ], g = [0, ].else p witness (b, a, [0, ]) q witness (a, b, [0, ]),Definition 13, construction process, d(p, q) [0, ] addedD(+ ), d(p, q) [0, ] added D(+ ) again,d(p, q) [0, +) added D(+ ). Since [0, ] [0, ] [0, +) =[0, ], g = [0, ].else, Definition 13, g = [0, +).C2:q witness (a, b, [0, ]), Definition 13, constructionprocess, d(p, q) [0, ] added D(+ ), d(p, q) [0, )added D(+ ). Since [0, ] [0, ) = [0, ], g = [0, ].else q witness (b, a, (, )), Definition 13, construction process, d(p, q) (, ) added D(+ ), d(p, q)[0, ) added D(+ ). Since (, ) [0, ) = (, ), g = (, ).else, Definition 13, g = [0, +).C3:p witness (a, b, (, )), Definition 13, constructionprocess, d(p, q) (, ) added D(+ ), d(p, q) [0, )added D(+ ). Since (, ) [0, ) = (, ), g = (, ).else p witness (b, a, [0, ]), Definition 13, constructionprocess, d(p, q) [0, ] added D(+ ), d(p, q) [0, )added D(+ ). Since [0, ] [0, ) = [0, ], g = [0, ].else, Definition 13, g = [0, +).C4:pair p, q witness (a, b, [0, 2]), Definition 13,construction process, d(p, q) [0, 2] added D(+ ),d(p, q) [0, ) added D(+ ). Since [0, 2] [0, ) = [0, 2],g = [0, 2].else exactly one p, q witness (a, b, (, )) (b, a, (, )),Definition 13, construction process, d(p, q) (, )added D(+ ), d(p, q) [0, ) added D(+ ). Since(, ) [0, ) = (, ), g = (, ).else p witness (a, b, (, )) q witness (b, a, (, )),Definition 13, construction process, d(p, q) (, ) addedD(+ ), d(p, q) (, ) added D(+ ) again,d(p, q) [0, ) added D(+ ). Since (, ) (, ) [0, ) =(, ), g = (, ).else, Definition 13, g = [0, +).C5:pair p, q witness (a, b, [0, 4]), Definition 13,construction process, d(p, q) [0, 4] added D(+ ), then,737fiDu & Alechinad(p, q) (2, ) added satisfy formulas, d(p, q)[0, ) added D(+ ). Since [0, 4] (2, ) [0, ) = (2, 4],g = (2, 4].else, Definition 13, d(p, q) (2, ) added satisfy formulas, d(p, q) [0, ) added D(+ ). Since (2, ) [0, ) =(2, ), g = (2, ).C6, Definition 13, d(p, q) (4, ) added, d(p, q) [0, )added D(+ ). Since (4, ) [0, ) = (4, ), g = (4, ).Therefore, g {{0}, [0, ], (, ), [0, 2], (2, ), (2, 4], (4, ), [0, )}.Lemma 16 (Calculation Composition) (m, n), (s, t), (m, ), (s, ), {l},{r} non-negative non-empty intervals, H1 , H2 , H non-negative intervals,following calculation rules hold:1. {l} {r} = [l r, l + r], l r;2. {l} (s, t) = (s l, + l), l;3. {l} (s, t) = [0, + l), l (s, t);4. {l} (s, t) = (l t, + l), l;5. {l} (s, +) = (s l, +), l;6. {l} (s, +) = [0, +), < l;7. (m, n) (s, t) = (s n, + n), n;8. (m, n) (s, t) = [0, + n), (m, n) (s, t) 6= ;9. (m, n) (s, +) = (s n, +), n;10. (m, n) (s, +) = [0, +), < n;11. (m, +) (s, +) = [0, +);12. H1 = ;13. H1 H2 = H2 H1 ;14. (H1 H2 ) H = (H1 H) (H2 H);15. ( k Hk ) H = k (Hk H), k N>0 ;16. (H1 H2 ) H = (H1 H) (H2 H), (H1 H2 ) 6= ;17. (H1 H2 ) H = H1 (H2 H).738fiQualitative Spatial Logics Buffered GeometriesProof.[for Rule 15] Suppose ( k Hk ) H.S Lemma 15, exist d1 k Hkd2 H {d1 } {d2 }. Since d1 k Hk , exists k N>0d1 Hk . SinceSd1 Hk k, d2 H, Definition 7, Hk H, k.Therefore k (Hk H).Supposek (Hk H). Then, thereSexists k N>0 Hk H.Since Hk k Hk , Definition 7, ( k Hk ) H.Proof.[for Rule 16] Suppose H1 H2 6= . Then, Hi 6= , {1, 2}. Since H1 , H2 , Hnon-negative intervals, intersection rules Definition 7, (H1 H2 ) H(H1 H)(H2 H) non-negative intervals. Let L = (H1 H2 )H, R = (H1 H)(H2 H).H = , Rule 12, L = R = ; otherwise, show L = R cases:H1 H2 H2 H1 : H1 H2 , Definition 7, H1 H H2 H.L = H2 H = R. H2 H1 , similarly, L = H1 H = R.H1 6 H2 H2 6 H1 : Without loss generality, let us suppose glb(H1 ) glb(H2 )lub(H1 ) lub(H2 ). Then, glb(H1 H2 ) = glb(H2 ), lub(H1 H2 ) = lub(H1 ).prove L = R, sufficient show following properties hold:1. lub(L) = lub(R);2. glb(L) = glb(R);3. lub(L) L iff lub(R) R;4. glb(L) L iff glb(R) R.Rules 1-14 intersection rules, lub(L) = lub(H1 H2 ) + lub(H) = lub(H1 ) +lub(H). lub(R) = min(lub(H1 ) + lub(H), lub(H2 ) + lub(H)) = lub(H1 ) + lub(H).Thus, lub(L) = lub(R) (Property 1 holds).lub(H1 ) H1 lub(H) H, Rules 1-14 intersection rules, lub(L) Llub(R) R; otherwise, lub(L) 6 L lub(R) 6 R. Thus, Property 3 holds.prove Property 2 Property 4 cases:H H1 = H H2 = :lub(H) glb(H1 ):glb(L) = glb(H1 H2 ) lub(H) = glb(H2 ) lub(H).glb(R) = max(glb(H1 ) lub(H), glb(H2 ) lub(H)) = glb(H2 ) lub(H).Thus, glb(L) = glb(R) (Property 2 holds).glb(H2 ) H2 lub(H) H, Rules 1-14 intersection rules,glb(L) L glb(R) R; otherwise, glb(L) 6 L glb(R) 6 R. Thus,Property 4 holds.glb(H) lub(H2 ):glb(L) = glb(H) lub(H1 H2 ) = glb(H) lub(H1 ).glb(R) = max(glb(H) lub(H1 ), glb(H) lub(H2 )) = glb(H) lub(H1 ).Similar case above, clear Property 2 Property 4 hold.H H1 6= H H2 = : then, H (H1 H2 ) = .glb(L) = glb(H1 H2 ) lub(H) = glb(H2 ) lub(H).739fiDu & Alechinaglb(R) = max(0, glb(H2 ) lub(H)) = glb(H2 ) lub(H).Similar cases above, clear Property 2 Property 4 hold.H H1 = H H2 6= : then, H (H1 H2 ) = .glb(L) = glb(H) lub(H1 H2 ) = glb(H) lub(H1 ).glb(R) = max(glb(H) lub(H1 ), 0) = glb(H) lub(H1 ).Similar cases above, clear Property 2 Property 4 hold.H H1 6= H H2 6= :since H1 , H2 , H intervals H1 H2 6= , H (H1 H2 ) 6= .glb(L) = 0.glb(R) = max(0, 0) = 0.Rules 1-14, glb(L) L glb(R) R.clear Property 2 Property 4 hold.every case, Properties 1-4 hold.Therefore, L = R.Proof.[for Rule 17] LetL = (H1 H2 ) H, R = H1 (H2 H).Definition 7, LS= ( d1 H1 ,d2 H2 {d1 } {d2 }) H.Rule 15, L = d1 H1 ,d2 H2 (({d1 } {d2 }) H).Rule 13, ({d1 } {d2 }) H = H({d1 } {d2 }).Rule 15, H ({d1 } {d2 }) = dH ({d} ({d1 } {d2 })).Rule 13, L =d1 H1 ,d2 H2 ,dH (({d1 } {d2 }) {d}).Similarly, R = d1 H1 ,d2 H2 ,dH ({d1 } ({d2 } {d})).prove L = R, sufficient show({d1 } {d2 }) {d} = {d1 } ({d2 } {d}).Let l = ({d1 } {d2 }) {d}, l = [|d1 d2 |, d1 + d2 ] {d};r = {d1 } ({d2 } {d}), r = {d1 } [|d2 d|, d2 + d].prove l = r cases:[|d1 d2 |, d1 + d2 ]: Definition 7, l = [0, d1 + d2 + d].d1 + d2 d, d2 + d1 , d1 + d2 .Thus, d1 [|d2 d|, d2 + d]. Definition 7, r = [0, d1 + d2 + d].6 [|d1 d2 |, d1 + d2 ]:> d1 + d2 : Definition 7, l = [d d1 d2 , d1 + d2 + d].d1 < d2 = |d2 d|.Definition 7, r = [d d2 d1 , d1 + d2 + d].< |d1 d2 |: Definition 7, l = [|d1 d2 | d, d1 + d2 + d].d1 d2 : < d1 d2 , is, d1 > d2 + d.Definition 7, r = [d1 d2 d, d1 + d2 + d].d1 < d2 : < d2 d1 , is, d1 < d2 d.Definition 7, r = [d2 d1 , d1 + d2 + d].case, l = r. Therefore, L = R.740fiQualitative Spatial Logics Buffered GeometriesAppendix B. Alternative Proof Path-Consistency Lemmaappendix, would like provide sketch alternative proof ideaPath-Consistency Lemma, since may appeal readers proofpresented Section 5.3. alternative proof uses Lemma 44.Lemma 44 distance constraint d(p, q) h DS(+ ) h =6 , exist+d(p, q) m1 d(p, q) m2 DS( ) h = m1 m2 , m1 m2identity definable intervals.Proof. Lemma 31, d(p, q) h left-definable right-definable. Definition 16,exists sequence distance constraints d(pi , pi+1 ) hi (p1 = p, pn = q, 0 < < n)D(+ ), m1 = h1 ... hn1 , h m1 , h m1 greatestlower bound (both value openness). Definition 15, d(p, q) m1 DS(+ ).Lemma 21 Definition 14, m1 identity definable interval. Similarly, Definition 17, exists m2 h m2 , h m2 least upper bound(both value openness), d(p, q) m2 DS(+ ), m2 identity definable interval. intersection rules, h = m1 m2 .Proof.[sketch alternative proof Path-Consistency Lemma] Suppose D(+ )path-consistent. exist d(p, q) g1 d(p, q) g2 DS(+ ), g1 6= ,g2 6= , g1 g2 = . Lemma 44, exist d(p, q) m1 d(p, q) s1 DS(+ )g1 = m1 s1 , m1 s1 identity definable intervals. Similarly,g2 = m2 s2 , m2 s2 identity definable intervals. g1 g2 =holds iff one following holds: m1 m2 = , m1 s2 = , s1 m2 = , s1 s2 = .Without loss generality, let us suppose m1 m2 = . Lemma 35, m1 m2 =iff (m1 m2 ) {0} = . Let = m1 m2 . identity definable interval.Since d(p, q) m1 d(p, q) m2 DS(+ ), m1 6= , m2 6= , Lemma 31,d(p, q) m1 d(p, q) m2 left-definable right-definable. Since d(p, q) m2 ,d(q, p) m2 . Lemma 29, d(p, p) left-definable right-definable. restproof almost proof Path-Consistency Lemma (startingLemma 23, glb(h0 ) {0, , 2, 3, 4}) presented Section 5.3. discuss differentways obtain given greatest lower bound (the role similar h0 ) checkwhether derived every valid case.Appendix C. Consequences Path-Consistency Lemmaappendix, state explicitly implications Path-Consistency Lemma.Lemma 45 Let finite consistent set formulas. distance constraint d(p, q) hDS(+ ), h 6= .Proof. Follows immediately proof Path-Consistency Lemma.Lemma 46 Let finite consistent set formulas. distance constraint d(p, p) hDS(+ ), 0 h.741fiDu & AlechinaProof. Suppose distance constraint d(p, p) h DS(+ ) 0 6 h. Definition 13Definition 15, d(p, p) {0} D(+ ). Definition 15, d(p, p) (h {0}) = .contradicts fact d(p, p) DS(+ ) (by Lemma 45). Therefore, 0 h.ReferencesAiello, M., Pratt-Hartmann, I., & van Benthem, J. (Eds.). (2007). Handbook SpatialLogics. Springer.Allen, J. F. (1983). Maintaining Knowledge Temporal Intervals. CommunicationsACM, 26 (11), 832843.Balbiani, P., Condotta, J., & del Cerro, L. F. (1999). New Tractable SubclassRectangle Algebra. Proceedings 16th International Joint ConferenceArtifical Intelligence, pp. 442447.Bennett, B. (1996). Application Qualitative Spatial Reasoning GIS. Proceedings1st International Conference GeoComputation, Vol. I, pp. 4447.Bennett, B., Cohn, A. G., & Isli, A. (1997). Logical Approach Incorporating Qualitative Spatial Reasoning GIS (Extended Abstract). Proceedings 3rdInternational Conference Spatial Information Theory, Vol. 1329 Lecture NotesComputer Science, pp. 503504. Springer.Chen, J., Cohn, A. G., Liu, D., Wang, S., OuYang, J., & Yu, Q. (2015). surveyqualitative spatial representations. Knowledge Engineering Review, 30 (1), 106136.Clementini, E., & Felice, P. D. (1996). algebraic model spatial objects indeterminate boundaries. Proceedings GISDATA specialist meeting GeographicObjects Undeterminate Boundaries, pp. 155169.Clementini, E., & Felice, P. D. (1997). Approximate Topological Relations. InternationalJournal Approximate Reasoning, 16 (2), 173204.Clementini, E., Felice, P. D., & Hernandez, D. (1997). Qualitative Representation Positional Information. Artificial Intelligence, 95 (2), 317356.Cohn, A. G., & Gotts, N. M. (1996a). Representing Spatial Vagueness: Mereological Approach. Proceedings 5th International Conference Principles KnowledgeRepresentation Reasoning, pp. 230241.Cohn, A. G., & Gotts, N. M. (1996b). Egg-Yolk Representation RegionsIndeterminate Boundaries. Proceedings GISDATA Specialist MeetingGeographical Objects Undetermined Boundaries, pp. 171187.Cohn, A. G., & Renz, J. (2008). Qualitative Spatial Representation Reasoning.Handbook Knowledge Representation, pp. 551596. Elsevier.de Kleer, J. (1986). assumption-based TMS. Artificial Intelligence, 28 (2), 127162.Du, H. (2015). Matching Disparate Geospatial Datasets Validating Matches using SpatialLogic. Ph.D. thesis, School Computer Science, University Nottingham, UK.742fiQualitative Spatial Logics Buffered GeometriesDu, H., & Alechina, N. (2014a). Logic Part Whole Buffered Geometries.Proceedings 21st European Conference Artificial Intelligence, pp. 997998.Du, H., & Alechina, N. (2014b). Logic Part Whole Buffered Geometries.Proceedings 7th European Starting AI Researcher Symposium, pp. 91100.Du, H., Alechina, N., Hart, G., & Jackson, M. (2015). Tool Matching Crowd-sourcedAuthoritative Geospatial Data. Proceedings International ConferenceMilitary Communications Information Systems, pp. 18. IEEE.Du, H., Alechina, N., Jackson, M., & Hart, G. (2016).Method Matching Crowd-sourced Authoritative Geospatial Data. Transactions GIS.http://dx.doi.org/10.1111/tgis.12210.Du, H., Alechina, N., Stock, K., & Jackson, M. (2013). Logic NEAR FAR.Proceedings 11th International Conference Spatial Information Theory, Vol.8116 Lecture Notes Computer Science, pp. 475494. Springer.Du, H., Nguyen, H., Alechina, N., Logan, B., Jackson, M., & Goodwin, J. (2015). UsingQualitative Spatial Logic Validating Crowd-Sourced Geospatial Data. Proceedings 29th AAAI Conference Artificial Intelligence (the 27th ConferenceInnovative Applications Artificial Intelligence), pp. 39483953.Egenhofer, M. J. (1997). Query processing spatial-query-by-sketch. Journal VisualLanguages Computing, 8 (4), 403424.Egenhofer, M. J., & Franzosa, R. D. (1991). Point Set Topological Spatial Relations. International Journal Geographical Information Systems, 5 (2), 161174.Egenhofer, M. J., & Herring, J. R. (1991). Categorizing Binary Topological RelationsRegions, Lines, Points Geographic Databases. Tech. rep., UniversityMaine.Fine, K. (1975). Vagueness, truth logic. Synthese, 30, 263300.Frank, A. U. (1991). Qualitative Spatial Reasoning Cardinal Directions. Proceedings7th Austrian Conference Artificial Intelligence, pp. 157167.Frank, A. U. (1996). Qualitative Spatial Reasoning: Cardinal Directions Example.International Journal Geographical Information Science, 10 (3), 269290.Goyal, R. K., & Egenhofer, M. J. (2001). Similarity Cardinal Directions. Jensen, C. S.,Schneider, M., Seeger, B., & Tsotras, V. J. (Eds.), Advances Spatial TemporalDatabases, Vol. 2121 Lecture Notes Computer Science, pp. 3655. Springer.Guesgen, H. W., & Albrecht, J. (2000). Imprecise reasoning geographic informationsystems. Fuzzy Sets Systems, 113 (1), 121131.Haklay, M. (2010). good volunteered geographical information? comparativestudy OpenStreetMap Ordnance Survey datasets. Environment PlanningB: Planning Design, 37 (4), 682703.ISO Technical Committee 211 (2003). ISO 19107:2003 Geographic information Spatialschema. Tech. rep., International Organization Standardization (TC 211).743fiDu & AlechinaJackson, M., Rahemtulla, H., & Morley, J. (2010). synergistic use authenticatedcrowd-Sourced data emergency response. Proceedings 2nd InternationalWorkshop validation GeoInformation products crisis management, pp. 9199.Kopczynski, M. (2006). Efficient spatial queries sketches. Proceedings ISPRSTechnical Commission II Symposium, pp. 1924.Kutz, O. (2007). Notes Logics Metric Spaces. Studia Logica, 85 (1), 75104.Kutz, O., Sturm, H., Suzuki, N., Wolter, F., & Zakharyaschev, M. (2002). AxiomatizingDistance Logics. Journal Applied Non-Classical Logics, 12 (3-4), 425440.Kutz, O., Wolter, F., Sturm, H., Suzuki, N., & Zakharyaschev, M. (2003). Logics metricspaces. ACM Transactions Computational Logic, 4 (2), 260294.Lehmann, F., & Cohn, A. G. (1994). EGG/YOLK Reliability Hierarchy: SemanticData Integration Using Sorts Prototypes. Proceedings 3rd InternationalConference Information Knowledge Management, pp. 272279.Li, S., Liu, W., & Wang, S. (2013). Qualitative constraint satisfaction problems: extended framework landmarks. Artificial Intelligence, 201, 3258.Li, S., Long, Z., Liu, W., Duckham, M., & Both, A. (2015). redundant topologicalconstraints. Artificial Intelligence, 225, 5176.Ligozat, G. . (1998). Reasoning Cardinal Directions. Journal Visual Languages &Computing, 9 (1), 2344.Lutz, C., & Milicic, M. (2007). Tableau Algorithm Description Logics ConcreteDomains General TBoxes. Journal Automated Reasoning, 38 (1-3), 227259.Mackworth, A. K., & Freuder, E. C. (1985). Complexity Polynomial NetworkConsistency Algorithms Constraint Satisfaction Problems. Artificial Intelligence,25 (1), 6574.Mallenby, D. (2007). Grounding Geographic Ontology Geographic Data. AAAISpring Symposium - Logical Formalizations Commonsense Reasoning, pp. 101106.Mallenby, D., & Bennett, B. (2007). Applying Spatial Reasoning Topographical DataGrounded Ontology. Proceedings 2nd International Conference GeoSpatialSemantics, No. 4853 Lecture Notes Computer Science, pp. 210227. Springer.Moratz, R., Renz, J., & Wolter, D. (2000). Qualitative Spatial Reasoning LineSegments. Proceedings 14th European Conference Artificial Intelligence,pp. 234238.Moratz, R., & Wallgrun, J. O. (2012). Spatial reasoning augmented points: Extendingcardinal directions local distances. Journal Spatial Information Science, 5 (1),130.OpenStreetMap (2012). Free Wiki World Map. http://www.openstreetmap.org.Ordnance Survey (2012). Ordnance Survey. http://www.ordnancesurvey.co.uk.Pawlak, Z., Polkowski, L., & Skowron, A. (2007). Rough Set Theory. Wiley EncyclopediaComputer Science Engineering. John Wiley & Sons, Inc.744fiQualitative Spatial Logics Buffered GeometriesRandell, D. A., Cui, Z., & Cohn, A. G. (1992). Spatial Logic based Regions Connection. Proceedings 3rd International Conference Principles KnowledgeRepresentation Reasoning, pp. 165176.Renz, J., & Nebel, B. (2007). Qualitative Spatial Reasoning Using Constraint Calculi.Aiello, M., Pratt-Hartmann, I., & van Benthem, J. (Eds.), Handbook Spatial Logics,pp. 161215. Springer.Roy, A. J., & Stell, J. G. (2001). Spatial Relations Indeterminate Regions. International Journal Approximate Reasoning, 27 (3), 205234.Schockaert, S., Cock, M. D., Cornelis, C., & Kerre, E. E. (2008a). Fuzzy region connectioncalculus: interpretation based closeness. International Journal ApproximateReasoning, 48 (1), 332347.Schockaert, S., Cock, M. D., Cornelis, C., & Kerre, E. E. (2008b). Fuzzy region connection calculus: Representing vague topological information. International JournalApproximate Reasoning, 48 (1), 314331.Schockaert, S., Cock, M. D., & Kerre, E. E. (2009). Spatial reasoning fuzzy regionconnection calculus. Artificial Intelligence, 173 (2), 258298.Sirin, E., Parsia, B., Grau, B. C., Kalyanpur, A., & Katz, Y. (2007). Pellet: practicalOWL-DL reasoner. Journal Web Semantics, 5 (2), 5153.Skiadopoulos, S., & Koubarakis, M. (2004). Composing cardinal direction relations. Artificial Intelligence, 152 (2), 143171.Smith, N. J. (2008). Vagueness Degrees Truth. Oxford University Press.Stocker, M., & Sirin, E. (2009). PelletSpatial: Hybrid RCC-8 RDF/OWL Reasoning Query Engine. Proceedings 5th International Workshop OWL:Experiences Directions.Sturm, H., Suzuki, N., Wolter, F., & Zakharyaschev, M. (2000). Semi-qualitative ReasoningDistances: Preliminary Report. Proceedings Logics ArtificialIntelligence, European Workshop, JELIA, pp. 3756.van Beek, P. (1992). Reasoning Qualitative Temporal Information. Artificial Intelligence, 58 (1-3), 297326.Wallgrun, J. O., Wolter, D., & Richter, K. (2010). Qualitative matching spatial information. Proceedings 18th ACM SIGSPATIAL International SymposiumAdvances Geographic Information Systems, pp. 300309.Wolter, F., & Zakharyaschev, M. (2003). Reasoning Distances. Proceedings18th International Joint Conference Artificial Intelligence, pp. 12751282.Wolter, F., & Zakharyaschev, M. (2005). logic metric topology. JournalSymbolic Logic, 70 (3), 795828.Zadeh, L. A. (1975). Fuzzy logic approximate reasoning. Synthese, 30 (3-4), 407428.Zimmermann, K. (1995). Measuring without Measures: Delta-Calculus. Proceedings2nd International Conference Spatial Information Theory, pp. 5967.745fiJournal Artificial Intelligence Research 56 (2016) 573611Submitted 11/15; published 8/16Study Proxies Shapley Allocations Transport CostsHaris AzizHARIS . AZIZ @ DATA 61. CSIRO . AUData61/CSIRO University New South Wales (UNSW),Sydney, AustraliaCasey CahanCCAH 002@ AUCKLANDUNI . AC . NZUniversity Auckland,Auckland, New ZealandCharles GrettonCHARLES @ HIVERY. COMHivery,Sydney, Australia;Australian National University (ANU),Canberra, Australia;Griffith University,Gold Coast, AustraliaPhilip KilbyPHILIP. KILBY @ DATA 61. CSIRO . AUData61/CSIRO Australian National University (ANU),Canberra, AustraliaNicholas MatteiNICHOLAS . MATTEI @ DATA 61. CSIRO . AUData61/CSIRO University New South Wales (UNSW),Sydney, AustraliaToby WalshTOBY. WALSH @ DATA 61. CSIRO . AUData61/CSIRO University New South Wales (UNSW),Sydney, AustraliaAbstractsurvey existing rules thumb, propose novel methods, comprehensively evaluatenumber solutions problem calculating cost serve location single-vehicletransport setting. Cost serve analysis applications strategically operationallytransportation settings. problem formally modeled traveling salesperson game (TSG),cooperative transferable utility game agents correspond locations traveling salesperson problem (TSP). total cost serve locations TSP length optimaltour. allocation divides total cost among individual locations, thus providing cost servethem. one important normative division schemes cooperative games,Shapley value gives principled fair allocation broad variety games including TSG.consider number direct sampling-based procedures calculating Shapley value,prove approximating Shapley value TSG within constant factor NP-hard.Treating Shapley value ideal baseline allocation, survey six proxiesrelatively easy compute. proxies rules thumb proceduresinternational delivery companies use(d) cost allocation methods. perform experimentalevaluation using synthetic Euclidean games well games derived real-world tours calculated scenarios involving fast-moving goods; deliveries made road networkevery day. explore several computationally tractable allocation techniques good proxiesShapley value problem instances size complexity commercially relevant.c2016AI Access Foundation. rights reserved.fiA ZIZ , C AHAN , G RETTON , K ILBY, ATTEI , & WALSH1. Introductionstudy transport scenarios deliveries consumer goods made depot locationsroad network. location customer, e.g., vending machine shop,requested goods, e.g. soda, milk, crisps. vendor plans implements deliveriesfaced two vexing problems. first difficult hurdle solving combinatorial optimizationproblem routing scheduling vehicles deliver goods cost-effective manner. Many varieties first problem exist (Golden, Raghavan, & Wasil, 2008), proposes shallrefer vehicle routing problem (VRP). begin investigation supposing VRPsolved heuristically, therefore assignment locations routes (and delivery vehicles)made.second vexing problem determining evaluate cost serve location.Specifically, vendor must decide apportion costs transportation locationequitable manner. results cost serve analysis variety important applications. Using allocation directly vendor course charge locations allocated portiontransportation costs. realistically, vendors use cost allocations (re)negotiatingcontracts customers; extracting higher per-unit delivery prices expensive customers. Supply chain managers also reference cost allocations deciding whetherinclude/continue trade particular location. Techniques informed cost allocations planning profitable transport business recently reviewed Ozener, Ergun, Savelsbergh(2013). Finally, provided market conditions favourable, sales managers instructed acquire new customers territories existing cost allocations relatively high order sharecost delivery among locations.Addressing second vexing problem, paper stems work fast-moving consumer goods company operates nationally Australia New Zealand. companyserves nearly 20,000 locations weekly using fleet 600 vehicles. industry partnerincreasing economic pressure realise productivity improvements optimisation logistical operations. key aspect endeavour understand contribution locationoverall cost distribution. study, focus individual route level singletruck, apportion costs deliveries route constituent locations.formalise setting traveling salesperson game (TSG) (Potters, Curiel, & Tijs, 1992),cost serve locations given solution underlying traveling salesperson problem(TSP). formalised game, use principled solution concepts cooperative gametheory, particular Shapley value (Shapley, 1953), order allocate costs locationsfair economically efficient manner. unique axiomatic properties Shapley valueenticing industry partner, allocation fair reasonable comprehensible sense.Charging customers fair manner provides strong justification delivery prices encouragestrust operator customer.Calculating Shapley value game notoriously hard problem (Chalkiadakis, Elkind,& Wooldridge, 2011). direct calculation Shapley value TSG requires computationoptimal solutions exponentially many distinct instances TSP. Sampling proceduresused approximating value, however offer practical solution largergames. Moreover, prove polynomial-time -approximation Shapley valueconstant 1 unless P = NP. order practically applicable, must ablecalculate cost allocation location route, 600 unique routes, may change574fiA TUDY P ROXIES HAPLEY LLOCATIONS RANSPORT C OSTSdaily weekly customers change order volumes. Hence, need methods returnvalues within minutes, hours. Allocations also used heuristically evaluate assignmentslocations trucks larger VRP. setting must able estimate cost allocationswithin seconds fractions second, minutes.circumvent computational difficulties calculating Shapley values, work exploressix proxies1 Shapley value. investigates three simple rules thumb, including simpledistance measure seen employed various industrial engagements. includeanalyze good proxies relative Shapley value, stated ideal cost allocation rule.proxies develop offer tractable alternatives Shapley value, cases appealallocation concepts cooperative game theory (Peleg & Sudholter, 2007; Curiel, 2008).Two proxies appeal well-known Held-Karp (Held & Karp, 1962) Christofides(Christofides, 1976) TSP heuristics, respectively.report detailed experimental comparison proxies using large corpus synthetic Euclidean games, problems derived real-world tours calculated fast-moving consumergoods businesses cities Auckland (New Zealand), Canberra, Sydney (Australia).experimentation uncovers novel computationally cheap proxy gives good approximationsShapley value. evaluation also considers ranking locationsleast costlyinduced Shapley proxy values. Ranking locations common request industrialpartner relevant when, example, interested identifying costly locationsserve. find two proxies, one novel proxy, provide good ranking accuracyrespect rank induced Shapley value.2. Preliminariesuse framework cooperative game theory gain deeper understanding deliverycost allocation problems (Peleg & Sudholter, 2007; Chalkiadakis et al., 2011). cooperativegame theory, game pair (N, c) N set agents size |N| = n secondterm c : 2N R characteristic function. Taking N, c(S) cost subset S. costallocation vector x = (x1 , . . . , xn ) denoting cost xi allocated agent n. restrictattention economically efficient cost allocations, allocations satisfying xi = c(N)i.e. sum allocated costs equal cost serving grand coalition.cooperative game (N, c), solution concept assigns agent N cost(N, c). may one allocation satisfying properties particular solutionconcept, thus necessarily single-valued, might give set cost allocations (Peleg &Sudholter, 2007). sometimes omit (N, c) notation solution conceptscontext clear. minimal requirement solution concept anonymity, meaningcost allocation must depend identities locations. Prominent solution conceptsinclude core, least core, Shapley value. 0, say cost allocation(multiplicative) -core (N, c) (1 + )c(S) N (Faigle & Kern, 1993).0-core referred simply core. core -core empty. -core1. use word proxy instead approximation ease discussion and, technically, many measuresstand-ins Shapley value, approximations it; i.e., give guarantee quantitatively provableapproximation.575fiA ZIZ , C AHAN , G RETTON , K ILBY, ATTEI , & WALSHnon-empty smallest possible called least core. particular referredleast core value.2work focuses single-valued solution concept called Shapley value (Shapley,1953). Writing SVi (N, c) Shapley value agent i, formally have:SVi (N, c) =|S|!(|N| |S| 1)!(c(S {i}) c(S)).|N|!SN\{i}(1)words, Shapley value divides costs based marginal cost contributions agents.traveling salesperson problem (TSP) salesperson must visit set locations N ={1, . . . , n} {0} starting ending special depot location 0. i, j N {0} 6= j, di jstrictly positive distance traversed traveling location j. Here, di j = travelingdirectly j impossible. Taking distinct i, j, k N {0}, problem symmetricdi j = ji i, j N {0}. satisfies triangle inequality di j + jkdik (Garey & Johnson, 1979).TSP Euclidean location given coordinates (two dimensional) Euclideanspace; therefore di j Euclidean distance j. Euclidean TSP symmetricsatisfies triangle inequality.tour given finite sequence locations starts ends depot 0. lengthtour sum distances consecutive locations. example, length [0, 1, 2, 0]d01 + d12 + d20 . optimal solution TSP minimum length tour visits every location.NP-hard find optimal tour, generally polynomial-time -approximationunless P = NP (Sahni & Gonzalez, 1976). -approximation given optimisationproblem algorithm runs instance x returns feasible solution F(x)cost c(F(x)) related optimal solution OPT (x) follows (Papadimitriou, 1994):c(F(x)).c(OPT (x))Informally, bound relative error approximation function. i, j di j finite,triangle inequality, symmetry hold, polynomial-time approximations exist TSPproblem (Held & Karp, 1962; Christofides, 1976).Given TSP, corresponding traveling salesperson game (TSG) pair (N, c). N setagents corresponds set locations.3 second term c : 2N R characteristicfunction. Taking N, c(S) length shortest tour locations S. costallocation vector x = (x1 , . . . , xn ) denoting cost xi allocated location N.special depot location, shall always take x0 = 0 (Potters et al., 1992). Typically, depotoperated agent distributing costs want incur costs himself. Hence,refer n number locations, corresponding TSP n + 1 points.2. 0-core transport game focus work empty. However, game convex, Shapleyvalue lies core (Tamir, 1989).3. focus restriction general games delivery games (TSGs) therefore use locationinstead agent ease exposition.576fiA TUDY P ROXIES HAPLEY LLOCATIONS RANSPORT C OSTS3. Useful Properties Shapley Valuediscussing cost allocations industrial partners, concept fairness often primary concern. fair principled cost allocation scheme would allow explain chargescustomers objective way; making whole process transparent. Shapley valuegeneral games unique assignment costs satisfies three natural axioms: (1) anonymity,cost allocated particular location depends impact visiting locationstotal cost; (2) efficiency, entire cost serving N locations allocated; (3) strongmonotonicity (Young, 1985), given two games (N, c) (N, c0 ), : ci (S) c0 (S) = (N, c)(N, c0 ); marginal contribution ci (S) player total cost coalition is:(c(S) c(S \ {i})ci (S) =c(S {i}) c(S)/ S.Due derivative axiomatic properties, Shapley value termedimportant normative payoff division scheme cooperative game theory (Winter, 2002).axioms alone make Shapley value attractive cost allocation setting.Shapley value additional attractive properties terms existence computabilityused cost allocation scheme. example, whereas 0-core empty, thereforeyield allocation (Tamir, 1989), Shapley value always exists TSG setting.logistics, often fixed cost associated serving particular location, e.g., specialparking permitting. treat variant TSG locations associated fixedcost addition transportation costs e.g. parking loading fees Shapleyvalue allocate fixed costs associated locations. Formally, given fixed costf (i) serving location i, f (i) need removed computing Shapley value,follows. Suppose c characteristic function TSG defined above, c0 satisfiesidentity c0 (S) = c(S) + f (i). Then, additivity propertity Shapley value (Shapley,1953) SVi (N, c0 ) = SVi (N, c) + f (i).delivery settings, additional observation charging locations according Shapley value may incentivise recruit new customers vicinity. Locations recruitnearby locations vendor reasonably expect lower transportation costs allocated. detail, consider vendor serving locations N = {1, . . . , n}. vendors perspective,adding new location, n + 1, existing delivery route clearly good idea revenuegenerated delivering location greater marginal cost c(N {n + 1}) c(N)new delivery. existing locations vicinity n + 1 already paying deliveries, charging additional customer marginal quantity c(N {n + 1}) c(N) typicallyunfair. case, existing customers would likely subsidizing new customers, thereforedisincentivised finding new business vendor. Shapley value mitigates this,expected provide recruitment incentives. Making discussion concrete, supposegame Euclidean N = {x} single agent distance 100 depot new agentdistance 5 x. transportation cost serving {x, y} high 210. Clearly, charging new agent c({x, y}) c({x}) = 10 x continues pay around 200 unfair.hand, vendor allocates costs according Shapley value, existing customerscosts decrease new agent joins.Another possible benefit delivery settings that, characteristic function concaveShapley value lies non-empty 0-core. Formally, concavity satisfied N,577fiA ZIZ , C AHAN , G RETTON , K ILBY, ATTEI , & WALSHN \ {i} : c(S {i}) c(S) c(T {i}) c(T ). Charging customers according Shapley/corevalues actually guarantees incentivised recruit. Specifically, N : SVi (N{n+1}, c) < SVi (N, c). words, Shapley allocation costs existing locations decreasesnew customer n + 1 added. Unfortunately general TSGs necessarily concavecharacteristic functions. However, long cost function roughly concavenecessary existing locations realise savings. practice synergies, incentivesrecruitment routes charge according Shapley value. empiricaldata, even game concave frequently observe incentives given Shapleyallocation. compared charging customers according marginal contribution costs,explicitly disincentivise recruitment. Summarizing, agent knows locationscharged according Shapley value, typically expect incentives recruit new locationsvicinity.4. Computing Shapley Valuefocus shifts computing Shapley value. Considering games general,noted direct evaluation Equation 1 requires sum exponentially many quantities.direct approach calculation Shapley value therefore practical gamereasonable size. Indeed, starting Mann Shapley (1962), authors motivate auxiliary restrictions constraints, example size importance coalitions, order describegames Shapley value calculated. recent literature proposes variety approaches directly calculate Shapley value certain games (Conitzer & Sandholm, 2006;Ieong & Shoham, 2005), however efficient calculation value TSGs remained elusive.require accurate baseline order experimentally evaluate proxies later developShapley value TSG. purpose investigate exact general sampling-basedapproximations Shapley value. treat transport setting specifically, describing novelprocedure exact evaluation Shapley value TSG following Bellmans dynamicprogramming solution underlying TSP. also discuss general Shapley valueevaluated approximately using sampling procedure. study sampling approach TSGsusing two distinct characterisations Shapley value amenable sampling-basedevaluation. perform detailed empirical study sampling-based evaluations using SyntheticTSG instances underlying TSP model Euclidean. closing give hardness proofrelating computation Shapley value TSGs, showing approximation Shapley value TSGs intractable.4.1 Dynamic Programmingfound steps performed dynamic programming (DP) algorithm underlyingTSP expose marginsi.e. terms form c(S {i}) c(S)that summed direct evaluation Equation 1. Shapley value TSG therefore computed sideeffect DP procedure computes optimal solution underlying TSP. procedureformally captured Algorithm 1: DP-TSP-Shapley. algorithm written assumes distance costs symmetric location 0 special depot location, assumptionsrelaxed general case simply computing Shapley values leveraging dynamicprogramming.578fiA TUDY P ROXIES HAPLEY LLOCATIONS RANSPORT C OSTSAlgorithm 1 DP-TSP-ShapleyInput: N = {1, . . . , n} {0} locations di j cost travel j.Output: List SV SVi N.12345678910111213141516171819202122// c(S, j) length shortest path starting 0, locations S, ending j.c []// (S) length shortest tour locations starting ending location 0.[]SV []{1, . . . , |N|}c({0, i}, i) d0,i({i}) 2 d0iSVi (|N|1)!|N|! ({i})end{2, . . . , |N|}subset N sizeSDEPOT {0}jc(SDEPOT , j) miniS,i6= j c(SDEPOT \ { j}, i) + di jend(S) min jS c(SDEPOT , j) + j0SVi |S1|!(|N||S1|1)!(T (S) (S \ {i}))|N|!endendendideas made concrete following procedure outlined Bellman (1962).equations heart TSP solution procedure recursively define cost function, c(S, j),shortest path locations starting depot 0 ending j.4c({ j}, j) = d0 j .c(S, j) =min (c(S \ { j}, k) + dk j ).kS,k6= jFollowing recursive definition, DP process iteratively tabulates c(S, j) successivelylarger coalitions S. iteration subset size |S| < |N| procedure tabulates quantitiesc(S, j) taking |S| = n. computing values c(S, 0) |S| < |N|, access characteristic function evaluation c(S) subtours locations S, follows:c(S) = c(S, 0) = min(c(S, j) + j0 ).jSTherefore, one incrementally evaluate sum Equation 1 TSG, calculating optimal subtours progressively larger coalitions (supersets) within classical DP procedure. Intuitively, compute tour using Bellmans algorithm, additionally evaluating c(S, 0)4. notations depart slightly Bellmans seminal work. Whereas take c(S, j) cost optimaltour-prefix path (i.e. starting depot 0 ending j), Bellman originally took c(S, j) cost optimaltour-suffix paths starting j, traversing locations ending depot 0.579fiA ZIZ , C AHAN , G RETTON , K ILBY, ATTEI , & WALSHencountered subset obtain quantities required calculate marginal costs locations.therefore highlighted concrete relationship classical procedure TSPShapley value corresponding TSG. dynamic programming algorithm fast18 locations, size table number subsets become unmanageable.4.2 Computational Complexityconsider, general setting TSG, complexity calculating Shapleyvalue. prove Shapley value location TSG cannot approximatedwithin constant factor polynomial-time unless P = NP.Theorem 1 polynomial-time -approximation Shapley value locationTSG constant 1 unless P = NP.Proof. Let G(N, E) graph nodes N edges E. -approximation exists usesolve NP-complete Hamiltonian cycle problem G. First, G, construct completeweighted undirected graph G0 (N, E 0 ), (i, j) weight 1 (i, j) transitive closureE, otherwise weight n!. Hamiltonian cycle G, Shapley valueN TSG posed G0 1. Suppose Hamiltonian cycle G.show exists permutation N induces large Shapley value node j follows:repeatedly add node N\ j remains Hamiltonian cycle amongst elements; node add j. marginal cost adding j least n!.Shapley value j average cost adding coalition N \ j, therefore Shapleyvalue least . Even though edge weights G0 large, represent G0 compactlyO(log(n) + n2 log()) space. -approximation G0 j therefore decides existenceHamiltonian cycle G.q4.3 Sampling-Based EvaluationUsing either dynamic programming solution, indeed state-of-the-art TSP solver Concorde (Applegate, Bixby, Chvatal, & Cook, 2007) direct calculation Shapley value,find impractical compute exact Shapley value instances TSG larger 10locations (recall include depot, hence corresponding TSPs 11 points).direct method requires exponential number characteristic function computations,requires solving NP-hard problem. Figure 4.3 shows exponential increase runtimecomputing Shapley value experimental setup (described detail Section 4.4) viadirect enumeration method. obtain accurate baseline games commercially interestingsize investigation turns sampling procedures. Indeed, Shapley valuepopulation average reasonable estimate value using sampling procedure.first use sampling approximate Shapley value games proposed studiedMann Shapley (1960). Perhaps elegant general method proposed MannShapley called Type-0 sampling. method repeatedly draws permutation locationsuniformly random. marginal cost agent calculated taking differencecost serving agents including permutation cost serving agentsproceeding i. repeatedly sampling permutations computing marginal costs includingagent way, arrive unbiased estimate Shapley value.580fiA TUDY P ROXIES HAPLEY LLOCATIONS RANSPORT C OSTSFigure 1: Runtime computing Shapley value via brute force enumeration calls Concorde instances 4 10 locations. graphs show meanstandard deviation running time 1,070 games per number locations. Comparing Figure 3 observe time increasing exponentially, practicallimit hit around 10 locations.Type-0 sampling appeared years various guises, reported differentnames literature approximating power indicesof Shapley value onecoalitional games. recent rediscovery Type-0 sampling ApproShapley algorithmCastro, Gomez, Tejada (2009); also provide asymptotic bounds sampling errorApproShapley. ApproShapley shall focus sampling work, however prior givingdetails, worth briefly reviewing classes game sampling-based evaluationsexplored. Bachrach, Markakis, Resnick, Procaccia, Rosenschein, Saberi (2010)previously examined Type-0 sampling simple gamesi.e. value coalition either0 1deriving bounds probably approximately correct. words, actual Shapleyvalue lies within given error range high probability. Continuing line work, Maleki,Tran-Thanh, Hines, Rahwan, Rogers (2013) show range variance marginalcontribution players known ahead time, focused (termed stratified) samplingtechniques may able decrease number samples required achieve given error bound.methods approximating Shapley value, specifically weighted voting games,appeared literature including based multi-linear extensions (Leech, 2003; Owen,1972) focused random sampling (Fatima, Wooldridge, & Jennings, 2008, 2007). recently,Type-0 sampling computing Shapley value applied planning setting setdelivery companies attempt pool resources order effectively service probabilisticset orders appear within territory rolling horizon (Kimms & Kozeletskyi, 2015).581fiA ZIZ , C AHAN , G RETTON , K ILBY, ATTEI , & WALSHcalculate Shapley value TSG via sampling employ Type-0 method suggestedMann Shapley (1960) Castro et al. (2009) called ApproShapley; pseudocode givenAlgorithm 2. Writing (N) set |N|! permutation orders locations N, taking (N)write subset N precedes location . alternative formulationShapley value written terms (N), noting value equals marginal costlocation construct coalitions possible ways, follows.SVi (N, c) =1(c(i {i}) c(i )).|N|! (N)(2)sampled permutation, ApproShapley evaluates characteristic function|N| computing length optimal tour set locations i-sized prefix.construction, cost allocation produced ApproShapley economically efficient. smallimportant optimisation, work cache result evaluation characteristicfunction avoid solving TSP twice. Note lines 15 17 Algorithm 2,normalize values sum 1.0, strictly necessary since given algorithm efficient.However, include code proxies algorithms surveyed return cost vectorsums 1.0.Algorithm 2 ApproShapleyAlgorithm 3 SubsetShapleyInput: N = {1, . . . , n} locations cost c(S) Input: N = {1, . . . , n} locations cost c(S)serve subset N samples.serve subset N samples.Output: List SV SVi N.Output: List SV SVi N.123456789101112131415161718SV []1 |N|SVi 0endSampleNumber 1// RAND(X) returns random element X.Perm RAND((N))0/1 |N|{Permi }SVPermi SVPermi +(c(S)c(S\{Permi }))endendTotalValue SVi1 |N|SVi SVi (c(N)/TotalValue)endreturn SV582123456789101112131415161718192021SV []1 |N|SVi 0endSampleNumber 11 |N|0/j 1 n// RAND(X) returns random element X.t6= j RAND({0, 1}) = 1{ j}endendSVi SVi + |S|!(n |S| 1)!(c(S {i}) c(S))endendTotalValue SVi1 |N|SVi SVi (c(N)/TotalValue)endreturn SVfiA TUDY P ROXIES HAPLEY LLOCATIONS RANSPORT C OSTSwork, also considered alternative sampling method, samples permutations, rather subsets locations implied formulation Equation 1 Section 2.fewer subsets permutations, seemingly advantage sampling-basedevaluation Shapley value. Using limited number subsets estimate Shapley valueexplored, shown effective measure, Papapetrou, Gionis, Mannila (2011).name method SubsetShapley provide pseudocode Algorithm 3. Like ApproShapley,method produces economically efficient allocation. ApproShapley, estimate SViupdated per drawn permutation SubsetShapley draw single randomsubset, update estimate one location. Thus, SubsetShapley, every iterationsampling loop Line 6 draw different set N \ {i} uniformly random location i, making two methods comparable based total number updates per location periteration. However, use one sample ApproShapley locations one sample perlocation SubsetShapley. i, update SVi weighted marginal contribution,formally SVi SVi + |S|!(n |S| 1)!(c(S i) c(S)). coefficient |S|!(n |S| 1)! ensuressubset sampled locations, account number permutations locationsordered location i. Note without term, algorithm convergeShapley value limit.4.4 Experimental Setup Evaluation Sampling Methodsimportant later experimental evaluation confidentsampled sufficient number times sufficient number games establish confidencesampling scheme ensure statistical significance results. must ensure that,every game, taken enough samples high probability low errorindividual Shapley value. overall evaluation must ensure sampled enoughgames representative population possible games. section describedexperimental setup derive precise statistical bounds results.proxies estimators Shapley value consider yield economically efficientallocations cost optimal tour. reason, discuss Shapley valueproxies terms induced fractional (also called normalized) allocation costoptimal tour. Formally, iSV = SVi/ jn SV j . Fractional allocations allow us compare efficientnon-efficient cost allocations equal footing, way would used operational contextstransport settings. formulation also enables us allocate cost optimal routesolve NP-hard TSP once.generated collection Euclidian games call Synthetic dataset. Syntheticdataset generate locations |N| {4, . . . , 20} 100 100 unit square. coordinateslocations generated independent identically distributed (i.i.d.) manner represented32-bit floating point values.5 n players add depot location, also chosen uniformlyrandom square. Hence, reported results total n + 1 locationsunderlying routing problem n locations must costs allocated them. timing experiments reported performed computer Intel Xeon E5405 CPU running 2.0 GHz4 GB RAM running Debian 6.0 (build 2.6.32-5-amd64 Squeeze10). Additional computingpower non-timing experiments provided Data61/NICTAs heterogenous compute cluster.5. Corpus available online https://github.com/nmattei/ShapleyTSG583fiA ZIZ , C AHAN , G RETTON , K ILBY, ATTEI , & WALSHuse statistical measures report empirical results. provide brief overviewkey concepts refer reader textbook Corder Foreman (2009) details.Denote |x| absolute value quantity x. Writing x denotes average set {x1 , . . . , xn }p let nxi denote estimate value xi set X. standard deviation ( ) x is: =21/ni=1 (xi x) . measure accuracy use root mean squared errors (RMSE), commonmetric quantify error number predictions. set k paired observations X ={x1 , . . . , xk } estimatesqX = {x1 , . . . , xk }, RMSE X X (the RMSE Xrespect X) is: RMSE = 1/k ki=1 (xi xi )2 .perform similar analysis Castro et al. (2009) determine number samplesrequired high confidence values obtained via sampling methods setting.establish error sampling procedure probability greater 1 ,use central limit theorem assumption errors normally distributed, giving:2ci | ) 1 ,= P(|SVi SV2Z N(0, 1) normal random variable. Given game, know variancelocations permutations, infeasible compute value. estimate) minimum (xi ) change cost function individualvariance given maximum (xmaxminlocation N. co-located depot minimum impact cost xmin= 0.100 100 unit square maximum possible distance two points 100 2 142.greatest impact cost location added opposite depotalong diagonal, causing increase cost equal xmax= 2 142 = 284.maximum variance random variable reached variable takes two extremevalues probability 1/2. use following inequality estimate variance:2No. Samples Z/2)2xi + xminxi + xmin(xi xmin112 (xmaxmax)2 + (xminmax)2 max.22224Applying previous equation yields formula determining error setting:)2(xmaxci SVi | ) 1 .= P(|SV4 2tolerate error bound significant effect size gamesactually use testing (as sampling time consuming). Selecting = 0.75 meanslocations Shapley value 0.75 distance units (kilometers),low fast moving consumer goods setting. derivation error gives us absolutebound pairwise error locations actual Shapley value SVi estimated Shapleyci , must derive maximum possible error points order bound errorvalue SVSV. total error game given by:2No. Samples Z/2SVi +nj (SV j + )=SVi +nj (SV j ) + n=SVi + 0.75.nj (SV j ) + 0.75nObserve nj (SV j ) exact cost grand tour points overestimate0.75n. Hence, overestimating grand tour 15 distance units (kilometers)n = 20 instances. Thus error locations SV negligible. want small,giving us high confidence converged; set = 0.005, giving us 99.5% probability584fiA TUDY P ROXIES HAPLEY LLOCATIONS RANSPORT C OSTSerror less . Substituting error equation (Z0.05/2 = Z0.0025 = 2.81)get:(2 142)2283, 0524 0.752order draw conclusions calculated RMSE values must statistical confidence mean (RMSE) set games. RMSE average normalizedvalues, take values [0, 1]. Assuming errors problem normallydistributed bound variance using techniques described above, arrivingvariance 2 = 1/4. use standard techniques statistics engineering (Natrella, Croarkin, & Guthrie, 2012) determine number games need use order95% confidence interval absolute error measurement RMSE,aggregate measure error locations games, within 0.03 (or roughly 3%):21/422No. Games = 1, 070 Z=0.05= (1.96)1, 067.2(0.03)2No. Samples = 300, 000 2.812Intuitively, means 95% sure re-ran entire experiment new valuesmean error particular proxy would fall within 3%. Hence say mean error value,measured set 1,070 games, accurate.compare performance ApproShapley SubsetShapley using Synthetic dataset.use Concorde (Applegate et al., 2007) evaluate characteristic function TSGsolving underlying TSP.optimal tour lengths calculated Concorde cached speed running time. Therefore,never re-evaluate TSP set points. TSPs less four locations evaluated brute force. game Synthetic dataset calculated exact Shapley valueevery location, could compare sampled allocation exact counterparts.find ApproShapley method sampling permutations provides faster convergence,seen Figure 2. 1000 iterations ApproShapley achieves RMSE 0.01, significantly smaller standard deviation SubsetShapley. Moving 100, 000 samples bottomrow Figure 2, see mean RMSE ApproShapley still significantly lowerSubsetShapley.Figures 3 4 depict mean running time number calls Concorde two algorithms, respectively. see ApproShapley runs faster SubsetShapley instancestested. difference runtime grows number locations increases. ApproShapleysfaster running time likely due need randomly generating one permutation insteadn sets. Figure 4 provides insight behavior two algorithms. SubsetShapley fillscache much quicker ApproShapley, explains later flattening runtime curveApproShapley seen Figure 3. methods eventually evaluate 217 possible points, saturating cache. However, early filling cache SubsetShapley translate fasteroverall runtime. practice, ApproShapley achieves lower error, earlier, continues convergetowards error 0 faster SubsetShapley.585fiA ZIZ , C AHAN , G RETTON , K ILBY, ATTEI , & WALSHFigure 2: Comparison accuracy ApproShapley (left) SubsetShapley(right) 10,000iterations (top) 100,000 iterations (bottom) TSGs 10 locations. graphsshow RMSE standard deviation 1,070 instances sampledactual Shapley values. ApproShapley converges fewer samples stable samples SubsetShapley.586fiA TUDY P ROXIES HAPLEY LLOCATIONS RANSPORT C OSTSFigure 3: Comparison runtime performance ApproShapley (left) SubsetShapley (right)TSGs 4 17 locations (not including depot). graphs show meanstandard deviation 1,070 instances running time respective algorithm.ApproShapley needs generate one permutation, compared SampleShapleys n sets, generally runs quickly.Figure 4: Comparison number calls Concorde function sample number madeApproShapley (left) SubsetShapley (right) TSGs 4 17 locations (notincluding depot). graphs show mean standard deviation 1,070 instances number calls Concorde. SubsetShapley fills cache much quickerApproShapley, explains later flattening runtime curve ApproShapley seen Figure 3. However, earlier cache filling lead decreaserunning time.587fiA ZIZ , C AHAN , G RETTON , K ILBY, ATTEI , & WALSH5. Proxies Shapley Valueuse ApproShapley requires solve NP-hard problem time evaluatecharacteristic function. feasible small TSG instances less dozen locations,however create unacceptable computational burden larger, realistically sized games.describe variety proxies Shapley value require much less computationpractice. seen proxies use real-world allocate costs, henceinclusion analysis. define discuss proxies terms induced fractionalallocation, iSV = SVi/ jn SV j , discussed Section 4.4. overview worst case practicalrunning times algorithms presented Table 1.MethodProxyWorst CaseRuntime10 Loc.ApproShapley (Concorde)Exponential30sec.4,500 sec.> 90,000 sec.Shortcut Distance ( HORT )Exponential5 sec.10 sec.15 sec.Exponential20 sec.25 sec.30 sec.Depot Distance ( EPOT )O(n)1 sec.1 sec.1 sec.Moat Packing ( OAT )Exponential5 sec.5 sec.5 sec.Christofides ( C HRIS )O(n3 )30 sec.2,500 sec.40,000 sec.Exponential5 sec.5 sec.5 sec.Re-routed MarginBlend( R EROUTE )( B LEND )Practical Running Time20 Loc.30 Loc.Table 1: Summary proxies Shapley value surveyed paper.5.1 Depot Distance ( EPOT )distance depot i.e. di0 location straightforward proxy.allocate cost location proportional di0 . fraction allocation locationiD EPOT =di0.ni=1 di0proxy, location twice distant depot another pay twice cost.evaluate proxy time linear number locations. practice, computingvalue instantaneous.5.2 Shortcut Distance ( HORT )Another proxy straightforward calculate used commercial routingsoftware shortcut distance. change cost realized skipping locationtraversing given optimal tour. Without loss generality, suppose optimal tour visits locations according sequence [0, 1, 2, . . . ]. Formally, HORTi = di1,i + di,i+1 di1,i+1 ,locations 0 n + 1 depot, di j cost travel location j. fractional588fiA TUDY P ROXIES HAPLEY LLOCATIONS RANSPORT C OSTSallocation given shortcut distanceHORT =HORTi.jN HORT jevaluate proxy solving one TSP instance one operation per location. practicecompute metric less 30 seconds.5.3 Re-routed Margin ( R EROUTE )location N, R EROUTEi defined c(N) c(N\i)). allocation playercomputed two calls optimal TSP solver. fractional allocationiR EROUTE =(c(N) c(N\i)).j=N (c(N) c(N\ j))evaluate proxy solving n + 1 TSPs: one grand tour one leavinglocation. practice compute metric nearly instantaneously.5.4 Christofides Approximation ( C HRIS )sophisticated proxy obtained use heuristic performing characteristic functionevaluations ApproShapley, rather solving individual induced TSPs optimally.proxy use sampling estimate Shapley value use approximation algorithmestimate underlying TSP cost. approximate underlying TSP characteristic function,Christofides (1976) heuristic, O(|N|3 ) time procedure used. obtain fractional quantityiC HRIS , divide allocation location total allocated costs. Assuming symmetricdistance matrix satisfying triangle inequality, Christofides heuristic guaranteed yieldtour within 3/2 length optimal tour.briefly describe heuristic works. TSP instance represented complete undirected graph G = (V, E), one vertex V location, edge E everydistinct pair vertices. i, j V edge (i, j) E weight di j . tour obtainedfollows: (1) compute minimum spanning tree (MST) G, (2) find minimum weight perfectmatching complete graph vertices odd degree MST (typically performedusing Hungarian Algorithm), (3) calculate Eulerian tour graph obtained combiningMST Step 1 matched edges Step 2 (this guaranteed yield Eulerianmultigraph, i.e., graph every vertex even degree), (4) obtain final tour TSPremoving duplicate locations Eulerian tour.best case, call Christofides heuristic return solution exactlysolution TSP. Hence, method requires many calls per number locations derivedSection 4.4. Figure 5 shows runtime ApproShapley replace calls Concordecalls program solves TSP using Christofides heuristic. Comparing resultsFigure 3, see small numbers locations ( 10) runtimes ConcordeChristofides heuristic almost same. However, number locations grows,Christofides heuristic shows significant speed improvement. commercially interesting sizes,20 locations 300,000 samples, computing C HRIS takes order 2500 seconds (about 30minutes). practically computable ( 12 hours) problems 30+ locations, shownTable 1.589fiA ZIZ , C AHAN , G RETTON , K ILBY, ATTEI , & WALSHFigure 5: Runtime performance ApproShapley calls Christofides heuristic TSGs4 17 locations (not including depot). graphs show mean standarddeviation 1,070 instances running time respective algorithm. Comparing Figure 3 see Christofides heuristic decreases runtimedecrease grows larger number locations increases.5.5 Nested Moat-Packing ( OAT )Another way allocate costs TSGs based dividing locations regions using conceptcalled moat (Cook, Cunningham, Pulleylank, & Schrijver, 1998). Intuitively, given EuclidianTSP set locations N depot location 0, moat closed strip constant widthseparates set locations N compliment S. assume without loss generality0 always S. order deliver location S, one would need traverse moatorder reach points set S, cross moat return point S. Hence,reasonable cost allocation charge locations twice cost traversing moatsurrounding S. locations delivery truck, would reason crossterritory moat surrounding S. following use techniques described Faigle,Fekete, Hochstattler, Kern (1998) additionally refined Ozener et al. (2013)extensions setting.Formally, given set locations N {0}, let N compliment S, letset bipartitions locations {S, S} assumption 0 S. Let wS,Swidth (distance) moat set S. refer vector moats ~w widthindividual moat wS . Locations cannot occur moat, moatstrip unoccupied area map. Additionally, one needs consider circular moatsgives minimal straight-line distance location inside moat less equalmoat width. order well-formed cost allocation want moats large590fiA TUDY P ROXIES HAPLEY LLOCATIONS RANSPORT C OSTSpossible, i.e., maximum width. Hence, formulate linear program find maximummoat packing:MPV = maxwS,SS,SMwS,S 0s.t.{S, S}(3)wS,S di j i, j N {0}.iS, jSThough Equation 3 exponentially many constraints, solved time polynomialnumber locations using dual techniques, returning at-most polynomial number moatswidth 0. shall use notation ~w refer small set moats non-zero width (Faigleet al., 1998). vector moat widths ~w given solution Equation 3 may manymoats overlap, leading ambiguities widths allocate locations. Thus,arbitrary solution Equation 3 yield cost allocation. reason must refinemaximum moat packing maximum nested moat packing. nested moat packing twodistinct intersecting subsets cannot encapsulated non-empty moat unless onecoalitions subset other. Formally, packing nested S0 , S00 s.t.wS0 > 0 wS00 > 0, S0 S00 6 0/ either S0 S00 S00 S0 . optimal solution ~wEquation 3 yielding objective value MPV set partitions corresponding nestedpacking MPV (Cornuejols, Naddef, & Pulleyblank, 1985; Faigle et al., 1998).nested moat packing clear set moats must crossed reach locationlocation, point derive cost allocation location. Figure 6concrete example nested moat packing 6 locations. Figure 6 6 locationsmoat (light colors). Additionally, moats locations 5 6 surroundedouter darker moat set {5, 6}.Given non-nested vector moats ~w follow post-processing procedure describedOzener et al. (2013). nesting criteria defined violated must three distinctnon-empty sets locations S, S0 S00 , wSS0 > 0 wS0 S00 > 0. Given ~w updatevalues follows: let min{wSS0 , wS0 S00 }, make following assignment updates moatwidths: wS wS + , wS00 wS00 + , wSS0 wSS0 , wS0 S00 wS0 S00 . iterativeprocedure terminates yielding nested packing, taking exponential time worst case. However,experiments found nesting takes fraction second. leaves usallocation:1wS1wSiM OAT ==.MPV wS >0,iS |S| wS >0 wS wS >0,iS |S|two key observations allocation derived (nested) moat packing.First, 2 MPV , i.e., sum crossing moats twice, exactly value Held-Karprelaxation underlying TSP TSP symmetric satisfies triangle inequality (Held& Karp, 1962). Thus, 2 MPV lower bound optimal tour underlying TSP3 MPV upper bound.6 Secondly, observe allocation derived nested moatpacking xi cost location satisfies xi c(N) N : xi (1 + )c(S).constraints exactly multiplicative core defined preliminaries6. tightness bounds Held-Karp relaxation, i.e., integrality gap, longstanding open questioncombinatorial optimisation (Cook et al., 1998).591fiA ZIZ , C AHAN , G RETTON , K ILBY, ATTEI , & WALSHFigure 6: optimal nested moat-packing (colored regions) optimal tour (line) TSP6 locations. locations indicated labels {1,2, . . ., 6}, occur centerlight moats. moat around locations 1 4 (light colored regions) associatedone location. moat around set locations {5, 6} (dark colored region)nested. larger moat encloses two smaller, independent moats (light colored regions) around locations 5 6, respectively. 7 moats total, optimaltour must cross moat twice.-core. Although allocation achieved using nested moat packing distribute 3 MPVeconomically efficient, core allocation approximate cost. Faigle et al. (1998) presentproof nested moat packing provides 21 -core allocation respect actual costfunction location distributing 3 MPV ; conjecture 13 .5.6 Blended Proxy ( B LEND )interesting question whether blending set proxies practically computablecould provide good estimate actual Shapley value. Framing prediction machinelearning problem, want learn model predict output SV given input set consistingeasily computable proxies, { EPOT , HORT , R EROUTE , OAT }. analysis sectioncarried using SciKitLearn (Pedregosa et al., 2011), machine learning library Python.First, need decide sort model best setting. proxiesattempting estimate value, correlated. Consequently, one place startuse principal component analysis (PCA) (Bishop, 2006) understand much variancecaptured low dimensional model given input set. use SciKitLearn run PCAdecomposition set Synthetic data. SciKitLearn uses linear algebra packageSciPy perform singular value decomposition (SVD) data matrix; keepingsignificant singular vectors project data lower dimensional spaces. decompositionshows 98% variance explained one component (vector), depicted Figure7. Hence, simple linear blend subset proxies provide good predictive power.592fiA TUDY P ROXIES HAPLEY LLOCATIONS RANSPORT C OSTSFigure 7: explained variance ratio SV given PCA decomposition input set{ EPOT , HORT , R EROUTE , OAT } (left). Using 1 dimensional model input setexplain 98% observed variance SV . Prediction error graph 10 foldcross-validation Synthetic dataset linear model blending EPOT OAT(right). point represents prediction error location Synthetic datasetdotted line (y = x) would ideal predictor. correlation actualpredicted values 10 fold cross-validation R2 = 0.8825 = 0.0025.selected simple linear model, must decide elements inputset proxies, { EPOT , HORT , R EROUTE , OAT }, use. want use minimal setfeatures, using many features cause overfitting (Bishop, 2006). selection, useSciKit (Pedregosa et al., 2011) perform k-best feature selection takes inputset turn computes cross correlation element others, convertedusing ANOVA score significance (p) value feature. compare scoresfind individual elements input set significant. findscores elements input set statistically significant, hence useable. Lookingnormalized scores themselves, { EPOT = 1.0, HORT = 0.0260, R EROUTE = 0.4946, OAT =0.6305}, see EPOT OAT two highest scoring indicators. choose limitlinear model two highest scoring elements EPOT OAT significantly higherscoring others adding elements may cause overfitting.model input variables train on, need learn modelperform cross-validation. tests take full Synthetic dataset perform 10-foldcross-validation (Bishop, 2006). perform k-fold cross-validation, take dataset breakk equal sized folds F = { f1 , . . . , fk }. hold one pieces trainingset turn (i.e., train F \ { fi }) use test set (i.e., predict fi ). select 10 foldsused cross-validation use stratified k-fold sampling, ensures every k foldstatistical distribution whole training set (Pedregosa et al., 2011). Since usinglinear model, use coefficient determination, R2 , fitness measure. 10 foldcross-validation, get mean R2 = 0.8825 standard deviation = 0.0025. graph593fiA ZIZ , C AHAN , G RETTON , K ILBY, ATTEI , & WALSHpredicted SV function actual value shown Figure 7. low showsmodel robust high value R2 indicates model good predictor. trainingentire Synthetic dataset get final model:B LEND = 0.579 EPOT + 0.318 OAT + 0.009.6. Analysis Nave Proxiesrefer three proxies EPOT , HORT R EROUTE nave. Contrastingly, callC HRIS , OAT B LEND sophisticated proxies. formulation nave proxies EPOTHORT make amenable direct analysis worst case performance. considersettings nave proxies EPOT HORT perform quite badly.order illustrate this, consider TSG depot one corner square dimension one location 3 corners. Locations nearest depot indexed 13, third location indexed 2.Location 1Location 2DepotLocation 3nave proxies yield following allocations:SVEPOT HORT1, 3 0.299a 0.293a 0.333a2 0.402a 0.415a 0.333aObserve EPOT performs well case (maximum 11% error) HORT (minimum 16% error).identify pathological cases HORT EPOT proxies performpoorly. first result demonstrates EPOT HORT may under-estimate true Shapleyvalue badly.Theorem 2 exists n location TSP problem which, location i, ratio iDEPOT/iSVgoes 0 n goes . instance, ratio iSHORT/iSV goes 0 n goes (n)locations.Proof. Suppose first n 1 locations distance depot, whilst nth locationlocated distance opposite direction depot.Locations 1, . . . , n 1Depot594Location nfiA TUDY P ROXIES HAPLEY LLOCATIONS RANSPORT C OSTSNote normalization constant SV , jn SV j = 4a. nSV = 2a/4a = 1/2 since costadding nth location coalition 2a. Leaving, < n,iSV =2a/(n1)4a=1.2(n 1)hand, normalization constant EPOT , ni=1 di0 = na since locationsequidistant depot. Giving, n, iD EPOT = 1n .Thus < n,1/niD EPOT2n 1==SV1/2(n1)ngoes 2 n . hand,1/nnD EPOT1==1/2nSV2ngoes 0 n .Note shortcut proxy, HORT performs poorly example. < n, HORT = 0since locations co-located, leaving nS HORT = 1. < n iSV = 1/2(n 1). Thus,< n,HORT0=0=SV1/2(n1)nS HORT1==2SV1n/2qsecond result demonstrates HORT also grossly over-estimate true Shapleyvalue.Theorem 3 exists n location TSG ratio iSV/iDEPOT goes 0 n goes(n) locations.Proof. Suppose first n 1 locations distance depot, whilst nth locationlocated distance (n + 1)a depot opposite direction.Locations 1, . . . , n 1a(n + 1)DepotLocation nNote normalization constant SV , jn SV j = 2a + 2a(n + 1) = 2a(n + 2). Shapley2avalue SVi < n n1, thusiSV =2a/n12a(n + 2)=5951.(n 1)(n + 2)fiA ZIZ , C AHAN , G RETTON , K ILBY, ATTEI , & WALSHfractional Shapley allocation location nnSV =2a(n + 1) 1= .2a(n + 2) 2normalization constant EPOT ni=1 di0 = a(n 1) + a(n + 1) = 2an. location nassignment distance based proxynD EPOT =a(n + 1) n + 1=.2an2n< n,iD EPOT =1= .2an 2nThus, location nnSVnD EPOTgoes 1 n goes .< niSViD EPOT=1/2=n+1/2n1/(n1)(n+2)12n==2n2n + 12n(n 1)(n + 2)goes 0 n goes .HORT < n, HORT = 0 leaving nS HORT = 1. Thus, nSV/nSHORT = 1/2< n, iSV/iSHORT undefined.qthird result demonstrates HORT may under-estimate Shapley value badly evensimple examples may embedded larger problems.Theorem 4 exists 2 location TSG instancelocations.HORT/ SV= 0 one twoProof. Suppose first location located distance depot second locationlocated distance farther road.DepotLocation 1Location 2first location 1S HORT = 0, since removing effect distance secondlocation. leaves 2S HORT = 1. Shapley value first locationSV =2a 0+ = a.22gives SV = a/4 thusHORT0== 0.a/4SVqfourth final result demonstrates HORT may over-estimate Shapley value badly.596fiA TUDY P ROXIES HAPLEY LLOCATIONS RANSPORT C OSTSTheorem 5 exists four location TSG SV/ SHORT = 0 two four cities.Proof. Consider four location TSG locations 1 2 depotcities 3 4 distance depot other.Location 1kaDepotLocation 4kaLocation 2Location 3note k a, hide terms O(). marginal cost savedskipping location , means locations allocation accordingHORT , namely {1, . . . , 4}, HORT = 1/4.Note normalization constant SV , jn SV j = 2ka + O(). compute Shapleyvalues locations 1 2 observe that, given permutation, location adds multiple, thus symmetry, {3, 4},iSV =O()2ka + O()compute Shapley value locations 3 4 observe that, matter permutation appear, first contributes 2ka contributes . Consequently,symmetry, locations {3, 4},iSV=2ka+O()21= .2ka + O() 2Thus, locations {1, 2},SVHORT=O()2ka+O()1/4=4O().2ka + O()term goes 0 k goes .qgames illustrated illustrate poor performance proxies relativelysimple extremely degenerate. real-world settings would expect locations delivery setup along straight line symmetrical box. Hence motivated compareproxies using data accurately reflects domain hope deploy proxies.7. Empirical Studyimplemented six proxies discussed, along version ApproShapley usesConcorde (Applegate et al., 2007) evaluate characteristic function TSG. codedata used project available public Git repository at: https://github.com/nmattei/ShapleyTSG. Rather calculating SV direct enumeration baseline compare proxies,597fiA ZIZ , C AHAN , G RETTON , K ILBY, ATTEI , & WALSHestimate value using ApproShapley Concorde. described Section 4.4, methodachieves extremely good approximation true Shapley value computable reasonable time testing games 20 locations.use corpus 1070 Synthetic games, constructed described Section 4.4, gamesn {4, . . . , 20} locations. also test corpus 119 Real-World games generatedlarge VRPs cities Auckland (New Zealand), Canberra, Sydney (Australia).Heuristic solutions VRPs calculated using Indigo solver (Kilby & Verden, 2011).Indigo flexible heuristic solver implementing Adaptive Large Neighbourhood Search,basic structure described detail Ropke Pisinger (2006).7 give indication scale difficulty VRPs, Auckland model comprises 1, 166 locationsserved using fleet 25 vehicles 7 day period. heuristic solutionscollect collect tours length 10 20 create TSGs testing. real-world distancematrices asymmetric (in cases asymmetry negligible), induce symmetric problemsresolving greater di j ji , i.e., setting di j = ji = max{di j , ji }. total, obtain71 Real-World games size 10 (14 Auckland, 5 Canberra, 52 Sydney) 48 gamessize 20 (10 Auckland, 7 Canberra, 31 Sydney).8evaluate well proxies perform approximating SV use several different test statistics, briefly review (Corder & Foreman, 2009). Already discussed Section 4.4root-mean-squared-error (RMSE) game. Additionally, may want know maximum absolute point-wise error (MAPE), i.e., maximum absolute error valuespoint-wise estimate:MAPE = arg maxx,x[X,X] |x x|.use measure particular game compare average maximum absolute pointwise error set games (MAPE). value lets us know, average,overcharging particular customer (unlike RMSE tells us aggregate error). Noteusing arguments Section 4.4 guarantees acuracey MAPERMSE, i.e., 3%.One question often repeated consultation logistics companiesexpensive customer? order know focus efforts contract negotiations sales functions, companies desire understanding rank ordering cost servicing locations.use Kendalls , written KT first introduced Kendall (1938), compare ranking, i.e.,least expensive expensive, locations induced Shapley allocation proxies.value measures amount disagreement two rankings. customary reportnormalized value (correlation coefficient) 1 -1, = 1 means two listsperfectly correlated (equal) = 1 means two lists perfectly anti-correlated (theyequal one list reversed). intuitive interpretation two lists %orderings two lists same.detail, let X two partial orders set items. b X say Xconcordant (a, b). = b X say tie, otherwise (a, b)7. Indigo strong vehicle routing solution platform, recently computing 5 new best solutions 1, 000 customer problems VRPTW benchmark library. solutions computed using Indigo certifiedDr. Geir Hasle, Chief Research Scientist SINTEF maintainer VRPTW benchmark library,best currently known September 24th 2013. http://www.sintef.no/Projectweb/TOP/VRPTW/Homberger-benchmark/1000-customers.8. Due commercial agreements industrial partners cannot release Real-World games.598fiA TUDY P ROXIES HAPLEY LLOCATIONS RANSPORT C OSTSdiscordant. number concordant pairs, N discordant pairs, ties exclusively X,U ties exclusively , normalised KT distance X is:=pMN.(M + N + ) (M + N +U)analysis makes use significance, p-value, computed . p-value computed using two-tailed t-test null hypothesis correlationorderings ( = 0). Taking significance threshold customary 0.05, reject nullhypothesis p 0.05. p 0.05 fail reject null hypothesis. Hence, p-value0.05 statistically significant result. Intuitively means unlikely two random lists would show high degree correlation say two lists significantlycorrelated.7.1 Synthetic DataFigure 8 gives overview data, showing RMSE proxy SVgame sizes Synthetic data. Tables 2 5 give in-depth look performanceproxies variety interesting measures including RMSE, MAPE, , number statisticallysignificant s, number games correctly identified top elements. general, HORTR EROUTE proxies far worst, particularly terms approximating Shapley value,also terms ranking induced corresponding allocations. computationallyexpensive proxy R EROUTE always dominates HORT ; though proxies dominatedEPOT , OAT , B LEND , C HRIS tests save one.10 LocationsRMSE15 LocationsRMSE20 LocationsRMSEDataRMSEShortcut DistanceRe-routed MarginDepot Distance0.38500.25650.09940.09680.06990.02750.33420.21680.09500.07640.05330.02350.29920.19150.08930.06060.04240.01950.37270.24930.09780.05640.04880.0059Moat-PackingChristofidesBlend0.16170.04950.07100.05020.02160.01910.14370.05260.07420.0370.01770.01680.13020.05230.07330.02930.01420.01540.15640.05200.07450.01970.00460.0075Table 2: Average root mean squared error (RMSE) standard deviation ( ) Synthetic datagames 10, 15, 20 locations. Lower better.599fiA ZIZ , C AHAN , G RETTON , K ILBY, ATTEI , & WALSHFigure 8: Performance proxies according to: (left) RMSE 1070 games generatednumber locations, (right) 1070 games generated numberlocations. error bands correspond plus minus one standard deviation.vertical axis plot inverted ease comparison, i.e., correlatedlists towards bottom graph (1.0).10 LocationsMAPE15 LocationsMAPE20 LocationsMAPEDataMAPEShortcut DistanceRe-routed MarginDepot Distance0.28020.18660.06370.10880.08050.02380.22780.14600.05890.08430.05960.02260.19440.12030.05230.07000.04610.01930.26050.17410.06200.11550.0890.0261Moat-PackingChristofidesBlend0.10780.03110.04410.04520.01470.01540.08880.03180.04430.0350.01370.01550.07220.02990.04170.02520.01080.01450.10030.03290.04720.05080.01580.0224Table 3: Average maximum absolute error (MAPE) standard deviation ( ) Syntheticdata games 10, 15, 20 locations. Lower better.Looking first error estimation SV , top Figure 8 depicts RMSE1070 games proxy increase number locations per game. overalltrend positive proxy becoming accurate (lower RMSE) increase numberlocations. figure C HRIS strictly dominates proxies RMSE performance.However, also see B LEND EPOT competitive C HRIS terms RMSE. B LEND winner category practical purposes offers performance extremelyclose C HRIS , tighter distribution error EPOT , fraction computation600fiA TUDY P ROXIES HAPLEY LLOCATIONS RANSPORT C OSTStime. Table 2 shows detailed breakdown Figure 8 particular numbers locations.table allows us see RMSE B LEND never goes 0.1 0.01 largerinstances commercially interesting.Table 3 sheds light types error proxies likely make.see B LEND achieves better MAPE proxies save C HRIS , averageovercharging worst case 4.7% = 2.2% true Shapley value, mere 1%C HRIS . see EPOT fairly accurate proxy SV , overcharging6% = 2.6% largest instanes tested. However, EPOT strictly dominatedB LEND error measures considered computed similar time. Given B LENDcomputable fraction time C HRIS , competitive overall error, scalesbeyond commercially interesting problem sizes. clear winner measure.10 Locations15 Locations20 LocationsDataShortcut DistanceRe-routed MarginDepot Distance0.00980.47320.58150.24030.19470.17910.00310.41600.54000.19310.15050.1524-0.00760.39080.50180.16040.13970.1454-0.00270.48280.56590.01060.08920.0385Moat-PackingChristofidesBlend0.40980.71860.68340.22350.16630.15670.35260.67910.62060.17870.14300.13850.33920.64630.57060.16100.12860.13690.41900.70480.66160.08290.03740.0593Table 4: Average Kendalls tau rank correlation coefficient () Standard Deviation ( )Synthetic data games 10, 15, 20 locations. Higher better; +1 means twolists perfectly correlated 1 means two lists perfectly anti-correlated.10 Locations% Sig.% Top15 Locations% Sig.% Top20 Locations% Sig.% TopData% Sig.% TopShortcut DistanceRe-routed MarginDepot Distance1.49%18.13%16.44%19.81%77.75%68.59%3.73%49.90%70.18%9.62%73.08%51.40%5.32%60.74%84.85%6.91%67.00%46.26%4.72%53.15%69.67%10.45%69.85%52.04%Moat-PackingChristofidesBlend12.42%29.90%29.90%71.68%82.52%80.84%39.53%90.18%89.15%62.42%78.31%68.41%45.60%96.54%94.11%56.82%74.39%59.71%41.76%85.28%83.12%61.13%76.08%65.38%Table 5: (Left columns) percentage games 1070 statistically significant(p < 0.05) ranking induced SV ranking induced P ROXY . (Rightcolumns) percentage 1070 games expensive element accordingraking induced SV matched expensive element ranking inducedP ROXY . Higher better statistics.601fiA ZIZ , C AHAN , G RETTON , K ILBY, ATTEI , & WALSHTurning proxies performance ranking, bottom Figure 8 depicts averageKendalls tau rank correlation coefficient () standard deviation ( ) 1070 gamesproxy increase number locations per game. overall trend graph, opposedtop one, slightly negative. increase number locations, ranking computedproxy increasingly uncorrelated ranking induced SV . positive side,C HRIS , B LEND , EPOT , return lists 0.6 correlation, i.e., 60% pairselements ordered correctly. Table 4 gives closer look results particular numberslocations. see C HRIS B LEND maintain near 0.6 across range problems,hence correctly order pairs elements. two proxies strictly dominateproxies; even EPOT performs poorly measured .Table 5 gives us nuanced look ranking results. see larger gamespercentages statistically significant increase proxies locations, evendecrease. lists recovering significant portionpairwise relations compared total number pairwise relations. see termsstatistically significant s, C HRIS B LEND strictly dominate proxies almost 15%data considered. answer common customer question whose costingmost, results bit mixed. Comparing highest ranked elements seeperformance B LEND drops performance R EROUTE , surpassingly strongproxy measure. However, want top element according SV top 3elements according P ROXY , B LEND achieves feat 90% time. Thoughproxies see increase performance relaxed measure well, C HRIS B LEND90% numbers locations studied. Hence, see B LEND provides strongperformance practically computable running time across range game sizes.7.2 Real-World DataMeasuring performance proxies Real-World corpus Auckland, Canberra,Sydney, find overall quality allocation slightly lower compared measurementsSynthetic corpus. identified significant performance differences cities,therefore report data aggregate statistics Real-World corpus 71 games10 locations 48 games 20 locations. Tables 6 9 provide in-depth perspectiveperformance proxies Real-World dataset measures Syntheticdataset. see performance HORT R EROUTE strictly dominated accordingstatistical measures proxies; except R EROUTE ability select costlylocation surprisingly high accuracy.602fiA TUDY P ROXIES HAPLEY LLOCATIONS RANSPORT C OSTS10 LocationsRMSE20 LocationsRMSEGamesRMSEShortcut DistanceRe-routed MarginDepot Distance0.45110.43800.13800.14770.14720.0650.32450.30300.08380.09290.09340.03130.38780.37050.11090.06330.06750.0271Moat-PackingChristofidesBlend0.26920.15190.14420.14860.08230.06870.20880.11040.08260.09370.05290.02920.23900.13110.11340.03020.02070.0308Table 6: Average root mean squared error (RMSE) standard deviation ( ) Real-worlddata 71 games 10 locations 48 games 20 locations. Lower better.10 LocationsMAPE20 LocationsMAPEGamesMAPEShortcut DistanceRe-routed MarginDepot Distance0.36780.35680.08350.17160.16950.03880.24620.22860.03900.10500.10120.01180.31870.30510.06550.15990.15880.0378Moat-PackingChristofidesBlend0.21960.11780.09610.14630.07800.05230.14980.07390.04120.09590.05360.01340.19140.10010.07400.13280.07250.0493Table 7: Average maximum absolute error (MAPE) standard deviation ( ) Real-worlddata 71 games 10 locations 48 games 20 locations. Lower better.Turning first error estimation SV , see results reported Table 6strictly higher every measure every proxy compared results Table 2, corresponding test Synthetic dataset. see error decreases increase numberlocations proxies. difference Real-World Synthetic renderproxies unuseable. Observe RMSE B LEND increases 0.01 SyntheticReal-world RMSE EPOT increases 0.003. solid indicatorusefulness B LEND , none Real-World instances included training setmodel. interesting twist, computationally expensive C HRIS fares worseReal-World data, doubling error (an increase 0.05) respect Synthetic dataset.increase RMSE followed looking MAPE. Comparing Table 7 Synthetic dataset partner Table 3, see B LEND EPOT actually lower MAPElower Real-World datasets 20 locations. Observe comparing performanceaccording MAPE see B LEND EPOT separated 1% performance,strictly outperform metrics, even C HRIS . see B LEND EPOTreasonable proxies SV Real-world corpus, achieving overall RMSE less 0.09absolute worst error per location less 0.05 (5% true cost).603fiA ZIZ , C AHAN , G RETTON , K ILBY, ATTEI , & WALSHpossible explanation extremely good performance EPOT requires closer lookdistribution costs Real-World dataset. locations along route heuristicallyallocated larger VRP, allocations tend cluster around uniform allocation around0.050.08 per location, many locations equidistant depot. Consequently, RealWorld data seems drawn different distribution Synthetic data (i.e., locationsselected uniformly random). Thus, performance B LEND ideal, uniformlyrandom case strongly degenerate real-world case strong argument portabilityB LEND across domains.10 Locations20 LocationsGamesShortcut DistanceRe-routed MarginDepot Distance0.07560.36510.10550.30150.27930.34160.00610.47340.33220.21480.16020.19320.04080.41930.21880.03480.05420.1134Moat-PackingChristofidesBlend0.34800.24570.14980.25040.34080.32870.38140.54030.40930.17210.15890.18090.36470.39300.27960.01670.14730.1297Table 8: Average Kendalls rank correlation coefficient () Standard Deviation ( )Real-World data 71 games 10 locations 48 games 20 locations.Higher better; +1 means two lists perfectly correlated 1 means twolists perfectly anti-correlated.10 Locations% Sig.% Top20 Locations% Sig.% TopData% Sig.% TopShortcut DistanceRe-routed MarginDepot Distance4.22%28.16%12.67%12.67%57.74%53.52%8.33%72.91%43.75%18.75%70.83%60.41%5.88%46.21%25.21%15.12%63.02%56.30%Moat-PackingChristofidesBlend25.35%22.53%14.08%60.56%57.74%56.33%62.50%89.58%68.75%56.25%62.50%64.58%40.33%49.57%36.13%58.82%59.66%59.66%Table 9: (Left columns) percentage Real-World data 71 games 10 locations48 games 20 locations statistically significant (p < 0.05)ranking induced SV ranking induced P ROXY . (Right columns)percentage Real-World data 71 games 10 locations 48 games20 locations expensive element according raking induced SVmatched expensive element ranking induced P ROXY . Higher betterstatistics.604fiA TUDY P ROXIES HAPLEY LLOCATIONS RANSPORT C OSTSTables 8 9 give indication proxies perform terms ranking. closer lookTable 8 reveals difference B LEND EPOT . Again, comparing resultsSynthetic dataset shows proxies perform strictly worse Real-world data, exceptHORT manages go negative list correlation Synthetic dataset (barely)positive correlation Real-world dataset. Judging performance seeproxies still recovering 50% pairwise comparisons Real-World data. Again,also see good performance R EROUTE ranking metric. Additionally, games20 locations, R EROUTE , C HRIS , OAT , B LEND , C HRIS best.review Table 9 reveals measure lower overall, majorityranking correlations still statistically significant 20 location games. first glance proxies appears hold looking top element. Every proxy sees decreased performance 60% accuracy selecting top element, R EROUTE best performance,followed B LEND C HRIS . Relaxing notion top (most costly) elementSynthetic data, i.e., top element according SV top 3 elements accordingP ROXY , B LEND outperforms proxies (including R EROUTE ) 20 location games93% accuracy, comes within 3% outperforming R EROUTE entire corpus Real-worlddata 79% accuracy.summary see proxies perform worse terms RMSE RealWorld dataset Synthetic dataset. testing see B LEND , EPOT ,C HRIS perform proxies majority measures. comparing proxiesvariety decision criteria including practical running time, overall numerical error,ranking performance, B LEND emerges clear winner overall consistent performerSynthetic Real-world data.8. Related Worktheory cooperative games rich history various solution concepts allocatingcosts quantities proposed (Peleg & Sudholter, 2007; Young, 1994). additionShapley value, solution concepts include core, nucleolus bargaining set.these, Shapley value considered important allocation scheme cooperativegame theory (Winter, 2002).Application Shapley value spans well beyond transportation setting. example,Shapley value applied allocating cost network infrastructure (Koster, 2009; Marinakis, Migdalas, & Pardalos, 2008), promoting collaboration agents (Zlotkin & Rosenschein, 1994) prescribing allocation incentivises agents collaborate completiontasks, incentive compatible way share departmental costs corporations (Young,1985). Considering applications networks broadly, use Shapley value follows general framework, agents correspond nodes (or edges) graph (Curiel, 2008; Koster,2009; Marinakis et al., 2008; Tijs & Driessen, 1986; Aziz & de Keijzer, 2014). definitioncharacteristic function depends application domain, proposed evaluations basedon: (i) size maximum matching, (ii) network flow, (iii) weight minimum spanningtree, (iv) weight Hamiltonian cycle (Curiel, 2008; Deng & Fang, 2008). Allocationconcepts solely devised employed allocating costs. example, Shapley valueused measure importance agents social networks (Moretti & Patrone, 2008),605fiA ZIZ , C AHAN , G RETTON , K ILBY, ATTEI , & WALSHmeasure centrality nodes networks (Michalak, Aadithya, Szczepanski, Ravindran,& Jennings, 2013).Another solution concept used gauge importance agents Banzhafvalue (Banzhaf III, 1964). Banzhaf value defined simple voting games i.e. cooperativegames value coalition either zero one Banzhaf value agentsuitably extended general cooperative games. However, even within context simplevoting games, Banzhaf value suitable measuring influence agent lesssuitable allocate power agents (Felsenthal & Machover, 1998). Since focusallocate costs, focus Shapley value.solution concepts theory transferable utility (TU) cooperative games (Peleg& Sudholter, 2007; Chalkiadakis et al., 2011) used allocations costs, Shapleyvalue rarely received serious attention transportation science literature. associatedcomputational cost prohibitively high general case, consequently strong notionsfairness often taken secondary consideration. Though ApproShapley FPRAS (fullypolynomial-time randomized approximation scheme) computing Shapley value gameconvex (Liben-Nowell, Sharp, Wexler, & Woods, 2012), apply domain considered work. website Spliddit uses Shapley value split cab fares 6people (Goldman & Procaccia, 2014).prominent TU game solution concepts nucleolus core. TSGs introducedPotters (1992), addition describing game, authors describe variety gameknown routing game.9 latter auxiliary constraint forces locations visited,coalition, order traversed specific tour. Assuming tour correspondsoptimal underlying TSP, game non-empty core. Derks Kuipers (1997)presented quadratic-time procedure computing core allocation routing game.also characterize suboptimal tours specify routing games non-empty cores.noted known tractable procedures compute tour guarantees corenon-empty routing game. Conditions non-emptiness core TSGsdeveloped Tamir (1989). already noted Faigle et al. (1998) developed procedurecalculate multiplicative -core allocation Euclidean TSGs. Yengin (2012) develop notionfixed route game appointments admits tractable procedure computing Shapleyvalues. model suitable typical scenarios involve delivery goods locationsdepot. TU concepts TSGs routing games developed practical gas deliveryapplication Engevall et al. (1998).Turning attention vehicle routing problems transportation settings generally,Gothe-Lundgren, Jornsten, Varbrand (1996) develop column generation procedure calculate nucleolus homogeneous vehicle routing problem, i.e., vehicles equivalent.develop procedure determine core vehicle routing gameempty. Engevall et al. (2004) extend work practical setting distributing gas using heterogeneous fleet vehicles. recently Ozener et al. (2013) examine numbersolution conceptsincluding allocations derived according nested moat-packing Faigleet al. (1998), highly specialized approximation Shapley allocationin deriving costallocations real-world inventory routing problems. show TU game allocations, espe9. Note journal publication Potters et al. (1992) extends technical report introducing game early1987.606fiA TUDY P ROXIES HAPLEY LLOCATIONS RANSPORT C OSTScially core/duality-based allocations, significant advantages existing cost allocationsindustrial client using.9. Conclusions Future Workstudied problem fairly apportioning costs transportation scenarios, specifically TSGs.Shapley value appealing division concept task axiomatic fairness propertiesones appreciated commercial partners. Since Shapley value cannot evaluatedreasonable time, considered number proxies Shapley value. examined proxyperformance terms approximation quality respect Shapley valueinduced ranking locations Shapley value, key question operational business concerns.stand-out proxies respect measures tested Synthetic Real-world dataC HRIS B LEND , mixture EPOT OAT . However, taking computation timeaccount ability scale problems commercial interest: around 30 locations per route600 total routes delivery day, B LEND remains feasible.key extensions work general setting vehicle routing games (VRPs).Shapley value would useful quantify importance location synergies uniquemulti-vehicle model. transport companies interact desire understand impacttime windows (both duration position allowable service times), effect delivery frequency allocated costs. Thus, highly motivated rich variety problems availablefuture work. Additionally, future research consider weighted Shapley values situations coalitions (and therefore margins) likely occur others. Formalapproximation ratios, complement strong empirical evidence obtained, importantsubject future research. also remains need formal studies employ proxyallocations inform solutions hard optimisation problems transportation domains. Finally,scaling larger transportation scenarios may require abstracting locations meaningful way.approximation approach may fruitful proposed Soufiani, Charles, Chickering, Parkes (2014), agents partitioned groups assigned weights withingroups novel effective way.AcknowledgmentsData61/CSIRO (formerly known NICTA) funded Australian GovernmentDepartment Communications Australian Research Council ICT CentreExcellence Program. Casey Cahan supported Summer Research Scholarship Australian National University. Toby Walsh also receives support Asian Office AerospaceResearch Development (AOARD 124056) German Federal Ministry EducationResearch Alexander von Humboldt Foundation.would like thank Stefano Moretti Patrice Perny LIP6; Hossein Azari SoufianiHarvard University; David Rey Vinayak Dixit rCiti Project UniversityNew South Wales School Civil Environmental Engineering, Tommaso Urli Data61ANU; reviewers attendees 5th Workshop Cooperative Games MultiAgentSystems (CoopMAS-2014) helpful feedback comments early version work.607fiA ZIZ , C AHAN , G RETTON , K ILBY, ATTEI , & WALSHReferencesApplegate, D. L., Bixby, R. E., Chvatal, V., & Cook, W. J. (2007). traveling salesman problem:computational study. Princeton University Press.Aziz, H., & de Keijzer, B. (2014). Shapley meets Shapley. Proceeding 31st InternationalSymposium Theoretical Aspects Computer Science (STACS 2014), pp. 99111.Bachrach, Y., Markakis, E., Resnick, E., Procaccia, A. D., Rosenschein, J. S., & Saberi, A. (2010).Approximating power indices: theoretical empirical analysis. Autonomous AgentsMulti-Agent Systems, 20(2), 105122.Banzhaf III, J. F. (1964). Weighted voting doesnt work: mathematical analysis. Rutgers LawReview, 19, 317343.Bellman, R. (1962). Dynamic programming treatment travelling salesman problem. JournalACM (JACM), 9(1), 6163.Bishop, C. M. (2006). Pattern recognition machine learning. Springer.Castro, J., Gomez, D., & Tejada, J. (2009). Polynomial calculation shapley value basedsampling. Comput. Oper. Res., 36(5), 17261730.Chalkiadakis, G., Elkind, E., & Wooldridge, M. (2011). Computational aspects cooperative gametheory. Synthesis Lectures Artificial Intelligence Machine Learning, 5(6), 1168.Christofides, N. (1976). Worst-case analysis new heuristic travelling salesman problem..Tech. rep., DTIC Document.Conitzer, V., & Sandholm, T. (2006). Complexity constructing solutions core basedsynergies among coalitions. Artificial Intelligence, 170(6), 607619.Cook, W. J., Cunningham, W. H., Pulleylank, W. R., & Schrijver, A. (1998). Combinatorial Optimization. John Wiley & Sons, Inc.Corder, G. W., & Foreman, D. I. (2009). Nonparametric statistics non-statisticians: step-bystep approach. Wiley.Cornuejols, G., Naddef, D., & Pulleyblank, W. (1985). traveling salesman problem graphs3-edge cutsets. Journal ACM, 32(2), 383410.Curiel, I. (2008). Cooperative combinatorial games. Chinchuluun, A., Pardalos, P., Migdalas, A.,& Pitsoulis, L. (Eds.), Pareto Optimality, Game Theory Equilibria, Vol. 17 SpringerOptimization Applications, pp. 131157. Springer New York.Deng, X., & Fang, Z. (2008). Algorithmic cooperative game theory. Chinchuluun, A., Pardalos,P. M., Migdalas, A., & Pitsoulis, L. (Eds.), Pareto Optimality, Game Theory Equilibria,Vol. 17 Springer Optimization Applications. Springer-Verlag.Derks, J., & Kuipers, J. (1997). core routing games. International Journal GameTheory, 26(2), 193205.Engevall, S., Gothe-Lundgren, M., & Varbrand, P. (1998). traveling salesman game: application cost allocation gas oil company. Annals Operations Research, 82,203218.608fiA TUDY P ROXIES HAPLEY LLOCATIONS RANSPORT C OSTSEngevall, S., Gothe-Lundgren, M., & Varbrand, P. (2004). heterogeneous vehicle-routing game.Transportation Science, 38(1), 7185.Faigle, U., & Kern, W. (1993). approximately balanced combinatorial cooperative games.ZOR Methods Models Operations Research, 38(2), 141152.Faigle, U., Fekete, S., Hochstattler, W., & Kern, W. (1998). approximately fair cost allocationeuclidean tsp games. Operations-Research-Spektrum, 20(1), 2937.Fatima, S. S., Wooldridge, M., & Jennings, N. R. (2007). randomized method Shapleyvalue voting game. Proceedings 6th International Conference AutonomousAgents Multiagent Systems (AAMAS 07), pp. 157165, New York, New York, USA.Fatima, S. S., Wooldridge, M., & Jennings, N. R. (2008). linear approximation methodShapley value. Artificial Intelligence, 172(14), 16731699.Felsenthal, D. S., & Machover, M. (1998). Measurement Voting Power: Theory Practice,Problems Paradoxes. Edward Elgar Cheltenham.Garey, M. R., & Johnson, D. S. (1979). Computers Intractability: Guide TheoryNP-Completeness. New York: W.H. Freeman.Golden, B. L., Raghavan, S., & Wasil, E. A. (2008). Vehicle Routing Problem: Latest AdvancesNew Challenges: latest advances new challenges, Vol. 43. Springer.Goldman, J., & Procaccia, A. D. (2014). Spliddit: Unleashing fair division algorithms. JournalACM, 13(2), 4146.Gothe-Lundgren, M., Jornsten, K., & Varbrand, P. (1996). nucleolus basic vehiclerouting game. Mathematical Programming, 72(1), 83100.Held, M., & Karp, R. M. (1962). dynamic programming approach sequencing problems.Journal Society Industrial & Applied Mathematics, 10(1), 196210.Ieong, S., & Shoham, Y. (2005). Marginal contribution nets: compact representation schemecoalitional games. Proceedings 6th ACM conference Electronic Commerce (EC06), pp. 193202.Kendall, M. G. (1938). new measure rank correlation. Biometrika, 30(1/2), 8193.Kilby, P., & Verden, A. (2011). Flexible routing combing constraint programming, large neighbourhood search, feature-based insertion. 2nd Workshop Artificial IntelligenceLogistics. Barcelona, Spain.Kimms, A., & Kozeletskyi, I. (2015). Shapley value-based cost allocation cooperative traveling salesman problem rolling horizon planning. EURO Journal TransportationLogistics, 122.Koster, M. (2009). Cost Sharing. Springer-Verlag New York.Leech, D. (2003). Computing power indices large voting games. Management Science, 49(6),831837.Liben-Nowell, D., Sharp, A., Wexler, T., & Woods, K. (2012). Computing shapley value supermodular coalitional games. 18th International Conference Computing Combinatorics (COCOON 2012), pp. 568579.609fiA ZIZ , C AHAN , G RETTON , K ILBY, ATTEI , & WALSHMaleki, S., Tran-Thanh, L., Hines, G., Rahwan, T., & Rogers, A. (2013). Bounding estimation error sampling-based shapley value approximation with/without stratifying. CoRR,abs/1306.4265.Mann, I., & Shapley, L. S. (1960). Values large games IV: Evaluating electoral collegemonte carlo. Technical report, RAND Corporation, Santa Monica, CA, USA.Mann, I., & Shapley, L. S. (1962). Values large games IV: Evaluating electoral collegeexactly. Technical report, RAND Corporation, Santa Monica, CA, USA.Marinakis, Y., Migdalas, A., & Pardalos, P. M. (2008). Cost allocation combinatorial optimizationgames. Chinchuluun, A., Pardalos, P., Migdalas, A., & Pitsoulis, L. (Eds.), Pareto Optimality, Game Theory Equilibria, Vol. 17 Springer Optimization Applications,pp. 217244. Springer New York.Michalak, T. P., Aadithya, K. V., Szczepanski, P. L., Ravindran, B., & Jennings, N. R. (2013).Efficient computation Shapley value game-theoretic network centrality. JournalArtificial Intelligence Research, 46, 607650.Moretti, S., & Patrone, F. (2008). Transversality Shapley value. TOP, 16(1), 141.Natrella, M., Croarkin, C., & Guthrie, W. (2012). NIST/SEMATECH e-Handbook StatisticalMethods. U.S. Department Commerce. URL: http://www.itl.nist.gov/div898/handbook/.Owen, G. (1972). Multilinear extensions games. Management Science, 18(5-part-2), 6479.Ozener, O. O., Ergun, O., & Savelsbergh, M. (2013). Allocating cost service customersinventory routing. Oper. Res., 61(1), 112125.Papadimitriou, C. (1994). Computational Complexity. Addison-Wesley Publishing Company, Inc.Papapetrou, P., Gionis, A., & Mannila, H. (2011). Shapley value approach influence attribution. Proceedings 2011 European Conference Machine Learning PrinciplesKnowledge Discovery Databases (ECML PKDD 2011), pp. 549564. Springer.Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M.,Perrot, M., & Duchesnay, E. (2011). Scikit-learn: Machine learning Python. JournalMachine Learning Research, 12, 28252830.Peleg, B., & Sudholter, P. (2007). Introduction Theory Cooperative Games. Springer.Potters, J. A., Curiel, I. J., & Tijs, S. H. (1992). Traveling salesman games. Mathematical Programming, 53(1-3), 199211.Ropke, S., & Pisinger, D. (2006). adaptive large neighborhood search heuristic pickupdelivery problem time windows. Transportation Science, 40(4), 455472.Sahni, S., & Gonzalez, T. (1976). P-complete approximation problems. Journal ACM, 23(3),555565.Shapley, L. S. (1953). value n-person games. Kuhn, H., & Tucker, W. W. (Eds.), Contributions Theory Games, Vol. 2 Annals Mathematical Studies. Princeton UniversityPress.610fiA TUDY P ROXIES HAPLEY LLOCATIONS RANSPORT C OSTSSoufiani, H. A., Charles, D. J., Chickering, D. M., & Parkes, D. C. (2014). Approximating shapley value via multi-issue decomposition. Proceedings 13th International ConferenceAutonomous Agents Multiagent Systems (AAMAS 14), pp. 12091216.Tamir, A. (1989). core traveling salesman cost allocation game. Operations ResearchLetters, 8(1), 3134.Tijs, S. H., & Driessen, T. S. H. (1986). Game theory cost allocation problems. ManagementScience, 32(8), 10151028.Winter, E. (2002). Shapley value. Handbook Game Theory Economic Applications,chap. 53, pp. 20252054. Elsevier.Yengin, D. (2012). Appointment games fixed-route traveling salesman problems Shapleyvalue. International Journal Game Theory, 41(2), 271299.Young, H. P. (1985). Producer incentives cost allocation. Econometrica, 53(4), 757765.Young, H. P. (1994). Cost allocation. Handbook Game Theory Economic Applications,Vol. 2, pp. 11931235. Elsevier B.V.Young, H. P. (1985). Monotonic solutions cooperative games. International Journal GameTheory, 14(2), 6572.Zlotkin, G., & Rosenschein, J. S. (1994). Coalition, cryptography, stability: Mechanismscoalition formation task oriented domains. Proceedings 12th National ConferenceArtificial Intelligence (AAAI 1994), pp. 432437.611fiJournal Artificial Intelligence Research 56 (2016) 657-691Submitted 01/16; published 08/16Engineering NoteIBaCoP Planning System: Instance-Based Configured PortfoliosIsabel CenamorTomas de la RosaFernando FernandezICENAMOR @ INF. UC 3 . ESTROSA @ INF. UC 3 . ESFFERNAND @ INF. UC 3 . ESDepartamento de Informatica, Universidad Carlos III de MadridAvda. de la Universidad, 30. Leganes (Madrid). SpainAbstractSequential planning portfolios powerful exploiting complementary strengthdifferent automated planners. main challenge portfolio planner definebase planners run, assign running time planner decide ordercarried optimize planning metric. Portfolio configurations usually derivedempirically training benchmarks remain fixed evaluation phase. work,create per-instance configurable portfolio, able adapt every planning task.proposed system pre-selects group candidate planners using Pareto-dominance filteringapproach decides planners include time assigned according predictivemodels. models estimate whether base planner able solve given problem and,so, long take. define different portfolio strategies combine knowledgegenerated models. experimental evaluation shows resulting portfolios provideimprovement compared non-informed strategies. One proposed portfolioswinner Sequential Satisficing Track International Planning Competition held2014.1. IntroductionPlanning process chooses organizes actions anticipating outcomesaim achieving pre-stated objectives. Artificial Intelligence, Automated Planning (AP)computational study deliberation process (Ghallab, Nau, & Traverso, 2004). Automatedplanners systems that, regardless application domain, able receive declarativerepresentation environment, initial state set goals input. output synthesized plan achieve goals initial situation. context, InternationalPlanning Competition (IPC) excellent initiative foster studying development automated planning systems. IPC created 1998 set common framework comparingautomated planners.Different planning systems awards previous IPCs. However, one main invariantscompetition single planner always best planner (or least equal)every domain every problem. means that, although planner which, followingquality metrics competition, considered best, always find problemsdifferent domains planners outperform overall winner. Therefore, assumeAP community generated set single planners better others specificsituations. reason, discarding priori solvers seems meaningless.c2016AI Access Foundation. rights reserved.fiC ENAMOR , DE LA ROSA & F ERN ANDEZfact, idea reusing set individual base systems generate accurate solutions obtained separately new Artificial Intelligence. instance, MachineLearning, meta-classifiers use different base classifier increase coverage representationbias resulting classifier (Dietterich, 2000). problem solving, portfolios search algorithmsalso demonstrated outperform results single search strategy (Xu, Hutter,Hoos, & Leyton-Brown, 2008; Xu, Hoos, & Leyton-Brown, 2010; Malitsky, Sabharwal, Samulowitz, & Sellmann, 2013). example, SAT competition 2013 included special trackportfolios. automated planning community, planner portfolios also subject greatdeal interest. IPCs 2006 2014, portfolio approaches close winningtracks took part.However, although use portfolios become usual community, stillagreement planning portfolio (Vallati, Chrpa, & Kitchin, 2015). work,assume portfolio planners set base planners selection strategy. selectionstrategy generates specific portfolio configuration, whose goal maximize performance metrics. Therefore, configuration define three main elements: (1) sub-setplanners run, (2) long run planner? (3) order. manytechniques configure planning portfolio (Vallati, 2012), depending accurateare, chances selecting best planner given situation increase. Note that,definition, planner different configuration parameters modify behavior, parameterization considered different base planner, base planners considered blackboxes.number planners state art huge, first filtering select minimumnumber ensures best performance achieved, evaluated planning domain (or evenproblem domain). Obviously, good results current domains ensure goodresults new domains but, shown, good estimator. sense, Pareto efficiencybased approach (Censor, 1977) reduce number planners consider eligibleplanning portfolio presented. However, show mechanism, firstaforementioned questions answered partially since number candidate plannersmight still large.best solution portfolio configuration problem oracle predicts,given domain problem, planner obtain best performance longtake. Given oracle, work propose use predictive models, automatically generated Machine Learning Data Mining techniques. models summarize results candidate planners past: whether able solve planningproblems, well time required generate good solution (Cenamor, de la Rosa,& Fernandez, 2012, 2013). Given knowledge past, inductive hypothesis gives also usestimation behave future planning domains different problems,order planners implemented given accuracy predictions.Therefore, predictive models, able configure portfolio planning problem, like previous works use portfolios search (Gomes & Selman, 2001).renewed idea automated planning since recent works focused static (Helmert, 2006)domain-specific portfolios (Gerevini, Saetti, & Vallati, 2009, 2014), configurationportfolio fixed domains chosen one respectively.IBAC P (Instance-based Configured Portfolio) family planning portfolios builtcompeting IPC-2014. article first present IBAC P general framework658fiT IBAC P P LANNING YSTEMultimate goal building per-instance configurable portfolios. technique reproducedwhenever new automated planners new planning benchmarks arise. Then, describebuild different version IBAC P following defined processes. One versionswinner Sequential Satisficing Track IPC-2014. also include results empiricalstudy confirms good performance IBAC P planners compared different baseplanners different portfolio configuration strategies. Then, summarize related work,finally, last section sets conclusions future lines research.2. System Architecturesection, present general idea building planning portfolio configuredparticular planning task using predictive models. process seen generaltechnique given inputs (planners benchmarks) might change future due progressplanning community, new portfolio configurations generated usenew inputs.2.1 Portfolio Constructionconsider construction instance-based planning portfolio comprises three mainparts. (1) Planner filtering, making pre-selection good candidate planners setknown available planners. proposed pre-selection technique based multi-criteria approximation. previously unexplored technique selecting set planners providesenough diversity planner portfolio. (2) Performance modeling, providing predictorsplanners behavior function planning task features. research, include setwell-known features (Cenamor et al., 2012), built preprocessing stepFAST OWNWARD (Helmert, 2006). also take advantage output informationtranslation process (Fawcett, Vallati, Hutter, Hoffmann, Hoos, & Leyton-Brown, 2014)heuristic values computed first step search process FAST OWNWARD. addition,use several totally new features characteristics relaxed plan initial stateproposed. Finally, (3) strategy selection: establish procedure combines performancepredictions output portfolio configuration. propose novel strategy selectionexploit effectiveness predictive models. Next, explain detailsconstruction steps.2.1.1 P LANNER F ILTERINGplanner filtering process consists pre-selection good candidate base plannerslarger amount available planners. Even though sufficient evidenceoverall best planner across variety benchmarks, verified empiricallydominance planners others. Therefore make sense include, baseplanners, always worse terms performance metrics. want filtering processselect diverse, small, subset planners elements among divideavailable execution time.work, propose multi-criteria pre-selection mechanism focuses two IPC metrics (quality time) alternative extended ones planner filtering. example,FDSS (Helmert, Roger, Seipp, Karpas, Hoffmann, Keyder, Nissim, Richter, & Westphal, 2011)659fiC ENAMOR , DE LA ROSA & F ERN ANDEZuses selection planners maximizes coverage; MIP LAN (Nunez, Borrajo, & LinaresLopez, 2015) uses portfolio configuration obtains best achievable performance termsscore.filtering propose run candidate planners representative set benchmarksevaluate terms time quality. consider metrics proposeapproach based Pareto-efficiency (Censor, 1977) allows us determine dominanceplanners multi-criteria fashion. particular, select planner candidateportfolio best planner least one domain terms IPC-2011 multi-criteria QTscore (Linares Lopez, Celorrio, & Olaya, 2015). Briefly, single problem, metric computestuple hQ, planner, Q quality planners best solutiontime used find solution. Then, given planner, p, dominance relations prest planners computed.tuple hQ, Pareto-dominates tuple hQ , Q Q < . Plannerp gets NN points, N number tuples p Pareto-dominates another planner,N number different tuples planner p appears. Finally, QT-Pareto scoredomain sum points achieved problems domain. idea selectionmechanism follows: planner shows good dominance property given domain,included portfolio good candidate solving problemsdomain even planning tasks similar characteristics. Therefore, simple strategyfilter first pool planners given procedure selects plannersmaximum QT-Pareto score least one domain. refer procedure QT-Pareto ScoreFiltering.2.1.2 P ERFORMANCE ODELINGGiven planning task, want predict selected base planners perform orderdecide whether include make good assignment time orderingconfiguring portfolio. Thus, modeling planner behavior function planningtask features becomes key process building instance-based portfolios. learn predictivemodels follow Data Mining approach, shown Figure 1. case, start setcandidate planners set planning benchmarks. output process set modelspredict performance candidate planners. defined data mining goalcreation two predictive models. First, whether planner able solve problem(i.e. classification task) and, so, time required compute best plan (i.e.,regression task).first step mining process comprises generation training test datasets.one hand, planners run set benchmarks obtain performance data.data includes outcome execution (success failure) and, positive cases, timeelapsed finding best solution. hand, planning tasks processed extract setfeatures characterize them. features extended set previously proposedset (Cenamor et al., 2013). According mechanism generating features, classifyfollowing categories:PDDL features: Basic features extracted PDDL representation domainproblem files, instance, number actions, objects goals.660fiT IBAC P P LANNING YSTEMFigure 1: General Diagram Learning Planning Performance Predictive ModelsFD Instantiation features: Fast-Downward pre-processor instantiates translatesplanning tasks finite domain representation (Helmert, 2009). output takegeneral information number instantiated actions number relevantfacts, data specific FD-translator, number auxiliary atoms.SAS+ features: finite domain representation SAS+ associated Causal Graph(CG) set Domain Transition Graphs (DTGs). CG extract basic properties(e.g., number variables edges), ratios properties. regardsDTGs, number graphs problem corresponds number edges CG,makes difficult encode general attributes DTG. Therefore, summarize DTGs characteristics aggregating relevant properties graphs. Thus,features DTGs statistics maximum, average standarddeviation graph properties.Heuristic features: initial state, compute heuristic values using set widely-usedunit cost heuristic functions (e.g., hmax , hFF ,. . . ). compute heuristicsinitial state, obtained reasonable cost. use unit cost heuristicsobtain domain-independent estimation helps characterization problem sizeand/or difficulty.Fact Balance Features: Using relaxed plan (RP ) initial state, extracted computing hFF heuristic, also compute set features represent fact balanceRP . define fact balance fact p, number times p appears addedeffect action belonging RP , minus number times p deleted effectaction RP , considering original actions deletes ignored. intuitionbehind fact balances high positive values would characterize easier (relaxed) problemsgiven domain, since achieved facts need deleted many times. Givennumber relevant facts planning task variable, compute statistics (i.e., min, max,average variance) fact balance relevant facts. Additionally, computestatistics considering facts goals, following procedure.661fiC ENAMOR , DE LA ROSA & F ERN ANDEZcomplete set 89 features listed organized category Appendix A.Data Integration process Figure 1 receives features performance datasets inputsproduce final dataset according modeling goal. dataset classification task,training/test instance includes planning task features plus planner name Booleanfeature indicating whether planner solved planning task. dataset regression taskincludes cases planning tasks solved. make exclusionmake sense model estimate planning time beyond given time limitcases time unknown. training/test instance regression dataset includesplanning task features, planner name time planner used find best solution.Feature Selection optional process reducing number features usedmodeling. procedure applied might irrelevant redundant featurescould degrade modeling capabilities learning techniques (Blum & Langley, 1997).outcome process dependent original data. Thus, decision whether applytaken based results model evaluation.Modeling process, use off-the-shelf data-mining tool provides set learningalgorithms classification regression. generated models evaluatedEvaluation process determine best model classification regression tasks.many different ways carrying model evaluation comparison (Han, Kamber, & Pei,2011; Witten & Frank, 2005), reflect generalization ability different modelsmaking predictions unseen data.2.1.3 TRATEGY ELECTIONstrategy selection final step construction IBAC P planner. Selecting strategy implies decide transform predictions best models actualportfolio configuration. several alternatives range ignoring model predictions trusting completely. classification model, candidate planner getyes/no prediction given new planning task. direct use Boolean variable makes difficultdecide planners include portfolio. Consider, instance. two extreme cases:(1) planners get positive prediction, include them? (2) planners getnegative prediction, planner include portfolio? Instead using Booleanprediction propose rank predictions confidence positive class,make selection planners according ranking. Then, planner assignedslide total time, assignment carried uniformly dependently, again,predictive models learned. Therefore, depending use make predictivemodels, propose three basic strategies:1. Equal Time (ET): strategy use predictive models all. assignequal time planner (uniform strategy). idea behind strategyplanners less time one. strategy obtained good resultsportfolios (Seipp, Braun, Garimort, & Helmert, 2012).2. Best N confidence (BN): strategy include subset N planners bestprediction confidence positive class portfolio. Then, get equal timesolving planning task. case, idea select subset promising plannersspend time solving planning task.662fiT IBAC P P LANNING YSTEM3. Best N Estimated Time (BNE): subset planners selected mentioned before,time assigned proportionally estimated time provided regression model.2.2 Portfolio Configurationinstance-based configuration portfolio implies subset base planners timeassigned one varies function planning task features. set candidate planners,predictive models configuration strategy previously fixed construction phase.Algorithm 1 shows use components configure portfolio given planningtask.Algorithm 1: Algorithm configuring portfolio particular planning task.Data: Problem (), Domain (d), Set base planners (Pini ), Classification model (C),Regression model (R), Available time (T ), Strategy (SN )Result: Portfolio Configuration: sequence planners assigned runtime,Portfolio = [hp1 , t1 i, . . . , hpc , tc i]Portfolio=[];SN == ET/*(No classification regression models available)*/n = size(Pini );p Piniappend(hp, Tn i, Portfolio);elsehF, tF = extractFeatures(d, );pk Pinipredictionhpk , confk predict (C, hF, pk i);sorted candidates sort(prediction, key = conf );p sorted candidates[i . . . N ];SN == BN/*Classification model available, applying Best N confidence strategy*/= 1 NFappend(hpi ,N i, Portfolio);else/*Regression model available, applying Best N Estimated Time*/= 1 Nti = predict time(R, hF, pi i);= scaleTime(t, tF );= 1 Nappend(hpi , ti i, Portfolio) ;method receives problem (), domain (d), set base planners (Pini ), classification model (C), regression model (R), time available (T ) portfolio configurationstrategy (SN {ET, BN, BN E}). procedure calls several functions described below:663fiC ENAMOR , DE LA ROSA & F ERN ANDEZextractFeatures: feature extraction procedure used portfolio construction phase. pair (domain, problem) function outputs set features F .function also computes time (tF ) time spent extracting features.predict: function query classification model C. receives new instancerepresented tuple hF, pi, F previously computed features, pplanner name. result function ignore class, keep predictionconfidence positive class, forming tuple hp, conf i. output representsconfidence planner p solve problem.predict time: function uses model R estimate execution time subsetplanners PN Pini established best N candidates terms classification confidence. classification model, function receives input tuplehF, pi.scaleTime: function transforms vector estimated times another proportionalvector sum fits available time, original time bound minustime used compute features tF . Thus, time assigned planner computedF )tformula = (TPtNi=1 tioutput algorithm sequence planners assigned time. executionparticular configuration portfolio comprises sequential execution base plannersensuring CPU process exceed assigned time.3. IBaCoP Planning Systemsection describe follow approach presented Section 2 build differentportfolios.3.1 Candidate Plannersinitial set planners includes 27 planners Sequential Satisficing Track IPC-2011plus LPG- TD (Gerevini, Saetti, & Serina, 2006). Although LGP- Td compete IPC-2011considered worthwhile include still considered state-of-the-art planner duegreat performance previous competitions.first step apply QT-Pareto Score Filtering described subsection 2.1.1 reduceinitial set candidate planners. benchmarks computing QT-Pareto Score setdomains problems Sequential Satisficing Track IPC-2011.Table 1 shows best planner terms QT-Pareto score domain. Additionally,include number problems solved best planner highlight correlation amongvalues. QT-Pareto score values closer 20 reflect planner able beatplanners problems. P ROBE best planner 4 domains. Howeverplanners stood one domain. reinforces motivation find diverse subsetplanners. Finally, 28 initial planners, QT-Pareto score filtering pre-selected candidateplanners subset 11 planners, made of: LAMA -2011, PROBE , ARVAND , FDSS 2, FD - AUTOTUNE -1, FD - AUTOTUNE -2, LAMAR , LAMA -2008, MADAGASCAR , YAHSP 2- MTLPG- TD. brief description planners found Appendix D.664fiT IBAC P P LANNING YSTEMPlannerPROBEPROBEPROBEPROBEARVANDMADAGASCARLAMA -2008LAMA -2011FD - AUTOTUNE -1FD - AUTOTUNE -2FDSS -2LAMARYAHSP 2- MTLPG- TDDomainscanalyzerwoodworkingtidybotbarmanpegsolparcprintertransportopenstackssokobannomysteryelevatorsparkingvisitallfloortiletotalQT16.5918.5516.7719.4218.8817.6317.8417.3017.5616.7317.8418.1218.7411.96243.77Coverage2020182020201920191920202012267Table 1: List best planners ordered QT-Pareto score domain IPC-2011.Table 2 shows ranking planners IPC results (i.e., planner ordering establishedquality score) (Linares Lopez et al., 2015) selected QT-Pareto ScoreFiltering. worth noting attention 10 11 best planners IPC built topFD, reduces diversity planners. However, QT-Pareto Score Filteringincludes 8 them. addition, pointed last three selections QT-ParetoScore Filtering planners lower positions table which, demonstratedlater, increases diversity portfolio performance.Ranking1234567891011172224plannerLAMA -2011FDSS -1FDSS -2FD - AUTOTUNE -1ROAMERFORKUNIFORMFD - AUTOTUNE -2PROBEARVANDLAMA -2008LAMARYAHSP 2- MTMADAGASCARLPG- TDEligibleFDTable 2: List 11 best planners ordered score IPC-2011. third column shows whetherselected QT-Pareto Score Filtering. forth column shows plannersbuilt top FD.665fiC ENAMOR , DE LA ROSA & F ERN ANDEZ3.2 Performance Modelsinputs performance modeling phase candidate planners (i.e., 11 candidatesselected previous section) benchmark planning tasks selected purpose. Next,describe generated training data, inputs produce specific instancesIBAC P planners.3.2.1 RAINING DATAtraining data learning process requires set domains problems used gatherinput features. need wide range domains problems generalize future unknownplanning tasks properly. included planning problems available IPC-2006 onwards. mention test set explicitly, always refer satisficing trackscompetitions. included domain problems are:IPC-2006: openstacks, pathways, rovers, storage, tpp trucks.IPC-2008: cybersec, elevators, openstacks, pegsol, pipesworld, scanalyzer, sokoban, transport woodworking.IPC-2011: barman, elevators, floortile, nomystery, visitall, tidybot, openstacks, parcprinter,parking, pegsol, sokoban, scanalyzer, transport woodworking.Learning track IPC-2008: gold-miner, matching-bw, n-puzzle, parking, thoughful sokoban.Learning track IPC-2011: barman, blockworld, depots, gripper, parking, rovers satellite,spanner tpp.list obtained 45 different domain descriptions. Although representalternative encodings domain, included. Candidate planners runbenchmarks obtain features related performance planners. Thus, usedtotal 1, 251 planning tasks. performance data comprises 13, 761 instances (i.e., 1, 251 problems 11 planners) 8, 697 successful 5, 394 failed. proportion instancessolved candidate planner different. Table 16 Appendix C shows per-planner summaryperformance data.89 features representing planning task automatically generated domainproblem definitions. PDDL features, FD instantiation SAS+ features computedusing FAST-D OWNWARD pre-processor. computation time needed extract featuresnegligible compared SAS+ translation, given compute sums statisticsdata provided SAS+ representation. heuristic features computed using FASTD OWNWARD search engine, fact balance features generated using relaxed planninggraph structures (of initial state) provided FF planner (Hoffmann, 2003). FASTD OWNWARD pre-processor could fail instantiating planning task. case, regardingfeatures computed missing values assumed.Table 3 shows success rate extracting features type training problems, average maximum time seconds extract them. PDDL, FD SAS+features extracted FD pre-processor success rate.time required compute heuristic features time calculating heuristic valueinitial state, calculated FD pre-process finished successfully.666fiT IBAC P P LANNING YSTEMClassPDDLFDSAS+HeuristicFact BalanceTotalSuccess97%97%97%87.54%93%-Average (s.)6.9752.7322.6020.205.20107.7Max (s.)46.00141.4060.6030.5021.20299.7# features816508789Table 3: Summary extracted features average maximum time seconds (s.)extract them. processes top two first step planners basedFD.3.2.2 F EATURE ELECTIONcarried feature selection process two main reasons. one hand, featuresmight irrelevant whilst others might redundant modeling purpose. Therefore wantanalyze whether possible obtain better models using subset available features.hand, study allow us recognize relevant features characterizingplanning task.feature selection carried using J48 algorithm, top-down induction algorithmbuild decision trees (Quinlan, 1993), selecting features appear top nodestree (Grabczewski & Jankowski, 2005). Decision trees make implicit feature selectionmodel includes queries features considered relevant. applying feature selectionprocess feature dataset, total number features decreased 89 34. leadsdataset size reduction around 62%. Table 4 contains list features resultingfeature selection process. selection chooses features categories. modelingevaluation process kept datasets separate, one available features (f-all)one selected features (f-34).3.2.3 C LASSIFICATION ODELStrained classifiers using 31 classification algorithms provided Weka (Witten & Frank,2005), includes different model types decision trees, rules, support vector machinesinstance based learning. recall training instances include planning task featuresdescribed Section 2.1.2 plus planner name Boolean feature indicating whetherplanner solved planning task not. performance predictive models evaluated10-fold cross-validation uniform random permutation training data. bestmodel datasets f-all f-34 generated Rotation Forest (Rodriguez, Kuncheva,& Alonso, 2006), achieving 93.39 92.35% accuracy respectively. results quitebetter result default model (ZeroR), obtained 61.72% accuracy. Seeresults classification models Table 14 Appendix B.Even though good accuracy classification model guarantee good performanceportfolio, result great starting point selecting promising planners. accuracyresults feature selection showed small differences compared results obtained667fiC ENAMOR , DE LA ROSA & F ERN ANDEZTypePDDL(4)CG & DTG(11)FeaturestypesgoalobjectsfunctionsnumberVariablesCGinputEdgeCGStdoutputEdgeCGAvgoutputWeightCGMaxoutputWeightCGAvgoutputEdgeHVStdoutputWeightHVMaxnumberVariablesDTGtotalEdgesDTGinputWeightDTGMaxhvRatioTypeFD(6)Heuristics(7)Balance(6)Featuresauxiliary atomsimplied effects removedtranslator factstranslator total mutex groups sizenum relevant factsnum instance actionsAdditiveContext-enhanced additiveFFGoal countLandmark countLandmark-cutMaxrp fact balance avgrp fact balance varrp goal balance minrp goal balance avgrp goal balance varh ff ratioTable 4: List features feature selection. complete set features listed Appendix A.features. 3 algorithms statistically better accuracy f-34 dataset ninesimilar accuracy, cases best achieved accuracy3.2.4 R EGRESSION ODELStrained regression models positive instances classification trainingphase. classification phase, planners proportion instances,case, planners number instances given solved differentnumber problems. Nevertheless consider relevant bias modelsinclude planner name, somehow encodes single models planner, groupedmodel. trained models 20 regression algorithms, also provided Weka.best algorithm f-all Decision Table (Kohavi, 1995) Relative Absolute Error(RAE) 49.87 best one f-34 Bagging (Breiman, 1996) RAE 50.62.Nevertheless, simplicity selected Decision Table model regression taskdatasets (f-all f-34). decision justified results show significantdifference t-test result. following sections, regression model always refertrained Decision Table algorithm. See results regression modelsTable 15 Appendix B.668fiT IBAC P P LANNING YSTEM3.3 IBaCoP Strategiesconsidered various strategies configuration IBAC P portfolios. liststrategies ordered depending use make knowledge provided predictivemodels. experiments, configuration run 1800 seconds. namedportfolios according names given IPC-2014.IBAC P: portfolio uses equal time strategy (ET) set 11 candidate planners previously filtered QT-Pareto Score Filtering procedure. Therefore, single plannersrun 163 seconds. strategy use predictive models. planner usingstrategy awarded runner-up sequential satisficing track IPC-2014.IBAC P2: portfolio uses Best N confidence strategy (BN), N = 5. means5 planners best prediction confidence solving problem includedconfiguration. run time assigned uniformly planner (360 seconds).strategy, using f-34 model winner sequential satisficing track IPC-2014.1IBAC P2-B5E: portfolio uses Best estimated time strategy (BNE) N = 5. follows procedure IBAC P2 select 5 planners, time assignedscaling time prediction provided regression model (Decision Table). strategyparticipated learning track IPC-2014 name LIBAC P2. casetraining data models generated domain separately, since learning trackprovides training problem set domain priori.addition, built portfolio configurations serve baseline comparison.Overall Equal Time (OET): strategy non-informed strategy carryplanner filtering use predictive models. assigns equal time available planner.Given 28 planners (all participants IPC-2011 plus LPG-td), plannerrun 64 seconds. planner see need planner filtering since,although already obtains results close current state art base planners, resultsimproved selecting reduced set planners.Best 11 Planners (B11): strategy selects top 11 planners IPC-2011 ordered scorecompetition, shown Table 2. Although selecting best 11 planners goodchoice intuitively, show table selection reduces planner diversityportfolio, since top planners competition based FD, exception Probe. strategy comparable implemented BUS portfolio (Howe,Dahlman, Hansen, Scheetz, & von Mayrhauser, 1999), control strategy ordering planners allocating time derived performance study data.Random 5 Planners (Rand): strategy one baselines compare best 5 confidence strategy (IBAC P2). Given planning task, strategy takes random sample5 planners population 11 candidate planners selected QT-Pareto filtering,1. Predictive models submitted IBAC P2 IPC-2014 trained different benchmark set. casebest accuracy achieved Random Forest (Breiman, 2001).669fiC ENAMOR , DE LA ROSA & F ERN ANDEZassigns equal time them. expect wise selection 5 planners (IBAC P2)average better random selection.Default 5 Planners (Def): case, strategy always includes 5 best planners termsquality score training data. 5 planners subset 11 candidateplanners selected QT-Pareto filtering (i.e., LAMA -2011, PROBE , FD -AUTOTUNE -1,LAMA -2008 FD - AUTOTUNE -2). Then, time assigned equitably. want seewhether using best 5 planners better making per-instance selection 5 planners.3.4 Implementation Detailssection describe engineering details incorporated IBAC Pplanners. instance, competition rules proposed include domains conditional effects.this, included parser translates tasks conditional effectsequivalent planning task without property. translator based previous translator ADL2STRIPS (Hoffmann, Edelkamp, Thiebaux, Englert, dos Santos Liporace, & Trug, 2006).Specifically, implemented compilation creates artificial actions effect evaluations (Nebel, 2000).Furthermore, many 11 candidate planners built FAST-D OWNWARD framework, among things, separate planning process sub-process translation,pre-processing search. Indeed, translation pre-process steps already executedfeature generation given task performed. take advantage fact avoidfirst two steps repeatedly planners included configurationportfolio regarding task. version compatibility reasons procedure dividedtwo groups. output FD pre-process, used feature extraction, also usedsearch input LAMA -2011, FDSS -2 FD - AUTOTUNE (1 & 2). previous FD pre-processor2 used common LAMA -2008, ARVAND LAMAR . optimization usedstrategies evaluated. remaining planners totally independent FD pre-processing.Moreover, bugs arose execution IPC-2014, issues domainmodels required updates (Vallati, Chrpa, & McMcluskey, 2014a), planners updatedMercury (Vallati, Chrpa, & McMcluskey, 2014b). issues also fixed priorrunning experimental evaluation presented article.4. Experimental Evaluationsection, describe settings experimental evaluation present resultsplanners benchmarks used IPC-2014, specifically, Sequential Satisficingtrack. addition, provide analysis diversity planner selection achievedconfigurations.4.1 Experimental Settingsevaluated different portfolio strategies described Section 3.3, permits differentportfolio configurations created. IBAC P2 IBAC P2-B5E run two predictivemodel versions, one trained features (f-all) one trained selected fea2. version corresponds version used submit planners IPC-2011670fiT IBAC P P LANNING YSTEMtures (f-34). Random strategy run 5 times average reported. addition,included JASPER ERCURY planners comparison. planners also competed IPC-2014. ERCURY (Domshlak, Hoffmann, & Katz, 2015) second best plannerterms IPC score JASPER (Xie, Muller, & Holte, 2014) second best planner termsproblems solved (coverage). test set used benchmarks IPC-2014,updates described Section 3.4. test set comprises 14 domains 20 problemsdomain.Experiments run cluster Intel XEON 2.93 Ghz nodes, 8 GB RAM,using Linux Ubuntu 12.04 LTS. planners cutoff 1, 800 seconds 4 GB RAM.IBAC P configurations requiring feature extraction, process limited 4 GB RAM(following IPC competition rules) 300 seconds (which maximum time used trainingset obtain features, described Table 3). time extract features includedexecution portfolio where, worse case, feature extraction process took 300 secondsand, therefore, candidate planners 1, 500 run. system extractfeatures time, input features treated missing values.4.2 ResultsTable 5 shows results evaluated planners using IPC quality score. recallscore gives ratio QQi planner problem, Qi quality best solutionfound planner i, Q best solution found planner. planner solveproblem score 0.HikingOpenstacksThoughtfulGEDBarmanParkingVisitallMaintenanceTetrisChildsnackTransportFloortileCityCarCaveDivingTotalIBaCoP2f-allf-34IBaCoP2-B5Sf-allf-34MercuryJasperOETB11DefRandIBaCoP18,919.612.719.414.618.020.05.116.30.019.92.04.17.018.118.816.417.919.017.015.410.016.20.012.02.011.58.018.215.414.518.316.717.613.315.05.012.08.94.86.00.019.217.219.417.116.713.88.115.911.53.43.83.48.80.018.719.219.216.317.218.013.711.69.32.66.94.15.07.018.416.317.413.013.811.615.014.511.98.98.212.39.47.019.017.819.217.516.916.315.215.613.319.210.316.212.56.318.918.619.217.617.118.118.015.512.518.411.515.39.07.018.618.517.417.517.118.118.015.411.918.911.617.26.27.018.818.217.617.617.218.518.015.515.715.011.117.59.97.018.618.319.217.517.218.118.015.413.618.912.112.07.787.0177.6182.1165.7158.3168.8177.6215.3216.5213.4217.4213.7Table 5: Results IBAC P configurations. table also includes results Jasper, Mercuryfour baseline configurations, OET, Best 11, Default Random.overall best planner IBAC P2-B5E (f-all), closely followed IBAC P2 (f-all).difference two configurations negligible. configurations using predictivemodels much better OET, Default, Best 11 Random. IBAC P good performance, comparable best performance. Moreover, big difference671fiC ENAMOR , DE LA ROSA & F ERN ANDEZconfigurations planners (Jasper Mercury). IBAC P based configurations 32points higher cases.Figure 2 details evolution number problems solved function run-timeelapsed. far right-hand point figure represents final coverage. best plannerterms coverage IBAC P, 249 problems, second IBAC P2 (f-all) 246.Figure 2, planners show two different behaviors. one hand, asymptotic growingnumber problems solved demonstrates giving time planners permitnumber problems solved increased. JASPER extreme case, 300 secondsalmost unable improve. ERCURY problem, well portfolio configurationstake care diversity. However, IBAC P, IBAC P2 IBAC P2-B5E,selected diverse set planners, show growing behavior throughout time.250200Problems150100IBaCoPIBaCoP2IBaCoP2-B5ERandomJasperDefaultMercuryBest110ET500020040060080010001200140016001800TimeFigure 2: Comparison IBAC P configurations, baseline configurations, MercuryJasper planners.results derive insights regarding different configurations. scoredifference OET IBAC P reveals importance making pre-selection candidateplanner accurate filtering procedure. Pareto-dominance approach allows ussmaller set planners, means time per planner. trade-offtime per planner loosing diversity solvers, results suggestimportant maintain diversity increasing running time per planner. instance,11 best IPC-2011 planners (B11) obtain worse results using original 28 (OET), eventhough B11 base planners longer running time. However, QT-Pareto filtering approachable reduce number planners sacrificing diversity, produces goodresults.Reducing number planners portfolio configuration 11 5 puts riskdiversity solvers, shown results Def approach (the best 5 planners terms672fiT IBAC P P LANNING YSTEMperformance) Rand (the random selection 5 planners). Nevertheless, IBAC P2 (f-all)(f-34) perform quite better Def Rand, demonstrates classification modelsselect average good subset planners solving particular task. results quitepromising exploiting empirical performance models planning portfolios. However,current setting, results IBAC P2 quite similar IBAC P. Thus, classification modelsmanage reduce set planners without deteriorating performance fixed portfolio,hardly contribute better overall performance.Table 6 presents number problems solved 11 candidate planners. finalcolumn maximum number problems solved complete set candidateplanners (i.e., problem solved least one candidate planners solvedproblem). optimal selection 5 planners planning task would lead 253 problemssolved. IBAC P2 close optimum, confirming ability selecting good candidatesportfolio. default configuration solved 193 problems, average number problemssolved random configuration 207 problems. far best possiblevalue.HikingThoughtfulOpenstacksTetrisGEDTransportParkingBarmanMaintenanceCityCarVisitallChildsnackFloortileCaveDivingtotallama111815209201520207120020probe201241420129198010020FDA11816191520714151050220lama0820172080121313142220FDA220122010622850050lamar20142013071815180220arvand20202018050017192810fdss22017121720101681650320ya2-mt41300020003220000LPG208101400080171903500000061402007Max20202018202020201719202019716613014311481120128146627455260Table 6: Results candidate planners defined Table 1 maximum number problemssolved complete set planners.set 5 planners selected per-instance configuration, regressionmodels contribute better performance. task estimating run time neededsolve problem difficult classification task (Schwefel, Wegener, & Weinert, 2013).Additionally, given aggregated time predictions could exceed time limit, proposal rescales estimations alters real predictions. One alternative proposal keepreal prediction run planners order established confidence classificationprediction, one reaches time limit. However, preliminary experimentsdevelopment planner showed us approach compensate risk losingdiversity due fewer planner executions.Another aspect analyzed performance planners new domains. IPC2014 incorporated seven new domains, means QT-Pareto Filtering predictivemodels trained them. domains Cave Diving, Child-Snack, CityCar,673fiC ENAMOR , DE LA ROSA & F ERN ANDEZGED, Hiking, Maintenance Tetris. results conclude behaviorIBAC P configurations new domains average similar performance previously seendomains.4.3 Per-Instance Selection Plannersprevious section showed benefit configuring portfolio per problemset selected planners better adjusted problem, using fewer planners, providingexecution time planner. section want analyze diversity plannerselections made IBAC P2 see predictive models classifying planners goodsolving specific domains identifying properties specific problemsdifferent domains. Note test problems given domain usually range easy hard.increase difficulty mainly due larger size problems. Nevertheless, increaseaffects learning features different scale intensity.lama-2011probefd-autotune-1lama-2008fd-autotune-2lamararvandfdss-2yahsp2-mtLPG-tdmadagascaritaVisrtponslTratfughouThtrisTengrkiPa tackseneOp nanceintingHikGE eilortFlo ryCkCitacsnildChingDivCa nrmBaFigure 3: Proportion number times planner selected domain. reddots, proportion IBAC P2 (f-all), blue dots, proportion IBAC P2(f-34).Figure 3 shows diversity planners according selection made IBAC P2 (bluedots f-34 red dots f-all). x axis shows IPC-2014 domains axis lists11 candidate planners portfolio use. size dots proportionalnumber times planner selected particular domain, i.e. number problemsplanner selected. domain five dots one column (one domain), meansselected portfolio configuration problems domain. However, every674fiT IBAC P P LANNING YSTEMcolumn five dots reveals use different 5-planner sets different problemsdomain. highlight analysis 11 planners selectedleast one domain, 13 14 domains selections involve 5 planners.Note, instance, LAMA -2011 best priori confidence solving problems,sometimes used (i.e., selected 6 times Floortile 11 times Openstacks).Furthermore, planners low priori probability selected, frequentlyused domains (like LPG- TD Floortile).Table 7 shows sum number times planner selected. maximum number times planner could selected 14 20 = 280. last column reportsaverage standard deviation number times planner selectedper domain approximations (all reduced set features).LAMA -2011PROBEFD - AUTOTUNE -1LAMA -2008FD - AUTOTUNE -2LAMARARVANDFDSS -2YAHSP 2- MTLPG- TDMADAGASCARf-342482001731739315265122952935f-all25620615115788133111149713145Average18,0014,5011,5714,506,4610,186,299,685,932,142,86STD4,026,716,296,717,136,985,907,947,264,995,73Table 7: Number times candidate planner selected two different classificationmodels (f-34 f-all).addition previous analysis, wanted delve underlying mechanismachieve per-instance selection planners. recall planners selected basedconfidence success prediction. Therefore, order achieve different 5-planner setsdomain, ranking prediction confidence vary throughout problem.visualize confirm fact, selected Tetris domain, one newdomains IPC-2014 shows good diversity selection shown Figure 3. domainsimplified version well-known Tetris game.heatmap success prediction confidences appears Figure 4. glance realized general, planner higher success rate training time obtains higher confidence,confidence ranking varies throughout different problems domain. Another wayread picture 5 darkest squares per column form set selected planners. instance, lama-2011 selected problems probe selected 18 times. handMadagascar selected, LPG-td selected 3 times.675fiC ENAMOR , DE LA ROSA & F ERN ANDEZlama-2011probefd-autotune-1lama-2008Scorefd-autotune-2lamararvandfdss-2yahsp2-mtLPG-tdmadagascar012345678910 11 12 13 14 15 16 17 18 19Figure 4: Success prediction confidence provided classification model (f-all) plannerproblem tetris Domain. Scale goes 0.0 (white) confidence1.0 (dark blue) complete confidence.5. Related Worksection, summarize relevant research portfolio configuration relateswork. addition, summarize different approaches characterization planningtasks, cornerstone work predict behavior planners.idea exploiting synergy different solvers improve performance individual ones applied propositional satisfiability problems (SAT), constraint satisfaction problems(CSP), answer set programming (ASP) scope paper, Automated Planning.SAT area carried extensive research importance selecting componentsportfolio (Xu, Hutter, Hoos, & Leyton-Brown, 2012) select component (Lindauer, Hoos, Hutter, & Schaub, 2015b) automatically. study strategy selection areaincludes per-instance selections (Lindauer, Hoos, & Hutter, 2015a). addition, intensivestudy solver runtime prediction (Hutter, Xu, Hoos, & Leyton-Brown, 2015), including goodcharacterization satisfiability task. fields Artificial Intelligence, CSP portfolio configurations based machine learning techniques SUNNY (Amadini, Gabbrielli, &Mauro, 2014b) empirical research (Amadini, Gabbrielli, & Mauro, 2014a). exampleASP, ASP-based Solver Scheduling (Hoos, Kaminski, Schaub, & Schneider, 2012) multicriteria optimization problem provides corresponding ASP encodings. paperreport main systems related Automated Planning detail.5.1 Portfolios Automated PlanningHowe et al. (1999) describes one first planner portfolios. implement system calledBUS runs 6 planners whose goal find solution shortest period time.achieve it, run planners portions time circular order one findssolution. portfolio, planners sorted following estimation provided linear676fiT IBAC P P LANNING YSTEMregression model success run-time so, case, use predictive modelsbehavior planners decide order execution. However, use 5 featuresextracted PDDL description. domain, count number actionsnumber predicates. problem, count number objects, number predicatesinitial conditions number goals. BUS minimizes expected cost implementingsequence algorithms one works, contrast IBAC P IBAC P2, stopassigned time over.Fast Downward Stone Soup (FDSS, Helmert et al., 2011) based Fast Downward (FD)planning system (Helmert, 2006), several versions different tracks. FDSS approachselect combine heuristics search algorithms. configuration combination searchalgorithm group heuristics. training, evaluate possible configurations timelimit, select set configurations maximizes coverage. portfolio presentedIPC-2011 Sequential Satisficing Track, sort configurations decreasing ordercoverage, hence beginning algorithms likely succeed quickly. time limitcomponent lowest value would still lead portfolio score training phase.However, order important, since setting communicates quality best solutionfound far following one, value used improve performance nextsetting. Therefore, FDSS include configurations within FD framework. Conversely,IBAC P IBAC P2 build portfolio using mixture generic planners different stylestechniques. Indeed FDSS one IBAC P candidate planners.PbP (Gerevini et al., 2009) configures domain-specific portfolio. portfolio incorporatesmacro-actions specific knowledge domains. incorporation knowledge establishes order subset planners contain macro-actions. running time assignedround-robin strategy. portfolio incorporates seven planners (the latest version, PbP2,adds lama-2008, see Gerevini et al., 2014). automatic portfolio configuration PbP IBA C P aims build different types planning systems: domain-optimized portfolio plannergiven domain PbP IBAC P efficient domain-independent planner portfolio.IBAC P PbP configuration processes significantly different. PbP uses several plannersfocus macro-actions whilst IBAC P uses generic planners. execution schedulingstrategy PbP runs selected planners round-robin rather sequentially caseIBAC P.Fast Downward Cedalion (Seipp, Sievers, Helmert, & Hutter, 2015) algorithm automatically configuring sequential planning portfolios parametric planner. Given parametricplanner set training instances, selects pair planner time iteratively. enditeration instances current portfolio finds best solution removedtraining set. algorithm stops total run time added configurationsreaches portfolio time limit training set becomes empty. Configurations generatedusing SMAC (Hutter, Hoos, & Leyton-Brown, 2011) model-based algorithm configuratorremaining training instances. Cedalion configuration problemsdifferent configuration per version IBAC P different configuration per problem. diversity candidate planner limited IBAC P may completely include independent baseplanners. configuration processes resulting configured portfolios CedalionFDSS.Fast Downward Uniform (Seipp et al., 2012) portfolio runs 21 automatically configured FastDownward instantiations sequentially amount time. Uniform portfolio approaches677fiC ENAMOR , DE LA ROSA & F ERN ANDEZconfigured using automatic parameter tuning framework ParamILS (Hutter, Hoos, LeytonBrown, & Stutzle, 2009) find fast configurations Fast Downward planning system 21planning domains separately. runtime, configurations found run sequentiallyamount time 85 seconds.MiPlan (Nunez et al., 2015) sequential portfolio using Mixed-Integer Programming,computes portfolio obtains best achievable performance respect selectiontraining planning tasks. case created sequential portfolio subsetsequential planners fixed times whilst IBAC P2 different configurations per problem.approximation, planner consider portfolios, components.contrast, IBAC P IBAC P2 includes planners appear competitions, i.e.black boxes.5.2 Features Planning Problemsconstruction models predict performance planners novel idea. Roberts etal. (2008, 2009) showed models learned planners performance known benchmarks2008 obtain high accuracy predicting whether planner succeed not.use 19-32 features extracted domain problem definition. main differenceapproach also include features based SAS+ , heuristics initial statefact balance relaxed plan. features come ground instantiationproblem, key differentiate tasks share feature values PDDLlevel.Torchlight (Hoffmann, 2011) toolkit allows search space topology analyzedwithout actually running search. analysis based relation topologydelete relaxation heuristics causal graph well DTGs. feature extractionprocess built top FF planner (Hoffmann & Nebel, 2001).Recently, Fawcett et al. (2014) generated models accurately predicting planner runtime. models exploit large set instance features, including many features depictedSection 2.1.2. features derived PDDL SAS+ representations problem, SAT encoding planning problem short runs planners. featuresextracted Torchlight (Hoffmann, 2011). experimental results work indicateperformance models generated able produce accurate run time predictions. studyempirical performance models applied portfolio configurations.6. Conclusion Future Workwork introduced framework creation configurable planning portfolios,IBAC P. first step portfolio creation find small number planners maintainsdiversity initial planner set based QT-Pareto score filtering. train predictivemodels select promising sub-set planners solving particular planning task.experimental evaluation confirmed great performance IBAC P IBAC P2IPC-2014. summarize lessons learned development current IBAC Pportfolios following:really matters generation good portfolio selection diverse setplanners. shown QT-Pareto score filtering reduces set candidate plan678fiT IBAC P P LANNING YSTEMners preserving diversity. filtering produces better results rankingsbased coverage quality score.selection smaller sets planners portfolio configuration (e.g., sub-set 5planners experiments) dangerous given portfolio might lose planner diversity.observed situation Def Random configurations, select 5 11planners.portfolio configurations using classification models able select good subset5 planners, uniformly distributed time outperformed selection providedrandom default selection number planners.Estimating runtime solving problem still difficult reason regression models providing additional useful information portfolio construction.current form, predictive models hardly contribute overall performanceportfolio. Per-instance configurations using classification models achieve similar performance fixed portfolio, running fewer planners.Even though current architecture benefits using predictive models limited,results promising good performance IBAC P2 compared baselineconfigurations. think room research direction. argumentstatic portfolio configurations (including IBAC P) limited components fixedtime bound base planner. performance upper-limit, computed MiPlan,smaller achievable performance dynamic configuration.per-instance configuration portfolio strategy could assign different times base planners.future work want study additional features better characterization planningtasks. computation could carried pre-process step, even informationfirst evaluated search nodes, could help making predictive models accurate.models could incorporate information, instance, landmark graph time elapsedcomputing initial state heuristics. future work study importance createdfeatures, including comparison different groups accordance semanticsfeatures.7. Acknowledgmentsthank authors base planners work based largely previous effort.work partially supported Spanish projects TIN2011-27652-C03-02, TIN201238079-C03-02 TIN2014-55637-C2-1-R.679fiC ENAMOR , DE LA ROSA & F ERN ANDEZA. Appendix: Complete Feature DescriptionAppendix present list features used characterize planning task.feature include brief description computed. Features groupedcategory separate tables.A.1 PDDL FeaturesN.12345678NameObjectsGoalsInitTypesActionsPredicatesAxiomsFunctionsDescriptionnumber objects problem.number goals problem.number facts initial state.number types domain.number actions domain.number predicates domain.number axioms domain.number functions domain.Table 8: PDDL Features.A.2 FD Instantiation FeaturesN.910111213NameRelevant factsCost metricGenerated rulesRelevant atomsAuxiliary atoms14Final queue length1518Total queue pushesImpliedeffectsremovedEffect preconditionsaddedTranslator variables19Derived variables2021222324Translator factsMutex groupsTotal mutex sizeTranslator operatorsTotal task size1617Descriptionnumber facts marked relevant FF instantiation.Whether action costs used not.number created rules translation process create SAS+ task.number relevant atoms found translator process.number auxiliary atoms found translator process.length queue end translation. queue auxiliarylist used translation process compute model.number times element pushed queue.number implied effects removed. implied effectstranslator knows already included.number implied effects added.number created variables SAS+ formulation.number state variables correspond derived predicatesartificial variables directly affected operator applications.number facts pre-process takes account.number mutex groups.sum mutex group sizes.number instantiated operators SAS+ formulation.allowed memory translation process.Table 9: Features extracted console output FD system.680fiT IBAC P P LANNING YSTEMA.3 SAS+ Feature Descriptionrecall CG, high-level variables variables defined valuegoal. Although common definition CG consider edges weighted,FD system computes edge weights CG number instantiated actions inducededge. also consider weights computing features.N.Name25262728Number VariablesHigh-Level VariablesTotalEdgesTotalWeight29VERatio30WERatio31WVRatio32HVRatio33-35InputEdge36-38InputWeight39-41OutputEdge42-44OutputWeight45-47InputEdgeHV48-50InputWeightHV51-53OutputEdgeHV54-57OutputWeightHVDescriptionGeneral Featuresnumber variables CG.number high-level variables.number edges.sum edge weights.CG Ratiosratio total number variables total numberedges. ratio shows level connection CG.ratio sum weights number edges.ratio shows average weight edges.ratio sum weights number variables.ratio number high-level variables total number variables. ratio shows percentage variables involvedproblem goals.Statistics CGMaximum, average standard deviation number incomingedges variable.Maximum, average standard deviation sum weightsincoming edges variable.Maximum, average standard deviation number outgoingedges variable.Maximum, average standard deviation sum weightsincoming edges variable.Statistics high-level Variablesnumber incoming edges high level variables.value produces three new features following computationInputEdgeCG (features 33-35).edge weight sum incoming edges high levelvariables. value produces three new features followingcomputation InputWeightCG.number outgoing edges high level variables.sum weights incoming edges high level variables.Table 10: Features Causal Graph.681fiC ENAMOR , DE LA ROSA & F ERN ANDEZN.Name5859Number VerticesTotal Edges60Total Weight61edVa Ratio62weEdRatio63weVaRatio64-66Input Edge67-69Input Weight70-72Output Edge73-75Output WeightDescriptionGeneral Aggregated Features DTGsum number nodes DTGs.sum number edges DTGs.sum edge weights DTGs. edge weight DTGcorresponds cost applying action induced edge.DTG Ratiosratio total number edges total numbersvariables. ratio shows level connection DTG.ratio sum weights number edges.ratio shows number restrictions need make transition.ratio sum weights number variables.Statistics DTGsMaximum, average standard deviation number incomingedges vertex DTG.Maximum, average standard deviation sum weightsincoming edges nodes.Maximum, average standard deviation number outgoingedges vertex DTG.Maximum, average standard deviation sum weightsoutgoing edges nodes.Table 11: Features aggregate information DTGs.682fiT IBAC P P LANNING YSTEMA.4 Heuristic FeaturesN.Name76Max77Landmark cut78LandmarkcountGoal count79FF80Additive81Causal Graph82ContextenhancedadditiveDescription(Bonet, Loerincs, & Geffner, 1997; Bonet & Geffner, 2000) maximumaccumulated costs paths goal propositions relaxedproblem.(Helmert & Domshlak, 2009) sum costs disjunctive actionlandmark represents cut justification graph towards goal propositions.(Richter, Helmert, & Westphal, 2008) sum costs minimumcost achiever unsatisfied required landmark.number unsatisfied goals.(Hoffmann & Nebel, 2001) cost plan reaches goalsrelaxed problem ignores negative interactions.(Bonet et al., 1997; Bonet & Geffner, 2000) sum accumulated costspaths goal propositions relaxed problem.(Helmert, 2004) cost reaching goal given search statesolving number sub problems planning task derivedcausal graph.(Helmert & Geffner, 2008) causal graph heuristic modified use pivotsdefine contexts relevant heuristic computation.Table 12: Unit cost heuristics included features.A.5 Fact BalanceN.Name83-85RP init86-88RP goal89Ratio ffDescriptionMinimum, average variance number times factinitial state deleted computation relaxed plan.Minimum, average variance number times goaldeleted computation relaxed plan.ratio value max FF heuristic. proportionshows idea parallelization plan.Table 13: Fact balance features.683fiC ENAMOR , DE LA ROSA & F ERN ANDEZB. Appendix: Learning Resultsappendix shows detailed results machine learning algorithms used train predictive models.B.1 ClassificationAlgorithmrules.ZeroRrules.Ridorrules.PARTrules.JRiprules.DecisionTablerules.ConjunctiveRuletrees.REPTreetrees.RandomTreetrees.RandomForesttrees.LMTtrees.J48trees.ADTreetrees.NBTreetrees.DecisionStumplazy.LWLlazy.IBk -K 1lazy.IBk -K 3lazy.IBk -K 5meta.RotationForestmeta.AttributeSelectedClassifiermeta.ClassificationViaClusteringmeta.ClassificationViaRegressionmeta.Baggingmeta.MultiClassClassifierfunctions.SimpleLogisticfunctions.MultilayerPerceptronfunctions.RBFNetworkfunctions.SMObayes.NaiveBayesbayes.NaiveBayesUpdateablebayes.BayesNetf-all dataset61.72 0.0382.52 2.4890.81 0.8987.21 1.3885.78 0.9869.33 1.2089.08 0.8586.39 1.8190.96 0.7891.11 0.7290.84 1.0175.46 1.2490.38 0.8867.96 0.9667.96 0.9685.93 0.8486.04 0.9085.36 0.9193.39 0.7089.69 0.8952.32 1.9890.82 0.8490.99 0.7477.15 1.0976.37 1.1287.27 1.6567.71 1.0375.39 1.1669.00 0.9869.00 0.9875.43 1.29f-34 dataset61.72 0.0381.76 2.1189.62 0.8986.26 1.2084.94 1.3769.64 1.6188.06 0.8987.91 0.9590.27 0.8590.03 0.9489.24 0.8774.39 1.3089.47 0.9264.10 1.3063.48 1.8682.97 1.0384.13 1.0384.17 1.0192.35 0.7388.64 1.0057.99 2.6689.80 0.7589.83 0.8575.02 1.1474.48 1.2388.65 1.0168.10 1.1773.94 1.1068.87 0.9768.87 0.9775.05 1.21Table 14: Accuracy standard deviation training algorithm using 10-fold crossvalidation. Also, results t-test (OMahony, 1986) two training sets shown.Symbols , means statistically significant improvement degradation respectively.significance level t-test 0.05 baseline left column.684fiT IBAC P P LANNING YSTEMB.2 Regressiontrees.DecisionStumptrees.REPTreetrees.RandomTreetrees.RandomForestfunctions.M5Prules.ConjunctiveRulerules.DecisionTablerules.M5Rulesmeta.Baggingmeta.AdditiveRegressionlazy.IBk 1lazy.IBk 3lazy.IBk 5lazy.KStarlazy.LWLfunctions.LinearRegressionfunctions.MultilayerPerceptronfunctions.LeastMedSqfunctions.RBFNetworkfunctions.SMOregf-all datasetRAE82.09 2.36 0.42 0.0557.70 3.40 0.66 0.0559.28 6.06 0.55 0.0752.54 2.66 0.71 0.0460.44 13.26 0.59 0.1887.31 2.79 0.38 0.0649.87 3.03 0.69 0.0490.60 138.25 0.58 0.1850.95 2.71 0.74 0.0480.91 3.21 0.51 0.0492.96 11.09 0.36 0.0674.31 6.31 0.47 0.0673.03 5.91 0.47 0.0669.26 3.35 0.44 0.0581.82 2.30 0.43 0.0577.71 2.55 0.55 0.0486.01 72.86 0.66 0.0566.36 2.94 0.33 0.0894.201.60 0.23 0.0557.01 2.88 0.48 0.05f-34 datasetRAE82.09 2.36 0.42 0.0556.69 3.36 0.67 0.0553.71 4.54 0.61 0.0645.62 2.68 0.76 0.0356.38 4.22 0.65 0.0987.25 2.80 0.39 0.0651.19 2.78 0.68 0.0565.84 12.74 0.61 0.1450.62 2.58 0.74 0.0479.93 3.29 0.51 0.0466.73 5.17 0.54 0.0663.57 4.25 0.60 0.0564.38 3.81 0.60 0.0567.75 3.360.470.0581.82 2.33 0.43 0.0578.58 2.52 0.51 0.0481.59 45.93 0.66 0.0566.29 3.01 0.31 0.0794.251.540.210.0458.75 2.62 0.45 0.05Table 15: Results 10-fold cross-validation regression models. RAE RelativeAbsolute Error correlation coefficient. small RAE values better.Symbols , means statistically significant improvement degradation respectively.significance level t-test 0.05 baseline left column.685fiC ENAMOR , DE LA ROSA & F ERN ANDEZC. Appendix: Training ResultsopenstackspathwaysroversstoragetpptruckspipesworldcybersecOpenstacks-adlopenstackspegsolscanalyzersokobantransportwoodworkingelevatorsbarmanelevatorsfloortilenomysteryopenstacksparcprinterparkingpegsolscanalyzersokobantidybottransportvisitallwoodworkingGold-minerMatching-bwN-puzzleparkingsokobanthoughtfulbarmanblocksworlddepotsgripperparkingroverssatellitespannertppTotalL-11ProbeFDA1L-08FDA2LamarArvandFDSS2ya2-mtLPG30304040301942283130303029182330202061020202020201916192020302529282309291018301603030304040308442431303030271030292020561414192020171820202030152024231853030093010020302640403018402831303030291725302020710202019202019151122030243025300022006303030302940403016382831303030251726254631220120202015141920143023292818002100132930303029404030223326313030272718243061791920149201716171051426239163000150301242906303040403015432731303030251725276113122002020201419311929172730170006019301021273040403015462831303030819303002031920204202021715102030166173000004430203030040403020422831303030291530301720712192020202019181562000003001320009302202500404030041001222701123212081001331517002020193025201328220162900301303027304040241133710100000001202000001508030223013157029630011303011130404030211401615272129200001702001711010013010022000000140159#3030404030305030313030303030303020202020202020202020202020203030303030303030303030303030309989609278778698358238376305054361251Table 16: Solved problems training phase. first part table results IPC2005, second part IPC-2008 IPC-2011 satisficing tracks. two last rows(from Gold-miner tpp) IPC-2008-2011 learning track. last columnnumber problems included training.686fiT IBAC P P LANNING YSTEMD. Appendix: Plannersfollowing list set planners pre-selected candidates Pareto-dominance filteringdescribed Section 3.1Arvand (Nakhost, Muller, Valenzano, & Xie, 2011): stochastic planner uses MonteCarlo random walks balance exploration exploitation heuristic search. versionuses online learning algorithm find best configuration parameters givenproblem.Fast Downward Autotune-1 Fast Downward Autotune-2 (Fawcett, Helmert, Hoos, Karpas,Roger, & Seipp, 2011): two instantiations FD planning system automatically configured performance wide range planning domains, using well-known ParamILSconfigurator. planners use three main types search combination several heuristics.Fast Downward Stone Soup-2 (Helmert et al., 2011) (FDSS-2): sequential portfolioseveral search algorithms heuristics. Given results training benchmarks,best combination algorithms heuristics found hill-climbing search. Here,information communicated component solvers quality bestsolution found far.LAMA-2008 LAMA-2011 (Richter & Westphal, 2010; Richter, Westphal, & Helmert,2011) propositional planner based combination landmark count heuristicFF heuristic. search performs set weighted iteratively decreasing weights.planner developed within FD Planning System (Helmert, 2006).Lamar (Olsen & Bryce, 2011) modification LAMA planner includes randomized construction landmark count heuristic.Madagascar (Rintanen, 2011): implements several innovations SAT planning, includingcompact parallelized/interleaved search strategies SAT-based heuristics.Probe (Lipovetzky & Geffner, 2011): exploits idea wisely constructed lookaheadsprobes, action sequences computed without searching given statequickly go deep state space, terminating either goal failure. techniqueintegrated within standard greedy best first search.YAHSP2-MT (Vidal, 2011) extracts information relaxed plan order generatelookahead states. strategy implemented complete best-first search algorithm,modified take helpful actions account.LPG-td (Gerevini et al., 2006) based stochastic local search space particularaction graphs derived planning problem specification.ReferencesAmadini, R., Gabbrielli, M., & Mauro, J. (2014a). Portfolio approaches constraint optimizationproblems. TPLP, 8426, 2135.687fiC ENAMOR , DE LA ROSA & F ERN ANDEZAmadini, R., Gabbrielli, M., & Mauro, J. (2014b). SUNNY: lazy portfolio approach constraintsolving. TPLP, 14(4-5), 509524.Blum, A. L., & Langley, P. (1997). Selection relevant features examples machine learning.Artificial intelligence, 97(1), 245271.Bonet, B., & Geffner, H. (2000). Planning heuristic search: New results. Recent AdvancesAI Planning, pp. 360372. Springer.Bonet, B., Loerincs, G., & Geffner, H. (1997). robust fast action selection mechanismplanning. AAAI/IAAI, pp. 714719.Breiman, L. (1996). Bagging predictors. Machine Learning, 24(2), 123140.Breiman, L. (2001). Random forests. Machine learning, 45(1), 532.Cenamor, I., de la Rosa, T., & Fernandez, F. (2012). Mining IPC-2011 results. ProceedingsThird Workshop International Planning Competition - ICAPS.Cenamor, I., de la Rosa, T., & Fernandez, F. (2013). Learning predictive models configure planning portfolios. Proceedings Workshop Planning Learning - ICAPS.Censor, Y. (1977). Pareto optimality multiobjective problems. Applied Mathematics Optimization, 4(1), 4159.Dietterich, T. G. (2000). Ensemble methods machine learning. Kittler, J., & Roli, F.(Eds.), Multiple Classifier Systems, First International Workshop, MCS 2000, Cagliari, Italy,June 21-23, 2000, Proceedings, Vol. 1857 Lecture Notes Computer Science, pp. 115.Springer.Domshlak, C., Hoffmann, J., & Katz, M. (2015). Red-black planning: new systematic approachpartial delete relaxation. Artificial Intelligence, 221, 73114.Fawcett, C., Helmert, M., Hoos, H., Karpas, E., Roger, G., & Seipp, J. (2011). FD-Autotune:Domain-specific configuration using fast-downward. Proceedings WorkshopPlanning Learning - ICAPS, 2011(8).Fawcett, C., Vallati, M., Hutter, F., Hoffmann, J., Hoos, H. H., & Leyton-Brown, K. (2014). Improved features runtime prediction domain-independent planners. Proceedings24th International Conference Automated Planning Scheduling (ICAPS-14).Gerevini, A., Saetti, A., & Vallati, M. (2009). automatically configurable portfolio-based plannermacro-actions: PbP. Proceedings 19th International Conference AutomatedPlanning Scheduling (ICAPS-09).Gerevini, A., Saetti, A., & Serina, I. (2006). approach temporal planning schedulingdomains predictable exogenous events. Journal Artificial Intelligence Research, 25,187231.Gerevini, A., Saetti, A., & Vallati, M. (2014). Planning automatic portfolio configuration:PbP approach. Journal Artificial Intelligence Research, 50, 639696.Ghallab, M., Nau, D., & Traverso, P. (2004). Automated planning: theory & practice. Access Onlinevia Elsevier.Gomes, C. P., & Selman, B. (2001). Algorithm portfolios. Artificial Intelligence, 126(1), 4362.688fiT IBAC P P LANNING YSTEMGrabczewski, K., & Jankowski, N. (2005). Feature selection decision tree criterion. Proceedings Fifth International Conference Hybrid Intelligent Systems (HIS05), pp.212217. IEEE.Han, J., Kamber, M., & Pei, J. (2011). Data mining: concepts techniques. Elsevier.Helmert, M. (2004). planning heuristic based causal graph analysis. Proceedings14th International Conference Automated Planning Scheduling (ICAPS-04), Vol. 16,pp. 161170.Helmert, M. (2006). Fast Downward Planning System. Journal Artificial Intelligence Research, 26, 191246.Helmert, M. (2009). Concise finite-domain representations PDDL planning tasks. ArtificialIntelligence, 173, 503535.Helmert, M., & Domshlak, C. (2009). Landmarks, critical paths abstractions: Whats difference anyway?. Proceedings 19th International Conference AutomatedPlanning Scheduling (ICAPS-09).Helmert, M., & Geffner, H. (2008). Unifying causal graph additive heuristics. Proceedings 18th International Conference Automated Planning Scheduling (ICAPS08), pp. 140147.Helmert, M., Roger, G., Seipp, J., Karpas, E., Hoffmann, J., Keyder, E., Nissim, R., Richter, S.,& Westphal, M. (2011). Fast downward stone soup. Seventh International PlanningCompetition, IPC-7 planner abstracts, 38.Hoffmann, J. (2003). metric-FF planning system: Translating ignoring delete lists numericstate variables. Journal Artificial Intelligence Research, 20, 291341.Hoffmann, J. (2011). Analyzing search topology without running search: connectioncausal graphs h+. Journal Artificial Intelligence Research, 41, 155229.Hoffmann, J., Edelkamp, S., Thiebaux, S., Englert, R., dos Santos Liporace, F., & Trug, S. (2006).Engineering benchmarks planning: domains used deterministic part IPC-4.Journal Artificial Intelligence Research, 26, 453541.Hoffmann, J., & Nebel, B. (2001). FF planning system: Fast plan generation heuristicsearch. Journal Artificial Intelligence Research, 14, 253302.Hoos, H., Kaminski, R., Schaub, T., & Schneider, M. T. (2012). aspeed: ASP-based solver scheduling. ICLP (Technical Communications), 17, 176187.Howe, A. E., Dahlman, E., Hansen, C., Scheetz, M., & von Mayrhauser, A. (1999). Exploitingcompetitive planner performance. Biundo, S., & Fox, M. (Eds.), Recent Advances AIPlanning, 5th European Conference Planning, ECP99, Durham, UK, September 8-10,1999, Proceedings, Vol. 1809 Lecture Notes Computer Science, pp. 6272. Springer.Hutter, F., Hoos, H. H., & Leyton-Brown, K. (2011). Sequential model-based optimizationgeneral algorithm configuration. Learning Intelligent Optimization, pp. 507523.Springer.Hutter, F., Hoos, H. H., Leyton-Brown, K., & Stutzle, T. (2009). ParamILS: automatic algorithmconfiguration framework. Journal Artificial Intelligence Research, 36, 267306.689fiC ENAMOR , DE LA ROSA & F ERN ANDEZHutter, F., Xu, L., Hoos, H., & Leyton-Brown, K. (2015). Algorithm runtime prediction: Methodsevaluation (extended abstract). Yang, Q., & Wooldridge, M. (Eds.), ProceedingsTwenty-Fourth International Joint Conference Artificial Intelligence, IJCAI 2015, BuenosAires, Argentina, July 25-31, 2015, pp. 41974201. AAAI Press.Kohavi, R. (1995). power decision tables. Machine Learning: ECML-95, pp. 174189.Springer.Linares Lopez, C., Celorrio, S. J., & Olaya, A. G. (2015). deterministic part seventhinternational planning competition. Artificial Intelligence, 223, 82119.Lindauer, M. T., Hoos, H. H., & Hutter, F. (2015a). sequential algorithm selection parallelportfolio selection. Dhaenens, C., Jourdan, L., & Marmion, M. (Eds.), Learning Intelligent Optimization - 9th International Conference, LION 9, Lille, France, January 12-15,2015. Revised Selected Papers, Vol. 8994 Lecture Notes Computer Science, pp. 116.Springer.Lindauer, M. T., Hoos, H. H., Hutter, F., & Schaub, T. (2015b). Autofolio: automatically configured algorithm selector. Journal Artificial Intelligence Research, 53, 745778.Lipovetzky, N., & Geffner, H. (2011). Searching plans carefully designed probes.Proceedings 21st International Conference Automated Planning Scheduling(ICAPS-11), pp. 154161.Malitsky, Y., Sabharwal, A., Samulowitz, H., & Sellmann, M. (2013). Algorithm portfolios basedcost-sensitive hierarchical clustering. Proceedings Twenty-Third internationaljoint conference Artificial Intelligence, pp. 608614. AAAI Press.Nakhost, H., Muller, M., Valenzano, R., & Xie, F. (2011). Arvand: art random walks.Seventh International Planning Competition, IPC-7 planner abstracts, 1516.Nebel, B. (2000). compilability expressive power propositional planning formalisms.Journal Artificial Intelligence Research, 12, 271315.Nunez, S., Borrajo, D., & Linares Lopez, C. (2015). Automatic construction optimal staticsequential portfolios AI planning beyond. Artificial Intelligence, 226, 75101.Olsen, A., & Bryce, D. (2011). Randward Lamar: Randomizing FF heuristic. SeventhInternational Planning Competition, IPC-7 planner abstracts, 55.OMahony, M. (1986). Sensory evaluation food: statistical methods procedures, Vol. 16.CRC Press.Quinlan, J. R. (1993). C4. 5: programs machine learning, Vol. 1. Morgan kaufmann.Richter, S., Helmert, M., & Westphal, M. (2008). Landmarks revisited. AAAI, Vol. 8, pp. 975982.Richter, S., & Westphal, M. (2010). LAMA planner: Guiding cost-based anytime planninglandmarks. Journal Artificial Intelligence Research, 39(1), 127177.Richter, S., Westphal, M., & Helmert, M. (2011). Lama 2008 2011. Seventh InternationalPlanning Competition, IPC-7 planner abstracts, 50.Rintanen, J. (2011). Madagascar: Efficient planning SAT. Seventh International PlanningCompetition, IPC-7 planner abstracts, 61.690fiT IBAC P P LANNING YSTEMRoberts, M., & Howe, A. (2009). Learning planner performance. Artificial Intelligence, 173,536561.Roberts, M., Howe, A. E., Wilson, B., & desJardins, M. (2008). makes planners predictable?.Proceedings 18th International Conference Automated Planning Scheduling (ICAPS-08), pp. 288295.Rodriguez, J. J., Kuncheva, L. I., & Alonso, C. J. (2006). Rotation forest: new classifier ensemblemethod. IEEE Transactions Pattern Analysis Machine Intelligence, 28(10), 16191630.Schwefel, H.-P., Wegener, I., & Weinert, K. (2013). Advances computational intelligence: Theorypractice. Springer Science & Business Media.Seipp, J., Braun, M., Garimort, J., & Helmert, M. (2012). Learning portfolios automatically tunedplanners. McCluskey, L., Williams, B., Silva, J. R., & Bonet, B. (Eds.), ProceedingsTwenty-Second International Conference Automated Planning Scheduling, ICAPS2012, Atibaia, Sao Paulo, Brazil, June 25-19, 2012. AAAI.Seipp, J., Sievers, S., Helmert, M., & Hutter, F. (2015). Automatic configuration sequentialplanning portfolios. Bonet, B., & Koenig, S. (Eds.), Proceedings Twenty-NinthAAAI Conference Artificial Intelligence, January 25-30, 2015, Austin, Texas, USA., pp.33643370. AAAI Press.Vallati, M. (2012). guide portfolio-based planning. Multi-disciplinary Trends ArtificialIntelligence, pp. 5768. Springer.Vallati, M., Chrpa, L., & Kitchin, D. E. (2015). Portfolio-based planning: State art, commonpractice open challenges. AI Communications, 29, 117.Vallati, M., Chrpa, L., & McMcluskey, L. (2014a).https://helios.hud.ac.uk/scommv/IPC-14/benchmark.html.CompetitionDomains.Vallati, M., Chrpa, L., & McMcluskey, L. (2014b). Source code Erratum Deterministic part.https://helios.hud.ac.uk/scommv/IPC-14/errPlan.html.Vidal, V. (2011). YAHSP2: Keep simple, stupid. Seventh International Planning Competition,IPC-7 planner abstracts, 83.Witten, I. H., & Frank, E. (2005). Data Mining: Practical Machine Learning Tools Techniques.2nd Edition, Morgan Kaufmann.Xie, F., Muller, M., & Holte, R. (2014). Jasper: art exploration greedy best first search.Planner abstracts. IPC-2014.Xu, L., Hoos, H., & Leyton-Brown, K. (2010). Hydra: Automatically configuring algorithmsportfolio-based selection. Proceedings Twenty-Fourth AAAI Conference ArtificialIntelligence (AAAI 2010), pp. 210216.Xu, L., Hutter, F., Hoos, H., & Leyton-Brown, K. (2012). Evaluating component solver contributions portfolio-based algorithm selectors. Theory Applications SatisfiabilityTestingSAT 2012, pp. 228241. Springer.Xu, L., Hutter, F., Hoos, H. H., & Leyton-Brown, K. (2008). SATzilla: Portfolio-based algorithmselection SAT. Journal Artificial Intelligence Research, 32, 565606.691fiJournal Artificial Intelligence Research 56 (2016) 329-378Submitted 12/15; published 06/16DL-Lite Contraction RevisionZhiqiang Zhuangz.zhuang@griffith.edu.auInstitute Integrated Intelligent SystemsGriffith University, AustraliaZhe Wangzhe.wang@griffith.edu.auSchool Information Communication TechnologyGriffith University, AustraliaKewen Wangk.wang@griffith.edu.auSchool Information Communication TechnologyGriffith University, AustraliaGuilin Qigqi@seu.edu.cnSchool Computer Science EngineeringSoutheast University, ChinaState Key Lab Novel Software TechnologyNanjing University, ChinaAbstractTwo essential tasks managing description logic knowledge bases eliminating problematic axioms incorporating newly formed ones. elimination incorporationformalised operations contraction revision belief change. paper,deal contraction revision DL-Lite family model-theoreticapproach. Standard description logic semantics yields infinite number modelsDL-Lite knowledge bases, thus difficult develop algorithms contraction revision involve DL models. key approach introduction alternativesemantics called type semantics replace standard semantics characterisingstandard inference tasks DL-Lite. Type semantics several advantagesstandard one. succinct importantly, finite signature, semanticsalways yields finite number models. define model-based contractionrevision functions DL-Lite knowledge bases type semantics provide representation theorems them. Finally, finiteness succinctness type semantics allowus develop tractable algorithms instantiating functions.1. IntroductionDescription logic (DL) (Baader, Calvanese, McGuinness, Nardi, & Patel-Schneider, 2003)knowledge bases (KBs) subject frequent change. instance, outdated incorrectaxioms eliminated KBs newly formed ones incorporatedthem. Therefore mandatory task managing DL KBs deal changes.field belief change, extensive work done formalising various kindschanges KBs. particular, elimination old knowledge called contractionincorporation new knowledge called revision. deal changes DL KBs,makes sense take advantage existing techniques belief change. fact, manyinvestigated contraction revision DL KBs (DL contraction DL revisionc2016AI Access Foundation. rights reserved.fiZhuang, Wang, Wang, & Qishort) (Qi et al., 2006; Qi & Du, 2009; Qi et al., 2008; Ribeiro & Wassermann, 2009;Wang et al., 2015).dominant approach belief change called AGM framework (Alchourron,Gardenfors, & Makinson, 1985; Gardenfors, 1988). framework, KBchanges made called belief set logically closed set formulas. AGM.contraction function takes input belief set K formula returns output.belief set K entail . Taking inputs, AGM revision functionreturns belief set K entails . framework also provides rationality postulatescapture intuitions behind rational contraction revision. hallmarkframework called representation theorems proved ensure AGMcontraction revision functions sound also complete respectrationality postulates.Regardless wide acceptance, limitation AGM frameworkminimal requirement underlying logic, logic subsumes classical propositionallogic. means underlying logic must fully support truth functional logicalconnectives negation disjunction. Since DLs so, AGM framework incompatible DLs cannot applied directly deal changesDLs KBs. incompatibility major difficulty defining DL contraction revision. Additionally, DL revision involved AGM revision. AGM revision aimsresolve inconsistency caused incorporating new formula. Since meaningful DL KB consistent coherent (i.e, absence unsatisfiable concepts),DL revision resolve inconsistency also incoherence. Finally, despitemathematical elegance, AGM framework grappled issue computational efficiency crucially important DL KBs. Therefore, DL contractionrevision lead tractable instantiations time respectingrationality postulates AGM contraction revision.Due many difficulties, existing works DL contraction revision fullysatisfactory. None provides representation theorem contraction revisionfunction except work Ribeiro Wassermann (2009) inherits representation results general work (Hansson & Wassermann, 2002).1 defining DLrevision, many appreciate incoherence resolving nature, revisions cannotguarantee coherence outputs (Qi et al., 2006; Ribeiro & Wassermann, 2009; Wanget al., 2015). Qi Du (2009) appreciate incoherence resolving nature, postulates provided capturing properties revision function formulatedappropriately capture rationales behind incoherence resolving.paper, provide DL contraction revision overcome limitations.Specifically, define contraction revision functions logically closed DL-LitecoreDL-LiteR KBs. DL-Litecore DL-LiteR main languages DL-Lite family(Calvanese et al., 2007). defining functions take model-theoretic approach1. Hansson Wassermann (2002) proved series representation theorems contraction revisionfunctions monotonic compact logics. Since Ribeiro Wassermann (2009) consideredcontraction revision functions DLs monotonic compact, representationresults Hansson Wassermann (2002) also hold contraction revision functions. Sinceapproach defining contraction revision different Hansson Wassermann (2002),inherit representation results prove scratch.330fiDL-Lite Contraction Revisionsimilar Katsuno Mendelzon (1992). Instead DL models functionsbased models newly defined semantics DL-Lite called type semantics. Giventype semantics equivalent DL semantics respect major DL-Lite reasoningtasks, models type semantics (i.e., type models) succinct DL models.importantly, given finite signature, DL-Lite KB finite number type models,whereas usually infinite number DL models.fully appreciate incoherence resolving nature DL revision reflectdefinition revision functions postulates capturing properties.able prove AGM-style representation theorems contraction revisionfunctions. theorems crucial guarantee functions definedmethod behave properly (in sense satisfying set commonly accepted postulates)properly behaved functions defined method. additionrigorous mathematical properties, provide tractable algorithms instantiatingcontraction revision functions.material paper presented previously Zhuang, Wang, Wang,Qi (2014).2. DL-LiteDL-Litecore core language DL-Lite family. following syntaxB | RR P | PC B | BE R | Rdenotes atomic concept; P atomic role, P inverse atomic role P ; Bbasic concept either atomic concept unqualified existential quantification;C general concept either basic concept negation; R basic roleeither atomic role inverse; E general role either basic rolenegation. also include language nullary predicates > denoteuniversal false universal truth respectively. assume set basic concepts,denoted B, set basic roles, denoted R, finite. inverse roleR = P , write R represent P .DL-Litecore KB consists TBox ABox. sometime denote KB(T , A) TBox KB ABox KB. TBox finite setconcept inclusions form B v C, B v , > v C. basic concept> appear left-hand side concept inclusion. ABox finite setconcept assertions form A(a) role assertions form P (a, b),atomic concept, P atomic role, a, b individuals. assume set individuals,denoted D, finite. Throughout paper, individual names denoted lower caseRoma letters (a, b, . . .).major extension DL-Litecore DL-LiteR . extends DL-Litecore role inclusions form R v E. basic roles appear left-hand side roleinclusion. concept role inclusion also called TBox axiom concept roleassertion also called ABox axiom. Throughout paper, TBox ABox axiomsdenoted lower case Greek letters (, , . . .).semantics DL-Lite language given terms interpretations. interpretation = (I , ) consists nonempty domain interpretation function331fiZhuang, Wang, Wang, & Qiassigns atomic concept subset AI , atomic role P binaryrelation P , individual element aI . interpretationfunction extended general concept, general roles, special symbols follows:=>I =(P )I = {(b, a) | (a, b) P }(R)I = {a | b.(a, b) RI }(B)I = \ B(R)I = \ RIset interpretations denoted . interpretation satisfies conceptinclusion B v C B C , role inclusion R v E RI E , concept assertion A(a)aI AI , role assertion P (a, b) (aI , bI ) P .satisfies KB K (a TBox ABox A) satisfies axioms K (resp. ,A). model KB K (a TBox , ABox A, axiom ) written |= K, (resp.|= , |= A, |= ) satisfies K (resp. , A, ). denote modelsKB K (a TBox , ABox A, axiom ) |K| (resp. |T |, |A|, ||). Two axiomslogically equivalent, written , identical set models.KB K (a TBox , ABox A, axiom ) entails axiom , written K |= (resp.|= , |= , |= ), models K (resp. , A, ) also models . KB(a TBox, ABox axiom) consistent least one model inconsistentotherwise. use K , , denote respectively (unique) inconsistent KB, TBox,ABox. use |= denote tautology (e.g., v A) 6|=one.closure TBox , denoted cl(T ), set TBox axioms|= . say TBox closed = cl(T ). closure DL-Lite TBoxfinite. Actually, size cl(T ) quadratic respect size (Pan & Thomas,2007). closure ABox respect TBox , denoted clT (A), setABox axioms (T , A) |= . say ABox closed respect= clT (A). closure ABox respect TBox DL-Lite finitecomputed efficiently chase procedure (Calvanese et al., 2007). Section4 Section 5, TBoxes ABoxes assumed closed.basic concept B satisfiable respect TBox modelB non-empty unsatisfiable otherwise. easy see B unsatisfiablerespect B v cl(T ). TBox coherent basic conceptssatisfiable incoherent otherwise.2defining contraction revision functions DL-Lite KBs, need refernotion conjunction axioms. Given set axioms {1 , . . . , n }, conjunctiondenoted 1 n . interpretation model 1 n satisfiesconjuncts |1 n | = |1 | |n |.2. DL literatures, often coherence comes absence unsatisfiable atomic concepts. SinceDL-Lite unsatisfiable non-atomic concepts like R also unexpected use stricter conditioncoherence.332fiDL-Lite Contraction Revision3. Type Semanticssection, provide alternative semantics DL-Lite, namely type semantics.short, type semantics takes semantics underlying propositional logic (i.e., propositional semantics) basis equips extra facilities take care nonpropositional behaviours DL-Lite.first introduce simplified version type semantics called ct-type semantics (ctstands Core TBox), sufficient characterising standard inference tasksDL-Litecore TBoxes. important consideration defining semantics succinctness,defined semantics avoid redundancy. consideration, cttype semantics facility required characterising inferencetasks DL-Litecore TBoxes. Accordingly, DL-Litecore TBoxes allow inferencesinvolve role inclusions ABox axioms, ct-type semantics incapable characterisinginferences. Next, extend ct-type semantics facilities role inclusionsresults another simplified version type semantics called t-type semantics (tstands TBox). semantics sufficient characterising standard inferencetasks DL-LiteR TBoxes. Again, sake succinctness, t-type semantics intendedcapture inference tasks DL-LiteR TBoxes only, thus incapable characterisinginvolving ABox axioms. inferences, introduce a-type semantics (astands ABox). semantics sufficient DL-LiteR ABoxes (with backgroundTBox). also simplified version type semantics, built upon ct-typet-type semantics. Finally, introduce full version type semantics, sufficientDL-LiteR KBs. semantics includes facilities t-type a-type semantics.Figure 1 shows hierarchy type semantics.typet-typect-typea-typeFigure 1: rectangle represents semantics, largest representing type semantics. rectangle containing one smaller ones indicates representedsemantics larger rectangle subsumes smaller ones.333fiZhuang, Wang, Wang, & Qipropositional origin assumption finite signature guarantee finitenesstype semantics. mentioned, important consideration defining type semanticssuccinctness. succinct semantics efficient computations involving models. Type semantics replace ct-type semantics, t-type semantics, a-typesemantics characterising DL-Litecore TBoxes, DL-LiteR TBoxes, DL-LiteR ABoxes;t-type semantics replace ct-type semantics characterising DL-Litecore TBoxes. However, waste computational power use type semantics characteriseinstance DL-LiteR TBoxes facilities ABox axioms redundant DL-LiteRTBoxes. holds using t-type semantics characterising DL-Litecore TBoxesfacilities t-type semantics role inclusions redundant DL-Litecore TBoxes.finiteness succinctness significant advantages type semantics DL semantics DL-Lite KBs need represented model-theoretically relatedcomputational tasks involve models.Depending application scenarios, changes DL-Lite KBs may applied(1) whole KB restricted either (2) TBox (3) ABox background(unchanged) TBox. take model-theoretic approach addressing changes,suitable semantics scenario (1) type semantics; scenario (2) ct-typet-type semantics; scenario (3) a-type semantics.3.1 Characterising DL-Litecore TBoxesstart ct-type semantics. Standard inference tasks DL-Lite TBoxeschecking satisfiability, subsumption, equivalence, disjointness, consistency reduced checking whether entailment relationship holds TBoxaxioms. Given TBox , basic concept B satisfiableentail B v ; subsumed B entails v B; Bequivalent entails v B B v A; disjoint Bentails v B; inconsistent |= > v . reason,defining ct-type (t-type) semantics, suffices focus entailment relationshipsDL-Litecore (resp. DL-LiteR ) TBox axioms.propositional semantics standard DL semantics, notion interpretations. Analogously, type semantics, central notion types.3 definitiontype given Section 3.4. ct-type semantics, need simplified version,called ct-type. ct-type possibly empty set basic concepts (i.e., B).example, B = {R, R , A}, {R, A} ct-type simplicity sometimeswrite RA.4 denote set ct-types tc . consider basic conceptspropositional atoms, concept inclusion B v C propositional formula B C,ct-type nothing interpretation (represented atoms interpreted true) propositional logic. Given DL-Litecore TBox , use kT ktc denote set propositionalmodels corresponding propositional formulas . Note kT ktc tc .Many entailment relationships DL-Litecore TBox axioms propositionalsense entailments also hold treat axioms propositional formulas3. notion types mentioned work Kontchakov et al. (2010), similar structuresct-types paper cannot capture role inclusions ABox axioms.4. work DL-Lite throughout paper. Since DL-Lite allow quantified existential quantifications R.C, ct-type RA cannot confused concept R.A334fiDL-Lite Contraction Revisionconsider propositional semantics. example, entailment v B B v Cv C also holds corresponding propositional formulas B, B C,C, propositional semantics. expected entailmentspropositional. following example shows common pattern non-propositionalentailments. Note that, R v R v entail one another but, propositionalsemantics, corresponding propositional formulas R R not. reasonsimple. DLs, role R represents binary relation axiom R vR v indicate relation empty. Propositional logic facilitybinary relations entailments involving relations, thus characteriseentailments.clear, ct-type semantics, need facilities propositional semanticscharacterise propositional entailments extra one characterise nonpropositional ones. capture facilities conditions ct-typesatisfies DL-Litecore TBox. DL semantics, interpretations satisfying TBox calledmodels TBox. Analogously, ct-type satisfying DL-Litecore TBox called ctmodel TBox.Definition 1 ct-model DL-Litecore TBox ct-type1. kT ktc2. |= R v R 6 .ct-type satisfy TBox , firstly propositional model secondlyentails R v , contain basic concept R. first conditionguarantees proper handling propositional entailments second guaranteesnon-propositional entailments.Example 1 Consider fragment (slightly modified) NCI KB concerning heart diseasesassociated anatomic locations, consists concepts Heart Disease (HD),Cardiovascular System (CS), Respiratory System (RS), Organ System (OS), wellrole relates diseases primary locations Disease Primary Anatomic Site(Loc). Let DL-Litecore TBox consist following concept inclusions: HD v Loc,Loc v CS, HD v OS, RS v OS, CS v OS, RS v CS.ct-models 1 = {HD, Loc}, 2 = {Loc , CS, OS}, 3 = {RS, OS}.concept inclusion Loc v RS , |= Loc v |= Loc v ,neither 1 2 ct-model .denote set ct-models TBox |T |tc . ct-models conjunctionaxioms 1 2 n , denoted |1 2 n |tc , defined|1 2 n |tc = |{1 , 2 , . . . , n }|tc .ct-models negated (conjunctions of) axiom(s) , denoted ||tc , definedtc \ ||tc .335fiZhuang, Wang, Wang, & Qinotions entailment, logical equivalence, consistency ct-type semanticsdefined manner DL semantics. ct-type semantics, TBox entailingconjunction axioms written |=tc .make non-propositional behaviour ct-type semantics explicit, proposefollowing notion role-complete set ct-types. set ct-types role-complete if,R R, whenever ct-type R , ct-type 0R 0 ( 0 may identical). Roughly speaking, role-completenessindicates set ct-types complete information role R.show set ct-models DL-Litecore TBox role-complete.Proposition 1 Let DL-Litecore TBox. |T |tc role-complete.show connections DL models ct-models DLLitecore TBox. Let DL interpretation. element domain I,induces unique ct-type follows(I, d) = {B B | B }.call (I, d) ct-type induced I. model TBoxct-type induced ct-model TBox.Proposition 2 Let DL-Litecore TBox DL interpretation. |T | iff(I, d) |T |tc .Moreover, ct-model TBox, DL model TBox constructedreversing inducing process.Proposition 3 Let DL-Litecore TBox ct-model . |T |(I, d) = .connections, prove entailments DL-Litecore TBoxesaxioms induced ct-type semantics identical induced DL semantics.Theorem 1 Let DL-Litecore TBox conjunction DL-Litecore TBox axioms.|= iff |=tc .Thus ct-type semantics effective DL semantics characterising standard inferences tasks DL-Litecore TBoxes. comparison DL semantics, ct-type semanticsclear advantage finite succinct. DL-Litecore TBox usuallyinfinite number DL models always finite number ct-models.Proposition 4 Let DL-Litecore TBox. 2n ct-models, nnumber basic concepts.working coherent TBox , ct-type semantics shares oneproperty DL semantics, set ct-models identical intersectionset ct-models axiom . property turns essentialdeveloping algorithms eliminating incorporating axioms DL-Lite KBs. allowsus deal axiom one one model-theoretic setting.336fiDL-Lite Contraction RevisionTheorem 2 Let DL-Litecore TBox = {1 , . . . , n }. coherent,|T |tc = |1 |tc |n |tc .shown, ct-type semantics shares many crucial properties DL semantics,however differs DL one dealing unions axioms.Theorem 3 Let DL-Litecore TBox , DL-Litecore TBox axioms. |T ||| || |T |tc ||tc ||tc converse necessarily hold.counter example, suppose B {A, B, C, D}, {A v D}, v B,C v D. Lets work ct-models. ct-type satisfy vcontains D, get ct-models eliminating unsatisfyingct-types set ct-types, |T |tc = c \ {AB, AC, ABC}. Similarly, ct-typesatisfy v B C v contains C B D,||tc ||tc = c \ {AC}. Note |T |tc ||tc ||tc . let DL interpretation= {a, b}, AI = {a}, B = {b}, C = {b}, DI = {a}. Since |= ,6|= v B, 6|= C v D, |T | 6 ||||. Roughly speaking, reasonbehaviour type semantics (and simplified versions) variant propositionalsemantics lacks first-order structure DL semantics. Identificationbehaviour turns crucial proving representation theorem contractionfunctions.DLs inexpressibility problem sets DL interpretationssyntactic representation. exception DL-Litecore ct-type semantics. Givenset ct-types , may DL-Litecore TBox whose set ct-models .cases, define corresponding DL-Litecore TBox oneminimal set ct-models including .Definition 2 Let set ct-types. DL-Litecore TBox corresponding DL0Litecore TBox iff |T |tc DL-Litecore TBox0|T |tc |T |tc .Given set ct-types, may several corresponding TBoxes. Let B consists{R, R , A} set ct-types consists A, , R. Notect-type contains R none contains R . Proposition 1,TBox whose set ct-models contains must ct-model contains R . Since,current set basic concepts B, four ct-types containing RR , R A, R R, R AR, four corresponding TBoxes{A v R, v R , R v R }, {A v R, R v A, R v R }, {A v R, vR , R v R}, {R v A, R v R}, onect-types ct-models.shown example, R ct-types R not,several ways generating corresponding TBox. Intuitively concepts,dont many choices one generating corresponding TBox. Clearly,role-complete show role-completeness sufficient guaranteeuniqueness corresponding TBox.Theorem 4 Let set ct-types. role-complete, uniquecorresponding DL-Litecore TBox .337fiZhuang, Wang, Wang, & Qi3.2 Characterising DL-LiteR TBoxesCt-type semantics able characterise entailments DL-Litecore TBox axioms,DL-LiteR ones, involve role inclusions. subsection,present t-type semantics able so.characterise entailments involving role inclusions, need introduce copyB 0 set basic concepts B copy R0 set basic roles R. So,B = {A, R, R } R = {R, R }, B 0 = {A0 , (R)0 , (R )0 } R0 = {R0 , (R )0 }.also need notion extension DL-LiteR TBox. Let DL-LiteR TBox.extension, denoted , TBox obtained adding new conceptinclusion concept inclusion B v C new role inclusion R0 v E 0role inclusion R v E . Note C = B, C 0 denotes B 0 ; E = R,E 0 denotes R0 .t-type possibly empty set basic concepts, basic roles, copies (i.e.,B R B 0 R0 ). Intuitively, t-type combines two ct-types (for pairs individuals)set roles (between pairs individuals). pair individuals a, b(a, b) relation captured role R, B part t-type aims characteriseconstraints (in way ct-type characterises constraintsindividual); B 0 part aims characterises constraints b (in wayct-type characterises constraints individual); R R0 part aimscharacterise constraints R (which ct-type consider). denoteset t-types tr .consider basic concepts, basic roles, copies propositional atoms,concept inclusion B v C role inclusion R v E propositional formulas B CR E, t-type nothing interpretation (represented atoms interpretedtrue) propositional logic. DL-LiteR TBox , use kT ktr denote setpropositional models corresponding propositional formulas . Note kT ktrtr .DL-Litecore permits non-propositional entailments, DL-LiteR .one group non-propositional entailments DL-Litecore , four identifiedDL-LiteR . (1) Apart implying R v , R v also implies R v R R vR . (2) role inclusion R v implies concept inclusion R v S, R v ,role inclusion R v . (3) role inclusion R v implies R v . (4)concept inclusion R v implies role inclusion R v S.following, give conditions t-type satisfies DL-LiteR TBox. call t-types t-models TBox.Definition 3 t-model DL-LiteR TBox t-type1. kT ktr ;2. |= R v R 6 (R)0 6 ;3. |= R v R implies , (R)0 implies (S)0 ;4. R R (R )0 ;5. R iff (R )0 R R.338fiDL-Lite Contraction Revisiont-type satisfy , firstly propositional model , takes carepropositional entailments. conditions 25 take care four groups nonpropositional entailments summarised earlier. Conditions 4 5 required t-typet-model (independent TBox), referred model conditionst-type semantics. Note use copies basic concepts basic roles necessary.Consider TBox two axioms R v v . entails R vR v (by (3) (4)). would expect t-models also satisfy R v S. copiesdiscarded, t-type = {R, S, R, S} would t-model (omitting R0 (R)0Definition 3), yet clearly satisfy R v S. cannot resolved addingcondition Definition 3 (for details refer proof Lemma 7 Appendix B).Example 2 (contd Example 1) Consider another role associates diseasesanatomic sites, Disease Associated Anatomic Site (Das), DL-LiteR TBox obtained adding role inclusion Loc v Das .t-models 10 = {HD, Loc, Das, Loc, Das, (Loc )0 , (Das )0 , (Loc )0 , (Das )0 ,(CS)0 , (OS)0 }, 20 = {Loc , Das , CS, OS, Das , (Das)0 , (Das)0 }, 30 = {RS, OS}.Given DL-LiteR TBox , denote set t-models |T |tr . t-modelsconjunction DL-LiteR TBox axioms denoted defined mannerct-type semantics. t-models negated (conjunction of) axiom(s) ,5 denoted||tr , defined{ tr \ ||tr | satisfies model conditions}.notions entailment, logical equivalence, consistency t-type semanticsdefined manner DL semantics. t-type semantics, TBox entailingconjunction axioms written |=tr .DL-Litecore , establish connection DL models t-modelsDL-LiteR TBox. Let DL interpretation d, e pair (not necessarilydistinct) elements domain I. I, e induce t-type follows(I, d, e) ={B B | B } {R R | (d, e) RI }{B 0 B 0 | e B } {R0 R0 | (e, d) RI }.call (I, d, e) t-type induced e I. show DLinterpretation I, DL model TBox t-type inducedt-model TBox.Proposition 5 Let DL-LiteR TBox DL interpretation. |T |iff (I, d, e) |T |tr pair d, e .Moreover, given t-model TBox , DL model constructedreversing inducing process.5. simplicity, assume Definition 3 apply tautologies define, tautologicalaxiom , ||tr = tr ||tr = .339fiZhuang, Wang, Wang, & QiProposition 6 Let DL-LiteR TBox t-model . |T |d, e (I, d, e) = .connections, prove t-type semantics induces setentailments DL-LiteR TBox axioms induced DL semantics.Theorem 5 Let DL-LiteR TBox conjunction DL-LiteR TBox axioms.|= iff |=tr .Extended roles copies basic concepts, number t-typesct-types. However, compared DL semantics, t-type semantics stilladvantage finite succinct.Proposition 7 Let DL-LiteR TBox. 22n+2m t-models, nnumber basic concepts basic roles.sets ct-types, proposed condition called role-complete. condition characterises property ct-type semantics set ct-models DL-LitecoreTBox role-complete, role-complete set ct-types corresponds unique DLLitecore TBox. give corresponding role-complete condition sets t-types.set t-types role-complete t-types satisfy model conditionst-type semantics, R R, whenever t-type R(R)0 , t-type 0 {R, R0 } 0 6= ( 0 mayidentical).set t-types , corresponding DL-LiteR TBoxes definedway set ct-types. Also shown analogously |T |tr role-completeDL-LiteR TBox role-complete guarantees existence uniquecorresponding DL-LiteR TBox.Theorem 6 Let set t-types. role-complete, uniquecorresponding DL-LiteR TBox .Moreover, properties ct-type semantics conjunctions unions axioms (i.e., Theorem 2 Theorem 3) also hold t-type semantics.far shown t-type semantics possesses every property ct-type semantics,except number possible models. connections? fact, t-typesemantics generalises ct-type semantics sense DL-Litecore TBox ,ct-models exactly B-projections t-models .Proposition 8 Let DL-Litecore TBox. |T |tc = { B | |T |tr }.Hence, t-types contain information ct-types, enoughcapture semantics DL-Litecore TBoxes.Finally, extend notion coherence sets t-types. set t-typescoherent t-types satisfy model conditions t-type semantics,satisfy B v R v R B B R R (i.e.,6 |B v |tr 6 |R v R|tr ). coherent set t-types, every basic340fiDL-Lite Contraction Revisionconcept role contained t-types . Therefore coherent set t-typesalways role-complete. clear that, defining contraction revision functionsDL-LiteR TBoxes, sets t-types encounter always coherent thusrole-complete means always unique corresponding TBoxes. let Trfunction takes input set t-types coherent,Tr (M ) closure corresponding DL-LiteR TBox, otherwise Tr (M ) = .3.3 Characterising DL-LiteR ABoxes (with Background TBox)T-type semantics extends ct-type semantics ability characterising entailmentsinvolving role inclusions. them, however, incapable characterising entailmentsABox axioms. subsection, introduce a-type semantics ableso. TBoxes, also reduce standard inferences tasks ABoxeschecking entailment relationships ABox axioms. Thus defining a-typesemantics, suffices focus entailment relationships.Although focus entailments ABox axioms, important noteentailments induced background TBox. example, A(a) entails B(a),must background TBox entails v B. sake simplicity,sometimes denote ABox indicate background TBox .TBox captures subsumption relationships concepts (i.e., concept inclusions)roles (i.e., role inclusions) whereas ABox captures assertionsindividuals (i.e., concept assertions) pairs individuals (i.e., role assertions).ABox, individual asserted element basic concept pairindividuals asserted element basic role. end, introducecopy B B copy Rab R pair a, b D. So,B = {A, R, R }, R = {R, R }, = {a, b}, B = {Aa , Ra , (R )a }, B b ={Ab , Rb , (R )b }, Rab = {Rab , (R )ab }, Rba = {Rba , (R )ba }, Raa = {Raa , (R )aa },Rbb = {Rbb , (R )bb }.empty set copies basic conceptsa-typepossiblyabroles (i.e., aD B a,bD R ). denote set a-types ar .DL-LiteR TBox , let TBox consists conceptinclusion B v C individual D, concept inclusion B v C ,role inclusion R v E pair individuals a, b D, role inclusion Rab v E ab .consider copies basic concepts roles propositional atoms, concept inclusionB v C propositional formula B C, role inclusion R v E propositional formulaR E, concept assertion A(a) propositional formula Aa , (i.e., atomic formula)role inclusion P (a, b) propositional formula P ab , (i.e., atomic formula) a-typenothing interpretation (represented atoms interpreted true) propositionallogic. DL-LiteR ABox use kAT kar denote set propositional modelscorresponding propositional formulas A. Note kAT kar ar .Since entailments axioms ABox lot axiomsbackground TBox , embed facilities t-type semantics6 a-typesemantics. considerations, conditions a-type satisfiesDL-LiteR ABox defined follows call a-types a-models ABox.6. Note conditions 15 Definition 4 adapted Definition 3.341fiZhuang, Wang, Wang, & QiDefinition 4 a-model DL-LiteR ABox a-type1. kAT kar ;2. |= R v (R)a 6 D;3. |= R v (R)a implies (S)a D;4. Rab (R)a (R )b ;5. Rab iff (R )ba R R pair a, b D.Similarly, conditions 4 5 referred model conditions a-type semantics.Example 3 (contd Example 2) Consider DL-LiteR KB K = (T , A) consistsconcept assertion HD(d) role assertion Loc(d, s). a-model 00 ={HDd , (Loc)d , (Das)d , Locds , Dasds , (Loc )sd , (Das )sd , (Loc )s , (Das )s , CSs , OSs }.denote set a-models DL-LiteR ABox |AT |ar . a-modelsconjunction 1 2 n DL-LiteR ABox axioms respect background TBox, denoted |(1 2 n )T |ar , defined|(1 2 n )T |ar = |{1 , 2 , . . . , n }T |ar .a-models negated (conjunction of) axiom(s) respect background TBox, denoted |T |ar , defined|T |ar \ |T |ar|T |ar set a-models empty ABox background TBox .notions entailment, logical equivalence, consistency a-type semanticsdefined manner DL semantics. a-type semantics, ABox entailingconjunction ABox axioms written |=ar .establish connection DL models a-models DL-LiteRABox, connection tighter one. Compared ct-type t-type semantics, atype semantics contains information individuals, thus closer DL interpretation.reason, DL interpretation induces exactly one a-type. inducing processfollows.(I) ={B c B c | c D, cI B } {Rcb Rcb | c, b D, (cI , bI ) RI }.call (I) a-type induced I. show DL interpretation I,model KB K = (T , A) model a-type induceda-model ABox .Proposition 9 Let K = (T , A) DL-LiteR KB DL interpretation.|K| iff |T | (I) |AT |ar .Moreover, given a-model ABox , DL model KB K = (T , A)constructed reversing inducing process.342fiDL-Lite Contraction RevisionProposition 10 Let K = (T , A) DL-LiteR KB a-type. |AT |ar ,|K| (I) = .connections, show a-type semantics induces setentailments DL-LiteR ABox axioms (with background TBox) induced DLsemantics.Theorem 7 Let K = (T , A) DL-LiteR KB, conjunction DL-LiteR ABox axioms.K |= iff |=ar .Although include multiple copies basic concepts roles capture DL-LiteRABoxes, semantics still superior DL semantics terms finitenesssuccinctness.2Proposition 11 Let DL-LiteR ABox. 2nm+n l a-models,n number individuals, basic concepts, l basic roles.Moreover properties ct-type t-type semantics conjunctions unions TBoxaxioms also hold a-type semantics ABox axioms.important note that, TBoxes 0 equivalent, amodels may different 0 (i.e., |AT |ar 6= |AT 0 |ar ). Thus identifycorresponding ABoxes set a-types fix background TBox.Definition 5 Let DL-LiteR TBox set a-types. correspondingDL-LiteR ABox respect DL-LiteR ABox |AT |arDL-LiteR ABox A0 |A0T |ar |AT |ar .Note ABox empty set, set a-models seta-types, still background TBox restrict set satisfying a-types.set a-types DL-LiteR TBox , say consistent a-typesa-models empty ABox background TBox (i.e., |T |ar ).show set a-types , consistency background TBox ensuresexistence uniqueness corresponding DL-LiteR ABox respect .Theorem 8 Let DL-LiteR TBox set a-types. consistent, unique corresponding DL-LiteR ABox respect .let ATr function takes input set a-typesconsistent , ATr (M ) closure corresponding DL-LiteR ABoxrespect , otherwise ATr (M ) inconsistent ABox .3.4 Characterising DL-LiteR KBsshown t-type a-type semantics capable characterising respectivelyentailments TBox KB ABox. Intuitively, combiningtwo characterise entailments KB. fact full version typesemantics defined.343fiZhuang, Wang, Wang, & Qi0Recall t-type subsetR R0 B 0 R0 copies BB BaR a-type subset aD B a,bD Rab B Rab copiesB R individual pair individualsa, b. AStype unionpair t-type a-type, B B 0 R R0 aD B a,bD Rab . denoteset types r .0Note type , t-type part obtained intersectingB Bab0R R a-type part obtained intersecting aD B a,bD R .type satisfies KB K = (T , A) t-model a-model. call types type models K.Definition 6 type model DL-LiteR KB K = (T , A) type |T |tr|AT |ar .denote set type models DL-LiteR KB K |K|r . type modelsconjunction negation DL-LiteR axioms7 denoted definedmanner ct-type semantics. notions entailment, logical equivalence, consistency type semantics defined manner DL semantics. typesemantics, KB K entailing DL-LiteR axiom written K |=r .establish connection DL models type models KB.Let DL interpretation. pair (not necessarily distinct) elements d, edomain I, e induce type follows.(I, d, e) = (I, d, e) (I).call (I, d, e) type induced e I. Note (I, d, e) induces t-type(I) induces a-type forms receptively t-type a-type partinduced type. show DL interpretation I, model KBtype induced type model KB.Proposition 12 Let K DL-LiteR KB DL interpretation. |K| iff(I, d, e) |K|r pair d, e .Also construct DL model KB type model KB.Proposition 13 Let K DL-LiteR KB. |K|r , |K| d, e(I, d, e) = .connections, show type semantics induces set entailments DL-LiteR axioms induced DL semantics.Theorem 9 Let K DL-LiteR KB conjunction DL-LiteR axioms.K |= iff K |=r .Since type semantics obtained combining t-type a-type semantics, inheritsfiniteness succinctness properties two.7. DL-LiteR axioms either DL-LiteR TBox axiom DL-LiteR ABox axiom.344fiDL-Lite Contraction Revision2Proposition 14 Let K DL-LiteR KB. K 2(n+2)m+(n +2)l type models,n number individuals, basic concepts, l basic roles.give corresponding role-complete condition sets types. set typesrole-complete types satisfy model conditions t-type semanticsa-type semantics, R R, whenever typeR , (R)0 , (R)a D, type 0{R, R0 , Rbc } 0 6= b, c ( 0 may identical, pair among a, b, cmay identical).set types , corresponding KB defined way ct-typesemantics. Also shown analogously |K|r role-complete DL-LiteRKB K role-complete guarantees existence unique corresponding DLLiteR KB.Theorem 10 Let set types. role-complete, unique corresponding DL-LiteR KB .introduced versions type semantics, ranging simplestct-type comprehensive one presented subsection. Assumingsignature, tc tr , ar r , ar tr r . characterising abilitiesdepicted Figure 1 match subset relationships.4. Axiom Eliminationsection, deal elimination axioms DL-Lite KBs. severalapplication scenarios elimination, (1) eliminate axioms TBoxABox considered; (2) eliminate axioms ABox backgroundTBox assumed remains unchanged; (3) eliminate axioms KBTBox ABox considered subject change. discussedprevious section, although type semantics used scenarios, wastecomputational power use scenarios (1) (2) simpler ct-type, t-typea-type semantics used. pursue scenario (1) twohandled manner. difference scenariosswitch underlying semantics a-type type semantics..strategy axiom elimination define contraction function takesinput logically closed TBox conjunction TBox axioms returns output.TBox entailed. convenience, called original TBox,.contracting axiom, contracted TBox.defining contraction functions, approach inspired KatsunoMendelzon (1992). However, take general approach explicit orderingmodels assumed instead propositional models work t-type models.Also assume original TBox coherent.present contraction functions DL-LiteR TBoxes DL-Litecoreones defined instantiated analogously. Thus KBs, TBoxes, ABoxes, axiomsassumed DL-LiteR ones throughout section.345fiZhuang, Wang, Wang, & Qi4.1 Eliminating Axioms TBoxIntuitively, model set TBox contains counter-models axiom (i.e,models ) TBox imply . Thus, eliminate axiom TBoxfirst add counter-models form intermediate modelset obtain corresponding TBox model set. Since intermediate model setcontains counter-models , sure obtained TBox entail .Note apply approach, decision made counter-modelsadd. extralogical information required making decision could provideddomain expert KB. study theoretical properties assumeselection function plays role decision making. limiting case setcounter-models empty contracting axiom tautology. possiblestop TBox implying tautology, convenient reasonable way nothingreturn original TBox. line intuition, selection function returnempty set cases. Formally, selection function sett-types , (M ) non-empty subset unless empty.Selection function restricted handle special caseentail . case, model set contains counter-models . Intuitively, askedeliminate axiom entailed TBox nothing doneoriginal TBox returned outcome. line intuition, selectionfunction required faithful intersection modelsempty, selection function picks intersecting models others.Formally, selection function faithful respect TBox (M ) = |T |trwhenever |T |tr 6= , set t-types .considerations, contraction function called T-contraction functiondefined follows. Recall Tr function takes input set t-typescoherent, Tr (M ) closure corresponding DL-LiteRTBox, otherwise Tr (M ) = ..Definition 7 function T-contraction function TBox iff conjunctionsTBox axioms.= Tr (|T |tr (||tr ))faithful selection function .Note r-model intermediate model set |T |tr (||tr ) satisfies modelconditions t-type semantics, since original TBox assumed coherent,|T |tr (||tr ) includes models must coherent.present properties T-contraction functions. commonly acceptedAGM postulates contraction best capture desirable properties contractionfunctions. following, adapt AGM postulates alternativescurrent contraction problem closed TBox , conjunctionsTBox axioms....(T 1) = cl(T )..(T 2)346fiDL-Lite Contraction Revision..(T 3) 6|= , =..(T 4) 6|= , 6|=..(T 5) cl((T ) {})...(T de) |= |T |tr ||tr ||tr |=...(T 6) , =.According postulates, contraction function syntax-insensitive (T 6).produces closed TBox (T 1) entail contracting axiom unless..tautology (T 4). produced TBox larger original one (T 2)..contracting axiom entailed, nothing done (T 3)..AGM origin (T 5) called Recovery main postulate formalisingminimal change principle contraction. requires information loss contraction minimal original TBox recovered expandingcontracted TBox contracting axiom. Recovery criticised many researchers among Hansson (1991) argued emerging property ratherfundamental postulate contraction. One evidence contractionitself, satisfaction relies also properties (viz, AGM-compliance) underlying logic(Ribeiro et al., 2013). particular DLs including DL-Lite incompatibleRecovery.Due controversy Recovery, many proposed alternative postulatescapturing minimal change principle. quest proper postulate DL-Lite,notice Recovery replaced following postulate Disjunctive Elimination(Ferme et al., 2008):..K K K .Disjunctive Elimination captures principle minimal change stating conditionretaining formula contraction. formula original belief setdisjunction contacting formula retained contractionformula retained. Since disjunction axioms permitted DL-Lite, adapting.postulate describe disjunction terms models, thus postulate (T de)..Notice use t-models instead DL models (T de). Due property t-type..semantics unions axioms, |T | || || implies |T |tr ||tr ||trvice versa. Thus using DL models instead t-models enforces stricter conditionretaining means less number axioms retained contraction.obvious principle minimal change favours use t-models.....show T-contraction function satisfies (T 1)(T 4), (T de), (T 6)functions satisfying postulates T-contraction functions. words,set postulates fully characterises properties T-contraction function....Theorem 11 function T-contraction function TBox iff satisfies (T 1)...(T 4), (T de), (T 6).347fiZhuang, Wang, Wang, & Qipresented definition T-contraction functions properties.clear T-contraction function cannot seen update operatorKB update literatures. operators (e.g., Winsletts operator, see Winslett, 1990) usuallyapply fixed rule update semantics (e.g., WIDTIO) determining update outcome.T-contraction functions, rule deciding contraction outcome simulatedassociated selection function. important note that, intentionally leave opendetails selection function except require faithful. Thusflexible enough simulate rules respect faithfulness condition. fact,T-contraction function represents general framework dealing changes DL KBssubsumes many update operators sense rules operators appliedsimulated faithful selection functions. remaining section,provide algorithm called TCONT implements one operator.Algorithm 1: TCONTInput: TBox conjunction TBox axiomOutput: TBox1 tautology 6|=2return := ;6Let = PickCounterModel ();foreach6|=tr:= \ {};7return := ;345TCONT takes input TBox conjunction TBox axioms , returnoutput TBox. TCONT first checks tautology implied (line 1)case returned (line 2). Otherwise procedure PickCounterModel appliedpicks counter-model (line 3). TCONT checks counter-modelaxiom (line 4). axiom satisfied (line 5) removed(line 6). Finally, whatever left returned (line 7).procedure PickCounterModel takes conjunction TBox axioms returncounter-model . Essential, goal obtain t-modelachieved example following two steps: (1) Consider one conjunct 1 ,1 = B v B, B, let contain B D; otherwise 1 = B v(or 1 = R v R, R), let contain B (resp., R S);otherwise 1 = R v S, let contain R (or contain R S). (2) Addelements satisfies model conditions t-type semantics.TCONT runs polynomial time respect size . particular,checking whether tautology entails takes polynomial time (line 1), procedurePickCounterModel shown runs linear time, satisfiability check (line 5)runs linear time.Proposition 15 Let TBox let conjunction TBox axioms.TCONT (T , ) terminates returns TBox polynomial time respect size348fiDL-Lite Contraction Revision...function = TCONT (T , ), T-contractionfunction.Example 4 (contd Example 2) logical closure contains axiom, among others,Loc v RS, derived Loc v CS RS v CS.contract := Loc v RS , TCONT takes input.line 3, suppose counter-model = {Loc , Das , CS, RS, OS} selected.satisfy Loc v RS RS v CS, hence two axioms (and two)eliminated closure.5. Axiom Incoporationsection, deal incorporation axioms DL-Lite KBs. Similaraxioms elimination, three application scenarios often encountered,(1) incorporate axioms TBox ABox considered; (2) incorporateaxioms ABox background TBox assumed remains unchanged;(3) incorporate axioms KB TBox ABox consideredsubject change. previous discussions, best use ct-type t-type semanticsscenario (1), a-type semantics scenario (2), scenario (3) usefull version type semantics. focus scenarios (1) (2). managing DLKBs, scenario (3) less common handled (Wang et al., 2015)similar approach.8Similar axiom elimination, strategy define revision function takesinput logically closed TBox (or ABox ) conjunction TBox axioms (resp.ABox axioms) returns output TBox (resp. ABox )entailed. convenience, (or ) called original TBox (resp. ABox ),revising axiom, (resp. ) revised TBox (resp. ABox ). definingfunctions, assume original TBox coherent; original ABox consistentbackground TBox9 background TBox coherent.AGM framework, revision constructed indirectly contraction via.Levi identity (Levi, 1991). Formally, let contraction function belief set K,.revision function K defined K = Cn((K ) {}) formulas .Since syntax DL-Lite permit axiom negation approach applicableDL-Lite. define revision functions directly model-theoretic approach.contraction approach inspired Katsuno Mendelzon (1992) basedtype semantics.present revision functions DL-LiteR DL-Litecore definedinstantiated analogously. Thus KBs, TBoxes, ABoxes, axioms assumedDL-LiteR ones throughout section.8. also proposed alternative semantic characterisation DL-Lite used structures couldexponentially larger type. Hence, polynomial time algorithm available.9. consistent background TBox, ABox consistent.349fiZhuang, Wang, Wang, & Qi5.1 Incorporating Axioms TBoxstart revision function incorporating axioms TBox. presentingfunction, need clarify fundamental difference AGM revision revisionDL TBoxes (TBox revision short). AGM revision aims incorporate new beliefsresolving inconsistency. TBox revision goes beyond inconsistency resolving.addition consistency, meaningful DL TBoxes coherent, thus TBox revisionresolve inconsistency incoherence caused incorporating new axioms.10give intuitions behind revision function. model set TBoxsubset axiom entails . Thus incorporate axiomTBox , pick models form intermediate model set obtaincorresponding TBox. Since intermediate model set subset ,sure obtained TBox entails .Note apply approach, decision made modelspick. contraction, selection function assumed. Previously, contraction,selection function returns empty set limiting case contracting axiomtautology. limiting case revising axiom incoherent.possible return coherent TBox entails incoherent axiom, convenientreasonable way nothing return inconsistent TBox. Formally, definethat, function selection function set t-types , (M )non-empty subset unless incoherent.illustrate new definition selection function, suppose revising ,incoherent axiom. discussed, case, revision fails. Since incoherent,set t-models must incoherent. definition selection function guaranteesempty set t-types picked means revision outcome expected inconsistentTBox indicating failure revision.faithfulness condition also modified contraction case. selectionfunction faithful respect TBox satisfies1. coherent, |T |tr (M ),2. |T |tr coherent, (M ) = |T |tr .revising , condition 1 deals case models overlapsmeans {} consistent. line principle minimal change,case, selection function pick overlapping models preserve manypossible original TBox axioms. Condition 2 deals caseoverlapping exists also coherent. Since case {} consistentalso coherent, revision boils set union operation (i.e., cl(T {})).selection function therefore picks overlapping models others.illustrate new notion faithfulness, suppose revising , t-modelsoverlap (i.e., ||tr |T |tr ) set overlapping t-models coherent.Since incoherence resolve, intuitive way deal revisionadd without making change, union10. fact concentrate incoherence resolving ABox considered. definition,coherent TBox must consistent. Inconsistency resolving thus part incoherence resolving.350fiDL-Lite Contraction Revisionrevision outcome. Since set t-models union ||tr |T |tr revisionoutcome obtained taking corresponding TBox t-types picked selectionfunction, clear revision intuitive selection function pickedt-types ||tr |T |tr other. terms, call selection function faithful.addition faithfulness, selection function guarantee t-types pickedcoherent, thus introduce following condition. say selection functioncoherent preserving coherent sets t-types , (M ) coherent.considerations, revision function called T-revision function definedfollows.Definition 8 function T-revision function TBox iff conjunctionsTBox axioms= Tr ((||tr ))faithful coherent preserving selection function.present properties T-revision functions. Since AGM revision dealsinconsistency, AGM revision postulates formulated capture rationale behindinconsistency resolving process. TBox revision also deals incoherence, thuspostulates TBox revision capture rationale behind inconsistencyalso incoherence resolving. replacing conditions consistency coherence,AGM revision postulates reformulated follows TBox revision, closedTBox , conjunctions TBox axioms.(T 1) = cl(T )(T 2) |=(T 3) coherent, cl(T {})(T 4) {} coherent, cl(T {})(T 5) coherent, coherent(T 6) , =(T f ) incoherent, =According postulates, revised TBox closed (T 1), entails revisingaxiom (T 2); revising axiom coherent, revised TBox entails axiomentailed original TBox revising axiom (T 3); revisingaxiom causes incoherence revised TBox closure original TBoxrevising axiom (T 4). revised TBox coherent whenever revising axiom(T 5); Also revision function syntax-insensitive (T 6). Since revising axiomrevised TBox, revising axiom incoherent revised TBoxmust so. failure postulate (T f ) requires case simply returninconsistent TBox. purpose TBox revision incorporate axiom resolveincoherence caused. input axiom incoherent, revision doomed351fiZhuang, Wang, Wang, & Qifailure. fails ground argue proper revision outcomeis, comes convention take. Following AGM revision, takeconvention returning inconsistent TBox. AGM origin (T f ) statesrevising formula inconsistent return inconsistent belief set, deducibleAGM postulates thus postulated explicitly AGM framework.show T-revision function satisfies (T 1)(T 6), (T f )functions satisfying postulates T-revision functions. words, setpostulates fully characterises properties T-revision function.Theorem 12 function T-revision function TBox iff satisfies (T 1)(T 6) (T f ).T-contraction function, T-revision function update operator, ratherrepresents general framework incorporating axioms DL-Lite TBox.T-contraction function, T-revision function subsumes many update operators.following, provide algorithm called TREVI implements one operator.Algorithm 2: TREVIInput: TBox conjunction TBox axiomsOutput: TBox1 incoherent2return := ;8// N set atomic concepts atomic roles B Rforeach F N{} |= F v FLet = PickSatModel (, F );foreach6|=tr:= \ {};9return := cl(T {});34567TREVI takes input TBox conjunction TBox axioms , returnoutput TBox. TREVI starts checking whether incoherent (line 1),returns inconsistent TBox (line 2). Otherwise, checks atomic conceptatomic role unsatisfiable union (line 34). unionincoherent one concept role unsatisfiable. unsatisfiableconcept role F , procedure PickSatModel applied picks t-modelincludes F (line 5). TREVI checks axiom (line 6). axiomsatisfiable (line 7), removed (line 8). Finally, closureunion whatever left returned (line 9).procedure PickSatModel takes conjunction TBox axiom atomic concept role F return t-model includes F . achieved examplefollowing four steps: (1) Let contain F , extend satisfies {}1111. Recall given TBox , represents extension .352fiDL-Lite Contraction Revisionpropositionally. (2) |= R v R, R contains R (or (R)0 ), letcontain (resp., (S)0 ). (3) extend satisfies model conditionst-type semantics. (4) Repeat steps (1)(3) till longer changes.TREVI runs polynomial time respect size . particular,checking coherence (line 1) takes polynomial time, concept role satisfiability check (line 4) takes polynomial time, procedure PickSatModel shown takespolynomial time, satisfiability check (line 7) takes linear time.Proposition 16 Let TBox let conjunction TBox axioms.TREVI (T , ) terminates computes TBox polynomial time respect sizefunction = TREVI (T , ), T-revisionfunction.Example 5 (contd Example 2) Adding = Das v RS introduces incoherence,i.e., {} |= Loc v , due Loc v CS, RS v CS, Loc v Das .revise , TREVI takes input. line 5, suppose t-type= {Loc , Das , CS, RS, OS} picked. satisfy Loc v RS RS v CS,hence two axioms (and two) eliminated closureachieve coherent union .5.2 Incorporating Axioms ABoxturn revision function incorporating axioms ABox. dealingTBox revision, argued since meaningful TBox coherent, essential taskrevision incoherence resolving. Coherence longer issue here, assumebackground TBox coherent remains unchanged throughout revision process.Therefore need concern inconsistency resolving. Also noteworking a-type semantics now.idea defining T-revision function also used here. First, pickmodels revising ABox axiom form intermediate model set, returncorresponding ABox revised ABox. decision making models pickmodelled selection function. Formally, function selection functionset a-types , (M ) non-empty subset unless empty.Recall mean ABox background TBox . revisingaxiom , special case models overlap indicatingKB (T , {}) consistent. Since inconsistency resolve, simplyreturn union {} revised ABox. line intuition, selectionfunction pick overlapping models say selectionfunction faithful. Formally, selection function faithful respect ABox(M ) = |AT |ar whenever |AT |ar 6= .considerations, revision function called A-revision function definedfollows. Recall ATr function takes input set a-typesconsistent , ATr (M ) closure correspondingDL-LiteR ABox respect , otherwise ATr (M ) inconsistent ABox .353fiZhuang, Wang, Wang, & QiDefinition 9 function A-revision function ABox iff conjunctionsABox axioms= ATr ((|{}T |ar ))faithful selection function.present properties A-contraction functions. AGM postulates revisioncommonly accepted capture desirable properties revision. following,adapt current revision problem closed ABox, ,conjunctions ABox axioms.(A 1) = clT (AT )(A 2) |=ar(A 3) clT (AT {})(A 4) |(A {})T |ar 6= , clT (AT {})(A 5) |{}T |ar 6= , |(AT )T |ar 6=(A 6) , =(A f ) |{}T |ar = , =incoherence resolving picture, adapted postulates like AGMorigins concern inconsistency resolving. According postulates, revised ABoxclosed (A 1); entails revising axiom (A 2); entails axiomentailed original ABox revising axiom (A 3); closure unionoriginal ABox revising axiom revising axiom causes inconsistency(A 4), consistent whenever revising axiom (A 5); Also revisionfunction syntax-insensitive (A 6). limiting case revising axiominconsistent, since possible revised ABox entailstime consistent, take convention return inconsistent ABox revisedABox (A f ).show A-revision function satisfies (A 1)(A 6) (A f )functions satisfying postulates A-revision functions. words, setpostulates fully characterises properties A-revision function.Theorem 13 function A-revision function ABox iff satisfies (A1)(A 6), (A f ).T-contraction T-revision functions, A-revision function updateoperator, rather represents general framework incorporating axioms DL-LiteABoxes. T-contraction T-revision functions, A-revision function subsumesmany update operators. following, provide algorithm called AREVIimplements one operator.AREVI takes input TBox , ABox A, conjunction ABox axioms ,return output ABox. AREVI first checks inconsistent (line 1)354fiDL-Lite Contraction RevisionAlgorithm 3: AREVIInput: TBox , ABox A, conjunction ABox axiomsOutput: ABox1 inconsistent2return := ;34consistent (T , A)return := clT (A {});8Let = PickModel ();foreach6|=ar:= \ {}9return := clT (A {});567case inconsistent ABox returned (line 2). Otherwise, revising axiomconsistent original ABox background TBox union axiomoriginal ABox returned (lines 34). Otherwise procedure PickModel appliedpicks a-model (line 5). AREVI checks a-model axiom(line 6). axiom satisfied (line 7) removed (line 8).Finally, whatever left combined logical closure returned (line9).procedure PickModel takes ABox axiom returns a-model .achieved example following five steps: (1) Let contain propositionalforms conjuncts (recall propositional form ABox axiom A(a) Aa ).(2) Extend satisfies propositionally. (3) |= R v R, Rcontains (R)a D, let contain (S)a . (4) extendsatisfies model conditions a-type semantics. (5) Repeat steps (2)(4) tilllonger changes.Proposition 17 Let ABox conjunction ABox axioms.AREVI (T , A, ) terminates computes ABox polynomial time respectsize function = AREVI (T , A, ),A-revision function.AREVI runs polynomial time respect size , . particular,checking consistency (line 1) (T , A) (line 3) takepolynomial time. procedure PickModel shown runs polynomial time,satisfiability check (line 7) takes linear time.Example 6 (contd Example 3) Adding = RS(s) introduces inconsistency, dueaxioms Loc(d, s) A, Loc v CS RS v CS .revise , AREVI takes , A, inputs. line suppose a-type ={HDd , (Loc)d , (Das)d , Dasds , (Das )sd , (Das )s , RSs , OSs } picked. satisfyLoc(d, s), hence assertion (and one) eliminated closureachieve consistent union .355fiZhuang, Wang, Wang, & Qi6. Related Workdealing changes DL KBs, many like us considering belief change problem (Qi et al., 2006; Qi & Du, 2009; Qi et al., 2008; Ribeiro & Wassermann, 2009; Wanget al., 2015). Qi et al. (2006) gave weakening based approach revising ALC KBs.idea weaken TBox axioms inconsistencies resolved. Qi Du (2009)Wang et al. (2015) adapted Dalals (1988) Satohs operators (1988) respectivelyrevising DL KBs. main issue works revision postulatesformulated appropriately capture rationales incoherence resolving. Moreover, adapted revision operators cannot guarantee coherence revision outcome.contrast approach, Qi et al. (2008) Ribeiro Wassermann (2009) studiedcontraction revision TBoxes KBs necessarily closed. particular, Qi et al. (2008) adapted kernel revision (Hansson, 1994). Ribeiro Wassermann(2009) adapted partial meet contraction revision (Hansson, 1999) kernel contractionrevision (Hansson, 1994), semi-revision (Hansson, 1997).Due popularity DL-Lite, many worked problem updating DLLite KBs (De Giacomo et al., 2009; Calvanese et al., 2010; Kharlamov & Zheleznyakov,2011; Kharlamov et al., 2013; Lenzerini & Savo, 2011, 2012). update howeverdifferent meaning update operation belief revision literatures (Katsuno &Mendelzon, 1991). works, interpreted contraction revisionmainly focused issues expressibility update outcome. alsotackled expressibility issues assuming type semantics. Due succinctnessfiniteness type semantics, issue settled relatively easy. workscomparable ours, Lenzerini Savo (2011) dealt instance levelupdate, TBox remains unchanged ABox undergoes changes. LaterLenzerini Savo (2012) extended approach updating inconsistent KBs. mainidea first obtain ABoxes (called repairs) consistent backgroundTBox; differ minimally original ABox; accomplish insertion deletioncertain axioms. intersection repairs taken update outcome.problem setting similar A-revision functions. Although targetedexpressive DL-Lite (i.e., DL-LiteA,id ), considering DL-LiteR ideasimulated A-revision function. restricting associated selection function,A-revision function always return outcome approach.Grau et al. (2012) studied operations contract revise time.constraint states set axioms incorporated eliminatedfirst specified. operation maps KB another satisfies constraint.operation reduces revision contraction function making empty calledeliminating set incorporating set respectively. However, identifypostulates characterise contraction revision functions. workingDL-Lite, functions simulated T-contraction T-revision function.general setting, Ribeiro et al. (2013) identified properties monotonic logiccontraction function defined satisfies Recovery postulate.result, DL-Lite one logic, consistent (i.e., Theorem 11).Axiom negation supported DLs required defining revision functionscontraction functions via Levi identity. Flouris, Huang, Pan, Plexousakis,356fiDL-Lite Contraction RevisionWache (2006) proposed several notions negated axioms DLs. also explorednotions inconsistent incoherent TBoxes emphasised importance resolvingincoherence addition inconsistency.Similar T-revision function, group works usually referred ontology debuggingalso deals unsatisfiable concepts (e.g., Kalyanpur et al., 2006). method usedbased notion Minimal Unsatisfiability Preserving Sub-TBoxes (MUPS ).unsatisfiable concept B, MUPS based method first computes MUPSsB, computes minimal hitting set MUPSs. incoherence resolvedremoving axioms minimal hitting set. TREVI deals problemefficient way. Roughly speaking minimal hitting set MUPSs correspondst-model formed line 5 TREVI , thus avoid computations MUPSsminimal hitting sets significant saving computational power.7. ConclusionDue diversity DLs, difficult impossible come generalisedcontraction revision functions work best DLs. DL uniquedeserve treated individually make uniqueness. distinguishing feature DL-Lite allows restricted version existentialuniversal quantifiers. taking advantage feature, developed type semanticsDL-Lite resembles underlying semantics propositional logic. definedinstantiated contraction revision functions DL-Lite KBs whose outcomesobtained manipulating type models KBs contracting revising axioms.first contribution development type semantics DL-Lite. Given typesemantics equivalent DL semantics characterising standard inference tasksDL-Lite, outperforms DL semantics terms finiteness succinctness. secondcontribution axiomatic characterisation contraction revision functions.key obtaining result T-revision functions reformulation AGM revisionpostulates inconsistency centred incoherence centred. TBox revision dealsinconsistency also incoherence, postulates TBox revision must capturerationales behind incoherence resolving. third contribution providing tractablealgorithms instantiate contraction revision functions.future work, plan study contraction revision DLsexpressive DL-Lite. Since DLs may allow unrestricted existential universalquantifiers, concepts formed unbound nesting quantifies. semanticcharacterisation kinds concepts type semantics may possible.need techniques tailored DLs.Acknowledgementthank anonymous reviews comments helped improve papersubstantially.357fiZhuang, Wang, Wang, & QiAppendixpresenting proofs technical results, introduce notionssimplify presentation proofs.First, given ABox A, write P (a, b) mean P (b, a) A.present notion chase. Given DL-LiteR (DL-Litecore ) ABox TBox, chase w.r.t. , denoted chaseT (A) defined procedurally follows: initiallytake chaseT (A) := A, exhaustively apply following rules:A(s) chaseT (A), v A0 , A0 (s) 6 chaseT (A), chaseT (A) :=chaseT (A) {A0 (s)};A(s) chaseT (A), v R , R(s, t) chaseT (A),chaseT (A) := chaseT (A) {R(s, v)} v fresh constantappeared chaseT (A) before;R(s, t) chaseT (A), R v , A(s) 6 chaseT (A), chaseT (A) :=chaseT (A) {A(s)};R(s, t) chaseT (A), R v , S(s, t) chaseT (A),chaseT (A) := chaseT (A) {S(s, v)} v fresh constantappeared chaseT (A) before;R(s, t) chaseT (A), R v , S(s, t) 6 chaseT (A), chaseT (A) :=chaseT (A) {S(s, t)}.Note rules, R role inverse role. wellknown ABox induces unique interpretation IA domain IAconsists constants A; concept name A, AIA = {d | A(d) A};role name P , P IA = {(e, f ) | P (e, f ) A}. proofs, slightly misusenotation let denote also interpretation induced A. way, chaseT (A)also used denote interpretation.Finally, present notions positive inclusions, negative inclusions, closuresnegative inclusions. call TBox axioms forms B v R v positive inclusions(PIs), call axioms forms B v R v negative inclusions (NIs),B, B R, R. Given DL-LiteR (DL-Litecore ) TBox , closure NIs, denoted cln(T ), defined inductively follows:NIs cln(T );B1 v B2 B2 v B3 B3 v B2 cln(T ), B1 v B3 cln(T );R1 v R2 R2 v B B v R2 cln(T ), R1 v B cln(T );R1 v R2 R2 v B B v R2 cln(T ), R1 v B cln(T );R1 v R2 R2 v R3 R3 v R2 cln(T ), R1 v R3 cln(T );R v R R v R R v R cln(T ), three cln(T ).358fiDL-Lite Contraction Revisionclear definition |= cln(T ). following result shown(Calvanese et al., 2007) provides method build DL models using chase.result used prove Propositions 3 6.Lemma 1 Let (T , A) DL-LiteR (DL-Litecore ) KB. model cln(T ),chaseT (A) model (T , A).Proof Proposition 1|T |tc R R R, condition 2 Definition 1,6|= R v . Thus, model RI 6= . Suppose (d, e) RI , let0 = (I, e). Then, R 0 . Also, Proposition 2, 0 |T |tc .Proof Proposition 2direction, suffices show axiom B v C ,B implies C . Let = (I, d). Suppose B , definitionB . Since ct-model , satisfies B v C propositionally. C basicconcept, B implies C ; otherwise C = negated basic concept,B implies 6 . cases, definition , C .direction, let = (I, d) arbitrary . first showkT ktc . concept inclusion B v C , assume B , B . Cbasic concept, model implies C , turn implies C .C = negated basic concept, similar argument, 6 DI 6 .is, satisfies B v C propositionally. is, kT ktc . second half Definition 1,|= R v RI = . Clearly, R 6 . shown |T |tc .proving Proposition 3, first show following lemma preparation.Lemma 2 DL-Litecore TBox , ct-model satisfies kcln(T )ktc .Proof : Towards contradiction, suppose exists ct-model NIcln(T ) satisfy propositionally. show must violateNI (which contradicts fact ct-model ). proveinduction. convenience, assume inclusions cln(T ) added inductivelyfollowing definition.initialization, violates NI . induction steps, showadded cln(T ) due another axiom already cln(T ), violates .two cases: (1) Suppose B1 v B3 , added due PI B1 v B2 NI B2 v B3(or B3 v B2 ) cln(T ). Then, fact satisfy propositionally,{B1 , B3 } . Also, satisfies B1 v B2 , B2 . Hence, violates NI B2 v B3(B3 v B2 ). (2) Suppose R v R, added due R v R cln(T ). Then,fact satisfy propositionally, R . R v R |= R v ,condition 2 Definition 1, violates NI R v R .Proof Proposition 3359fiZhuang, Wang, Wang, & Qiconstruct interpretation using chase: Let constant, NCset concept names,={A(d) | NC }{R(d, ed,R ) | R , ed,R fresh constant}.Take = chaseT (A ). want show model (I , d) = .show former, Lemma 1, need show model cln(T ).Towards contradiction, suppose case, axiom B vcln(T ) violated . case |= B(s) |= D(s)constant . essentially four cases (note B symmetric):(i) Suppose B concept names, |= B(s) |= D(s)= d, B(d) , D(d) . construction , {B, D} thuspropositionally satisfy B v D. is, 6 kcln(T )ktc , violates Lemma 2.(ii) Suppose B = R R concept name, |= B(s)= R(d, t) t, |= D(d) B(d) . Thus, {B, R}thus propositionally satisfy B v R. Again, 6 kcln(T )ktc , violatesLemma 2.(iii) Suppose B = R R = R 6= S, |= B(s)|= D(s) R(s, t) S(s, u) t, u. =t, u fresh constants. case, {R, S} propositionallysatisfy R v S, violates Lemma 2.(iv) Suppose B = R R = R, |= B(s) R(s, t)t. = fresh constant; = fresh constant.former case, R , latter case, R . Since R v cln(T ),|= R v |= R v . cases violate fact |T |tc condition 2Definition 1.shown model cln(T ), thus model .remains show (I , d) = . Since clear (A , d) = ,definition chase, (A , d) (I , d). need show (I , d) (A , d).equivalent show \ contain assertion form A(d)R(d, s) assertion R(d, t) (as otherwise, R, respectively,(I , d) \ (A , d) according definition (I , d)). Towards contradiction, supposeassertion \ . chase rules, happen chaserule applicable assertion g form B(d) S(d, t). Let g firstassertions triggers chase rule. chase rules, observe g must .Suppose g = B(d), construction , B . g triggers chaserule B v , condition 1 Definition 1, propositionally satisfiesB v A, hence A(d) , contradiction (the applicabilityof) chase rule; otherwise, g triggers chase rule B v R ,condition 1 Definition 1, kB v Rktc , thus R R(d, u)u, contradicts chase rule.Suppose g = S(d, t), construction , . g triggeringchase rule v A, condition 1 Definition 1, propositionally satisfies360fiDL-Lite Contraction Revisionv A. , A(d) , contradicts chase rule. gtriggering chase rule v R, propositionally satisfies v R,thus R R(d, u) u, contradiction.shown \ contain assertion form A(d) R(d, s).Thus, (I , d) = (A , d) = .Proof Theorem 1|T | 6= , model |T |. Let , = (I, d).Proposition 2, |T |tc . is, |T |tc 6= . Conversely, suppose |T |tc 6= , let |T |tc .Proposition 3, model . is, |T | =6 . Thus, |T | empty|T |tc empty. |T | |T |tc empty, statement trivially holds.follows, assume |T | |T |tc non-empty.direction, want show 6|= |T |tc 6 ||tc . Then,model satisfy . is, TBox axiomconjunction satisfied I. Without loss generality, assume contains(single) TBox axiom. Suppose B v C. Then, domain elementB 6 C . Let = (I, d). Since model , Proposition 2,ct-model . C basic concept, B implies B 6 C ,turn implies C 6 . C = negated basic concept, similar argument,B implies . is, propositionally satisfy B v C 6 kktc .shown |T |tc 6 ||tc .direction, want show |T |tc 6 ||tc 6|= . Since|T |c 6 ||tc , ct-type |T |tc 6 ||tc . Proposition 3, existsmodel domain elements (I, d) = . need showmodel . Suppose otherwise, model , Proposition 2,must ct-model , contradicts fact 6 ||tc . Thus, model ,shown 6|= .Proof Theorem 2|T |tc , condition 1 Definition 1, propositionally satisfies =1, . . . , n. Moreover, coherent, monotonicity DL-Lite, exists R R|= R v . Hence, |i |tc = 1, . . . , n. is, |T |tc |1 |tc |n |tc .Conversely, |1 |tc |n |tc , condition 1 Definition 1, kT ktc . Further,coherent, exists R R |= R v . Hence, |T |tc . is,|1 |tc |n |tc |T |tc .Proof Theorem 3|T |tc , Proposition 3, model |T |(I, d) = . Since |T | || ||, || ||. Suppose without loss generality||, Proposition 2, (I, d) ||tc . is, ||tc . shown|T |tc ||tc ||tc .presenting proof Theorem 4, first show Lemmas 3 4 regardingunion TBoxes (or equivalently, conjunction TBox axioms). similar way361fiZhuang, Wang, Wang, & QiLemma 2, show following lemma. difference cannot assumect-type |T1 |tc |T2 |tc satisfies |T1 T2 |tc , thus cannot apply condition 2Definition 1 proof.Lemma 3 two DL-Litecore TBox T1 T2 , role-complete set ct-types|T1 |tc |T2 |tc , holds kcln(T1 T2 )ktc .Proof : Towards contradiction, suppose exists ct-type NIcln(T1 T2 ) satisfy propositionally. show ct-type 0exists violates NI T1 T2 , contradicts fact |T1 |tc |T2 |tc(as 0 |T1 |tc |T2 |tc implies 0 satisfies NIs T1 T2 propositionally). Similarproof Lemma 2, prove induction.initialization, T1 T2 let 0 = 0 violates NI T1 T2 .induction steps, show added cln(T1 T2 ) due another axiomalready cln(T1 T2 ), show 00 exists violates (and eventuallytake 0 = 00 T1 T2 ). two cases: (1) Suppose B1 v B3 , addeddue PI B1 v B2 T1 T2 NI B2 v B3 (or B3 v B2 ) cln(T1 T2 ). Then,fact satisfy propositionally, {B1 , B3 } . Also, satisfies B1 v B2T1 T2 , B2 . Hence, let 00 = 00 violates NI B2 v B3 (B3 v B2 ). (2) SupposeR v R, added due R v R cln(T1 T2 ). Then, factsatisfy propositionally, R . role-complete, exists 00R 00 . Hence, 00 violates NI R v R .Lemma 4 Let set ct-types, 1 , 2 two conjunctions DL-Litecore TBoxaxioms. Suppose role-complete, |1 |tc |2 |tc implies |1 2 |tc .Proof : , want show |1 2 |tc . end, constructmodel 1 2 . Let Ti set axioms (as conjuncts) = 1, 2,proof Proposition 6, = chaseT1 T2 (A ). show model T1 T2(I , d) = similar way proof Proposition 3 (by using Lemma 3 insteadLemma 2). Except case (iv): Suppose violates R v R cln(T1 T2 ). Notethat, different proof Proposition 3, cannot assume either R v R cln(T1 )R v R cln(T2 ) (That is, cannot apply condition 2 Definition 1). Yetstill derive contradiction. violates R v R R(d, t) freshconstant R(s, d) fresh constant. former case, R ,propositionally satisfy R v R. is, 6 kcln(T1 T2 )ktc , violatesLemma 3. latter case, R . Since role-complete, R 0 0 .Hence, 0 propositionally satisfy R v R. is, 0 6 kcln(T1 T2 )ktc ,violates Lemma 3.Now, shown model 1 2 (I , d) = . Proposition 2,|1 2 |tc .Proof Theorem 4Suppose two TBoxes T1 T2 corresponding . is, |T1 |tc|T2 |tc . Lemma 4, |T1 T2 |tc . minimality requirement362fiDL-Lite Contraction Revisioncorresponding TBox, |T1 T2 |tc6 |Ti |tc = 1, 2. is, |T1 T2 |tc = |Ti |tc = 1, 2.Theorem 1, T1 equivalent T2 .Proof Proposition 5direction, suffices show concept inclusion B v C, B implies C ; additionally, e (not necessarily 6= e)role inclusion R v E , (d, e) RI implies (d, e) E . Let = (I, d, e).Suppose B , definition B . Since t-model ,propositionally satisfies B v C. C basic concept, B implies C ;otherwise C = negated basic concept, B implies 6 . cases,definition , C . role inclusion R v E, suppose (d, e) RI ,definition R . t-model , propositionally satisfies R v E.E role E ; otherwise E = negated role 6 . cases,definition , (d, e) E .direction, let = (I, d, e) arbitrary d, e . first showsatisfies condition 1 Definition 3. concept inclusion B v C , assumeB , definition , B . C basic concept, modelimplies C , turn implies C . C = negated basic concept,similar argument, 6 DI 6 . is, propositionally satisfies B v C.similar way, concept inclusion B 0 v C 0 B 0 , C 0 B 0 , showsatisfies B 0 v C 0 . role inclusion R v E , assume R . Then,definition , (d, e) RI . Since model , (d, e) E . E role E ;otherwise E = negated role 6 . Thus, propositionally satisfies kR v E.Similarly, satisfies R0 v E 0 role inclusion R0 v E 0 . shownkT ktr .next show satisfies conditions 25 Definition 3. condition 2, |=R v RI = . Clearly, 6 (R)I e 6 (R )I . definition , R 6(R )0 6 . condition 3, |= R v (R)I (S)I . Suppose R ,definition implies (R)I . Then, (S)I , thus . Similarly,suppose (R)0 , implies e (R)I . Hence, e (S)I , thus (S)0 .condition 4, R definition , (d, e) RI , implies (R)Ie (R )I . definition , R (R )0 . Condition 5 clearly satisfieddefinition (I, d, e).shown satisfies conditions Definition 3, is, |T |tr .proving Proposition 6, first show Lemma 5. Note cln(T ) obtainedcln(T ) adding copy axiom cln(T ), kcln(T )ktr set t-typespropositional models cln(T ) .Lemma 5 DL-LiteR TBox , t-model satisfies kcln(T )ktr .Proof : Towards contradiction, suppose exists t-model NIcln(T ) propositionally satisfy . show must violateNI , induction. consider NI cln(T ), casecopy NI cln(T ), i.e., cln(T ) \ cln(T ), shown similarly.363fiZhuang, Wang, Wang, & Qiinitialization, violates NI . induction steps,show added cln(T ) due another axiom already cln(T ), violates .(1) Suppose B1 v B3 , added due PI B1 v B2 NI B2 v B3 (orB3 v B2 ) cln(T ). Then, fact propositionally satisfy ,{B1 , B3 } . Also, satisfies B1 v B2 , B2 . Hence, violates NIB2 v B3 (B3 v B2 ).(2) Suppose R1 v B, added due PI R1 v R2 NI R2 v B (orB v R2 ) cln(T ). Then, fact propositionally satisfy ,{R1 , B} . Also, condition 3 Definition 3, R2 . Hence, violates NIR2 v B (B v R2 ).(3) Suppose R1 v B, added due PI R1 v R2 NI R2 v B (orB v R2 ) cln(T ). Then, fact propositionally satisfy ,{R1 , B} . Also, condition 3 Definition 3, R2 . Hence, violates NIR2 v B (B v R2 ).(4) Suppose R1 v R3 , added due PI R1 v R2 NI R2 v R3 (orR3 v R2 ) cln(T ). Then, fact propositionally satisfy ,{R1 , R3 } . Also, satisfies R1 v R2 , R2 . Hence, violates NIR2 v R3 (R3 v R2 ).(5) Suppose R v R, added due NI R v R (or R v R) cln(T ).Then, fact propositionally satisfy , R . R vR |= R v (R v R |= R v ), condition 2 Definition 3, violates NIR v R (R v R).(6) Suppose R v R, added due NI R v R (or R v R ) cln(T ). Then,fact propositionally satisfy , R . Also, condition 4Definition 3, R . R v R |= R v (R v R |= R v ),condition 2 Definition 3, violates NI R v R (R v R ).Proof Proposition 6construct interpretation using chase: Let d, e two distinct constants,NC NC0 set concept names B B 0 , respectively,={A(d) | NC } {A(e) | A0 NC0 } {R(d, e) | R R }{R(d, fd,R ) | R B , R 6 , fd,R fresh constant}{R(e, fe,R ) | (R)0 B 0 , R 6 , fe,R fresh constant}.Take = chaseT (A ). want show model (I , d, e) = .show former, Lemma 1, need show model cln(T ).Towards contradiction, suppose case, axiom B vR v cln(T ) violated .(1) Suppose violates B v D, case |= B(s) |= D(s)constant . essentially four cases:364fiDL-Lite Contraction Revision(i) Suppose B concept names, |= B(s) |= D(s)= = e, B(s) , D(s) . = construction ,{B, D} thus propositionally satisfy B v D; otherwise = e{B 0 , D0 } propositionally satisfy B 0 v D0 . cases violate Lemma 5.(ii) Suppose B = R R concept name, |= B(s) == e, R(s, t) t. |= D(s) D(s) . Suppose without lossgenerality = (similar (i), case = e shownway). = e construction , R , condition 4 Definition 3,R ; otherwise, fresh constant, R . cases, {B, R} thuspropositionally satisfy B v R, violates Lemma 5.(iii) Suppose B = R R = R 6= S,|= B(s) |= D(s) R(s, t) S(s, u) t, u.(a) = = e, t, u fresh constants; (b) = = u = e; (c)= e = u = d. case (a), suppose w.o.l.g = d, {R, S}propositionally satisfy R v S, violates Lemma 5. case (b), {R, S} ,condition 4 Definition 3, {R, S} , violates Lemma 5. case (c),{R , } . condition 4 Definition 3, {(R)0 , (S)0 } , hencepropositionally satisfy (R)0 v (S)0 . violates Lemma 5.(iv) Suppose B = R R = R, |= B(s) R(s, t)t. (a) = = e, fresh constant; (b) == e, fresh constant; (c) = = e; (d) = e = d.case (a), suppose w.l.o.g. = d, R . case (c), R , condition 4Definition 3, R . cases, propositionally satisfy R v R,violates Lemma 5. case (b), suppose w.l.o.g. = d, R . case (d), R ,condition 4 Definition 3, R . Since |= R v , violates condition 2Definition 3.(2) Suppose violates R v S, case |= R(s, t) |= S(s, t)constants s, . case (a) = = e, (b) = e= d, (c) R = = = e fresh constant, (d) R = = = efresh constant. case (a), {R, S} , propositionally satisfyR v S. case (b), {R , } , condition 5 Definition 3, {R0 , 0 } .Hence, propositionally satisfy R0 v 0 . neither case, kcln(T )ktrviolates Lemma 5. case (c), |= R v R, is, |= R v . = R ,otherwise = e (R)0 , violates condition 2 Definition 3. Similarly,case (d), |= R v . = R , otherwise = e (R )0 ,violates condition 2 Definition 3.shown model cln(T ), thus model .remains show (I , d, e) = . Since clear (A , d, e) = ,definition chase, (A , d, e) (I , d, e). need show (I , d, e)(A , d, e). equivalent show \ contain assertionform A(d), A(e), R(d, e), R(d, s) R(e, s) fresh constantassertion R(d, t) respectively R(e, t) (as otherwise, A, A0 , R, R, (R)0 ,respectively, (I , d, e) \ (A , d, e) according definition (I , d, e)).Towards contradiction, suppose assertion \ . chaserules, happen chase rule applicable assertion g form B(d),365fiZhuang, Wang, Wang, & QiB(e), S(d, e), S(d, t) S(e, t) fresh constant chase. Let g firstassertions triggers chase rule, chase rules, g must .Suppose g = B(d), construction , B . g triggers chaserule B v , condition 1 Definition 3, propositionally satisfiesB v A, hence A(d) , contradiction (the applicabilityof) chase rule; otherwise, g triggers chase rule B v R ,condition 1 Definition 3, propositionally satisfies B v R, thus RR(d, u) u, contradicts chase rule. caseg = B(e) shown similarly, replacing propositionally satisfying B vpropositionally satisfying B 0 v A0 , propositionally satisfying B v Rpropositionally satisfying B 0 v (R)0 .Suppose g = S(d, e), construction , . conditions 45 Definition 3, , (S )0 , (S )0 .g triggers chase rule v R , condition 1 Definition 3,propositionally satisfies v R, R R(d, e) , contradictschase rule.g triggers chase rule v R , condition 1 Definition 3,propositionally satisfies (S )0 v R0 , R0 . condition 5 Definition 3,R R (d, e) , contradicts chase rule.g triggers chase rule v A, propositionally satisfies v A., A(d) , contradicts chase rule.g triggers chase rule v A, propositionally satisfies (S )0 vA0 . (S )0 , A0 A(e) , contradicts chase rule.g triggers chase rule v R, propositionally satisfies vR. , R . Hence, R(d, u) u,contradiction.Similarly show case g triggering chase rule v R.Suppose g = S(d, t) fresh constant, construction ,.g triggers chase rule v R , R(d, t) added. condition 3Definition 3, R , construction , R(d, u)u. chase rules, R(d, t) behaves differently R(d, u)chase. Thus, could equally consider g = R(d, u) discussion. is,application chase rule v R effect proof.g triggering chase rule v A, condition 1 Definition 3,propositionally satisfies v A. , A(d) ,contradicts chase rule.g triggering chase rule v R, propositionally satisfies vR, thus R R(d, u) u, contradiction.case g = S(e, t) shown similarly way.366fiDL-Lite Contraction Revisionshown \ contain assertion form A(d), A(e), R(d, e),R(e, d), R(d, s) R(e, s). Thus, (I , d, e) = (A , d, e) = .Proof Theorem 5|T | =6 model |T |. Let d, e = (I, d, e).Proposition 5, |T |tr . is, |T |tr 6= . Conversely, suppose |T |tr 6= , let |T |tr .Proposition 6, model . is, |T | 6= . |T | |T |tr empty,statement trivially holds. follows, assume |T | |T |tr non-empty.direction, want show 6|= |T |tr 6 ||tr . Then,model satisfy . Similar proof Theorem 1,assume w.l.o.g. contains single TBox axiom. axiom formB v C. Then, domain element B 6 C . Let= (I, d, d). Since model , Proposition 5, |T |tr . C basicconcept, B implies B 6 C , turn implies C 6 . C =negated basic concept, similar argument, B implies . is,propositionally satisfy B v C. condition 1 Definition 3, 6 ||tr . Supposeform R v E. Then, domain elements d, e (d, e) RI(d, e) 6 E . Let = (I, d, e). Again, Proposition 5, |T |tr . E role,R implies (d, e) RI (d, e) 6 E , turn implies E 6 . E =negated role, similar argument, R implies . is,propositionally satisfy R v E. condition 1 Definition 3, 6 ||tr . showncases |T |tr 6 ||tr .direction, want show |T |tr 6 ||tr 6|= . Since|T |r 6 ||tr , t-type |T |tr 6 ||tr . Proposition 6, existmodel domain elements d, e (I, d, e) = . need showmodel . Suppose otherwise, model , Proposition 5,must t-model , contradicts fact 6 ||tr . Hence, model, shown 6|= .presenting proof Theorem 6, first show Lemma 6 Lemma 7.two lemmas extend Lemma 3 Lemma 4 respectively DL-LiteR .Lemma 6 two DL-LiteR TBox T1 T2 , role-complete set t-types|T1 |tr |T2 |tr , holds kcln(T1 T2 )ktr .Proof : Towards contradiction, suppose exists t-type NIcln(T1 T2 ) propositionally satisfy . show t-typeexists violates NI T1 T2 , contradicts fact |T1 |tr |T2 |tr .Similar proof Lemma 5, prove induction. presentcase NI cln(T1 T2 ), case copy NI cln(T1 T2 ),i.e., cln(T1 T2 ) \ cln(T1 T2 ), shown similarly. Without loss generality,assume axioms added cln(T1 T2 ) incrementally according definitioncopies (e.g., B 0 v C 0 ) added immediately original axioms (B v B) added.initialization, T1 T2 violates NI T1 T2 . inductionsteps, show added cln(T1 T2 ) due another axiom already367fiZhuang, Wang, Wang, & Qicln(T1 T2 ) , show 0 exists violates . proof cases (1)(4)proof Lemma 5, simply let 0 = .cases (5), suppose R v R, added due NI R v R (or R v R)cln(T1 T2 ) . Note (R )0 v (R )0 (resp., R0 v R0 ) also cln(T1 T2 ) . Then,fact propositionally satisfy , R . role balance, exists0 R 0 R0 0 . R 0 , conditions 4 5 Definition 3, (R )0 0 ,0 violates NI (R )0 v (R )0 (resp., R v R). R0 0 , conditions 4 5Definition 3, R 0 R 0 , 0 violates NI R v R (resp.,R0 v R0 ).case (6), suppose R v R, added due NI R v R (or R v R )cln(T1 T2 ) . Note (R)0 v (R)0 (resp., (R )0 v (R )0 ) also cln(T1 T2 ) .Then, fact propositionally satisfy , R . conditions 45 Definition 3, R (R )0 . Hence, violates NI R v R (resp.,(R )0 v (R )0 ).Lemma 7 Let set t-types 1 , 2 two conjunctions DL-LiteR TBoxaxioms. Suppose role-complete, |1 |tr |2 |tr implies |1 2 |tr .Proof : , want show |1 2 |tr . end, constructmodel 1 2 way proof Proposition 6. Let Tiset axioms = 1, 2, constructed way proofProposition 6, take = chaseT1 T2 (A ). show model T1 T2(I , d, e) = similar way proof Proposition 6 (using Lemma 6 insteadLemma 5), except cases (1) (iv) (2).case (1) (iv), suppose violates R v R cln(T1 T2 ). Differentproof Proposition 6, cannot assume R v R either cln(T1 ) cln(T2 ), thuscannot apply condition 2 Definition 3. Yet still derive contradiction. violatesR v R R R . former case, propositionally satisfyR v R. is, 6 kcln(T1 T2 )ktr , violates Lemma 6. latter case, sincerole-complete, R 0 t-type 0 . Hence, 0 propositionallysatisfy R v R, violates Lemma 6.case (2), suppose violates R v cln(T1 T2 ). case {R, S} ,{R , } , R = {R, R , (R)0 , (R )0 } 6= . first two casesshown way proof Proposition 6. third case R = S,different proof Proposition 6, cannot assume R v R either cln(T1 )cln(T2 ), thus cannot apply condition 2 Definition 3. Note facts{R, R , (R)0 , (R )0 } 6= role-complete, exists t-type0 R 0 R0 0 . R 0 0 propositionally satisfy R v R;otherwise R 0 , 0 propositionally satisfy R0 v R0 . cases, Lemma 6violated.Now, shown model T1 T2 (I , d, e) = . Proposition 5,|1 2 |tr .Proof Theorem 6368fiDL-Lite Contraction Revisiontheorem proved similarly Theorem 4, proof based Lemmas 67.Proof Proposition 8ct-type |T |tc , Proposition 3, model(I, d) = . Let 0 t-type 0 = (I, d, d). Then,definitions (I, d) (I, d, d), = 0 B. Also, Proposition 5, 0 |T |tr . is,{ B | |T |tr }, hence |T |tc { B | |T |tr }.Conversely, ct-type { B | |T |tr }, t-type 0 |T |tr= 0 B. Proposition 6, model d, e(I, d, e) = 0 . easy see = (I, d) definitions (I, d) (I, d, e).Proposition 2, |T |tc . is, { B | |T |tr } |T |tc .following theorem generalises Theorem 2 DL-LiteR .Theorem 14 Let DL-LiteR TBox = {1 , . . . , n }. coherent|T |tr = |1 |tr |n |tr .Proof : |T |tr , satisfies conditions 4 5 Definition 3. Also, = 1, . . . , n,condition 1 Definition 3 w.r.t. , propositionally satisfies . is, satisfiescondition 1 w.r.t. . Further, coherent, monotonicity DL-Lite, existsR R |= R v , trivially satisfies condition 2 w.r.t. . Moreover,|= R v S, |= R v S, condition 3 Definition 3 w.r.t. , satisfiescondition 3 w.r.t. . Hence, |i |tr = 1, . . . , n. is, |T |tr |1 |tr |n |tr .Conversely, |1 |tr |n |tr , construct modelway proofs Proposition 6 Lemma 7 induces .Proposition 5, |T |tr . is, |1 |tr |n |tr |T |tr .Proof Proposition 9direction, since model , suffices show model A,i.e., concept assertion A(a) A, aI AI , role assertion P (a, b) A,(aI , bI ) P . Let = (I), kAT kar propositionally satisfies Aa . is,Aa . definition , aI AI . Similarly, propositionally satisfies P abP ab , hence (aI , bI ) P . shown |K|.direction, need show second half statement since|K| implies |T |. Let = (I), want show |AT |ar . condition 1Definition 4, show satisfies way proof Proposition 5.A(a) P (a, b) A, since aI AI (aI , bI ) P , AaP ab . is, propositionally satisfies A. shown kAT kar . Further,shown satisfies conditions 25 Definition 4 similar mannerproof Proposition 5 (roughly, replacing Definition 3 Definition 4,D, e b D, B B , B 0 B b , R Rab , R (R)a , (R)0(R)b , on).presenting proof Proposition 10, first show Lemma 8. lemmaproved manner Lemma 5. Note cln(T )a TBox consists369fiZhuang, Wang, Wang, & Qicopy concept inclusion cln(T ) individual D, copyrole inclusion cln(T ) pair individuals D.Lemma 8 DL-LiteR KB K = (T , A), a-model propositional modelcln(T )a .Proof Proposition 10Similar before, construct interpretation using chase: Let NCaset concept names B (with D),Aa ={A(a) | D, Aa NCa } {R(a, b) | a, b D, Rab Rab }{R(a, fa,R ) | D, (R)a B , Rab 6 b D, fa,R fresh constant}.Take = chaseT (Aa ). want show model K (I ) = .show former, first show Aa . concept assertion A(a) A,a-model K, propositionally satisfies Aa . is, Aa hence A(a) Aa .Similarly, role assertion P (a, b) A, P ab P (a, b) Aa . shownAa . show model K, want show model(T , Aa ). Lemma 1, need show Aa model cln(T ).shown similar way proof Proposition 6 (roughly, replacing Definition 3Definition 4, Lemma 5 Lemma 8, D, e b D, BB , B 0 B b , R Rab , R (R)a , (R)0 (R)b , on).remains show (I ) = . Again, need show (I )(Aa ). shown similar way proof Proposition 6.Proof Theorem 7|K| =6 model |K|. Let = (I). Proposition 9, |AT |ar .is, |AT |ar 6= . Conversely, suppose |AT |ar 6= , let |AT |ar . Proposition 10,model K. is, |K| 6= . |K| |AT |ar empty, statementtrivially holds. follows, assume |K| |AT |ar non-empty.direction, want show K 6|= |AT |ar 6 ||ar . Then,model K satisfy . Let = (I). Proposition 9,|AT |ar . Similar proof Theorem 1, assume w.l.o.g. containssingle ABox assertion. Suppose form A(a). Then, aI 6 AI . definition(I), Aa 6 , hence 6 kkar . condition 1 Definition 4, 6 ||ar . Supposeform P (a, b). Then, (aI , bI ) 6 P . Again, definition (I), P ab 6 ,hence 6 ||ar . cases, |AT |ar 6 ||ar .direction, want show |AT |ar 6 ||ar K 6|= . Since|AT |ar 6 ||ar , a-type |AT |ar 6 ||ar . Proposition 10,exist model K (I) = . need show model. Suppose otherwise, model , Proposition 9, must a-model, contradicts fact 6 ||ar . Hence, model , shownK 6|= .Proof Theorem 8Let = {A(a) | Aa } {P (a, b) | P ab }.want show unique corresponding ABox w.r.t. .370fiDL-Lite Contraction Revisionfirst show corresponding ABox. show |AT |ar , needshow a-type , satisfies conditions 15 Definition 4. consistent, |T |ar . Hence, propositional model , satisfies conditions 25.Also, construction A, satisfies Aa A(a) satisfies P abP (a, b) A. is, kAT kar . shown |AT |ar .Further, ABox A0 |A0T |ar a-type , sincekA0T kar , satisfies Aa concept assertion A(a) A0 satisfies P abrole assertion P (a, b) A0 . Note holds a-type . constructionA, A0 A. is, |AT |ar |A0T |ar . Thus, corresponding ABox. Also, basedobservation, corresponding ABox must equivalent A.following theorem proved similar Theorem 14. Since one TBoxconcerned, proof require Lemmas 6 7.Theorem 15 Let K = (T , A) DL-LiteR KB = {1 , . . . , n }.|AT |ar = |{1 }T |ar |{n }T |ar .Proof Proposition 12Let K = (T , A). direction, suffices show (i)concept inclusion B v C , B implies C ; (ii) e (notnecessarily 6= e) role inclusion R v E , (d, e) RI implies (d, e) E ;(iii) concept assertion A(a) A, aI AI , role assertion P (a, b) A,(aI , bI ) P . Let = (I, d, e) = (I, d, e) (I). Conditions (i) (ii) shownproof Proposition 5. is, |T |. Then, condition (iii) shown proofProposition 9.direction, let = (I, d, e) arbitrary d, e . is,= (I, d, e) (I). want show (I, d, e) |T |tr (I) |AT |ar ,shown proofs Propositions 5 9, respectively.Proof Proposition 13Let K = (T , A). construct interpretation using chase: Letdefined proofs Propositions 6 10, = chaseT (A Aa ).want show model K (I , d, e) = . show former,Aa proof Proposition 10, need show model(T , Aa ). Lemma 1, suffices show Aa model cln(T ),shown proofs Propositions 6 10. show later, (I , d, e) = ,need show (I , d, e) (A , d, e). definition (I , d, e),suffices show (I , d, e) (A , d, e) (I ) (Aa ),shown proofs Propositions 6 10.Proof Theorem 9shown way proof Theorem 7 |K| empty|K|r empty. follows, assume |K| |K|r non-empty.direction, want show K 6|= |K|r 6 ||r . Then,model K satisfy . Similar proof Theorem 1,assume w.l.o.g. contains single TBox axiom single ABox assertion. Suppose371fiZhuang, Wang, Wang, & Qiform B v C R v E, way proof Theorem 5,construct = (I, d, e) d, e |K|r 6 ||r . Supposeform A(a) P (a, b), way proof Theorem 7, show|K|r 6 ||r .direction shown way proof Theorem 7.presenting proof Theorem 10, first show Lemma 9 Lemma 10.two extend Lemmas 6 7 respectively DL-LiteR KBs.Lemma 9 two DL-LiteR TBox T1 T2 , role-complete set types|T1 |r |T2 |r , types must satisfy cln(T1 T2 ) cln(T1 T2 )a .Proof : Towards contradiction, suppose exists type NI cln(T1T2 ) cln(T1 T2 )a propositionally satisfy . show typeexists violates NI T1 T2 T1a T2a , contradicts fact|T1 |r |T2 |r . shown similar way proof Lemma 6.axiom cln(T1 T2 ) concerned, proof Lemma 6. axiomcln(T1 T2 )a concerned, proof adapted replacing B B , B 0 B b , RRab , R (R)a , (R)0 (R)b , on.Lemma 10 Let set types 1 , 2 two conjunctions DL-LiteR axioms.Suppose role-complete, |1 |r |2 |r implies |1 2 |r .Proof : type , want show |1 2 |r . end,construct model 1 2 way proof Proposition 13. Letsets respectively TBox axioms ABox axioms 1 2 ,set ABox axioms Aa proof Proposition 13, take= chaseT (A Aa ). show model (T , A) (I , d, e) =similar way proofs Lemma 7 Proposition 13. Proposition 12,|1 2 |r .Proof Theorem 10theorem proved similarly Theorem 4 proof based Lemma 9Lemma 10.following theorem proved similar Theorem 14.Theorem 16 Let K DL-LiteR KB K = {1 , . . . , n }. K coherent|K|r = |1 |r |n |r .Proof Theorem 11.one direction, suppose T-contraction function TBox as....sociated selection function . need show satisfies (T 1)(T 4), (T de),.....(T 6). (T 1), (T 2), (T 4) (T 6) follow directly definition T-contraction..function. show proof (T 3) (T de).372fiDL-Lite Contraction Revision.(T 3): Suppose 6|= . |T |tr 6 ||tr implies |T |tr ||tr 6= . Thus.faithfulness , (||tr ) |T |tr . Thus = Tr (|T |tr (||tr )) = Tr (|T |tr ) = ...(T de): prove contrapositive. Suppose |= 6|= ....|T |tr ||tr |T |tr 6 ||tr . remains show |T |tr 6 ||tr ||tr . Assume |T |tr.||tr ||tr . Since definition T-contraction function, |T |tr (||tr ) |T |tr ,|T |tr (||tr ) ||tr ||tr implies (||tr ), ||. Thus|T |tr (||tr )) ||tr . follows definition corresponding TBoxes.|T |tr (||tr )) |T |tr ||tr , contradiction!..direction, suppose function TBox satisfies (T 1)...(T 4), (T de), (T 6). Let defined.(||tr ) = ||tr |T |trconjunctions TBox axioms . set t-types , conjunctionTBox axiom ||tr = , define (M ) = |T |tr whenever |T |tr 6=otherwise. need show (1) faithful selection function (2).= Tr (|T |tr (||tr )).Part (1): faithful selection function, function first..follows directly definition (T 6).prove selection function, suppose = . need show (M ) = .TBox axiom tautology, ||tr = = . Thus (M ) = (||tr ) =.||tr |T |tr = . suppose 6= . need show (M ) 6= . definition, result trivially holds conjunction TBox axioms ||tr = ..||tr = , since ||tr 6= implies 6|= , follows (T 4)..|T |tr ||tr 6= . Thus (M ) = (||tr ) = ||tr |T |tr 6= .prove faithful respect , suppose |T |tr 6= . need show(M ) = |T |tr . Again, result trivially holds conjunction TBox axioms||tr = . ||tr = , since ||tr |T |tr 6=..implies 6|= , follows (T 3) |T |tr = |T |tr . Thus (M ) = (||tr ) =.||tr |T |tr = ||tr |T |tr ...Part (2): Since (T 1) implies closed Tr function returns closed.TBoxes, suffices show |Tr (|T |tr (||tr ))|tr = |T |tr ....follows (T 2) implies |T |tr |T |tr . follows..definition (||tr ) |T |tr . |T |tr (||tr ) |T |tr implies.definition corresponding TBoxes |Tr (|T |tr (||tr ))|tr |T |tr ..remains show |T |tr |Tr (|T |tr (||tr ))|tr . Assume contrary.|T |r 6 |Tr (|T |tr (||tr ))|tr . Let conjunction TBox axioms..||tr = |Tr (|T |tr (||tr ))|tr . |= 6|= . follows (T de)..|T |r 6 ||tr ||tr = ||tr |Tr (|T |tr (||tr ))|tr . Let u |T |tr , u ||tru ||tr |Tr (|T |tr (||tr ))|tr u ||tr , definition ,.u (||tr ). Thus either case, u ||tr |Tr (|T |tr (||tr ))|tr implies |T |tr||tr |Tr (|T |tr (||tr ))|tr = ||tr ||tr , contradiction!Proof Proposition 15.complexity results explained earlier. Let function..= TCONT (T , ) TBox conjunction axioms . need show373fiZhuang, Wang, Wang, & Qi...T-contraction function. Theorem 11 suffices show satisfies (T 1)(T 4),.....(T de), (T 6). (T 2), (T 3) (T 6) trivially satisfied.Let counter-model picked line 3 TCONT . Since axioms violet..removed line 6, ||tr . Suppose = {1 , . . . , n }...Theorem 14, |1 |tr |n |tr = |T |tr implies |T |tr .....obvious 6|= , (T 4) satisfied. (T 1), suppose |= .....need show . Since |= implies |T |r ||r |T |r ,.||tr |=tr . Thus removed line 6 means ...(T de), suppose |T |tr ||tr ||tr . follows 6 ||tr.|T |r ||tr . Thus removed line 6 TCONT means..Proof Theorem 12one direction, suppose T-revision function TBox associatedselection function . need show satisfies (T 1)(T 6) (T f ).(T 1), (T 2), (T 6), (T f ) follow immediately definition T-revision.show proof (T 3)(T 5).(T 3): Suppose coherent. follows definition T-revision function(||tr ) |T |tr . Since faithful respect , |T |tr ||tr (||tr ). Thus|T |tr ||tr |T |tr implies cl(T {}).(T 4): Suppose {} coherent. |T |tr ||tr coherent. followsfaithfulness (||tr ) = |T |tr ||tr . definition T-revision function,(||tr ) |T |tr . |T |tr ||tr |T |tr implies = cl(T {}).(T 5): Suppose coherent. Since coherent preserving, (||tr ) coherent.definition T-revision function, (||tr ) |T |tr . Thus |T |tr also coherentimplies coherent.direction, suppose function TBox satisfies (T 1)(T 6)(T f ). Let defined(||tr ) = |T |trconjunctions TBox axioms . set t-types , conjunctionTBox axioms ||tr = , define (M ) = incoherent; (M ) =|T |tr |T |tr coherent; (M ) = coherent |T |tr incoherent.need show (1) faithful coherent preserving selection function(2) = Tr ((||tr )).Part (1): faithful coherent preserving selection function,function first. follows directly definition (T 6). Letset t-types. Suppose coherent. need show (M ) 6= . definition ,result trivially holds conjunction TBox axioms ||tr = .||tr = , (T 5) |T |tr coherent.(||tr ) = |T |tr 6= .Suppose incoherent. need show (M ) = . definition , resulttrivially holds conjunction TBox axioms ||tr = .||tr = , follows (T f ) |T |tr = . Thus (||tr ) = |T |tr = .374fiDL-Lite Contraction Revisionfaithfulness, suppose coherent. need show |T |tr (M ).definition , result trivially holds conjunction TBox axioms||tr = . ||tr = , follows (T 3)cl(T {}) implies |T |tr ||tr |T |tr . Since (||tr ) = |T |tr ,|T |tr ||tr (||tr ). suppose |T |tr coherent. needshow (M ) = |T |tr . result trivially holds conjunctionTBox axioms ||tr = . ||tr = , follows(T 3) (T 4) = cl(T {}) implies |T |tr ||tr = |T |tr . Since(||tr ) = |T |tr , |T |tr ||tr = (||tr ).coherent preserving, suppose coherent. need show (M ) coherent.result trivially holds conjunction TBox axioms||tr = . ||tr = , follows (T 5)coherent implies |T |tr coherent. Since (||tr ) = |T |tr , (||tr ) coherent.Part (2): definition , (||tr ) = |T |tr . Since follows (T 1)closed, definition Tr = Tr (|T |tr ) = Tr ((||tr )).Proof Proposition 16complexity results explained earlier. Let function= TREVI (T , ) TBox conjunction TBox axioms . need showT-revision function. Theorem 12, suffices show satisfies (T 1)(T 6)(T f ). (T 1), (T 2), (T 6), (T f ) trivially satisfied.(T 3), {} inconsistent cl(T {}) includes axioms thuspostulates holds trivially. suppose {} consistent. |T {}|tr 6=|T {}|tr |T |tr . new axiom added throughout TREVI , |T{}|tr |REV I(T , )|tr implies TREVI (T , ) cl(T {}). (T 4), suppose{} coherent. condition line 4 never fulfilled thus axioms getremoved means TREVI (T , ) = cl(T {}). focus (T 5). Givent-type atomic concept role F , definition t-model F6|=tr B v . hard see {1 , . . . , n } (|T |tr ||tr ) |TREVI (T , )|trt-models picked line 5 TREVI . Due line 48 TREVI , F|T |tr ||tr |=tr F v {1 , . . . , n } F meansF {1 , . . . , n } (|T |tr ||tr ) 6|=tr F v implies |TREVI (T , )|tr 6|=tr F v .Thus TREVI (T , ) coherent.Proof Theorem 13one direction, suppose A-revision function associated selectionfunction . need show satisfies (A 1)(A 6), (A f ).(A 1), (A 2), (A 6), (A f ) follow immediately definition A-revisionfunction. show proof (A 3)(A 5).(A 3), (A 4): |(A {})T |ar = , two postulates hold trivially. suppose|(A {})T |ar 6= implies |AT |ar |{}T |ar 6= . Since faithful, |AT |ar|{}T |ar = (|{}T |ar ). follows definition A-revision function= ATr ((|{}T |ar )) = ATr (|AT |ar |{}T |ar ) = cl(AT {}).(A 5): Suppose |{}T |ar 6= . definition , (|{}T |ar ) 6= . Sincefollows definition A-revision function (|{}T |ar ) |AT |ar , |AT |ar 6= .375fiZhuang, Wang, Wang, & Qidirection, suppose function ABox satisfies (A 1)(A 6), (A f ). Let defined(|{}T |ar ) = |(AT )T |arconjunctions ABox axioms . set a-types , conjunctionABox axioms |{}T |ar = , define (M ) = |AT |ar whenever|AT |ar 6= otherwise. need show (1) faithful selection function(2) = ATr ((|{}T |ar )).Part (1): faithful selection function, function first.follows directly definition (A6). Let set a-types. Suppose 6=. need show (M ) 6= . definition , result trivially holdsconjunction ABox axioms |{}T |ar = . |{}T |ar = ,(A 5) |(AT )T |ar 6= . (|{}T |ar ) = |(AT )T |ar 6= .Suppose = . need show (M ) = . axiom inconsistent, |{}T |ar = . follows (A f ) case |(AT )T |ar = , thus(|{}T |ar ) = |(AT )T |ar = . faithfulness, suppose |AT |ar 6= , need show(M ) = |AT |ar . result holds trivially |{}T |ar = .|{}T |ar = , |AT |ar |{}T |ar 6= implies|A {}T |ar 6= . follows (A 3) (A 4) |clT (A {}T )|ar = |AT |ar .Thus (|{}T |ar ) = |(AT )T |ar = |cl(A {}T )|ar = |AT |ar |{}T |ar .Part (2): definition , (|{}T |ar ) = |(AT )T |ar . Since follows(A1) closed, definition ATr = ATr (|(AT )T |ar ) =ATr ((|{}T |ar )).Proof Proposition 17complexity results explained earlier. Let function= AREVI (AT , ) ABox conjunction ABox axioms . needshow A-revision function. Theorem 13, suffices show satisfies (A1)(A6)(A f ). (A 1), (A 2), (A 6), (A f ) trivially satisfied.(A 3), suppose inconsistent consistent (T , A). line 2line 3 AREVI (T , A, ) guarantee postulate holds. suppose consistentinconsistent (T , A). Since new axiom added lines 68, line9 returned ABox must subset clT (AT {}). (A 4), suppose consistent(T , A). line 4 AREVI (T , A, ) guarantees clT (AT {}) = .(A 5), suppose consistent . consistent (T , A), ABoxreturned line 4 must consistent, inconsistent (T , A), lines 68 guaranteeaxioms inconsistent removed, thus ABox returnedline 9 also consistent.ReferencesAlchourron, C. E., Gardenfors, P., & Makinson, D. (1985). logic theory change:Partial meet contraction revision functions. Journal Symbolic Logic, 50 (2),510530.376fiDL-Lite Contraction RevisionBaader, F., Calvanese, D., McGuinness, D., Nardi, D., & Patel-Schneider, P. (Eds.). (2003).Description Logic Handbook. CUP, Cambridge, UK.Calvanese, D., De Giacomo, G., Lembo, D., Lenzerini, M., & Rosati, R. (2007). Tractablereasoning efficient query answering description logics: DL-Lite family.Journal Automatic Reasoning, 39 (3), 385429.Calvanese, D., Kharlamov, E., Nutt, W., & Zheleznyakov, D. (2010). Evolution DL-Liteknowledge bases. Proceedings 9th International Semantic Web Conference(ISWC-2010), pp. 112128.Dalal, M. (1988). Investigations theory knowledge base revision. Proceedings7th National Conference Artificial Intelligence (AAAI-1988), pp. 475479.De Giacomo, G., Lenzerini, M., Poggi, A., & Rosati, R. (2009). instance-level updateerasure description logic ontologies. Journal Logic Computation, 19 (5),745770.Ferme, E., Krevneris, M., & Reis, M. (2008).axiomatic characterizationensconcement-based contraction. Journal Logic Computation, 18 (5), 739753.Flouris, G., Huang, Z., Pan, J. Z., Plexousakis, D., & Wache, H. (2006). Inconsistencies,negations changes ontologies. Proceedings 21st National ConcferenceArtificial Intelligence (AAAI-2006).Gardenfors, P. (1988). Knowledge Flux: Modelling Dynamics Epistemic States.MIT Press.Grau, B. C., Ruiz, E. J., Kharlamov, E., & Zhelenyakov, D. (2012). Ontology evolutionsemantic constraints. Proceedings 13th International ConferencePrinciples Knowledge Representation Reasoning (KR-2012), pp. 137147.Hansson, S. O. (1991). Belief Contraction Without Recovery. Studia Logica, 50 (2), 251260.Hansson, S. O. (1994). Kernel contraction. Journal Symbolic Logic, 59 (3), 845859.Hansson, S. O. (1997). Semi-revision. Journal Applied Non-Classical Logics, 7 (1-2),151175.Hansson, S. O. (1999). Textbook Belief Dynamics Theory Change Database Updating. Kluwer.Hansson, S. O., & Wassermann, R. (2002). Local change. Studia Logica, 70 (1), 4976.Kalyanpur, A., Parsia, B., Sirin, E., & Cuenca-Grau, B. (2006). Repairing unsatisfiableconcepts OWL ontologies. Proceedings 3rd European Semantic Web Conference (ESWC-2006), pp. 170184.Katsuno, H., & Mendelzon, A. O. (1991). difference updating knowledgebase revising it. Proceedings 2nd International Conference PrinciplesKnowledge Representation Reasoning (KR-1991), pp. 387394.Katsuno, H., & Mendelzon, A. O. (1992). Propositional knowledge base revision minimal change. Artificial Intelligence, 52 (3), 263294.377fiZhuang, Wang, Wang, & QiKharlamov, E., & Zheleznyakov, D. (2011). Capturing instance level ontology evolutionDL-Lite. Proceedings 10th International Semantic Web Conference (ISWC2011), pp. 321337.Kharlamov, E., Zheleznyakov, D., & Calvanese, D. (2013). Capturing model-based ontologyevolution instance level: case DL-Lite. Journal Computer SystemSciences, 79 (6), 835872.Kontchakov, R., Wolter, F., & Zakharyaschev, M. (2010). Logic-based ontology comparisonmodule extraction, application DL-Lite. Artificial Intelligence, 174 (15),10931141.Lenzerini, M., & Savo, D. F. (2011). evolution instance level DL-Liteknowledge bases. Proceedings 24th International Workshop DescriptionLogics (DL-2011).Lenzerini, M., & Savo, D. F. (2012). Updating inconsistent description logic knowledgebases. Proceedings 20th European Conference Artificial Intelligence(ECAI-2012), pp. 516521.Levi, I. (1991). Fixation Beliefs Udoing. Cambridge University Press.Pan, J. Z., & Thomas, E. (2007). Approximating OWL-DL ontologies. Proceedings22nd National Conference Artificial Intelligence (AAAI-2007), pp. 14341439.Qi, G., & Du, J. (2009). Model-based revision operators terminologies descriptionlogics. Proceedings 21st International Joint Conferences Artificial Intelligence (IJCAI-2009), pp. 891897.Qi, G., Haase, P., Huang, Z., Ji, Q., Pan, J. Z., & Volker, J. (2008). kernel revision operatorterminologies algorithms evaluation. Proceedings 7th InternationalSemantic Web Conference (ISWC-2008), pp. 419434.Qi, G., Liu, W., & Bell, D. A. (2006). Knowledge base revision description logics.Proceedings 10th European Conference Logics Artificial Intelligence(JELIA-2006), pp. 386398.Ribeiro, M. M., & Wassermann, R. (2009). Base revision ontology debugging. JournalLogic Computation, 19 (5), 721743.Ribeiro, M. M., Wassermann, R., Flouris, G., & Antoniou, G. (2013). Minimal change:Relevance recovery revisited. Artificial Intelligence, 201, 5980.Satoh, K. (1988). Nonmonotonic reasoning minimal belief revision. ProceedingsInternational Conference Fifth Generation Computer Systems, pp. 455462.Wang, Z., Wang, K., & Topor, R. W. (2015). DL-Lite ontology revision basedalternative semantic characterization. ACM Transaction Computational Logic,16 (4), 31:131:37.Winslett, M. (1990). Updating Logical Databases. Cambridge University Press.Zhuang, Z., Wang, Z., Wang, K., & Qi, G. (2014). Contraction revision DLLite TBoxes. Proceedings 28th AAAI Conference Atificial Intelligence(AAAI-2014), pp. 11491156.378fiJournal Artificial Intelligence Research 56 (2016) 89Submitted 10/15; published 05/16Optimal Any-Angle Pathfinding PracticeDaniel Harabordaniel.harabor@nicta.com.auUniversity MelbourneNational ICT Australia, Victoria Laboratory115 Batman St, Melbourne, 3003, AustraliaAlban Grastienalban.grastien@nicta.com.auNational ICT Australia, Canberra Laboratory7 London Circuit, Canberra, 2601, AustraliaDindar Ozdindar.oz@yasar.edu.trYasar UniversityBornova, Izmir, 35100, TurkeyVural Aksakalliaksakalli@sehir.edu.trIstanbul Sehir UniversityAltunizade, Istanbul, 34662, TurkeyAbstractAny-angle pathfinding fundamental problem robotics computer games.goal find shortest path pair points grid mappath artificially constrained points grid. Prior research focusedapproximate online solutions. number exact methods exist requiresuper-linear space pre-processing time. study, describe Anya: newoptimal any-angle pathfinding algorithm. works find approximate any-anglepaths searching individual points grid, Anya finds optimal pathssearching sets states represented intervals. interval identified on-thefly. interval Anya selects single representative point uses computeadmissible cost estimate entire set. Anya always returns optimal pathone exists. Moreover without offline pre-processing introductionadditional memory overheads. range empirical comparisons show Anyacompetitive several recent (sub-optimal) online pre-processing based techniquesorder magnitude faster common benchmark algorithm,grid-based implementation A*.1. IntroductionAny-angle pathfinding common navigation problem robotics computer games.takes input pair points uniform two-dimensional grid asks shortestpath artificially constrained points grid. anyangle paths desirable compute typically shorter grid-constrainedcounterparts following trajectory give appearance realismintelligence; e.g. player computer game. Despite apparent simplicity anyc2016AI Access Foundation. rights reserved.fiHarabor, Grastien, Oz & Aksakalliangle pathfinding surprisingly challenging. far many successful popular methodsproposed, yet involve trade-offs kind. beginexamples highlight, broad strokes, main research trends limitations,date.communities Artificial Intelligence Game Development any-anglepathfinding problem often solved efficiently using technique known string pulling.idea compute grid-optimal path smooth result; either partpost-processing step (e.g. Pinter, 2001; Botea, Muller, & Schaeffer, 2004) interleavingstring pulling online search (e.g. Ferguson & Stentz, 2005; Nash, Daniel, Koenig, &Felner, 2007). Regardless particular approach, string pulling techniques sufferdisadvantages: (i) require computation finding pathand; (ii) yield approximately shortest paths.communities Robotics Computational Geometry related general problem well-studied: finding Euclidean shortest paths polygonalobstacles plane. Visibility Graphs (Lozano-Perez & Wesley, 1979) Continuous Dijkstra paradigm (Mitchell, Mount, & Papadimitriou, 1987) among best knowninfluential techniques originate line research. Even thoughmethods optimal efficient practice nevertheless sufferoften undesirable properties: (i) search graph1 must pre-computed offlinepre-processing step; (ii) map changes point search graph invalidatedmust recomputed, usually scratch.date, clear exists any-angle pathfinding algorithm simultaneously online, optimal also practically efficient (i.e. least fast practicegrid-based pathfinding using A* search). manuscript, present new workanswers open question affirmative introducing new any-angle pathfindingalgorithm called Anya. approach bears similarity existing worksliterature, notably algorithms based Continuous Dijkstra paradigm.rough overview:methods search individual nodes grid, Anya searchescontiguous sets states form intervals.Anya interval single representative point used derive admissible cost estimate (i.e f -value) points set.progress search process Anya projects interval, one row gridonto another, target reached.Anya always finds optimal any-angle path, one exists. addition Anyarely pre-computation introduce memory overheads (in formauxiliary data structures) beyond required maintain open closed list.theoretical description algorithm previously appeared literature (Harabor& Grastien, 2013). study extend work several ways: (i) give1. distinguish search graph input grid map. Though contexts termscoincide exactly true general. particular search graph may subset inputgrid may related entirely separate data structure.90fiOptimal Any-Angle Pathfinding PracticeVisibleVisibleNon-visible22211100012Non-visible2100120120012Figure 1: Examples visible non-visible pairs points.detailed conceptual description Anya algorithm provide extended theoreticalargument optimality completeness; (ii) discuss practical considerationsarise implementing algorithm give technical description one possibleefficient implementation; (iii) make detailed empirical comparisons showingAnya competitive range recent sub-optimal techniques literature,including based offline pre-processing, one order magnitude betterbenchmark grid-based implementation A*; (iv) discuss range possibleextensions improving current results.2. Optimal Any-Angle Pathfinding Problemgrid planar subdivision consisting W H square cells. cell open setinterior points traversable non-traversable. vertices associatedcell called discrete points grid. Edges grid interpretedopen intervals intermediate points; one representing transition twodiscrete points. type point p = (x, y) unique coordinate x [0, W ]= [0, H], discrete points limited subset integer x values.discrete intermediate point traversable adjacent least one traversablecell. Otherwise non-traversable. discrete point common exactly fouradjacent cells called intersection. intersection three adjacent cellstraversable one called corner. Two points visible one anotherconnected straight-line path (i.e. sequence adjacent points, eitherintermediate discrete) not: (i) pass non-traversable point(ii) pass intersection formed two diagonally-adjacent non-traversable cells.Figure 1 shows examples help better illustrate idea.any-angle path sequence points hp1 , . . . , pk pi visible pi1pi+1 . length cumulative distance every successivepair pointsp0000d(p1 , p2 )+. . .+d(pk1 , pk ). function d(p = (x, y), p = (x , )) = (x x )2 + (y 0 )2uniform Euclidean distance metric. say pi turning point segments(pi1 , pi ) (pi , pi+1 ) form angle equal 180 2 . Finally, any-angle pathfindingproblem one requires input pair discrete points, t, asks anyangle path connecting them. point designates source (equivalently, start) location2. well-known turning points optimal any-angle paths corner points; e.g. shownMitchell et al. (1987).91fiHarabor, Grastien, Oz & Aksakallipoint designates target (equivalently, goal) location. path optimalexists alternative any-angle path strictly shorter.Figure 2 provides example optimal any-angle pathfinding problem.seen source, target obstacles discrete positions however pathneed follow grid. Notice also trajectory path appears muchrealistic alternative restricted turning modulo 45 deg 90 deg.43210012345678Figure 2: Example any-angle pathfinding problem together solution.3. Overview AnyaConsider any-angle instance shown Figure 3. example optimal pathneeds first head towards corner point n change directiontoward target t. One possible approach solving problem involves computingvisibility graph: i.e., identifying pairs corners visible one another,also visible start target locations, searching pathgraph. main drawback case visibility graph quite large (upquadratic size grid) expensive compute.alternative approach, avoids overheads, solve problem online.Unfortunately online search methods generally consider discrete points gridimmediate neighbours. example, expanding point commongenerate neighbours: (1, 0), (2, 1), (3, 0) exampleFigure3. A*f -valuethree neighbours is, respectively, 1 + 34 ' 6.83, 1 + 20 ' 5.47,1 + 26 ' 6.1 (using Euclidean-distanceheuristic). comparison optimalany-angle path cost 10 + 5 ' 5.4. Immediately see heuristichand satisfy one essential properties A* search: f -valuenode always underestimate actual distance goal. Withoutproperty A* guaranteed optimal.issue described comes fact optimal path gopoints (1, 0), (2, 1), (3, 0). Instead optimal path crosses row 1 point y1 ,part search space. ensure optimality consider pointsy1 rather discrete points grid. however manypoints including e.g., points y10 (leading (3, 6)), apriori seems reasonablecandidate expansion, appear optimal path.92fiOptimal Any-Angle Pathfinding Practice654n32y2y101y100123456Figure 3: pathfinding n, online algorithms A* Theta*expand discrete points grid never intermediate points yi .general need consider potential yi points defined fraction whh {0, . . . , H} w {1, . . . , W }. set quadratic n = min(W, H). understand why, consider Farey Sequence order n, sequence (ordered increasingnumber) rational numbers 0 1 written fraction whosedenominator integer lower n. instance, Farey Sequence order n = 6is: 0, 16 , 15 , 14 , 31 , 25 , 21 , 53 , 23 , 43 , 54 , 65 , 1. Notice 13 = 26 , explains lengthsequence n(n + 1) 2; still asymptotic cardinality sequence known23n(Graham, Knuth, & Patashnik, 1989, ch. 9).2Since quadratic behaviour Farey Sequence makes impractical enumeratepotential yi points propose consider, instead individual points, set pointsappear together part contiguous interval grid. example Figure 3would consider points lying (0, 1) (3, 1), timepart single A* search node. framework need to:define formally Anya search node,define set successors search node,define compute f -value search node,prove optimality returned path,terminate search path available,ensure Anya algorithm efficient practice.93fiHarabor, Grastien, Oz & Aksakalli4. Algorithm Descriptionsection presents detail Anya algorithm properties. Since Anyavariant A* first present search space: search nodes, successors nodeevaluation function used rank nodes search. give pseudo-codedescription algorithm discuss properties. Improvements make Anyaefficient practice presented next section.4.1 Anya Search Nodesdefine notion interval, core Anya.Definition 1 grid interval set contiguous pairwise visible points drawndiscrete row grid. interval defined terms endpoints b.possible exception b, interval contains intermediate discretenon-corner points.definition, points interval share position, positiveinteger. Moreover, x position points interval (including endpointsb) rational number3 . use normal parentheses ( ) indicateinterval endpoint open square brackets [ ] indicate interval endpointclosed. example, interval = (a, b] open (i.e. include)closed (i.e. include) b.Identifying intervals simple: row grid naturally divided maximally contiguous sets traversable non-traversable points. traversable set formstentative interval split, repeatedly necessary, corner pointsend points intervals. Intervals also identified operation called projection. discuss procedure next sub-section. note intervalsproduced way projection also non-discrete non-corner endpoints.significant advantage Anya construct intervals on-the-fly. allowsus start answering queries immediately discrete start-target pair. Similaralgorithms, e.g. Continuous Dijkstra (Mitchell et al., 1987), require pre-processing stepqueries answered single fixed start point.Definition 2 search node (I, r) tuple r 6 point called rootinterval point p visible r. represent start node itself,set = [s] assume r located plane visible s; cost rcase zero.A* search node, together parents, traditionally represents single pathdefined travelling straight line points search nodes rootcurrent node. Anya search node similarly defines paths obtained visitingroots nodes ending interval current node. node thereforerepresents many paths root search node always last (common) turning3. per problem definition, every point (x, y) appearing optimal any-angle path belongsFarey Sequence points rational.94fiOptimal Any-Angle Pathfinding Practicepoint paths: always either root parent node one endpoints parent interval.Besides start node, treat special case, two typessearch nodes: cone nodes flat nodes. example cone node shown Figure 4.nodes characterised fact root r rowassociated interval I. Notice example although interval = [a, b] maximal,endpoints obstacles, corners indeed even discrete pointsgrid (here left endpoint (2.5, 4) right endpoint b (5.5, 4)). Examplesflat nodes shown Figure 5. two nodes are: ((a1 , b1 ], r) ((a2 , b2 ], r). Flatnodes characterised fact root r row interval I.Notice examples given a1 = r (resp. a2 = b1 ) excluded first (resp.second) interval. semantics every search node current position locatedsomewhere interval reach point any-angle path whoserecent turning point r.554b433221r = a1b1 = a2b21r000123456Figure 4: Example cone search node.0123456Figure 5: Example two flat search nodes.4.2 Searching Anya: Successorssuccessors search node n identified computing intervals sets traversablepoints; row grid current node n rows immediatelyadjacent. want guarantee point set reached rootn via local path taut. Taut simply means pull endpointspath cannot make shorter. provide formal definition successordiscuss definition applied practice.Definition 3 successor search node (I, r) search node (I 0 , r0 )1. points p0 0 , exists point p local path hr, p, p0 taut;2. r0 last common point shared paths hr, p, p0 i;3. 0 maximal according points definition search node.95fiHarabor, Grastien, Oz & Aksakallifirst requirement (tautness) implies successor p0 0 reachedroot current node r path locally optimal. use propertynext subsection show Anya always finds globally optimal path one existsall. third property, requiring successor interval maximal,exists purpose practical efficiency: simply put, want arbitrarilysmall arbitrarily many successors. Instead, make successor intervallarge possible. second property two interpretations. r0 = r saysuccessor node observable. Similarly r0 = p say successornon-observable. explore ideas turn.5v14v2u2u3v3r03=bu121r001234567Figure 6: Successors cone search node, n = ([a, b], r). five successors: ([v1 , v2 ], r) ((v2 , v3 ], r) observable ((r0 , u1 ], r0 ), ((v3 , u2 ), r0 ),([u2 , u3 ], r0 ) not.5432bce100123456Figure 7: Successors flat search node, n = ((a, b], a). two successors: ((b, c], a)observable ([d, e], b) not.96fiOptimal Any-Angle Pathfinding PracticeAlgorithm 1 Computing successor set1: function successors(n = (I, r)). Takes input current node2:n start node3:return generate-start-successors(I = [s])4:end5:successors6:n flat node7:p endpoint farthest r. Successor interval starts p8:successors generate-flat-successors(p, r). Observable successors9:p turning point taut local path beginning r10:successors successors generate-cone-successors(p, p, r) . Non-observable successors11:end12:else. node flat, must cone13:left endpoint14:b right endpoint15:successors generate-cone-successors(a, b, r). Observable successors16:turning point taut local path beginning r17:successors successors generate-flat-successors(a, r). Non-observable18:successors successors generate-cone-successors(a, a, r). Non-observable19:end20:b turning point taut local path beginning r21:successors successors generate-flat-successors(b, r). Non-observable22:successors successors generate-cone-successors(b, b, r). Non-observable23:end24:end25: end functionobservable successor characterised fact points p0 0 visiblecurrent root point r. case last common point shared local pathsform hr, p, p0 r. Observable successors computed projecting current intervalnext row. projection identifies maximal interval Imax splitinternal corner point point. interval produced split operation leadsnew observable successor, successors share root point original(parent) node. process illustrated Figure 6 interval = [a, b] projectedonto next row. projection identifies maximal observable interval Imax = [v1 , v3 ]subsequently split create two observable successors: ([v1 , v2 ], r) ((v2 , v3 ], r).comparison, non-observable successor characterised fact points0p 0 visible current root r. case local paths formhr, p, p0 must pass (visibility obstructing) corner point whose identity r0 := p.Figure 6 illustrates process computing non-observable successors. First,non-observable points right current interval = [a, b], construct singleflat successor 0 = (b, u1 ] root r0 := b. Non-observable points also existleft current interval local path point (from r a)taut. non-observable successors found rows grid adjacentcurrent interval I. projecting corner endpoint b onto next row gridconstruct two non-observable successors: ((v3 , u2 ), b) ([u2 , u3 ], b).Algorithm 1 give overview procedure generates successor setsearch node. overview sub-functions appearing Algorithm 1 given97fiHarabor, Grastien, Oz & Aksakalliappendix. implementation straightforward, requiring nothing complicatedgrid scanning operations linear projections.important note stage Anya perform visibility checksgeneration successor nodes. Visibility checks heart many contemporary online algorithms, including Theta* (Nash & Koenig, 2013), must determinewhether successor visible node (e.g. grand-parent node).one hand visibility checks help Theta* et al. find shorter paths expand fewer nodestraditional A* search. hand, computational overhead introducedchecks means run-times often larger A*. comparison Anyaprojects interval I, one row grid next, process involves localreasoning. particular determine projection Imax valid, invalidneeds clipped simply testing traversability cells located above,left right current interval proposed Imax . eliminationvisibility checks important practical advantage Anya. see Section 9,Anya finds shorter paths online methods Theta* et al. alsousually much efficient terms running time.illustrate Algorithm 1 using previous examples. Consider flat node ((a, b], a)Figure 7. point p Line 7 set b observable flat successor ((b, c], a)generated Line 8. Furthermore since b turning point (Line 9), intervalImax = [d, e] considered. Since Imax contains interior corner points splitsingle non-observable cone successor (I = Imax , b) generated (Line 10).Next, consider cone node ([a, b], r) Figure 6. First generate observablesuccessors (Line 15): interval [a, b] projected maximal interval Imax = [v1 , v3 ]identified. Imax split internal corner point v2 leading two observable conesuccessor nodes, (I1 = (v1 , v2 ], r) (I2 = (v2 , v3 ], r). Notice line-of-sight visibilitycheck required here. Next since b turning point look non-observable successorswell (Lines 20-22). flat successor ((b, u1 ], b) generated per previous example.Meanwhile maximal (non-observable) cone interval Imax = (v3 , u3 ] also identified.interval split internal corner point u2 resulting two non-observable conesuccessor nodes, (I3 = (v3 , u2 ], b) (I4 = (u2 , u3 ], b).Algorithm 1 treats start node (Lines 2-4) special case root pointlocated grid. successors start node (i) non-observable intervalsroot (ii) found left right start location, rowimmediately start location row immediately below.4.3 Evaluating Anya Search Nodesearch procedure Anya, similarly A*, always expands promising node found far. therefore necessary evaluate root interval pair.evaluation corresponds estimate f minimal length pathsource target current interval. optimality condition A*estimate optimistic (i.e. never larger actual optimal path length).classical A* search node n corresponds single point p grid valuef (n) computed sum g(p), length path source p, h(p),(under)estimation length shortest path p target.98fiOptimal Any-Angle Pathfinding Practicesearch node n = (I, r) represents set points f value minimum f valuepoints node:f (n) = inf f (s, r, p, t)pIf (s, r, p, t) (under)estimate shortest path r p.noted that, set points p continuous potentially open,minimum replaced infimum. Since points interval visible r,value broken follows:f (s, r, p, t) = g(r) + d(r, p) + h(p)d(r, p) distance points r p.Finding point interval minimises f value may seem like hard problemsince interval contains large number points want avoid generatingthem. However straight-line distance heuristic h (h(p) = d(p, t)) makes easy isolatepoint p minimises f value, thanks two simple geometric observations.precise heuristics available could make harder find point p.Lemma 1 Let r two points s.t. interval row rowrows r t. point p infimal f -value pointclosest intersection straight-line path ht, ri row I.line r intersects interval point p intersection.Otherwise point p one endpoints interval. event precondition Lemma 1 satisfied, possible replace mirrored version t0thus satisfy precondition. case described Lemma 2.Lemma 2 mirrored point t0 target interval d(p, t) = d(p, t0 )p I.Lemma 2 trivial geometrical result. lemmas illustrated Figure 8.4.4 Search Proceduresearch procedure employed Anya presented Algorithm 2. follows patternA* uses priority queue, open, stores yet-to-be-expanded search nodesordered f value. node stores pointer parent. step searchAnya extracts best node open checks corresponding interval containstarget. event target found (Line 6) returned path sequenceroot points constructed following back-pointers, current node startlocation. target found current node expanded successorsadded priority queue (Line 8). successors may considered redundantsafely discarded without insertion priority queue (Line 9). discussaspect algorithm Section 6; suffices know successorsoptimal path. expansion process continues target foundopen list exhausted, case algorithm returns failure (Line 14).next sections prove fundamental properties algorithm:correctness, optimality completeness.99fiHarabor, Grastien, Oz & Aksakalli4t1t04t23t32b1t40012r3456Figure 8: illustration Lemmas 1 2. evaluate node n = ([a, b], r).points t1 t04 correspond case row target intersects intervalI; t2 t3 not; t4 mirrored target t04 must used.5. Correctness Optimalitysection prove Anya correct always finds optimal path. particularshow (i) optimal path appears search space, (ii) targetexpanded found optimal path, (iii) node search spacereached finite number steps. topics termination completenessdiscussed Section 6.begin analysis recalling search node n = (I, r) represents set potentialpaths (from r r point p I). Following semantics sayn search node path r intersects .Lemma 3 n = (I, r) search node optimal path then: either n containstarget n least one successor n0 also search node .Proof: Start node: n start node = [s] r located grid. Additionally, n search node (hypothesis). Algorithm 1 (Line 3) scans traversableAlgorithm 2 Anya1:2:3:4:5:6:7:8:9:10:11:12:13:14:input: Grid, source location s, target locationopen {(I = [s], r0 )}open empty(I, r) pop(open)return path to(I)end(I 0 , r0 ) successors(I, r)prune(I 0 , r0 )open open {(I 0 , r0 )}endendendreturn null100. Start node root r0 located grid. Successor pruningfiOptimal Any-Angle Pathfinding Practicepoints grid visible adjacent s. points locatedleft right located rows immediately immediatelyrow s. Algorithm 1 assigns points interval 0 0associated successor node root r0 = s. Every optimal path must pass= [s] traversable points reached withoutpassing interval associated successor s. sufficient satisfylemma.nodes: n arbitrary node 6 (if successorsdone). definition r p (apriori unknown) intersectioninterval I. two possibilities consider, depending whetherp turning point not. show cases successor n whoseinterval 0 intersects , sufficient satisfy lemma.Case 1 p turning point. Algorithm 1 (Lines 8 15) scans pointsadjacent (straight-line) visible r I. point assignedsuccessor observable interval 0 root point r. Thus least onesuccessors n intersects every straight line path r p meansleast one successor n intersects .Case 2 p turning point. case p must corner endpoint I, otherwisetaut thus cannot optimal. Algorithm 1 (Lines 10, 17, 18, 21) scanspoints adjacent reachable r p taut local path.points located row p row immediately adjacent.point assigned successor non-observable interval 0 rootr = p. process exhaustive points reachable taut local path,r though p, must assigned interval. Thus must intersect least onesuccessors n.Corollary 4 path source target, open list alwayscontains search node optimal path (or node currently processed).Proof: induction.base case: initial search node node path s.inductive case: assume open list contains search node optimal path.node removed expanded. node containtarget know Lemma one successor generated search nodeoptimal path. Therefore new search node optimal path insertedopen list.Lemma 5 first expanded node contains target corresponds one optimalpath t.101fiHarabor, Grastien, Oz & AksakalliProof: Sketch. First notice f -value node indeed minimal valuenodes interval, means f estimate () actual costtarget. Second notice that, given search node (I, r) successor (I 0 , r0 ),point p0 0 , f -value p0 greater equal f -value pointp ( p = r0 r0 6= r; p intersection (r, p0 ) otherwise); f functiontherefore monotonically increasing. Finally, f function search node (I, r)length path I. Hence f function nodes representing sub-optimalpath eventually exceed optimal path distance, f function nodesrepresenting optimal path always remain value.Lemma 6 target reachable Anya eventually expand node whose intervalincludes target.Proof: contradiction, assume Anya expand node whose interval includestarget. Lemma 5 know failure expand node means Anyaexpand infinitely many nodes. shall prove implies f valuenodes unbounded and, therefore, target reachable.search nodes (I 0 , r0 ), interval 0 different row parent(I, r). Therefore, nodes, value g(p0 ) larger value g(p) 1 more.happen node flat, bounded numbersuccessive flat nodes.4 Hence infinite sequence successive Anya nodes infinite length. Finally Anya node bounded number successors, meaninginfinite number expansions generate infinite number successive nodes.6. Completeness Terminationspecified policy Anya detect nodespreviously expanded. context optimal A* search policy essentialprevent cyclical re-expansion ensure algorithm eventually terminates,even path start target locations. section describepolicy Anya. Conceptually similar A* closed list approach workstracking best g-value associated every root location (cf. every search node)encountered search.motivating example consider Figure 9 root r reached via two pathsdifferent length. example green path strictly longer red pathpoints reached via green path g-value strictly largerpoint reached via red path. Figure 10 shows similar example greenred paths reach root point r cost, resulting two identical copiessuccessor node (I, r). Without strategy handle root-level redundanciessearch process generate many unnecessary nodes slow progress goal.Moreover, exists path start target location, search may4. And, furthermore, value g(p) increase significantly unobservable flat cone.102fiOptimal Any-Angle Pathfinding Practice665r4322110r430501234560Figure 9: Root r reached via two pathsdifferent lengths.123456Figure 10: Root r reached via twopaths equal length.terminate (e.g. input graph contains cycles possible endlessly generatecopies states ever increasing g-values).propose following strategy avoid root-level redundancies:1. store hash table visited roots best g-values. call tableroot history apply similar way (and indeed lieu of) traditionalA* closed list.2. generating search node n check root already root historyg-value less equal current g-value.3. current g-value root improves value stored root historyadd node open list. also update g-cost root5 roothistory list.4. Alternatively, current g-value root improve value storedroot history simply discard node (i.e. added open).root history implemented hash table. size O(n) n numberdiscrete points given input map. show keeping root history listaffect correctness optimality search Anya indeed completeterminate.Lemma 7 Anya search prunes sub-optimal paths.5. Similar updates nodes closed list sometimes performed context incremental,bounded cost bounded sub-optimal search. updates performed part operation callednode re-opening. updates node re-opening. particular root points neverdirectly expanded thus never appear open list (Anya search comprise root-interval pairs).103fiHarabor, Grastien, Oz & AksakalliProof: Trivial. search node root sub-optimal g value, representssub-optimal path.Lemma 8 Anya always terminates.Proof: Anya terminate, must explore paths arbitrary length. pathsmust eventually involve root twice root different in-between. Letn n0 two search nodes. g value associated n0 must higherg value associated n and, therefore, node n0 must pruned. Indeed sufficientlylong paths pruned open list eventually empty.Lemma 9 Anya redundant node pruning keeps least one optimal path.Proof: search node n = (I, r) removed exists another search node n0 (butdifferent search parents) smaller (or equal) g-value kept. Assumen search node optimal path p1 , . . . , pk , let pi point pathintersects I. Since g-value n similar n0 , exists another pathp01 , . . . , p0i , pi+1 , . . . , pk similar length, path pruned.7. Practical Pruning StrategiesA* orders nodes expansion evaluating ranking promising appear(i.e. f -values). is, however, possible alter order expansion withoutcompromising guarantees provided A*: correctness, optimality completeness.Indeed strategy even positive effect efficiency overall search.section discuss two practical strategies modify expansion order speedsearch. enhancements applied on-the-fly focus reducing sizepriority queue. first strategy, Cul-de-sac Pruning, identifies nodes safelydiscarded cannot possibly lead goal. second strategy, IntermediatePruning, similar works avoiding explicit generation nodessingle successor (these successors expanded immediately, without addedopen list).7.1 Cul-de-sac PruningOne way reducing size priority queue involves early identification culde-sacs (cds). cds search node successor contain target.definition cds need added open list since expansion cannotlead target. simple test identify cds nodes given Algorithm 3 wayprocedure Is-cul-de-sac.Early pruning cds nodes speeds search (and reduces required memory) preventing unnecessary operations open also reducing size list,104fiOptimal Any-Angle Pathfinding PracticeAlgorithm 3 Cul-de-sac intermediate node pruning.1: function Is-cul-de-sac(n = (I, r)). Assumes contain target point2:Imax projection n. Flat projection cone projection depending n3:Imax valid. Valid means every p Imax visible r4:return f alse. n cannot cul-de-sac; least one successor interval 0 Imax5:end6:return true. n cul-de-sac; cannot projected successors7: end function8: function Is-Intermediate(n = (I, r)). Assumes contain target point9:n flat node10:p endpoint furthest r11:p turning point taut local path prefix hr, pi12:return f alse. n least one non-observable successor; cannot intermediate13:end14:else. n flat node must cone node15:closed endpoint also corner point16:return f alse. n least one non-observable successor; cannot intermediate17:end18:0 interval projecting r19:0 contains corner points20:return f alse. n one observable successors; cannot intermediate21:end22:end23:return true24: end functionmakes every operation faster. reference, open list implementedbinary heap add remove operation time complexity log n, nsize list. Examples cds pruning, cone nodes flat nodes, illustratedFigure 11 Figure 12. cases current node root shown blueintervals red pruned.7.2 Intermediate Pruningsecond pruning strategy described pushing expansion one directionfar possible long increase branching factor. Practically, searchnode generated guaranteed one successor, immediatelygenerate successor instead originally intended node. said successor alsoone successor process recursively applied. Examples showing applicationintermediate pruning given Figure 13 cone nodes Figure 14 flat nodes.simple test identify intermediate nodes given Algorithm 3 way procedureIs-Intermediate.first obvious benefit intermediate pruning reduction number operations open list. However second benefit pushing expansion nodelead cul-de-sac. happens node added open list,helps keep size list small operations list fast.potential issue Intermediate Pruning recursive application nonpromising successor nodes could costly (in terms time) simply adding105fiHarabor, Grastien, Oz & Aksakalli443322c1e fbrbc1r0001234560Figure 11: Cul-de-sacs cone nodes:nodes ([c, d), r) ((e, f ], r) generated.2123456Figure 12: Cul-de-sac flat nodes: node((b, c], r) generated.4314r3bc2rbc1000123456Figure 13: Intermediate node ([a, b], r)one successor, ([c, d], r), immediately generated.0123456Figure 14: Intermediate node ([a, b], r)one successor, ((b, c], r) immediately generated.nodes open. discuss issue detail Section 7.3. also noterun-time experiments application Intermediate Pruning net positive effectperformance search.7.3 Discussionintroduced two different ways nodes frontier searchpruned: Cul-de-sac Pruning Intermediate Pruning. modify expansion ordersearch improve performance along single fixed path. pruningaway sterile branches skipping intermediate locations actual branchingoccurs. Similar strategies previously discussed literature. example Culde-sac Pruning based set principles Dead-end Heuristic (Bjornsson &Halldorsson, 2006); although method reasons locally applied purely online.Intermediate Pruning shares similarities Fast Expansion (Sun, Yeoh, Chen, &106fiOptimal Any-Angle Pathfinding PracticeKoenig, 2009); main difference prune nodes without reference f -value.Intermediate Pruning also similar Jump Point Search (Harabor & Grastien, 2014)applied outside context symmetry breaking extended sets points takenintervals rather applied individual cells grid.Anyas root history list, discussed Section 6, also regarded type pruningenhancement. case reason generally set possible pathscould used reach given point prune away successors cannotpossibly optimal path. approach taken similar principle(but practice) pruning redundant states real-time search (Sturtevant &Bulitko, 2011).Pruning search nodes Anya difficult classical A* searchmany modern progenitors. Anya node represents set positionsrather one. Consider example Figure 15; particularly interestedinterval [a, b] generated root r1 r2 . shortest pathr1 ( 14.24 14.99 r2 ). However obstacle putcell labeled O, optimal path switches r2 ( 15.62 15.94).diagram suggests that, given target two search nodes sharinginterval, may possible prune either them.situation described Figure 15 uncommon practice examplesmay motivate us derive new sophisticated pruning rules enhanceperformance Anya algorithm. must careful however weigh improvedpruning power new techniques overhead applying firstinstance. example, alternative (arguably, better) approach avoiding redundantnode expansions keep interval history list addition (or instead of) root history.method would certainly avoid problem outlined Figure 15 manypossible intervals roots, means size hash table potentiallymuch larger memory accesses potentially slower. Additionally, comparing intervalsequality membership requires extra time may worth investment6 .6. attempted similar experiment results clearly positive.107fiHarabor, Grastien, Oz & Aksakalli111098765br14r23210012345678910111213Figure 15: Illustrating search nodes cannot trivially pruned search nodesn1 = ([a, b], r1 ) n2 = ([a, b], r2 ): obstacle optimal pathgoes n1 (red); otherwise goes n2 (blue).8. Experimental Setupconduct experiments seven benchmark problem sets taken Nathan Sturtevantswell known repository (Sturtevant, 2012). Three benchmarks originate popularcomputer games often appear literature. are: Baldurs Gate II , DragonAge Origins StarCraft. maps benchmarks vary size; severalthousand nodes several million. remaining four benchmarks comprise gridssize 512 512 randomly placed obstacles varying densities, 10% 40%.Table 1 gives overview benchmark problems. give number mapsinstances per problem set distribution number node expansions requiredreference algorithm, A* using octile distance heuristic7 , solve problemsbenchmark set. latter metric gives us baseline comparing difficultyproblems appearing benchmark set.7. Octile distance analogous Manhattan distance generalised 8-connected grids.108fiOptimal Any-Angle Pathfinding PracticeBenchmark#Maps#InstancesBaldurs Gate IIDragon AgeStarCraftRandom 10%Random 20%Random 30%Random 40%7515675101010109316015946519823016770177401920035360Nodes Expanded A*MinQ1 Median MeanQ3Max StDev21662019 6302 917086720913616225880 14080 19150 126800 197443 480826840 50000 70110 578900 6350722395481886 148559280392137493869 8606 146805376099054 352014190 20290 3371096090 191623 1252042850 51920 83770 169900 43558Table 1: overview seven benchmark problems used experiments. givenumber maps problem instances benchmark distribution nodes expandedreference algorithm (A*) solving problems benchmark set.compare purely online optimal Anya algorithm number state-ofthe-art any-angle techniques. are: Theta* (Nash et al., 2007), Lazy Theta* (Nash,Koenig, & Tovey, 2010), Field A* (Uras & Koenig, 2015a) any-angle varianttwo-level Subgoal Graphs (SUB-TL) (Uras & Koenig, 2015b). approachesnear-optimal guaranteed return shortest path. methods Theta*, LazyTheta* Field A* purely online. SUB-TL relies offline pre-processingstep improve performance search. use C++ implementationsalgorithms; source codes made publicly available Uras Koenig (2015a).Anya implemented Java executed JVM 1.8. allow comparisons acrossdifferent implementation languages use A* algorithm (Hart, Nilsson, & Raphael,1968), implemented C++ Java, reference point8 . compare performance Anya Java implementation A* algorithmsC++ implementation A*. experiments performed 3GHz Intel Core i7machine 8GB RAM running OSX 10.8.4. Source code implementationAnya available https://bitbucket.org/dharabor/pathfinding.9. Resultsevaluate performance using three different metrics: search time, nodes expandedpath length. results presented relative benchmark algorithm, A*,combine standard octile distance heuristic. example, comparing searchtime nodes expanded, give figures relative speedup algorithm vsA*. paradigm search time speedup 2 means twice fast nodeexpansion speedup 2 means half many nodes expanded. comparing pathlength give percent improvement path length vs A*. cases higher better.8. C++ implementation due Uras Koenig (2015a); Java implementation own.109fiHarabor, Grastien, Oz & AksakalliBenchmarkBaldurs Gate IIDragon AgeStarCraft 40%Random 10%Random 20%Random 30%Random 40%Avg. Node Expansion SpeedupAnya Theta* L.Theta* F.A* SUB-TL91.131.951.96 1.01 907.1019.601.051.05 0.9057.4540.731.271.27 0.95 166.000.802.342.38 1.146.600.771.231.17 0.802.561.060.820.75 0.641.682.200.900.86 0.822.40Avg. Path Length Improvement (%)Anya Theta* L.Theta* F.A* SUB-TL4.65% 4.62%4.61% 4.38%4.58%4.34% 4.27%4.22% 4.05%4.28%5.02% 4.95%4.92% 4.70%4.88%4.77% 4.63%4.58% 3.83%4.59%4.57% 4.34%4.15% 3.26%4.30%4.44% 4.12%3.77% 3.12%4.03%4.14% 3.95%3.48% 3.22%3.74%Table 2: compare performance algorithm terms average node expansion speedupaverage path length improvement. metrics taken respect reference algorithm(A*). cases higher better.begin Table 2 shows average performance figures nodes expandedpath length seven benchmark problem sets. make following observations:Anya best four purely-online algorithms, expanding fewer nodesfive seven benchmarks. three benchmarks drawn real computergames Anya expands one order fewer nodes, average, nearest purelyonline contemporary. pre-processing-based SUB-TL algorithm expands fewernodes, average.Anya, methods comparison, struggles achieve speedupfour random benchmarks. two four cases performancereference A* algorithm. Again, pre-processing-based SUB-TL algorithmable achieve consistent, though much reduced, node expansion speedup.Anya, optimal, shows best improvement path length; however algorithms comparison close optimal, average.Next, evaluate performance terms search time. Rather taking simpleaverage per benchmark basis (or across benchmarks) instead sort instancesaccording difficulty, measured number node expansions requiredreference A* algorithm solve problem. approach gives holistic overviewperformance reduces effect bias associated selection instancescomprise benchmark set9 . Results analysis given Figure 16.make following observations:Anya often one order magnitude faster reference A* algorithm benchmarks drawn real computer games. Performance mixedfour random benchmarks, evaluated methods struggling achievespeedup.9. per Table 1, problem instances regarded easy often outnumber instancesregarded hard. difference effect skewing performance indicatorscomputed simple averages instances benchmark set.110fiOptimal Any-Angle Pathfinding PracticeBenchmarksBaldur's Gate IIAnyaTheta*Lazy Theta*Field A*SUBTLSpeedup vs A*100101000AnyaTheta*Lazy Theta*Field A*SUBTL100Speedup vs A*100010110.10.1102103104Nodes Expanded A*105106102103Dragon Age Origins104Nodes Expanded A*105106StarCraftAnyaTheta*Lazy Theta*Field A*SUBTLSpeedup vs A*100101000AnyaTheta*Lazy Theta*Field A*SUBTL100Speedup vs A*100010110.10.1102103104Nodes Expanded A*105106102Random; 512x512 10% obstaclesSpeedup vs A*105106Random; 512x512 20% obstaclesAnyaTheta*Lazy Theta*Field A*SUBTL100104Nodes Expanded A*101000AnyaTheta*Lazy Theta*Field A*SUBTL100Speedup vs A*100010310110.10.1102103104Nodes Expanded A*105106102Random; 512x512 30% obstaclesSpeedup vs A*105106Random; 512x512 40% obstaclesAnyaTheta*Lazy Theta*Field A*SUBTL100104Nodes Expanded A*101000AnyaTheta*Lazy Theta*Field A*SUBTL100Speedup vs A*100010310110.10.1102103104Nodes Expanded A*105106102103104Nodes Expanded A*105106Figure 16: Search time speedup. compare performance seven benchmarksterms search time. Figures given relative speedup vs. reference A* algorithm.Problem instances sorted difficulty using A* node expansion rank. Noteplot log-log.111fiHarabor, Grastien, Oz & AksakalliAnya fastest four purely online methods evaluation. performance often comparable pre-processing based SUB-TL technique and,particularly challenging instances StarCraft domain, Anya non-dominated10 .Anyas performance terms search time less value suggested(previously evaluated) node expansion metric. reflects fact nodeexpansion made Anya involves analysing grid; looking roots searchingintervals.9.1 Discussionseen Anya compares well current state-of-the-art any-angle pathfindingalgorithms. (almost) apples-to-apples comparison three contemporary purelyonline search technique (Theta*, Lazy Theta* Field A*) seen Anya usuallyexpands fewer nodes per search terminates one order magnitude faster.results underscored fact Anya online algorithmguaranteed return Euclidean-optimal path. may surmise that, many casesapplications, Anya appears preferable alternative algorithms.Next, make apples-to-oranges comparison purely online Anya algorithm near-optimal offline enhanced SUB-TL algorithm. seenAnya usually fast SUB-TL performance sometimes comparable.Moreover, Anya retains advantage solving especially challenging instances drawnreal computer games. SUB-TL appears preferable Anya cases additional space time available create store associated subgoal graph casesoverheads amortised many online instances. extra spacetime available, cases map subject change (e.g. new obstaclesadded existing obstacles removed), Anya appears preferable SUB-TL.main strength Anya searches sets nodes grid ratherconsidering individual locations one time. Expansion thus consideredmacro operator, meaning Anya bears similarity speedup techniques usinghierarchical abstraction; e.g. HPA* (Botea et al., 2004). important differenceAnya constructs abstract graph on-the-fly rather part pre-processing step.One current drawback associated Anya nodes contain overlappingintervals. occurs interval reachable two different root points, neitherpruned (e.g. root locations reached first time;illustrated Figure 15). nodes are, either part whole, redundantprovided f -value smaller optimal distance goalbeget yet redundant successors. see behaviour especially resultsbenchmarks Random 10% Random 20% SUB-TL achieves speedup severalfactors Anya struggles maintain parity reference A* algorithm. seemsreasonable improve current algorithm attempting identify overlaps orderprune consideration. efficient effective algorithm achievinggoal subject work.10. Pareto sense; i.e. problem instances Anya better SUB-TL accordingmetric interest node expansions search time112fiOptimal Any-Angle Pathfinding Practice10. Related WorkAmong simplest popular approaches solving any-angle pathfindingproblem string-pulling. main idea find path input grid map, oftenusing variant A* (Hart et al., 1968), post-process path orderremove unnecessary turning points. Several methods appeared literatureGame Development; e.g. see work Pinter (2001) Botea et al. (2004).number algorithms improve string-pulling interleaving node expansionpath post-processing online search. Particular examples include Field D* (Ferguson& Stentz, 2005) Field A* (Uras & Koenig, 2015a), use linear interpolationsmooth grid paths one cell time, Theta* (Nash et al., 2007), introducesshortcut time successful line-of-sight check made; parent currentnode successors. Though still sub-optimal many cases approachesnevertheless attractive able search purely online efficientpractice. addition two examples given numerous works, oftenappearing literature Artificial Intelligence, apply improve basicinterleaving idea. refer interested reader Nash & Koenig, 2013 recent surveyoverview.Accelerated A* (Sislak, Volf, & Pechoucek, 2009) online any-angle algorithmconjectured optimal strong theoretical argument made. SimilarTheta*, differs primarily line-of-sight checks performed set expandednodes rather single ancestor. size set loosely bounded and,challenging problems, include large proportion nodes closed list.One recent successful line research involves combination string-pullingoffline pre-processing step. works compelling significantlyimprove performance purely online search; terms solution qualityalso running time. Block A* (Yap, Burch, Holte, & Schaeffer, 2011) one example.sub-optimal algorithm pre-computes database Euclidean-optimal distancespossible tile configurations certain size (e.g. possible 3x3 blocks). databaseobviates need explicit visibility checks indeed type online string-pulling.pre-processing step needs performed exactly once; database remains validtiles map change indeed map changes entirely. Another recentwork improves Theta* combining algorithm pre-processing based graphabstraction technique (Uras & Koenig, 2015b). approach, referred Section 9SUB-TL, shown improve running time solution quality Block A*.main disadvantage (vs. Block A*) abstract graph needs re-computedrepaired time map changes.Euclidean Shortest Path Problem well known well researched topicareas Computational Geometry Computer Graphics. seen generalisation Any-angle Pathfinding Problem. asks shortest path planeimpose restrictions obstacle shape obstacle placement (cf. grid alignedpolygons made unit squares).Visibility graphs (Lozano-Perez & Wesley, 1979) family well-known populartechniques optimally solving Euclidean Shortest Path Problem. Searchinggraphs requires O(n2 log2 n) time approach much faster practice.113fiHarabor, Grastien, Oz & Aksakallitwo main disadvantages: (i) computing graph requires offline pre-processingstep O(n2 ) space store; (ii) graph static must recomputed repairedenvironment changes. sophisticated variants Tangent Graphs (Liu &Arimoto, 1992) Silhouette Points (Young, 2001) particularly efficient variantsvisibility graphs disadvantages apply.Another family exact approaches solving Euclidean Shortest Path Problembased Continuous Dijkstra paradigm (Mitchell et al., 1987). efficient algorithms (Hershberger & Suri, 1999) involves pre-computation requiring O(n log2 n) space O(n log2 n) time. result Shortest Path Map; planarsubdivision environment used find Euclidean shortest pathO(log2 n) time; queries originating fixed source. Like visibility graphs,approach also introduces additional memory overheads (storing subdivision)pre-processing step must re-executed time environment start locationchanges.11. Conclusionstudy any-angle pathfinding: problem commonly found areas roboticscomputer games. problem involves finding shortest path two points gridasks path artificially constrained fixed points grid.best known online algorithms any-angle problem, date, compute approximatesolutions rather optimal shortest paths. Additionally online methodsable achieve consistent speedup vs. A* algorithm common reference pointmeasuring performance literature. work present new online, optimal practically efficient any-angle technique: Anya. works obtain goodperformance reasoning grid level method considers sets pointsgrid taken together contiguous intervals. approach requires revisitingclassical definition search nodes successors requires introduction newtechnique computing f -value node. give thorough algorithmic description new search paradigm give theoretical arguments completenessoptimality preserving characteristics.(almost) apples-to-apples comparison evaluate Anya three contemporary near-optimal online techniques: Theta*, Lazy Theta* Field A*. showthat, range popular benchmarks, Anya faster alternatives,guaranteeing find optimal shortest path. apples-to-oranges comparisonevaluate Anya SUB-TL: fast pre-processing-based near-optimal any-angletechnique. show Anya non-dominated compared SUB-TL evenmaintains advantage particularly challenging instances drawn real computer games. Another advantage that, unlike SUB-TL, Anya assume mapstatic; i.e. readily applied pathfinding problems involving dynamically changingterrain.Any-angle pathfinding received significant attention AI Game Development communities open question whether optimalonline algorithm exists. Anya answers question affirmative.114fiOptimal Any-Angle Pathfinding Practice11.1 Future Workseveral possible directions future work. Perhaps obvious development improvements extensions current Anya algorithm. example,believe empirical performance Anya could enhanced generating successorsnodes contain redundant (or partially redundant) intervals. One possibilitykeep closed list previously encountered intervals. stronger variant ideainvolves bounding g-value grid intervals generating successor nodesleast one point inside candidate interval relaxed. related orthogonal improvement involves pre-processing grid identifying intervals apriori. enhancementspeed search avoiding entirely grid scanning interval projection operationscurrently necessary order generate node.seen reasoning sets points grid, rather individuallocations, computationally beneficial. believe type search paradigmemployed Anya generalised improve performance grid-optimal searchaddition any-angle pathfinding.final suggestion work, believe Anya might also generalisedtwo-dimensional maps arbitrarily shaped polygonal obstacles, rather grids.benefit generalisation would avoid discretisation worldpath searched for. would even improve quality path returnedoptimal any-angle path often non optimal non-discretised version map.Acknowledgementsthank Tansel Uras assistance source codes used experimental sectionpaper. also thank Adi Botea Patrik Haslum helpful suggestionsearly development work.work Daniel Harabor Alban Grastien supported NICTA. NICTAfunded Australian Government represented Department Broadband,Communications Digital Economy Australian Research CouncilICT Centre Excellence program.work Dindar Oz Vural Aksakalli supported Scientific Technological Research Council Turkey (TUBITAK), Grant No. 113M489.115fiHarabor, Grastien, Oz & AksakalliAppendix A.provide additional details implementation Anyas successor set generationalgorithm. method depends basic operations technically simple: gridscanning, traversability tests linear projection operations. attempt reproduce mechanical details operations. Instead focus presentation towardintuitive understanding overall process.Algorithm 4 Computing successor set, supplemental.1: function generate-start-successors(a traversable discrete start location s)12:Construct maximal half-closed interval Imaxcontaining points observable left23:Construct maximal half-closed interval Imaxcontaining points observable right34:Construct maximal closed interval Imaxcontaining points observable row45:Construct maximal closed interval Imaxcontaining points observable rowk6:intervals Split Imaxcorner point take union7:Construct intervals new (cone flat) successor node r =8:return start successors9: end function10: function generate-flat-successors(an interval endpoint p, root point r)11:p0 first corner point (else farthest obstacle vertex) row p hr, p, p0 taut12:Imax new maximal interval endpoints p (open) p0 (closed)13:points r p row. Observable successors14:successors new flat node n = (Imax , r)15:else16:successors new flat node n = (Imax , p). Non-observable flat successors17:end18:return successors19: end function20: function generate-cone-successors(an interval endpoint a, interval endpoint b, root point r)21:b r row. Non-observable successors flat node22:r0 b, whichever farthest r. Previously established turning point23:p point adjacent row, reached via right-angle turn. Obstacle following24:Imax maximum closed interval, beginning p entirely observable r025:else == b. Non-observable successors cone node26:r027:p point adjacent row, computed via linear projection r28:Imax maximum closed interval, beginning p entirely observable r029:else. Observable successors cone node30:r0 r31:p point adjacent row, computed via linear projection r32:p0 point adjacent row, computed via linear projection r b33:Imax maximum closed interval, endpoints b, entirely observable r034:end35:{ split Imax corner point }36:n0 new search node interval root point r037:successors successors38:end39:return successors40: end function116fiOptimal Any-Angle Pathfinding PracticeReferencesBjornsson, Y., & Halldorsson, K. (2006). Improved Heuristics Optimal Path-findingGame Maps. Proceedings Second Artificial Intelligence InteractiveDigital Entertainment Conference, June 20-23, 2006, Marina del Rey, California, pp.914.Botea, A., Muller, M., & Schaeffer, J. (2004). Near Optimal Hierarchical Path-Finding.Journal Game Development, 1 (1), 728.Ferguson, D., & Stentz, A. (2005). Field D*: Interpolation-based Path PlannerReplanner. Robotics Research: Results 12th International Symposium, ISRR2005, October 12-15, 2005, San Francisco, CA, USA, pp. 239253.Graham, R. L., Knuth, D. E., & Patashnik, O. (1989). Concrete Mathematics - Foundation Computer Science. Addison-Wesley.Harabor, D. D., & Grastien, A. (2013). Optimal Any-Angle Pathfinding Algorithm.Proceedings Twenty-Third International Conference Automated PlanningScheduling, ICAPS 2013, Rome, Italy, June 10-14, 2013.Harabor, D. D., & Grastien, A. (2014). Improving Jump Point Search. ProceedingsTwenty-Fourth International Conference Automated Planning Scheduling,ICAPS 2014, Portsmouth, New Hampshire, USA, June 21-26, 2014.Hart, P. E., Nilsson, N. J., & Raphael, B. (1968). Formal Basis Heuristic Determination Minimum Cost Paths. IEEE Transactions Systems ScienceCybernetics, 4 (2), 100107.Hershberger, J., & Suri, S. (1999). Optimal Algorithm Euclidean Shortest PathsPlane. SIAM Journal Computing, 28 (6), 22152256.Liu, Y.-H., & Arimoto, S. (1992). Path Planning Using Tangent Graph MobileRobots Among Polygonal Curved Obstacles. International Journal RoboticsResearch, 11, 376382.Lozano-Perez, T., & Wesley, M. A. (1979). Algorithm Planning Collision-Free PathsAmong Polyhedral Obstacles. Communications ACM, 22 (10), 560570.Mitchell, J. S. B., Mount, D. M., & Papadimitriou, C. H. (1987). Discrete GeodesicProblem. SIAM Journal Computing, 16 (4), 647668.Nash, A., Daniel, K., Koenig, S., & Felner, A. (2007). Theta*: Any-Angle Path Planning Grids. Proceedings Twenty-Second AAAI Conference ArtificialIntelligence, July 22-26, 2007, Vancouver, British Columbia, Canada, pp. 11771183.Nash, A., & Koenig, S. (2013). Any-Angle Path Planning. AI Magazine, 34 (4), 9.Nash, A., Koenig, S., & Tovey, C. A. (2010). Lazy Theta*: Any-angle Path PlanningPath Length Analysis 3D. Proceedings Twenty-Fourth AAAI ConferenceArtificial Intelligence, AAAI 2010, Atlanta, Georgia, USA, July 11-15, 2010.Pinter, M. (2001). Toward Realistic Pathfinding. Game Developer Magazine, 8 (4).117fiHarabor, Grastien, Oz & AksakalliSislak, D., Volf, P., & Pechoucek, M. (2009). Accelerated A* Trajectory Planning: Gridbased Path Planning Comparison. 4th ICAPS Workshop Planning PlanExecution Real-World Systems.Sturtevant, N. (2012). Benchmarks Grid-Based Pathfinding. Transactions Computational Intelligence AI Games, 4 (2), 144 148.Sturtevant, N. R., & Bulitko, V. (2011). Learning Going WhenceCame: h- g-Cost Learning Real-Time Heuristic Search. 22nd International Joint Conference Artificial Intelligence, IJCAI 2011, pp. 365370.Sun, X., Yeoh, W., Chen, P.-A., & Koenig, S. (2009). Simple Optimization TechniquesA*-based Search. 8th International Joint Conference Autonomous AgentsMultiagent Systems, AAMAS 2009, Budapest, Hungary, May 10-15, 2009, Volume 2,pp. 931936.Uras, T., & Koenig, S. (2015a). Empirical Comparison Any-Angle Path-PlanningAlgorithms. Proceedings Eighth Annual Symposium Combinatorial Search,SOCS 2015, 11-13 June 2015, Ein Gedi, Dead Sea, Israel, pp. 206211.Uras, T., & Koenig, S. (2015b). Speeding-Up Any-Angle Path-Planning Grids.Proceedings Twenty-Fifth International Conference Automated PlanningScheduling, ICAPS 2015, Jerusalem, Israel, June 7-11, 2015, pp. 234238.Yap, P., Burch, N., Holte, R. C., & Schaeffer, J. (2011). Block A*: Database-Driven SearchApplications Any-Angle Path-Planning. Proceedings Twenty-FifthAAAI Conference Artificial Intelligence, AAAI 2011, San Francisco, California,USA, August 7-11, 2011.Young, T. (2001). Optimizing Points-of-Visibility Pathfinding. Game ProgrammingGems 2, pp. 324329. Charles River Media.118fiJournal Artificial Intelligence Research 56 (2016) 197-245Submitted 07/15; published 06/16Two Aspects Relevance Structured Argumentation:Minimality ParaconsistencyDiana Grootersdianagrooters@gmail.comORTEC Finance Rotterdam, NetherlandsHenry PrakkenH.Prakken@uu.nlDepartment Information Computing Sciences, Utrecht UniversityFaculty Law, University GroningenNetherlandsAbstractpaper studies two issues concerning relevance structured argumentationcontext ASPIC + framework, arising combined use strict defeasibleinference rules. One issue arises strict inference rules correspond classical logic.longstanding problem trivialising effect classical Ex Falso principleavoided satisfying consistency closure postulates. paper, problemsolved disallowing chaining strict rules, resulting variant ASPIC + framework called ASPIC ? , disallowing application strict rules inconsistent setsformulas. Thus effect Rescher & Manors paraconsistent notion weak consequenceembedded ASPIC ? .Another issue minimality arguments. arguments apply defeasible inferencerules, cannot required subset-minimal premises, since defeasible rulesbased information may well make argument stronger. paper insteadminimality required applications strict rules throughout argument. shownplausible assumptions affect set conclusions. addition, circular arguments new ASPIC ? framework excluded way satisfiesclosure consistency postulates generates finitary argumentation frameworksknowledge base set defeasible rules finite. latter result exclusionchaining strict rules essential.Finally, combined results paper shown proper extensionclassical-logic argumentation preferences defeasible rules.1. IntroductionOne tradition logical study argumentation allow arguments combinestrict defeasible inference rules. approach introduced AI Pollock (1987,1990, 1992, 1994, 1995), studied past also e.g. Lin Shoham (1989), SimariLoui (1992), Vreeswijk (1997), Prakken Sartor (1997) Garcia Simari (2004)currently studied e.g. Dung Thang (2014), Dung (2014, 2016) workASPIC + framework (Prakken, 2010; Modgil & Prakken, 2013, 2014). Strict inferencerules intended capture deductively valid inferences, truth premisesguarantee truth conclusion. Defeasible inference rules meant capture presumptive inferences, premises create presumption favour conclusion,refuted evidence contrary. Much research tradition shownidea defeasible inference rules makes sense. example, Pollock appliedc2016AI Access Foundation. rights reserved.fiGrooters & Prakkenformalize theory defeasible epistemic reasons, includes reasons concerning perception, memory, enumerative induction, statistical syllogism temporal persistence.Moreover, several publications ASPIC + use defeasible inference rules formalise Walton(1996)-style presumptive argumentation schemes (Prakken, 2010; Modgil & Prakken, 2014)apply legal reasoning (Prakken, Wyner, Bench-Capon, & Atkinson, 2015)policy debate (Bench-Capon, Prakken, & Visser, 2011). Finally, Garcia Simaris (2004)Defeasible Logic Programming approach applied many domains.tradition, two issues arise concerning relevance, namely, minimality arguments paraconsistency strict-rule application. study issuescontext ASPIC + framework. choice ASPIC + purposes justified framework nature, allows study various classes instantiations.Moreover, shown various approaches reconstructed instantiations ASPIC + framework. Prakken (2010) showed assumption-basedargumentation reconstructed Dung, Mancarella, Toni (2007) instanceabstract argumentation (Dung, 1995), result carries original formulation assumption-based argumentation (Bondarenko, Dung, Kowalski, & Toni, 1997)known semantics except semi-stable eager semantics (cf. Caminada, Sa, Alcantara,& Dvorak, 2015). Furthermore, Modgil Prakken (2013) reconstructed two formsclassical argumentation premise attack studied Gorogiannis Hunter (2011)several uses Tarskian abstract logics studied Amgoud Besnard (2013)instances ASPIC + . reasons, results terms ASPIC + representativelarge classes argumentation systems.ASPIC + sometimes criticised fact allows instantiations badproperties criticism besides point, since ignores framework natureASPIC + (Prakken & Modgil, 2012). framework instead concrete system,ASPIC + intended allow study properties various instantiations,whether satisfy rationality postulates Caminada Amgoud (2007).idea framework allow bad instantiations identified.Therefore, framework cannot criticised existence bad instantiations.Moreover, growing body results good instantiations ASPIC + (Caminada& Amgoud, 2007; Prakken, 2010; Modgil & Prakken, 2013; Dung, 2014, 2016; Caminada,Modgil, & Oren, 2014; Grooters & Prakken, 2014; Wu & Podlaszewski, 2015) paperaims identifying another class good instantiations.One relevance issue discussed paper minimality arguments. deductiveapproaches argumentation (e.g., Besnard & Hunter, 2008; Gorogiannis & Hunter, 2011;Amgoud & Besnard, 2013) arguments required subset-minimal set premises.However, arguments apply defeasible inference rules, requirement undesirable, since defeasible rules based information may well make argumentstronger. example, Observations done ideal circumstances usually correctstronger Observations usually correct. Note remark applystrict inference rules, still makes sense improve efficiency requiring strictinference rules applied subset-minimal set formulas. far, systemdefeasible-rule tradition enforces requirement. One contribution paperASPIC + approach show plausible conditionsargument ordering, affect set conclusions.198fiTwo Aspects Relevance Structured ArgumentationAnother aspect minimality circularity. far presentations ASPIC +prevented arguments repeating conclusions subarguments. Yet argumentation theory circular arguments generally regarded fallacious, makes senseexclude them. paper prove results rationality postulates affected it. Moreover, prove excluding non-circular argumentscomputational benefits.Another relevance issue arises strict inference rules chosen correspondclassical logic. longstanding unsolved problem originally identified Pollock (1994,1995) trivialising effect classical Ex Falso principle avoidedtwo arguments use defeasible rules contradictory conclusions. problemespecially hard since solution arguably preserve satisfaction rationalitypostulates consistency strict closure (Caminada & Amgoud, 2007).nutshell, problem follows. Suppose two arguments contradictoryconclusions . strict inference rules include Ex Falso principleinconsistent set implies formula, two arguments combinedargument formula . combined argument potentially defeatargument applying Ex Falso inference rule joint conclusions.arguments contradictory conclusions, argument potentiallythreat, clearly undesirable, since conflict general unrelated .Pollock (1994, 1995) thought avoided trivialising arguments allowing multiple labellings, Caminada (2005) showed Pollocks solutionfully avoid them. problem genuine one, since arguably real needargumentation systems allow combinations strict defeasible inferencesthat, moreover, allow full reasoning power deductive logic. Although manycases less expressiveness may suffice, full theory logic argumentation cannotexclude general case.solve problem, two approaches possible. One change definitionsargumentation framework, derive strict inference rulesweaker logic classical logic. first approach taken Wu (2012) WuPodlaszewski (2015), ASPIC + framework require argumentset conclusions subarguments classically consistent. showsolution works restricted version ASPIC + without preferences, givecounterexamples consistency postulates case preferences.second approach solve problem replace classical logic sourcestrict rules weaker, monotonic paraconsistent logic, order invalidate ExFalso principle valid strict inference rule. paper explores possibility. firstshow two well-known paraconsistent logics, system C Da Costa (1974)Logic Paradox Priest (1979, 1989), cannot used purposes, sinceinduce violation postulate indirect consistency. show using RescherManors (1970) paraconsistent consequence notion satisfies closure consistencypostulates also avoids trivialisation. thus initially taking second approach,combine first approach (changing definitions) sinceturn chaining strict rules arguments disallowed. change turnmotivates new interpretation Caminada Amgouds (2007) strict-closure postulate199fiGrooters & Prakkenintroduction new rationality postulate logical closure. contributionpaper based extends results Grooters Prakken (2014).making contributions, argue combination shed lightrelation adapted version ASPIC + classical argumentation studiedBesnard Hunter (2008) Gorogiannis Hunter (2011), argumentsessentially classical proofs consistent subset-minimal subsets classicalknowledge base. two versions classical argumentation premise attackadapted version ASPIC + shown proper extension defeasible rulespreferences. observation justifies combined treatment issues (minimalityarguments paraconsistency) paper.Caminada, Carnielli, Dunne (2012) formulated new set rationality postulatesaddition Caminada Amgoud (2007), characterise casestrivialisation problem avoided (called postulates non-interference crashresistance). Wu (2012) Wu Podlaszewski (2015) prove adaptationASPIC + consistent arguments new postulates satisfied completesemantics. However, attempt prove Caminada et al.s (2012) postulates,two reasons. First, want obtain results semantics well and, second,argue Section 10 Caminada et al.s postulates fact capture stronger intuitivenotion one study paper.remainder article organised follows. First Section 2 ASPIC +framework summarised Section 3 rationality postulates Caminada Amgoud (2007) presented. Section 4 trivialisation problem illustrateddetail, Section 5 instantiations ASPIC + paraconsistent logics LP C studied attempt avoid trivialisation face inconsistency.shown instantiations violate rationality postulate indirect consistency. Section 6 Rescher Manors (1970) paraconsistent consequence notionintroduced another attempt avoid trivialisation. turns embeddingASPIC + requires adaptation ASPIC + framework framework called ASPIC ? , disallows chaining strict rules, turn motivates new notions strictclosure indirect consistency. Section 7 first main contribution paper proved: satisfaction closure consistency postulates instantiationASPIC ? framework Rescher Manors consequence notion. Section 8second third main contribution presented: equivalence result versionsASPIC ? without minimality constraints strict inferences, proofs showversion ASPIC ? excludes circular arguments well-behaved. Section 9present fourth main result, namely, ASPIC ? minimal arguments properlygeneralises two versions classical argumentation. Finally, Section 10 discussresults put context related work.2. ASPIC + Frameworksection, ASPIC + framework reviewed. Since makes use Dungs (1995)theory abstract argumentation, theory first briefly summarised. abstractargumentation framework (AF ) pair (A, D), set argumentsbinary relation defeat. argument defeats argument B (A, B) D.200fiTwo Aspects Relevance Structured Argumentationset arguments defeats argument B argument defeatsB. set defeats set 0 argument 0 defeats A. setarguments said conflict-free attack itself; otherwise conflicting.set defends argument iff BinA defeats existsC defeats B. set admissible conflict-free defends attackingargument attacking S. argumentation framework zero extensions,intuitively maximal sets arguments accepted together sinceconflict-free defend members attacks. Formally, extensionsadmissible sets additional properties. defined according Dungscharacteristic function.Definition 2.1. [Dungs characteristic function F ] FAF : 2A 2A FAF (S) ={A A|A defended S}.Henceforth subscript AF omitted danger confusion.Definition 2.2. [Extensions abstract argumentation frameworks] AF =(A, D) E A:E conflict-free iff A, B E (A, B) D.E admissible iff E conflict-free E defends E.E complete extension AF iff E conflict-free FAF (E) = E.E preferred extension AF iff E set-inclusion-maximal complete extensionAF .E stable extension AF iff E conflict-free 6 E existsB E B defeats A.E grounded extension AF iff E set-inclusion-minimal complete extension AF .Finally, {complete, preferred, grounded, stable}, X sceptically credulously-justified X belongs all, respectively least one, extension. notionsextensions proposed literature paper confinefour notions.ASPIC + framework (Prakken, 2010; Modgil & Prakken, 2013) gives structureDungs arguments defeat relation. work Vreeswijk (1997) defines arguments directed acyclic inference graphs formed applying strict defeasible inferencerules premises formulated logical language. Intuitively, strict rules guaranteetruth consequent antecedents true, defeasible rules createpresumption favour truth consequent antecedents true. Arguments attacked (ordinary) premises applications defeasibleinference rules. attacks succeed defeats, partly determined preferences.acceptability status arguments defined applying Dungs (1995)semantics abstract argumentation frameworks resulting set argumentsdefeat relation.201fiGrooters & Prakkenspecial case symmetric negation version ASPIC + definedModgil Prakken (2013) presented, minor improvements. Nontrivialimprovements indicated made.ASPIC + system framework specifying systems. said above,framework intended allow study properties instantiations, whethersatisfy rationality postulates Caminada Amgoud (2007). enddefines notion abstract argumentation system structure consisting logicallanguage L unary negation symbol , set R consisting two disjoint subsets RsRd strict defeasible inference rules, naming convention n L defeasiblerules order talk L applicability defeasible rules. elementsleft undefined general specified specific instantiation.Definition 2.3. [Argumentation systems] argumentation system triple =(L, R, n) where:L nonempty logical language unary negation symbol .R = Rs Rd set strict (Rs ) defeasible (Rd ) inference rules form1 , . . . , n 1 , . . . , n respectively (where , meta-variablesranging wff L), Rs Rd = .n : Rd L naming convention defeasible rules.Informally, n(r) wff L, says rule r R applicable. write =case = = . Note part logical language Lmetalinguistic function symbol obtain concise definitions. Furthermore,danger confusion, sometimes write sequence antecedents strictdefeasible rule set.Example 2.1. example argumentation systemL = {p, p, q, q, r, r, s, s, t, t, r1 , r2 , r1 , r2 },Rs = {p, r s; r1 }, Rd = {q r; s},n(q r) = r1 n(t s) = r2 .ASPIC + framework abstracts origins strict defeasible rules.Several ways identify rules possible. One way, quite usual AI, let rulesexpress domain-specific knowledge. example, strict rules could contain terminological knowledge bachelors married, defeasible rules could containdefeasible generalisations Birds fly defeasible norms Thou shalt lie.Another way base rules general accounts deductive defeasible reasoning. example, strict rules might chosen correspond monotonic logicdefeasible rules might instantiated argument schemes (Walton, 1996).two ways identify inference rules pragmatically different; formally, ASPIC +framework treats rules inference rule regardless origin. paper abstractorigin defeasible rules focus choice strict rules.particular concerned instantiations ASPIC + strict rules chosencorrespond monotonic logic (although several results apply generally).202fiTwo Aspects Relevance Structured Argumentationinstantiations Rs defined follows, given monotonic consequence notion `Llogic L:Rs = {S | `L finite}Rs defined way logical language L, say Rs correspondslogic L.Definition 2.4. [Knowledge bases] knowledge base = (L, R, n) set K Lconsisting two disjoint subsets Kn Kp (the necessary ordinary premises).Intuitively, necessary premises certain knowledge thus cannot attacked,whereas ordinary premises uncertain thus attacked.Definition 2.5. [Consistency strict closure] X L, let closure Xstrict rules, denoted ClRs (X), smallest set containing X consequentstrict rule Rs whose antecedents ClRs (X). set X Ldirectly consistent iff @ , X = ;indirectly consistent iff ClRs (X) directly consistent.Example 2.2. example argumentation system, example directly inconsistentset {p, p} example directly consistent indirectly inconsistent set{p, r, s}. Finally, example closure strict rules ClRs ({p, r}) = {p, r, s, r1 }.Arguments constructed step-by-step knowledge bases chaining inferencerules directed acyclic graphs (or trees formula used once).follows, given argument function Prem returns premises, Conc returnsconclusion Sub returns sub-arguments, TopRule returns last rule usedargument.Definition 2.6. [Argument] argument basis knowledge base Kargumentation system (L, R, n) is:1. K with:Prem(A) = {};Conc(A) = ;Sub(A) = {};TopRule(A) = undefined.2. A1 , . . . / A1 , . . . , arguments Rs /Rd contains strict/defeasiblerule Conc(A1 ), . . . , Conc(An ) / , with:Prem(A) = Prem(A1 ) . . . Prem(An ),Conc(A) = ,Sub(A) = Sub(A1 ) . . . Sub(An ) {A};TopRule(A) = Conc(A1 ), . . . , Conc(An ) / .203fiGrooters & Prakkenfunctions Func also defined sets arguments = {A1 , . . . , }follows: Func(S) = Func(A1 ) . . . Func(An ). argument uses strict rules,argument said strict, otherwise defeasible. argument necessarypremises, argument firm, otherwise plausible. argument definePremn (A) = Prem(A) Kn Premp (A) = Prem(A) Kp . set argumentsconsist necessary premise denoted N P (S).Example 2.3. example argumentation system combined knowledge baseKn = {p} Kp = {q, t}, following arguments constructed:A1A2A3A4====pqA2 rA5 =A6 =A7 =A3A1 , A4A5 r1Argument A1 strict firm, A2 A3 strict plausible, remainingarguments defeasible plausible.Figure 1 arguments visualised. type premise indicatedsuperscript defeasible inferences displayed dotted lines. dotted boxesthick arrows explained Example 2.4.Figure 1: Arguments attacks Example 2.1. premises bottomconclusion top tree. Thin vertical links boxesinferences thick diagonal links attacks. type premiseindicated superscript defeasible inferences, underminable premisesrebuttable conclusions displayed dotted lines.Arguments attacked three ways: ordinary premises (underminingattack), defeasible inference (undercutting attack) conclusion defeasibleinference.Definition 2.7. [Attack] argument attacks argument B iff undercuts, rebutsundermines B, where:204fiTwo Aspects Relevance Structured Argumentationundercuts argument B (on B 0 ) iff Conc(A) = n(r) B 0 Sub(B)B 0 top rule r defeasible.rebuts argument B (on B 0 ) iff Conc(A) = B 0 Sub(B) formB100 , . . . , Bn00 .undermines argument B (on ) iff Conc(A) = Premp (B).Example 2.4. running example A6 rebuts A5 therefore also A6 rebuts A7A5 (since A5 subargument A7 ). Note A5 rebut A6 since A6 stricttop rule. Furthermore, A7 undercuts A4 A6 A4 . Figure 1 rebuttable conclusionsvisualised dotted boxes direct defeat relations displayed thick arrows.Note indirect attacks A6 A7 A7 A6 explicitly visualised.argument basic fallible argument 1 iff Kp TopRule(A) Rd . basicfallible argument thus argument defeasible top rule equates ordinarypremise attacked final conclusion inference. set basicfallible arguments set arguments denoted F A(S).Argumentation systems plus knowledge bases form argumentation theories.turn combined preference ordering set arguments constructibletheory, induce structured argumentation frameworks. Like elements argumentationsystems, nature ASPIC + argument ordering undefined generalspecified specific instantiation.Definition 2.8. [Structured Argumentation Frameworks] Let argumentation theory (AS, K). structured argumentation framework ( SAF) defined ,triple hA, C, set finite arguments constructed K AS,binary relation A, (X, ) C iff X attacks .Unlike Modgil Prakken (2013) consider versions ASPIC + requirearguments consistent premises (except briefly Section 9 purposes comparison). approach strict arguments inconsistent premises handledchoice let Rs correspond paraconsistent logic, prevent trivialisation, i.e.,prevent systems generate arguments random conclusion contradictions. Furthermore, leave users whether want ensuredefeasible rules inconsistent antecedents. left user since allowingdefeasible rules inconsistent antecedents cause trivialisation. Furthermore,results proved paper still hold whether defeasible rules inconsistent antecedentsexcluded not.notion defeat defined follows. Undercutting attacks succeeddefeats independently preferences arguments, since meant expressexceptions defeasible inference rules. Rebutting undermining attacks succeedattacked argument stronger attacking argument (A B definedusual B B 6 A).Definition 2.9. [Defeat] defeats B iff:1. renaming Dung Thangs (2014) notion basic defeasible argument.205fiGrooters & Prakken1. undercuts B;2. rebuts/undermines B B 0 B 0 .Example 2.5. running example A6 defeats A5 A7 unless A6 A5 . Furthermore,preference relation A4 A7 , even A7 A4 , A7 defeatsA4 (and thus A7 also defeats A6 ).SAFs generate abstract argumentation frameworks sense Dung (1995),used evaluate arguments conclusions:Definition 2.10. [Argumentation frameworks]abstract argumentation framework (AF ) corresponding SAF = hA, C,pair (A, D) defeat relation determined SAF .Let {complete, preferred, grounded, stable} let L definingSAF . wff L sceptically -justified SAF conclusion sceptically-justified argument, credulously -justified SAF sceptically -justifiedconclusion credulously -justified argument.Example 2.6. Suppose running example A6 defeats A5 A7A7 defeats A4 A6 . resulting AF visualised twice Figure 2. groundedextension {A1 , A2 , A3 } two preferred extensions E1 = {A1 , A2 , A3 , A4 , A6 }E2 = {A1 , A2 , A3 , A5 , A7 }. preferred extensions also stable. two preferredextensions visualised Figure 2: members extension coloured white.Figure 2: Two preferred extensions Dung AF Example 2.1.finally need notion strict continuation set arguments, defineslightly different way Modgil Prakken (2013). new definition arguablysimpler affect proofs Modgil Prakken. identifies argumentsformed extending set arguments strict inferences new argument,new argument attacked arguments extends.Definition 2.11. [Strict continuations] set strict continuations set arguments smallest set satisfying following conditions:206fiTwo Aspects Relevance Structured Argumentation1. argument strict continuation {A}.2. A1 , . . . , S1 , . . . , Sn {1, . . . , n}, Ai strictcontinuation Si B1 , . . . , Bn strict-and-firm arguments,Conc(A1 ), . . . , Conc(An ), Conc(B1 ), . . . , Conc(Bm ) strict rule Rs ,A1 , . . . , , B1 , . . . , Bn strict continuation S1 . . . Sn .argument strict continuation arguments A1 , . . . , , strict argument{Conc(A1 ), . . . , Conc(An )}.Example 2.7. running example arguments strict continuationsA6 strict continuation {A1 , A4 } A7 strict continuation A5 . Supposetemporarily add strict rule p, r2 Rs . A8 = A1 , A6 r2 strictcontinuation {A6 }.3. Rationality PostulatesExtensions abstract argumentation frameworks intuitively maximal sets argumentsrationally accepted together given frameworks. Dungs (1995) varioussemantics, yielding different types extensions, seen various alternative waysformalize rationality constraints acceptable sets arguments. argumentsstructure, additional rationality constraints defined extensionsabstract argumentation semantics. Caminada Amgoud (2007) proposed followingrationality postulates structured argumentation.Subargument closure: every extension E, argument Esubarguments E.Closure strict rules: every extension E, set Conc(E) closedapplication strict rules.Direct consistency: every extension E, set Conc(E) directly consistent.Indirect consistency: every extension E, set Conc(E) indirectlyconsistent.Note closure strict rules direct consistency together imply indirect consistency.Modgil Prakken (2013) identify set conditions ASPIC + satisfiesfour postulates. first condition set strict rules either closedtransposition closed contraposition.Definition 3.1. [Closure transposition, (Modgil & Prakken, 2013)] setstrict rules Rs said closed transposition rule 1 , . . . , n Rsrules form 1 , . . . , i1 , , i+1 , . . . , n 2 alsobelong Rs . argumentation theory (AS, K) closed transposition strictrules Rs closed transposition.2. Note wff one = . example, p = p p = p.207fiGrooters & PrakkenDefinition 3.2. [Closure contraposition, (Modgil & Prakken, 2013)]argumentation system said closed contraposition X L, Xholds ClRs (X) ClRs (X\{} {}). argumentation theory (AS, K) closed contraposition argumentationsystem closed contraposition.second condition states argument ordering following properties:Definition 3.3. [Reasonable argument ordering, (Modgil & Prakken, 2013)]reasonable argument ordering if:A, B, strict firm B plausible defeasible, B A;A, B, B strict firm, B A;A, A0 , B, C, C A, B A0 strict continuation {A},C A0 , A0 B;Let {C1 , . . . , Cn } finite subset = 1, . . . , n let C +/i strictcontinuation {C1 , . . . , Ci1 , Ci+1 , . . . , Cn }. case i, C +/iCi .Modgil Prakken (2013) identify several types argument orderings reasonable.third condition axiom consistency.Definition 3.4. [Axiom consistent, (Modgil & Prakken, 2013)] argumentationtheory axiom consistent ClRs (Kn ) consistent.Modgil Prakken (2013) prove argumentation theory satisfies threeconditions induces extensions satisfy four rationality postulates.4. Trivialisation Problemsection illustrate trivialisation problem detail. following abstractexample illustrates problems arise strict rules argumentation systemcorrespond classical logic, i.e. X Rs X ` X finite (where` denotes classical consequence).Example 4.1. Let Rd = {p q; r q; s}, Kp = Kn = {p, r, t}, Rscorresponds classical logic. corresponding AF includes following arguments:A1 : pB1 : rD1 :A2 : A1 qB2 : B1 q2 : D1C: A2 , B2Figure 3 displays arguments attack relations. Dotted lines indicate defeasible inferences dotted boxes indicate rebuttable conclusions. Argument C attacks D2 .208fiTwo Aspects Relevance Structured ArgumentationFigure 3: Illustrating trivialisationWhether C defeats D2 depends argument ordering plausible argument orderingspossible C 6 D2 C defeats D2 . problematic, sinceformula, defeasible argument unrelated A2 B2 , D2 , can, dependingargument ordering, defeated C. Clearly, extremely harmful, sinceexistence single case mutual rebutting attack, common, couldtrivialise system. noted simply disallowing application strict rulesinconsistent sets formulas help, since argument stillconstructed follows:A3 :C 0:A2 qA3 , B2Note argument C 0 apply strict inference rule inconsistent setformulas.example suggests following formalisation property trivialisation.Definition 4.1 (Trivialising argumentation systems). argumentation systemtrivialising iff , L knowledge bases K {, } K strictargument basis K constructed conclusion .interested defining classes non-trivialising argumentation systems.argumentation system example clearly trivialising since Rs contains strictrules , , L.Example 4.1 cause problems preferred stable semantics, since A2B2 attack least one attacks (with non-circular argumentorderings) succeed defeat. Therefore, preferred stable extensions contain eitherA2 B2 both. Since A2 B2 attack C (by directly attacking onesubarguments), C preferred stable extension defeated least one argumentextension, C extensions, D2 extensions.intuitively correct since connection D2 arguments A2B2 .fact, semantics defined Dung (1995) problems Example 4.1grounded semantics. Since A2 B2 defeat other, neithergrounded extension. extension defend D2 C thereforecontain D2 .209fiGrooters & PrakkenPollock (1994, 1995) thought just-given line reasoning preferred semanticssuffices show recursive-labelling approach (which later Jakobovits &Vermeir, 1999 proved equivalent preferred semantics) adequately dealsproblem. However, Caminada (2005) showed example extended waysalso cause problems preferred stable semantics. Essentially, replacedfacts p r defeasible arguments p r let arguments defeatedself-defeating argument. one hand, self-defeating arguments cannotextension, since extensions conflict free. However, self-defeating argumentdefeated arguments, prevents argument defeatsacceptable respect extension. example, A2 B2 defeatedself-defeating argument otherwise undefeated, neither A2 B2extension, argument extension defends D2 C.critic ASP IC + Pollocks approach might argue problem causedcombination strict (i.e., deductive) defeasible inference rules. Indeed, classical argumentation (Besnard & Hunter, 2008; Gorogiannis & Hunter, 2011) problemeasily avoided requiring premises argument consistent. However, reasons believe classical logic strong able modelforms defeasible reasoning; see, instance, discussions Brewka (1991), Ginsberg(1994) Prakken (2012). Furthermore, assumption-based argumentation (ABA)reconstructed Dung et al. (2007), strict inference rulesrequire classical, require premises argumentsconsistent, problem may may arise depending instantiated.reconstructed ASPIC + Prakken (2010), ABA arguments built ordinarypremises Kp strict inference rules Rs . following example (in notationASPIC + reconstruction ABA) shows trivialisation problem also ariseABA.Example 4.2. Take Kp = {p, p, s} let Rs correspond classical logic, i.e,Rs iff finite set wff classically implies . following argumentsconstructed.A:pB : pC : A, BD:strivialising argument C prevents argument extension.problem instantiate and/or redefine ASPIC + way avoidstrivialising effects strict inferences inconsistent set, still satisfyingrationality postulates Caminada Amgoud (2007).5. Instantiating ASPIC + Two Well-Known Paraconsistent Togicssaid introduction, one way avoid trivialisation derive strict rulesASPIC + paraconsistent logic. logical consequence relation `L saidparaconsistent explosive, i.e. hold B210fiTwo Aspects Relevance Structured Argumentation{A, A} `L B. section investigate strategy two well-known paraconsistentlogics, Logic Paradox Priest (1979, 1989) system C Da Costa (1974).Another well-known paraconsistent logic family relevant logics. However, logicnonmonotonic (Read, 1988, p. 100). problem since idea ASPIC +strict rules based logic, logic monotonic. reason, relevancelogics considered paper.5.1 Logic ParadoxLogic Paradox (Priest, 1979, 1989) obtained relaxing assumption classicalpropositional logic sentence cannot true false. Sentences LogicParadox (LP ) two truth values instead one. set possible truth values{{1}, {0}, {0, 1}}, {0, 1} paradoxical true false.semantics propositional version LP follows.1. (a) 1 v(A) 0 v(A)(b) 0 v(A) 1 v(A)2. (a) 1 v(A B) 1 v(A) 1 v(B)(b) 0 v(A B) 0 v(A) 0 v(B)3. (a) 1 v(A B) 1 v(A) 1 v(B)(b) 0 v(A B) 0 v(A) 0 v(B)4. (a) 1 v(A B) 0 v(A) 1 v(B)(b) 0 v(A B) 1 v(A) 0 v(B)interpretation model formula f 1 v(f ) holds interpretation. model set formulas model every formulaset. semantical notion logical consequence defined follows:LP evaluations v either 1 v(A) B , v(B) = {0}shown LP coincides propositional logic tautologiesvalid inferences. particular, although AA B tautology LP , correspondinginferences {A A} |=LP B also {A, A} |=LP B invalid. counterexample,consider evaluation B strictly false A, undetermined (both truefalse), undetermined well. 0 v(A A), valuationpostulates 1 v(A B). Therefore, {A A} 2LP B also {A, A} 2LP B.Therefore, Logic Paradox paraconsistent logic.turns postulate indirect consistency satisfied case strictrules ASPIC + instantiated valid inferences Logic Paradox, is,Rs iff finite |=LP . following counterexample broughtattention Graham Priest (personal communication).Example 5.1. Take SAF defined argumentation theory knowledge baseKp Kn , Kn = Kp = {a, b, c, b c}. suppose Rs211fiGrooters & Prakkencorresponds Logic Paradox defeasible rules (Rd = ).Finally, assume arguments least one ordinary premise equally preferredaccording argument ordering SAF .easily checked Kp implies least one a, b c must paradoxical.Therefore, exists argument A1 : a, ab, ac, bc (aa)(bb)(cc).Since tautologies preserved Logic Paradox, (bb), (aa) (cc)also entailed K. implies exists argument A2 : a, ab, ac, bc((a a) (b b) (c c)). arguments use strict rulesattacked premises. However, exist argument built Kconclusion Kp . show this, Kp model foundv(d) 6= {0} holds 1 v(d).Model 1: show case follows Kp .Take model v(a) = {1}, v(b) = {1} v(c) = {0, 1}. clear v(a) ={1}, v(a b) = {1}, v(a c) = {0, 1} v(b c) = {0, 1}, 1 v(a).Model 2: show case (a b) follows Kp .Take model v(a) = {1}, v(b) = {1} v(c) = {0, 1}. clear v(a) ={1}, v(a b) = {1}, v(a c) = {0, 1} v(b c) = {0, 1}, 1 v((a b)).Model 3: show case (a c) follows Kp .Take model v(a) = {1}, v(b) = {0, 1} v(c) = {1}. clear v(a) ={1}, v(a b) = {0, 1}, v(a c) = {1} v(b c) = {0, 1}, 1 v((a c)).Model 4: show case (b c) follows Kp .Take model v(a) = {0, 1}, v(b) = {0} v(c) = {0}. clear v(a) ={0, 1}, v(ab) = {0, 1}, v(ac) = {0, 1} v(bc) = {1}, 1 v((bc)).means arguments defeat one arguments A1 A2 ,A1 A2 elements complete extension E. means (aa)(bb)(cc)((a a) (b b) (c c)) elements Conc(E). Therefore, argumentationtheory satisfy postulate direct consistency.5.2 Da Costas Basic C-system: Csystem C Da Costa (1974) adds axioms positivelogic, negation free first-order logic (these added axioms called Dialectic DoubleNegation (DDN) Exclusive Middle (EM) respectively). C certain aspectsdual intuitionistic logic, since intuitionistic logic axiom EM invalidaxiom Non-Contradiction (NC, (A A)) valid. C , axiom EM valid NCinvalid. Intuitionistic logic tolerates incomplete situations avoid inconsistency,C-systems admit inconsistent situations, incomplete situations removed.example, C possible three sentences A, A, true. However unlikeLogic Paradox, sentences one truth value. Next semanticsproof theory given sound complete respect other.propositional version C following bivalent valuation formulas builtlogical language L.1. v(A B) = 1 v(A) = 1 v(B) = 12. v(A B) = 1 v(A) = 1 v(B) = 1212fiTwo Aspects Relevance Structured Argumentation3. v(A B) = 1 v(A) = 0 v(B) = 14. v(A) = 1 v(A) = 05. v(A) = 1 v(A) = 1interpretation formula f valuation form model v(f ) = 1interpretation. interpretation model set formulasmodel every formula set. semantical logical consequence:C evaluations v either v(A) = 1 B , v(B) = 0easy show {A, A} 0C B, since following valuation function chosen:v(A), v(A) = 1 v(B) = 0.Replacing LP C source strict rules ASPIC + still yields counterexamples direct consistency. (In example, denotes material implication).Example 5.2. Suppose knowledge base K = Kp Kn , Kn =Kp = {a, b, b} following valuation: v(a) = 1, v(a b) = 1v(a b) = 1. suppose Rs corresponds C defeasiblerules (Rd = ). Finally, assume basic fallible arguments equally strong.following two arguments exist: A1 : a, b b A2 : a, b b.shown Figure 4. two arguments use strict rule. meansarguments defeated premises. However, none a, (a b),(a b) ClRs (K), arguments defeat A1 A2premises. Therefore, A1 A2 elements complete extension E, meansb, b Conc(E).Figure 4: Arguments Example 5.26. Another Attempt: Instantiating ASPIC + Weak Consequencesection investigate use another paraconsistent consequence notion, socalled weak consequence relation originally proposed Rescher Manor (1970). basicidea sentence weakly follows set sentences classically followsleast one consistent subset S. notion clearly related classical argumentation,formally show Section 9. also inspired early consistency-based approachesargumentation, Krause, Ambler, Elvang-Gransson, Fox (1995).knowledge, first use system defeasible rules. first discussweak-consequence notion define used overcome trivialisationproblem ASPIC + .213fiGrooters & Prakken6.1 Weak ConsequenceWeak consequence standard propositional language formally defined follows.Definition 6.1 (Weak consequence relation, `W ). `Wmaximal consistent subset ` classical logic.Note word maximal fact required, since according LindenbaumsLemma every consistent set formulas extended maximally consistent one.easy see {a, a} `W b hold, {a, a} maximalconsistent subset {a, a}. Therefore, consequence relation paraconsistent.next discuss three common properties.[Reflexivity] , `W .property holds `W . belongs maximally consistent subset. classical logic, holds , ` . Therefore, obviously holds`W .[Monotonicity] `W , , `W .monotonicity property proven `W follows. must maximalconsistent subset ` . Since , must exist maximalconsistent extension , 0 , 0 ` . Therefore, , `W .[Cut] , `W `W , `W .rule hold. counterexample, consider set = {a, b}.`W b , b `W b, case `W b.Since Cut rule hold, naive instantiation ASPIC + strict ruleslogic W would avoid explosion, shown following example:Example 6.1. Consider following knowledge base Kp = {p, p, r}, Kn = , instantiatestrict rules valid inferences finite sets logic W let Rd = .following arguments constructed:A1 : pB : pD:rA2 : A1 p rC : A2 , B rArgument C concludes r.underlying reason problem Cut rule hold `W ,example Kp 0W r. want ASPIC + strict part behave according `W ,chaining strict rules excluded.3 Example 6.1, argument C allowed3. similar idea suggested Martin Caminada personal communication. discuss ideaSection 10.214fiTwo Aspects Relevance Structured Argumentationsince A2 already strict top rule. prohibition chaining strict rulesprevent trivialisation. end, ASPIC + notion argument must redefined,results ASPIC ? framework.6.2 ASPIC ? Frameworkchange ASPIC + framework ASPIC ? framework disallowingchaining strict rules arguments. first need change definition argument:Definition 6.2. [Argument? ASPIC ? ] argument? basis knowledgebase K = (K, ) argumentation system (L, R, n, 0 ) is:1. KPrem() = {},Conc() = ,Sub(A) = {},TopRule(A) = undefined.2. A1 , . . . , A1 , . . . , arguments? defeasible top ruleK exists strict rule Conc(A1 ), . . . , Conc(An ) Rs .Prem(A) = Prem(A1 ) . . . Prem(An ),Conc(A) = ,Sub(A) = Sub(A1 ) . . . Sub(An ) {A},TopRule(A) = Conc(A1 ), . . . , Conc(An ) .3. A1 , . . . , A1 , . . . , arguments? exists defeasible ruleConc(A1 ), . . . , Conc(An ) Rd .Prem(A) = Prem(A1 ) . . . Prem(An ),Conc(A) = ,Sub(A) = Sub(A1 ) . . . Sub(An ) {A},TopRule(A) = Conc(A1 ), . . . , Conc(An ) .Arguments? special case normal arguments. Therefore, definitions(sets ) arguments case term argument replaced argument?without problems. Attack defeat arguments? . Structuredabstract argumentation frameworks ASPIC ? framework exceptcontain arguments? . Accordingly, notions justified defensiblearguments? conclusions still defined Section 2.new ASPIC ? framework motivates new interpretation strict-closure postulate case Rs corresponds logic L. fact weak-consequence notion `Wsatisfy Cut rule shows closure Rs general coincideclosure consequence notion logic Rs corresponds. fact,Example 6.1 easily extended counterexample `W , since r strictclosure {p, p} {p, p} 6`W r. turn gives reason doubt whether fullclosure strict rules always desirable. Arguably desirability dependsproperties logic Rs corresponds: logic satisfy Cut rule,full strict closure desirable. Instead, desirable casesextensions closed consequence adopted logic Rs .215fiGrooters & Prakkenprove ASPIC ? strict rules correspond `W . end restrictstrict-closure postulate allowed applications strict rules will, moreover,introduce new rationality postulate logical closure.think analysis compatible Caminada Amgouds (2007) reasonproposing strict-closure postulate, since arguably implicit assumption behindpostulate formulas strict closure always reachable argumentsconstructed arguments extension:idea closure answer argumentation-engineclosed strict rules. is, provide engine strict ruleb (. . . ), together various rules, inference engine outputsjustified conclusion, also output b justified conclusion.Consequently, b also supported acceptable argument (emphasisadded current authors). (Caminada & Amgoud, 2007, p. 294).quote compatible new account strict closure new logical-closurepostulate, since Rs based `W , implicit assumption strictclosure extension equates supportable acceptable argument satisfied.formalise new interpretation strict-closure postulate, new notions strictclosure indirect consistency needed. first explain notation. RecallSection 2 set arguments, F A(S) denotes set basic fallible argumentsN P (S) denotes set necessary premises argument S.corresponding notions ASPIC ? denoted F A? (S) N P ? (S). Next,set arguments? let # defined F A? (S) N P ? (S). set # containsarguments? strict top rule, arguments?conclusions strict rule applied form new arguments? . strictclosure indirect consistency redefined follows.Definition 6.3. [Closure? ] X L, let closure ? X strict rules,? (X), smallest set containing X consequent strict ruledenoted ClRRs whose antecedents X. set arguments? said closed? strict? (Conc(S # )).rules Conc(S) = ClRnew strict closure notion amounts one-steps application strict rules.Definition 6.3 clarified following example.Example 6.2. Suppose following sets: Kn = {p}, Kp = {q, t} Rd = {qr, s} RS corresponds classical propositional consequence. followingset arguments? constructed.A1 = pA5 = A3A2 = qA6 = A1 , A4 p rA3 =A7 = A5 vA4 = A2 rexample # = {A1 , . . . , A5 } allowed apply strict rulesarguments? . closed? strict rules, since example p q/ Conc(S)? (Conc(S # )).argument? A8 = A1 , A2 p q constructed, p q ClR216fiTwo Aspects Relevance Structured ArgumentationDefinition 6.4. [Indirect consistency? ] set X L indirectly consistent ?? (X). Otherwise indirectly inconsistent? . setL , ClRarguments? said indirectly consistent ? Conc(S # ) indirectly consistent? .Henceforth consistent ? mean indirectly consistent ? . rest ASPIC ?framework ASPIC + framework.7. Verifying Postulates ASPIC ? Weak Consequencesection investigate class instantiations just-defined ASPIC ? frameworklanguage L (nonempty) propositional language set Rs strict rulescorresponds Rescher Manors (1970) notion weak consequence language.precisely:Rs = {S | `W finite}speak instantiations ASPIC ? ASPIC ? SAFs `W .theorem states SAFs avoid trivialisation sense Definition 4.1.Theorem 7.1. ASPIC ? SAFs `W defined argumentation theorytrivialising argumentation system.remains investigated whether class instantiations ASPIC ? satisfiesCaminada Amgouds (2007) rationality postulates newly proposed postulatelogical closure. end, first formally specify postulates ASPIC ? .4Definition 7.1. [Rationality postulates ASPIC ? ] Let = (A, C, ) ASPIC ?structured argumentation framework defined ASPIC ? = (L, R, n)K = Kn Kp . Let AF abstract argumentation framework correspondinglet {complete, preferred, grounded, stable}. Then:satisfies closure subarguments postulate iff -extensions E AFholds argument? E subarguments? E;satisfies consistency postulate iff -extensions E AF holdsConc(E) consistent ? ;satisfies strict closure postulate iff -extensions E AF holds? (Conc(E # )).Conc(E) = ClRRs corresponds logic L, satisfies L-closure postulate iff extensions E AF L holds Conc(E) `L Conc(E).Since grounded (preferred, stable) extensions also complete extensions, sufficesprove postulates complete extensions. one way prove first threepostulates try adapt proofs Modgil Prakken (2013) ASPIC +4. Caminada Amgoud (2007) also propose postulates intersection extensions conclusion sets, since satisfaction directly follows satisfaction postulates individualextensions, postulates ignored.217fiGrooters & PrakkenASPIC ? . However, problem general, Rs corresponds `W ,closure transposition contraposition hold ASPIC ? . first givecounterexample closure transposition.Example 7.1. Since {a, b, c} `W b monotonicity logic W ,holds {a, b, c, a} `W b. means a, b, c, b Rs . However,maximal consistent subset {a, b, (a b), a} proves c classicallogic. Therefore {a, b, (a b), a} 0W c a, b, (a b), c/ Rs .means strict rules Rs argumentation system argumentationtheory = (AS, K) instantiated valid inferences logic W ,argumentation theory closed transposition.similar counterexample given closure contraposition.Example 7.2. Consider knowledge base Kn = Kp = {a, b, c, a}. Since{a, b, c, a} `W ab {a, b, (ab), a} 0W c, follows ab ClRs ({a, b, c, a}),c ClRs ({a, b, (a b), a}) (because chaining strict rules allowed).Therefore, strict rules Rs argumentation system argumentationtheory instantiated valid inferences logic W , closedcontraposition.Therefore, results Modgil Prakken (2013) cannot directly usedpurposes. However, somewhat different formal setting, Dung Thang (2014) provideweaker conditions satisfying rationality postulates, impliedimply closure transposition contraposition. therefore use resultsguidance verifying postulates just-defined class instantiations ASPIC ? .Dung Thang (2014) formulate results terms adaptation AmgoudBesnards (2013) abstract-logic approach abstract argumentation abstract attacksupport relations arguments. defining adaptation applycall rule-based systems. turns purposes need DungThangs general abstract framework instead adapt definitionsrule-based instantiation ASPIC ? . so, definitionsresults indicate counterpart Dung Thangs rule-based systems. definitionsimplicitly assume given ASPIC ? structured argumentation framework.Definition 7.1. [Base argument? , (cf. Dung & Thang, 2014, Def. 6)] Letargument? BA finite set subarguments? A. BA base? (Conc(BA));Conc(A) ClRargument? C, C defeats C defeats BA.following example shows intuitive idea base.Example 7.3. Take Kn = , Kp = {a, b}, Rs = {c d} Rd = {a, b c}.following arguments? constructed: A1 : a, A2 : b, A3 : A1 , A2 c A4 : A3 d.See Figure 5.A4 attacked subarguments? A1 , A2 , A3 strict toprule. Every argument? attacks A1 A2 also attacks A3 , every argument?218fiTwo Aspects Relevance Structured ArgumentationFigure 5: Arguments Example 7.3attacks A4 also attacks A3 . easy see every argument? attacks A3 alsoattacks A4 . Conc(A4 ) ClRs (Conc(A3 )), {A3 } base A4 . kindreasoning applies fact set {A1 , A2 , A3 } also base A4 .However note set {A1 , A2 } base A4 , A3 rebutted withoutA1 A2 attacked.Definition 7.2. [Generation arguments? , (cf. Dung & Thang, 2014, Def. 7)]argument? said generated set arguments? S, base BB Sub(S). set arguments? generated denoted GN (S).following lemma follows definition GN (S).Lemma 7.2. [(cf. Dung & Thang, 2014, Lemma 1(2))] every set arguments?S, Sub(S) GN (S).Theorem 7.3. [(cf. Dung & Thang, 2014, Thm. 1)] Let E complete extension,GN (E) = E.Note Lemma 7.2 Theorem 7.3 immediately imply closure? subarguments?postulate since every complete extension E: Sub(E) GN (E) = E.Theorem 7.4. ASPIC ? SAF satisfies closure? subarguments? postulate.Definition 7.3. [Compact, (cf. Dung & Thang, 2014, Def. 8)] ASPIC ? SAFcompact set arguments? S, GN (S) closed? strict rules. equal? (Conc(GN (S)# )).Conc(GN (S)) = ClRfollowing two theorems later combined proving closure? subarguments?postulate.Theorem 7.5. [(cf. Dung & Thang, 2014, Thm. 2)] compact ASPIC ? SAFsatisfies closure? strict rules postulate.Theorem 7.6. ASPIC ? SAF `W compact.Definition 7.4. [Cohesive, (cf. Dung & Thang, 2014, Def. 9)] ASPIC ? SAFcohesive, inconsistent? set arguments? S, GN (S) conflicting (attacks itself).219fiGrooters & PrakkenTheorem 7.7. [(cf. Dung & Thang, 2014, Thm. 3)] cohesive ASPIC ? SAFsatisfies consistency? postulate.next two definitions needed proving cohesiveness.Definition 7.5. [Self-contradiction axiom, (cf. Dung & Thang, 2014, Def. 14)]ASPIC ? SAF said satisfy self-contradiction axiom, minimal inconsistent? (X).set X L: X ClRDefinition 7.6. [Axiom consistent? ] ASPIC ? SAF axiom consistent?? (K ) consistent? .ClRnTheorem 7.8. [(cf. Dung & Thang, 2014, Thm. 5)] compact, axiom consistent?ASPIC ? SAF reasonable argument ordering satisfies self-contradiction axiom,SAF cohesive.Theorem 7.9. ASPIC ? SAF `W satisfies self-contradiction axiom.Combining Theorem 7.5, 7.6, 7.7, 7.8 7.9 results following important conclusion.Theorem 7.10. ASPIC ? SAF `W axiom consistent? reasonable argument ordering satisfies strict-closure? consistency? postulates.Finally, satisfaction proved newly proposed postulate logical closure.Below, `CL denotes classical consequence.Lemma 7.11. ASPIC ? SAF `W satisfies following property: setarguments? holds Conc(S) `CL , Conc(S # ) `CL .Lemma 7.12. ASPIC ? SAF `W satisfies following property: set Earguments? holds Conc(E) strictly closed consistent , Conc(E)classically consistent.Theorem 7.13. ASPIC ? SAF `W axiom consistent? reasonable argument ordering satisfies logical closure postulate.concluded identified class instantiations new ASPIC ? framework Rescher Manors (1970) weak consequence notion satisfiesconsistency closure postulates preventing trivialisation case rebuttingarguments. order obtain results, ASPIC + framework adaptedprohibiting chaining strict rules, resulting new ASPIC ? framework newnotions strict closure indirect consistency, plus new postulate closurelogical consequence.8. Minimality Argumentssection address two aspects minimality arguments. first explain issueinference rules non-minimal sets antecedents detail, investigateeffects requiring strict rules minimal antecedent sets, finally studyissue non-circular arguments.220fiTwo Aspects Relevance Structured Argumentation8.1 Issue Minimality Antecedent Setssaid introduction, deductive approaches argumentation (e.g., Besnard & Hunter,2008; Gorogiannis & Hunter, 2011; Amgoud & Besnard, 2013) require argumentssubset-minimal set premises. example, Gorogiannis Hunter define argument(S, p) set well-formed propositional formulas p well-formedpropositional formula, that:1. consistent classical propositional logic2. implies p classical propositional logic3. proper subset implies p classical propositional logic(Hunter 2007 explores relaxations properties notion approximate arguments.) However, arguments apply defeasible inference rules, third requirement undesirable, since defeasible rules based specific information maywell stronger. Consider following example:Example 8.1. Consider argumentation system Rs = , Rd = {p q; p, rq; r q} consider knowledge base Kn = {p, r} Kp = . followingarguments constructed.A1 :A2 :B1 :B2 :C:pA1 qrA1 , B1 qB1 qreal-world example two defeasible rules q consider exampleintroduction: Observations done ideal circumstances usually correctreasonable account rule strength stronger Observations usually correct.consider argument ordering arguments compared specificity.C B2 A2 C incomparable. semantics unique extension{A1 , A2 , B1 , B2 } results, intuitively correct outcome. However, argumentsrequired subset-minimal premises, B2 cannot constructedoutcome different: grounded extension {A1 , B1 } two preferredstable extensions, namely, {A1 , B1 , A2 } {A1 , B1 , C}.analysis holds defeasible versus strict rules. Consider defeasible rulep q strict rule p, r q: clearly want rule argumentq premises p r, since could well stronger defeasible argument qpremise p. However, noted above, analysis apply strict inferencerules, since strict inference guarantees conclusion given premises. still makessense strict inference rules applied subset-minimal set formulas.requirement, number arguments generated significantlyreduced case restriction introduced, result efficient system.following example illustrates problems arise without requirementstrict rules subset-minimal sets antecedents.221fiGrooters & PrakkenExample 8.2. Suppose strict rules argumentation system instantiatedclassical logic consider knowledge base Kn = Kp = {p, q, r, s, t, u}.among things, argument A1 : p, q p q constructed. Since classical logicmonotonic, following arguments (and more) p q also constructed.A2 : p, q, r p qA4 : p, q, p qA6 : p, q, r, p qA3 : p, q, p qA5 : p, q, u p qA7 : p, q, t, u p qarguments table Example 8.2 considered redundant given A1 .Recall defeasible rules different, since specific defeasible rulesconclusion may well stronger, explained. problem adaptminimality requirement setting defeasible rules investigate extentaffects conclusions drawn.8.2 Minimal Arguments? ASPIC ? Frameworknext investigate whether excluding strict inferences non-subset-minimal setsformulas makes difference. line focus paper prove resultsASPIC ? , proofs ASPIC + counterparts would entirely similar.particular, want know whether conclusions drawn argumentation framework affected case arguments? required minimal. proverather weak condition argument? ordering conclusionscases.First, described idea minimal arguments formally defined. maindifference Definition 6.2 clause (2) disallows application strict rulesnon-minimal antecedent set.Definition 8.1. [Minimal argument? ] minimal argument? basis knowledge base K = (K, ) argumentation system (L, R, n, 0 ):1. KPrem() = {},Conc() = ,Sub(A) = {},TopRule(A) = undefined.2. A1 , . . . , A1 , . . . , minimal arguments? defeasible top ruleK exists strict rule Conc(A1 ), . . . , Conc(An ) Rsexist strict rule a1 , . . . , ai {a1 , . . . , ai } Conc({A1 , . . . , })Rs .Prem(A) = Prem(A1 ) . . . Prem(An ),Conc(A) = ,Sub(A) = Sub(A1 ) . . . Sub(An ) {A},TopRule(A) = Conc(A1 ), . . . , Conc(An ) .222fiTwo Aspects Relevance Structured Argumentation3. A1 , . . . , A1 , . . . , arguments? exists defeasible ruleConc(A1 ), . . . , Conc(An ) Rd .Prem(A) = Prem(A1 ) . . . Prem(An ),Conc(A) = ,Sub(A) = Sub(A1 ) . . . Sub(An ) {A},TopRule(A) = Conc(A1 ), . . . , Conc(An ) .Recall functions defined definition also defined setsarguments? .easy see minimality constraint exclude constructionargument B2 Example 8.1, desired.order show non-minimal arguments? required obtain rightextensions, first define notions minimal extended version argument? .Informally, minimal version argument? argument?except non-minimal strict rule replaced minimal versionstrict rule, i.e., strict rule consequent subset-minimal antecedentset. may lead deletion subarguments? A, namely, subarguments?deleted antecedents. Conversely, extended version minimal argument?argument except zero strict rulesreplaced non-minimal version strict rules. may lead additionsubarguments? , namely, subarguments? added antecedents.Definition 8.2. [A ] argument? A, argument? minimalargument? iff:K = A;form A1 , . . . , =, . . . , Aj that:Conc({Ai , . . . , Aj }) minimal subset Conc({A1 , . . . , })Conc(Ai ), . . . , Conc(Aj ) Rs ;?every k {i, . . . , j} holdsk minimal argument Ak ;form A1 , . . . , =1 , . . . , every1 n holds Ai minimal argument? Ai .set arguments? S, define minimal arguments? S.Below, write , mean argument? minimal argument? A.Note guaranteed unique. example, argument? : pq, qp ptwo minimal variants, namely A1 : p q p A2 : q p p.Obviously, following structured argumentation frameworks ASPIC ? frameworkscontain arguments? .Definition 8.3. [Minimal SAF , SAF ] SAF = (A, C, ), let SAF minimalSAF SAF = (A , C , ). C defined C (A ) =(A ).223fiGrooters & PrakkenDefinition 8.4. [Extended argument? , A+ ] argument? A, argument? A+extended argument? iff:K A+ = A;= A1 , . . . , , A+ = A01 , . . . , A0m n every A0i(1 n) holds A0i extended argument? A+Ai ;= A1 , . . . , , A+ = A01 , . . . , A0n every A0i (1 n)holds A0i extended argument? A+Ai .Below, whenever write A+ , mean argument? A+ extended argument?A.Note also A+ A+ . general, A+ unique.following example clarifies definitions given above.Example 8.3. Consider SAF following arguments:A1 : pA3 : rA2 : p qA4 : p, r qA1 , A2 A3 minimal arguments? , SAF contains three arguments? .A2 minimal argument? corresponding A4 , A2 =4 . Now, also easy see.FurthermoreoneexampleA+indeed A4 A+42.2results follow, needed argument? A, A+ cannotstrictly preferred argument ordering cannot strictly preferred. implied current definition reasonable argument? ordering.illustrated example.Example 8.4. Consider Example 8.3, assume p r Kn assumefollowing argument? ordering (where x means usual x x):A1 A2 ; A3 A4 , A2 A4 . argument? ordering satisfies properties reasonableargument? ordering nevertheless counterintuitive, since differenceA2 A4 A4 contains non-minimal version strict rule applied A2 : sincestrict rules guarantee consequent given antecedent, A4 intuitivelystrictly preferred A2 .Therefore, introduce definition tolerable argument? ordering.Definition 8.5. [Tolerable argument? ordering] tolerable argument? orderingif:every A+ A, A+ A;A, .224fiTwo Aspects Relevance Structured ArgumentationIntuitively, argument? ordering tolerable replacing minimal strict ruleone non-minimal versions cannot make argument? stronger replacingnon-minimal strict rule minimal version cannot make argument? weaker.strict inference rules, meant capture deductively valid inferences,natural property, since operations always amount adding, respectively, deletingattackable subarguments? .next lemmas needed proving equivalence conclusionsdrawn minimal non-minimal structured argumentation frameworks.Lemma 8.1. argument? extended argument? A+ following holds:A0 Sub(A) argument? A00 Sub(A+ ) A00 = A0+ .lemma clarified example below.Example 8.5. Take following arguments? :A1 : p p qB1 : p, r p qA2 : A1B2 : B1?0obvious B2 A+2 . Lemma 8.1 states every subargument+00+0?0A2 subargument B A2 B = . example, take A1 .B1 subargument? B2 B1 = A+1.preceding result needed proving argument? attacks/defeats B,minimal argument? corresponding attacks/defeats every extended version B.Lemma 8.2. tolerable argument? ordering argument? defeats/attacks B,every defeats/attacks every B + .following lemma follows Lemma 8.2.Lemma 8.3. tolerable argument? ordering, complete extensions E:1. E, E every ;2. B/ E, B +/ E B + .following lemma states Dungs characteristic function (see Definition 2.1)monotonic bijection complete extensions SAF onto complete extensionsSAF . lemma based results Dung, Toni, Mancarella (2010)ABA framework, discussed below. First, following definition needed.Definition 8.6. [S ] set arguments S, let set argumentsargument minimal.Lemma 8.4. Let SAF = (A , C , ) minimal structured argumentation framework corresponding SAF = (A, C, ) (for ASPIC ? framework), let AFabstract argumentation framework corresponding SAF . Let tolerable argument?ordering. Also, let C C sets complete extensions SAF SAF respectively let FAF Dungs characteristic function AF . Then:225fiGrooters & Prakken1. E C : FAF (E) = E.2. E C : FAF (E ) = E E C .Clause (1) says set acceptable minimal arguments? w.r.t. extensionminimal SAF change also non-minimal arguments? considered. Clause (2)says set acceptable arguments? w.r.t. extension non-minimal SAFchange minimal arguments? considered.Below, main result stated conclusions drawn SAFSAF s. theorem also based results Dung et al. (2010) ABAframework.Theorem 8.5. Let SAF = (A , C , ) minimal structured argumentation framework corresponding SAF = (A, C, ) (for ASPIC ? framework). Let tolerableargument? ordering. Take {complete, grounded, preferred, stable} F definedDefinition 2.1, then:1. Let E extension SAF , E extension SAF .2. Let E extension SAF , F (E) extension SAF .combining results concluded conclusions drawnASPIC ? structured argumentation framework affected case arguments?required minimal.results generalise Modgil Prakken (2013), prove resultspecial case arguments minimal sets premises (their Proposition 28).related work Dung et al. (2010), study minimal arguments assumption-basedargumentation (ABA) reconstructed Dung et al. (2007) terms Dungs (1995)abstract argumentation frameworks. mentioned above, version ABAreconstructed special case ASPIC + framework without preferences defeasiblerules. fact, Dung et al. (2010) define notion non-redundant argument,general notion minimal argument defined paper. non-redundantargument turn defined terms less redundant relation. Dung et al. (2007)prove similar results ABA Theorem 8.5 ASPIC ? . constructionsproofs clearly inspired work Dung et al. (2010). However, purely formal linkASPIC ? cannot established, since unlike ABA, ASPIC ? allow chainingstrict rules. reason make detailed formal comparison here.8.3 Disallowing Repetition Conclusionsaddress second aspect minimality, studying modification ASPIC ?circular arguments? avoided, is, argument? cannotconclusion one subarguments? . requirement part ASPIC +defined Prakken (2010) Modgil Prakken (2013) part systemVreeswijk (1997), ASPIC -style definition argument originates.argumentation theory, circular arguments generally regarded fallacious, seemsgood idea exclude them. addition, computational benefits, shownsection. First Definition 6.2 argument? modified follows.226fiTwo Aspects Relevance Structured ArgumentationDefinition 8.7. [Strongly minimal arguments? ASPIC ? ] Strongly minimal arguments?defined arguments? Definition 8.1 except following condition addedclauses (2) (3):6 Conc{(A1 , . . . , )}.call structured argumentation framework strongly minimal setarguments? defined Definition 8.7.Disallowing repetition conclusions general change extensions, following example shows.Example 8.6. Consider argumentation theory Kn = Kp = Rd = { p,p, p p}. least two arguments? p least one p:A1 :A2 :B:pA1 ppordering rules Rd p < p < p p (where x < meansstrictly preferred x), last-link ordering defined ModgilPrakken (2013), B defeats A1 A2 defeats B, yields grounded extensioncontains neither three arguments? . However, A2 cannot constructed,grounded extension contains B.noted, excluding circular arguments? avoids fallacies also computational benefits. particular, shown K Rd finite,argument? finite number attackers. words Dung (1995)means induced abstract argumentation framework finitary. shown Dung,finitary AFs number computational benefits. Among things, groundedextension constructed iterative application F operator (see Definition 2.1above) empty set.prove result, first introduce notation relative given .r defined Rs Rd let Cons(r) = .set R let Cons(T ) = { | = Cons(r) r }.Let X = {S Rs | K Cons(Rd )}. Informally, X set strict rulespotentially applicable AT, is, antecedents belongK consequent defeasible rule. Note setequal Rs , since might rules Rs apply set well-formedformulas L yet/ Kand defeasible rule consequent .L define X = {S X | }. Informally, X setpotentially applicable rules formula .next prove following lemma.Lemma 8.6. finite L holds X finite.227fiGrooters & Prakkenfollowing result proven.Theorem 8.7. Let SAF = (A, C, ) strongly minimal structured argumentationframework corresponding argumentation theory finite K Rd .L set {A | Conc(A) = } finite.Note result cannot proved without exclusion arguments chainstrict rules, since otherwise infinite sets arguments conclusion pgenerated constructing arguments follows n, provides counterexampleTheorem 8.7 case arguments excluded:A1 = pA2 = A1 pA3 = A2 p...= An1 premains verify rationality postulates strongly minimal SAFs. turnsresult Section 7 needs reproved only-if half Theorem 7.6.Theorem 8.8. strongly minimal ASPIC ? SAF `W compact.Combined results Section 7 impliesTheorem 8.9. strongly minimal ASPIC ? SAF `W axiom consistent?reasonable argument? ordering satisfies closure? consistency? postulates.9. ASPIC ? Generalisation Classical Argumentationsection explain combining two main contributions far, classinstantiations ASPIC ? obtained proper generalisation three respectstwo versions classical argumentation defined Besnard Hunter (2008) Gorogiannis Hunter (2011)5 . two versions holds particular interestsince Gorogiannis Hunter proven two seven versionsclassical argumentation satisfy consistency postulates. observationexplained follows. Modgil Prakken (2013) prove classical argumentationtwo forms premise attack called direct undercut direct defeat reconstructedfollowing class instantiations ASPIC + : defeasible rules, preference relationsarguments, ordinary premises, L negation symbol definedDefinition 2.3, arguments indirectly consistent premise sets, strict rulesinstantiated classically valid inferences finite sets premises. ModgilPrakken (2013) also prove requiring arguments subset-minimal premisesimplying conclusion change result. shownclassical-logic instantiations prohibition ASPIC ? chain strict rulesmake difference ASPIC + either. show this, showninstantiations ASPIC + Rs corresponds Rescher Manors (1970) notion5. thank Sanjay Modgil suggesting us personal communication228fiTwo Aspects Relevance Structured Argumentationweak consequence arguments? minimal proper generalisationsclassical argumentation. actually prove case nontrivial preferencespreferences fully determined premises; result caseempty preference relation follows special case.minimal ASPIC + ASPIC ? argumentation theory mean argumentation theory arguments arguments? subset-minimal set premises.Clearly, arguments? means also minimal sense Definition 8.1.Moreover, c-structured argumentation framework notion Modgil Prakken(2013): restricted present context amounts requirement SAFsarguments classically consistent premises. notion minimal c-structured argumentation framework defined accordingly.Theorem 9.1. Let argumentation system L classical-logic languageRs corresponding classical logic, let K = Kp knowledge base L. Letobtained removing Rs inference rules invalid according`W . Let = (AS, K) minimal ASPIC + argumentation theory = (AS , K)corresponding minimal ASPIC ? argumentation theory. let SAF = (A, C, )SAF = (A , C , ) be, respectively, minimal c-structured argumentation frameworkdefined ASPIC + minimal structured argumentation framework definedASPIC ? that:A, B, C A: Prem(A) = Prem(B) (C iff C B C iffB C). Likewise ., B : B iff B .6{complete, grounded, preferred, stable} holds that:1. Let E extension SAF , E extension SAFConc(E) = Conc(E ).2. Let E extension SAF , F (E) extension SAFConc(E) = Conc(F (E)).special case result case without defeasible rules, necessarypremises preferences ASPIC ? `W gives conclusion sets classical argumentation. first proper extension classical argumentation preferences.second proper extension necessary premises Kn third proper extension case defeasible rules, observing ASPIC ? strict ruleapplied conclusion least one defeasible subargument? applied subsetminimal classically consistent set formulas classically implies consequentstrict rule.10. Summary, Discussion Conclusionsection summarise discuss results put context relatedwork.6. well-defined since construction SAF SAF fact `W draws inferencesinconsistent sets holds A.229fiGrooters & Prakken10.1 Summary Resultspaper tackled several related issues concerning relevance structured argumentation. carried investigations context ASPIC + framework,consolidates extends one main AI approaches argumentation: modelling combined reasoning strict defeasible inference rules. One main contributionpaper solve long-standing trivialisation problem first identified Pollock (1994,1995). problem tame trivialising effect Ex Falso principle classicallogic way preserves consistency closure properties. solve problem,instantiated strict rules ASPIC + Rescher Manors (1970) paraconsistentlogic W. make work, disallow chaining strict rules arguments sincelogic W satisfy Cut rule; resulted adapted framework ASPIC ?new view postulate strict closure. argued importantwhether conclusion sets closed strict rules whether closedconsequence notion logic strict rules correspond. Accordingly,modified notion strict closure introduced new rationality postulate logicalclosure. proved new versions consistency closure postulates ASPIC ? .also investigated whether two well-known paraconsistent logics, system CDa Costa (1974) Logic Paradox Priest (1979, 1989), suitable sourcesstrict rules. showed cases would lead violation indirect consistency. future research want consider paraconsistent logics wantgenerally study properties paraconsistent logic satisfy usefulsource strict rules ASPIC ? .second issue studied paper minimality arguments. first showednatural assumption argument ordering, restricting strict-rule application subset-minimal sets formulas affect conclusions drawn ASPIC ? .many cases make reasoning efficient ignoring irrelevant information. also noted result easily adapted ASPIC + . disallowedcircular arguments ASPIC ? showed may change status argumentsaffect satisfaction rationality postulates. addition, provedfinite set defeasible rules finite knowledge base ASPIC ? without circulararguments computationally attractive property induced abstract argumentation frameworks finitary sense Dung (1995). latter result cannotadapted ASPIC + since crucially relies prohibition chain strict rules.results minimality hold independently choice strict rules.Finally, proved combining contributions paper version ASPIC ?obtained proper generalisation two versions classical argumentationpremise attack defined Besnard Hunter (2008) Gorogiannis Hunter (2011).two versions particular interest since Gorogiannis Hunterproven two seven versions classical argumentation satisfyconsistency postulates.noted course investigations changed ASPIC +framework originally defined Prakken (2010). fact, first so.Modgil Prakken (2013) consider four variants ASPIC + framework. First,consider versions without constraint arguments consistent premises230fiTwo Aspects Relevance Structured Argumentationvariants define variant conflict-freeness setsarguments defined relative defeat relation attack relation. Furthermore, Caminada et al. (2014) study variant ASPIC + arguments alsorebutted conclusions derived strict rules, provided least one subargumentordinary premise applies defeasible rule. Finally, Wu Podlaszewski (2015) studyvariant ASPIC + set conclusions subarguments argumentconsistent. Thus work ASPIC + resulted family frameworksbased core ideas making different design choices specific points, newmembers family may result future. think healthy situation,since amounts systematic investigation effects different design choices withincommon approach, may applicable certain kinds problems.10.2 Discussion Related Worknext discuss related work.10.2.1 Additional Rationality Postulates Caminada et al. (2012)mentioned introduction, Caminada et al. (2012) formulate new set rationalitypostulates addition Caminada Amgoud (2007), characterise casestrivialisation problem avoided (called postulates non-interferencecrash-resistance). Wu (2012) Wu Podlaszewski (2015) prove adaptationASPIC + consistent arguments preferences new postulatessatisfied complete semantics. investigate semantics. However,attempt prove Caminada et al.s postulates, two reasons. First, wantobtain results case preferences semantics well and, second,seems us Caminada et al.s postulates fact capture stronger intuitive notionone study paper, proving satisfaction new postulateswould required purposes paper.intuition Caminada et al.s notion trivialisation follows. considerknowledge bases, pairs sets formulas L sets defeasible rules.Two knowledge bases defined disjoint composed disjoint setsatomic formulas L. knowledge base KB1 = (K1 , D1 ) contaminatingevery knowledge base KB2 = (K2 , D2 ) set extensions (under given semantics)KB1 KB1 KB2 (where (K1 , D1 ) (K2 , D2 ) = (K1 K2 , D1 D2 ).system said satisfy crash resistance exist contaminatingknowledge base system.consider ASPIC + ASPIC ? instantiation Kp = , Kn = {p} singledefeasible rule d, n( d) = d. instantiation stable extensions, sinceargument = defeats defeated argument.knowledge base (, { d}) contaminating, since syntactically disjoint knowledgebase added extensions. generally, situation ariseknowledge base stable extensions.However, paper interested excluding situationstaming contaminating effect Ex Falso principle. end introducedsimpler definition trivialisation Definition 4.1 managed avoid trivialisation231fiGrooters & Prakkenthus defined even stable semantics, since just-given example casetrivialisation sense Definition 4.1. conclude Caminada et al.spostulates capture stronger notion notion paraconsistency (the focuspaper). future would interesting study whether class instantiationsASPIC ? satisfies Caminada et al.s postulates, would ideally precededstudy exactly captured postulates.10.2.2 Wu (2012), Wu Podlaszewski (2015)alternative attempt solve trivialisation problem, Wu Podlaszewski (2015)introduced inconsistency-cleaned ASP IC Lite system. system similarargumentation formalism treated Caminada Amgoud (2007) seensystem specified ASPIC + arguments equally preferred. Wu Podlaszewski define argument consistent set conclusions subargumentsdirectly consistent. argumentation framework inconsistency-cleaned inconsistent arguments removed. Wu Podlaszewski prove inconsistency-cleanedversion ASP IC Lite satisfies original rationality postulates CaminadaAmgoud (2007) new postulates Caminada et al. (2012). solvestrivialisation problem retaining known results closure consistency. However,Wu (2012) Wu Podlaszewski (2015) provide counterexample (originally dueLeon van der Torre) satisfaction consistency strict-closure postulates casepreferences added last-link argument ordering Prakken (2010) applied.example originally presented ASP IC Lite system, translatedASPIC + framework.Example 10.1. [(Wu, 2012; Wu & Podlaszewski, 2015)] Given knowledge base K = ,Rd = { p; p q; p q} Rs instantiated valid inferences classicallogic. Assume p priority 1 (lowest), p q priority 2 (middle)p q priority 3 (highest). case, construct following argumentsassociated (last-link principle) preferences. Table 1 depicts argumentsgenerated7 Figure 6 shows defeat relation arguments.ArgumentA1 : pA2 : p qA3 : A1 qA4 : A1 , A2 qA5 : A1 , A3 (p q)A6 : A2 , A3 pPreference(1)(2)(3)(1)(1)(2)Table 1: Six argumentsFigure 6: Partial abstract AF7. Note classical reasoning allows generation infinitely arguments irrelevantsix arguments.232fiTwo Aspects Relevance Structured ArgumentationArgument A6 inconsistent argument. according solution proposedWu Podlaszewski case without preferences, A6 needs deletedargumentation framework. Figure 7 shows resulting argumentation framework.Figure 7: Inconsistency-cleaned versioncomplete extension E = {A1 , A2 , A3 , A4 , A5 }. satisfy closurestrict rules A2 A3 E A6 E. Moreover, direct consistencyalso satisfied since A3 A4 complete extension E, conclusionsq p q consistent.Arguments A3 A4 opposite conclusions without preferences A4 would defeatA3 . However, preference ordering chosen counterexample, A4 weakerA3 A4 cannot defeat A3 . A3 A4 also attacked subarguments.fact arguments complete extension causes problem.Every argument concludes p uses A1 subargument, impliesinconsistent argument removed framework. Therefore,A1 defeated argument, means A3 A4 completeextension. concluded inconsistent argument like A6 really neededdefeat A3 A4 .Furthermore, observed A6 deleted, problems all.Figure 6 shows case one complete extension E = {A2 }.satisfies consistency strict closure. Therefore, example, undesirableA6 removed.Consider next ASPIC ? framework defined paper. argumentsconstructed original framework. way, one complete extension{A2 } and, explained above, rationality postulates satisfied. approachASPIC ? therefore general solution Wu Podlaszewski (2015),since applies frameworks include preferences defeasible rules.noted idea forbid chaining strict rules earlier suggestedus Martin Caminada (personal communication). However, combined suggestionidea disallow inconsistent arguments; case, counterexampleconsistency strict closure still constructed. Apart this, one could saylogic W , satisfy Cut rule, provides theoretical foundationidea disallow chaining strict rules.first sight, would seem system allows inconsistent arguments flawedeven satisfies consistency closure postulates. However, note argumentsnever extension. Although explained Caminada (2005),inconsistent arguments sometimes prevent arguments extension,problem arguments based Ex Falso principle, since233fiGrooters & Prakkenprinciple holds matter logic. Among things, means allowing argumentsbased Ex Falso would dramatically increase number (counter)arguments,would lead computational problems. contrast, inconsistent arguments argumentA6 Example 10.1 arise specific modeling choices Rd dictated logicproliferate, logical computational point viewneed exclude them.10.3 Dungs (2014) Rule-Based SystemsDung (2014) introduces formalism rule-based systems (further studied Dung, 2016),essentially notational variant ASPIC + restricted literal languages emptyknowledge bases (necessary ordinary facts represented strict defeasible rulesempty antecedents). Dung introduces three new rationality postulates. postulateattack monotonicity informally says strengthening argument cannot eliminateattack argument another. postulate credulous cumulativity informallymeans changing conclusion argument extension necessary factcannot eliminate extension. Finally, property irrelevance redundant defaultssays adding redundant defaults change set extensions.Dung investigates argument orderings studied Modgil Prakken (2013)whether satisfy consistency postulates, positive negativeresults. results valuable, Dung (2014) unfortunately, somewhat confusesmatters referring orderings studied Modgil Prakken ASPIC +semantics. Thus overlooks distinction ASPIC + general frameworkinstantiations framework. argument orderings studied Modgil Prakkeninherent ASPIC + framework example orderings. ASPIC +framework variants leave every room ways define argument ordering.noted Dung (2016) refer Modgil Prakkens orderingsASPIC + semantics thus respects orderings inherentASPIC + .present purposes Dungs findings strictly speaking irrelevant sincestudied particular argument orderings. future research would interesting investigate whether use particular argument orderings ASPIC ? satisfies Dungs postulatesattack monotonicity irrelevance redundant defaults. However, disagreeDung (2014) credulous cumulativity would desirable property. contrary,Prakken Vreeswijk (2002, section 4.4) believe property insteadundesirable, since strengthening defeasible conclusion indisputable fact may makearguments stronger before. give power defeat argumentsbefore. may well result loss extensionconclusion promoted indisputable fact.10.4 Conclusionpaper successfully addressed open issues concerning relevance structured argumentation. solved trivialisation problems arise argumentation includes full classical logic created prospects reducing computational complexity enforcing minimality non-circularity arguments ensuring234fiTwo Aspects Relevance Structured Argumentationclosure consistency results. done context ASPIC approach,resulting new variant ASPIC + framework called ASPIC ? well-behavedclass instantiations new framework. class instantiations shownproper generalisation classical argumentation preferences defeasible rules.class instantiations properties main contribution paper.paper employed flexible attitude towards design choices within ASPICapproach. thus shown approach fruitful one, provided distinctionframeworks instantiations kept mind.Acknowledgementthank three JAIR reviewers many useful comments earlier versionspaper.Appendix A. Proofsappendix contains proofs results reported paper.A.1 Proofs Section 7Theorem 7.1 ASPIC ? SAFs `W defined argumentation theorytrivialising argumentation system.Proof. end, must show argumentation system propositional language Rs defined, knowledge base K defined{, } K L hold argument?conclusion constructed basis K AS. Consider AS.choose K = Kn Kp Kp = Kn = {, } formula L (guaranteed exist since L assumed nonempty). definition `W clearly holdsK 6`W ( ) since consistent subset K classically imply contradiction.exists strict argument? ( ) basis K AS.Theorem 7.3 Let E complete extension, GN (E) = E.Proof. First note according Definition 7.2 set arguments? Sub(S)GN (S), therefore E GN (E).Suppose argument? C defeats argument? GN (E). Let BA baseBA Sub(E), C defeats BA. Hence C defeats Sub(E) defeatsE. Since E complete extension, every defeat E counter defeated E.defended E, E. Therefore GN (E) E.Theorem 7.5 compact ASPIC ? SAF satisfies closure? strict rules postulate.Proof. Let E complete extension. compactness? implies GN (E) closed?strict rules. Theorem 7.3 E closed? strict rules.Theorem 7.6 ASPIC ? SAF `W compact.235fiGrooters & PrakkenProof.? (GN (S)# )][Conc(GN (S)) ClR? (GN (S)# ). needs shownLet set arguments? ClR? (X).Conc(GN (S)). Let X minimal subset Conc(GN (S)# ) ClR?Hence strict argument A0 X conclusion . let SX minimal set arguments? GN (S)# s.t. Conc(SX ) = X. Let argument? obtainedreplacing leaf A0 (viewed directed acyclic graph) labelled literalX argument? conclusion SX . Note possible sincearguments? SX basic fallible arguments? necessary premises. obvious conclusion . shown SX base A. Suppose Bargument? defeating A. Since A0 strict argument? X, B must defeat basic falliblesubargument? SX . Hence B defeats SX . Thus GN (S). Hence Conc(GN (S)).? (GN (S)# )][Conc(GN (S)) ClR? (GN (S)# ).Suppose Conc(GN (S)), shown ClR?Conc(GN (S)) means argument GN (S) Conc(A) = .Suppose form Kp , F A? (GN (S)) thus GN (S)# .? () N P ? (GN (S)) respecSuppose form Kn , ClR? (GN (S)# ).tively, ClRSuppose form A1 , . . . , , F A? (GN (S)) thus GN (S)# .Finally, suppose form A1 , . . . , , since A1 , . . . , basic fallible? (GN (S)# ).arguments? A1 , . . . , GN (S)# . Therefore ClR? (GN (S)# ).concluded ClRproven AF compact.Theorem 7.7 cohesive ASPIC ? SAF satisfies consistency? postulate.Proof. Let E complete extension. Suppose E inconsistent? . cohesion,follows GN (E) conflicting. Theorem 7.3 states E conflicting.contradiction since E complete extension, E consistent? .Theorem 7.8 compact, axiom consistent? ASPIC ? SAF reasonable argument?ordering satisfies self-contradiction axiom, SAF cohesive.Proof. Let inconsistent? set arguments? take minimal inconsistent? subset0 Sub(S). Definition 6.4 combined axiom consistency? minimality 0causes 0 6= contains basic fallible arguments? necessary premises. Remark 0 cannot consist necessary premises, axiom consistency? .note Conc(S 0 ) minimal inconsistent set. Since AF satisfies self? (Conc(S 0 )). Let Bcontradiction axiom, Conc(S 0 ) holds ClR?0weakest argument Conc(B) = . Note B cannot necessary premise reasonable argument? ordering fact 0 mustcontain basic fallible arguments? . construction 0 holds 0 GN (S 0 )# .? (Conc(GN (S 0 )# )). compactness AF followsTherefore ClRConc(GN (S 0 )). Therefore, argument? GN (S 0 )Conc(A) = . Hence attacks B. base 0 , concluded basicfallible subarguments? 0 . B weakest argument? 0 ,236fiTwo Aspects Relevance Structured Argumentationreasonable argument? ordering fact basic fallible subarguments?0 implies B. means defeats B. Since B 0 GN (S 0 ) GN (S),GN (S) conflicting. Therefore, AF cohesive.Theorem 7.9 ASPIC ? SAF `W satisfies self-contradiction axiom.Proof. proved every minimal inconsistent? set X L holds? (X). Let X minimally inconsistent? set take = X\.X, ClRNote maximal consistent? subset X S, ` (where ` denotesclassical entailment). deduction theorem classical logic ` , implies` . Since maximal consistent? subset X, X `W . holds every? (X). concluded AF satisfies self-contradictionX, ClRaxiom.Lemma 7.11 ASPIC ? SAF `W satisfies following property: setarguments? holds Conc(S) `CL , Conc(S # ) `CL .Proof. Suppose Conc(S) `CL . Consider argument? Ti \ # .definition `W choice Rs holds Conc(Ti ) `CL i. since `CLsatisfies Cut rule, Conc(S # ) `CL .Lemma 7.12 ASPIC ? SAF `W satisfies following property: set Earguments? holds Conc(E) strictly closed consistent , Conc(E)classically consistent.Proof. Assume Conc(E) strictly closed consistent suppose contradiction Conc(E) `CL , . Consider subset-minimal E Conc(S) `CL, . Lemma 7.11 Conc(S # ) `CL , . Consider minimal # Conc(T ) `CL , . Note cannot empty.Conc(T ) holds Conc(T )\{} `CL . choice holds Conc(T )\{}classically consistent. since Conc(E) strictly closed argument? #strict top rule, exists argument? 0 E 0Conc(T 0 ) = Conc(T ) \ {}. Conc(E) consistent .Theorem 7.13 ASPIC ? SAF `W axiom consistent? reasonable argument ordering satisfies logical closure postulate.Proof. Suppose Conc(E) `W complete extension E consider subsetminimal E Conc(S) `W . definition `W choice Rsholds Conc(S) classically consistent Conc(S) `CL . Lemma 7.11Conc(S # ) `CL . Moreover, Conc(S # ) classically consistentLemma 7.12 fact subset classically consistent set also classicallyconsistent (note Theorem 7.10 Conc(E) strictly closed consistent ,conditions Lemma 7.12 fulfilled). Conc(S # ) `W choice Rsexists strict rule Conc(S # ) . Since argument? # strict top rule,Conc(E) strict closure? E.237fiGrooters & PrakkenA.2 Proofs Section 8.2Lemma 8.1 argument? extended argument? A+ following holds:A0 Sub(A) argument? A00 Sub(A+ ) A00 = A0+ .Proof. proof proof induction height argument? (viewed directedacyclic graph). Suppose element K (so height 1), A0 A+equal A. means A00 also equal easy see A00 = A0+ .Suppose lemma holds arguments? height {1, 2, . . .}.proven arguments? height + 1. Take arbitrary argument? height + 1take subargument? A0 A. Note cannot element K since heightgreater 1. Therefore, form A1 , . . . , / .two possibilities: either (i) A0 subargument? one arguments? A1 , . . . , ,(ii) A0 equal A.(i). j {1, . . . , n} A0 subargument? Aj . Aj height i,000+00must A00 Sub(A+j ) = . According Definition 8.4,?+subargument .(ii). A0 equal A. Take A00 A+ , follows A00 = A0+ A00 Sub(A+ ).proved every argument? lemma holds.Lemma 8.2 tolerable argument? ordering argument? defeats/attacks B,every defeats/attacks every B + .Proof.Attack(i). Suppose undercuts B, Conc(A) = n(r) defeasible top rule rB 0 Sub(B). definition , conclusion every A.Lemma 8.1 holds every B + , subargument? B 00 Sub(B + )B 00 = B 0+ . Note B 0 B 00 defeasible top rule, since definitionB + , strict rules mutate. follows every undercuts B +B 00 .(ii). Suppose undermines B, Conc(A) ordinary premise B 0 . EveryB + also ordinary premise. definition (Definition 8.2), conclusionevery A. Therefore undermines every B + B 0 .(iii). Suppose rebuts B, Conc(A) conclusion basic fallibleargument? B 0 Sub(B). conclusion every A. Lemma 8.1,holds every B + , subargument? B 00 Sub(B + ) B 00 = B 0+ .Note conclusions B 0 B 00 B 00 also defeasible toprule. Therefore rebuts B + B 00 .DefeatSuppose argument? defeats B. means attacks B subargument? B 0 .(i) undercuts B, (ii) B 0 .(i). Suppose undercuts B, follows every undercuts B + B 00 (seereasoning attack (i)). implies defeats every B + .(ii). Otherwise case B 0 . Note that, tolerableargument? ordering, A. proof attack shown every attacks238fiTwo Aspects Relevance Structured Argumentationevery B + B 00 , B 00 = B 0+ . tolerable argument? ordering causes B 00 B 0 .Therefore every B 00 holds B 00 , every defeats every B + .Lemma 8.3 tolerable argument? ordering, complete extensions E:1. E, E every ;2. B/ E, B +/ E B + .Proof. (1). Suppose E let B argument? defeating . Lemma8.2, B defeats A. Therefore, must argument? C E C defeats B.Hence, defended E thus E.(2). Suppose B/ E. exists argument? defeats Bexist C E defeats A. According Lemma 8.2, defeats every B + ,B + defended E hence B +/ E.Lemma 8.4 Let SAF = (A , C , ) minimal structured argumentation framework corresponding SAF = (A, C, ) (for ASPIC ? framework), let AFabstract argumentation framework corresponding SAF . Let tolerable argument?ordering. Also, let C C sets complete extensions SAF SAF respectively let FAF Dungs characteristic function AF . Then:1. E C : FAF (E) = E.2. E C : FAF (E ) = E E C .Proof. Take two sets arguments? X X . Suppose FAF (Y )FAF (X), must argument? defended X . SinceX , defended . Therefore, monotonicity FAF (E) respectset inclusion obvious.(1). shown FAF (E) indeed function C C FAF (E) =E showing FAF (E) complete extension SAF , E complete extensionSAF . Let E complete extension SAF . First shown Eadmissible set SAF .E conflict-free , conflict-free A. also defeats minimalargument? defeating E contains every minimal argument? defended E.E B defeats A, take B . B defeatsLemma 8.2, C E defeats B . C also defeats B Lemma 8.2combined fact B B + . means defended E.concluded E defeats every argument? defeats E. already shown Econflict-free, E admissible set.argument? FAF (E), defended E , everyE. Therefore, (FAF (E) E) = , thus FAF (E) = E.Now, shown FAF (E) complete extension. Let defendedFAF (E) let B defeat A. Hence, C FAF (E) defeating B. C FAF (E)means C defended E, thus E defeats every argument? defeating C. Supposeargument? defeats C , also defeats C Lemma 8.2. Therefore, E must defeatD, C defended E. Since E complete extension SAF , C E. C239fiGrooters & Prakkendefeats B (Lemma 8.2). Therefore, E defeats B. Thus, defended E, thereforeFAF (E). consequence, FAF (E) complete.(2). Let E complete extension SAF . shown E complete SAF .First shown E admissible set . Since E conflict-free, Econflict-free well.Lemma 8.3 fact E complete SAF , minimal versionarguments? E belongs E. Let defeat E . Hence, B E defeating A.According Lemma 8.3 B E, B E . Hence, B defeats (Lemma 8.2). Thus,E admissible.minimal argument? defended E defended E hence belongs EE . E therefore complete.Since E E E complete, clear FAF (E ) FAF (E) = E.shown argument? defended E also defended E . Let argument?defended E SAF let B argument? defeating A. Hence, argument?C E defeating B C E defeats B. Hence E defeats B. Thus defendedE SAF . concluded FAF (E ) FAF (E) = E, i.e. FAF (E ) = E.[Injective] Take X, C FAF (X) = FAF (Y ). obvious FAF (X) =FAF (Y ) . according proof point (1) FAF (X) = X FAF (Y ) = .Therefore, follows X = .[Surjective] shown C X C FAF (X) = .take X , proof point (2) provides X CFAF (X) = .[Bijective] Injectivity surjectivity provides FAF bijection C onto C.Theorem 8.5 Let SAF = (A , C , ) minimal structured argumentation framework corresponding SAF = (A, C, ) (for ASPIC ? framework). Let tolerableargument? ordering. Take {complete, grounded, preferred, stable}, then:1. Let E extension SAF , E extension SAF .2. Let E extension SAF , F (E) extension SAF .Proof.[T = complete]Let C C sets complete extensions SAF SAF , respectively. Lemma8.4 states FAF bijection C C. immediately provides F (E) C.second point Lemma 8.4 states E C .[T {grounded, preferred}]Lemma 8.4 follows immediately E C , FAF (E) minimalmaximal respect set inclusion E E minimal maximal respectivelyC . Hence E grounded preferred SAF FAF (E) groundedpreferred SAF , respectively.[T = stable](1). Take E stable extension SAF . Suppose contradiction Estable extension SAF . must argument? ,/ Edefeated argument? E . However,/ E implies/ E since?minimal. E stable, thus must argument B E B defeats A.240fiTwo Aspects Relevance Structured ArgumentationB E (Lemma 8.3) defeats A. clear B E , contradictionfact E defeat A. Therefore, E stable extension SAF .(2). Take E stable extension SAF . Suppose contradiction FAF (E)stable extension SAF . must argument?/ FAF (E) FAF (E) defeat A./ FAF (E) means defendedE. Therefore, must argument? B defeatsC E defeats B. Since E stable SAF , B E E defeats A. impliesFAF (E) defeats A, contradiction fact FAF (E) defeatA. Therefore, FAF (E) stable extension SAF .A.3 Proofs Section 8.3Lemma 8.6 finite L holds X finite.Proof. Note single strict rule , since KRd finite, set sets equal set antecedents strict rule Xalso finite.Theorem 8.7 Let SAF = (A, C, ) strongly minimal structured argumentation framework corresponding argumentation theory finite K Rd . Lset {A | Conc(A) = } finite.Proof. prove result iteratively constructing set finite arguments?constructible K AS.A0 = K Ad0 = As0 = ;Ai+1 = Ai Adi+1 = Asi+1Adi+1 = {A = Q | Q Ai Conc(Q) Rd 6 Conc(Q)}.Asi+1 = {A = Q | Q Ai Conc(Q) Rs 6 Conc(Q)B Q holds TopRule(B) Rs }.easy see ni=0 Ai set constructible finite arguments? .prove theorem, note first A0 finite since K finite Adi finitesince Rd finite. contrast, Asi infinite since given choice Rs ,set wff infinite number strict rules applies. However, follows Lemma 8.6L exists finite number arguments? Asiconclusion . Together observations imply L existsfinite number arguments? conclusion Ai .left prove construction finite. follows factK Rd finite fact Definition 8.7 rules cannot repeatedargument? . point, Adi = . also Asi+1 = since strict rules cannotchained argument? . construction set finite arguments? constructibleK finite, L step construction finiteset arguments? conclusion created. proves theorem.241fiGrooters & PrakkenTheorem 8.8 strongly minimal ASPIC ? SAF `W compact.Proof.? (GN (S)# )][Conc(GN (S)) ClR? (GN (S)# ). needs shownLet set arguments? ClR? (X).Conc(GN (S)). Let X minimal subset Conc(GN (S)# ) ClRHence strict argument? A0 X conclusion . let SXminimal set arguments? GN (S)# s.t. Conc(SX ) = X. problemrepetition conclusions here, since X A0 = . Let argument? obtainedreplacing leaf A0 (viewed directed acyclic graph) labelled literalX argument? conclusion SX . Note repeats conclusionneed since clearly Conc(GN (S)) done. Otherwise,construction possible since, firstly, arguments? SX basic fallible arguments?necessary premises and, second, definition GN (S)# holdsrepeat conclusion X. obvious conclusion . shownSX base A. Suppose B argument? defeating A. Since A0 strict argument?X, B must defeat basic fallible subargument? SX . Hence B defeats SX . ThusGN (S). Hence Conc(GN (S)).? (GN (S)# )][Conc(GN (S)) ClRTheorem 7.6.proven strongly minimal SAF compact.A.4 Proofs Section 9Theorem 9.1 Let argumentation system L classical-logic languageRs corresponding classical logic, let K = Kp knowledge base L. Letobtained removing Rs inference rules invalid according`W . Let = (AS, K) minimal ASPIC + argumentation theory = (AS , K)corresponding minimal ASPIC ? argumentation theory. let SAF = (A, C, )SAF = (A , C , ) be, respectively, minimal c-structured argumentation frameworkdefined ASPIC + minimal structured argumentation framework definedASPIC ? that:A, B, C A: Prem(A) = Prem(B) C iff C B C iffB C. Likewise ., B : B iff B .{complete, grounded, preferred, stable} holds that:1. Let E extension SAF , E extension SAFConc(E) = Conc(E ).2. Let E extension SAF , F (E) extension SAFConc(E) = Conc(F (E)).Proof. First, easy see A. Next, show induction structurearguments? argument? \ exists argument? BPrem(A) = Prem(B) Conc(A) = Conc(B).242fiTwo Aspects Relevance Structured Argumentationbase case K. B = A. inductive case consider =B1 , . . . , Bn . induction hypothesis arguments? B1 , . . . , Bn satisfy coni inductionditions. Bi (1 n) form Bi = Bki , . . . , Bmhypothesis yields Bk , . . . , Bm K. replacing Bi Bki , . . . , BmB1 , . . . , Bn yields argument? B satisfying conditions. Note corresponding strict inference rule Rs since classical logic satisfies cut rulearguments? consistent premises.easy prove along lines proof Lemma 8.2 argument?argument? attacked/defeated also attacked/defeated Bvice versa, argument? attacking/defeating also attacks/defeats B viceversa. proof Theorem 9.1 completed along lines proofTheorem 8.5.ReferencesAmgoud, L., & Besnard, P. (2013). Logical limits abstract argumentation frameworks.Journal Applied Non-classical Logics, 23, 229267.Bench-Capon, T., Prakken, H., & Visser, W. (2011). Argument schemes two-phasedemocratic deliberation. Proceedings Thirteenth International ConferenceArtificial Intelligence Law, pp. 2130, New York. ACM Press.Besnard, P., & Hunter, A. (2008). Elements Argumentation. MIT Press, Cambridge,MA.Bondarenko, A., Dung, P., Kowalski, R., & Toni, F. (1997). abstract, argumentationtheoretic approach default reasoning. Artificial Intelligence, 93, 63101.Brewka, G. (1991). Nonmonotonic Reasoning: Logical Foundations Commonsense. Cambridge University Press, Cambridge.Caminada, M. (2005). Contamination formal argumentation systems. ProceedingsSeventeenth Belgian-Dutch Conference Artificial Intelligence (BNAIC-05),Brussels, Belgium.Caminada, M., & Amgoud, L. (2007). evaluation argumentation formalisms.Artificial Intelligence, 171, 286310.Caminada, M., Carnielli, W., & Dunne, P. (2012). Semi-stable semantics. Journal LogicComputation, 22, 12071254.Caminada, M., Modgil, S., & Oren, N. (2014). Preferences unrestricted rebut. Parsons, S., Oren, N., Reed, C., & Cerutti, F. (Eds.), Computational Models Argument.Proceedings COMMA 2014, pp. 209220. IOS Press, Amsterdam etc.Caminada, M., Sa, S., Alcantara, J., & Dvorak, W. (2015). differenceassumption-based argumentation abstract argumentation. IFCoLog JournalLogic Applications, 2, 1534.Da Costa, N. (1974). theory inconsistent formal systems. Notre Dame JournalFormal Logic, 15 (4), 497510.243fiGrooters & PrakkenDung, P. (1995). acceptability arguments fundamental role nonmonotonic reasoning, logic programming, nperson games. Artificial Intelligence, 77,321357.Dung, P. (2014). axiomatic analysis structured argumentation prioritized defaultreasoning. Proceedings 21st European Conference Artificial Intelligence(ECAI 2014), pp. 267272.Dung, P. (2016). axiomatic analysis structured argumentation priorities. Artificial Intelligence, 231, 107150.Dung, P., Mancarella, P., & Toni, F. (2007). Computing ideal sceptical argumentation.Artificial Intelligence, 171, 642674.Dung, P., & Thang, P. (2014). Closure consistency logic-associated argumentation.Journal Artificial Intelligence Research, 49, 79109.Dung, P., Toni, F., & Mancarella, P. (2010). design guidelines practical argumentation systems. Baroni, P., Cerutti, F., Giacomin, M., & Simari, G. (Eds.),Computational Models Argument. Proceedings COMMA 2010, pp. 183194. IOSPress, Amsterdam etc.Garcia, A., & Simari, G. (2004). Defeasible logic programming: argumentative approach.Theory Practice Logic Programming, 4, 95138.Ginsberg, M. (1994). AI nonmonotonic reasoning. Gabbay, D., Hogger, C., & Robinson, J. (Eds.), Handbook Logic Artificial Intelligence Logic Programming,pp. 133. Clarendon Press, Oxford.Gorogiannis, N., & Hunter, A. (2011). Instantiating abstract argumentation classicallogic arguments: postulates properties. Artificial Intelligence, 175, 14791497.Grooters, D., & Prakken, H. (2014). Combining paraconsistent logic argumentation.Parsons, S., Oren, N., Reed, C., & Cerutti, F. (Eds.), Computational ModelsArgument. Proceedings COMMA 2014, pp. 301312. IOS Press, Amsterdam etc.Hunter, A. (2007). Real arguments approximate arguments. Proceedings 22ndNational Conference Artificial Intelligence (AAAI-07), pp. 6671.Jakobovits, H., & Vermeir, D. (1999). Robust semantics argumentation frameworks.Journal Logic Computation, 9, 215261.Krause, P., Ambler, S., Elvang-Gransson, M., & Fox, J. (1995). logic argumentationreasoning uncertainty. Computational Intelligence, 11 (1), 113131.Lin, F., & Shoham, Y. (1989). Argument systems. uniform basis nonmonotonicreasoning. Principles Knowledge Representation Reasoning: ProceedingsFirst International Conference, pp. 245255, San Mateo, CA. Morgan KaufmannPublishers.Modgil, S., & Prakken, H. (2013). general account argumentation preferences.Artificial Intelligence, 195, 361397.Modgil, S., & Prakken, H. (2014). ASPIC+ framework structured argumentation:tutorial. Argument Computation, 5, 3162.244fiTwo Aspects Relevance Structured ArgumentationPollock, J. (1987). Defeasible reasoning. Cognitive Science, 11, 481518.Pollock, J. (1990). theory defeasible reasoning. International Journal IntelligentSystems, 6, 3354.Pollock, J. (1992). reason defeasibly. Artificial Intelligence, 57, 142.Pollock, J. (1994). Justification defeat. Artificial Intelligence, 67, 377408.Pollock, J. (1995). Cognitive Carpentry. Blueprint Build Person. MITPress, Cambridge, MA.Prakken, H. (2010). abstract framework argumentation structured arguments.Argument Computation, 1, 93124.Prakken, H. (2012). reflections two current trends formal argumentation.Logic Programs, Norms Action. Essays Honour Marek J. SergotOccasion 60th Birthday, pp. 249272. Springer, Berlin/Heidelberg.Prakken, H., & Modgil, S. (2012). Clarifying misconceptions ASPIC+ framework. Verheij, B., Woltran, S., & Szeider, S. (Eds.), Computational ModelsArgument. Proceedings COMMA 2012, pp. 442453. IOS Press, Amsterdam etc.Prakken, H., & Sartor, G. (1997). Argument-based extended logic programming defeasible priorities. Journal Applied Non-classical Logics, 7, 2575.Prakken, H., & Vreeswijk, G. (2002). Logics defeasible argumentation. Gabbay, D.,& Gunthner, F. (Eds.), Handbook Philosophical Logic (Second edition)., Vol. 4, pp.219318. Kluwer Academic Publishers, Dordrecht/Boston/London.Prakken, H., Wyner, A., Bench-Capon, T., & Atkinson, K. (2015). formalisationargumentation schemes legal case-based reasoning ASPIC+. Journal LogicComputation, 25, 11411166.Priest, G. (1979). logic paradox. Journal Philosophical Logic, 8 (1), 219241.Priest, G. (1989). Reasoning truth. Artificial Intelligence, 39 (2), 231244.Read, S. (1988). Relevant logic. Blackwell, Oxford.Rescher, N., & Manor, R. (1970). inference inconsistent premises. JournalTheory Decision, 1, 179219.Simari, G., & Loui, R. (1992). mathematical treatment defeasible argumentationimplementation. Artificial Intelligence, 53, 125157.Vreeswijk, G. (1997). Abstract argumentation systems. Artificial Intelligence, 90, 225279.Walton, D. (1996). Argumentation Schemes Presumptive Reasoning. Lawrence ErlbaumAssociates, Mahwah, NJ.Wu, Y. (2012). Argument Conclusion. Argument-based Approaches Discussion, Inference Uncertainty. Doctoral Dissertation Faculty Sciences, TechnologyCommunication, University Luxemburg.Wu, Y., & Podlaszewski, M. (2015). Implementing crash-resistence non-interferencelogic-based argumentation. Journal Logic Computation, 25, 303333.245fiJournal Artificial Intelligence Research 56 (2016) 429-461Submitted 02/16; published 07/16Efficient Mechanism Design Online SchedulingXujin ChenXiaodong Huxchen@amss.ac.cnxdhu@amss.ac.cnAMSS, Chinese Academy Science, Beijing, ChinaTie-Yan LiuWeidongTao Qintyliu@microsoft.comweima@microsoft.comtaoqin@microsoft.comMicrosoft Research, Beijing, ChinaPingzhong Tangkenshin@mail.tsinghua.edu.cnTsinghua University, Beijing, ChinaChangjun Wangwcj@amss.ac.cnBeijing University Technology, Beijing, ChinaBo Zhengzhengb10@mails.tsinghua.edu.cnTsinghua University, Beijing, ChinaAbstractpaper concerns mechanism design online scheduling strategic setting.setting, job owned self-interested agent may misreport releasetime, deadline, length, value job, need determineschedule jobs, also payment agent. focus designincentive compatible (IC) mechanisms, study maximization social welfare (i.e.,aggregated value completed jobs) competitive analysis. first derive two lowerbounds competitive ratio deterministic IC mechanism characterizelandscape research: one bound 5, holds equal-length jobs;bound ln + 1 o(1), holds unequal-length jobs, maximum ratiolengths two jobs. propose deterministic IC mechanism showsimple mechanism works well two models: (1) preemption-restartmodel, mechanism achieve optimal competitive ratio 5 equal-length jobs1near optimal ratio ( (1)2 + o(1)) ln unequal-length jobs, 0 < < 1small constant; (2) preemption-resume model, mechanism achieveoptimal competitive ratio 5 equal-length jobs near optimal competitive ratio(within factor 2) unequal-length jobs.1. IntroductionOnline scheduling widely studied literature (Baruah, Koren, Mao, Mishra,Raghunathan, Rosier, Shasha, & Wang, 1992; Baruah, Haritsa, & Sharma, 1994; Porter,2004; Zheng, Fung, Chan, Chin, Poon, & Wong, 2006; Ting, 2008), job characterized release time, deadline, length, value successful completiondeadline. Inspired emerging areas like computational economics cloud computing, consider strategic setting online scheduling problem, jobowned self-interested agent may incentive manipulate schedulc2016AI Access Foundation. rights reserved.fiChen, Hu, Liu, Ma, Qin, Tang, Wang & Zhenging algorithm order better off. specific, agent may deliberately delayrelease time job, inflate length, misreport value deadline.Given situation, carefully designed online scheduling mechanism neededregulate strategic behaviors agents (approximately) optimize systemobjectives. work, focus maximization social welfare, i.e., totalvalue completed jobs.1 use competitive analysis (Lavi & Nisan, 2004) evaluateperformance mechanism, compares social welfare implementedmechanism (without knowledge future jobs) optimal offlineallocation (with knowledge future jobs).work, consider two scheduling models: preemption-restart model (Ting,2008) preemption-resume model (Porter, 2004). preempted, jobs firstmodel restart beginning; jobs second model resumebreak point. Since preemption always assumed work, two models alsoreferred restart model resume model, respectively, involved jobscalled non-resumable resumable, respectively.1.1 Problem Formulationconsider online scheduling models infinite time period = R0 . Supposesingle machine processes one job given time. Jobs cometime, use J denote set jobs. job j J owned self-interestedagent (which also denoted j simplicity); characterized private typej = (rj , dj , lj , vj ) R>0 R>0 , rj release time2 , dj deadline,lj length (i.e., processing time), vj value job completeddeadline.resumable job j completed processed lj time units totalrelease time rj deadline dj , non-resumable job j completedprocessed lj consecutive time units release time rj deadlinedj .Let = maxi,jJ llji maximum ratio lengths two jobs.simplicity, assume job lengths normalized, i.e., lj [1, ] j J, assumeknown advance following practice work Chan et al. (2004) Ting(2008).study direct revelation mechanisms, agent participates simplydeclaring type job j = (rj , dj , lj , vj ) time rj . use denote profilereported types agents. Given declared types agents, mechanismused schedule/allocate jobs determine payment agent.consider reasonable mechanisms (1) schedule job reporteddeadline (2) schedule job processed reported length.Given certain mechanism job sequence , use qj (, t) denote whetherjob j completed time (if completed, qj (, t) = 1; otherwise qj (, t) = 0).1. also referred weighted throughput scheduling literature.2. Note release time also referred arrival time online auction literature (Parkes, 2007).earliest time agent full knowledge job. Thus earliest time jobavailable scheduling process.430fiEfficient Mechanism Design Online Schedulingvalue agent j extracts mechanism represented qj (, dj )vj ,Psocial welfare mechanism represented W (M, ) = j qj (, dj )vj .Let pj () denote amount money mechanism charges agent j. assume agents quasi-linear preferences (Nisan, 2007), i.e., utility agent juj (, j ) = qj (, dj )vj pj ().Since agents self-interested, may misreport types strategic way.easy see misreport shorter length dominated strategy; otherwise,job cannot completed even scheduled mechanism (since lj < lj ). Therefore,agents underreport lengths jobs. Similar work Porter(2004), assume system return completed job agent j dj .3way, restrict agents report dj dj . addition, assume agentknowledge job release time, also rj rj .Considering potential misreport agents, concerned incentivecompatible individually rational mechanisms. mechanism incentive compatible(IC) if, agent j, regardless behaviors agents, truthful reportingtype maximizes utility. mechanism individually rational (IR)job j, truthful reporting leads non-negative utility. addition, would also likemechanism (approximately) maximize social welfare. say mechanism (strictly)c-competitive exist job sequence c W (M, ) < W (opt, ),opt denotes optimal offline mechanism4 . Sometimes also saycompetitive ratio c.1.2 Related Workonline scheduling problem studied non-strategic setting (Lipton &Tomkins, 1994; Borodin & El-Yaniv, 1998; Bar-Noy, Guha, Naor, & Schieber, 2001; Zhenget al., 2006; Kolen, Lenstra, Papadimitriou, & Spieksma, 2007; Ting, 2008; Nguyen, 2011)(whose focus algorithm design) strategic setting (Nisan & Ronen, 2001; Lavi &Nisan, 2004; Friedman & Parkes, 2003; Porter, 2004; Hajiaghayi, Kleinberg, Mahdian, &Parkes, 2005; Parkes, 2007) (whose focus mechanism design).Non-strategic setting. case = 1, lower bound 4 competitiveratio deterministic algorithm given Woeginger (1994). 4.56-competitivedeterministic algorithm constructed Zheng et al. (2006) restart model,4.24-competitive deterministic algorithm designed Kim (2011) restartresume models. 2-competitive randomized algorithm introduced restart modelwork Fung et al. (2014), lower bound 1.693 provided work EpsteinLevin (2010). restricting release time deadlines integers, randomizedealgorithm competitive ratio e11.582 proposed Chin et al. (2006),deterministic algorithm competitive ratio 2 2 1 1.828 proposed Englert et3. Actually, viewed decision mechanism designer rather assumption.decision crucial ensure incentive compatibility see later.4. Since care social welfare performance opt competitive analysis,depends schedule, regardless payments, also call opt optimal offline allocation,simply optimal allocation.431fiChen, Hu, Liu, Ma, Qin, Tang, Wang & Zhengal. (2012). best lower bounds currently 1.25 randomized algorithms (Chin &Fung, 2003) 1.618 deterministic algorithms (Hajek, 2001).general values , lower bound competitive ratio deterministicalgorithm derived work Chan et al. (2004). lower bound improved65/6 )2 ln 1 Ting Fung (2008), algorithm competitive ratio log + O(given restart model. scheduling problem discrete time consideredwork Durr, Jez Nguyen (2012). particular, lower bound improvedmodel.ln o(1), (3 + o(1)) ln -competitive algorithm designed resume qrandomized algorithm competitive ratio O(log()) lower bound ( logloglog )provided Canetti Irani (1998).Assuming maximum ratio value densities (value divided length)two jobs bounded known number , (1 + )2 -competitive algorithmgiven Koren Shasha (1995). bound (1 + )2 optimal matching lowerbound given Baruah et al. (1992).also rich literature concerned non-preemptive scheduling (Lipton &Tomkins, 1994; Goldman, Parwatikar, & Suri, 2000; Goldwasser, 2003; Ding & Zhang,2006; Ding, Ebenlendr, Sgall, & Zhang, 2007; Ebenlendr & Sgall, 2009). However,easily verified algorithm bounded competitive ratio cannot designedsetting unrestricted values arbitrary release time. Therefore, commonassumption added non-preemptive scheduling problem proportional values, i.e.,value job proportional length. work Goldman et al. (2000),tight upper lower bound 2 given deterministic competitivenessjobs equal length (thus, equal value), 6(blog2 c + 1)-competitive randomizedalgorithm provided general value , matching (log ) lower bound (Lipton &Tomkins, 1994) within constant factor.Strategic setting. work Lavi Nisan (2015), assuming integer timepoints, scheduling problem = 1 case studied. authors showincentive compatible mechanism obtain constant competitive ratio,payment must made job completed. Hence, propose familysemi-myopic algorithms competitive ratio 3, assumption semi-myopicstrategies. work Hajiaghayi et al. (2005), specific scheduling problem= 1 considered restart model. deterministic IC mechanism competitiveratio 5 designed, lower bound 2 given deterministic IC mechanism.However, knowledge, case > 1 either restart model resume modelstudied perspective mechanism design (considering incentiveissues). work fills gap.Assuming maximum ratio value densities (value divided length)two jobs bounded known number , IC mechanism competitiveratio (1 + )2 + 1 designed Porter (2004), proved (1 + )2 + 1lower bound competitive ratio deterministic mechanism.Recently, online scheduling mechanisms investigated cloud computing (Zaman & Grosu, 2012; Azar, Ben-Aroya, Devanur, & Jain, 2013; Zhang, Li, Jiang, Liu,Vasilakos, & Liu, 2013; Lucier, Menache, Naor, & Yaniv, 2013; Mashayekhy, Nejad, Grosu,& Vasilakos, 2014; Wu, Gu, Li, Tao, Chen, & Ma, 2014). works, mechanisms432fiEfficient Mechanism Design Online Schedulingdesigned allocate computational resources users, users use virtual machines entire period requested. model, jobs non-preemptive,differs setting.1.3 Resultsmain results summarized follows.First, order characterize boundary research, derive two lower boundscompetitive ratio online deterministic IC mechanism. One bound 5,holds situation jobs equal length (i.e., = 1). boundimproves previous lower bound 2 (Hajiaghayi et al., 2005). boundln + 1 o(1), characterizes asymptotical property competitive ratiovariance job lengths, i.e., , sufficiently large.Second, design simple mechanism 1 prove restart resumemodels 1 IC, also achieves good social welfare.restart model, 1 competitive ratio + 2 + (1 + 1 ) small (in1particular, ratio 5 = 1), ( (1)2 + o(1)) ln large ( 16enough), 0 < < 1 small constant.resume model, 1 competitive ratio ( + 1)(1 + 1 ) + 12small (in particular, ratio 5 = 1), ( (1)2 + o(1)) ln large( 16 enough), slightly worse restart model (withinfactor 2).also worth mentioning that:Comparing lower bounds, see that, restart resumemodels, 1 optimal equal-length jobs ( = 1), near optimal (withinconstant factor) unequal-length jobs.comparison best-known algorithms without considering incentive compat566ibility, asymptotically speaking, 1 improves best-known ratio log+O( ) (Ting,12008) restart model ( (1)2 + o(1)) ln ; improves best-known ratio2(3+o(1)) ln (Durr, Jez, & Nguyen, 2012) resume model ( (1)2 +o(1)) ln .Thus even one care strategic aspect, 1 would stillnice algorithm use.Note designing mechanisms online scheduling problems generally difficultsince combines challenges mechanism design (i.e., ensuring incentive compatibility)challenges online algorithms (i.e., dealing uncertainty future inputs).would like highlight main techniques used work tackle challenges.(1) allocation rule mechanism 1 uses carefully selected function trade-offthree key elements: value, length, degree completion. trade-off functiondelicate sense ensures efficiency monotonicitycrucial incentive compatibility.433fiChen, Hu, Liu, Ma, Qin, Tang, Wang & Zheng(2) order obtain good competitive ratios resume model, design two nontrivial virtual charging schemes bound performance proposed mechanism:integral charging scheme segmental charging scheme.focus single machine model paper, work extends multipleidentical machines. One way extension similar work Lucier et al. (2013),assumed h machines allocated job giventime, parameter h stands common parallelism bound system.details extension found Appendix E. Another way extend resultsmultiple identical machines assume job j needs fixed number machinesprocessed. Please refer working paper (Ma, Zheng, Qin, Tang, & Liu,2014) details.52. Lower Boundssection, present two lower bounds competitive ratio deterministicIC mechanism, hold restart resume models.competitive analysis interpreted game designer onlinemechanism adversary. Given mechanism 1 , adversary selects sequencejobs maximizes competitive ratio, ratio social welfare obtainedoffline optimal algorithm social welfare obtained 1 . Therefore, keyproving lower bounds construct subtle adversary behaviors.first introduce two notions, dominant job shadow job.Definition 2.1 (Dominant Job). deterministic IC mechanism competitiveratio c, job called dominant job release time ri , vi largerc times total value jobs whose release time later ri .easy see that, order obtain reasonable competitive ratio, dominant jobtight deadline, mechanism must schedule release time ri . Otherwise,consider case jobs released ri . case, mechanismcannot obtain competitive ratio c gives dominant job i.Definition 2.2 (Shadow Job). Suppose job tight deadline, i.e., di = ri + li ,job i0 called shadow job i, i0 parameters (ri , li , vi ) i, exceptlater deadline (d0i > di ).Clearly, shadow job i0 flexible completed later. shadowjobs, show following lemma holds IC mechanism non-trivialcompetitive ratio.Lemma 2.3 (Shadow Job Argument). deterministic IC mechanismnon-trivial competitive ratio c, completes job (with tight deadline di )scenario I, scenario 0 , substitutes shadow job i0 job i, mustalso complete job i0 time di .5. working paper, consider restart model, ignore misreport release timedeadline.434fiEfficient Mechanism Design Online SchedulingProof. Suppose completed job i0 di scenario 0 , could consider subsidiary scenario 00 , includes jobs scenario 0 adds several dominant jobs.Remember call job dominant value sufficiently large (see Definition 2.1).dominant jobs released one one di , di + 1, . . . , di + bd0i di c respectively,denoted 0, 1, . . . , bd0i di c accordingly, di deadline job d0ideadline shadow job i0 . Whats more, dominant jobs unit lengthtight deadline. claim that, achieve desired (non-trivial) competitive ratio,must complete dominant jobs, thus time interval [di , d0i ) occupied. (Thereason follows: schedule dominant job j {0, 1, . . . , bd0i di c},consider scenario 000 , includes jobs release time later di + j00 . Since scenario 000 indistinguishable 00 time di + j, knowschedule dominant job j scenario 000 , hence cannot obtain competitive ratio c.)subsidiary scenario 00 indistinguishable scenario 0 time di ,job i0 completed di . Furthermore, existence dominant jobs,job i0 completed finally. However, job i0 falsely declares typejob i, i.e., misreports deadline di , would completed timedi better off, contradicting incentive compatibility6 .following, derive lower bounds leveraging Lemma 2.3. First, followingtheorem specifies lower bound jobs equal length (i.e., = 1). Noteresult concerns strategic setting, Woeginger (1994) shows competitiveratio deterministic algorithm non-strategic setting least 4.Theorem 2.4. = 1, deterministic IC mechanism obtain competitive ratioless 5.prove theorem, addition using adversary argument similarwork Woeginger (1994), need perturb job sequence leverageshadow job argument.Intuitively, construct special job set, tight-deadline jobs released oneone, two jobs collide (that is, deadline one job laterrelease time other, mechanism, impossible two jobscompleted). values jobs carefully selected later releasedjob valuable earlier one (predecessor), value differencetwo neighboring jobs constrained small-enough additive constant. Furthermore,job set, values first last jobs set obey specific amplification.Along execution mechanism, adversary would release series jobsets. mechanism completes one job, adversary stops releasing job.subtleness lies choosing time release job sets: mechanism almostcompletes job job set, adversary may release new job set whose jobscollide job collide predecessor job a. way,mechanism would abandon current job complete it,optimal allocation completes: (1) several jobs previous job sets, (2)6. scenario contradicts monotonicity condition (see strict definition start Section 3.2);Theorem 1.15 work Parkes (2007) shows monotonicity necessary incentivecompatibility.435fiChen, Hu, Liu, Ma, Qin, Tang, Wang & Zhengvaluable job (i.e., last job) newly released job set, (3) job a.7 However,mechanism complete job a. discrepancy leads lower boundcompetitive ratio. detailed proof found below.SETr1vq = wv2v1 = vFigure 1: Structure SET (v, w, t, )Proof. Suppose contradiction exists deterministic IC mechanismachieves competitive ratio 5 0 < < 1. adopt notation SETintroduced Woeginger (1994). Define SET (v, w, t, ) (for w v > 0, > 0 > 0)set jobs {1, 2, . . . , q} satisfying following properties:(1) v1 = v, vq = w, vj < vj+1 vj + 1 j q 1. Hence, q integerless wve. call magnifying parameter SET .(2) lj = dj rj = 1, j, i.e., jobs unit-length tight deadlines.(3) 0 r1 < < rq < < d1 < < dq , thus, two jobs collide other.call split point SET .define release time SET release time first job. Figure 1 showsvisual structure SET (v, w, t, ). adversary behavior follows.Adversary Behavior: adversary release SET one another depending. First, SET0 = SET (1, , 1/2, ) released time 0, = 4 /2 < /4.definition SET , know first job SET0 value 1, last jobSET0 value , value difference two neighboring jobs upperbounded .Next, specify: (1) adversary release new SETi (i 1), (2)adversary sets parameters SETi (i 1). (1), specify Algorithm 1.notations used Algorithm 1 detailed Table 1.7. proof, construct new scenario, job perturbed later deadline, thuscompleted later. make use shadow job argument analysis, makes lowerbound increased 1, compared previous lower bound non-strategic setting.436fiEfficient Mechanism Design Online SchedulingSETijob ijrij , dij vijwitijobjobTable 1: Summary notation proof Theorem 2.4i-th released SET , full, SET (vi1 , wi , ti , )j-th job SETi .release time, deadline value job ij.value last job SETisplit point SETimagnifying parameter SETitrigger job SETi1preceding job SETi1Algorithm 1: Adversary Behavior1: Initial: Release SET0 time 0.2: completed job,3:almost completes j-th job (j 2) SETi (Precisely, executingjob ij di(j1) rij period time since rij ).4:Release SETi+1 time di(j1) .5:else6:release job.7:end8: endworth mentioning that: (i) SETi+1 triggered non-first job SETialmost completed, call job trigger job. (ii) SET releasedjob completed .Suppose trigger jobs SET0 , . . . , SETi1 named 1 , . . . , successively. Accordingly, denote job release time earlier trigger job 1, . . . , i,call preceding jobs. Line 4 Algorithm 1, know new SETireleased deadline i. Note trigger job preceding joblocated SETi1 .specify parameters SETi = SET (vi1 , wi , ti , ), 1. RememberSET0 defined SET (1, , 1/2, ), = 4 /2 < /4.adversary sets vi1 equal value trigger job SETi1 ,vi = vi1 . Note vi1 value first job SETi .Pi1adversary sets = /2i , wi = max{( 1)vi1 j=1vj1 , vi1 } 2,w1 = ( 1)v11 .adversary sets ti = (di + di )/2, di di deadlines trigger jobpreceding job i. Note setting ti = (di + di )/2, jobs SETireleased di di . Hence, new jobs collide trigger jobnone collides job i.Figure 2 illustrates adversary releases new SET example.example, almost completes j-th job (j 2) SETi . SETi+1 releaseddeadline job i(j 1), value first job SETi+1 equal vij .437fiChen, Hu, Liu, Ma, Qin, Tang, Wang & Zhengri1SETivi1SETi+1vijvi(j1)v(i+1)1 = vijti+1Figure 2: example SETi+1 SETiAccording Algorithm 1, always gives trigger jobs switches schedulejob newly released SET , adversary release new SET one another.One may wonder whether adversary release new SET infinitely. words,subscript SETi tend infinity?answer no, seen definition wi . Since 2 < < 4,Lemma 4.3 work Woeginger (1994), finite numberP(denote k) steps, vk1must less corresponding sum term ( 1)vk1 k1j=1 vj1 , wk = vk1must hold. Remember vk1 wk denote value first job last job SETkrespectively, thus exists one job SETk . According Algorithm 1, matterwhether completes job not, adversary release job. Therefore,SETk ultimate SET job k1 ultimate job.far, clarified adversarys behaviors. Next, show derivelower bound based adversary.According Algorithm 1 structure SET , know adversary allowscomplete one job. Actually, completed job be: (1) first job SET0(i.e., job 01); (2) trigger job , 1 k; first job SETi , 1 < k (i.e., jobi1); (3) ultimate job k1. Let us analyze one one.(1) completes job 01, consider scenario job 01 substitutedshadow job 010 , whose deadline late enough (i.e., even started executeddeadline last job SET0 , still completed time). AccordingLemma 2.3, mechanism must complete job 010 time 1, thus abandonlast job (with value w0 = ) SET0 . Therefore, obtains social welfarev01 = 1. However, optimal allocation (which first completes last job SET0job 010 ) obtains social welfare + 1. contradicts factcompetitive ratio 5 , since + 1 = (4 /2) + 1 > 5 .(2) completes trigger job job i1, 1 k, without loss generality,denote job job ij, know vij = vi = vi1 . completes job ij,1 k, similarly, consider scenario job ij substitutedshadow job (ij)0 , whose deadline late enough. Lemma 2.3, must completejob (ij)0 time dij , obtaining social welfare vij = vi = vi1 . However, socialwelfare optimal allocationP(which completes jobsP1, . . . , i, last job SETi ,job (ij)0 ) least ij=1 vj + wi + vij > ij=1 (vj1 j1 ) + wi + vij >438fiEfficient Mechanism Design Online SchedulingP2 + ( 1)vi1 i1j=1 vj1 + vij > ( + 1)vi1 /2 > (5 )vi1 .contradicts fact competitive ratio 5 .Pij=1 vj1(3) completes ultimate job k1, consider scenario adversaryreleases two copies job k1 SETk . Clearly, scenario, choose onecopy complete. denote completed copy job (k1)1 job(k1)2 . consider scenario job (k1)1 substituted shadow job(k1)0 , whose deadline unit-time later job k1. According Lemma 2.3,must complete job (k1)0 dk1 obtains social welfare vk1 . However,0optimal allocation (which completesPk jobs 1, . . . , k, job (k1)Pk 2 , job (k1) )obtain social welfare least j=1 vj + vk1 + vk1 > j=1 (vj1 j1 ) + wk + vk1 >PkPk1j=1 vj1 + vk1 > ( + 1)vk1 /2 > (5 )vk1 . Rememberj=1 vj 2 + ( 1)vk1PSETk , vk1 = wk = ( 1)vk1 k1j=1 vj1 . contradicts factcompetitive ratio 5 .Second, understand asymptotic property lower bound large,construct scenarios inspired example Durr et al. (2012) obtain followingtheorem.Theorem 2.5. sufficiently large, deterministic IC mechanism obtaincompetitive ratio less ln + 1 o(1). particular, deterministic IC mechanismobtain competitive ratio less ln + 0.94 16.Proof. convenience analysis, denote =Let us consider following adversary behaviors.ln ,r = de 1, assume 16.Adversary Behavior: time 0, long job B type B = (0, , , ) released,well two short jobs a1 a1 type (0, 1, 1, 1). Moreover, integermoment 0 1, mechanism schedules job B [0, t), two short jobsat+1 at+1 unit length released t, tight deadline + 1, new jobreleased otherwise. values jobs satisfy:(1< ,v(at ) = v(at ) =(1)1e.Note job job type, cases analyzed at0naturally applied at0 .According adversary behavior, know adversary allows completeone job. Actually, completed job be: (1) job at0 t0 < ; (2) job at0t0 ; (3) job B. analyze three cases follows.(1) mechanism schedules job at0 t0 < , consider scenarioincludes jobs B, a1 , a1 , . . . , at0 1 , at0 1 , at0 job a0t0 . Here, job a0t0 type (t01, + 1, 1, 1), shadow job at0 . According Lemma 2.3, mechanism mustcomplete job a0t0 t0 obtains social welfare 1. However, scenario,optimal mechanism complete job B first, schedule a0t0 timecomplete it, optimal social welfare + 1. ratio + 1.439fiChen, Hu, Liu, Ma, Qin, Tang, Wang & Zheng(2) mechanism schedules job at0 t0 , consider scenarioincludes jobs B, a1 , a1 , . . . , at0 1 , at0 1 , at0 , job a0t0 . Here, job a0t0 type (t01, t0 + 1, 1, et0 /1 ), shadow job at0 . According Lemma 2.3, mechanismschedule job a0t0 time t0 complete a0t0 . Thus, mechanismobtains social welfare v(a0t0 ). However, one optimal mechanismsschedule complete jobs = 1, . . . , t0 , schedule a0t0 time t0complete it, resulting following optimal social welfaret0Xde 1 +e1+et01=r+t0Xe1+et01Zt0rt0e 1 + e 1rt=r+1t=det0r+t0=r e 1 + ( + 1)e 1 = f (, r) + ( + 1)e 1 = f (, r) + ( + 1)v(a0t0 ).rHere, introduced function f defined f (, r) r e 1 . Considering= ln r = 1, r (0, 1]. ex 1 + x sides converge1 x approaches 0,rf (, r) = r e 1 rr= 0,(2)f (, r) approaches 0 grows. ratio + 1 o(1).(3) mechanism schedules completes job B, obtaining social welfare ,consider scenario includes jobs a1 , a1 , . . . , , job B 0 . Here, jobB 0 type B 0 = (0, 2, , ), shadow job B. Similarly, claimIC mechanism schedule job B 0 time 0 complete time . Thus,mechanism obtains social welfare v(B 0 ). However, one optimalmechanisms schedule complete small jobs = 0 1,schedule complete job B 0 . leads social welfare leastZXX11de 1 +e + = r +e 1 +e + r +rt=r+1t=der111+ e+ = f (, r) + e+ = f (, r) + eln 1 +lnln=f (, r) + + = f (, r) + 2+ = f (, r) + (+ 1)v(B 0 ).eee=r e(3)16, e ln . equation larger f (, r) +( + 1)v(B 0 ). Therefore ratio + 1 o(1).Combining three cases together, prove nonexistence ( ln + 1 o(1))competitive mechanisms. Since f (, r) 0.06 16, competitive ratioleast ln + 0.94 16.3. Mechanism Designsection, describe simple mechanism 1 (whose allocation payment rulesgiven Algorithm 2), works surprisingly well restart resume440fiEfficient Mechanism Design Online Schedulingmodels, handles settings different values unified framework.contrast, previous works (Durr et al., 2012) need design separate differentalgorithms deal different values .3.1 Mechanism 1introducing mechanism, first introduce concept valid active timeuncompleted job j, time t, denoted(min{s|x(t0 ) = j, t0 [s, t)}, restart modelej (t) := Rresume model0 (x(s) = j)ds,(4)x(t) mechanisms allocation function, maps time point available job, 0 machine idle.8 () indicator function returns 1argument true, zero otherwise. Note ej () also take vectorargument. example, ej (, t) shorthand ej (t) job sequence .seen restart model, time t, job j received allocationtime t0 < preempted that, ej (t) = t0 . resumemodel, ej (t) accumulated processing time job j time t.say job j feasible time (1) reported release time t; (2)completed yet; (3) enough time completed reporteddeadline, i.e., dj lj ej (t). use JF (t) denote set feasible jobs timet.According Algorithm 2, time t, 1 assigns priority score, vj lj ej (,t) ,feasible job j JF (t), always processes feasible job highestpriority (ties broken favor job smaller rj ). located (0, 1)determined later competitive analysis. payment rule 1essentially critical-value payment (Parkes, 2007), similar secondprice auction. Hence, payment equal minimum bid agents makeremain allocated.9 following pseudocode, j denotes reported types jobsj.8. Equation 4, since s=t valid candidate minimization, exist s, s.t.,x(t0 ) = j, t0 [s, t) restart model, ej (t) = 0.9. Note use critical-value payment, payment completed job j dependsjobs types rj dj . mechanism allows returning completed job reporteddeadline, calculation critical-value payment face trouble: possible agent j misreportsmuch later deadline obtain cheaper payment, job completed returnedtrue deadline. reason restrict mechanism return completed job reporteddeadline. worth mentioning that, payment must made job completed, (Lavi& Nisan, 2015) shown incentive compatible mechanism obtain constantcompetitive ratio.441fiChen, Hu, Liu, Ma, Qin, Tang, Wang & ZhengAlgorithm 2:Allocation RuletimeJF (t) 6=x(t) arg maxjJF (t) (vj lj ej (,t) )else x(t) 0endPayment Rulejob jqj (, dj ) = 1pj () = min(v 0 |qj (((rj , dj , lj , v 0 ), j ), dj ) = 1)jjelse pj () = 0endintuition mechanism two-fold. First, ensure efficiency, one must tradevalue length: job larger value higher priority, job largerremaining length lower priority. 1 uses simple priority function achievetradeoff: seen, priority score vj lj ej (,t) job positively correlatedvalue negatively correlated remaining length. Second, ensure IC,1 uses critical-value payment rule monotone10 allocation rule.Note allocation rule payment rule implemented efficiently.allocation rule, enough consider time point new jobs arriveexisting jobs completed. And, give algorithms Appendix showpayment agent computed polynomial time.Clearly, critical-value payment rule, 1 individually rational.following subsection, prove incentive compatibility.3.2 Incentive Compatibilitycall allocation rule mechanism monotone, job truthfully reported typej = (rj , dj , lj , vj ) cannot completed mechanism, dominated11 declarationtype j = (rj , dj , lj , vj ) cannot make completed either.According Theorem 1.13 work Parkes (2007), order establishtruthfulness mechanism, enough prove monotonicity allocation rule.Theorem 3.1. Mechanism 1 incentive compatible, restart model resumemodel.Proof. prove monotonicity allocation rule 1 . Assume job jcompleted 1 j truthfully declared (we denote case rue).show j cannot completed either declaring j = (rj , dj , lj , vj ), rj rj ,lj lj , dj dj vj vj . denote case F alse.Suppose job j ever executed k > 0 times rue case, definefollowing points execution job j: let tsi tpi ith time job j starts10. strict definition monotonicity start Section 3.2.11. say type j dominated type j (denoted j j ) rj rj , dj dj ,lj lj vj vj .442fiEfficient Mechanism Design Online Schedulingexecution preempted respectively, = 1, 2, . . . , k, let ta = arg inf (ej (t) +dj < lj ) time job j abandoned. job j never started, setts1 = tp1 = ta .also refer P = [rj , ts1 ) [tp1 , ts2 ) . . . [tpk , ta ] = P0 P1 . . . Pk pending periodjob j, = [ts1 , tp1 ) [ts2 , tp2 ) . . . [tsk , tpk ) = A1 A2 . . . Ak executing periodjob j.first consider monotonicity regard rj , regardless variables. Clearly,definition ta , declaring rj > ta could cause job completed. Thus,restrict attention rj [rj , ta ] = P A.necessary condition job j completed (in F alse) job jexecuted sometime period P . However, according Lemma 3.2 (see below), job jcannot executed P . Therefore, declaring rj rj cannot cause job completed.Intuitively, Lemma 3.2 says that, case rue F alse, set jobsscheduled period P must same. Thus, job j cannot executed period P .consider dj , lj vj . proof essentially proof rj :declaring dj dj , lj lj vj vj improve job js priority, result,cannot change execution jobs pending period P . declaring dj dj ,lj lj vj vj cannot cause job completed. proves allocationrule 1 monotone.following, formally introduce Lemma 3.2, used theorem.lemma introduce additional notation: case rue F alse, denoteJ J respectively set jobs ever executed P , denoterespectively set jobs ever pending A.Lemma 3.2. (1) J = , (2) J = , (3) J = J.Proof. Consider job I, according defintion I, case rue, joblower priority job j period P .Relation (1) means that, case rue, job cannot executed period P .obvious, since job j (with higher priority i) pending period P .Relation (2) means that, case F alse, job cannot executed period P either.prove contradiction. Suppose job executed time point P .denote ti = min{t P |x(t) = i}, assume ti Pn 0 n k. observationpending period Pn , 0 n k.h(n)Observation 3.3. pending period Pn , 1 schedules jobs sequence12 jn1 . . . jn(h(n) 1, number active jobs Pn ) case rue, know (1)h(n)release time job jn2 . . . jn period Pn ; particular, release time jobh(n)jn1 Pn (n 1) exactly time tpn (2) job jn1 . . . jn either completed abandonedPn ; idle time Pn .Here, use fj (t) denote priority job j time t. Suppose that, caseh(n)rue, job jni (one jn1 . . . jn ) executed ti , priority fjni (ti ).12. job may appear sequence preempted resumed/restarted later.443fiChen, Hu, Liu, Ma, Qin, Tang, Wang & Zhengcase F alse, since executed ti , according Observation 3.3, deducepriority job time ti , i.e., fi (ti ) must larger fjni (ti ).Therefore, deduce must executed sometime period Ui =(A1 . . .An ). Otherwise, also executed time ti case rue, contradictingfact I. Similarly, denote si = min{t Ui |x(t) = i}, assume si1 n.claim, case F alse, priority job time si , i.e., fi (si ) satisfiesinequality below.(pfjni (ti ) |An |+|An1 |++|Am+1 |+|tm si | ,fi (si ) >pfjni (ti ) |tm si | ,n 1;= n.Otherwise, priority job time ti fjni (ti ) (consider caseperiods [si , tpm ), Am+1 , . . . , An1 , allocated i).According definition si , know si first time executed periodA. Therefore, priority job si remains shifting case ruecase F alse. However, case rue, job j executed time si (hence,priority larger fi (si )), periods [si , tpm ), Am+1 , . . . , An1 , allocatedj. Therefore, time ti , job j priority larger fjni (ti ), contradicting factjni executed time ti .Relation (3) means that, matter case rue case F alse, jobs executedperiod P same. Relation (3) derived naturally Relation (2).4. Competitive Analysissection, show mechanism 1 performs quite well terms social welfarecomparison optimal offline allocation, full knowledge futurejobs beginning execution.perform competitive analysis, need design virtual charging schemes.certain virtual charging scheme, every job j completed optimal allocation opt,charge value (or partial value) job f completed 1 . virtual chargingscheme satisfies property every job f completed 1 receives total chargecvf , succeed showing 1 competitive ratio c.Designing ingenious virtual charging scheme crucial competitive analysis.following, design different virtual charging schemes obtain competitiveratio 1 restart model resume model respectively.use parameter priority function mechanism 1 , first derivecompetitive ratios functions . specify later (in Section 4.3) choosesuitable (with respect ) optimize performance 1 , derive competitiveratios terms .Here, introduce notation used Section 4.1 Section 4.2.Denote (1, 2, . . . , F ) sequence jobs completed 1 time. job fsequence, let tf time job f completed, convenience denote t0 = 0.Divide time F + 1 intervals = [tf 1 , tf ), f = 1, 2, . . . , F , [tF , +).444fiEfficient Mechanism Design Online Scheduling4.1 Analysis Restart Modelstudy restart model first. assume, without loss generality, optimalallocation opt interrupt allocation, since interrupted jobs non-resumable.following theorem.Theorem 4.1. restart model, 1 competitive ratio11+1+ 1.Proof. introduce virtual charging scheme follows. completed job jopt, also completed mechanism 1 , value charged itself.Otherwise (i.e., job j completed 1 ), consider time sj j beginsexecution opt. Note opt interrupt allocation, j exactly allocatedtime period [sj , sj + lj ). sj must time interval (recall = [tf 1 , tf )),charge value j f . Define j := tf sj time amount sjtf . job j feasible time sj , according Lemma 4.2, know priorityjobs j time sj vf tf sj = vf j ; meanwhile, priority j timesj vj lj . vj lj vf j , i,e., vj vf j lj . defer formal statementproof Lemma 4.2 end subsection.calculate maximum total value charged completed job f 1 .time interval , denote (1, 2, . . . , m), sequence jobs opt whose starting time sjbelongs ordered s1 > s2 > > sm . Remember define j := tf sjtime amount sj tf . clear 0 < 1 < 2 < <j lj j1 2 j m, since j allocated completed time interval[sj , sj1 ]. Furthermore, job lengths normalized, i.e., 1 lj , deducethat:(0j = 1j(5)j 1 j 2.Recall < 1 Pf may also completed opt. Therefore total chargejob f vf +j=1 vj , upper boundedvf + vfXj=1j ljvf (1 +l1+Xj1) vf (1 +l1+j=1j=21+shows mechanism 1 ( 1m1X1j) vf (1 ++Xj=0+ 1)-competitive.1Actually, competitive ratio obtained way tight, i.e., ratio 1+ 1 + 1best possible 1 . give example Appendix B show tightness.Lemma 4.2. time point sj , job j (6= f ) feasible time sj ,priority j sj vf tf sj . Moreover, value j, vj , vf tf sj lj .Proof. Note that, sj time interval , according definition , knowf unique job completed 1 . prove lemmaenumerating possible cases.(1) executing job sj job f , know priority job f timesj exactly vf tf sj (because priority job f time tf vf ). Clearly, priorityj sj larger job f , thus larger vf tf sj .445j ).fiChen, Hu, Liu, Ma, Qin, Tang, Wang & Zheng(2) executing job sj job f , assume 1 executes job j1 , . . . , jkf successively13 time period [sj , tf ), k 1. Since f unique jobcompleted , deduce that: j1 preempted j2 , j2 preempted j3 ,...,jkpreempted f , finally f completed time tf . Denote 1 , . . . , k time pointsj1 , . . . , jk preempted respectively. also denote fj (t) priority jobj time t. use backward induction: First, know priority job jkk larger job f , i.e., fjk (k ) vf tf k . Then, since jk1 preemptedjk k1 , know priority jk1 k1 larger jk .Hence, fjk1 (k1 ) fjk (k1 ) = fjk (k ) k k1 vf tf k1 . eventually,get fj1 (1 ) vf tf t1 . Since j1 executed time sj , deducefj1 (sj ) vf tf sj . Clearly, priority j time sj (i.e., vj lj ) largerj1 , thus larger vf tf sj .arranging vj lj ej (sj ) vf tf sj , get vj vf tf sj lj +ej (sj ) vf tf sj lj ,ej (sj ) 0 valid active time job j time sj .remarks Lemma 4.2: (1) f unique job completed 1time interval , priorities executing jobs monotonically increase . (2)Lemma 4.2 applies restart model resume model. (3) Lemma 4.2 providesuseful tool relate priority feasible job (j) time point (sj )completed job f .4.2 Analysis Resume ModelCompared restart model, competitive analysis resume model muchcomplicated, resume model, job executed several disjointedtime intervals. charging scheme used previous subsection longer works,need design new virtual charging scheme.introducing new virtual charging scheme, introduce notationused subsection. Let (j) denote number disjoint time segments(j)allocated completed job j opt, s1j , s2j , . . . , sj denote corresponding startingtime segment.say allocation contains violation exist two completed jobs j,two segments starting time sai , sci sbj , sdj sai < sbj < sci < sdj .allocation called standard contain violation. means allocationstandard, completed job, starting time execution two segmentsanother jobs allocation, completion time also time interval (i.e.,two segments). provide obvious yet useful fact offlineoptimal allocation below.Claim 4.3. exists optimal allocation standard.detailed proof, please refer Appendix C. Without loss generality, assumeoptimal allocation opt standard.Claim 4.4 presents important property standard allocation, usedfollowing proofs.13. Here, j1 job j, affect analysis.446fiEfficient Mechanism Design Online SchedulingClaim 4.4. execution opt, job js execution-starting time twosegments another jobs allocation, job js completion time also timeinterval (i.e., two segments).analyze competitive ratio 1 resume model, propose two new virtualcharging schemes (referred integral charging scheme segmental charging scheme,respectively). integral charging scheme, charge whole value job joptimal allocation opt job completed mechanism 1 ; segmentalcharging scheme, charge value j segment, different segmentsjob may charged different jobs completed mechanism 1 . using twoschemes, Theorem 4.5 upper bound competitive ratio mechanism 1 1+112+ ln +1 respectively. discussed Section 4.3, two ratios work situationsdifferent values, i.e., first one works well small second one workswell large .Theorem 4.5. resume model, competitive ratio 11+ 1.particular, satisfies , competitive ratio 1 min{ 1 + 1,2ln + 1}.1+proof theorem given Section 4.2.1 Section 4.2.2.4.2.1 Integral Charging SchemeRemember denote (1, 2, . . . , F ) sequence jobs completed 1 time.job f sequence, denote tf time job f completed.integral charging scheme, restrict total number jobs (excluding f itself)charged job f : allow number exceed btf tf 1 c+1. particular,introduce notation saturation Definition 4.6.Definition 4.6 (Saturated). job f , number jobs (excluding f )charged f less btf tf 1 c + 1, say f unsaturated; otherwise fsaturated.Let W denote set jobs completed opt, Wf W denote set jobsj W s1j . Let denote set jobs W whose values alreadycharged jobs completed 1 .integral charging scheme described Scheme 1. simplicity, refer Line1 2 Step 1, Line 4 11 Step 2, Line 12 21 Step 3.give intuitive explanations Step 2 Step 3.Step 2, job f (f = 1, . . . , F ), pick btf tf 1 c + 1 jobs Wfcharge values f . rule picking jobs follows largest s1j first, k-thpicked job14 s1j later tf k + 1.14. slight abuse notations, still denote job j, thus start time first segment s1j .447fiChen, Hu, Liu, Ma, Qin, Tang, Wang & ZhengSCHEME 1: Integral Charging Scheme1: Initial: .2: job W , also completed mechanism 1 , charge value itself,add A.3: W \ 6= ,4:f = 1 F ,5:k = 0 btf tf 1 c,6:J k := {j 0 | (s1j 0 tf k) (j 0 Wf \ A)};7:J k 6= ,8:Set j = arg maxj 0 J k (s1j 0 ), add j A, charge value f .9:end10:end11:end12:f = F 1,13:Wf \ 6= ,14:Set j = arg maxj 0 Wf \A (s1j 0 ), add j A;15:16:17:18:19:20:21:22:(j)sj +hj 0 hj F f ,Charge js value unsaturated job smallest completion timeset {f + 1, . . . , f + hj };(j)else sj [tF , +),Charge js value unsaturated job smallest completion timeset {f + 1, . . . , F };endendendendStep 3, consider jobs (in W ) whose values charged job first(j)two steps. Consider job j s1j located interval sjlocated +hj (or[tF , +)). charge value unsaturated job job set {f + 1, . . . , f + hj } (or{f + 1, . . . , F }). rule selecting unsaturated job follows smallest completion timefirst.show three steps jobs W charged completed jobs1 (see Claim 4.9). First, give two observations below.Observation 4.7. integral charging scheme, job f {1, 2, . . . , F }time , number jobs charged f start time opt [t, tf )(charged step 2) btf tc + 1.Observation 4.8. integral charging scheme, job f {1, 2, . . . , F } completedmechanism 1 , total number jobs charged f (excluding f )btf tf 1 c + 1.Observation 4.7 derived Lines 5-6 Scheme 1, Observation 4.8 derivedrestriction saturated job charged more.448fiEfficient Mechanism Design Online SchedulingClaim 4.9. integral charging scheme, jobs W charged jobscompleted mechanism 1 .Proof. Suppose contrary exists Wf charged jobRt{f, f + 1, . . . , f + hi }. Here, introduce notation ei (t) = 0 (opt(s) = i)ds denotevalid active time resumable job time opt. Since length every jobleast 1,15 exists allocation segment [s0 , s00 ] job ei (s0 ) < 1, ei (s00 ) 1,opt(t) = [s0 , s00 ]. Suppose s00 belongs +h . definition hi ,h hi .According assumption, know: (a) charged f . (b) jobs{f + 1, f + 2, . . . , f + h} saturated charging process chargejob i.point (a), deduce Step 2, least btf s1i c + 1 jobs (whosevalues charged f ) s1j (s1i , tf ] (by Observation 4.7). Otherwise wouldcharged f Step 2. denote Ja set btf s1i c + 1 jobs.point (b), recall job f 0 (f 0 {f + 1, . . . , f + h}) saturatedbti ti1 c+ 1 jobs whose values charged f 0 (see Definition 4.6). Hence, deduceleast (btf +1 tf c + 1) + + (btf +h tf +h1 c + 1) jobs (whose valuescharged {f + 1, . . . , f + h}) starting time satisfying s1j (s1i , tf +h ].particular, among jobs, (btf +h s00 c + 1) jobs s1j (s00 , tf +h ](whose value must charged f + h).16 Therefore, deduce least(btf +1 tf c + 1) + + (btf +h tf +h1 c + 1) (btf +h s00 c + 1)jobs (whose values charged {f + 1, . . . , f + h}) s1j (s1i , s00 ] (denote Jbset jobs).Note Ja Jb = , jobs Ja charged f , jobs Jb charged{f + 1, . . . , f + h}. Therefore, deduce number jobs start time contained(s1i , s00 ] least |Ja | + |Jb |, i.e.,(btf s1i c + 1) + (btf +1 tf c + 1) + + (btf +h tf +h1 c + 1) (btf +h s00 c + 1)>(tf +h s1i ) (btf +h s00 c + 1) bs00 s1i c 1.(6)So, bs00 s1i c 1 jobs different [s1i , s00 ]. Recall assumeopt standard, hence, jobs entirely scheduled (s1i , s00 ), i.e., time segmentsjob allocated (s1i , s00 ) (Claim 4.4). Since length every job least1, reach contradiction.According integral charging scheme, charges completed job f threeorigins, corresponding three steps Scheme 1. Step 1, obviously, chargejob f vf . calculate maximum total charge Step 2.15. stated Problem Formulation section, assume job lengths located [1, ] simplicity.However, scaling, results proofs easily generalized case [lmin , lmin ],lmin shortest length jobs.16. Because: (i) Step 2, might (btf +h s00 c + 1) jobs s1j (s00 , tf +h ] couldcharged f + h; (ii) Step 3, jobs s1j +h could charged f + h.449fiChen, Hu, Liu, Ma, Qin, Tang, Wang & ZhengSuppose total number jobs charged f Step 2 m. rename1, 2, . . . , according 1 2 , claim vj vf j lj (Lemma 4.2 usedhere), j := tf s1j , 1 j m. According rule picking jobs Step 2,j j 1. clear sum values jobsvfXj=1j lj vfXj1 = vfj=1Xj .(7)j=0remains calculate maximum total charge Step 3. According Observation4.8, know number jobs charged f Step 3 btf tf 1 c+1m.need bound value job j. key build relationshipvalue value job f . However, according charging rule Step3, start time s1j job j located time interval . case, cannotuse Lemma 4.2 directly derive inequality like vj vf j lj . remainscheck whether j feasible tf 1 (note tf 1 left endpoint time interval ).define critical time job tj := dj lj . prove tj tf 1 ,job j must feasible time tf 1 1 . Thus, applying Lemma 4.2, easily getvj vf tf tf 1 lj vf tf tf 1 .(8)Fortunately, following lemma shows tj tf 1 holds.Lemma 4.10. According charging scheme, j Wf charged completed jobf + k (where 1 k hj ), critical time job j satisfies tj tf +k1 .Proof. prove lemma contradiction suppose tj < tf +k1 . total length(j)jobs whose opt allocation s1j sj(j)(sjdj s1j lj = (dj lj ) s1j = tj s1j < tf +k1 s1j .(j)+ lj) s1j lj ,(9)Since j charged f +k, Step 3 know jobs {f +1, f +2, . . . , f +k1}saturated. Thus, least(btf s1j c + 1) + (btf +1 tf c + 1) + + (btf +k1 tf +k2 c + 1) btf +k1 s1j c + 1 (10)jobs whose start time belongs interval (s1j , tf +k1 ).Recall opt standard. Hence, jobs allocated time segmentsfirst segment last segment job j (according Claim 4.4), Equation (9)Equation (10) constitute contradiction since every jobs length least 1.Combining analysis above, know that: (1) total charge f Step 1vf ; (2) assumingjobs charged f Step 2, total chargePm jjobs vf j=0according Equation (7); (3) number jobs chargedf Step 3 btf tf 1 c + 1 according Definition 4.6, value450fiEfficient Mechanism Design Online Schedulingjob vf tf tf 1 according Equation (8). Therefore, total chargefvf + vfm1Xbtf tf 1 c+1j + (btf tf 1 c + 1 m)vf tf tf 1 vf (1 +j=0Xj ),j=0upper bounded vf (1 + 1), indicating competitive ratio mechanism1 upper bounded1+ 1.4.2.2 Segmental Charging Scheme(j)Recall use s1j , s2j , . . . , sjdenote starting time time segments allocated(j)1j , 2j , . . . , j(j)job j opt. Letdenote time segments, lj1 , lj2 , . . . , lj denotelength them.segmental charging scheme, segment kj given value j ljk ,vj := ljj value density job j. describe segmental charging schemeScheme 2. simplicity, refer Line 2 3 Type-1 charge, Line 4 5 Type-2charge, Line 6 7 Type-3 charge.SCHEME 2: Segmental Charging Scheme1:2:3:4:5:6:7:8:9:segment kj optmechanism 1 also completes j deadline,Charge value j ljk j.else skj f {1, 2, , . . . , F }, j vf j 1 , j := tf skj ,Charge value j ljk f .elseCharge j ljk f , f first job completed 1 time tj on,tj critical time job j.endendclear Type-1 charge received job f vf . Next, boundType-2 Type-3 charges.vLemma 4.11. total Type-2 charge job f receives lnf .Proof. Let R2 denote set job segments whose charges f Type-2.kj R2 , charge j ljk . Line 4 Scheme 2, know j vf j 1 ,j = tf skj . Thus total Type-2 chargeXkj R2j ljkvfXkj R2j 1 ljkvfX Zkj R2jj ljkx1Zdx vf0x1 dxvf,lnsecond inequity holds < 1. Therefore, f receives total Type-2 chargevlnf .451fiChen, Hu, Liu, Ma, Qin, Tang, Wang & Zhengfollowing, study Type-3 charge denote R3 set job segmentsconstitute Type-3 charges f .First, claim that, satisfies condition, get [skj , skj + ljk ] [tf , tf +lj ] [tf , tf + ] kj R3 (Claim 4.12).Claim 4.12. satisfies function: g(x) = x x 1 x ;[skj , skj + ljk ] [tf , tf + lj ], kj R3 .Proof. prove [skj , skj + ljk ] [tf , tf + lj ], need prove inequality below:tf skj tf + lj ljk .(11)inequality skj tf + lj ljk holds (skj + ljk ) lj dj lj = tj tf .Next prove tf skj . Suppose skj 0 f 0 (If 0 later , mightequal ). according Type-3 charging rule, j =vjlj0> vf 0 j 1 ,j0 = tf skj .use condition : g(x) = x x 1 x .0vvllj j , hence ljj vj lj 1 . Combining two inequalities ( ljj > vf 0 j 10vjlj0vj lj 1 ), vj lj > vf 0 j , thus vj > vf 0 j lj , contradicts factf 0 completed tf 0 priority vf 0 (Lemma 4.2 used here). Therefore,tf skj .Claim 4.12, know allocation segments PType-3 charges frestricted interval [tf , tf + ]. Hence, derive k R3 ljk .jLemma 4.13. satisfies function: g(x) = x x 1 x ; total).Type-3 charge job f receives vf ( 1ln +Proof. According Type-3 charging rule, j completed mechanism;consider critical point j, i.e., tj (in time interval ), applying Lemma 4.2,vvdeduce vj lj vf tf tj vf . Therefore ljj flj . boundljtotal Type-3 charge f receivesXkj R3j ljk =X vjX vfX ljkkljkl=v,fljg(lj )lj lj jkkkj R3j R3Note function g(lj ) = lj lj increasing 1 lj1lj ln.g(lj )(1 lj1ln(12)j R31lnlj , >1ln .1lndecreasing(13)Claim 4.12, know [skj , skj + ljk ] [tf , tf + lj ] [tf , tf + ] kj R3 .1Therefore, one hand, kj R3 lnlj (denote set R3 ),452fiEfficient Mechanism Design Online Schedulingkkj R3a lj; hand, kj R3 ljP1set R3b ), k Rb ljk ln.3jThen, (12) becomesPXkj R3Xj ljk vfkj R3X ljkX ljkljk= vf (+)g(lj )g(lj )g(lj )kkbj R3Pvf (1lnkj R3aljkP+kj R3bj R3ljk) vf (means Type-3 charge bounded vf ( 1ln +(denote(14)1ln+),1).Based Lemmas 4.11 4.13, obtain ,17 total chargejob f completed mechanism 1 vf ( 1 + 2ln + 1). implies1competitive ratio mechanism 1 upper bounded + 2ln + 1.4.3 Discussionsadvantage mechanism handle settings different valuesunified framework. need set parameter different values Theorem4.1 Theorem 4.5 adapt different settings job lengths (as shownfollowing corollaries).Corollary 4.14. setting = 1(1)2 ln , > 0 arbitrary small constant,1mechanism 1 achieves competitive ratio ( (1)2 + o(1)) ln restart model2competitive ratio ( (1)2 + o(1))lnresume model.proof found Appendix D. Corollary 4.14, followingdiscussions:1(1) restart model, mechanism 1 achieves competitive ratio ( (1)2 +o(1)) ln ,566improves upon best-known algorithmic result log+ O( ) (Ting, 2008)standard online scheduling without strategic behavior.(2) resume model, large, mechanism 1 achieves competitive ratio2( (1)2 + o(1)) ln , slightly worse result obtained restartmodel (within factor 2). Asymptotically speaking, 1 near optimal, sincecompetitive ratio order (w.r.t. ) lower bound shownTheorem 2.5. Furthermore, analysis generalizes results obtained Durr etal. (2012) continuous value time strategic setting.(3) relatively small, ratios given Corollary 4.14 become loose.particular, approaches 1, ratios approach infinity since lnapproaches 0. case, need different setting (see Corollary 4.15).1117. Note that, function g(x) = x x increasing 1 x lndecreasing x ln. Therefore,need require , naturally derive g(x) = x x 1 x .453fiChen, Hu, Liu, Ma, Qin, Tang, Wang & ZhengCorollary 4.15. choosing = +1, competitive ratio mechanism 1 + 2 +1(1 + ) < + 2 + e restart model ( + 1)(1 + 1 ) + 1 resume model.Similarly, following discussions:(1) competitive ratio 1 linear , since (1 + 1 ) bounded e.(2) particular, = 1, ratios corollary become 5restart resume model, matches lower bound given Theorem 2.4.regard, say 1 optimal. hand, also showslower bound 5 Theorem 2.4 tight.5. Conclusion Future Workpaper, studied online scheduling problem strategic setting. summarized Table 2, proved restart model resume model,competitive ratio IC mechanism cannot less 5 = 1 cannot lessln + 1 o(1) large . designed simple IC mechanism 1 schedule jobssingle machine proved near optimal approximation guarantees (in termssocial efficiency) restart model resume model competitiveanalysis: shown Table 2, mechanism optimal terms competitive ratiorestart model resume model = 1, near optimalrestart model large enough.Table 2: Summary bounds competitive ratioRestart ModelResume ModelModel=1asymptotic=1asymptoticLB IC mech.5+1o(1)5lnln + 1 o(1)12UB proposed mech.5( (1)5( (1)2 + o(1)) ln2 + o(1)) lnproving lower bounds, introduce shadow job argument reflectsIC constraint. argument helpful extending bounds non-strategic settingstrategic setting. second contribution work design several virtualcharging schemes analyze competitive ratio mechanism. ideasvirtual charging schemes methodological significance may used addressproblems.multiple directions explore future.interesting problem whether IC competitive mechanism designedhybrid model, exist resumable non-resumable jobs. Many newstrategic issues may arise hybrid model. example, resumable job disguisenon-resumable job get better off?Another open problem whether tighter competitive analysis 1 maderesume model. conjecture competitive ratio obtained 11+ 1 + 1, restart model resume model.uniform form: 1Furthermore, given popularity cloud computing todays industry,practical importance extend work setting job scheduling multiple heterogeneous machines.454fiEfficient Mechanism Design Online SchedulingAppendix A. Algorithms Critical-Value PaymentPlease note critical time point Algorithm 3 Algorithm 4 means timepoint new jobs arrive existing jobs completed.Algorithm 3: Compute critical-value payment restart modeljob j completedRun Algorithm 1 without job j. Let set critical time points[rj , dj ).everyexists job k x(t) = k, define ft = vk lk ek (t) ;else ft = 0;end-forevery time point t0 [rj , dj lj )0Define ft0 = max{ft / : (T [t0 , t0 + lj ))};end-forLet f = mint0 ft0 .pj = f / lj .endAlgorithm 4: Compute critical-value payment resume modeljob j completedRun Algorithm 1 without job j.Let {t0 , t1 , . . . , tm } set (denoted ) critical time points [rj , dj ),t0 = rj .Denote period two critical time point zi = ti ti1 ,= 1, 2, . . . , m.every tiexists job k x(ti ) = k, define fti = vk lk ek (ti ) ;else fti = 0;end-forInitially, , h = 0.h < ljt0 = arg minti \T fti , ties broken favor smaller ti ;Initially, e0 = 0;0every time point ti t0 satisfies fti ft0 ee0 = e0 + zi ;ti/ ,add ti , h = h + zi ;end-forend-whileLet t01 earliest critical time point . Let = arg maxti fti .Denote critical time points t01 t02 , t03 , . . . , t0k .Denote relevant periods critical time points z10 , z20 , . . . , zk0 , z .00pj = ft / lj (z1 ++zk ) .end455fiChen, Hu, Liu, Ma, Qin, Tang, Wang & ZhengAppendix B. Example Analysis TightnessExample B.1. two types jobs: long short. length long jobs ,length short jobs 1. Let p large integer, number long shortjobs p + 1 p 1 respectively. first long job J0l released time 0, type0l = (0, , , 1). p 2 1, job Jil type il = (i( ), (i + 1) i, , ).llLong job Jp1type p1= ((p 1)( ), (p + 2), , (p1) + ). Job Jpl typelp = (p(), (p+1)p, , p+ ). Here, small constants satisfying p 1. meanwhile, short jobs follows. j = 1, . . . , p1, denoteJjs jth short job, whose type js = (j (p1), j +1(p1), 1, (j+1)+(p1) ).j = 1, . . . , p 1.lverified one job Jp1completed mechanism 1 ,(p1)social welfare. optimal solution, short jobsllcompleted successively, social welfarecompleted, that, Jp Jp123(p1)(++ ... +) + (p1) + p . Therefore, competitive ratiop1mechanism 1 least ( p2 + . . . + + 1) + 1 + = 1+ 1 + , tends111+ 1 + 1, p .Appendix C. Proof Claim 4.3Proof. Suppose optimal allocation opt standard, i.e., exist completed jobtwo segments beginning time sai sci completed job j two segmentsbeginning time sbj sdj sai < sbj < sci < sdj . following processobtain standard optimal allocation: length job js b-th segment (denote ljb )larger c-th segment (denote lic ), exchange c-th segment jsb-th segment located [sbj , sbj + lic ]; otherwise, exchange js b-th segment c-thsegment located [sci + lic ljb , scj + lic ]. segments, order remainsunchanged. easy see new allocation still feasible obtainssocial welfare. kind exchanges violation, obtainstandard optimal allocation.Appendix D. Proof Corollary 4.14Proof. every constant c < 1 large enough x, (1 xc )x e.large enough, choosing = 1 (1 )2 ln , 1,(1 )2 ln (1) ln (1) ln)e(1) ln (1) = o().lnusing Taylors theorem, know= (1ln = (1 + o(1))(1 ) = (1 + o(1))Thus competitive ratio11c2 ln.1+ 1 + 1 = ( (1)2 + o(1)) ln restart model,2ln2+ 1 + 1 = ( c2 ln (2 + o(1)) + o( ln )) + 1 = ( (1)2 + o(1)) ln resume model,respectively.456fiEfficient Mechanism Design Online SchedulingAppendix E. Multiple Machines ExtensionSuppose C identical machines, process one jobgiven time. Similar work Lucier et al. (2013), assume hmachines allocated single job given time. parameter standscommon parallelism bound system.notion preemption specified follow: job may processed numbermachines 1 h, number machines allocated job may fluctuate,number decreases 0, treat job preempted. Thus, notationpreemption-restart preemption-resume defined accordingly.job j J characterized private type j = (rj , dj , sj , vj ). Instead lj ,use sj denote jobs size (e.g., number machine hours required completejob). Without causing confusion, let maximum ratio sizestwo jobs: = maxi,jJ ssji . simplicity, assume job sizes fall [1, ]. = 1,jobs identical size; otherwise different sizes.E.1 Simple Case: h = 1case, design new mechanism 2 based single-machine mechanism 1 .payment rule 2 exactly 1 , allocation rule shownAlgorithm 2, also similar 1 . Since job processedone machine, mechanism choose C jobs (if any) JF (t) highestpriorities vi si ei (,t) execute. Note valid active time job j timecomputedC ZXej (t) =(xi (s) = j)ds.(15)i=1t0P() indicator function, t0 = arg maxst [ Ci=1 (xi (s) = j)] = 0.say, treating resumable jobs non-resumable jobs simple. summarizetheoretical properties 2 Theorem E.1.Algorithm 5: allocation rule Mechanism 2|JF (t)| Cprocess C jobs highest priorities JF (t);else process jobs JF (t).endTheorem E.1. Mechanism 2 IC following properties:(+1) ,1 (1 )2restart model, setting =1)get competitive ratio + 2 +(1 +2 ; setting =ln arbitrary small > 0, get1another competitive ratio ( (1)2 + o(1)) ln .resume model, setting = 1 (1 )2 ln arbitrary small > 0,2get competitive ration ( (1)2 + o(1)) ln .theorem, following discussions.457fiChen, Hu, Liu, Ma, Qin, Tang, Wang & Zheng(1) Similar done single-machine setting, restart model,give two competitive ratios 2 . small, first ratio better(in particular, = 1, competitive ratio becomes 5 thus optimalaccording Theorem 2.4). large, second ratio better insteadnear optimal according Theorem 2.5.(2) Different obtained single-machine setting, resumemodel, cannot match lower bound 5 = 1 multi-machine setting.Proof. proof Theorem E.1 essentially proof single machinesetting, virtual charging scheme, charge completed job optimal allocationjob completed 2 exactly machine. differenceintegral charging scheme resume model apply multiple machinessetting more. use segment charging scheme resume model.E.2 General Case: h 1handle general case, design new mechanism 3 , divides C machinesbC/hc equally-sized virtual machines (each consisting h machines), treats everyvirtual machine single machine performing scheduling. is, virtualmachine used process one job, remaining C bC/hc h machinesidle.Algorithm 6: allocation rule Mechanism 3(1) Divide C machines bC/hc equal-sized virtual machines.(2) Run mechanism 2 following modification:Capacity: bC/hc.Demand size: sj /h job j.compared case h = 1, setting h 1 imposes flexibilitiesoptimal offline allocation. example, job may processed number machines1 h optimal allocation might always executed exactlyh machines. Fortunately, use similar segmental charging idea h = 1 caseresolve challenge get competitive ratio shown following theorem.4Theorem E.2. Mechanism 3 IC competitive ratio ( (1)2 + o(1)) ln3 setting = 1 (1 )2resume model.lnarbitrary small > 0, matter restart modelfollowing discussions theorem. setting h 1complicated could always obtain results setting h = 1.particular, h divides C, idle machine may obtaincompetitive ratio setting h = 1. However, h divide C, idlemachines introduce additional factor 2 competitive ratio. Besides,competitive ratio restart model better resume model,competitive ratio cannot reach 5 = 1.458fiEfficient Mechanism Design Online SchedulingProof. need show exists optimal allocation (we viewjobs resumable jobs optimal allocation) time every jobprocessed either exactly h machines machine (assuming h divides C).directly use results obtained special case h = 1. Suppose opt optimaloffline allocation J set jobs completed opt. j J , usemj (t) denote number machines processing j time opt.divide time intervals [tk , tk+1 ), k = 0, 1, 2, , time interval[tk , tk+1 ), mj (t) change j J . show allocate jobstime interval [tk , tk+1 ). bC/hc virtual machines, allocate jobsone one, i.e., previous virtual machine full, start allocate jobsanother empty virtual machine tk (empty respect [tk ,Rtk+1 )). Besides,every job allocated continuously one one size allocation tkk+1 mj (t)dt.easily verified allocation every job j J allocated legitimately(j allocated [rj , dj ] processed h machines time)complete deadline since j legitimately completed opt.ReferencesAzar, Y., Ben-Aroya, N., Devanur, N. R., & Jain, N. (2013). Cloud scheduling setupcost. Proceedings twenty-fifth annual ACM symposium Parallelismalgorithms architectures, pp. 298304. ACM.Bar-Noy, A., Guha, S., Naor, J., & Schieber, B. (2001). Approximating throughputmultiple machines real-time scheduling. SIAM Journal Computing, 31 (2),331352.Baruah, S., Koren, G., Mao, D., Mishra, B., Raghunathan, A., Rosier, L., Shasha, D., &Wang, F. (1992). competitiveness on-line real-time task scheduling. RealTime Systems, 4 (2), 125144.Baruah, S. K., Haritsa, J., & Sharma, N. (1994). On-line scheduling maximize taskcompletions. Proceedings Real-Time Systems Symposium, pp. 228236. IEEE.Borodin, A., & El-Yaniv, R. (1998). Online computation competitive analysis, Vol. 2.Cambridge University Press Cambridge.Chin, F. Y., & Fung, S. P. (2003). Online scheduling partial job values: timesharing randomization help?. Algorithmica, 37 (3), 149164.Ding, J., Ebenlendr, T., Sgall, J., & Zhang, G. (2007). Online scheduling equal-lengthjobs parallel machines. Proceedings 15th annual European conferenceAlgorithms, pp. 427438. Springer-Verlag.Ding, J., & Zhang, G. (2006). Online scheduling hard deadlines parallel machines.Algorithmic Aspects Information Management, pp. 3242. Springer.Durr, C., Jez, L., & Nguyen, K. T. (2012). Online scheduling bounded length jobsmaximize throughput. Journal Scheduling, 15 (5), 653664.459fiChen, Hu, Liu, Ma, Qin, Tang, Wang & ZhengEbenlendr, T., & Sgall, J. (2009). lower bound scheduling unit jobs immediatedecision parallel machines. Approximation Online Algorithms, pp. 4352.Springer-Verlag.Friedman, E. J., & Parkes, D. C. (2003). Pricing wifi starbucks: issues online mechanismdesign. Proceedings 4th ACM conference Electronic commerce, pp. 240241. ACM.Goldman, S. A., Parwatikar, J., & Suri, S. (2000). Online scheduling hard deadlines.Journal Algorithms, 34 (2), 370389.Goldwasser, M. H. (2003). Patience virtue: effect slack competitivenessadmission control. Journal Scheduling, 6 (2), 183211.Hajek, B. (2001). competitiveness on-line scheduling unit-length packetshard deadlines slotted time. Proceedings 35th annual ConferenceInformation Sciences Systems.Hajiaghayi, M., Kleinberg, R., Mahdian, M., & Parkes, D. C. (2005). Online auctionsre-usable goods. Proceedings 6th ACM conference Electronic commerce,pp. 165174. ACM.Kolen, A. W., Lenstra, J. K., Papadimitriou, C. H., & Spieksma, F. C. (2007). Intervalscheduling: survey. Naval Research Logistics (NRL), 54 (5), 530543.Lavi, R., & Nisan, N. (2004). Competitive analysis incentive compatible on-line auctions.Theoretical Computer Science, 310, 159180.Lavi, R., & Nisan, N. (2015). Online ascending auctions gradually expiring items.Journal Economic Theory, 156, 4576.Lipton, R. J., & Tomkins, A. (1994). Online interval scheduling. ProceedingsFifth Annual ACM-SIAM Symposium Discrete Algorithms, Vol. 94, pp. 302311.Lucier, B., Menache, I., Naor, J. S., & Yaniv, J. (2013). Efficient online schedulingdeadline-sensitive jobs. Proceedings 25th ACM symposium Parallelismalgorithms architectures, pp. 305314. ACM.Ma, W., Zheng, B., Qin, T., Tang, P., & Liu, T. (2014). Online mechanism design cloudcomputing. CoRR, abs/1403.1896.Mashayekhy, L., Nejad, M. M., Grosu, D., & Vasilakos, A. V. (2014). Incentive-compatibleonline mechanisms resource provisioning allocation clouds. Cloud Computing (CLOUD), 2014 IEEE 7th International Conference on, pp. 312319. IEEE.Nguyen, K. T. (2011). Improved online scheduling maximizing throughput equal lengthjobs. Computer ScienceTheory Applications, pp. 429442. Springer.Nisan, N. (2007). Introduction mechanism design (for computer scientists). Algorithmicgame theory, 209, 242.Nisan, N., & Ronen, A. (2001). Algorithmic mechanism design. Games EconomicBehavior, 35, 166196.Parkes, D. C. (2007). Online mechanisms. Algorithmic Game Theory, ed. N. Nisan, T.Roughgarden, E. Tardos, V. Vazirani, Cambridge University Press, 411439.460fiEfficient Mechanism Design Online SchedulingPorter, R. (2004). Mechanism design online real-time scheduling. Proceedings5th ACM conference Electronic commerce, pp. 6170. ACM.Ting, H.-F. (2008). near optimal scheduler on-demand data broadcasts. TheoreticalComputer Science, 401 (1), 7784.Wu, X., Gu, Y., Li, G., Tao, J., Chen, J., & Ma, X. (2014). Online mechanism designVMS allocation private cloud. Network Parallel Computing, pp. 234246.Springer.Zaman, S., & Grosu, D. (2012). online mechanism dynamic vm provisioningallocation clouds. 5th International Conference Cloud Computing (CLOUD),pp. 253260. IEEE.Zhang, H., Li, B., Jiang, H., Liu, F., Vasilakos, A. V., & Liu, J. (2013). frameworktruthful online auctions cloud computing heterogeneous user demands.Proceedings INFOCOM, pp. 15101518. IEEE.Zheng, F., Fung, S. P., Chan, W.-T., Chin, F. Y., Poon, C. K., & Wong, P. W. (2006). Improved on-line broadcast scheduling deadlines. Computing Combinatorics,pp. 320329. Springer.461fiJournal Artificial Intelligence Research 56 (2016) 269327Submitted 12/15; published 05/16Combining Delete Relaxation Critical-Path Heuristics:Direct CharacterizationMaximilian FickertJorg HoffmannMarcel Steinmetz9 MAFICK @ STUD . UNI - SAARLAND . DEHOFFMANN @ CS . UNI - SAARLAND . DESTEINMETZ @ CS . UNI - SAARLAND . DESaarland University, Saarbrucken, GermanyAbstractRecent work shown improve delete relaxation heuristics computingrelaxed plans, i. e., hFF heuristic, compiled planning task C representsgiven set C fact conjunctions explicitly. compilation view partialdelete relaxation simple elegant, meaning respect original planningtask opaque, size C grows exponentially |C |. herein provide direct characterization, without compilation, making explicit approach arisescombination delete-relaxation critical-path heuristics. Designing equationscharacterizing novel view h+ one hand, generalized version hC hmhand, show h+ (C ) characterized terms combinedhC+ equation. naturally generalizes standard delete-relaxation framework: understanding framework relaxation singleton facts atomic subgoals, onerefine relaxation using conjunctions C atomic subgoals instead. Thanksexplicit view, identify precise source complexity hFF (C ), namely maximization sets supported atomic subgoals relaxed plan extraction,easy singleton-fact subgoals NP-complete general case. Approximatingproblem greedily, obtain polynomial-time hCFF version hFF (C ), supersedingC compilation achievesC compilation, superseding modified cecomplexity reduction information loss. Experiments IPC benchmarksshow theoretical advantages translate empirical ones.1. Introductiondelete relaxation classical planning (McDermott, 1999; Bonet & Geffner, 2001) originates work using STRIPS representation, state variables Boolean, actioneffects conjunctions literals, action preconditions well goal restrictedconjunctions positive literals (facts). relaxation assumes negative(delete) effect literals, hence name. generally, i. e., non-Boolean state variables, amounts assuming state variables accumulate values, ratherswitching them. optimal delete-relaxed planning still NP-hard, satisficing delete-relaxed planning polynomial-time (Bylander, 1994). Relaxed plan heuristics,employing satisficing delete-relaxed planning generation inadmissible heuristicfunctions, proved highly successful (e. g. Hoffmann & Nebel, 2001; Gerevini, Saetti,& Serina, 2003; Richter & Westphal, 2010). form key ingredient successful satisficing planning (not giving optimality guarantee), particular almost winnerssatisficing-planning tracks International Planning Competitions (IPC).c2016AI Access Foundation. rights reserved.fiF ICKERT & H OFFMANN & TEINMETZDespite success, pitfalls delete-relaxation heuristics (for example, ignoringresource consumption) known since long time, intenseefforts outset take deletes account (e. g., Fox & Long, 2001; &Kambhampati, 2001; Gerevini et al., 2003; Helmert, 2004; van den Briel, Benton, Kambhampati, & Vossen, 2007; Helmert & Geffner, 2008; Cai, Hoffmann, & Helmert, 2009; Baier& Botea, 2009; Coles, Coles, Fox, & Long, 2013; Alcazar, Borrajo, Fernandez, & Fuentetaja,2013). Two recent approaches, red-black planning (Domshlak, Hoffmann, & Katz, 2015)refer explicit conjunctions, devised allow systematically:partial delete relaxation, principle render heuristic estimate perfect. hereinfocus explicit conjunctions.summarize results follows, need basic notation conceptswell known planning community. denote perfect heuristic, returning precise remaining cost, h ; heuristic returning cost optimal relaxed plan h+ ;relaxed plan heuristics hFF (from system FF first introduced,see Hoffmann & Nebel, 2001). assume common method computing relaxed planheuristics relaxed plan extraction best-supporter function (Keyder & Geffner, 2008).assume default best-supporter function derived max heuristichmax (Bonet & Geffner, 2001). Relaxed plan extraction hmax best-supporter function is,unit-cost problems, equivalent original formulation terms relaxed planninggraphs Hoffmann Nebel (2001).Explicit conjunctions first introduced Haslum (2009) compilation-basedcharacterization critical-path relaxation introduced earlier Haslum Geffner(2000). relaxation assumes that, goal set facts (a fact conjunctionneeds achieved point plan), suffices achieve costlysubgoal (subconjunction) size m. Here, parameter correspondingheuristic denoted hm . special case = 1 equivalent max heuristic, i. e., h1 =hmax . Haslums (2009) compiled planning task represents size- conjunctionc via newly introduced -fluent c , arranges preconditions effects-fluents h1 (m ) = hm .Subsequently, Haslum (2012) introduced modified compilation C , admitsarbitrary sets C conjunctions guarantees admissibility h+ compilation,i. e., h+ (C ) h , true h+ (m ). furthermore showedmethod converges h , i. e., h+ (C ) = h appropriately chosen C. downsideC size worst-case exponential |C |: Say action supportconjunction c c regressed a, i. e., makes part c true makes none cfalse. order guarantee admissibility h+ (C ), C explicitly enumerates subsetsC 0 C conjunctions c occurrence action plan may support.C compilation (Keyder, Hoffmann, & Haslum, 2012,size explosion tackled ceC still2014), handles possibly-supported c separate conditional effect. ceguarantees convergence, yet loses information ignores cross-context conditions, i. e.,precondition -fluents arise combination several supported c C 0 .One thing evident history explicit conjunctions resulting heuristic functions combine information inherent critical-path relaxation, informationinherent delete relaxation. way, exactly? compilation viewsimple elegant, meaning respect original planning task opaque.270fiC OMBINING h+ hm : IRECT C HARACTERIZATIONfirst simple observation step size- conjunctions arbitrary conjunctions C specific the, historically, simultaneous step critical-path partialdelete relaxation heuristics. hm heuristic straightforwardly generalizable consider,size- subgoals, arbitrary set C subgoals. Intuitively, chooseset atomic subgoals, kept intact critical-path relaxation. denotegeneralized heuristic hC .second simple observation delete relaxation viewed allowingachieve fact separation: achieving facts p goal set/conjunction one-by-one,negative effects within step matter concern facts p.Given this, h+ characterized terms equation related characterizingh1 , requiring achieve size-1 subgoals instead achieving singlecostly one.Putting two observations together, obtain natural generalization standard delete-relaxation framework: standard delete relaxation, like h1 , workssingleton facts atomic subgoals, one use conjunctions C atomic subgoals instead.spell form two heuristic functions denote hC+ hCFF :(1) h1 -like equation characterizing h+ translates hC -like equation characterizinghC+ , equivalent h+ (C ).(2) Relaxed plan extraction obtain hFF h1 best-supporter function translatesrelaxed plan extraction obtain hCFF (a relaxed plan C ) hC best-supporterfunction.Result (1) theoretical interest. formulates hC+ = h+ (C ) without compilation,shedding different light Haslums (2012) equivalent proposal. Result (2)immediate practical ramifications. provides alternative technique obtain relaxedplans C , exponentially efficient worst case requireexhaustively enumerate subsets C 0 C. hC best-supporter function computedtime polynomial |C |, similar hm . Intuitively, critical path pertains singleatomic subgoals, need enumerate combinations atomic subgoals here.relaxed plan extraction, avoid enumeration identifying tackling precisesource complexity.Relaxed plan extraction hC complex relaxed plan extraction h1two reasons, first corresponds Haslums (2012) observations, yet secondone becomes apparent new direct formulation:(a) ensure convergence, allowing hFF (C ) find real plans limit, needcollect set action occurrences, i. e., pairs ( a, C 0 ) action set supportedconjunctions C 0 , instead set actions standard setting (where actionsmerely support facts direct effect).(b) every occurrence ( a, C 0 ) selected relaxed plan extraction, C 0large possible, atomic subgoals (conjunctions) may overlap, incurringrisk dramatic overestimation (e. g., achieving every fact pair global goal separately). NP-complete find cardinality-maximal C 0 incurinfeasible cross-context conditions.understand this, consider action support least one subgoal c Crelaxed plan extraction. need decide current subgoals c0 C271fiF ICKERT & H OFFMANN & TEINMETZsupport action occurrence. standard setting, c c0singletons (e. g. c = { p} c0 = {q}), one simply support c0 positive effects(e. g. add( a) = { p, q}). general case, arbitrary C, longerpart c0 contained positive effects gets propagated new subgoalregressing (e. g. r1 c10 = {q, r1 } r2 c20 = {q, r2 }). Combinations severalc0 may incur cross-context conditions (e. g. {r1 , r2 }) harder achieve conjunctionsc0 isolation.refer (b), maximization |C 0 | relaxed plan extraction, subgoalsupport selection problem. striking underlying phenomena supportedconjunction sets C 0 cross-context conditions previously identified addressed,yet put specific context relevant relaxed plan extraction. perspective, Haslum (2012) solves NP-complete problem enumeratively, putting solutioncandidates (choices C 0 ) memory form compiled task C ; Keyder etC compilation over-simplifies problem, ignoring cross-context conal.s (2012, 2014) ceditions completely. Yet, cardinality-maximal C 0 real issue, dont simplyselect subset-maximal C 0 instead? Using simple greedy approximation effect,obtain hCFF , extracting relaxed plans C polynomial time without ignorecross-context conditions. heuristic supersedes, theoretical perspectiveC compilations.far hFF concerned, C cepoint necessary mention observation (2) entirely new.Alcazar et al. (2013) already devised heuristic call FFm , extracting relaxed planhm best-supporter function (they implement = 2). essentially(2), without generality arbitrary conjunction set C (which easily fixed).However, Alcazar et al.s work conducted part much broader scope addressingheuristic search regression planning, investigate (2) detail. designFFm recognize, therefore appropriately address, (a) (b). Regarding(b), FFm always selects single conjunction C 0 = {c} support, trivial approximationNP-complete |C 0 |-maximization problem, may lead dramatic overestimation.overestimation counter-acted given FFm also disregards (a), collecting setactions standard relaxed plan extraction methods. loses convergencevalue FFm bounded number actions defeating purpose method.empirical perspective, matters clear-cut. Obviously, one construct cases computational advantage C , information advanC FF2 , leads exponential savings. IPC benchmarks another matter.tage ceEvaluating heuristic functions, find larger conjunction sets C typically leadsmaller search spaces, hCFF indeed much faster hFF (C ) large C.Unfortunately, even slowdown hCFF typically outweighs search space reduction,best overall performance often obtained small C. positive side,techniques yield advantages even small C, IPC benchmarks largeC beneficial.C compilations SecWe next introduce basic notation well C ce+Ction 2. spell direct characterization h ( ) Section 3, spellgeneralized relaxed plan extraction methods Section 4. summarize implementation experiments Section 5, concluding Section 6. proofsreplaced main text brief proof sketches. Full proofs available Appendix A.272fiC OMBINING h+ hm : IRECT C HARACTERIZATION2. Notations Technical Backgrounduse STRIPS framework. planning task tuple = (F , A, , G) Fset facts, set actions, F initial state, G F goal. actiontriple (pre( a), add( a), del( a)) precondition, add list, delete list, subsetF . henceforth tacitly assume given input task = (F , A, , G).state subset facts F . Action applicable pre( a) s; case,applying leads state (s add( a)) \ del( a). plan sequence iterativelyapplicable actions leading state contains goal G . plan taskplan initial state . plan optimal length minimal among plans.assume throughout add( a) del( a) = . natural common assumption action adding p also delete without loss generalityfacts intersection equivalently removed add( a). assumptionnecessary achieving fact separation view delete relaxation,outlined introduction.Note that, simplicity, consider unit costs: action costs 1, plan qualityplan length. results straightforwardly extend arbitrary non-negative actioncosts, plan quality measured terms summed-up cost.Example 1 illustration, frequently consider following car-driving example.car moves one-way line X Z locations, X Z. car move consumesfuel unit, cars tank holds one unit must refuel Y.encode STRIPS, design task = (F , A, , G) follows. F = {carX, carY,carZ, f uel }, = {carX, f uel }, G = {carZ }. consists of: XY precondition{carX, f uel }, add list {carY }, delete list {carX, f uel }; aYZ precondition {carY, f uel },add list {carZ }, delete list {carY, f uel }; f uel precondition {carY }, add list{ f uel }, empty delete list. plan task h XY , f uel , aYZ i.Given planning task , denote set states S. heuristic (also heuristicfunction) function h : 7 N0+ {} mapping states natural numbers including0, indicate state dead-end. perfect heuristic h maps statelength optimal plan (or plan s). heuristic hadmissible h(s) h (s) S. Abusing notation, often identify heuristich value h(I) initial state. statements made generalize arbitrary statessetting := s. h(0 ), denote heuristic whose value given applyingh modified task 0 . make explicit h computed itself, write h().characterize heuristic functions terms equations regressed subgoals.regression fact set G action a, R( G, a), defined add( a) G 6= del( a)G = . case, R( G, a) = ( G \ add( a)) pre( a); otherwise, write R( G, a) = .critical-path relaxation (Haslum & Geffner, 2000) assumes that, goal setfacts, suffices achieve costly subgoal size m. Here, parametercorresponding heuristic denoted hm . Precisely, hm defined hm := h(G)h function fact sets G satisfiesGI01 + minaA,R(G,a)6= h( R( G, a)) | G |(1)h( G ) =0maxG0 G,|G0 |m h( G )else273fiF ICKERT & H OFFMANN & TEINMETZeasy see exactly one h, assuming two functions h, h0h( G ) 6= h0 ( G ) recursively leads contradiction initial state.1 argument applies h-defining equations considered herein, henceforth assumeuniqueness given.= 1, definition hm becomes identical max heuristic hmax (Bonet& Geffner, 2001), assumes that, achieve goal fact set, enough achievemaximum costly single fact. = |F |, hm = h simply subgoals size > |F |exist. Computing hm takes time exponential polynomial size .delete relaxation assumes delete lists empty; plan relaxationrelaxed plan. ideal delete-relaxation heuristic h+ maps length optimalrelaxed plan s. optimal relaxed planning NP-complete (Bylander, 1994). relaxed plan heuristic maps length some, necessarily optimal, relaxed plans, computed easily (Hoffmann & Nebel, 2001). resulting heuristic functions admissible, often informative practice satisficing planning.follow common approach considering idealized heuristic h+ theoretical examinations delete relaxation (compare, e. g., Hoffmann, 2005, 2011; Bonet &Helmert, 2010), considering effective approximation relaxed plan heuristics practice.Relaxed plan heuristics differ find relaxed plan. flexible wayspecifying best-supporter functions introduced Keyder Geffner (2008).best-supporter function maps fact p action relaxed plan usesupport p. Given function, relaxed plan extraction starts goals, keeps selecting best supporters, opening preconditions new subgoal facts, initial statefacts reached. denote heuristic arising process hFF (disambiguating context needed). detailed formal characterization relaxedplan extraction given Section 4, details technically relevant.Practical best-supporter functions based hmax = h1 , selecting p actionA, R( G, a) 6= minimizing expression middle case Equation 1 = 1(where subgoal G singleton set { p} identified element p).Alternatively, one assign best supporters based additive heuristic hadd (Bonet &Geffner, 2001) instead, differs h1 using sum, rather maximum,estimated cost facts goal set (bottom case Equation 1). Note(in hmax hadd ) may several actions eligible best supporter. Henceconstruction best-supporter functions encompasses tie-breaking, sensechoosing action set actions supporting given fact p. tie-breakinglarge effect empirical performance relaxed plan heuristic. getback detail experiments.Throughout paper, concerned conjunctions c. Following STRIPSconvention formulating conjunctive conditions (action preconditions goal)fact sets, conjunction c fact set c F , e. g. c = { p, q}. However, improvereadability, often notate c conjunctive formula instead, e. g. c = p q.1. Matters complicated case 0-cost actions, recursion may lead cyclespoint-wise maximal h unique.274fiC OMBINING h+ hm : IRECT C HARACTERIZATIONhenceforth assume given set C conjunctions. practice, C computed input task , prior search.2 assume throughout C containssingleton conjunctions, {{ p} | p F } C. convenience, notating factsspecial case conjunctions. sometimes identify singleton conjunctions { p}respective facts p, i. e., notate without set brackets avoid clutter; notep also notation get writing { p} conjunctive formula.convenient introduce shorthand operation collecting atomicconjunctions contained fact set. Given set facts X F , assuming givenconjunction set C described, define X C := {c | c C, c X }. sometimesextend notation sets X = { X1 , . . . , Xn } fact sets, X C defined pointwise,i. e., X C := XiC .C compilation relatives based representing conjunctions explicitly,terms introducing new facts called -fluents. introduce onefluent, c , c C. correspondence shorthand introduced, factset X F , X C := {c | c C, c X } denote set collecting -fluentsatomic conjunctions entailed X. extend notation sets fact setspointwise manner above.Using notations, C defined follows:Definition 1 Let = (F , A, , G) planning task, C set conjunctions containing singleton conjunctions. explicit-C compilation C planning task (F C , AC ,C , G C ). Here, F C , C , G C defined per shorthand. set actions ACcontains action a[C 0 ], every pair 6= C 0 {c C | R(c, a) 6= }, a[C 0 ]givenpre( a[C 0 ]) = [cC 0 (pre( ) ( c\ add( a)))]C ,add( a[C 0 ]) = {c | c C 0 }.C identical C except pre( [C 0 ]) =cross-context explicit-C compilation nc0C{pre( a) (c \ add( a)) | c C } .refer pairs ( a, C 0 ) action set C 0 supported conjunctions, correspondingC , action occurrences. refer c \ add( ),compiled actions a[C 0 ] C nc0c C , context c a: support c, context must true precedingC , extend original prestate. captured preconditions C nccondition contexts supported conjunctions. C , context collectedC done supported conjunction inacross supported conjunctions, ncdividually.definition C diverges original definition Haslum (2012) severalminor ways. First, distinguish explicitly actions original effectsvs. supported conjunctions, instead expressing add list part set C 0conjunctions may supported. possible C assumed containsingleton conjunctions. consequence, demand C 0 6= (otherwiseaction would effect thus useless). Second, automatically include2. details exactly done relevant contribution. briefly describemethods use (adopted Keyder et al., 2012, 2014) discussion experiments, Section 5.275fiF ICKERT & H OFFMANN & TEINMETZc facts relying context consisting non-deleted preconditions. Third,demand C 0 downward closed, i. e., contain subsumed conjunctions c0 ,exists c C 0 c0 c. Fourth, include delete effects.None changes consequences results present. Changes twofour introduce superfluous actions, simplifying presentation affectingresults. fourth change suitable use C generating deleterelaxation heuristics. consistent use language, speak relaxed plansC nevertheless.also modify notation bit, relative Haslum (2012). notate actions0a[C 0 ] instead AC . convenient. furthermore somewhat modified definition a[C 0 ] preconditions, exploiting C 0 6= . Namely, pre( a[C 0 ]) =[ cC0 (pre( a) (c \ add( a)))]C C equivalent perhaps intuitively straightSforward definition, namely pre( a[C 0 ]) = [pre( a) cC0 (c \ add( a))]C used Haslum.C , precondition pre( [C 0 ]) = {pre( ) ( c \ add( )) | c C 0 }C , given pointFor ncwise interpretation C superscript, equals cC0 [pre( a) (c \ add( a))]C ,perhaps intuitively straightforward definition pre( a[C 0 ]) =pre( a)C cC0 [pre( a) (c \ add( a))]C . Observe modifications allow writeaction preconditions terms regression, thanks R(c, a) = pre( a) (c \ add( a)).C , reads pre( [C 0 ]) =C , precondition reads pre( a[C 0 ]) = [ cC0 R(c, a)]C . nc0C{ R(c, a) | c C } . simplified notations conveniently link-in conceptsintroduce later on.Note finally C compilation introduces atomic conjunctions action preconditions goal, even ones subsumed other, larger, atomic conjunctions contained precondition/goal. stick convention throughout, simplicity. practice, ignore subsumed conjunctions. remainder paper,corresponds modified C superscript, including conjunctions c C, c X,exist c0 C, c0 X, c ( c0 ; correspondingly Csuperscript. leaves results intact exactly stated.Example 2 Reconsider car-driving example task Example 1. delete relaxationignores negative effect XY , shortest relaxed plan h XY , aYZ h+ = 2.However, say set C contain (all singleton conjunctions well as) c = carY fuel.Then, C , c precondition actions aYZ [C 0 ], i. e., actions adding goal carZ.actions adding c form arefuel [C 0 ] c C 0 . Hence h XY , aYZrelaxed plan C . Instead, need perform refueling action, example relaxed planh XY [{carY }], arefuel [{carY fuel}], aYZ [{carZ }]i. get h+ (C ) = 3 = h ().C exponential |C | action occurrences enumerThe growth C ncate subsets C 0 C supported conjunctions. complexity necessary because,C ) would admissible. simple example, sayotherwise, h+ (C ) h+ (ncgoal n { g1 , . . . , gn }, C contains singleton conjunctions well factpairs, single action achieving { g1 , . . . , gn }. h (n ) = 1,h+ (Cn ) = 1 thanks optimal plan h a[C 0 ]i C 0 set conjunctions,C 0 = C = {{ gi } | 1 n} {{ gi , g j } | 1 6= j n}. However, achieveevery conjunction separately Cn , is, included AC actionsn(n1)form a[{c}] c C, would get h+ (Cn ) = n +would2276fiC OMBINING h+ hm : IRECT C HARACTERIZATIONachieve every conjunction c C separate compiled action. (This observationbecome relevant later on, compare Example 7 Section 4.3.1.)C latter, former, ignoresdifference C nctermed cross-context conditions: conjunction preconditions a[C 0 ] Carise combination several c C 0 . Precisely, cross-context conditionC 0 conjunction c C c cC0 [pre( a) (c \ add( a))],C compilation, preexist single c C 0 c pre( a) (c \ add( a)). nc0condition a[C ] contain cross-context conditions, superscriptC pre( a[C 0 ]) = {pre( a) (c \ add( a)) | c C 0 }C , i. e., collection conjunctions, done context c C 0 separately. contrast C where,pre( a[C 0 ]) = [ cC0 (pre( a) (c \ add( a)))]C , conjunctions collected unioncontexts across c C 0 .Example 3 illustrate cross-context conditions, consider following abstract example (given Keyder et al., 2014, part proof Theorem 3). = (F , A, , G)F = { g1 , g2 , p, q1 , q2 }, = {q1 }, G = { g1 , g2 } consists of: g1 precondition { p, q1 },add list g1 , empty delete list; g2 precondition { p, q2 }, add list g2 , empty delete list;p empty precondition, add list { p}, empty delete list; aq2 precondition {q1 }, addlist {q2 }, delete list {q1 , p}. construction, q1 q2 mutex; achieving g1 requiresq1 thus done first, via p g1 . achieve g2 , require q2 . Getting q2 aq2deletes p, must apply p second time applying g2 . delete relaxationnever need apply action twice, h+ = 4 < 5 = h .Say set C contain cq1p = q1 p, cq2p = q2 p, cq1q2 = q1 q2 .relaxed plan C must contain two occurrences p : cq1p cq2p required achievegoal; p action support conjunctions (note aq2 deletes p);p [{cq1p , cq2p }] supporting conjunctions single action occurrence unreachablecross-context condition cq1q2 . Consequently, h+ (C ) = 5 = h ().C , [{ ccontrast, ncpq1p , cq2p }] cross-context condition, h aq2 [{ q2 }],C ) = 4 = h + ( ) < h ( ).p [{ p, cq1p , cq2p }], g1 [{ g1 }], g2 [{ g2 }]i relaxed plan h+ (ncC compilation, achievesKeyder et al. (2012, 2014) introduced ceCeffect nc size polynomial |C |. done augmenting every originalaction one conditional effect c C regressed a,adding c requiring context c \ add( a) effect condition.C compilation equivalent C sense h+ ( C ) = h+ ( C ). InThe cenccencC , conditional effects settuitively, occurrence action ceC action [C 0 ] C conjunctionsconjunctions C 0 fire, equivalent nccec C 0 handled separately, ignoring cross-context conditions.3 Given equivalence,theoretical discussion heuristic functions properties sizeC C matter refer throughout C ratherdifference nccencC . simplifies matters switch formalismsce(STRIPS vs. without conditional effects).C ) h+ ( C ) proved Keyder et al. (2014) proof Lemma 2,3. Technically, h+ (cencC ) h+ ( C ) symmetric.opposite direction h+ (cenc277fiF ICKERT & H OFFMANN & TEINMETZC correWe see Section 4.3 complexity reduction C cespondence complexity reduction subgoal-support selection problem (maximization |C 0 | relaxed plan extraction): problem NP-complete C ,C.polynomial-time nc3. Combining Delete Relaxation Critical Paths: hC+spell observation (1) introduction, characterizing combinationdelete relaxation critical paths directly, without compilation, termsheuristic function call hC+ . Section 3.1 starts simple novel viewstwo components, Section 3.2 combines equation characterizing hC+ . Section 3.3 sketches proof correctness i. e., hC+ = h+ (C ). Section 3.4 summarizesproperties hC+ , pointing combination delete relaxation critical paths naturally generalizes components.3.1 Novel Views hm h+First, consider following straightforward characterization h , relaxeddifferent manners below: h := h(G) h function fact sets G satisfies0GIh( G ) =(2)1 + minaA,R(G,a)6= h( R( G, a)) elseequation obviously characterizes optimal planning, therewith h : minimizeplan length actions support subgoal G.Slightly rephrasing critical-path relaxation, assumes that, achieve subgoal G,suffices achieve costly atomic subgoal, notion atomic subgoalparameter. traditional formulation, parameter instantiated factsets size m. need restrictive. atomic subgoalsarbitrary set fact-sets, words: arbitrary set C conjunctions. merelyneed replace subgoal-selection mechanisms hm (Equation 1) accordingly generalized ones. denote resulting heuristic hC , defined hC := h(G) hfunction fact sets G satisfiesGI01 + minaA,R(G,a)6= h( R( G, a)) G C(3)h( G ) =0maxG0 G,G0 C h( G )elseTrivially, hC = hm C consists conjunctions size m. shall see below,hC = h1 (C ) one would expect. latter property useful theoreticalperspective though, connecting hC known results h1 . practice, hC computed like hm , fixed point process value assignments atomic subgoals C,taking time polynomial |C |, contrast size C . particular implementationdescribed Section 5.1.worth pointing simple generalization hm hC alreadyquite useful:278fiC OMBINING h+ hm : IRECT C HARACTERIZATIONExample 4 Consider car-driving example Example 1, without refuel action.modified task unsolvable, h2 = recognizes that. Yet, need reason factpairs arrive conclusion: Considering single fact pair C = {c} c = carY fuel,like Example 2, suffices get hC = , c becomes precondition achieving goal,action c regressed. particular example 4facts thus 6 fact pairs, could scale arbitrarily adding solvable parts, blowingcomputational overhead h2 still recognizing unsolvability using single fact pair c.Getting back discussion alternate ways relax h , i. e., Equation 2, observeEquation 3 uses correct regression semantics, relaxes subgoals considered.delete relaxation viewed approaching vice-versa, keeping correctsubgoaling relaxing regression semantics. immediately visible following straightforward characterization h+ , h+ := h(G) h functionfact sets G satisfies0GIh( G ) =(4)1 + minaA,6=Gadd(a) h(( G \ add( a)) pre( a)) elseidentical Equation 2 except pretending R( G, a) 6= even del( a) G 6=, i. e., replacing R( G, a) relaxed concept asks non-empty add-listintersection.basic observation towards combining hC h+ underlying relaxationprinciples, though seem unrelated given Equations 3 4, viewedrelaxations pertaining subgoaling structure. becomes visible followingalternative characterization h+ :Lemma 1 Let = (F , A, , G) planning task, let h function fact sets Gsatisfies0GIh( G ) =(5)01 + minaA,6=G0 ={ p| pG,R({ p},a)6=} h(( G \ G ) pG0 R({ p}, a)) elseh(G) = h+ .Proof: Observe that, singleton fact sets G = { p}, (a) regressability G trivializesadd-list intersection, i. e., R({ p}, a) 6= iff p add( a), add( a) del( a) =get p 6 del( a); (b) G regressed regression simply generates action precondition new subgoal, i. e., R({ p}, a) = pre( a). Equation 5simplifies0GIh( G ) =1 + minaA,6=G0 =Gadd(a) h(( G \ G 0 ) pre( a)) elseG 0 = G add( a) G \ G 0 = G \ add( a), equivalent Equation 4.per Equation 5, delete relaxation understood splitting subgoalssingleton facts, considering regression separately respect these. singletonregression trivializes, effect need worry part subgoal279fiF ICKERT & H OFFMANN & TEINMETZsupport, parts action may contradict.4 reformulation awkward useful standard setting, exhibits possible refinementsetting: instead singleton facts, consider atomic subgoals form arbitraryset C conjunctions.3.2 Combined HeuristicConsider Equation 5, compare following equation characterizing h1 :GI01 + minaA,R({ p},a)6= h( R({ p}, a)) G = { p}(6)h( G ) =max pG h({ p})elseEquation 5 understood less relaxed version Equation 6. decomposesubgoal G atomic subgoals, instantiated singleton facts, minimizeactions regressing atomic subgoals. difference that, Equation 6 pickssingle costly atomic subgoal, Equation 5 requires achieve every atomic subgoal (inparticular, including ones supported a, i. e., G \ G 0 , recursive invocationh). set G consists exactly atomic subgoals, Equation 5 need thirdcase identifying atomic subgoals.Now, hC generalizes h1 considering general atomic subgoals C. Applyingsimilar generalization Equation 5, obtain desired combination deleterelaxation critical paths:Definition 2 Let = (F , A, , G) planning task, C set conjunctions containing singleton conjunctions. critical-path delete relaxation heuristic, short C-relaxationheuristic, defined hC+ := h(G C ), h function conjunction sets G satisfies(0c G : ch( G ) =(7)C001 + minaA,6=G0 {c|cG,R(c,a)6=} h(( G \ G ) Gr ) elseGr0 defined Gr0 := cG0 R(c, a).cross-context critical-path delete relaxation heuristic, short nc-C-relaxation heurisC + , defined identically hC + except define G 0 : = { R ( c, ) | c G 0 }.tic, denoted hncrRecall that, fact set X, X C := {c | c C, c X } denotes set atomicC+ )conjunctions contained X, set fact sets (as case Gr0 hncapply notation pointwise. convention, refer expression ( G \CG 0 ) Gr0 Equation 7, related equations, recursive subgoal, Gr0regressed subgoal.Intuitively, hC+ supports atomic subgoals C individually regression hC ,instead achieving costly one, achieves them. parallelsprevious comparison h+ h1 . subgoals G recursed setsconjunctions, difference h+ (Equation 5) atomic subgoals conjunctions4. independence assumptions identified Keyder Geffner (2009) somewhat related this.formulation pertains delete relaxation heuristic h+ itself, ignoring negative side effects; whereasKeyder Geffners observations pertain simplifying assumptions approximations h+ , ignoringpositive side effects.280fiC OMBINING h+ hm : IRECT C HARACTERIZATIONinstead single facts, difference hC (Equation 3) estimate cost setsatomic subgoals instead single atomic subgoals. initializing call G C insertsatomic conjunctions global goal, recursive subgoals insert atomicconjunctions regressed subgoal Gr0 . Hence, like Equation 5 set G consistsexactly atomic subgoals, need third case identifying atomicsubgoals.top case Equation 7 self-explanatory. bottom case generalizesEquation 5. atomic subgoals non-unit conjunctions, differencearguments Lemma 1, regression longer trivializes: allowed contradictconjunction c G 0 , R(c, a) may proper superset pre( a), longer trivializingaction precondition. Hence, difference Equation 5, complex notationnecessary. also major new source complexity, relative Equation 5,namely need allow G 0 subset supportable atomic subgoals, rathersetting G 0 entire set. corresponds aforementioned subgoal-supportselection problem. illustrate problem Example 5 below; Section 4.3 conductsin-depth analysis context relaxed plan extraction.C + is, notation suggests, designed matchdifference hC+ hncC . expressions0difference C ncc G 0 R ( c, ) vs. { R ( c, ) | c G }obvious correspondence action preconditions Definition 1 (thanksmodifications respect original definition). pair ( a, G 0 ) action subset supported atomic subgoals hC+ equation corresponds C action a[C 0 ]C + C . instructive alternative way read rewhere C 0 = G 0 , similarly hncncVgressed subgoals Gr0 terms conjunctions. gives cG0 R(c, a) = cG0 R(c, a) =VV00c G 0 ,p R(c,a) p, vs. { R ( c, ) | c G } = { p R(c,a) p | c G }: one large conjunction vs.several small ones. makes difference larger conjunctions may contain largerCatomic subgoals, captured Definition 2 respective use Gr0 .Example 5 Consider, Example 2 (page 276), car-driving example C containingsingleton conjunctions well c = carY fuel. get hC+ = h({carZ }), i. e., h definedper Equation 7, applied conjunction set containing single goal atomic conjunctioncarZ. ( a, G 0 ) pair supporting carZ ( aYZ , {carZ }). Selecting ( a, G 0 ), getrecursive subgoal G = {carY, fuel, carY fuel}. carY fuel cannot supported XYdeletes fuel, supporting action subgoal arefuel . Say select action,G 0 := {carY fuel}. recursive subgoal {carY, fuel} conjunctionscarY fuel G included G 0 . detail, recursive subgoal results exSpression ( G \ G 0 ) [ cG0 R(c, a)]C = ({carY, fuel, carY fuel} \ {carY fuel}) R(carYfuel, arefuel )C = {carY, fuel} {carY }C = {carY, fuel} {carY }. subgoal resolvedusing ( XY , {carY }), yielding hC+ = h+ (C ) = 3 due relaxed plan Example 2.consider, Example 3 (page 277), abstract example C containing singleton conjunctions well cq1p = q1 p, cq2p = q2 p, cq1q2 = q1 q2 .hC+ = h({ g1 , g2 }), requiring support two goal facts (written singleton conjunctive formulas here). done ( g1 , { g1 }) ( g2 , { g2 }) respectively; usingthese, get recursive subgoal G = {q1 , q2 , p, q1 p, q2 p}. non-trivialsubgoal-support selection problem. Ignoring subsumed subgoals q1 , q2 , p tackledside effect tackling non-subsumed ones q1 p q2 p, choose (a)281fiF ICKERT & H OFFMANN & TEINMETZ( p , {q1 p, q2 p}), (b) ( p , {q1 p}), (c) ( p , {q2 p}). choose (a), subgoalfully supported i. e., G \ G 0 = , yet [ cG0 R(c, p )]C = {q1 , q2 }C = {q1 , q2 , q1 q2 }.cross-context conjunction q1 q2 supported action, cannot get initial stateway. Instead, need take either (b) (c), yielding recursive subgoals (b) {q2 p, q1 }respectively (c) {q1 p, q2 }, necessitates support aq2 well another occurrencep , leading hC+ = h+ (C ) = h = 5.C + instead, option (a) produces different subgoal { R ( c, ) | c G 0 }C =Using hnc{{q1 }, {q2 }}C = {q1 , q2 }, containing cross-context conjunction q1 q2 . subgoalC + = h+ ( C ) = h+ = 4.feasible, requires support aq2 , leading hncnc3.3 Proof Correctnessprove Equation 7 indeed capture h+ (C ), i. e., h+ (C ) = hC+ ().illustration, first consider simple case C contains singleton conjunctions:Proposition 1 Let = (F , A, , G) planning task, C = {{ p} | p F }.h+ = hC + .Proof: C = {{ p} | p F }, recursive subgoals G Equation 7 sets singletonfact-sets, instead perceive G set facts. re-write Equation 7 to:0GIh( G ) =01 + minaA,6=G0 { pG| R({ p},a)6=} h(( G \ G ) pG0 R({ p}, a)) elseidentical Equation 5 except G 0 allowed subset { p G |R({ p}, a) 6= }. However, R({ p}, a) = pre( a), minimum bottom casealways achieved using G 0 = { p G | R({ p}, a) 6= }, yield smallerrecursive subgoals G 0 { p G | R({ p}, a) 6= }. concludes proofLemma 1.Observe Proposition 1 proves h+ (C ) = hC+ () C = {{ p} | p F }:singleton conjunctions only, h+ = h+ (C ), Proposition 1 h+ (C ) = h+ =hC+ () desired. extend general case, arbitrary conjunction sets:Theorem 1 Let = (F , A, , G) planning task, C set conjunctions containingsingleton conjunctions. h+ (C ) = hC+ ().Proof Sketch: apply Equation 5 C , characterizing h+ (C ). Making explicitindividual facts C -fluents, obtain: h+ (C ) = h({c | c G C }),h function fact sets G satisfies h( G ) =0c G : c C001 + mina[C0 ]AC ,6=G0 ={c |c G,R({c },a[C0 ])6=} h(( G \ G ) Gr ) elseGr0 defined Gr0 := c G0 R({c }, a[C 0 ]).condition R({c }, a[C 0 ]) 6= simplifies c C 0 , exactly-fluents added a[C 0 ]. G 0 = {c | c G, c C 0 } minimizationa[C 0 ] supporting non-empty subset subgoals c . c principle282fiC OMBINING h+ hm : IRECT C HARACTERIZATIONinclude C 0 are, definition C , exactly R(c, a) 6= .point including c c 6 G, support subgoals yetresult larger precondition. Hence, renaming C 0 G 0 order unify notation,obtain h( G ) =0c G : c C1 + mina[G0 ]AC ,6=G0 {c|c G,R(c,a)6=} h(( G \ G 0 ) Gr0 ) elseGr0 defined Gr0 := cG0 R({c }, a[ G 0 ]).Comparing equation Equation 7, easy see equationsexact correspondence via () G = {c | c G [7]}, G [7] denotes subgoal setsEquation 7. (*) true definition initializing calls, hC+ () = h(G C ) respectively h+ (C ) = h({c | c G C }). (*) invariant bottom casesequations, Gr0 = cG0 R({c }, a[ G 0 ]) = pre( a[ G 0 ]) = [ cG0 (pre( a) (c \ add( a)))]C =[ cG0 R(c, a)]C , matches regressed subgoal Gr0 [7] = [ cG0 R(c, a)]C Equation 7 desired.C hC + :similar proof shows correspondence ncncTheorem 2 Let = (F , A, , G) planning task, C set conjunctions containingC ) = h C + ( ).singleton conjunctions. h+ (ncnc3.4 Properties Combinationcombination delete relaxation critical paths, per Definition 2, naturallygeneralizes properties components. follows known results alongfollowing simple observation:5Theorem 3 Let = (F , A, , G) planning task, C set conjunctions containingC ) = h C ( ).singleton conjunctions. h1 (C ) = h1 (ncProof Sketch: Consider first C . Applying Equation 6 (page 6) characterizing h1 C ,get h1 (C ) = h(G C ) h function fact sets G satisfiesG C001 + mina[C0 ]AC ,R(G,a[C0 ])6= h( R( G, a[C ])) G = {c }, c Ch( G ) =maxc G h({c })elseObserve that, middle case, must c C 0 otherwise c 6 add( a[C 0 ]);point including conjunctions C 0 , i. e., C 0 ) {c}, yield larger recursive subgoal R( G, a[C 0 ]). Hence re-writeprevious equation to:G C01 + mina[{c}]AC ,R(G,a[{c}])6= h( R( G, a[{c}])) G = {c }, c Ch( G ) =maxc G h({c })elseC ) part observation, using different proof5. Keyder et al. (2014) already proved h1 (C ) h1 (ncargument.283fiF ICKERT & H OFFMANN & TEINMETZComparing equation Equation 3 (page 278) characterizing hC , easy seeequations exact correspondence via () G = {c | c C, c G [3]}, G [3]denotes subgoal (fact) sets Equation 3.C identical because, single-conjunction sets C 0 = { c }, twoargument nccompilations coincide.Note that, step first second equation stated proof, exponential size C reduced polynomial-size compilation like Cincludes actions a[{c}] pairs c C c regresseda. Intuitively, h1 considers singleton subgoals, need enumerate supported conjunction sets size greater 1. reduced compilation essentiallyversion Haslums (2009) compilation arbitrary conjunction sets C. (This simplegeneralization mentioned Haslum works C .)Together results Haslum (2012) Keyder et al. (2014), well basic knownresults h1 h+ , Theorems 1 3 immediately imply properties one wouldC + have:naturally expect hC+ hncCorollary 1 Let planning task. Then, set C conjunctions containingsingleton conjunctions, have:C + hC + h ;(i) hC , h+ hncC + = iff hC = .(ii) hC+ = iff hncC + converge h , i. e., exist sets C conjunctionsFurthermore, hC+ hncC+C+ = h .(iii) h = h respectively (iv) hncC + = h+ ( C ) Theorem 3 hC = h1 ( C ),Proof: Regarding (i): Theorem 2, hncncncCC+1+C ) = h+ ( C ) hence hC + = h+ ( C ),h hnc follows h h . h+ (nccencceC + holds corresponding result Keyder et al. (2014) (h+ h+ ( C ),h+ hncceC drops preconditions C , get hC + = h+ ( C ) h+ ( C ),Corollary 1). ncncnc+CCh ( ) = h + Theorem 1. Finally, hC+ h holds corresponding resultHaslum (2012) (h+ (C ) h , Theorem 4).C + = h+ ( C ), hC = h1 ( C ) = h1 ( C ),Regarding (ii): hC+ = h+ (C ), hncncncfollows h+ = iff h1 = .Finally, (iii) holds convergence h+ (C ) (Haslum, 2012, Theorem 5) hC+ =+C ) (Keyder et al., 2014,h (C ) per Theorem 1, (iv) holds convergence h+ (ceC ) = h+ ( C ) hC + = h+ ( C ) per Theorem 2.Theorem 5) h+ (nccencnc4. Extracting Relaxed Plans: hCFFobserved that, like standard setting, hC+ = iff hC = , i. e., relaxed planexists iff critical-path component heuristic solvable. hC behaves like h1role deciding relaxed plan existence. then, hC also fulfill role h1relaxed plan extraction, i. e., finding necessarily optimal relaxed plan?Implicitly, already done C compilation, via relaxed plan extraction h1 (C ) = hC , construction wasteful computing hC actuallyrequire exponential blow-up inherent C . make without blow-up?284fiC OMBINING h+ hm : IRECT C HARACTERIZATIONindicated introduction (in observation (2)), answer yes, formheuristic function denote hCFF : Relaxed plan extraction obtain hFF h1 bestsupporter function translates relaxed plan extraction obtain hCFF hC best-supporterfunction. Thanks direct formulation, using compilation, hCFF computes relaxedplans C time polynomial |C |.spell detail, start Section 4.1 by, similarly before, reformulatingstandard relaxed plan extraction way preparing generalization arbitrary conjunction sets C. Section 4.2 specifies generalization proves correct. Section 4.3analyzes subgoal-support selection problem, benign standard settingNP-complete general case; define heuristic function hCFF using greedysolution problem.Throughout, use equation-based formulations generalize directly arbitrary action costs. improve readability, also include pseudo-code formulationsapply simpler unit-cost case. Like before, distinguish variC ).ants taking cross-context conditions account (C ) vs. (nc4.1 Relaxed Plan Extraction h1Relaxed plan extraction first formulated terms best-supporter functions Keyder Geffner (2008). advantage traditional relaxed planning graph formulations (Hoffmann & Nebel, 2001) best-supporter functions flexible,allowing use hadd instead h1 , generalizing arbitrary action costs. shallsee, best-supporter formulation also generalizes easily use hC instead h1 .Best-supporter functions map facts actions. Based h1 , fact p mappedaction achieving h1 ({ p}), i. e., achieving minimum h1 equation (Equation 6,page 279). unit-cost actions, latter equivalent h1 (pre( a)) = h1 ({ p}) 1.hence write Keyder Geffners formulation relaxed plans FF input task= (F , A, , G) FF := pG ( g), function facts p satisfies:( p) =pI(q) { a} A,p add( a), h1 (pre( a)) = h1 ({ p}) 1 elseqpre( a)(8)easy see action set FF sequentialized form relaxed plan .generalization arbitrary cost done working h1 (pre( a)) = h1 ({ p})c( a) instead, using modified action-costs function c0 := c + e, e > 0, case0-cost actions (e principle chosen preserve optimality;minor concern relaxed-plan heuristic functions inadmissible anyway).Note special case h1 (pre( a)) = p add( a), henceh1 ({ p}) = . situation, p best supporter Keyder Geffnersformulation. formulation, ( p) undefined (i. e., notation equal1). abstract issue throughout present subsection, assumingh1 ({ p}) < facts. deal issue extension conjunctionsets C, partial function.Note furthermore intentionally specify function satisfies Equation 8. relaxed plan FF unique tie-breaking. Keyder Geffners285fiF ICKERT & H OFFMANN & TEINMETZformulation moves tie-breaking definition best supporters. findconvenient, purposes here, make tie-breaking explicit part equations (i. e., Equation 8 relaxed-plan equations below).Towards generalization arbitrary C, first change Keyder Geffners equation account positive side effects, extent supporting, action,open subgoals action best supporter. Reformulating Equation 8end, obtain FF := (G), function fact sets G satisfies:GI0(( G \ G ) pre( a)) { a} A,(9)(G) =6= G 0 = { p G | p add( a), h1 (pre( a)) = h1 ({ p}) 1} elseCompared Equation 8, need recurse single facts sets facts,recursive call knows open facts select entire best-supportedsubset thereof.6Equation 9 correspondence typical relaxed planning graph based implementations, depicted Algorithm 1. definition G 0 equation correspondsmaintenance TRUE flags facts relaxed planning graph layers, uponselecting action layer add effects marked TRUE (to see this, observe that, h1 (pre( a)) = 1, h1 (pre( a)) = h1 ({ p}) 1 iff h1 ({ p}) = i).extend Algorithm 1 relaxed plan extraction hC below. Note Algorithm 1deviates bit common descriptions, explicitly including computation h1instead assuming input relaxed planning graph caching outcome computation. simplify notation tie easily extension below.step Equation 8 Equation 9 benign standard setting, sensepractical impact expected small: single action typically addmany open facts, i. e., support many open atomic subgoals. Yet stepparamount importance generalization atomic subgoals arbitrary conjunctionsC. general setting, atomic subgoals typically overlap, supporting subgoalmeans add part it, may well case many subgoals.finally need formulate delete relaxation, terms relaxing regression semantics, terms splitting subgoals singleton facts, consideringcorrect regression semantics separately respect singleton-factsubgoals. words, need use formulation underlying h+ Equation 5.reminder convenience, equation is: h( G ) =0GI1 + minaA,6=G0 ={ p| pG,R({ p},a)6=} h(( G \ G 0 ) pG0 R({ p}, a)) elsesimilar transformation step Equation 9, obtain FF := (G),function fact sets G satisfies ( G ) =GI00(( G \ G ) Gr ) { a} A,(10)6= G 0 = { p | p G, R({ p}, a) 6= , h1 ( R({ p}, a)) = h1 ({ p}) 1} else6. Note selection dynamic function open facts, opposed up-front designbest-supporter function sharing supporting actions much possible. importantstandard setting here. Yet, discuss detail below, become important using arbitraryconjunctions C atomic subgoals.286fiC OMBINING h+ hm : IRECT C HARACTERIZATIONAlgorithm 1: Relaxed plan extraction h1 .123456789101112compute h1 ({ p}) p F:= max pG h1 ({ p})=return:= m, . . . , 1Gi := { p | p G , h1 ({ p}) = }:=:= m, . . . , 1ex. p Gi s.t. p TRUEselect 6= { p Gi | p TRUE } add( a),h1 (pre( a)) = 1foreach p Gi add( a)mark p TRUE14foreach q pre( a)Gh1 ({q}) := Gh1 ({q}) {q}15:= { a}1316returnGr0 defined Gr0 := pG0 R({ p}, a).Relative Equation 9, simple reformulation, using regression notationtrivializes singleton conjunctions. particular, subgoal Gr0 = pG0 R({ p}, a) generated action simplifies pre( a) here. Relative Equation 5, instead heuristicvalue, compute relaxed plan. Instead minimizing action choicescorresponds h+ , impose use best supporters corresponds relaxedplan extraction h1 .4.2 Relaxed Plan Extraction hCEquation 10, obtain CFF similar generalizations made get h+hC+ . Extending hC sets G conjunctions hC ( G ) := maxcG hC (c), definitionreads:Definition 3 Let = (F , A, , G) planning task, C set conjunctions containing singleton conjunctions. hC -based critical-path delete-relaxed plan, short C-relaxedplan, set CFF action occurrences ( a, G 0 ) CFF = (G C ), partialfunction conjunction sets G defined G C satisfies ( G ) =c G : cC000(( G \ G ) Gr ) {( a, G )} A,(11)6= G 0 {c | c G, R(c, a) 6= , hC ( R(c, a)) = hC (c) 1},hC ( Gr0 ) = hC ( G 0 ) 1else287fiF ICKERT & H OFFMANN & TEINMETZGr0 defined Gr0 := cG0 R(c, a).hC -based cross-context critical-path delete-relaxed plan, short nc-C-relaxed plan,CFF action occurrences property, except define G 0 : = { R ( c, ) |set ncr0c G }.definition parallels definition hC+ (Definition 2). subgoaling structuresame, sets conjunctions C must achievedregression. Instead heuristic value, compute relaxed plan (consisting actionoccurrences, action plus supported subgoals G 0 hC+ C , opposed actionsstandard case). Instead minimizing action occurrence choices, imposeuse best supporters according hC . major new source complexity, relativeEquation 10, allow G 0 subset best-supported atomic subgoals,similarly Definition 2. relaxed plan always exist, allowpartial define CFF defined goal. show below, possibleiff hC < , i. e., C-relaxed plan exists iff relaxed plan C exists.Observe that, relative Equation 10, added new additional conditionhC ( Gr0 ) = hC ( G 0 ) 1. understand condition, consider (a) Gr0 = cG0 R(c, a)union regressions individual supported subgoal c G 0 , (b)every subgoal c G 0 hC ( R(c, a)) = hC (c) 1, i. e., action selectedbest supporter c. (b), one would surmise hC ( Gr0 ) = hC ( G 0 ) 1,regressions R(c, a) Gr0 one step easier solve original counterpartsc G. so, however, cross-context conditions: otherwise,union (a) may difficult achieve components. get backdetail Section 4.3. now, keep mind additional conditionhC ( Gr0 ) = hC ( G 0 ) 1 required due possible cross-context conditions. (We remarkcondition equivalent hC ( Gr0 ) < hC ( G 0 ), hC value cannot decrease1 single regression step; written hC ( Gr0 ) = hC ( G 0 ) 1 merely usespecific write-up.)instructive consider CFF procedural perspective. Algorithm 2 providescorresponding extension Algorithm 1. previously computed h1 facts,compute hC conjunctions C. previously subgoal sets Gisets facts, sets conjunctions C. previously new subgoalsgenerated selected actions precondition facts, atomic conjunctions contained regressed subgoal Gr0 . Note pointwise interpretation ncC-relaxed plans, Gr0 = { R(c, a) | c G 0 } set fact sets. line 10, selectaction occurrence ( a, G 0 ) instead action a, resulting additional choicesupported atomic subgoals G 0 , accordingly complicated structure regressed subgoal Gr0 . Note that, every c Gi , hC (c) = and, actiona0 , hC ( R(c, a0 )) hC (c) 1 = 1. Furthermore, hC ( Gr0 ) = 1 hC ( R(c, a)) 1every c G 0 . Putting observations together, get hC ( R(c, a)) = hC (c) 1every c G 0 , choice G 0 Algorithm 2 equivalent Equation 11.Example 6 Consider, Example 5 (page 281), car-driving example C containingsingleton conjunctions well c = carY fuel. hC ({carX }) = 0, hC ({fuel}) = 0,hC ({carY }) = 1, hC ({carY, fuel}) = 2, hC ({carZ }) = 3.288fiC OMBINING h+ hm : IRECT C HARACTERIZATIONAlgorithm 2: Relaxed plan extraction hC . C-relaxed plan, use Gr0 =00c G 0 R ( c, ); nc-C-relaxed plan, use Gr = { R ( c, ) | c G }.123456789101112compute hC (c) c C:= maxcC,cG hC (c)=return:= m, . . . , 1Gi := {c | c C, c G , hC (c) = }:=:= m, . . . , 1ex. c Gi s.t. c TRUEselect ( a, G 0 ) A, 6= G 0 {c Gi | R(c, a) 6= , c TRUE },hC ( Gr0 ) = 1foreach c0 G 0mark c0 TRUE14foreach c0 C s.t. ex. c Gr0 c0 cG h C ( c 0 ) : = Gh C ( c 0 ) { c 0 }15:= {( a, G 0 )}1316returnTracing Equation 11 initializing call CFF = ({carZ }), get exactrecursive development Example 5. First, carZ supported ( aYZ , {carZ }),hC ({carY, fuel}) = 2 = 3 1 = hC ({carZ }) 1 required supported conjunctionc = carZ. Similarly, hC ( Gr0 ) = hC ( G 0 ) 1 Gr0 = {carY, fuel} G 0 = {carZ } singlesupported conjunctions difference. recursive subgoal {carY, fuel, carY fuel}.supporting action carY fuel arefuel . action best supporter carY fuelhC ({carY }) = 1 = hC ({carY, fuel}) 1. action best supporter fuel though,fuel true initially hC ({fuel}) = 0. possible choice supporting carY fuelarefuel G 0 := {carY fuel}. get Gr0 = {carY } hC ( Gr0 ) = 1 = hC ( G 0 ) 1desired. recursive subgoal {fuel, carY }, supported ( XY , {carY }) yielding Gr0 = {carX }hC ( Gr0 ) = 0 = hC ( G 0 ) 1.Taking procedural perspective Algorithm 2, start inserting carZ G3 . layer= 3 support ( aYZ , {carZ }) Gr0 = {carY, fuel} hC ( Gr0 ) = 2 = 1.Similarly, layers 2 1 mirror exactly respective recursive invocations Equation 11.next prove C-relaxed plans nc-C-relaxed plans indeed correspondC . start C-relaxed plans:relaxed plans C respectively ncTheorem 4 Let = (F , A, , G) planning task, C set conjunctions containingsingleton conjunctions. C-relaxed plan CFF sequentialized form relaxedplan C .289fiF ICKERT & H OFFMANN & TEINMETZProof Sketch: Sequencing CFF h( a0 , G00 ), . . . , ( an1 , Gn0 1 )i inverse order actionoccurrence selection Equation 11, i. e., placing outcome recursive invocationsfront, h a0 [ G00 ], . . . , an1 [ Gn0 1 ]i relaxed plan C . easy show inductionlength sequence. G0 , . . . , Gn recursive subgoals generatedEquation 11, si state applying ai [ Gi0 ] C , holds {c | c Gi }si . obvious = 0. holds i, also holds + 1 (a) Gi+1 \ Gi0part Gi+1 also part Gi hence true induction hypothesis; (b) Gi0 partGi+1 made true ai [ Gi0 ], applicable si induction hypothesisprecondition conjunctions contained Gi .almost identical proof shows corresponding property nc-C-relaxed plans:Theorem 5 Let = (F , A, , G) planning task, C set conjunctions containingCFF sequentialized form relaxedsingleton conjunctions. nc-C-relaxed plan ncC.plan ncC exists C-relaxed planFinally, relaxed plan C respectively ncrespectively nc-C-relaxed plan exists. simply propertiesfully determined critical-path component. proof shows via derivingintermediate equation, Equation 12 below, characterizes behavior CFFCFF restricting choice supported subgoal sets G 0 singletons. Equation 12ncplay important role comparison related work, experiments.Theorem 6 Let = (F , A, , G) planning task, C set conjunctions containingsingleton conjunctions. C-relaxed plan exists nc-C-relaxed plan existshC < .Proof Sketch: show claim two parts, (a) C-relaxed plan existshC < , (b) nc-C-relaxed plan exists hC < .directions follow corollaries (a) Theorems 3 4 respectively (b) Theorems 3 5:hC = , neither C-relaxed plan nc-C-relaxed plan exist, otherwiseC would exist Theorem 4 respectively Theorem 5,relaxed plan C respectively ncC ) per Theorem 3.contradiction hC = = h1 (C ) = h1 (ncCFF restricting choicedirections, consider versions CFF ncsupported subgoal sets G 0 singletons, i. e., single conjunctions G 0 = {c}. CFFCFF simplifiesnccG C ( c ), (.) partial function conjunctionsc satisfiescI0 ) {( a, { c })} A,(c(c) =(12)0Cc R(c,a)CCR(c, a) 6= , h ( R(c, a)) = h (c) 1 elseNote similarity Equation 8 (page 285): back common notationrelaxed plan extraction (over C instead singleton facts), extracting best supportersone-by-one.changing subgoaling structure, one transform Equation 12 form(G) partial function fact sets G satisfies290fiC OMBINING h+ hm : IRECT C HARACTERIZATIONGI( R( G, a)) {( a, G )} A,(G) =R( G, a) 6= , hC ( R( G, a)) = hC ( G ) 1 G C0elseG 0 G,G 0 C ( G )(13)Comparing hC equation (Equation 3, page 278), clear subgoalingstructure two equations coincides subgoals c hC (c) < , particular,hC < Equation 13 solution defined G . Therefore, Equation 12solution defined c G C . Equation 12 captures restricted version CFFCFF , C-relaxed nc-C-relaxed plans exist desired.nc4.3 Subgoal-Support Selection Problemfar shown C-relaxed plans nc-C-relaxed plans extracted.CFF defined. Givenyet explained actual heuristic functions hCFF hncCFFCFFCTheorem 6, hhnc return case h = . case hC < ,description relaxed plan extraction far specify choose supportedCFF .subgoal sets G 0 . Taking choice particular ways yields functions hCFF hncchoice non-trivial number possible action occurrences worst-caseexponential |C |. refer choice subgoal-support selection problem.start discussing optimization objective problem. fix soluCFF CFF order, defining desired heuristics hCFF hCFFtions, ncnccorresponding specializations Equation 11. close section brief discussionprior work light findings.4.3.1 PTIMIZATION BJECTIVEAssume hC < . argued proof Theorem 6, know Equation 12solution, principle could restrict | G 0 | = 1, resulting |A| |C |different action occurrence choices. However, result dramatic overestimation:Example 7 Consider task = (F , A, , G) F = { g1 , . . . , gn }, = , G ={ g1 , . . . , gn } contains single action whose precondition delete list emptywhose add list { g1 , . . . , gn }. Obviously, h = h+ = hFF = 1. However, even C containing singleton conjunctions { gi }, Equation 12 results dramatic overestimation:C-relaxed plan collect separate occurrence ( a, { gi }) every gi , resulting relaxed planlength n. C also contains fact-pair conjunctions { gi , g j } get C-relaxed plan sizen(n1)n+. general, get C-relaxed plan size |C |.2extreme example, similar situations arise whenever conjunctions overlap, action adding single fact p possible supporter conjunctions containp. With, e. g., conjunctions containing fact pairs, means number top-levelgoal conjunctions supported least |G|. domains many top-level goal facts including current IPC benchmarks and, generally, e. g. typical transportation, construction,puzzle problems clearly detrimental. (Replacing G single fact new goal-achieveraction moves problem precondition action.)291fiF ICKERT & H OFFMANN & TEINMETZessentially observation made Haslum (2009, 2012), non-admissibilityh+ (m ) every conjunction must achieved separately, prompted designC every action may achieve arbitrary subsets conjunctions. newparticular context consider issue, namely choice G 0 CFFCFF per Equation 11: moved issue generic planning-task levelncspecific subgoal-support selection level. specific perspective identifiesprecise source complexity, far relaxed plan extraction concerned: choosesets G 0 Equation 11 equivalently, implement line 10 Algorithm 2 manneravoiding overestimation extent possible?intuitive answer question, given Example 7, certainly choose G 0 largepossible. intuition entirely correct. detail Example 9 (Appendix A),cases supporting conjunction c, even though feasible, better donelater recursion, action whose precondition easier combine c.Nevertheless, employ | G 0 | maximization here, deeming safe presume overlapping conjunctions per Example 7 much practically relevant contrivedsituations per Example 9.convenient introduce terminology feasible choices G 0 . perEquation 11, possible choices G 0 best supporter everyc G 0 , i. e., hC ( R(c, a)) = hC (c) 1, overall regressed subgoal feasible,hC ( Gr0 ) = hC ( G 0 ) 1. case, say CFF context, i. e., Gr0 = cG0 R(c, a),CFF context, i. e., G 0 = { R ( c, ) | c G 0 }, G 0G 0 C-feasible. say ncrnc-C-feasible.maximization problems are:Definition 4 C-SubgoalSupport denote following problem:Given planning task , set conjunctions C containing singleton conjunctions,G C, action , K N. exist G 0 {c G | R(c, a) 6= , h1 ( R(c, a)) =h1 (c) 1} G 0 C-feasible | G 0 | K?define nc-C-SubgoalSupport accordingly nc-C-feasible G 0 .CFF H EURISTIC4.3.2 hncCFF , i. e., nc-C-SubgoalSupport, easysubgoal-support selection problem nc0solve. Indeed, choice G nc-C-feasible:Proposition 2 Let planning task, C set conjunctions containing singletonconjunctions, G C, action . G 0 {c G | R(c, a) 6= , h1 ( R(c, a)) =h1 (c) 1} nc-C-feasible.Proof: definition, G 0 nc-C-feasible hC ( Gr0 ) = hC ({ R(c, a) | c G 0 }) = hC ( G 0 )1. Now, hC ({ R(c, a) | c G 0 }) = maxcG0 hC ( R(c, a)) construction equalsmaxcG0 (hC (c) 1). latter equals (maxcG0 hC (c)) 1 = hC ( G 0 ) 1 desired.CFF , additional condition hC ( G 0 ) = hC ( G 0 ) 1 Definition 3words, ncrredundant. maximize | G 0 |, simply include G 0 c hC ( R(c, a)) =CFF as:hC ( G 0 ) 1. Accordingly, define heuristic function hnc292fiC OMBINING h+ hm : IRECT C HARACTERIZATIONDefinition 5 Let = (F , A, , G) planning task, C set conjunctions conCFF =taining singleton conjunctions. nc-C-relaxed plan heuristic defined hncCFF = | CFF | CFF = (G C ) satisfies ( G ) =hC = , otherwise hncncncc G : cC000(14)(( G \ G ) Gr ) {( a, G )} A,0CC6= G = {c | c G, R(c, a) 6= , h ( R(c, a)) = h (c) 1} elseGr0 defined Gr0 := { R(c, a) | c G 0 }.words, restrict Equation 11 maximal choice G 0 middle case, usingG 0 = {c G | R(c, a) 6= , hC ( R(c, a)) = hC (c) 1} instead G 0 {c G | R(c, a) 6=C variant regressed subgoal G 0 ,, hC ( R(c, a)) = hC (c) 1}. use ncrC0C0drop condition h ( Gr ) = h ( G ) 1 redundant variant.4.3.3 hCFF H EURISTICMatters simple CFF , i. e., C-SubgoalSupport, requires C-feasible setsCFF setting trivialG 0 opposed nc-C-feasible ones. feasible choice G 0 ncCFF ignores cross-context conditions. ignoring conditions,(Proposition 2) ncCFF, longer true:Example 8 Consider, Example 5 (page 281), abstract example conjunctions cq1p =q1 p, cq2p = q2 p, cq1q2 = q1 q2 . supporting goal facts, getsubgoal G = {q1 , q2 , p, q1 p, q2 p}. Ignore, like Example 5, subsumed subgoals q1 , q2 , ptackled side effect tackling non-subsumed ones q1 p q2 p.possible supporting action latter subgoals p adds p (q1 true initially,action adding q2 deletes p cannot support q2 p). three possible choices G 0 :0 : = { q p, q p }, G 0 : = { q p }, G 0 : = { q p }.G12221121G10 , Gr0 = cG10 R(c, a) = {q1 } hC ({q1 }) = 0 = hC ({q1 , p}) 1. G10 CSfeasible. G20 , Gr0 = cG20 R(c, a) = {q2 } hC ({q2 }) = 1 = hC ({q2 , p}) 1. G200 C-feasible, G 0 =0 R ( c, ) = { q1 , q2 },C-feasible well. However, G12c G12rcorresponding atomic conjunction q1 q2 . Selecting atomic subgoals q1 p q2 p,even though feasible individually, incurs cross-context condition q1 q2 , atomicconjunction present regression either q1 p q2 p individually. example0 )1 =constructed hC ({q1 , q2 }) = , hence particular hC ({q1 , q2 }) = 6= hC ( G12Ch ({{q1 , p}, {q2 , p}}) 1 = 1.0 CFF , get G 0 = { R ( c, ) | c G 0 } = {{ q }, { q }} instead.Note that, G1221ncr12words, get set containing two small conjunctions q1 q2 , instead set containingone big conjunction q1 q2 . hC ({{q1 }, {q2 }}) = 1 = hC ({{q1 , p}, {q2 , p}}) 1,0 (not C-feasible but) nc-C-feasible.G12example shows, cross-context conditions may render particular combinations supported conjunctions G 0 infeasible. used formulation, comesurprise maximizing | G 0 | avoiding combinations computationally hard:Theorem 7 C-SubgoalSupport NP-complete.293fiF ICKERT & H OFFMANN & TEINMETZProof Sketch: Membership guess check. Hardness via reduction Hitting Set:Given set elements E collection subsets b E elements, constructionthat, particular point C-relaxed plan extraction, choosing G 0 amountschoosing E0 E, E0 C-feasible (results hC value 6= ) iff exists bb E0 . Given this, E0 \ E hitting set, maximizing | E0 | equivalent findingminimum-size set.hard find cardinality-maximal feasible set supported conjunctionsCFF . Presuming want invest effort solve problem exactly(many times extraction C-relaxed plan every search state), needapproximate solution. canonical choice approximating cardinality-maximalitysubset-maximality. say G 0 {c G | R(c, a) 6= , h1 ( R(c, a)) = h1 (c) 1}subset-maximally C-feasible G 0 C-feasible and, every G 00 G 0 ( G 00 {cG | R(c, a) 6= , h1 ( R(c, a)) = h1 (c) 1}, G 00 C-feasible. heuristic function hCFFdefined using corresponding restriction Equation 11:Definition 6 Let = (F , A, , G) planning task, C set conjunctions containing singleton conjunctions. C-relaxed plan heuristic defined hCFF = hC = ,otherwise hCFF = | CFF | CFF = (G C ) satisfies ( G ) =c G : cC000(( G \ G ) Gr ) {( a, G )} A,(15)6= G 0 {c | c G, R(c, a) 6= , hC ( R(c, a)) = hC (c) 1},G subset-maximally C-feasibleelseGr0 defined Gr0 :=c G0R(c, a).subset-maximally C-feasible set G 0 found simple greedy algorithms,adding conjunctions one-by-one shown Algorithm 3. candidate conjunctionsc G R(c, a) 6= hC ( R(c, a)) = hC (c) 1. Starting empty G 0 ,try candidate c exactly once. suffices get subset-maximal G 0 because, G 0grow, adding c feasible first time around adding c cannotfeasible later either.Algorithm 3: Greedy selection subset-maximally C-feasible set supported subgoals G 0 C-relaxed plan extraction. Implements line 10 Algorithm 2 obtainheuristic function hCFF .1 select c Gi , c TRUE2 select R ( c, ) 6 = hC ( R ( c, )) = hC ( c ) 13 G0 := {c}4 foreach c0 Gi s.t. c0 TRUE i, R ( c0 , ) 6 = , hC ( R ( c0 , )) = hC ( c0 ) 15G 0 {c0 } C-feasible6G 0 := G 0 {c0 }remark that, Example 9 (Appendix A) shows, cases selectingnon-subset-maximally C-feasible G 0 leads strictly smaller C-relaxed plan.words, like cardinality maximization, subset-maximization fail-safe.294fiC OMBINING h+ hm : IRECT C HARACTERIZATION4.3.4 P REVIOUS W ORKS R ELATED UBGOAL -S UPPORT ELECTION P ROBLEMInterestingly, subgoal-support selection problem never previously identified, already solved. plainly put, previous works areaviewed solving problem abstract level, identifying preciselyproblem is, thus ending solutions solve problem, using unnecessarily drastic measures. Mostly due compilation view, relaxedplan extraction becomes standard technique, yet subgoal-support selection problemsolved STRIPS level, form compiled task. single noncompilation-view prior work, Alcazar et al. (2013), conducted part muchbroader scope, address subgoal-support selection problem detail.Let us closer look Alcazar et al.s heuristic, FFm (which implement= 2). extracts relaxed plan hm , restricting C contain exactly conjunctions size m. restriction easily removed. notation, FFm correspondsC-relaxed plan extracted using equation: FFm = cG C (c)cI0(c) =(16)0C ( c ) { } A,c R(c,a)R(c, a) 6= , hC ( R(c, a)) = hC (c) 1 elseCFF simplify reThis almost exactly definitions CFF nc0stricting choice G support single conjunction G 0 = {c}, i. e., almostidentical Equation 12 derived proof Theorem 6. Repeating Equation 12CFF =convenience: CFF = nccG C ( c )cI0 ) {( a, { c })} A,(c(c) =0Cc R(c,a)R(c, a) 6= , hC ( R(c, a)) = hC (c) 1 elsedifference two equations FFm collects set actionsCFF collect set (single-supported-subgoal) actionstandard setting, CFF ncoccurrences.sense, Alcazar et al.s approach over-simplifies choice G 0 , singleton sets.C rather C because, | G 0 | = 1, cross-context conditionseffectively tackles ncnever occur. would furthermore run risk excessive overestimation pointedExample 7 actually collect action occurrences, rather actions. lattermight viewed trick avoid overestimation, yet theoretical perspectiverather defeats purpose using explicit conjunctions first place. Whereas relaxedplanning C converges h , FFm bounded number actions, |A|.Altogether, findings allow understand prior work subject follows:0C Compilation (Haslum, 2012): Includes one compiled action aG every possiblepair action possible set supported subgoals G 0 . sense, solvesNP-complete problem C-SubgoalSupport enumeratively, in-memory.7Lesson learned hCFF : need pre-generate possible conjunction subsetsaction could support. focus subgoals actually arise relaxed planextraction.07. Plus, without actually giving optimality guarantee: optimal aG set choicesrelaxed plan extraction/the best-supporter function, guarantee selected.295fiF ICKERT & H OFFMANN & TEINMETZC Compilation (Keyder et al., 2012, 2014): Includes one conditional effect everycepair action possibly supported conjunction c. ignores cross-contextconditions hence trivializes C-SubgoalSupport nc-C-SubgoalSupport.Lesson learned hCFF : need ignore cross-context conditions completely.greedily select supported conjunctions whose cross-context conditions feasible.FFm (Alcazar et al., 2013): Restricts conjunction set C size- conjunctionshm . Restricts supported subgoals G 0 single conjunctions, thus trivializingC compilation.C-SubgoalSupport ignoring cross-context conditions like ceCollects actions instead action occurrences, losing convergence h .Lesson learned hCFF : weaknesses avoided.5. ExperimentsCFF heuristic functions relative closelyevaluate benefits hCFF hncrelated previous heuristics. state key issues consider terms fourhypotheses, formulating major expectations regarding algorithm behavior IPC benchmarks, standard means evaluation planning community:8(H1) hCFF relative hFF (C ), hypothesis (H1) avoiding exponential blowup |C | typically yields faster heuristic thus improved performance.C ), hypothesis (H2) accounting cross-context(H2) hCFF relative hFF (ceconditions yield informed heuristic thus improved performance.difference typically (H1) vs. (H2) intended. Crosscontext conditions presumably important particular cases, whereasadvantage hCFF smaller representation presumably helps cases.CFF relative hFF ( C ), hypothesis (H3) implemen(H3) hCFF hncceCFF typically effective thus yields improved performance.tation hCFF hncCFF direct, using compilation, thusexpect hCFF hncFFC ).specialized h (ceCFF hFF ( C ) equivalent except implementation,Note hncceuse information scaling behavior |C |. (Incontrast comparison hCFF vs. hFF (C ), dominated (H1)drastically different scaling behavior |C |.)(H4) furthermore compare hCFF variant denote hCFF, per Equation 12| G 0 |=10restrict | G | = 1, hypothesis (H4) non-trivial subgoalsupport selection hCFF typically yields informed heuristic thus improved performance.finally include variant denote hCFF, per Equation 16 | G 0 | = 1| G 0 |=1Aset actions (as opposed action occurrences) selected. serves comparisonAlcazar et al.s (2013) work.8. hypotheses intended formal statements statistically accept reject;intended exhaustive representation issues discuss. merely serve redthread discussion large-scale experiments.296fiC OMBINING h+ hm : IRECT C HARACTERIZATIONSection 5.1 describes key points implementation, Section 5.2 explainsexperimental setup. Section 5.3 provides comprehensive results, across planner variants,small conjunction sets C turn best terms overall performance.Section 5.4 analyzes behavior function growing C.5.1 ImplementationCFF FD (Helmert, 2006). furthermore implementedimplemented hC , hCFF , hncCaddC-additive heuristic h, defined exactly like hC (Equation 3) except maximization atomic subgoals replaced summation subgoals:GI01 + minaA,R(G,a)6= h( R( G, a)) G C(17)h( G ) =0elseG0 G,G0 C h( G )words, step hC hCadd parallels hmax hadd (Bonet & Geffner,2001). dont use hCadd heuristic function per se: contrast standard setting,atomic subgoals overlap general case, summation doesnt make sense.use hCadd alternative best-supporter function relaxed plan extraction.purpose, turns fairly useful empirically.computing critical-path heuristics becomes expensive many conjunctions,key practicality efficient implementation hC . end, extend counterbased algorithm originally implemented FF (Hoffmann & Nebel, 2001) computing h1(aka relaxed planning graph). extended algorithm easily described modification original algorithm. Assume input state s. FFs original algorithm associatesprecondition action counter, denoted count( a), initialized|pre( a)|. Facts p maintained priority queue ordered associated v( p) value,equals h1 ( p) p dequeued. queue initialized facts ptrue s, associated value v( p) = 0. main loop dequeues facts, activatesnew actions, maintains v values. fact p dequeued, loop actionsp pre( a) decrements count( a). results count( a) = 0 action activated, enqueuing every q add( a) value v(q) = 1 + max p0 pre(a) v( p0 ), reducingv(q) value case q already queue higher value.9 algorithmstops either goal facts dequeued h1 (s) = max pG v( p), queuebecome empty h1 (s) = .extension hC works much way. need maintain valuesv(.) conjunctions c C instead single facts, need maintain counters pairsaction supported conjunction instead actions. Precisely, create countercount(c, a) every c C R(c, a) 6= R(c, a) containmutex, i. e., fact pair known unreachable. latter corresponds mutex pruningC . reduces computational effort welldiscussed Keyder et al. (2014) C cestrengthens heuristic.extended algorithm, counter count(c, a) initialized |{c0 | c0 C, c0R(c, a)}|, i. e., number sub-conjunctions need make true orderable achieve c using (remember C contains singleton conjunctions).9. general action costs c( a), one simply use v(q) = c( a) + max p0 pre( a) v( p0 ) here.297fiF ICKERT & H OFFMANN & TEINMETZqueue contains conjunctions c0 C instead facts. initialized conjunctions c0 s, v(c0 ) = 0. Dequeueing conjunction c0 , loop counterscount(c, a) c0 R(c, a), decrementing count(c, a). results count(c, a) = 0,enqueue/upgrade c value 1 + maxc0 C,c0 R(c,a) v(c0 ).compute hCadd , use exact algorithm except maximization replaced summation, i. e., instead 1 + maxc0 C,c0 R(c,a) v(c0 ) use 1 + c0 C,c0 R(c,a) v(c0 ).Based conjunction values v(c) computed hC respectively hCadd , impleCFF follows Algorithm 2 (page 288). particular, selectmentation hCFF hncCFF fix G 0 = { c | caction/supported subgoals ( a, G 0 ) previously discussed. hncCFF0G, R(c, a) 6= , v( R(c, a)) = v(c) 1}. h , select G per Algorithm 3 (page 294).Here, v(c) values single conjunctions c readily available. Values v( X ) factset X required X := R(c, a), well check C-feasibility hCFF(Algorithm 3 line 5), X := c00 G0 {c0 } c00 G 0 current set supportedsubgoals c0 candidate inclusion set. compute v( X ) loopfacts p X, using lists C [ p] containing c C p c, maximizingrespectively summing v(c) c C [ p] c X.Helpful actions (Hoffmann & Nebel, 2001), i. e., FD preferred operators, definedsimilarly hFF . action applicable state preferred C-relaxed plancontains a, i. e., action/supported subgoals pair form ( a, G 0 ). correspondsC ), based compiled actionsselection preferred operators hFF (C ) hFF (cea[C 0 ] occurring relaxed plans respective compilations (Keyder et al., 2014).performance satisficing search planning known brittle respectminor differences heuristic functions (e. g. Valenzano, Sturtevant, Schaeffer, & Xie,2014). important also setting. unavoidable implementation differencesnew heuristics predecessors turn major complicationfair comparison. heuristics extract relaxed plans hC hCadd best-supporterC ) via compilation, hCFF hCFF not.function, yet hFF (C ) hFF (cencrelaxed plan extraction algorithms work different representations. particular,choice action hFF (C ), i. e., compiled action a[C 0 ], correspondschoice action/supported subgoals pair ( a, G 0 ) hCFF . design, hFF (C ) cannotdistinguish choosing vs. choosing G 0 . contrast, design hCFF chooses firstaction assembles G 0 greedy C-feasible maximization.offset unavoidable differences relaxed plan extraction, experiment acrossC , tiea variety tie-breaking strategies choice best supporters. C ceCFF applies actions supportbreaking applies compiled actions a[C 0 ], hCFF hncCCing conjunction c h ( R(c, a)) = h (c) 1 respectively hadd ( R(c, a)) =hadd (c) 1. strategies are:Arbitrary: Choose arbitrary best supporter, i. e., first one find. used (withC ).hadd best supporters) FDs implementation hFF , well hFF (C ) hFF (ceRandom: Choose random best supporter. use 3 different random seeds experiments gauge performance variance incurred criterion. turnsthat, almost cases, variance small performance change relativearbitrary tie-breaking consistent across random seeds, i. e., consistently positive298fiC OMBINING h+ hm : IRECT C HARACTERIZATIONconsistently negative. Thus random tie-breaking exhibits reliable behavioralgorithm option.Difficulty (hC only): tie breaking mechanism used FF (Hoffmann & Nebel,2001). selects best supporter minimizes summed-up hmax valuesCFF , translates summing hCsupporters preconditions. hCFF hncvalues conjunctions contained regressed subgoals R(c, a). Remainingties broken arbitrarily.use 3 tie-breaking strategies hC , first 2 strategies hCadd ,CFF , hFF ( C ), hFF ( C ), hCFF , hCFF.6 heuristics hCFF , hncce| G 0 |=1| G 0 |=1A5.2 Experiments Setup Designable compare different heuristic functions directly, comparisonsuse conjunction set C every heuristic. find sets C using exactmethods, implementation, used Keyder et al. (2014). motivationcontribution pertain methods finding C, re-using established methods provides better comparability. understand experiments,necessary understand generation C detail, give brief summary only.Keyder et al.s (2014) method variant method proposed earlier Haslum(2012). pre-process actual search, C learned iteratively refining deleterelaxed plan initial state. Starting empty C, relaxed plan + C generated. + real plan (a plan original input task), process stops. Else,analysis step finds set C 0 new conjunctions exclude + , i. e., +longer relaxed plan C setting C := C C 0 . process iterates. Runningad infinitum, one eventually find plan input task. typicallyfeasible. find instead set C heuristic search, algorithm applies both, timelimit T, size limit x C relative action set size increase, i. e., |AC |/|A|. eithertwo criteria applies, process stops hands current set C search.(Each limit may set , meaning termination criterion disabled.)heuristics experiments use explicit conjunctions, useset C, separate generation C actual experiments. apply separateruntime limits C-generation search respectively, reportperformance search C-learning. Given this, merely serves meanskeep experiments feasible even large size limits x. fix 30 minutes.Throughout, use FDs lazy-greedy best-first search dual open queue preferred operators (Helmert, 2006), profits search space pruning affordedpreferred operators, yet preserves completeness keeping pruned nodessecond open queue. canonical search algorithm satisficing planningdelete-relaxation heuristics, widely used baseline yields competitive performancereasonably simple. (Textbook single-queue greedy best-first search lags far behind state art, either use preferred operators, loses solutionscases preferred operators restrictive.) experiments runcluster machines Intel Xeon E5-2660 processors running 2.2 GHz. memory limit set 4 GB. used benchmarks satisficing tracks tworecent International Planning Competitions, IPC11 IPC14. include299fiF ICKERT & H OFFMANN & TEINMETZ400000104hFF (C )hCFF3703601033000003503403301023202000003101013002901000002801002702600122223242526272829210101 110100(a)101(b)hCFFhFF (C ).10210310421hFF (hadd arb)hFF (hadd rnd)2223242526272829210(c)hCFFFigure 1: Data previewvs.(a) Number countersvs. numberCCactions |A | , function size limit x. (b) States per secondx = , x-axis hCFF , y-axis hFF (C ). (c) Total coverage function x. (b)(c), hCFF hFF (C ) run hCadd using random tie-breaking.(c), also include hFF baseline, two tie-breaking variants: haddarbitrary tie-breaking corresponds FD default, hadd random tiebreaking better performance benchmarks. Recall comparisoneffort C-learning included hCFF hFF (C ) (see text).IPC14 CityCar domain, FD translator generates actions conditionaleffects, supported implementation. domain, test suite 20 instances (some domains used IPC11 IPC14 two test suites).6 heuristic functions, 5 best-supporter definitions (hC vs. hCadd , tie breaking),numeric size-limit parameter x, experiments space large. motivateorganize exploration space follows, Figure 1 gives data preview.major benefit hCFF hFF (C ) better scaling |C |. One would expect manifest itself, large C, (a) smaller representation thus (b) fasterheuristic function. Figure 1 (a) (b) confirm indeed so.10 x = ,C contains conjunctions learned within 30 minutes, get speed-ups 14 ordersmagnitude. Now, good news, turns cases large C detrimental. search space size generally decrease increasing size limitx, heuristic functions also become slower. slowdown dramatic hFF (C ).much less dramatic hCFF , still typically enough outweigh search spacereduction. Figure 1 (c) shows effect: overall coverage becomes worse growingx, dramatically hFF (C ), benign manner still almost monotonicallyhCFF . best overall coverage often (across heuristics, configurations, domains)obtained x = 2, also best setting x Keyder et al.s (2014) experiments.hFF baselines Figure 1 (c) based hCFF using singleton conjunctions (x = 1), better comparability methods, 5 tiebreaking strategies disposal. comparison baselines, hCFF consistently out10. Figure 1 (a), x = 2 number counters hCFF exceeds |AC |. cannot happen theory,per Definition 1, C includes action a[{c}] every counter count(c, a). happenpractice due handling facts, i. e., actions original pre/add/del lists: Definition 1handles singleton conjunctions, implementation C uses effective special-casehandling per Haslums (2012) definition.300fiC OMBINING h+ hm : IRECT C HARACTERIZATIONhCFFhCdomainBarman11Barman14CaveDiving14ChildSnack14Elevators11Floortile11Floortile14GED14Hiking14Maintenance14Nomystery11Openstacks11Openstacks14Parcprinter11Parking11Parking14Pegsol11Scanalyzer11Sokoban11Tetris14Thoughtful14Tidybot11Transport11Transport14Visitall11Visitall14Woodworking11hCaddarb rnd dif arb rnd317211202020151071917165120171871311002020207019202020171361917102017201816713152018420837141820202014101019151381202017891362302083722020202014971915128420201781014129164201277120202020161472016121492020178111613717420282 340 318 338 368hFF (C )hadd (C )arb rnd dif arb rndh1 ( C )01728202020161172015122020181841310201542000701220202010106168121992018167101510134201070152020201611102015134020201831515521642014770192020201511122014121152019175121410716420107018202020131162016720202020178101911714420285 293 310 351349C)hFF (cehadd (Cce )arb rnd dif arbrndh1 (Cce )1172920202016117201482020181841313203020007012202020119616911168201916991730134202170152020201511112013104020201641515413020158702020202015111219129125201917411141010164202170192020201212620161020192020179101811914420269 295 287 350356hCCFFhnchCaddarb rnd dif arb rnd21721120202015107201716512018187121100202020702020202016126191610202020201791215201842083714182020201211101915147120201789135230201147220202020151072013128520201781014146184201377120202020151472017131492020177111613918420282 345 316 345 372C ), hCFF , differTable 1: Coverage results x = 2, hCFF , hFF (C ), hFF (cencCCaddent best-supporter functions (h vs. h) tie-breaking strategies. Best results highlighted boldface. Abbreviations: arb arbitrary tie-breaking; rndrandom tie-breaking (per-instance median seed, see also Table 2); dif difficultytie-breaking.performs even competitive, non-standard hFF variant random tie-breaking.hFF (C ), true small x values. Recall, however, reportperformance search, C-learning: data evaluates exclusivelymerits respective heuristic functions, overhead required obtainfirst place. stick rationale throughout, differences explicitconjunction heuristics contribution here. completeness, Appendix B showscoverage plots counting C-learning part solving effort, i. e., 30-minute limittime taken C-learning search together.Given typically detrimental effect large x, follows first (Section 5.3)explore case x = 2, examining detail space heuristic functions best supporters. Subsequently (Section 5.4), examine detail happens scale x.make latter experiments feasible, fix heuristic function performant best-supporter method; turn out, scaling x, methodevery heuristic, namely hCadd random tie-breaking.301fiF ICKERT & H OFFMANN & TEINMETZ5.3 Small C: Heuristics, Best Supporters, Tie Breaking x = 2examine first performance main heuristic functions, i. e., hCFF vs. comC ), well hCFF essentiallypeting previous variants hFF (C ) hFF (cencC ). discussperceived alternative, no-compilation, implementation hFF (ceCFFCFFCFFbehavior h|G0 |=1 h|G0 |=1A , relative h , below. Consider Table 1.striking observation data differences heuristic functionsdominated tie-breaking strategies. function tie-breaking, rangeC ), 282overall coverage 282368 hCFF , 285351 hFF (C ), 269356 hFF (ceCFF . relatively small role heuristic function differences, x = 2, makes372 hncCFF different scaling C, cross-context conditions,sense advantages hCFF hncnon-compilation implementation naturally impact larger C is.cases though even small C makes difference.Comparing tie-breaking strategies, hCadd best supporters superior hC best supporters, typically per domain almost consistently total. makes senseheuristics run risk over-estimation, hCadd better hC finding cheap relaxed plans. several cases combination heuristicCFF hC difficulty tietie-breaking method works exceptionally well, e. g. hCFF /hncCFF hC arbitrary tie-breaking Parcprinter11,breaking ChildSnack14, hCFF /hncFFCFFCaddh ( )/h (ce ) harbitrary tie-breaking Barman11. performance peaks consistent across tie-breaking methods respective heuristics,consider outliers caused brittleness search.Comparing heuristic functions h vs. h0 , way identifying strong advantagesconsider domains Table 1 h consistent advantage h0 , i. e., hleast good h0 tie-breaking methods, strictly better least one method.Call advantage strict h strictly better tie-breaking methods. comparison hCFF vs. hFF (C ), hCFF consistent advantage 5 domains (ChildSnack14,Elevators11, Openstacks14, Tetris14, Transport14), hFF (C ) consistentadvantage 2 domains (Sokoban11 Visitall14). advantage strict hCFFElevators: cases, tie-breaking methods work equally wellheuristics. Overall, despite noise data (somewhat) favor hCFF .C ) yields similar picture, 4 consistent (non-strict)comparison hCFF vs. hFF (ceCFFadvantages h(ChildSnack14, Elevators11, Openstacks14, Sokoban11) vs. 1 conC ) (Tidybot11). illuminating offsetsistent (non-strict) advantage hFF (ceCFFobservations data hnc : every domain hCFF consistent adC ), hCFF also consistent advantage hFF ( C ),vantage hFF (cencceCFF consistent disadvantage vs. hFF ( C ) hCFF ,domain hncceC )Tidybot11. Hence reason differences hCFF hFF (cecross-context conditions. Presumably, cross-context conditions occurspecific situations, small C play role.CFF .evidence towards conclusion comes comparison hCFF vs. hncActually, regarding cross-context conditions, comparison informativeC ): all, hCFF hCFF differ accounting respecthat hCFF vs. hFF (cenctively accounting cross-context conditions. terms consistent advantages,CFF , 8 consistent (non-strict) advantages hCFFcomparison clearly favor hncnc302fiC OMBINING h+ hm : IRECT C HARACTERIZATIONhCFFworst med bestdomains1s2s3Barman11Barman14CaveDiving14ChildSnack14Elevators11Floortile11Floortile14GED14Hiking14Maintenance14Nomystery11Openstacks11Openstacks14Parcprinter11Parking11Parking14Pegsol11Scanalyzer11Sokoban11Tetris14Thoughtful14Tidybot11Transport11Transport14Visitall11Visitall14Woodworking11138722020202018136201813149202016811151491742012571202020201413720161014112020179101612718420127721920202015137201612168202018712161171742010570192020201310620168137202016691497174201277120202020161472016121492020178111613717420158742020202018157201815171220201810131715918420375 363 366336368400hFF (C )worst med bestvars1s2s3130110004010232300222132100+5+5011000+4+41+1+32+8+70011+2+222+20030711820202013116201692020202017111117128144201071172020201386201692017202017910181161442011721920202013126191492019202017711171171342010701720202011561914620162020177814951342010701820202013116201672020202017810191171442031741920202015156201614202020201712141914914420358 344 349319349383var2 131 7001 +22 20000000 24 30 61 12 +20 30 +93 +15000 +1004 +61 21 +41 +22 11 30000Table 2: Coverage results x = 2, showing effect different random seedsrandom tie-breaking hCadd best-supporters. s1, s2, s3 denote3 random seeds (fixed throughout experiment). best, med,worst columns assess per-instance aggregation methods, selectingbest/median/worst seed per instance respectively. var assesses per-domainperformance variance, terms difference best worst coverage. assesses per-domain consistency performance change relativebaseline, i. e., relative hCadd best-supporters arbitrary tie-breaking.shows maximum absolute difference, + coverage betterseeds, coverage worse seeds, otherwise, i. e., coverageactually gets better worse depending seed.vs. 2 consistent (non-strict) advantages hCFF . However, examining closely,advantages small scale. Whereas, comparisons above, averagecoverage difference consistent-advantage domains typically 2 3,CFF usually 0.2 maximum 0.8.comparison hCFF hncconclude regarding experimental hypotheses, (H1) advantage hCFFC ) thanksthanks better scaling |C |, (H2) advantage hCFF hFF (ceCFF hFF ( C ) thankscross-context conditions, (H3) advantage hCFF hncceimplementation? Table 1 provides evidence favor (H1) (H3), thoughdomains, subject substantial noise tie-breaking. support (H2).evidence suggests that, x = 2, taking cross-context conditions accountneither substantial positive effects substantial negative effects.hFF (C )303fiF ICKERT & H OFFMANN & TEINMETZwords order regarding use random tie-breaking. crucial observations that, per domain, (a) variance random seeds typically small,(b) performance change relative baseline typically consistent. makes random tie-breaking comparable non-randomized algorithm options. said,domains either (a) (b) false, total differences would sum up.counteract per-instance aggregation method obtain per-instance datareduces variance relative individual seeds, interpolates seedsterms overall performance. per-instance median seed, 3 random seeds ranexperiments, turns suitable (for coverage, counts instance solvedleast 2 3 randomized runs solved it). used per-instance median seedFigure 1 Table 1, use cases random tie-breakingemployed. Table 2 shows data supporting observations design decisions.quick look var columns Table 2 confirms observation (a). differencebest worst per-seed coverage 2 except 5 domains hCFF3 domains hFF (C ). hand, looking bottom row comparingseeds, differences add up. would especially selectbest worst seed per instance (best worst columns), resulting coveragedifferences around 70 total. However, using median (med column) seedresults per-instance aggregation desired properties.Regarding observation (b), consider columns Table 2. domainsperformance relative baseline consistent, i. e., gets better worse dependingrandom seed, marked symbol. symbols sparsetable. 4 27 domains hCFF , 2 hFF (C ),randomization changes performance consistently. (It rarely deteriorates performancehCFF , hFF (C ) picture mixed depending domain.) showsclearly random tie-breaking reliable baseline.Indeed, findings contradict Keyder et al.s (2014) use random tie-breakingmeasure noise. Keyder et al.s idea account brittleness search randomizing baseline heuristic h (in case, hFF hadd best supporters arbitrarytie-breaking), measuring Table 2 yet ignoring distinctions +, , , i. e.,considering absolute maximum difference. deem heuristic h0 significantly better h improvement h larger intuitively, largerrandom noise. However, approach assumes random tie-breaking yields distribution around baseline average, much data. Considerexample hFF (C ) Barman11. According Keyder et al.s method, random noise13, heuristic significantly better hFF (C ) must henceincrease coverage least 14. noise effect random tie-breakingconsistently detrimental. Similar examples abound.conclude that, specific context experiments, Keyder et al.s measureappropriate random tie-breaking typically source noise. contrary, performance 3 separate runs random tie-breaking reliably reportedlike single planner run, per-instance median seed aggregation.Let us finally consider behavior hCFF relative hCFFtrivializes sub| G 0 |=1goal support selection, relative hCFFalso trivializes C-relaxed plan| G 0 |=1A(into set actions instead action occurrences). Table 3 shows data.304fiC OMBINING h+ hm : IRECT C HARACTERIZATIONhCFFhCdomainBarman11Barman14CaveDiving14ChildSnack14Elevators11Floortile11Floortile14GED14Hiking14Maintenance14Nomystery11Openstacks11Openstacks14Parcprinter11Parking11Parking14Pegsol11Scanalyzer11Sokoban11Tetris14Thoughtful14Tidybot11Transport11Transport14Visitall11Visitall14Woodworking11hCaddarb rnd dif arb rnd317211202020151071917165120171871311002020837141820202014101019151381202017891362302020701920202017136191710201720181671315201842083722020202014971915128420201781014129164201277120202020161472016121492020178111613717420282 318 340 338 368hChCFF| G 0 |=1hCaddarb rnd dif arb rnd22721020202010471917132120191679100012420317101720201910561917112020191741411411132000711820202014452020314320201714101220201120136712020202014662016820202017413141382072013472202020201356191572020201611141613720520273 291 315 335 335hCFF| G 0 |=1AhChCaddarb rnd dif arb rnd017282020201276201616422017168910101020317101220202011109191711302020171013123210200070192020201510520165201420191711916102092061722020202015108191671562020178141311920820117020202020141072016818132020161012159519520263 291 333 352 346Table 3: Coverage results x = 2, hCFF vs. hCFFhCFF, different best| G 0 |=1A| G 0 |=1supporter functions tie-breaking strategies. Best results highlighted boldface. Abbreviations Table 1.Like Table 1, lot noise due tie-breaking strategies. Note thoughheuristics shown work representation basedimplementation. differences subgoal support selection/relaxed plandefinition. Comparisons tie-breaking strategies across heuristics hence direct.data shows clear advantage hCFF hCFF. every tie-breaking strategy,| G 0 |=1hCFF strictly better total (the margin varying 3 hCadd arbitrary tiebreaking 33 hCadd random tie-breaking). Using per-domain comparisonacross tie-breaking strategies, hCFF consistent advantage 10 domains (3strict), consistent disadvantage 2 domains (both strict, namely twoVisitall variants). Comparing search space size states per second commonly solvedinstances, advantages hCFF partly due quality (e. g. Parking11 994.4 vs. 8, 091.1geometric mean state evaluations), supporting hypothesis (H4) non-trivialsubgoal support selection hCFF yields informed heuristic hCFF.| G 0 |=1also several cases advantage speed (e. g. Elevators11 4713.0 vs. 4591.1states per second). possible cause lies different states evaluated:hCFF easier evaluate, typically case states closer goal.(We remark that, like standard relaxed plan heuristic, heuristic function305fiF ICKERT & H OFFMANN & TEINMETZruntime, typically 90% more, spent computation best-supporter function,hC respectively hCadd case.)Relative hCFF, hCFF still advantage picture mixed.| G 0 |=1Aterms total coverage, hCFF dominant except hCadd arbitrary tie-breaking. Perdomain, hCFF consistent advantage 6 cases (1 strict), vs. consistent disadvantage4 cases (none strict). appears that, least setting x IPC benchmarks, relative hCFFprone over-estimation, trivialized C-relaxed plan| G 0 |=1hCFFresult better heuristic. Comparing hCFFvs. hCFFdirectly, hCFF| G 0 |=1A| G 0 |=1A| G 0 |=1| G 0 |=1Adominates total except hC arbitrary tie-breaking, consistent advantage 5 domains (3 strict), consistent disadvantage 3 domains (none strict).terms search space size states per second commonly solved instances, advantages hCFFmostly due quality (most notably Parking11, 1102.7 vs. 8091.1| G 0 |=1Astate evaluations), except Hiking14 hCFFfaster (526.7 vs. 639.8 states per| G 0 |=1Asecond).5.4 Scaling C: Performance Function xexamine search behavior C becomes larger. naturally presented termsplots performance measures function size limit x. Keep mind, especiallycomparison hFF baselines, C-learning conducted separatelysearch, separate 30-minute time limit. Appendix B shows coverageplots included below, counting C-learning part solving effort.following discussions, include brief summaries data.Figure 2 shows total coverage function x. Consider first Figure 2 (b) settles question competitive tie-breaking strategies. seenTables 1 2, total coverage x = 2 maximal using hadd random tie-breaking,almost heuristic functions. two exceptions hFF (C ) hCFF.| G 0 |=1Athese, hadd arbitrary tie-breaking works better hadd random tie-breakingx = 2 (2 instances solved hFF (C ), 6 instances solved hCFF). How| G 0 |=1Aever, Figure 2 (b) shows, advantage arbitrary tie-breaking x = 2 turnssubstantial disadvantage larger values x. Hence, remainder experiments, fix hadd random tie-breaking best-supporter method throughout.Consider Figure 2 (a). previously hinted (Figure 1 (c)), total coverage tendsdecrease function x. heuristics consistently outperform hFF baselinesthough, except hFF (C ) whose per-state runtime overhead drags coveragehFF x 32 (and except temporary dip hCFFhFF random| G 0 |=1tie-breaking x = 16).CFF , hFF ( C ), hFF ( C ) decrease, relatively speaking,Note hCFF , hncce5steeply x = 2 , less steeply afterwards. because, around point,many domains C-learning reaches time limit size limit. hFF (C )remaining domains still becomes substantially worse, heuristicsless pronounced. Observe coverage difference x = 2 x = 210 ,except hFF (C ), small, around 20 instances. Indeed, decrease causeddomains only, namely Barman, Parking, Sokoban, Visitall.306fiC OMBINING h+ hm : IRECT C HARACTERIZATION37037036036035035034034033033032032031031030029028027026025024021300CFFhnchCFF290C)hFF (ceFFh (C )hCFF| G 0 |=1AhCFF| G 0 |=1hFF (arb)hFF (rnd)222802702602502402324add(a) h252627282921021hFF (C ) (rnd)hFF (C ) (arb)hCFF(rnd)| G 0 |=1AhCFF(arb)0| G |=1A2223242526272829210(b) hadd , random vs. arbitrary tie-breaking, random tie-breakingFigure 2: Total coverage function size bound x. (a) heuristics using haddrandom tie-breaking (median per-instance seed; heuristic functions listed topdown order coverage x = 2). (b) hadd random vs. arbitrarytie-breaking hFF (C ) hCFF, two heuristics arbitrary| G 0 |=1Atie-breaking yields higher total coverage x = 2. (a), comparisoninclude hFF baseline, using hadd arbitrary tie-breaking (FD default),using hadd random tie-breaking. Recall comparison effortC-learning included explicit-conjunction heuristics.CFF , hFF ( C ) fairly close other, follow simThe curves hCFF , hncceCFFC ), hCFF consistently betterilar pattern. hconsistently better hFF (cencFFC96CFF ,h (ce ) except x = 2 . x 2 , consistent advantage hCFF hncindicating beneficial impact cross-context conditions.Regarding hCFFhCFF, latter consistently much better former.| G 0 |=1| G 0 |=1AhCFFachieves competitive performance x 25 . heuristics exhibit| G 0 |=1Aclear trend x. latter due difference heuristic function speed (asshall see below, speed hCFFhCFFsimilar hCFF , across x values).| G 0 |=1| G 0 |=1ARather, caused particular behaviors domains causing coverage decline tendency Figure 2 (a). Compared heuristics, hCFFhCFFscale| G 0 |=1| G 0 |=1Abetter x Barman (only hCFF) Visitall (both hCFFhCFF). also| G 0 |=1A| G 0 |=1| G 0 |=1Acases, e. g. Barman hCFFParking hCFFhCFF,| G 0 |=1| G 0 |=1| G 0 |=1Aheuristics bad begin with, solving first place instances lostheuristics larger x, hence suffering less large x.domains Barman, Parking, Sokoban, Visitall, heuristicsuffering large x hFF (C ), heuristic suffers all. heuristics,growing x marginally negative effect, inconclusive effect, effect all.also 4 domains heuristics tend improve coverage x grows.Figure 3 shows data these. coverage growth x consistent across307fiF ICKERT & H OFFMANN & TEINMETZ2020151510105501222232425262728290122102223(a) Maintenance20151510105522232425262725262728292102829210(b) Parcprinter20012242829012210(c) Tetris222324252627(d) ThoughtfulFigure 3: Coverage individual domains. explicit-conjunction heuristics use haddrandom tie-breaking. Recall comparison hFF baselineseffort C-learning included explicit-conjunction heuristics.heuristics Parcprinter. domains, picture mixed, lotCFFvariance Maintenance Thoughtful, mainly hCFF, hCFF, hCFF , hnc| G 0 |=1 | G 0 |=1Aprofiting large x Tetris. (The curves remain flat Tetris x 27C-learning time-limit applies new conjunctions added.)picture change imposing 30-minute limit time taken Clearning search together? Naturally, tendency coverage decline growingx becomes stronger, yet relative performance explicit-conjunction heuristics remainssimilar.total coverage, shown Figure 7 (page 323), performance substantially worsesearch-only already x = 2 (by 20-30 instances), declines steeplyx heuristics. relative performance heuristics, however, conclusionsCFF beatremain exactly above. respect baselines, hCFF hncFFnon-default h (random tie-breaking), x = 2. inferior default hFF308fiC OMBINING h+ hm : IRECT C HARACTERIZATIONbaseline beat explicit-conjunction heuristics medium-large x values (x = 16x = 32), except hFF (C ) worse x 8, hCFFmarginally| G 0 |=1beats default hFF x = 2.individual domains Figure 3, questions whether (a) x > 2 still improves coverage including effort C-learning, whether (b) explicitconjunction heuristics still beat hFF baselines. Figures 8 9 (pages 324325) show, answer (a) (b) yes. Maintenance Thoughtful, much changes respect Figure 3. Parcprinter Tetris, large valuesx detrimental, moderate ones arent. Coverage increases certain point,namely x = 23 Parcprinter x = 24 Tetris, decreases point.Let us get back hypotheses (H1)(H4). Figure 2 (a) confirms (H1) hCFF typically advantage hFF (C ); somewhat supports (H2) cross-context conC ) (and, directly, hCFF ); confirmsditions hCFF yield advantage hFF (cencCFFCFFC ); (H4) confirms hCFFhnc advantage hFF (ce(H3) hadvantage hCFF. examine reason advantages, thus evaluate| G 0 |=1specific claim hypothesis, consider fine-granular performancemeasures, namely search space size (number state evaluations), states per second,search runtime, commonly solved instances.specific claim hypothesis (H1) that, thanks avoiding exponential blowup |C |, hCFF typically faster hFF (C ) thus improves performance. Figure 4confirms this. top row plots shows main data (the data overall performance).see top left plot, terms quality two heuristics similar. termsspeed, hFF (C ) suffers growing x, overall consistently individualdomains, effect runtime, like coverage discussed above, suffers well.much less hCFF , leading dramatic performance advantage large x.11hand, hCFF also suffers large x, though less hFF (C ),advantage hFF (C ) overall mute. relevant domains where,thanks speed advantage, hCFF benefits growing x, hence improvesbest performance obtainable hFF (C ) value x. coverage, happensMaintenance Tetris (both, without including C-learning, cf. Figures 3, 8,9). search runtime commonly solved instances, happens Hiking, Pegsol,Tetris, Thoughtful. Figure 4 showcases Hiking Tetris, also useshowcases nicely illustrate main points.CFF ,next compare three heuristic functions other, namely hCFF , hncFFCh (ce ). serves examine hypotheses (H2) (H3). specific claimformer asserts that, thanks accounting cross-context conditions, hCFFC ). latter asserts implementation hCFF hCFF typiinformed hFF (cencC ). advantage compare three heuristics togethercally faster hFF (ceas, evaluate importance cross-context conditions, comparison hCFFCFF direct. Figure 5 shows data.hncCFF fasterHypothesis (H3) confirmed consistently, small scale. hCFF hncFFCh (ce ) across x values overall, small advantage grows x.11. remark that, x = 1 hCFF hFF (C ) variants hFF , hardly speeddifferences, neither hCFF hFF (C ) compared FDs standard implementation hFF .309fiF ICKERT & H OFFMANN & TEINMETZ105104hCFFhCFF (*)hFF (C )hFF (C ) (*)103102212223242526272829210103103102102101101100 122223242526272829210100 1222232425262728292102829Overall: search space size, states per second, search runtime105103103104102102103101101102212223242526272829100 122223242526272829100 12222324252627Hiking: search space size, states per second, search runtime10510310310410210210310110110221222324252627100 12222324252627100 12222324252627Tetris: search space size, states per second, search runtimeFigure 4: Data hCFF vs. hFF (C ). Geometric means. curves use instancessolved values x, curves (*) solved heuristics (174instances overall), without (*) solved respective heuristic.Essentially behavior occurs every individual domain, single exception,C ) consistently faster. Hiking Tetris Figure 5 twonamely Tidybot hFF (ceCFF consistently (across almosttypical examples. terms coverage, hCFF hncFFCvalues x) dominate h (ce ) Barman, Elevators, Hiking, Maintenance, Sokoban,Visitall; opposite happens Parking Tidybot.Regarding (H2), top left plot Figure 5 shows, three heuristics yield similarsearch space sizes overall. domains hCFF consistently, across valuesCFF . However, domains hCFFx, yields smaller search spaces hncnotable advantage large values x. mainly Hiking Tetris, shownFigure 5. Tetris, advantage basically consistent beyond x = 24 . Hiking behavessimilarly except degradation largest two x values. domains, hCFF alsoCFF . Overall, support hypothesiscorresponding coverage advantages hnc310fiC OMBINING h+ hm : IRECT C HARACTERIZATION104103103102105104hCFFhCFFnc103 12hFF (Cce )2223242526272829210102 122223242526272829210101 1222232425262728292102627282921026272829210Overall: search space size, states per second, search runtime101031031021025104103 122223242526272829210101 122223242526272829210101 1222232425Hiking: search space size, states per second, search runtime103103102102105104103 122223242526272829210101 122223242526272829210101 1222232425Tetris: search space size, states per second, search runtimeCFF vs. hFF ( C ). Geometric means. curves useFigure 5: Data hCFF vs. hncceinstances solved values x heuristics (225 instances overall). Notethat, better readability, y-scales show 2 orders magnitude (in difference Figure 4).(H2) weak, data give evidence cross-context conditions can,cases, advantage.context, worth coming back briefly discussion Table 1, x =CFF somewhat favor hCFF (many2, comparison hCFF hncncper-domain advantages, small scale). growing x, picture becomesCFF consistentlyfavorable hCFF , though still small scale. domain hncCFFfaster, higher quality, yields better coverage, h . hand,hCFF consistently faster Barman, GED, Openstacks, plus favorable behaviorHiking Tetris shown Figure 5.311fiF ICKERT & H OFFMANN & TEINMETZ103105103hCFFhCFF0|G =1|hCFF|G0 =1|A1041021021032110122232425262728292101012122232425262728292102122232425262728292102627282921026272829210Overall: search space size, states per second, search runtime1051031031041021021032110122232425262728292101012122232425262728292102122232425Hiking: search space size, states per second, search runtime1010351031041021021032110122232425262728292101012122232425262728292102122232425Tetris: search space size, states per second, search runtimeFigure 6: Data hCFF vs. hCFFvs. hCFF. Geometric means. curves use| G 0 |=1| G 0 |=1Ainstances solved values x heuristics (208 instances overall). Notethat, better readability, y-scales show 2 orders magnitude (in difference Figure 4).Let us finally consider hypothesis (H4), asserts that, thanks non-trivialsubgoal support selection, hCFF typically yields informed heuristic hCFF.| G 0 |=1comparison completeness. Figure 6 shows data.include hCFF| G 0 |=1Athree heuristics perform similarly overall. partly due particularities common instance basis. Several domains all, hardly, containedinstance basis Figure 6. pertains particular Barman, Maintenance, Parcprinter, Parking, hCFF large coverage advantages hCFF.| G 0 |=1Nevertheless, Figure 6 allows confirm (H4), albeit small scale. overall,consistently across values x, hCFF slightly smaller search space hCFF. (It| G 0 |=1also slightly faster hCFF, consequently results slightly better runtime.) Per| G 0 |=1312fiC OMBINING h+ hm : IRECT C HARACTERIZATIONdomain, hCFF search advantages 9 cases, disadvantages 3 cases. Figure 6 showcases Hiking Tetris, respectively represent domain classes,heuristics benefit growing x.domains, like overall hCFF slight speed advantage hCFF.| G 0 |=1already observed discussion Table 3, must caused differentstates evaluated, hCFF evaluating states close goal thus faster.terms coverage, hCFF clearly dominates hCFFoverall (Figure 2),| G 0 |=1strong advantages 8 domains (e. g. Maintenance Parcprinter, cf. Figure 3),hCFFadvantages 3 domains (Tetris cf. Figure 3, Thoughtful, Visitall).| G 0 |=1Let us finally compare hCFFhCFF. overall coverage difference clearly| G 0 |=1| G 0 |=1Afavor hCFF. Per domain, hCFFcoverage advantage 5 domains| G 0 |=1A| G 0 |=1Adisadvantage 9, yet disadvantages typically marginal whereas advantagessubstantial. Regarding speed search space size commonly solved instances,speed similar almost universally. Search space size also often similar(in overall, hCFFhCFFalmost indistinguishable). exceptions| G 0 |=1| G 0 |=1Aindividual domains, specifically Elevators, Pegsol, Visitall hCFFbetter,| G 0 |=1GED, Hiking, Parking hCFFbetter.| G 0 |=1ASumming observations, data confirms (H1) impressively, caveatIPC domains large C beneficial. (H3) (H4) confirmed consistently, overall across domains. evidence (H2)weaker, good cases Hiking Tetris. entirely unexpected givencross-context conditions occur specific situations, important theory but, evidently, rare practice far reflected IPC benchmarks.6. Contribution Summary Future Workwork contributes new understanding recent compilation-based partial delete relaxation heuristics, terms combination delete relaxation critical-pathheuristics. key insight view priori unrelated relaxationsdefined underlying set atomic subgoals, relaxation consists decomposing non-atomic conjunctive goals atomic subgoals. Critical-path heuristics require achieve costly atomic subgoal, delete relaxation requiresachieve atomic subgoals. standard delete-relaxation framework becomesspecial case atomic subgoals singleton facts, entire standard machinery h+ , relaxed plan existence testing based h1 , additive heuristic hadd , relaxedplan extraction best-supporter function extends naturally along dimensionallowing arbitrary atomic subgoals C instead.direct characterization identifies precise new source complexity relaxed plan extraction process, namely selecting subset atomic subgoals supportgiven action. Thanks this, design new C-relaxed plan heuristics, hCFFCFF , avoiding shortcomings previous compilation-based heuristics. theoreticalhncadvantages hCFF reflected empirically IPC benchmarks. improvementstate art, overall, marginal though, relates new heuristicsimplementation advantages theoretical ones.313fiF ICKERT & H OFFMANN & TEINMETZview, main value work lies understanding compilationheuristics actually do, spelling framework C-delete relaxation, replacingC ) direct natural hCFF respectively hCFF .hFF (C ) hFF (cencnew heuristics may yield dramatic benefits cases, certainly reliable somewhat efficient predecessors, reasonuse them. nice side benefit simple yet useful generalization hm hC .believe still many exciting avenues future research area.expect results help many them, efficient directimplementation, alternate less opaque formulation.obvious topic use backward search instead forward search, parallelingdesign HSP-r (Bonet & Geffner, 1999) need compute hCinitial state. Alcazar et al. (2013) already took direction, didnt explore detail.still glaring hole understanding C-relaxation heuristics, namelyrole conjunction set C. good sets C? find them? literaturefar offers preliminary answers second question, offers answerfirst one. particular, proof convergence via hm : select large enoughhm = h , simulate via C hC = hm , hC hC+ gethC+ = h QED. completely ignores (a) free choose set C,size-m conjunctions, (b) hC lower bound hC+ , trivial onehC+ typically much higher (similarly well-known relation h1 h+ ).many/which conjunctions actually needed render hC+ perfect?Preliminary results obtained bottom-up approach trying identifyplanning fragments small sets C suffice obtain hC+ = h (Hoffmann, Steinmetz, &Haslum, 2014). approach proved exceedingly difficult though, complexcase considerations already trivial fragments. instead explore top-down,identifying conjunctions needed render hC+ perfect, thus guide C-learningmechanisms? IPC benchmarks suffice use fact pairs,h+ topology (Hoffmann, 2005) change ones? learn somethingparticular planning sub-structures handled?practical approach design alternate C-learning methods. particular,learn C search? Learning mistakes proved extremely successful constraint-satisfaction problems like SAT (e. g. Marques-Silva & Sakallah, 1999;Moskewicz, Madigan, Zhao, Zhang, & Malik, 2001)? Recent work (Steinmetz & Hoffmann,2016) devised approach dead-end detection, refining hC dead-endsbecome known search (i. e., search explored descendants). depth-first search, algorithm approaches elegance clause learning SAT, learning generalizing knowledge refuted search subtrees.search guidance non-dead-end states? usefully refine hCFFsearch? difficulty that, whenever C increased, re-adjust relativeordering states principle would need re-evaluate hCFF entire open list.interesting option local search: use hill-climbing local minimum reached,refine C eliminate local minimum hCFF search surface. words:caught local minimum, rather giving heuristic relying searchinstead commonly done across AI sub-areas refine heuristic exhibit exit.314fiC OMBINING h+ hm : IRECT C HARACTERIZATIONsummary, satisfied understanding C-relaxation heuristics,believe key fully exploiting power lies better understandingdesign methods finding atomic subgoals C.Acknowledgmentswork partially supported German Research Foundation (DFG),grant HO 2169/5-1. thank anonymous reviewers, whose comments helped improve paper.Appendix A. ProofsTheorem 1 Let = (F , A, , G) planning task, C set conjunctions containingsingleton conjunctions. h+ (C ) = hC+ ().Proof: Denote = (F , A, , G). proof via comparing two equations. Equationsimply Equation 7 (page 280, hC+ equation), characterizing hC+ (). derive Equation II applying Equation 5 (page 279, non-standard characterization h+ ) C ,characterizing h+ (C ).Repeating Equation 7, convenience: hC+ () = h(G C ), h functionconjunction sets G satisfies h( G ) =(0c G : cC001 + minaA,6=G0 {c|cG,R(c,a)6=} h(( G \ G ) Gr ) elseGr0 defined Gr0 := cG0 R(c, a).Reconsider Equation 5, written as: h+ () = h(G), hfunction fact sets G satisfies h( G ) =0GI1 + minaA,6=G0 ={ p| pG,R({ p},a)6=} h(( G \ G 0 ) Gr0 ) elseGr0 defined Gr0 := pG0 R({ p}, a).apply previous equation C . Making explicit individual factsC -fluents, obtain: h+ (C ) = h({c | c G C }), h functionfact sets G satisfies h( G ) =0c G : c C001 + mina[C0 ]AC ,6=G0 ={c |c G,R({c },a[C0 ])6=} h(( G \ G ) Gr ) elseGr0 defined Gr0 := c G0 R({c }, a[C 0 ]).One already see correspondence Equation I, conjunctions c corresponding -fluents c . major difference set action/supportedsubgoal-set pairs minimized bottom cases.Consider set G 0 = {c | c G, R({c }, a[C 0 ]) 6= } supported atomic subgoalsper last equation. condition R({c }, a[C 0 ]) 6= simplifies c C 0 ,315fiF ICKERT & H OFFMANN & TEINMETZexactly -fluents added a[C 0 ]. Thus, removing G 0 variablefixed anyway, equation simplifies to: h( G ) =0c G : c C001 + mina[C0 ]AC ,6={c |c G,cC0 } h(( G \ G ) Gr ) elseminimize actions a[C 0 ] C , C 0 needs supportnon-empty subset subgoal -fluents/conjunctions c . possible choicesC 0 ? c principle include C 0 , i. e. subgoals principlesupport using action are, definition C , exactly R(c, a) 6= .Observe point including c c 6 G: supportsubgoals yet result larger precondition. Hence choice C 0 exactly6= C 0 {c | c G, R(c, a) 6= }. Renaming C 0 G 0 order unify notationEquation I, yields final Equation II: h+ (C ) = h({c | c G C }), hfunction fact sets G satisfies h( G ) =0c G : c C1 + mina[G0 ]AC ,6=G0 {c|c G,R(c,a)6=} h(( G \ G 0 ) Gr0 ) elseGr0 defined Gr0 := cG0 R({c }, a[ G 0 ]).spell correspondence Equations I, viewtree whose root node initializing call respective input tasks goal.two trees isomorphic sense one-to-one mapping treenodes, and, using suffixes [ ] [ ] identify respective tree, pair G [ ]G [ ] corresponding tree nodes have:() G [ ] = {c | c G [ ]}true definition root nodes ( ) hC+ () = h(G C ) respectively ( ) h+ (C ) =h({c | c G C }).Consider corresponding bottom-case nodes (*). Observe choice atomicsubgoals c G 0 sides: Equation allows c G [ ]R(c, a) 6= , Equation allows c G [ ] R(c, a) 6= .map children nodes using action supported subgoal set0G [ ] = G 0 [ ] =: G 0 sides. use action Equation action a[ G 0 ]Equation I. Consider recursive subgoals, ( G [ ] \ G 0 ) [ cG0 R(c, a)]C Equation I,( G [ ] \ {c | c G 0 }) cG0 R({c }, a[ G 0 ]) Equation I.G \ G 0 parts expressions exact match (*) construction,remains consider [ cG0 R(c, a)]C vs. cG0 R({c }, a[ G 0 ]). regression singletonfact sets yields action precondition, cG0 R({c }, a[ G 0 ]) simplifies pre( a[ G 0 ]).definition C , equals [ cG0 (pre( a) (c \ add( a)))]C . R(c, a) = pre( a)(c \ add( a)), equals [ cG0 R(c, a)]C . desired match [ cG0 R(c, a)]Cobvious, showing (*) preserved, concludes argument.Theorem 2 Let = (F , A, , G) planning task, C set conjunctions containingC ) = h C + ( ).singleton conjunctions. h+ (ncnc316fiC OMBINING h+ hm : IRECT C HARACTERIZATIONProof: proof similar Theorem 1. Equation same, exceptC + . Equation II exactly same,Gr0 := { R(c, a) | c G 0 } per definition hncdifference Gr0 := cG0 R({c }, a[ G 0 ]) interpreted perC , opposed C . arguments exactly same, exdefinition nccept last part proof showing correspondence { R(c, a) | c G 0 }C vs.({c }, a[ G 0 ]). regression singleton fact sets yields action preconc G 0 RSC , equalsdition, cG0 R({c }, a[ G 0 ]) simplifies pre( a[ G 0 ]). definition nc{pre( a) (c \ add( a)) | c G 0 }C . R(c, a) = pre( a) (c \ add( a)), equals { R(c, a) |c G 0 }C , matching { R(c, a) | c G 0 }C desired.Theorem 3 Let = (F , A, , G) planning task, C set conjunctions containingC ) = h C ( ).singleton conjunctions. h1 (C ) = h1 (ncProof: Denote = (F , A, , G). Consider first C . compare respective characterizingequations. First, Equation 6 (page 279) characterizes h1 ; applying C , geth1 (C ) = h(G C ) h function fact sets G satisfiesG C001 + mina[C0 ]AC ,R(G,a[C0 ])6= h( R( G, a[C ])) G = {c }, c Ch( G ) =maxc G h({c })elseObserve that, middle case, must c C 0 otherwise c 6 add( a[C 0 ]);point including conjunctions C 0 , i. e., C 0 ) {c}, yield larger recursive subgoal R( G, a[C 0 ]). Hence re-writeprevious equation to:G C01 + mina[{c}]AC ,R(G,a[{c}])6= h( R( G, a[{c}])) G = {c }, c Ch( G ) =maxc G h({c })elseRefer Equation I.Recall hC () = h(G) h function fact sets G satisfiesGI01+minh(R(G,))GCh( G ) =aA,R( G,a)6=0maxG0 G,G0 C h( G )elseRefer Equation II.Similarly proof Theorem 1, view equations tree whoseroot node initializing call (I) h1 (C ) = h(G C ) respectively (II) hC () = h(G).two trees isomorphic sense one-to-one mappingtree nodes, and, using suffixes [I] [II] identify respective tree, pair G [ ]G [ ] corresponding tree nodes have:() G [ ] = {c | c C, c G [ ]}obviously true root nodes, obviously invariant bottom casemap children node pairs corresponding c G [ ] respectively c G [ ].317fiF ICKERT & H OFFMANN & TEINMETZConsider corresponding middle-case nodes G [ ] = {c } G [ ] = c.First, C actions a[{c}] definition satisfy R( G [ ], a[{c}]) = R({c }, a[{c}]) 6= .choice a[{c}] thus corresponds choice actions original taskaction a[{c}] included C . exactly actionsc regressed, R(c, a) 6= , hence R( G [ ], a) = R(c, a) 6= .choice actions minimized sides, map childrennode pairs corresponding a.pair, recursive subgoal Gr [ ] generated (II) R(c, a) = (c \ add( a))pre( a). recursive subgoal Gr [ ] generated (I) R({c }, a[{c}]) = pre( a[{c}]),definition C equals [(c \ add( a)) pre( a)]C . latter defined {c0 | c0C, c0 (c \ add( a)) pre( a)}. equals {c0 | c0 C, c0 Gr [ ], showing (*)concluding argument.C identical because, single-conjunction sets C 0 = { c }, twoargument ncC ).compilations coincide (specifically, precondition a[{c}] C ncTheorem 4 Let = (F , A, , G) planning task, C set conjunctions containingsingleton conjunctions. C-relaxed plan CFF sequentialized form relaxedplan C .Proof: Denote = (F , A, , G). Recall definition CFF , CFF = (G C ),partial function conjunction sets G defined G C satisfies ( G ) =c G : cC000(( G \ G ) Gr ) {( a, G )} A,6= G 0 {c | c G, R(c, a) 6= , hC ( R(c, a)) = hC (c) 1},hC ( Gr0 ) = hC ( G 0 ) 1elseGr0 defined Gr0 := cG0 R(c, a).Starting (G C ), say keep tracing recursive invocations equation, using suitable ( a, G 0 ) choice CFF whenever bottom case equation applies.construction (because CFF = (G C )), make choices wayeventually reach top case, terminate. Denote h( a0 , G00 ), . . . , ( an1 , Gn0 1 )iinverted sequence action occurrences selected along trace, i. e., deeper recursion steps correspond smaller indices, ( a0 , G00 ) action occurrence whose selectionlead terminating top case, ( an1 , Gn0 1 ) action occurrence selectedinitializing call. show h a0 [ G00 ], . . . , an1 [ Gn0 1 ]i relaxed plan C .Denote Gi , 1 n, subgoal tackled selection ( ai1 , Gi01 )middle case, denote G0 final subgoal tackled top case. Denote sistate resulting applying h a0 [ G00 ], . . . , ai1 [ Gi01 ]i C . show induction(*) {c | c Gi } si . = n, Gn = G C , shows sn G C desired.Induction base case, = 0: Here, (*) follows directly definition because, topcase fired G0 , c G0 c , hence c C = s0 .induction step, assume (*) true i. show holds + 1.construction, Gi recursive subgoal ( Gi+1 \ Gi0 ) [ cGi0 R(c, ai )]C . Denote lefthalf expression LH := Gi+1 \ Gi0 , right half RH := [ cGi0 R(c, ai )]C .318fiC OMBINING h+ hm : IRECT C HARACTERIZATIONinduction hypothesis, () {c | c LH RH } si . Consider Gi+1si+1 . First, atomic subgoals achieved ( ai , Gi0 ), namely Gi+1 \ Gi0 , tackledLH: () {c | c LH } = {c | c Gi+1 \ Gi0 } si . planning delete-freeimmediately yields {c | c Gi+1 \ Gi0 } si+1 . Second, atomic subgoalsachieved ( ai , Gi0 ), namely Gi0 , clearly true si+1 well: simplyadd( a[ Gi0 ]) = {c | c Gi0 }.remains show ai [ Gi0 ] applicable si . definition C , preconSdition [ cGi0 (pre( a) (c \ add( a)))]C . R(c, a) = pre( a) (c \ add( a)), equals[ cGi0 R(c, a)]C . latter exactly pre( ai [ Gi0 ]) = {c | c RH }, done() {c | c RH } si concludes proof.Theorem 5 Let = (F , A, , G) planning task, C set conjunctions containingCFF sequentialized form relaxedsingleton conjunctions. nc-C-relaxed plan ncCplan nc .CFF , CFF = (G C ),Proof: Denote = (F , A, , G). Recall definition ncpartial function conjunction sets G defined G C satisfies ( G ) =c G : cC000(( G \ G ) Gr ) {( a, G )} A,6= G 0 {c | c G, R(c, a) 6= , hC ( R(c, a)) = hC (c) 1},hC ( Gr0 ) = hC ( G 0 ) 1elseGr0 defined Gr0 := { R(c, a) | c G 0 }.proof Theorem 4 remains valid exactly written, except RH =C,{ R(c, a) | c Gi0 }C . need show ai [ Gi0 ] applicable si . definition ncprecondition {pre( a) (c \ add( a)) | c Gi0 }C . R(c, a) = pre( a) (c \ add( a)),equals { R(c, a) | c Gi0 }C . latter exactly pre( ai [ Gi0 ]) = {c | c RH },done () {c | c RH } si .Theorem 6 Let = (F , A, , G) planning task, C set conjunctions containingsingleton conjunctions. C-relaxed plan exists nc-C-relaxed plan existshC < .Proof: Denote = (F , A, , G). show claim two parts, (a) C-relaxed planexists hC < , (b) nc-C-relaxed plan exists hC < .consider first part (a).direction corollary Theorems 3 4: hC = , Theorem 3h1 (C ) = relaxed plan C exist, Theorem 4 impliesC-relaxed plan cannot exist either.direction, say hC < . need show exists C-relaxedplan. end, consider simpler version equation defining CFF (Equation 11), restricting choice G 0 singletons G 0 = {c}. easy simplifications,get: ( G ) =c G : c(( G \ {c}) R(c, a)C ) {( a, {c})} A,c G, R(c, a) 6= , hC ( R(c, a)) = hC (c) 1 else319fiF ICKERT & H OFFMANN & TEINMETZRecall, equations below, function partial, defines Crelaxed plan defined (the atomic conjunctions of) global goal G C .Observe that, previous equation, always support single atomicsubgoal anyhow, need recurse sets atomic subgoals. insteadrecurse single atomic subgoals, replace initializing recursive calls,sets atomic subgoals, union call elements. resultscharacterization given Equation 12: CFF = cG C (c), partialfunction conjunctions c satisfies (c) =cI0 ) {( a, { c })} A,(c0Cc R(c,a)R(c, a) 6= , hC ( R(c, a)) = hC (c) 1 elseNote similarity Equation 8 (page 285): back common notationrelaxed plan extraction (over C instead singleton facts), extracting best supportersone-by-one.Towards proving claim, transform equation way making linkhC obvious. Instead union operations initial recursive calls,enumerate atomic subgoals contained given set facts (G respectively R(c, a)),recurse directly fact sets, G, introduce third case performing unionatomic conjunctions contained G. hence get characterization givenEquation 13: CFF = (G), partial function fact sets G satisfies(G) =GI( R( G, a)) {( a, G )} A,R( G, a) 6= , hC ( R( G, a)) = hC ( G ) 1 G C0elseG 0 G,G 0 C ( G )Compare Equation 3 defining hC : h( G ) =GI01 + minaA,R(G,a)6= h( R( G, a)) G CmaxG0 G,G0 C h( G 0 )elsebottom cases equations obvious correspondence. G h( G ) < ,middle cases correspondence, too, sense choice action occurrences Equation 13 exactly choice minimizing actions Equation 3: hC ( G ) <, actions minimizing 1 + hC ( R( G, a)) hC ( R( G, a)) = hC ( G ) 1.So, finite-value subgoals, subgoaling structure two equations coincides,particular, hC < , exists solution Equation 13 definedG . Therefore, Equation 12 solution defined c G C . Equation 12 capturesrestricted version CFF , applying conditions smaller choice action occurrences, implies exists C-relaxed plan desired, concluding part (a)proof.part (b), direction follows manner corollary Theorems 3 5. direction also follows manner, difference lies definition Gr0 , singleton G 0 difference disappears: G 0 = {c}320fiC OMBINING h+ hm : IRECT C HARACTERIZATIONCFF , vs. G 0 = R ( c, ) CFF . atomic conjunctions conwe Gr0 = { R(c, a)} ncrtained expressions same. Hence, restricting choice G 0 singletons,CFF simplifies Equation 12 exactly above,equation defining ncproof identical.Example 9 construct example advantage select smaller set supportedsubgoals G 0 , even though larger set strict superset would feasible. constructionextends abstract example (Example 3).Consider planning task = (F , A, , G) defined follows. F = { g1 , g2 , p, q1 , q2 , r1 ,N1N1r2 , r10 , r20 , q10 , . . . , q10 , q20 , . . . , q20 }, = {q1 , q2 }, G = { g1 , g2 }. consists of: (abbreviatingaction form : precondition facts positive negative effect literals)Achieving goals. a[ g1 ]: p, q1 g1 . a[ g2 ]: p, q2 g2 .Achieving p. a1 [ p]: r1 p. a2 [ p]: r2 p.Achieving preconditions p. a[r1 ]: r10 r1 , q2 . a[r2 ]: r20 r2 , q1 .Achieving preconditions ri . a[r10 ]: r10 . a[r20 ]: r20 .1Nr2 , q1 . 1 N: a[q10 ]: q10 .1Nr1 , q2 . 1 N: a[q20 ]: q20 .Reachieving q1 . a[r2 , q1 ]: q10 , . . . , q10Reachieving q2 . a[r1 , q2 ]: q20 , . . . , q20construction, achieving r1 q1 takes 2 steps, achieving r1 q2 takes N + 1 steps;symmetrically, achieving r2 q2 takes 2 steps, achieving r2 q1 takes N + 1 steps.CFF mustuse properties construct case smallest-possible nc-C-relaxed plan ncuse a1 [ p] achieve p g1 , use a2 [ p] achieve p g2 , thus relying non-maximal setsbest-supported subgoals relaxed plan extraction. arguments apply C-relaxedplans CFF which, example, behave identically.Say C contains singleton conjunctions well c pq1 = p q1 , c pq2 = p q2 ,cr1q2 = r1 q2 , cr2q1 = r2 q1 .CFF =Constructing nc-C-relaxed plan according Definition 3 (page 287), start nc({ g1 , g2 }) requiring support two (atomic-singleton-conjunction) goal facts.done ( a[ g1 ], { g1 }) ( a[ g2 ], { g2 }) respectively. using bottomcase Equation 11, get recursive subgoal G = {c pq1 , c pq2 } (as well subsumedconjunctions p, q1 , q2 irrelevant following discussion). a1 [ p] a2 [ p]support conjunctions. Indeed, best supporterconjunctions.see this, observe first hC (c pq1 ) = 3 e. g. via a[r10 ], a[r1 ], a1 [ p]; hC (c pq2 ) = 3e. g. via a[r20 ], a[r2 ], a2 [ p]; clear already, ai [ p] best supporter c pqi . Regarding cross-over combinations, R(c pq2 , a1 [ p]) = cr1q2 ; a[r1 ] deletes q2 ,1Nregressed via a[r1 , q2 ], leading subgoal {q20 , . . . , q20 } whose hC value clearly 1,hC ( R(c pq2 , a1 [ p])) = 2 = hC (c pq2 ) 1 desired. Similarly, R(c pq1 , a2 [ p]) = cr2q1 whose hCvalue 2 desired.So, subgoal G = {c pq1 , c pq2 }, choose among six action occurrences, using either0 := {c00a1 [ p] a2 [ p] support either G12pq1 , c pq2 }, G1 : = { c pq1 }, G2 : = { c pq2 }.321fiF ICKERT & H OFFMANN & TEINMETZNow, cross-over combinations suitable far hC concerned,suitable obtain shortest relaxed plans. Say include c pq2 supported subgoal seta1 [ p]. regressed subgoal cr1q2 , requiring us use a[r1 , q2 ] well N actions a[q20 ],N + 1 actions total. hand, using a1 [ p] support c pq1 , regressed subgoal{r1 , q1 } two singleton conjunctions supported using action occurrences( a[r10 ], {r10 }), ( a[r1 ], {r1 }) (recall q1 true initially). Similarly, using a2 [ p] supportc pq1 incurs cost N + 1 using a2 [ p] support c pq2 incurs cost 2.Getting back choice subgoal G = {c pq1 , c pq2 }, use ( a1 [ p], G10 ),thereafter use ( a2 [ p], G20 ) get nc-C-relaxed plan cost 8: 2 previously achievingfacts gi , 2 two occurrences achieving c pqi , 4 afterwards achieving facts ri . Similarlyuse ( a2 [ p], G20 ) first. If, however, start action occurrence, incurcost N + 1 least one c pqi , exceeding optimal cost 8 sufficiently large N.0 ) feasible, supports strict supersetparticular, action occurrence ( a1 [ p], G12atomic subgoals supported ( a1 [ p], G10 ), leads strictly larger relaxed plan.Theorem 7 C-SubgoalSupport NP-complete.Proof: Membership obvious guess check. hardness, show polynomialreduction Hitting Set problem set B subsets b E finite setelements E, question whether exists hitting set size L.Denote E = {e1 , . . . , en }. construct planning task = (F , A, , G) follows.F := E { p0 , p1 , p2 } { g1 , . . . , gn }, := { p0 }, G := { g1 , . . . , gn }. action set contains a[ p1 ] precondition p0 , add p1 , empty delete, well a[ p2 ] precondition p1 , add p2 , empty delete. action set furthermore contains action a[ei ]every ei E, pre( a[ei ]) = { p0 }, add( a[ei ]) = {ei }, del( a[ei ]) = { p2 } {e j |ex. b B : {ei , e j } b}. Finally, action set contains actions a[ g1 ], . . . , a[ gn ]pre( a[ gi ]) = {ei , p2 }, add( a[ gi ]) = { gi }, delete empty. set C := {{ p} | pF } B {{ei , p2 } | ei E}.think hC terms (C-)relaxed planning graph (RPG), layercorresponds conjunctions g hC ( g) t. None conjunctions b Bachieved, exists action b regressed. However,facts ei E achieved isolation. Consider layer 1 RPG. key propertyexploit (*) subset E0 = {e1 , . . . , ek } E feasible layer 1, i. e. hC ( E0 ) 1,iff exist b B s.t. b E0 . right left, b E0 E0 infeasiblesimply hC (b) = . Vice versa, say exist b B s.t. b E0 .c0 E0 , c0 C set singleton conjunctions {ei }, get hC ( E0 ) = 1{ei } achieved single action.RPG layer 1, apply a[ p2 ]. ei already present, getconjunctions {e1 , p2 }, . . . , {en , p2 } layer 2. this, a[ gi ] actions become feasible,goal reached layer 3.Consider relaxed plan extraction. get goal, must select a[ gi ] actions.Say selected sequence. get subgoal {{e1 , p2 }, . . . , {en , p2 }}layer 2 (plus subsumed singleton conjunctions, omit readability).action regressed a[ p2 ]: recall a[ei ] actions deletep2 . maximal subset G 0 := {{ei1 , p2 }, . . . , {eik , p2 }} {{e1 , p2 }, . . . , {en , p2 }}choose support?322fiC OMBINING h+ hm : IRECT C HARACTERIZATIONsubset yields new generated subgoal {ei1 , . . . , eik , p1 } RPG layer 1.p1 achieved a[ p1 ] interact anything critical: hC ({ei1 , . . . , eik ,p1 }) = hC ({ei1 , . . . , eik }). Denote E0 := {ei1 , . . . , eik }. action occurrence ( a[ p2 ], G 0 )C-feasible RPG layer 1 iff E0 feasible RPG layer 1. (*), latter case iffexist b B s.t. b E0 . then, consider E \ E0 . construction,hitting set iff E0 feasible: E \ E0 hitting set b fully contained E0 ,b fully contained E0 E \ E0 must hit every b. Setting K := n L, thus getexists C-feasible G 0 | G 0 | K iff exists feasible E0 | E0 | n Liff exists hitting set size n (n L) = L. concludes proof.370370350350330330310310290290270270CFFhnchCFF25023021019017015021250230C)hFF (ceFFh (C )hCFF| G 0 |=1AhCFF| G 0 |=1hFF arbhFF rnd22232101901702425262728292101502122(a) Search-only coverage23242526272829210(b) Inclusive coverageFigure 7: Total coverage.Appendix B. Coverage Including C-Learning Time Limitgive coverage plots Section 5.4, imposing 30-minute limit Clearning search together (inclusive Figures 7, 8, 9). convenience, alsoinclude search-only plots Section 5.4.ReferencesAlcazar, V., Borrajo, D., Fernandez, S., & Fuentetaja, R. (2013). Revisiting regressionplanning. Rossi, F. (Ed.), Proceedings 23rd International Joint ConferenceArtificial Intelligence (IJCAI13), pp. 22542260. AAAI Press/IJCAI.Baier, J. A., & Botea, A. (2009). Improving planning performance using low-conflict relaxedplans. Gerevini, A., Howe, A., Cesta, A., & Refanidis, I. (Eds.), Proceedings19th International Conference Automated Planning Scheduling (ICAPS09), pp. 1017. AAAI Press.323fiF ICKERT & H OFFMANN & TEINMETZ20201515101055012222324252627282901221022Maintenance search-only2015151010552223242526272824252627282921029210Maintenance inclusive200122329012210Parcprinter search-only22232425262728Parcprinter inclusiveFigure 8: Coverage individual domains.Bonet, B., & Geffner, H. (1999). Planning heuristic search: New results. Biundo, S.,& Fox, M. (Eds.), Proceedings 5th European Conference Planning (ECP99), pp.6072. Springer-Verlag.Bonet, B., & Geffner, H. (2001). Planning heuristic search. Artificial Intelligence, 129(12),533.Bonet, B., & Helmert, M. (2010). Strengthening landmark heuristics via hitting sets.Coelho, H., Studer, R., & Wooldridge, M. (Eds.), Proceedings 19th European Conference Artificial Intelligence (ECAI10), pp. 329334, Lisbon, Portugal. IOS Press.Bylander, T. (1994). computational complexity propositional STRIPS planning. Artificial Intelligence, 69(12), 165204.Cai, D., Hoffmann, J., & Helmert, M. (2009). Enhancing context-enhanced additiveheuristic precedence constraints. Gerevini, A., Howe, A., Cesta, A., & Refanidis, I. (Eds.), Proceedings 19th International Conference Automated PlanningScheduling (ICAPS09), pp. 5057. AAAI Press.Coles, A. J., Coles, A., Fox, M., & Long, D. (2013). hybrid LP-RPG heuristic modellingnumeric resource flows planning. Journal Artificial Intelligence Research, 46, 343412.324fiC OMBINING h+ hm : IRECT C HARACTERIZATION2020151510105501222232425262728290122102223Tetris search-only20151510105522232425262725262728292102829210Tetris inclusive20012242829012210Thoughtful search-only222324252627Thoughtful inclusiveFigure 9: Coverage individual domains.Do, M. B., & Kambhampati, S. (2001). Sapa: domain-independent heuristic metric temporal planner. Cesta, A., & Borrajo, D. (Eds.), Proceedings 6th European Conference Planning (ECP01), pp. 109120. Springer-Verlag.Domshlak, C., Hoffmann, J., & Katz, M. (2015). Red-black planning: new systematicapproach partial delete relaxation. Artificial Intelligence, 221, 73114.Fox, M., & Long, D. (2001). Hybrid STAN: Identifying managing combinatorial optimisation sub-problems planning. Nebel, B. (Ed.), Proceedings 17th International Joint Conference Artificial Intelligence (IJCAI-01), pp. 445450, Seattle, Washington, USA. Morgan Kaufmann.Gerevini, A., Saetti, A., & Serina, I. (2003). Planning stochastic local searchtemporal action graphs. Journal Artificial Intelligence Research, 20, 239290.Haslum, P. (2009). hm ( P) = h1 ( Pm ): Alternative characterisations generalisation hmax hm . Gerevini, A., Howe, A., Cesta, A., & Refanidis, I. (Eds.),Proceedings 19th International Conference Automated Planning Scheduling(ICAPS09), pp. 354357. AAAI Press.325fiF ICKERT & H OFFMANN & TEINMETZHaslum, P. (2012). Incremental lower bounds additive cost planning problems.Bonet, B., McCluskey, L., Silva, J. R., & Williams, B. (Eds.), Proceedings 22ndInternational Conference Automated Planning Scheduling (ICAPS12), pp. 7482.AAAI Press.Haslum, P., & Geffner, H. (2000). Admissible heuristics optimal planning. Chien, S.,Kambhampati, R., & Knoblock, C. (Eds.), Proceedings 5th International ConferenceArtificial Intelligence Planning Systems (AIPS-00), pp. 140149, Breckenridge, CO.AAAI Press, Menlo Park.Helmert, M. (2004). planning heuristic based causal graph analysis. Koenig, S.,Zilberstein, S., & Koehler, J. (Eds.), Proceedings 14th International ConferenceAutomated Planning Scheduling (ICAPS04), pp. 161170, Whistler, Canada. Morgan Kaufmann.Helmert, M. (2006). Fast Downward planning system. Journal Artificial IntelligenceResearch, 26, 191246.Helmert, M., & Geffner, H. (2008). Unifying causal graph additive heuristics.Rintanen, J., Nebel, B., Beck, J. C., & Hansen, E. (Eds.), Proceedings 18th International Conference Automated Planning Scheduling (ICAPS08), pp. 140147. AAAIPress.Hoffmann, J. (2005). ignoring delete lists works: Local search topology planningbenchmarks. Journal Artificial Intelligence Research, 24, 685758.Hoffmann, J. (2011). Analyzing search topology without running search: connection causal graphs h+ . Journal Artificial Intelligence Research, 41,155229.Hoffmann, J., & Nebel, B. (2001). FF planning system: Fast plan generationheuristic search. Journal Artificial Intelligence Research, 14, 253302.Hoffmann, J., Steinmetz, M., & Haslum, P. (2014). take render h+ ( c )perfect?. ICAPS 2014 Workshop Heuristics Search Domain-IndependentPlanning (HSDIP14).Keyder, E., & Geffner, H. (2008). Heuristics planning action costs revisited.Ghallab, M. (Ed.), Proceedings 18th European Conference Artificial Intelligence(ECAI-08), pp. 588592, Patras, Greece. Wiley.Keyder, E., & Geffner, H. (2009). Trees shortest paths vs. Steiner trees: Understandingimproving delete relaxation heuristics. Boutilier, C. (Ed.), Proceedings21st International Joint Conference Artificial Intelligence (IJCAI 2009), pp. 17341739,Pasadena, California, USA. Morgan Kaufmann.Keyder, E., Hoffmann, J., & Haslum, P. (2012). Semi-relaxed plan heuristics. Bonet,B., McCluskey, L., Silva, J. R., & Williams, B. (Eds.), Proceedings 22nd International Conference Automated Planning Scheduling (ICAPS12), pp. 128136. AAAIPress.Keyder, E., Hoffmann, J., & Haslum, P. (2014). Improving delete relaxation heuristicsexplicitly represented conjunctions. Journal Artificial Intelligence Research,50, 487533.326fiC OMBINING h+ hm : IRECT C HARACTERIZATIONMarques-Silva, J., & Sakallah, K. (1999). GRASP: search algorithm propositionalsatisfiability. IEEE Transactions Computers, 48(5), 506521.McDermott, D. V. (1999). Using regression-match graphs control search planning.Artificial Intelligence, 109(1-2), 111159.Moskewicz, M., Madigan, C., Zhao, Y., Zhang, L., & Malik, S. (2001). Chaff: Engineeringefficient SAT solver. Proceedings 38th Conference Design Automation(DAC-01), Las Vegas, Nevada, USA. IEEE Computer Society.Richter, S., & Westphal, M. (2010). LAMA planner: Guiding cost-based anytime planning landmarks. Journal Artificial Intelligence Research, 39, 127177.Steinmetz, M., & Hoffmann, J. (2016). Towards clause-learning state space search: Learningrecognize dead-ends. Schuurmans, D., & Wellman, M. (Eds.), Proceedings30th AAAI Conference Artificial Intelligence (AAAI16). AAAI Press.Valenzano, R. A., Sturtevant, N. R., Schaeffer, J., & Xie, F. (2014). comparisonknowledge-based GBFS enhancements knowledge-free exploration. Chien,S., Do, M., Fern, A., & Ruml, W. (Eds.), Proceedings 24th International ConferenceAutomated Planning Scheduling (ICAPS14). AAAI Press.van den Briel, M., Benton, J., Kambhampati, S., & Vossen, T. (2007). LP-based heuristicoptimal planning. Bessiere, C. (Ed.), Proceedings Thirteenth InternationalConference Principles Practice Constraint Programming (CP07), Vol. 4741Lecture Notes Computer Science, pp. 651665. Springer-Verlag.327fiJournal Artificial Intelligence Research 56 (2016) 119-152Submitted 07/15; published 05/16Budgeted Optimization Constrained ExperimentsJavad Azimijaazimi@microsoft.comMicrosoft, Sunnyvale, CA, USAXiaoli Z. Fernxfern@eecs.oregonstate.eduSchool EECS, Oregon State UniversityAlan Fernafern@eecs.oregonstate.eduSchool EECS, Oregon State UniversityAbstractMotivated real-world problem, study novel budgeted optimization problemgoal optimize unknown function f () given budget requestingsequence samples function. setting, however, evaluating functionprecisely specified points practically possible due prohibitive costs. Instead,request constrained experiments. constrained experiment, denoted Q,specifies subset input space experimenter sample function from.outcome Q includes sampled experiment x, function output f (x). Importantly,constraints Q become looser, cost fulfilling request decreases,uncertainty location x increases. goal manage trade-off selectingset constrained experiments best optimize f () within budget. studyproblem two different settings, non-sequential (or batch) setting setconstrained experiments selected once, sequential setting experimentsselected one time. evaluate proposed methods settings usingsynthetic real functions. experimental results demonstrate efficacyproposed methods.1. Introductionwork motivated experimental design problem optimizing power outputnano-enhanced microbial fuel cells. Microbial fuel cells (MFCs) (Bond & Lovley, 2003;Fan, Hu, & Liu, 2007; Park & ZeikusG, 2003; Reguera, McCarthy, Mehta, Nicoll, Tuominen,& Lovley, 2005) use micro-organisms break organic matter generate electricity.particular MFC design, critical optimize biological energeticsmicrobial/electrode interface system, research shown depend stronglysurface properties anodes (Park & ZeikusG, 2003; Reguera et al., 2005).motivates design nano-enhanced anodes, nano-structures (e.g., carbon nanowire) grown anode surface improve MFCs power output. Unfortunately,little understanding interaction various possible nano-enhancementsMFC capabilities different micro-organisms. Thus, optimizing anode designparticular application largely guesswork. goal develop algorithms aidprocess.Traditional experimental design, Bayesian optimization response surface methods(Myers, Montgomery, & Anderson-Cook, 1995; Jones, 2001; Brochu, Cora, & De Freitas,2010) commonly assume experimental inputs specified precisely attemptc2016AI Access Foundation. rights reserved.fiAzimi, Fern, & Fernoptimize design requesting specific experiments. example, requesting anodetested nano-wire specific length density. However, parametersunlike usual experimental control parameters (such temperature) easily setprecise values. Manufacturing nano-structures rather art difficultachieve precise parameter setting. Instead, practical request constrainedexperiments, place constraints parameters. example, may specifyintervals length density nano-wire. Given request, nano-materialssatisfy given set constraints produced cost, typicallyincrease tighter constraints.Note possible alternative requesting constrained experiments would treatnano-structure manufacturing parameters experimental inputs. inputsprecisely specified, hence standard methods used. However, approachyield satisfactory solution problem. particular, mappingmanufacturing parameters produced nano-structures extremely noisy. makesdifficult find manufacturing parameters optimize expected MFC poweroutput. Further, scientists primarily interested learning nano-structureproperties optimize MFC power output, rather knowing specific manufacturing parameters, vary significantly lab lab. Thus focus directlyoptimizing space nano-structure properties via constrained experiments.Based motivation, paper, study associated budgeted optimization problem where, given budget, goal optimize unknown function f ()requesting set constrained experiments. Solving problem requires careful consideration trade-off cost uncertainty constrained experiment:weakening constraints lower cost experiment, increase uncertaintylocation next observation. Prior work experimental design, stochasticoptimization, active learning directly apply constrained experimentstypically assume precise experiments.problem formulated theoretical framework partially observableMarkov decision processes (POMDPs), optimal solution corresponds findingoptimal POMDP policy. However, solving optimal even near-optimal policiescomputationally intractable, even case traditional optimization problems.led researchers develop variety myopic policies context traditionaloptimization, observed achieve good performance, even comparisonsophisticated, less myopic strategies (Moore & Schneider, 1995; Jones, 2001; Madani,Lizotte, & Greiner, 2004; Brochu et al., 2010).problem considered two different settings, non-sequential sequential.non-sequential setting, also referred batch setting, constrainedexperiments must selected once. setting appropriate applicationsmultiple experimental facilities experiments time consumingrun sequentially. contrast, sequential setting allows us request one constrainedexperiment time, wait outputs previous experiments makingnext request. sequential setting advantage allows us use maximumavailable information selecting experiment, generally expected outperformnon-sequential setting total running time concern. paper,study settings.120fiBudgeted Optimization Constrained Experimentsnon-sequential setting, introduce non-decreasing submodular objective function select batch constrained experiments within given budget. given setconstrained experiments, objective measures expected maximum output. presentcomputationally efficient greedy algorithm approximately optimizes proposed objective.sequential setting, build set classic myopic policiesshown achieve good performance traditional optimization (Moore & Schneider, 1995;Jones, 2001; Madani et al., 2004; Brochu et al., 2010) introduce non-trivial extensionsmake applicable constrained experiments.present experimental evaluations settings using synthetic functionsfunctions generated real-world experimental data. results indicate that,settings, proposed methods outperform competing baselines.remainder paper organized follows. introduce backgroundrelated work Section 2. Section 3 describes problem setup. non-sequentialsetting studied Section 4. Section 5 introduces proposed methods selecting constrained experiments sequential setting. Section 6 presents empirical evaluationproposed methods. conclude paper discuss future work Section 7.2. Background Related WorkGiven unknown black box function costly evaluate, interested findingextreme point (minimizer maximizer) function via small number functionevaluations. solve problem, Bayesian Optimization (BO) approachesheavily studied (Jones, 2001; Brochu et al., 2010) demonstrated significant promises.two key components basic framework Bayesian Optimization. firstcomponent probabilistic model underlying function built based priorinformation (i.e., existing observed experiments). Gaussian process (GP) regressionwidely used literature BO purpose (Brochu et al., 2010).unobserved point, GP models function output normal random variable,mean predicting expected function output point variance capturinguncertainty associated prediction.second key component BO selection criterion used determineexperiment select based learned model. existing literature, variousselection criteria proposed combination exploringunexplored input space function (i.e., areas high variance) exploitingpromising area (i.e., area large mean). selection criterion either sequential(Jones, 2001; Locatelli, 1997; Moore, Schneider, Boyan, & Lee, 1998; Srinivas, Krause,Kakade, & Seeger, 2010) one experiment asked iteration nonsequential (Schonlau, 1997; Azimi, Fern, & Fern, 2010; Ginsbourger, Riche, & Carrarog,2010) batch experiments requested iteration.review successful sequential selection criteria literatureBO. One first sequential policies based selecting sample maximumprobability improving (MPI) best current observation, ymax (assuming wantmaximize f ), given margin (Elder, 1992; Stuckman, 1988). Let best currentobservation ymax . goal MPI select next experiment highest121fiAzimi, Fern, & Fernprobability producing output smaller (1 + )ymax . One issue approachperformance often sensitive value margin parameter (Jones,2001). small values , MPI focus promising area firstmove onto unexplored areas. contrast, large values , MPI primarily exploreconverge slowly. Selecting proper value challenging practice.maximum expected improvement (MEI) (Locatelli, 1997) criterion avoids issueselects experiment directly maximizes expected improvement currentbest observation. Heuristics based upper-confidence bounds also explored(Srinivas et al., 2010), shown competitive MEI given appropriateparameter selection. However, selecting best parameters particular applicationempirically challenge. alternative approach received attentionThompson Sampling (Chapelle & Li, 2011), randomized strategy managingexploration-exploitation trade-off. approach first samples underlying uncertainty,case unknown function f , returns experiment maximizessample. work, focused extending deterministic methodsBO constrained experiments. Extending alternatives Thompson samplingpotentially interesting future direction.Recently, researchers begun consider non-sequential batch Bayesian optimization (Azimi et al., 2010; Ginsbourger et al., 2010; Desautels, Krause, & Burdick, 2014),selects multiple experiments once. Non-sequential BO considered appropriate applications need capability run multiple experimentssimultaneously. One non-sequential approach (Azimi et al., 2010) selects k > 1 experimentsmatching behavior executing given sequential policy (e.g., MEI) ksteps. another non-sequential approach (Ginsbourger et al., 2010), authors triedselect batch experiments lead highest expected improvement. However, shown expected improvement set jointly normal randomvariables closed form solution k > 2, solved efficiently using numerical methods. Instead, simple heuristics proposed approximateexpected improvement select batch accordingly. recently, algorithmbased upper-confidence bounds also introduced (Desautels et al., 2014),computationally cheaper prior work requires careful parameter selection.Note aforementioned approaches assume unknown functionaim optimize sampled precisely specified points, making unsuitabletasks motivating nano application, sampling function exact locations impractical. proposed sequential approaches paper, previouslypresented less detail (Azimi et al., 2010). paper, provide completeformal description sequential approaches additional empirical results.addition, introduce evaluate batch selection algorithm chooses batchconstrained experiments iteration.3. Problem SetupLet X Rd d-dimensional input space, dimension bounded [Ai ,Bi ]. often refer elements X experiments. assume unknownreal-valued function f (x) : X <, represents expected value dependent122fiBudgeted Optimization Constrained Experimentsvariable running experiment x. motivating application, f (x) expectedpower output produced particular nano-structure x. Conducting experiment xproduces noisy outcome = f (x) + , noise term.traditional optimization settings (Jones, 2001; Brochu et al., 2010), goal findx X approximately optimizes f () requesting set experiments observingoutcomes. Since sampling function exactly specified points prohibitivelyexpensive application, request constrained experiments, define subsetexperiments X . Specifically, define constrained experiment hyper-rectangleX , denoted Q = (q1 , q2 , , qd ), qi = (ai , bi ) Ai ai < bi Bi definesrange values considered admissible input dimension i. Notecomputational reasons, work consider discretized input space,input dimension divided equal-sized intervals. such, constrained experiment Qindicate dimension first (specified ai ) last (specified bi )intervals included hyper-rectangle. remainder paper,interchangeably use terms constrained experiment, hyper-rectangle query.Given constrained experiment Q, experimenter first construct experimentx (we assume x precisely measured produced) satisfiesgiven constraints Q, run experiment, return noisy observation f (x). Notex random variable given Q, assume conditional distribution, px (x|Q),known priori part problem inputs. precisely, query Q,experimenter return 2-tuple (x, y), where:x = (x1 , x2 , , xd ) experiment satisfies constraints Q,noisy observation function f () x, = f (x) + .practice, cost c fulfilling constrained experiment variable depending size hyper-rectangle. particular, higher cost associatedtighter constraints smaller hyper-rectangles. assume cost modeleddeterministic function fc (), provided us part inputs. example,motivating application, fc () dominated time required produce nanomaterial satisfies given constraints, inversely correlated sizeconstraints. addition, must operate within total budget B. Thus, objectivefind set queries within budget B leads best estimate maximizerfunction input space X .summarize, inputs problem include set prior experiments (whichcould potentially empty), budget B, deterministic cost function fc () fulfillingconstrained experiment Q, conditional probability density function px (x|Q)specific experiment x generated given constrained experiment Q.Given inputs, task select set constrained experiments Q = {Q1 , Q2 ,, Qk } whose total cost within budget B. Running selected constrained experimentsresult set k tuples (xi , yi )ki=1 , must determine final outputx {x1 , . . . , xk }, prediction maximizer f () among observedexperiments. Note restrict returning experiment actuallyobserved, even cases might predict non-observed experimentbetter. formulation matches objective motivating application produce123fiAzimi, Fern, & Ferngood nano-structure x using given budget, rather make predictionnano-structure might good.study problem two different settings, non-sequential (or batch) sequential.non-sequential setting, must decide entire set queries time.contrast, sequential setting requests constrained experiments sequentially onetime: receiving result previous request, another query selectedpresented experimenter. procedure repeated reach budget limit.following two sections, introduce proposed solutions settings.4. Non-sequential Approachsection, consider non-sequential setting, must select entirebatch queries Q within given budget B once. Note general batchesmulti-sets repeated queries, may desirable noisy settings.also called non-adaptive (Goel et al., 2006; Krause et al., 2008) Batch (Azimi et al.,2010) setting. setting commonly used applications must start multipleexperiments cannot wait outputs previous queries decidenext queries (Tatbul et al., 2003).4.1 Objective FunctionLet QB set feasible solutions Q QB total cost Qgreater budget B. goal find optimal multi-set queriesQ = {Q1 , Q2 , , Qk } QB . define mean optimal, let us consideroutcome queries, set tuples: (xi , yi ) , = 1, 2, . . . , k. xiexperiments produced experimenter given queries yi representexperimental output (i.e., noisy observation f (xi )). select final outputx {x1 , . . . , xk } believed achieve maximal f () value. such,Q QB , measure good Q based maximal value resultingset queries. Specifically, captured by:hhiififiJ(Q) = E(x1 ,,x|Q| ) E(y1 ,,y|Q| ) max y1 , . . . , y|Q| fiD, (x1 , , x|Q| ) ,(1)first expectation taken possible values xi s, representindividual experiments created query Q, second expectation takenpossible yi s, represents experimental outcomes xi s. mentionedpreviously, xi distributed according px (xi |Qi ), part inputs.distribution yi given xi depends posterior distribution f () given D.work, use Gaussian processes model distribution f (). Consequently,set yi jointly normal conditioned xi D.Since input space discretized, enumerate possible constrained experiments denote QM = {Q1 , Q2 , ..., QM }, total number possibleconstrained experiments, let c1 , . . . , cM corresponding cost (i.e., ci = fc (Qi )).Let = {1, ..., } subset indices QS denote collection queriesindexed S, i.e., QS = {Qi : S}. goal stated selectingcorresponding QS maximizes objective (Equation 1) subject constraint124fiBudgeted Optimization Constrained ExperimentsPci B. Unfortunately, optimizing objective intractable due combinatorial nature problem exponentially many possible solutions consider.reformulate objective demonstrate non-decreasing submodular setfunction introduce algorithm approximation guarantee.Specifically, consider slightly different equivalent view queryingprocess. far view chosen, query Q QS resultexperiment x, viewed random sample drawn distributionpx (x|Q) (note work px uniform within hyper-rectangle definedquery). process point view, clearly matter whether randomdraw happens Q chosen, beginning whole process Qchosen. Following reasoning, could assume every possible query QM ,random experiment drawn beginning process results storedused later selected. Let XM = {x1 , . . . , xM } denote random variablesrepresenting outcome random draw Q1 , ..., QM respectively. objectivereformulated as:hhiififiJ(S) = EXS E(y1 ,...,y|S| ) max y1 , . . . , y|S| fiD, XS ,(2)XS = {xi : S} subset XM defined S, yi noisyoutcomes xi XS .4.2 Approximation AlgorithmSince standard batch Bayesian Optimization special case optimizing J(S), hardness optimizing J(S) follows NP-hardness Bayesian Optimization. Thus,show J(S) non-decreasing submodular set function present algorithm bounded approximation factor.Definition 1. Suppose finite set, g : 2S R+ submodular set functionS1 S2 x \ S2 , holds g(S1 {x}) g(S1 ) g(S2 {x}) g(S2 ).Thus, set function submodular adding element smaller set providesless improvement adding element larger superset. Also, set functionnon-decreasing set element x g(S) g(S {x}).show J(S) submodular, rewrite objective function definingJXM (S) inner expectation Equation 2 fixed realization random variableXM :hfifiJXM (S) = E(y1 ,...,y|S| ) max y1 , . . . , y|S| fiD, XS .Lemma 1. given XM , JXM (S), returns expected maximum setjointly distributed random variables, monotonically non-decreasing submodular setfunction.proof Appendix.proposed objective function, J(S) = EXM [JXM (S)] takes expectation JXM (S)possible values XM . JXM (S) non-decreasing, easy verifyJ(S) also non-decreasing. note set submodular functions125fiAzimi, Fern, & Fernclosed expectation, thus conclude proposed objective, J(S),non-decreasing submodular function.present proposed algorithm optimizing J(S). inputs algorithm include set possible constrained experiments, QM = {Q1 , ..., QM }, associated costsP c1 , ..., cM , total budget B, output subset = {1, ..., }ci B. first introduce simple greedy algorithm, beginsinitial empty set = greedily adds one constrained experiment (its index)time total cost reaches B. step, let current set Ctotal cost S, greedy algorithm selects index that:= argmaxiS;c/BCJ(S i) J(S).ci(3)words, step, algorithm greedily selects new constrained experimentwithin budget leads largest cost-normalized improvementobjective.known simple greedy algorithm bounded approximationfactor (Khuller, Moss, & Naor, 1999). Previous work (Khuller et al., 1999; Krause &Guestrin, 2005) introduced small change greedy algorithm provides usbounded approximation factor. particular, one needs consider, alternativeoutput greedy algorithm, single query within budget achievesbest objective value (denoted Sa ). comparing alternative outputgreedy algorithm, guaranteed achieve bounded approximation factor.complete algorithm summarized Algorithm 1. approximation boundalgorithm follows following theorem.Theorem 1. (Khuller et al., 1999) Let J() monotonically non-decreasing submodularset function J() = 0, output Algorithm 1. Suppose OPToptimal solution, following bound guaranteed"|S |+1 #11J(S )1 1J(OPT)2|S | + 11 e1J(OPT).2e(4)dominating factor run time linear dependence , numberpossible queries. Note discretized setting consider, exponentialnumber dimensions. scientific application domains motivate work,number dimensions typically small (e.g., 2 3). However, workingfine resolution discretization, computation time still significant. addressissue, next section describe simple strategy soundly pruning candidatequeries consideration, yields significant speedups. Problems significantlyhigher dimensions, however, require continuous rather discretized optimization.126fiBudgeted Optimization Constrained ExperimentsAlgorithm 1 Greedy Non-Sequential AlgorithmInput: D, B > 0, {Q1 , ..., QM }, {c1 , ..., cM }POutput: set indices = {1, ..., } ci B- = argmaxiS,ci B J({i})Sa {i }- , C 0(C < B)J(S {i}) J(S)Select that:= argmaxciiS,c/ BC- C C + ci- {i }endJ(S) J(Sa )- returnelse- return SaendAlgorithm 2 Accelerated Greedy AlgorithmInput: D, B > 0, {Q1 , ..., QM }, {c1 , ..., cM }POutput: set indices = {1, ..., }, s.t., sS ci B- = argmaxiS,ci B J({i})- Sa {i }-S , C = 0, (i) = J({i})/ci , = 1, . . . ,(C < B)trueSet z = argmaxi:iS\S (i), ci B C, re-calculate (z)J(S {z}) J(S)(z) =cz(z) maxiS\{Sz} (i)Breakendend- C C + cz , {z}endJ(S) J(Sa )- returnelse- return Saend127fiAzimi, Fern, & Fern4.3 Accelerated Greedy Algorithmsection, following prior applications submodular optimization (e.g. Krause &Guestrin, 2005), describe accelerated greedy algorithm, yields significant gainscomputational efficiency. greedy step, let represent set queriesselected far. make another greedy selection, need compute costnormalized incremental difference (i) = J(Si)J(S)candidate query i,ci/ ci B C. computation expensive number candidatequeries often large. Fortunately, carefully maintaining normalized incrementaldifferences calculated first greedy step, avoid recomputing large majoritylater iterations.Specifically, first iteration compute (i) values S. sortdecreasing order based values, select first query removelist. next iteration, move next query sorted listrecompute value. value remains largest, immediately selectquery remove list, proceed next iteration without recomputingvalues. Otherwise, proceed evaluate next query sorted listfinding one whose recomputed value greater stored valuesselect query. submodular property objective guarantees approachmakes choices full greedy algorithm, effectively avoids large numbercomputations values practice. proposed accelerated algorithm summarizedAlgorithm 2.4.4 Computation Expected Maximumgiven set S, compute J(S), need compute expected maximum valueset jointly distributed random variables (y1 , ..., y|S| ). Unfortunately, expectedmaximum set dependent random variables generally closed-formsolution (Ross, 2008). Instead, use Monte-Carlo simulation approach computingexpected maximum value. Specifically, given S, compute J(S), first sample oneexperiment Q QS , resulting {x1 , ..., x|S| }. sample yiposterior distribution py (y1 , ..., y|S| |x1 , ..., x|S| , D) take maximum sampledyi s. repeated l independent times expected maximum obtainedaveraging across l results. Note computation expected maximumstandardvalue simulation exact. Denoting simulated results J,Chernoff bounds used bound error J(S) high probability. Assumingbounded error, |J(S) J(S)|0, following theorem holdsnon-decreasing submodular objective functions:Theorem 2. (Krause & Guestrin, 2005) Let J() non-decreasing submodular function= maxS:c(S)B J(S ) cost constrained optimizer J. J()|J(S) J(S)|S, Algorithm 1 run using J place J, returnedset satisfies following approximation bound, cmin = mini ci :1 e11 c(S )J(S)J(S )+ |S | .(5)2e2 cmin128fiBudgeted Optimization Constrained Experiments5. Sequential Approachsection, consider sequential setting (Deshpande et al., 2004; Silberstein et al.,2006) query selected one time result previous querybecomes available. commonly studied setting Bayesian Optimizationappropriate many applications single experimental facilityprocess queries.inputs problem remain same, include B, total budget, fc ()cost function, px (x|Q), distribution constructed experiment x given query Q,D, set observed experiments. sequential setting, given inputs mustrequest sequence (one time) constrained experiments whose total cost withinbudget.Leveraging extensive body research traditional Bayesian Optimization, design sequential selection policies extending number well-established myopic sequential selection policies literature. existing policies traditional BayesianOptimization viewed defining greedy heuristic assigns score candidate experiment x based current experimental state, denote (Dc , Bc ),Dc represent current set prior experiments, Bc represents currentremaining budget. reviewed Section 2, many existing heuristics observed perform well traditional Bayesian Optimization problems. Unfortunatelycannot directly used problem since select individual specific experimentsrather constrained experiments, require.5.1 Model-Free PoliciesModel-free policies consider statistical models data making selection.work consider two model-free policies, Round Robin (RR) Biased Round Robin(BRR), motivated previous work budgeted multi-armed bandit problems(Lizotte et al., 2003; Madani et al., 2004).5.1.1 Round Robin (RR)multi-armed bandit setting, RR policy seeks keep number pullsarm uniform possible. context constrained experiments, applyprinciple keep experiments uniformly distributed possible experimentalspace X . Given current experimental state (Dc , Bc ), define RR policy returnlargest hyper-rectangle least costly query Q contain previousexperiment Dc . cost Q exceeds current budget Bc , return constrainedexperiment cost Bc contains fewest experiments Dc . Ties brokenrandomly. Note RR outputs previous queries effectselecting next queries. However, exact location previous experimentssignificant effect next query selection. Therefore, consider RRnon-sequential approach.129fiAzimi, Fern, & Fern5.1.2 Biased Round Robin (BRR)BRR policy behaves identically RR, except repeats previously selected constrained experiment long outcome constrained experiment improvedperformance exceed budget. particular, given current experimental state (Dc , Bc ), query Q repeated long results outcome improvescurrent best outcome set Dc , fc (Q) Bc . Otherwise, RR policyfollowed. policy analogous BRR multi-armed bandit problems (Madani et al.,2004) arm pulled long positive outcome.5.2 Model-Based Policiesmodel-based policies, assumed conditional posterior distribution p(y|Dc , x)outcome individual experiment x Q learned set currentlyobserved experiments Dc . Existing model-based myopic policies traditional experimentaldesign typically select experiment x maximizes certain heuristics computedstatistics posterior (Jones, 2001). heuristics provide different mechanismstrading exploration (probing unexplored regions experimental space)exploitation (probing areas appear promising) given Dc . Note experiment xfixed known point experimental design literature running realexperiment lab since assumed ask particular fixed point.However, constrained experiment application, ask hyper-rectangle queryQ rather fixed experiment point x. Therefore conditional posterior distributionconstrained experiment Q defined p(y|Q, Dc ) , Ex|Q [p(y|x, Dc )]. definition corresponds process drawing experiment x Q drawingoutcome x p(y|x, Dc ). p() effectively allows us treat constrained experimentsindividual experiments traditional optimization problem. Thus, define heuristics constrained experiments computing statistics posteriorp(), used traditional optimization. work consider four heuristics.5.2.1 Maximum Mean (MM)context traditional optimization individual experiments, MM heuristic,also known PMAX (Moore & Schneider, 1995; Moore et al., 1998; Schneider & Moore,2002), simply selects experiment largest expected outcome accordingcurrent posterior. constrained experiments, MM heuristic computes expectedoutcome given query according current posterior p(). MM arbitraryquery Q computed follows:MM(Q|Dc ) = E [y|Q, Dc ] , p(y|Q, Dc ).(6)MM purely exploitative heuristic weakness oftengreedy get stuck poor local maximum point exploring rest experimental space.130fiBudgeted Optimization Constrained Experiments5.2.2 Maximum Upper Interval (MUI)MUI heuristic, also known IEMAX previous work (Moore & Schneider, 1995;Moore et al., 1998; Schneider & Moore, 2002), attempts overcome greedy natureMM exploring areas non-trivial probability achieving good result measuredupper bound 95% confidence interval output prediction. Thus, MUIheuristic arbitrary constrained experiments Q calculated follow (assumingGaussian process used estimating posterior distribution f ()):p(7)MUI(Q|Dc ) = E [y|Q, Dc ] + 1.96 Var [y|Q, Dc ], p(y|Q, Dc ).Intuitively, MUI aggressively explore untouched regions experimental spacesince outcomes regions high posterior variance. However, experimentation continues long time uncertainty decreases, MUI focuspromising areas behaves like MM. Note MUI specific case generalheuristic GP-UCB (Cox & John, 1992, 1997), constant 1.96 replacedvarying parameter results certain theoretical guarantees. Empirically GP-UCBobserved perform comparatively MEI heuristic introduce latersection.5.2.3 Maximum Probability Improvement (MPI)context individual experiments, MPI heuristic corresponds selectingexperiment highest probability generating outcome outperformsbest current outcome Dc . issue basic MPI strategytendency behave similar MM focuses areas currently look promising,rather exploring unknown areas. reason behavior basic MPItake consideration amount improvement current best outcome.particular, typical posterior assign small amounts variancesoutcomes well explored regions. means might good probabilityobserving small amounts improvement, probability substantial improvementsmall. Hence, common consider use margin using MPI,refer MPI(). Let yc represent current best outcome observedDc , MPI (Q|Dc ) equal probability outcome constrainedexperiment Q greater ((1 + )yc ) (assuming non-negative yc values). MPIheuristic given by:MPI (Q|Dc ) = p (y (1 + )yc |Q, Dc ) ,p(y|Q, Dc ).(8)MPI() heuristic sensitive margin parameter. Adjusting marginsmall large causes heuristic change behavior exploitiveexplorative.5.2.4 Maximum Expected Improvement (MEI)maximum expected improvement (Locatelli, 1997) heuristic seeks improvebasic MPI heuristic without requiring margin parameter . Rather focusprobability improvement, considers expected amount improvement according131fiAzimi, Fern, & Ferncurrent posterior. particular let I(y, yc ) = max(y yc , 0). Then, MEI heuristicdefined as:MEI(Q|Dc ) = Ey [I(y, yc )|Dc , x] , p(y|Q, Dc ).(9)5.3 Cost-Sensitive Policiesintroduced sequential heuristics take variable cost constrainedexperiment account. heuristic value used selection criterion,costly constrained experiment might selected. fact, nature heuristicstypically assign highest score constrained experiments maximallyconstrained centered around individual experiment maximizes heuristic.Unfortunately, constrained experiments also maximally costly. generally,assume cost constrained experiment Q monotonically decreases sizeregion support, natural case consider. easy showheuristics, value constrained experiment Q monotonicallynon-decreasing respect cost Q. true reducing sizeconstrained experiment would remove points heuristic values lessconstrained experiment value.Thus, maximizing defined heuristics leads selection costlyexperiments, might consume budget warranted. suggestsfundamental trade-off heuristic values cost constrainedexperiments must address. Below, introduce two approaches attemptaddress trade defining cost-sensitive policies cost insensitive heuristics.5.3.1 Cost Normalized (CN) PoliciesCost normalized policies widely used budgetd optimization settingscosts non-uniform across experiments, (e.g., see Krause et al., 2008; Snoek, Larochelle,& Adams, 2012). simply selects constrained experiment achieves highestexpected improvement per unit cost, rate improvement.Suppose H heuristic. define corresponding CN policy heuristicconstrained experiment Q given current experimental state {Dc , Bc } follows:H(Q|Dc )CNH (Dc , Bc ) = argmax,(10)fc (Q)Q:fc (Q)<BcH(Q|Dc ) assigns score constrained experiment Q given set observed experiments Dc .cost normalization approach natural baseline suggestedcontext optimization problems (e.g., Krause et al., 2008). However,prior work, actual empirical evaluations involved uniform cost models thuslittle empirical data regarding performance normalization. setting constrained experiments, uniform cost model option, since selecting among constrained experiments varying variable cost fundamental aspect problem. Thus,empirical evaluation, Section 6, necessarily provides substantial evaluationnormalization principle.132fiBudgeted Optimization Constrained ExperimentsUnfortunately, experimental results show proposed cost normalized approachoutperformed random policy cases. prompts us introduceconstrained minimum cost(CMC) approach select constrained experimentexpected perform better random policy spending amountbudget.5.3.2 Constrained Minimum Cost (CMC) Policiesheuristic constrained experiments H(Q|Dc ), assigns score constrainedexperiment Q given set observed experiments Dc , define associated CMCpolicy. principle behind CMC select least cost constrained experimentsatisfies following two conditions:Condition 1: approximately optimizes heuristic value,Condition 2: expected improvement (EI) worse randompolicy spending amount budget.first condition encourages selection constrained experiments look promising according H, might result selection overly costly experiment.second condition helps place limit much willing pay achievegood heuristic value. Specifically, willing pay cost c singleconstrained experiment Q expected improvement worse achievedset random experiments whose total cost c.formalize policy, first make notion approximately optimize preciseintroducing parameterized version CMC policy show parameterautomatically selected via condition 2. given heuristic H, let h valuehighest scoring constrained experiment fits within current budget. Notenecessarily one constrained (i.e. expensive) experimentswithin budget. given parameter [0, 1], CMCH, policy selects leastcost constrained experiment achieves heuristic value least h . Formally,defined(11)CMCH, (Dc , Bc ) = argmin {fc (Q) | H(Q|Dc ) h }Q:fc (Q)Bcvalue controls trade cost Q heuristic value.Smaller/larger values result less/more costly experiments, smaller/largerheuristic values. preliminary work, experimented CMCH, policyfound difficult select value worked well across wide rangeoptimization problems, cost structures, budgets. motivated introductioncondition 2 help us adaptively select appropriate value decisionpoint.formalize CMC class policies. objective select largestvalue experiment suggested CMCH, satisfies condition 2.guarantee selected constrained experiment will: 1) achieve heuristic value closepossible h , 2) outperforms random policy given cost allocation.following, define EIR(Dc , C) expected improvement set random133fiAzimi, Fern, & Fernexperiments total cost C Q constrained experiment returnedCMCH, . Also, let EI(Q ) expected improvement constraint experiment Qc cost. parameter-free CMC policy defined as:CMCH (Dc , Bc ) = CMCH, (Dc , Bc )= arg max{ [0, 1] | EI(Q |Dc ) EIR(Dc , dc e)}.(12)practice compute EIR(Dc , C) EI(Q ) via Monte Carlo simulation.straightforward process cases. EIR compute one EIR sample, randomlyselect set experiments within budget C sample outcomes experiments via Gaussian Process conditioned Dc . improvement best outcometaken result EIR sample. estimate EIR average LEIR samples. similar process used EI, except rather drawing randomexperiments sample use experiments Q .following steps summarize overall computational process CMC-MEI.1. Compute h maximizing H constrained experiments fall within current budget. Note requires optimizing set minimum-sizedconstrained experiments cost less budget.2. Perform discretized line search find according Equation 12.3. Return CMCH, (Dc , Bc ) according Equation 11.6. Experimental Resultsgoal evaluate performance proposed policies scenarios resemble typical real-world scientific applications. particular, experimental domainsmotivate work paper focus low-dimensional optimization problems.choice based two reasons. First, typical budgets number total experimentsoften limited, makes optimizing many dimensions impractical. practice,scientists often carefully select key dimensions consider. Second,real-world applications, motivating problem, prohibitively difficult satisfy constraints couple experimental variables. Thus, relevantscenarios us many problems moderate numbers experiments smalldimensionality.6.1 Experimental Setupdescribe set experiments.6.1.1 Test Functionsevaluate policies using five 2-dimensional functions [0, 1]2 . first three functions: Cosines, Rosenbrock, Discontinuous benchmarks widely usedprevious studies stochastic optimization (Anderson, Moore, & Cohn, 2000; Brunato,Battiti, & Pasupuleti, 2006; Azimi et al., 2010). mathematical expressionsfunctions listed Table 1 contour plots given Figure 1.134fiBudgeted Optimization Constrained ExperimentsCosinesRosenbrockHydrogenFuel CellDiscontinuousFigure 1: Contour plots test functions.135fiAzimi, Fern, & FernTable 1: Benchmark Functions.FunctionCosinesRosenbrockDiscontinuousMathematical representation1 (u2 + v 2 0.3cos(3u) 0.3 cos(3v)),u = 1.6x 0.5, v = 1.6y 0.510 100(y x2 )2 (1 x)21 2((x 0.5)2 + (y 0.5)2 ) x < 0.5,0 otherwisetwo remaining functions derived real-world experimental data sets involving hydrogen production motivating fuel cell application. formerutilize data collected part study biosolar hydrogen production (Burrows, Wong,Fern, Chaplen, & Ely, 2009), goal maximize hydrogen productioncyanobacteria Synechocystis sp. PCC 6803 optimizing pH Nitrogen levelsgrowth media. data set contains 66 samples uniformly distributed 2-d inputspace. data used simulate true function fitting Gaussian Process (GP)model RBF kernel, picking kernel parameters via standard validation techniquescross validation. model simulated experimental design processsampling posterior GP obtain noisy outcomes requested experiments.purpose, used zero-mean Gaussian noise model variance equal 0.01.See Figure 1 contour plot.motivating application (described introduction), utilize dataset initial microbial fuel cell experiments using anodes different nano-enhancements.particular, anode coated gold nano-particles different fabricationconditions leading varying particle densities, shapes, sizes. constructionanode required approximately two days.1 anode installed microbial fuelcell run using pure Shewanella oneidensis bacterial cultures grown fed-batch modeone week recording current density regular intervals. temporally averagedcurrent density taken dependent variable optimized modifyingnano-structure. characterize nano-structure anode, captured images usingscanning electron microscopy used standard image processing software compute twofeatures: average area individual particles, average circularity individual particles.features selected independent variables design sinceroughly controlled fabrication process appear influence currentdensity. Unfortunately, due high cost running experiments, preciselymotivation paper, data set currently consists 16 data points,relatively uniformly distributed experimental space. Due sparse data,utilize polynomial Bayesian regression degree 4, rather Gaussian processesRBF kernels, simulate true function. See Figure 1 contour plot.1. first round experiments constraints provided scientist constructing anodes.Rather goal generate diverse set anodes provide good set data seedingexperimental design process. construction time would likely two days presenceconstraints since number growth conditions trials would necessary.136fiBudgeted Optimization Constrained Experiments6.1.2 Model Definitionswork, assume density px (x|Q) experiments given query Quniform ranges specified Q.compute conditional posterior p(y|x, D) required non-sequential approachmodel-based sequential heuristics, use Gaussian process (Rasmussen & Williams,2006) zero mean prior covariance specified RBF kernel function:1|xi xj |2 ),(13)2length scale parameter considered distancemove input space function value changes significantly, f signalvariance specifies maximum possible variance point. paper2set = 0.02 signal variance f = ymax, ymax upper bound outputvalues (this typically easy elicit scientists serves set prior uncertaintynon-trivial probability assigned expected range output).optimize parameters, empirically verify GP behaved reasonablytest functions.cov (xi , xj ) = k(xi , xj ) = f exp(6.1.3 Cost Functionmotivating application, cost setting running fuel cell experimentgiven constrained experiment request roughly considered two components.first component corresponds cost setting experiment (producingnano-structure) satisfies given constraints, variable dependingsize constraints. tighter constraints, costly be.second component corresponds cost running constrained experiment,generally constant. two-component cost structure common real-worldapplications portion experimental process controlled preciselyuniform costs across different queries, portions less controllablecost inversely proportional size constraints place them.capture structure, define following cost function fc () : Q <+ :slopefc (Q) = 1 +.(bi ai )(14)i=1formulation, constant one captures stationary part cost,second term captures variable portion inversely proportional sizeconstrains query Q. value slope parameter dictates quicklycost increases size constrained experiment decreases. evaluate proposedapproaches considering three different slope values; slope = 0.1, 0.15, 0.30. Noteproposed approaches readily applied cost functions.6.1.4 Discretizing Input Spacementioned previously, policies assume input space discretized. particular, divide input dimension 100 equal-length subintervals. Note137fiAzimi, Fern, & Fernimplementation appropriate low dimensional optimization problems,described previously situation often encounter real-world applications.6.1.5 Evaluation Settingsevaluation, test proposed policies comparison random policy(i.e., policy always selects entire input space constrained experiment).Given budget B function f () optimized, run policy resultsset observed experiments Dc . Let x experiment Dc predictedmaximum expected outcome . regret policy particular rundefined ymax , ymax maximum value f (). test functionchoice budget cost structure (i.e., choice slope), evaluate policyaveraging regret 200 runs. run starts n = 5 randomly selected initialpoints = {(x0 , y0 ), , , (x5 , y5 )}, policies used select constrainedexperiments budget runs out, point regret measured. orderease comparison regret values across different functions, report normalizedregret values, computed dividing regret policy mean regretachieved random policy. normalized regret less one indicates approachoutperforms random, value greater one indicates approach worserandom. first round experiments, fixed total budget B = 15examine effect cost-model slope parameter values 0.1, 0.15 0.3. laterexperiments, consider larger budgets.Note non-sequential policy used consume experimental budgetonce. However, practice typically limit number constrainedexperiments processed simultaneously due limited resources. such,non-sequential setting policy used select five simultaneous queries subjectbudget constraint. repeat process budget consumed.run time selecting single experiment sequential setting orderminutes (generally five) experiments un-optimized matlab implementation. run time selecting batch five fewer queries never30 minutes.6.2 Results Discussionsresults individual functions shown Table 2, corresponding standarddeviations shown inside parentheses. first row table presents resultsnon-sequential greedy algorithm (NS-Greedy). Rows 2 6 show performancemodel-based sequential policies CMC CN cost policies. Note that,CN policy, report results CN-MEI, performed best among CN policies.addition, nice interpretation maximizing rate expected improvement perunit cost. Finally, last row shows performance model-free sequential policies.order provide assessment overall performance different methods, Table 3presents normalized regrets policy averaged across five functions.different columns table correspond different slope values cost function.discuss results different methods detail.138fiBudgeted Optimization Constrained ExperimentsTable 2: Normalized regrets individual functions varying cost models (i.e., slopes)slope = 0.1slope = 0.15slope = 0.30Cosines, Normalized Regret (95% Confidence Interval)NS-Greedy0.767 (0.04)0.838 (0.05)0.841 (0.05)CN-MEI0.569 (0.05)0.714 (0.06)0.826 (0.06)CMC-MEI0.417 (0.04)0.514 (0.06)0.794 (0.06)CMC-MPI(0.2)0.535 (0.05)0.584 (0.06)0.616 (0.06)CMC-MUI0.797 (0.06)0.804 (0.06)0.817 (0.06)CMC-MM0.708 (0.07)0.767 (0.07)0.736 (0.06)RR/BRR0.84(0.06)/0.83(0.06)0.86(0.06)/0.86(0.06)0.89(0.06)/0.88(0.06)Discontinuous, Normalized Regret (95% Confidence Interval)NS-Greedy0.528 (0.06)0.690 (0.06)0.748 (0.05)CN-MEI0.527 (0.06)0.497 (0.06)0.626 (0.08)CMC-MEI0.564 (0.06)0.677 (0.08)0.779 (0.09)0.954 (0.11)0.940 (0.10)0.951 (0.11)CMC-MPI(0.2)CMC-MUI0.710 (0.10)0.709 (0.11)0.693 (0.09)CMC-MM1.289 (0.15)1.225 (0.16)1.116 (0.16)RR/BRR0.60(0.07)/0.58(0.07)0.61(0.07)/0.60(0.07)0.63(0.08)/0.63(0.08)Rosenbrock, Normalized Regret (95% Confidence Interval)NS-Greedy0.650 (0.05)0.877 (0.06)0.930 (0.06)CN-MEI0.602 (0.06)0.665 (0.07)0.736 (0.08)CMC-MEI0.547 (0.35)0.556 (0.39)0.630 (0.47)CMC-MPI(0.2)0.503 (0.05)0.594 (0.06)0.608 (0.07)CMC-MUI0.805 (0.11)0.974 (0.16)0.913 (0.14)CMC-MM0.721 (0.09)0.740 (0.10)0.662 (0.08)RR/BRR0.89(0.12)/0.88(0.12)0.93(0.12)/0.92(0.12)0.96(0.14)/0.95(0.14)Hydrogen, Normalized Regret (95% Confidence Interval)NS-Greedy0.879 (0.06)0.969 (0.08)0.993 (0.09)CN-MEI0.176 (0.04)0.354 (0.06)0.852 (0.09)CMC-MEI0.129 (0.04)0.233 (0.06)0.420 (0.07)CMC-MPI(0.2)0.408 (0.09)0.449 (0.10)0.613 (0.10)CMC-MUI0.716 (0.08)0.695 (0.08)0.868 (0.09)0.728 (0.11)0.605 (0.10)0.691 (0.11)CMC-MMRR/BRR1.10(0.09)/1.06(0.09) 1.16(0.10)/1.23(0.10) 1.17(0.09)/1.14(0.09)Fuel Cell, Normalized Regret (95% Confidence Interval)NS-Greedy0.980 (0.02)0.985 (0.02)0.995 (0.03)CN-MEI0.929 (0.02)0.950 (0.02)0.986 (0.03)CMC-MEI0.931 (0.02)0.908 (0.02)0.940 (0.02)CMC-MPI(0.2)0.932 (0.02)0.930 (0.03)0.943 (0.03)CMC-MUI0.971 (0.03)0.973 (0.03)0.995 (0.03)CMC-MM0.945 (0.03)0.963 (0.04)0.963 (0.05)RR/BRR1.03(0.03)/1.02(0.03)1.04(0.03)/1.04(0.03)1.04(0.03)/1.04(0.03)139fiAzimi, Fern, & FernTable 3: Normalized Overall Regrets.slope = 0.1 slope = 0.15 slope = 0.30NS-Greedy0.7600.8710.901CN-MEI0.5600.6360.805CMC-MEI0.5170.5780.712CMC-MPI(0.2)0.6660.6980.746CMC-MUI0.8000.8310.857CMC-MM0.8740.8890.834RR0.8970.9250.944BR0.8790.9110.9346.2.1 Non-Sequentialfirst examine performance non-sequential greedy policy (NS-Greedy).Recall present normalized regret results, thus smaller value indicatesbetter performance. Further, policy outperforms random whenever normalized regretless 1.Table 2, observe proposed greedy algorithm (NS-Greedy) performsconsistently better random policy functions. Among functions,seen performance advantage NS-Greedy significant slopeparameter cost function smaller. consistent expectation:smaller slope, cost query grows slower tighten constraints.allow algorithm aggressively select tighter constraints based posteriormodel function. fact, slope large enough, one would expect optimalpolicy completely random.Comparing sequential approaches, first observe NS-Greedy compared favorably two model-free methods. surprising RR/BRRconsider posterior model function selecting queries. hand, alsoobserve NS-Greedy algorithm performs significantly worse best modelbased sequential policies, CMC-MEI. result expected sequentialpolicies allow us update improve model function query. Therefore, generally expect sequential policies perform better non-sequential methodscommon phenomenon active learning literature (Azimi, Fern, Fern, Borradaile, & Heeringa, 2012).6.2.2 Sequentialsection examine performance sequential policies, including modelfree model-based methods.Model-Free Policies. Table 3 see RR BRR achieve improvementrandom approximately 10% across different slopes. shows heuristictrying evenly cover space pays compared random. BRR also observedperform slightly better RR, indicates additional exploitive behaviorBRR pays overall. Looking individual results Table 2, see140fiBudgeted Optimization Constrained ExperimentsHydrogen Fuel Cell functions, BRR RR perform worse random.investigation reveals reason poor performance RR/BRR biastoward experiments near center input space. bias result factconstrained experiments (hyper-rectangles) required fall completely withinexperimental space fewer hyper-rectangles contain points nearedges corners. Hydrogen Fuel Cell functions optimal points nearcorners space, explaining poor performance.Model-Based Policies. focus performance proposed model-basedsequential policies. averaged overall results (Table 3), first observationmodel-based policies general perform better random policy. Specifically,looking results individual functions, see model-based policies outperformrandom, exception CMC-MM Discontinuous function. showstwo proposed approaches considering cost able avoid catastrophic choicesexpend budget quickly warranted.analysis poor performance CMC-MM Discontinuous function revealed CMC-MM would often get stuck poor local optima cease explorespace adequately. Although step CMC-MM policy determined selectionbetter random near term, translate long term improvementrandom due lack exploration. Discontinuous function particularlyprone elicit behavior due fact large sub-optimal nearlyuniform region, difficult CMC-MM escape from. overly greedy performance consistent prior observations MM heuristic largely addressedheuristics provide measure exploratory value. fact, CMC-MMhighly dependent initial given random points. example, initial given pointschosen non-optimal region, 50% input spaceDiscontinuous function, CMC-MM approach cannot give satisfactory performance. seen standard deviation CMC-MM, highermodel-based model-free methods. shows performance CMC-MMchanges significantly iteration initial points.addition, Table 3, seen model-based approaches outperform model free approaches. indicates heuristics consideringGP probabilistic model providing useful information effectively guidingconstrained experiment selection.Comparing different model-based heuristics, see MEI-based methods (CNMEI CMC-MEI) top contenders among methods. Examining resultsindividual functions, see holds functions except Rosenbrock,CMC-MPI slightly better MEI-based methods. Upon closer examinationbehavior MPI MEI heuristics, found MPI often selects slightlyfewer experiments MEI, believe due fact MEI heuristic tendssmoother MPI experimental space. smoothness MEI allowsCMC policy select less constrained queries still achieve approximately optimalheuristic value, leading constrained experiments. general recommend CMCMEI preferable heuristic use based consistently superior performancefact parameter free.141fiAzimi, Fern, & Fern0.5CMCMEICNMEICMCMPINSGreedyRandom0.4Regret0.4Regret0.5CMCMEICNMEICMCMPINSGreedyRandom0.30.30.20.20.10.1102030Budget40500106020CosinesBudget405060Rosenbrock0.350.6CMCMEICNMEICMCMPINSGreedyRandom0.30.25CMCMEICNMEICMCMPINSGreedyRandom0.55Regret0.20.150.10.50.450.050102030Budget40500.410602030HydrogenBudget4050Fuel CellCMCMEICNMEICMCMPINSGreedyRandom0.10.08RegretRegret300.060.040.02102030Budget405060DiscontinuousFigure 2: Un-normalized regret function budget (slope=0.1).14260fiBudgeted Optimization Constrained Experimentsalso interested comparing performance two proposed schemeshandling cost, namely CN CMC. Focusing CMC-MEI CN-MEI, seeCMC-MEI generally outperforms CN-MEI. differences behaviortwo policies appear subtle, experimental investigation show CN-MEI tendsoverly conservative toward selecting costly experiments comparison CMC-MEI,especially later stages experimental process.6.3 Varying Budgetround experiments, fixed cost model slope 0.1 varied budget10 60 units increments 10. interested examining relative performancedifferent model-based policies (including sequential non-sequential) comparedrandom policy increase budget.Figure 2 plots absolute regret (rather normalized regret) versus budgetbest sequential policies including CMC-MEI, CN-MEI CMC-MPI,proposed non-sequential policy (NS-Greedy). also plotted performancerandom policy reference baseline. use experimental setting usedpreviously. Specifically, sequential methods, iteration select one querybudget completely consumed. proposed non-sequential approach, selectfive queries iteration budget consumed.First, observe performance NS-Greedy continues dominate Randomincrease budget. suggests performance advantage NS-GreedyRandom robust amount experimental budget. also observe NS-Greedygenerally outperformed lead sequential policies, CMC-MEI, CMCMPI. consistent previous observations fixed budget varying slope.Finally, see polices based MEI MPI heuristics generally achieve bestperformance across wide range budgets. particular, consistently maintainsignificant advantage Random. MEI-based CMC-MPI policies roughlycomparable functions except Fuel Cell function. case CMC-MPIslightly outperforms CMC-MEI large budgets.Overall, given results previous experiments, CMC-MEI still considered recommended method, due combination good performance, smoothnessrobustness. CMC-MEI also preferable require selectionmargin parameter.6.4 Comparison Precise Experimentssection, compare performance using constrained experiments performance achieved using precisely specified experiments. particular, compare CMC-MEIprecise counterpart MEI. this, use CMC-MEI select fifteenconstrained experiments (with infinite budget) function, step evaluateregret. repeated 100 times generate average performance curveCMC-MEI function number constrained experiments. done twodifferent cost models slope set 0.1 0.3 respectively, resulting two curvesCMC-MEI. Similarly, use MEI select sequence fifteen precisely specified experiments generate average performance curve (over 100 random runs). Finally,143fiAzimi, Fern, & Fern0.80.7MEICMCMEI(0.1)CMCMEI(0.3)Random1Regret0.6Regret1.5MEICMCMEI(0.1)CMCMEI(0.3)Random0.50.40.50.30.20.115100115# Experiments5Cosines0.5MEICMCMEI(0.1)CMCMEI(0.3)Random0.7Regret0.20.60.50.40.1510115# Experiments510# ExperimentsHydrogenFuel Cell0.2MEICMCMEI(0.1)CMCMEI(0.3)Random0.15RegretRegret0.80.30015RosenbrockMEICMCMEI(0.1)CMCMEI(0.3)Random0.410# Experiments0.10.0501510# Experiments15DiscontinuousFigure 3: Un-normalized regret function number experiments.14415fiBudgeted Optimization Constrained Experimentsreference point, also plot performance experiments selected randomly.Figure 3 shows performance curves MEI, CMC-MEI (with slope = 0.1 0.3 respectively) random.figure see cases CMC-MEI performed comparably MEI.cases, observe detrimental effective use constrained experiments.compare efficiency CMC-MEI slopes 0.3 0.1 (larger slopes yield higherexperimental costs), see cases comparable. However, FuelCell Hydrogen functions, smaller slope consistently better (by small margin).Further, also two functions precise experiments show significantadvantage CMC-MEI (in particular slope =0.3). likely explanationoptimal regions two functions fairly small, highly peaked nearboundaries. make difficult effectively explore region using constrainedexperiments, especially larger slopes.6.5 Comparison Constant Window Experimentsfinal experiments, compare performance CMC-MEI approachConstant Window (CW) approach, constant constraint sizes used throughoutoptimization process. goal understand importance dynamically selectingwindow size done CMC-MEI. consider three different window sizes, denotedCW5, CW20, CW50, correspond constraint sizes 5%, 20% 50%dimension respectively. Thus, cost CW5 significantly CW50precision control final selected samples. compare CWapproaches CMC-MEI. cost model parameter set slope = 0.1budget varied 10 60 denomination 10. results provided Figure 4.First, observe best performing CW approach varies significantly acrossbenchmarks budgets. indicates choosing window size particularapplication non-trivial. Second, see CMC-MEI, adaptively selectswindow size, performs best competitive best CW approach.another indication CMC-MEI effective strategy choosing window sizes.analysis experiments indicates CMC-MEI tends select experiments closeCW50 beginning decreases window size several experiments.7. Summary Future DirectionsMotivated real-world application, paper introduced novel framework budgeted Bayesian optimization constrained experiments. framework, insteadasking samples unknown function precisely specified inputs, ask constrained experiment cost constrained experiments variable dependingtightness constraints. studied problem two different settings.non-sequential setting, multiple constrained experiments selected once.setting, introduced non-decreasing submodular objective function presentedgreedy algorithm approximately optimizing proposed objective. Empirical evaluationindicates proposed non-sequential algorithm consistently outperforms baselinerandom policy across different budget cost configurations.145fiAzimi, Fern, & Fern0.60.4CMCMEICW 5CW 20CW 500.30.4RegretRegret0.50.30.20.10.20.110CMCMEICW 5CW 20CW 5020304050010602030Budget405060BudgetCosinesRosenbrock0.25CMCMEICW 5CW 20CW 50Regret0.150.10.60.50.05010203040500.4106020304050BudgetBudgetHydrogenFuel Cell0.12CMCMEICW 5CW 20CW 500.10.08RegretRegret0.2CMCMEICW 5CW 20CW 500.70.060.040.020102030405060BudgetDiscontinuousFigure 4: CMC-MEI performance versus constant window size experiments.14660fiBudgeted Optimization Constrained Experimentssequential setting problem, one constrained experiment selectediteration. extended number classic Bayesian optimization experiment designheuristics constrained experiments. Direct use heuristics select constrainedexperiments select overly tight constraints consume budget once. Thus,introduced two general cost policies, namely CN CMC, achieve balancemoderating cost experiments optimizing heuristics. experiments showsequential policies generally outperform non-sequential policy, proposed CNCMC cost policies effective dispensing budget rationally. Overall foundCMC used MEI heuristic (CMC-MEI) demonstrated robust performanceparameter-free, making recommended method.work described focused methods optimizing low-dimensional functions,typical types scientific engineering applications motivatedwork. Extending methods higher dimensions requires optimizing selection criteriacontinuous rather discrete input spaces. number straightforwardapproaches future work could include evaluating approachesdesigning sophisticated ones. interesting direction future workcontinue enriching cost action models supported Bayesian Optimization methodsclosely match needs real-world applications. Solutions extendedmodels require tighter integration planning scheduling techniquesideas developed far traditional Bayesian Optimization.Acknowledgmentsresearch supported NSF grant IIS 1320943.Appendix A. Proof Lemma 1Lemma 1. Let XM = {x1 , . . . , xM } denote random variables representing outcomerandom draw QM = {Q1 , ..., QM } respectively QM set possiblequeries. given XM , JXM (S), returns expected maximum setjointly distributed random variables, monotonically non-decreasing submodular setfunction.Proof. Suppose finite set. g : 2S R+ submodular set functionS1 S2 x \ S2 , holds g(S1 {x}) g(S1 ) g(S2 {x}) g(S2 ).addition set function g() called monotonically non-decreasing g(S1 ) g(S2 ).first prove E[max()] monotonic function showsubmodular objective function.Assume S1 = {x1 , x2 , , xp } p k. need provefifihhfifiE max (y1 , y2 , , yp , , yk ) fiD E max (y1 , y2 , , yp ) fiD .147(15)fiAzimi, Fern, & Fernuse definition expectation prove result.fifiE max (y1 , y2 , , yp , , yk ) fiDZZ= max (y1 , y2 , , yp , , yk ) py1 ,y2 ,,yp ,,yk |D dy1 dy2 dyp dykZZmax (y1 , y2 , , yp ) py1 ,y2 ,,yp ,,yk |D dy1 dy2 dyp dykZZZZ= max (y1 , y2 , , yp )py1 ,y2 ,,yp ,,yk |D dyp+1 dyk dy1 dy2 dypZZ= max (y1 , y2 , , yp ) py1 ,y2 ,,yp |D dy1 dy2 dypfihfi= E max (y1 , y2 , , yp ) fiD .(16)hshows E[max()] nondecreasing monotonic function.prove submodularity property, need showfifihhfifiE max (y1 , y2 , , yp , ) fiD E max (y1 , y2 , , yp ) fiDfifihhfifiE max (y1 , y2 , , yp , , yk , ) fiD E max (y1 , y2 , , yp , , yk ) fiD .(17)prove this, start right hand side inequality basic definitionexpectation.fifihhfifiE max (y1 , y2 , , yp , , yk , ) fiD E max (y1 , y2 , , yp , , yk ) fiDZZ= max (y1 , y2 , , yp , , yk , ) py1 ,y2 ,,yp ,,yk ,y |D dy1 dy2 dyp dyk dyZZmax (y1 , y2 , , yp , , yk ) py1 ,y2 ,,yp ,,yk |D dy1 dy2 dyp dykZZ= max (y1 , y2 , , yp , , yk , ) py1 ,y2 ,,yp ,,yk ,y |D dy1 dy2 dyp dyk dyZZmax (y1 , y2 , , yp , , yk ) py1 ,y2 ,,yp ,,yk ,y |D dy1 dy2 dyp dyk dyZZ= [max (y1 , y2 , , yp , , yk , ) max (y1 , y2 , , yp , , yk )](18)148fiBudgeted Optimization Constrained Experimentspy1 ,y2 ,,yp ,,yk ,y |D dy1 dy2 dyp dyk dyZZ[max (y1 , y2 , , yp , ) max (y1 , y2 , , yp )]py1 ,y2 ,,yp ,,yk ,y |D dy1 dy2 dyp dyk dyZZ=Z[max (y1 , y2 , , yp , ) max (y1 , y2 , , yp )] py1 ,y2 ,,yp ,y |D dy1 dy2 dyp dyZmax (y1 , y2 , , yp , ) py1 ,y2 ,,yp ,y |D dy1 dy2 dyp dyZZmax (y1 , y2 , , yp ) py1 ,y2 ,,yp |D dy1 dy2 dypfifihhfifi= E max (y1 , y2 , , yp , ) fiD E max (y1 , y2 , , yp ) fiD=(19)Notice inequality holds prove:max (y1 , y2 , , yp , , yk , ) max (y1 , y2 , , yp , , yk )max (y1 , y2 , , yp , ) max (y1 , y2 , , yp )(20)two possible cases follows:(max (y1 , y2 , , yp , , yk , ) =max (y1 , y2 , , yp , , yk ) .(21)1. first case, max (y1 , y2 , , yp , , yk , ) = ,also max (y1 , y2 , , yp , ) = . Hence,max (y1 , y2 , , yp , , yk , ) max (y1 , y2 , , yp , , yk )= max (y1 , y2 , , yp , , yk )max (y1 , y2 , , yp )(22)= max (y1 , y2 , , yp , ) max (y1 , y2 , , yp )2. second case, max (y1 , y2 , , yp , , yk , ) = max (y1 , y2 , , yp , , yk ),thenmax (y1 , y2 , , yp , , yk , ) max (y1 , y2 , , yp , , yk )=0max (y1 , y2 , , yp , ) max (y1 , y2 , , yp )= max (y1 , y2 , , yp , ) max (y1 , y2 , , yp )Notice max (y1 , y2 , , yp , ) max (y1 , y2 , , yp ) always non-negative.149(23)fiAzimi, Fern, & FernReferencesAnderson, B., Moore, A., & Cohn, D. (2000). nonparametric approach noisy costlyoptimization. ICML.Azimi, J., Fern, A., & Fern, X. (2010). Batch bayesian optimization via simulation matching.NIPS, pp. 109117.Azimi, J., Fern, A., Fern, X. Z., Borradaile, G., & Heeringa, B. (2012). Batch active learningvia coordinated matching. ICML.Azimi, J., Fern, X., Fern, A., Burrows, E., Chaplen, F., Fan, Y., Liu, H., Jaio, J., & Schaller,R. (2010). Myopic policies budgeted optimization constrained experiments.AAAI.Bond, D. R., & Lovley, D. R. (2003). Electricity production geobacter sulfurreducensattached electrodes. Applications Environmental Microbiology, 69, 15481555.Brochu, E., Cora, V. M., & De Freitas, N. (2010). tutorial bayesian optimizationexpensive cost functions, application active user modeling hierarchicalreinforcement learning. arXiv preprint arXiv:1012.2599.Brunato, M., Battiti, R., & Pasupuleti, S. (2006). memory-based rash optimizer. AAAI06 Workshop Heuristic Search, Memory Based Heuristics applications.Burrows, E. H., Wong, W.-K., Fern, X., Chaplen, F. W., & Ely, R. L. (2009). Optimizationph nitrogen enhanced hydrogen production synechocystis sp. pcc 6803via statistical machine learning methods. Biotechnology Progress, 25, 10091017.Chapelle, O., & Li, L. (2011). empirical evaluation thompson sampling. Advancesneural information processing systems, pp. 22492257.Cox, D. D., & John, S. (1992). statistical method global optimization. IEEEConference Systems, Man Cybernetics, pp. 12411246.Cox, D. D., & John, S. (1997). Sdo: statistical method global optimization.Multidisciplinary Design Optimization: State-of-the-Art, pp. 315329.Desautels, T., Krause, A., & Burdick, J. W. (2014). Parallelizing exploration-exploitationtradeoffs gaussian process bandit optimization. Journal Machine LearningResearch, 15 (1), 38733923.Deshpande, A., Guestrin, C., Madden, S. R., Hellerstein, J. M., & Hong, W. (2004). Modeldriven data acquisition sensor networks. VLDB 04: Proceedings Thirtiethinternational conference large data bases, pp. 588599. VLDB Endowment.Elder, J.F., I. (1992). Global rd optimization probes expensive: grope algorithm. IEEE International Conference Systems, Man Cybernetics, pp.577582.Fan, Y., Hu, H., & Liu, H. (2007). Enhanced coulombic efficiency power densityair-cathode microbial fuel cells improved cell configuration. Journal PowerSources, press.Ginsbourger, D., Riche, R. L., & Carrarog, L. (2010). Kriging well-suited parallelizeoptimization..150fiBudgeted Optimization Constrained ExperimentsGoel, A., Guha, S., & Munagala, K. (2006). Asking right questions: model-driven optimization using probes. PODS 06: Proceedings twenty-fifth ACM SIGMODSIGACT-SIGART symposium Principles database systems, pp. 203212.Jones, D. R. (2001). taxonomy global optimization methods based response surfaces.Journal Global Optimization, 21, 345383.Khuller, S., Moss, A., & Naor, J. (1999). budgeted maximum coverage problem. Inf.Process. Lett., 70 (1), 3945.Krause, A., & Guestrin, C. (2005). note budgeted maximization submodularfunctions. Technical Report, CMU-CALD-05-103.Krause, A., Singh, A., & Guestrin, C. (2008). Near-optimal sensor placements Gaussian processes: Theory, Efficient Algorithms Empirical Studies. JournalMachine Learning Research, 9, 235284.Lizotte, D., Madani, O., & Greiner, R. (2003). Budgeted learning naive-bayes classifiers.UAI.Locatelli, M. (1997). Bayesian algorithms one-dimensional global optimization. JournalGlobal Optimization, 10 (1), 5776.Madani, O., Lizotte, D., & Greiner, R. (2004). Active model selection. UAI.Moore, A., & Schneider, J. (1995). Memory-based stochastic optimization. NIPS.Moore, A., Schneider, J., Boyan, J., & Lee, M. S. (1998). Q2: Memory-based active learningoptimizing noisy continuous functions. ICML, pp. 386394.Myers, R. H., Montgomery, D. C., & Anderson-Cook, C. M. (1995). Response surfacemethodology: process product optimization using designed experiments. Wiley.Park, D. H., & ZeikusG, J. G. (2003). Improved fuel cell electrode designs producingelectricity microbial degradation. Biotechnol Bioeng, 81 (3), 348355.Rasmussen, C. E., & Williams, C. K. I. (2006). Gaussian Processes Machine Learning.MIT.Reguera, G., McCarthy, K. D., Mehta, T., Nicoll, J. S., Tuominen, M. T., & Lovley, D. R.(2005). Extracellular electron transfer via microbial nanowires. Nature, 10981101.Ross, A. M. (2008). Computing Bounds Expected Maximum Correlated NormalVariables . Methodology Computing Applied Probability.Schneider, J., & Moore, A. (2002). Active learning discrete input spaces. InterfaceSymposium.Schonlau, M. (1997). Computer Experiments Global Optimization. Ph.D. thesis, University Waterloo.Silberstein, A., Braynardand, R., Ellis, C., Munagala, K., & Yang, J. (2006). samplingbased approach optimizing top-k queries sensor networks. ICDE 06: Proceedings 22nd International Conference Data Engineering, p. 68, Washington,DC, USA. IEEE Computer Society.151fiAzimi, Fern, & FernSnoek, J., Larochelle, H., & Adams, R. P. (2012). Practical bayesian optimization machinelearning algorithms. Advances neural information processing systems, pp. 29512959.Srinivas, N., Krause, A., Kakade, S., & Seeger, M. (2010). Gaussian process optimizationbandit setting: regret experimental design. Proc. InternationalConference Machine Learning (ICML).Stuckman, B. E. (1988). global search method optimizing nonlinear systems. IEEEtransactions systems, man, cybernetic, Vol. 18, pp. 965977.Tatbul, N., Cetintemel, U., Zdonik, S. B., Cherniack, M., & Stonebraker, M. (2003). Loadshedding data stream manager. VLDB 2003: Proceedings 29th international conference large data bases, pp. 309320. VLDB Endowment.152fiJournal Artificial Intelligence Research 56 (2016) 61-87Submitted 09/15; published 05/16Automatic Wordnet Development Low-ResourceLanguages using Cross-Lingual WSDNasrin Taghizadehnsr.taghizadeh@ut.ac.irSchool Electrical Computer EngineeringCollege Engineering, University Tehran, Tehran, IranHesham Failihfaili@ut.ac.irSchool Electrical Computer EngineeringCollege Engineering, University Tehran, Tehran, IranAbstractWordnets eective resource natural language processing informationretrieval, especially semantic processing meaning related tasks. far, wordnetsconstructed many languages. However, automatic development wordnets low-resource languages well studied. paper, ExpectationMaximization algorithm used create high quality large scale wordnets poorresource languages. proposed method benefits possessing cross-lingual word sensedisambiguation develops wordnet using bi-lingual dictionary monolingual corpus. proposed method executed Persian languageresulting wordnet evaluated several experiments. results showinduced wordnet precision score 90% recall score 35%.1. IntroductionOne important projects natural language processing yearsconstruction English wordnet (WordNet) Princeton University direction George A. Miller (1995). WordNet consists lexical database, Englishwords grouped sets cognitive synonyms called synsets. eectiveness WordNet wide range language technology applications inspired many researchers createwordnets languages. first attempts led construction EuroWordNet (Vossen, 1998) BalkaNet (Tufis, Cristea, & Stamou, 2004). EuroWordNetdeals European languages English, Dutch, German, French, Spanish, Italian,Czech Estonian; BalkaNet covers languages Balkan zone. interconnect wordnets dierent languages, EuroWordNet links synsets languageinterlingual index (ILI). ILI allows find equivalent synsets across languagesconnected ILI.Although first wordnet created manually, several automatic semi-automatictechniques used developing wordnets. methods usuallydivided merge expansion approaches (Fellbaum & Vossen, 2012; Oliver & Climent, 2012; Erjavec & Fiser, 2006). However, methods combine mergeexpansion models benefit advantages approaches (Prabhu, Desai,Redkar, Prabhugaonkar, Nagvenkar, & Karmali, 2012; Apidianaki & Sagot, 2014).merge approach, small wordnet created manually, contains high-levelc2016AI Access Foundation. rights reserved.fiTaghizadeh & Failibasic concepts. Next, small wordnet developed using automatic semi-automatictechniques. process, mono-lingual resources language-specific propertiesemployed. Wordnets created manner later mapped onto either WordNetILI. using expansion approach, multilingual wordnet constructed translating words inside synsets WordNet (or existing wordnets) targetlanguage using multi-lingual resources. Therefore structure original wordnetpreserved words translated.Among dierent methods proposed wordnet construction, applicable low-resource languages. Methods follow merge approach labourintensive time-consuming. Moreover, need vast knowledge language also require many resources, main obstacle low-resource languages- makes approach inapplicable practice. hand, methodsfollow expansion approach usually adopt WordNet structure find correcttranslation associated words WordNet synsets target language.process, multilingual resources comparable corpora (Kaji & Watanabe, 2006),parallel corpora (Oliver & Climent, 2012; Kazakov & Shahid, 2009; Fiser, 2009; Diab, 2004),thesaurus (Gunawan & Saputra, 2010), machine translators (Saveski & Trajkovski, 2010)multiple bi-lingual machine readable dictionaries (Atserias, Climent, Farreres, Rigau,& Rodrguez, 2000; Patanakul & Charnyote, 2005; Bond, Isahara, Kanzaki, & Uchimoto,2008; Lam, Al Tarouti, & Kalita, 2014) used, causes bottleneck low-resourcelanguages.Taking deeper look expansion-based methods, synset WordNetkept words associated translated target language. bi-lingualdictionary usually employed English words inside WordNet synsets translated.Since dictionaries translate word sense word sense, rather word word,translations ambiguous disambiguated. Looking carefully,translating English words inside WordNet synset, set candidate words targetlanguage obtained; equivalent senses English wordsthus omitted. Methods following expansion approach rankcandidate words omit low-rated ones candidate sets. task scoringcandidate words WordNet synsets considered optimization problem, (sub)optimal values found using algorithms Expectation-Maximization (Montazery& Faili, 2011). proposed method extension work low-resource languages.paper, problem automatically constructing large scale high qualitywordnets low-resource languages studied. two major approaches, mergeexpansion, first one suitable; requires vast knowledgetarget language also many language resources. preferred approach utilize wordnets languages adopting structure translating content.Finding correct senses target language words AI-complete problem (Mallery,1988), is, analogy NP-completeness complexity theory, problemwhose diculty equivalent solving central problems AI (Navigli, 2009).paper, iterative optimization method based cross-lingual WSD proposed findlocal optimum problem reasonable time. main idea iterativelyimprove estimation probability selecting WordNet synsets wordstarget language. Additionally, proposed method needs resources suitable62fiAutomatic Wordnet Development Low-Resource Languagespoor-resource languages. investigate performance proposed method, Persian selected poor-resource language resulting wordnet examinedconducting several experiments.roadmap paper follows: Section 2 presents related works; Section 3explains wordnet construction problem proposed formulation; Section 4 presentscase study Persian language error analysis; conclusions given futureworks suggested last section, Section 5.2. Related Worksection, automatic methods constructing wordnets reviewedbased expansion approach. main stage expansion-based methods findingset words lexicalizes concept captured given synset existing wordnetanother language. candidate words usually extracted dictionary scoringsystem utilized find correct words.work Kaji Watanabe (2006), gloss information WordNetused automatic construction Japanese wordnet. Given English synset,calculates score Japanese translation candidates according glossappended synset. score defined sum correlations translation candidates associated words appear gloss. pair wordsdeemed associated amount mutual information predefined threshold. Since availability bi-lingual corpora limited, iterative approachproposed calculating pair-wise correlations.Another study creating wordnet automatically expanding WordNet describesRomanian wordnet. work Barbu Barbu Mititelu (2005), order identifyRomanian words corresponding WordNet synset, several heuristics proposed. According first heuristic, words related synset share common meaning.Therefore, intersection translations words associated WordNet synsetsconsidered. second heuristic states synset hypernym sharemeaning. Therefore, intersection word translations given WordNet synsethypernym selected Romanian synset. According third heuristic,translations domain label selected given WordNet synset.fourth heuristic, Romanian word selected English translations words baseddefinition maximum similarity words gloss given synset.research conducted Patanakul Charnyote (2005), semi-automatic expanding approach presented construct Thai wordnet. Candidates linksThai words WordNet synsets derived WordNet translations. rank links, 13 criteria used categorized three groups:monosemic, polysemic, structural criteria. Monosemic criteria focus English wordsone meaning assume English words one synsetWordNet. Polysemic criteria focus English words multiple meanings,believe English words multiple synsets WordNet. Structural criteriafocus structural relations among synsets respect wordnet 1.7.Another idea creating wordnet use word-aligned parallel corpus n languages, annotate word lexical sense tag consists n-tuple aligned63fiTaghizadeh & Failiwords. result, occurrences given word text language L consideredsense, provided tagged multi-lingual synset. However, kind corpus easily available languages. research,conducted Oliver Climent (2012), two strategies automatic constructioncorpora proposed: (i) machine translation sense-tagged corpora, (ii)automatic sense tagging bi-lingual word-aligned corpora. results Spanishlanguage showed first strategy works better second. suggestslexical selection errors made machine translation systems less importantsense tagging errors.BabelNet project, undertaken Navigli Ponzetto (2010, 2012a),large multi-lingual semantic network constructed. project, original wordnetused lexicographic resource well Wikipedia pages dierent languagesencyclopedic knowledge. First mapping English Wikipedia pagessynsets original wordnet established. Given Wikipedia page wmapping, Babel synset created using wordnet synset s, page w, inter-languagelinks, translation w languages. project, coverageresulting network analyzed comparing gold-standard wordnetsterms synset coverage, word coverage, synset extra coverage. results showsynset coverage varies dierent languages 52% Italian 86% French.work Bond Foster (2013), open multi-lingual wordneteighty languages developed. project, common interface accessing multiplewordnets created gathering existing freely available wordnets dierent languages automatically linking WordNet. Next, wordnets extendedusing Unicode Common Locale Data Repository (UCLDR) Wiktionary. rankcandidate links WordNet synsets Wiktionary, several similarity measuresemployed. results show precision score 85%-99% measured sense.Arabic wordnet created follows EuroWordNet methodology manuallyencoding set base concepts maximizing compatibility across Arabic Englishwordnets (Black, Elkateb, & Vossen, 2006; Elkateb, Black, Rodrguez, Alkhalifa, Vossen,Pease, & Fellbaum, 2006). Next, project, performed Rodrquez et al.(2008), machine learning algorithm employed extending Arabic wordnetaugmenting formal specification senses synsets. order associate Arabicwords WordNet synsets, Bayesian network four layers proposed. Fourlayers respectively represent: Arabic words; corresponding English translationArabic words first layer; synsets English words second layer;WordNet synsets linked synsets layer three. set candidates word-synsetbuilt pairs <x, y>, x Arabic word WordNet synset thirdlayer Bayesian network non-null probability pathx y. score link calculated posterior probability y, givenevidence provided network. tuples score threshold selectedinclusion final set candidates word-synset. best results methodproposed study noted score 71% precision.work Boudabous et al. (2013), Arabic wordnet enriched via addingsemantic relations synsets. method consisted two main phases; firstphase consisted defining morpho-lexical patterns using study corpora extracted64fiAutomatic Wordnet Development Low-Resource LanguagesArabic Wikipedia. second phase consisted using morpho-lexical patterns, definedprevious phase, order extract new semantic relations Arabic Wikipedia.Extracted relations validated, added Arabic wordnet data base.Piasecki et al. (2011) proposed algorithm automatically expanding Polishwordnet. method uses heterogeneous knowledge sources, extractedlarge corpus, combines based weighted voting scheme. method extractspotential instances lexicon-semantic relations corpus measures semanticsimilarity lexical units. analyzes eect using dierent knowledge resourcesperformance algorithm. Due high accuracy results, approachsaid good basis semi-automatic methods constructing wordnets usinghuman knowledge correct output automatic approaches.Lam et al. (2014) proposed automatic method constructing wordnet synsetsuses publicly available wordnets, machine translator bi-lingual dictionaries.purpose, synset existing wordnet translated target language,ranking method applied resulting translation candidates find besttranslations. generate candidate synsets, three approaches proposed; first onedirectly translates synsets WordNet target language. second one uses intermediate wordnets handle ambiguities synset translations. case dictionariesavailable, addition wordnets intermediate languages, third approachused. experimental results showed resulting wordnets coverage19%, 65%, 37%, 21% 83% Karbi, Arabic, Assamese, Dimasa Vietnameselanguages, respectively.project, conducted Hasanuzzaman et al. (2014), methodconstructing Tempo-wordnet suggested. According method, WordNetaugmented temporal information following two-step process: firststep, synsets WordNet classified atemporal temporal. Next, synsetsassociated past, present future probabilities. obtained Tempo-wordnetused time-related applications.work Shamsfard (2008), semi-automated method proposed developing Persian lexical ontology called FarsNet. 1,500 verbs 1,500 nounsgathered manually make wordnets core. that, two heuristics Word SenseDisambiguation (WSD) method used find likely related Persian synsets.practical evaluation proposed automatic method used studt shows score70% correctness covers 6,500 entries WordNet. extension work(Shamsfard, Hesabi, Fadaei, Mansoory, Famian, Bagherbeigi, Fekri, Monshizadeh, & Assi,2010a), known first published Persian wordnet, FarsNet, contains18,000 Persian words covers 6,500 WordNet synsets.research, performed Montazery Faili (2010), automaticapproach Persian wordnet construction based WordNet introduced.proposed approach uses two mono-lingual corpora English Persian, bilingual dictionary order construct mapping WordNet synsets Persianwords using two dierent methods; links selected directly using heuristicsrecognize links unambiguous. types links ambiguous,scoring method used select appropriate synset. practical evaluation links500 randomly selected Persian words shows 76.4% quality terms accuracy.65fiTaghizadeh & Failiaugmenting Persian wordnet unambiguous words, total accuracyautomatically extracted Persian wordnet becomes 82.6%.3. Iterative Method Wordnet Constructionconstruct multi-lingual wordnet, several methods presented; however,paid attention low-resource languages. Creating wordnet scratchlanguages time-consuming expensive process. Instead, new wordnets coulddeveloped adopting structure existing wordnets languages (usually WordNet) translating words associated synsets target language. Oneimportant advantage approach resulting wordnet aligned WordNet ILI, thus interesting contrastive semantic analysis particularlyuseful multi-lingual tasks multi-lingual information retrieval (Dini, Peters, Liebwald, Schweighofer, Mommers, & Voermans, 2005; Otegi, Arregi, Ansa, & Agirre, 2015)multi-lingual semantic web (Buitelaar & Cimiano, 2014). main assumptionone develop wordnet using expansion approach conceptssemantic relations common among dierent languages. Therefore, language-specificconcepts relations may covered resulting wordnet.general, regardless approach taken, main step toward constructingcomplete wordnet generate synonym sets. section, automatic methodextracting synsets languages limited resources proposed. proposed methodfollows expansion approach; start, wordnet initialized WordNet synsets.every WordNet synset s, translations English words inside extracted bilingual dictionary links translation words WordNet synsets established.Since dictionaries translate word word, word sense word sense, translationsambiguous. Therefore, task score links find incorrect ones. considerscores probability selecting candidate synset word targetlanguage.paper, task finding correct translation words associatedWordNet synsets regarded optimization problem. sensed-tagged corpus similarEnglish SemCor (Landes, Leacock, & Tengi, 1998) exists target language,problem creating wordnet converted maximum likelihood estimation (MLE).English SemCor corpus sense-tagged corpus created Princeton Universitywordnet project research team. corpus consists subset Brown Corpuscontains 700,000 words. SemCor words POS tagged200,000 content words sense-tagged reference WordNet lexical database.Since resources may exist, use word sense disambiguation method findcorrect sense word raw corpus. shown research, conductedMallery (1988), WSD AI-complete problem whose diculty equivalent solvingcentral problems AI. class problems analogous NP-complete problemscomplexity theory, classified dicult problems. proposedidea use iterative algorithm finds local optima problemiterations reasonable time. work regarded extension workperformed Montazery Faili (2011). proposed method adopts66fiAutomatic Wordnet Development Low-Resource LanguagesMono-lingualcorpusextractunique wordsBi-lingualDictionarywextract EnglishtranslationsWordNet(w, e)extractWordNetsynsets(w, s)EM algorithm(w, s, p)Synsetstargetlanguagedeletinglow-rated linksFigure 1: overview proposed approach constructing wordnetwork low-resource languages; method additionally attempts solve majordrawbacks.idea proposed work Montazery Faili (2011) wordnet construction,use bi-lingual dictionary well raw-corpus. First, Farsi wordcorpus, translations extracted bi-lingual dictionary. Next, synsetsEnglish translations considered candidate synsets Farsi word.score calculated pair Farsi words WordNet synsets using expectationmaximization (EM) algorithm. expectation step, use relative-based WSDmethod (PMI), co-occurrence frequency pairs words Farsi languageused disambiguate words corpus. Experimental results showedprecision method varies dierent POS tags. highest precision shownadjectives 89.7%; next adverbs, 65.6%; lowest precisionnouns 61.6%.major drawbacks method calculating co-occurrence pair words target language usually requires large corpus, mayeasily found low-resource languages; important qualityresulting wordnet highly depends co-occurrence values. result, proposechange expectation step PMI-based algorithm WSD procedureperformed without needing additional corpus language resources. Figure 1represents overview proposed method. Next, experimental analysis,re-implement work baseline compare proposed method it.EM iterative algorithm finding maximum likelihood parameters statistical model cases equations cannot directly solved. models typicallyconsist latent variables addition unknown parameters known data observations.is, either missing values among data, model formulatedsimply assuming existence additional unobserved data points. basicidea EM follows:1. actual sucient statistics data, compute parametervalues maximize likelihood data. problem learningprobabilistic model complete data.67fiTaghizadeh & FailiMaximization Stepinitial valuesParameters w,ssense-tagged corpusExpectation StepFigure 2: Expectation-Maximization algorithm wordnet construction2. actually succeed learning model parameters, could computeprobability distribution values missing attributes.case problem, EM algorithm find probability mappingword target language candidate synsets. candidate synset representscorrect sense word target language, expected sense occurscorpus containing word. observed data words corpus targetlanguage; unseen part data WordNet sense tag words.Th EM algorithm switches two stages: 1) finding approximate distributionmissing data given parameters; 2) finding better parameters given approximation. first step known expectation E-step, second stepcalled maximization M-step. Figure 2 represents overview EM algorithmused learning words connected WordNet synsets. Next, details stepproposed algorithm presented.3.1 E-StepSimilar work Montazery Faili (2011), word target language,w, WordNet synset, s, w,s defined probability choosing WordNetsynset word w, P (s|w). words, number times word w appearslarge corpus sense divided total number appearance w. is:w, :w :w,s [0, 1].w,s = 1.(1)(2)step, current values parameters w,s used label corpus sensetags. word w appearing corpus, appropriate sense among candidateWordNet synsets chosen. task, unsupervised cross-lingual wordsense disambiguation (WSD) could employed. WSD algorithms aim resolve wordambiguity without use annotated corpora. Unsupervised WSD well-studied taskliterature. Among these, two categories knowledge-based algorithms gainedpopularity: overlap- graph-based methods. former owns success simpleintuition underlies family algorithms, diusion latter startedgrowing development semantic networks (Basile, Caputo, & Semeraro, 2014).68fiAutomatic Wordnet Development Low-Resource LanguagesWithin graph-based framework WSD, graph built lexical knowledgebase (usually WordNet) representing possible senses word sequencedisambiguated. Graph nodes correspond word senses, whereas edges represent dependencies senses. dependencies include hypernymy, synonymy, antonymy, etc.Next, graph structure analyzed determine importance node. Findingright sense word sequence amounts identifying importantnode among set graph nodes representing candidate senses. main challengegraph-based WSD methods create graph, especially dependencieschosen graphs edges, connectivity measure usedscore nodes graph.research, conducted Navigli Lapata (2010), comprehensivestudy unsupervised graph-based WSD conducted. evaluated wide rangelocal global measures graph connectivity aim isolatingparticularly suited task. Local measures include degree, page-rank, HITS, KPPbetweenness, whereas global measures consist compactness, graph entropy, edgedensity. results indicate local measures yield better performance globalones. best local measures Degree PageRank.task wordnet development, adapt graph-based WSD method presentedwork Navigli Lapata (2010), problem sense labelling corpususing current parameters w,s . assumed true sense wordcorpus determined senses words sentence. every sentencecorpus, following procedure executed:word w sentence, candidate WordNet synsets picked, oneterminal node synset graph created. set terminal nodescalled Vw .terminal node v, depth-first search (DFS) WordNet graph performed. Every time node v Vw (w = w ) along path length L encountered,intermediate nodes edges path v v added graph. Lparameter algorithm usually takes small values 3, 4 5.Terminal nodes graph scored according degree follows: nodev Vw ,deg(v)C(v) =,(3)maxuVw (deg(u))deg(v) number edges terminating v graph G = (V, E):deg(v) = |{(u, v) E : u, v V }|,(4)Relations chosen graphs edges consist lexical semantic relationsdefined WordNet addition gloss relation. pair synsets connectedvia gloss relation unambiguous word w occurs gloss s. wordw must unambiguous; otherwise, connected appropriatesense w (Navigli & Lapata, 2010). use gloss relation WSD procedure, sensedisambiguated glosses WordNet utilized (Semantically Tagged glosses, 2016),69fiTaghizadeh & Failiword forms glosses WordNets synsets manually linked contextappropriate sense WordNet. Therefore, gloss relation established ,appears correct sense word gloss .time complexity calculating degree measure less PageRank,performance shown better; last step WSD procedure,degree measure preferred scoring nodes graph. illustrate stepsWSD procedure, provide example next section.3.1.1 WSD Persian Sentenceorder better understand WSD procedure, example presented. Consider following Persian sentence means Workers thirty years service become retired..|{z}puncJPAJ PAKAK. @Yg K . @X. IYYK| {z } | {z } | {z } | {z } |{z} |{z} | {z } |{z} | {z }verbadjnounnoun noun numnounprepnounPreposition, number punctuation tags involved wordnet/retired sentence. According Aryanignored. Consider word J PAK.pour dictionary, word three translations: emeritus; pensionary; retired. Accordingwordnet 3.0: first translation one noun synset one adjective synset;second one two noun synsets; third one eleven verb synsets oneadjective synset. Since word noun adjective Persian corpus, verbsynsets ignored. definitions synsets follows:{10051861} (noun.person) emeritus#1 (a professor minister retiredassigned duties){01645490} (adj.all) emeritus#1 (honorably retired assigned duties retaining title along additional title emeritus professor emeritus){10414612} (noun.person) pensioner#1, pensionary#1 (the beneficiary pensionfund){10176913} (noun.person) hireling#1, pensionary#2 (a person worksmoney){00035368} (adj.all) retired#1 (no longer active work profession). /retired consists fiveTherefore, candidate set Persian word J PAKsynsets. general, synsets could correct sense sentence.However, POS tag word given sentence come aidWSD procedure order filter synsets. Indeed WSD procedure,70fiAutomatic Wordnet Development Low-Resource LanguagesTable 1: Persian words candidate synsets.Persian wordYJPA@XK .gIYJ PAK.POSnounnounnounnounnounadjectiveverbTranslationsemployee,worker,memberrelieve, own,yearbackground,antecedent,history,record, servicework, job, activity,professionretired, emeritus,wind, grow, lapse,branch, become,candidate synsetsselected synsetcorrect10workern131440have1nyearn1record1n33730job1n3242retired1agrowv337synsets POS given POS sentence involved./retired adjective POS sentence, adjectiveSince word J PAK.synsets involved graphs construction. Following stepswords sentence leads finding candidate synsets wordaccounted WSD graph. Table 1 represents Persian words, translations,number candidate synsets regarding POS tag Persian words.candidate synsets represent terminal nodes WSD graph. Figure 3 shows,candidate synsets Persian word given sentence grouped dottedbox.next step, DFS algorithm run terminal node WordNet graphlength three. Upon finding path one terminal node another,intermediate nodes edges added WSD graph. Part WSD graphshown Figure 3. word graph associated POS, denotedsubscript: n stands noun, v verb, adjective, r adverb. superscriptdenotes sense number associated word WordNet 3.0. graph three/become A/yearseparate components; one component word Kcomponent remaining words. means word given sentence indicatessense words.construction WSD graph, correct sense Persian worddetermined. this, synset degree among candidate set/retired;word chosen correct synset word. Consider word J PAK.1WSD graph Figure 3, node retireda degree one; whereas nodeemeritus1a degree zero. selected sense word retired1a . Usingdegree measure, selected sense word given sentence determined,represented bold box. Table 1 summarizes steps taken WSD proceduregiven sentence. last column shows, selected sense words/become.correct except K. A/background71fiTaghizadeh & Faili.J PAKretired1aworkn3workn1record1nmove2nwind3nhistoryn2photographyn1unf ortunate1ngrown3ancendent1nrelative1njob10njob6nwind1nbe1n...processorn1have1nwind2nperson1njob7nemployee1n......decade1nyearn1period1nyearn3season1nyearn2workern1YJPAjob1n...@XgIYactivityn1traveln1YKoccupation1n employment2nservice5nK .prof ession1nemeritus1a...JPA.J PAKg K . @XAK. @Y. IYFigure 3: Part WSD graph sentence YK3.2 M-Stepmaximization step, new estimation models parameters calculatedbased sense-tagged corpus resulted expectation step. Similarwork Montazery Faili (2011), iteration j, new value parameter w,s ,denotes probability assigning sense tag word w, equal averagingconditional probability P (s|j1 ) dierent occurrences w corpus, j1set parameters w,s iteration j 1. formal notation,nP (si |w1n , j1 )i=1w=w,s=sjw,s=,(5)N (w)jw,sdenotes value w,s iteration j, w1n presents sequence corpus wordsN (w) number occurrence w w1n .iteration EM algorithm, likelihood data given new parametervalues least great likelihood given old ones. EM behaves similargradient descent; step, adjusts parameter values improvelikelihood data. follows EM converges set parameter valueslocally maximizes likelihood.proposed EM method repeated changes probability selectingcandidate synset word target language becomes negligible. So, enditeration, maximum change probabilities computed. value lesst, algorithm stops. execution EM algorithm, links scorethreshold tremove (w,s tremove ) deleted wordnet. Also72fiAutomatic Wordnet Development Low-Resource Languages. /retired per iteration.Table 2: Assigned probabilities word J PAKSynset IDNoun:10051861Adjective:01645490Noun:10414612Noun:10176913Adjective:00035368EntropyCorrect77773Itr #00.20.20.20.20.22.1502Itr #10.111110.298850.111110.111110.367811.8340Itr # 20.111110.083150.111110.111110.583501.7880Itr #30.1111100.111110.111110.666661.7797Itr #40.1111100.111110.111110.666661.7781Itr #50.1111100.111110.111110.666661.7768iteration, links current score ignored corresponding sensespresented graphs construction WSD procedure. end,words target language mapped onto synset WordNet makesynsets resulting wordnet.better follow process updating probabilities word per iteration,example presented here. demonstrating probability adjustment iteration,/retired. expectation step, words corpusconsider word J PAK.disambiguated. Next maximization step, new value probabilitiescomputed. Table 2 represents probabilities synsets assigned word J PAK./retired iteration. first second columns show synset IDcorrection synsets specified word, respectively. following columns representprobability values first five iterations. Values less 0.005 considered0. table shows probabilities start uniformly; iteration,probability correct synsets increases probability incorrect synsetsfrequent enough corpus decreases change. Indeed,/retired corpus, taggednumber occurrences word J PAK.specific WordNet sense iteration iteration 1, probability/retired change iteration i.sense given word J PAK.value becomes greater, probability increases value becomes smaller,probability decreases. particular example, five iterations, synset achievinghighest probability correct synset. iteration three, probability word. /retired assigned second synset goes 3.9E-7,J PAKthreshold. next iterations, synset considered WSD procedureprobability zero. last row table presents entropy valuerespect iteration. steady decrease entropy indicates iteration,distinction candidates synsets word becomes clear, leadsidentification correct synsets. subject analysis entropy wordper iteration discussed later Section 4.2.1.4. Case Study: Persian Languagesection, proposed method automatic wordnet construction appliedPersian low-resource language. following subsections, experimental setupevaluation methods described; that, results presented.73fiTaghizadeh & Faili4.1 Experimental Setup Datasection, required resources setup experiments explained1 .construct wordnet Persian language, Bijankhan Persian corpus2used. collection gathered daily news common texts,documents categorized dierent subjects political, cultural on.Bijankhan contains ten million manually-tagged words tag set containing 550fine-grained Persian POS tags (Oroumchian, Tasharofi, Amiri, Hojjat, & Raja, 2006).Although POS tags explicitly used proposed method, get better WSDresults, one use POS tags prune synsets along tags candidateset word explained Section 3.1.1. result, WSD procedure,synsets POS tag word corpus taken part. WordNet,four categories tags included: noun, verb, adverb adjective. Thus wordscorpus tags pronoun preposition ignored.Bijankhan large corpus. low-resource languages may largecorpus. order evaluate behaviour proposed method corpus sizelimited, part Bijankhan picked training Persian wordnet.PMI-based graph-based method conducted using part. partincludes nearly 13% total size corpus. remaining 87% usedtesting phase coverage wordnet corpus evaluated.details coverage analysis presented Section 4.2.4. Also, complete analysiseect corpus size quality final wordnet presented Section4.4.words corpus appear inflected forms may founddictionary. Therefore beginning proposed algorithm, lemmatizerused dierent inflected forms words converted base form.example, plural nouns converted singular form. this, STeP-1 tool(Shamsfard, Jafari, & Ilbeygi, 2010b) utilized. STeP-1 package setfundamental tools Persian text processing provides support tokenization, spellchecking, morphological analysis, POS tagging.Another required resource proposed method bi-lingual machine readabledictionary. electronic version Aryanpour dictionary3 used extractEnglish equivalent Bijankhan words. Also, WordNet version 3.0 usedextract synsets English equivalents.WSD procedure, context word sentence containing word.depth-first search WSD performed maximum depth 3 similarwork Navigli Ponzetto (2012b). mentioned Section 3.2, probabilityWordNet sense given word w less equal t, sense ignoredWSD process EM algorithm. experiments, set = 0.005.1. source code freely available download http://ece.ut.ac.ir/en/node/9402. See http://ece.ut.ac.ir/dbrg/bijankhan/3. See http://www.aryanpour.com74fiAutomatic Wordnet Development Low-Resource LanguagesIterationEntropyTable 3: Entropy values respect iteration0123452.15025 1.83406 1.78804 1.77978 1.77813 1.7768061.776774.2 Evaluation Resultssection, results evaluation proposed method various experimentspresented.4.2.1 Convergence Proposed MethodEM algorithm iterates expectation maximization steps,criteria satisfied. experiment, iteration, entropy synset probabilities per word calculated average entropy words considered.changing value two consecutive iterations becomes near zero, EM algorithm stops. Formally, entropy probability distribution defined equation6:H(w) =w,s log(w,s ).(6)Entropy best understood measure uncertainty, entropy largerrandom values. Indeed first, links Persian word equal probability,maximum entropy granted. iteration, links sink thresholdprobability thus probability links increases. expected finalstep incorrect links obtain low probability correct links obtainhigh probability. Therefore, entropy analysis demonstrate behaviour EMmethod changing probabilities. Table 3, entropy values per iteration shown.iteration 6, changing entropy values reaches predetermined threshold 0.001EM algorithm stops.4.2.2 Precision Recall Wordnetprimary goal work construct high quality wordnet low-resourcelanguages. execution EM algorithm, probability assigning candidatesynset word target language finalized. probabilities sortedlinks probability threshold tremove removed finalwordnet. value tremove determines size wordnet aects qualitywordnet. So, experiments conducted used dierent values tremoveincluding 0.1, 0.05, 0.02, 0.01, 0.005 0.0.evaluate resulting wordnet, re-implemented PMI-based method (Montazery & Faili, 2011) compared wordnet baseline. experiments,wordnet referred graph-based wordnet, contrast PMI-basedwordnet. evaluation process, two data sets used: 1) FarsNet 2) Manual judges.FarsNet semi-manually created wordnet Persian, available two versions;second release FarsNet contains 36,000 Persian words phrasesorganized 20,000 synsets nouns, adjectives, adverbs verbs. FarsNet 275fiTaghizadeh & Failialso inter-lingual relations connect Persian synsets English onesPrinceton wordnet 3.0.second data set consists subset 1,750 links resulting wordnet,selected randomly judged manually. link (w, s) given two annotatorsdecide Persian word w semantically equal WordNet s. ensureaccuracy judges, annotators selected among people native speakersPersian time learn English professionally. case disagreement two judges, third annotator asked decide link. inter-annotatoragreement 80%, means 80% judgements, two annotators agreed.Additionally, computed Cohens Kappa coecient (Cohen, 1960), two annotators,takes account amount agreement could expected occurchance. Kappa computed follows:=po pe,1 pe(7)po relative observed agreement among annotators, pe hypotheticalprobability chance agreement. two annotators, Kappa value 0.55.general, annotators complete agreement, = 1. agreementannotators would expected chance (as given pe ), 0.carrying manual judgements, precision recall resulting wordnetmeasured set.precision resulting wordnet defined number correct linkswordnet also exist test data correct links, divided total number linkswordnet exist test data. Also, recall wordnet definednumber correct links wordnet also exist test set correct links, dividedtotal number correct links test set. accuracy wordnet anothermeasure, defined number correct links wordnet also existtest set plus number incorrect links test set exist wordnet,divided total number links test set. definitions precision, recall,accuracy wordnet also used BabelNet project (Navigli & Ponzetto,2010).Figure 4a Figure 4b represent precision recall PMI-based methodproposed method according FarsNet. shown, precision recallwordnet better PMI-based method. figures, precision 18%,seems low wordnet considered reliable resource language.Additionally, recall 49%. due lack correct links FarsNet.evaluation resulting wordnet according FarsNet link (w, s) placedone categories:Persian word w exist FarsNet. link ignored counted.Persian word w exists FarsNet; however WordNet synset given it.link ignored counted.Persian word w exists FarsNet least one WordNet synset given it.one WordNet synsets, link counted correct else countedincorrect.76fiAutomatic Wordnet Development Low-Resource LanguagesWordNet sense distinctions fine-grained, meaning several WordNetsynsets may mapped onto one synset FarsNet; givenFarsNet. Therefore, correct links wordnet counted incorrect. Figure 4cshows accuracy wordnets according FarsNet, shows graph-basedwordnet surpasses PMI-based wordnet.reasons low precision according FarsNet follows:Translations Persian words inaccurate incomplete, meaningcorrect WordNet synset according FarsNet exist candidate set.J/motaalleqAt/possession, three equivalentexample, Persian word HAEnglish words written Aryanpour dictionary: Appurtenance, Paraphernalia,J/possession determinedBelongings. wordnet, correct synsets HAfollows: {13244109} (noun.possession), property#1, belongings#1, holding#2(something owned; tangible intangible possession owned someone;hat property; man property). However according FarsNet,correct synset {00032613} (noun.Tops) possession#2 (anything ownedJ/possession synset Noun-02671421possessed). evaluation, link HAconsidered incorrect penalized.Persian word lemmatized correctly; English translations consequently candidate set contain correct synset. example, Persianword @P K. /bArAk/Barak proper noun, stemmer recognizes PA K./bAr/load stem, means load.resolve problems, set manually judged links used secondexperiment. Figure 5 represents precision recall resulting wordnet dierentvalues tremove according manual judges. Parameter tremove demonstrates threshold,links score lower deleted final wordnet. Highvalues tremove result wordnet high precision low recall. hand,low values tremove cause low precision high recall wordnet. Thus trade-oprecision recall. tremove = 0.1, precision PMI-based wordnet86%, precision wordnet created proposed method 90% accordingmanual judges. tremove = 0, means links contained finalwordnet, precision 74%. Therefore, initial wordnet seen without executingEM algorithm 74% precision. Figures 4d 5c show another quality measurewordnets, F -measure. Definition F -measure complete analysispresented Section 4.3.4.2.3 Size Polysemy Rate WordnetOne important aspects wordnets size. Large wordnets may tensthousands sysnsets (Miller, 1995; Patanakul & Charnyote, 2005; Black et al., 2006;Piasecki et al., 2011). hand, wordnets polysemic wordsuseful NLP IR tasks. Polysemic words words onesense wordnet. Finding correct sense polysemic words great significanceautomatic wordnet construction.77fiTaghizadeh & FailiGraph-basedPMI-based0.15RecallPrecision0.80.6Graph-basedPMI-based0.102 102 4 102 6 102 8 1020.40.102 102 4 102 6 102 8 102tremove0.1tremove(a) Precision(b) Recall0.8F1Accuracy0.250.70.6Graph-basedPMI-based0.502 102 4 102 6 102 8 1020.2Graph-basedPMI-based0.150.102 102 4 102 6 102 8 102tremove0.1tremove(c) Accuracy(d) F-measureFigure 4: Comparison wordnets according FarsNet.0.8Recall0.85Graph-basedPMI-based0.802 102 4 102 6 102 8 102Graph-basedPMI-based0.60.40.102 102 4 102 6 102 8 102tremovetremove(a) Precision(b) Recall0.8Graph-basedPMI-based0.7F1Precision0.90.60.502 102 4 102 6 102 8 1020.1tremove(c) F-measureFigure 5: Comparison wordnets according manual judges.780.1fiAutomatic Wordnet Development Low-Resource LanguagesTable 4: Comparison size wordnetsThreshold0.10.050.020.010.0050PMI-based wordnetunique words word-synset polysemy11,88027,3580.6311,96936,9220.7111,97449,0700.7611,97458,8740.7811,97471,7610.8011,974141,1030.85Graph-based wordnetunique words word-synset polysemy11,89929,9440.7311,97243,6900.7911,97261,8230.8011,97274,6190.8011,97286,8790.8311,972141,1030.85section, size resulting wordnet polysemy rate two wordnets,PMI-based graph-based wordnets, reported. Table 4 presents number uniquewords, number Persian word-WordNet synset links proportion polysemic words based dierent values tremove . tremove decreases 0.1 0.01,unique words contained wordnets, number word-synset links increases,also proportion polysemic words unique words wordnet increases.seen, wordnet created result graph-based method surpasses PMI-basedwordnet.links included wordnet, polysemic words 85% uniquewords. However, wordnet, removing links probability less0.1, 73% words polysemic, 10% better PMI-base wordnet.tremove = 0.1, wordnets 12,000 unique words. Since methodsexecuted corpus, significant dierence sizes.4.2.4 Coverage Wordnetevaluate coverage resulting wordnet, interested observing coverageWordNet synsets also coverage language words. section, threeexperiments performed: 1) core concepts coverage, 2) WordNet synset coverage,3) corpus coverage.first experiment, coverage wordnet core synsets evaluated. BoydGraber et al. (2006) published list 5,000 word-senses WordNet 3.0,contains 5,000 frequently used word-senses (Core WordNet, 2015). Coveragewordnet list regarded covering common conceptslanguage. core wordnet used measure percentage synsets listcovered PMI-based graph-based wordnets. Figure 6a represents core coveragedierent values tremove . Selecting links, (tremove = 0), causes coverage 88%core wordnet, choosing links probable 0.1, leads coverage53% 34% core wordnet graph-based PMI-based wordnets, respectively.second experiment, coverage wordnets WordNet synsets studied.Since resulting wordnet multi-lingual wordnet, coverage WordNetsynsets measure quality. Figure 6b represents coverage PMI-basedgraph-based wordnets WordNet 3.0 synsets dierent values tremove . figureshows graph-based wordnet covers WordNet synsets PMI-based wordnetvalues tremove . example, selecting links probability higher 0.1,79fiGraph-basedPMI-based0.8Core CoverageWordNet synsets CoverageTaghizadeh & Faili0.60.402 102 4 102 6 102 8 1020.10.25Graph-basedPMI-based0.20.150.12 102 4 102 6 102 8 1020tremove0.1tremove(a) Core Coverage(b) WordNet CoverageFigure 6: Coverage wordnets core synsets synsets WordNet.Table 5: Comparison coverage wordnets.FarsNetPMI-based wordnetgraph-based wordnetCoverage Bijankhan (unique words)3,05011,52311,543graph-based wordnet covers 14% WordNet synsets; PMI-based wordnetcovers 10% WordNet synsets.third experiment, coverage wordnets Bijankhan corpus evaluated.Bijankhan large corpus proposed method trained 13% it. restcorpus used measuring word coverage wordnets. Table 5 demonstratesnumber unique words corpus, covered PMI-based graph-based wordnets,tremove = 0.1. evaluation also performed FarsNet baselinealso presented Table 5. Although training testing corpus separate,significance dierence FarsNet EM-based wordnets coverage.4.3 Parameter Selectionproposed method wordnet construction convergence EM algorithm,set links words target language synsets source languageobtained. links scored lower threshold tremove removed finalwordnet. previous experiments showed, value tremove aects qualityresulting wordnet. experiments section 4.2.2 illustrated changing tremove0.005 0.1 positive eect precision negative eect recallresulting wordnet. Indeed, trade-o precision recall.question may arise; best value tremove .section, F -measure used investigate quality wordnet, consideringprecision recall. formula F1 follows:F1 = 2.precision.recall,precision + recall80(8)fiAutomatic Wordnet Development Low-Resource LanguagesF1 harmonic mean precision recall. order gain insightoptimum value tremove , F1 resulting wordnet calculateddierent values tremove . precision recall, F1 calculatedmanual judgement FarsNet. Figure 5c shows F1 decreases 77%50% tremove increases 0.005 0.1 graph-based wordnet accordingmanual judgement. means precision value importantrecall rate precision decreasing higher rate recallincreasing. Therefore, gain precise wordnet, increase tremove ; however,must accept loosing recall.hand, Figure 4d shows highest value F1 graph-basedwordnet obtained tremove = 0.1 according FarsNet. fact meansrecall value eect F1 precision value. reason dierencedue low precision values obtained evaluation accordingFarsNet, reported Section 4.2.2. FarsNet lacks correct mappingsPersian words WordNet synsets. Indeed wordnet construction, precisionfinal wordnet important recall.Finally, choosing threshold tremove important eect quality resultingwordnet. However, matter depends application. applications,precise wordnet preferential large accurate enough one.cases, greater values tremove preferential. Although, applications highrecall needed, one choose low values tremove .4.4 Eect Corpus Size Dictionarysection, eect required resources final wordnet looked at.proposed method needs bi-lingual dictionary mono-lingual corpus. previousexperiments, Aryanpour dictionary Bijankhan corpus used. SinceBijankhan large corpus 13% used previous experiments.investigate eect corpus size quality resulting wordnet, proposedmethod executed using four sizes Bijankhan: 5%, 10%, 20% 50%.Additionally, examine eect dictionary quality final wordnet,Google translator4 used another experiment instead Aryanpour; resultingwordnet compared wordnet created using Aryanpour sizeBijankhan. link removal threshold tremove experiments section0.1. resulting wordnets evaluated precision, recall, accuracy, coverageWordNet core synsets, coverage synsets WordNet, numberPersian words.shown Figure 7, size corpus increases 5% 50%Bijankhan corpus using dictionary, measures increase except precision,either change changes slightly. result beyond expectation. Indeed, precision resulting wordnet depends precision WSDprocedure depend size corpus. However, new possible senseswords discovered increasing size corpus therefore recall,accuracy, coverage size wordnet increase growth corpus size.4. http://translate.google.com/81fiTaghizadeh & FailiAryanpour dictionaryGoogle translator0.50.380.90.89accuracyrecallprecision0.360.340.320.480.460.30.280.885 10205 1050205 105020(a) precision50sizesizesize(b) recall(c) accuracy1040.560.161.40.520.50.480.46number wordssynset coveragecore coverage0.540.140.121.210.10.80.445 1020505 1020size(d) core coverage50size(e) WordNet synset coverage5 102050size(f) number wordsFigure 7: Evaluation resulting wordnet trained dierent sizes Bijankhan.Figure 7f demonstrates, wordnet least 10,000 words, corpus sizeleast 10% Bijankhan corpus. Figure 7 also illustrates wordnet trainedAryanpour dictionary excels wordnet derived Google translator.experiment demonstrates dictionary heavily aects final wordnet evencorpus size. result, small corpus large dictionary resultsprecise wordnet large corpus small dictionary.last experiment, proposed method executed using full Bijankhancorpus Aryanpour dictionary. precision, recall accuracy resultingwordnet 90%, 41% 52%, respectively. Comparing wordnet, createdusing 13% Bijankhan dictionary, recall accuracy increased 6%3%, accordingly; precision change. wordnet 15,406 Persianword covers 61% core synsets WordNet. Considering synsetsWordNet, covers 20% them.5. Conclusionpaper, EM algorithm employed order develop wordnet lowresourced languages. successfully applied unsupervised cross lingual WSD expectation step algorithm. proposed method use features specific82fiAutomatic Wordnet Development Low-Resource Languagestarget language, used languages generate wordnets.Resources needed proposed algorithm include bi-lingual dictionary monolingual corpus. proposed method belongs expansion approach createsmulti-lingual wordnet word target language, equivalent synsetWordNet known.proposed method applied Persian language quality resulting wordnet examined several experiments. precision 18% accordingFarsNet 90% according manual judgement. reason dierenceWordNet synsets fine-grained comparison FarsNet synsets,synsets FarsNet mapped onto one synset WordNet;however FarsNet provides one two WordNet synsets FrasNet synsets.problem means correct links resulting wordnet consideredincorrect thus reported precision becomes low. Also, resulting wordnetcontains 12,000 words Persian language using 13% Bijankhancorpus, several wordnets languages. Additionally, 53% coresynsets 14% synsets WordNet covered. Analysis eects corpussize dictionary size resulting wordnet showed dictionary size aectprecision wordnet corpus size therefore important uselarge-enough dictionaries.Acknowledgementsresearch part supported Institute Research Fundamental Sciences(No. CS1395-4-19).ReferencesApidianaki, M., & Sagot, B. (2014). Data-driven synset induction disambiguationwordnet development. Language Resources Evaluation, 48 (4), 655677.Atserias, J., Climent, S., Farreres, X., Rigau, G., & Rodrguez, H. (2000). Combiningmultiple methods automatic construction multilingual wordnets. Amsterdamstudies theory history linguistic science series 4, 327340.Barbu, E., & Barbu Mititelu, V. (2005). case study automatic building wordnets.Proceedings OntoLex 2005- Ontologies Rexical Resources, pp. 8590, JejuIsland, Korea. Asian Federation Natural Language Processing.Basile, P., Caputo, A., & Semeraro, G. (2014). enhanced lesk word sense disambiguationalgorithm distributional semantic model. Proceedings COLING 2014,25th International Conference Computational Linguistics: Technical Papers,pp. 15911600, Dublin, Ireland. International Committee Computational Linguistics.Black, W., Elkateb, S., & Vossen, P. (2006). Introducing Arabic wordnet project.Proceedings Third International WordNet Conference (GWC-06), pp. 295299,South Jeju Island, Korea. Global WordNet Association.83fiTaghizadeh & FailiBond, F., & Foster, R. (2013). Linking extending open multilingual wordnet. Proceedings 51st Annual Meeting Association Computational Linguistics,pp. 13521362, Sofia, Bulgaria. Association Computational Linguistics.Bond, F., Isahara, H., Kanzaki, K., & Uchimoto, K. (2008). Boot-strapping wordnet usingmultiple existing wordnets. Proceedings Sixth International ConferenceLanguage Resources Evaluation (LREC08), pp. 16191624, Marrakech, Morocco.European Language Resources Association (ELRA).Boudabous, M. M., Chaaben Kammoun, N., Khedher, N., Belguith, L. H., & Sadat, F.(2013). Arabic wordnet semantic relations enrichment morpho-lexical patterns. Proceeding 1st International Conference Communications, Signal Processing, Applications (ICCSPA), pp. 16, American University Sharjah,United Arab Emirates. IEEE.Boyd-Graber, J., Fellbaum, C., Osherson, D., & Schapire, R. (2006). Adding dense, weightedconnections WordNet. Proceedings third International WordNet Conference (GWC-06), pp. 2935, South Jeju Island, Korea. Global WordNet Association.Buitelaar, P., & Cimiano, P. (2014). Towards Multilingual Semantic Web. SpringerBerlin Heidelberg.Cohen, J. (1960). coecient agreement nominal scales. Educational Psychological Measurement, 20 (1), 3746.CoreWordNet (2015)core-wordnet.txt.http://wordnetcode.princeton.edu/standoff-files/Diab, M. (2004). feasibility bootstrapping Arabic wordnet leveraging parallelcorpora English WordNet. Proceedings Arabic Language TechnologiesResources, Cairo, NEMLAR.Dini, L., Peters, W., Liebwald, D., Schweighofer, E., Mommers, L., & Voermans, W. (2005).Cross-lingual legal information retrieval using WordNet architecture. Proceedings10th international conference Artificial intelligence law (ACAIL), pp.163167, Bologna, Italy. ACM.Elkateb, S., Black, W., Rodrguez, H., Alkhalifa, M., Vossen, P., Pease, A., & Fellbaum, C.(2006). Building wordnet Arabic. Proceedings 5th international conference Language Resources Evaluation (LREC 2006), Genoa, Italy. EuropeanLanguage Resources Association (ELRA).Erjavec, T., & Fiser, D. (2006). Building Slovene wordnet. Proceedings 5th International Conference Language Resources Evaluation (LREC 2006), Genoa,Italy. European Language Resources Association (ELRA).Fellbaum, C., & Vossen, P. (2012). Challenges multilingual wordnet. Language Resources Evaluation, 46 (2), 313326.Fiser, D. (2009). Human language technology. Leveraging Parallel Corpora ExistingWordnets Automatic Construction Slovene Wordnet, pp. 359368. SpringerBerlin Heidelberg.84fiAutomatic Wordnet Development Low-Resource LanguagesGunawan, G., & Saputra, A. (2010). Building synsets Indonesian wordnet monolingual lexical resources. Proceedings International Conference Asian LanguageProcessing (IALP), pp. 297300, Harbin, China. IEEE.Hasanuzzaman, M., Caen, F., Dias, G., Ferrari, S., & Mathet, Y. (2014). Propagation strategies building temporal ontologies. Proceedings 14rd conference EuropeanChapter Association Computational Linguistics, pp. 611, Guthenburg, Sweden. Association Computational Linguistics.Kaji, H., & Watanabe, M. (2006). Automatic construction Japanese wordnet. Proceedings 5th International Conference Language Resources Evaluation(LREC 2006), Genoa, Italy. European Language Resources Association (ELRA).Kazakov, D., & Shahid, A. R. (2009). Unsupervised construction multilingual wordnetparallel corpora. Proceedings Workshop Natural Language ProcessingMethods Corpora Translation, Lexicography, Language Learning, pp. 912,Borovets, Bulgaria. Association Computational Linguistics.Lam, K. N., Al Tarouti, F., & Kalita, J. (2014). Automatically constructing wordnet synsets.52nd Annual Meeting Association Computational Linguistics (ACL 2014),pp. 106111, Baltimore, USA. Association Computational Linguistics.Landes, S., Leacock, C., & Tengi, R. I. (1998). Building semantic concordances. WordNet:electronic lexical database, 199 (216), 199216.Mallery, J. C. (1988). Thinking foreign policy: Finding appropriate role artificially intelligent computers. Ph.D. thesis, MIT Political Science Department.Miller, G. A. (1995). WordNet: lexical database english. Communications ACM,38 (11), 3941.Montazery, M., & Faili, H. (2010). Automatic Persian wordnet construction. Proceedings23rd International Conference Computational Linguistics: Posters, pp. 846850, Beijing, China. Association Computational Linguistics.Montazery, M., & Faili, H. (2011). Unsupervised learning Persian wordnet construction.Proceedings Recent Advances Natural Language Processing (RANLP), pp.302308, Hissar, Bulgaria. Association Computational Linguistics.Navigli, R. (2009). Word sense disambiguation: survey.(CSUR), 41 (2), 10.ACM Computing SurveysNavigli, R., & Lapata, M. (2010). experimental study graph connectivity unsupervised word sense disambiguation. Pattern Analysis Machine Intelligence, IEEETransactions on, 32 (4), 678692.Navigli, R., & Ponzetto, S. P. (2010). BabelNet: Building large multilingual semantic network. Proceedings 48th annual meeting association computational linguistics, pp. 216225, Uppsala, Sweden. Association ComputationalLinguistics.Navigli, R., & Ponzetto, S. P. (2012a). BabelNet: automatic construction, evaluationapplication wide-coverage multilingual semantic network. Artificial Intelligence, 193, 217250.85fiTaghizadeh & FailiNavigli, R., & Ponzetto, S. P. (2012b). Multilingual WSD lines code:BabelNet API. Proceedings 50th Annual Meeting Association Computational Linguistics (ACL 2012), pp. 6772, Jeju, Republic Korea. AssociationComputational Linguistics.Oliver, A., & Climent, S. (2012). Parallel corpora wordnet construction: machine translation vs. automatic sense tagging. Proceedings 13th International ConferenceIntelligent Text Processing Computational Linguistics, pp. 110121, New Delhi,India. Springer.Oroumchian, F., Tasharofi, S., Amiri, H., Hojjat, H., & Raja, F. (2006). Creating feasiblecorpus Persian POS tagging. Tech. rep. TR3/06, University Wollongong, Dubai.Otegi, A., Arregi, X., Ansa, O., & Agirre, E. (2015). Using knowledge-based relatednessinformation retrieval. Knowledge Information Systems, 44 (3), 689718.Patanakul, S., & Charnyote, P. (2005). Construction Thai wordnet lexical databasemachine readable dictionary. Conference Proceedings: tenth MachineTranslation Summit, pp. 8792, Phuket, Thailand. Language Technology World.Piasecki, M., Kurc, R., & Broda, B. (2011). Heterogeneous knowledge sources graph-basedexpansion Polish wordnet. Intelligent Information Database Systems,Vol. 6591, pp. 307316. Springer.Prabhu, V., Desai, S., Redkar, H., Prabhugaonkar, N., Nagvenkar, A., & Karmali, R. (2012).ecient database design IndoWordNet development using hybrid approach.Proceedings 3rd Workshop South Southeast Asian Natural LanguageProcessing (SANLP), pp. 229236, Mumbai, India. International Committee Computational Linguistics.Rodrquez, H., Farwell, D., Ferreres, J., Bertran, M., Alkhalifa, M., & Mart, M. A.(2008). Arabic wordnet: Semi-automatic extensions using Bayesian inference.Proceedings Sixth International Conference Language Resources Evaluation (LREC08), Marrakech, Morocco. European Language Resources Association(ELRA).Saveski, M., & Trajkovski, I. (2010). Automatic construction wordnets using machine translation language modeling. Proceedings 13th InternationalMulticonference, pp. 7883, Ljubljana, Slovenia. Information Society.Semantically Tagged glosses (2016) http://wordnet.princeton.edu/glosstag.shtml.Shamsfard, M. (2008). Towards semi automatic construction lexical ontology Persian. Proceedings 6th International Conference Language ResourcesEvaluation (LREC 2008), Marrakech, Morocco. European Language Resources Association (ELRA).Shamsfard, M., Hesabi, A., Fadaei, H., Mansoory, N., Famian, A., Bagherbeigi, S., Fekri, E.,Monshizadeh, M., & Assi, S. M. (2010a). Semi automatic development FarsNet;Persian wordnet. Proceedings 5th Global WordNet Conference, Mumbai, India.Global WordNet Association.86fiAutomatic Wordnet Development Low-Resource LanguagesShamsfard, M., Jafari, H. S., & Ilbeygi, M. (2010b). STeP-1: set fundamental toolsPersian text processing. Proceedings 7th International Conference Language Resources Evaluation (LREC 2010), Valletta, Malta. European LanguageResources Association (ELRA).Tufis, D., Cristea, D., & Stamou, S. (2004). BalkaNet: Aims, methods, results perspectives. general overview. Romanian Journal Information Science Technology,7 (1-2), 943.Vossen, P. (1998). Introduction EuroWordNet. EuroWordNet: multilingual databaselexical semantic networks, pp. 117. Springer.87fi